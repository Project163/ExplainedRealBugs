<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 20:28:47 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-6883] Serializer for collection of Scala case classes are generated with different anonymous class names in 1.3</title>
                <link>https://issues.apache.org/jira/browse/FLINK-6883</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;p&gt;In the Scala API, serializers are generated using Scala macros (via the &lt;tt&gt;org.apache.flink.streaming.api.scala.createTypeInformation(..)&lt;/tt&gt; util).&lt;br/&gt;
The generated serializers are inner anonymous classes, therefore classnames will differ depending on when / order that the serializers are generated.&lt;/p&gt;

&lt;p&gt;From 1.1 / 1.2 to Flink 1.3, the generated classnames for a serializer for a collections of case classes (e.g. &lt;tt&gt;List&lt;span class=&quot;error&quot;&gt;&amp;#91;SomeUserCaseClass&amp;#93;&lt;/span&gt;&lt;/tt&gt;) will be different. In other words, the exact same user code written in the Scala API, compiling it with 1.1 / 1.2 and with 1.3 will result in different classnames.&lt;/p&gt;

&lt;p&gt;This is problematic for restoring older savepoints that have Scala case class collections in their state, because the old serializer cannot be recovered (due to the generated classname change).&lt;/p&gt;

&lt;p&gt;For now, I&apos;ve managed to identify that the root cause for this is that in 1.3 the &lt;tt&gt;TypeSerializer&lt;/tt&gt; base class additionally extends the &lt;tt&gt;TypeDeserializer&lt;/tt&gt; interface. Removing this extending resolves the problem. The actual reason for why this affects the generated classname is still being investigated.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13078850">FLINK-6883</key>
            <summary>Serializer for collection of Scala case classes are generated with different anonymous class names in 1.3</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="tzulitai">Tzu-Li (Gordon) Tai</assignee>
                                    <reporter username="tzulitai">Tzu-Li (Gordon) Tai</reporter>
                        <labels>
                            <label>flink-rel-1.3.1-blockers</label>
                    </labels>
                <created>Sat, 10 Jun 2017 11:09:49 +0000</created>
                <updated>Tue, 23 Jan 2018 14:20:35 +0000</updated>
                            <resolved>Tue, 13 Jun 2017 05:54:04 +0000</resolved>
                                    <version>1.3.0</version>
                                    <fixVersion>1.3.1</fixVersion>
                    <fixVersion>1.4.0</fixVersion>
                                    <component>API / Scala</component>
                    <component>API / Type Serialization System</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="16045980" author="githubbot" created="Sun, 11 Jun 2017 13:50:29 +0000"  >&lt;p&gt;GitHub user tzulitai opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4103&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4103&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-6883&quot; title=&quot;Serializer for collection of Scala case classes are generated with different anonymous class names in 1.3&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-6883&quot;&gt;&lt;del&gt;FLINK-6883&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;scala, state&amp;#93;&lt;/span&gt; Fix restore of Scala type states&lt;/p&gt;

&lt;p&gt;    This PR is based on #4090. Together with #4090 as a whole, this PR fixes restoring savepoints of Scala type states.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;What this fixes&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;    This PR fixes the problem that the exact same job code written with the Flink Scala API using Scala types as state, will generate different classnames for the anonymous classed serializers of case classes / collection types when compiled against pre-1.3 version and Flink 1.3.&lt;/p&gt;

&lt;p&gt;    The root cause of this is that prior to this PR in 1.3, the `TypeSerializer` base class additionally implements the `TypeDeserializer` interface. This alters Scala compiler&apos;s generation order of the anonymous serializer classes, and therefore ends up in different generated names.&lt;/p&gt;

&lt;p&gt;    To fix this, the `TypeSerializer` base now no longer implements `TypeDeserializer`, while not affecting any user-facing interfaces of the serializer compatibility functionality (i.e. `CompatibilityResult`, `TypeSerializer#snapshotConfiguration`, `TypeSerializer#ensureCompatibility` interfaces are not broken).&lt;/p&gt;

&lt;p&gt;    With this fix, we can at least guarantee that Scala jobs with Scala type states will be able to be restored across Flink majors versions, when 1) the same compiler is used, and 2) the user code is remained untouched (invocation order of the Scala `createTypeInformation` macro remains the same).&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Tests&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;    This PR also adds tests to guard against future problems like this. Includes:&lt;br/&gt;
    1. A `ScalaSerializersMigrationTest` to guard against different generated classnames for anonymous serializers across changes to the codebase. The tested classnames are what they were in Flink 1.1 and 1.2.&lt;br/&gt;
    2. A `scala.StatefulJobSavepointITCase` to test end-to-end migration from 1.2.x / 1.3.x for Scala jobs. The 1.2 savepoints were generated under the `release-1.2` branch.&lt;/p&gt;



&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/tzulitai/flink&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/tzulitai/flink&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-6883&quot; title=&quot;Serializer for collection of Scala case classes are generated with different anonymous class names in 1.3&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-6883&quot;&gt;&lt;del&gt;FLINK-6883&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4103.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4103.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #4103&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit c9696c20d61ecba26fc19b4a7cdbb16586d30894&lt;br/&gt;
Author: Tzu-Li (Gordon) Tai &amp;lt;tzulitai@apache.org&amp;gt;&lt;br/&gt;
Date:   2017-06-08T06:52:04Z&lt;/p&gt;

&lt;p&gt;    &lt;span class=&quot;error&quot;&gt;&amp;#91;hotfix&amp;#93;&lt;/span&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;scala&amp;#93;&lt;/span&gt; Fix instantiation of Scala serializers&apos; config snapshot classes&lt;/p&gt;

&lt;p&gt;    Prior to this commit, the configuration snapshot classes of Scala&lt;br/&gt;
    serializers did not have the proper default empty constructor that is&lt;br/&gt;
    used for deserializing the configuration snapshot.&lt;/p&gt;

&lt;p&gt;    Since some Scala serializers&apos; config snapshots extend the Java&lt;br/&gt;
    CompositeTypeSerializerConfigSnapshot, their config snapshot classes are&lt;br/&gt;
    also changed to be implemented in Java since in Scala we can only call a&lt;br/&gt;
    single base class constructor from subclasses.&lt;/p&gt;

&lt;p&gt;commit 7e20e6251385e04334ec0f06dbaa5f1f0315b530&lt;br/&gt;
Author: Tzu-Li (Gordon) Tai &amp;lt;tzulitai@apache.org&amp;gt;&lt;br/&gt;
Date:   2017-06-08T13:29:45Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-6869&quot; title=&quot;Scala serializers do not have the serialVersionUID specified&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-6869&quot;&gt;&lt;del&gt;FLINK-6869&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;scala&amp;#93;&lt;/span&gt; Specify serialVersionUID for all Scala serializers&lt;/p&gt;

&lt;p&gt;    Previously, Scala serializers did not specify the serialVersionUID, and&lt;br/&gt;
    therefore prohibited restore from previous Flink version snapshots&lt;br/&gt;
    because the serializers&apos; implementations changed.&lt;/p&gt;

&lt;p&gt;    The serialVersionUIDs added in this commit are identical to what they&lt;br/&gt;
    were (as generated by Java) in Flink 1.2, so that we can at least&lt;br/&gt;
    restore state that were written with the Scala serializers as of 1.2.&lt;/p&gt;

&lt;p&gt;commit bdcf354fa7416f6e1ea1251433b4d97292b219c6&lt;br/&gt;
Author: Tzu-Li (Gordon) Tai &amp;lt;tzulitai@apache.org&amp;gt;&lt;br/&gt;
Date:   2017-06-10T20:41:35Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-6869&quot; title=&quot;Scala serializers do not have the serialVersionUID specified&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-6869&quot;&gt;&lt;del&gt;FLINK-6869&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;core&amp;#93;&lt;/span&gt; Tolerate serialVersionUID mismatches for anonymous classed serializers&lt;/p&gt;

&lt;p&gt;    This commit lets the TypeSerializerSerializationProxy be tolerable for&lt;br/&gt;
    serialVersionUID mismatches when reading anonymous classed serializers.&lt;/p&gt;

&lt;p&gt;    Our Scala case class serializers require this since they use Scala&lt;br/&gt;
    macros to be generated at compile time, and therefore is not possible to&lt;br/&gt;
    fix a certain serialVersionUID for them.&lt;/p&gt;

&lt;p&gt;    This commit also updates the streaming state docs to educate the user to&lt;br/&gt;
    avoid using anonymous classes for their state serializers.&lt;/p&gt;

&lt;p&gt;commit 31a2977c62abed0f985fdc79539b90c4152b60f1&lt;br/&gt;
Author: Tzu-Li (Gordon) Tai &amp;lt;tzulitai@apache.org&amp;gt;&lt;br/&gt;
Date:   2017-06-11T09:02:38Z&lt;/p&gt;

&lt;p&gt;    &lt;span class=&quot;error&quot;&gt;&amp;#91;hotfix&amp;#93;&lt;/span&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;cep&amp;#93;&lt;/span&gt; Fix incorrect CompatibilityResult.requiresMigration calls in CEP&lt;/p&gt;

&lt;p&gt;commit b294637c59c27ecef58ad67a123d2cfb401f51d2&lt;br/&gt;
Author: Tzu-Li (Gordon) Tai &amp;lt;tzulitai@apache.org&amp;gt;&lt;br/&gt;
Date:   2017-06-11T13:30:36Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-6883&quot; title=&quot;Serializer for collection of Scala case classes are generated with different anonymous class names in 1.3&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-6883&quot;&gt;&lt;del&gt;FLINK-6883&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;core&amp;#93;&lt;/span&gt; Refactor TypeSerializer to not implement TypeDeserializer&lt;/p&gt;

&lt;p&gt;    The separation of the TypeDeserializer interface from the TypeSerializer&lt;br/&gt;
    base class is due to the fact that additionally implementing the&lt;br/&gt;
    TypeDeserializer interface alters the generation order of anonymos&lt;br/&gt;
    serializer classes for Scala case classes and collections.&lt;/p&gt;

&lt;p&gt;    Instead, the TypeDeserializer is now used as a mixin on the&lt;br/&gt;
    TypeDeserializerAdapter utility, which now serves as a bridge for&lt;br/&gt;
    both directions (i.e. TypeSerializer to TypeDeserializer, and vice&lt;br/&gt;
    versa). No user interfaces are broken due to this change.&lt;/p&gt;

&lt;p&gt;commit ff0522f6c7f0d8f645736f2769dca90de50179ae&lt;br/&gt;
Author: Tzu-Li (Gordon) Tai &amp;lt;tzulitai@apache.org&amp;gt;&lt;br/&gt;
Date:   2017-06-11T13:31:42Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-6883&quot; title=&quot;Serializer for collection of Scala case classes are generated with different anonymous class names in 1.3&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-6883&quot;&gt;&lt;del&gt;FLINK-6883&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;tests&amp;#93;&lt;/span&gt; Add migration tests for Scala jobs&lt;/p&gt;

&lt;p&gt;    This commit adds migration ITCases for jobs written using the Scala API.&lt;br/&gt;
    An extra concern for migration of Scala jobs is that Scala case classes&lt;br/&gt;
    and collections use anonymous generated serializers, which may affect&lt;br/&gt;
    state restore.&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="16046544" author="githubbot" created="Mon, 12 Jun 2017 13:18:52 +0000"  >&lt;p&gt;Github user aljoscha commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4103#discussion_r121388886&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4103#discussion_r121388886&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-tests/src/test/scala/org/apache/flink/api/scala/migration/StatefulJobSavepointITCase.scala &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,298 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.api.scala.migration&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.api.common.accumulators.IntCounter&lt;br/&gt;
    +import org.apache.flink.api.common.functions.RichFlatMapFunction&lt;br/&gt;
    +import org.apache.flink.api.common.state.&lt;/p&gt;
{ListState, ListStateDescriptor, ValueState, ValueStateDescriptor}
&lt;p&gt;    +import org.apache.flink.api.java.functions.KeySelector&lt;br/&gt;
    +import org.apache.flink.configuration.Configuration&lt;br/&gt;
    +import org.apache.flink.runtime.state.memory.MemoryStateBackend&lt;br/&gt;
    +import org.apache.flink.streaming.api.TimeCharacteristic&lt;br/&gt;
    +import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction&lt;br/&gt;
    +import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment&lt;br/&gt;
    +import org.apache.flink.streaming.api.functions.sink.RichSinkFunction&lt;br/&gt;
    +import org.apache.flink.streaming.api.functions.source.SourceFunction&lt;br/&gt;
    +import org.apache.flink.streaming.api.watermark.Watermark&lt;br/&gt;
    +import org.apache.flink.test.checkpointing.utils.SavepointMigrationTestBase&lt;br/&gt;
    +import org.apache.flink.util.Collector&lt;br/&gt;
    +import org.apache.flink.api.java.tuple.Tuple2&lt;br/&gt;
    +import org.apache.flink.runtime.state.&lt;/p&gt;
{AbstractStateBackend, FunctionInitializationContext, FunctionSnapshotContext}
&lt;p&gt;    +import org.apache.flink.api.scala._&lt;br/&gt;
    +import org.apache.flink.api.scala.migration.CustomEnum.CustomEnum&lt;br/&gt;
    +import org.apache.flink.contrib.streaming.state.RocksDBStateBackend&lt;br/&gt;
    +import org.apache.flink.streaming.util.migration.MigrationVersion&lt;br/&gt;
    +import org.junit.runner.RunWith&lt;br/&gt;
    +import org.junit.runners.Parameterized&lt;br/&gt;
    +import org.junit.&lt;/p&gt;
{Ignore, Test}
&lt;p&gt;    +&lt;br/&gt;
    +import scala.util.&lt;/p&gt;
{Failure, Try}
&lt;p&gt;    +&lt;br/&gt;
    +object StatefulJobSavepointITCase {&lt;br/&gt;
    +&lt;br/&gt;
    +  @Parameterized.Parameters(name = &quot;Migrate Savepoint / Backend: &lt;/p&gt;
{0}
&lt;p&gt;&quot;)&lt;br/&gt;
    +  def parameters: util.Collection&lt;span class=&quot;error&quot;&gt;&amp;#91;(MigrationVersion, String)&amp;#93;&lt;/span&gt; = &lt;/p&gt;
{
    +    util.Arrays.asList(
    +      (MigrationVersion.v1_2, AbstractStateBackend.MEMORY_STATE_BACKEND_NAME),
    +      (MigrationVersion.v1_2, AbstractStateBackend.ROCKSDB_STATE_BACKEND_NAME),
    +      (MigrationVersion.v1_3, AbstractStateBackend.MEMORY_STATE_BACKEND_NAME),
    +      (MigrationVersion.v1_3, AbstractStateBackend.ROCKSDB_STATE_BACKEND_NAME))
    +  }
&lt;p&gt;    +&lt;br/&gt;
    +  // TODO to generate savepoints for a specific Flink version / backend type,&lt;br/&gt;
    +  // TODO change these values accordingly, e.g. to generate for 1.3 with RocksDB,&lt;br/&gt;
    +  // TODO set as (MigrationVersion.v1_3, AbstractStateBackend.ROCKSDB_STATE_BACKEND_NAME)&lt;br/&gt;
    +  val GENERATE_SAVEPOINT_VER: MigrationVersion = null&lt;br/&gt;
    +  val GENERATE_SAVEPOINT_BACKEND_TYPE: String = &quot;&quot;&lt;br/&gt;
    +&lt;br/&gt;
    +  val NUM_ELEMENTS = 4&lt;br/&gt;
    +}&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * ITCase for migration Scala state types across different Flink versions.&lt;br/&gt;
    + */&lt;br/&gt;
    +@RunWith(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Parameterized&amp;#93;&lt;/span&gt;)&lt;br/&gt;
    +class StatefulJobSavepointITCase(&lt;br/&gt;
    +    migrationVersionAndBackend: (MigrationVersion, String))&lt;br/&gt;
    +  extends SavepointMigrationTestBase with Serializable {&lt;br/&gt;
    +&lt;br/&gt;
    +  @Ignore&lt;br/&gt;
    +  @Test&lt;br/&gt;
    +  def testCreateSavepoint(): Unit = {&lt;br/&gt;
    +    val env = StreamExecutionEnvironment.getExecutionEnvironment&lt;br/&gt;
    +    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)&lt;br/&gt;
    +&lt;br/&gt;
    +    StatefulJobSavepointITCase.GENERATE_SAVEPOINT_BACKEND_TYPE match &lt;/p&gt;
{
    +      case AbstractStateBackend.ROCKSDB_STATE_BACKEND_NAME =&amp;gt;
    +        env.setStateBackend(new RocksDBStateBackend(new MemoryStateBackend()))
    +      case AbstractStateBackend.MEMORY_STATE_BACKEND_NAME =&amp;gt;
    +        env.setStateBackend(new MemoryStateBackend())
    +      case _ =&amp;gt; throw new UnsupportedOperationException
    +    }&lt;br/&gt;
    +&lt;br/&gt;
    +    env.setStateBackend(new MemoryStateBackend)&lt;br/&gt;
    +    env.enableCheckpointing(500)&lt;br/&gt;
    +    env.setParallelism(4)&lt;br/&gt;
    +    env.setMaxParallelism(4)&lt;br/&gt;
    +&lt;br/&gt;
    +    env&lt;br/&gt;
    +      .addSource(&lt;br/&gt;
    +        new CheckpointedSource(4)).setMaxParallelism(1).uid(&quot;checkpointedSource&quot;)&lt;br/&gt;
    +      .keyBy(&lt;br/&gt;
    +        new KeySelector&lt;span class=&quot;error&quot;&gt;&amp;#91;(Long, Long), Long&amp;#93;&lt;/span&gt; {
    +          override def getKey(value: (Long, Long)): Long = value._1
    +        }&lt;br/&gt;
    +      )&lt;br/&gt;
    +      .flatMap(new StatefulFlatMapper)&lt;br/&gt;
    +      .addSink(new AccumulatorCountingSink)&lt;br/&gt;
    +&lt;br/&gt;
    +    executeAndSavepoint(&lt;br/&gt;
    +      env,&lt;br/&gt;
    +      &quot;src/test/resources/stateful-scala-udf-migration-itcase-flink&quot;&lt;br/&gt;
    +        + StatefulJobSavepointITCase.GENERATE_SAVEPOINT_VER + &quot;-&quot;&lt;br/&gt;
    +        + StatefulJobSavepointITCase.GENERATE_SAVEPOINT_BACKEND_TYPE + &quot;-savepoint&quot;,&lt;br/&gt;
    +      new Tuple2(&lt;br/&gt;
    +        AccumulatorCountingSink.NUM_ELEMENTS_ACCUMULATOR,&lt;br/&gt;
    +        StatefulJobSavepointITCase.NUM_ELEMENTS&lt;br/&gt;
    +      )&lt;br/&gt;
    +    )&lt;br/&gt;
    +  }&lt;br/&gt;
    +&lt;br/&gt;
    +  @Test&lt;br/&gt;
    +  def testRestoreSavepoint(): Unit = {&lt;br/&gt;
    +    val env = StreamExecutionEnvironment.getExecutionEnvironment&lt;br/&gt;
    +    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)&lt;br/&gt;
    +&lt;br/&gt;
    +    migrationVersionAndBackend._2 match {    +      case AbstractStateBackend.ROCKSDB_STATE_BACKEND_NAME =&amp;gt;    +        env.setStateBackend(new RocksDBStateBackend(new MemoryStateBackend()))    +      case AbstractStateBackend.MEMORY_STATE_BACKEND_NAME =&amp;gt;    +        env.setStateBackend(new MemoryStateBackend())    +      case _ =&amp;gt; throw new UnsupportedOperationException    +    }
&lt;p&gt;    +&lt;br/&gt;
    +    env.setStateBackend(new MemoryStateBackend)&lt;br/&gt;
    +    env.enableCheckpointing(500)&lt;br/&gt;
    +    env.setParallelism(4)&lt;br/&gt;
    +    env.setMaxParallelism(4)&lt;br/&gt;
    +&lt;br/&gt;
    +    env&lt;br/&gt;
    +      .addSource(&lt;br/&gt;
    +        new CheckpointedSource(4)).setMaxParallelism(1).uid(&quot;checkpointedSource&quot;)&lt;br/&gt;
    +      .keyBy(&lt;br/&gt;
    +        new KeySelector&lt;span class=&quot;error&quot;&gt;&amp;#91;(Long, Long), Long&amp;#93;&lt;/span&gt; &lt;/p&gt;
{
    +          override def getKey(value: (Long, Long)): Long = value._1
    +        }
&lt;p&gt;    +      )&lt;br/&gt;
    +      .flatMap(new StatefulFlatMapper)&lt;br/&gt;
    +      .addSink(new AccumulatorCountingSink)&lt;br/&gt;
    +&lt;br/&gt;
    +    restoreAndExecute(&lt;br/&gt;
    +      env,&lt;br/&gt;
    +      SavepointMigrationTestBase.getResourceFilename(&lt;br/&gt;
    +        &quot;stateful-scala-udf-migration-itcase-flink&quot;&lt;br/&gt;
    +          + migrationVersionAndBackend._1 + &quot;-&quot;&lt;br/&gt;
    +          + migrationVersionAndBackend._2 + &quot;-savepoint&quot;),&lt;br/&gt;
    +      new Tuple2(AccumulatorCountingSink.NUM_ELEMENTS_ACCUMULATOR, 4)&lt;br/&gt;
    +    )&lt;br/&gt;
    +  }&lt;br/&gt;
    +&lt;br/&gt;
    +  @SerialVersionUID(1L)&lt;br/&gt;
    +  private object CheckpointedSource &lt;/p&gt;
{
    +    var CHECKPOINTED_STRING = &quot;Here be dragons!&quot;
    +  }
&lt;p&gt;    +&lt;br/&gt;
    +  @SerialVersionUID(1L)&lt;br/&gt;
    +  private class CheckpointedSource(val numElements: Int)&lt;br/&gt;
    +      extends SourceFunction&lt;span class=&quot;error&quot;&gt;&amp;#91;(Long, Long)&amp;#93;&lt;/span&gt; with CheckpointedFunction {&lt;br/&gt;
    +&lt;br/&gt;
    +    private var isRunning = true&lt;br/&gt;
    +    private var state: ListState&lt;span class=&quot;error&quot;&gt;&amp;#91;CustomCaseClass&amp;#93;&lt;/span&gt; = _&lt;br/&gt;
    +&lt;br/&gt;
    +    @throws&lt;span class=&quot;error&quot;&gt;&amp;#91;Exception&amp;#93;&lt;/span&gt;&lt;br/&gt;
    +    override def run(ctx: SourceFunction.SourceContext&lt;span class=&quot;error&quot;&gt;&amp;#91;(Long, Long)&amp;#93;&lt;/span&gt;) {&lt;br/&gt;
    +      ctx.emitWatermark(new Watermark(0))&lt;br/&gt;
    +      ctx.getCheckpointLock synchronized {&lt;br/&gt;
    +        var i = 0&lt;br/&gt;
    +        while (i &amp;lt; numElements) {&lt;br/&gt;
    +          {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Are the additional `{}` blocks needed?&lt;/p&gt;</comment>
                            <comment id="16046545" author="githubbot" created="Mon, 12 Jun 2017 13:18:52 +0000"  >&lt;p&gt;Github user aljoscha commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4103#discussion_r121386625&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4103#discussion_r121386625&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-core/src/main/java/org/apache/flink/api/common/typeutils/CompatibilityResult.java &amp;#8212;&lt;br/&gt;
    @@ -60,10 +62,10 @@&lt;br/&gt;
     	 *&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@return a result that signals migration is necessary, also providing a convert deserializer.&lt;br/&gt;
     	 */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static &amp;lt;T&amp;gt; CompatibilityResult&amp;lt;T&amp;gt; requiresMigration(TypeDeserializer&amp;lt;T&amp;gt; convertDeserializer) {&lt;br/&gt;
    +	public static &amp;lt;T&amp;gt; CompatibilityResult&amp;lt;T&amp;gt; requiresMigration(@Nonnull TypeDeserializer&amp;lt;T&amp;gt; convertDeserializer) {&lt;br/&gt;
     		Preconditions.checkNotNull(convertDeserializer, &quot;Convert deserializer cannot be null.&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;return new CompatibilityResult&amp;lt;&amp;gt;(true, Preconditions.checkNotNull(convertDeserializer));&lt;br/&gt;
    +		return new CompatibilityResult&amp;lt;&amp;gt;(true, convertDeserializer);
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Does `@Nonnull` always ensure that the argument is not null? We could simply keep the additional `Preconditions` check.&lt;/p&gt;</comment>
                            <comment id="16046565" author="githubbot" created="Mon, 12 Jun 2017 13:35:59 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4103#discussion_r121393465&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4103#discussion_r121393465&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-core/src/main/java/org/apache/flink/api/common/typeutils/CompatibilityResult.java &amp;#8212;&lt;br/&gt;
    @@ -60,10 +62,10 @@&lt;br/&gt;
     	 *&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@return a result that signals migration is necessary, also providing a convert deserializer.&lt;br/&gt;
     	 */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static &amp;lt;T&amp;gt; CompatibilityResult&amp;lt;T&amp;gt; requiresMigration(TypeDeserializer&amp;lt;T&amp;gt; convertDeserializer) {&lt;br/&gt;
    +	public static &amp;lt;T&amp;gt; CompatibilityResult&amp;lt;T&amp;gt; requiresMigration(@Nonnull TypeDeserializer&amp;lt;T&amp;gt; convertDeserializer) {&lt;br/&gt;
     		Preconditions.checkNotNull(convertDeserializer, &quot;Convert deserializer cannot be null.&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;return new CompatibilityResult&amp;lt;&amp;gt;(true, Preconditions.checkNotNull(convertDeserializer));&lt;br/&gt;
    +		return new CompatibilityResult&amp;lt;&amp;gt;(true, convertDeserializer);
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    I&apos;ve removed this `Preconditions.checkNotNull` because it was already done in the factory method (see L66). So this one is just redundant, the removal is not related to the `Nonnull` annotation.&lt;/p&gt;</comment>
                            <comment id="16046566" author="githubbot" created="Mon, 12 Jun 2017 13:36:49 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4103#discussion_r121393653&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4103#discussion_r121393653&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-tests/src/test/scala/org/apache/flink/api/scala/migration/StatefulJobSavepointITCase.scala &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,298 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.api.scala.migration&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.api.common.accumulators.IntCounter&lt;br/&gt;
    +import org.apache.flink.api.common.functions.RichFlatMapFunction&lt;br/&gt;
    +import org.apache.flink.api.common.state.&lt;/p&gt;
{ListState, ListStateDescriptor, ValueState, ValueStateDescriptor}
&lt;p&gt;    +import org.apache.flink.api.java.functions.KeySelector&lt;br/&gt;
    +import org.apache.flink.configuration.Configuration&lt;br/&gt;
    +import org.apache.flink.runtime.state.memory.MemoryStateBackend&lt;br/&gt;
    +import org.apache.flink.streaming.api.TimeCharacteristic&lt;br/&gt;
    +import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction&lt;br/&gt;
    +import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment&lt;br/&gt;
    +import org.apache.flink.streaming.api.functions.sink.RichSinkFunction&lt;br/&gt;
    +import org.apache.flink.streaming.api.functions.source.SourceFunction&lt;br/&gt;
    +import org.apache.flink.streaming.api.watermark.Watermark&lt;br/&gt;
    +import org.apache.flink.test.checkpointing.utils.SavepointMigrationTestBase&lt;br/&gt;
    +import org.apache.flink.util.Collector&lt;br/&gt;
    +import org.apache.flink.api.java.tuple.Tuple2&lt;br/&gt;
    +import org.apache.flink.runtime.state.&lt;/p&gt;
{AbstractStateBackend, FunctionInitializationContext, FunctionSnapshotContext}
&lt;p&gt;    +import org.apache.flink.api.scala._&lt;br/&gt;
    +import org.apache.flink.api.scala.migration.CustomEnum.CustomEnum&lt;br/&gt;
    +import org.apache.flink.contrib.streaming.state.RocksDBStateBackend&lt;br/&gt;
    +import org.apache.flink.streaming.util.migration.MigrationVersion&lt;br/&gt;
    +import org.junit.runner.RunWith&lt;br/&gt;
    +import org.junit.runners.Parameterized&lt;br/&gt;
    +import org.junit.&lt;/p&gt;
{Ignore, Test}
&lt;p&gt;    +&lt;br/&gt;
    +import scala.util.&lt;/p&gt;
{Failure, Try}
&lt;p&gt;    +&lt;br/&gt;
    +object StatefulJobSavepointITCase {&lt;br/&gt;
    +&lt;br/&gt;
    +  @Parameterized.Parameters(name = &quot;Migrate Savepoint / Backend: &lt;/p&gt;
{0}
&lt;p&gt;&quot;)&lt;br/&gt;
    +  def parameters: util.Collection&lt;span class=&quot;error&quot;&gt;&amp;#91;(MigrationVersion, String)&amp;#93;&lt;/span&gt; = &lt;/p&gt;
{
    +    util.Arrays.asList(
    +      (MigrationVersion.v1_2, AbstractStateBackend.MEMORY_STATE_BACKEND_NAME),
    +      (MigrationVersion.v1_2, AbstractStateBackend.ROCKSDB_STATE_BACKEND_NAME),
    +      (MigrationVersion.v1_3, AbstractStateBackend.MEMORY_STATE_BACKEND_NAME),
    +      (MigrationVersion.v1_3, AbstractStateBackend.ROCKSDB_STATE_BACKEND_NAME))
    +  }
&lt;p&gt;    +&lt;br/&gt;
    +  // TODO to generate savepoints for a specific Flink version / backend type,&lt;br/&gt;
    +  // TODO change these values accordingly, e.g. to generate for 1.3 with RocksDB,&lt;br/&gt;
    +  // TODO set as (MigrationVersion.v1_3, AbstractStateBackend.ROCKSDB_STATE_BACKEND_NAME)&lt;br/&gt;
    +  val GENERATE_SAVEPOINT_VER: MigrationVersion = null&lt;br/&gt;
    +  val GENERATE_SAVEPOINT_BACKEND_TYPE: String = &quot;&quot;&lt;br/&gt;
    +&lt;br/&gt;
    +  val NUM_ELEMENTS = 4&lt;br/&gt;
    +}&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * ITCase for migration Scala state types across different Flink versions.&lt;br/&gt;
    + */&lt;br/&gt;
    +@RunWith(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Parameterized&amp;#93;&lt;/span&gt;)&lt;br/&gt;
    +class StatefulJobSavepointITCase(&lt;br/&gt;
    +    migrationVersionAndBackend: (MigrationVersion, String))&lt;br/&gt;
    +  extends SavepointMigrationTestBase with Serializable {&lt;br/&gt;
    +&lt;br/&gt;
    +  @Ignore&lt;br/&gt;
    +  @Test&lt;br/&gt;
    +  def testCreateSavepoint(): Unit = {&lt;br/&gt;
    +    val env = StreamExecutionEnvironment.getExecutionEnvironment&lt;br/&gt;
    +    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)&lt;br/&gt;
    +&lt;br/&gt;
    +    StatefulJobSavepointITCase.GENERATE_SAVEPOINT_BACKEND_TYPE match &lt;/p&gt;
{
    +      case AbstractStateBackend.ROCKSDB_STATE_BACKEND_NAME =&amp;gt;
    +        env.setStateBackend(new RocksDBStateBackend(new MemoryStateBackend()))
    +      case AbstractStateBackend.MEMORY_STATE_BACKEND_NAME =&amp;gt;
    +        env.setStateBackend(new MemoryStateBackend())
    +      case _ =&amp;gt; throw new UnsupportedOperationException
    +    }&lt;br/&gt;
    +&lt;br/&gt;
    +    env.setStateBackend(new MemoryStateBackend)&lt;br/&gt;
    +    env.enableCheckpointing(500)&lt;br/&gt;
    +    env.setParallelism(4)&lt;br/&gt;
    +    env.setMaxParallelism(4)&lt;br/&gt;
    +&lt;br/&gt;
    +    env&lt;br/&gt;
    +      .addSource(&lt;br/&gt;
    +        new CheckpointedSource(4)).setMaxParallelism(1).uid(&quot;checkpointedSource&quot;)&lt;br/&gt;
    +      .keyBy(&lt;br/&gt;
    +        new KeySelector&lt;span class=&quot;error&quot;&gt;&amp;#91;(Long, Long), Long&amp;#93;&lt;/span&gt; {
    +          override def getKey(value: (Long, Long)): Long = value._1
    +        }&lt;br/&gt;
    +      )&lt;br/&gt;
    +      .flatMap(new StatefulFlatMapper)&lt;br/&gt;
    +      .addSink(new AccumulatorCountingSink)&lt;br/&gt;
    +&lt;br/&gt;
    +    executeAndSavepoint(&lt;br/&gt;
    +      env,&lt;br/&gt;
    +      &quot;src/test/resources/stateful-scala-udf-migration-itcase-flink&quot;&lt;br/&gt;
    +        + StatefulJobSavepointITCase.GENERATE_SAVEPOINT_VER + &quot;-&quot;&lt;br/&gt;
    +        + StatefulJobSavepointITCase.GENERATE_SAVEPOINT_BACKEND_TYPE + &quot;-savepoint&quot;,&lt;br/&gt;
    +      new Tuple2(&lt;br/&gt;
    +        AccumulatorCountingSink.NUM_ELEMENTS_ACCUMULATOR,&lt;br/&gt;
    +        StatefulJobSavepointITCase.NUM_ELEMENTS&lt;br/&gt;
    +      )&lt;br/&gt;
    +    )&lt;br/&gt;
    +  }&lt;br/&gt;
    +&lt;br/&gt;
    +  @Test&lt;br/&gt;
    +  def testRestoreSavepoint(): Unit = {&lt;br/&gt;
    +    val env = StreamExecutionEnvironment.getExecutionEnvironment&lt;br/&gt;
    +    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)&lt;br/&gt;
    +&lt;br/&gt;
    +    migrationVersionAndBackend._2 match {    +      case AbstractStateBackend.ROCKSDB_STATE_BACKEND_NAME =&amp;gt;    +        env.setStateBackend(new RocksDBStateBackend(new MemoryStateBackend()))    +      case AbstractStateBackend.MEMORY_STATE_BACKEND_NAME =&amp;gt;    +        env.setStateBackend(new MemoryStateBackend())    +      case _ =&amp;gt; throw new UnsupportedOperationException    +    }
&lt;p&gt;    +&lt;br/&gt;
    +    env.setStateBackend(new MemoryStateBackend)&lt;br/&gt;
    +    env.enableCheckpointing(500)&lt;br/&gt;
    +    env.setParallelism(4)&lt;br/&gt;
    +    env.setMaxParallelism(4)&lt;br/&gt;
    +&lt;br/&gt;
    +    env&lt;br/&gt;
    +      .addSource(&lt;br/&gt;
    +        new CheckpointedSource(4)).setMaxParallelism(1).uid(&quot;checkpointedSource&quot;)&lt;br/&gt;
    +      .keyBy(&lt;br/&gt;
    +        new KeySelector&lt;span class=&quot;error&quot;&gt;&amp;#91;(Long, Long), Long&amp;#93;&lt;/span&gt; &lt;/p&gt;
{
    +          override def getKey(value: (Long, Long)): Long = value._1
    +        }
&lt;p&gt;    +      )&lt;br/&gt;
    +      .flatMap(new StatefulFlatMapper)&lt;br/&gt;
    +      .addSink(new AccumulatorCountingSink)&lt;br/&gt;
    +&lt;br/&gt;
    +    restoreAndExecute(&lt;br/&gt;
    +      env,&lt;br/&gt;
    +      SavepointMigrationTestBase.getResourceFilename(&lt;br/&gt;
    +        &quot;stateful-scala-udf-migration-itcase-flink&quot;&lt;br/&gt;
    +          + migrationVersionAndBackend._1 + &quot;-&quot;&lt;br/&gt;
    +          + migrationVersionAndBackend._2 + &quot;-savepoint&quot;),&lt;br/&gt;
    +      new Tuple2(AccumulatorCountingSink.NUM_ELEMENTS_ACCUMULATOR, 4)&lt;br/&gt;
    +    )&lt;br/&gt;
    +  }&lt;br/&gt;
    +&lt;br/&gt;
    +  @SerialVersionUID(1L)&lt;br/&gt;
    +  private object CheckpointedSource &lt;/p&gt;
{
    +    var CHECKPOINTED_STRING = &quot;Here be dragons!&quot;
    +  }
&lt;p&gt;    +&lt;br/&gt;
    +  @SerialVersionUID(1L)&lt;br/&gt;
    +  private class CheckpointedSource(val numElements: Int)&lt;br/&gt;
    +      extends SourceFunction&lt;span class=&quot;error&quot;&gt;&amp;#91;(Long, Long)&amp;#93;&lt;/span&gt; with CheckpointedFunction {&lt;br/&gt;
    +&lt;br/&gt;
    +    private var isRunning = true&lt;br/&gt;
    +    private var state: ListState&lt;span class=&quot;error&quot;&gt;&amp;#91;CustomCaseClass&amp;#93;&lt;/span&gt; = _&lt;br/&gt;
    +&lt;br/&gt;
    +    @throws&lt;span class=&quot;error&quot;&gt;&amp;#91;Exception&amp;#93;&lt;/span&gt;&lt;br/&gt;
    +    override def run(ctx: SourceFunction.SourceContext&lt;span class=&quot;error&quot;&gt;&amp;#91;(Long, Long)&amp;#93;&lt;/span&gt;) {&lt;br/&gt;
    +      ctx.emitWatermark(new Watermark(0))&lt;br/&gt;
    +      ctx.getCheckpointLock synchronized {&lt;br/&gt;
    +        var i = 0&lt;br/&gt;
    +        while (i &amp;lt; numElements) {&lt;br/&gt;
    +          {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Ah, I pasted this part from Java code &#128517; will cleanup.&lt;/p&gt;</comment>
                            <comment id="16046821" author="githubbot" created="Mon, 12 Jun 2017 17:37:48 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4103&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4103&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Fixed failing migration tests when scala-2.11 is used.&lt;br/&gt;
    Merging once Travis gives green light!&lt;/p&gt;</comment>
                            <comment id="16047433" author="githubbot" created="Tue, 13 Jun 2017 05:18:44 +0000"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4103&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4103&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16047453" author="tzulitai" created="Tue, 13 Jun 2017 05:51:25 +0000"  >&lt;p&gt;Fixed for 1.3.1 via 39c8270d39684765484fa4b6b2711e5714b81b64.&lt;br/&gt;
Fixed for master via 69fada3d0b4c686f29c356f00eb49039f416879f.&lt;/p&gt;</comment>
                            <comment id="16335823" author="shashank734" created="Tue, 23 Jan 2018 14:20:35 +0000"  >&lt;p&gt;I am facing something similar issue in 1.4 where i have created save point and restored both in same 1.4 without changing application code. I am using CEP.&lt;/p&gt;


&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
java.lang.IllegalStateException: Could not initialize keyed state backend.
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initKeyedState(AbstractStreamOperator.java:293)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:225)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeOperators(StreamTask.java:692)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeState(StreamTask.java:679)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:253)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:718)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
Caused by: java.io.InvalidClassException: org.apache.flink.cep.scala.pattern.Pattern$$anon$3; invalid descriptor &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; field 
	at java.io.ObjectStreamClass.readNonProxy(ObjectStreamClass.java:723)
	at java.io.ObjectInputStream.readClassDescriptor(ObjectInputStream.java:833)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1609)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1521)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1781)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:373)
	at org.apache.flink.cep.nfa.NFA$NFASerializer.deserializeCondition(NFA.java:1171)
	at org.apache.flink.cep.nfa.NFA$NFASerializer.deserializeStates(NFA.java:1129)
	at org.apache.flink.cep.nfa.NFA$NFASerializer.deserialize(NFA.java:917)
	at org.apache.flink.cep.nfa.NFA$NFASerializer.deserialize(NFA.java:820)
	at org.apache.flink.runtime.state.heap.StateTableByKeyGroupReaders$StateTableByKeyGroupReaderV2V3.readMappingsInKeyGroup(StateTableByKeyGroupReaders.java:133)
	at org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.restorePartitionedState(HeapKeyedStateBackend.java:575)
	at org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.restore(HeapKeyedStateBackend.java:446)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.createKeyedStateBackend(StreamTask.java:773)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initKeyedState(AbstractStreamOperator.java:283)
	... 6 more
Caused by: java.lang.IllegalArgumentException: illegal signature
	at java.io.ObjectStreamField.&amp;lt;init&amp;gt;(ObjectStreamField.java:122)
	at java.io.ObjectStreamClass.readNonProxy(ObjectStreamClass.java:721)
	... 21 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 43 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3g4on:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>