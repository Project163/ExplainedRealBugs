<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 20:33:35 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-9349] KafkaConnector Exception  while fetching from multiple kafka topics</title>
                <link>https://issues.apache.org/jira/browse/FLINK-9349</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;p&gt;./flink-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09Fetcher.java&lt;br/&gt;
&#160;&lt;br/&gt;
It seems the List&#160;subscribedPartitionStates was being modified when&#160;runFetchLoop iterated the List.&lt;br/&gt;
This can happen if, e.g.,&#160;FlinkKafkaConsumer runs the following code concurrently:&lt;br/&gt;
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; kafkaFetcher.addDiscoveredPartitions(discoveredPartitions);&lt;br/&gt;
&#160;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
 java.util.ConcurrentModificationException
	at java.util.LinkedList$ListItr.checkForComodification(LinkedList.java:966)
	at java.util.LinkedList$ListItr.next(LinkedList.java:888)
	at org.apache.flink.streaming.connectors.kafka.internal.Kafka09Fetcher.runFetchLoop(Kafka09Fetcher.java:134)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="13158941">FLINK-9349</key>
            <summary>KafkaConnector Exception  while fetching from multiple kafka topics</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="Sergey Nuyanzin">Sergey Nuyanzin</assignee>
                                    <reporter username="vishalsantv">Vishal Santoshi</reporter>
                        <labels>
                    </labels>
                <created>Sun, 13 May 2018 14:49:26 +0000</created>
                <updated>Thu, 11 Jul 2019 07:45:39 +0000</updated>
                            <resolved>Wed, 23 May 2018 07:15:30 +0000</resolved>
                                    <version>1.4.0</version>
                                    <fixVersion>1.4.3</fixVersion>
                    <fixVersion>1.5.0</fixVersion>
                                    <component>Connectors / Kafka</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>7</watches>
                                                                                                                <comments>
                            <comment id="16473518" author="yuzhihong@gmail.com" created="Sun, 13 May 2018 15:04:42 +0000"  >&lt;p&gt;The root cause analysis, from me, was based on quick inspection of the code.&lt;/p&gt;

&lt;p&gt;Vishal, can you attach the complete stack trace if you have it ?&lt;/p&gt;

&lt;p&gt;If you can describe your flow (or write unit test) which reproduces the exception, that would help find the root cause.&lt;/p&gt;</comment>
                            <comment id="16473520" author="yuzhihong@gmail.com" created="Sun, 13 May 2018 15:10:03 +0000"  >&lt;p&gt;It seems synchronization should be added for adding to subscribedPartitionStates and iterating subscribedPartitionStates List.&lt;/p&gt;</comment>
                            <comment id="16478886" author="sergey nuyanzin" created="Thu, 17 May 2018 10:52:11 +0000"  >&lt;p&gt;Hello &lt;br/&gt;
I was able to write a test (based on existing) to reproduce this and one more related issue.&lt;br/&gt;
the second one is &lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Caused by: java.lang.NullPointerException
	at java.util.LinkedList$ListItr.next(LinkedList.java:893)
	at org.apache.flink.streaming.connectors.kafka.internal.Kafka09Fetcher.doCommitInternalOffsetsToKafka(Kafka09Fetcher.java:228)
	at org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher.commitInternalOffsetsToKafka(AbstractFetcher.java:293)&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;as a result of concurrent work with non thread safe subscribedPartitionStates (LinkedList). &lt;br/&gt;
from my point of view there could be 2 possible solutions:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;added synchronization as you mentioned&lt;/li&gt;
	&lt;li&gt;use threadsafe collection e.g. CopyOnWriteArrayList instead of LinkedList for subscribedPartitionStates&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;both options pass the test&lt;br/&gt;
by the way the code snippet for the test is attached &lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12923907/12923907_Flink9349Test.java&quot; title=&quot;Flink9349Test.java attached to FLINK-9349&quot;&gt;Flink9349Test.java&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt; &lt;/p&gt;

&lt;p&gt;in the second option no synchronized is required and it might be an option if partitionStates are not frequent otherwise it makes sense to use synchronization&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yuzhihong%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;yuzhihong@gmail.com&quot;&gt;yuzhihong@gmail.com&lt;/a&gt; could you please point to more appropriate approach?&lt;/p&gt;</comment>
                            <comment id="16479257" author="tzulitai" created="Thu, 17 May 2018 15:54:54 +0000"  >&lt;p&gt;This indeed looks like a problem with concurrently accessing the partition state list.&lt;/p&gt;

&lt;p&gt;However, I would like to avoid synchronizing the iteration, if possible. That loop is a critical path, adding that synchronization could be harmful for performance.&#160;&lt;tt&gt;CopyOnWriteArrayList&lt;/tt&gt;&#160;seems like a better approach here; the costly add operations is ok since partition discovery should not happen often anyways.&lt;/p&gt;</comment>
                            <comment id="16479259" author="tzulitai" created="Thu, 17 May 2018 15:56:30 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Sergey+Nuyanzin&quot; class=&quot;user-hover&quot; rel=&quot;Sergey Nuyanzin&quot;&gt;Sergey Nuyanzin&lt;/a&gt; this looks like a blocker bug that we probably should fix ASAP.&lt;br/&gt;
 Are you working on this already? If yes, do you have an ETA on the PR?&lt;br/&gt;
If possible, we should try to get this fix in 1.5.0, which is just around the corner.&lt;/p&gt;</comment>
                            <comment id="16479274" author="yuzhihong@gmail.com" created="Thu, 17 May 2018 16:02:10 +0000"  >&lt;blockquote&gt;&lt;p&gt;both options pass the test&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It seems Sergey already tried using CopyOnWriteArrayList.&lt;br/&gt;
It would be better if Sergey can provide PR since he has created the test (and tried a fix).&lt;/p&gt;</comment>
                            <comment id="16479275" author="tzulitai" created="Thu, 17 May 2018 16:02:18 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Sergey+Nuyanzin&quot; class=&quot;user-hover&quot; rel=&quot;Sergey Nuyanzin&quot;&gt;Sergey Nuyanzin&lt;/a&gt; your attached file for the test looks good.&lt;br/&gt;
We can probably move forward by preparing the PR &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&#160;Are you familiar with the process?&lt;/p&gt;</comment>
                            <comment id="16479278" author="yuzhihong@gmail.com" created="Thu, 17 May 2018 16:03:50 +0000"  >&lt;p&gt;Sergey:&lt;br/&gt;
Please add Apache license header to the test when you create the PR.&lt;/p&gt;</comment>
                            <comment id="16479281" author="sergey nuyanzin" created="Thu, 17 May 2018 16:04:43 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tzulitai&quot; class=&quot;user-hover&quot; rel=&quot;tzulitai&quot;&gt;tzulitai&lt;/a&gt; thank you for your comment&lt;br/&gt;
started to in progress give me several minutes&lt;/p&gt;</comment>
                            <comment id="16479282" author="yuzhihong@gmail.com" created="Thu, 17 May 2018 16:04:43 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=trohrmann%40apache.org&quot; class=&quot;user-hover&quot; rel=&quot;trohrmann@apache.org&quot;&gt;trohrmann@apache.org&lt;/a&gt;:&lt;br/&gt;
Please see the above.&lt;/p&gt;

&lt;p&gt;It would be better if the fix can be part of next RC.&lt;/p&gt;</comment>
                            <comment id="16479354" author="tzulitai" created="Thu, 17 May 2018 16:46:39 +0000"  >&lt;p&gt;I&apos;ve just talked offline with Till about this issue.&lt;br/&gt;
 We should try to get a fix for this ASAP, but not block 1.5.0 RCs on this since it was already a bug in 1.4.0.&lt;br/&gt;
 At least we get a fix in soon, so that in case we have to open yet another RC, it will be included.&lt;/p&gt;</comment>
                            <comment id="16479355" author="githubbot" created="Thu, 17 May 2018 16:47:33 +0000"  >&lt;p&gt;GitHub user snuyanzin opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6040&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6040&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-9349&quot; title=&quot;KafkaConnector Exception  while fetching from multiple kafka topics&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-9349&quot;&gt;&lt;del&gt;FLINK-9349&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;Kafka Connector&amp;#93;&lt;/span&gt; KafkaConnector Exception while fetching from multiple kafka topics&lt;/p&gt;

&lt;p&gt;    &lt;b&gt;Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;    &lt;b&gt;Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.&lt;/b&gt;&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Contribution Checklist&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Make sure that the pull request corresponds to a &lt;span class=&quot;error&quot;&gt;&amp;#91;JIRA issue&amp;#93;&lt;/span&gt;(&lt;a href=&quot;https://issues.apache.org/jira/projects/FLINK/issues&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/projects/FLINK/issues&lt;/a&gt;). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Name the pull request in the form &quot;&lt;span class=&quot;error&quot;&gt;&amp;#91;FLINK-XXXX&amp;#93;&lt;/span&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;component&amp;#93;&lt;/span&gt; Title of the pull request&quot;, where &lt;b&gt;FLINK-XXXX&lt;/b&gt; should be replaced by the actual issue number. Skip &lt;b&gt;component&lt;/b&gt; if you are unsure about which is the best component.&lt;br/&gt;
      Typo fixes that have no associated JIRA issue should be named following this pattern: `&lt;span class=&quot;error&quot;&gt;&amp;#91;hotfix&amp;#93;&lt;/span&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;docs&amp;#93;&lt;/span&gt; Fix typo in event time introduction` or `&lt;span class=&quot;error&quot;&gt;&amp;#91;hotfix&amp;#93;&lt;/span&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;javadocs&amp;#93;&lt;/span&gt; Expand JavaDoc for PuncuatedWatermarkGenerator`.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following &lt;span class=&quot;error&quot;&gt;&amp;#91;this guide&amp;#93;&lt;/span&gt;(&lt;a href=&quot;http://flink.apache.org/contribute-code.html#best-practices&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://flink.apache.org/contribute-code.html#best-practices&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Each pull request should address only one issue, not mix up code from multiple issues.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Each commit in the pull request has a meaningful commit message (including the JIRA id)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;    *&lt;b&gt;(The sections below can be removed for hotfixes of typos)&lt;/b&gt;*&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;What is the purpose of the change&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;    &lt;b&gt;(For example: This pull request makes task deployment go through the blob server, rather than through RPC. That way we avoid re-transferring them on each deployment (during recovery).)&lt;/b&gt;&lt;/p&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Brief change log&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;    fix synchronization issue&lt;br/&gt;
    + test to verify it&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Verifying this change&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;    This change added tests and can be verified as follows:&lt;br/&gt;
    via org.apache.flink.streaming.connectors.kafka.internal.Flink9349Test#testConcurrentPartitionsDiscoveryAndLoopFetching&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Does this pull request potentially affect one of the following parts:&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Dependencies (does it add or upgrade a dependency): (no)&lt;/li&gt;
	&lt;li&gt;The public API, i.e., is any changed class annotated with `@Public(Evolving)`: ( no)&lt;/li&gt;
	&lt;li&gt;The serializers: (no)&lt;/li&gt;
	&lt;li&gt;The runtime per-record code paths (performance sensitive): (no )&lt;/li&gt;
	&lt;li&gt;Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: ( no )&lt;/li&gt;
	&lt;li&gt;The S3 file system connector: ( no)&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Documentation&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Does this pull request introduce a new feature? ( no)&lt;/li&gt;
	&lt;li&gt;If yes, how is the feature documented? (not applicable)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/snuyanzin/flink&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/snuyanzin/flink&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-9349&quot; title=&quot;KafkaConnector Exception  while fetching from multiple kafka topics&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-9349&quot;&gt;&lt;del&gt;FLINK-9349&lt;/del&gt;&lt;/a&gt;_KafkaConnector_Exception_while_fetching_from_multiple_kafka_topics&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6040.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6040.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #6040&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 8de5f37549607460659e171f9c3b48d0090383c0&lt;br/&gt;
Author: snuyanzin &amp;lt;snuyanzin@...&amp;gt;&lt;br/&gt;
Date:   2018-05-17T16:12:04Z&lt;/p&gt;

&lt;p&gt;    added test and fix for &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-9349&quot; title=&quot;KafkaConnector Exception  while fetching from multiple kafka topics&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-9349&quot;&gt;&lt;del&gt;FLINK-9349&lt;/del&gt;&lt;/a&gt; by usage of CopyOnWriteArrayList&lt;/p&gt;

&lt;p&gt;commit eee524e2d2a86af5252ed939000c12a2604917e9&lt;br/&gt;
Author: snuyanzin &amp;lt;snuyanzin@...&amp;gt;&lt;br/&gt;
Date:   2018-05-17T16:35:10Z&lt;/p&gt;

&lt;p&gt;    fix checkstyle&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="16479385" author="githubbot" created="Thu, 17 May 2018 17:02:20 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6040#discussion_r189031751&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6040#discussion_r189031751&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/Flink9349Test.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,207 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka.internal;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.api.common.serialization.SimpleStringSchema;&lt;br/&gt;
    +import org.apache.flink.core.testutils.MultiShotLatch;&lt;br/&gt;
    +import org.apache.flink.core.testutils.OneShotLatch;&lt;br/&gt;
    +import org.apache.flink.metrics.groups.UnregisteredMetricsGroup;&lt;br/&gt;
    +import org.apache.flink.streaming.api.functions.source.SourceFunction;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaCommitCallback;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionStateSentinel;&lt;br/&gt;
    +import org.apache.flink.streaming.runtime.tasks.TestProcessingTimeService;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedDeserializationSchema;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedDeserializationSchemaWrapper;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.ConsumerRecords;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.KafkaConsumer;&lt;br/&gt;
    +import org.junit.Test;&lt;br/&gt;
    +import org.junit.runner.RunWith;&lt;br/&gt;
    +import org.mockito.invocation.InvocationOnMock;&lt;br/&gt;
    +import org.mockito.stubbing.Answer;&lt;br/&gt;
    +import org.powermock.core.classloader.annotations.PrepareForTest;&lt;br/&gt;
    +import org.powermock.modules.junit4.PowerMockRunner;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util.ArrayList;&lt;br/&gt;
    +import java.util.Collections;&lt;br/&gt;
    +import java.util.HashMap;&lt;br/&gt;
    +import java.util.List;&lt;br/&gt;
    +import java.util.Map;&lt;br/&gt;
    +import java.util.Properties;&lt;br/&gt;
    +import java.util.concurrent.CountDownLatch;&lt;br/&gt;
    +import java.util.concurrent.ExecutorService;&lt;br/&gt;
    +import java.util.concurrent.Executors;&lt;br/&gt;
    +import java.util.concurrent.TimeUnit;&lt;br/&gt;
    +import java.util.concurrent.atomic.AtomicReference;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.junit.Assert.assertFalse;&lt;br/&gt;
    +import static org.mockito.Mockito.anyLong;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.doAnswer;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.mock;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.when;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.whenNew;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * Unit tests for the &lt;/p&gt;
{@link Flink9349Test}
&lt;p&gt;.&lt;br/&gt;
    + */&lt;br/&gt;
    +@RunWith(PowerMockRunner.class)&lt;br/&gt;
    +@PrepareForTest(KafkaConsumerThread.class)&lt;br/&gt;
    +public class Flink9349Test {&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testConcurrentPartitionsDiscoveryAndLoopFetching() throws Exception {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Likewise, Kafka 08 / 09 / 010 / 011 should all have this test coverage.&lt;/p&gt;</comment>
                            <comment id="16479386" author="githubbot" created="Thu, 17 May 2018 17:02:20 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6040#discussion_r189031075&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6040#discussion_r189031075&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/Flink9349Test.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,207 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka.internal;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.api.common.serialization.SimpleStringSchema;&lt;br/&gt;
    +import org.apache.flink.core.testutils.MultiShotLatch;&lt;br/&gt;
    +import org.apache.flink.core.testutils.OneShotLatch;&lt;br/&gt;
    +import org.apache.flink.metrics.groups.UnregisteredMetricsGroup;&lt;br/&gt;
    +import org.apache.flink.streaming.api.functions.source.SourceFunction;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaCommitCallback;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionStateSentinel;&lt;br/&gt;
    +import org.apache.flink.streaming.runtime.tasks.TestProcessingTimeService;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedDeserializationSchema;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedDeserializationSchemaWrapper;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.ConsumerRecords;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.KafkaConsumer;&lt;br/&gt;
    +import org.junit.Test;&lt;br/&gt;
    +import org.junit.runner.RunWith;&lt;br/&gt;
    +import org.mockito.invocation.InvocationOnMock;&lt;br/&gt;
    +import org.mockito.stubbing.Answer;&lt;br/&gt;
    +import org.powermock.core.classloader.annotations.PrepareForTest;&lt;br/&gt;
    +import org.powermock.modules.junit4.PowerMockRunner;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util.ArrayList;&lt;br/&gt;
    +import java.util.Collections;&lt;br/&gt;
    +import java.util.HashMap;&lt;br/&gt;
    +import java.util.List;&lt;br/&gt;
    +import java.util.Map;&lt;br/&gt;
    +import java.util.Properties;&lt;br/&gt;
    +import java.util.concurrent.CountDownLatch;&lt;br/&gt;
    +import java.util.concurrent.ExecutorService;&lt;br/&gt;
    +import java.util.concurrent.Executors;&lt;br/&gt;
    +import java.util.concurrent.TimeUnit;&lt;br/&gt;
    +import java.util.concurrent.atomic.AtomicReference;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.junit.Assert.assertFalse;&lt;br/&gt;
    +import static org.mockito.Mockito.anyLong;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.doAnswer;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.mock;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.when;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.whenNew;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * Unit tests for the &lt;/p&gt;
{@link Flink9349Test}
&lt;p&gt;.&lt;br/&gt;
    + */&lt;br/&gt;
    +@RunWith(PowerMockRunner.class)&lt;br/&gt;
    +@PrepareForTest(KafkaConsumerThread.class)&lt;br/&gt;
    +public class Flink9349Test {&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testConcurrentPartitionsDiscoveryAndLoopFetching() throws Exception {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I think we should have a similar test, but move it to `Kafka09FetcherTest`.&lt;/p&gt;</comment>
                            <comment id="16479387" author="githubbot" created="Thu, 17 May 2018 17:02:20 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6040#discussion_r189031464&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6040#discussion_r189031464&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/Flink9349Test.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,207 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka.internal;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.api.common.serialization.SimpleStringSchema;&lt;br/&gt;
    +import org.apache.flink.core.testutils.MultiShotLatch;&lt;br/&gt;
    +import org.apache.flink.core.testutils.OneShotLatch;&lt;br/&gt;
    +import org.apache.flink.metrics.groups.UnregisteredMetricsGroup;&lt;br/&gt;
    +import org.apache.flink.streaming.api.functions.source.SourceFunction;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaCommitCallback;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionStateSentinel;&lt;br/&gt;
    +import org.apache.flink.streaming.runtime.tasks.TestProcessingTimeService;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedDeserializationSchema;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedDeserializationSchemaWrapper;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.ConsumerRecords;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.KafkaConsumer;&lt;br/&gt;
    +import org.junit.Test;&lt;br/&gt;
    +import org.junit.runner.RunWith;&lt;br/&gt;
    +import org.mockito.invocation.InvocationOnMock;&lt;br/&gt;
    +import org.mockito.stubbing.Answer;&lt;br/&gt;
    +import org.powermock.core.classloader.annotations.PrepareForTest;&lt;br/&gt;
    +import org.powermock.modules.junit4.PowerMockRunner;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util.ArrayList;&lt;br/&gt;
    +import java.util.Collections;&lt;br/&gt;
    +import java.util.HashMap;&lt;br/&gt;
    +import java.util.List;&lt;br/&gt;
    +import java.util.Map;&lt;br/&gt;
    +import java.util.Properties;&lt;br/&gt;
    +import java.util.concurrent.CountDownLatch;&lt;br/&gt;
    +import java.util.concurrent.ExecutorService;&lt;br/&gt;
    +import java.util.concurrent.Executors;&lt;br/&gt;
    +import java.util.concurrent.TimeUnit;&lt;br/&gt;
    +import java.util.concurrent.atomic.AtomicReference;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.junit.Assert.assertFalse;&lt;br/&gt;
    +import static org.mockito.Mockito.anyLong;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.doAnswer;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.mock;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.when;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.whenNew;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * Unit tests for the &lt;/p&gt;
{@link Flink9349Test}
&lt;p&gt;.&lt;br/&gt;
    + */&lt;br/&gt;
    +@RunWith(PowerMockRunner.class)&lt;br/&gt;
    +@PrepareForTest(KafkaConsumerThread.class)&lt;br/&gt;
    +public class Flink9349Test {&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testConcurrentPartitionsDiscoveryAndLoopFetching() throws Exception {&lt;br/&gt;
    +&lt;br/&gt;
    +		// test data&lt;br/&gt;
    +		final KafkaTopicPartition testPartition = new KafkaTopicPartition(&quot;test&quot;, 42);&lt;br/&gt;
    +		final Map&amp;lt;KafkaTopicPartition, Long&amp;gt; testCommitData = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
    +		testCommitData.put(testPartition, 11L);&lt;br/&gt;
    +&lt;br/&gt;
    +		// to synchronize when the consumer is in its blocking method&lt;br/&gt;
    +		final OneShotLatch sync = new OneShotLatch();&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- the mock consumer with blocking poll calls ----&lt;br/&gt;
    +		final MultiShotLatch blockerLatch = new MultiShotLatch();&lt;br/&gt;
    +&lt;br/&gt;
    +		KafkaConsumer&amp;lt;?, ?&amp;gt; mockConsumer = mock(KafkaConsumer.class);&lt;br/&gt;
    +		when(mockConsumer.poll(anyLong())).thenAnswer(new Answer&amp;lt;ConsumerRecords&amp;lt;?, ?&amp;gt;&amp;gt;() {&lt;br/&gt;
    +&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public ConsumerRecords&amp;lt;?, ?&amp;gt; answer(InvocationOnMock invocation) throws InterruptedException &lt;/p&gt;
{
    +				sync.trigger();
    +				blockerLatch.await();
    +				return ConsumerRecords.empty();
    +			}
&lt;p&gt;    +		});&lt;br/&gt;
    +&lt;br/&gt;
    +		doAnswer(new Answer&amp;lt;Void&amp;gt;() {&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public Void answer(InvocationOnMock invocation) &lt;/p&gt;
{
    +				blockerLatch.trigger();
    +				return null;
    +			}
&lt;p&gt;    +		}).when(mockConsumer).wakeup();&lt;br/&gt;
    +&lt;br/&gt;
    +		// make sure the fetcher creates the mock consumer&lt;br/&gt;
    +		whenNew(KafkaConsumer.class).withAnyArguments().thenReturn(mockConsumer);&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- create the test fetcher -----&lt;br/&gt;
    +&lt;br/&gt;
    +		@SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
    +		SourceFunction.SourceContext&amp;lt;String&amp;gt; sourceContext = mock(SourceFunction.SourceContext.class);&lt;br/&gt;
    +		Map&amp;lt;KafkaTopicPartition, Long&amp;gt; partitionsWithInitialOffsets =&lt;br/&gt;
    +			Collections.singletonMap(new KafkaTopicPartition(&quot;test&quot;, 42), KafkaTopicPartitionStateSentinel.GROUP_OFFSET);&lt;br/&gt;
    +		KeyedDeserializationSchema&amp;lt;String&amp;gt; schema = new KeyedDeserializationSchemaWrapper&amp;lt;&amp;gt;(new SimpleStringSchema());&lt;br/&gt;
    +&lt;br/&gt;
    +		final Kafka09Fetcher&amp;lt;String&amp;gt; fetcher = new Kafka09Fetcher&amp;lt;&amp;gt;(&lt;br/&gt;
    +			sourceContext,&lt;br/&gt;
    +			partitionsWithInitialOffsets,&lt;br/&gt;
    +			null, /* periodic watermark extractor */&lt;br/&gt;
    +			null, /* punctuated watermark extractor */&lt;br/&gt;
    +			new TestProcessingTimeService(),&lt;br/&gt;
    +			10, /* watermark interval */&lt;br/&gt;
    +			this.getClass().getClassLoader(),&lt;br/&gt;
    +			&quot;task_name&quot;,&lt;br/&gt;
    +			schema,&lt;br/&gt;
    +			new Properties(),&lt;br/&gt;
    +			0L,&lt;br/&gt;
    +			new UnregisteredMetricsGroup(),&lt;br/&gt;
    +			new UnregisteredMetricsGroup(),&lt;br/&gt;
    +			false);&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- run the fetcher -----&lt;br/&gt;
    +&lt;br/&gt;
    +		final AtomicReference&amp;lt;Throwable&amp;gt; error = new AtomicReference&amp;lt;&amp;gt;();&lt;br/&gt;
    +		int fetchTasks = 2;&lt;br/&gt;
    +		final CountDownLatch latch = new CountDownLatch(fetchTasks);&lt;br/&gt;
    +		ExecutorService service = Executors.newFixedThreadPool(fetchTasks + 1);&lt;br/&gt;
    +&lt;br/&gt;
    +		service.submit(new Thread(&quot;fetcher runner &quot;) {&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public void run() {&lt;br/&gt;
    +				try &lt;/p&gt;
{
    +					latch.await();
    +					fetcher.runFetchLoop();
    +				}
&lt;p&gt; catch (Throwable t) &lt;/p&gt;
{
    +					error.set(t);
    +				}
&lt;p&gt;    +			}&lt;br/&gt;
    +		});&lt;br/&gt;
    +		for (int i = 0; i &amp;lt; fetchTasks; i++) {&lt;br/&gt;
    +			service.submit(new Thread(&quot;add partitions &quot; + i) {&lt;br/&gt;
    +&lt;br/&gt;
    +				@Override&lt;br/&gt;
    +				public void run() {&lt;br/&gt;
    +					try {&lt;br/&gt;
    +						List&amp;lt;KafkaTopicPartition&amp;gt; newPartitions = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
    +						for (int i = 0; i &amp;lt; 1000; i++) &lt;/p&gt;
{
    +							newPartitions.add(testPartition);
    +						}
&lt;p&gt;    +						fetcher.addDiscoveredPartitions(newPartitions);&lt;br/&gt;
    +						latch.countDown();&lt;br/&gt;
    +						//latch.await();&lt;br/&gt;
    +						for (int i = 0; i &amp;lt; 100; i++) &lt;/p&gt;
{
    +							fetcher.addDiscoveredPartitions(newPartitions);
    +							Thread.sleep(1L);
    +						}
&lt;p&gt;    +					} catch (Throwable t) &lt;/p&gt;
{
    +						error.set(t);
    +					}
&lt;p&gt;    +				}&lt;br/&gt;
    +			});&lt;br/&gt;
    +		}&lt;br/&gt;
    +&lt;br/&gt;
    +		service.awaitTermination(1L, TimeUnit.SECONDS);&lt;br/&gt;
    +&lt;br/&gt;
    +		// wait until the fetcher has reached the method of interest&lt;br/&gt;
    +		sync.await();&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- trigger the offset commit -----&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I don&apos;t think this is required for the scope of interest of this test, is it?&lt;/p&gt;</comment>
                            <comment id="16479388" author="githubbot" created="Thu, 17 May 2018 17:02:20 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6040#discussion_r189029077&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6040#discussion_r189029077&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/internals/AbstractFetcher.java &amp;#8212;&lt;br/&gt;
    @@ -507,7 +507,7 @@ private void updateMinPunctuatedWatermark(Watermark nextWatermark) {&lt;br/&gt;
     			SerializedValue&amp;lt;AssignerWithPunctuatedWatermarks&amp;lt;T&amp;gt;&amp;gt; watermarksPunctuated,&lt;br/&gt;
     			ClassLoader userCodeClassLoader) throws IOException, ClassNotFoundException {&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;List&amp;lt;KafkaTopicPartitionState&amp;lt;KPH&amp;gt;&amp;gt; partitionStates = new LinkedList&amp;lt;&amp;gt;();&lt;br/&gt;
    +		List&amp;lt;KafkaTopicPartitionState&amp;lt;KPH&amp;gt;&amp;gt; partitionStates = new CopyOnWriteArrayList&amp;lt;&amp;gt;();
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Would be nice to have a comment on why we need to use a `CopyOnWriteArrayList`&lt;/p&gt;</comment>
                            <comment id="16479391" author="githubbot" created="Thu, 17 May 2018 17:05:38 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6040&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6040&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Thanks for the PR @snuyanzin!&lt;br/&gt;
    I had some comments, please let me know what you think.&lt;/p&gt;

&lt;p&gt;    Also, some general contribution tips:&lt;br/&gt;
    1. I would suggest the title of the PR to be something along the lines of &quot;&lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-9349&quot; title=&quot;KafkaConnector Exception  while fetching from multiple kafka topics&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-9349&quot;&gt;&lt;del&gt;FLINK-9349&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; Fix ConcurrentModificationException when add discovered partitions&quot;. That directly makes it clear what exactly is being fixed.&lt;br/&gt;
    2. The message of the first commit of the PR should also be appropriately set to be similar to the title (most of the time if it is a 1-commit PR, the title of the PR and the commit message can be identical).&lt;/p&gt;</comment>
                            <comment id="16479404" author="githubbot" created="Thu, 17 May 2018 17:15:08 +0000"  >&lt;p&gt;Github user snuyanzin commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6040#discussion_r189035401&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6040#discussion_r189035401&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/internals/AbstractFetcher.java &amp;#8212;&lt;br/&gt;
    @@ -507,7 +507,7 @@ private void updateMinPunctuatedWatermark(Watermark nextWatermark) {&lt;br/&gt;
     			SerializedValue&amp;lt;AssignerWithPunctuatedWatermarks&amp;lt;T&amp;gt;&amp;gt; watermarksPunctuated,&lt;br/&gt;
     			ClassLoader userCodeClassLoader) throws IOException, ClassNotFoundException {&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;List&amp;lt;KafkaTopicPartitionState&amp;lt;KPH&amp;gt;&amp;gt; partitionStates = new LinkedList&amp;lt;&amp;gt;();&lt;br/&gt;
    +		List&amp;lt;KafkaTopicPartitionState&amp;lt;KPH&amp;gt;&amp;gt; partitionStates = new CopyOnWriteArrayList&amp;lt;&amp;gt;();
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Yes you are right. A question: is it allowed to specify a link on the issue comment where it was decided to use CopyOnWriteArrayList? Or is it better to have explanation in a comment only?&lt;/p&gt;</comment>
                            <comment id="16479414" author="githubbot" created="Thu, 17 May 2018 17:19:52 +0000"  >&lt;p&gt;Github user tedyu commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6040#discussion_r189036753&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6040#discussion_r189036753&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/internals/AbstractFetcher.java &amp;#8212;&lt;br/&gt;
    @@ -507,7 +507,7 @@ private void updateMinPunctuatedWatermark(Watermark nextWatermark) {&lt;br/&gt;
     			SerializedValue&amp;lt;AssignerWithPunctuatedWatermarks&amp;lt;T&amp;gt;&amp;gt; watermarksPunctuated,&lt;br/&gt;
     			ClassLoader userCodeClassLoader) throws IOException, ClassNotFoundException {&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;List&amp;lt;KafkaTopicPartitionState&amp;lt;KPH&amp;gt;&amp;gt; partitionStates = new LinkedList&amp;lt;&amp;gt;();&lt;br/&gt;
    +		List&amp;lt;KafkaTopicPartitionState&amp;lt;KPH&amp;gt;&amp;gt; partitionStates = new CopyOnWriteArrayList&amp;lt;&amp;gt;();
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    explanation can be made with a comment.&lt;br/&gt;
    No need to link to issue comment.&lt;/p&gt;</comment>
                            <comment id="16479426" author="githubbot" created="Thu, 17 May 2018 17:26:43 +0000"  >&lt;p&gt;Github user snuyanzin commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6040#discussion_r189038623&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6040#discussion_r189038623&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/Flink9349Test.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,207 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka.internal;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.api.common.serialization.SimpleStringSchema;&lt;br/&gt;
    +import org.apache.flink.core.testutils.MultiShotLatch;&lt;br/&gt;
    +import org.apache.flink.core.testutils.OneShotLatch;&lt;br/&gt;
    +import org.apache.flink.metrics.groups.UnregisteredMetricsGroup;&lt;br/&gt;
    +import org.apache.flink.streaming.api.functions.source.SourceFunction;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaCommitCallback;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionStateSentinel;&lt;br/&gt;
    +import org.apache.flink.streaming.runtime.tasks.TestProcessingTimeService;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedDeserializationSchema;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedDeserializationSchemaWrapper;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.ConsumerRecords;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.KafkaConsumer;&lt;br/&gt;
    +import org.junit.Test;&lt;br/&gt;
    +import org.junit.runner.RunWith;&lt;br/&gt;
    +import org.mockito.invocation.InvocationOnMock;&lt;br/&gt;
    +import org.mockito.stubbing.Answer;&lt;br/&gt;
    +import org.powermock.core.classloader.annotations.PrepareForTest;&lt;br/&gt;
    +import org.powermock.modules.junit4.PowerMockRunner;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util.ArrayList;&lt;br/&gt;
    +import java.util.Collections;&lt;br/&gt;
    +import java.util.HashMap;&lt;br/&gt;
    +import java.util.List;&lt;br/&gt;
    +import java.util.Map;&lt;br/&gt;
    +import java.util.Properties;&lt;br/&gt;
    +import java.util.concurrent.CountDownLatch;&lt;br/&gt;
    +import java.util.concurrent.ExecutorService;&lt;br/&gt;
    +import java.util.concurrent.Executors;&lt;br/&gt;
    +import java.util.concurrent.TimeUnit;&lt;br/&gt;
    +import java.util.concurrent.atomic.AtomicReference;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.junit.Assert.assertFalse;&lt;br/&gt;
    +import static org.mockito.Mockito.anyLong;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.doAnswer;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.mock;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.when;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.whenNew;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * Unit tests for the &lt;/p&gt;
{@link Flink9349Test}
&lt;p&gt;.&lt;br/&gt;
    + */&lt;br/&gt;
    +@RunWith(PowerMockRunner.class)&lt;br/&gt;
    +@PrepareForTest(KafkaConsumerThread.class)&lt;br/&gt;
    +public class Flink9349Test {&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testConcurrentPartitionsDiscoveryAndLoopFetching() throws Exception {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Do you mean to have such test as a separate method in each Kafka KafkaXYFetcherTest class? &lt;/p&gt;
</comment>
                            <comment id="16480198" author="githubbot" created="Fri, 18 May 2018 05:51:13 +0000"  >&lt;p&gt;Github user snuyanzin commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6040#discussion_r189168596&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6040#discussion_r189168596&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/Flink9349Test.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,207 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka.internal;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.api.common.serialization.SimpleStringSchema;&lt;br/&gt;
    +import org.apache.flink.core.testutils.MultiShotLatch;&lt;br/&gt;
    +import org.apache.flink.core.testutils.OneShotLatch;&lt;br/&gt;
    +import org.apache.flink.metrics.groups.UnregisteredMetricsGroup;&lt;br/&gt;
    +import org.apache.flink.streaming.api.functions.source.SourceFunction;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaCommitCallback;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionStateSentinel;&lt;br/&gt;
    +import org.apache.flink.streaming.runtime.tasks.TestProcessingTimeService;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedDeserializationSchema;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedDeserializationSchemaWrapper;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.ConsumerRecords;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.KafkaConsumer;&lt;br/&gt;
    +import org.junit.Test;&lt;br/&gt;
    +import org.junit.runner.RunWith;&lt;br/&gt;
    +import org.mockito.invocation.InvocationOnMock;&lt;br/&gt;
    +import org.mockito.stubbing.Answer;&lt;br/&gt;
    +import org.powermock.core.classloader.annotations.PrepareForTest;&lt;br/&gt;
    +import org.powermock.modules.junit4.PowerMockRunner;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util.ArrayList;&lt;br/&gt;
    +import java.util.Collections;&lt;br/&gt;
    +import java.util.HashMap;&lt;br/&gt;
    +import java.util.List;&lt;br/&gt;
    +import java.util.Map;&lt;br/&gt;
    +import java.util.Properties;&lt;br/&gt;
    +import java.util.concurrent.CountDownLatch;&lt;br/&gt;
    +import java.util.concurrent.ExecutorService;&lt;br/&gt;
    +import java.util.concurrent.Executors;&lt;br/&gt;
    +import java.util.concurrent.TimeUnit;&lt;br/&gt;
    +import java.util.concurrent.atomic.AtomicReference;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.junit.Assert.assertFalse;&lt;br/&gt;
    +import static org.mockito.Mockito.anyLong;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.doAnswer;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.mock;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.when;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.whenNew;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * Unit tests for the &lt;/p&gt;
{@link Flink9349Test}
&lt;p&gt;.&lt;br/&gt;
    + */&lt;br/&gt;
    +@RunWith(PowerMockRunner.class)&lt;br/&gt;
    +@PrepareForTest(KafkaConsumerThread.class)&lt;br/&gt;
    +public class Flink9349Test {&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testConcurrentPartitionsDiscoveryAndLoopFetching() throws Exception {&lt;br/&gt;
    +&lt;br/&gt;
    +		// test data&lt;br/&gt;
    +		final KafkaTopicPartition testPartition = new KafkaTopicPartition(&quot;test&quot;, 42);&lt;br/&gt;
    +		final Map&amp;lt;KafkaTopicPartition, Long&amp;gt; testCommitData = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
    +		testCommitData.put(testPartition, 11L);&lt;br/&gt;
    +&lt;br/&gt;
    +		// to synchronize when the consumer is in its blocking method&lt;br/&gt;
    +		final OneShotLatch sync = new OneShotLatch();&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- the mock consumer with blocking poll calls ----&lt;br/&gt;
    +		final MultiShotLatch blockerLatch = new MultiShotLatch();&lt;br/&gt;
    +&lt;br/&gt;
    +		KafkaConsumer&amp;lt;?, ?&amp;gt; mockConsumer = mock(KafkaConsumer.class);&lt;br/&gt;
    +		when(mockConsumer.poll(anyLong())).thenAnswer(new Answer&amp;lt;ConsumerRecords&amp;lt;?, ?&amp;gt;&amp;gt;() {&lt;br/&gt;
    +&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public ConsumerRecords&amp;lt;?, ?&amp;gt; answer(InvocationOnMock invocation) throws InterruptedException &lt;/p&gt;
{
    +				sync.trigger();
    +				blockerLatch.await();
    +				return ConsumerRecords.empty();
    +			}
&lt;p&gt;    +		});&lt;br/&gt;
    +&lt;br/&gt;
    +		doAnswer(new Answer&amp;lt;Void&amp;gt;() {&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public Void answer(InvocationOnMock invocation) &lt;/p&gt;
{
    +				blockerLatch.trigger();
    +				return null;
    +			}
&lt;p&gt;    +		}).when(mockConsumer).wakeup();&lt;br/&gt;
    +&lt;br/&gt;
    +		// make sure the fetcher creates the mock consumer&lt;br/&gt;
    +		whenNew(KafkaConsumer.class).withAnyArguments().thenReturn(mockConsumer);&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- create the test fetcher -----&lt;br/&gt;
    +&lt;br/&gt;
    +		@SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
    +		SourceFunction.SourceContext&amp;lt;String&amp;gt; sourceContext = mock(SourceFunction.SourceContext.class);&lt;br/&gt;
    +		Map&amp;lt;KafkaTopicPartition, Long&amp;gt; partitionsWithInitialOffsets =&lt;br/&gt;
    +			Collections.singletonMap(new KafkaTopicPartition(&quot;test&quot;, 42), KafkaTopicPartitionStateSentinel.GROUP_OFFSET);&lt;br/&gt;
    +		KeyedDeserializationSchema&amp;lt;String&amp;gt; schema = new KeyedDeserializationSchemaWrapper&amp;lt;&amp;gt;(new SimpleStringSchema());&lt;br/&gt;
    +&lt;br/&gt;
    +		final Kafka09Fetcher&amp;lt;String&amp;gt; fetcher = new Kafka09Fetcher&amp;lt;&amp;gt;(&lt;br/&gt;
    +			sourceContext,&lt;br/&gt;
    +			partitionsWithInitialOffsets,&lt;br/&gt;
    +			null, /* periodic watermark extractor */&lt;br/&gt;
    +			null, /* punctuated watermark extractor */&lt;br/&gt;
    +			new TestProcessingTimeService(),&lt;br/&gt;
    +			10, /* watermark interval */&lt;br/&gt;
    +			this.getClass().getClassLoader(),&lt;br/&gt;
    +			&quot;task_name&quot;,&lt;br/&gt;
    +			schema,&lt;br/&gt;
    +			new Properties(),&lt;br/&gt;
    +			0L,&lt;br/&gt;
    +			new UnregisteredMetricsGroup(),&lt;br/&gt;
    +			new UnregisteredMetricsGroup(),&lt;br/&gt;
    +			false);&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- run the fetcher -----&lt;br/&gt;
    +&lt;br/&gt;
    +		final AtomicReference&amp;lt;Throwable&amp;gt; error = new AtomicReference&amp;lt;&amp;gt;();&lt;br/&gt;
    +		int fetchTasks = 2;&lt;br/&gt;
    +		final CountDownLatch latch = new CountDownLatch(fetchTasks);&lt;br/&gt;
    +		ExecutorService service = Executors.newFixedThreadPool(fetchTasks + 1);&lt;br/&gt;
    +&lt;br/&gt;
    +		service.submit(new Thread(&quot;fetcher runner &quot;) {&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public void run() {&lt;br/&gt;
    +				try &lt;/p&gt;
{
    +					latch.await();
    +					fetcher.runFetchLoop();
    +				}
&lt;p&gt; catch (Throwable t) &lt;/p&gt;
{
    +					error.set(t);
    +				}
&lt;p&gt;    +			}&lt;br/&gt;
    +		});&lt;br/&gt;
    +		for (int i = 0; i &amp;lt; fetchTasks; i++) {&lt;br/&gt;
    +			service.submit(new Thread(&quot;add partitions &quot; + i) {&lt;br/&gt;
    +&lt;br/&gt;
    +				@Override&lt;br/&gt;
    +				public void run() {&lt;br/&gt;
    +					try {&lt;br/&gt;
    +						List&amp;lt;KafkaTopicPartition&amp;gt; newPartitions = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
    +						for (int i = 0; i &amp;lt; 1000; i++) &lt;/p&gt;
{
    +							newPartitions.add(testPartition);
    +						}
&lt;p&gt;    +						fetcher.addDiscoveredPartitions(newPartitions);&lt;br/&gt;
    +						latch.countDown();&lt;br/&gt;
    +						//latch.await();&lt;br/&gt;
    +						for (int i = 0; i &amp;lt; 100; i++) &lt;/p&gt;
{
    +							fetcher.addDiscoveredPartitions(newPartitions);
    +							Thread.sleep(1L);
    +						}
&lt;p&gt;    +					} catch (Throwable t) &lt;/p&gt;
{
    +						error.set(t);
    +					}
&lt;p&gt;    +				}&lt;br/&gt;
    +			});&lt;br/&gt;
    +		}&lt;br/&gt;
    +&lt;br/&gt;
    +		service.awaitTermination(1L, TimeUnit.SECONDS);&lt;br/&gt;
    +&lt;br/&gt;
    +		// wait until the fetcher has reached the method of interest&lt;br/&gt;
    +		sync.await();&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- trigger the offset commit -----&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    thank you &lt;br/&gt;
    you are right I removed it and some other useless stuff from this test&lt;/p&gt;</comment>
                            <comment id="16480207" author="githubbot" created="Fri, 18 May 2018 06:06:27 +0000"  >&lt;p&gt;Github user snuyanzin commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6040&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6040&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @tzulitai, @tedyu thnk you for your review, comments and contribution tips&lt;br/&gt;
    I did updates which includes moving test into AbstractFetcherTest and making it kafka connector version independent&lt;/p&gt;

&lt;p&gt;    Could you please help me a bit?&lt;br/&gt;
    Suddenly the travis build failed on YARNSessionCapacitySchedulerITCase (only on flink travis, on my fork it passed several times).  It does not look like result of changes as there is nothing related to yarn. Anyway I tried to investigate it. I found several similar issues on jira however they are closed. &lt;/p&gt;

&lt;p&gt;    Also I downloaded logs mentioned in failed travis job &lt;/p&gt;

&lt;p&gt;    &amp;gt; Uploading to transfer.sh&lt;br/&gt;
    &lt;a href=&quot;https://transfer.sh/JspTz/24547.10.tar.gz&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://transfer.sh/JspTz/24547.10.tar.gz&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    based on them it looks like there was a connectivity issue with one of the ApplicationMaster &lt;br/&gt;
    as log yarn-tests/container_1526608500321_0007_01_000001/job-manager.log is full of&lt;br/&gt;
    &amp;gt; 2018-05-18 01:56:49,448 WARN  akka.remote.transport.netty.NettyTransport                    - Remote connection to &lt;span class=&quot;error&quot;&gt;&amp;#91;null&amp;#93;&lt;/span&gt; failed with java.net.ConnectException: Connection refused: travis-job-2a2afdc5-7bf8-4597-946e-16551a5ebbc4/127.0.1.1:43980&lt;br/&gt;
    2018-05-18 01:56:49,449 WARN  akka.remote.ReliableDeliverySupervisor                        - Association with remote system &lt;span class=&quot;error&quot;&gt;&amp;#91;akka.tcp://flink@travis-job-2a2afdc5-7bf8-4597-946e-16551a5ebbc4:43980&amp;#93;&lt;/span&gt; has failed, address is now gated for &lt;span class=&quot;error&quot;&gt;&amp;#91;50&amp;#93;&lt;/span&gt; ms. Reason: [Association failed with &lt;span class=&quot;error&quot;&gt;&amp;#91;akka.tcp://flink@travis-job-2a2afdc5-7bf8-4597-946e-16551a5ebbc4:43980&amp;#93;&lt;/span&gt;] Caused by: &lt;span class=&quot;error&quot;&gt;&amp;#91;Connection refused: travis-job-2a2afdc5-7bf8-4597-946e-16551a5ebbc4/127.0.1.1:43980&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;    very strange thing &lt;/p&gt;

&lt;p&gt;    &amp;gt; Remote connection to &lt;span class=&quot;error&quot;&gt;&amp;#91;null&amp;#93;&lt;/span&gt; &lt;/p&gt;
</comment>
                            <comment id="16480262" author="githubbot" created="Fri, 18 May 2018 07:02:17 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6040&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6040&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @snuyanzin the failing `YARNSessionCapacitySchedulerITCase` is known to be bit flaky, so you can safely ignore that for now. I&apos;ll take another look at your changes soon. Thanks!&lt;/p&gt;</comment>
                            <comment id="16486691" author="githubbot" created="Wed, 23 May 2018 04:36:52 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6040#discussion_r190111078&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6040#discussion_r190111078&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/internals/AbstractFetcherTest.java &amp;#8212;&lt;br/&gt;
    @@ -390,6 +398,102 @@ public void testPeriodicWatermarksWithNoSubscribedPartitionsShouldYieldNoWaterma&lt;br/&gt;
     		assertEquals(100, sourceContext.getLatestWatermark().getTimestamp());&lt;br/&gt;
     	}&lt;/p&gt;

&lt;p&gt;    +	@Test&lt;br/&gt;
    +	public void testConcurrentPartitionsDiscoveryAndLoopFetching() throws Exception {&lt;br/&gt;
    +		// test data&lt;br/&gt;
    +		final KafkaTopicPartition testPartition = new KafkaTopicPartition(&quot;test&quot;, 42);&lt;br/&gt;
    +&lt;br/&gt;
    +		final Map&amp;lt;KafkaTopicPartition, Long&amp;gt; testCommitData = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
    +		testCommitData.put(testPartition, 11L);&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- create the test fetcher -----&lt;br/&gt;
    +&lt;br/&gt;
    +		@SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
    +		SourceContext&amp;lt;String&amp;gt; sourceContext = PowerMockito.mock(SourceContext.class);&lt;br/&gt;
    +		Map&amp;lt;KafkaTopicPartition, Long&amp;gt; partitionsWithInitialOffsets =&lt;br/&gt;
    +			Collections.singletonMap(testPartition, KafkaTopicPartitionStateSentinel.GROUP_OFFSET);&lt;br/&gt;
    +&lt;br/&gt;
    +		final TestFetcher&amp;lt;String&amp;gt; fetcher = new TestFetcher&amp;lt;&amp;gt;(&lt;br/&gt;
    +			sourceContext,&lt;br/&gt;
    +			partitionsWithInitialOffsets,&lt;br/&gt;
    +			null, /* periodic assigner */&lt;br/&gt;
    +			null, /* punctuated assigner */&lt;br/&gt;
    +			new TestProcessingTimeService(),&lt;br/&gt;
    +			10);&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- run the fetcher -----&lt;br/&gt;
    +&lt;br/&gt;
    +		final AtomicReference&amp;lt;Throwable&amp;gt; error = new AtomicReference&amp;lt;&amp;gt;();&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Flink provides a `CheckedThread` utility so you don&apos;t have to do this thread error referencing.&lt;/p&gt;</comment>
                            <comment id="16486692" author="githubbot" created="Wed, 23 May 2018 04:36:52 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6040#discussion_r190110928&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6040#discussion_r190110928&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/internals/AbstractFetcherTest.java &amp;#8212;&lt;br/&gt;
    @@ -390,6 +398,102 @@ public void testPeriodicWatermarksWithNoSubscribedPartitionsShouldYieldNoWaterma&lt;br/&gt;
     		assertEquals(100, sourceContext.getLatestWatermark().getTimestamp());&lt;br/&gt;
     	}&lt;/p&gt;

&lt;p&gt;    +	@Test&lt;br/&gt;
    +	public void testConcurrentPartitionsDiscoveryAndLoopFetching() throws Exception {&lt;br/&gt;
    +		// test data&lt;br/&gt;
    +		final KafkaTopicPartition testPartition = new KafkaTopicPartition(&quot;test&quot;, 42);&lt;br/&gt;
    +&lt;br/&gt;
    +		final Map&amp;lt;KafkaTopicPartition, Long&amp;gt; testCommitData = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
    +		testCommitData.put(testPartition, 11L);&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- create the test fetcher -----&lt;br/&gt;
    +&lt;br/&gt;
    +		@SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
    +		SourceContext&amp;lt;String&amp;gt; sourceContext = PowerMockito.mock(SourceContext.class);&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    It is unnecessary to use a power mock here. A dummy implementation of a `SourceContext` will be better.&lt;/p&gt;</comment>
                            <comment id="16486693" author="githubbot" created="Wed, 23 May 2018 04:36:52 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6040#discussion_r190111239&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6040#discussion_r190111239&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/internals/AbstractFetcherTest.java &amp;#8212;&lt;br/&gt;
    @@ -390,6 +398,102 @@ public void testPeriodicWatermarksWithNoSubscribedPartitionsShouldYieldNoWaterma&lt;br/&gt;
     		assertEquals(100, sourceContext.getLatestWatermark().getTimestamp());&lt;br/&gt;
     	}&lt;/p&gt;

&lt;p&gt;    +	@Test&lt;br/&gt;
    +	public void testConcurrentPartitionsDiscoveryAndLoopFetching() throws Exception {&lt;br/&gt;
    +		// test data&lt;br/&gt;
    +		final KafkaTopicPartition testPartition = new KafkaTopicPartition(&quot;test&quot;, 42);&lt;br/&gt;
    +&lt;br/&gt;
    +		final Map&amp;lt;KafkaTopicPartition, Long&amp;gt; testCommitData = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
    +		testCommitData.put(testPartition, 11L);&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- create the test fetcher -----&lt;br/&gt;
    +&lt;br/&gt;
    +		@SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
    +		SourceContext&amp;lt;String&amp;gt; sourceContext = PowerMockito.mock(SourceContext.class);&lt;br/&gt;
    +		Map&amp;lt;KafkaTopicPartition, Long&amp;gt; partitionsWithInitialOffsets =&lt;br/&gt;
    +			Collections.singletonMap(testPartition, KafkaTopicPartitionStateSentinel.GROUP_OFFSET);&lt;br/&gt;
    +&lt;br/&gt;
    +		final TestFetcher&amp;lt;String&amp;gt; fetcher = new TestFetcher&amp;lt;&amp;gt;(&lt;br/&gt;
    +			sourceContext,&lt;br/&gt;
    +			partitionsWithInitialOffsets,&lt;br/&gt;
    +			null, /* periodic assigner */&lt;br/&gt;
    +			null, /* punctuated assigner */&lt;br/&gt;
    +			new TestProcessingTimeService(),&lt;br/&gt;
    +			10);&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- run the fetcher -----&lt;br/&gt;
    +&lt;br/&gt;
    +		final AtomicReference&amp;lt;Throwable&amp;gt; error = new AtomicReference&amp;lt;&amp;gt;();&lt;br/&gt;
    +		int fetchTasks = 5;&lt;br/&gt;
    +		final CountDownLatch latch = new CountDownLatch(fetchTasks);&lt;br/&gt;
    +		ExecutorService service = Executors.newFixedThreadPool(fetchTasks + 1);&lt;br/&gt;
    +&lt;br/&gt;
    +		service.submit(new Thread(&quot;fetcher runner&quot;) {&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public void run() {&lt;br/&gt;
    +				try &lt;/p&gt;
{
    +					latch.await();
    +					fetcher.runFetchLoop();
    +				}
&lt;p&gt; catch (Throwable t) &lt;/p&gt;
{
    +					error.set(t);
    +				}
&lt;p&gt;    +			}&lt;br/&gt;
    +		});&lt;br/&gt;
    +&lt;br/&gt;
    +		for (int i = 0; i &amp;lt; fetchTasks; i++) {&lt;br/&gt;
    +			service.submit(new Thread(&quot;add partitions &quot; + i) {&lt;br/&gt;
    +				@Override&lt;br/&gt;
    +				public void run() {&lt;br/&gt;
    +					try {&lt;br/&gt;
    +						List&amp;lt;KafkaTopicPartition&amp;gt; newPartitions = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
    +						for (int i = 0; i &amp;lt; 1000; i++) &lt;/p&gt;
{
    +							newPartitions.add(testPartition);
    +						}
&lt;p&gt;    +						fetcher.addDiscoveredPartitions(newPartitions);&lt;br/&gt;
    +						latch.countDown();&lt;br/&gt;
    +						for (int i = 0; i &amp;lt; 100; i++) &lt;/p&gt;
{
    +							fetcher.addDiscoveredPartitions(newPartitions);
    +							Thread.sleep(1L);
    +						}
&lt;p&gt;    +					} catch (Throwable t) &lt;/p&gt;
{
    +						error.set(t);
    +					}
&lt;p&gt;    +				}&lt;br/&gt;
    +			});&lt;br/&gt;
    +		}&lt;br/&gt;
    +&lt;br/&gt;
    +		service.awaitTermination(1L, TimeUnit.SECONDS);&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- trigger the offset commit -----&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    We should be able to ignore offset commit triggering in this test&lt;/p&gt;</comment>
                            <comment id="16486694" author="githubbot" created="Wed, 23 May 2018 04:36:52 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6040#discussion_r190114844&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6040#discussion_r190114844&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/internals/AbstractFetcherTest.java &amp;#8212;&lt;br/&gt;
    @@ -390,6 +398,102 @@ public void testPeriodicWatermarksWithNoSubscribedPartitionsShouldYieldNoWaterma&lt;br/&gt;
     		assertEquals(100, sourceContext.getLatestWatermark().getTimestamp());&lt;br/&gt;
     	}&lt;/p&gt;

&lt;p&gt;    +	@Test&lt;br/&gt;
    +	public void testConcurrentPartitionsDiscoveryAndLoopFetching() throws Exception {&lt;br/&gt;
    +		// test data&lt;br/&gt;
    +		final KafkaTopicPartition testPartition = new KafkaTopicPartition(&quot;test&quot;, 42);&lt;br/&gt;
    +&lt;br/&gt;
    +		final Map&amp;lt;KafkaTopicPartition, Long&amp;gt; testCommitData = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
    +		testCommitData.put(testPartition, 11L);&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- create the test fetcher -----&lt;br/&gt;
    +&lt;br/&gt;
    +		@SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
    +		SourceContext&amp;lt;String&amp;gt; sourceContext = PowerMockito.mock(SourceContext.class);&lt;br/&gt;
    +		Map&amp;lt;KafkaTopicPartition, Long&amp;gt; partitionsWithInitialOffsets =&lt;br/&gt;
    +			Collections.singletonMap(testPartition, KafkaTopicPartitionStateSentinel.GROUP_OFFSET);&lt;br/&gt;
    +&lt;br/&gt;
    +		final TestFetcher&amp;lt;String&amp;gt; fetcher = new TestFetcher&amp;lt;&amp;gt;(&lt;br/&gt;
    +			sourceContext,&lt;br/&gt;
    +			partitionsWithInitialOffsets,&lt;br/&gt;
    +			null, /* periodic assigner */&lt;br/&gt;
    +			null, /* punctuated assigner */&lt;br/&gt;
    +			new TestProcessingTimeService(),&lt;br/&gt;
    +			10);&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- run the fetcher -----&lt;br/&gt;
    +&lt;br/&gt;
    +		final AtomicReference&amp;lt;Throwable&amp;gt; error = new AtomicReference&amp;lt;&amp;gt;();&lt;br/&gt;
    +		int fetchTasks = 5;&lt;br/&gt;
    +		final CountDownLatch latch = new CountDownLatch(fetchTasks);&lt;br/&gt;
    +		ExecutorService service = Executors.newFixedThreadPool(fetchTasks + 1);&lt;br/&gt;
    +&lt;br/&gt;
    +		service.submit(new Thread(&quot;fetcher runner&quot;) {&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public void run() {&lt;br/&gt;
    +				try {&lt;br/&gt;
    +					latch.await();&lt;br/&gt;
    +					fetcher.runFetchLoop();&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    The sequence here seems a bit odd to me.&lt;/p&gt;

&lt;p&gt;    I think we should be testing this as follows:&lt;br/&gt;
    1. Run the fetch loop, and let it be blocked on record emitting (which then should let it be blocked mid-iteration)&lt;br/&gt;
    2. Add a discovered partition; this should not throw an exception.&lt;/p&gt;</comment>
                            <comment id="16486695" author="githubbot" created="Wed, 23 May 2018 04:36:52 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6040#discussion_r190112933&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6040#discussion_r190112933&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/internals/AbstractFetcher.java &amp;#8212;&lt;br/&gt;
    @@ -507,7 +507,11 @@ private void updateMinPunctuatedWatermark(Watermark nextWatermark) {&lt;br/&gt;
     			SerializedValue&amp;lt;AssignerWithPunctuatedWatermarks&amp;lt;T&amp;gt;&amp;gt; watermarksPunctuated,&lt;br/&gt;
     			ClassLoader userCodeClassLoader) throws IOException, ClassNotFoundException {&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;List&amp;lt;KafkaTopicPartitionState&amp;lt;KPH&amp;gt;&amp;gt; partitionStates = new LinkedList&amp;lt;&amp;gt;();&lt;br/&gt;
    +		/**&lt;br/&gt;
    +		 *  CopyOnWrite as adding discovered partitions could happen in parallel&lt;br/&gt;
    +		 *  with different threads iterating by 
{@link AbstractFetcher#subscribedPartitionStates}
&lt;p&gt; results&lt;br/&gt;
    +		 */&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    I think we usually don&apos;t have Javadoc blocks within methods. A regular comment with `//` would do.&lt;/p&gt;</comment>
                            <comment id="16486696" author="githubbot" created="Wed, 23 May 2018 04:36:52 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6040#discussion_r190120209&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6040#discussion_r190120209&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/internals/AbstractFetcherTest.java &amp;#8212;&lt;br/&gt;
    @@ -390,6 +398,102 @@ public void testPeriodicWatermarksWithNoSubscribedPartitionsShouldYieldNoWaterma&lt;br/&gt;
     		assertEquals(100, sourceContext.getLatestWatermark().getTimestamp());&lt;br/&gt;
     	}&lt;/p&gt;

&lt;p&gt;    +	@Test&lt;br/&gt;
    +	public void testConcurrentPartitionsDiscoveryAndLoopFetching() throws Exception {&lt;br/&gt;
    +		// test data&lt;br/&gt;
    +		final KafkaTopicPartition testPartition = new KafkaTopicPartition(&quot;test&quot;, 42);&lt;br/&gt;
    +&lt;br/&gt;
    +		final Map&amp;lt;KafkaTopicPartition, Long&amp;gt; testCommitData = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
    +		testCommitData.put(testPartition, 11L);&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- create the test fetcher -----&lt;br/&gt;
    +&lt;br/&gt;
    +		@SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
    +		SourceContext&amp;lt;String&amp;gt; sourceContext = PowerMockito.mock(SourceContext.class);&lt;br/&gt;
    +		Map&amp;lt;KafkaTopicPartition, Long&amp;gt; partitionsWithInitialOffsets =&lt;br/&gt;
    +			Collections.singletonMap(testPartition, KafkaTopicPartitionStateSentinel.GROUP_OFFSET);&lt;br/&gt;
    +&lt;br/&gt;
    +		final TestFetcher&amp;lt;String&amp;gt; fetcher = new TestFetcher&amp;lt;&amp;gt;(&lt;br/&gt;
    +			sourceContext,&lt;br/&gt;
    +			partitionsWithInitialOffsets,&lt;br/&gt;
    +			null, /* periodic assigner */&lt;br/&gt;
    +			null, /* punctuated assigner */&lt;br/&gt;
    +			new TestProcessingTimeService(),&lt;br/&gt;
    +			10);&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- run the fetcher -----&lt;br/&gt;
    +&lt;br/&gt;
    +		final AtomicReference&amp;lt;Throwable&amp;gt; error = new AtomicReference&amp;lt;&amp;gt;();&lt;br/&gt;
    +		int fetchTasks = 5;&lt;br/&gt;
    +		final CountDownLatch latch = new CountDownLatch(fetchTasks);&lt;br/&gt;
    +		ExecutorService service = Executors.newFixedThreadPool(fetchTasks + 1);&lt;br/&gt;
    +&lt;br/&gt;
    +		service.submit(new Thread(&quot;fetcher runner&quot;) {&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public void run() {&lt;br/&gt;
    +				try {&lt;br/&gt;
    +					latch.await();&lt;br/&gt;
    +					fetcher.runFetchLoop();&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    The final `checkedThread.sync()` would always fail with the `ConcurrentModificationException` if the test is designed like this.&lt;/p&gt;</comment>
                            <comment id="16486697" author="githubbot" created="Wed, 23 May 2018 04:36:52 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6040#discussion_r190111529&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6040#discussion_r190111529&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/internals/AbstractFetcherTest.java &amp;#8212;&lt;br/&gt;
    @@ -416,9 +520,16 @@ protected TestFetcher(&lt;br/&gt;
     				false);&lt;br/&gt;
     		}&lt;/p&gt;

&lt;p&gt;    +		/**&lt;br/&gt;
    +		 * Emulation of partition&apos;s iteration which is required for&lt;br/&gt;
    +		 * &lt;/p&gt;
{@link AbstractFetcherTest#testConcurrentPartitionsDiscoveryAndLoopFetching}
&lt;p&gt;.&lt;br/&gt;
    +		 * @throws Exception&lt;br/&gt;
    +		 */&lt;br/&gt;
     		@Override&lt;br/&gt;
     		public void runFetchLoop() throws Exception {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;throw new UnsupportedOperationException();&lt;br/&gt;
    +			for (KafkaTopicPartitionState ignored: subscribedPartitionStates()) {&lt;br/&gt;
    +				Thread.sleep(10L);
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    This would only let the test fail &quot;occasionally&quot;, right?&lt;br/&gt;
    I would like this to be changed, so that we always have the test failing without the copy on write fix.&lt;br/&gt;
    We could do this by having a dummy source context that blocks on record emit.&lt;/p&gt;</comment>
                            <comment id="16486698" author="githubbot" created="Wed, 23 May 2018 04:36:52 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6040#discussion_r190120134&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6040#discussion_r190120134&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/internals/AbstractFetcherTest.java &amp;#8212;&lt;br/&gt;
    @@ -390,6 +398,102 @@ public void testPeriodicWatermarksWithNoSubscribedPartitionsShouldYieldNoWaterma&lt;br/&gt;
     		assertEquals(100, sourceContext.getLatestWatermark().getTimestamp());&lt;br/&gt;
     	}&lt;/p&gt;

&lt;p&gt;    +	@Test&lt;br/&gt;
    +	public void testConcurrentPartitionsDiscoveryAndLoopFetching() throws Exception {&lt;br/&gt;
    +		// test data&lt;br/&gt;
    +		final KafkaTopicPartition testPartition = new KafkaTopicPartition(&quot;test&quot;, 42);&lt;br/&gt;
    +&lt;br/&gt;
    +		final Map&amp;lt;KafkaTopicPartition, Long&amp;gt; testCommitData = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
    +		testCommitData.put(testPartition, 11L);&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- create the test fetcher -----&lt;br/&gt;
    +&lt;br/&gt;
    +		@SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
    +		SourceContext&amp;lt;String&amp;gt; sourceContext = PowerMockito.mock(SourceContext.class);&lt;br/&gt;
    +		Map&amp;lt;KafkaTopicPartition, Long&amp;gt; partitionsWithInitialOffsets =&lt;br/&gt;
    +			Collections.singletonMap(testPartition, KafkaTopicPartitionStateSentinel.GROUP_OFFSET);&lt;br/&gt;
    +&lt;br/&gt;
    +		final TestFetcher&amp;lt;String&amp;gt; fetcher = new TestFetcher&amp;lt;&amp;gt;(&lt;br/&gt;
    +			sourceContext,&lt;br/&gt;
    +			partitionsWithInitialOffsets,&lt;br/&gt;
    +			null, /* periodic assigner */&lt;br/&gt;
    +			null, /* punctuated assigner */&lt;br/&gt;
    +			new TestProcessingTimeService(),&lt;br/&gt;
    +			10);&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- run the fetcher -----&lt;br/&gt;
    +&lt;br/&gt;
    +		final AtomicReference&amp;lt;Throwable&amp;gt; error = new AtomicReference&amp;lt;&amp;gt;();&lt;br/&gt;
    +		int fetchTasks = 5;&lt;br/&gt;
    +		final CountDownLatch latch = new CountDownLatch(fetchTasks);&lt;br/&gt;
    +		ExecutorService service = Executors.newFixedThreadPool(fetchTasks + 1);&lt;br/&gt;
    +&lt;br/&gt;
    +		service.submit(new Thread(&quot;fetcher runner&quot;) {&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public void run() {&lt;br/&gt;
    +				try {&lt;br/&gt;
    +					latch.await();&lt;br/&gt;
    +					fetcher.runFetchLoop();&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    So, IMO, the test should look something like this:&lt;/p&gt;

&lt;p&gt;    ```&lt;br/&gt;
    		final OneShotLatch fetchLoopWaitLatch = new OneShotLatch();&lt;br/&gt;
    		final OneShotLatch stateIterationBlockLatch = new OneShotLatch();&lt;/p&gt;

&lt;p&gt;    		final TestFetcher&amp;lt;String&amp;gt; fetcher = new TestFetcher&amp;lt;&amp;gt;(&lt;br/&gt;
    			sourceContext,&lt;br/&gt;
    			partitionsWithInitialOffsets,&lt;br/&gt;
    			null, /* periodic assigner */&lt;br/&gt;
    			null, /* punctuated assigner */&lt;br/&gt;
    			new TestProcessingTimeService(),&lt;br/&gt;
    			10,&lt;br/&gt;
    			fetchLoopWaitLatch,&lt;br/&gt;
    			stateIterationBlockLatch);&lt;/p&gt;

&lt;p&gt;    		// ----- run the fetcher -----&lt;/p&gt;

&lt;p&gt;    		final CheckedThread checkedThread = new CheckedThread() {&lt;br/&gt;
    			@Override&lt;br/&gt;
    			public void go() throws Exception &lt;/p&gt;
{
    				fetcher.runFetchLoop();
    			}
&lt;p&gt;    		};&lt;br/&gt;
    		checkedThread.start();&lt;/p&gt;

&lt;p&gt;    		// wait until state iteration begins before adding discovered partitions&lt;br/&gt;
    		fetchLoopWaitLatch.await();&lt;br/&gt;
    		fetcher.addDiscoveredPartitions(Collections.singletonList(testPartition));&lt;/p&gt;

&lt;p&gt;    		stateIterationBlockLatch.trigger();&lt;br/&gt;
    		checkedThread.sync();&lt;br/&gt;
    ```&lt;/p&gt;</comment>
                            <comment id="16486821" author="githubbot" created="Wed, 23 May 2018 07:12:26 +0000"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6040&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6040&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16486835" author="tzulitai" created="Wed, 23 May 2018 07:15:30 +0000"  >&lt;p&gt;Merged. Fixed via,&lt;/p&gt;

&lt;p&gt;1.6.0 -&#160;049994274c9d4fc07925a7639e4044506b090d10&lt;br/&gt;
1.5.1 -&#160;bc4a402d09304a21c82299e368442c8a6e4ae427&lt;br/&gt;
1.4.3 -&#160;61c44d902d081ad5bf0e1654f62f70567d25fde8&lt;/p&gt;</comment>
                            <comment id="16486921" author="githubbot" created="Wed, 23 May 2018 08:54:07 +0000"  >&lt;p&gt;Github user snuyanzin commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6040&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6040&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @tzulitai thank you for your review and comments&lt;br/&gt;
    based on your comments I have a question. Could you please clarify it?&lt;/p&gt;

&lt;p&gt;    You mentioned Flink&apos;s `OneShotLatch ` and `CheckedThread ` at the same time in some Kafka connector&apos;s tests used `AtomicReference`, `Thread` and etc. (I used one of them as an example while writing my version of the test). Just to be on the sage am I right that `OneShotLatch ` and `CheckedThread ` in tests are more preferable or are there some rules/limitations/whatever?&lt;/p&gt;</comment>
                            <comment id="16486925" author="githubbot" created="Wed, 23 May 2018 08:56:36 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6040&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6040&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Using `CheckedThread` is more preferable, as it simplifies some of the test code.&lt;br/&gt;
    But yes, the utility was introduced at a later point in time in Flink, so some parts of the test code might still be using `Thread`s and `AtomicReference`s.&lt;/p&gt;</comment>
                            <comment id="16488878" author="pnowojski" created="Thu, 24 May 2018 12:14:02 +0000"  >&lt;p&gt;Because of cancelled 1.5.0 RC5 this fix will make it to 1.5.0 RC6.&#160;FYI&#160;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tzulitai&quot; class=&quot;user-hover&quot; rel=&quot;tzulitai&quot;&gt;tzulitai&lt;/a&gt;&#160;you might want to run some additional tests for 1.5.0 RC6 to make sure that everything works as expected, because this bug fix was not included in normal release testing, and a lot of voters will probably just carry over their votes&#160;to new RC without testing RC6.&lt;/p&gt;</comment>
                            <comment id="16489409" author="tzulitai" created="Thu, 24 May 2018 17:12:06 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pnowojski&quot; class=&quot;user-hover&quot; rel=&quot;pnowojski&quot;&gt;pnowojski&lt;/a&gt; thanks for the reminder. Yes, I will test this with RC6.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                            <outwardlinks description="duplicates">
                                        <issuelink>
            <issuekey id="13244244">FLINK-13204</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12923907" name="Flink9349Test.java" size="6479" author="Sergey Nuyanzin" created="Thu, 17 May 2018 10:51:55 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 25 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3tncf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>