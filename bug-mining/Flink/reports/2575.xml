<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 20:34:28 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-10005] StreamingFileSink ignores checkpoint/processing time rolling policies</title>
                <link>https://issues.apache.org/jira/browse/FLINK-10005</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;p&gt;The &lt;tt&gt;StreamingFileSink&lt;/tt&gt; supports different policies to determine whether a new part file should be created; on each checkpoint, once a certain size is reached or on processing time.&lt;/p&gt;

&lt;p&gt;This feature only works correctly for size thresholds, other policies are ignored.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13175819">FLINK-10005</key>
            <summary>StreamingFileSink ignores checkpoint/processing time rolling policies</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="kkl0u">Kostas Kloudas</assignee>
                                    <reporter username="chesnay">Chesnay Schepler</reporter>
                        <labels>
                            <label>pull-request-available</label>
                    </labels>
                <created>Tue, 31 Jul 2018 12:19:25 +0000</created>
                <updated>Wed, 2 Oct 2019 17:50:11 +0000</updated>
                            <resolved>Wed, 1 Aug 2018 08:21:30 +0000</resolved>
                                    <version>1.6.0</version>
                                    <fixVersion>1.6.0</fixVersion>
                                    <component>Connectors / Common</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="16563588" author="till.rohrmann" created="Tue, 31 Jul 2018 12:32:51 +0000"  >&lt;p&gt;I would like to make this a critical issue for &lt;tt&gt;1.6.0&lt;/tt&gt;. It is not strictly blocking the release, because the &lt;tt&gt;StreamingFileSink&lt;/tt&gt; is a new connector which can still have some limitations. We just need to document them properly. What do you think &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=chesnay&quot; class=&quot;user-hover&quot; rel=&quot;chesnay&quot;&gt;chesnay&lt;/a&gt;?&lt;/p&gt;</comment>
                            <comment id="16563608" author="zentol" created="Tue, 31 Jul 2018 12:44:14 +0000"  >&lt;p&gt;I don&apos;t regard broken features as &quot;limitations&quot;.&lt;/p&gt;

&lt;p&gt;If we know that certain parts of a new feature don&apos;t work then they shouldn&apos;t be in the release. We either fix or remove them.&lt;/p&gt;</comment>
                            <comment id="16563643" author="zentol" created="Tue, 31 Jul 2018 13:12:36 +0000"  >&lt;p&gt;Let me clarify the extent of this issue: If you use checkpoint/processing time based policies the same output file will be repeatedly overwritten by the sink, rendering the sink &lt;em&gt;completely&lt;/em&gt; unusable.&lt;/p&gt;</comment>
                            <comment id="16564738" author="kkl0u" created="Wed, 1 Aug 2018 04:59:40 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=chesnay&quot; class=&quot;user-hover&quot; rel=&quot;chesnay&quot;&gt;chesnay&lt;/a&gt; Do you have an example of that?&lt;/p&gt;</comment>
                            <comment id="16564741" author="kkl0u" created="Wed, 1 Aug 2018 05:02:44 +0000"  >&lt;p&gt;Is it that the &lt;tt&gt;RollingPolicyTest.testRollOnCheckpointPolicy()&lt;/tt&gt; should fail?&lt;/p&gt;</comment>
                            <comment id="16564784" author="kkl0u" created="Wed, 1 Aug 2018 06:02:57 +0000"  >&lt;p&gt;Nevermind, I found it and opened a PR. Sorry for taking over the issue but my schedule is a bit unpredictable these days.&lt;/p&gt;</comment>
                            <comment id="16564801" author="githubbot" created="Wed, 1 Aug 2018 06:28:51 +0000"  >&lt;p&gt;kl0u opened a new pull request #6466: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10005&quot; title=&quot;StreamingFileSink ignores checkpoint/processing time rolling policies&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10005&quot;&gt;&lt;del&gt;FLINK-10005&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;DataStream API&amp;#93;&lt;/span&gt; StreamingFileSink: sets initialPartCounter=maxUsed in new Buckets&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/6466&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6466&lt;/a&gt;&lt;/p&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;What is the purpose of the change&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;   In the `StreamingFileSink` whenever a new bucket is created, we provide it with the initial part counter to use for its part files. In addition, when a bucket has no in-progress or pending files, it is removed from the state. &lt;/p&gt;

&lt;p&gt;   This behavior was leading to the problem:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;bucket X starts with partCounter = 0&lt;/li&gt;
	&lt;li&gt;creates a new part file, i.e. partCounter = 1&lt;/li&gt;
	&lt;li&gt;everything gets committed and bucket X is empty so it gets removed from the state.&lt;/li&gt;
	&lt;li&gt;a new element comes in, and the bucket starts again with 0.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   This was solved for when recovering from a failure, but not if this deletion and creation of a bucket happens in normal execution. This PR solves this issue.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Brief change log&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;   We change the `initialPartCounter` argument passed when creating a new bucket, to be the `max` across all local buckets. This allows to be sure that we do not ovewrite anything and also we do not need to the `blind searches` for file names in order to determine a valid part counter. The latter would not work in eventually-consistent filesystems.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Verifying this change&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;   Updated existing tests to cover this scenario.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Does this pull request potentially affect one of the following parts:&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Dependencies (does it add or upgrade a dependency): (yes / *&lt;b&gt;no&lt;/b&gt;*)&lt;/li&gt;
	&lt;li&gt;The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / *&lt;b&gt;no&lt;/b&gt;*)&lt;/li&gt;
	&lt;li&gt;The serializers: (yes / *&lt;b&gt;no&lt;/b&gt;* / don&apos;t know)&lt;/li&gt;
	&lt;li&gt;The runtime per-record code paths (performance sensitive): (yes / *&lt;b&gt;no&lt;/b&gt;* / don&apos;t know)&lt;/li&gt;
	&lt;li&gt;Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / *&lt;b&gt;no&lt;/b&gt;* / don&apos;t know)&lt;/li&gt;
	&lt;li&gt;The S3 file system connector: (yes / *&lt;b&gt;no&lt;/b&gt;* / don&apos;t know)&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Documentation&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Does this pull request introduce a new feature? (yes / *&lt;b&gt;no&lt;/b&gt;*)&lt;/li&gt;
	&lt;li&gt;If yes, how is the feature documented? (*&lt;b&gt;not applicable&lt;/b&gt;* / docs / JavaDocs / not documented)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16564834" author="githubbot" created="Wed, 1 Aug 2018 06:48:03 +0000"  >&lt;p&gt;kl0u commented on issue #6466: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10005&quot; title=&quot;StreamingFileSink ignores checkpoint/processing time rolling policies&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10005&quot;&gt;&lt;del&gt;FLINK-10005&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;DataStream API&amp;#93;&lt;/span&gt; StreamingFileSink: sets initialPartCounter=maxUsed in new Buckets&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/6466#issuecomment-409468035&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6466#issuecomment-409468035&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   You mean without checking what the `rollingPolicy` says?&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16564836" author="githubbot" created="Wed, 1 Aug 2018 06:49:57 +0000"  >&lt;p&gt;zentol commented on issue #6466: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10005&quot; title=&quot;StreamingFileSink ignores checkpoint/processing time rolling policies&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10005&quot;&gt;&lt;del&gt;FLINK-10005&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;DataStream API&amp;#93;&lt;/span&gt; StreamingFileSink: sets initialPartCounter=maxUsed in new Buckets&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/6466#issuecomment-409468434&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6466#issuecomment-409468434&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   no.&lt;/p&gt;

&lt;p&gt;   instead of &lt;br/&gt;
   ```&lt;br/&gt;
   if (info != null &amp;amp;&amp;amp; rollingPolicy.shouldRollOnProcessingTime(info, timestamp)) &lt;/p&gt;
{
    				bucket.closePartFile();
   }
&lt;p&gt;   ```&lt;br/&gt;
   do&lt;br/&gt;
   ```&lt;br/&gt;
   if (info != null &amp;amp;&amp;amp; rollingPolicy.shouldRollOnProcessingTime(info, timestamp)) &lt;/p&gt;
{
    				bucket.rollPartFile(timestamp);
   }
&lt;p&gt;   ```&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16564842" author="githubbot" created="Wed, 1 Aug 2018 06:51:48 +0000"  >&lt;p&gt;kl0u commented on issue #6466: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10005&quot; title=&quot;StreamingFileSink ignores checkpoint/processing time rolling policies&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10005&quot;&gt;&lt;del&gt;FLINK-10005&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;DataStream API&amp;#93;&lt;/span&gt; StreamingFileSink: sets initialPartCounter=maxUsed in new Buckets&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/6466#issuecomment-409468865&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6466#issuecomment-409468865&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Because this will close the previous AND open a new part file, meaning all buckets, active and inactive, will always be in state. Which we do not want. Also, it means that a bucket that has specified a `rollingInterval` will have a lot of empty part files, as they will open, receive no elements, close due to timeout, and open the next one.&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16564894" author="githubbot" created="Wed, 1 Aug 2018 07:38:45 +0000"  >&lt;p&gt;zentol commented on a change in pull request #6466: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10005&quot; title=&quot;StreamingFileSink ignores checkpoint/processing time rolling policies&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10005&quot;&gt;&lt;del&gt;FLINK-10005&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;DataStream API&amp;#93;&lt;/span&gt; StreamingFileSink: sets initialPartCounter=maxUsed in new Buckets&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/6466#discussion_r206781260&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6466#discussion_r206781260&lt;/a&gt;&lt;/p&gt;



&lt;p&gt; ##########&lt;br/&gt;
 File path: flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/RollingPolicyTest.java&lt;br/&gt;
 ##########&lt;br/&gt;
 @@ -134,24 +137,67 @@ public void testRollOnCheckpointPolicy() throws Exception {&lt;br/&gt;
 			// we take a checkpoint so we roll.&lt;br/&gt;
 			testHarness.snapshot(1L, 1L);&lt;/p&gt;

&lt;p&gt;+			for (File file: FileUtils.listFiles(outDir, null, true)) {&lt;br/&gt;
+				if (Objects.equals(file.getParentFile().getName(), &quot;test1&quot;)) &lt;/p&gt;
{
+					Assert.assertTrue(file.getName().contains(&quot;.part-0-1.inprogress.&quot;));
+				}
&lt;p&gt; else if (Objects.equals(file.getParentFile().getName(), &quot;test2&quot;)) &lt;/p&gt;
{
+					Assert.assertTrue(file.getName().contains(&quot;.part-0-0.inprogress.&quot;));
+				}
&lt;p&gt;+			}&lt;br/&gt;
+&lt;br/&gt;
 			// this will create a new part file&lt;br/&gt;
 			testHarness.processElement(new StreamRecord&amp;lt;&amp;gt;(Tuple2.of(&quot;test1&quot;, 4), 4L));&lt;br/&gt;
 			TestUtils.checkLocalFs(outDir, 3, 0);&lt;/p&gt;

&lt;p&gt;+			testHarness.notifyOfCompletedCheckpoint(1L);&lt;br/&gt;
+			for (File file: FileUtils.listFiles(outDir, null, true)) {&lt;br/&gt;
+				if (Objects.equals(file.getParentFile().getName(), &quot;test1&quot;)) &lt;/p&gt;
{
+					Assert.assertTrue(
+							file.getName().contains(&quot;.part-0-2.inprogress.&quot;) || file.getName().equals(&quot;part-0-1&quot;)
+					);
+				}
&lt;p&gt; else if (Objects.equals(file.getParentFile().getName(), &quot;test2&quot;)) &lt;/p&gt;
{
+					Assert.assertEquals(&quot;part-0-0&quot;, file.getName());
+				}
&lt;p&gt;+			}&lt;br/&gt;
+&lt;br/&gt;
 			// and open and fill .part-0-2.inprogress&lt;br/&gt;
 			testHarness.processElement(new StreamRecord&amp;lt;&amp;gt;(Tuple2.of(&quot;test1&quot;, 5), 5L));&lt;br/&gt;
 			testHarness.processElement(new StreamRecord&amp;lt;&amp;gt;(Tuple2.of(&quot;test1&quot;, 6), 6L));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;TestUtils.checkLocalFs(outDir, 3, 0);                    // nothing committed yet&lt;br/&gt;
+			TestUtils.checkLocalFs(outDir, 1, 2);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; 			// we take a checkpoint so we roll.&lt;br/&gt;
 			testHarness.snapshot(2L, 2L);&lt;/p&gt;

&lt;p&gt; 			testHarness.processElement(new StreamRecord&amp;lt;&amp;gt;(Tuple2.of(&quot;test2&quot;, 7), 7L));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;TestUtils.checkLocalFs(outDir, 4, 0);&lt;br/&gt;
+			TestUtils.checkLocalFs(outDir, 2, 2);&lt;br/&gt;
+&lt;br/&gt;
+			for (File file: FileUtils.listFiles(outDir, null, true)) {&lt;br/&gt;
+				if (Objects.equals(file.getParentFile().getName(), &quot;test1&quot;)) {&lt;br/&gt;
+					Assert.assertTrue(&lt;br/&gt;
+							file.getName().contains(&quot;.part-0-2.inprogress.&quot;) || file.getName().equals(&quot;part-0-1&quot;)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; Review comment:&lt;br/&gt;
   you can create a better error message here using hamcrest.&lt;br/&gt;
   ```&lt;br/&gt;
   assertThat(&lt;br/&gt;
   	file.getName(),&lt;br/&gt;
   	either(containsString(&quot;.part-0-2.inprogress.&quot;))&lt;br/&gt;
   		.or(equalTo(&quot;part-0-1&quot;)));&lt;br/&gt;
   ```&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16564895" author="githubbot" created="Wed, 1 Aug 2018 07:38:45 +0000"  >&lt;p&gt;zentol commented on a change in pull request #6466: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10005&quot; title=&quot;StreamingFileSink ignores checkpoint/processing time rolling policies&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10005&quot;&gt;&lt;del&gt;FLINK-10005&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;DataStream API&amp;#93;&lt;/span&gt; StreamingFileSink: sets initialPartCounter=maxUsed in new Buckets&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/6466#discussion_r206779932&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6466#discussion_r206779932&lt;/a&gt;&lt;/p&gt;



&lt;p&gt; ##########&lt;br/&gt;
 File path: flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/sink/filesystem/Buckets.java&lt;br/&gt;
 ##########&lt;br/&gt;
 @@ -237,7 +234,7 @@ void onElement(IN value, SinkFunction.Context context) throws Exception &lt;/p&gt;
{
 					subtaskIndex,
 					bucketId,
 					bucketPath,
-					initMaxPartCounter,
+					maxPartCounterUsed,
 					partFileWriterFactory);
 			activeBuckets.put(bucketId, bucket);
 		}

&lt;p&gt; Review comment:&lt;br/&gt;
   can we add a comment just below that `final PartFileInfo&amp;lt;BucketID&amp;gt; info = bucket.getInProgressPartInfo();` is `null`  directly after a snapshot and for new empty buckets? This is the detail I was missing when trying to understand how this fixes the issue.&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16564901" author="githubbot" created="Wed, 1 Aug 2018 07:42:05 +0000"  >&lt;p&gt;kl0u commented on a change in pull request #6466: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10005&quot; title=&quot;StreamingFileSink ignores checkpoint/processing time rolling policies&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10005&quot;&gt;&lt;del&gt;FLINK-10005&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;DataStream API&amp;#93;&lt;/span&gt; StreamingFileSink: sets initialPartCounter=maxUsed in new Buckets&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/6466#discussion_r206782046&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6466#discussion_r206782046&lt;/a&gt;&lt;/p&gt;



&lt;p&gt; ##########&lt;br/&gt;
 File path: flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/sink/filesystem/Buckets.java&lt;br/&gt;
 ##########&lt;br/&gt;
 @@ -237,7 +234,7 @@ void onElement(IN value, SinkFunction.Context context) throws Exception &lt;/p&gt;
{
 					subtaskIndex,
 					bucketId,
 					bucketPath,
-					initMaxPartCounter,
+					maxPartCounterUsed,
 					partFileWriterFactory);
 			activeBuckets.put(bucketId, bucket);
 		}

&lt;p&gt; Review comment:&lt;br/&gt;
   OK.&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16564902" author="githubbot" created="Wed, 1 Aug 2018 07:42:11 +0000"  >&lt;p&gt;kl0u commented on a change in pull request #6466: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10005&quot; title=&quot;StreamingFileSink ignores checkpoint/processing time rolling policies&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10005&quot;&gt;&lt;del&gt;FLINK-10005&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;DataStream API&amp;#93;&lt;/span&gt; StreamingFileSink: sets initialPartCounter=maxUsed in new Buckets&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/6466#discussion_r206782080&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6466#discussion_r206782080&lt;/a&gt;&lt;/p&gt;



&lt;p&gt; ##########&lt;br/&gt;
 File path: flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/RollingPolicyTest.java&lt;br/&gt;
 ##########&lt;br/&gt;
 @@ -134,24 +137,67 @@ public void testRollOnCheckpointPolicy() throws Exception {&lt;br/&gt;
 			// we take a checkpoint so we roll.&lt;br/&gt;
 			testHarness.snapshot(1L, 1L);&lt;/p&gt;

&lt;p&gt;+			for (File file: FileUtils.listFiles(outDir, null, true)) {&lt;br/&gt;
+				if (Objects.equals(file.getParentFile().getName(), &quot;test1&quot;)) &lt;/p&gt;
{
+					Assert.assertTrue(file.getName().contains(&quot;.part-0-1.inprogress.&quot;));
+				}
&lt;p&gt; else if (Objects.equals(file.getParentFile().getName(), &quot;test2&quot;)) &lt;/p&gt;
{
+					Assert.assertTrue(file.getName().contains(&quot;.part-0-0.inprogress.&quot;));
+				}
&lt;p&gt;+			}&lt;br/&gt;
+&lt;br/&gt;
 			// this will create a new part file&lt;br/&gt;
 			testHarness.processElement(new StreamRecord&amp;lt;&amp;gt;(Tuple2.of(&quot;test1&quot;, 4), 4L));&lt;br/&gt;
 			TestUtils.checkLocalFs(outDir, 3, 0);&lt;/p&gt;

&lt;p&gt;+			testHarness.notifyOfCompletedCheckpoint(1L);&lt;br/&gt;
+			for (File file: FileUtils.listFiles(outDir, null, true)) {&lt;br/&gt;
+				if (Objects.equals(file.getParentFile().getName(), &quot;test1&quot;)) &lt;/p&gt;
{
+					Assert.assertTrue(
+							file.getName().contains(&quot;.part-0-2.inprogress.&quot;) || file.getName().equals(&quot;part-0-1&quot;)
+					);
+				}
&lt;p&gt; else if (Objects.equals(file.getParentFile().getName(), &quot;test2&quot;)) &lt;/p&gt;
{
+					Assert.assertEquals(&quot;part-0-0&quot;, file.getName());
+				}
&lt;p&gt;+			}&lt;br/&gt;
+&lt;br/&gt;
 			// and open and fill .part-0-2.inprogress&lt;br/&gt;
 			testHarness.processElement(new StreamRecord&amp;lt;&amp;gt;(Tuple2.of(&quot;test1&quot;, 5), 5L));&lt;br/&gt;
 			testHarness.processElement(new StreamRecord&amp;lt;&amp;gt;(Tuple2.of(&quot;test1&quot;, 6), 6L));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;TestUtils.checkLocalFs(outDir, 3, 0);                    // nothing committed yet&lt;br/&gt;
+			TestUtils.checkLocalFs(outDir, 1, 2);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; 			// we take a checkpoint so we roll.&lt;br/&gt;
 			testHarness.snapshot(2L, 2L);&lt;/p&gt;

&lt;p&gt; 			testHarness.processElement(new StreamRecord&amp;lt;&amp;gt;(Tuple2.of(&quot;test2&quot;, 7), 7L));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;TestUtils.checkLocalFs(outDir, 4, 0);&lt;br/&gt;
+			TestUtils.checkLocalFs(outDir, 2, 2);&lt;br/&gt;
+&lt;br/&gt;
+			for (File file: FileUtils.listFiles(outDir, null, true)) {&lt;br/&gt;
+				if (Objects.equals(file.getParentFile().getName(), &quot;test1&quot;)) {&lt;br/&gt;
+					Assert.assertTrue(&lt;br/&gt;
+							file.getName().contains(&quot;.part-0-2.inprogress.&quot;) || file.getName().equals(&quot;part-0-1&quot;)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; Review comment:&lt;br/&gt;
   OK.&lt;/p&gt;


&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16564937" author="githubbot" created="Wed, 1 Aug 2018 08:17:39 +0000"  >&lt;p&gt;asfgit closed pull request #6466: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10005&quot; title=&quot;StreamingFileSink ignores checkpoint/processing time rolling policies&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10005&quot;&gt;&lt;del&gt;FLINK-10005&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;DataStream API&amp;#93;&lt;/span&gt; StreamingFileSink: sets initialPartCounter=maxUsed in new Buckets&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/6466&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6466&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/sink/filesystem/Bucket.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/sink/filesystem/Bucket.java&lt;br/&gt;
index ec59233c0e5..a350096e38b 100644&lt;br/&gt;
&amp;#8212; a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/sink/filesystem/Bucket.java&lt;br/&gt;
+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/sink/filesystem/Bucket.java&lt;br/&gt;
@@ -115,6 +115,18 @@ public Bucket(&lt;br/&gt;
 		this.pending = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
 	}&lt;/p&gt;

&lt;p&gt;+	/**&lt;br/&gt;
+	 * Gets the information available for the currently&lt;br/&gt;
+	 * open part file, i.e. the one we are currently writing to.&lt;br/&gt;
+	 *&lt;br/&gt;
+	 * &amp;lt;p&amp;gt;This will be null if there is no currently open part file. This&lt;br/&gt;
+	 * is the case when we have a new, just created bucket or a bucket&lt;br/&gt;
+	 * that has not received any data after the closing of its previously&lt;br/&gt;
+	 * open in-progress file due to the specified rolling policy.&lt;br/&gt;
+	 *&lt;br/&gt;
+	 * @return The information about the currently in-progress part file&lt;br/&gt;
+	 * or &lt;/p&gt;
{@code null}
&lt;p&gt; if there is no open part file.&lt;br/&gt;
+	 */&lt;br/&gt;
 	public PartFileInfo&amp;lt;BucketID&amp;gt; getInProgressPartInfo() &lt;/p&gt;
{
 		return currentPart;
 	}
&lt;p&gt;diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/sink/filesystem/Buckets.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/sink/filesystem/Buckets.java&lt;br/&gt;
index 7e9dd61e035..e62c425fc2f 100644&lt;br/&gt;
&amp;#8212; a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/sink/filesystem/Buckets.java&lt;br/&gt;
+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/sink/filesystem/Buckets.java&lt;br/&gt;
@@ -70,8 +70,6 @@&lt;/p&gt;

&lt;p&gt; 	private final Map&amp;lt;BucketID, Bucket&amp;lt;IN, BucketID&amp;gt;&amp;gt; activeBuckets;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private long initMaxPartCounter;&lt;br/&gt;
-&lt;br/&gt;
 	private long maxPartCounterUsed;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; 	private final RecoverableWriter fileSystemWriter;&lt;br/&gt;
@@ -114,7 +112,6 @@&lt;br/&gt;
 				bucketer.getSerializer()&lt;br/&gt;
 		);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;this.initMaxPartCounter = 0L;&lt;br/&gt;
 		this.maxPartCounterUsed = 0L;&lt;br/&gt;
 	}&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -137,7 +134,7 @@ void initializeState(final ListState&amp;lt;byte[]&amp;gt; bucketStates, final ListState&amp;lt;Long&amp;gt;&lt;br/&gt;
 		for (long partCounter: partCounterState.get()) &lt;/p&gt;
{
 			maxCounter = Math.max(partCounter, maxCounter);
 		}
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;initMaxPartCounter = maxCounter;&lt;br/&gt;
+		maxPartCounterUsed = maxCounter;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; 		// get the restored buckets&lt;br/&gt;
 		for (byte[] recoveredState : bucketStates.get()) {&lt;br/&gt;
@@ -151,7 +148,7 @@ void initializeState(final ListState&amp;lt;byte[]&amp;gt; bucketStates, final ListState&amp;lt;Long&amp;gt;&lt;br/&gt;
 			final Bucket&amp;lt;IN, BucketID&amp;gt; restoredBucket = bucketFactory.restoreBucket(&lt;br/&gt;
 					fileSystemWriter,&lt;br/&gt;
 					subtaskIndex,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;initMaxPartCounter,&lt;br/&gt;
+					maxPartCounterUsed,&lt;br/&gt;
 					partFileWriterFactory,&lt;br/&gt;
 					bucketState&lt;br/&gt;
 			);&lt;br/&gt;
@@ -200,8 +197,6 @@ void snapshotState(&lt;br/&gt;
 			final PartFileInfo&amp;lt;BucketID&amp;gt; info = bucket.getInProgressPartInfo();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; 			if (info != null &amp;amp;&amp;amp; rollingPolicy.shouldRollOnCheckpoint(info)) &lt;/p&gt;
{
-				// we also check here so that we do not have to always
-				// wait for the &quot;next&quot; element to arrive.
 				bucket.closePartFile();
 			}

&lt;p&gt;@@ -237,13 +232,19 @@ void onElement(IN value, SinkFunction.Context context) throws Exception &lt;/p&gt;
{
 					subtaskIndex,
 					bucketId,
 					bucketPath,
-					initMaxPartCounter,
+					maxPartCounterUsed,
 					partFileWriterFactory);
 			activeBuckets.put(bucketId, bucket);
 		}

&lt;p&gt; 		final PartFileInfo&amp;lt;BucketID&amp;gt; info = bucket.getInProgressPartInfo();&lt;br/&gt;
 		if (info == null || rollingPolicy.shouldRollOnEvent(info, value)) &lt;/p&gt;
{
+
+			// info will be null if there is no currently open part file. This
+			// is the case when we have a new, just created bucket or a bucket
+			// that has not received any data after the closing of its previously
+			// open in-progress file due to the specified rolling policy.
+
 			bucket.rollPartFile(currentProcessingTime);
 		}
&lt;p&gt; 		bucket.write(value, currentProcessingTime);&lt;br/&gt;
diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/LocalStreamingFileSinkTest.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/LocalStreamingFileSinkTest.java&lt;br/&gt;
index 4b1e7436772..a0c438e1847 100644&lt;br/&gt;
&amp;#8212; a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/LocalStreamingFileSinkTest.java&lt;br/&gt;
+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/LocalStreamingFileSinkTest.java&lt;br/&gt;
@@ -323,7 +323,7 @@ public void testInactivityPeriodWithLateNotify() throws Exception &lt;/p&gt;
{
 					Assert.assertEquals(&quot;test1@1\n&quot;, fileContents.getValue());
 				}
&lt;p&gt; else if (fileContents.getKey().getParentFile().getName().equals(&quot;test2&quot;)) &lt;/p&gt;
{
 					bucketCounter++;
-					Assert.assertEquals(&quot;part-0-0&quot;, fileContents.getKey().getName());
+					Assert.assertEquals(&quot;part-0-1&quot;, fileContents.getKey().getName());
 					Assert.assertEquals(&quot;test2@1\n&quot;, fileContents.getValue());
 				}
&lt;p&gt; else if (fileContents.getKey().getParentFile().getName().equals(&quot;test3&quot;)) {&lt;br/&gt;
 					bucketCounter++;&lt;br/&gt;
@@ -346,11 +346,11 @@ public void testInactivityPeriodWithLateNotify() throws Exception &lt;/p&gt;
{
 					Assert.assertEquals(&quot;test2@1\n&quot;, fileContents.getValue());
 				}
&lt;p&gt; else if (fileContents.getKey().getParentFile().getName().equals(&quot;test3&quot;)) &lt;/p&gt;
{
 					bucketCounter++;
-					Assert.assertEquals(&quot;part-0-0&quot;, fileContents.getKey().getName());
+					Assert.assertEquals(&quot;part-0-2&quot;, fileContents.getKey().getName());
 					Assert.assertEquals(&quot;test3@1\n&quot;, fileContents.getValue());
 				}
&lt;p&gt; else if (fileContents.getKey().getParentFile().getName().equals(&quot;test4&quot;)) &lt;/p&gt;
{
 					bucketCounter++;
-					Assert.assertEquals(&quot;part-0-0&quot;, fileContents.getKey().getName());
+					Assert.assertEquals(&quot;part-0-3&quot;, fileContents.getKey().getName());
 					Assert.assertEquals(&quot;test4@1\n&quot;, fileContents.getValue());
 				}
&lt;p&gt; 			}&lt;br/&gt;
@@ -437,8 +437,8 @@ public void testScalingDownAndMergingOfStates() throws Exception {&lt;br/&gt;
 							inProgressFilename.contains(&quot;.part-1-0.inprogress&quot;)&lt;br/&gt;
 						)&lt;br/&gt;
 				) &lt;/p&gt;
{
-						counter++;
-				}
&lt;p&gt; else if (parentFilename.equals(&quot;test2&quot;) &amp;amp;&amp;amp; inProgressFilename.contains(&quot;.part-1-0.inprogress&quot;)) &lt;/p&gt;
{
+					counter++;
+				}
&lt;p&gt; else if (parentFilename.equals(&quot;test2&quot;) &amp;amp;&amp;amp; inProgressFilename.contains(&quot;.part-1-1.inprogress&quot;)) &lt;/p&gt;
{
 					counter++;
 				}
&lt;p&gt; 			}&lt;br/&gt;
@@ -476,7 +476,7 @@ public void testScalingDownAndMergingOfStates() throws Exception &lt;/p&gt;
{
 						counter++;
 						Assert.assertTrue(fileContents.getValue().equals(&quot;test1@1\n&quot;) || fileContents.getValue().equals(&quot;test1@0\n&quot;));
 					}
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;} else if (parentFilename.equals(&quot;test2&quot;) &amp;amp;&amp;amp; filename.contains(&quot;.part-1-0.inprogress&quot;)) 
{
+				}
&lt;p&gt; else if (parentFilename.equals(&quot;test2&quot;) &amp;amp;&amp;amp; filename.contains(&quot;.part-1-1.inprogress&quot;)) &lt;/p&gt;
{
 					counter++;
 					Assert.assertEquals(&quot;test2@1\n&quot;, fileContents.getValue());
 				}
&lt;p&gt;diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/RollingPolicyTest.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/RollingPolicyTest.java&lt;br/&gt;
index db54de941b9..f16a9085d9d 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/RollingPolicyTest.java&lt;br/&gt;
+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/RollingPolicyTest.java&lt;br/&gt;
@@ -25,12 +25,19 @@&lt;br/&gt;
 import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;&lt;br/&gt;
 import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+import org.apache.commons.io.FileUtils;&lt;br/&gt;
+import org.junit.Assert;&lt;br/&gt;
 import org.junit.ClassRule;&lt;br/&gt;
 import org.junit.Test;&lt;br/&gt;
 import org.junit.rules.TemporaryFolder;&lt;/p&gt;

&lt;p&gt; import java.io.File;&lt;br/&gt;
 import java.io.IOException;&lt;br/&gt;
+import java.util.Objects;&lt;br/&gt;
+&lt;br/&gt;
+import static org.hamcrest.CoreMatchers.containsString;&lt;br/&gt;
+import static org.hamcrest.CoreMatchers.either;&lt;br/&gt;
+import static org.hamcrest.CoreMatchers.equalTo;&lt;/p&gt;

&lt;p&gt; /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Tests for different 
{@link RollingPolicy rolling policies}
&lt;p&gt;.&lt;br/&gt;
@@ -134,24 +141,74 @@ public void testRollOnCheckpointPolicy() throws Exception {&lt;br/&gt;
 			// we take a checkpoint so we roll.&lt;br/&gt;
 			testHarness.snapshot(1L, 1L);&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+			for (File file: FileUtils.listFiles(outDir, null, true)) {&lt;br/&gt;
+				if (Objects.equals(file.getParentFile().getName(), &quot;test1&quot;)) &lt;/p&gt;
{
+					Assert.assertTrue(file.getName().contains(&quot;.part-0-1.inprogress.&quot;));
+				}
&lt;p&gt; else if (Objects.equals(file.getParentFile().getName(), &quot;test2&quot;)) &lt;/p&gt;
{
+					Assert.assertTrue(file.getName().contains(&quot;.part-0-0.inprogress.&quot;));
+				}
&lt;p&gt;+			}&lt;br/&gt;
+&lt;br/&gt;
 			// this will create a new part file&lt;br/&gt;
 			testHarness.processElement(new StreamRecord&amp;lt;&amp;gt;(Tuple2.of(&quot;test1&quot;, 4), 4L));&lt;br/&gt;
 			TestUtils.checkLocalFs(outDir, 3, 0);&lt;/p&gt;

&lt;p&gt;+			testHarness.notifyOfCompletedCheckpoint(1L);&lt;br/&gt;
+			for (File file: FileUtils.listFiles(outDir, null, true)) {&lt;br/&gt;
+				if (Objects.equals(file.getParentFile().getName(), &quot;test1&quot;)) &lt;/p&gt;
{
+					Assert.assertTrue(
+							file.getName().contains(&quot;.part-0-2.inprogress.&quot;) || file.getName().equals(&quot;part-0-1&quot;)
+					);
+				}
&lt;p&gt; else if (Objects.equals(file.getParentFile().getName(), &quot;test2&quot;)) &lt;/p&gt;
{
+					Assert.assertEquals(&quot;part-0-0&quot;, file.getName());
+				}
&lt;p&gt;+			}&lt;br/&gt;
+&lt;br/&gt;
 			// and open and fill .part-0-2.inprogress&lt;br/&gt;
 			testHarness.processElement(new StreamRecord&amp;lt;&amp;gt;(Tuple2.of(&quot;test1&quot;, 5), 5L));&lt;br/&gt;
 			testHarness.processElement(new StreamRecord&amp;lt;&amp;gt;(Tuple2.of(&quot;test1&quot;, 6), 6L));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;TestUtils.checkLocalFs(outDir, 3, 0);                    // nothing committed yet&lt;br/&gt;
+			TestUtils.checkLocalFs(outDir, 1, 2);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; 			// we take a checkpoint so we roll.&lt;br/&gt;
 			testHarness.snapshot(2L, 2L);&lt;/p&gt;

&lt;p&gt; 			testHarness.processElement(new StreamRecord&amp;lt;&amp;gt;(Tuple2.of(&quot;test2&quot;, 7), 7L));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;TestUtils.checkLocalFs(outDir, 4, 0);&lt;br/&gt;
+			TestUtils.checkLocalFs(outDir, 2, 2);&lt;br/&gt;
+&lt;br/&gt;
+			for (File file: FileUtils.listFiles(outDir, null, true)) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+				if (Objects.equals(file.getParentFile().getName(), &amp;quot;test1&amp;quot;)) {
+					Assert.assertThat(
+							file.getName(),
+							either(containsString(&quot;.part-0-2.inprogress.&quot;))
+									.or(equalTo(&quot;part-0-1&quot;))
+					);
+				} else if (Objects.equals(file.getParentFile().getName(), &amp;quot;test2&amp;quot;)) {
+					Assert.assertThat(
+							file.getName(),
+							either(containsString(&quot;.part-0-3.inprogress.&quot;))
+									.or(equalTo(&quot;part-0-0&quot;))
+					);
+				}&lt;br/&gt;
+			}&lt;br/&gt;
 &lt;br/&gt;
 			// we acknowledge the last checkpoint so we should publish all but the latest in-progress file&lt;br/&gt;
 			testHarness.notifyOfCompletedCheckpoint(2L);&lt;br/&gt;
+&lt;br/&gt;
 			TestUtils.checkLocalFs(outDir, 1, 3);&lt;br/&gt;
+			for (File file: FileUtils.listFiles(outDir, null, true)) {&lt;br/&gt;
+				if (Objects.equals(file.getParentFile().getName(), &quot;test1&quot;)) {
+					Assert.assertThat(
+							file.getName(),
+							either(equalTo(&quot;part-0-2&quot;)).or(equalTo(&quot;part-0-1&quot;))
+					);
+				} else if (Objects.equals(file.getParentFile().getName(), &quot;test2&quot;)) {+					Assert.assertThat(+							file.getName(),+							either(containsString(&quot;.part-0-3.inprogress.&quot;))+									.or(equalTo(&quot;part-0-0&quot;))+					);+				}+			}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt; 		}&lt;br/&gt;
 	}&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;






&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16564944" author="kkl0u" created="Wed, 1 Aug 2018 08:21:30 +0000"  >&lt;p&gt;Merged on master with 296f6a8264d6bf15c08bff5766bdda939600fbe7&lt;/p&gt;

&lt;p&gt;and on release-1.6 with&#160;0bb960e2acc1301c4fd4a23fab9c21b3df8718ae&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 15 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3wiov:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>