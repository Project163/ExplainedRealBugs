<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 20:18:06 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-1442] Archived Execution Graph consumes too much memory</title>
                <link>https://issues.apache.org/jira/browse/FLINK-1442</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;p&gt;The JobManager archives the execution graphs, for analysis of jobs. The graphs may consume a lot of memory.&lt;/p&gt;

&lt;p&gt;Especially the execution edges in all2all connection patterns are extremely many and add up in memory consumption.&lt;/p&gt;

&lt;p&gt;The execution edges connect all parallel tasks. So for a all2all pattern between n and m tasks, there are n*m edges. For parallelism of multiple 100 tasks, this can easily reach 100k objects and more, each with a set of metadata.&lt;/p&gt;

&lt;p&gt;I propose the following to solve that:&lt;/p&gt;

&lt;p&gt;1.  Clear all execution edges from the graph (majority of the memory consumers) when it is given to the archiver.&lt;/p&gt;

&lt;p&gt;2. Have the map/list of the archived graphs behind a soft reference, to it will be removed under memory pressure before the JVM crashes. That may remove graphs from the history early, but is much preferable to the JVM crashing, in which case the graph is lost as well...&lt;/p&gt;

&lt;p&gt;3. Long term: The graph should be archived somewhere else. Somthing like the History server used by Hadoop and Hive would be a good idea.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12769893">FLINK-1442</key>
            <summary>Archived Execution Graph consumes too much memory</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="mxm">Maximilian Michels</assignee>
                                    <reporter username="sewen">Stephan Ewen</reporter>
                        <labels>
                    </labels>
                <created>Sat, 24 Jan 2015 20:39:20 +0000</created>
                <updated>Thu, 28 Feb 2019 14:01:50 +0000</updated>
                            <resolved>Thu, 5 Feb 2015 12:39:54 +0000</resolved>
                                    <version>0.9</version>
                                    <fixVersion>0.9</fixVersion>
                                    <component>Runtime / Coordination</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="14292249" author="mxm" created="Mon, 26 Jan 2015 19:08:53 +0000"  >&lt;p&gt;Here is my take on your first two points. Is there any job where the memory stress is particular high? It would be good to test the changes.&lt;/p&gt;</comment>
                            <comment id="14292251" author="githubbot" created="Mon, 26 Jan 2015 19:09:42 +0000"  >&lt;p&gt;GitHub user mxm opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/344&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/344&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-1442&quot; title=&quot;Archived Execution Graph consumes too much memory&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-1442&quot;&gt;&lt;del&gt;FLINK-1442&lt;/del&gt;&lt;/a&gt; Reduce memory consumption of archived execution graph&lt;/p&gt;



&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/mxm/flink&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/mxm/flink&lt;/a&gt; flink-1442&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/344.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/344.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #344&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit baa04e386b70d3c928ceb07e78e50016f20520f0&lt;br/&gt;
Author: Max &amp;lt;max@posteo.de&amp;gt;&lt;br/&gt;
Date:   2015-01-26T18:31:47Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-1442&quot; title=&quot;Archived Execution Graph consumes too much memory&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-1442&quot;&gt;&lt;del&gt;FLINK-1442&lt;/del&gt;&lt;/a&gt; Reduce memory consumption of archived execution graph&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="14292498" author="githubbot" created="Mon, 26 Jan 2015 22:08:43 +0000"  >&lt;p&gt;Github user StephanEwen commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/344#issuecomment-71548964&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/344#issuecomment-71548964&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Looks good so far. I see that you removed the LRU code. Was that on purpose?&lt;/p&gt;

&lt;p&gt;    Leaving it in may be a good idea, because the soft references are cleared in arbitrary order. It may make newer jobs disappear before older ones. Having the LRU in would mean things behave as previously as long as the memory is sufficient, and the soft reference clearing kicks in as a safety valve.&lt;/p&gt;</comment>
                            <comment id="14293258" author="githubbot" created="Tue, 27 Jan 2015 09:41:34 +0000"  >&lt;p&gt;Github user mxm commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/344#issuecomment-71616952&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/344#issuecomment-71616952&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @StephanEwen Yes, that was on purpose. The previous two data structures (`HashMap` and `Queue`) are now replaced by the `LinkedHashMap` which serves the same functionality. It might not be obvious but the `LinkedHashMap` preserves the order of the inserted items. From `scala.collection.mutable.LinkedHashMap`:&lt;/p&gt;

&lt;p&gt;    &amp;gt; This class implements mutable maps using a hashtable.&lt;br/&gt;
    &amp;gt; The iterator and all traversal methods of this class visit elements in the order they were inserted.&lt;/p&gt;

&lt;p&gt;    That&apos;s why `graphs.iterator.next()` always returns the least recently inserted item.&lt;/p&gt;
</comment>
                            <comment id="14304846" author="githubbot" created="Wed, 4 Feb 2015 09:45:53 +0000"  >&lt;p&gt;Github user tillrohrmann commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/344#discussion_r24072128&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/344#discussion_r24072128&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/MemoryArchivist.scala &amp;#8212;&lt;br/&gt;
    @@ -25,48 +25,82 @@ import org.apache.flink.runtime.jobgraph.JobID&lt;br/&gt;
     import org.apache.flink.runtime.messages.ArchiveMessages._&lt;br/&gt;
     import org.apache.flink.runtime.messages.JobManagerMessages._&lt;/p&gt;

&lt;p&gt;    +import scala.collection.mutable.LinkedHashMap&lt;br/&gt;
    +import scala.ref.SoftReference&lt;br/&gt;
    +&lt;br/&gt;
     class MemoryArchivist(private val max_entries: Int) extends Actor with ActorLogMessages with&lt;br/&gt;
     ActorLogging {&lt;br/&gt;
       /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Map of execution graphs belonging to recently started jobs with the time stamp of the last&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* received job event.&lt;br/&gt;
    +   * received job event. The insert order is preserved through a LinkedHashMap.&lt;br/&gt;
        */&lt;/li&gt;
	&lt;li&gt;val graphs = collection.mutable.HashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;JobID, ExecutionGraph&amp;#93;&lt;/span&gt;()&lt;/li&gt;
	&lt;li&gt;val lru = collection.mutable.Queue&lt;span class=&quot;error&quot;&gt;&amp;#91;JobID&amp;#93;&lt;/span&gt;()&lt;br/&gt;
    +  val graphs = LinkedHashMap[JobID, SoftReference&lt;span class=&quot;error&quot;&gt;&amp;#91;ExecutionGraph&amp;#93;&lt;/span&gt;]()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       override def receiveWithLogMessages: Receive = {&lt;br/&gt;
    +    /* Receive Execution Graph to archive */&lt;br/&gt;
         case ArchiveExecutionGraph(jobID, graph) =&amp;gt; {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;graphs.update(jobID, graph)&lt;br/&gt;
    +      // wrap graph inside a soft reference&lt;br/&gt;
    +      graphs.update(jobID, new SoftReference(graph))&lt;br/&gt;
    +&lt;br/&gt;
    +      // clear all execution edges of the graph&lt;br/&gt;
    +      val iter = graph.getAllExecutionVertices().iterator()&lt;br/&gt;
    +      while (iter.hasNext) 
{
    +        iter.next().clearExecutionEdges()
    +      }
&lt;p&gt;    +&lt;br/&gt;
           cleanup(jobID)&lt;br/&gt;
         }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         case RequestArchivedJobs =&amp;gt; &lt;/p&gt;
{
    -      sender ! ArchivedJobs(graphs.values)
    +      sender ! ArchivedJobs(getAllGraphs())
         }

&lt;p&gt;         case RequestJob(jobID) =&amp;gt; {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;graphs.get(jobID) match {&lt;/li&gt;
	&lt;li&gt;case Some(graph) =&amp;gt; sender ! JobFound(jobID, graph)&lt;/li&gt;
	&lt;li&gt;case None =&amp;gt; sender ! JobNotFound(jobID)&lt;br/&gt;
    +      getGraph(jobID) match 
{
    +        case graph: ExecutionGraph =&amp;gt; sender ! JobFound(jobID, graph)
    +        case _ =&amp;gt; sender ! JobNotFound(jobID)
           }
&lt;p&gt;         }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         case RequestJobStatus(jobID) =&amp;gt; {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;graphs.get(jobID) match {&lt;/li&gt;
	&lt;li&gt;case Some(eg) =&amp;gt; sender ! CurrentJobStatus(jobID, eg.getState)&lt;/li&gt;
	&lt;li&gt;case None =&amp;gt; sender ! JobNotFound(jobID)&lt;br/&gt;
    +      getGraph(jobID) match 
{
    +        case graph: ExecutionGraph =&amp;gt; sender ! CurrentJobStatus(jobID, graph.getState)
    +        case _ =&amp;gt; sender ! JobNotFound(jobID)
           }
&lt;p&gt;         }&lt;br/&gt;
       }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def cleanup(jobID: JobID): Unit = {&lt;/li&gt;
	&lt;li&gt;if (!lru.contains(jobID)) {&lt;/li&gt;
	&lt;li&gt;lru.enqueue(jobID)&lt;br/&gt;
    +  /**&lt;br/&gt;
    +   * Gets all graphs that have not been garbage collected.&lt;br/&gt;
    +   * @return An iterable with all valid ExecutionGraphs&lt;br/&gt;
    +   */&lt;br/&gt;
    +  def getAllGraphs() = graphs.values.flatMap(ref =&amp;gt; ref.get match 
{
    +    case Some(graph) =&amp;gt; Seq(graph)
    +    case _ =&amp;gt; Seq()
    +  }
&lt;p&gt;)&lt;br/&gt;
    +&lt;br/&gt;
    +  /**&lt;br/&gt;
    +   * Gets a graph with a jobID if it has not been garbage collected.&lt;br/&gt;
    +   * @param jobID&lt;br/&gt;
    +   * @return ExecutionGraph or null&lt;br/&gt;
    +   */&lt;br/&gt;
    +  def getGraph(jobID: JobID) = graphs.get(jobID) match {&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Why are we doing that? Why not working on the Option type? I don&apos;t like null.&lt;/p&gt;</comment>
                            <comment id="14304847" author="githubbot" created="Wed, 4 Feb 2015 09:46:21 +0000"  >&lt;p&gt;Github user tillrohrmann commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/344#discussion_r24072159&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/344#discussion_r24072159&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/MemoryArchivist.scala &amp;#8212;&lt;br/&gt;
    @@ -25,48 +25,82 @@ import org.apache.flink.runtime.jobgraph.JobID&lt;br/&gt;
     import org.apache.flink.runtime.messages.ArchiveMessages._&lt;br/&gt;
     import org.apache.flink.runtime.messages.JobManagerMessages._&lt;/p&gt;

&lt;p&gt;    +import scala.collection.mutable.LinkedHashMap&lt;br/&gt;
    +import scala.ref.SoftReference&lt;br/&gt;
    +&lt;br/&gt;
     class MemoryArchivist(private val max_entries: Int) extends Actor with ActorLogMessages with&lt;br/&gt;
     ActorLogging {&lt;br/&gt;
       /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Map of execution graphs belonging to recently started jobs with the time stamp of the last&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* received job event.&lt;br/&gt;
    +   * received job event. The insert order is preserved through a LinkedHashMap.&lt;br/&gt;
        */&lt;/li&gt;
	&lt;li&gt;val graphs = collection.mutable.HashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;JobID, ExecutionGraph&amp;#93;&lt;/span&gt;()&lt;/li&gt;
	&lt;li&gt;val lru = collection.mutable.Queue&lt;span class=&quot;error&quot;&gt;&amp;#91;JobID&amp;#93;&lt;/span&gt;()&lt;br/&gt;
    +  val graphs = LinkedHashMap[JobID, SoftReference&lt;span class=&quot;error&quot;&gt;&amp;#91;ExecutionGraph&amp;#93;&lt;/span&gt;]()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       override def receiveWithLogMessages: Receive = {&lt;br/&gt;
    +    /* Receive Execution Graph to archive */&lt;br/&gt;
         case ArchiveExecutionGraph(jobID, graph) =&amp;gt; {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;graphs.update(jobID, graph)&lt;br/&gt;
    +      // wrap graph inside a soft reference&lt;br/&gt;
    +      graphs.update(jobID, new SoftReference(graph))&lt;br/&gt;
    +&lt;br/&gt;
    +      // clear all execution edges of the graph&lt;br/&gt;
    +      val iter = graph.getAllExecutionVertices().iterator()&lt;br/&gt;
    +      while (iter.hasNext) 
{
    +        iter.next().clearExecutionEdges()
    +      }
&lt;p&gt;    +&lt;br/&gt;
           cleanup(jobID)&lt;br/&gt;
         }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         case RequestArchivedJobs =&amp;gt; &lt;/p&gt;
{
    -      sender ! ArchivedJobs(graphs.values)
    +      sender ! ArchivedJobs(getAllGraphs())
         }

&lt;p&gt;         case RequestJob(jobID) =&amp;gt; {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;graphs.get(jobID) match {&lt;/li&gt;
	&lt;li&gt;case Some(graph) =&amp;gt; sender ! JobFound(jobID, graph)&lt;/li&gt;
	&lt;li&gt;case None =&amp;gt; sender ! JobNotFound(jobID)&lt;br/&gt;
    +      getGraph(jobID) match {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Why not working directly on graphs.get(jobID) and matching on the Option type?&lt;/p&gt;</comment>
                            <comment id="14304848" author="githubbot" created="Wed, 4 Feb 2015 09:47:32 +0000"  >&lt;p&gt;Github user tillrohrmann commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/344#discussion_r24072204&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/344#discussion_r24072204&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/MemoryArchivist.scala &amp;#8212;&lt;br/&gt;
    @@ -25,48 +25,82 @@ import org.apache.flink.runtime.jobgraph.JobID&lt;br/&gt;
     import org.apache.flink.runtime.messages.ArchiveMessages._&lt;br/&gt;
     import org.apache.flink.runtime.messages.JobManagerMessages._&lt;/p&gt;

&lt;p&gt;    +import scala.collection.mutable.LinkedHashMap&lt;br/&gt;
    +import scala.ref.SoftReference&lt;br/&gt;
    +&lt;br/&gt;
     class MemoryArchivist(private val max_entries: Int) extends Actor with ActorLogMessages with&lt;br/&gt;
     ActorLogging {&lt;br/&gt;
       /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Map of execution graphs belonging to recently started jobs with the time stamp of the last&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* received job event.&lt;br/&gt;
    +   * received job event. The insert order is preserved through a LinkedHashMap.&lt;br/&gt;
        */&lt;/li&gt;
	&lt;li&gt;val graphs = collection.mutable.HashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;JobID, ExecutionGraph&amp;#93;&lt;/span&gt;()&lt;/li&gt;
	&lt;li&gt;val lru = collection.mutable.Queue&lt;span class=&quot;error&quot;&gt;&amp;#91;JobID&amp;#93;&lt;/span&gt;()&lt;br/&gt;
    +  val graphs = LinkedHashMap[JobID, SoftReference&lt;span class=&quot;error&quot;&gt;&amp;#91;ExecutionGraph&amp;#93;&lt;/span&gt;]()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       override def receiveWithLogMessages: Receive = {&lt;br/&gt;
    +    /* Receive Execution Graph to archive */&lt;br/&gt;
         case ArchiveExecutionGraph(jobID, graph) =&amp;gt; {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;graphs.update(jobID, graph)&lt;br/&gt;
    +      // wrap graph inside a soft reference&lt;br/&gt;
    +      graphs.update(jobID, new SoftReference(graph))&lt;br/&gt;
    +&lt;br/&gt;
    +      // clear all execution edges of the graph&lt;br/&gt;
    +      val iter = graph.getAllExecutionVertices().iterator()&lt;br/&gt;
    +      while (iter.hasNext) 
{
    +        iter.next().clearExecutionEdges()
    +      }
&lt;p&gt;    +&lt;br/&gt;
           cleanup(jobID)&lt;br/&gt;
         }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         case RequestArchivedJobs =&amp;gt; &lt;/p&gt;
{
    -      sender ! ArchivedJobs(graphs.values)
    +      sender ! ArchivedJobs(getAllGraphs())
         }

&lt;p&gt;         case RequestJob(jobID) =&amp;gt; {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;graphs.get(jobID) match {&lt;/li&gt;
	&lt;li&gt;case Some(graph) =&amp;gt; sender ! JobFound(jobID, graph)&lt;/li&gt;
	&lt;li&gt;case None =&amp;gt; sender ! JobNotFound(jobID)&lt;br/&gt;
    +      getGraph(jobID) match 
{
    +        case graph: ExecutionGraph =&amp;gt; sender ! JobFound(jobID, graph)
    +        case _ =&amp;gt; sender ! JobNotFound(jobID)
           }
&lt;p&gt;         }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         case RequestJobStatus(jobID) =&amp;gt; {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;graphs.get(jobID) match {&lt;/li&gt;
	&lt;li&gt;case Some(eg) =&amp;gt; sender ! CurrentJobStatus(jobID, eg.getState)&lt;/li&gt;
	&lt;li&gt;case None =&amp;gt; sender ! JobNotFound(jobID)&lt;br/&gt;
    +      getGraph(jobID) match 
{
    +        case graph: ExecutionGraph =&amp;gt; sender ! CurrentJobStatus(jobID, graph.getState)
    +        case _ =&amp;gt; sender ! JobNotFound(jobID)
           }
&lt;p&gt;         }&lt;br/&gt;
       }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def cleanup(jobID: JobID): Unit = {&lt;/li&gt;
	&lt;li&gt;if (!lru.contains(jobID)) {&lt;/li&gt;
	&lt;li&gt;lru.enqueue(jobID)&lt;br/&gt;
    +  /**&lt;br/&gt;
    +   * Gets all graphs that have not been garbage collected.&lt;br/&gt;
    +   * @return An iterable with all valid ExecutionGraphs&lt;br/&gt;
    +   */&lt;br/&gt;
    +  def getAllGraphs() = graphs.values.flatMap(ref =&amp;gt; ref.get match 
{
    +    case Some(graph) =&amp;gt; Seq(graph)
    +    case _ =&amp;gt; Seq()
    +  }
&lt;p&gt;)&lt;br/&gt;
    +&lt;br/&gt;
    +  /**&lt;br/&gt;
    +   * Gets a graph with a jobID if it has not been garbage collected.&lt;br/&gt;
    +   * @param jobID&lt;br/&gt;
    +   * @return ExecutionGraph or null&lt;br/&gt;
    +   */&lt;br/&gt;
    +  def getGraph(jobID: JobID) = graphs.get(jobID) match {&lt;br/&gt;
    +    case Some(softRef) =&amp;gt; softRef.get match &lt;/p&gt;
{
    +      case Some(graph) =&amp;gt; graph
    +      case None =&amp;gt; null
         }
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Match is not exhaustive. What happens if graphs.get(jobID) returns None.&lt;/p&gt;</comment>
                            <comment id="14304854" author="githubbot" created="Wed, 4 Feb 2015 09:52:35 +0000"  >&lt;p&gt;Github user StephanEwen commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/344#issuecomment-72827283&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/344#issuecomment-72827283&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I have actually merged this branch locally and already addressed most of the issues you found&lt;/p&gt;</comment>
                            <comment id="14304857" author="githubbot" created="Wed, 4 Feb 2015 09:54:16 +0000"  >&lt;p&gt;Github user tillrohrmann commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/344#discussion_r24072529&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/344#discussion_r24072529&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/MemoryArchivist.scala &amp;#8212;&lt;br/&gt;
    @@ -25,48 +25,82 @@ import org.apache.flink.runtime.jobgraph.JobID&lt;br/&gt;
     import org.apache.flink.runtime.messages.ArchiveMessages._&lt;br/&gt;
     import org.apache.flink.runtime.messages.JobManagerMessages._&lt;/p&gt;

&lt;p&gt;    +import scala.collection.mutable.LinkedHashMap&lt;br/&gt;
    +import scala.ref.SoftReference&lt;br/&gt;
    +&lt;br/&gt;
     class MemoryArchivist(private val max_entries: Int) extends Actor with ActorLogMessages with&lt;br/&gt;
     ActorLogging {&lt;br/&gt;
       /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Map of execution graphs belonging to recently started jobs with the time stamp of the last&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* received job event.&lt;br/&gt;
    +   * received job event. The insert order is preserved through a LinkedHashMap.&lt;br/&gt;
        */&lt;/li&gt;
	&lt;li&gt;val graphs = collection.mutable.HashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;JobID, ExecutionGraph&amp;#93;&lt;/span&gt;()&lt;/li&gt;
	&lt;li&gt;val lru = collection.mutable.Queue&lt;span class=&quot;error&quot;&gt;&amp;#91;JobID&amp;#93;&lt;/span&gt;()&lt;br/&gt;
    +  val graphs = LinkedHashMap[JobID, SoftReference&lt;span class=&quot;error&quot;&gt;&amp;#91;ExecutionGraph&amp;#93;&lt;/span&gt;]()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       override def receiveWithLogMessages: Receive = {&lt;br/&gt;
    +    /* Receive Execution Graph to archive */&lt;br/&gt;
         case ArchiveExecutionGraph(jobID, graph) =&amp;gt; {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;graphs.update(jobID, graph)&lt;br/&gt;
    +      // wrap graph inside a soft reference&lt;br/&gt;
    +      graphs.update(jobID, new SoftReference(graph))&lt;br/&gt;
    +&lt;br/&gt;
    +      // clear all execution edges of the graph&lt;br/&gt;
    +      val iter = graph.getAllExecutionVertices().iterator()&lt;br/&gt;
    +      while (iter.hasNext) 
{
    +        iter.next().clearExecutionEdges()
    +      }
&lt;p&gt;    +&lt;br/&gt;
           cleanup(jobID)&lt;br/&gt;
         }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         case RequestArchivedJobs =&amp;gt; &lt;/p&gt;
{
    -      sender ! ArchivedJobs(graphs.values)
    +      sender ! ArchivedJobs(getAllGraphs())
         }

&lt;p&gt;         case RequestJob(jobID) =&amp;gt; {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;graphs.get(jobID) match {&lt;/li&gt;
	&lt;li&gt;case Some(graph) =&amp;gt; sender ! JobFound(jobID, graph)&lt;/li&gt;
	&lt;li&gt;case None =&amp;gt; sender ! JobNotFound(jobID)&lt;br/&gt;
    +      getGraph(jobID) match {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Ah I see, because of the soft ref. Why not letting getGraph return an Option type instead of null. Null values are evil. &lt;span class=&quot;error&quot;&gt;&amp;#91;Tony Hoares multi-billion dollar mistake&amp;#93;&lt;/span&gt; (&lt;a href=&quot;http://en.wikipedia.org/wiki/Tony_Hoare#Quotations&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://en.wikipedia.org/wiki/Tony_Hoare#Quotations&lt;/a&gt;)&lt;/p&gt;</comment>
                            <comment id="14304867" author="githubbot" created="Wed, 4 Feb 2015 10:00:32 +0000"  >&lt;p&gt;Github user tillrohrmann commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/344#discussion_r24072836&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/344#discussion_r24072836&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/MemoryArchivist.scala &amp;#8212;&lt;br/&gt;
    @@ -25,48 +25,82 @@ import org.apache.flink.runtime.jobgraph.JobID&lt;br/&gt;
     import org.apache.flink.runtime.messages.ArchiveMessages._&lt;br/&gt;
     import org.apache.flink.runtime.messages.JobManagerMessages._&lt;/p&gt;

&lt;p&gt;    +import scala.collection.mutable.LinkedHashMap&lt;br/&gt;
    +import scala.ref.SoftReference&lt;br/&gt;
    +&lt;br/&gt;
     class MemoryArchivist(private val max_entries: Int) extends Actor with ActorLogMessages with&lt;br/&gt;
     ActorLogging {&lt;br/&gt;
       /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Map of execution graphs belonging to recently started jobs with the time stamp of the last&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* received job event.&lt;br/&gt;
    +   * received job event. The insert order is preserved through a LinkedHashMap.&lt;br/&gt;
        */&lt;/li&gt;
	&lt;li&gt;val graphs = collection.mutable.HashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;JobID, ExecutionGraph&amp;#93;&lt;/span&gt;()&lt;/li&gt;
	&lt;li&gt;val lru = collection.mutable.Queue&lt;span class=&quot;error&quot;&gt;&amp;#91;JobID&amp;#93;&lt;/span&gt;()&lt;br/&gt;
    +  val graphs = LinkedHashMap[JobID, SoftReference&lt;span class=&quot;error&quot;&gt;&amp;#91;ExecutionGraph&amp;#93;&lt;/span&gt;]()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       override def receiveWithLogMessages: Receive = {&lt;br/&gt;
    +    /* Receive Execution Graph to archive */&lt;br/&gt;
         case ArchiveExecutionGraph(jobID, graph) =&amp;gt; {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;graphs.update(jobID, graph)&lt;br/&gt;
    +      // wrap graph inside a soft reference&lt;br/&gt;
    +      graphs.update(jobID, new SoftReference(graph))&lt;br/&gt;
    +&lt;br/&gt;
    +      // clear all execution edges of the graph&lt;br/&gt;
    +      val iter = graph.getAllExecutionVertices().iterator()&lt;br/&gt;
    +      while (iter.hasNext) 
{
    +        iter.next().clearExecutionEdges()
    +      }
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Why not using Scala power: graph.getAllExecutionVertices.asScala.foreach &lt;/p&gt;
{ _.clearExecutionEdges }
&lt;p&gt; with import scala.collection.JavaConvertes.iterableAsScalaIterableConverter in scope.&lt;/p&gt;</comment>
                            <comment id="14304875" author="githubbot" created="Wed, 4 Feb 2015 10:07:42 +0000"  >&lt;p&gt;Github user tillrohrmann commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/344#discussion_r24073173&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/344#discussion_r24073173&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/MemoryArchivist.scala &amp;#8212;&lt;br/&gt;
    @@ -25,48 +25,82 @@ import org.apache.flink.runtime.jobgraph.JobID&lt;br/&gt;
     import org.apache.flink.runtime.messages.ArchiveMessages._&lt;br/&gt;
     import org.apache.flink.runtime.messages.JobManagerMessages._&lt;/p&gt;

&lt;p&gt;    +import scala.collection.mutable.LinkedHashMap&lt;br/&gt;
    +import scala.ref.SoftReference&lt;br/&gt;
    +&lt;br/&gt;
     class MemoryArchivist(private val max_entries: Int) extends Actor with ActorLogMessages with&lt;br/&gt;
     ActorLogging {&lt;br/&gt;
       /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Map of execution graphs belonging to recently started jobs with the time stamp of the last&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* received job event.&lt;br/&gt;
    +   * received job event. The insert order is preserved through a LinkedHashMap.&lt;br/&gt;
        */&lt;/li&gt;
	&lt;li&gt;val graphs = collection.mutable.HashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;JobID, ExecutionGraph&amp;#93;&lt;/span&gt;()&lt;/li&gt;
	&lt;li&gt;val lru = collection.mutable.Queue&lt;span class=&quot;error&quot;&gt;&amp;#91;JobID&amp;#93;&lt;/span&gt;()&lt;br/&gt;
    +  val graphs = LinkedHashMap[JobID, SoftReference&lt;span class=&quot;error&quot;&gt;&amp;#91;ExecutionGraph&amp;#93;&lt;/span&gt;]()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       override def receiveWithLogMessages: Receive = {&lt;br/&gt;
    +    /* Receive Execution Graph to archive */&lt;br/&gt;
         case ArchiveExecutionGraph(jobID, graph) =&amp;gt; {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;graphs.update(jobID, graph)&lt;br/&gt;
    +      // wrap graph inside a soft reference&lt;br/&gt;
    +      graphs.update(jobID, new SoftReference(graph))&lt;br/&gt;
    +&lt;br/&gt;
    +      // clear all execution edges of the graph&lt;br/&gt;
    +      val iter = graph.getAllExecutionVertices().iterator()&lt;br/&gt;
    +      while (iter.hasNext) 
{
    +        iter.next().clearExecutionEdges()
    +      }
&lt;p&gt;    +&lt;br/&gt;
           cleanup(jobID)&lt;br/&gt;
         }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         case RequestArchivedJobs =&amp;gt; &lt;/p&gt;
{
    -      sender ! ArchivedJobs(graphs.values)
    +      sender ! ArchivedJobs(getAllGraphs())
         }

&lt;p&gt;         case RequestJob(jobID) =&amp;gt; {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;graphs.get(jobID) match {&lt;/li&gt;
	&lt;li&gt;case Some(graph) =&amp;gt; sender ! JobFound(jobID, graph)&lt;/li&gt;
	&lt;li&gt;case None =&amp;gt; sender ! JobNotFound(jobID)&lt;br/&gt;
    +      getGraph(jobID) match 
{
    +        case graph: ExecutionGraph =&amp;gt; sender ! JobFound(jobID, graph)
    +        case _ =&amp;gt; sender ! JobNotFound(jobID)
           }
&lt;p&gt;         }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         case RequestJobStatus(jobID) =&amp;gt; {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;graphs.get(jobID) match {&lt;/li&gt;
	&lt;li&gt;case Some(eg) =&amp;gt; sender ! CurrentJobStatus(jobID, eg.getState)&lt;/li&gt;
	&lt;li&gt;case None =&amp;gt; sender ! JobNotFound(jobID)&lt;br/&gt;
    +      getGraph(jobID) match 
{
    +        case graph: ExecutionGraph =&amp;gt; sender ! CurrentJobStatus(jobID, graph.getState)
    +        case _ =&amp;gt; sender ! JobNotFound(jobID)
           }
&lt;p&gt;         }&lt;br/&gt;
       }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def cleanup(jobID: JobID): Unit = {&lt;/li&gt;
	&lt;li&gt;if (!lru.contains(jobID)) {&lt;/li&gt;
	&lt;li&gt;lru.enqueue(jobID)&lt;br/&gt;
    +  /**&lt;br/&gt;
    +   * Gets all graphs that have not been garbage collected.&lt;br/&gt;
    +   * @return An iterable with all valid ExecutionGraphs&lt;br/&gt;
    +   */&lt;br/&gt;
    +  def getAllGraphs() = graphs.values.flatMap(ref =&amp;gt; ref.get match 
{
    +    case Some(graph) =&amp;gt; Seq(graph)
    +    case _ =&amp;gt; Seq()
    +  }
&lt;p&gt;)&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Did want to do ```graphs.values.flatMap&lt;/p&gt;
{ _.get }
&lt;p&gt;```? Returns the all archived ```ExecutionGraphs```. Helps if you add the return type to ```getAllGraphs```.&lt;/p&gt;</comment>
                            <comment id="14304876" author="githubbot" created="Wed, 4 Feb 2015 10:08:13 +0000"  >&lt;p&gt;Github user tillrohrmann commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/344#discussion_r24073205&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/344#discussion_r24073205&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-runtime/src/test/scala/org/apache/flink/runtime/testingUtils/TestingMemoryArchivist.scala &amp;#8212;&lt;br/&gt;
    @@ -31,10 +31,11 @@ trait TestingMemoryArchivist extends ActorLogMessages {&lt;/p&gt;

&lt;p&gt;       def receiveTestingMessages: Receive = {&lt;br/&gt;
         case RequestExecutionGraph(jobID) =&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;graphs.get(jobID) match {&lt;/li&gt;
	&lt;li&gt;case Some(executionGraph) =&amp;gt; sender ! ExecutionGraphFound(jobID, executionGraph)&lt;/li&gt;
	&lt;li&gt;case None =&amp;gt; sender ! ExecutionGraphNotFound(jobID)&lt;br/&gt;
    +      val executionGraph = getGraph(jobID)&lt;br/&gt;
    +      if (executionGraph != null) {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Again ```null```&lt;/p&gt;</comment>
                            <comment id="14304878" author="githubbot" created="Wed, 4 Feb 2015 10:09:59 +0000"  >&lt;p&gt;Github user StephanEwen commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/344#issuecomment-72829676&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/344#issuecomment-72829676&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    You can also shoot yourself in the foot with Option, no? There is no substitute for thorough programming...&lt;/p&gt;</comment>
                            <comment id="14304882" author="githubbot" created="Wed, 4 Feb 2015 10:13:26 +0000"  >&lt;p&gt;Github user tillrohrmann commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/344#discussion_r24073474&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/344#discussion_r24073474&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/MemoryArchivist.scala &amp;#8212;&lt;br/&gt;
    @@ -25,48 +25,82 @@ import org.apache.flink.runtime.jobgraph.JobID&lt;br/&gt;
     import org.apache.flink.runtime.messages.ArchiveMessages._&lt;br/&gt;
     import org.apache.flink.runtime.messages.JobManagerMessages._&lt;/p&gt;

&lt;p&gt;    +import scala.collection.mutable.LinkedHashMap&lt;br/&gt;
    +import scala.ref.SoftReference&lt;br/&gt;
    +&lt;br/&gt;
     class MemoryArchivist(private val max_entries: Int) extends Actor with ActorLogMessages with&lt;br/&gt;
     ActorLogging {&lt;br/&gt;
       /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Map of execution graphs belonging to recently started jobs with the time stamp of the last&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* received job event.&lt;br/&gt;
    +   * received job event. The insert order is preserved through a LinkedHashMap.&lt;br/&gt;
        */&lt;/li&gt;
	&lt;li&gt;val graphs = collection.mutable.HashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;JobID, ExecutionGraph&amp;#93;&lt;/span&gt;()&lt;/li&gt;
	&lt;li&gt;val lru = collection.mutable.Queue&lt;span class=&quot;error&quot;&gt;&amp;#91;JobID&amp;#93;&lt;/span&gt;()&lt;br/&gt;
    +  val graphs = LinkedHashMap[JobID, SoftReference&lt;span class=&quot;error&quot;&gt;&amp;#91;ExecutionGraph&amp;#93;&lt;/span&gt;]()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       override def receiveWithLogMessages: Receive = {&lt;br/&gt;
    +    /* Receive Execution Graph to archive */&lt;br/&gt;
         case ArchiveExecutionGraph(jobID, graph) =&amp;gt; {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;graphs.update(jobID, graph)&lt;br/&gt;
    +      // wrap graph inside a soft reference&lt;br/&gt;
    +      graphs.update(jobID, new SoftReference(graph))&lt;br/&gt;
    +&lt;br/&gt;
    +      // clear all execution edges of the graph&lt;br/&gt;
    +      val iter = graph.getAllExecutionVertices().iterator()&lt;br/&gt;
    +      while (iter.hasNext) 
{
    +        iter.next().clearExecutionEdges()
    +      }
&lt;p&gt;    +&lt;br/&gt;
           cleanup(jobID)&lt;br/&gt;
         }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         case RequestArchivedJobs =&amp;gt; &lt;/p&gt;
{
    -      sender ! ArchivedJobs(graphs.values)
    +      sender ! ArchivedJobs(getAllGraphs())
         }

&lt;p&gt;         case RequestJob(jobID) =&amp;gt; {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;graphs.get(jobID) match {&lt;/li&gt;
	&lt;li&gt;case Some(graph) =&amp;gt; sender ! JobFound(jobID, graph)&lt;/li&gt;
	&lt;li&gt;case None =&amp;gt; sender ! JobNotFound(jobID)&lt;br/&gt;
    +      getGraph(jobID) match 
{
    +        case graph: ExecutionGraph =&amp;gt; sender ! JobFound(jobID, graph)
    +        case _ =&amp;gt; sender ! JobNotFound(jobID)
           }
&lt;p&gt;         }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         case RequestJobStatus(jobID) =&amp;gt; {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;graphs.get(jobID) match {&lt;/li&gt;
	&lt;li&gt;case Some(eg) =&amp;gt; sender ! CurrentJobStatus(jobID, eg.getState)&lt;/li&gt;
	&lt;li&gt;case None =&amp;gt; sender ! JobNotFound(jobID)&lt;br/&gt;
    +      getGraph(jobID) match 
{
    +        case graph: ExecutionGraph =&amp;gt; sender ! CurrentJobStatus(jobID, graph.getState)
    +        case _ =&amp;gt; sender ! JobNotFound(jobID)
           }
&lt;p&gt;         }&lt;br/&gt;
       }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def cleanup(jobID: JobID): Unit = {&lt;/li&gt;
	&lt;li&gt;if (!lru.contains(jobID)) {&lt;/li&gt;
	&lt;li&gt;lru.enqueue(jobID)&lt;br/&gt;
    +  /**&lt;br/&gt;
    +   * Gets all graphs that have not been garbage collected.&lt;br/&gt;
    +   * @return An iterable with all valid ExecutionGraphs&lt;br/&gt;
    +   */&lt;br/&gt;
    +  def getAllGraphs() = graphs.values.flatMap(ref =&amp;gt; ref.get match 
{
    +    case Some(graph) =&amp;gt; Seq(graph)
    +    case _ =&amp;gt; Seq()
    +  }
&lt;p&gt;)&lt;br/&gt;
    +&lt;br/&gt;
    +  /**&lt;br/&gt;
    +   * Gets a graph with a jobID if it has not been garbage collected.&lt;br/&gt;
    +   * @param jobID&lt;br/&gt;
    +   * @return ExecutionGraph or null&lt;br/&gt;
    +   */&lt;br/&gt;
    +  def getGraph(jobID: JobID) = graphs.get(jobID) match &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {    +    case Some(softRef) =&amp;gt; softRef.get match {
    +      case Some(graph) =&amp;gt; graph
    +      case None =&amp;gt; null
         }    +  }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;while (lru.size &amp;gt; max_entries) {&lt;/li&gt;
	&lt;li&gt;val removedJobID = lru.dequeue()&lt;/li&gt;
	&lt;li&gt;graphs.remove(removedJobID)&lt;br/&gt;
    +  /**&lt;br/&gt;
    +   * Remove old ExecutionGraphs belonging to a jobID&lt;br/&gt;
    +   * * if more than max_entries are in the queue.&lt;br/&gt;
    +   * @param jobID&lt;br/&gt;
    +   */&lt;br/&gt;
    +  private def cleanup(jobID: JobID): Unit = {&lt;br/&gt;
    +    while (graphs.size &amp;gt; max_entries) {&lt;br/&gt;
    +      // get first graph inserted&lt;br/&gt;
    +      val (jobID, value) = graphs.iterator.next()
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    ```graphs.head``` equivalent to ```graphs.iterator.next()``` and shorter.&lt;/p&gt;</comment>
                            <comment id="14304893" author="githubbot" created="Wed, 4 Feb 2015 10:24:10 +0000"  >&lt;p&gt;Github user tillrohrmann commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/344#issuecomment-72831706&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/344#issuecomment-72831706&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    It is true that thorough programming is always the best, but Option encourages you to apply a more functional programming style which in many cases results in safer code. For example, Scala warns you if the pattern matching on Option is not exhaustive, whereas this is not the case for arbitrary classes and null values.&lt;/p&gt;</comment>
                            <comment id="14304895" author="githubbot" created="Wed, 4 Feb 2015 10:27:44 +0000"  >&lt;p&gt;Github user tillrohrmann commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/344#issuecomment-72832200&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/344#issuecomment-72832200&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Ah ok perfect that you already addressed the minor issues.&lt;/p&gt;</comment>
                            <comment id="14305356" author="githubbot" created="Wed, 4 Feb 2015 16:00:53 +0000"  >&lt;p&gt;Github user mxm commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/344#discussion_r24093420&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/344#discussion_r24093420&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/MemoryArchivist.scala &amp;#8212;&lt;br/&gt;
    @@ -25,48 +25,82 @@ import org.apache.flink.runtime.jobgraph.JobID&lt;br/&gt;
     import org.apache.flink.runtime.messages.ArchiveMessages._&lt;br/&gt;
     import org.apache.flink.runtime.messages.JobManagerMessages._&lt;/p&gt;

&lt;p&gt;    +import scala.collection.mutable.LinkedHashMap&lt;br/&gt;
    +import scala.ref.SoftReference&lt;br/&gt;
    +&lt;br/&gt;
     class MemoryArchivist(private val max_entries: Int) extends Actor with ActorLogMessages with&lt;br/&gt;
     ActorLogging {&lt;br/&gt;
       /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Map of execution graphs belonging to recently started jobs with the time stamp of the last&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* received job event.&lt;br/&gt;
    +   * received job event. The insert order is preserved through a LinkedHashMap.&lt;br/&gt;
        */&lt;/li&gt;
	&lt;li&gt;val graphs = collection.mutable.HashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;JobID, ExecutionGraph&amp;#93;&lt;/span&gt;()&lt;/li&gt;
	&lt;li&gt;val lru = collection.mutable.Queue&lt;span class=&quot;error&quot;&gt;&amp;#91;JobID&amp;#93;&lt;/span&gt;()&lt;br/&gt;
    +  val graphs = LinkedHashMap[JobID, SoftReference&lt;span class=&quot;error&quot;&gt;&amp;#91;ExecutionGraph&amp;#93;&lt;/span&gt;]()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       override def receiveWithLogMessages: Receive = {&lt;br/&gt;
    +    /* Receive Execution Graph to archive */&lt;br/&gt;
         case ArchiveExecutionGraph(jobID, graph) =&amp;gt; {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;graphs.update(jobID, graph)&lt;br/&gt;
    +      // wrap graph inside a soft reference&lt;br/&gt;
    +      graphs.update(jobID, new SoftReference(graph))&lt;br/&gt;
    +&lt;br/&gt;
    +      // clear all execution edges of the graph&lt;br/&gt;
    +      val iter = graph.getAllExecutionVertices().iterator()&lt;br/&gt;
    +      while (iter.hasNext) 
{
    +        iter.next().clearExecutionEdges()
    +      }
&lt;p&gt;    +&lt;br/&gt;
           cleanup(jobID)&lt;br/&gt;
         }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         case RequestArchivedJobs =&amp;gt; &lt;/p&gt;
{
    -      sender ! ArchivedJobs(graphs.values)
    +      sender ! ArchivedJobs(getAllGraphs())
         }

&lt;p&gt;         case RequestJob(jobID) =&amp;gt; {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;graphs.get(jobID) match {&lt;/li&gt;
	&lt;li&gt;case Some(graph) =&amp;gt; sender ! JobFound(jobID, graph)&lt;/li&gt;
	&lt;li&gt;case None =&amp;gt; sender ! JobNotFound(jobID)&lt;br/&gt;
    +      getGraph(jobID) match {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Nice read. I&apos;ll try to avoid null the next time.&lt;/p&gt;</comment>
                            <comment id="14305357" author="githubbot" created="Wed, 4 Feb 2015 16:00:57 +0000"  >&lt;p&gt;Github user mxm commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/344#discussion_r24093429&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/344#discussion_r24093429&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/MemoryArchivist.scala &amp;#8212;&lt;br/&gt;
    @@ -25,48 +25,82 @@ import org.apache.flink.runtime.jobgraph.JobID&lt;br/&gt;
     import org.apache.flink.runtime.messages.ArchiveMessages._&lt;br/&gt;
     import org.apache.flink.runtime.messages.JobManagerMessages._&lt;/p&gt;

&lt;p&gt;    +import scala.collection.mutable.LinkedHashMap&lt;br/&gt;
    +import scala.ref.SoftReference&lt;br/&gt;
    +&lt;br/&gt;
     class MemoryArchivist(private val max_entries: Int) extends Actor with ActorLogMessages with&lt;br/&gt;
     ActorLogging {&lt;br/&gt;
       /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Map of execution graphs belonging to recently started jobs with the time stamp of the last&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* received job event.&lt;br/&gt;
    +   * received job event. The insert order is preserved through a LinkedHashMap.&lt;br/&gt;
        */&lt;/li&gt;
	&lt;li&gt;val graphs = collection.mutable.HashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;JobID, ExecutionGraph&amp;#93;&lt;/span&gt;()&lt;/li&gt;
	&lt;li&gt;val lru = collection.mutable.Queue&lt;span class=&quot;error&quot;&gt;&amp;#91;JobID&amp;#93;&lt;/span&gt;()&lt;br/&gt;
    +  val graphs = LinkedHashMap[JobID, SoftReference&lt;span class=&quot;error&quot;&gt;&amp;#91;ExecutionGraph&amp;#93;&lt;/span&gt;]()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       override def receiveWithLogMessages: Receive = {&lt;br/&gt;
    +    /* Receive Execution Graph to archive */&lt;br/&gt;
         case ArchiveExecutionGraph(jobID, graph) =&amp;gt; {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;graphs.update(jobID, graph)&lt;br/&gt;
    +      // wrap graph inside a soft reference&lt;br/&gt;
    +      graphs.update(jobID, new SoftReference(graph))&lt;br/&gt;
    +&lt;br/&gt;
    +      // clear all execution edges of the graph&lt;br/&gt;
    +      val iter = graph.getAllExecutionVertices().iterator()&lt;br/&gt;
    +      while (iter.hasNext) 
{
    +        iter.next().clearExecutionEdges()
    +      }
&lt;p&gt;    +&lt;br/&gt;
           cleanup(jobID)&lt;br/&gt;
         }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         case RequestArchivedJobs =&amp;gt; &lt;/p&gt;
{
    -      sender ! ArchivedJobs(graphs.values)
    +      sender ! ArchivedJobs(getAllGraphs())
         }

&lt;p&gt;         case RequestJob(jobID) =&amp;gt; {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;graphs.get(jobID) match {&lt;/li&gt;
	&lt;li&gt;case Some(graph) =&amp;gt; sender ! JobFound(jobID, graph)&lt;/li&gt;
	&lt;li&gt;case None =&amp;gt; sender ! JobNotFound(jobID)&lt;br/&gt;
    +      getGraph(jobID) match 
{
    +        case graph: ExecutionGraph =&amp;gt; sender ! JobFound(jobID, graph)
    +        case _ =&amp;gt; sender ! JobNotFound(jobID)
           }
&lt;p&gt;         }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         case RequestJobStatus(jobID) =&amp;gt; {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;graphs.get(jobID) match {&lt;/li&gt;
	&lt;li&gt;case Some(eg) =&amp;gt; sender ! CurrentJobStatus(jobID, eg.getState)&lt;/li&gt;
	&lt;li&gt;case None =&amp;gt; sender ! JobNotFound(jobID)&lt;br/&gt;
    +      getGraph(jobID) match 
{
    +        case graph: ExecutionGraph =&amp;gt; sender ! CurrentJobStatus(jobID, graph.getState)
    +        case _ =&amp;gt; sender ! JobNotFound(jobID)
           }
&lt;p&gt;         }&lt;br/&gt;
       }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def cleanup(jobID: JobID): Unit = {&lt;/li&gt;
	&lt;li&gt;if (!lru.contains(jobID)) {&lt;/li&gt;
	&lt;li&gt;lru.enqueue(jobID)&lt;br/&gt;
    +  /**&lt;br/&gt;
    +   * Gets all graphs that have not been garbage collected.&lt;br/&gt;
    +   * @return An iterable with all valid ExecutionGraphs&lt;br/&gt;
    +   */&lt;br/&gt;
    +  def getAllGraphs() = graphs.values.flatMap(ref =&amp;gt; ref.get match 
{
    +    case Some(graph) =&amp;gt; Seq(graph)
    +    case _ =&amp;gt; Seq()
    +  }
&lt;p&gt;)&lt;br/&gt;
    +&lt;br/&gt;
    +  /**&lt;br/&gt;
    +   * Gets a graph with a jobID if it has not been garbage collected.&lt;br/&gt;
    +   * @param jobID&lt;br/&gt;
    +   * @return ExecutionGraph or null&lt;br/&gt;
    +   */&lt;br/&gt;
    +  def getGraph(jobID: JobID) = graphs.get(jobID) match {&lt;br/&gt;
    +    case Some(softRef) =&amp;gt; softRef.get match &lt;/p&gt;
{
    +      case Some(graph) =&amp;gt; graph
    +      case None =&amp;gt; null
         }
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Right you are.&lt;/p&gt;</comment>
                            <comment id="14305358" author="githubbot" created="Wed, 4 Feb 2015 16:01:01 +0000"  >&lt;p&gt;Github user mxm commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/344#discussion_r24093436&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/344#discussion_r24093436&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/MemoryArchivist.scala &amp;#8212;&lt;br/&gt;
    @@ -25,48 +25,82 @@ import org.apache.flink.runtime.jobgraph.JobID&lt;br/&gt;
     import org.apache.flink.runtime.messages.ArchiveMessages._&lt;br/&gt;
     import org.apache.flink.runtime.messages.JobManagerMessages._&lt;/p&gt;

&lt;p&gt;    +import scala.collection.mutable.LinkedHashMap&lt;br/&gt;
    +import scala.ref.SoftReference&lt;br/&gt;
    +&lt;br/&gt;
     class MemoryArchivist(private val max_entries: Int) extends Actor with ActorLogMessages with&lt;br/&gt;
     ActorLogging {&lt;br/&gt;
       /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Map of execution graphs belonging to recently started jobs with the time stamp of the last&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* received job event.&lt;br/&gt;
    +   * received job event. The insert order is preserved through a LinkedHashMap.&lt;br/&gt;
        */&lt;/li&gt;
	&lt;li&gt;val graphs = collection.mutable.HashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;JobID, ExecutionGraph&amp;#93;&lt;/span&gt;()&lt;/li&gt;
	&lt;li&gt;val lru = collection.mutable.Queue&lt;span class=&quot;error&quot;&gt;&amp;#91;JobID&amp;#93;&lt;/span&gt;()&lt;br/&gt;
    +  val graphs = LinkedHashMap[JobID, SoftReference&lt;span class=&quot;error&quot;&gt;&amp;#91;ExecutionGraph&amp;#93;&lt;/span&gt;]()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       override def receiveWithLogMessages: Receive = {&lt;br/&gt;
    +    /* Receive Execution Graph to archive */&lt;br/&gt;
         case ArchiveExecutionGraph(jobID, graph) =&amp;gt; {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;graphs.update(jobID, graph)&lt;br/&gt;
    +      // wrap graph inside a soft reference&lt;br/&gt;
    +      graphs.update(jobID, new SoftReference(graph))&lt;br/&gt;
    +&lt;br/&gt;
    +      // clear all execution edges of the graph&lt;br/&gt;
    +      val iter = graph.getAllExecutionVertices().iterator()&lt;br/&gt;
    +      while (iter.hasNext) 
{
    +        iter.next().clearExecutionEdges()
    +      }
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Much nicer! Thanks.&lt;/p&gt;</comment>
                            <comment id="14305359" author="githubbot" created="Wed, 4 Feb 2015 16:01:08 +0000"  >&lt;p&gt;Github user mxm commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/344#discussion_r24093445&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/344#discussion_r24093445&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/MemoryArchivist.scala &amp;#8212;&lt;br/&gt;
    @@ -25,48 +25,82 @@ import org.apache.flink.runtime.jobgraph.JobID&lt;br/&gt;
     import org.apache.flink.runtime.messages.ArchiveMessages._&lt;br/&gt;
     import org.apache.flink.runtime.messages.JobManagerMessages._&lt;/p&gt;

&lt;p&gt;    +import scala.collection.mutable.LinkedHashMap&lt;br/&gt;
    +import scala.ref.SoftReference&lt;br/&gt;
    +&lt;br/&gt;
     class MemoryArchivist(private val max_entries: Int) extends Actor with ActorLogMessages with&lt;br/&gt;
     ActorLogging {&lt;br/&gt;
       /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Map of execution graphs belonging to recently started jobs with the time stamp of the last&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* received job event.&lt;br/&gt;
    +   * received job event. The insert order is preserved through a LinkedHashMap.&lt;br/&gt;
        */&lt;/li&gt;
	&lt;li&gt;val graphs = collection.mutable.HashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;JobID, ExecutionGraph&amp;#93;&lt;/span&gt;()&lt;/li&gt;
	&lt;li&gt;val lru = collection.mutable.Queue&lt;span class=&quot;error&quot;&gt;&amp;#91;JobID&amp;#93;&lt;/span&gt;()&lt;br/&gt;
    +  val graphs = LinkedHashMap[JobID, SoftReference&lt;span class=&quot;error&quot;&gt;&amp;#91;ExecutionGraph&amp;#93;&lt;/span&gt;]()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       override def receiveWithLogMessages: Receive = {&lt;br/&gt;
    +    /* Receive Execution Graph to archive */&lt;br/&gt;
         case ArchiveExecutionGraph(jobID, graph) =&amp;gt; {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;graphs.update(jobID, graph)&lt;br/&gt;
    +      // wrap graph inside a soft reference&lt;br/&gt;
    +      graphs.update(jobID, new SoftReference(graph))&lt;br/&gt;
    +&lt;br/&gt;
    +      // clear all execution edges of the graph&lt;br/&gt;
    +      val iter = graph.getAllExecutionVertices().iterator()&lt;br/&gt;
    +      while (iter.hasNext) 
{
    +        iter.next().clearExecutionEdges()
    +      }
&lt;p&gt;    +&lt;br/&gt;
           cleanup(jobID)&lt;br/&gt;
         }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         case RequestArchivedJobs =&amp;gt; &lt;/p&gt;
{
    -      sender ! ArchivedJobs(graphs.values)
    +      sender ! ArchivedJobs(getAllGraphs())
         }

&lt;p&gt;         case RequestJob(jobID) =&amp;gt; {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;graphs.get(jobID) match {&lt;/li&gt;
	&lt;li&gt;case Some(graph) =&amp;gt; sender ! JobFound(jobID, graph)&lt;/li&gt;
	&lt;li&gt;case None =&amp;gt; sender ! JobNotFound(jobID)&lt;br/&gt;
    +      getGraph(jobID) match 
{
    +        case graph: ExecutionGraph =&amp;gt; sender ! JobFound(jobID, graph)
    +        case _ =&amp;gt; sender ! JobNotFound(jobID)
           }
&lt;p&gt;         }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         case RequestJobStatus(jobID) =&amp;gt; {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;graphs.get(jobID) match {&lt;/li&gt;
	&lt;li&gt;case Some(eg) =&amp;gt; sender ! CurrentJobStatus(jobID, eg.getState)&lt;/li&gt;
	&lt;li&gt;case None =&amp;gt; sender ! JobNotFound(jobID)&lt;br/&gt;
    +      getGraph(jobID) match 
{
    +        case graph: ExecutionGraph =&amp;gt; sender ! CurrentJobStatus(jobID, graph.getState)
    +        case _ =&amp;gt; sender ! JobNotFound(jobID)
           }
&lt;p&gt;         }&lt;br/&gt;
       }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def cleanup(jobID: JobID): Unit = {&lt;/li&gt;
	&lt;li&gt;if (!lru.contains(jobID)) {&lt;/li&gt;
	&lt;li&gt;lru.enqueue(jobID)&lt;br/&gt;
    +  /**&lt;br/&gt;
    +   * Gets all graphs that have not been garbage collected.&lt;br/&gt;
    +   * @return An iterable with all valid ExecutionGraphs&lt;br/&gt;
    +   */&lt;br/&gt;
    +  def getAllGraphs() = graphs.values.flatMap(ref =&amp;gt; ref.get match 
{
    +    case Some(graph) =&amp;gt; Seq(graph)
    +    case _ =&amp;gt; Seq()
    +  }
&lt;p&gt;)&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    I wanted to ignore the null value results for garbage collected items in `graphs`.&lt;/p&gt;</comment>
                            <comment id="14305360" author="githubbot" created="Wed, 4 Feb 2015 16:01:17 +0000"  >&lt;p&gt;Github user mxm commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/344#discussion_r24093464&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/344#discussion_r24093464&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/MemoryArchivist.scala &amp;#8212;&lt;br/&gt;
    @@ -25,48 +25,82 @@ import org.apache.flink.runtime.jobgraph.JobID&lt;br/&gt;
     import org.apache.flink.runtime.messages.ArchiveMessages._&lt;br/&gt;
     import org.apache.flink.runtime.messages.JobManagerMessages._&lt;/p&gt;

&lt;p&gt;    +import scala.collection.mutable.LinkedHashMap&lt;br/&gt;
    +import scala.ref.SoftReference&lt;br/&gt;
    +&lt;br/&gt;
     class MemoryArchivist(private val max_entries: Int) extends Actor with ActorLogMessages with&lt;br/&gt;
     ActorLogging {&lt;br/&gt;
       /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Map of execution graphs belonging to recently started jobs with the time stamp of the last&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* received job event.&lt;br/&gt;
    +   * received job event. The insert order is preserved through a LinkedHashMap.&lt;br/&gt;
        */&lt;/li&gt;
	&lt;li&gt;val graphs = collection.mutable.HashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;JobID, ExecutionGraph&amp;#93;&lt;/span&gt;()&lt;/li&gt;
	&lt;li&gt;val lru = collection.mutable.Queue&lt;span class=&quot;error&quot;&gt;&amp;#91;JobID&amp;#93;&lt;/span&gt;()&lt;br/&gt;
    +  val graphs = LinkedHashMap[JobID, SoftReference&lt;span class=&quot;error&quot;&gt;&amp;#91;ExecutionGraph&amp;#93;&lt;/span&gt;]()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       override def receiveWithLogMessages: Receive = {&lt;br/&gt;
    +    /* Receive Execution Graph to archive */&lt;br/&gt;
         case ArchiveExecutionGraph(jobID, graph) =&amp;gt; {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;graphs.update(jobID, graph)&lt;br/&gt;
    +      // wrap graph inside a soft reference&lt;br/&gt;
    +      graphs.update(jobID, new SoftReference(graph))&lt;br/&gt;
    +&lt;br/&gt;
    +      // clear all execution edges of the graph&lt;br/&gt;
    +      val iter = graph.getAllExecutionVertices().iterator()&lt;br/&gt;
    +      while (iter.hasNext) 
{
    +        iter.next().clearExecutionEdges()
    +      }
&lt;p&gt;    +&lt;br/&gt;
           cleanup(jobID)&lt;br/&gt;
         }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         case RequestArchivedJobs =&amp;gt; &lt;/p&gt;
{
    -      sender ! ArchivedJobs(graphs.values)
    +      sender ! ArchivedJobs(getAllGraphs())
         }

&lt;p&gt;         case RequestJob(jobID) =&amp;gt; {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;graphs.get(jobID) match {&lt;/li&gt;
	&lt;li&gt;case Some(graph) =&amp;gt; sender ! JobFound(jobID, graph)&lt;/li&gt;
	&lt;li&gt;case None =&amp;gt; sender ! JobNotFound(jobID)&lt;br/&gt;
    +      getGraph(jobID) match 
{
    +        case graph: ExecutionGraph =&amp;gt; sender ! JobFound(jobID, graph)
    +        case _ =&amp;gt; sender ! JobNotFound(jobID)
           }
&lt;p&gt;         }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         case RequestJobStatus(jobID) =&amp;gt; {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;graphs.get(jobID) match {&lt;/li&gt;
	&lt;li&gt;case Some(eg) =&amp;gt; sender ! CurrentJobStatus(jobID, eg.getState)&lt;/li&gt;
	&lt;li&gt;case None =&amp;gt; sender ! JobNotFound(jobID)&lt;br/&gt;
    +      getGraph(jobID) match 
{
    +        case graph: ExecutionGraph =&amp;gt; sender ! CurrentJobStatus(jobID, graph.getState)
    +        case _ =&amp;gt; sender ! JobNotFound(jobID)
           }
&lt;p&gt;         }&lt;br/&gt;
       }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def cleanup(jobID: JobID): Unit = {&lt;/li&gt;
	&lt;li&gt;if (!lru.contains(jobID)) {&lt;/li&gt;
	&lt;li&gt;lru.enqueue(jobID)&lt;br/&gt;
    +  /**&lt;br/&gt;
    +   * Gets all graphs that have not been garbage collected.&lt;br/&gt;
    +   * @return An iterable with all valid ExecutionGraphs&lt;br/&gt;
    +   */&lt;br/&gt;
    +  def getAllGraphs() = graphs.values.flatMap(ref =&amp;gt; ref.get match 
{
    +    case Some(graph) =&amp;gt; Seq(graph)
    +    case _ =&amp;gt; Seq()
    +  }
&lt;p&gt;)&lt;br/&gt;
    +&lt;br/&gt;
    +  /**&lt;br/&gt;
    +   * Gets a graph with a jobID if it has not been garbage collected.&lt;br/&gt;
    +   * @param jobID&lt;br/&gt;
    +   * @return ExecutionGraph or null&lt;br/&gt;
    +   */&lt;br/&gt;
    +  def getGraph(jobID: JobID) = graphs.get(jobID) match &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {    +    case Some(softRef) =&amp;gt; softRef.get match {
    +      case Some(graph) =&amp;gt; graph
    +      case None =&amp;gt; null
         }    +  }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;while (lru.size &amp;gt; max_entries) {&lt;/li&gt;
	&lt;li&gt;val removedJobID = lru.dequeue()&lt;/li&gt;
	&lt;li&gt;graphs.remove(removedJobID)&lt;br/&gt;
    +  /**&lt;br/&gt;
    +   * Remove old ExecutionGraphs belonging to a jobID&lt;br/&gt;
    +   * * if more than max_entries are in the queue.&lt;br/&gt;
    +   * @param jobID&lt;br/&gt;
    +   */&lt;br/&gt;
    +  private def cleanup(jobID: JobID): Unit = {&lt;br/&gt;
    +    while (graphs.size &amp;gt; max_entries) {&lt;br/&gt;
    +      // get first graph inserted&lt;br/&gt;
    +      val (jobID, value) = graphs.iterator.next()
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Thanks.&lt;/p&gt;</comment>
                            <comment id="14305468" author="githubbot" created="Wed, 4 Feb 2015 16:41:57 +0000"  >&lt;p&gt;Github user hsaputra commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/344#discussion_r24097257&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/344#discussion_r24097257&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-runtime/src/test/scala/org/apache/flink/runtime/testingUtils/TestingMemoryArchivist.scala &amp;#8212;&lt;br/&gt;
    @@ -31,10 +31,11 @@ trait TestingMemoryArchivist extends ActorLogMessages {&lt;/p&gt;

&lt;p&gt;       def receiveTestingMessages: Receive = {&lt;br/&gt;
         case RequestExecutionGraph(jobID) =&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;graphs.get(jobID) match {&lt;/li&gt;
	&lt;li&gt;case Some(executionGraph) =&amp;gt; sender ! ExecutionGraphFound(jobID, executionGraph)&lt;/li&gt;
	&lt;li&gt;case None =&amp;gt; sender ! ExecutionGraphNotFound(jobID)&lt;br/&gt;
    +      val executionGraph = getGraph(jobID)&lt;br/&gt;
    +      if (executionGraph != null) {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    I like @tillrohrmann to use Option as alternative to null. In Java land, Guava&apos;s Optional could be use to do similar thing (which I think will be part of Java8)&lt;/p&gt;</comment>
                            <comment id="14307133" author="githubbot" created="Thu, 5 Feb 2015 12:25:57 +0000"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/344&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/344&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14307146" author="stephanewen" created="Thu, 5 Feb 2015 12:39:54 +0000"  >&lt;p&gt;Fixed via 9d181a86a0870204113271b6e45f611cba04fc7d and 8ae0dc2d768aecfa3129df553f43d827792b65d7&lt;/p&gt;</comment>
                            <comment id="14307476" author="githubbot" created="Thu, 5 Feb 2015 16:12:20 +0000"  >&lt;p&gt;Github user StephanEwen commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/344#issuecomment-73073294&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/344#issuecomment-73073294&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I addressed the issues already in a cleanup commit built on top of #317 . See 56b7f85b4f6d522765df19a9710a098092ccde56&lt;/p&gt;</comment>
                            <comment id="14307484" author="githubbot" created="Thu, 5 Feb 2015 16:19:18 +0000"  >&lt;p&gt;Github user hsaputra commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/344#issuecomment-73074657&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/344#issuecomment-73074657&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Awesome! Thanks for the update.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12819246">FLINK-1843</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 41 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i24rlj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>