<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 20:36:07 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-10736] Shaded Hadoop S3A end-to-end test failed on Travis</title>
                <link>https://issues.apache.org/jira/browse/FLINK-10736</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;p&gt;The &lt;tt&gt;Shaded Hadoop S3A end-to-end test&lt;/tt&gt; failed on Travis because it could not find a file stored on S3:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
org.apache.flink.client.program.ProgramInvocationException: Job failed. (JobID: f28270bedd943ed6b41548b60f5cea73)
	at org.apache.flink.client.program.&lt;span class=&quot;code-keyword&quot;&gt;rest&lt;/span&gt;.RestClusterClient.submitJob(RestClusterClient.java:268)
	at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:487)
	at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:475)
	at org.apache.flink.client.program.ContextEnvironment.execute(ContextEnvironment.java:62)
	at org.apache.flink.examples.java.wordcount.WordCount.main(WordCount.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:529)
	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:421)
	at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:427)
	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:813)
	at org.apache.flink.client.cli.CliFrontend.runProgram(CliFrontend.java:287)
	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:213)
	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:1050)
	at org.apache.flink.client.cli.CliFrontend.lambda$main$11(CliFrontend.java:1126)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
	at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1126)
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:146)
	at org.apache.flink.client.program.&lt;span class=&quot;code-keyword&quot;&gt;rest&lt;/span&gt;.RestClusterClient.submitJob(RestClusterClient.java:265)
	... 21 more
Caused by: java.io.IOException: Error opening the Input Split s3:&lt;span class=&quot;code-comment&quot;&gt;//[secure]/flink-end-to-end-test-shaded-s3a [0,44]: No such file or directory: s3://[secure]/flink-end-to-end-test-shaded-s3a
&lt;/span&gt;	at org.apache.flink.api.common.io.FileInputFormat.open(FileInputFormat.java:824)
	at org.apache.flink.api.common.io.DelimitedInputFormat.open(DelimitedInputFormat.java:470)
	at org.apache.flink.api.common.io.DelimitedInputFormat.open(DelimitedInputFormat.java:47)
	at org.apache.flink.runtime.operators.DataSourceTask.invoke(DataSourceTask.java:170)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:704)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:748)
Caused by: java.io.FileNotFoundException: No such file or directory: s3:&lt;span class=&quot;code-comment&quot;&gt;//[secure]/flink-end-to-end-test-shaded-s3a
&lt;/span&gt;	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2255)
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2149)
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2088)
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:699)
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.FileSystem.open(FileSystem.java:950)
	at org.apache.flink.fs.s3.common.hadoop.HadoopFileSystem.open(HadoopFileSystem.java:120)
	at org.apache.flink.fs.s3.common.hadoop.HadoopFileSystem.open(HadoopFileSystem.java:37)
	at org.apache.flink.api.common.io.FileInputFormat$InputSplitOpenThread.run(FileInputFormat.java:996)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;https://api.travis-ci.org/v3/job/448770093/log.txt&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://api.travis-ci.org/v3/job/448770093/log.txt&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The solution could to harden this test case.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13195378">FLINK-10736</key>
            <summary>Shaded Hadoop S3A end-to-end test failed on Travis</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="azagrebin">Andrey Zagrebin</assignee>
                                    <reporter username="trohrmann">Till Rohrmann</reporter>
                        <labels>
                            <label>pull-request-available</label>
                            <label>test-stability</label>
                    </labels>
                <created>Wed, 31 Oct 2018 13:25:10 +0000</created>
                <updated>Wed, 2 Oct 2019 17:43:55 +0000</updated>
                            <resolved>Fri, 16 Nov 2018 20:17:03 +0000</resolved>
                                    <version>1.7.0</version>
                                    <fixVersion>1.7.0</fixVersion>
                                    <component>Test Infrastructure</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="16673512" author="stephanewen" created="Fri, 2 Nov 2018 18:21:17 +0000"  >&lt;p&gt;I would suggest to change that test to use a constant test data file. That would save us the brittleness of relying on an S3 upload in a bash script and would save us form eventual consistency visibility issues.&lt;/p&gt;</comment>
                            <comment id="16677998" author="zentol" created="Wed, 7 Nov 2018 10:27:59 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sewen&quot; class=&quot;user-hover&quot; rel=&quot;sewen&quot;&gt;sewen&lt;/a&gt; Are you suggesting to permanently upload the input file to S3?&lt;/p&gt;</comment>
                            <comment id="16678012" author="till.rohrmann" created="Wed, 7 Nov 2018 10:39:01 +0000"  >&lt;p&gt;Yes or to use a publicly available file stored in S3.&lt;/p&gt;</comment>
                            <comment id="16679421" author="zentol" created="Thu, 8 Nov 2018 08:22:08 +0000"  >&lt;p&gt;Would this be as simple as running the test once without deleting the uploaded file, and for subsequent runs do not do any upload/delete?&lt;/p&gt;</comment>
                            <comment id="16679719" author="till.rohrmann" created="Thu, 8 Nov 2018 12:57:56 +0000"  >&lt;p&gt;Yes, this should work if the bucket never gets cleared.&lt;/p&gt;</comment>
                            <comment id="16681562" author="githubbot" created="Fri, 9 Nov 2018 15:03:27 +0000"  >&lt;p&gt;azagrebin opened a new pull request #7077: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10736&quot; title=&quot;Shaded Hadoop S3A end-to-end test failed on Travis&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10736&quot;&gt;&lt;del&gt;FLINK-10736&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;E2E tests&amp;#93;&lt;/span&gt; Use already uploaded to s3 file in shaded s3 e2e tests&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/7077&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/7077&lt;/a&gt;&lt;/p&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;What is the purpose of the change&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;   Remove s3 put/delete from s3 shaded e2e tests and use already uploaded, never  #deleted&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Verifying this change&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;   run s3 shaded e2e tests&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Does this pull request potentially affect one of the following parts:&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Dependencies (does it add or upgrade a dependency): (no)&lt;/li&gt;
	&lt;li&gt;The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)&lt;/li&gt;
	&lt;li&gt;The serializers: (no)&lt;/li&gt;
	&lt;li&gt;The runtime per-record code paths (performance sensitive): (no)&lt;/li&gt;
	&lt;li&gt;Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)&lt;/li&gt;
	&lt;li&gt;The S3 file system connector: (no, just tests)&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Documentation&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Does this pull request introduce a new feature? (no)&lt;/li&gt;
	&lt;li&gt;If yes, how is the feature documented? (not applicable)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16683457" author="azagrebin" created="Mon, 12 Nov 2018 09:31:56 +0000"  >&lt;p&gt;I uploaded the words file as public s3 object in Flink testing:&lt;br/&gt;
&lt;a href=&quot;https://s3.eu-central-1.amazonaws.com/flink-e2e-tests/words&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://s3.eu-central-1.amazonaws.com/flink-e2e-tests/words&lt;/a&gt;&lt;br/&gt;
or&lt;br/&gt;
s3://flink-e2e-tests/words&lt;/p&gt;</comment>
                            <comment id="16686204" author="githubbot" created="Wed, 14 Nov 2018 08:36:46 +0000"  >&lt;p&gt;dawidwys commented on a change in pull request #7077: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10736&quot; title=&quot;Shaded Hadoop S3A end-to-end test failed on Travis&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10736&quot;&gt;&lt;del&gt;FLINK-10736&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;E2E tests&amp;#93;&lt;/span&gt; Use already uploaded to s3 file in shaded s3 e2e tests&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/7077#discussion_r233352507&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/7077#discussion_r233352507&lt;/a&gt;&lt;/p&gt;



&lt;p&gt; ##########&lt;br/&gt;
 File path: flink-end-to-end-tests/test-scripts/test_shaded_hadoop_s3a.sh&lt;br/&gt;
 ##########&lt;br/&gt;
 @@ -22,15 +22,11 @@&lt;br/&gt;
 source &quot;$(dirname &quot;$0&quot;)&quot;/common.sh&lt;br/&gt;
 source &quot;$(dirname &quot;$0&quot;)&quot;/common_s3.sh&lt;/p&gt;

&lt;p&gt;-s3_put $TEST_INFRA_DIR/test-data/words $ARTIFACTS_AWS_BUCKET flink-end-to-end-test-shaded-s3a&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;make sure we delete the file at the end&lt;br/&gt;
-function shaded_s3a_cleanup 
{
-  s3_delete $ARTIFACTS_AWS_BUCKET flink-end-to-end-test-shaded-s3a
-}
&lt;p&gt;-trap shaded_s3a_cleanup EXIT&lt;br/&gt;
-&lt;br/&gt;
 start_cluster&lt;/p&gt;&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+bucket=flink-e2e-tests&lt;/p&gt;

&lt;p&gt; Review comment:&lt;br/&gt;
   Could we move the definition of `bucket` to `common_s3.sh`? Also I think we should use uppercase here to follow the convention of constants like: `TEST_INFRA_DIR`, `ARTIFACTS_AWS_BUCKET` etc.&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16686263" author="githubbot" created="Wed, 14 Nov 2018 09:41:00 +0000"  >&lt;p&gt;zentol commented on a change in pull request #7077: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10736&quot; title=&quot;Shaded Hadoop S3A end-to-end test failed on Travis&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10736&quot;&gt;&lt;del&gt;FLINK-10736&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;E2E tests&amp;#93;&lt;/span&gt; Use already uploaded to s3 file in shaded s3 e2e tests&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/7077#discussion_r233374112&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/7077#discussion_r233374112&lt;/a&gt;&lt;/p&gt;



&lt;p&gt; ##########&lt;br/&gt;
 File path: flink-end-to-end-tests/test-scripts/test_shaded_presto_s3.sh&lt;br/&gt;
 ##########&lt;br/&gt;
 @@ -22,15 +22,11 @@&lt;br/&gt;
 source &quot;$(dirname &quot;$0&quot;)&quot;/common.sh&lt;br/&gt;
 source &quot;$(dirname &quot;$0&quot;)&quot;/common_s3.sh&lt;/p&gt;

&lt;p&gt;-s3_put $TEST_INFRA_DIR/test-data/words $ARTIFACTS_AWS_BUCKET flink-end-to-end-test-shaded-presto-s3&lt;/p&gt;

&lt;p&gt; Review comment:&lt;br/&gt;
   Could we still do the put (to guard against accidental deletes etc.) without affecting test stability?&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16686338" author="githubbot" created="Wed, 14 Nov 2018 10:40:38 +0000"  >&lt;p&gt;azagrebin commented on a change in pull request #7077: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10736&quot; title=&quot;Shaded Hadoop S3A end-to-end test failed on Travis&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10736&quot;&gt;&lt;del&gt;FLINK-10736&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;E2E tests&amp;#93;&lt;/span&gt; Use already uploaded to s3 file in shaded s3 e2e tests&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/7077#discussion_r233394753&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/7077#discussion_r233394753&lt;/a&gt;&lt;/p&gt;



&lt;p&gt; ##########&lt;br/&gt;
 File path: flink-end-to-end-tests/test-scripts/test_shaded_presto_s3.sh&lt;br/&gt;
 ##########&lt;br/&gt;
 @@ -22,15 +22,11 @@&lt;br/&gt;
 source &quot;$(dirname &quot;$0&quot;)&quot;/common.sh&lt;br/&gt;
 source &quot;$(dirname &quot;$0&quot;)&quot;/common_s3.sh&lt;/p&gt;

&lt;p&gt;-s3_put $TEST_INFRA_DIR/test-data/words $ARTIFACTS_AWS_BUCKET flink-end-to-end-test-shaded-presto-s3&lt;/p&gt;

&lt;p&gt; Review comment:&lt;br/&gt;
   I think this is good point. We can just remove trap with deletion of the file atm.&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16686339" author="githubbot" created="Wed, 14 Nov 2018 10:41:47 +0000"  >&lt;p&gt;azagrebin commented on a change in pull request #7077: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10736&quot; title=&quot;Shaded Hadoop S3A end-to-end test failed on Travis&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10736&quot;&gt;&lt;del&gt;FLINK-10736&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;E2E tests&amp;#93;&lt;/span&gt; Use already uploaded to s3 file in shaded s3 e2e tests&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/7077#discussion_r233394753&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/7077#discussion_r233394753&lt;/a&gt;&lt;/p&gt;



&lt;p&gt; ##########&lt;br/&gt;
 File path: flink-end-to-end-tests/test-scripts/test_shaded_presto_s3.sh&lt;br/&gt;
 ##########&lt;br/&gt;
 @@ -22,15 +22,11 @@&lt;br/&gt;
 source &quot;$(dirname &quot;$0&quot;)&quot;/common.sh&lt;br/&gt;
 source &quot;$(dirname &quot;$0&quot;)&quot;/common_s3.sh&lt;/p&gt;

&lt;p&gt;-s3_put $TEST_INFRA_DIR/test-data/words $ARTIFACTS_AWS_BUCKET flink-end-to-end-test-shaded-presto-s3&lt;/p&gt;

&lt;p&gt; Review comment:&lt;br/&gt;
   I think this is good point. We can just remove trap with deletion of the file atm. It should address the eventual consistency problem and then we will rethink the concept of static s3 content. I pushed a commit for that.&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16686340" author="githubbot" created="Wed, 14 Nov 2018 10:42:01 +0000"  >&lt;p&gt;azagrebin commented on a change in pull request #7077: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10736&quot; title=&quot;Shaded Hadoop S3A end-to-end test failed on Travis&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10736&quot;&gt;&lt;del&gt;FLINK-10736&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;E2E tests&amp;#93;&lt;/span&gt; Use already uploaded to s3 file in shaded s3 e2e tests&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/7077#discussion_r233394753&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/7077#discussion_r233394753&lt;/a&gt;&lt;/p&gt;



&lt;p&gt; ##########&lt;br/&gt;
 File path: flink-end-to-end-tests/test-scripts/test_shaded_presto_s3.sh&lt;br/&gt;
 ##########&lt;br/&gt;
 @@ -22,15 +22,11 @@&lt;br/&gt;
 source &quot;$(dirname &quot;$0&quot;)&quot;/common.sh&lt;br/&gt;
 source &quot;$(dirname &quot;$0&quot;)&quot;/common_s3.sh&lt;/p&gt;

&lt;p&gt;-s3_put $TEST_INFRA_DIR/test-data/words $ARTIFACTS_AWS_BUCKET flink-end-to-end-test-shaded-presto-s3&lt;/p&gt;

&lt;p&gt; Review comment:&lt;br/&gt;
   I think this is good point. We can just remove trap with deletion of the file atm. It should address the eventual consistency problem and then we will rethink the concept of static s3 content for e2e tests. I pushed a commit for that.&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16689958" author="stephanewen" created="Fri, 16 Nov 2018 20:17:03 +0000"  >&lt;p&gt;Fixed in&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;1.8.0 via aabbd6576c7608cf78f2da5b7c5170135ad2db54&lt;/li&gt;
	&lt;li&gt;1.7.0 via db9b5385d8cca9b839fce9247fd70d9ccafd99c8&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="16691395" author="githubbot" created="Mon, 19 Nov 2018 08:39:24 +0000"  >&lt;p&gt;StephanEwen commented on issue #7077: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10736&quot; title=&quot;Shaded Hadoop S3A end-to-end test failed on Travis&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10736&quot;&gt;&lt;del&gt;FLINK-10736&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;E2E tests&amp;#93;&lt;/span&gt; Switch to new IT_CASE_S3* and static/temp s3 content&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/7077#issuecomment-439812225&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/7077#issuecomment-439812225&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Fixed in aabbd6576c7608cf78f2da5b7c5170135ad2db54&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16691396" author="githubbot" created="Mon, 19 Nov 2018 08:39:24 +0000"  >&lt;p&gt;StephanEwen closed pull request #7077: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10736&quot; title=&quot;Shaded Hadoop S3A end-to-end test failed on Travis&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10736&quot;&gt;&lt;del&gt;FLINK-10736&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;E2E tests&amp;#93;&lt;/span&gt; Switch to new IT_CASE_S3* and static/temp s3 content&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/7077&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/7077&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/flink-end-to-end-tests/test-scripts/common_s3.sh b/flink-end-to-end-tests/test-scripts/common_s3.sh&lt;br/&gt;
index 5c16bb75bea..4a778f3ad6b 100644&lt;br/&gt;
&amp;#8212; a/flink-end-to-end-tests/test-scripts/common_s3.sh&lt;br/&gt;
+++ b/flink-end-to-end-tests/test-scripts/common_s3.sh&lt;br/&gt;
@@ -17,40 +17,43 @@&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;limitations under the License.&lt;br/&gt;
 ################################################################################&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;-if [[ -z &quot;$ARTIFACTS_AWS_BUCKET&quot; ]]; then&lt;br/&gt;
+if [[ -z &quot;$IT_CASE_S3_BUCKET&quot; ]]; then&lt;br/&gt;
     echo &quot;Did not find AWS environment variables, NOT running the e2e test.&quot;&lt;br/&gt;
     exit 0&lt;br/&gt;
 else&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;echo &quot;Found AWS bucket $ARTIFACTS_AWS_BUCKET, running the e2e test.&quot;&lt;br/&gt;
+    echo &quot;Found AWS bucket $IT_CASE_S3_BUCKET, running the e2e test.&quot;&lt;br/&gt;
 fi&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-if [[ -z &quot;$ARTIFACTS_AWS_ACCESS_KEY&quot; ]]; then&lt;br/&gt;
+if [[ -z &quot;$IT_CASE_S3_ACCESS_KEY&quot; ]]; then&lt;br/&gt;
     echo &quot;Did not find AWS environment variables, NOT running the e2e test.&quot;&lt;br/&gt;
     exit 0&lt;br/&gt;
 else&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;echo &quot;Found AWS access key $ARTIFACTS_AWS_ACCESS_KEY, running the e2e test.&quot;&lt;br/&gt;
+    echo &quot;Found AWS access key $IT_CASE_S3_ACCESS_KEY, running the e2e test.&quot;&lt;br/&gt;
 fi&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-if [[ -z &quot;$ARTIFACTS_AWS_SECRET_KEY&quot; ]]; then&lt;br/&gt;
+if [[ -z &quot;$IT_CASE_S3_SECRET_KEY&quot; ]]; then&lt;br/&gt;
     echo &quot;Did not find AWS environment variables, NOT running the e2e test.&quot;&lt;br/&gt;
     exit 0&lt;br/&gt;
 else&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;echo &quot;Found AWS secret key $ARTIFACTS_AWS_SECRET_KEY, running the e2e test.&quot;&lt;br/&gt;
+    echo &quot;Found AWS secret key $IT_CASE_S3_SECRET_KEY, running the e2e test.&quot;&lt;br/&gt;
 fi&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-AWS_REGION=&quot;${AWS_REGION:-eu-west-1}&quot;&lt;br/&gt;
-AWS_ACCESS_KEY=$ARTIFACTS_AWS_ACCESS_KEY&lt;br/&gt;
-AWS_SECRET_KEY=$ARTIFACTS_AWS_SECRET_KEY&lt;br/&gt;
+# config AWS client&lt;br/&gt;
+AWS_REGION=&quot;${IT_CASE_S3_REGION:-eu-east-1}&quot;&lt;br/&gt;
+AWS_ACCESS_KEY=$IT_CASE_S3_ACCESS_KEY&lt;br/&gt;
+AWS_SECRET_KEY=$IT_CASE_S3_SECRET_KEY&lt;/p&gt;

&lt;p&gt; s3util=&quot;java -jar ${END_TO_END_DIR}/flink-e2e-test-utils/target/S3UtilProgram.jar&quot;&lt;/p&gt;

&lt;p&gt;+SHADED_S3_INPUT=s3://$IT_CASE_S3_BUCKET/static/words&lt;br/&gt;
+&lt;br/&gt;
 ###################################&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Setup Flink s3 access.&lt;br/&gt;
 #&lt;/li&gt;
	&lt;li&gt;Globals:&lt;/li&gt;
	&lt;li&gt;FLINK_DIR
	&lt;ol&gt;
		&lt;li&gt;ARTIFACTS_AWS_ACCESS_KEY&lt;/li&gt;
		&lt;li&gt;ARTIFACTS_AWS_SECRET_KEY&lt;br/&gt;
+#   IT_CASE_S3_ACCESS_KEY&lt;br/&gt;
+#   IT_CASE_S3_SECRET_KEY&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
	&lt;li&gt;Arguments:&lt;/li&gt;
	&lt;li&gt;None&lt;/li&gt;
	&lt;li&gt;Returns:&lt;br/&gt;
@@ -68,8 +71,8 @@ function s3_setup 
{
   trap s3_cleanup EXIT
 
   cp $FLINK_DIR/opt/flink-s3-fs-hadoop-*.jar $FLINK_DIR/lib/
-  echo &quot;s3.access-key: $ARTIFACTS_AWS_ACCESS_KEY&quot; &amp;gt;&amp;gt; &quot;$FLINK_DIR/conf/flink-conf.yaml&quot;
-  echo &quot;s3.secret-key: $ARTIFACTS_AWS_SECRET_KEY&quot; &amp;gt;&amp;gt; &quot;$FLINK_DIR/conf/flink-conf.yaml&quot;
+  echo &quot;s3.access-key: $IT_CASE_S3_ACCESS_KEY&quot; &amp;gt;&amp;gt; &quot;$FLINK_DIR/conf/flink-conf.yaml&quot;
+  echo &quot;s3.secret-key: $IT_CASE_S3_SECRET_KEY&quot; &amp;gt;&amp;gt; &quot;$FLINK_DIR/conf/flink-conf.yaml&quot;
 }&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt; s3_setup&lt;br/&gt;
@@ -78,7 +81,7 @@ s3_setup&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;List s3 objects by full path prefix.&lt;br/&gt;
 #&lt;/li&gt;
	&lt;li&gt;Globals:
	&lt;ol&gt;
		&lt;li&gt;ARTIFACTS_AWS_BUCKET&lt;br/&gt;
+#   IT_CASE_S3_BUCKET&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
	&lt;li&gt;Arguments:&lt;/li&gt;
	&lt;li&gt;$1 - s3 full path key prefix&lt;/li&gt;
	&lt;li&gt;Returns:&lt;br/&gt;
@@ -86,14 +89,14 @@ s3_setup&lt;br/&gt;
 ###################################&lt;br/&gt;
 function s3_list {&lt;br/&gt;
   AWS_REGION=$AWS_REGION \&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;${s3util} --action listByFullPathPrefix --s3prefix &quot;$1&quot; --bucket $ARTIFACTS_AWS_BUCKET&lt;br/&gt;
+  ${s3util} --action listByFullPathPrefix --s3prefix &quot;$1&quot; --bucket $IT_CASE_S3_BUCKET&lt;br/&gt;
 }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; ###################################&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Download s3 object.&lt;br/&gt;
 #&lt;/li&gt;
	&lt;li&gt;Globals:
	&lt;ol&gt;
		&lt;li&gt;ARTIFACTS_AWS_BUCKET&lt;br/&gt;
+#   IT_CASE_S3_BUCKET&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
	&lt;li&gt;Arguments:&lt;/li&gt;
	&lt;li&gt;$1 - local path to save file&lt;/li&gt;
	&lt;li&gt;$2 - s3 object key&lt;br/&gt;
@@ -102,14 +105,14 @@ function s3_list {&lt;br/&gt;
 ###################################&lt;br/&gt;
 function s3_get {&lt;br/&gt;
   AWS_REGION=$AWS_REGION \&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;${s3util} --action downloadFile --localFile &quot;$1&quot; --s3file &quot;$2&quot; --bucket $ARTIFACTS_AWS_BUCKET&lt;br/&gt;
+  ${s3util} --action downloadFile --localFile &quot;$1&quot; --s3file &quot;$2&quot; --bucket $IT_CASE_S3_BUCKET&lt;br/&gt;
 }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; ###################################&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Download s3 objects to folder by full path prefix.&lt;br/&gt;
 #&lt;/li&gt;
	&lt;li&gt;Globals:
	&lt;ol&gt;
		&lt;li&gt;ARTIFACTS_AWS_BUCKET&lt;br/&gt;
+#   IT_CASE_S3_BUCKET&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
	&lt;li&gt;Arguments:&lt;/li&gt;
	&lt;li&gt;$1 - local path to save folder with files&lt;/li&gt;
	&lt;li&gt;$2 - s3 key full path prefix&lt;br/&gt;
@@ -121,14 +124,14 @@ function s3_get_by_full_path_and_filename_prefix {&lt;br/&gt;
   local file_prefix=&quot;${3-}&quot;&lt;br/&gt;
   AWS_REGION=$AWS_REGION \&lt;br/&gt;
   ${s3util} --action downloadByFullPathAndFileNamePrefix \&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;--localFolder &quot;$1&quot; --s3prefix &quot;$2&quot; --s3filePrefix &quot;${file_prefix}&quot; --bucket $ARTIFACTS_AWS_BUCKET&lt;br/&gt;
+    --localFolder &quot;$1&quot; --s3prefix &quot;$2&quot; --s3filePrefix &quot;${file_prefix}&quot; --bucket $IT_CASE_S3_BUCKET&lt;br/&gt;
 }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; ###################################&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Upload file to s3 object.&lt;br/&gt;
 #&lt;/li&gt;
	&lt;li&gt;Globals:
	&lt;ol&gt;
		&lt;li&gt;ARTIFACTS_AWS_BUCKET&lt;br/&gt;
+#   IT_CASE_S3_BUCKET&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
	&lt;li&gt;Arguments:&lt;/li&gt;
	&lt;li&gt;$1 - local file to upload&lt;/li&gt;
	&lt;li&gt;$2 - s3 bucket&lt;br/&gt;
@@ -144,8 +147,8 @@ function s3_put {&lt;br/&gt;
   contentType=&quot;application/octet-stream&quot;&lt;br/&gt;
   dateValue=`date -R`&lt;br/&gt;
   stringToSign=&quot;PUT\n\n${contentType}\n${dateValue}\n${resource}&quot;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;s3Key=$ARTIFACTS_AWS_ACCESS_KEY&lt;/li&gt;
	&lt;li&gt;s3Secret=$ARTIFACTS_AWS_SECRET_KEY&lt;br/&gt;
+  s3Key=$IT_CASE_S3_ACCESS_KEY&lt;br/&gt;
+  s3Secret=$IT_CASE_S3_SECRET_KEY&lt;br/&gt;
   signature=`echo -en ${stringToSign} | openssl sha1 -hmac ${s3Secret} -binary | base64`&lt;br/&gt;
   curl -X PUT -T &quot;${local_file}&quot; \&lt;br/&gt;
     -H &quot;Host: ${bucket}.s3.amazonaws.com&quot; \&lt;br/&gt;
@@ -174,8 +177,8 @@ function s3_delete {&lt;br/&gt;
   contentType=&quot;application/octet-stream&quot;&lt;br/&gt;
   dateValue=`date -R`&lt;br/&gt;
   stringToSign=&quot;DELETE\n\n${contentType}\n${dateValue}\n${resource}&quot;&lt;/li&gt;
	&lt;li&gt;s3Key=$ARTIFACTS_AWS_ACCESS_KEY&lt;/li&gt;
	&lt;li&gt;s3Secret=$ARTIFACTS_AWS_SECRET_KEY&lt;br/&gt;
+  s3Key=$IT_CASE_S3_ACCESS_KEY&lt;br/&gt;
+  s3Secret=$IT_CASE_S3_SECRET_KEY&lt;br/&gt;
   signature=`echo -en ${stringToSign} | openssl sha1 -hmac ${s3Secret} -binary | base64`&lt;br/&gt;
   curl -X DELETE \&lt;br/&gt;
     -H &quot;Host: ${bucket}.s3.amazonaws.com&quot; \&lt;br/&gt;
@@ -189,7 +192,7 @@ function s3_delete {&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
	&lt;li&gt;Delete s3 objects by full path prefix.&lt;br/&gt;
 #&lt;/li&gt;
	&lt;li&gt;Globals:
	&lt;ol&gt;
		&lt;li&gt;ARTIFACTS_AWS_BUCKET&lt;br/&gt;
+#   IT_CASE_S3_BUCKET&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
	&lt;li&gt;Arguments:&lt;/li&gt;
	&lt;li&gt;$1 - s3 key full path prefix&lt;/li&gt;
	&lt;li&gt;Returns:&lt;br/&gt;
@@ -197,7 +200,7 @@ function s3_delete {&lt;br/&gt;
 ###################################&lt;br/&gt;
 function s3_delete_by_full_path_prefix {&lt;br/&gt;
   AWS_REGION=$AWS_REGION \&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;${s3util} --action deleteByFullPathPrefix --s3prefix &quot;$1&quot; --bucket $ARTIFACTS_AWS_BUCKET&lt;br/&gt;
+  ${s3util} --action deleteByFullPathPrefix --s3prefix &quot;$1&quot; --bucket $IT_CASE_S3_BUCKET&lt;br/&gt;
 }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; ###################################&lt;br/&gt;
@@ -206,7 +209,7 @@ function s3_delete_by_full_path_prefix {&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;because SQL is used to query the s3 object.&lt;br/&gt;
 #&lt;/li&gt;
	&lt;li&gt;Globals:
	&lt;ol&gt;
		&lt;li&gt;ARTIFACTS_AWS_BUCKET&lt;br/&gt;
+#   IT_CASE_S3_BUCKET&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
	&lt;li&gt;Arguments:&lt;/li&gt;
	&lt;li&gt;$1 - s3 file object key&lt;/li&gt;
	&lt;li&gt;$2 - s3 bucket&lt;br/&gt;
@@ -215,7 +218,7 @@ function s3_delete_by_full_path_prefix {&lt;br/&gt;
 ###################################&lt;br/&gt;
 function s3_get_number_of_lines_in_file {&lt;br/&gt;
   AWS_REGION=$AWS_REGION \&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;${s3util} --action numberOfLinesInFile --s3file &quot;$1&quot; --bucket $ARTIFACTS_AWS_BUCKET&lt;br/&gt;
+  ${s3util} --action numberOfLinesInFile --s3file &quot;$1&quot; --bucket $IT_CASE_S3_BUCKET&lt;br/&gt;
 }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; ###################################&lt;br/&gt;
@@ -224,7 +227,7 @@ function s3_get_number_of_lines_in_file {&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;because SQL is used to query the s3 objects.&lt;br/&gt;
 #&lt;/li&gt;
	&lt;li&gt;Globals:
	&lt;ol&gt;
		&lt;li&gt;ARTIFACTS_AWS_BUCKET&lt;br/&gt;
+#   IT_CASE_S3_BUCKET&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
	&lt;li&gt;Arguments:&lt;/li&gt;
	&lt;li&gt;$1 - s3 key prefix&lt;/li&gt;
	&lt;li&gt;$2 - s3 bucket&lt;br/&gt;
@@ -236,5 +239,5 @@ function s3_get_number_of_lines_by_prefix {&lt;br/&gt;
   local file_prefix=&quot;${3-}&quot;&lt;br/&gt;
   AWS_REGION=$AWS_REGION \&lt;br/&gt;
   ${s3util} --action numberOfLinesInFilesWithFullAndNamePrefix \&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;--s3prefix &quot;$1&quot; --s3filePrefix &quot;${file_prefix}&quot; --bucket $ARTIFACTS_AWS_BUCKET&lt;br/&gt;
+    --s3prefix &quot;$1&quot; --s3filePrefix &quot;${file_prefix}&quot; --bucket $IT_CASE_S3_BUCKET&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/flink-end-to-end-tests/test-scripts/test_shaded_hadoop_s3a.sh b/flink-end-to-end-tests/test-scripts/test_shaded_hadoop_s3a.sh&lt;br/&gt;
index 3d838675852..43edb555b99 100755
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/flink-end-to-end-tests/test-scripts/test_shaded_hadoop_s3a.sh&lt;br/&gt;
+++ b/flink-end-to-end-tests/test-scripts/test_shaded_hadoop_s3a.sh&lt;br/&gt;
@@ -22,15 +22,8 @@&lt;br/&gt;
 source &quot;$(dirname &quot;$0&quot;)&quot;/common.sh&lt;br/&gt;
 source &quot;$(dirname &quot;$0&quot;)&quot;/common_s3.sh&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-s3_put $TEST_INFRA_DIR/test-data/words $ARTIFACTS_AWS_BUCKET flink-end-to-end-test-shaded-s3a&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;make sure we delete the file at the end&lt;br/&gt;
-function shaded_s3a_cleanup 
{
-  s3_delete $ARTIFACTS_AWS_BUCKET flink-end-to-end-test-shaded-s3a
-}
&lt;p&gt;-trap shaded_s3a_cleanup EXIT&lt;br/&gt;
-&lt;br/&gt;
 start_cluster&lt;/p&gt;&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-$FLINK_DIR/bin/flink run -p 1 $FLINK_DIR/examples/batch/WordCount.jar --input s3:/$resource --output $TEST_DATA_DIR/out/wc_out&lt;br/&gt;
+$FLINK_DIR/bin/flink run -p 1 $FLINK_DIR/examples/batch/WordCount.jar --input $SHADED_S3_INPUT --output $TEST_DATA_DIR/out/wc_out&lt;/p&gt;

&lt;p&gt; check_result_hash &quot;WordCountWithShadedS3A&quot; $TEST_DATA_DIR/out/wc_out &quot;72a690412be8928ba239c2da967328a5&quot;&lt;br/&gt;
diff --git a/flink-end-to-end-tests/test-scripts/test_shaded_presto_s3.sh b/flink-end-to-end-tests/test-scripts/test_shaded_presto_s3.sh&lt;br/&gt;
index bd33b410dfd..2a52bfd15b9 100755&lt;br/&gt;
&amp;#8212; a/flink-end-to-end-tests/test-scripts/test_shaded_presto_s3.sh&lt;br/&gt;
+++ b/flink-end-to-end-tests/test-scripts/test_shaded_presto_s3.sh&lt;br/&gt;
@@ -22,15 +22,8 @@&lt;br/&gt;
 source &quot;$(dirname &quot;$0&quot;)&quot;/common.sh&lt;br/&gt;
 source &quot;$(dirname &quot;$0&quot;)&quot;/common_s3.sh&lt;/p&gt;

&lt;p&gt;-s3_put $TEST_INFRA_DIR/test-data/words $ARTIFACTS_AWS_BUCKET flink-end-to-end-test-shaded-presto-s3&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;make sure we delete the file at the end&lt;br/&gt;
-function shaded_presto_s3_cleanup 
{
-  s3_delete $ARTIFACTS_AWS_BUCKET flink-end-to-end-test-shaded-presto-s3
-}
&lt;p&gt;-trap shaded_presto_s3_cleanup EXIT&lt;br/&gt;
-&lt;br/&gt;
 start_cluster&lt;/p&gt;&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-$FLINK_DIR/bin/flink run -p 1 $FLINK_DIR/examples/batch/WordCount.jar --input s3:/$resource --output $TEST_DATA_DIR/out/wc_out&lt;br/&gt;
+$FLINK_DIR/bin/flink run -p 1 $FLINK_DIR/examples/batch/WordCount.jar --input $SHADED_S3_INPUT --output $TEST_DATA_DIR/out/wc_out&lt;/p&gt;

&lt;p&gt; check_result_hash &quot;WordCountWithShadedPrestoS3&quot; $TEST_DATA_DIR/out/wc_out &quot;72a690412be8928ba239c2da967328a5&quot;&lt;br/&gt;
diff --git a/flink-end-to-end-tests/test-scripts/test_streaming_file_sink.sh b/flink-end-to-end-tests/test-scripts/test_streaming_file_sink.sh&lt;br/&gt;
index 6c8d0b85435..e810e68bde7 100755&lt;br/&gt;
&amp;#8212; a/flink-end-to-end-tests/test-scripts/test_streaming_file_sink.sh&lt;br/&gt;
+++ b/flink-end-to-end-tests/test-scripts/test_streaming_file_sink.sh&lt;br/&gt;
@@ -24,9 +24,9 @@ source &quot;$(dirname &quot;$0&quot;)&quot;/common_s3.sh&lt;/p&gt;

&lt;p&gt; set_conf_ssl &quot;mutual&quot;&lt;/p&gt;

&lt;p&gt;-OUT=out&lt;br/&gt;
+OUT=temp/test_streaming_file_sink-$(uuidgen)&lt;br/&gt;
 OUTPUT_PATH=&quot;$TEST_DATA_DIR/$OUT&quot;&lt;br/&gt;
-S3_OUTPUT_PATH=&quot;s3://$ARTIFACTS_AWS_BUCKET/$OUT&quot;&lt;br/&gt;
+S3_OUTPUT_PATH=&quot;s3://$IT_CASE_S3_BUCKET/$OUT&quot;&lt;/p&gt;

&lt;p&gt; mkdir -p $OUTPUT_PATH&lt;/p&gt;

&lt;p&gt;diff --git a/flink-filesystems/flink-s3-fs-hadoop/src/test/java/org/apache/flink/fs/s3hadoop/HadoopS3FileSystemBehaviorITCase.java b/flink-filesystems/flink-s3-fs-hadoop/src/test/java/org/apache/flink/fs/s3hadoop/HadoopS3FileSystemBehaviorITCase.java&lt;br/&gt;
index c8aaaeef74d..9dc5c6de1ef 100644&lt;br/&gt;
&amp;#8212; a/flink-filesystems/flink-s3-fs-hadoop/src/test/java/org/apache/flink/fs/s3hadoop/HadoopS3FileSystemBehaviorITCase.java&lt;br/&gt;
+++ b/flink-filesystems/flink-s3-fs-hadoop/src/test/java/org/apache/flink/fs/s3hadoop/HadoopS3FileSystemBehaviorITCase.java&lt;br/&gt;
@@ -36,12 +36,12 @@&lt;br/&gt;
  */&lt;br/&gt;
 public class HadoopS3FileSystemBehaviorITCase extends FileSystemBehaviorTestSuite {&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String BUCKET = System.getenv(&quot;ARTIFACTS_AWS_BUCKET&quot;);&lt;br/&gt;
+	private static final String BUCKET = System.getenv(&quot;IT_CASE_S3_BUCKET&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String TEST_DATA_DIR = &quot;tests-&quot; + UUID.randomUUID();&lt;br/&gt;
+	private static final String TEST_DATA_DIR = &quot;temp/tests-&quot; + UUID.randomUUID();&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String ACCESS_KEY = System.getenv(&quot;ARTIFACTS_AWS_ACCESS_KEY&quot;);&lt;/li&gt;
	&lt;li&gt;private static final String SECRET_KEY = System.getenv(&quot;ARTIFACTS_AWS_SECRET_KEY&quot;);&lt;br/&gt;
+	private static final String ACCESS_KEY = System.getenv(&quot;IT_CASE_S3_ACCESS_KEY&quot;);&lt;br/&gt;
+	private static final String SECRET_KEY = System.getenv(&quot;IT_CASE_S3_SECRET_KEY&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; 	@BeforeClass&lt;br/&gt;
 	public static void checkCredentialsAndSetup() throws IOException &lt;/p&gt;
{
diff --git a/flink-filesystems/flink-s3-fs-hadoop/src/test/java/org/apache/flink/fs/s3hadoop/HadoopS3FileSystemITCase.java b/flink-filesystems/flink-s3-fs-hadoop/src/test/java/org/apache/flink/fs/s3hadoop/HadoopS3FileSystemITCase.java
index 6dbdac511f4..c75dd79b274 100644
--- a/flink-filesystems/flink-s3-fs-hadoop/src/test/java/org/apache/flink/fs/s3hadoop/HadoopS3FileSystemITCase.java
+++ b/flink-filesystems/flink-s3-fs-hadoop/src/test/java/org/apache/flink/fs/s3hadoop/HadoopS3FileSystemITCase.java
@@ -67,11 +67,11 @@
 		return Arrays.asList(&quot;s3&quot;, &quot;s3a&quot;);
 	}

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String TEST_DATA_DIR = &quot;tests-&quot; + UUID.randomUUID();&lt;br/&gt;
+	private static final String TEST_DATA_DIR = &quot;temp/tests-&quot; + UUID.randomUUID();&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String BUCKET = System.getenv(&quot;ARTIFACTS_AWS_BUCKET&quot;);&lt;/li&gt;
	&lt;li&gt;private static final String ACCESS_KEY = System.getenv(&quot;ARTIFACTS_AWS_ACCESS_KEY&quot;);&lt;/li&gt;
	&lt;li&gt;private static final String SECRET_KEY = System.getenv(&quot;ARTIFACTS_AWS_SECRET_KEY&quot;);&lt;br/&gt;
+	private static final String BUCKET = System.getenv(&quot;IT_CASE_S3_BUCKET&quot;);&lt;br/&gt;
+	private static final String ACCESS_KEY = System.getenv(&quot;IT_CASE_S3_ACCESS_KEY&quot;);&lt;br/&gt;
+	private static final String SECRET_KEY = System.getenv(&quot;IT_CASE_S3_SECRET_KEY&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; 	/**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Will be updated by 
{@link #checkCredentialsAndSetup()}
&lt;p&gt; if the test is not skipped.&lt;br/&gt;
diff --git a/flink-filesystems/flink-s3-fs-hadoop/src/test/java/org/apache/flink/fs/s3hadoop/HadoopS3RecoverableWriterExceptionTest.java b/flink-filesystems/flink-s3-fs-hadoop/src/test/java/org/apache/flink/fs/s3hadoop/HadoopS3RecoverableWriterExceptionTest.java&lt;br/&gt;
index 634fa00344d..8883378c33f 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/flink-filesystems/flink-s3-fs-hadoop/src/test/java/org/apache/flink/fs/s3hadoop/HadoopS3RecoverableWriterExceptionTest.java&lt;br/&gt;
+++ b/flink-filesystems/flink-s3-fs-hadoop/src/test/java/org/apache/flink/fs/s3hadoop/HadoopS3RecoverableWriterExceptionTest.java&lt;br/&gt;
@@ -55,9 +55,9 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; 	// ----------------------- S3 general configuration -----------------------&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String ACCESS_KEY = System.getenv(&quot;ARTIFACTS_AWS_ACCESS_KEY&quot;);&lt;/li&gt;
	&lt;li&gt;private static final String SECRET_KEY = System.getenv(&quot;ARTIFACTS_AWS_SECRET_KEY&quot;);&lt;/li&gt;
	&lt;li&gt;private static final String BUCKET = System.getenv(&quot;ARTIFACTS_AWS_BUCKET&quot;);&lt;br/&gt;
+	private static final String ACCESS_KEY = System.getenv(&quot;IT_CASE_S3_ACCESS_KEY&quot;);&lt;br/&gt;
+	private static final String SECRET_KEY = System.getenv(&quot;IT_CASE_S3_SECRET_KEY&quot;);&lt;br/&gt;
+	private static final String BUCKET = System.getenv(&quot;IT_CASE_S3_BUCKET&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; 	private static final long PART_UPLOAD_MIN_SIZE_VALUE = 7L &amp;lt;&amp;lt; 20;&lt;br/&gt;
 	private static final int MAX_CONCURRENT_UPLOADS_VALUE = 2;&lt;br/&gt;
@@ -66,7 +66,7 @@&lt;/p&gt;

&lt;p&gt; 	private static final Random RND = new Random();&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String TEST_DATA_DIR = &quot;tests-&quot; + UUID.randomUUID();&lt;br/&gt;
+	private static final String TEST_DATA_DIR = &quot;temp/tests-&quot; + UUID.randomUUID();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; 	private static final Path basePath = new Path(&quot;s3://&quot; + BUCKET + &apos;/&apos; + TEST_DATA_DIR);&lt;/p&gt;

&lt;p&gt;diff --git a/flink-filesystems/flink-s3-fs-hadoop/src/test/java/org/apache/flink/fs/s3hadoop/HadoopS3RecoverableWriterTest.java b/flink-filesystems/flink-s3-fs-hadoop/src/test/java/org/apache/flink/fs/s3hadoop/HadoopS3RecoverableWriterTest.java&lt;br/&gt;
index 4a1368a815e..bbc02f34c21 100644&lt;br/&gt;
&amp;#8212; a/flink-filesystems/flink-s3-fs-hadoop/src/test/java/org/apache/flink/fs/s3hadoop/HadoopS3RecoverableWriterTest.java&lt;br/&gt;
+++ b/flink-filesystems/flink-s3-fs-hadoop/src/test/java/org/apache/flink/fs/s3hadoop/HadoopS3RecoverableWriterTest.java&lt;br/&gt;
@@ -63,9 +63,9 @@&lt;/p&gt;

&lt;p&gt; 	// ----------------------- S3 general configuration -----------------------&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String ACCESS_KEY = System.getenv(&quot;ARTIFACTS_AWS_ACCESS_KEY&quot;);&lt;/li&gt;
	&lt;li&gt;private static final String SECRET_KEY = System.getenv(&quot;ARTIFACTS_AWS_SECRET_KEY&quot;);&lt;/li&gt;
	&lt;li&gt;private static final String BUCKET = System.getenv(&quot;ARTIFACTS_AWS_BUCKET&quot;);&lt;br/&gt;
+	private static final String ACCESS_KEY = System.getenv(&quot;IT_CASE_S3_ACCESS_KEY&quot;);&lt;br/&gt;
+	private static final String SECRET_KEY = System.getenv(&quot;IT_CASE_S3_SECRET_KEY&quot;);&lt;br/&gt;
+	private static final String BUCKET = System.getenv(&quot;IT_CASE_S3_BUCKET&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; 	private static final long PART_UPLOAD_MIN_SIZE_VALUE = 7L &amp;lt;&amp;lt; 20;&lt;br/&gt;
 	private static final int MAX_CONCURRENT_UPLOADS_VALUE = 2;&lt;br/&gt;
@@ -74,7 +74,7 @@&lt;/p&gt;

&lt;p&gt; 	private static final Random RND = new Random();&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String TEST_DATA_DIR = &quot;tests-&quot; + UUID.randomUUID();&lt;br/&gt;
+	private static final String TEST_DATA_DIR = &quot;temp/tests-&quot; + UUID.randomUUID();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; 	private static final Path basePath = new Path(&quot;s3://&quot; + BUCKET + &apos;/&apos; + TEST_DATA_DIR);&lt;/p&gt;

&lt;p&gt;diff --git a/flink-filesystems/flink-s3-fs-presto/src/test/java/org/apache/flink/fs/s3presto/PrestoS3FileSystemBehaviorITCase.java b/flink-filesystems/flink-s3-fs-presto/src/test/java/org/apache/flink/fs/s3presto/PrestoS3FileSystemBehaviorITCase.java&lt;br/&gt;
index 812404ce639..e7c69b4523b 100644&lt;br/&gt;
&amp;#8212; a/flink-filesystems/flink-s3-fs-presto/src/test/java/org/apache/flink/fs/s3presto/PrestoS3FileSystemBehaviorITCase.java&lt;br/&gt;
+++ b/flink-filesystems/flink-s3-fs-presto/src/test/java/org/apache/flink/fs/s3presto/PrestoS3FileSystemBehaviorITCase.java&lt;br/&gt;
@@ -36,12 +36,12 @@&lt;br/&gt;
  */&lt;br/&gt;
 public class PrestoS3FileSystemBehaviorITCase extends FileSystemBehaviorTestSuite {&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String BUCKET = System.getenv(&quot;ARTIFACTS_AWS_BUCKET&quot;);&lt;br/&gt;
+	private static final String BUCKET = System.getenv(&quot;IT_CASE_S3_BUCKET&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String TEST_DATA_DIR = &quot;tests-&quot; + UUID.randomUUID();&lt;br/&gt;
+	private static final String TEST_DATA_DIR = &quot;temp/tests-&quot; + UUID.randomUUID();&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String ACCESS_KEY = System.getenv(&quot;ARTIFACTS_AWS_ACCESS_KEY&quot;);&lt;/li&gt;
	&lt;li&gt;private static final String SECRET_KEY = System.getenv(&quot;ARTIFACTS_AWS_SECRET_KEY&quot;);&lt;br/&gt;
+	private static final String ACCESS_KEY = System.getenv(&quot;IT_CASE_S3_ACCESS_KEY&quot;);&lt;br/&gt;
+	private static final String SECRET_KEY = System.getenv(&quot;IT_CASE_S3_SECRET_KEY&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; 	@BeforeClass&lt;br/&gt;
 	public static void checkCredentialsAndSetup() throws IOException &lt;/p&gt;
{
diff --git a/flink-filesystems/flink-s3-fs-presto/src/test/java/org/apache/flink/fs/s3presto/PrestoS3FileSystemITCase.java b/flink-filesystems/flink-s3-fs-presto/src/test/java/org/apache/flink/fs/s3presto/PrestoS3FileSystemITCase.java
index cc5c9935202..0ec693ebc8a 100644
--- a/flink-filesystems/flink-s3-fs-presto/src/test/java/org/apache/flink/fs/s3presto/PrestoS3FileSystemITCase.java
+++ b/flink-filesystems/flink-s3-fs-presto/src/test/java/org/apache/flink/fs/s3presto/PrestoS3FileSystemITCase.java
@@ -68,12 +68,12 @@
 		return Arrays.asList(&quot;s3&quot;, &quot;s3p&quot;);
 	}

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String BUCKET = System.getenv(&quot;ARTIFACTS_AWS_BUCKET&quot;);&lt;br/&gt;
+	private static final String BUCKET = System.getenv(&quot;IT_CASE_S3_BUCKET&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String TEST_DATA_DIR = &quot;tests-&quot; + UUID.randomUUID();&lt;br/&gt;
+	private static final String TEST_DATA_DIR = &quot;temp/tests-&quot; + UUID.randomUUID();&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String ACCESS_KEY = System.getenv(&quot;ARTIFACTS_AWS_ACCESS_KEY&quot;);&lt;/li&gt;
	&lt;li&gt;private static final String SECRET_KEY = System.getenv(&quot;ARTIFACTS_AWS_SECRET_KEY&quot;);&lt;br/&gt;
+	private static final String ACCESS_KEY = System.getenv(&quot;IT_CASE_S3_ACCESS_KEY&quot;);&lt;br/&gt;
+	private static final String SECRET_KEY = System.getenv(&quot;IT_CASE_S3_SECRET_KEY&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; 	@BeforeClass&lt;br/&gt;
 	public static void checkIfCredentialsArePresent() {&lt;br/&gt;
diff --git a/flink-filesystems/flink-s3-fs-presto/src/test/java/org/apache/flink/fs/s3presto/PrestoS3RecoverableWriterTest.java b/flink-filesystems/flink-s3-fs-presto/src/test/java/org/apache/flink/fs/s3presto/PrestoS3RecoverableWriterTest.java&lt;br/&gt;
index 580d957db23..1f37fc24c5b 100644&lt;br/&gt;
&amp;#8212; a/flink-filesystems/flink-s3-fs-presto/src/test/java/org/apache/flink/fs/s3presto/PrestoS3RecoverableWriterTest.java&lt;br/&gt;
+++ b/flink-filesystems/flink-s3-fs-presto/src/test/java/org/apache/flink/fs/s3presto/PrestoS3RecoverableWriterTest.java&lt;br/&gt;
@@ -42,16 +42,16 @@&lt;/p&gt;

&lt;p&gt; 	// ----------------------- S3 general configuration -----------------------&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String ACCESS_KEY = System.getenv(&quot;ARTIFACTS_AWS_ACCESS_KEY&quot;);&lt;/li&gt;
	&lt;li&gt;private static final String SECRET_KEY = System.getenv(&quot;ARTIFACTS_AWS_SECRET_KEY&quot;);&lt;/li&gt;
	&lt;li&gt;private static final String BUCKET = System.getenv(&quot;ARTIFACTS_AWS_BUCKET&quot;);&lt;br/&gt;
+	private static final String ACCESS_KEY = System.getenv(&quot;IT_CASE_S3_ACCESS_KEY&quot;);&lt;br/&gt;
+	private static final String SECRET_KEY = System.getenv(&quot;IT_CASE_S3_SECRET_KEY&quot;);&lt;br/&gt;
+	private static final String BUCKET = System.getenv(&quot;IT_CASE_S3_BUCKET&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; 	private static final long PART_UPLOAD_MIN_SIZE_VALUE = 7L &amp;lt;&amp;lt; 20;&lt;br/&gt;
 	private static final int MAX_CONCURRENT_UPLOADS_VALUE = 2;&lt;/p&gt;

&lt;p&gt; 	// ----------------------- Test Specific configuration -----------------------&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String TEST_DATA_DIR = &quot;tests-&quot; + UUID.randomUUID();&lt;br/&gt;
+	private static final String TEST_DATA_DIR = &quot;temp/tests-&quot; + UUID.randomUUID();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; 	private static final Path basePath = new Path(&quot;s3://&quot; + BUCKET + &apos;/&apos; + TEST_DATA_DIR);&lt;/p&gt;

&lt;p&gt;diff --git a/flink-yarn/src/test/java/org/apache/flink/yarn/YarnFileStageTestS3ITCase.java b/flink-yarn/src/test/java/org/apache/flink/yarn/YarnFileStageTestS3ITCase.java&lt;br/&gt;
index e1e95b1c379..f4e5227affb 100644&lt;br/&gt;
&amp;#8212; a/flink-yarn/src/test/java/org/apache/flink/yarn/YarnFileStageTestS3ITCase.java&lt;br/&gt;
+++ b/flink-yarn/src/test/java/org/apache/flink/yarn/YarnFileStageTestS3ITCase.java&lt;br/&gt;
@@ -53,12 +53,12 @@&lt;br/&gt;
  */&lt;br/&gt;
 public class YarnFileStageTestS3ITCase extends TestLogger {&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String BUCKET = System.getenv(&quot;ARTIFACTS_AWS_BUCKET&quot;);&lt;br/&gt;
+	private static final String BUCKET = System.getenv(&quot;IT_CASE_S3_BUCKET&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String TEST_DATA_DIR = &quot;tests-&quot; + UUID.randomUUID();&lt;br/&gt;
+	private static final String TEST_DATA_DIR = &quot;temp/tests-&quot; + UUID.randomUUID();&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String ACCESS_KEY = System.getenv(&quot;ARTIFACTS_AWS_ACCESS_KEY&quot;);&lt;/li&gt;
	&lt;li&gt;private static final String SECRET_KEY = System.getenv(&quot;ARTIFACTS_AWS_SECRET_KEY&quot;);&lt;br/&gt;
+	private static final String ACCESS_KEY = System.getenv(&quot;IT_CASE_S3_ACCESS_KEY&quot;);&lt;br/&gt;
+	private static final String SECRET_KEY = System.getenv(&quot;IT_CASE_S3_SECRET_KEY&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; 	@ClassRule&lt;br/&gt;
 	public static final TemporaryFolder TEMP_FOLDER = new TemporaryFolder();&lt;br/&gt;
diff --git a/tools/travis_mvn_watchdog.sh b/tools/travis_mvn_watchdog.sh&lt;br/&gt;
index 63c177258c4..55121158529 100755&lt;br/&gt;
&amp;#8212; a/tools/travis_mvn_watchdog.sh&lt;br/&gt;
+++ b/tools/travis_mvn_watchdog.sh&lt;br/&gt;
@@ -72,9 +72,9 @@ TRACE_OUT=&quot;${ARTIFACTS_DIR}/jps-traces.out&quot;&lt;br/&gt;
 UPLOAD_TARGET_PATH=&quot;travis-artifacts/${TRAVIS_REPO_SLUG}/${TRAVIS_BUILD_NUMBER}/&quot;&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;These variables are stored as secure variables in &apos;.travis.yml&apos;, which are generated per repo via&lt;/li&gt;
	&lt;li&gt;the travis command line tool.&lt;br/&gt;
-UPLOAD_BUCKET=$ARTIFACTS_AWS_BUCKET&lt;br/&gt;
-UPLOAD_ACCESS_KEY=$ARTIFACTS_AWS_ACCESS_KEY&lt;br/&gt;
-UPLOAD_SECRET_KEY=$ARTIFACTS_AWS_SECRET_KEY&lt;br/&gt;
+UPLOAD_BUCKET=$IT_CASE_S3_BUCKET&lt;br/&gt;
+UPLOAD_ACCESS_KEY=$IT_CASE_S3_ACCESS_KEY&lt;br/&gt;
+UPLOAD_SECRET_KEY=$IT_CASE_S3_SECRET_KEY&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt; ARTIFACTS_FILE=${TRAVIS_JOB_NUMBER}.tar.gz&lt;/p&gt;





&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3zugf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>