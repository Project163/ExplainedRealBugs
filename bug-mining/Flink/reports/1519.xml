<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 20:27:10 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-5122] Elasticsearch Sink loses documents when cluster has high load</title>
                <link>https://issues.apache.org/jira/browse/FLINK-5122</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;p&gt;My cluster had high load and documents got not indexed. This violates the &quot;at least once&quot; semantics in the ES connector.&lt;/p&gt;

&lt;p&gt;I gave pressure on my cluster to test Flink, causing new indices to be created and balanced. On those errors the bulk should be tried again instead of being discarded.&lt;/p&gt;

&lt;p&gt;Primary shard not active because ES decided to rebalance the index:&lt;br/&gt;
2016-11-15 15:35:16,123 ERROR org.apache.flink.streaming.connectors.elasticsearch2.ElasticsearchSink  - Failed to index document in Elasticsearch: UnavailableShardsException[&lt;span class=&quot;error&quot;&gt;&amp;#91;index-name&amp;#93;&lt;/span&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;3&amp;#93;&lt;/span&gt; primary shard is not active Timeout: &lt;span class=&quot;error&quot;&gt;&amp;#91;1m&amp;#93;&lt;/span&gt;, request: [BulkShardRequest to &lt;span class=&quot;error&quot;&gt;&amp;#91;index-name&amp;#93;&lt;/span&gt; containing &lt;span class=&quot;error&quot;&gt;&amp;#91;20&amp;#93;&lt;/span&gt; requests]]&lt;/p&gt;

&lt;p&gt;Bulk queue on node full (I set queue to a low value to reproduce error):&lt;br/&gt;
22:37:57,702 ERROR org.apache.flink.streaming.connectors.elasticsearch2.ElasticsearchSink  - Failed to index document in Elasticsearch: RemoteTransportException[&lt;span class=&quot;error&quot;&gt;&amp;#91;node1&amp;#93;&lt;/span&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;192.168.1.240:9300&amp;#93;&lt;/span&gt;[indices:data/write/bulk&lt;span class=&quot;error&quot;&gt;&amp;#91;s&amp;#93;&lt;/span&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;p&amp;#93;&lt;/span&gt;]]; nested: EsRejectedExecutionException[rejected execution of org.elasticsearch.transport.TransportService$4@727e677c on EsThreadPoolExecutor[bulk, queue capacity = 1, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@51322d37&lt;span class=&quot;error&quot;&gt;&amp;#91;Running, pool size = 2, active threads = 2, queued tasks = 1, completed tasks = 2939&amp;#93;&lt;/span&gt;]];&lt;/p&gt;

&lt;p&gt;I can try to propose a PR for this.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13022319">FLINK-5122</key>
            <summary>Elasticsearch Sink loses documents when cluster has high load</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="melmoth">static-max</assignee>
                                    <reporter username="melmoth">static-max</reporter>
                        <labels>
                    </labels>
                <created>Mon, 21 Nov 2016 21:47:05 +0000</created>
                <updated>Fri, 3 Mar 2017 09:13:54 +0000</updated>
                            <resolved>Fri, 24 Feb 2017 15:40:00 +0000</resolved>
                                    <version>1.2.0</version>
                                    <fixVersion>1.3.0</fixVersion>
                                    <component>Connectors / Common</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="15685987" author="fhueske" created="Tue, 22 Nov 2016 07:49:16 +0000"  >&lt;p&gt;Thanks for reporting this issue &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=melmoth&quot; class=&quot;user-hover&quot; rel=&quot;melmoth&quot;&gt;melmoth&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Would be great if you&apos;d provide a fix. I gave you contributor permissions for JIRA, so you can assign this issue to yourself is you want to work on it.&lt;br/&gt;
Thanks, Fabian&lt;/p&gt;</comment>
                            <comment id="15686075" author="f.pompermaier" created="Tue, 22 Nov 2016 08:35:36 +0000"  >&lt;p&gt;Can you try and see if our pull request fix your issue (&lt;a href=&quot;https://github.com/apache/flink/pull/2790/)?&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2790/)?&lt;/a&gt; We also had this problem of missing documents and the problem was the RunTimeException thrown by the close().&lt;/p&gt;</comment>
                            <comment id="15686338" author="melmoth" created="Tue, 22 Nov 2016 10:35:31 +0000"  >&lt;p&gt;Your PR does not handle single failed bulk requests, as far as I can see.&lt;br/&gt;
But your PR makes me think about the whole error handling concept in the ES sink. hasFailure will only be checked on close(), which does not make sense to me. Would it be smarter to throw the Exception immediately when a document cannot be indexed? This way Flink will restart the job from a checkpoint and try again.&lt;/p&gt;</comment>
                            <comment id="15686354" author="f.pompermaier" created="Tue, 22 Nov 2016 10:43:20 +0000"  >&lt;p&gt;Actually the current ES sink implementation check in afterBulk if any error occurred. If so, it keeps track of this and then in the close() it throws an Exception. It is right or wrong depending on the indexing policy you chose: if there&apos;s a bad document and you want all indexing to abort it is wrong, if for you it is ok, otherwise it is wrong. Probably it could be useful to retry to index the failed bulk if the occurred error is one of those indicating a cluster congestion (as in your use case), but I don&apos;t know whether this is possible or not&lt;/p&gt;</comment>
                            <comment id="15690712" author="githubbot" created="Wed, 23 Nov 2016 16:58:47 +0000"  >&lt;p&gt;GitHub user static-max opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2861&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2861&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-5122&quot; title=&quot;Elasticsearch Sink loses documents when cluster has high load&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-5122&quot;&gt;&lt;del&gt;FLINK-5122&lt;/del&gt;&lt;/a&gt; Index requests will be retried if the error is only temp&#8230;&lt;/p&gt;

&lt;p&gt;    This PR will re-add index requests to the BulkProcessor if the error is temporay, like&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Generel timeout errors&lt;/li&gt;
	&lt;li&gt;No master&lt;/li&gt;
	&lt;li&gt;UnavailableShardsException (Rebalancing, Node down)&lt;/li&gt;
	&lt;li&gt;Bulk queue on node full&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/static-max/flink&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/static-max/flink&lt;/a&gt; flink-connector-elasticsearch2-robust&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2861.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2861.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #2861&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 2ea8bd099100203d73af9b3a5e616e6d6d1cd50d&lt;br/&gt;
Author: Max Kuklinski &amp;lt;max.kuklinski@live.de&amp;gt;&lt;br/&gt;
Date:   2016-11-23T16:54:11Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-5122&quot; title=&quot;Elasticsearch Sink loses documents when cluster has high load&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-5122&quot;&gt;&lt;del&gt;FLINK-5122&lt;/del&gt;&lt;/a&gt; Index requests will be retried if the error is only temporary on Elasticsearch side. Covered are: Timeouts, No Master, UnavailableShardsException, bulk queue on node full&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="15712488" author="f.pompermaier" created="Thu, 1 Dec 2016 17:06:09 +0000"  >&lt;p&gt;Hi @static-max,&lt;br/&gt;
we&apos;ve closed our PR to add the possibility to configure the number of shards and replicas of an ES index (&lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-4491&quot; title=&quot;Handle index.number_of_shards in the ES connector&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-4491&quot;&gt;&lt;del&gt;FLINK-4491&lt;/del&gt;&lt;/a&gt;) moving out such an Helper class into a public repository (&lt;a href=&quot;https://github.com/okkam-it/flink-examples/blob/master/src/main/java/it/okkam/datalinks/batch/flink/elasticsearch/ElasticsearchHelper.java&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/okkam-it/flink-examples/blob/master/src/main/java/it/okkam/datalinks/batch/flink/elasticsearch/ElasticsearchHelper.java&lt;/a&gt;) to help anyone needing such a feature (as suggested by &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=fhueske&quot; class=&quot;user-hover&quot; rel=&quot;fhueske&quot;&gt;fhueske&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;However, since in that PR we&apos;ve suggested to remove the RuntimeException in the close() to not lose any bulk (and this is more related to this ticket) I suggest to check if that fix could be needed also in yout PR (and make this lenient/strict behaviour configurable as suggested by &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rmetzger&quot; class=&quot;user-hover&quot; rel=&quot;rmetzger&quot;&gt;rmetzger&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Best,&lt;br/&gt;
Flavio&lt;/p&gt;</comment>
                            <comment id="15722846" author="githubbot" created="Mon, 5 Dec 2016 17:47:23 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2861#discussion_r90916177&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2861#discussion_r90916177&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-elasticsearch2/src/main/java/org/apache/flink/streaming/connectors/elasticsearch2/ElasticsearchSink.java &amp;#8212;&lt;br/&gt;
    @@ -186,22 +191,44 @@ public void beforeBulk(long executionId, BulkRequest request) {&lt;/p&gt;

&lt;p&gt;     			@Override&lt;br/&gt;
     			public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {&lt;br/&gt;
    +				boolean allRequestsRepeatable = true;&lt;br/&gt;
     				if (response.hasFailures()) {&lt;br/&gt;
     					for (BulkItemResponse itemResp : response.getItems()) {&lt;br/&gt;
     						if (itemResp.isFailed()) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;LOG.error(&quot;Failed to index document in Elasticsearch: &quot; + itemResp.getFailureMessage());&lt;/li&gt;
	&lt;li&gt;failureThrowable.compareAndSet(null, new RuntimeException(itemResp.getFailureMessage()));&lt;br/&gt;
    +							// Check if index request can be retried&lt;br/&gt;
    +							String failureMessageLowercase = itemResp.getFailureMessage().toLowerCase();&lt;br/&gt;
    +							if (failureMessageLowercase.contains(&quot;timeout&quot;) || failureMessageLowercase.contains(&quot;timed out&quot;) // Generic timeout errors&lt;br/&gt;
    +									|| failureMessageLowercase.contains(&quot;UnavailableShardsException&quot;.toLowerCase()) // Shard not available due to rebalancing or node down&lt;br/&gt;
    +									|| (failureMessageLowercase.contains(&quot;data/write/bulk&quot;) &amp;amp;&amp;amp; failureMessageLowercase.contains(&quot;bulk&quot;)) // Bulk index queue on node full &lt;br/&gt;
    +								) {&lt;br/&gt;
    +								LOG.debug(&quot;Retry batch: &quot; + itemResp.getFailureMessage());
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    I would log here at a higher logging level.&lt;br/&gt;
    Also, could you not use string concatenation here and use the &quot;Retry batch: {}&quot;, itemResp.getFailureMessage()); pattern?&lt;/p&gt;</comment>
                            <comment id="15722845" author="githubbot" created="Mon, 5 Dec 2016 17:47:23 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2861#discussion_r90916759&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2861#discussion_r90916759&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-elasticsearch2/src/main/java/org/apache/flink/streaming/connectors/elasticsearch2/ElasticsearchSink.java &amp;#8212;&lt;br/&gt;
    @@ -186,22 +191,44 @@ public void beforeBulk(long executionId, BulkRequest request) {&lt;/p&gt;

&lt;p&gt;     			@Override&lt;br/&gt;
     			public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {&lt;br/&gt;
    +				boolean allRequestsRepeatable = true;&lt;br/&gt;
     				if (response.hasFailures()) {&lt;br/&gt;
     					for (BulkItemResponse itemResp : response.getItems()) {&lt;br/&gt;
     						if (itemResp.isFailed()) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;LOG.error(&quot;Failed to index document in Elasticsearch: &quot; + itemResp.getFailureMessage());&lt;/li&gt;
	&lt;li&gt;failureThrowable.compareAndSet(null, new RuntimeException(itemResp.getFailureMessage()));&lt;br/&gt;
    +							// Check if index request can be retried&lt;br/&gt;
    +							String failureMessageLowercase = itemResp.getFailureMessage().toLowerCase();&lt;br/&gt;
    +							if (failureMessageLowercase.contains(&quot;timeout&quot;) || failureMessageLowercase.contains(&quot;timed out&quot;) // Generic timeout errors
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    This string-based error matching seems to be a pretty unstable mechanism.&lt;br/&gt;
    Can you add a flag to control whether the mechanism is enabled, and disable it by default (but document it on the ES connector page)&lt;/p&gt;</comment>
                            <comment id="15722848" author="githubbot" created="Mon, 5 Dec 2016 17:47:23 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2861#discussion_r90916979&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2861#discussion_r90916979&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-elasticsearch2/src/main/java/org/apache/flink/streaming/connectors/elasticsearch2/ElasticsearchSink.java &amp;#8212;&lt;br/&gt;
    @@ -186,22 +191,44 @@ public void beforeBulk(long executionId, BulkRequest request) {&lt;/p&gt;

&lt;p&gt;     			@Override&lt;br/&gt;
     			public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {&lt;br/&gt;
    +				boolean allRequestsRepeatable = true;&lt;br/&gt;
     				if (response.hasFailures()) {&lt;br/&gt;
     					for (BulkItemResponse itemResp : response.getItems()) {&lt;br/&gt;
     						if (itemResp.isFailed()) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;LOG.error(&quot;Failed to index document in Elasticsearch: &quot; + itemResp.getFailureMessage());&lt;/li&gt;
	&lt;li&gt;failureThrowable.compareAndSet(null, new RuntimeException(itemResp.getFailureMessage()));&lt;br/&gt;
    +							// Check if index request can be retried&lt;br/&gt;
    +							String failureMessageLowercase = itemResp.getFailureMessage().toLowerCase();&lt;br/&gt;
    +							if (failureMessageLowercase.contains(&quot;timeout&quot;) || failureMessageLowercase.contains(&quot;timed out&quot;) // Generic timeout errors&lt;br/&gt;
    +									|| failureMessageLowercase.contains(&quot;UnavailableShardsException&quot;.toLowerCase()) // Shard not available due to rebalancing or node down&lt;br/&gt;
    +									|| (failureMessageLowercase.contains(&quot;data/write/bulk&quot;) &amp;amp;&amp;amp; failureMessageLowercase.contains(&quot;bulk&quot;)) // Bulk index queue on node full &lt;br/&gt;
    +								) 
{
    +								LOG.debug(&quot;Retry batch: &quot; + itemResp.getFailureMessage());
    +								reAddBulkRequest(request);
    +							}
&lt;p&gt; else { // Cannot retry action&lt;br/&gt;
    +								allRequestsRepeatable = false;&lt;br/&gt;
    +								LOG.error(&quot;Failed to index document in Elasticsearch: &quot; + itemResp.getFailureMessage());&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    {} instead of string concat.&lt;/p&gt;</comment>
                            <comment id="15722847" author="githubbot" created="Mon, 5 Dec 2016 17:47:23 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2861#discussion_r90917039&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2861#discussion_r90917039&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-elasticsearch2/src/main/java/org/apache/flink/streaming/connectors/elasticsearch2/ElasticsearchSink.java &amp;#8212;&lt;br/&gt;
    @@ -186,22 +191,44 @@ public void beforeBulk(long executionId, BulkRequest request) {&lt;/p&gt;

&lt;p&gt;     			@Override&lt;br/&gt;
     			public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {&lt;br/&gt;
    +				boolean allRequestsRepeatable = true;&lt;br/&gt;
     				if (response.hasFailures()) {&lt;br/&gt;
     					for (BulkItemResponse itemResp : response.getItems()) {&lt;br/&gt;
     						if (itemResp.isFailed()) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;LOG.error(&quot;Failed to index document in Elasticsearch: &quot; + itemResp.getFailureMessage());&lt;/li&gt;
	&lt;li&gt;failureThrowable.compareAndSet(null, new RuntimeException(itemResp.getFailureMessage()));&lt;br/&gt;
    +							// Check if index request can be retried&lt;br/&gt;
    +							String failureMessageLowercase = itemResp.getFailureMessage().toLowerCase();&lt;br/&gt;
    +							if (failureMessageLowercase.contains(&quot;timeout&quot;) || failureMessageLowercase.contains(&quot;timed out&quot;) // Generic timeout errors&lt;br/&gt;
    +									|| failureMessageLowercase.contains(&quot;UnavailableShardsException&quot;.toLowerCase()) // Shard not available due to rebalancing or node down&lt;br/&gt;
    +									|| (failureMessageLowercase.contains(&quot;data/write/bulk&quot;) &amp;amp;&amp;amp; failureMessageLowercase.contains(&quot;bulk&quot;)) // Bulk index queue on node full &lt;br/&gt;
    +								) 
{
    +								LOG.debug(&quot;Retry batch: &quot; + itemResp.getFailureMessage());
    +								reAddBulkRequest(request);
    +							}
&lt;p&gt; else &lt;/p&gt;
{ // Cannot retry action
    +								allRequestsRepeatable = false;
    +								LOG.error(&quot;Failed to index document in Elasticsearch: &quot; + itemResp.getFailureMessage());
    +								failureThrowable.compareAndSet(null, new RuntimeException(itemResp.getFailureMessage()));	
    +							}
&lt;p&gt;     						}&lt;br/&gt;
     					}&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;hasFailure.set(true);&lt;br/&gt;
    +					if (!allRequestsRepeatable) 
{
    +						hasFailure.set(true);
    +					}
&lt;p&gt;     				}&lt;br/&gt;
     			}&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     			@Override&lt;br/&gt;
     			public void afterBulk(long executionId, BulkRequest request, Throwable failure) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;LOG.error(failure.getMessage());&lt;/li&gt;
	&lt;li&gt;failureThrowable.compareAndSet(null, failure);&lt;/li&gt;
	&lt;li&gt;hasFailure.set(true);&lt;br/&gt;
    +				if (failure instanceof ClusterBlockException // Examples: &quot;no master&quot;&lt;br/&gt;
    +						|| failure instanceof ElasticsearchTimeoutException // ElasticsearchTimeoutException sounded good, not seen in stress tests yet&lt;br/&gt;
    +						) &lt;br/&gt;
    +				{&lt;br/&gt;
    +					LOG.debug(&quot;Retry batch on throwable: &quot; + failure.getMessage());
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    String concat&lt;/p&gt;</comment>
                            <comment id="15722849" author="githubbot" created="Mon, 5 Dec 2016 17:47:23 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2861#discussion_r90917049&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2861#discussion_r90917049&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-elasticsearch2/src/main/java/org/apache/flink/streaming/connectors/elasticsearch2/ElasticsearchSink.java &amp;#8212;&lt;br/&gt;
    @@ -186,22 +191,44 @@ public void beforeBulk(long executionId, BulkRequest request) {&lt;/p&gt;

&lt;p&gt;     			@Override&lt;br/&gt;
     			public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {&lt;br/&gt;
    +				boolean allRequestsRepeatable = true;&lt;br/&gt;
     				if (response.hasFailures()) {&lt;br/&gt;
     					for (BulkItemResponse itemResp : response.getItems()) {&lt;br/&gt;
     						if (itemResp.isFailed()) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;LOG.error(&quot;Failed to index document in Elasticsearch: &quot; + itemResp.getFailureMessage());&lt;/li&gt;
	&lt;li&gt;failureThrowable.compareAndSet(null, new RuntimeException(itemResp.getFailureMessage()));&lt;br/&gt;
    +							// Check if index request can be retried&lt;br/&gt;
    +							String failureMessageLowercase = itemResp.getFailureMessage().toLowerCase();&lt;br/&gt;
    +							if (failureMessageLowercase.contains(&quot;timeout&quot;) || failureMessageLowercase.contains(&quot;timed out&quot;) // Generic timeout errors&lt;br/&gt;
    +									|| failureMessageLowercase.contains(&quot;UnavailableShardsException&quot;.toLowerCase()) // Shard not available due to rebalancing or node down&lt;br/&gt;
    +									|| (failureMessageLowercase.contains(&quot;data/write/bulk&quot;) &amp;amp;&amp;amp; failureMessageLowercase.contains(&quot;bulk&quot;)) // Bulk index queue on node full &lt;br/&gt;
    +								) 
{
    +								LOG.debug(&quot;Retry batch: &quot; + itemResp.getFailureMessage());
    +								reAddBulkRequest(request);
    +							}
&lt;p&gt; else &lt;/p&gt;
{ // Cannot retry action
    +								allRequestsRepeatable = false;
    +								LOG.error(&quot;Failed to index document in Elasticsearch: &quot; + itemResp.getFailureMessage());
    +								failureThrowable.compareAndSet(null, new RuntimeException(itemResp.getFailureMessage()));	
    +							}
&lt;p&gt;     						}&lt;br/&gt;
     					}&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;hasFailure.set(true);&lt;br/&gt;
    +					if (!allRequestsRepeatable) 
{
    +						hasFailure.set(true);
    +					}
&lt;p&gt;     				}&lt;br/&gt;
     			}&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     			@Override&lt;br/&gt;
     			public void afterBulk(long executionId, BulkRequest request, Throwable failure) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;LOG.error(failure.getMessage());&lt;/li&gt;
	&lt;li&gt;failureThrowable.compareAndSet(null, failure);&lt;/li&gt;
	&lt;li&gt;hasFailure.set(true);&lt;br/&gt;
    +				if (failure instanceof ClusterBlockException // Examples: &quot;no master&quot;&lt;br/&gt;
    +						|| failure instanceof ElasticsearchTimeoutException // ElasticsearchTimeoutException sounded good, not seen in stress tests yet&lt;br/&gt;
    +						) &lt;br/&gt;
    +				
{
    +					LOG.debug(&quot;Retry batch on throwable: &quot; + failure.getMessage());
    +					reAddBulkRequest(request);
    +				}
&lt;p&gt; else { &lt;br/&gt;
    +					LOG.error(&quot;Failed to index bulk in Elasticsearch. &quot; + failure.getMessage());&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    String concat&lt;/p&gt;</comment>
                            <comment id="15722850" author="githubbot" created="Mon, 5 Dec 2016 17:47:23 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2861#discussion_r90917303&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2861#discussion_r90917303&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-elasticsearch2/src/main/java/org/apache/flink/streaming/connectors/elasticsearch2/ElasticsearchSink.java &amp;#8212;&lt;br/&gt;
    @@ -227,6 +254,21 @@ public void afterBulk(long executionId, BulkRequest request, Throwable failure)&lt;br/&gt;
     		requestIndexer = new BulkProcessorIndexer(bulkProcessor);&lt;br/&gt;
     	}&lt;/p&gt;

&lt;p&gt;    +	/**&lt;br/&gt;
    +	 * Adds all requests of the bulk to the BulkProcessor. Used when trying again.&lt;br/&gt;
    +	 * @param bulkRequest&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public void reAddBulkRequest(BulkRequest bulkRequest) {&lt;br/&gt;
    +		//TODO Check what happens when bulk contains a DeleteAction and IndexActions and the DeleteAction fails because the document already has been deleted. This may not happen in typical Flink jobs.&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    So what about this TODO? Can we somehow filter these requests?&lt;/p&gt;</comment>
                            <comment id="15742225" author="githubbot" created="Mon, 12 Dec 2016 15:06:50 +0000"  >&lt;p&gt;Github user static-max commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2861#discussion_r91963737&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2861#discussion_r91963737&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-elasticsearch2/src/main/java/org/apache/flink/streaming/connectors/elasticsearch2/ElasticsearchSink.java &amp;#8212;&lt;br/&gt;
    @@ -186,22 +191,44 @@ public void beforeBulk(long executionId, BulkRequest request) {&lt;/p&gt;

&lt;p&gt;     			@Override&lt;br/&gt;
     			public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {&lt;br/&gt;
    +				boolean allRequestsRepeatable = true;&lt;br/&gt;
     				if (response.hasFailures()) {&lt;br/&gt;
     					for (BulkItemResponse itemResp : response.getItems()) {&lt;br/&gt;
     						if (itemResp.isFailed()) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;LOG.error(&quot;Failed to index document in Elasticsearch: &quot; + itemResp.getFailureMessage());&lt;/li&gt;
	&lt;li&gt;failureThrowable.compareAndSet(null, new RuntimeException(itemResp.getFailureMessage()));&lt;br/&gt;
    +							// Check if index request can be retried&lt;br/&gt;
    +							String failureMessageLowercase = itemResp.getFailureMessage().toLowerCase();&lt;br/&gt;
    +							if (failureMessageLowercase.contains(&quot;timeout&quot;) || failureMessageLowercase.contains(&quot;timed out&quot;) // Generic timeout errors
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    I didn&apos;t find an alternative to check strings. I will add a flag an disable it by default.&lt;/p&gt;</comment>
                            <comment id="15742228" author="githubbot" created="Mon, 12 Dec 2016 15:09:00 +0000"  >&lt;p&gt;Github user static-max commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2861#discussion_r91964180&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2861#discussion_r91964180&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-elasticsearch2/src/main/java/org/apache/flink/streaming/connectors/elasticsearch2/ElasticsearchSink.java &amp;#8212;&lt;br/&gt;
    @@ -186,22 +191,44 @@ public void beforeBulk(long executionId, BulkRequest request) {&lt;/p&gt;

&lt;p&gt;     			@Override&lt;br/&gt;
     			public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {&lt;br/&gt;
    +				boolean allRequestsRepeatable = true;&lt;br/&gt;
     				if (response.hasFailures()) {&lt;br/&gt;
     					for (BulkItemResponse itemResp : response.getItems()) {&lt;br/&gt;
     						if (itemResp.isFailed()) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;LOG.error(&quot;Failed to index document in Elasticsearch: &quot; + itemResp.getFailureMessage());&lt;/li&gt;
	&lt;li&gt;failureThrowable.compareAndSet(null, new RuntimeException(itemResp.getFailureMessage()));&lt;br/&gt;
    +							// Check if index request can be retried&lt;br/&gt;
    +							String failureMessageLowercase = itemResp.getFailureMessage().toLowerCase();&lt;br/&gt;
    +							if (failureMessageLowercase.contains(&quot;timeout&quot;) || failureMessageLowercase.contains(&quot;timed out&quot;) // Generic timeout errors&lt;br/&gt;
    +									|| failureMessageLowercase.contains(&quot;UnavailableShardsException&quot;.toLowerCase()) // Shard not available due to rebalancing or node down&lt;br/&gt;
    +									|| (failureMessageLowercase.contains(&quot;data/write/bulk&quot;) &amp;amp;&amp;amp; failureMessageLowercase.contains(&quot;bulk&quot;)) // Bulk index queue on node full &lt;br/&gt;
    +								) {&lt;br/&gt;
    +								LOG.debug(&quot;Retry batch: &quot; + itemResp.getFailureMessage());
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    What level do you suggest? Personally I don&apos;t care for retried batches as long as the data gets into my ES cluster. When logging this as INFO or WARN, the logfile will get pretty messy on a cluster with high traffic.&lt;/p&gt;</comment>
                            <comment id="15742469" author="githubbot" created="Mon, 12 Dec 2016 17:06:40 +0000"  >&lt;p&gt;Github user static-max commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2861#discussion_r91991599&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2861#discussion_r91991599&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-elasticsearch2/src/main/java/org/apache/flink/streaming/connectors/elasticsearch2/ElasticsearchSink.java &amp;#8212;&lt;br/&gt;
    @@ -227,6 +254,21 @@ public void afterBulk(long executionId, BulkRequest request, Throwable failure)&lt;br/&gt;
     		requestIndexer = new BulkProcessorIndexer(bulkProcessor);&lt;br/&gt;
     	}&lt;/p&gt;

&lt;p&gt;    +	/**&lt;br/&gt;
    +	 * Adds all requests of the bulk to the BulkProcessor. Used when trying again.&lt;br/&gt;
    +	 * @param bulkRequest&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public void reAddBulkRequest(BulkRequest bulkRequest) {&lt;br/&gt;
    +		//TODO Check what happens when bulk contains a DeleteAction and IndexActions and the DeleteAction fails because the document already has been deleted. This may not happen in typical Flink jobs.&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Currently I&apos;m not aware of a way to filter these requests.&lt;/p&gt;</comment>
                            <comment id="15761291" author="githubbot" created="Mon, 19 Dec 2016 14:29:00 +0000"  >&lt;p&gt;Github user rmetzger commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2861&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2861&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I checked the elasticsearch documentation and some user forum from ES, and indeed it seems that they do not include any retry logic into their clients.&lt;/p&gt;</comment>
                            <comment id="15761324" author="githubbot" created="Mon, 19 Dec 2016 14:43:34 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2861#discussion_r93043759&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2861#discussion_r93043759&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-elasticsearch2/src/main/java/org/apache/flink/streaming/connectors/elasticsearch2/ElasticsearchSink.java &amp;#8212;&lt;br/&gt;
    @@ -186,22 +191,44 @@ public void beforeBulk(long executionId, BulkRequest request) {&lt;/p&gt;

&lt;p&gt;     			@Override&lt;br/&gt;
     			public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {&lt;br/&gt;
    +				boolean allRequestsRepeatable = true;&lt;br/&gt;
     				if (response.hasFailures()) {&lt;br/&gt;
     					for (BulkItemResponse itemResp : response.getItems()) {&lt;br/&gt;
     						if (itemResp.isFailed()) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;LOG.error(&quot;Failed to index document in Elasticsearch: &quot; + itemResp.getFailureMessage());&lt;/li&gt;
	&lt;li&gt;failureThrowable.compareAndSet(null, new RuntimeException(itemResp.getFailureMessage()));&lt;br/&gt;
    +							// Check if index request can be retried&lt;br/&gt;
    +							String failureMessageLowercase = itemResp.getFailureMessage().toLowerCase();&lt;br/&gt;
    +							if (failureMessageLowercase.contains(&quot;timeout&quot;) || failureMessageLowercase.contains(&quot;timed out&quot;) // Generic timeout errors
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Okay. I agree that there seems to be no better way to handle this.&lt;/p&gt;</comment>
                            <comment id="15761341" author="githubbot" created="Mon, 19 Dec 2016 14:53:50 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2861#discussion_r93045771&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2861#discussion_r93045771&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-elasticsearch2/src/main/java/org/apache/flink/streaming/connectors/elasticsearch2/ElasticsearchSink.java &amp;#8212;&lt;br/&gt;
    @@ -186,22 +191,44 @@ public void beforeBulk(long executionId, BulkRequest request) {&lt;/p&gt;

&lt;p&gt;     			@Override&lt;br/&gt;
     			public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {&lt;br/&gt;
    +				boolean allRequestsRepeatable = true;&lt;br/&gt;
     				if (response.hasFailures()) {&lt;br/&gt;
     					for (BulkItemResponse itemResp : response.getItems()) {&lt;br/&gt;
     						if (itemResp.isFailed()) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;LOG.error(&quot;Failed to index document in Elasticsearch: &quot; + itemResp.getFailureMessage());&lt;/li&gt;
	&lt;li&gt;failureThrowable.compareAndSet(null, new RuntimeException(itemResp.getFailureMessage()));&lt;br/&gt;
    +							// Check if index request can be retried&lt;br/&gt;
    +							String failureMessageLowercase = itemResp.getFailureMessage().toLowerCase();&lt;br/&gt;
    +							if (failureMessageLowercase.contains(&quot;timeout&quot;) || failureMessageLowercase.contains(&quot;timed out&quot;) // Generic timeout errors&lt;br/&gt;
    +									|| failureMessageLowercase.contains(&quot;UnavailableShardsException&quot;.toLowerCase()) // Shard not available due to rebalancing or node down&lt;br/&gt;
    +									|| (failureMessageLowercase.contains(&quot;data/write/bulk&quot;) &amp;amp;&amp;amp; failureMessageLowercase.contains(&quot;bulk&quot;)) // Bulk index queue on node full &lt;br/&gt;
    +								) {&lt;br/&gt;
    +								LOG.debug(&quot;Retry batch: &quot; + itemResp.getFailureMessage());
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    I can not assess how often retries are needed.&lt;br/&gt;
    Users can also manually increase the log level if needed. So we can leave it as is.&lt;br/&gt;
    However, I&apos;m wondering whether we want to include a metric that counts the number of retries that occurred.&lt;/p&gt;</comment>
                            <comment id="15761400" author="githubbot" created="Mon, 19 Dec 2016 15:07:37 +0000"  >&lt;p&gt;Github user rmetzger commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2861&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2861&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @fpompermaier Why is throwing exceptions in close causing document losses? As far as I can see ES is flushing all outstanding batches on close().&lt;/p&gt;</comment>
                            <comment id="15761427" author="githubbot" created="Mon, 19 Dec 2016 15:17:38 +0000"  >&lt;p&gt;Github user fpompermaier commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2861&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2861&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @rmetzger Are you sure that Logstash doesn&apos;t perform any retry logic..? For example &lt;a href=&quot;https://logstash.jira.com/browse/LOGSTASH-720&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://logstash.jira.com/browse/LOGSTASH-720&lt;/a&gt; seems to implement that..&lt;br/&gt;
    You can test document loss in the close()  method trying to index a malformed document...if you have 10 M documents and one of those is malformed is very probable that a lot of them will not be indexed (I opened a ticket only for that &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-5353&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/FLINK-5353&lt;/a&gt;)&lt;/p&gt;</comment>
                            <comment id="15761429" author="githubbot" created="Mon, 19 Dec 2016 15:18:21 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2861#discussion_r93049169&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2861#discussion_r93049169&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-elasticsearch2/src/main/java/org/apache/flink/streaming/connectors/elasticsearch2/ElasticsearchSink.java &amp;#8212;&lt;br/&gt;
    @@ -227,6 +264,37 @@ public void afterBulk(long executionId, BulkRequest request, Throwable failure)&lt;br/&gt;
     		requestIndexer = new BulkProcessorIndexer(bulkProcessor);&lt;br/&gt;
     	}&lt;/p&gt;

&lt;p&gt;    +	/**&lt;br/&gt;
    +	 * Adds all requests of the bulk to the BulkProcessor. Used when trying again.&lt;br/&gt;
    +	 * @param bulkRequest&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public void reAddBulkRequest(BulkRequest bulkRequest) {&lt;br/&gt;
    +		//TODO Check what happens when bulk contains a DeleteAction and IndexActions and the DeleteAction fails because the document already has been deleted. This may not happen in typical Flink jobs.&lt;br/&gt;
    +&lt;br/&gt;
    +		for (IndicesRequest req : bulkRequest.subRequests()) {&lt;br/&gt;
    +			if (req instanceof ActionRequest) {&lt;br/&gt;
    +				// There is no waiting time between index requests, so this may produce additional pressure on cluster&lt;br/&gt;
    +				bulkProcessor.add((ActionRequest&amp;lt;?&amp;gt;) req);&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Do you know if the BulkProcessor is thread safe? I assume multiple threads will add bulks concurrently (because of the calls from the callbacks)&lt;/p&gt;</comment>
                            <comment id="15761430" author="githubbot" created="Mon, 19 Dec 2016 15:18:21 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2861#discussion_r93048695&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2861#discussion_r93048695&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-elasticsearch2/src/main/java/org/apache/flink/streaming/connectors/elasticsearch2/ElasticsearchSink.java &amp;#8212;&lt;br/&gt;
    @@ -186,22 +198,47 @@ public void beforeBulk(long executionId, BulkRequest request) {&lt;/p&gt;

&lt;p&gt;     			@Override&lt;br/&gt;
     			public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {&lt;br/&gt;
    +				boolean allRequestsRepeatable = true;&lt;br/&gt;
     				if (response.hasFailures()) {&lt;br/&gt;
     					for (BulkItemResponse itemResp : response.getItems()) {&lt;br/&gt;
     						if (itemResp.isFailed()) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;LOG.error(&quot;Failed to index document in Elasticsearch: &quot; + itemResp.getFailureMessage());&lt;/li&gt;
	&lt;li&gt;failureThrowable.compareAndSet(null, new RuntimeException(itemResp.getFailureMessage()));&lt;br/&gt;
    +							// Check if index request can be retried&lt;br/&gt;
    +							String failureMessageLowercase = itemResp.getFailureMessage().toLowerCase();&lt;br/&gt;
    +							if (checkErrorAndRetryBulk &amp;amp;&amp;amp; (&lt;br/&gt;
    +									failureMessageLowercase.contains(&quot;timeout&quot;) || failureMessageLowercase.contains(&quot;timed out&quot;) // Generic timeout errors&lt;br/&gt;
    +									|| failureMessageLowercase.contains(&quot;UnavailableShardsException&quot;.toLowerCase()) // Shard not available due to rebalancing or node down&lt;br/&gt;
    +									|| (failureMessageLowercase.contains(&quot;data/write/bulk&quot;) &amp;amp;&amp;amp; failureMessageLowercase.contains(&quot;bulk&quot;)) // Bulk index queue on node full&lt;br/&gt;
    +									)&lt;br/&gt;
    +								) {&lt;br/&gt;
    +								LOG.debug(&quot;Retry bulk: {}&quot;, itemResp.getFailureMessage());&lt;br/&gt;
    +								reAddBulkRequest(request);
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Are you sure that the `BulkRequest` is only added once even if it contains multiple failed `BulkItemResponse`s?&lt;/p&gt;</comment>
                            <comment id="15761439" author="githubbot" created="Mon, 19 Dec 2016 15:21:10 +0000"  >&lt;p&gt;Github user rmetzger commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2861&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2861&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @fpompermaier I guess logstash is just a client to ES that implement its own retry logic (similar to Flink).&lt;br/&gt;
    I&apos;ll check out the JIRA.&lt;/p&gt;</comment>
                            <comment id="15761480" author="githubbot" created="Mon, 19 Dec 2016 15:39:13 +0000"  >&lt;p&gt;Github user fpompermaier commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2861&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2861&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Indeed my goal is to replace completely Logstash with Flink becuase Flink is much faster (Logstash is the official Elasticsearch indexer). On a 10 M docs Flink takes 10 min while Logstash 30 min (on our preliminary tests)&lt;/p&gt;</comment>
                            <comment id="15794694" author="githubbot" created="Tue, 3 Jan 2017 10:16:12 +0000"  >&lt;p&gt;Github user static-max commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2861#discussion_r94382504&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2861#discussion_r94382504&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-elasticsearch2/src/main/java/org/apache/flink/streaming/connectors/elasticsearch2/ElasticsearchSink.java &amp;#8212;&lt;br/&gt;
    @@ -186,22 +198,47 @@ public void beforeBulk(long executionId, BulkRequest request) {&lt;/p&gt;

&lt;p&gt;     			@Override&lt;br/&gt;
     			public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {&lt;br/&gt;
    +				boolean allRequestsRepeatable = true;&lt;br/&gt;
     				if (response.hasFailures()) {&lt;br/&gt;
     					for (BulkItemResponse itemResp : response.getItems()) {&lt;br/&gt;
     						if (itemResp.isFailed()) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;LOG.error(&quot;Failed to index document in Elasticsearch: &quot; + itemResp.getFailureMessage());&lt;/li&gt;
	&lt;li&gt;failureThrowable.compareAndSet(null, new RuntimeException(itemResp.getFailureMessage()));&lt;br/&gt;
    +							// Check if index request can be retried&lt;br/&gt;
    +							String failureMessageLowercase = itemResp.getFailureMessage().toLowerCase();&lt;br/&gt;
    +							if (checkErrorAndRetryBulk &amp;amp;&amp;amp; (&lt;br/&gt;
    +									failureMessageLowercase.contains(&quot;timeout&quot;) || failureMessageLowercase.contains(&quot;timed out&quot;) // Generic timeout errors&lt;br/&gt;
    +									|| failureMessageLowercase.contains(&quot;UnavailableShardsException&quot;.toLowerCase()) // Shard not available due to rebalancing or node down&lt;br/&gt;
    +									|| (failureMessageLowercase.contains(&quot;data/write/bulk&quot;) &amp;amp;&amp;amp; failureMessageLowercase.contains(&quot;bulk&quot;)) // Bulk index queue on node full&lt;br/&gt;
    +									)&lt;br/&gt;
    +								) {&lt;br/&gt;
    +								LOG.debug(&quot;Retry bulk: {}&quot;, itemResp.getFailureMessage());&lt;br/&gt;
    +								reAddBulkRequest(request);
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Your&apos;re right, it gets added multiple times, I&apos;ll fix that.&lt;/p&gt;</comment>
                            <comment id="15794823" author="githubbot" created="Tue, 3 Jan 2017 11:19:30 +0000"  >&lt;p&gt;Github user static-max commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2861&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2861&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    The path of the connector has changed to &quot;flink/flink-connectors/flink-connector-elasticsearch2/&quot;, how should I handle the conflict? Open a new PR?&lt;/p&gt;</comment>
                            <comment id="15814010" author="githubbot" created="Tue, 10 Jan 2017 06:00:32 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2861&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2861&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Hi @static-max, thank you for working on this, it&apos;ll be an important fix for proper at least once support for the ES connector.&lt;/p&gt;

&lt;p&gt;    Recently, the community has agreed to first restructure the multiple ES connector version, so that important fixes like this one can be done once and for all across all versions (1.x, 2.x, and 5.x which is currently pending). Here&apos;s the discussion: &lt;a href=&quot;http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-ElasticSearch-in-Flink-Strategy-td15049.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-ElasticSearch-in-Flink-Strategy-td15049.html&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;    Could we wait just a little a bit on this PR, and once the ES connector refactoring is complete, we can come back and rebase this PR on that? You can follow the progress here: #2767. I&apos;m trying to come up with the restructure PR within the next day.&lt;br/&gt;
    Very sorry for the extra wait needed on this, but it&apos;ll be good for the long run, hope you can understand &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15825630" author="githubbot" created="Tue, 17 Jan 2017 08:23:09 +0000"  >&lt;p&gt;Github user static-max commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2861&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2861&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Ok, let&apos;s wait for the restructure and rebase this PR to support at least once.&lt;br/&gt;
    BTW, we&apos;re using my PR in production and haven&apos;t lost a single document since then.&lt;/p&gt;</comment>
                            <comment id="15842367" author="githubbot" created="Fri, 27 Jan 2017 07:38:02 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2861&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2861&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Hi @static-max! Will you be ok with me opening a PR for &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-5487&quot; title=&quot;Proper at-least-once support for ElasticsearchSink&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-5487&quot;&gt;&lt;del&gt;FLINK-5487&lt;/del&gt;&lt;/a&gt;(&lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-5487&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/FLINK-5487&lt;/a&gt;) by basing it on this PR? Your contribution will be included in that PR. The main thing is that &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-5487&quot; title=&quot;Proper at-least-once support for ElasticsearchSink&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-5487&quot;&gt;&lt;del&gt;FLINK-5487&lt;/del&gt;&lt;/a&gt; won&apos;t be complete without retry for temporary ES errors, and since you&apos;ve already solved that in this PR, it&apos;ll be a good idea to use it.&lt;/p&gt;</comment>
                            <comment id="15842418" author="githubbot" created="Fri, 27 Jan 2017 09:06:39 +0000"  >&lt;p&gt;Github user static-max commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2861&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2861&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Hi @tzulitai, sure, go ahead &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15842431" author="githubbot" created="Fri, 27 Jan 2017 09:24:54 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2861&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2861&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Small note (would be great if @static-max can clarify this also) on matching the exceptions:&lt;br/&gt;
    We can actually use `BulkItemResponse.getFailure().getCause()` to get a `Throwable` that we can use to match the exceptions, instead of unstable String matching.&lt;/p&gt;

&lt;p&gt;    After surfing the Javadocs a bit, I think the below exceptions refer to what you were trying to catch before:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;`EsRejectedExecutionException`: the queue on ES node is temporarily full&lt;/li&gt;
	&lt;li&gt;`ElasticsearchTimeoutException`: timeout from Elasticsearch&lt;/li&gt;
	&lt;li&gt;`ClusterBlockException`: no master&lt;/li&gt;
	&lt;li&gt;`UnavailableShardsException` - currently no shards available&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    @static-max do you think the above exception types are reasonable to be considered &quot;temporary&quot; (have you seen them before in logs)? I&apos;m personally a bit unsure of the last two. I don&apos;t have that much experience with operating an Elasticsearch cluster, and their Javadocs really don&apos;t say much, so some input from you will be helpful &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15842574" author="githubbot" created="Fri, 27 Jan 2017 11:10:57 +0000"  >&lt;p&gt;Github user static-max commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2861&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2861&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    In my tests BulkItemResponse.getFailure().getCause() returns a RemoteTransportException like this:&lt;br/&gt;
    `RemoteTransportException[&lt;span class=&quot;error&quot;&gt;&amp;#91;Harrier&amp;#93;&lt;/span&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;127.0.0.1:9302&amp;#93;&lt;/span&gt;[indices:data/write/bulk&lt;span class=&quot;error&quot;&gt;&amp;#91;s&amp;#93;&lt;/span&gt;]]; nested: RemoteTransportException[&lt;span class=&quot;error&quot;&gt;&amp;#91;Harrier&amp;#93;&lt;/span&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;127.0.0.1:9302&amp;#93;&lt;/span&gt;[indices:data/write/bulk&lt;span class=&quot;error&quot;&gt;&amp;#91;s&amp;#93;&lt;/span&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;p&amp;#93;&lt;/span&gt;]]; nested: EsRejectedExecutionException[rejected execution of org.elasticsearch.transport.TransportService$4@3a0f3a6e on EsThreadPoolExecutor[bulk, queue capacity = 2, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@5ac266bd&lt;span class=&quot;error&quot;&gt;&amp;#91;Running, pool size = 8, active threads = 8, queued tasks = 2, completed tasks = 206&amp;#93;&lt;/span&gt;]];`&lt;/p&gt;

&lt;p&gt;    So the nested Exception needs to be checked. That&apos;s possible, I will implement that change.&lt;/p&gt;

&lt;p&gt;    The last to Exceptions are common when a new Index gets created (if you have new index by day for example), or when a node leaves the cluster and no master can be elected (no quorum),&lt;/p&gt;</comment>
                            <comment id="15842735" author="githubbot" created="Fri, 27 Jan 2017 12:36:46 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2861&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2861&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Thank you for the detail information! I&apos;ve actually already started some work on top of your commits in this PR, so I think it&apos;ll be easier to proceed to implement the nested exceptions checks on my side.&lt;/p&gt;</comment>
                            <comment id="15842849" author="githubbot" created="Fri, 27 Jan 2017 13:09:06 +0000"  >&lt;p&gt;Github user static-max commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2861&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2861&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @tzulitai OK, if you need any help feel free to ask.&lt;br/&gt;
    Are there plans to switch from the TransportClient to a pure HTTP client? That would reduce the Elasticsearch dependencies and would decouple the cluster&apos;s version from the TransportClient version used by Flink. In that case we won&apos;t get a Throwable anymore.&lt;/p&gt;</comment>
                            <comment id="15842861" author="githubbot" created="Fri, 27 Jan 2017 13:19:51 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2861&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2861&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Thanks! There wasn&apos;t any discussion on switching to a pure HTTP client, but we might need to take into consideration the effort it will take and benefits the result will offer if we were to go for that.&lt;/p&gt;

&lt;p&gt;    Regarding handling the `Throwable`: I&apos;m currently trying out `BulkItemResponse.getFailure().getStatus` which returns a `RestStatus`. From the Javadocs, it seems like we can just target some of the status codes: &lt;a href=&quot;https://www.javadoc.io/doc/org.elasticsearch/elasticsearch/2.3.5&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://www.javadoc.io/doc/org.elasticsearch/elasticsearch/2.3.5&lt;/a&gt;. I&apos;m currently thinking about just handling `TOO_MANY_REQUESTS` (ex. EsRejectedExecutionEx is one of these) and `INTERNAL_SERVER_ERROR` (the timeout exception has this code). What do you think? &lt;/p&gt;</comment>
                            <comment id="15848072" author="tzulitai" created="Wed, 1 Feb 2017 06:55:13 +0000"  >&lt;p&gt;I would like to handle this issue together with &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-5353&quot; title=&quot;Elasticsearch Sink loses well-formed documents when there are malformed documents&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-5353&quot;&gt;&lt;del&gt;FLINK-5353&lt;/del&gt;&lt;/a&gt; with a different approach: let the user provide a &lt;tt&gt;FailedActionRequestHandler&lt;/tt&gt; that implements how to deal with an action request that failed, ex. drop it or re-add it to the &lt;tt&gt;BulkProcessor&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;The reason for this is that there is actually quite a variety of different reasons an action request can fail, and for different cases, can be treated to be &quot;temporary&quot; differently. For example, in &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-5353&quot; title=&quot;Elasticsearch Sink loses well-formed documents when there are malformed documents&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-5353&quot;&gt;&lt;del&gt;FLINK-5353&lt;/del&gt;&lt;/a&gt;, malformed documents can somewhat be &quot;temporary&quot; if the erroneous field is reprocessed. Instead of handling these case by case, I propose to let user implement logic for them.&lt;/p&gt;

&lt;p&gt;The handler will look something like this:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;interface&lt;/span&gt; FailedActionRequestHandler {
    &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; onFailure(ActionRequest originalRequest, Throwable failure, RequestIndexer indexer);
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The ElasticsearchSink will still try to retry a bulk request (with backoff) for obvious temporary errors like &lt;tt&gt;EsRejectedExecutionException&lt;/tt&gt;, and will only call &lt;tt&gt;onFailure&lt;/tt&gt; after the retries. There the user can decide whether they want to re-add it to be requested through the &lt;tt&gt;RequestIndexer&lt;/tt&gt; or just drop it. The method should return &lt;tt&gt;true&lt;/tt&gt; / &lt;tt&gt;false&lt;/tt&gt; depending on whether they&apos;d like to fail the sink because of that failure.&lt;/p&gt;

&lt;p&gt;What do you think? Sorry for being picky about how to resolve this. I think it&apos;ll be best to find a good long-term solution, as from the current state of the ES issues I have a feeling that things will start to get unmaintainable once new exception handling cases pop out, so it&apos;ll be helpful to know what actual ES Flink users think of the idea.&lt;/p&gt;</comment>
                            <comment id="15882858" author="githubbot" created="Fri, 24 Feb 2017 15:16:42 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2861&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2861&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Hi @static-max!&lt;br/&gt;
    Your contribution has been merged with aaac7c2 and 3743e89.&lt;br/&gt;
    Thanks a lot for your work! Could you please manually close this PR?&lt;/p&gt;</comment>
                            <comment id="15882899" author="tzulitai" created="Fri, 24 Feb 2017 15:40:00 +0000"  >&lt;p&gt;Resolved in &lt;tt&gt;master&lt;/tt&gt; via &lt;a href=&quot;http://git-wip-us.apache.org/repos/asf/flink/3743e89&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://git-wip-us.apache.org/repos/asf/flink/3743e89&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15893961" author="githubbot" created="Fri, 3 Mar 2017 09:13:54 +0000"  >&lt;p&gt;Github user static-max closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2861&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2861&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="13034821">FLINK-5487</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13028665">FLINK-5353</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 37 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i36lo7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>