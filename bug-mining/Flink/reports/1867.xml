<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 20:29:25 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-7143] Partition assignment for Kafka consumer is not stable</title>
                <link>https://issues.apache.org/jira/browse/FLINK-7143</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;h3&gt;&lt;a name=&quot;ImportantNotice%3A&quot;&gt;&lt;/a&gt;Important Notice: &lt;/h3&gt;

&lt;p&gt;Upgrading jobs from 1.2.x exhibits no known problems. Jobs from 1.3.0 and 1.3.1 with incorrect partition assignments cannot be automatically fixed by upgrading to Flink 1.3.2 via a savepoint, because the upgraded version would resume the wrong partition assignment from the savepoint. A workaround is to assign a different uuid to the Kafka source (so the offsets won&apos;t be resumed from the savepoint) and let it start from the latest offsets committed to Kafka instead. Note that this may violate exactly-once semantics and introduce some duplicates, because Kafka&apos;s committed offsets are not guaranteed to be 100% up date date with Flink&apos;s internal offset tracking. To maximize the alignment between the offsets in Kafka and those tracked by Flink, we suggest to abort the 1.3.x job via the &quot;cancel with savepoint&quot; command (&lt;a href=&quot;https://ci.apache.org/projects/flink/flink-docs-release-1.3/setup/savepoints.html#cancel-job-with-savepoint&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://ci.apache.org/projects/flink/flink-docs-release-1.3/setup/savepoints.html#cancel-job-with-savepoint&lt;/a&gt;) during the upgrade process.&lt;/p&gt;

&lt;h3&gt;&lt;a name=&quot;OriginalIssueDescription&quot;&gt;&lt;/a&gt;Original Issue Description&lt;/h3&gt;
&lt;p&gt;While deploying Flink 1.3 release to hundreds of routing jobs, we found some issues with partition assignment for Kafka consumer. some partitions weren&apos;t assigned and some partitions got assigned more than once.&lt;/p&gt;

&lt;p&gt;Here is the bug introduced in Flink 1.3. &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;	&lt;span class=&quot;code-keyword&quot;&gt;protected&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; void initializeSubscribedPartitionsToStartOffsets(...) {
                ...
		&lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; i = 0; i &amp;lt; kafkaTopicPartitions.size(); i++) {
			&lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (i % numParallelSubtasks == indexOfThisSubtask) {
				&lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (startupMode != StartupMode.SPECIFIC_OFFSETS) {
					subscribedPartitionsToStartOffsets.put(kafkaTopicPartitions.get(i), startupMode.getStateSentinel());
				}
                ...
         }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The bug is using array index &lt;tt&gt;i&lt;/tt&gt; to mod against &lt;tt&gt;numParallelSubtasks&lt;/tt&gt;. if the &lt;tt&gt;kafkaTopicPartitions&lt;/tt&gt; has different order among different subtasks, assignment is not stable cross subtasks and creates the assignment issue mentioned earlier. &lt;/p&gt;

&lt;p&gt;fix is also very simple, we should use partitionId to do the mod &lt;tt&gt;if (kafkaTopicPartitions.get&amp;#40;i&amp;#41;.getPartition() % numParallelSubtasks == indexOfThisSubtask)&lt;/tt&gt;. That would result in stable assignment cross subtasks that is independent of ordering in the array.&lt;/p&gt;

&lt;p&gt;marking it as blocker because of its impact.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13086103">FLINK-7143</key>
            <summary>Partition assignment for Kafka consumer is not stable</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="tzulitai">Tzu-Li (Gordon) Tai</assignee>
                                    <reporter username="stevenz3wu">Steven Zhen Wu</reporter>
                        <labels>
                    </labels>
                <created>Mon, 10 Jul 2017 23:03:42 +0000</created>
                <updated>Thu, 3 Aug 2017 10:09:50 +0000</updated>
                            <resolved>Thu, 3 Aug 2017 10:09:50 +0000</resolved>
                                    <version>1.3.1</version>
                                    <fixVersion>1.3.2</fixVersion>
                    <fixVersion>1.4.0</fixVersion>
                                    <component>Connectors / Kafka</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>7</watches>
                                                                                                                <comments>
                            <comment id="16081386" author="tzulitai" created="Tue, 11 Jul 2017 00:02:46 +0000"  >&lt;p&gt;IIRC, sorting the fetched partition list was removed in favor of using the hashCode of &lt;tt&gt;KafkaTopicPartition&lt;/tt&gt; s for the mod operation. This must have been a remnant from that change ...&lt;/p&gt;

&lt;p&gt;Apparently, the current partition assignment tests do not have enough coverage. We also need a test that verifies assignment stability in the case of different fetched partitions ordering.&lt;/p&gt;</comment>
                            <comment id="16081420" author="stevenz3wu" created="Tue, 11 Jul 2017 00:27:18 +0000"  >&lt;p&gt;regarding test coverage, we need a test that verify the assignment is &lt;tt&gt;partitionId % parallelism&lt;/tt&gt;. right now, it is totally random and test coverage didn&apos;t catch it.&lt;/p&gt;</comment>
                            <comment id="16081474" author="tzulitai" created="Tue, 11 Jul 2017 01:28:23 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=henryz&quot; class=&quot;user-hover&quot; rel=&quot;henryz&quot;&gt;henryz&lt;/a&gt; yes. One small thing, it would need to be &lt;tt&gt;partition.hashCode() % parallelism&lt;/tt&gt;, since we allow multiple topics and therefore cannot rely only on the partition id.&lt;/p&gt;</comment>
                            <comment id="16081665" author="stevenz3wu" created="Tue, 11 Jul 2017 05:01:47 +0000"  >&lt;p&gt;I see. change is for support of multiple topics. hashCode() can work as a stable assignment. My only complain is that it makes it non-obvious to see the assignment pattern (especially for single topic case). not critical.&lt;/p&gt;

&lt;p&gt;I don&apos;t known if it make sense to do assignment using &lt;tt&gt;partitionId % parallelism&lt;/tt&gt; for each topic in the multiple-topics case?&lt;/p&gt;</comment>
                            <comment id="16082306" author="aljoscha" created="Tue, 11 Jul 2017 14:45:28 +0000"  >&lt;p&gt;IMHO, if we used &lt;tt&gt;partitionId % parallelism&lt;/tt&gt; in the multi-topics cases we could get bad utilisation. For example, assume we have 10 parallel source instances, we read from two topics, each topic has 5 partitions. Now, if we used &lt;tt&gt;partitionId % parallelism&lt;/tt&gt; each of the firsts 5 source instances would read two partitions (one from each topic) while the lasts 5 source instances would not read any partition. Does that make sense?&lt;/p&gt;</comment>
                            <comment id="16082538" author="githubbot" created="Tue, 11 Jul 2017 17:11:07 +0000"  >&lt;p&gt;GitHub user tzulitai opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4301&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4301&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-7143&quot; title=&quot;Partition assignment for Kafka consumer is not stable&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-7143&quot;&gt;&lt;del&gt;FLINK-7143&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; Fix indeterminate partition assignment in FlinkKafkaConsumer&lt;/p&gt;

&lt;p&gt;    This PR changes the mod operation for partition assignment from `i % numTasks == subtaskIndex` to `partition.hashCode % numTasks == subtaskIndex`.&lt;/p&gt;

&lt;p&gt;    The bug was initially caused by #3378, when moving away from sorting the partition list. Apparently, the tests for partition assignment was not strict enough and did not catch this. This PR additionally adds verifications that the partitions end up in the expected subtasks, and that different partition ordering will still have the same partition assignments.&lt;/p&gt;

&lt;p&gt;    Note: a fix is not required for the `master` branch, since the partition discovery changes already indirectly fixed the issue. However, test coverage for deterministic assignment should likewise be improved in `master` as well. A separate PR will be opened for that.&lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/tzulitai/flink&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/tzulitai/flink&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-7143&quot; title=&quot;Partition assignment for Kafka consumer is not stable&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-7143&quot;&gt;&lt;del&gt;FLINK-7143&lt;/del&gt;&lt;/a&gt;-1.3&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4301.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4301.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #4301&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 563f605d00f5d184fce2eb505c59033f22d3d0ab&lt;br/&gt;
Author: Tzu-Li (Gordon) Tai &amp;lt;tzulitai@apache.org&amp;gt;&lt;br/&gt;
Date:   2017-07-11T17:03:01Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-7143&quot; title=&quot;Partition assignment for Kafka consumer is not stable&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-7143&quot;&gt;&lt;del&gt;FLINK-7143&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; Fix indeterminate partition assignment in FlinkKafkaConsumer&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="16082540" author="githubbot" created="Tue, 11 Jul 2017 17:11:39 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4301&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4301&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    R: @aljoscha @rmetzger &lt;/p&gt;</comment>
                            <comment id="16082617" author="githubbot" created="Tue, 11 Jul 2017 17:41:46 +0000"  >&lt;p&gt;GitHub user tzulitai opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4302&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4302&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    (master) &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-7143&quot; title=&quot;Partition assignment for Kafka consumer is not stable&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-7143&quot;&gt;&lt;del&gt;FLINK-7143&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; Stricter tests for deterministic partition assignment&lt;/p&gt;

&lt;p&gt;    This is the forward port of the fix in #4301. The `master` branch does not require a fix, because the new `AbstractPartitionDiscoverer` already correctly uses `partition.hashCode % numTasks` for the partition discovery filtering.&lt;/p&gt;

&lt;p&gt;    This PR simply makes the tests for partition assignment more strict, to guard against breaking deterministic assignment in the future.&lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/tzulitai/flink&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/tzulitai/flink&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-7143&quot; title=&quot;Partition assignment for Kafka consumer is not stable&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-7143&quot;&gt;&lt;del&gt;FLINK-7143&lt;/del&gt;&lt;/a&gt;-master&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4302.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4302.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #4302&lt;/p&gt;

&lt;hr /&gt;

&lt;hr /&gt;</comment>
                            <comment id="16082619" author="stevenz3wu" created="Tue, 11 Jul 2017 17:42:35 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aljoscha&quot; class=&quot;user-hover&quot; rel=&quot;aljoscha&quot;&gt;aljoscha&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tzulitai&quot; class=&quot;user-hover&quot; rel=&quot;tzulitai&quot;&gt;tzulitai&lt;/a&gt; agree that &lt;tt&gt;partitionId % parallelism&lt;/tt&gt; is probably not a good idea for multiple topics case. what about the old sorting way? Sort the list by (topic, partition) tuple first. Then do a simple round-robin assignment (mod). Advantage is that it is much easier to see the assignment pattern (comparing to hashCode). That usually can help with debugging: easier to figure out expected assignment.&lt;/p&gt;

&lt;p&gt;Here is an example of 3 topics and parallelism of 4&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;[subtask-id] topic, partition
[0] t1, p0
[1] t1, p1
[2] t1, p2
[3] t2, p0
[0] t3, p0
[1] t3, p1
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
</comment>
                            <comment id="16082643" author="tzulitai" created="Tue, 11 Jul 2017 17:51:46 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stevenz3wu&quot; class=&quot;user-hover&quot; rel=&quot;stevenz3wu&quot;&gt;stevenz3wu&lt;/a&gt; Sorting would not work, because dynamically discovered topic and partitions (a feature in Flink 1.4) will potentially break the sorting (although partition id is always ascending, again, we need to consider multiple topics). That&#8217;s why it was changed in the first place. If we&#8217;re considering a solution only for 1.3 where partition metadata fetching only happens a single time on startup, it would make sense to continue using sorting + round-robin assignment.&lt;/p&gt;</comment>
                            <comment id="16082912" author="githubbot" created="Tue, 11 Jul 2017 20:29:37 +0000"  >&lt;p&gt;Github user StephanEwen commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4301&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4301&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I think that would fix the bug. There are two things I would like to improve, though:&lt;/p&gt;

&lt;p&gt;      1. Relying on `hashCode()` makes very implicit assumptions about the behavior of the hash code implementation. This does not really document/articulate well how critical this `int` value that we rely on is. For example, by Java specification, hashCode may vary between processes - it only needs to be stable within a single JVM. Our hash code implementation happens to be stable currently, as long as the JDK does not change the implementation of the String hash code method (which they could in theory do in any minor release, although they have not done that in a while).&lt;/p&gt;

&lt;p&gt;      2. It is crucial that the distribution of partitions is uniform. That is a bit harder to guarantee when all sources pick up their own set of topics. At the least, distribution should be uniform of the partitions within a topic. For example, the topic defines &quot;where to start&quot; in the parallel subtasks, and the partitions then go &quot;round robin&quot;.&lt;br/&gt;
    Well, as it happens, this is actually the implementation of the hash code function, but again, this looks a bit like it &quot;coincidentally&quot; behaves like that, rather than that we have a strict contract for that behavior. For example, changing the hashCode from `31 * topic + partition` to `31 * partition + topic` results in non-uniform distribution, but is an equally valid hashCode.&lt;/p&gt;

&lt;p&gt;    I would suggest to have a function `int assignmentIndex()` or so, for which we define the above contract. We should also have tests that this distributes partitions within a single topic uniform.&lt;/p&gt;</comment>
                            <comment id="16083960" author="githubbot" created="Wed, 12 Jul 2017 13:21:27 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4301&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4301&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @StephanEwen thanks for the review. Your suggestion makes a lot of sense.&lt;/p&gt;

&lt;p&gt;    I&apos;ve fixed this up as the following:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Have a new method `KafkaTopicAssigner.assign(KafkaTopicPartition partition, int numSubtasks)` that defines a strict contract, such that when locally used to filter out partitions, the resulting distribution of the partitions of a single topic are guaranteed to be:&lt;br/&gt;
      1. Uniformly distributed across subtasks&lt;br/&gt;
      2. Partitions are round-robin distributed (strictly CLOCKWISE w.r.t. ascending subtask indices) by using the partition id as the offset from a starting index determined using the topic name. The extra directional contract makes this more stricter than what we had before, which we may be round-robin assigning partitions counter-clockwise. This should make the actual assignment scheme much more predictable as perceived by the user, since they just need to know the start index of a specific topic to understand how the partitions of the topic are distributed across subtasks. We could add some log that states the start index of the topics it is consuming.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Strengthen the tests in `KafkaConsumerPartitionAssignmentTest` to test this contract. Uniform distribution was already tested in that suite of tests, the change makes the tests also verify the &quot;clockwise round-robin since some start index&quot; contract.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="16083966" author="githubbot" created="Wed, 12 Jul 2017 13:24:46 +0000"  >&lt;p&gt;Github user StephanEwen commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4301#discussion_r126952350&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4301#discussion_r126952350&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/internals/KafkaTopicPartitionAssigner.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,56 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka.internals;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * Utility for assigning Kafka partitions to consumer subtasks.&lt;br/&gt;
    + */&lt;br/&gt;
    +public class KafkaTopicPartitionAssigner {&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Returns the index of the target subtask that a specific Kafka partition should be&lt;br/&gt;
    +	 * assigned to.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * &amp;lt;p&amp;gt;The resulting distribution of partitions of a single topic has the following contract:&lt;br/&gt;
    +	 * &amp;lt;ul&amp;gt;&lt;br/&gt;
    +	 *     &amp;lt;li&amp;gt;1. Uniformly distributed across subtasks&amp;lt;/li&amp;gt;&lt;br/&gt;
    +	 *     &amp;lt;li&amp;gt;2. Partitions are round-robin distributed (strictly clockwise w.r.t. ascending&lt;br/&gt;
    +	 *     subtask indices) by using the partition id as the offset from a starting index&lt;br/&gt;
    +	 *     (i.e., the index of the subtask which partition 0 of the topic will be assigned to,&lt;br/&gt;
    +	 *     determined using the topic name).&amp;lt;/li&amp;gt;&lt;br/&gt;
    +	 * &amp;lt;/ul&amp;gt;&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * &amp;lt;p&amp;gt;The above contract is crucial and cannot be broken. Consumer subtasks rely on this&lt;br/&gt;
    +	 * contract to locally filter out partitions that it should not subscribe to, guaranteeing&lt;br/&gt;
    +	 * that all partitions of a single topic will always be assigned to some subtask in a&lt;br/&gt;
    +	 * uniformly distributed manner.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @param partition the Kafka partition&lt;br/&gt;
    +	 * @param numParallelSubtasks total number of parallel subtasks&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @return index of the target subtask that the Kafka partition should be assigned to.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public static int assign(KafkaTopicPartition partition, int numParallelSubtasks) {&lt;br/&gt;
    +		int startIndex = Math.abs(partition.getTopic().hashCode() * 31 % numParallelSubtasks);&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Minor detail: `Math.abs` does not work for `Integer.MIN_VALUE`, so it is slightly safe to do&lt;br/&gt;
    ```java&lt;br/&gt;
    int startIndex = ((partition.getTopic().hashCode() * 31) &amp;amp; 0x7FFFFFFF) % numParallelSubtasks);&lt;br/&gt;
    ```&lt;/p&gt;</comment>
                            <comment id="16084027" author="githubbot" created="Wed, 12 Jul 2017 14:04:21 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4301#discussion_r126962857&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4301#discussion_r126962857&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/internals/KafkaTopicPartitionAssigner.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,56 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka.internals;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * Utility for assigning Kafka partitions to consumer subtasks.&lt;br/&gt;
    + */&lt;br/&gt;
    +public class KafkaTopicPartitionAssigner {&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Returns the index of the target subtask that a specific Kafka partition should be&lt;br/&gt;
    +	 * assigned to.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * &amp;lt;p&amp;gt;The resulting distribution of partitions of a single topic has the following contract:&lt;br/&gt;
    +	 * &amp;lt;ul&amp;gt;&lt;br/&gt;
    +	 *     &amp;lt;li&amp;gt;1. Uniformly distributed across subtasks&amp;lt;/li&amp;gt;&lt;br/&gt;
    +	 *     &amp;lt;li&amp;gt;2. Partitions are round-robin distributed (strictly clockwise w.r.t. ascending&lt;br/&gt;
    +	 *     subtask indices) by using the partition id as the offset from a starting index&lt;br/&gt;
    +	 *     (i.e., the index of the subtask which partition 0 of the topic will be assigned to,&lt;br/&gt;
    +	 *     determined using the topic name).&amp;lt;/li&amp;gt;&lt;br/&gt;
    +	 * &amp;lt;/ul&amp;gt;&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * &amp;lt;p&amp;gt;The above contract is crucial and cannot be broken. Consumer subtasks rely on this&lt;br/&gt;
    +	 * contract to locally filter out partitions that it should not subscribe to, guaranteeing&lt;br/&gt;
    +	 * that all partitions of a single topic will always be assigned to some subtask in a&lt;br/&gt;
    +	 * uniformly distributed manner.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @param partition the Kafka partition&lt;br/&gt;
    +	 * @param numParallelSubtasks total number of parallel subtasks&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @return index of the target subtask that the Kafka partition should be assigned to.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public static int assign(KafkaTopicPartition partition, int numParallelSubtasks) {&lt;br/&gt;
    +		int startIndex = Math.abs(partition.getTopic().hashCode() * 31 % numParallelSubtasks);&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Good catch!&lt;/p&gt;</comment>
                            <comment id="16084038" author="githubbot" created="Wed, 12 Jul 2017 14:12:10 +0000"  >&lt;p&gt;Github user aljoscha commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4301&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4301&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Quick comment for my own clarification: when restoring from a 1.3.x savepoint, the new assignment logic will not be used, right? In Flink 1.3.x there is no dynamic partition discovery and so when restarting from state we have to strictly stick to what we have in the (operator) state to avoid reading partitions on multiple subtasks.&lt;/p&gt;

&lt;p&gt;    If this is true, this also means that folks that have savepoints on 1.3.x cannot them if they want to benefit from this fix, right?&lt;/p&gt;</comment>
                            <comment id="16084045" author="githubbot" created="Wed, 12 Jul 2017 14:16:36 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4301&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4301&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Yes, that is true. This assignment logic is only applied on fresh starts.&lt;/p&gt;</comment>
                            <comment id="16084053" author="githubbot" created="Wed, 12 Jul 2017 14:23:51 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4301&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4301&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    This would then mean we discourage restoring from a 1.3.x savepoint, because the state is potentially incorrect.&lt;br/&gt;
    I wonder if we then actually want to always fetch partitions on startup (fresh or from savepoint) to deal with this (just a fix for the `release-1.3` branch)?&lt;/p&gt;</comment>
                            <comment id="16084107" author="githubbot" created="Wed, 12 Jul 2017 15:03:12 +0000"  >&lt;p&gt;Github user StephanEwen commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4301&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4301&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Do we have a test for the case where there are fewer partitions than sources so that some sources do not get partitions on restore? To make sure they do not accidentally re-discover?&lt;/p&gt;</comment>
                            <comment id="16084110" author="githubbot" created="Wed, 12 Jul 2017 15:07:26 +0000"  >&lt;p&gt;Github user StephanEwen commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4301&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4301&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Just to double-check: I see that the state of the partitions is in a `ListState`. That means after recovery, they can be differently distributed than before. Does that not conflict with the discovery and assignment of partitions?&lt;/p&gt;</comment>
                            <comment id="16084202" author="githubbot" created="Wed, 12 Jul 2017 15:50:14 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4301&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4301&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @StephanEwen&lt;br/&gt;
    Regarding no-rediscover on restore test:&lt;br/&gt;
    yes, could say that it is covered in `KafkaConsumerTestBase.runMultipleSourcesOnePartitionExactlyOnceTest()`. It&apos;s an end-to-end exactly-once test for the case where Flink source subtask count &amp;gt; partition count.&lt;/p&gt;

&lt;p&gt;    Regarding `ListState`:&lt;br/&gt;
    The redistribution of `ListState` doesn&apos;t conflict with discovery and assignment of partitions in the `release-1.3` case (where there is no partition discovery), because we don&apos;t respect the partition assignment logic if we&apos;re starting from savepoints. We only consider what&apos;s in the restored state. See also @aljoscha&apos;s comment above.&lt;/p&gt;

&lt;p&gt;    For `master` where partition discovery is already merged, the `ListState` is a union list state, where all subtasks are broadcasted with all partition states. On restore, the restored union list state is filtered again with the assignment logic.&lt;/p&gt;</comment>
                            <comment id="16087236" author="githubbot" created="Fri, 14 Jul 2017 12:11:09 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4301&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4301&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @aljoscha on some second thinking, I don&apos;t think we can easily deal with the fact that, when restoring from 1.3.1 / 1.3.0 savepoints in 1.3.2, users will not benefit from this bug fix.&lt;/p&gt;

&lt;p&gt;    There is basically no guarantee in what the distribution would be when restoring from 1.3.1 / 1.3.0, and therefore no way to manipulate it to follow the new fixed distribution scheme we introduce here.&lt;br/&gt;
    It may be possible if we force a migrate to union list state in 1.3.2, but I&apos;m not really sure that we want to do that ..&lt;/p&gt;</comment>
                            <comment id="16089969" author="githubbot" created="Mon, 17 Jul 2017 15:31:45 +0000"  >&lt;p&gt;Github user aljoscha commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4301&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4301&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Yes, I don&apos;t think we can get around this when restoring from &quot;old&quot; state.&lt;/p&gt;

&lt;p&gt;    I also have another suspicion: I don&apos;t think that `KafkaConsumerTestBase.runMultipleSourcesOnePartitionExactlyOnceTest()` accurately catches some cases and I think there is a problem that we cannot accurately detect whether we are restoring or whether we are opening from scratch. Consider this case: 5 partitions, 5 parallel source instances. Now we rescale to 10 parallel source instances. Some sources don&apos;t get state, so they think that we are starting from scratch and they will run partition discovery. Doesn&apos;t this mean that they could possibly read from a topic where already another source is reading from, because it got the state for that? (Not this doesn&apos;t occur on master because all sources get all state.)&lt;/p&gt;</comment>
                            <comment id="16090028" author="githubbot" created="Mon, 17 Jul 2017 16:28:03 +0000"  >&lt;p&gt;Github user aljoscha commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4301&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4301&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Note, that this doesn&apos;t normally occur because the strategy for assigning Kafka partitions and for assigning operator state is the same (right now). However, this means that you will have active partition discovery for parallel instances that didn&apos;t previously have state: assume we have 1 partition and 1 parallel source. Now we add a new partition and restart the Flink job. Now, parallel instance 1 will still read from partition 0, parallel instance 2 will think that it didn&apos;t restart (because it didn&apos;t get state) and will discover partitions and take ownership of partition 1.&lt;/p&gt;

&lt;p&gt;    (This is with current `release-1.3` branch code.)&lt;/p&gt;</comment>
                            <comment id="16090113" author="githubbot" created="Mon, 17 Jul 2017 17:19:08 +0000"  >&lt;p&gt;Github user aljoscha commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4301&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4301&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Oye, this is more complicated than I thought. On `release-1.3` the assignment actually works if the Kafka brokers always return the partitions in the same order. The reason is that the assignment of partitions and the assignment of operator state (in `RoundRobinOperatorStateRepartitioner`) is aligned. This meant that it&apos;s not a problem when sources think that they are &quot;fresh&quot; (not restored from state) because they didn&apos;t get any state. If they tried to assign a partition to themselves this would also mean that they have the state for that (again, because partition assignment and operator state assignment are aligned). &lt;/p&gt;

&lt;p&gt;    This PR breaks the alignment because the `startIndex` is not necessarily `0`. However, this is not caught by any tests because the `StateAssignmentOperation` has an optimisation where it doesn&apos;t repartition operator state if the parallelism doesn&apos;t change. If we deactivate that optimisation by turning this line into `if (true)`: &lt;a href=&quot;https://github.com/apache/flink/blob/b1f762127234e323b947aa4a363935f87be1994f/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/StateAssignmentOperation.java#L561-L561&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/blob/b1f762127234e323b947aa4a363935f87be1994f/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/StateAssignmentOperation.java#L561-L561&lt;/a&gt; the test in Kafka09ITCase will fail.&lt;/p&gt;

&lt;p&gt;    The fix is to properly forward the information of whether we&apos;re restored in `initializeState()`, I did a commit for that: &lt;a href=&quot;https://github.com/aljoscha/flink/tree/finish-pr-4301-kafka-13-fixings&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/aljoscha/flink/tree/finish-pr-4301-kafka-13-fixings&lt;/a&gt;. The problem is that it is not easy to change the tests to catch this bug. I think an ITCase that uses Kafka and does a savepoint and rescaling would do the trick.&lt;/p&gt;</comment>
                            <comment id="16091448" author="githubbot" created="Tue, 18 Jul 2017 11:40:47 +0000"  >&lt;p&gt;GitHub user tzulitai opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4357&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4357&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    (release-1.3) &lt;span class=&quot;error&quot;&gt;&amp;#91;FLINK-7143, FLINK-7195&amp;#93;&lt;/span&gt; Collection of Kafka fixes for release-1.3&lt;/p&gt;

&lt;p&gt;    This PR subsumes #4344 and #4301, including changes in both PRs merged and conflicts resolved.&lt;br/&gt;
    Apparently, some new tests added in one of the PRs relies also on the fix of the other PR, so opening this one to have a better overall view of the status of the fixes.&lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/tzulitai/flink&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/tzulitai/flink&lt;/a&gt; kafka-13-fixes&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4357.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4357.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #4357&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 919b23a6e1c650a3d08f5418a53e712e86d51506&lt;br/&gt;
Author: Tzu-Li (Gordon) Tai &amp;lt;tzulitai@apache.org&amp;gt;&lt;br/&gt;
Date:   2017-07-11T17:03:01Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-7143&quot; title=&quot;Partition assignment for Kafka consumer is not stable&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-7143&quot;&gt;&lt;del&gt;FLINK-7143&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; Fix indeterminate partition assignment in FlinkKafkaConsumer&lt;/p&gt;

&lt;p&gt;    Apart from fixing the previous incorrect, indeterministic assignment&lt;br/&gt;
    logic, this commit also adds an explicitly defined method that properly&lt;br/&gt;
    states a strict contract for the assignment, instead of just relying on&lt;br/&gt;
    some hashCode implementation that doesn&apos;t convey this contract as well&lt;br/&gt;
    as the importance of the assignment&apos;s deterministic characteristic well.&lt;/p&gt;

&lt;p&gt;commit 00bcdbf24c177276f203063f905886becfe23db5&lt;br/&gt;
Author: Tzu-Li (Gordon) Tai &amp;lt;tzulitai@apache.org&amp;gt;&lt;br/&gt;
Date:   2017-07-14T11:51:03Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-7195&quot; title=&quot;FlinkKafkaConsumer should not respect fetched partitions to filter restored partition states&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-7195&quot;&gt;&lt;del&gt;FLINK-7195&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; Remove partition list querying when restoring state in FlinkKafkaConsumer&lt;/p&gt;

&lt;p&gt;    Previously, querying the partition list and using it to filter out&lt;br/&gt;
    restored partition states is problematic since the queried partition&lt;br/&gt;
    list may be missing partitions due to temporary downtime of Kafka&lt;br/&gt;
    brokers. Effectively, this caused the potential dropping of state on&lt;br/&gt;
    restores.&lt;/p&gt;

&lt;p&gt;    This commit fixes this by completely removing partition querying if&lt;br/&gt;
    we&apos;re restoring state (as notified by&lt;br/&gt;
    FunctionInitializationContext.isRestored()). The subscribed partitions&lt;br/&gt;
    will always be exactly what the restored state contains.&lt;/p&gt;

&lt;p&gt;commit a4ca2f559b1d530e68ce3516035964f569ff7c7f&lt;br/&gt;
Author: Aljoscha Krettek &amp;lt;aljoscha.krettek@gmail.com&amp;gt;&lt;br/&gt;
Date:   2017-07-17T17:06:09Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-7143&quot; title=&quot;Partition assignment for Kafka consumer is not stable&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-7143&quot;&gt;&lt;del&gt;FLINK-7143&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; Fix detection of restored bit in Kafka Consumer&lt;/p&gt;

&lt;p&gt;    Before, the problem was that empty state was associated with the source&lt;br/&gt;
    not being restored. However, a source can have empty restored state in&lt;br/&gt;
    one of two cases:&lt;/p&gt;

&lt;p&gt;    1. The source was not restored.&lt;br/&gt;
    2. The overall job was restored but the source simply didn&apos;t get any&lt;br/&gt;
    operator state assigned.&lt;/p&gt;

&lt;p&gt;commit faf957209220d2779062321d7ab58c9356906ad8&lt;br/&gt;
Author: Aljoscha Krettek &amp;lt;aljoscha.krettek@gmail.com&amp;gt;&lt;br/&gt;
Date:   2017-07-18T08:35:54Z&lt;/p&gt;

&lt;p&gt;    &lt;span class=&quot;error&quot;&gt;&amp;#91;hotfix&amp;#93;&lt;/span&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; Make checkpoint methods final in KafkaConsumerBase&lt;/p&gt;

&lt;p&gt;    This prevents concrete Kafka Source implementations from accidentally&lt;br/&gt;
    overriding the checkpointing methods. This would be problematic when not&lt;br/&gt;
    providing tests. We test the checkpoint methods of the ConsumerBase but&lt;br/&gt;
    derived methods would not be tested.&lt;/p&gt;

&lt;p&gt;commit 5180f898c48ce2e416547dcdf76caef72c5a8dee&lt;br/&gt;
Author: Aljoscha Krettek &amp;lt;aljoscha.krettek@gmail.com&amp;gt;&lt;br/&gt;
Date:   2017-07-18T09:57:46Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-7143&quot; title=&quot;Partition assignment for Kafka consumer is not stable&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-7143&quot;&gt;&lt;del&gt;FLINK-7143&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; Add test for Kafka Consumer rescaling&lt;/p&gt;

&lt;p&gt;    This verifies that the consumer always correctly knows whether it is&lt;br/&gt;
    restored or not and is not affected by changes in the partitions as&lt;br/&gt;
    reported by Kafka.&lt;/p&gt;

&lt;p&gt;    Previously, operator state reshuffling could lead to partitions being&lt;br/&gt;
    subscribed to multiple times.&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="16091449" author="githubbot" created="Tue, 18 Jul 2017 11:41:36 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4357&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4357&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    R: @aljoscha &lt;/p&gt;</comment>
                            <comment id="16092282" author="githubbot" created="Tue, 18 Jul 2017 22:35:12 +0000"  >&lt;p&gt;Github user stevenzwu commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4357#discussion_r128113797&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4357#discussion_r128113797&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumerBase.java &amp;#8212;&lt;br/&gt;
    @@ -517,16 +519,13 @@ public void initializeState(FunctionInitializationContext context) throws Except&lt;br/&gt;
     					LOG.debug(&quot;Using the following offsets: {}&quot;, restoredState);&lt;br/&gt;
     				}&lt;br/&gt;
     			}&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (restoredState != null &amp;amp;&amp;amp; restoredState.isEmpty()) 
{
    -				restoredState = null;
    -			}
&lt;p&gt;     		} else &lt;/p&gt;
{
     			LOG.info(&quot;No restore state for FlinkKafkaConsumer.&quot;);
     		}
&lt;p&gt;     	}&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     	@Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void snapshotState(FunctionSnapshotContext context) throws Exception {&lt;br/&gt;
    +	public final void snapshotState(FunctionSnapshotContext context) throws Exception {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    @tzulitai what&apos;s the reason to make this final? In our router use case, we override the snapshotState method to no-op. We disabled Flink checkpoint by setting checkpoint interval to Long.MAX_VALUE. we can&apos;t set Flink checkpoint to false, because otherwise Kafka consumer auto.commit will be hard-coded to true. &lt;/p&gt;

&lt;p&gt;    @zhenzhongxu ^&lt;/p&gt;</comment>
                            <comment id="16092528" author="githubbot" created="Wed, 19 Jul 2017 03:48:46 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4357#discussion_r128147294&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4357#discussion_r128147294&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumerBase.java &amp;#8212;&lt;br/&gt;
    @@ -517,16 +519,13 @@ public void initializeState(FunctionInitializationContext context) throws Except&lt;br/&gt;
     					LOG.debug(&quot;Using the following offsets: {}&quot;, restoredState);&lt;br/&gt;
     				}&lt;br/&gt;
     			}&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (restoredState != null &amp;amp;&amp;amp; restoredState.isEmpty()) 
{
    -				restoredState = null;
    -			}
&lt;p&gt;     		} else &lt;/p&gt;
{
     			LOG.info(&quot;No restore state for FlinkKafkaConsumer.&quot;);
     		}
&lt;p&gt;     	}&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     	@Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void snapshotState(FunctionSnapshotContext context) throws Exception {&lt;br/&gt;
    +	public final void snapshotState(FunctionSnapshotContext context) throws Exception {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    @stevenzwu the snapshotState method was actually never intended to be overriden, hence making it final here to state that clearly. For example, the verrsion-specific implementations for `FlinkKafkaConsumerBase` may override that and have incorrect implementations, where as our tests would never realize it.&lt;/p&gt;</comment>
                            <comment id="16092533" author="githubbot" created="Wed, 19 Jul 2017 03:51:16 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4357#discussion_r128147490&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4357#discussion_r128147490&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumerBase.java &amp;#8212;&lt;br/&gt;
    @@ -517,16 +519,13 @@ public void initializeState(FunctionInitializationContext context) throws Except&lt;br/&gt;
     					LOG.debug(&quot;Using the following offsets: {}&quot;, restoredState);&lt;br/&gt;
     				}&lt;br/&gt;
     			}&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (restoredState != null &amp;amp;&amp;amp; restoredState.isEmpty()) 
{
    -				restoredState = null;
    -			}
&lt;p&gt;     		} else &lt;/p&gt;
{
     			LOG.info(&quot;No restore state for FlinkKafkaConsumer.&quot;);
     		}
&lt;p&gt;     	}&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     	@Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void snapshotState(FunctionSnapshotContext context) throws Exception {&lt;br/&gt;
    +	public final void snapshotState(FunctionSnapshotContext context) throws Exception {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    On the other hand, I think that this part of your description is strange:&lt;br/&gt;
    &amp;gt;we can&apos;t set Flink checkpoint to false, because otherwise Kafka consumer auto.commit will be hard-coded to true.&lt;br/&gt;
    This should not be the case (at least starting from Flink 1.3.x). The &quot;auto.commit&quot; is independent of checkpointing. If you don&apos;t enable checkpointing, &quot;auto.commit&quot; decides whether or not periodic checkpointing is used. Otherwise, you can still disable offset committing with checkpointing on by using `FlinkKafkaConsumer#disableOffsetCommittingOnCheckpoints`.&lt;/p&gt;</comment>
                            <comment id="16092535" author="githubbot" created="Wed, 19 Jul 2017 03:51:58 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4357#discussion_r128147542&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4357#discussion_r128147542&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumerBase.java &amp;#8212;&lt;br/&gt;
    @@ -517,16 +519,13 @@ public void initializeState(FunctionInitializationContext context) throws Except&lt;br/&gt;
     					LOG.debug(&quot;Using the following offsets: {}&quot;, restoredState);&lt;br/&gt;
     				}&lt;br/&gt;
     			}&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (restoredState != null &amp;amp;&amp;amp; restoredState.isEmpty()) 
{
    -				restoredState = null;
    -			}
&lt;p&gt;     		} else &lt;/p&gt;
{
     			LOG.info(&quot;No restore state for FlinkKafkaConsumer.&quot;);
     		}
&lt;p&gt;     	}&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     	@Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void snapshotState(FunctionSnapshotContext context) throws Exception {&lt;br/&gt;
    +	public final void snapshotState(FunctionSnapshotContext context) throws Exception {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    See &lt;a href=&quot;https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/connectors/kafka.html#kafka-consumers-offset-committing-behaviour-configuration&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/connectors/kafka.html#kafka-consumers-offset-committing-behaviour-configuration&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="16094205" author="githubbot" created="Thu, 20 Jul 2017 05:33:07 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4357&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4357&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @aljoscha do you have any last comments on these changes?&lt;/p&gt;</comment>
                            <comment id="16094373" author="githubbot" created="Thu, 20 Jul 2017 08:46:53 +0000"  >&lt;p&gt;Github user aljoscha commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4357&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4357&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I don&apos;t but I would like to give @StephanEwen a chance to comment on the changes that make the checkpoint-related methods on the consumer base final.&lt;/p&gt;</comment>
                            <comment id="16095460" author="githubbot" created="Thu, 20 Jul 2017 22:05:01 +0000"  >&lt;p&gt;Github user stevenzwu commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4357#discussion_r128642186&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4357#discussion_r128642186&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumerBase.java &amp;#8212;&lt;br/&gt;
    @@ -517,16 +519,13 @@ public void initializeState(FunctionInitializationContext context) throws Except&lt;br/&gt;
     					LOG.debug(&quot;Using the following offsets: {}&quot;, restoredState);&lt;br/&gt;
     				}&lt;br/&gt;
     			}&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (restoredState != null &amp;amp;&amp;amp; restoredState.isEmpty()) 
{
    -				restoredState = null;
    -			}
&lt;p&gt;     		} else &lt;/p&gt;
{
     			LOG.info(&quot;No restore state for FlinkKafkaConsumer.&quot;);
     		}
&lt;p&gt;     	}&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     	@Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void snapshotState(FunctionSnapshotContext context) throws Exception {&lt;br/&gt;
    +	public final void snapshotState(FunctionSnapshotContext context) throws Exception {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    @tzulitai looks like the behavior was changed/fix in 1.3. Here is the Kafka09Fetcher.java code from 1.2 that was causing the behavior I described earlier.&lt;/p&gt;

    &lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    		&lt;span class=&quot;code-comment&quot;&gt;// &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; checkpointing is enabled, we are not automatically committing to Kafka.
&lt;/span&gt;    		kafkaProperties.setProperty(
    				ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,
    				&lt;span class=&quot;code-object&quot;&gt;Boolean&lt;/span&gt;.toString(!enableCheckpointing));
    &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="16095464" author="githubbot" created="Thu, 20 Jul 2017 22:09:37 +0000"  >&lt;p&gt;Github user stevenzwu commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4357#discussion_r128642917&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4357#discussion_r128642917&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumerBase.java &amp;#8212;&lt;br/&gt;
    @@ -517,16 +519,13 @@ public void initializeState(FunctionInitializationContext context) throws Except&lt;br/&gt;
     					LOG.debug(&quot;Using the following offsets: {}&quot;, restoredState);&lt;br/&gt;
     				}&lt;br/&gt;
     			}&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (restoredState != null &amp;amp;&amp;amp; restoredState.isEmpty()) 
{
    -				restoredState = null;
    -			}
&lt;p&gt;     		} else &lt;/p&gt;
{
     			LOG.info(&quot;No restore state for FlinkKafkaConsumer.&quot;);
     		}
&lt;p&gt;     	}&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     	@Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void snapshotState(FunctionSnapshotContext context) throws Exception {&lt;br/&gt;
    +	public final void snapshotState(FunctionSnapshotContext context) throws Exception {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    ```&lt;br/&gt;
    the verrsion-specific implementations for FlinkKafkaConsumerBase may override that and have incorrect implementations, where as our tests would never realize it.&lt;br/&gt;
    ```&lt;br/&gt;
    @tzulitai why would this be a concern for FlinkKafkaConsumerBase. if version-specific implementations have bugs, they should have test to catch and prevent bugs. We do need the capability to override the snapshot method to no-op. what would be your suggested alternative?&lt;/p&gt;</comment>
                            <comment id="16096204" author="githubbot" created="Fri, 21 Jul 2017 12:51:23 +0000"  >&lt;p&gt;Github user aljoscha commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4357#discussion_r128754182&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4357#discussion_r128754182&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumerBase.java &amp;#8212;&lt;br/&gt;
    @@ -517,16 +519,13 @@ public void initializeState(FunctionInitializationContext context) throws Except&lt;br/&gt;
     					LOG.debug(&quot;Using the following offsets: {}&quot;, restoredState);&lt;br/&gt;
     				}&lt;br/&gt;
     			}&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (restoredState != null &amp;amp;&amp;amp; restoredState.isEmpty()) 
{
    -				restoredState = null;
    -			}
&lt;p&gt;     		} else &lt;/p&gt;
{
     			LOG.info(&quot;No restore state for FlinkKafkaConsumer.&quot;);
     		}
&lt;p&gt;     	}&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     	@Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void snapshotState(FunctionSnapshotContext context) throws Exception {&lt;br/&gt;
    +	public final void snapshotState(FunctionSnapshotContext context) throws Exception {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    The concern here is that not making the methods final makes it easy for contributors to accidentally override them. We don&apos;t have specific unit tests for the 0.9 `FlinkKafkaConsumer` or the 0.10 `FlinkKafkaConsumer` and only test the base `FlinkKafkaConsumerBase`. This is OK, as long as specific implementations don&apos;t override important methods. If the `FlinkKafkaConsumer090` did override the `snapshot()`/`restore()` methods, for example, no tests would catch this.&lt;/p&gt;

&lt;p&gt;    @tzulitai I don&apos;t want to discuss here about these methods to much since we want to get the fixes in for release 1.3.2. A way around the problem is to turn the `FlinkKafkaConsumerBaseTest` into an abstract `FlinkKafkaConsumerBaseTestBase` that has an abstract method `createTestingConsumer(List&amp;lt;KafkaTopicPartition&amp;gt; mockFetchedPartitions)` that creates a &quot;dummy&quot; consumer for a specific Kafka version. Then we would have individual `FlinkKafkaConsumer09Test`, `FlinkKafkaConsumer010Test` and so on that derive form the abstract test base and just implement the method for creating the testing consumer.&lt;/p&gt;

&lt;p&gt;    What do you think?&lt;/p&gt;</comment>
                            <comment id="16096210" author="githubbot" created="Fri, 21 Jul 2017 12:58:37 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4357#discussion_r128755399&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4357#discussion_r128755399&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumerBase.java &amp;#8212;&lt;br/&gt;
    @@ -517,16 +519,13 @@ public void initializeState(FunctionInitializationContext context) throws Except&lt;br/&gt;
     					LOG.debug(&quot;Using the following offsets: {}&quot;, restoredState);&lt;br/&gt;
     				}&lt;br/&gt;
     			}&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (restoredState != null &amp;amp;&amp;amp; restoredState.isEmpty()) 
{
    -				restoredState = null;
    -			}
&lt;p&gt;     		} else &lt;/p&gt;
{
     			LOG.info(&quot;No restore state for FlinkKafkaConsumer.&quot;);
     		}
&lt;p&gt;     	}&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     	@Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void snapshotState(FunctionSnapshotContext context) throws Exception {&lt;br/&gt;
    +	public final void snapshotState(FunctionSnapshotContext context) throws Exception {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    @aljoscha I think that&apos;s a good approach to avoid making the methods final for now. Would also be a good opportunity to clean up the `FlinkKafkaConsumerBaseTest` test a bit &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="16097144" author="githubbot" created="Sat, 22 Jul 2017 07:14:56 +0000"  >&lt;p&gt;Github user aljoscha commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4357&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4357&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I reviewed this a last time with Stephan, offline, and we think we should go with this version. Overriding the snapshot methods is most likely not necessary anymore but if it is we will &quot;un-finalize&quot; them again before release.&lt;/p&gt;

&lt;p&gt;    @tzulitai Do you want to have the honours of merging? &#128515; &lt;/p&gt;
</comment>
                            <comment id="16097254" author="githubbot" created="Sat, 22 Jul 2017 11:32:56 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4357&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4357&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @aljoscha sure, merging this now &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="16097283" author="tzulitai" created="Sat, 22 Jul 2017 12:06:13 +0000"  >&lt;p&gt;Fixed for &lt;tt&gt;release-1.3&lt;/tt&gt; in 3369cfe200bf2cb7ed04caf19d8599075e4cfe21.&lt;br/&gt;
This ticket should only be closed after related tests are merged to master.&lt;/p&gt;</comment>
                            <comment id="16097740" author="githubbot" created="Sun, 23 Jul 2017 18:08:56 +0000"  >&lt;p&gt;Github user zentol commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4302&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4302&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @tzulitai You can merge this.&lt;/p&gt;</comment>
                            <comment id="16097875" author="githubbot" created="Mon, 24 Jul 2017 02:20:27 +0000"  >&lt;p&gt;Github user tzulitai closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4357&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4357&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16097879" author="githubbot" created="Mon, 24 Jul 2017 02:25:48 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4302&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4302&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Merging &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;/p&gt;</comment>
                            <comment id="16097881" author="githubbot" created="Mon, 24 Jul 2017 02:29:14 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4301&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4301&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    This was merged via #4357. Closing ..&lt;/p&gt;</comment>
                            <comment id="16097882" author="githubbot" created="Mon, 24 Jul 2017 02:29:15 +0000"  >&lt;p&gt;Github user tzulitai closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4301&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4301&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16097916" author="githubbot" created="Mon, 24 Jul 2017 03:56:17 +0000"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4302&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4302&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16098092" author="githubbot" created="Mon, 24 Jul 2017 08:48:08 +0000"  >&lt;p&gt;GitHub user tzulitai opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4387&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4387&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-7143&quot; title=&quot;Partition assignment for Kafka consumer is not stable&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-7143&quot;&gt;&lt;del&gt;FLINK-7143&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; Forward ports of new Kafka tests to master&lt;/p&gt;

&lt;p&gt;    This PR forward ports all new tests added in #4357 to `master`, so that the behaviors is correctly guarded there also.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Changes&lt;br/&gt;
    1. Introduce `KafkaTopicPartitionAssigner` class to master branch, which strictly defines the partition assignment contract (discussed in #4301).&lt;br/&gt;
    2. Port rescaling unit test. Note that some tested behaviors needed to be changed due to the differences between 1.3 and 1.4 for the Kafka consumer.&lt;br/&gt;
    3. Make checkpoint methods final.&lt;br/&gt;
    4. (new change, not a port) Remove invalid `checkRestoredNullCheckpointWhenFetcherNotReady` test, which was testing a legacy behavior of the consumer&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/tzulitai/flink&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/tzulitai/flink&lt;/a&gt; kafka-forward-ports&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4387.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4387.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #4387&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 7295777984084fc470edaee44e1bc32881409665&lt;br/&gt;
Author: Tzu-Li (Gordon) Tai &amp;lt;tzulitai@apache.org&amp;gt;&lt;br/&gt;
Date:   2017-07-24T07:00:16Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-7143&quot; title=&quot;Partition assignment for Kafka consumer is not stable&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-7143&quot;&gt;&lt;del&gt;FLINK-7143&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; Introduce KafkaTopicPartitionAssigner with stricter assignment contracts&lt;/p&gt;

&lt;p&gt;    This commit refactors the local partition assignment logic to be located&lt;br/&gt;
    in a strict contract-defining method, to make it explicit of the&lt;br/&gt;
    expected partition to subtask assignment without relying solely on&lt;br/&gt;
    hashCode&apos;s of kafka partitions.&lt;/p&gt;

&lt;p&gt;commit 72a8c42505ba791f51001129a08744b104a171d7&lt;br/&gt;
Author: Aljoscha Krettek &amp;lt;aljoscha.krettek@gmail.com&amp;gt;&lt;br/&gt;
Date:   2017-07-18T09:57:46Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-7143&quot; title=&quot;Partition assignment for Kafka consumer is not stable&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-7143&quot;&gt;&lt;del&gt;FLINK-7143&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; Add test for Kafka Consumer rescaling&lt;/p&gt;

&lt;p&gt;    This verifies that the consumer always correctly knows whether it is&lt;br/&gt;
    restored or not and is not affected by changes in the partitions as&lt;br/&gt;
    reported by Kafka.&lt;/p&gt;

&lt;p&gt;    Previously, operator state reshuffling could lead to partitions being&lt;br/&gt;
    subscribed to multiple times.&lt;/p&gt;

&lt;p&gt;commit 100936c7ab0b7bca4dab10143aa184dc31e2fd46&lt;br/&gt;
Author: Aljoscha Krettek &amp;lt;aljoscha.krettek@gmail.com&amp;gt;&lt;br/&gt;
Date:   2017-07-18T08:35:54Z&lt;/p&gt;

&lt;p&gt;    &lt;span class=&quot;error&quot;&gt;&amp;#91;hotfix&amp;#93;&lt;/span&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; Make checkpoint methods final in KafkaConsumerBase&lt;/p&gt;

&lt;p&gt;    This prevents concrete Kafka Source implementations from accidentally&lt;br/&gt;
    overriding the checkpointing methods. This would be problematic when not&lt;br/&gt;
    providing tests. We test the checkpoint methods of the ConsumerBase but&lt;br/&gt;
    derived methods would not be tested.&lt;/p&gt;

&lt;p&gt;commit b0cf8779b76b5fe94beb4ffb6ba9adad16280be6&lt;br/&gt;
Author: Tzu-Li (Gordon) Tai &amp;lt;tzulitai@apache.org&amp;gt;&lt;br/&gt;
Date:   2017-07-24T08:34:37Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-7248&quot; title=&quot;Invalid checkRestoredNullCheckpointWhenFetcherNotReady test in FlinkKafkaConsumerBaseTest&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-7248&quot;&gt;&lt;del&gt;FLINK-7248&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka, tests&amp;#93;&lt;/span&gt; Remove invalid checkRestoredNullCheckpointWhenFetcherNotReady test&lt;/p&gt;

&lt;p&gt;    This test is an invalid remnant from recent major Kafka consumer&lt;br/&gt;
    refactorings. The actual behaviour is covered by&lt;br/&gt;
    checkRestoredCheckpointWhenFetcherNotReady. When the fetcher is not yet&lt;br/&gt;
    ready and exposed and a checkpoint happens, we fallback to using any&lt;br/&gt;
    restored state as the checkpoint.&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="16098099" author="aljoscha" created="Mon, 24 Jul 2017 08:54:49 +0000"  >&lt;p&gt;Fixed on release-1.3 in&lt;br/&gt;
6e0d90ccfd9c457fd99add5833cd7bf6c976d6b8&lt;br/&gt;
b0564322c61168c3a7bb23bdca3db0648454a691&lt;/p&gt;</comment>
                            <comment id="16104949" author="githubbot" created="Fri, 28 Jul 2017 13:33:02 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4387&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4387&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Merging this now, as the changes were already reviewed when applying them onto `release-1.3`.&lt;/p&gt;</comment>
                            <comment id="16104998" author="githubbot" created="Fri, 28 Jul 2017 13:54:34 +0000"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4387&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4387&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16105102" author="tzulitai" created="Fri, 28 Jul 2017 15:23:39 +0000"  >&lt;p&gt;Tests are now also ported to master, via e111d7730ec6032dc14579bd274e7822f7176e39.&lt;br/&gt;
Closing this!&lt;/p&gt;</comment>
                            <comment id="16112514" author="aljoscha" created="Thu, 3 Aug 2017 09:54:35 +0000"  >&lt;p&gt;Re-open to adapt issue test.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 15 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3hcgv:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>