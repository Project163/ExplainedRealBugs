<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 20:25:13 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-4702] Kafka consumer must commit offsets asynchronously</title>
                <link>https://issues.apache.org/jira/browse/FLINK-4702</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;p&gt;The offset commit calls to Kafka may occasionally take very long.&lt;br/&gt;
In that case, the &lt;tt&gt;notifyCheckpointComplete()&lt;/tt&gt; method blocks for long and the KafkaConsumer cannot make progress and cannot perform checkpoints.&lt;/p&gt;

&lt;p&gt;Kafka 0.9+ have methods to commit asynchronously.&lt;br/&gt;
We should use those and make sure no more than one commit is concurrently in progress, to that commit requests do not pile up.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13008031">FLINK-4702</key>
            <summary>Kafka consumer must commit offsets asynchronously</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="sewen">Stephan Ewen</assignee>
                                    <reporter username="sewen">Stephan Ewen</reporter>
                        <labels>
                    </labels>
                <created>Tue, 27 Sep 2016 19:08:07 +0000</created>
                <updated>Fri, 30 Sep 2016 14:48:52 +0000</updated>
                            <resolved>Fri, 30 Sep 2016 14:48:51 +0000</resolved>
                                    <version>1.1.2</version>
                                    <fixVersion>1.1.3</fixVersion>
                    <fixVersion>1.2.0</fixVersion>
                                    <component>Connectors / Kafka</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="15527146" author="githubbot" created="Tue, 27 Sep 2016 19:12:48 +0000"  >&lt;p&gt;GitHub user StephanEwen opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2559&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2559&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-4702&quot; title=&quot;Kafka consumer must commit offsets asynchronously&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-4702&quot;&gt;&lt;del&gt;FLINK-4702&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka connector&amp;#93;&lt;/span&gt; Commit offets to Kafka asynchronously&lt;/p&gt;

&lt;p&gt;    The offset commit calls to Kafka may occasionally take very long. In that case, the notifyCheckpointComplete() method blocks for long and the KafkaConsumer cannot make progress and cannot perform checkpoints.&lt;/p&gt;

&lt;p&gt;    This pull request changes the offset committing to use Kafka&apos;s `commitAsync()` method.&lt;br/&gt;
    It also makes sure that no more than one commit is concurrently in progress, to that commit requests do not pile up.&lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/StephanEwen/incubator-flink&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/StephanEwen/incubator-flink&lt;/a&gt; kafka_commit_async&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2559.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2559.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #2559&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit eafba8600863c18e09397366485bcfc6ff44960f&lt;br/&gt;
Author: Stephan Ewen &amp;lt;sewen@apache.org&amp;gt;&lt;br/&gt;
Date:   2016-09-27T18:59:35Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-4702&quot; title=&quot;Kafka consumer must commit offsets asynchronously&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-4702&quot;&gt;&lt;del&gt;FLINK-4702&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka connector&amp;#93;&lt;/span&gt; Commit offets to Kafka asynchronously&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="15529398" author="githubbot" created="Wed, 28 Sep 2016 12:02:39 +0000"  >&lt;p&gt;Github user StephanEwen commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2559&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2559&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @robert and @tzulitai What is your take on this?&lt;/p&gt;</comment>
                            <comment id="15529448" author="githubbot" created="Wed, 28 Sep 2016 12:28:15 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2559&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2559&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Just had a look at the API of `commitAsync`, and it seems like the committed offsets back to Kafka through this API (likewise for `commitSync`) need to be `lastProcessedMessageOffset + 1` (&lt;a href=&quot;https://kafka.apache.org/090/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#commitAsync(java.util.Map,%20org.apache.kafka.clients.consumer.OffsetCommitCallback)&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://kafka.apache.org/090/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#commitAsync(java.util.Map,%20org.apache.kafka.clients.consumer.OffsetCommitCallback)&lt;/a&gt;(&lt;a href=&quot;https://kafka.apache.org/090/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#commitAsync(java.util.Map,%20org.apache.kafka.clients.consumer.OffsetCommitCallback&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://kafka.apache.org/090/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#commitAsync(java.util.Map,%20org.apache.kafka.clients.consumer.OffsetCommitCallback&lt;/a&gt;))).&lt;/p&gt;

&lt;p&gt;    This mainly effects that when starting from group offsets in Kafka, `FlinkKafkaConsumer09` currently starts from the wrong offset. There&apos;s a separate JIRA for this bug: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-4618&quot; title=&quot;FlinkKafkaConsumer09 should start from the next record on startup from offsets in Kafka&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-4618&quot;&gt;&lt;del&gt;FLINK-4618&lt;/del&gt;&lt;/a&gt;(&lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-4618&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/FLINK-4618&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;    Another contributor had already picked up &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-4618&quot; title=&quot;FlinkKafkaConsumer09 should start from the next record on startup from offsets in Kafka&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-4618&quot;&gt;&lt;del&gt;FLINK-4618&lt;/del&gt;&lt;/a&gt;, so I&apos;d say it&apos;s ok to leave this PR as it is. I&apos;ll help check on &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-4618&quot; title=&quot;FlinkKafkaConsumer09 should start from the next record on startup from offsets in Kafka&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-4618&quot;&gt;&lt;del&gt;FLINK-4618&lt;/del&gt;&lt;/a&gt; progress and make sure it gets merged after this PR.&lt;/p&gt;

&lt;p&gt;    Minus the above, this looks good to me. +1&lt;/p&gt;</comment>
                            <comment id="15529450" author="githubbot" created="Wed, 28 Sep 2016 12:28:48 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2559#discussion_r80903481&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2559#discussion_r80903481&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09Fetcher.java &amp;#8212;&lt;br/&gt;
    @@ -86,6 +90,9 @@&lt;br/&gt;
     	/** Flag to mark the main work loop as alive */&lt;br/&gt;
     	private volatile boolean running = true;&lt;/p&gt;

&lt;p&gt;    +	/** Flag indicating whether a commit of offsets to Kafka it currently happening */&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    nit: it --&amp;gt; &quot;is&quot;?&lt;/p&gt;</comment>
                            <comment id="15529455" author="githubbot" created="Wed, 28 Sep 2016 12:30:59 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2559&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2559&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Btw, just curious, does 0.8 Kafka connector have the same issue with sync committing? I haven&apos;t looked into the code for this, but just wondering if we need a ticket for 0.8 too.&lt;/p&gt;</comment>
                            <comment id="15529483" author="githubbot" created="Wed, 28 Sep 2016 12:42:56 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2559&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2559&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @StephanEwen I think you&apos;ve tagged the wrong Github ID for Robert &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15529493" author="githubbot" created="Wed, 28 Sep 2016 12:48:02 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2559#discussion_r80906814&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2559#discussion_r80906814&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09Fetcher.java &amp;#8212;&lt;br/&gt;
    @@ -301,4 +316,16 @@ public void commitSpecificOffsetsToKafka(Map&amp;lt;KafkaTopicPartition, Long&amp;gt; offsets)&lt;br/&gt;
     		}&lt;br/&gt;
     		return result;&lt;br/&gt;
     	}&lt;br/&gt;
    +&lt;br/&gt;
    +	private class CommitCallback implements OffsetCommitCallback {&lt;br/&gt;
    +&lt;br/&gt;
    +		@Override&lt;br/&gt;
    +		public void onComplete(Map&amp;lt;TopicPartition, OffsetAndMetadata&amp;gt; offsets, Exception exception) {&lt;br/&gt;
    +			commitInProgress = false;&lt;br/&gt;
    +&lt;br/&gt;
    +			if (exception != null) {&lt;br/&gt;
    +				LOG.warn(&quot;Committing offsets to Kafka failed. This does not compromise Flink&apos;s checkpoints&quot;, exception);&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    The exception message isn&apos;t included in the log warning.&lt;/p&gt;</comment>
                            <comment id="15529502" author="githubbot" created="Wed, 28 Sep 2016 12:50:56 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2559#discussion_r80907326&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2559#discussion_r80907326&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09Fetcher.java &amp;#8212;&lt;br/&gt;
    @@ -285,7 +293,14 @@ public void commitSpecificOffsetsToKafka(Map&amp;lt;KafkaTopicPartition, Long&amp;gt; offsets)&lt;/p&gt;

&lt;p&gt;     		if (this.consumer != null) {&lt;br/&gt;
     			synchronized (consumerLock) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;this.consumer.commitSync(offsetsToCommit);&lt;br/&gt;
    +				if (!commitInProgress) 
{
    +					commitInProgress = true;
    +					this.consumer.commitAsync(offsetsToCommit, offsetCommitCallback);
    +				}
&lt;p&gt;    +				else {&lt;br/&gt;
    +					LOG.warn(&quot;Committing previous checkpoint&apos;s offsets to Kafka not completed. &quot; +&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    If the user sets a relatively short checkpoint interval, will this be flooding log?&lt;/p&gt;</comment>
                            <comment id="15529504" author="githubbot" created="Wed, 28 Sep 2016 12:51:32 +0000"  >&lt;p&gt;Github user StephanEwen commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2559&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2559&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Thanks @tzulitai for looking at this. I will leave the offset then as it is (fixed via followup) and &lt;/p&gt;

&lt;p&gt;    The Kafka 0.8 connector needs a similar change. This here is encountered by a user, so I wanted to get the 0.9 fix in faster. Will do a follow-up for Kafka 0.8. Will also correct the issue tag &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;    I have no good idea how to test this, though, so any thoughts there are welcome!&lt;/p&gt;</comment>
                            <comment id="15529532" author="githubbot" created="Wed, 28 Sep 2016 13:05:04 +0000"  >&lt;p&gt;Github user StephanEwen commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2559#discussion_r80909904&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2559#discussion_r80909904&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09Fetcher.java &amp;#8212;&lt;br/&gt;
    @@ -285,7 +293,14 @@ public void commitSpecificOffsetsToKafka(Map&amp;lt;KafkaTopicPartition, Long&amp;gt; offsets)&lt;/p&gt;

&lt;p&gt;     		if (this.consumer != null) {&lt;br/&gt;
     			synchronized (consumerLock) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;this.consumer.commitSync(offsetsToCommit);&lt;br/&gt;
    +				if (!commitInProgress) 
{
    +					commitInProgress = true;
    +					this.consumer.commitAsync(offsetsToCommit, offsetCommitCallback);
    +				}
&lt;p&gt;    +				else {&lt;br/&gt;
    +					LOG.warn(&quot;Committing previous checkpoint&apos;s offsets to Kafka not completed. &quot; +&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Possibly yes. But on the other hand, this should be pretty visible if it happens.&lt;br/&gt;
    I would expect that with proper options to participate in group checkpoint committing, most Flink jobs run without committing to Kafka/ZooKeeper.&lt;/p&gt;</comment>
                            <comment id="15529580" author="githubbot" created="Wed, 28 Sep 2016 13:23:51 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2559&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2559&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @StephanEwen &lt;br/&gt;
    On a second look, I think the `commitSpecificOffsetsToKafka` method was designed to commit synchronously in the first place. `AbstractFetcher` holds a Map of all current pending offsets for committing by checkpointID, and on every `notifyCheckpointComplete` the offsets are removed from the Map before `commitSpecificOffsetsToKafka` is called.&lt;/p&gt;

&lt;p&gt;    So, for async committing, I think we need to remove cleaning up the offsets in `AbstractFetcher#notifyCheckpointComplete()` and instead clean them up in a new separate callback handle method in `AbstractFetcher`.&lt;/p&gt;</comment>
                            <comment id="15529596" author="githubbot" created="Wed, 28 Sep 2016 13:27:08 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2559#discussion_r80914495&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2559#discussion_r80914495&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09Fetcher.java &amp;#8212;&lt;br/&gt;
    @@ -285,7 +293,14 @@ public void commitSpecificOffsetsToKafka(Map&amp;lt;KafkaTopicPartition, Long&amp;gt; offsets)&lt;/p&gt;

&lt;p&gt;     		if (this.consumer != null) {&lt;br/&gt;
     			synchronized (consumerLock) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;this.consumer.commitSync(offsetsToCommit);&lt;br/&gt;
    +				if (!commitInProgress) 
{
    +					commitInProgress = true;
    +					this.consumer.commitAsync(offsetsToCommit, offsetCommitCallback);
    +				}
&lt;p&gt;    +				else {&lt;br/&gt;
    +					LOG.warn(&quot;Committing previous checkpoint&apos;s offsets to Kafka not completed. &quot; +&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    I agree, makes sense.&lt;/p&gt;</comment>
                            <comment id="15529655" author="githubbot" created="Wed, 28 Sep 2016 13:44:39 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2559&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2559&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Seems like currently only the 0.8 Kafka connector have tests related to offset committing (in `Kafka08ITCase`).&lt;/p&gt;

&lt;p&gt;    My two cents for testing this for now is that a IT test for correct offset committing back to Kafka in the 0.9 connector is sufficient (can take a look at `Kafka08ITCase#testOffsetInZookeeper`, but replacing `ZookeeperOffsetHandler` with the new `KafkaConsumer` methods). &lt;/p&gt;</comment>
                            <comment id="15532184" author="githubbot" created="Thu, 29 Sep 2016 08:40:46 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2559#discussion_r81089161&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2559#discussion_r81089161&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09Fetcher.java &amp;#8212;&lt;br/&gt;
    @@ -301,4 +316,16 @@ public void commitSpecificOffsetsToKafka(Map&amp;lt;KafkaTopicPartition, Long&amp;gt; offsets)&lt;br/&gt;
     		}&lt;br/&gt;
     		return result;&lt;br/&gt;
     	}&lt;br/&gt;
    +&lt;br/&gt;
    +	private class CommitCallback implements OffsetCommitCallback {&lt;br/&gt;
    +&lt;br/&gt;
    +		@Override&lt;br/&gt;
    +		public void onComplete(Map&amp;lt;TopicPartition, OffsetAndMetadata&amp;gt; offsets, Exception exception) {&lt;br/&gt;
    +			commitInProgress = false;&lt;br/&gt;
    +&lt;br/&gt;
    +			if (exception != null) {&lt;br/&gt;
    +				LOG.warn(&quot;Committing offsets to Kafka failed. This does not compromise Flink&apos;s checkpoints&quot;, exception);&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Oops, this is actually correct, sorry.&lt;/p&gt;</comment>
                            <comment id="15532771" author="githubbot" created="Thu, 29 Sep 2016 13:25:05 +0000"  >&lt;p&gt;Github user StephanEwen commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2559&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2559&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @tzulitai Thanks for thorough review!&lt;/p&gt;

&lt;p&gt;    I don&apos;t understand the problem why the `commitSpecificOffsetsToKafka` method is designed to commit synchronously. The `FlinkKafkaConsumerBase` has the pending checkpoints (I think that is what you refer to). It removes the HashMap of &quot;offsets to commit&quot; from the `pendingCheckpoints` Map synchronously, before even calling the fetcher to commit.&lt;br/&gt;
    After that, it looks to me like it does not make a difference how that Map &quot;offsets to commit&quot; is used (sync or async)...&lt;/p&gt;
</comment>
                            <comment id="15532900" author="githubbot" created="Thu, 29 Sep 2016 14:15:24 +0000"  >&lt;p&gt;Github user StephanEwen commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2559&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2559&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Actually, just discovered that the problem is different all together.&lt;/p&gt;

&lt;p&gt;    While the KafkaConsumer is polling for new data (with a timeout), it holds the consumer lock. If no data comes in Kafka, the lock is not released before the poll timeout is over.&lt;br/&gt;
    During that time, neither a &quot;commitSync&quot; nor &quot;commitAsync&quot; call can be fired off. The `notifyCheckpointComplete` method hence blocks until the poll timeout is over and the lock is released.&lt;/p&gt;

&lt;p&gt;    We can fix this by making sure that the consumer is &quot;woken up&quot; to release the lock, and by making sure the lock acquisition is fair, so the committer will get it next.&lt;/p&gt;

&lt;p&gt;    For the sake of releasing the lock fast in the committer method, it should still be an asynchronous commit.&lt;/p&gt;</comment>
                            <comment id="15533206" author="githubbot" created="Thu, 29 Sep 2016 16:11:36 +0000"  >&lt;p&gt;GitHub user StephanEwen opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2574&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2574&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-4702&quot; title=&quot;Kafka consumer must commit offsets asynchronously&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-4702&quot;&gt;&lt;del&gt;FLINK-4702&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka connector&amp;#93;&lt;/span&gt; Commit offsets to Kafka asynchronously and don&apos;t block on polls&lt;/p&gt;

&lt;p&gt;    This fix is quite critical!&lt;/p&gt;

&lt;p&gt;    While the KafkaConsumer is polling for new data (with a timeout), it holds the consumer lock. If no data comes in Kafka, the lock is not released before the poll timeout is over.&lt;/p&gt;

&lt;p&gt;    During that time, no offset commit can make progress, because it needs the consumer lock. The `notifyCheckpointComplete()` method of the Kafka Consumer hence blocks until the poll timeout is over and the lock is released. For low-throughput Kafka Topics, this can cause wildly long checkpoint delays.&lt;/p&gt;

&lt;p&gt;    This changes `notifyCheckpointComplete()` to only &quot;schedule&quot; offsets to be committed, while the main fetcher thread actually kick off the asynchronous offset commits. That way, there is no interference between the `notifyCheckpointComplete()` method (which is executed under checkpoint lock) and the consumer lock.&lt;/p&gt;

&lt;p&gt;    In fact, the only KafkaConsumer method accessed concurrently to the main fetcher thread is `wakeup()` which is actually thread-safe (where the rest of the KafkaConsumer is not). The consumer lock was hence completely removed.&lt;/p&gt;


&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/StephanEwen/incubator-flink&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/StephanEwen/incubator-flink&lt;/a&gt; kafka_09_fix&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2574.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2574.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #2574&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 0846fd907db7d52d7e5fb7d704c5e1c13462e331&lt;br/&gt;
Author: Stephan Ewen &amp;lt;sewen@apache.org&amp;gt;&lt;br/&gt;
Date:   2016-09-29T16:09:51Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-4702&quot; title=&quot;Kafka consumer must commit offsets asynchronously&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-4702&quot;&gt;&lt;del&gt;FLINK-4702&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka connector&amp;#93;&lt;/span&gt; Commit offsets to Kafka asynchronously and don&apos;t block on polls&lt;/p&gt;

&lt;p&gt;    Letting the Kafka commit block on polls means that &apos;notifyCheckpointComplete()&apos; may take&lt;br/&gt;
    very long. This is mostly relevant for low-throughput Kafka topics.&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="15533210" author="githubbot" created="Thu, 29 Sep 2016 16:12:42 +0000"  >&lt;p&gt;Github user StephanEwen commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2559&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2559&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Closing this for #2574 &lt;/p&gt;</comment>
                            <comment id="15533211" author="githubbot" created="Thu, 29 Sep 2016 16:12:42 +0000"  >&lt;p&gt;Github user StephanEwen closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2559&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2559&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15533216" author="githubbot" created="Thu, 29 Sep 2016 16:13:55 +0000"  >&lt;p&gt;Github user StephanEwen commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2574&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2574&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @tzulitai @rmetzger We need to make sure that the Kafka 0.10 code picks up this change and the test case.&lt;/p&gt;</comment>
                            <comment id="15535141" author="githubbot" created="Fri, 30 Sep 2016 05:59:55 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2574#discussion_r81282102&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2574#discussion_r81282102&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/Kafka09FetcherTest.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,300 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.core.testutils.MultiShotLatch;&lt;br/&gt;
    +import org.apache.flink.core.testutils.OneShotLatch;&lt;br/&gt;
    +import org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext;&lt;br/&gt;
    +import org.apache.flink.streaming.api.operators.StreamingRuntimeContext;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internal.Kafka09Fetcher;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedDeserializationSchema;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedDeserializationSchemaWrapper;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.SimpleStringSchema;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.ConsumerRecords;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.KafkaConsumer;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.OffsetAndMetadata;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.OffsetCommitCallback;&lt;br/&gt;
    +import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.junit.Test;&lt;br/&gt;
    +import org.junit.runner.RunWith;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.mockito.Mockito;&lt;br/&gt;
    +import org.mockito.invocation.InvocationOnMock;&lt;br/&gt;
    +import org.mockito.stubbing.Answer;&lt;br/&gt;
    +import org.powermock.core.classloader.annotations.PrepareForTest;&lt;br/&gt;
    +import org.powermock.modules.junit4.PowerMockRunner;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util.Collections;&lt;br/&gt;
    +import java.util.HashMap;&lt;br/&gt;
    +import java.util.List;&lt;br/&gt;
    +import java.util.Map;&lt;br/&gt;
    +import java.util.Map.Entry;&lt;br/&gt;
    +import java.util.Properties;&lt;br/&gt;
    +import java.util.concurrent.BlockingQueue;&lt;br/&gt;
    +import java.util.concurrent.LinkedBlockingQueue;&lt;br/&gt;
    +import java.util.concurrent.atomic.AtomicReference;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.junit.Assert.assertEquals;&lt;br/&gt;
    +import static org.junit.Assert.assertFalse;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.mockito.Mockito.any;&lt;br/&gt;
    +import static org.mockito.Mockito.anyLong;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.doAnswer;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.mock;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.when;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.whenNew;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * Unit tests for the &lt;/p&gt;
{@link Kafka09Fetcher}
&lt;p&gt;.&lt;br/&gt;
    + */&lt;br/&gt;
    +@RunWith(PowerMockRunner.class)&lt;br/&gt;
    +@PrepareForTest(Kafka09Fetcher.class)&lt;br/&gt;
    +public class Kafka09FetcherTest {&lt;br/&gt;
    +&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testCommitDoesNotBlock() throws Exception {&lt;br/&gt;
    +&lt;br/&gt;
    +		// test data&lt;br/&gt;
    +		final KafkaTopicPartition testPartition = new KafkaTopicPartition(&quot;test&quot;, 42);&lt;br/&gt;
    +		final Map&amp;lt;KafkaTopicPartition, Long&amp;gt; testCommitData = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
    +		testCommitData.put(testPartition, 11L);&lt;br/&gt;
    +&lt;br/&gt;
    +		// to synchronize when the consumer is in its blocking method&lt;br/&gt;
    +		final OneShotLatch sync = new OneShotLatch();&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- the mock consumer with blocking poll calls ----&lt;br/&gt;
    +		final MultiShotLatch blockerLatch = new MultiShotLatch();&lt;br/&gt;
    +		&lt;br/&gt;
    +		KafkaConsumer&amp;lt;?, ?&amp;gt; mockConsumer = mock(KafkaConsumer.class);&lt;br/&gt;
    +		when(mockConsumer.poll(anyLong())).thenAnswer(new Answer&amp;lt;ConsumerRecords&amp;lt;?, ?&amp;gt;&amp;gt;() {&lt;br/&gt;
    +			&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public ConsumerRecords&amp;lt;?, ?&amp;gt; answer(InvocationOnMock invocation) throws InterruptedException {&lt;br/&gt;
    +				sync.trigger();&lt;br/&gt;
    +				blockerLatch.await();&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    We are not ensuring that `blockLatch` is returned here, correct? Like my comment above, perhaps we should check that to to ensure that `wakeup` is called in `commitSpecificOffsetsToKafka`.&lt;/p&gt;</comment>
                            <comment id="15535142" author="githubbot" created="Fri, 30 Sep 2016 05:59:55 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2574#discussion_r81277067&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2574#discussion_r81277067&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/Kafka09FetcherTest.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,300 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.core.testutils.MultiShotLatch;&lt;br/&gt;
    +import org.apache.flink.core.testutils.OneShotLatch;&lt;br/&gt;
    +import org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext;&lt;br/&gt;
    +import org.apache.flink.streaming.api.operators.StreamingRuntimeContext;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internal.Kafka09Fetcher;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedDeserializationSchema;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedDeserializationSchemaWrapper;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.SimpleStringSchema;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.ConsumerRecords;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.KafkaConsumer;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.OffsetAndMetadata;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.OffsetCommitCallback;&lt;br/&gt;
    +import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.junit.Test;&lt;br/&gt;
    +import org.junit.runner.RunWith;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.mockito.Mockito;&lt;br/&gt;
    +import org.mockito.invocation.InvocationOnMock;&lt;br/&gt;
    +import org.mockito.stubbing.Answer;&lt;br/&gt;
    +import org.powermock.core.classloader.annotations.PrepareForTest;&lt;br/&gt;
    +import org.powermock.modules.junit4.PowerMockRunner;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util.Collections;&lt;br/&gt;
    +import java.util.HashMap;&lt;br/&gt;
    +import java.util.List;&lt;br/&gt;
    +import java.util.Map;&lt;br/&gt;
    +import java.util.Map.Entry;&lt;br/&gt;
    +import java.util.Properties;&lt;br/&gt;
    +import java.util.concurrent.BlockingQueue;&lt;br/&gt;
    +import java.util.concurrent.LinkedBlockingQueue;&lt;br/&gt;
    +import java.util.concurrent.atomic.AtomicReference;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.junit.Assert.assertEquals;&lt;br/&gt;
    +import static org.junit.Assert.assertFalse;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.mockito.Mockito.any;&lt;br/&gt;
    +import static org.mockito.Mockito.anyLong;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.doAnswer;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.mock;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.when;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.whenNew;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * Unit tests for the &lt;/p&gt;
{@link Kafka09Fetcher}
&lt;p&gt;.&lt;br/&gt;
    + */&lt;br/&gt;
    +@RunWith(PowerMockRunner.class)&lt;br/&gt;
    +@PrepareForTest(Kafka09Fetcher.class)&lt;br/&gt;
    +public class Kafka09FetcherTest {&lt;br/&gt;
    +&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testCommitDoesNotBlock() throws Exception {&lt;br/&gt;
    +&lt;br/&gt;
    +		// test data&lt;br/&gt;
    +		final KafkaTopicPartition testPartition = new KafkaTopicPartition(&quot;test&quot;, 42);&lt;br/&gt;
    +		final Map&amp;lt;KafkaTopicPartition, Long&amp;gt; testCommitData = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
    +		testCommitData.put(testPartition, 11L);&lt;br/&gt;
    +&lt;br/&gt;
    +		// to synchronize when the consumer is in its blocking method&lt;br/&gt;
    +		final OneShotLatch sync = new OneShotLatch();&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- the mock consumer with blocking poll calls ----&lt;br/&gt;
    +		final MultiShotLatch blockerLatch = new MultiShotLatch();&lt;br/&gt;
    +		&lt;br/&gt;
    +		KafkaConsumer&amp;lt;?, ?&amp;gt; mockConsumer = mock(KafkaConsumer.class);&lt;br/&gt;
    +		when(mockConsumer.poll(anyLong())).thenAnswer(new Answer&amp;lt;ConsumerRecords&amp;lt;?, ?&amp;gt;&amp;gt;() {&lt;br/&gt;
    +			&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public ConsumerRecords&amp;lt;?, ?&amp;gt; answer(InvocationOnMock invocation) throws InterruptedException &lt;/p&gt;
{
    +				sync.trigger();
    +				blockerLatch.await();
    +				return ConsumerRecords.empty();
    +			}
&lt;p&gt;    +		});&lt;br/&gt;
    +&lt;br/&gt;
    +		doAnswer(new Answer&amp;lt;Void&amp;gt;() {&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public Void answer(InvocationOnMock invocation) &lt;/p&gt;
{
    +				blockerLatch.trigger();
    +				return null;
    +			}
&lt;p&gt;    +		}).when(mockConsumer).wakeup();&lt;br/&gt;
    +&lt;br/&gt;
    +		// make sure the fetcher creates the mock consumer&lt;br/&gt;
    +		whenNew(KafkaConsumer.class).withAnyArguments().thenReturn(mockConsumer);&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- create the test fetcher -----&lt;br/&gt;
    +&lt;br/&gt;
    +		@SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
    +		SourceContext&amp;lt;String&amp;gt; sourceContext = mock(SourceContext.class);&lt;br/&gt;
    +		List&amp;lt;KafkaTopicPartition&amp;gt; topics = Collections.singletonList(new KafkaTopicPartition(&quot;test&quot;, 42));&lt;br/&gt;
    +		KeyedDeserializationSchema&amp;lt;String&amp;gt; schema = new KeyedDeserializationSchemaWrapper&amp;lt;&amp;gt;(new SimpleStringSchema());&lt;br/&gt;
    +		StreamingRuntimeContext context = mock(StreamingRuntimeContext.class);&lt;br/&gt;
    +		&lt;br/&gt;
    +		final Kafka09Fetcher&amp;lt;String&amp;gt; fetcher = new Kafka09Fetcher&amp;lt;&amp;gt;(&lt;br/&gt;
    +				sourceContext, topics, null, null, context, schema, new Properties(), 0L, false);&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- run the fetcher -----&lt;br/&gt;
    +&lt;br/&gt;
    +		final AtomicReference&amp;lt;Throwable&amp;gt; error = new AtomicReference&amp;lt;&amp;gt;();&lt;br/&gt;
    +		final Thread fetcherRunner = new Thread(&quot;fetcher runner&quot;) {&lt;br/&gt;
    +&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public void run() {&lt;br/&gt;
    +				try &lt;/p&gt;
{
    +					fetcher.runFetchLoop();
    +				}
&lt;p&gt; catch (Throwable t) &lt;/p&gt;
{
    +					error.set(t);
    +				}
&lt;p&gt;    +			}&lt;br/&gt;
    +		};&lt;br/&gt;
    +		fetcherRunner.start();&lt;br/&gt;
    +&lt;br/&gt;
    +		// wait until the fetcher has reached the method of interest&lt;br/&gt;
    +		sync.await();&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- trigger the offset commit -----&lt;br/&gt;
    +		&lt;br/&gt;
    +		final AtomicReference&amp;lt;Throwable&amp;gt; commitError = new AtomicReference&amp;lt;&amp;gt;();&lt;br/&gt;
    +		final Thread committer = new Thread(&quot;committer runner&quot;) {&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public void run() {&lt;br/&gt;
    +				try &lt;/p&gt;
{
    +					fetcher.commitSpecificOffsetsToKafka(testCommitData);
    +				}
&lt;p&gt; catch (Throwable t) &lt;/p&gt;
{
    +					commitError.set(t);
    +				}
&lt;p&gt;    +			}&lt;br/&gt;
    +		};&lt;br/&gt;
    +		committer.start();&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- ensure that the committer finishes in time  -----&lt;br/&gt;
    +		committer.join(30000);&lt;br/&gt;
    +		assertFalse(&quot;The committer did not finish in time&quot;, committer.isAlive());&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- test done, wait till the fetcher is done for a clean shutdown -----&lt;br/&gt;
    +		fetcher.cancel();&lt;br/&gt;
    +		fetcherRunner.join();&lt;br/&gt;
    +&lt;br/&gt;
    +		// check that there were no errors in the fetcher&lt;br/&gt;
    +		final Throwable caughtError = error.get();&lt;br/&gt;
    +		if (caughtError != null) &lt;/p&gt;
{
    +			throw new Exception(&quot;Exception in the fetcher&quot;, caughtError);
    +		}
&lt;p&gt;    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Might as well also check `commitError`?&lt;/p&gt;</comment>
                            <comment id="15535143" author="githubbot" created="Fri, 30 Sep 2016 05:59:55 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2574#discussion_r81277361&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2574#discussion_r81277361&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/Kafka09FetcherTest.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,300 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.core.testutils.MultiShotLatch;&lt;br/&gt;
    +import org.apache.flink.core.testutils.OneShotLatch;&lt;br/&gt;
    +import org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext;&lt;br/&gt;
    +import org.apache.flink.streaming.api.operators.StreamingRuntimeContext;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internal.Kafka09Fetcher;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedDeserializationSchema;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedDeserializationSchemaWrapper;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.SimpleStringSchema;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.ConsumerRecords;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.KafkaConsumer;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.OffsetAndMetadata;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.OffsetCommitCallback;&lt;br/&gt;
    +import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.junit.Test;&lt;br/&gt;
    +import org.junit.runner.RunWith;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.mockito.Mockito;&lt;br/&gt;
    +import org.mockito.invocation.InvocationOnMock;&lt;br/&gt;
    +import org.mockito.stubbing.Answer;&lt;br/&gt;
    +import org.powermock.core.classloader.annotations.PrepareForTest;&lt;br/&gt;
    +import org.powermock.modules.junit4.PowerMockRunner;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util.Collections;&lt;br/&gt;
    +import java.util.HashMap;&lt;br/&gt;
    +import java.util.List;&lt;br/&gt;
    +import java.util.Map;&lt;br/&gt;
    +import java.util.Map.Entry;&lt;br/&gt;
    +import java.util.Properties;&lt;br/&gt;
    +import java.util.concurrent.BlockingQueue;&lt;br/&gt;
    +import java.util.concurrent.LinkedBlockingQueue;&lt;br/&gt;
    +import java.util.concurrent.atomic.AtomicReference;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.junit.Assert.assertEquals;&lt;br/&gt;
    +import static org.junit.Assert.assertFalse;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.mockito.Mockito.any;&lt;br/&gt;
    +import static org.mockito.Mockito.anyLong;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.doAnswer;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.mock;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.when;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.whenNew;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * Unit tests for the &lt;/p&gt;
{@link Kafka09Fetcher}
&lt;p&gt;.&lt;br/&gt;
    + */&lt;br/&gt;
    +@RunWith(PowerMockRunner.class)&lt;br/&gt;
    +@PrepareForTest(Kafka09Fetcher.class)&lt;br/&gt;
    +public class Kafka09FetcherTest {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Should we also add a test to make sure that `KafkaConsumer` is immediately called `wakeup()` in `commitSpecificOffsetsToKafka`? Otherwise we are not ensuring the behaviour of &quot;committing offsets back to Kafka on checkpoints&quot;&lt;/p&gt;

&lt;p&gt;    Perhaps this can be integrated into `testCommitDoesNotBlock()`.&lt;/p&gt;</comment>
                            <comment id="15535144" author="githubbot" created="Fri, 30 Sep 2016 05:59:55 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2574#discussion_r81275213&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2574#discussion_r81275213&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09Fetcher.java &amp;#8212;&lt;br/&gt;
    @@ -283,10 +296,16 @@ public void commitSpecificOffsetsToKafka(Map&amp;lt;KafkaTopicPartition, Long&amp;gt; offsets)&lt;br/&gt;
     			}&lt;br/&gt;
     		}&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (this.consumer != null) {&lt;/li&gt;
	&lt;li&gt;synchronized (consumerLock) 
{
    -				this.consumer.commitSync(offsetsToCommit);
    -			}
&lt;p&gt;    +		if (commitInProgress) &lt;/p&gt;
{
    +			LOG.warn(&quot;Committing offsets to Kafka takes longer than the checkpoint interval. &quot; +
    +					&quot;Some checkpoints may be subsumed before committed. &quot; +
    +					&quot;This does not compromise Flink&apos;s checkpoint integrity.&quot;);
    +		}
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Will it make sense to simply move this warning to the `if (toCommit != null &amp;amp;&amp;amp; !commitInProgress)` block in the main thread? That&apos;s where `commitInProgress` will actually determine whether or not the current offsets to commit will be dropped. Also, the actual committing should happen right after anyways because of `consumer.wakeup()`, so I don&apos;t see the purpose of an eager warning here.&lt;/p&gt;</comment>
                            <comment id="15535475" author="githubbot" created="Fri, 30 Sep 2016 09:00:18 +0000"  >&lt;p&gt;Github user StephanEwen commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2574#discussion_r81302867&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2574#discussion_r81302867&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/Kafka09FetcherTest.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,300 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.core.testutils.MultiShotLatch;&lt;br/&gt;
    +import org.apache.flink.core.testutils.OneShotLatch;&lt;br/&gt;
    +import org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext;&lt;br/&gt;
    +import org.apache.flink.streaming.api.operators.StreamingRuntimeContext;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internal.Kafka09Fetcher;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedDeserializationSchema;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedDeserializationSchemaWrapper;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.SimpleStringSchema;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.ConsumerRecords;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.KafkaConsumer;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.OffsetAndMetadata;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.OffsetCommitCallback;&lt;br/&gt;
    +import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.junit.Test;&lt;br/&gt;
    +import org.junit.runner.RunWith;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.mockito.Mockito;&lt;br/&gt;
    +import org.mockito.invocation.InvocationOnMock;&lt;br/&gt;
    +import org.mockito.stubbing.Answer;&lt;br/&gt;
    +import org.powermock.core.classloader.annotations.PrepareForTest;&lt;br/&gt;
    +import org.powermock.modules.junit4.PowerMockRunner;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util.Collections;&lt;br/&gt;
    +import java.util.HashMap;&lt;br/&gt;
    +import java.util.List;&lt;br/&gt;
    +import java.util.Map;&lt;br/&gt;
    +import java.util.Map.Entry;&lt;br/&gt;
    +import java.util.Properties;&lt;br/&gt;
    +import java.util.concurrent.BlockingQueue;&lt;br/&gt;
    +import java.util.concurrent.LinkedBlockingQueue;&lt;br/&gt;
    +import java.util.concurrent.atomic.AtomicReference;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.junit.Assert.assertEquals;&lt;br/&gt;
    +import static org.junit.Assert.assertFalse;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.mockito.Mockito.any;&lt;br/&gt;
    +import static org.mockito.Mockito.anyLong;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.doAnswer;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.mock;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.when;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.whenNew;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * Unit tests for the &lt;/p&gt;
{@link Kafka09Fetcher}
&lt;p&gt;.&lt;br/&gt;
    + */&lt;br/&gt;
    +@RunWith(PowerMockRunner.class)&lt;br/&gt;
    +@PrepareForTest(Kafka09Fetcher.class)&lt;br/&gt;
    +public class Kafka09FetcherTest {&lt;br/&gt;
    +&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testCommitDoesNotBlock() throws Exception {&lt;br/&gt;
    +&lt;br/&gt;
    +		// test data&lt;br/&gt;
    +		final KafkaTopicPartition testPartition = new KafkaTopicPartition(&quot;test&quot;, 42);&lt;br/&gt;
    +		final Map&amp;lt;KafkaTopicPartition, Long&amp;gt; testCommitData = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
    +		testCommitData.put(testPartition, 11L);&lt;br/&gt;
    +&lt;br/&gt;
    +		// to synchronize when the consumer is in its blocking method&lt;br/&gt;
    +		final OneShotLatch sync = new OneShotLatch();&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- the mock consumer with blocking poll calls ----&lt;br/&gt;
    +		final MultiShotLatch blockerLatch = new MultiShotLatch();&lt;br/&gt;
    +		&lt;br/&gt;
    +		KafkaConsumer&amp;lt;?, ?&amp;gt; mockConsumer = mock(KafkaConsumer.class);&lt;br/&gt;
    +		when(mockConsumer.poll(anyLong())).thenAnswer(new Answer&amp;lt;ConsumerRecords&amp;lt;?, ?&amp;gt;&amp;gt;() {&lt;br/&gt;
    +			&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public ConsumerRecords&amp;lt;?, ?&amp;gt; answer(InvocationOnMock invocation) throws InterruptedException &lt;/p&gt;
{
    +				sync.trigger();
    +				blockerLatch.await();
    +				return ConsumerRecords.empty();
    +			}
&lt;p&gt;    +		});&lt;br/&gt;
    +&lt;br/&gt;
    +		doAnswer(new Answer&amp;lt;Void&amp;gt;() {&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public Void answer(InvocationOnMock invocation) &lt;/p&gt;
{
    +				blockerLatch.trigger();
    +				return null;
    +			}
&lt;p&gt;    +		}).when(mockConsumer).wakeup();&lt;br/&gt;
    +&lt;br/&gt;
    +		// make sure the fetcher creates the mock consumer&lt;br/&gt;
    +		whenNew(KafkaConsumer.class).withAnyArguments().thenReturn(mockConsumer);&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- create the test fetcher -----&lt;br/&gt;
    +&lt;br/&gt;
    +		@SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
    +		SourceContext&amp;lt;String&amp;gt; sourceContext = mock(SourceContext.class);&lt;br/&gt;
    +		List&amp;lt;KafkaTopicPartition&amp;gt; topics = Collections.singletonList(new KafkaTopicPartition(&quot;test&quot;, 42));&lt;br/&gt;
    +		KeyedDeserializationSchema&amp;lt;String&amp;gt; schema = new KeyedDeserializationSchemaWrapper&amp;lt;&amp;gt;(new SimpleStringSchema());&lt;br/&gt;
    +		StreamingRuntimeContext context = mock(StreamingRuntimeContext.class);&lt;br/&gt;
    +		&lt;br/&gt;
    +		final Kafka09Fetcher&amp;lt;String&amp;gt; fetcher = new Kafka09Fetcher&amp;lt;&amp;gt;(&lt;br/&gt;
    +				sourceContext, topics, null, null, context, schema, new Properties(), 0L, false);&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- run the fetcher -----&lt;br/&gt;
    +&lt;br/&gt;
    +		final AtomicReference&amp;lt;Throwable&amp;gt; error = new AtomicReference&amp;lt;&amp;gt;();&lt;br/&gt;
    +		final Thread fetcherRunner = new Thread(&quot;fetcher runner&quot;) {&lt;br/&gt;
    +&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public void run() {&lt;br/&gt;
    +				try &lt;/p&gt;
{
    +					fetcher.runFetchLoop();
    +				}
&lt;p&gt; catch (Throwable t) &lt;/p&gt;
{
    +					error.set(t);
    +				}
&lt;p&gt;    +			}&lt;br/&gt;
    +		};&lt;br/&gt;
    +		fetcherRunner.start();&lt;br/&gt;
    +&lt;br/&gt;
    +		// wait until the fetcher has reached the method of interest&lt;br/&gt;
    +		sync.await();&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- trigger the offset commit -----&lt;br/&gt;
    +		&lt;br/&gt;
    +		final AtomicReference&amp;lt;Throwable&amp;gt; commitError = new AtomicReference&amp;lt;&amp;gt;();&lt;br/&gt;
    +		final Thread committer = new Thread(&quot;committer runner&quot;) {&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public void run() {&lt;br/&gt;
    +				try &lt;/p&gt;
{
    +					fetcher.commitSpecificOffsetsToKafka(testCommitData);
    +				}
&lt;p&gt; catch (Throwable t) &lt;/p&gt;
{
    +					commitError.set(t);
    +				}
&lt;p&gt;    +			}&lt;br/&gt;
    +		};&lt;br/&gt;
    +		committer.start();&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- ensure that the committer finishes in time  -----&lt;br/&gt;
    +		committer.join(30000);&lt;br/&gt;
    +		assertFalse(&quot;The committer did not finish in time&quot;, committer.isAlive());&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- test done, wait till the fetcher is done for a clean shutdown -----&lt;br/&gt;
    +		fetcher.cancel();&lt;br/&gt;
    +		fetcherRunner.join();&lt;br/&gt;
    +&lt;br/&gt;
    +		// check that there were no errors in the fetcher&lt;br/&gt;
    +		final Throwable caughtError = error.get();&lt;br/&gt;
    +		if (caughtError != null) &lt;/p&gt;
{
    +			throw new Exception(&quot;Exception in the fetcher&quot;, caughtError);
    +		}
&lt;p&gt;    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Yes, should and will do that!&lt;/p&gt;</comment>
                            <comment id="15535482" author="githubbot" created="Fri, 30 Sep 2016 09:02:41 +0000"  >&lt;p&gt;Github user StephanEwen commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2574#discussion_r81303182&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2574#discussion_r81303182&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09Fetcher.java &amp;#8212;&lt;br/&gt;
    @@ -283,10 +296,16 @@ public void commitSpecificOffsetsToKafka(Map&amp;lt;KafkaTopicPartition, Long&amp;gt; offsets)&lt;br/&gt;
     			}&lt;br/&gt;
     		}&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (this.consumer != null) {&lt;/li&gt;
	&lt;li&gt;synchronized (consumerLock) 
{
    -				this.consumer.commitSync(offsetsToCommit);
    -			}
&lt;p&gt;    +		if (commitInProgress) &lt;/p&gt;
{
    +			LOG.warn(&quot;Committing offsets to Kafka takes longer than the checkpoint interval. &quot; +
    +					&quot;Some checkpoints may be subsumed before committed. &quot; +
    +					&quot;This does not compromise Flink&apos;s checkpoint integrity.&quot;);
    +		}
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    I would not want to put it into the main loop, because then the warning would come repeatedly, every time the poll happens.&lt;/p&gt;</comment>
                            <comment id="15535485" author="githubbot" created="Fri, 30 Sep 2016 09:04:20 +0000"  >&lt;p&gt;Github user StephanEwen commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2574#discussion_r81303450&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2574#discussion_r81303450&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09Fetcher.java &amp;#8212;&lt;br/&gt;
    @@ -283,10 +296,16 @@ public void commitSpecificOffsetsToKafka(Map&amp;lt;KafkaTopicPartition, Long&amp;gt; offsets)&lt;br/&gt;
     			}&lt;br/&gt;
     		}&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (this.consumer != null) {&lt;/li&gt;
	&lt;li&gt;synchronized (consumerLock) 
{
    -				this.consumer.commitSync(offsetsToCommit);
    -			}
&lt;p&gt;    +		if (commitInProgress) &lt;/p&gt;
{
    +			LOG.warn(&quot;Committing offsets to Kafka takes longer than the checkpoint interval. &quot; +
    +					&quot;Some checkpoints may be subsumed before committed. &quot; +
    +					&quot;This does not compromise Flink&apos;s checkpoint integrity.&quot;);
    +		}
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Also, in both cases, you cannot know whether it is dropped - it may still be that only one commit was delayed and the &quot;toCommit&quot; data will actually be picked up, but with a delay.&lt;/p&gt;

&lt;p&gt;    I have an idea though where we can check that properly.&lt;/p&gt;</comment>
                            <comment id="15535488" author="githubbot" created="Fri, 30 Sep 2016 09:05:38 +0000"  >&lt;p&gt;Github user StephanEwen commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2574#discussion_r81303617&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2574#discussion_r81303617&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/Kafka09FetcherTest.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,300 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.core.testutils.MultiShotLatch;&lt;br/&gt;
    +import org.apache.flink.core.testutils.OneShotLatch;&lt;br/&gt;
    +import org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext;&lt;br/&gt;
    +import org.apache.flink.streaming.api.operators.StreamingRuntimeContext;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internal.Kafka09Fetcher;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedDeserializationSchema;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedDeserializationSchemaWrapper;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.SimpleStringSchema;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.ConsumerRecords;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.KafkaConsumer;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.OffsetAndMetadata;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.OffsetCommitCallback;&lt;br/&gt;
    +import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.junit.Test;&lt;br/&gt;
    +import org.junit.runner.RunWith;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.mockito.Mockito;&lt;br/&gt;
    +import org.mockito.invocation.InvocationOnMock;&lt;br/&gt;
    +import org.mockito.stubbing.Answer;&lt;br/&gt;
    +import org.powermock.core.classloader.annotations.PrepareForTest;&lt;br/&gt;
    +import org.powermock.modules.junit4.PowerMockRunner;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util.Collections;&lt;br/&gt;
    +import java.util.HashMap;&lt;br/&gt;
    +import java.util.List;&lt;br/&gt;
    +import java.util.Map;&lt;br/&gt;
    +import java.util.Map.Entry;&lt;br/&gt;
    +import java.util.Properties;&lt;br/&gt;
    +import java.util.concurrent.BlockingQueue;&lt;br/&gt;
    +import java.util.concurrent.LinkedBlockingQueue;&lt;br/&gt;
    +import java.util.concurrent.atomic.AtomicReference;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.junit.Assert.assertEquals;&lt;br/&gt;
    +import static org.junit.Assert.assertFalse;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.mockito.Mockito.any;&lt;br/&gt;
    +import static org.mockito.Mockito.anyLong;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.doAnswer;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.mock;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.when;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.whenNew;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * Unit tests for the &lt;/p&gt;
{@link Kafka09Fetcher}
&lt;p&gt;.&lt;br/&gt;
    + */&lt;br/&gt;
    +@RunWith(PowerMockRunner.class)&lt;br/&gt;
    +@PrepareForTest(Kafka09Fetcher.class)&lt;br/&gt;
    +public class Kafka09FetcherTest {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    It is already integrated in the test, actually. Since in the mock consumer, the `poll()` call blocks forever and only wakes up on `wakeup()`, the test fails is `wakeup()` is not called.&lt;/p&gt;</comment>
                            <comment id="15535491" author="githubbot" created="Fri, 30 Sep 2016 09:06:00 +0000"  >&lt;p&gt;Github user StephanEwen commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2574#discussion_r81303671&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2574#discussion_r81303671&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/Kafka09FetcherTest.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,300 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.core.testutils.MultiShotLatch;&lt;br/&gt;
    +import org.apache.flink.core.testutils.OneShotLatch;&lt;br/&gt;
    +import org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext;&lt;br/&gt;
    +import org.apache.flink.streaming.api.operators.StreamingRuntimeContext;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internal.Kafka09Fetcher;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedDeserializationSchema;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedDeserializationSchemaWrapper;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.SimpleStringSchema;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.ConsumerRecords;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.KafkaConsumer;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.OffsetAndMetadata;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.OffsetCommitCallback;&lt;br/&gt;
    +import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.junit.Test;&lt;br/&gt;
    +import org.junit.runner.RunWith;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.mockito.Mockito;&lt;br/&gt;
    +import org.mockito.invocation.InvocationOnMock;&lt;br/&gt;
    +import org.mockito.stubbing.Answer;&lt;br/&gt;
    +import org.powermock.core.classloader.annotations.PrepareForTest;&lt;br/&gt;
    +import org.powermock.modules.junit4.PowerMockRunner;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util.Collections;&lt;br/&gt;
    +import java.util.HashMap;&lt;br/&gt;
    +import java.util.List;&lt;br/&gt;
    +import java.util.Map;&lt;br/&gt;
    +import java.util.Map.Entry;&lt;br/&gt;
    +import java.util.Properties;&lt;br/&gt;
    +import java.util.concurrent.BlockingQueue;&lt;br/&gt;
    +import java.util.concurrent.LinkedBlockingQueue;&lt;br/&gt;
    +import java.util.concurrent.atomic.AtomicReference;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.junit.Assert.assertEquals;&lt;br/&gt;
    +import static org.junit.Assert.assertFalse;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.mockito.Mockito.any;&lt;br/&gt;
    +import static org.mockito.Mockito.anyLong;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.doAnswer;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.mock;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.when;&lt;br/&gt;
    +import static org.powermock.api.mockito.PowerMockito.whenNew;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * Unit tests for the &lt;/p&gt;
{@link Kafka09Fetcher}
&lt;p&gt;.&lt;br/&gt;
    + */&lt;br/&gt;
    +@RunWith(PowerMockRunner.class)&lt;br/&gt;
    +@PrepareForTest(Kafka09Fetcher.class)&lt;br/&gt;
    +public class Kafka09FetcherTest {&lt;br/&gt;
    +&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testCommitDoesNotBlock() throws Exception {&lt;br/&gt;
    +&lt;br/&gt;
    +		// test data&lt;br/&gt;
    +		final KafkaTopicPartition testPartition = new KafkaTopicPartition(&quot;test&quot;, 42);&lt;br/&gt;
    +		final Map&amp;lt;KafkaTopicPartition, Long&amp;gt; testCommitData = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
    +		testCommitData.put(testPartition, 11L);&lt;br/&gt;
    +&lt;br/&gt;
    +		// to synchronize when the consumer is in its blocking method&lt;br/&gt;
    +		final OneShotLatch sync = new OneShotLatch();&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- the mock consumer with blocking poll calls ----&lt;br/&gt;
    +		final MultiShotLatch blockerLatch = new MultiShotLatch();&lt;br/&gt;
    +		&lt;br/&gt;
    +		KafkaConsumer&amp;lt;?, ?&amp;gt; mockConsumer = mock(KafkaConsumer.class);&lt;br/&gt;
    +		when(mockConsumer.poll(anyLong())).thenAnswer(new Answer&amp;lt;ConsumerRecords&amp;lt;?, ?&amp;gt;&amp;gt;() {&lt;br/&gt;
    +			&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public ConsumerRecords&amp;lt;?, ?&amp;gt; answer(InvocationOnMock invocation) throws InterruptedException {&lt;br/&gt;
    +				sync.trigger();&lt;br/&gt;
    +				blockerLatch.await();&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    See above.&lt;/p&gt;</comment>
                            <comment id="15535493" author="githubbot" created="Fri, 30 Sep 2016 09:06:57 +0000"  >&lt;p&gt;Github user StephanEwen commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2574&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2574&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Thanks for the review. I think some points you mentioned are already addressed, actually.&lt;/p&gt;

&lt;p&gt;    Will add the following:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;`commitSpecificOffsets()` can determine precisely when a commit request is subsumed by a new one&lt;/li&gt;
	&lt;li&gt;Extend test to catch errors from committer thread.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="15535912" author="githubbot" created="Fri, 30 Sep 2016 12:53:37 +0000"  >&lt;p&gt;Github user StephanEwen commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2574&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2574&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Merged in 92f4539afc714f7dbd293c3ad677b3b5807c6911&lt;/p&gt;

&lt;p&gt;    Addresses @tzulitai comments. Fixed the warning log message by using atomic swap when setting the next offsets to be committed. If it swaps for a non-null value, that value is skipped and subsumed by the newer offsets.&lt;/p&gt;</comment>
                            <comment id="15535913" author="githubbot" created="Fri, 30 Sep 2016 12:53:37 +0000"  >&lt;p&gt;Github user StephanEwen closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2574&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2574&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15536184" author="stephanewen" created="Fri, 30 Sep 2016 14:48:52 +0000"  >&lt;p&gt;Fixed in&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;1.2.0 via 92f4539afc714f7dbd293c3ad677b3b5807c6911&lt;/li&gt;
	&lt;li&gt;1.1.3 via 90d77594fffda1d8d15658d363c478ea6430514e&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 7 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i345of:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>