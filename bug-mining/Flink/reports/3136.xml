<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 20:40:22 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-13228] HadoopRecoverableWriterTest.testCommitAfterNormalClose fails on Travis</title>
                <link>https://issues.apache.org/jira/browse/FLINK-13228</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;p&gt;&lt;tt&gt;HadoopRecoverableWriterTest.testCommitAfterNormalClose&lt;/tt&gt; failed on Travis with&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
HadoopRecoverableWriterTest.testCommitAfterNormalClose &#194;&#187; IO The stream is closed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;https://api.travis-ci.org/v3/job/557293706/log.txt&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://api.travis-ci.org/v3/job/557293706/log.txt&lt;/a&gt;&lt;/p&gt;</description>
                <environment></environment>
        <key id="13244333">FLINK-13228</key>
            <summary>HadoopRecoverableWriterTest.testCommitAfterNormalClose fails on Travis</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="liyu">Yu Li</assignee>
                                    <reporter username="trohrmann">Till Rohrmann</reporter>
                        <labels>
                            <label>pull-request-available</label>
                            <label>test-stability</label>
                    </labels>
                <created>Thu, 11 Jul 2019 13:41:58 +0000</created>
                <updated>Mon, 28 Sep 2020 06:43:57 +0000</updated>
                            <resolved>Mon, 29 Jul 2019 13:01:41 +0000</resolved>
                                    <version>1.9.0</version>
                                    <fixVersion>1.9.0</fixVersion>
                                    <component>FileSystems</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                    <progress percentage="100">
                                    <originalProgress>
                                                    <row percentage="0" backgroundColor="#89afd7"/>
                                                    <row percentage="100" backgroundColor="transparent"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="100" backgroundColor="#51a825"/>
                                                    <row percentage="0" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </progress>
                                    <aggregateprogress percentage="100">
                                    <originalProgress>
                                                    <row percentage="0" backgroundColor="#89afd7"/>
                                                    <row percentage="100" backgroundColor="transparent"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="100" backgroundColor="#51a825"/>
                                                    <row percentage="0" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </aggregateprogress>
                                            <timeestimate seconds="0">0h</timeestimate>
                            <timespent seconds="1200">20m</timespent>
                                <comments>
                            <comment id="16888804" author="zentol" created="Fri, 19 Jul 2019 11:36:39 +0000"  >&lt;p&gt;Another instance: &lt;a href=&quot;https://travis-ci.org/apache/flink/jobs/560546002&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://travis-ci.org/apache/flink/jobs/560546002&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16891673" author="carp84" created="Wed, 24 Jul 2019 08:01:54 +0000"  >&lt;p&gt;Cannot reproduce the issue until now and have tried the below methods:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Try to run &lt;tt&gt;HadoopRecoverableWriterTest#testCommitAfterNormalClose&lt;/tt&gt; in local Intellij environment with &quot;Repeat until failure&quot; mode, ran thousands of times but cannot reproduce.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Try to run &lt;tt&gt;HadoopRecoverableWriterTest&lt;/tt&gt; repeatedly on travis but cannot reproduce
	&lt;ul&gt;
		&lt;li&gt;Modify the travis scripts to run &lt;tt&gt;HadoopRecoverableWriterTest&lt;/tt&gt; 100 times per &lt;a href=&quot;https://travis-ci.org/carp84/flink/builds/562680839&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;test&lt;/a&gt; with &lt;tt&gt;forkCount=1&lt;/tt&gt;, never reproduced&lt;/li&gt;
		&lt;li&gt;Modify the travis scripts to run &lt;a href=&quot;https://travis-ci.org/carp84/flink/builds/562715072&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;all tests&lt;/a&gt; in &lt;tt&gt;flink-hadoop-fs&lt;/tt&gt; module with &lt;tt&gt;forkCount=2&lt;/tt&gt;, never reproduced&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Try to modify the &lt;tt&gt;org.apache.hadoop.net.SocketOutputStream&lt;/tt&gt; class to produce &lt;tt&gt;ClosedByInterruptException&lt;/tt&gt; more aggressively, but still haven&apos;t got a way to reproduce
	&lt;ul&gt;
		&lt;li&gt;To do this, we need to apply the attached patch to hadoop 2.8.3 code base and do local install, then clone the &lt;a href=&quot;https://github.com/apache/flink-shaded&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;flink-shaded&lt;/a&gt; repository, switch to the &lt;tt&gt;release-7.0&lt;/tt&gt; branch, and run command &lt;tt&gt;mvn -DskipTests -Dhadoop.version=2.8.3 -pl flink-shaded-hadoop-2 install -am&lt;/tt&gt;&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="16891681" author="carp84" created="Wed, 24 Jul 2019 08:21:01 +0000"  >&lt;p&gt;From current investigation, the most weird part is that from travis log, there&apos;s an additional data flush in &lt;tt&gt;DataStreamer&lt;/tt&gt; as indicated below:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;23:31:07,549 INFO  org.apache.hadoop.hdfs.StateChange                            - DIR* completeFile: /tests/qoadqaraomqogyax/.part-0.inprogress.748c58f2-4db8-4b2f-a10a-e5ea09410b7e is closed by DFSClient_NONMAPREDUCE_-1957051875_1
23:31:07,552 WARN  org.apache.hadoop.hdfs.DataStreamer                           - DataStreamer Exception
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:478)
	at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:63)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:159)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:117)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at java.io.DataOutputStream.flush(DataOutputStream.java:123)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:766)
13:33:59,846 INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit     - allowed=true	ugi=travis (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tests/fsledeprirhvkkht/.part-0.inprogress.fd9bc50d-96aa-48bc-86e1-8662382b53c9	dst=null	perm=null	proto=rpc
13:33:59,848 INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit     - allowed=true	ugi=travis (auth:SIMPLE)	ip=/127.0.0.1	cmd=rename	src=/tests/fsledeprirhvkkht/.part-0.inprogress.fd9bc50d-96aa-48bc-86e1-8662382b53c9	dst=/tests/fsledeprirhvkkht/part-0	perm=travis:supergroup:rw-r--r--	proto=rpc
13:33:59,849 INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit     - allowed=true	ugi=travis (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/tests/fsledeprirhvkkht	dst=null	perm=null	proto=rpc
13:33:59,850 INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit     - allowed=true	ugi=travis (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/tests/fsledeprirhvkkht/part-0	dst=null	perm=null	proto=rpc
13:33:59,855 INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit     - allowed=true	ugi=travis (auth:SIMPLE)	ip=/127.0.0.1	cmd=delete	src=/tests/fsledeprirhvkkht	dst=null	perm=null	proto=rpc

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;However, when debugging with the attached hadoop patch, we could confirm the data will be split into two packets in our test case, and if any of them failed to write, we will see below exception:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;[DataStreamer for file /tests/yufomllbjxlhztth/.part-0.inprogress.fca749ce-ce9d-4fef-9f08-3937358b4c05 block BP-1698089270-127.0.0.1-1563951866468:blk_1073741825_1001] WARN  org.apache.hadoop.hdfs.DataStreamer  - DataStreamer Exception
java.nio.channels.ClosedByInterruptException
        at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:77)
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:179)
        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:137)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
        at java.io.DataOutputStream.flush(DataOutputStream.java:123)
        at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:766)
[DataXceiver for client DFSClient_NONMAPREDUCE_-1981706570_1 at /127.0.0.1:51159 [Receiving block BP-1698089270-127.0.0.1-1563951866468:blk_1073741825_1001]] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Exception for BP-1698089270-127.0.0.1-1563951866468:blk_1073741825_1001
java.io.IOException: Premature EOF from inputStream
        at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:208)
        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:521)
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:923)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:854)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:166)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:103)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:288)
        at java.lang.Thread.run(Thread.java:745)
Test testCommitAfterNormalClose(org.apache.flink.runtime.fs.hdfs.HadoopRecoverableWriterTest) failed with:
java.io.IOException: All datanodes [DatanodeInfoWithStorage[127.0.0.1:51147,DS-8f7fb1af-ce7d-4547-b50b-a53f00c40408,DISK]] are bad. Aborting...
        at org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1530)
        at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1465)
        at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
        at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;But if we throw &lt;tt&gt;ClosedByInterruptException&lt;/tt&gt; after the two packets completes, we will see no additional flush happened to &lt;tt&gt;DataStreamer&lt;/tt&gt;, as indicated by below log:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;24794 [PacketResponder: BP-695740659-127.0.0.1-1563954445528:blk_1073741825_1001, type=LAST_IN_PIPELINE] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - PacketResponder: BP-695740659-127.0.0.1-1563954445528:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating
24795 [IPC Server handler 4 on 53181] INFO  org.apache.hadoop.hdfs.StateChange  - DIR* completeFile: /tests/ujoamxgnvmlapclr/.part-0.inprogress.e3bcef26-5811-495b-961b-1c65b20acccf is closed by DFSClient_NONMAPREDUCE_1297545335_1
24796 [IPC Server handler 5 on 53181] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit  - allowed=true	ugi=jueding (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tests/ujoamxgnvmlapclr/.part-0.inprogress.e3bcef26-5811-495b-961b-1c65b20acccf	dst=null	perm=null	proto=rpc
24797 [IPC Server handler 6 on 53181] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit  - allowed=true	ugi=jueding (auth:SIMPLE)	ip=/127.0.0.1	cmd=rename	src=/tests/ujoamxgnvmlapclr/.part-0.inprogress.e3bcef26-5811-495b-961b-1c65b20acccf	dst=/tests/ujoamxgnvmlapclr/part-0	perm=jueding:supergroup:rw-r--r--	proto=rpc
24798 [IPC Server handler 7 on 53181] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit  - allowed=true	ugi=jueding (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/tests/ujoamxgnvmlapclr	dst=null	perm=null	proto=rpc
24799 [IPC Server handler 8 on 53181] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit  - allowed=true	ugi=jueding (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/tests/ujoamxgnvmlapclr/part-0	dst=null	perm=null	proto=rpc
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Will try to figure out where the additional flush comes from and I believe we could reproduce and resolve the issue once this is located.&lt;/p&gt;</comment>
                            <comment id="16891795" author="carp84" created="Wed, 24 Jul 2019 12:08:09 +0000"  >&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;23:31:07,552 WARN org.apache.hadoop.hdfs.DataStreamer - DataStreamer Exception
java.nio.channels.ClosedByInterruptException at
java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:478)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After&#160;a&#160;closer check of the above log, I found the root cause is &lt;tt&gt;ClosedByInterruptException&lt;/tt&gt; occurred after&#160;&lt;tt&gt;channel.write(buf)&lt;/tt&gt; completes writing data in &lt;tt&gt;SocketOutputStream#performIO&lt;/tt&gt;, and now I could stably reproduce the issue with the&#160;v2 hadoop patch as attached.&lt;/p&gt;

&lt;p&gt;Since all data are already written successfully and file also marked as complete by NameNode, we should silently ignore the &lt;tt&gt;ClosedByInterruptedException&lt;/tt&gt; instead of throwing it out as an error, which IMO is something hadoop should fix. Will file a JIRA for HDFS once find out a proper solution.&lt;/p&gt;

&lt;p&gt;As per how to fix the issue here, since the issue is thrown at closing the &lt;tt&gt;RecoverableFsDataOutputStream&lt;/tt&gt; (easily to confirm after flattening the try-with-resource to a normal try-catch), I think we could directly try-catch the exception and ignore it if failed to close the &lt;tt&gt;RecoverableFsDataOutputStream&lt;/tt&gt;, because this is irrelative to the target of the test case (checking whether commit after normal close works). Wdyt? &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=till.rohrmann&quot; class=&quot;user-hover&quot; rel=&quot;till.rohrmann&quot;&gt;till.rohrmann&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=chesnay&quot; class=&quot;user-hover&quot; rel=&quot;chesnay&quot;&gt;chesnay&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Will attach the draft patch here for a straight forward check.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16891808" author="carp84" created="Wed, 24 Jul 2019 12:26:20 +0000"  >&lt;p&gt;Please check the demo patch, actually all other test cases will fail (except for &lt;tt&gt;testCloseWithNoData&lt;/tt&gt; which has no try-with-resource) with the v2 hadoop patch, so the final PR will change all test methods in a similar way.&lt;/p&gt;

&lt;p&gt;And the given patch could also fix &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-12071&quot; title=&quot;HadoopRecoverableWriterTest fails on Travis&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-12071&quot;&gt;&lt;del&gt;FLINK-12071&lt;/del&gt;&lt;/a&gt; (actually these two are duplicates with the same root cause). For example, below is the exception I observed for &lt;tt&gt;testExceptionWritingAfterCloseForCommit&lt;/tt&gt; with v2 hadoop patch&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Caused by: java.lang.IllegalArgumentException: Self-suppression not permitted
	at java.lang.Throwable.addSuppressed(Throwable.java:1043)
	at org.apache.flink.core.fs.AbstractRecoverableWriterTest.testExceptionWritingAfterCloseForCommit(AbstractRecoverableWriterTest.java:308)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)
	... 22 more
Caused by: java.io.IOException: The stream is closed
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:147)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at java.io.DataOutputStream.flush(DataOutputStream.java:123)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:158)
	at org.apache.hadoop.hdfs.DataStreamer.closeStream(DataStreamer.java:987)
	at org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:839)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:834)
	Suppressed: java.io.IOException: The stream is closed
		at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:147)
		at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
		at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
		at java.io.FilterOutputStream.close(FilterOutputStream.java:158)
		at java.io.FilterOutputStream.close(FilterOutputStream.java:159)
		... 3 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="16891946" author="stephanewen" created="Wed, 24 Jul 2019 15:28:16 +0000"  >&lt;p&gt;Thanks for this diagnosis. I came to a similar conclusion:&lt;/p&gt;

&lt;p&gt;The test runs fulls (you can see the rename command, list command, open command for verification) and only after that can be the failure. That only leaves the closing or deleting of the file.&lt;/p&gt;

&lt;p&gt;In any case, the test passed properly.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=carp84&quot; class=&quot;user-hover&quot; rel=&quot;carp84&quot;&gt;carp84&lt;/a&gt; can you open a PR with the fix? will merge it asap.&lt;/p&gt;</comment>
                            <comment id="16893639" author="carp84" created="Fri, 26 Jul 2019 09:06:59 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sewen&quot; class=&quot;user-hover&quot; rel=&quot;sewen&quot;&gt;sewen&lt;/a&gt; for taking a look and sorry for the late response. Just submitted the PR, FYI.&lt;/p&gt;</comment>
                            <comment id="16895230" author="stephanewen" created="Mon, 29 Jul 2019 13:01:41 +0000"  >&lt;p&gt;Fixed in&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;1.9.0 via 0e9f463668378bd7469194ebf0af76e3c125f0d7&lt;/li&gt;
	&lt;li&gt;master via e373c4481e6a0ca0e1e73a6170b9e3da5cc9be5b&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="13225003">FLINK-12071</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="13320991">FLINK-18818</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12975604" name="FLINK-13228.hadoop.debug.patch" size="1888" author="liyu" created="Wed, 24 Jul 2019 08:03:36 +0000"/>
                            <attachment id="12975640" name="FLINK-13228.hadoop.debug.v2.patch" size="1933" author="liyu" created="Wed, 24 Jul 2019 12:08:50 +0000"/>
                            <attachment id="12975642" name="FLINK-13288.demo.patch" size="1204" author="liyu" created="Wed, 24 Jul 2019 12:19:51 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 16 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z04khc:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>