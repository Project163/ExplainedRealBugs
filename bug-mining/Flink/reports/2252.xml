<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 20:32:18 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-8543] Output Stream closed at org.apache.hadoop.fs.s3a.S3AOutputStream.checkOpen</title>
                <link>https://issues.apache.org/jira/browse/FLINK-8543</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;p&gt;I&apos;m hitting an issue with my BucketingSink from a streaming job.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; BucketingSink&amp;lt;Tuple2&amp;lt;&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;, &lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;&amp;gt;&amp;gt;(path)
         .setWriter(writer)
         .setBucketer(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; DateTimeBucketer&amp;lt;Tuple2&amp;lt;&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;, &lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;&amp;gt;&amp;gt;(formatString));
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;I can see that a few files have run into issues with uploading to S3:&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/12908801/12908801_Screen+Shot+2018-01-30+at+18.34.51.png&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt; &#160;&#160;&lt;/p&gt;

&lt;p&gt;The Flink console output is showing an exception being thrown by S3AOutputStream, so I&apos;ve grabbed the&#160;S3AOutputStream class from my cluster and added some additional logging&#160;to the checkOpen() method to log the &apos;key&apos;&#160;just before the exception is thrown:&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
/*
 * Decompiled with CFR.
 */
&lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt; org.apache.hadoop.fs.s3a;

&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; com.amazonaws.AmazonClientException;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; com.amazonaws.event.ProgressListener;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; com.amazonaws.services.s3.model.ObjectMetadata;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; com.amazonaws.services.s3.model.PutObjectRequest;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; com.amazonaws.services.s3.transfer.Upload;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; com.amazonaws.services.s3.transfer.model.UploadResult;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.io.BufferedOutputStream;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.io.File;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.io.FileOutputStream;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.io.IOException;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.io.InterruptedIOException;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.io.OutputStream;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.util.concurrent.atomic.AtomicBoolean;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.hadoop.classification.InterfaceAudience;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.hadoop.classification.InterfaceStability;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.hadoop.conf.Configuration;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.hadoop.fs.s3a.ProgressableProgressListener;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.hadoop.fs.s3a.S3AFileSystem;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.hadoop.fs.s3a.S3AUtils;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.hadoop.util.Progressable;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.slf4j.Logger;

@InterfaceAudience.Private
@InterfaceStability.Evolving
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;S3AOutputStream
&lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; OutputStream {
    &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; OutputStream backupStream;
    &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; File backupFile;
    &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; AtomicBoolean closed = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; AtomicBoolean(&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;);
    &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; key;
    &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Progressable progress;
    &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; S3AFileSystem fs;
    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Logger LOG = S3AFileSystem.LOG;

    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; S3AOutputStream(Configuration conf, S3AFileSystem fs, &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; key, Progressable progress) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
        &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.key = key;
        &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.progress = progress;
        &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.fs = fs;
        &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.backupFile = fs.createTmpFileForWrite(&lt;span class=&quot;code-quote&quot;&gt;&quot;output-&quot;&lt;/span&gt;, -1, conf);
        LOG.debug(&lt;span class=&quot;code-quote&quot;&gt;&quot;OutputStream &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; key &lt;span class=&quot;code-quote&quot;&gt;&apos;{}&apos;&lt;/span&gt; writing to tempfile: {}&quot;&lt;/span&gt;, (&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;)key, (&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;)&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.backupFile);
        &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.backupStream = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; BufferedOutputStream(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; FileOutputStream(&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.backupFile));
    }

    void checkOpen() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.closed.get()) &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt;;

        &lt;span class=&quot;code-comment&quot;&gt;// vvvvvv-- Additional logging --vvvvvvv
&lt;/span&gt;
        LOG.error(&lt;span class=&quot;code-quote&quot;&gt;&quot;OutputStream &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; key &lt;span class=&quot;code-quote&quot;&gt;&apos;{}&apos;&lt;/span&gt; closed.&quot;&lt;/span&gt;, (&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;)&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.key);


        &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; IOException(&lt;span class=&quot;code-quote&quot;&gt;&quot;Output Stream closed&quot;&lt;/span&gt;);
    }

    @Override
    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void flush() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
        &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.checkOpen();
        &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.backupStream.flush();
    }

    @Override
    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void close() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.closed.getAndSet(&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)) {
            &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt;;
        }
        &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.backupStream.close();
        LOG.debug(&lt;span class=&quot;code-quote&quot;&gt;&quot;OutputStream &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; key &lt;span class=&quot;code-quote&quot;&gt;&apos;{}&apos;&lt;/span&gt; closed. Now beginning upload&quot;&lt;/span&gt;, (&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;)&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.key);
        &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
            ObjectMetadata om = &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.fs.newObjectMetadata(&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.backupFile.length());
            Upload upload = &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.fs.putObject(&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.fs.newPutObjectRequest(&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.key, om, &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.backupFile));
            ProgressableProgressListener listener = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ProgressableProgressListener(&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.fs, &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.key, upload, &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.progress);
            upload.addProgressListener((ProgressListener)listener);
            upload.waitForUploadResult();
            listener.uploadCompleted();
            &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.fs.finishedWrite(&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.key);
        }
        &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (InterruptedException e) {
            &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; (InterruptedIOException)&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; InterruptedIOException(e.toString()).initCause(e);
        }
        &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (AmazonClientException e) {
            &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; S3AUtils.translateException(&lt;span class=&quot;code-quote&quot;&gt;&quot;saving output&quot;&lt;/span&gt;, &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.key, e);
        }
        &lt;span class=&quot;code-keyword&quot;&gt;finally&lt;/span&gt; {
            &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.backupFile.delete()) {
                LOG.warn(&lt;span class=&quot;code-quote&quot;&gt;&quot;Could not delete temporary s3a file: {}&quot;&lt;/span&gt;, (&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;)&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.backupFile);
            }
            &lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;.close();
        }
        LOG.debug(&lt;span class=&quot;code-quote&quot;&gt;&quot;OutputStream &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; key &lt;span class=&quot;code-quote&quot;&gt;&apos;{}&apos;&lt;/span&gt; upload complete&quot;&lt;/span&gt;, (&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;)&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.key);
    }

    @Override
    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void write(&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; b) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
        &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.checkOpen();
        &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.backupStream.write(b);
    }

    @Override
    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void write(&lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt;[] b, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; off, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; len) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
        &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.checkOpen();
        &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.backupStream.write(b, off, len);
    }

    &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; {
    }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;You can see from this addition log output that the S3AOutputStream#close() method *&lt;b&gt;appears&lt;/b&gt;* to be called before the S3AOutputStream#flush() method:&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2018-02-01 12:42:20,698 DEBUG org.apache.hadoop.fs.s3a.S3AFileSystem.Progress               - PUT landingzone/2018-02-01--1240/_part-0-0.in-progress: 128497 bytes
2018-02-01 12:42:20,910 DEBUG org.apache.hadoop.fs.s3a.S3AFileSystem.Progress               - PUT landingzone/2018-02-01--1240/_part-0-0.in-progress: 0 bytes
2018-02-01 12:42:20,910 DEBUG org.apache.hadoop.fs.s3a.S3AFileSystem.Progress               - PUT landingzone/2018-02-01--1240/_part-0-0.in-progress: 0 bytes
2018-02-01 12:42:20,910 DEBUG org.apache.hadoop.fs.s3a.S3AFileSystem.Progress               - PUT landingzone/2018-02-01--1240/_part-0-0.in-progress: 0 bytes
2018-02-01 12:42:20,910 DEBUG org.apache.hadoop.fs.s3a.S3AFileSystem.Progress               - PUT landingzone/2018-02-01--1240/_part-0-0.in-progress: 0 bytes
2018-02-01 12:42:20,910 DEBUG org.apache.hadoop.fs.s3a.S3AFileSystem.Progress               - PUT landingzone/2018-02-01--1240/_part-0-0.in-progress: 0 bytes
2018-02-01 12:42:20,911 DEBUG org.apache.hadoop.fs.s3a.S3AFileSystem                        - Finished write to landingzone/2018-02-01--1240/_part-0-0.in-progress
2018-02-01 12:42:20,911 DEBUG org.apache.hadoop.fs.s3a.S3AFileSystem                        - object_delete_requests += 1  -&amp;gt;  3

vvvvv- close() is called here? -vvvvv

2018-02-01 12:42:21,212 DEBUG org.apache.hadoop.fs.s3a.S3AFileSystem                        
- OutputStream &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; key &lt;span class=&quot;code-quote&quot;&gt;&apos;landingzone/2018-02-01--1240/_part-0-0.in-progress&apos;&lt;/span&gt; upload complete

vvvvv- flush() is called here? -vvvvv

2018-02-01 12:42:21,212 ERROR org.apache.hadoop.fs.s3a.S3AFileSystem                        
- OutputStream &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; key &lt;span class=&quot;code-quote&quot;&gt;&apos;landingzone/2018-02-01--1240/_part-0-0.in-progress&apos;&lt;/span&gt; closed.

2018-02-01 12:42:21,212 INFO  org.apache.flink.runtime.taskmanager.Task                     
- Attempting to fail task externally Source: Custom Source -&amp;gt; Map -&amp;gt; Sink: Unnamed (1/2) (510c8316d3a249e5ea5b8d8e693f7beb).
2018-02-01 12:42:21,214 INFO  org.apache.flink.runtime.taskmanager.Task                     - Source: Custom Source -&amp;gt; Map -&amp;gt; Sink: Unnamed (1/2) (510c8316d3a249e5ea5b8d8e693f7beb) switched from RUNNING to FAILED.
TimerException{java.io.IOException: Output Stream closed}
	at org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeService$TriggerTask.run(SystemProcessingTimeService.java:252)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
Caused by: java.io.IOException: Output Stream closed
	at org.apache.hadoop.fs.s3a.S3AOutputStream.checkOpen(S3AOutputStream.java:83)
	at org.apache.hadoop.fs.s3a.S3AOutputStream.flush(S3AOutputStream.java:89)
	at java.io.FilterOutputStream.flush(FilterOutputStream.java:140)
	at java.io.DataOutputStream.flush(DataOutputStream.java:123)
	at java.io.FilterOutputStream.flush(FilterOutputStream.java:140)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:141)
	at org.apache.avro.io.BufferedBinaryEncoder$OutputStreamSink.innerFlush(BufferedBinaryEncoder.java:220)
	at org.apache.avro.io.BufferedBinaryEncoder.flush(BufferedBinaryEncoder.java:85)
	at org.apache.avro.file.DataFileWriter.flush(DataFileWriter.java:368)
	at org.apache.avro.file.DataFileWriter.close(DataFileWriter.java:375)
	at org.apache.flink.streaming.connectors.fs.AvroKeyValueSinkWriter$AvroKeyValueWriter.close(AvroKeyValueSinkWriter.java:251)
	at org.apache.flink.streaming.connectors.fs.AvroKeyValueSinkWriter.close(AvroKeyValueSinkWriter.java:163)
	at org.apache.flink.streaming.connectors.fs.bucketing.BucketingSink.closeCurrentPartFile(BucketingSink.java:551)
	at org.apache.flink.streaming.connectors.fs.bucketing.BucketingSink.checkForInactiveBuckets(BucketingSink.java:493)
	at org.apache.flink.streaming.connectors.fs.bucketing.BucketingSink.onProcessingTime(BucketingSink.java:476)
	at org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeService$TriggerTask.run(SystemProcessingTimeService.java:249)
	... 7 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment>&lt;p&gt;IBM Analytics Engine - &lt;a href=&quot;https://console.bluemix.net/docs/services/AnalyticsEngine/index.html#introduction&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://console.bluemix.net/docs/services/AnalyticsEngine/index.html#introduction&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The cluster is based on Hortonworks Data Platform 2.6.2. The following components are made available.&lt;/p&gt;

&lt;p&gt;Apache Spark 2.1.1 Hadoop 2.7.3&lt;br/&gt;
Apache Livy 0.3.0&lt;br/&gt;
Knox 0.12.0&lt;br/&gt;
Ambari 2.5.2&lt;br/&gt;
Anaconda with Python 2.7.13 and 3.5.2&#160;&lt;br/&gt;
Jupyter Enterprise Gateway 0.5.0&#160;&lt;br/&gt;
HBase 1.1.2 *&#160;&lt;br/&gt;
Hive 1.2.1 *&lt;br/&gt;
Oozie 4.2.0 *&lt;br/&gt;
Flume 1.5.2 *&#160;&lt;br/&gt;
Tez 0.7.0 *&#160;&lt;br/&gt;
Pig 0.16.0 *&#160;&lt;br/&gt;
Sqoop 1.4.6 *&#160;&lt;br/&gt;
Slider 0.92.0 *&#160;&lt;/p&gt;</environment>
        <key id="13135418">FLINK-8543</key>
            <summary>Output Stream closed at org.apache.hadoop.fs.s3a.S3AOutputStream.checkOpen</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="snowch">chris snow</reporter>
                        <labels>
                    </labels>
                <created>Thu, 1 Feb 2018 13:26:09 +0000</created>
                <updated>Tue, 27 Mar 2018 23:10:37 +0000</updated>
                            <resolved>Tue, 27 Feb 2018 06:19:57 +0000</resolved>
                                    <version>1.4.0</version>
                                    <fixVersion>1.4.3</fixVersion>
                    <fixVersion>1.5.0</fixVersion>
                                    <component>Connectors / Common</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="16355322" author="zentol" created="Wed, 7 Feb 2018 11:12:25 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aljoscha&quot; class=&quot;user-hover&quot; rel=&quot;aljoscha&quot;&gt;aljoscha&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=kkl0u&quot; class=&quot;user-hover&quot; rel=&quot;kkl0u&quot;&gt;kkl0u&lt;/a&gt; Could it be that a timer can fire after the function was already closed? I&apos;ve found this code in the StreamTask class:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; (lock) {
	&lt;span class=&quot;code-comment&quot;&gt;// &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; is part of the main logic, so &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; fails, the task is considered failed
&lt;/span&gt;	closeAllOperators();

	&lt;span class=&quot;code-comment&quot;&gt;// make sure no &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; timers can come
&lt;/span&gt;	timerService.quiesce();

	&lt;span class=&quot;code-comment&quot;&gt;// only set the StreamTask to not running after all operators have been closed!
&lt;/span&gt;	&lt;span class=&quot;code-comment&quot;&gt;// See FLINK-7430
&lt;/span&gt;	isRunning = &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;;
}

&lt;span class=&quot;code-comment&quot;&gt;// make sure all timers finish
&lt;/span&gt;timerService.awaitPendingAfterQuiesce();
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="16355643" author="aljoscha" created="Wed, 7 Feb 2018 16:06:27 +0000"  >&lt;p&gt;I thought this shouldn&apos;t be possible because the code that calls the timer callback uses the same lock as the block you posted and also checks whether the timer service is not quiesced: &lt;a href=&quot;https://github.com/apache/flink/blob/d86c6b6bb32adee9d4b5c9098340a34e8a8a7f1d/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SystemProcessingTimeService.java#L249&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/blob/d86c6b6bb32adee9d4b5c9098340a34e8a8a7f1d/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SystemProcessingTimeService.java#L249&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So after that synchronized block exits that block shouldn&apos;t call the callback anymore.&lt;/p&gt;</comment>
                            <comment id="16356524" author="snowch" created="Thu, 8 Feb 2018 06:04:56 +0000"  >&lt;p&gt;One thing to note - I&apos;ve used the standard Flink&#160;1.4.0&#160;download for hadoop 2.7 but I am running on a Hortonworks&#160;based hadoop environment.&#160; I&apos;m not sure what impact this may have, but I thought it was worth mentioning in case this may have an impact.&lt;/p&gt;</comment>
                            <comment id="16359852" author="snowch" created="Sun, 11 Feb 2018 09:06:47 +0000"  >&lt;p&gt;I&apos;m hoping that I can get access to an internal cluster that will give me root access and hence more debugging capabilities.  I&#8217;m thinking of adding some code to print out the stacktrace and the thread ID from the flush() and close() methods.&lt;/p&gt;

&lt;p&gt;Are there any other areas that you would like me to investigate?&lt;/p&gt;</comment>
                            <comment id="16360470" author="aljoscha" created="Mon, 12 Feb 2018 09:14:40 +0000"  >&lt;p&gt;There is no other failure or anything suspicious in the logs, correct?&lt;/p&gt;</comment>
                            <comment id="16360477" author="aljoscha" created="Mon, 12 Feb 2018 09:17:28 +0000"  >&lt;p&gt;And was there a failure/restore cycle before that error popped up?&lt;/p&gt;</comment>
                            <comment id="16360480" author="aljoscha" created="Mon, 12 Feb 2018 09:25:56 +0000"  >&lt;p&gt;Also, I think it would help to have the stack trace at the logging statements that you added, i.e. add a &lt;tt&gt;ExceptionUtils.getStackTrace(new Exception())&lt;/tt&gt; in there.&lt;/p&gt;</comment>
                            <comment id="16364895" author="snowch" created="Wed, 14 Feb 2018 22:58:12 +0000"  >&lt;p&gt;I didn&#8217;t see any errors or suspicious entries in the logs prior to this error.&lt;/p&gt;

&lt;p&gt;I&#8217;ll try running again in a few days with the extra stacktrace logging.&lt;/p&gt;

&lt;p&gt;Thanks, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aljoscha&quot; class=&quot;user-hover&quot; rel=&quot;aljoscha&quot;&gt;aljoscha&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16365506" author="stevel@apache.org" created="Thu, 15 Feb 2018 13:18:31 +0000"  >&lt;blockquote&gt;&lt;p&gt;I am running on a Hortonworks based hadoop environment&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;which version? HDP-2.6&apos;s hadoop-aws module is essentially that of ASF Hadoop-2.8.0; &lt;br/&gt;
if you make sure that fast output is enabled &lt;tt&gt;fs.s3a.fast.upload = true&lt;/tt&gt; and the buffering =&amp;gt; disk &lt;tt&gt;fs.s3a.fast.upload.buffer=disk&lt;/tt&gt; to get the best upload perf. But that isn&apos;t going to do anything for flush(), which again does a state check when called.&lt;/p&gt;

&lt;p&gt;FWIW, the semantics of &lt;tt&gt;Flushable.flush()&lt;/tt&gt; are pretty vague, including what it is meant to do (hence hadoop&apos;s hflush/hsync methods, which the S3A streams explcitly don&apos;t support). There is a valid case for saying &quot;downgrade flush() on closed file to a warn + no-op&quot;.&lt;/p&gt;

&lt;p&gt;Moving to the fast output stream will make close() that much faster on big writes, as it will only be uploading the last block and any previous ones not yet uploaded; if the bandwidth to S3 &amp;gt; rate bytes are generated, it should just be the time to upload that last block: a few seconds. That may make a difference, though its still a bit of a race condition lurking. &lt;/p&gt;</comment>
                            <comment id="16365520" author="stevel@apache.org" created="Thu, 15 Feb 2018 13:29:35 +0000"  >&lt;p&gt;Created &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-15239&quot; title=&quot;S3ABlockOutputStream.flush() be no-op when stream closed&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-15239&quot;&gt;&lt;del&gt;HADOOP-15239&lt;/del&gt;&lt;/a&gt; ; I&apos;ll take a patch with a new test method in org.apache.hadoop.fs.s3a.ITestS3ABlockOutputArray, least expensive being adding &lt;tt&gt;stream.flush()&lt;/tt&gt; at the tail end of &lt;tt&gt;testBlocksClosed&lt;/tt&gt;. Your chance to contribute to the hadoop codebase &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="16370073" author="aljoscha" created="Tue, 20 Feb 2018 14:03:46 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stevel%40apache.org&quot; class=&quot;user-hover&quot; rel=&quot;stevel@apache.org&quot;&gt;stevel@apache.org&lt;/a&gt; That would fix this issue but I think it would be better to fix this at the Flink level, and to figure out why &lt;tt&gt;flush()&lt;/tt&gt; is called after &lt;tt&gt;close()&lt;/tt&gt; in the first place.&lt;/p&gt;</comment>
                            <comment id="16370094" author="aljoscha" created="Tue, 20 Feb 2018 14:18:27 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=snowch&quot; class=&quot;user-hover&quot; rel=&quot;snowch&quot;&gt;snowch&lt;/a&gt; Btw, what is the writer you&apos;re using in this case?&lt;/p&gt;</comment>
                            <comment id="16370123" author="snowch" created="Tue, 20 Feb 2018 14:57:07 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aljoscha&quot; class=&quot;user-hover&quot; rel=&quot;aljoscha&quot;&gt;aljoscha&lt;/a&gt; -&#160;I&#8217;m using AvroKeyValueSinkWriter &lt;a href=&quot;https://github.com/ibm-cloud-streaming-retail-demo/flink-on-iae-messagehub-to-s3/blob/master/src/main/java/com/ibm/cloud/flink/StreamingJob.java#L186&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/ibm-cloud-streaming-retail-demo/flink-on-iae-messagehub-to-s3/blob/master/src/main/java/com/ibm/cloud/flink/StreamingJob.java#L186&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stevel%40apache.org&quot; class=&quot;user-hover&quot; rel=&quot;stevel@apache.org&quot;&gt;stevel@apache.org&lt;/a&gt; - HDP 2.6.2:&#160;&lt;a href=&quot;https://console.bluemix.net/docs/services/AnalyticsEngine/index.html#introduction&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://console.bluemix.net/docs/services/AnalyticsEngine/index.html#introduction&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16370167" author="aljoscha" created="Tue, 20 Feb 2018 15:53:21 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=snowch&quot; class=&quot;user-hover&quot; rel=&quot;snowch&quot;&gt;snowch&lt;/a&gt; Do you see these failures for all files or only sporadically? Also, do you maybe have updated logs with stack traces that show who calls &lt;tt&gt;close()&lt;/tt&gt; and &lt;tt&gt;flush()&lt;/tt&gt;?&lt;/p&gt;</comment>
                            <comment id="16371403" author="snowch" created="Wed, 21 Feb 2018 13:27:15 +0000"  >&lt;p&gt;Sorry for the delay. I&apos;ve added the debug statements:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; S3AOutputStream(Configuration conf,
      S3AFileSystem fs,
      &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; key,
      Progressable progress)
      &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
    &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.key = key;
    &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.progress = progress;
    &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.fs = fs;


    backupFile = fs.createTmpFileForWrite(&lt;span class=&quot;code-quote&quot;&gt;&quot;output-&quot;&lt;/span&gt;,
        LocalDirAllocator.SIZE_UNKNOWN, conf);

    LOG.debug(&lt;span class=&quot;code-quote&quot;&gt;&quot;OutputStream &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; key &lt;span class=&quot;code-quote&quot;&gt;&apos;{}&apos;&lt;/span&gt; writing to tempfile: {}&quot;&lt;/span&gt;,
        key, backupFile);

    &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.backupStream = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; BufferedOutputStream(
        &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; FileOutputStream(backupFile));
  }


  &lt;span class=&quot;code-comment&quot;&gt;// ** print extra debug output **
&lt;/span&gt;
  void printStackTrace() {
    &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; threadId = &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.currentThread().getId();
    StringBuilder sb = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; StringBuilder();
    sb.append(&lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt; id: &quot;&lt;/span&gt; + &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.currentThread().getId() + &lt;span class=&quot;code-quote&quot;&gt;&quot; key: &quot;&lt;/span&gt; + key);
    &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (StackTraceElement ste : &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.currentThread().getStackTrace()) {
      sb.append(&lt;span class=&quot;code-quote&quot;&gt;&quot;\n     &quot;&lt;/span&gt; + ste);
    }
    &lt;span class=&quot;code-comment&quot;&gt;// I&apos;m being lazy - log the stacktrace as an error so it will get logged without having to 
&lt;/span&gt;    &lt;span class=&quot;code-comment&quot;&gt;// change the logger configuration 
&lt;/span&gt;    LOG.error(sb.toString());
  }

  /**
   * Check &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; the filesystem being open.
   * @&lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the filesystem is closed.
   */
  void checkOpen() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (closed.get()) {
      printStackTrace();
      &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; IOException(
              &lt;span class=&quot;code-quote&quot;&gt;&quot;Output Stream closed.  &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt; id: &quot;&lt;/span&gt; + &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.currentThread().getId() + &lt;span class=&quot;code-quote&quot;&gt;&quot; key: &quot;&lt;/span&gt; + key);
    }
  }

  @Override
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void flush() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
    checkOpen();
    backupStream.flush();
  }

  @Override
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void close() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {

    printStackTrace();

    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (closed.getAndSet(&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)) {
      &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt;;
    }

    backupStream.close();
    LOG.debug(&lt;span class=&quot;code-quote&quot;&gt;&quot;OutputStream &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; key &lt;span class=&quot;code-quote&quot;&gt;&apos;{}&apos;&lt;/span&gt; closed. Now beginning upload&quot;&lt;/span&gt;, key);

    &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
      &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; ObjectMetadata om = fs.newObjectMetadata(backupFile.length());
      Upload upload = fs.putObject(
          fs.newPutObjectRequest(
              key,
              om,
              backupFile));
      ProgressableProgressListener listener =
          &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ProgressableProgressListener(fs, key, upload, progress);
      upload.addProgressListener(listener);

      upload.waitForUploadResult();
      listener.uploadCompleted();
      &lt;span class=&quot;code-comment&quot;&gt;// This will delete unnecessary fake parent directories
&lt;/span&gt;      fs.finishedWrite(key);
    } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (InterruptedException e) {
      &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; (InterruptedIOException) &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; InterruptedIOException(e.toString())
          .initCause(e);
    } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (AmazonClientException e) {
      &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; translateException(&lt;span class=&quot;code-quote&quot;&gt;&quot;saving output&quot;&lt;/span&gt;, key , e);
    } &lt;span class=&quot;code-keyword&quot;&gt;finally&lt;/span&gt; {
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!backupFile.delete()) {
        LOG.warn(&lt;span class=&quot;code-quote&quot;&gt;&quot;Could not delete temporary s3a file: {}&quot;&lt;/span&gt;, backupFile);
      }
      &lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;.close();
    }
    LOG.debug(&lt;span class=&quot;code-quote&quot;&gt;&quot;OutputStream &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; key &lt;span class=&quot;code-quote&quot;&gt;&apos;{}&apos;&lt;/span&gt; upload complete&quot;&lt;/span&gt;, key);
  }

  @Override
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void write(&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; b) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
    checkOpen();
    backupStream.write(b);
  }

  @Override
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void write(&lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt;[] b, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; off, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; len) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
    checkOpen();
    backupStream.write(b, off, len);
  }

}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;And here is one of the yarn logs:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Log Contents:
2018-02-21 12:43:11,323 INFO  org.apache.flink.yarn.YarnTaskManagerRunner                   - --------------------------------------------------------------------------------
2018-02-21 12:43:11,326 INFO  org.apache.flink.yarn.YarnTaskManagerRunner                   -  Starting YARN TaskManager (Version: 1.4.0, Rev:3a9d9f2, Date:06.12.2017 @ 11:08:40 UTC)
2018-02-21 12:43:11,326 INFO  org.apache.flink.yarn.YarnTaskManagerRunner                   -  OS current user: clsadmin
2018-02-21 12:43:12,162 WARN  org.apache.hadoop.util.NativeCodeLoader                       - Unable to load &lt;span class=&quot;code-keyword&quot;&gt;native&lt;/span&gt;-hadoop library &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; your platform... using builtin-java classes where applicable
2018-02-21 12:43:12,263 INFO  org.apache.flink.yarn.YarnTaskManagerRunner                   -  Current Hadoop/Kerberos user: clsadmin
2018-02-21 12:43:12,263 INFO  org.apache.flink.yarn.YarnTaskManagerRunner                   -  JVM: Java HotSpot(TM) 64-Bit Server VM - Oracle Corporation - 1.8/25.112-b15
2018-02-21 12:43:12,263 INFO  org.apache.flink.yarn.YarnTaskManagerRunner                   -  Maximum heap size: 406 MiBytes
2018-02-21 12:43:12,264 INFO  org.apache.flink.yarn.YarnTaskManagerRunner                   -  JAVA_HOME: /usr/jdk64/jdk1.8.0_112
2018-02-21 12:43:12,266 INFO  org.apache.flink.yarn.YarnTaskManagerRunner                   -  Hadoop version: 2.7.3.2.6.2.0-205
        ... 
        omitted &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; brevity
        ... 
2018-02-21 12:43:12,267 INFO  org.apache.flink.yarn.YarnTaskManagerRunner                   - --------------------------------------------------------------------------------
2018-02-21 12:43:12,270 INFO  org.apache.flink.yarn.YarnTaskManagerRunner                   - Registered UNIX signal handlers &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; [TERM, HUP, INT]
2018-02-21 12:43:12,705 INFO  org.apache.flink.runtime.taskmanager.TaskManager              - Loading configuration from .
        ... 
        omitted &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; brevity
        ... 
property: taskmanager.memory.preallocate, &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;
2018-02-21 12:43:12,778 INFO  org.apache.flink.yarn.YarnTaskManagerRunner                   - Current working/local Directory: /disk1/hadoop-swap/yarn/local/usercache/clsadmin/appcache/application_1519207944666_0008,/disk2/hadoop-swap/yarn/local/usercache/clsadmin/appcache/application_1519207944666_0008
2018-02-21 12:43:12,778 INFO  org.apache.flink.yarn.YarnTaskManagerRunner                   - Current working Directory: /disk1/hadoop-swap/yarn/local/usercache/clsadmin/appcache/application_1519207944666_0008/container_e01_1519207944666_0008_01_000002
2018-02-21 12:43:12,778 INFO  org.apache.flink.yarn.YarnTaskManagerRunner                   - TM: remoteKeytabPath obtained &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;
2018-02-21 12:43:12,778 INFO  org.apache.flink.yarn.YarnTaskManagerRunner                   - TM: remoteKeytabPrincipal obtained &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;
2018-02-21 12:43:12,779 INFO  org.apache.flink.yarn.YarnTaskManagerRunner                   - Setting directories &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; temporary file /disk1/hadoop-swap/yarn/local/usercache/clsadmin/appcache/application_1519207944666_0008,/disk2/hadoop-swap/yarn/local/usercache/clsadmin/appcache/application_1519207944666_0008
2018-02-21 12:43:12,780 INFO  org.apache.flink.yarn.YarnTaskManagerRunner                   - YARN daemon is running as: clsadmin Yarn client user obtainer: clsadmin
2018-02-21 12:43:12,781 INFO  org.apache.flink.yarn.YarnTaskManagerRunner                   - ResourceID assigned &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; container: container_e01_1519207944666_0008_01_000002
2018-02-21 12:43:12,857 INFO  org.apache.flink.runtime.security.modules.HadoopModule        - Hadoop user set to clsadmin (auth:SIMPLE)
2018-02-21 12:43:13,007 INFO  org.apache.flink.runtime.util.LeaderRetrievalUtils            - Trying to select the network &lt;span class=&quot;code-keyword&quot;&gt;interface&lt;/span&gt; and address to use by connecting to the leading JobManager.
2018-02-21 12:43:13,008 INFO  org.apache.flink.runtime.util.LeaderRetrievalUtils            - TaskManager will &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; to connect &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 10000 milliseconds before falling back to heuristics
2018-02-21 12:43:13,012 INFO  org.apache.flink.runtime.net.ConnectionUtils                  - Retrieved &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; target address chs-van-484-dn001.bi.services.us-south.bluemix.net/172.16.16.130:38012.
2018-02-21 12:43:13,032 INFO  org.apache.flink.runtime.taskmanager.TaskManager              - TaskManager will use hostname/address &lt;span class=&quot;code-quote&quot;&gt;&apos;chs-van-484-dn001.bi.services.us-south.bluemix.net&apos;&lt;/span&gt; (172.16.16.130) &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; communication.
2018-02-21 12:43:13,042 INFO  org.apache.flink.runtime.taskmanager.TaskManager              - Starting TaskManager
2018-02-21 12:43:13,043 INFO  org.apache.flink.runtime.taskmanager.TaskManager              - Starting TaskManager actor system at chs-van-484-dn001.bi.services.us-south.bluemix.net:38907.
2018-02-21 12:43:13,045 INFO  org.apache.flink.runtime.taskmanager.TaskManager              - Trying to start actor system at chs-van-484-dn001.bi.services.us-south.bluemix.net:38907
2018-02-21 12:43:13,977 INFO  akka.event.slf4j.Slf4jLogger                                  - Slf4jLogger started
2018-02-21 12:43:14,145 INFO  akka.remote.Remoting                                          - Starting remoting
2018-02-21 12:43:14,504 INFO  akka.remote.Remoting                                          - Remoting started; listening on addresses :[akka.tcp:&lt;span class=&quot;code-comment&quot;&gt;//flink@chs-van-484-dn001.bi.services.us-south.bluemix.net:38907]
&lt;/span&gt;2018-02-21 12:43:14,522 INFO  org.apache.flink.runtime.taskmanager.TaskManager              - Actor system started at akka.tcp:&lt;span class=&quot;code-comment&quot;&gt;//flink@chs-van-484-dn001.bi.services.us-south.bluemix.net:38907
&lt;/span&gt;2018-02-21 12:43:14,539 INFO  org.apache.flink.runtime.metrics.MetricRegistryImpl           - No metrics reporter configured, no metrics will be exposed/reported.
2018-02-21 12:43:14,544 INFO  org.apache.flink.runtime.taskmanager.TaskManager              - Starting TaskManager actor
2018-02-21 12:43:14,552 INFO  org.apache.flink.runtime.io.network.netty.NettyConfig         - NettyConfig [server address: chs-van-484-dn001.bi.services.us-south.bluemix.net/172.16.16.130, server port: 0, ssl enabled: &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;, memory segment size (bytes): 32768, transport type: NIO, number of server threads: 1 (manual), number of client threads: 1 (manual), server connect backlog: 0 (use Netty&lt;span class=&quot;code-quote&quot;&gt;&apos;s &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;), client connect timeout (sec): 120, send/receive buffer size (bytes): 0 (use Netty&apos;&lt;/span&gt;s &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;)]
2018-02-21 12:43:14,559 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerConfiguration  - Messages have a max timeout of 10000 ms
2018-02-21 12:43:14,568 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerServices     - Temporary file directory &lt;span class=&quot;code-quote&quot;&gt;&apos;/disk1/hadoop-swap/yarn/local/usercache/clsadmin/appcache/application_1519207944666_0008&apos;&lt;/span&gt;: total 299 GB, usable 298 GB (99.67% usable)
2018-02-21 12:43:14,568 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerServices     - Temporary file directory &lt;span class=&quot;code-quote&quot;&gt;&apos;/disk2/hadoop-swap/yarn/local/usercache/clsadmin/appcache/application_1519207944666_0008&apos;&lt;/span&gt;: total 299 GB, usable 298 GB (99.67% usable)
2018-02-21 12:43:14,649 INFO  org.apache.flink.runtime.io.network.buffer.NetworkBufferPool  - Allocated 64 MB &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
2018-02-21 12:43:14,878 WARN  org.apache.flink.runtime.query.QueryableStateUtils            - Could not load Queryable State Client Proxy. Probable reason: flink-queryable-state-runtime is not in the classpath. Please put the corresponding jar from the opt to the lib folder.
2018-02-21 12:43:14,879 WARN  org.apache.flink.runtime.query.QueryableStateUtils            - Could not load Queryable State Server. Probable reason: flink-queryable-state-runtime is not in the classpath. Please put the corresponding jar from the opt to the lib folder.
2018-02-21 12:43:14,881 INFO  org.apache.flink.runtime.io.network.NetworkEnvironment        - Starting the network environment and its components.
2018-02-21 12:43:14,965 INFO  org.apache.flink.runtime.io.network.netty.NettyClient         - Successful initialization (took 69 ms).
2018-02-21 12:43:15,049 INFO  org.apache.flink.runtime.io.network.netty.NettyServer         - Successful initialization (took 84 ms). Listening on SocketAddress /172.16.16.130:33982.
2018-02-21 12:43:15,135 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerServices     - Limiting managed memory to 0.7 of the currently free heap space (234 MB), memory will be allocated lazily.
2018-02-21 12:43:15,148 INFO  org.apache.flink.runtime.io.disk.iomanager.IOManager          - I/O manager uses directory /disk1/hadoop-swap/yarn/local/usercache/clsadmin/appcache/application_1519207944666_0008/flink-io-9dbfdfda-909a-45da-aab7-d82d826edf4b &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; spill files.
2018-02-21 12:43:15,148 INFO  org.apache.flink.runtime.io.disk.iomanager.IOManager          - I/O manager uses directory /disk2/hadoop-swap/yarn/local/usercache/clsadmin/appcache/application_1519207944666_0008/flink-io-d4a1da3f-22a4-4a62-9589-050c2343dc59 &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; spill files.
2018-02-21 12:43:15,156 INFO  org.apache.flink.runtime.filecache.FileCache                  - User file cache uses directory /disk1/hadoop-swap/yarn/local/usercache/clsadmin/appcache/application_1519207944666_0008/flink-dist-cache-80d88a48-c391-44ec-b61d-cb293b20fe5a
2018-02-21 12:43:15,156 INFO  org.apache.flink.runtime.filecache.FileCache                  - User file cache uses directory /disk2/hadoop-swap/yarn/local/usercache/clsadmin/appcache/application_1519207944666_0008/flink-dist-cache-58fa3a64-ae94-44c9-b491-98c5dd89d426
2018-02-21 12:43:15,319 INFO  org.apache.flink.runtime.filecache.FileCache                  - User file cache uses directory /disk1/hadoop-swap/yarn/local/usercache/clsadmin/appcache/application_1519207944666_0008/flink-dist-cache-e6f21f36-1929-4e9c-a6e6-9225907dc7e4
2018-02-21 12:43:15,320 INFO  org.apache.flink.runtime.filecache.FileCache                  - User file cache uses directory /disk2/hadoop-swap/yarn/local/usercache/clsadmin/appcache/application_1519207944666_0008/flink-dist-cache-47bc021d-52c9-4f35-abe1-dd9024f08c8f
2018-02-21 12:43:15,344 INFO  org.apache.flink.yarn.YarnTaskManager                         - Starting TaskManager actor at akka:&lt;span class=&quot;code-comment&quot;&gt;//flink/user/taskmanager#-1252244271.
&lt;/span&gt;2018-02-21 12:43:15,345 INFO  org.apache.flink.yarn.YarnTaskManager                         - TaskManager data connection information: container_e01_1519207944666_0008_01_000002 @ chs-van-484-dn001.bi.services.us-south.bluemix.net (dataPort=33982)
2018-02-21 12:43:15,346 INFO  org.apache.flink.yarn.YarnTaskManager                         - TaskManager has 1 task slot(s).
2018-02-21 12:43:15,348 INFO  org.apache.flink.yarn.YarnTaskManager                         - Memory usage stats: [HEAP: 77/406/406 MB, NON HEAP: 35/36/-1 MB (used/committed/max)]
2018-02-21 12:43:15,363 INFO  org.apache.flink.yarn.YarnTaskManager                         - Trying to register at JobManager akka.tcp:&lt;span class=&quot;code-comment&quot;&gt;//flink@chs-van-484-dn001.bi.services.us-south.bluemix.net:38012/user/jobmanager (attempt 1, timeout: 500 milliseconds)
&lt;/span&gt;2018-02-21 12:43:15,711 INFO  org.apache.flink.yarn.YarnTaskManager                         - Successful registration at JobManager (akka.tcp:&lt;span class=&quot;code-comment&quot;&gt;//flink@chs-van-484-dn001.bi.services.us-south.bluemix.net:38012/user/jobmanager), starting network stack and library cache.
&lt;/span&gt;2018-02-21 12:43:15,723 INFO  org.apache.flink.yarn.YarnTaskManager                         - Determined BLOB server address to be chs-van-484-dn001.bi.services.us-south.bluemix.net/172.16.16.130:41971. Starting BLOB cache.
2018-02-21 12:43:15,734 INFO  org.apache.flink.runtime.blob.PermanentBlobCache              - Created BLOB cache storage directory /tmp/blobStore-cd4d61de-14b1-454e-9cd6-f91ed292bd04
2018-02-21 12:43:15,741 INFO  org.apache.flink.runtime.blob.TransientBlobCache              - Created BLOB cache storage directory /tmp/blobStore-b489e08d-4731-4751-9034-567ffef2d9a8
2018-02-21 12:43:16,333 INFO  org.apache.flink.yarn.YarnTaskManager                         - Received task Source: Custom Source -&amp;gt; Map -&amp;gt; Sink: Unnamed (1/1)
2018-02-21 12:43:16,334 INFO  org.apache.flink.runtime.taskmanager.Task                     - Source: Custom Source -&amp;gt; Map -&amp;gt; Sink: Unnamed (1/1) (fddbde91f648fdb9acebf5ac44a32740) switched from CREATED to DEPLOYING.
2018-02-21 12:43:16,335 INFO  org.apache.flink.runtime.taskmanager.Task                     - Creating FileSystem stream leak safety net &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; task Source: Custom Source -&amp;gt; Map -&amp;gt; Sink: Unnamed (1/1) (fddbde91f648fdb9acebf5ac44a32740) [DEPLOYING]
2018-02-21 12:43:16,347 INFO  org.apache.flink.runtime.taskmanager.Task                     - Loading JAR files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; task Source: Custom Source -&amp;gt; Map -&amp;gt; Sink: Unnamed (1/1) (fddbde91f648fdb9acebf5ac44a32740) [DEPLOYING].
2018-02-21 12:43:16,382 INFO  org.apache.flink.runtime.taskmanager.Task                     - Registering task at network: Source: Custom Source -&amp;gt; Map -&amp;gt; Sink: Unnamed (1/1) (fddbde91f648fdb9acebf5ac44a32740) [DEPLOYING].
2018-02-21 12:43:16,390 INFO  org.apache.flink.runtime.taskmanager.Task                     - Source: Custom Source -&amp;gt; Map -&amp;gt; Sink: Unnamed (1/1) (fddbde91f648fdb9acebf5ac44a32740) switched from DEPLOYING to RUNNING.
2018-02-21 12:43:16,401 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask           - No state backend has been configured, using &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt; state backend (Memory / JobManager)
2018-02-21 12:43:18,711 INFO  org.apache.hadoop.conf.Configuration.deprecation              - fs.s3a.server-side-encryption-key is deprecated. Instead, use fs.s3a.server-side-encryption.key
2018-02-21 12:43:19,631 WARN  org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory       - The &lt;span class=&quot;code-object&quot;&gt;short&lt;/span&gt;-circuit local reads feature cannot be used because libhadoop cannot be loaded.

*** 
*** vvvv NOT AN ERROR - JUST LOGGED AS ERROR vvvv ***
*** I think &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; can be ignored.  We &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; not see errors later on &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; file.
*** 

2018-02-21 12:43:20,000 ERROR org.apache.hadoop.fs.s3a.S3AFileSystem                        - &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt; id: 44 key: user/clsadmin/72566a07-0f62-42b2-84fe-99d2355477f7
     java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.getStackTrace(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:1556)
     org.apache.hadoop.fs.s3a.S3AOutputStream.printStackTrace(S3AOutputStream.java:81)
     org.apache.hadoop.fs.s3a.S3AOutputStream.close(S3AOutputStream.java:108)
     org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
     org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
     org.apache.flink.streaming.connectors.fs.bucketing.BucketingSink.reflectTruncate(BucketingSink.java:592)
     org.apache.flink.streaming.connectors.fs.bucketing.BucketingSink.initializeState(BucketingSink.java:362)
     org.apache.flink.streaming.util.functions.StreamingFunctionUtils.tryRestoreFunction(StreamingFunctionUtils.java:178)
     org.apache.flink.streaming.util.functions.StreamingFunctionUtils.restoreFunctionState(StreamingFunctionUtils.java:160)
     org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.initializeState(AbstractUdfStreamOperator.java:96)
     org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:259)
     org.apache.flink.streaming.runtime.tasks.StreamTask.initializeOperators(StreamTask.java:694)
     org.apache.flink.streaming.runtime.tasks.StreamTask.initializeState(StreamTask.java:682)
     org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:253)
     org.apache.flink.runtime.taskmanager.Task.run(Task.java:718)
     java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
2018-02-21 12:43:21,357 INFO  org.apache.flink.streaming.connectors.fs.bucketing.BucketingSink  - No state to restore &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; the BucketingSink (taskIdx=0).
2018-02-21 12:43:21,412 INFO  org.apache.flink.api.java.typeutils.TypeExtractor             - &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; field topic
2018-02-21 12:43:21,412 INFO  org.apache.flink.api.java.typeutils.TypeExtractor             - &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition is not a valid POJO type because not all fields are valid POJO fields.
2018-02-21 12:43:21,422 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase  - No restore state &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; FlinkKafkaConsumer.
2018-02-21 12:43:21,472 INFO  org.apache.kafka.clients.consumer.ConsumerConfig              - ConsumerConfig values:
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka02-prod02.messagehub.services.us-south.bluemix.net:9093, kafka04-prod02.messagehub.services.us-south.bluemix.net:9093, kafka03-prod02.messagehub.services.us-south.bluemix.net:9093, kafka01-prod02.messagehub.services.us-south.bluemix.net:9093, kafka05-prod02.messagehub.services.us-south.bluemix.net:9093]
	check.crcs = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;
	client.id =
        ... 
        omitted &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; brevity
        ... 
	value.deserializer = &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.kafka.common.serialization.ByteArrayDeserializer

2018-02-21 12:43:21,528 INFO  org.apache.kafka.common.security.authenticator.AbstractLogin  - Successfully logged in.
2018-02-21 12:43:21,613 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version : 0.11.0.2
2018-02-21 12:43:21,613 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId : 73be1e1168f91ee2
2018-02-21 12:43:21,938 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase  - Consumer subtask 0 will start reading the following 1 partitions from the committed group offsets in Kafka: [KafkaTopicPartition{topic=&lt;span class=&quot;code-quote&quot;&gt;&apos;transactions_load&apos;&lt;/span&gt;, partition=0}]
2018-02-21 12:43:21,950 INFO  org.apache.flink.runtime.state.DefaultOperatorStateBackend    - DefaultOperatorStateBackend snapshot (In-Memory Stream Factory, synchronous part) in thread &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;[Async calls on Source: Custom Source -&amp;gt; Map -&amp;gt; Sink: Unnamed (1/1),5,Flink Task Threads] took 4 ms.
2018-02-21 12:43:21,968 INFO  org.apache.kafka.clients.consumer.ConsumerConfig              - ConsumerConfig values:
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka02-prod02.messagehub.services.us-south.bluemix.net:9093, kafka04-prod02.messagehub.services.us-south.bluemix.net:9093, kafka03-prod02.messagehub.services.us-south.bluemix.net:9093, kafka01-prod02.messagehub.services.us-south.bluemix.net:9093, kafka05-prod02.messagehub.services.us-south.bluemix.net:9093]
	check.crcs = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;
	client.id =
        ... 
        omitted &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; brevity
        ... 
	ssl.truststore.type = JKS
	value.deserializer = &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.kafka.common.serialization.ByteArrayDeserializer

2018-02-21 12:43:21,974 INFO  org.apache.kafka.common.security.authenticator.AbstractLogin  - Successfully logged in.
2018-02-21 12:43:21,982 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version : 0.11.0.2
2018-02-21 12:43:21,983 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId : 73be1e1168f91ee2
2018-02-21 12:43:22,078 INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator kafka08-prod02.messagehub.services.us-south.bluemix.net:9093 (id: 2147483640 rack: &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; group kafka-flink-iae-streaming-demo.
2018-02-21 12:43:22,273 INFO  org.apache.flink.runtime.state.DefaultOperatorStateBackend    - DefaultOperatorStateBackend snapshot (In-Memory Stream Factory, synchronous part) in thread &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;[Async calls on Source: Custom Source -&amp;gt; Map -&amp;gt; Sink: Unnamed (1/1),5,Flink Task Threads] took 320 ms.
2018-02-21 12:43:22,316 INFO  org.apache.flink.runtime.state.DefaultOperatorStateBackend    - DefaultOperatorStateBackend snapshot (In-Memory Stream Factory, asynchronous part) in thread &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;[pool-4-thread-1,5,Flink Task Threads] took 34 ms.
...
repeated code omitted
...
2018-02-21 12:45:20,368 INFO  org.apache.flink.runtime.state.DefaultOperatorStateBackend    - DefaultOperatorStateBackend snapshot (In-Memory Stream Factory, asynchronous part) in thread &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;[pool-4-thread-1,5,Flink Task Threads] took 0 ms.
2018-02-21 12:45:20,368 INFO  org.apache.flink.runtime.state.DefaultOperatorStateBackend    - DefaultOperatorStateBackend snapshot (In-Memory Stream Factory, asynchronous part) in thread &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;[pool-4-thread-1,5,Flink Task Threads] took 0 ms.

*** 
*** vvvv NOT AN ERROR - JUST LOGGED AS ERROR vvvv ***
*** This is the code that first closes the output stream, resulting in the later flush throwing an exception
***

2018-02-21 12:45:21,426 ERROR org.apache.hadoop.fs.s3a.S3AFileSystem                        - &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt; id: 53 key: transactions_load_20180221/2018-02-21--1243/_part-0-0.in-progress
     java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.getStackTrace(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:1556)
     org.apache.hadoop.fs.s3a.S3AOutputStream.printStackTrace(S3AOutputStream.java:81)
     org.apache.hadoop.fs.s3a.S3AOutputStream.close(S3AOutputStream.java:108)
     org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
     org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
     org.apache.flink.streaming.connectors.fs.StreamWriterBase.close(StreamWriterBase.java:100)
     org.apache.flink.streaming.connectors.fs.AvroKeyValueSinkWriter.close(AvroKeyValueSinkWriter.java:161)
     org.apache.flink.streaming.connectors.fs.bucketing.BucketingSink.closeCurrentPartFile(BucketingSink.java:551)
     org.apache.flink.streaming.connectors.fs.bucketing.BucketingSink.checkForInactiveBuckets(BucketingSink.java:493)
     org.apache.flink.streaming.connectors.fs.bucketing.BucketingSink.onProcessingTime(BucketingSink.java:476)
     org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeService$TriggerTask.run(SystemProcessingTimeService.java:249)
     java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
     java.util.concurrent.FutureTask.run(FutureTask.java:266)
     java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
     java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
     java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
     java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
     java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)

*** 
*** vvvv NOT AN ERROR - JUST LOGGED AS ERROR vvvv ***
*** This stacktrace is printed immediately before the exception
***

2018-02-21 12:45:22,155 ERROR org.apache.hadoop.fs.s3a.S3AFileSystem                        - &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt; id: 53 key: transactions_load_20180221/2018-02-21--1243/_part-0-0.in-progress
     java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.getStackTrace(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:1556)
     org.apache.hadoop.fs.s3a.S3AOutputStream.printStackTrace(S3AOutputStream.java:81)
     org.apache.hadoop.fs.s3a.S3AOutputStream.checkOpen(S3AOutputStream.java:93)
     org.apache.hadoop.fs.s3a.S3AOutputStream.flush(S3AOutputStream.java:101)
     java.io.FilterOutputStream.flush(FilterOutputStream.java:140)
     java.io.DataOutputStream.flush(DataOutputStream.java:123)
     java.io.FilterOutputStream.flush(FilterOutputStream.java:140)
     java.io.BufferedOutputStream.flush(BufferedOutputStream.java:141)
     org.apache.avro.io.BufferedBinaryEncoder$OutputStreamSink.innerFlush(BufferedBinaryEncoder.java:220)
     org.apache.avro.io.BufferedBinaryEncoder.flush(BufferedBinaryEncoder.java:85)
     org.apache.avro.file.DataFileWriter.flush(DataFileWriter.java:368)
     org.apache.avro.file.DataFileWriter.close(DataFileWriter.java:375)
     org.apache.flink.streaming.connectors.fs.AvroKeyValueSinkWriter$AvroKeyValueWriter.close(AvroKeyValueSinkWriter.java:251)
     org.apache.flink.streaming.connectors.fs.AvroKeyValueSinkWriter.close(AvroKeyValueSinkWriter.java:163)
     org.apache.flink.streaming.connectors.fs.bucketing.BucketingSink.closeCurrentPartFile(BucketingSink.java:551)
     org.apache.flink.streaming.connectors.fs.bucketing.BucketingSink.checkForInactiveBuckets(BucketingSink.java:493)
     org.apache.flink.streaming.connectors.fs.bucketing.BucketingSink.onProcessingTime(BucketingSink.java:476)
     org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeService$TriggerTask.run(SystemProcessingTimeService.java:249)
     java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
     java.util.concurrent.FutureTask.run(FutureTask.java:266)
     java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
     java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
     java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
     java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
     java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
2018-02-21 12:45:22,155 INFO  org.apache.flink.runtime.taskmanager.Task                     - Attempting to fail task externally Source: Custom Source -&amp;gt; Map -&amp;gt; Sink: Unnamed (1/1) (fddbde91f648fdb9acebf5ac44a32740).

*** 
*** vvvv ACTUAL ERROR vvvv ***
***

2018-02-21 12:45:22,157 INFO  org.apache.flink.runtime.taskmanager.Task                     - Source: Custom Source -&amp;gt; Map -&amp;gt; Sink: Unnamed (1/1) (fddbde91f648fdb9acebf5ac44a32740) switched from RUNNING to FAILED.
TimerException{java.io.IOException: Output Stream closed.  &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt; id: 53 key: transactions_load_20180221/2018-02-21--1243/_part-0-0.in-progress}
	at org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeService$TriggerTask.run(SystemProcessingTimeService.java:252)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
Caused by: java.io.IOException: Output Stream closed.  &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt; id: 53 key: transactions_load_20180221/2018-02-21--1243/_part-0-0.in-progress
	at org.apache.hadoop.fs.s3a.S3AOutputStream.checkOpen(S3AOutputStream.java:95)
	at org.apache.hadoop.fs.s3a.S3AOutputStream.flush(S3AOutputStream.java:101)
	at java.io.FilterOutputStream.flush(FilterOutputStream.java:140)
	at java.io.DataOutputStream.flush(DataOutputStream.java:123)
	at java.io.FilterOutputStream.flush(FilterOutputStream.java:140)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:141)
	at org.apache.avro.io.BufferedBinaryEncoder$OutputStreamSink.innerFlush(BufferedBinaryEncoder.java:220)
	at org.apache.avro.io.BufferedBinaryEncoder.flush(BufferedBinaryEncoder.java:85)
	at org.apache.avro.file.DataFileWriter.flush(DataFileWriter.java:368)
	at org.apache.avro.file.DataFileWriter.close(DataFileWriter.java:375)
	at org.apache.flink.streaming.connectors.fs.AvroKeyValueSinkWriter$AvroKeyValueWriter.close(AvroKeyValueSinkWriter.java:251)
	at org.apache.flink.streaming.connectors.fs.AvroKeyValueSinkWriter.close(AvroKeyValueSinkWriter.java:163)
	at org.apache.flink.streaming.connectors.fs.bucketing.BucketingSink.closeCurrentPartFile(BucketingSink.java:551)
	at org.apache.flink.streaming.connectors.fs.bucketing.BucketingSink.checkForInactiveBuckets(BucketingSink.java:493)
	at org.apache.flink.streaming.connectors.fs.bucketing.BucketingSink.onProcessingTime(BucketingSink.java:476)
	at org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeService$TriggerTask.run(SystemProcessingTimeService.java:249)
	... 7 more
2018-02-21 12:45:22,172 INFO  org.apache.flink.runtime.taskmanager.Task                     - Triggering cancellation of task code Source: Custom Source -&amp;gt; Map -&amp;gt; Sink: Unnamed (1/1) (fddbde91f648fdb9acebf5ac44a32740).
2018-02-21 12:45:22,179 ERROR org.apache.flink.streaming.runtime.tasks.StreamTask           - Could not shut down timer service
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2067)
	at java.util.concurrent.ThreadPoolExecutor.awaitTermination(ThreadPoolExecutor.java:1465)
	at org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeService.shutdownAndAwaitPending(SystemProcessingTimeService.java:197)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:317)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:718)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Run with &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
${FLINK_HOME}/bin/flink run -m yarn-cluster -yn 1 $YARN_BACKGROUND /home/clsadmin/messagehub-to-s3-1.0-SNAPSHOT.jar   --kafka-brokers ${KAFKA_BROKERS}   --kafka-topic ${KAFKA_TOPIC}   --kafka-username ${KAFKA_USERNAME}   --kafka-password ${KAFKA_PASSWORD}   --kafka-group-id ${KAFKA_GROUP_ID}   --output-folder s3:&lt;span class=&quot;code-comment&quot;&gt;//${S3_BUCKET}/${S3_FOLDER}   --output-bucket-format-string ${S3_BUCKET_FORMAT_STRING}&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="16371405" author="snowch" created="Wed, 21 Feb 2018 13:31:06 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aljoscha&quot; class=&quot;user-hover&quot; rel=&quot;aljoscha&quot;&gt;aljoscha&lt;/a&gt; The failures seem to be happening for all files.   I&apos;m running my code on cloud cluster that I provisioned just for this testing.  I&apos;m happy to share credentials if you want to take a more detailed look.  My code is open source and the cluster will be destroyed after these tests, so there&apos;s nothing sensitive on it.&lt;/p&gt;</comment>
                            <comment id="16371445" author="aljoscha" created="Wed, 21 Feb 2018 14:08:19 +0000"  >&lt;p&gt;Ok, this confirms my suspicion. The problem is this part of the code: &lt;a href=&quot;https://github.com/apache/flink/blob/537a10ea2ff6a2d8507483c66f413f77884e77c4/flink-connectors/flink-connector-filesystem/src/main/java/org/apache/flink/streaming/connectors/fs/AvroKeyValueSinkWriter.java#L163:&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/blob/537a10ea2ff6a2d8507483c66f413f77884e77c4/flink-connectors/flink-connector-filesystem/src/main/java/org/apache/flink/streaming/connectors/fs/AvroKeyValueSinkWriter.java#L163:&lt;/a&gt; &lt;tt&gt;super.close()&lt;/tt&gt; will close the underlying stream, &lt;tt&gt;keyValueWriter.close()&lt;/tt&gt; will eventually call &lt;tt&gt;DataFileWriter.close()&lt;/tt&gt; which will in turn call &lt;tt&gt;flush()&lt;/tt&gt; on the underlying stream.&lt;/p&gt;

&lt;p&gt;You could try copying &lt;tt&gt;AvroKeyValueSinkWriter&lt;/tt&gt; to your own code and changing the order of calls in &lt;tt&gt;AvroKeyValueSinkWriter.close()&lt;/tt&gt;, or omitting &lt;tt&gt;super.close()&lt;/tt&gt; altogether.&lt;/p&gt;

&lt;p&gt;On a side note, the &lt;tt&gt;BucketingSink&lt;/tt&gt; does currently not work well with eventually consistent filesystems, such as S3, because it depends on directory listings being accurate for some operations. We are aware of that and want to fix this for Flink 1.6, though.&lt;/p&gt;</comment>
                            <comment id="16371485" author="snowch" created="Wed, 21 Feb 2018 14:35:40 +0000"  >&lt;p&gt;Unfortunately, commenting out the call to super.close() doesn&apos;t stop the exception:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    @Override
    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void close() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
        &lt;span class=&quot;code-comment&quot;&gt;// See https://issues.apache.org/jira/browse/FLINK-8543?focusedCommentId=16371445&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16371445
&lt;/span&gt;        &lt;span class=&quot;code-comment&quot;&gt;//&lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;.close(); //the order is important since &lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;.close flushes inside
&lt;/span&gt;        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (keyValueWriter != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
             keyValueWriter.close();
        }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;On the consistency note, hopefully, IBM COS S3 will be ok as it is immediately consistent?&lt;/p&gt;

&lt;p&gt;&amp;gt; COS is &#8216;immediately consistent&#8217; for data and &#8216;eventually consistent&#8217; for usage accounting.&lt;br/&gt;
Source: &lt;a href=&quot;https://ibm-public-cos.github.io/crs-docs/faq&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://ibm-public-cos.github.io/crs-docs/faq&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16371570" author="aljoscha" created="Wed, 21 Feb 2018 15:50:35 +0000"  >&lt;p&gt;Regarding IBM COS S3: Yes, that sounds good.&lt;/p&gt;

&lt;p&gt;Would you also have the debug logs with stracktraces for the run without &lt;tt&gt;super.close()&lt;/tt&gt;?&lt;/p&gt;</comment>
                            <comment id="16371613" author="snowch" created="Wed, 21 Feb 2018 16:24:54 +0000"  >&lt;p&gt;My apologies, the commented out close() didn&apos;t get deployed due to a compile error including my own version of the class so I removed the close with a bytecode modifier, i.e.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;class&lt;/b&gt;: org.apache.flink.streaming.connectors.fs.AvroKeyValueSinkWriter&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void close() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.keyValueWriter != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
            &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.keyValueWriter.close();
        }
    }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It is running without exception now, however, every file in COS S3 still has the valid-length suffix:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
...
transactions_load_20180221/2018-02-21--1454/_part-0-0.valid-length	   7.0 bytes	21/02/2018 14:56:10	
transactions_load_20180221/2018-02-21--1454/part-0-0	                   22.6 KB	21/02/2018 14:56:06	
transactions_load_20180221/2018-02-21--1455/_part-0-0.valid-length	   7.0 bytes	21/02/2018 14:56:05	
transactions_load_20180221/2018-02-21--1455/part-0-0	                   14.5 KB	21/02/2018 14:56:03
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I configured log output to debug level on Bucketing sink and see this in the logs:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2018-02-21 16:45:18,993 DEBUG org.apache.flink.streaming.connectors.fs.bucketing.BucketingSink  - Truncate is not supported.
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.streaming.connectors.fs.bucketing.BucketingSink.reflectTruncate(BucketingSink.java:599)
	at org.apache.flink.streaming.connectors.fs.bucketing.BucketingSink.initializeState(BucketingSink.java:362)
	at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.tryRestoreFunction(StreamingFunctionUtils.java:178)
	at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.restoreFunctionState(StreamingFunctionUtils.java:160)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.initializeState(AbstractUdfStreamOperator.java:96)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:259)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeOperators(StreamTask.java:694)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeState(StreamTask.java:682)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:253)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:718)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
Caused by: java.lang.UnsupportedOperationException: Not implemented by the S3AFileSystem FileSystem implementation
	at org.apache.hadoop.fs.FileSystem.truncate(FileSystem.java:1364)
	... 15 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Is this the reason for the .valid-length files?  If so, is this because the java code doesn&apos;t support Truncate or because the filesystem (IBM COS S3) doesn&apos;t support it? It looks as though it is not supported by my hadoop library:&lt;/p&gt;

&lt;p&gt;&lt;b&gt;File&lt;/b&gt;: ./hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; truncate(Path f, &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; newLength) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
    &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; UnsupportedOperationException(&lt;span class=&quot;code-quote&quot;&gt;&quot;Not implemented by the &quot;&lt;/span&gt; +
        getClass().getSimpleName() + &lt;span class=&quot;code-quote&quot;&gt;&quot; FileSystem implementation&quot;&lt;/span&gt;);
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
$ grep &lt;span class=&quot;code-quote&quot;&gt;&apos;truncate&apos;&lt;/span&gt; hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java
[[string not found]]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="16371773" author="aljoscha" created="Wed, 21 Feb 2018 18:07:15 +0000"  >&lt;p&gt;Yes, this is what I would expect since the Hadoop S3A filesystem does not support truncate. Do you know if IBM COS would support truncate? Could you use an alternative &lt;tt&gt;FileSystem&lt;/tt&gt; implementation?&lt;/p&gt;</comment>
                            <comment id="16371855" author="snowch" created="Wed, 21 Feb 2018 18:56:13 +0000"  >&lt;p&gt;Does Flink running on EMR using S3 have the same issue? &#160;If not, what AWS S3 API calls and filesystem implementation are used?&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;IBM COS S3 supports a subset of the most common AWS S3 API operations (&lt;a href=&quot;https://ibm-public-cos.github.io/crs-docs/api-reference#copy-an-object&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://ibm-public-cos.github.io/crs-docs/api-reference#copy-an-object&lt;/a&gt;).&lt;/p&gt;</comment>
                            <comment id="16371870" author="stevel@apache.org" created="Wed, 21 Feb 2018 19:05:49 +0000"  >&lt;p&gt;There&apos;s no truncate operation in the S3 protocol; so not in the s3a connector, nor in the azure or swift clients. Not fixable. Flush-after-close, that can be downgraded, but truncate isn&apos;t part of the S3 object model of immutable objects.. &lt;/p&gt;</comment>
                            <comment id="16371882" author="snowch" created="Wed, 21 Feb 2018 19:14:40 +0000"  >&lt;p&gt;So all files being accompanied by .valid-length is expected behavoir when saving to s3?&lt;/p&gt;

&lt;p&gt;If so, unless client applications can understand and use the .valid-length (which I don&#8217;t think will be the case), I don&#8217;t think this functionality makes sense with s3? &#160;I.e. am I trying to do something with Flink that it wasn&#8217;t designed to do?&lt;/p&gt;</comment>
                            <comment id="16371976" author="stevel@apache.org" created="Wed, 21 Feb 2018 20:38:31 +0000"  >&lt;blockquote&gt;&lt;p&gt;So all files being accompanied by .valid-length is expected behavoir when saving to s3?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;that&apos;s nothing to do with the s3a connector, so ask the flink team. &lt;/p&gt;

&lt;p&gt;One thing to always remember is: Object Stores are not Filesystems.&lt;/p&gt;

&lt;p&gt;Yes, they appear to support the same API, have that same metaphor of directories and files, but they are different, and if you try too hard, you will discover that things you expect aren&apos;t there. This may be one of them&lt;/p&gt;</comment>
                            <comment id="16372534" author="snowch" created="Thu, 22 Feb 2018 08:18:26 +0000"  >&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aljoscha&quot; class=&quot;user-hover&quot; rel=&quot;aljoscha&quot;&gt;aljoscha&lt;/a&gt; - what are your thoughts on this?&#160;&lt;/p&gt;</comment>
                            <comment id="16372643" author="aljoscha" created="Thu, 22 Feb 2018 10:45:32 +0000"  >&lt;p&gt;The comments are correct, S3 simply does not support truncate. The reason why we need it is that we need to trim files in case of recovering from a failure to get them back to the state they had when we did the last checkpoint. To ensure exactly-once semantics. The solution we came up with for filesystems that don&apos;t support truncate is the &lt;tt&gt;.valid-length&lt;/tt&gt; files because we simply cannot take back what was already written but you&apos;re right that the client reading those files needs to understand them.&lt;/p&gt;

&lt;p&gt;Coming up with a better solution will be one of the goals of the 1.6 release cycle, though.&lt;/p&gt;</comment>
                            <comment id="16372701" author="stevel@apache.org" created="Thu, 22 Feb 2018 11:26:59 +0000"  >&lt;p&gt;copying a subset of the old file to the new file would seem to be the way. Not ideal as you&apos;ve read everything in and write it back, plus with update inconsistency clients could still get the old file for a bit. Writting a .valid-length field is a better strategy when everything knows to check it.&lt;/p&gt;</comment>
                            <comment id="16372808" author="aljoscha" created="Thu, 22 Feb 2018 13:33:27 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=snowch&quot; class=&quot;user-hover&quot; rel=&quot;snowch&quot;&gt;snowch&lt;/a&gt; The solution for this bug, though, is to remove the &lt;tt&gt;super.close()&lt;/tt&gt; call, right?&lt;/p&gt;</comment>
                            <comment id="16373022" author="githubbot" created="Thu, 22 Feb 2018 16:26:48 +0000"  >&lt;p&gt;GitHub user aljoscha opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/5563&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/5563&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-8543&quot; title=&quot;Output Stream closed at org.apache.hadoop.fs.s3a.S3AOutputStream.checkOpen&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-8543&quot;&gt;&lt;del&gt;FLINK-8543&lt;/del&gt;&lt;/a&gt; Don&apos;t call super.close() in AvroKeyValueSinkWriter&lt;/p&gt;

&lt;p&gt;    The call to keyValueWriter.close() in AvroKeyValueSinkWriter.close()&lt;br/&gt;
    will eventually call flush() on the wrapped stream which fails if we&lt;br/&gt;
    close it before(). Now we call flush ourselves before closing the&lt;br/&gt;
    KeyValyeWriter, which internally closes the wrapped stream eventually.&lt;/p&gt;


&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/aljoscha/flink&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/aljoscha/flink&lt;/a&gt; jira-8543-fix-avro-writer-stream-close&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/5563.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/5563.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #5563&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 548dea4c0811ffbfaf1566aaae88551f29eb5af9&lt;br/&gt;
Author: Aljoscha Krettek &amp;lt;aljoscha.krettek@...&amp;gt;&lt;br/&gt;
Date:   2018-02-22T16:24:33Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-8543&quot; title=&quot;Output Stream closed at org.apache.hadoop.fs.s3a.S3AOutputStream.checkOpen&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-8543&quot;&gt;&lt;del&gt;FLINK-8543&lt;/del&gt;&lt;/a&gt; Don&apos;t call super.close() in AvroKeyValueSinkWriter&lt;/p&gt;

&lt;p&gt;    The call to keyValueWriter.close() in AvroKeyValueSinkWriter.close()&lt;br/&gt;
    will eventually call flush() on the wrapped stream which fails if we&lt;br/&gt;
    close it before(). Now we call flush ourselves before closing the&lt;br/&gt;
    KeyValyeWriter, which internally closes the wrapped stream eventually.&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="16373026" author="githubbot" created="Thu, 22 Feb 2018 16:28:12 +0000"  >&lt;p&gt;Github user aljoscha commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/5563&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/5563&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @IgorBerman Do you have an idea whether this change could cause problems? As the original implementer.&lt;/p&gt;
</comment>
                            <comment id="16376565" author="githubbot" created="Mon, 26 Feb 2018 09:09:11 +0000"  >&lt;p&gt;Github user zentol commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/5563#discussion_r170528786&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/5563#discussion_r170528786&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-filesystem/src/main/java/org/apache/flink/streaming/connectors/fs/AvroKeyValueSinkWriter.java &amp;#8212;&lt;br/&gt;
    @@ -158,7 +158,7 @@ public void open(FileSystem fs, Path path) throws IOException {&lt;/p&gt;

&lt;p&gt;     	@Override&lt;br/&gt;
     	public void close() throws IOException {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super.close(); //the order is important since super.close flushes inside&lt;br/&gt;
    +		flush(); // super.close() also does a flush&lt;br/&gt;
     		if (keyValueWriter != null) {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    the writer already flushes when calling close.&lt;/p&gt;

&lt;p&gt;    If the vriter is null we should call super.close() to make sure the stream is closed.&lt;/p&gt;</comment>
                            <comment id="16376571" author="githubbot" created="Mon, 26 Feb 2018 09:16:26 +0000"  >&lt;p&gt;Github user zentol commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/5563#discussion_r170530494&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/5563#discussion_r170530494&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-filesystem/src/main/java/org/apache/flink/streaming/connectors/fs/AvroKeyValueSinkWriter.java &amp;#8212;&lt;br/&gt;
    @@ -158,7 +158,7 @@ public void open(FileSystem fs, Path path) throws IOException {&lt;/p&gt;

&lt;p&gt;     	@Override&lt;br/&gt;
     	public void close() throws IOException {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super.close(); //the order is important since super.close flushes inside&lt;br/&gt;
    +		flush(); // super.close() also does a flush&lt;br/&gt;
     		if (keyValueWriter != null) {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    alternatively, we could wrap the stream we give to avro and ignore close() calls, and then close it ourselves. The we wouldn&apos;t rely on implementation details of avro.&lt;/p&gt;</comment>
                            <comment id="16376888" author="githubbot" created="Mon, 26 Feb 2018 13:50:09 +0000"  >&lt;p&gt;Github user aljoscha commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/5563#discussion_r170596001&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/5563#discussion_r170596001&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-filesystem/src/main/java/org/apache/flink/streaming/connectors/fs/AvroKeyValueSinkWriter.java &amp;#8212;&lt;br/&gt;
    @@ -158,7 +158,7 @@ public void open(FileSystem fs, Path path) throws IOException {&lt;/p&gt;

&lt;p&gt;     	@Override&lt;br/&gt;
     	public void close() throws IOException {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super.close(); //the order is important since super.close flushes inside&lt;br/&gt;
    +		flush(); // super.close() also does a flush&lt;br/&gt;
     		if (keyValueWriter != null) {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    I would be in favour of just having&lt;br/&gt;
    ```&lt;br/&gt;
    @Override&lt;br/&gt;
    public void close() throws IOException {&lt;br/&gt;
    	if (keyValueWriter != null) &lt;/p&gt;
{
    		keyValueWriter.close();
    	}
&lt;p&gt;    }&lt;br/&gt;
    ```&lt;br/&gt;
    then. It seems all usual stream/file stuff seems to close the stream that was handed in.&lt;/p&gt;</comment>
                            <comment id="16376889" author="githubbot" created="Mon, 26 Feb 2018 13:50:19 +0000"  >&lt;p&gt;Github user aljoscha commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/5563#discussion_r170596050&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/5563#discussion_r170596050&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-filesystem/src/main/java/org/apache/flink/streaming/connectors/fs/AvroKeyValueSinkWriter.java &amp;#8212;&lt;br/&gt;
    @@ -158,7 +158,7 @@ public void open(FileSystem fs, Path path) throws IOException {&lt;/p&gt;

&lt;p&gt;     	@Override&lt;br/&gt;
     	public void close() throws IOException {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super.close(); //the order is important since super.close flushes inside&lt;br/&gt;
    +		flush(); // super.close() also does a flush&lt;br/&gt;
     		if (keyValueWriter != null) {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    WDYT?&lt;/p&gt;
</comment>
                            <comment id="16376943" author="githubbot" created="Mon, 26 Feb 2018 14:25:02 +0000"  >&lt;p&gt;Github user zentol commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/5563#discussion_r170606282&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/5563#discussion_r170606282&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-filesystem/src/main/java/org/apache/flink/streaming/connectors/fs/AvroKeyValueSinkWriter.java &amp;#8212;&lt;br/&gt;
    @@ -158,7 +158,7 @@ public void open(FileSystem fs, Path path) throws IOException {&lt;/p&gt;

&lt;p&gt;     	@Override&lt;br/&gt;
     	public void close() throws IOException {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super.close(); //the order is important since super.close flushes inside&lt;br/&gt;
    +		flush(); // super.close() also does a flush&lt;br/&gt;
     		if (keyValueWriter != null) {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    For this issue that would be enough.&lt;/p&gt;

&lt;p&gt;    However we can still leak streams if `open()` throws an exception after `super.open()` returns. Then the stream is already open. If you use the BucketingSink this will leak the stream:&lt;/p&gt;

&lt;p&gt;    ```&lt;br/&gt;
    // openNewPartFile&lt;br/&gt;
    bucketState.writer.open(fs, inProgressPath); // if this throws exception the stream can be open&lt;br/&gt;
    bucketState.isWriterOpen = true;&lt;br/&gt;
    ```&lt;/p&gt;

&lt;p&gt;    ```&lt;br/&gt;
    // closeCurrentPartFile&lt;br/&gt;
    if (bucketState.isWriterOpen) &lt;/p&gt;
{ // not closing partially opened writer
    	bucketState.writer.close();
    	bucketState.isWriterOpen = false;
    }
&lt;p&gt;    ```&lt;/p&gt;

&lt;p&gt;    So we should modify open() to properly close the stream.&lt;/p&gt;</comment>
                            <comment id="16377016" author="githubbot" created="Mon, 26 Feb 2018 15:12:06 +0000"  >&lt;p&gt;Github user IgorBerman commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/5563&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/5563&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @aljoscha, sorry for late response, I needed to refresh this piece of code. Overall, I think it&apos;s ok. I mean at the end I&apos;ve used same scheme of implementing SinkWriter as other examples, so if you refactoring something in hierarchy then it should have same handling.&lt;br/&gt;
    From avro file perspective(internal writer) you are closing it, which would be sufficient. What problems do you expect here?&lt;/p&gt;</comment>
                            <comment id="16377027" author="githubbot" created="Mon, 26 Feb 2018 15:23:39 +0000"  >&lt;p&gt;Github user aljoscha commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/5563#discussion_r170626581&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/5563#discussion_r170626581&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-filesystem/src/main/java/org/apache/flink/streaming/connectors/fs/AvroKeyValueSinkWriter.java &amp;#8212;&lt;br/&gt;
    @@ -158,7 +158,7 @@ public void open(FileSystem fs, Path path) throws IOException {&lt;/p&gt;

&lt;p&gt;     	@Override&lt;br/&gt;
     	public void close() throws IOException {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super.close(); //the order is important since super.close flushes inside&lt;br/&gt;
    +		flush(); // super.close() also does a flush&lt;br/&gt;
     		if (keyValueWriter != null) {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Ah yeah, I would call super if the `keyValueWriter` is `null`. That should do it, right?&lt;/p&gt;</comment>
                            <comment id="16377028" author="githubbot" created="Mon, 26 Feb 2018 15:24:08 +0000"  >&lt;p&gt;Github user aljoscha commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/5563&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/5563&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @IgorBerman Thanks! I&apos;m expecting no problems, just wanted to see if you maybe had an opinion.&lt;/p&gt;</comment>
                            <comment id="16377032" author="githubbot" created="Mon, 26 Feb 2018 15:28:44 +0000"  >&lt;p&gt;Github user zentol commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/5563#discussion_r170628342&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/5563#discussion_r170628342&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-filesystem/src/main/java/org/apache/flink/streaming/connectors/fs/AvroKeyValueSinkWriter.java &amp;#8212;&lt;br/&gt;
    @@ -158,7 +158,7 @@ public void open(FileSystem fs, Path path) throws IOException {&lt;/p&gt;

&lt;p&gt;     	@Override&lt;br/&gt;
     	public void close() throws IOException {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super.close(); //the order is important since super.close flushes inside&lt;br/&gt;
    +		flush(); // super.close() also does a flush&lt;br/&gt;
     		if (keyValueWriter != null) {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    No, because the bucketingSink never calls `close()` on the writer if `open()` threw an exception.&lt;/p&gt;</comment>
                            <comment id="16377039" author="githubbot" created="Mon, 26 Feb 2018 15:31:58 +0000"  >&lt;p&gt;Github user aljoscha commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/5563#discussion_r170629598&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/5563#discussion_r170629598&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-filesystem/src/main/java/org/apache/flink/streaming/connectors/fs/AvroKeyValueSinkWriter.java &amp;#8212;&lt;br/&gt;
    @@ -158,7 +158,7 @@ public void open(FileSystem fs, Path path) throws IOException {&lt;/p&gt;

&lt;p&gt;     	@Override&lt;br/&gt;
     	public void close() throws IOException {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super.close(); //the order is important since super.close flushes inside&lt;br/&gt;
    +		flush(); // super.close() also does a flush&lt;br/&gt;
     		if (keyValueWriter != null) {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Ah, so it&apos;s a pre-existing problem. I&apos;ll add a `finally` block for that.&lt;/p&gt;</comment>
                            <comment id="16377080" author="githubbot" created="Mon, 26 Feb 2018 16:01:17 +0000"  >&lt;p&gt;Github user aljoscha commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/5563&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/5563&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @zentol I pushed an update.&lt;/p&gt;</comment>
                            <comment id="16377506" author="githubbot" created="Mon, 26 Feb 2018 20:11:06 +0000"  >&lt;p&gt;Github user aljoscha closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/5563&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/5563&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16377510" author="githubbot" created="Mon, 26 Feb 2018 20:11:30 +0000"  >&lt;p&gt;Github user aljoscha commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/5563&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/5563&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    merged&lt;/p&gt;</comment>
                            <comment id="16378103" author="aljoscha" created="Tue, 27 Feb 2018 06:19:58 +0000"  >&lt;p&gt;Fixed on release-1.4 in&lt;br/&gt;
b74c705157fef3e0d305fdd5bf1a006ae0a98666&lt;/p&gt;

&lt;p&gt;Fixed on master in&lt;br/&gt;
915213c7afaf3f9d04c240f43d88710280d844e3&lt;/p&gt;</comment>
                            <comment id="16378211" author="snowch" created="Tue, 27 Feb 2018 08:22:28 +0000"  >&lt;p&gt;Thanks for fixing this &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aljoscha&quot; class=&quot;user-hover&quot; rel=&quot;aljoscha&quot;&gt;aljoscha&lt;/a&gt;!&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;The comments are correct, S3 simply does not support truncate. The reason why we need it is that we need to trim files in case of recovering from a failure to get them back to the state they had when we did the last checkpoint. To ensure exactly-once semantics. The solution we came up with for filesystems that don&apos;t support truncate is the .valid-length files because we simply cannot take back what was already written but you&apos;re right that the client reading those files needs to understand them.&lt;/p&gt;

&lt;p&gt;Coming up with a better solution will be one of the goals of the 1.6 release cycle, though.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Is there a Jira ticket I can track to follow the changes that will happen in 1.6 to provide better handing for s3 sinks?&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="13144954">FLINK-8939</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13138770">HADOOP-15239</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13144954">FLINK-8939</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12908801" name="Screen Shot 2018-01-30 at 18.34.51.png" size="74753" author="snowch" created="Thu, 1 Feb 2018 13:27:44 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 38 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3pnin:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>