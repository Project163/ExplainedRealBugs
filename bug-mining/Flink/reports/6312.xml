<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 21:09:42 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-26306] [Changelog] Thundering herd problem with materialization</title>
                <link>https://issues.apache.org/jira/browse/FLINK-26306</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;p&gt;Quick note: CheckpointCleaner is not involved here.&lt;/p&gt;

&lt;p&gt;When a checkpoint is subsumed, SharedStateRegistry schedules its unused shared state for async deletion. It uses common IO pool for this and adds a Runnable per state handle. ( see SharedStateRegistryImpl.scheduleAsyncDelete)&lt;/p&gt;

&lt;p&gt;When a checkpoint is started, CheckpointCoordinator uses the same thread pool to initialize the location for it. (see CheckpointCoordinator.initializeCheckpoint)&lt;/p&gt;

&lt;p&gt;The thread pool is of fixed size &lt;a href=&quot;https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#jobmanager-io-pool-size&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;jobmanager.io-pool.size&lt;/a&gt;; by default it&apos;s the number of CPU cores) and uses FIFO queue for tasks.&lt;/p&gt;

&lt;p&gt;When there is a spike in state deletion, the next checkpoint is delayed waiting for an available IO thread.&lt;/p&gt;

&lt;p&gt;Back-pressure seems reasonable here (similar to CheckpointCleaner); however, this shared state deletion could be spread across multiple subsequent checkpoints, not neccesarily the next one.&lt;/p&gt;

&lt;p&gt;----&#160;&lt;/p&gt;

&lt;p&gt;I believe the issue is an pre-existing one; but it particularly affects changelog state backend, because 1) such spikes are likely there; 2) workloads are latency sensitive.&lt;/p&gt;

&lt;p&gt;In the tests, checkpoint duration grows from seconds to minutes immediately after the materialization.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13429969">FLINK-26306</key>
            <summary>[Changelog] Thundering herd problem with materialization</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="roman">Roman Khachatryan</assignee>
                                    <reporter username="roman">Roman Khachatryan</reporter>
                        <labels>
                            <label>pull-request-available</label>
                    </labels>
                <created>Tue, 22 Feb 2022 16:05:35 +0000</created>
                <updated>Thu, 10 Mar 2022 15:09:33 +0000</updated>
                            <resolved>Thu, 10 Mar 2022 15:09:21 +0000</resolved>
                                    <version>1.15.0</version>
                                    <fixVersion>1.15.0</fixVersion>
                                    <component>Runtime / State Backends</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="17496196" author="roman_khachatryan" created="Tue, 22 Feb 2022 16:23:31 +0000"  >&lt;p&gt;Theoretically, I think there are the following ways to solve this problem:&lt;br/&gt;
&#160;&lt;br/&gt;
1. Batch deletions and leave one thread idle (e.g. group 1K handles into 10 big batches handled by 11 IO threads)&lt;br/&gt;
2. Spread changelog materialization across multiple checkpoints; i.e. materialize different tasks at different times&lt;br/&gt;
3. Use fork-join pool and rely on its task ordering (IIUC the implementation)&lt;br/&gt;
4. Use separate thread pools&lt;br/&gt;
5. Use unbounded thread pool&lt;br/&gt;
&#160;&lt;br/&gt;
(1) seems the way to go to me.&lt;br/&gt;
(2) is not guaranteed to improve and is limited to changelog&lt;br/&gt;
(3) relies on fork-join pool implementation&lt;br/&gt;
(4) and (5) can be quickly ruled out because they remove back-pressure completly&lt;br/&gt;
&#160;&lt;br/&gt;
(1) leaves one thread free for new checkpoints &#160;initially. If the rest don&apos;t keep up with deletion, then the next spike will consume the remaining capacity, and checkpointing will be back-pressured.&lt;br/&gt;
The size/number of batches should be determined by the pool itself, so we&apos;d need to wrap the original IO executor.&lt;br/&gt;
&#160;&lt;br/&gt;
WDYT &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pnowojski&quot; class=&quot;user-hover&quot; rel=&quot;pnowojski&quot;&gt;pnowojski&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=trohrmann%40apache.org&quot; class=&quot;user-hover&quot; rel=&quot;trohrmann@apache.org&quot;&gt;trohrmann@apache.org&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ym&quot; class=&quot;user-hover&quot; rel=&quot;ym&quot;&gt;ym&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yunta&quot; class=&quot;user-hover&quot; rel=&quot;yunta&quot;&gt;yunta&lt;/a&gt; ?&lt;/p&gt;</comment>
                            <comment id="17496504" author="yunta" created="Wed, 23 Feb 2022 06:45:48 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=roman&quot; class=&quot;user-hover&quot; rel=&quot;roman&quot;&gt;roman&lt;/a&gt; After materialization, I think the new checkpoint would be delayed to trigger instead of increasing the checkpoint duration, right?&lt;/p&gt;

&lt;p&gt;Why batch the deletion would help this? I think the idle thread actually make the effect not back-pressure, right?&lt;/p&gt;</comment>
                            <comment id="17496551" author="roman_khachatryan" created="Wed, 23 Feb 2022 08:04:26 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yunta&quot; class=&quot;user-hover&quot; rel=&quot;yunta&quot;&gt;yunta&lt;/a&gt; after materialization, triggering does start, but then it&apos;s stuck in location initialization. &lt;br/&gt;
In the UI, it&apos;s shown as long checkpoint duration (because trigger timestamp is generated in the very beginning). In the logs, &quot;triggering&quot; is printed with a delay. Most importantly, checkpointing is delayed and end-to-en latency increases.&lt;/p&gt;

&lt;p&gt;Batching deletions helps by occupying less threads and leaving more for the new checkpoint.&lt;/p&gt;</comment>
                            <comment id="17496567" author="pnowojski" created="Wed, 23 Feb 2022 08:30:43 +0000"  >&lt;p&gt;I&apos;ve changed type of this issue to an improvement.&lt;/p&gt;

&lt;p&gt;&amp;gt; 2. Spread changelog materialization across multiple checkpoints; i.e. materialize different tasks at different times&lt;/p&gt;

&lt;p&gt;Can you &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=roman&quot; class=&quot;user-hover&quot; rel=&quot;roman&quot;&gt;roman&lt;/a&gt; elaborate why would that help? Is it because materialised parts of the changelog checkpoints are causing those deletion spikes? If so, why is that the case? Why is this only because of the &quot;materialised parts&quot;?&lt;/p&gt;

&lt;p&gt;Maybe we should think about some more fair thread pool for async jobs? For example every async IO job could get assigned an id/key, and each id/key would have it&apos;s own queue of tasks to perform. Based on that we could implement all kinds of fancy priority schemes, but we could start with something as simple as just going in a round robing fashion through all individual per id/key queues when polling for a new task to execute. This could be generic and flexible enough to be re-used in other use cases (I was thinking about something like that for the TMs IO executor in the past).&lt;/p&gt;

&lt;p&gt;Re batching. Isn&apos;t this more of an independent potential optimisation that we could consider independently of the main issue? Depending how long is single IO operation. If it&apos;s more then a couple of ms, I would prefer to leave them separate.&lt;/p&gt;</comment>
                            <comment id="17496634" author="roman_khachatryan" created="Wed, 23 Feb 2022 10:10:25 +0000"  >&lt;p&gt;&amp;gt; &amp;gt; 2. Spread changelog materialization across multiple checkpoints; i.e. materialize different tasks at different times&lt;br/&gt;
&amp;gt; Can you Roman Khachatryan elaborate why would that help? Is it because materialised parts of the changelog checkpoints are causing those deletion spikes? If so, why is that the case? Why is this only because of the &quot;materialised parts&quot;?&lt;/p&gt;

&lt;p&gt;Let me calrify what happens currently:&lt;br/&gt;
1. JM triggers a checkpoint&lt;br/&gt;
2. TMs send non-materialized changes&lt;br/&gt;
3. The above is repeated until the materialization happens (with 1s checkpoint and 10m materialization interval - that&apos;s 600 checkpoints times nr. of tasks))&lt;br/&gt;
4. Materialization finishes more or less simultaneously on all tasks&lt;br/&gt;
5. Checkpoint N is triggered - TMs don&apos;t send &quot;old&quot; non-materialized state (only mat. state + changelog after it)&lt;br/&gt;
6. Checkpoint N is completed, checkpoint (N - 1) is subsumed; all &quot;old&quot; non-materialized state is scheduled for async deletion&lt;br/&gt;
7. Checkpoint (N + 1) is triggered; but it is waiting for an IO thread to initialize the location&lt;/p&gt;

&lt;p&gt;It &lt;b&gt;is&lt;/b&gt; desirable to preserve this back-pressure from deletion to new checkpoints. But if possible, deletions should be spread more evenly. &lt;br/&gt;
So I was thinking that distributing different task materializations evenly should reduce the wait time (although it does not eliminate it completely).&lt;br/&gt;
The other way is to adjust threads workings (which I think is a better way).&lt;/p&gt;

&lt;p&gt;&amp;gt; Maybe we should think about some more fair thread pool for async jobs? For example every async IO job could get assigned an id/key, and each id/key would have it&apos;s own queue of tasks to perform. Based on that we could implement all kinds of fancy priority schemes, but we could start with something as simple as just going in a round robing fashion through all individual per id/key queues when polling for a new task to execute. This could be generic and flexible enough to be re-used in other use cases (I was thinking about something like that for the TMs IO executor in the past).&lt;/p&gt;

&lt;p&gt;I think only priorities won&apos;t work here because we&apos;d need to assign different priorities depending on the &quot;queue length&quot; to preserve back-pressure.&lt;br/&gt;
If we always prioritize new checkpoints over deletions, we&apos;ll likely end up with OOMs (the case before the CheckpointCleaner was introduced).&lt;br/&gt;
Having different queues would work I think - but with a check of the length of the deletion queue.&lt;/p&gt;

&lt;p&gt;&amp;gt; Re batching. Isn&apos;t this more of an independent potential optimisation that we could consider independently of the main issue? Depending how long is single IO operation. If it&apos;s more then a couple of ms, I would prefer to leave them separate.&lt;/p&gt;

&lt;p&gt;I think this can be viewed as an optimization if the problem solved by other means; or as an actual way to solve this problem.&lt;br/&gt;
The benefits of the batching solution are simplicity and lesser invasiveness.&lt;/p&gt;</comment>
                            <comment id="17496736" author="pnowojski" created="Wed, 23 Feb 2022 12:53:17 +0000"  >&lt;p&gt;Thanks for the explanation, I get it now.&lt;/p&gt;

&lt;p&gt;&amp;gt; 1. Batch deletions and leave one thread idle (e.g. group 1K handles into 10 big batches handled by 11 IO threads)&lt;/p&gt;

&lt;p&gt;Is this the right level to provide back pressure functionality? Would it even work if you hardcoded in the &lt;tt&gt;CheckpointCoordinator&lt;/tt&gt; assumptions about pool size and the number of used threads? We don&apos;t know how else this thread pool is being used.&lt;/p&gt;

&lt;p&gt;Apart of that. Don&apos;t we already have a backpressure mechanism on a higher level? &lt;tt&gt;CheckpointRequestDecider#numberOfCleaningCheckpointsSupplier&lt;/tt&gt; from &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-17073&quot; title=&quot;Slow checkpoint cleanup causing OOMs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-17073&quot;&gt;&lt;del&gt;FLINK-17073&lt;/del&gt;&lt;/a&gt;? It looks like simple fair io thread pool as I described above, without any priorities + addjusting/relaxing &lt;tt&gt;numberOfCleaningCheckpointsSupplier.getAsInt() &amp;gt; maxConcurrentCheckpointAttempts&lt;/tt&gt; check to something like &lt;tt&gt;numberOfCleaningCheckpointsSupplier.getAsInt() &amp;gt; maxConcurrentCheckpointAttempts + CONSTANT&lt;/tt&gt; would do the trick, wouldn&apos;t it?&lt;/p&gt;</comment>
                            <comment id="17496790" author="roman_khachatryan" created="Wed, 23 Feb 2022 14:10:29 +0000"  >&lt;p&gt;&amp;gt; &amp;gt; 1. Batch deletions and leave one thread idle (e.g. group 1K handles into 10 big batches handled by 11 IO threads)&lt;br/&gt;
&amp;gt; Would it even work if you hardcoded in the CheckpointCoordinator assumptions about pool size and the number of used threads? We don&apos;t know how else this thread pool is being used.&lt;br/&gt;
&#160;&lt;br/&gt;
We can avoid hardcoding these numbers and put the logic into some wrapper around the pool (with a new method that accepts a list of Runnables or Handles).&lt;br/&gt;
&#160;&lt;br/&gt;
&amp;gt; Is this the right level to provide back pressure functionality?&lt;br/&gt;
&amp;gt; Don&apos;t we already have a backpressure mechanism on a higher level? CheckpointRequestDecider#numberOfCleaningCheckpointsSupplier from &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-17073&quot; title=&quot;Slow checkpoint cleanup causing OOMs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-17073&quot;&gt;&lt;del&gt;FLINK-17073&lt;/del&gt;&lt;/a&gt;?&lt;br/&gt;
&#160;&lt;br/&gt;
Currently, deletions from SharedStateRegistry don&apos;t go through the CheckpointCleaner.&#160;SharedStateRegistry differs in the following:&lt;br/&gt;
1. A handle is associated with multiple checkpoints (or none at the time of deletion) - so a separate counter would be needed&lt;br/&gt;
2. It&apos;s not clear to me what number of pending handle deletions should be allowed&lt;br/&gt;
Conceptually, it should be a condition instead of a number:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;when scheduling shared state deletion upon checkpoint subsumption&lt;/li&gt;
	&lt;li&gt;if there are any deletions from a previous subsumption then don&apos;t allow new checkpoints&lt;/li&gt;
	&lt;li&gt;start allowing new checkpoints when there newPending &amp;lt;= oldPending&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This would be achieved by batchind deletions naturally; but implemented in CheckpointRequestDecider it could add extra complexity.&lt;br/&gt;
&#160;&lt;br/&gt;
The advantage of doing it in CheckpointRequestDecider is more accurate reporting of checkpoint duration.&lt;br/&gt;
&#160;&lt;br/&gt;
Maybe batching can be a short-term solution; which can be evolved gradually (by replacing executor in wrapper by multiple queues; and then checking queue size in CheckpointRequestDecider). WDYT?&lt;br/&gt;
&#160;&lt;br/&gt;
&amp;gt; It looks like simple fair io thread pool as I described above, without any priorities + addjusting/relaxing numberOfCleaningCheckpointsSupplier.getAsInt() &amp;gt; maxConcurrentCheckpointAttempts check to something like numberOfCleaningCheckpointsSupplier.getAsInt() &amp;gt; maxConcurrentCheckpointAttempts + CONSTANT would do the trick, wouldn&apos;t it?&lt;br/&gt;
IIUC, it wouldn&apos;t exert back-pressure, because the number of handles is not directly related to the number of checkpoints to clean (I&apos;m assuming seperate thread pools for discarding shared state and initializing new checkpoints).&lt;br/&gt;
Taken to the extreme, checkpoints might consist only of shared state, so checkpoint deletion will be fast. But then discarding shared state might take arbitrarily long.&lt;/p&gt;</comment>
                            <comment id="17496834" author="pnowojski" created="Wed, 23 Feb 2022 15:05:07 +0000"  >&lt;blockquote&gt;
&lt;p&gt;We can avoid hardcoding these numbers and put the logic into some wrapper around the pool (with a new method that accepts a list of Runnables or Handles).&lt;/p&gt;

&lt;p&gt;Maybe batching can be a short-term solution; which can be evolved gradually (by replacing executor in wrapper by multiple queues; and then checking queue size in CheckpointRequestDecider). WDYT?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I still do not think this is a good solution. You would need to assume what other users of the ioExecutor pool are doing and how are they using it, so tweaking the number of batches to &quot;size of the pool - 1&quot; sounds like a bad idea. At the same time I don&apos;t see a reason to rush this?&lt;/p&gt;

&lt;p&gt;I agree that expressing the right condition for which &lt;tt&gt;CheckpointRequestDecider&lt;/tt&gt; should be back pressured is quite tricky.&lt;/p&gt;</comment>
                            <comment id="17496841" author="roman_khachatryan" created="Wed, 23 Feb 2022 15:30:42 +0000"  >&lt;p&gt;Other users (such as handling RPC) wouldn&apos;t have to use the new method; the only impact on them will be the same speedup as for new checkpoints.&lt;/p&gt;

&lt;p&gt;Having this fix would be allow to enable changelog without increasing the jobmanager.io-pool.size (on object storages where deletes are slower) or experiencing these checkpoint pauses.&lt;/p&gt;

&lt;p&gt;If there is a simple working fix and everyone agrees on the approach then it could be implemented quickly. But it dshouldn&apos;t be rushed, I agree.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;edit:&lt;/p&gt;

&lt;p&gt;Queues approach seems similar to the one proposed earlier: &lt;a href=&quot;https://docs.google.com/document/d/1p0m4FAmpWxShFaicgHXKqKCHYjnsDnKzvyU7BuA9CT8/edit?usp=sharing&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://docs.google.com/document/d/1p0m4FAmpWxShFaicgHXKqKCHYjnsDnKzvyU7BuA9CT8/edit?usp=sharing&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Maybe it&apos;s time to consider it again, but it can be a bigger effort.&lt;/p&gt;</comment>
                            <comment id="17497408" author="ym" created="Thu, 24 Feb 2022 14:08:24 +0000"  >&lt;ol&gt;
	&lt;li&gt;Why using a separate pool for deletion is not a good idea?&lt;/li&gt;
	&lt;li&gt;If the answer to 1 is due to &quot;backpressure&quot;. When mentioning &quot;backpressure&quot;, do you mean triggering/starting new checkpoints faster than we can subsume/delete the old ones&apos; states?&#160;&lt;/li&gt;
	&lt;li&gt;If yes, then using separate pools, we can still pause triggering new checkpoint if state deletion speed not catching up&lt;/li&gt;
	&lt;li&gt;I agree that batching deletion and randomizing triggering materialization can mitigate the problem, but can not prevent the problem completely, because neither can not guarantee that &quot;checkpoint triggering speed &amp;lt; state deletion speed&quot;.&lt;/li&gt;
	&lt;li&gt;When talking about `backpressure`, isn&apos;t it usually related to data processing? I do not think checkpointing should affect normal data processing if that&apos;s the case.&lt;/li&gt;
&lt;/ol&gt;
</comment>
                            <comment id="17497947" author="ym" created="Fri, 25 Feb 2022 07:12:47 +0000"  >&lt;p&gt;The severity of this problem also depends on the test setup:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;how often checkpoint is triggered&lt;/li&gt;
	&lt;li&gt;how big the state is&lt;/li&gt;
	&lt;li&gt;materialization interval (maybe)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;If as the number/example shared above, triggering cp each 1s, I do agree that severity is an improvement/major.&lt;/p&gt;</comment>
                            <comment id="17504340" author="roman_khachatryan" created="Thu, 10 Mar 2022 15:09:21 +0000"  >&lt;p&gt;I&apos;ve extracted non-backend related issue into &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-26590&quot; title=&quot;Triggered checkpoints can be delayed by discarding shared state&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-26590&quot;&gt;FLINK-26590&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Backend-related fix (randomized materialization) merged into master as ed5e6144441bfbc020f525f9c10fd29cb3d83cbf.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10020">
                    <name>Cloners</name>
                                                                <inwardlinks description="is cloned by">
                                        <issuelink>
            <issuekey id="13433146">FLINK-26590</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13250244">FLINK-13698</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="12310760">
                    <name>Testing</name>
                                                                <inwardlinks description="Discovered while testing">
                                        <issuelink>
            <issuekey id="13415026">FLINK-25144</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            3 years, 35 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z0ztx4:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>