<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 20:28:58 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-7011] Instable Kafka testStartFromKafkaCommitOffsets failures on Travis</title>
                <link>https://issues.apache.org/jira/browse/FLINK-7011</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;p&gt;Example:&lt;br/&gt;
&lt;a href=&quot;https://s3.amazonaws.com/archive.travis-ci.org/jobs/246703474/log.txt?X-Amz-Expires=30&amp;amp;X-Amz-Date=20170627T065647Z&amp;amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;amp;X-Amz-Credential=AKIAJRYRXRSVGNKPKO5A/20170627/us-east-1/s3/aws4_request&amp;amp;X-Amz-SignedHeaders=host&amp;amp;X-Amz-Signature=dbfc90cfc386fef0990325b54ff74ee4d441944687e7fdaa73ce7b0c2b2ec0ea&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://s3.amazonaws.com/archive.travis-ci.org/jobs/246703474/log.txt?X-Amz-Expires=30&amp;amp;X-Amz-Date=20170627T065647Z&amp;amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;amp;X-Amz-Credential=AKIAJRYRXRSVGNKPKO5A/20170627/us-east-1/s3/aws4_request&amp;amp;X-Amz-SignedHeaders=host&amp;amp;X-Amz-Signature=dbfc90cfc386fef0990325b54ff74ee4d441944687e7fdaa73ce7b0c2b2ec0ea&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In general, the test &lt;tt&gt;testStartFromKafkaCommitOffsets&lt;/tt&gt; implementation is a bit of an overkill. Before continuing with the test, it writes some records just for the sake of committing offsets to Kafka and waits for some offsets to be committed (which leads to the instability), whereas we can do that simply using the test base&apos;s &lt;tt&gt;OffsetHandler&lt;/tt&gt;.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13082723">FLINK-7011</key>
            <summary>Instable Kafka testStartFromKafkaCommitOffsets failures on Travis</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="tzulitai">Tzu-Li (Gordon) Tai</assignee>
                                    <reporter username="tzulitai">Tzu-Li (Gordon) Tai</reporter>
                        <labels>
                    </labels>
                <created>Tue, 27 Jun 2017 07:04:28 +0000</created>
                <updated>Sat, 1 Jul 2017 12:22:23 +0000</updated>
                            <resolved>Sat, 1 Jul 2017 12:22:23 +0000</resolved>
                                    <version>1.3.1</version>
                    <version>1.4.0</version>
                                    <fixVersion>1.3.2</fixVersion>
                    <fixVersion>1.4.0</fixVersion>
                                    <component>Connectors / Kafka</component>
                    <component>Tests</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="16064447" author="githubbot" created="Tue, 27 Jun 2017 08:18:54 +0000"  >&lt;p&gt;GitHub user tzulitai opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4190&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4190&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-7011&quot; title=&quot;Instable Kafka testStartFromKafkaCommitOffsets failures on Travis&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-7011&quot;&gt;&lt;del&gt;FLINK-7011&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; Harden Kafka testStartFromKafkaCommitOffsets ITCases&lt;/p&gt;

&lt;p&gt;    Hardens `testStartFromKafkaCommitOffsets` in Kafka ITCases.&lt;/p&gt;

&lt;p&gt;    *&lt;b&gt;Description of what the test does:&lt;/b&gt;*&lt;br/&gt;
    The case verifies that whatever offset was committed to Kafka, Flink reads it correctly and can use that as the correct starting point for exactly-once. It is done in an end-to-end manner, verifying that the commit logic and read offset logic is coherent.&lt;/p&gt;

&lt;p&gt;    *&lt;b&gt;Problem:&lt;/b&gt;*&lt;br/&gt;
    The previous implementation was too strict. It tries 3 times to fetch some committed offsets. If none is fetched, the test fails. This PR changes it so that we retry infinitely until some offsets are committed. In the case that no offsets are ever committed due to incorrect offset commit logic, the test timeouts can guard against that.&lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/tzulitai/flink&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/tzulitai/flink&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-7011&quot; title=&quot;Instable Kafka testStartFromKafkaCommitOffsets failures on Travis&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-7011&quot;&gt;&lt;del&gt;FLINK-7011&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4190.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4190.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #4190&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 264172d3b788ff73aade75116e8342c132df24c0&lt;br/&gt;
Author: Tzu-Li (Gordon) Tai &amp;lt;tzulitai@apache.org&amp;gt;&lt;br/&gt;
Date:   2017-06-27T07:53:06Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-7011&quot; title=&quot;Instable Kafka testStartFromKafkaCommitOffsets failures on Travis&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-7011&quot;&gt;&lt;del&gt;FLINK-7011&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; Harden Kafka testStartFromKafkaCommitOffsets ITCases&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="16066416" author="githubbot" created="Wed, 28 Jun 2017 12:34:07 +0000"  >&lt;p&gt;Github user zentol commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4190#discussion_r124527723&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4190#discussion_r124527723&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java &amp;#8212;&lt;br/&gt;
    @@ -279,49 +279,43 @@ public void runStartFromKafkaCommitOffsets() throws Exception {&lt;/p&gt;

&lt;p&gt;     		final String topicName = writeSequence(&quot;testStartFromKafkaCommitOffsetsTopic&quot;, recordsInEachPartition, parallelism, 1);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler();&lt;br/&gt;
    +		// read some records so that some offsets are committed to Kafka&lt;br/&gt;
    +		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();&lt;br/&gt;
    +		env.getConfig().disableSysoutLogging();&lt;br/&gt;
    +		env.getConfig().setRestartStrategy(RestartStrategies.noRestart());&lt;br/&gt;
    +		env.setParallelism(parallelism);&lt;br/&gt;
    +		env.enableCheckpointing(20); // fast checkpoints to make sure we commit some offsets&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Long o1;&lt;/li&gt;
	&lt;li&gt;Long o2;&lt;/li&gt;
	&lt;li&gt;Long o3;&lt;/li&gt;
	&lt;li&gt;int attempt = 0;&lt;/li&gt;
	&lt;li&gt;// make sure that o1, o2, o3 are not all null before proceeding&lt;/li&gt;
	&lt;li&gt;do {&lt;/li&gt;
	&lt;li&gt;attempt++;&lt;/li&gt;
	&lt;li&gt;LOG.info(&quot;Attempt &quot; + attempt + &quot; to read records and commit some offsets to Kafka&quot;);&lt;br/&gt;
    -&lt;/li&gt;
	&lt;li&gt;final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();&lt;/li&gt;
	&lt;li&gt;env.getConfig().disableSysoutLogging();&lt;/li&gt;
	&lt;li&gt;env.getConfig().setRestartStrategy(RestartStrategies.noRestart());&lt;/li&gt;
	&lt;li&gt;env.setParallelism(parallelism);&lt;/li&gt;
	&lt;li&gt;env.enableCheckpointing(20); // fast checkpoints to make sure we commit some offsets&lt;br/&gt;
    -&lt;/li&gt;
	&lt;li&gt;env&lt;/li&gt;
	&lt;li&gt;.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))&lt;/li&gt;
	&lt;li&gt;.map(new ThrottledMapper&amp;lt;String&amp;gt;(consumePause))&lt;/li&gt;
	&lt;li&gt;.map(new MapFunction&amp;lt;String, Object&amp;gt;() {&lt;/li&gt;
	&lt;li&gt;int count = 0;&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public Object map(String value) throws Exception {&lt;/li&gt;
	&lt;li&gt;count++;&lt;/li&gt;
	&lt;li&gt;if (count == recordsToConsume) 
{
    -							throw new SuccessException();
    -						}&lt;/li&gt;
	&lt;li&gt;return null;&lt;br/&gt;
    +		env&lt;br/&gt;
    +			.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))&lt;br/&gt;
    +			.map(new ThrottledMapper&amp;lt;String&amp;gt;(consumePause))&lt;br/&gt;
    +			.map(new MapFunction&amp;lt;String, Object&amp;gt;() {&lt;br/&gt;
    +				int count = 0;&lt;br/&gt;
    +				@Override&lt;br/&gt;
    +				public Object map(String value) throws Exception {&lt;br/&gt;
    +					count++;&lt;br/&gt;
    +					if (count == recordsToConsume) 
{
    +						throw new SuccessException();
     					}&lt;/li&gt;
	&lt;li&gt;})&lt;/li&gt;
	&lt;li&gt;.addSink(new DiscardingSink&amp;lt;&amp;gt;());&lt;br/&gt;
    +					return null;&lt;br/&gt;
    +				}&lt;br/&gt;
    +			})&lt;br/&gt;
    +			.addSink(new DiscardingSink&amp;lt;&amp;gt;());&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;tryExecute(env, &quot;Read some records to commit offsets to Kafka&quot;);&lt;br/&gt;
    +		tryExecute(env, &quot;Read some records to commit offsets to Kafka&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    +		// make sure that we indeed have some offsets committed to Kafka&lt;br/&gt;
    +		Long o1 = null;&lt;br/&gt;
    +		Long o2 = null;&lt;br/&gt;
    +		Long o3 = null;&lt;br/&gt;
    +		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler();&lt;br/&gt;
    +		while (o1 == null &amp;amp;&amp;amp; o2 == null &amp;amp;&amp;amp; o3 == null) &lt;/p&gt;
{
    +			Thread.sleep(100);
     			o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0);
     			o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1);
     			o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2);
    -		}
&lt;p&gt; while (o1 == null &amp;amp;&amp;amp; o2 == null &amp;amp;&amp;amp; o3 == null &amp;amp;&amp;amp; attempt &amp;lt; 3);&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I would still add a hard-limit for attempts (20?); there&apos;s no benefit in a test that succeeds &lt;em&gt;at some point&lt;/em&gt;, if it doesn&apos;t finish relatively quickly the entire build will time out anyway.&lt;/p&gt;

&lt;p&gt;    Also, when running tests locally i suppose there isn&apos;t even a time-limit that would kill the test...&lt;/p&gt;</comment>
                            <comment id="16069444" author="githubbot" created="Fri, 30 Jun 2017 04:14:14 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4190&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4190&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @zentol the time limit is set on each individual version-specific `KafkaConsumerXXITCase`&lt;/p&gt;</comment>
                            <comment id="16069446" author="githubbot" created="Fri, 30 Jun 2017 04:17:19 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4190#discussion_r124961658&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4190#discussion_r124961658&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java &amp;#8212;&lt;br/&gt;
    @@ -279,49 +279,43 @@ public void runStartFromKafkaCommitOffsets() throws Exception {&lt;/p&gt;

&lt;p&gt;     		final String topicName = writeSequence(&quot;testStartFromKafkaCommitOffsetsTopic&quot;, recordsInEachPartition, parallelism, 1);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler();&lt;br/&gt;
    +		// read some records so that some offsets are committed to Kafka&lt;br/&gt;
    +		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();&lt;br/&gt;
    +		env.getConfig().disableSysoutLogging();&lt;br/&gt;
    +		env.getConfig().setRestartStrategy(RestartStrategies.noRestart());&lt;br/&gt;
    +		env.setParallelism(parallelism);&lt;br/&gt;
    +		env.enableCheckpointing(20); // fast checkpoints to make sure we commit some offsets&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Long o1;&lt;/li&gt;
	&lt;li&gt;Long o2;&lt;/li&gt;
	&lt;li&gt;Long o3;&lt;/li&gt;
	&lt;li&gt;int attempt = 0;&lt;/li&gt;
	&lt;li&gt;// make sure that o1, o2, o3 are not all null before proceeding&lt;/li&gt;
	&lt;li&gt;do {&lt;/li&gt;
	&lt;li&gt;attempt++;&lt;/li&gt;
	&lt;li&gt;LOG.info(&quot;Attempt &quot; + attempt + &quot; to read records and commit some offsets to Kafka&quot;);&lt;br/&gt;
    -&lt;/li&gt;
	&lt;li&gt;final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();&lt;/li&gt;
	&lt;li&gt;env.getConfig().disableSysoutLogging();&lt;/li&gt;
	&lt;li&gt;env.getConfig().setRestartStrategy(RestartStrategies.noRestart());&lt;/li&gt;
	&lt;li&gt;env.setParallelism(parallelism);&lt;/li&gt;
	&lt;li&gt;env.enableCheckpointing(20); // fast checkpoints to make sure we commit some offsets&lt;br/&gt;
    -&lt;/li&gt;
	&lt;li&gt;env&lt;/li&gt;
	&lt;li&gt;.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))&lt;/li&gt;
	&lt;li&gt;.map(new ThrottledMapper&amp;lt;String&amp;gt;(consumePause))&lt;/li&gt;
	&lt;li&gt;.map(new MapFunction&amp;lt;String, Object&amp;gt;() {&lt;/li&gt;
	&lt;li&gt;int count = 0;&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public Object map(String value) throws Exception {&lt;/li&gt;
	&lt;li&gt;count++;&lt;/li&gt;
	&lt;li&gt;if (count == recordsToConsume) 
{
    -							throw new SuccessException();
    -						}&lt;/li&gt;
	&lt;li&gt;return null;&lt;br/&gt;
    +		env&lt;br/&gt;
    +			.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))&lt;br/&gt;
    +			.map(new ThrottledMapper&amp;lt;String&amp;gt;(consumePause))&lt;br/&gt;
    +			.map(new MapFunction&amp;lt;String, Object&amp;gt;() {&lt;br/&gt;
    +				int count = 0;&lt;br/&gt;
    +				@Override&lt;br/&gt;
    +				public Object map(String value) throws Exception {&lt;br/&gt;
    +					count++;&lt;br/&gt;
    +					if (count == recordsToConsume) 
{
    +						throw new SuccessException();
     					}&lt;/li&gt;
	&lt;li&gt;})&lt;/li&gt;
	&lt;li&gt;.addSink(new DiscardingSink&amp;lt;&amp;gt;());&lt;br/&gt;
    +					return null;&lt;br/&gt;
    +				}&lt;br/&gt;
    +			})&lt;br/&gt;
    +			.addSink(new DiscardingSink&amp;lt;&amp;gt;());&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;tryExecute(env, &quot;Read some records to commit offsets to Kafka&quot;);&lt;br/&gt;
    +		tryExecute(env, &quot;Read some records to commit offsets to Kafka&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    +		// make sure that we indeed have some offsets committed to Kafka&lt;br/&gt;
    +		Long o1 = null;&lt;br/&gt;
    +		Long o2 = null;&lt;br/&gt;
    +		Long o3 = null;&lt;br/&gt;
    +		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler();&lt;br/&gt;
    +		while (o1 == null &amp;amp;&amp;amp; o2 == null &amp;amp;&amp;amp; o3 == null) &lt;/p&gt;
{
    +			Thread.sleep(100);
     			o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0);
     			o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1);
     			o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2);
    -		}
&lt;p&gt; while (o1 == null &amp;amp;&amp;amp; o2 == null &amp;amp;&amp;amp; o3 == null &amp;amp;&amp;amp; attempt &amp;lt; 3);&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I understand the argument. Perhaps it is also a fact that this test is covering too much into one single test, hence the awkwardness in making it stable.&lt;br/&gt;
    I think it is sufficient to have 2 separate tests that replace this:&lt;br/&gt;
    (a) test that committed Kafka offsets are correct (there is already a ITCase for this)&lt;br/&gt;
    (b) test that committed offsets are correctly picked up and used correctly (there is actually also a test for this already).&lt;/p&gt;

&lt;p&gt;    Hence, I would conclude that perhaps this test can be removed.&lt;/p&gt;</comment>
                            <comment id="16071079" author="githubbot" created="Sat, 1 Jul 2017 07:37:05 +0000"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4190&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4190&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16071194" author="tzulitai" created="Sat, 1 Jul 2017 12:22:23 +0000"  >&lt;p&gt;Merged for master via ba75bdef78dd3ea6d23666d63c94e96b668a8a94.&lt;br/&gt;
Merged for 1.3 via 87ff2890ccec895f950bd9d20e4394cae75e9d5c.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 20 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3grov:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>