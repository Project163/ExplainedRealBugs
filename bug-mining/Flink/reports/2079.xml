<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 20:31:03 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-4228] YARN artifact upload does not work with S3AFileSystem</title>
                <link>https://issues.apache.org/jira/browse/FLINK-4228</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;p&gt;The issue now is exclusive to running on YARN with s3a:// as your configured FileSystem. If so, the Flink session will fail on staging itself because it tries to copy the flink/lib directory to S3 and the S3aFileSystem does not support recursive copy.&lt;/p&gt;

&lt;h2&gt;&lt;a name=&quot;OldIssue&quot;&gt;&lt;/a&gt;Old Issue&lt;/h2&gt;
&lt;p&gt;Using the &lt;tt&gt;RocksDBStateBackend&lt;/tt&gt; with semi-async snapshots (current default) leads to an Exception when uploading the snapshot to S3 when using the &lt;tt&gt;S3AFileSystem&lt;/tt&gt;.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
AsynchronousException{com.amazonaws.AmazonClientException: Unable to calculate MD5 hash: /&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/folders/_c/5tc5q5q55qjcjtqwlwvwd1m00000gn/T/flink-io-5640e9f1-3ea4-4a0f-b4d9-3ce9fbd98d8a/7c6e745df2dddc6eb70def1240779e44/StreamFlatMap_3_0/dummy_state/47daaf2a-150c-4208-aa4b-409927e9e5b7/local-chk-2886 (Is a directory)}
	at org.apache.flink.streaming.runtime.tasks.StreamTask$AsyncCheckpointThread.run(StreamTask.java:870)
Caused by: com.amazonaws.AmazonClientException: Unable to calculate MD5 hash: /&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/folders/_c/5tc5q5q55qjcjtqwlwvwd1m00000gn/T/flink-io-5640e9f1-3ea4-4a0f-b4d9-3ce9fbd98d8a/7c6e745df2dddc6eb70def1240779e44/StreamFlatMap_3_0/dummy_state/47daaf2a-150c-4208-aa4b-409927e9e5b7/local-chk-2886 (Is a directory)
	at com.amazonaws.services.s3.AmazonS3Client.putObject(AmazonS3Client.java:1298)
	at com.amazonaws.services.s3.transfer.internal.UploadCallable.uploadInOneChunk(UploadCallable.java:108)
	at com.amazonaws.services.s3.transfer.internal.UploadCallable.call(UploadCallable.java:100)
	at com.amazonaws.services.s3.transfer.internal.UploadMonitor.upload(UploadMonitor.java:192)
	at com.amazonaws.services.s3.transfer.internal.UploadMonitor.call(UploadMonitor.java:150)
	at com.amazonaws.services.s3.transfer.internal.UploadMonitor.call(UploadMonitor.java:50)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
Caused by: java.io.FileNotFoundException: /&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/folders/_c/5tc5q5q55qjcjtqwlwvwd1m00000gn/T/flink-io-5640e9f1-3ea4-4a0f-b4d9-3ce9fbd98d8a/7c6e745df2dddc6eb70def1240779e44/StreamFlatMap_3_0/dummy_state/47daaf2a-150c-4208-aa4b-409927e9e5b7/local-chk-2886 (Is a directory)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.&amp;lt;init&amp;gt;(FileInputStream.java:138)
	at com.amazonaws.services.s3.AmazonS3Client.putObject(AmazonS3Client.java:1294)
	... 9 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Running with S3NFileSystem, the error does not occur. The problem might be due to &lt;tt&gt;HDFSCopyToLocal&lt;/tt&gt; assuming that sub-folders are going to be created automatically. We might need to manually create folders and copy only actual files for &lt;tt&gt;S3AFileSystem&lt;/tt&gt;. More investigation is required.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12990418">FLINK-4228</key>
            <summary>YARN artifact upload does not work with S3AFileSystem</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="nkruber">Nico Kruber</assignee>
                                    <reporter username="uce">Ufuk Celebi</reporter>
                        <labels>
                    </labels>
                <created>Mon, 18 Jul 2016 14:36:40 +0000</created>
                <updated>Mon, 20 Nov 2017 08:35:31 +0000</updated>
                            <resolved>Sat, 18 Nov 2017 14:36:07 +0000</resolved>
                                                    <fixVersion>1.4.0</fixVersion>
                                    <component>Runtime / State Backends</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>10</watches>
                                                                                                                <comments>
                            <comment id="15382502" author="aljoscha" created="Mon, 18 Jul 2016 16:01:11 +0000"  >&lt;p&gt;Maybe we should move to only allow the fully async snapshot mode for the RocksDB backend.&lt;/p&gt;</comment>
                            <comment id="15382512" author="uce" created="Mon, 18 Jul 2016 16:05:54 +0000"  >&lt;p&gt;Yeah, maybe. Can you create a separate issue for it to discuss it there? I guess that this issue is not the only thing triggering your suggestion, no? &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15387613" author="stephanewen" created="Thu, 21 Jul 2016 12:43:08 +0000"  >&lt;p&gt;+1 to make the fully asynchronous snapshots the default&lt;/p&gt;</comment>
                            <comment id="15389743" author="gjy" created="Fri, 22 Jul 2016 16:02:38 +0000"  >&lt;p&gt;We also want to use RocksDB with checkpointing to s3a. We would prefer if you patched this into 1.0.x&lt;/p&gt;</comment>
                            <comment id="15390493" author="cresny@gmail.com" created="Sat, 23 Jul 2016 03:19:31 +0000"  >&lt;p&gt;I added a pull request for this. I included the fink-yarn recursive staging upload.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/flink/pull/2288&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2288&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15420812" author="githubbot" created="Mon, 15 Aug 2016 10:08:02 +0000"  >&lt;p&gt;Github user uce commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2288&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2288&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    The associated issue for this can be found here: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-4228&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/FLINK-4228&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;    +1 to the points Stephan raised in his earlier comment.&lt;/p&gt;

</comment>
                            <comment id="15772388" author="tonycox" created="Fri, 23 Dec 2016 09:15:23 +0000"  >&lt;p&gt;what happens with this issue, is still relevant?&lt;/p&gt;</comment>
                            <comment id="15772906" author="cresny@gmail.com" created="Fri, 23 Dec 2016 13:31:01 +0000"  >&lt;p&gt;The issue now is exclusive to running on YARN with s3a:// as your configured FileSystem. If so, the Flink session will fail on staging itself because it tries to copy the flink/lib directory to S3 and the S3aFileSystem does not support recursive copy. &lt;/p&gt;</comment>
                            <comment id="15773024" author="tonycox" created="Fri, 23 Dec 2016 14:37:58 +0000"  >&lt;p&gt;so do you want to continue on this?&lt;/p&gt;</comment>
                            <comment id="15773084" author="cresny@gmail.com" created="Fri, 23 Dec 2016 15:13:25 +0000"  >&lt;p&gt;My last pull request is good to go so I guess it&apos;s up to you guys.&lt;/p&gt;</comment>
                            <comment id="16173277" author="aljoscha" created="Wed, 20 Sep 2017 14:39:57 +0000"  >&lt;p&gt;Moved to critical for the part about checking whether submission using s3a as configured FileSystem works.&lt;/p&gt;</comment>
                            <comment id="16173278" author="aljoscha" created="Wed, 20 Sep 2017 14:40:16 +0000"  >&lt;p&gt;The other parts are not relevant anymore since the RocksDB backend has evolved quite a bit.&lt;/p&gt;</comment>
                            <comment id="16203320" author="aljoscha" created="Fri, 13 Oct 2017 09:51:22 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cresny&quot; class=&quot;user-hover&quot; rel=&quot;cresny&quot;&gt;cresny&lt;/a&gt; Do you know if upload of YARN artefacts via S3A is still an issue? It looks like the code does a proper copy (at least in Hadoop 2.6.0).&lt;/p&gt;</comment>
                            <comment id="16225408" author="nicok" created="Mon, 30 Oct 2017 17:47:01 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aljoscha&quot; class=&quot;user-hover&quot; rel=&quot;aljoscha&quot;&gt;aljoscha&lt;/a&gt;: I successfully reproduced the error with a current snapshot of Flink 1.4 on EMR&lt;/p&gt;

&lt;p&gt;Steps to reproduce:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;setup the S3A filesystem as described in &lt;a href=&quot;https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/deployment/aws.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/deployment/aws.html&lt;/a&gt;, i.e.
	&lt;ul&gt;
		&lt;li&gt;adapt &lt;tt&gt;/etc/hadoop/conf/core-site.xml&lt;/tt&gt;&lt;/li&gt;
		&lt;li&gt;copy S3A filesystem dependency jars to &lt;tt&gt;flink/lib&lt;/tt&gt;&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;adapt default filesystem for hadoop in &lt;tt&gt;/etc/hadoop/conf/core-site.xml&lt;/tt&gt;, e.g.:
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;fs.defaultFS&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;s3a:&lt;span class=&quot;code-comment&quot;&gt;//nico-test/&amp;lt;/value&amp;gt;
&lt;/span&gt;  &amp;lt;/property&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
	&lt;li&gt;distribute new configuration to all nodes &amp;amp; restart to apply&lt;/li&gt;
	&lt;li&gt;try to run an example (here with two nodes) so that Flink tries to deploy the artefacts to YARN&apos;s (default) filesystem
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&amp;gt; cd flink
&amp;gt; HADOOP_CONF_DIR=/etc/hadoop/conf ./bin/flink run -m yarn-cluster -yn 2 -ys 1 -yjm 768 -ytm 1024 ./examples/batch/WordCount.jar
Using the result of &lt;span class=&quot;code-quote&quot;&gt;&apos;hadoop classpath&apos;&lt;/span&gt; to augment the Hadoop classpath: /etc/hadoop/conf:/usr/lib/hadoop/lib/*:/usr/lib/hadoop/.&lt;span class=&quot;code-comment&quot;&gt;//*:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/*:/usr/lib/hadoop-hdfs/.//*:/usr/lib/hadoop-yarn/lib/*:/usr/lib/hadoop-yarn/.//*:/usr/lib/hadoop-mapreduce/lib/*:/usr/lib/hadoop-mapreduce/.//*::/usr/lib/hadoop-lzo/lib/*:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/ddb/lib/emr-ddb-hadoop.jar:/usr/share/aws/emr/goodies/lib/emr-hadoop-goodies.jar:/usr/share/aws/emr/kinesis/lib/emr-kinesis-hadoop.jar:/usr/share/aws/emr/cloudwatch-sink/lib/*:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*
&lt;/span&gt;SLF4J: &lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt; path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/hadoop/flink/lib/slf4j-log4j12-1.7.7.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http:&lt;span class=&quot;code-comment&quot;&gt;//www.slf4j.org/codes.html#multiple_bindings &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; an explanation.
&lt;/span&gt;SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
2017-10-30 17:34:51,137 WARN  org.apache.hadoop.conf.Configuration                          - /etc/hadoop/conf/core-site.xml:an attempt to override &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; parameter: fs.s3.buffer.dir;  Ignoring.
2017-10-30 17:34:51,224 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - No path &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; the flink jar passed. Using the location of &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.flink.yarn.YarnClusterDescriptor to locate the jar
2017-10-30 17:34:51,224 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - No path &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; the flink jar passed. Using the location of &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.flink.yarn.YarnClusterDescriptor to locate the jar
2017-10-30 17:34:51,279 INFO  org.apache.hadoop.yarn.client.RMProxy                         - Connecting to ResourceManager at ip-172-31-22-149.eu-west-1.compute.internal/172.31.22.149:8032
2017-10-30 17:34:51,572 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Cluster specification: ClusterSpecification{masterMemoryMB=768, taskManagerMemoryMB=1024, numberTaskManagers=2, slotsPerTaskManager=1}
2017-10-30 17:34:53,253 WARN  org.apache.flink.yarn.YarnClusterDescriptor                   - The configuration directory (&lt;span class=&quot;code-quote&quot;&gt;&apos;/home/hadoop/flink/conf&apos;&lt;/span&gt;) contains both LOG4J and Logback configuration files. Please delete or rename one of them.
2017-10-30 17:34:53,268 INFO  org.apache.flink.yarn.Utils                                   - Copying from file:/home/hadoop/flink/conf/log4j.properties to s3a:&lt;span class=&quot;code-comment&quot;&gt;//nico-test/user/hadoop/.flink/application_1509384765476_0001/log4j.properties
&lt;/span&gt;2017-10-30 17:34:53,824 INFO  org.apache.flink.yarn.Utils                                   - Copying from file:/home/hadoop/flink/lib to s3a:&lt;span class=&quot;code-comment&quot;&gt;//nico-test/user/hadoop/.flink/application_1509384765476_0001/lib
&lt;/span&gt;
------------------------------------------------------------
 The program finished with the following exception:

java.lang.RuntimeException: Error deploying the YARN cluster
        at org.apache.flink.yarn.cli.FlinkYarnSessionCli.createCluster(FlinkYarnSessionCli.java:594)
        at org.apache.flink.yarn.cli.FlinkYarnSessionCli.createCluster(FlinkYarnSessionCli.java:81)
        at org.apache.flink.client.CliFrontend.createClient(CliFrontend.java:925)
        at org.apache.flink.client.CliFrontend.run(CliFrontend.java:264)
        at org.apache.flink.client.CliFrontend.parseParameters(CliFrontend.java:1054)
        at org.apache.flink.client.CliFrontend$1.call(CliFrontend.java:1101)
        at org.apache.flink.client.CliFrontend$1.call(CliFrontend.java:1098)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
        at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
        at org.apache.flink.client.CliFrontend.main(CliFrontend.java:1098)
Caused by: java.lang.RuntimeException: Couldn&apos;t deploy Yarn session cluster
        at org.apache.flink.yarn.AbstractYarnClusterDescriptor.deploySessionCluster(AbstractYarnClusterDescriptor.java:368)
        at org.apache.flink.yarn.cli.FlinkYarnSessionCli.createCluster(FlinkYarnSessionCli.java:592)
        ... 11 more
Caused by: com.amazonaws.SdkClientException: Unable to calculate MD5 hash: /home/hadoop/flink/lib (Is a directory)
        at com.amazonaws.services.s3.AmazonS3Client.putObject(AmazonS3Client.java:1624)
        at com.amazonaws.services.s3.transfer.internal.UploadCallable.uploadInOneChunk(UploadCallable.java:133)
        at com.amazonaws.services.s3.transfer.internal.UploadCallable.call(UploadCallable.java:125)
        at com.amazonaws.services.s3.transfer.internal.UploadMonitor.call(UploadMonitor.java:143)
        at com.amazonaws.services.s3.transfer.internal.UploadMonitor.call(UploadMonitor.java:48)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:748)
Caused by: java.io.FileNotFoundException: /home/hadoop/flink/lib (Is a directory)
        at java.io.FileInputStream.open0(Native Method)
        at java.io.FileInputStream.open(FileInputStream.java:195)
        at java.io.FileInputStream.&amp;lt;init&amp;gt;(FileInputStream.java:138)
        at com.amazonaws.util.Md5Utils.computeMD5Hash(Md5Utils.java:97)
        at com.amazonaws.util.Md5Utils.md5AsBase64(Md5Utils.java:104)
        at com.amazonaws.services.s3.AmazonS3Client.putObject(AmazonS3Client.java:1620)
        ... 8 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;
</comment>
                            <comment id="16233867" author="aljoscha" created="Wed, 1 Nov 2017 09:58:05 +0000"  >&lt;p&gt;Thanks, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=NicoK&quot; class=&quot;user-hover&quot; rel=&quot;NicoK&quot;&gt;NicoK&lt;/a&gt; for finding this! I think we should downgrade this to &quot;Critical&quot; since it&apos;s a very specific problem and we can provide a fix for this in a bug fix release? Unless you know a very quick fix for this?&lt;/p&gt;</comment>
                            <comment id="16235755" author="nicok" created="Thu, 2 Nov 2017 13:48:44 +0000"  >&lt;p&gt;I was working a bit with the initial PR and have a fix (with unit tests) coming up ...&lt;/p&gt;</comment>
                            <comment id="16236512" author="githubbot" created="Thu, 2 Nov 2017 20:16:13 +0000"  >&lt;p&gt;GitHub user NicoK opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-4228&quot; title=&quot;YARN artifact upload does not work with S3AFileSystem&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-4228&quot;&gt;&lt;del&gt;FLINK-4228&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;yarn/s3a&amp;#93;&lt;/span&gt; fix yarn resource upload s3a defaultFs&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;What is the purpose of the change&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;    If YARN is configured to use the `s3a` default file system, upload of the Flink jars will fail since its `org.apache.hadoop.fs.FileSystem#copyFromLocalFile()` does not work recursively on the given `lib` folder.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Brief change log&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;implement our own recursive upload (based on #2288)&lt;/li&gt;
	&lt;li&gt;add unit tests to verify its behaviour for both `hdfs://` and `s3://` (via S3A) resource uploads&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Verifying this change&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;    This change added tests and can be verified as follows:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;added a unit test for HDFS uploads via our `MiniDFSCluster`&lt;/li&gt;
	&lt;li&gt;added integration test to verify S3 uploads (via the S3A filesystem implementation of the `flink-s3-fs-hadoop` sub-project)&lt;/li&gt;
	&lt;li&gt;manually verified the test on YARN with both S3A and HDFS default file systems being set&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Does this pull request potentially affect one of the following parts:&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Dependencies (does it add or upgrade a dependency): (yes - internally)&lt;/li&gt;
	&lt;li&gt;The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)&lt;/li&gt;
	&lt;li&gt;The serializers: (no)&lt;/li&gt;
	&lt;li&gt;The runtime per-record code paths (performance sensitive): (no)&lt;/li&gt;
	&lt;li&gt;Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Documentation&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Does this pull request introduce a new feature? (no)&lt;/li&gt;
	&lt;li&gt;If yes, how is the feature documented? (JavaDocs)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/NicoK/flink&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/NicoK/flink&lt;/a&gt; flink-4228&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #4939&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 5d31f41e0e480820e9fec1efa84e5725364a136d&lt;br/&gt;
Author: Nico Kruber &amp;lt;nico@data-artisans.com&amp;gt;&lt;br/&gt;
Date:   2017-11-02T18:38:48Z&lt;/p&gt;

&lt;p&gt;    &lt;span class=&quot;error&quot;&gt;&amp;#91;hotfix&amp;#93;&lt;/span&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;s3&amp;#93;&lt;/span&gt; fix HadoopS3FileSystemITCase leaving test directories behind in S3&lt;/p&gt;

&lt;p&gt;commit bf47d376397a8e64625a031468d5f5d0a5486238&lt;br/&gt;
Author: Nico Kruber &amp;lt;nico@data-artisans.com&amp;gt;&lt;br/&gt;
Date:   2016-11-09T20:04:50Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-4228&quot; title=&quot;YARN artifact upload does not work with S3AFileSystem&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-4228&quot;&gt;&lt;del&gt;FLINK-4228&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;yarn/s3&amp;#93;&lt;/span&gt; fix for yarn staging with s3a defaultFs&lt;/p&gt;

&lt;p&gt;    + includes a new unit tests for recursive uploads to hfds:// targets&lt;br/&gt;
    + add a unit test for recursive file uploads to s3:// via s3a&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="16236587" author="githubbot" created="Thu, 2 Nov 2017 20:57:14 +0000"  >&lt;p&gt;Github user StephanEwen commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939#discussion_r148656504&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939#discussion_r148656504&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-filesystems/flink-s3-fs-hadoop/pom.xml &amp;#8212;&lt;br/&gt;
    @@ -182,6 +182,21 @@ under the License.&lt;br/&gt;
     			&amp;lt;version&amp;gt;${project.version}&amp;lt;/version&amp;gt;&lt;br/&gt;
     			&amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;&lt;br/&gt;
     		&amp;lt;/dependency&amp;gt;&lt;br/&gt;
    +		&amp;lt;!-- for HadoopS3FileSystemITCase.testRecursiveUploadForYarn --&amp;gt;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Would be great if we can avoid adding these dependencies.&lt;br/&gt;
    This couples projects that were really meant to be independent, even if just in test scope.&lt;/p&gt;

&lt;p&gt;    If this is about testing recursive upload, can this be written properly as a test case in this project?&lt;br/&gt;
    Or can the Yarn upload test be completely in the yarn test project, adding a dependency on this s3 project?&lt;/p&gt;</comment>
                            <comment id="16236588" author="githubbot" created="Thu, 2 Nov 2017 20:57:14 +0000"  >&lt;p&gt;Github user StephanEwen commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939#discussion_r148656918&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939#discussion_r148656918&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-filesystems/flink-s3-fs-hadoop/src/test/java/org/apache/flink/fs/s3hadoop/HadoopS3FileSystemITCase.java &amp;#8212;&lt;br/&gt;
    @@ -57,11 +62,52 @@&lt;br/&gt;
     	private static final String ACCESS_KEY = System.getenv(&quot;ARTIFACTS_AWS_ACCESS_KEY&quot;);&lt;br/&gt;
     	private static final String SECRET_KEY = System.getenv(&quot;ARTIFACTS_AWS_SECRET_KEY&quot;);&lt;/p&gt;

&lt;p&gt;    +	@Rule&lt;br/&gt;
    +	public TemporaryFolder tempFolder = new TemporaryFolder();&lt;br/&gt;
    +&lt;br/&gt;
     	@BeforeClass&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static void checkIfCredentialsArePresent() {&lt;br/&gt;
    +	public static void checkCredentialsAndSetup() throws IOException {&lt;br/&gt;
    +		// check whether credentials exist&lt;br/&gt;
     		Assume.assumeTrue(&quot;AWS S3 bucket not configured, skipping test...&quot;, BUCKET != null);&lt;br/&gt;
     		Assume.assumeTrue(&quot;AWS S3 access key not configured, skipping test...&quot;, ACCESS_KEY != null);&lt;br/&gt;
     		Assume.assumeTrue(&quot;AWS S3 secret key not configured, skipping test...&quot;, SECRET_KEY != null);&lt;br/&gt;
    +&lt;br/&gt;
    +		// initialize configuration with valid credentials
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    I would suggest to move this out of the &quot;setup&quot; method into the actual test.&lt;br/&gt;
    The setup logic is not shared (all other test methods don&apos;t assume that setup) and it also assumes existence of functionality that is tested in other test methods..&lt;/p&gt;</comment>
                            <comment id="16237462" author="githubbot" created="Fri, 3 Nov 2017 11:11:09 +0000"  >&lt;p&gt;Github user NicoK commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939#discussion_r148756295&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939#discussion_r148756295&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-filesystems/flink-s3-fs-hadoop/src/test/java/org/apache/flink/fs/s3hadoop/HadoopS3FileSystemITCase.java &amp;#8212;&lt;br/&gt;
    @@ -57,11 +62,52 @@&lt;br/&gt;
     	private static final String ACCESS_KEY = System.getenv(&quot;ARTIFACTS_AWS_ACCESS_KEY&quot;);&lt;br/&gt;
     	private static final String SECRET_KEY = System.getenv(&quot;ARTIFACTS_AWS_SECRET_KEY&quot;);&lt;/p&gt;

&lt;p&gt;    +	@Rule&lt;br/&gt;
    +	public TemporaryFolder tempFolder = new TemporaryFolder();&lt;br/&gt;
    +&lt;br/&gt;
     	@BeforeClass&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static void checkIfCredentialsArePresent() {&lt;br/&gt;
    +	public static void checkCredentialsAndSetup() throws IOException {&lt;br/&gt;
    +		// check whether credentials exist&lt;br/&gt;
     		Assume.assumeTrue(&quot;AWS S3 bucket not configured, skipping test...&quot;, BUCKET != null);&lt;br/&gt;
     		Assume.assumeTrue(&quot;AWS S3 access key not configured, skipping test...&quot;, ACCESS_KEY != null);&lt;br/&gt;
     		Assume.assumeTrue(&quot;AWS S3 secret key not configured, skipping test...&quot;, SECRET_KEY != null);&lt;br/&gt;
    +&lt;br/&gt;
    +		// initialize configuration with valid credentials
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    This is actually not for the new test, but for the cleanup: the current state of the `HadoopS3FileSystemITCase` leaves this (random) test directory behind. In order to delete this after the tests of the class finished, I thought we should make sure that it did not exist before so that we are not deleting something we shouldn&apos;t!&lt;/p&gt;</comment>
                            <comment id="16237468" author="githubbot" created="Fri, 3 Nov 2017 11:14:53 +0000"  >&lt;p&gt;Github user NicoK commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939#discussion_r148756933&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939#discussion_r148756933&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-filesystems/flink-s3-fs-hadoop/pom.xml &amp;#8212;&lt;br/&gt;
    @@ -182,6 +182,21 @@ under the License.&lt;br/&gt;
     			&amp;lt;version&amp;gt;${project.version}&amp;lt;/version&amp;gt;&lt;br/&gt;
     			&amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;&lt;br/&gt;
     		&amp;lt;/dependency&amp;gt;&lt;br/&gt;
    +		&amp;lt;!-- for HadoopS3FileSystemITCase.testRecursiveUploadForYarn --&amp;gt;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Yes this is about the recursive upload which needs to be tested once with hdfs and once more with s3.&lt;/p&gt;

&lt;p&gt;    Sure we could flip the dependency and let the tests in the `yarn` sub-project depend on `flink-s3-fs-hadoop` (and I don&apos;t mind which depends on which, actually) but wouldn&apos;t this be just the same but in reverse?&lt;/p&gt;</comment>
                            <comment id="16239334" author="githubbot" created="Sun, 5 Nov 2017 00:28:22 +0000"  >&lt;p&gt;Github user NicoK commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    ok, finally, this seems to be in shape&lt;/p&gt;

&lt;p&gt;    My try to fool Travis in running for every commit though resulted in all those failed runs (which should really be read as &quot;cancelled&quot; since they never ran).&lt;/p&gt;

&lt;p&gt;    FYI: If desired, I could also create separate issues for the missing cleanup in the `HadoopS3FileSystemITCase` and the flink dist.jar delivered twice.&lt;/p&gt;</comment>
                            <comment id="16240257" author="githubbot" created="Mon, 6 Nov 2017 13:10:33 +0000"  >&lt;p&gt;Github user tillrohrmann commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939#discussion_r149071109&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939#discussion_r149071109&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-yarn/src/main/java/org/apache/flink/yarn/Utils.java &amp;#8212;&lt;br/&gt;
    @@ -408,10 +437,12 @@ static ContainerLaunchContext createTaskExecutorContext(&lt;br/&gt;
     		// prepare additional files to be shipped&lt;br/&gt;
     		for (String pathStr : shipListString.split(&quot;,&quot;)) {&lt;br/&gt;
     			if (!pathStr.isEmpty()) {&lt;br/&gt;
    +				String[] pathWithKey = pathStr.split(&quot;=&quot;);&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Isn&apos;t this rather `keyWithPath`?&lt;/p&gt;</comment>
                            <comment id="16240258" author="githubbot" created="Mon, 6 Nov 2017 13:10:34 +0000"  >&lt;p&gt;Github user tillrohrmann commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939#discussion_r149070583&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939#discussion_r149070583&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-yarn/src/main/java/org/apache/flink/yarn/Utils.java &amp;#8212;&lt;br/&gt;
    @@ -117,27 +118,50 @@ public static void setupYarnClassPath(Configuration conf, Map&amp;lt;String, String&amp;gt; ap&lt;br/&gt;
     	}&lt;/p&gt;

&lt;p&gt;     	/**&lt;br/&gt;
    +	 * Copy a local file to a remote file system.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @param fs&lt;br/&gt;
    +	 * 		remote filesystem&lt;br/&gt;
    +	 * @param appId&lt;br/&gt;
    +	 * 		application ID&lt;br/&gt;
    +	 * @param localRsrcPath&lt;br/&gt;
    +	 * 		path to the local file&lt;br/&gt;
    +	 * @param homedir&lt;br/&gt;
    +	 * 		remote home directory base (will be extended)&lt;br/&gt;
    +	 * @param relativeTargetPath&lt;br/&gt;
    +	 * 		relative target path of the file (will be prefixed be the full home directory we set up)&lt;br/&gt;
    +	 *&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@return Path to remote file (usually hdfs)&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* @throws IOException&lt;br/&gt;
     	 */&lt;/li&gt;
	&lt;li&gt;public static Path setupLocalResource(&lt;/li&gt;
	&lt;li&gt;FileSystem fs,&lt;/li&gt;
	&lt;li&gt;String appId, Path localRsrcPath,&lt;/li&gt;
	&lt;li&gt;LocalResource appMasterJar,&lt;/li&gt;
	&lt;li&gt;Path homedir) throws IOException {&lt;br/&gt;
    +	static Tuple2&amp;lt;Path, LocalResource&amp;gt; setupLocalResource(&lt;br/&gt;
    +		FileSystem fs,&lt;br/&gt;
    +		String appId,&lt;br/&gt;
    +		Path localRsrcPath,
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    `localSrcPath`?&lt;/p&gt;</comment>
                            <comment id="16240259" author="githubbot" created="Mon, 6 Nov 2017 13:10:34 +0000"  >&lt;p&gt;Github user tillrohrmann commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939#discussion_r149071491&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939#discussion_r149071491&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-yarn/src/main/java/org/apache/flink/yarn/Utils.java &amp;#8212;&lt;br/&gt;
    @@ -117,27 +118,50 @@ public static void setupYarnClassPath(Configuration conf, Map&amp;lt;String, String&amp;gt; ap&lt;br/&gt;
     	}&lt;/p&gt;

&lt;p&gt;     	/**&lt;br/&gt;
    +	 * Copy a local file to a remote file system.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @param fs&lt;br/&gt;
    +	 * 		remote filesystem&lt;br/&gt;
    +	 * @param appId&lt;br/&gt;
    +	 * 		application ID&lt;br/&gt;
    +	 * @param localRsrcPath&lt;br/&gt;
    +	 * 		path to the local file&lt;br/&gt;
    +	 * @param homedir&lt;br/&gt;
    +	 * 		remote home directory base (will be extended)&lt;br/&gt;
    +	 * @param relativeTargetPath&lt;br/&gt;
    +	 * 		relative target path of the file (will be prefixed be the full home directory we set up)&lt;br/&gt;
    +	 *&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@return Path to remote file (usually hdfs)&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* @throws IOException&lt;br/&gt;
     	 */&lt;/li&gt;
	&lt;li&gt;public static Path setupLocalResource(&lt;/li&gt;
	&lt;li&gt;FileSystem fs,&lt;/li&gt;
	&lt;li&gt;String appId, Path localRsrcPath,&lt;/li&gt;
	&lt;li&gt;LocalResource appMasterJar,&lt;/li&gt;
	&lt;li&gt;Path homedir) throws IOException {&lt;br/&gt;
    +	static Tuple2&amp;lt;Path, LocalResource&amp;gt; setupLocalResource(&lt;br/&gt;
    +		FileSystem fs,&lt;br/&gt;
    +		String appId,&lt;br/&gt;
    +		Path localRsrcPath,&lt;br/&gt;
    +		Path homedir,&lt;br/&gt;
    +		String relativeTargetPath) throws IOException {&lt;br/&gt;
    +&lt;br/&gt;
    +		if (new File(localRsrcPath.toUri().getPath()).isDirectory()) 
{
    +			throw new IllegalArgumentException(&quot;File to copy must not be a directory: &quot; +
    +				localRsrcPath);
    +		}&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     		// copy resource to HDFS&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;String suffix = &quot;.flink/&quot; + appId + &quot;/&quot; + localRsrcPath.getName();&lt;br/&gt;
    +		String suffix = &quot;.flink/&quot; + appId + &quot;/&quot; + relativeTargetPath + &quot;/&quot; + localRsrcPath.getName();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     		Path dst = new Path(homedir, suffix);&lt;/p&gt;

&lt;p&gt;     		LOG.info(&quot;Copying from &quot; + localRsrcPath + &quot; to &quot; + dst);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;fs.copyFromLocalFile(localRsrcPath, dst);&lt;/li&gt;
	&lt;li&gt;registerLocalResource(fs, dst, appMasterJar);&lt;/li&gt;
	&lt;li&gt;return dst;&lt;br/&gt;
    +&lt;br/&gt;
    +		fs.copyFromLocalFile(false, true, localRsrcPath, dst);&lt;br/&gt;
    +&lt;br/&gt;
    +		// now create the resource instance&lt;br/&gt;
    +		LocalResource resource = Records.newRecord(LocalResource.class);&lt;br/&gt;
    +		registerLocalResource(fs, dst, resource);&lt;br/&gt;
    +		return Tuple2.of(dst, resource);&lt;br/&gt;
     	}&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static void registerLocalResource(FileSystem fs, Path remoteRsrcPath, LocalResource localResource) throws IOException {&lt;br/&gt;
    +	private static void registerLocalResource(
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Touching this code, could we change it that we create and return a `LocalResource` in this method?&lt;/p&gt;</comment>
                            <comment id="16240260" author="githubbot" created="Mon, 6 Nov 2017 13:10:34 +0000"  >&lt;p&gt;Github user tillrohrmann commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939#discussion_r149069107&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939#discussion_r149069107&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-yarn/src/main/java/org/apache/flink/yarn/AbstractYarnClusterDescriptor.java &amp;#8212;&lt;br/&gt;
    @@ -705,11 +707,12 @@ public ApplicationReport startAppMaster(&lt;br/&gt;
     		StringBuilder envShipFileList = new StringBuilder();&lt;/p&gt;

&lt;p&gt;     		// upload and register ship files&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;List&amp;lt;String&amp;gt; systemClassPaths = uploadAndRegisterFiles(systemShipFiles, fs, appId.toString(), paths, localResources, envShipFileList);&lt;br/&gt;
    +		List&amp;lt;String&amp;gt; systemClassPaths = uploadAndRegisterFiles(systemShipFiles, fs,&lt;br/&gt;
    +			homeDir, appId, paths, localResources, envShipFileList);
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    nit: I would either not break lines or break after each argument.&lt;/p&gt;</comment>
                            <comment id="16240261" author="githubbot" created="Mon, 6 Nov 2017 13:10:34 +0000"  >&lt;p&gt;Github user tillrohrmann commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939#discussion_r149071810&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939#discussion_r149071810&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-yarn/src/test/java/org/apache/flink/yarn/YarnFileStageTest.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,215 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.yarn;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.util.OperatingSystem;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.hadoop.fs.FSDataInputStream;&lt;br/&gt;
    +import org.apache.hadoop.fs.FileSystem;&lt;br/&gt;
    +import org.apache.hadoop.fs.LocatedFileStatus;&lt;br/&gt;
    +import org.apache.hadoop.fs.Path;&lt;br/&gt;
    +import org.apache.hadoop.fs.RemoteIterator;&lt;br/&gt;
    +import org.apache.hadoop.hdfs.MiniDFSCluster;&lt;br/&gt;
    +import org.apache.hadoop.yarn.api.records.ApplicationId;&lt;br/&gt;
    +import org.apache.hadoop.yarn.api.records.LocalResource;&lt;br/&gt;
    +import org.apache.hadoop.yarn.util.ConverterUtils;&lt;br/&gt;
    +import org.junit.AfterClass;&lt;br/&gt;
    +import org.junit.Assume;&lt;br/&gt;
    +import org.junit.Before;&lt;br/&gt;
    +import org.junit.BeforeClass;&lt;br/&gt;
    +import org.junit.ClassRule;&lt;br/&gt;
    +import org.junit.Rule;&lt;br/&gt;
    +import org.junit.Test;&lt;br/&gt;
    +import org.junit.rules.TemporaryFolder;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.io.DataOutputStream;&lt;br/&gt;
    +import java.io.File;&lt;br/&gt;
    +import java.io.FileOutputStream;&lt;br/&gt;
    +import java.util.ArrayList;&lt;br/&gt;
    +import java.util.Collections;&lt;br/&gt;
    +import java.util.HashMap;&lt;br/&gt;
    +import java.util.List;&lt;br/&gt;
    +import java.util.Map;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.hamcrest.MatcherAssert.assertThat;&lt;br/&gt;
    +import static org.hamcrest.Matchers.equalTo;&lt;br/&gt;
    +import static org.junit.Assert.assertEquals;&lt;br/&gt;
    +import static org.junit.Assert.assertFalse;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * Tests for verifying file staging during submission to YARN works.&lt;br/&gt;
    + */&lt;br/&gt;
    +public class YarnFileStageTest {&lt;br/&gt;
    +&lt;br/&gt;
    +	@ClassRule&lt;br/&gt;
    +	public static final TemporaryFolder CLASS_TEMP_DIR = new TemporaryFolder();&lt;br/&gt;
    +&lt;br/&gt;
    +	@Rule&lt;br/&gt;
    +	public TemporaryFolder tempFolder = new TemporaryFolder();&lt;br/&gt;
    +&lt;br/&gt;
    +	private static MiniDFSCluster hdfsCluster;&lt;br/&gt;
    +&lt;br/&gt;
    +	private static Path hdfsRootPath;&lt;br/&gt;
    +&lt;br/&gt;
    +	private org.apache.hadoop.conf.Configuration hadoopConfig;&lt;br/&gt;
    +&lt;br/&gt;
    +	// ------------------------------------------------------------------------&lt;br/&gt;
    +	//  Test setup and shutdown&lt;br/&gt;
    +	// ------------------------------------------------------------------------&lt;br/&gt;
    +&lt;br/&gt;
    +	@BeforeClass&lt;br/&gt;
    +	public static void createHDFS() throws Exception &lt;/p&gt;
{
    +		Assume.assumeTrue(!OperatingSystem.isWindows());
    +
    +		final File tempDir = CLASS_TEMP_DIR.newFolder();
    +
    +		org.apache.hadoop.conf.Configuration hdConf = new org.apache.hadoop.conf.Configuration();
    +		hdConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, tempDir.getAbsolutePath());
    +
    +		MiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(hdConf);
    +		hdfsCluster = builder.build();
    +		hdfsRootPath = new Path(hdfsCluster.getURI());
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	@AfterClass&lt;br/&gt;
    +	public static void destroyHDFS() {&lt;br/&gt;
    +		if (hdfsCluster != null) &lt;/p&gt;
{
    +			hdfsCluster.shutdown();
    +		}
&lt;p&gt;    +		hdfsCluster = null;&lt;br/&gt;
    +		hdfsRootPath = null;&lt;br/&gt;
    +	}&lt;br/&gt;
    +&lt;br/&gt;
    +	@Before&lt;br/&gt;
    +	public void initConfig() &lt;/p&gt;
{
    +		hadoopConfig = new org.apache.hadoop.conf.Configuration();
    +		hadoopConfig.set(org.apache.hadoop.fs.FileSystem.FS_DEFAULT_NAME_KEY, hdfsRootPath.toString());
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Verifies that nested directories are properly copied with a &amp;lt;tt&amp;gt;hdfs://&amp;lt;/tt&amp;gt; file&lt;br/&gt;
    +	 * system (from a &amp;lt;tt&amp;gt;&lt;a href=&quot;file:///absolute/path&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;file:///absolute/path&lt;/a&gt;&amp;lt;/tt&amp;gt; source path).&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testCopyFromLocalRecursiveWithScheme() throws Exception &lt;/p&gt;
{
    +		final FileSystem targetFileSystem = hdfsRootPath.getFileSystem(hadoopConfig);
    +		final Path targetDir = targetFileSystem.getWorkingDirectory();
    +
    +		testCopyFromLocalRecursive(targetFileSystem, targetDir, tempFolder, true);
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Verifies that nested directories are properly copied with a &amp;lt;tt&amp;gt;hdfs://&amp;lt;/tt&amp;gt; file&lt;br/&gt;
    +	 * system (from a &amp;lt;tt&amp;gt;/absolute/path&amp;lt;/tt&amp;gt; source path).&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testCopyFromLocalRecursiveWithoutScheme() throws Exception &lt;/p&gt;
{
    +		final FileSystem targetFileSystem = hdfsRootPath.getFileSystem(hadoopConfig);
    +		final Path targetDir = targetFileSystem.getWorkingDirectory();
    +
    +		testCopyFromLocalRecursive(targetFileSystem, targetDir, tempFolder, false);
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Verifies that nested directories are properly copied with the given filesystem and paths.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @param targetFileSystem&lt;br/&gt;
    +	 * 		file system of the target path&lt;br/&gt;
    +	 * @param targetDir&lt;br/&gt;
    +	 * 		target path (URI like &amp;lt;tt&amp;gt;hdfs://...&amp;lt;/tt&amp;gt;)&lt;br/&gt;
    +	 * @param tempFolder&lt;br/&gt;
    +	 * 		JUnit temporary folder rule to create the source directory with&lt;br/&gt;
    +	 * @param addSchemeToLocalPath&lt;br/&gt;
    +	 * 		whether add the &amp;lt;tt&amp;gt;&lt;a href=&quot;file://&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;file://&lt;/a&gt;&amp;lt;/tt&amp;gt; scheme to the local path to copy from&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public static void testCopyFromLocalRecursive(&lt;br/&gt;
    +			FileSystem targetFileSystem, Path targetDir, TemporaryFolder tempFolder,&lt;br/&gt;
    +			boolean addSchemeToLocalPath) throws Exception {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    nit: line breaks inconsistent.&lt;/p&gt;</comment>
                            <comment id="16240262" author="githubbot" created="Mon, 6 Nov 2017 13:10:34 +0000"  >&lt;p&gt;Github user tillrohrmann commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939#discussion_r149073723&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939#discussion_r149073723&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-yarn/pom.xml &amp;#8212;&lt;br/&gt;
    @@ -99,6 +99,13 @@ under the License.&lt;br/&gt;
     		&amp;lt;/dependency&amp;gt;&lt;/p&gt;

&lt;p&gt;     		&amp;lt;dependency&amp;gt;&lt;br/&gt;
    +			&amp;lt;groupId&amp;gt;org.apache.flink&amp;lt;/groupId&amp;gt;&lt;br/&gt;
    +			&amp;lt;artifactId&amp;gt;flink-s3-fs-hadoop&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
    +			&amp;lt;version&amp;gt;${project.version}&amp;lt;/version&amp;gt;&lt;br/&gt;
    +			&amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;&lt;br/&gt;
    +		&amp;lt;/dependency&amp;gt;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    We could move the S3 upload test to `flink-yarn-tests`. That way we would only add this dependency to `flink-yarn-tests`.&lt;/p&gt;</comment>
                            <comment id="16240263" author="githubbot" created="Mon, 6 Nov 2017 13:10:34 +0000"  >&lt;p&gt;Github user tillrohrmann commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939#discussion_r149069663&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939#discussion_r149069663&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-yarn/src/main/java/org/apache/flink/yarn/AbstractYarnClusterDescriptor.java &amp;#8212;&lt;br/&gt;
    @@ -981,25 +998,54 @@ public ApplicationReport startAppMaster(&lt;br/&gt;
     		return report;&lt;br/&gt;
     	}&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static List&amp;lt;String&amp;gt; uploadAndRegisterFiles(&lt;/li&gt;
	&lt;li&gt;Collection&amp;lt;File&amp;gt; shipFiles,&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Uploads and registers a single resource and adds it to &amp;lt;tt&amp;gt;localResources&amp;lt;/tt&amp;gt;.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @param key&lt;br/&gt;
    +	 * 		the key to add the resource under&lt;br/&gt;
    +	 * @param fs&lt;br/&gt;
    +	 * 		the remote file system to upload to&lt;br/&gt;
    +	 * @param appId&lt;br/&gt;
    +	 * 		application ID&lt;br/&gt;
    +	 * @param localRsrcPath&lt;br/&gt;
    +	 * 		local path to the file&lt;br/&gt;
    +	 * @param localResources&lt;br/&gt;
    +	 * 		map of resources&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @return the remote path to the uploaded resource&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	private static Path setupSingleLocalResource(&lt;br/&gt;
    +			String key,&lt;br/&gt;
     			FileSystem fs,&lt;/li&gt;
	&lt;li&gt;String appId,&lt;/li&gt;
	&lt;li&gt;List&amp;lt;Path&amp;gt; remotePaths,&lt;br/&gt;
    +			ApplicationId appId,&lt;br/&gt;
    +			Path localRsrcPath,
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Typo `localSrcPath`?&lt;/p&gt;</comment>
                            <comment id="16240264" author="githubbot" created="Mon, 6 Nov 2017 13:10:34 +0000"  >&lt;p&gt;Github user tillrohrmann commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939#discussion_r149070829&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939#discussion_r149070829&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-yarn/src/main/java/org/apache/flink/yarn/Utils.java &amp;#8212;&lt;br/&gt;
    @@ -117,27 +118,50 @@ public static void setupYarnClassPath(Configuration conf, Map&amp;lt;String, String&amp;gt; ap&lt;br/&gt;
     	}&lt;/p&gt;

&lt;p&gt;     	/**&lt;br/&gt;
    +	 * Copy a local file to a remote file system.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @param fs&lt;br/&gt;
    +	 * 		remote filesystem&lt;br/&gt;
    +	 * @param appId&lt;br/&gt;
    +	 * 		application ID&lt;br/&gt;
    +	 * @param localRsrcPath&lt;br/&gt;
    +	 * 		path to the local file&lt;br/&gt;
    +	 * @param homedir&lt;br/&gt;
    +	 * 		remote home directory base (will be extended)&lt;br/&gt;
    +	 * @param relativeTargetPath&lt;br/&gt;
    +	 * 		relative target path of the file (will be prefixed be the full home directory we set up)&lt;br/&gt;
    +	 *&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@return Path to remote file (usually hdfs)&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* @throws IOException&lt;br/&gt;
     	 */&lt;/li&gt;
	&lt;li&gt;public static Path setupLocalResource(&lt;/li&gt;
	&lt;li&gt;FileSystem fs,&lt;/li&gt;
	&lt;li&gt;String appId, Path localRsrcPath,&lt;/li&gt;
	&lt;li&gt;LocalResource appMasterJar,&lt;/li&gt;
	&lt;li&gt;Path homedir) throws IOException {&lt;br/&gt;
    +	static Tuple2&amp;lt;Path, LocalResource&amp;gt; setupLocalResource(&lt;br/&gt;
    +		FileSystem fs,&lt;br/&gt;
    +		String appId,&lt;br/&gt;
    +		Path localRsrcPath,&lt;br/&gt;
    +		Path homedir,&lt;br/&gt;
    +		String relativeTargetPath) throws IOException {&lt;br/&gt;
    +&lt;br/&gt;
    +		if (new File(localRsrcPath.toUri().getPath()).isDirectory()) 
{
    +			throw new IllegalArgumentException(&quot;File to copy must not be a directory: &quot; +
    +				localRsrcPath);
    +		}&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     		// copy resource to HDFS&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;String suffix = &quot;.flink/&quot; + appId + &quot;/&quot; + localRsrcPath.getName();&lt;br/&gt;
    +		String suffix = &quot;.flink/&quot; + appId + &quot;/&quot; + relativeTargetPath + &quot;/&quot; + localRsrcPath.getName();
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    What if `relativeTargetPath` is `&quot;&quot;`. Wouldn&apos;t that lead to `appId//localSrcPath.getName()`?&lt;/p&gt;</comment>
                            <comment id="16240265" author="githubbot" created="Mon, 6 Nov 2017 13:10:34 +0000"  >&lt;p&gt;Github user tillrohrmann commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939#discussion_r149070982&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939#discussion_r149070982&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-yarn/src/main/java/org/apache/flink/yarn/Utils.java &amp;#8212;&lt;br/&gt;
    @@ -117,27 +118,50 @@ public static void setupYarnClassPath(Configuration conf, Map&amp;lt;String, String&amp;gt; ap&lt;br/&gt;
     	}&lt;/p&gt;

&lt;p&gt;     	/**&lt;br/&gt;
    +	 * Copy a local file to a remote file system.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @param fs&lt;br/&gt;
    +	 * 		remote filesystem&lt;br/&gt;
    +	 * @param appId&lt;br/&gt;
    +	 * 		application ID&lt;br/&gt;
    +	 * @param localRsrcPath&lt;br/&gt;
    +	 * 		path to the local file&lt;br/&gt;
    +	 * @param homedir&lt;br/&gt;
    +	 * 		remote home directory base (will be extended)&lt;br/&gt;
    +	 * @param relativeTargetPath&lt;br/&gt;
    +	 * 		relative target path of the file (will be prefixed be the full home directory we set up)&lt;br/&gt;
    +	 *&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@return Path to remote file (usually hdfs)&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* @throws IOException&lt;br/&gt;
     	 */&lt;/li&gt;
	&lt;li&gt;public static Path setupLocalResource(&lt;/li&gt;
	&lt;li&gt;FileSystem fs,&lt;/li&gt;
	&lt;li&gt;String appId, Path localRsrcPath,&lt;/li&gt;
	&lt;li&gt;LocalResource appMasterJar,&lt;/li&gt;
	&lt;li&gt;Path homedir) throws IOException {&lt;br/&gt;
    +	static Tuple2&amp;lt;Path, LocalResource&amp;gt; setupLocalResource(&lt;br/&gt;
    +		FileSystem fs,&lt;br/&gt;
    +		String appId,&lt;br/&gt;
    +		Path localRsrcPath,&lt;br/&gt;
    +		Path homedir,&lt;br/&gt;
    +		String relativeTargetPath) throws IOException {&lt;br/&gt;
    +&lt;br/&gt;
    +		if (new File(localRsrcPath.toUri().getPath()).isDirectory()) 
{
    +			throw new IllegalArgumentException(&quot;File to copy must not be a directory: &quot; +
    +				localRsrcPath);
    +		}&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     		// copy resource to HDFS&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;String suffix = &quot;.flink/&quot; + appId + &quot;/&quot; + localRsrcPath.getName();&lt;br/&gt;
    +		String suffix = &quot;.flink/&quot; + appId + &quot;/&quot; + relativeTargetPath + &quot;/&quot; + localRsrcPath.getName();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     		Path dst = new Path(homedir, suffix);&lt;/p&gt;

&lt;p&gt;     		LOG.info(&quot;Copying from &quot; + localRsrcPath + &quot; to &quot; + dst);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;fs.copyFromLocalFile(localRsrcPath, dst);&lt;/li&gt;
	&lt;li&gt;registerLocalResource(fs, dst, appMasterJar);&lt;/li&gt;
	&lt;li&gt;return dst;&lt;br/&gt;
    +&lt;br/&gt;
    +		fs.copyFromLocalFile(false, true, localRsrcPath, dst);&lt;br/&gt;
    +&lt;br/&gt;
    +		// now create the resource instance&lt;br/&gt;
    +		LocalResource resource = Records.newRecord(LocalResource.class);&lt;br/&gt;
    +		registerLocalResource(fs, dst, resource);&lt;br/&gt;
    +		return Tuple2.of(dst, resource);&lt;br/&gt;
     	}&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static void registerLocalResource(FileSystem fs, Path remoteRsrcPath, LocalResource localResource) throws IOException {&lt;br/&gt;
    +	private static void registerLocalResource(&lt;br/&gt;
    +		FileSystem fs, Path remoteRsrcPath, LocalResource localResource) throws IOException {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Either put every parameter on a different line or please revert this change.&lt;/p&gt;</comment>
                            <comment id="16240275" author="githubbot" created="Mon, 6 Nov 2017 13:20:36 +0000"  >&lt;p&gt;Github user NicoK commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939#discussion_r149076237&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939#discussion_r149076237&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-yarn/pom.xml &amp;#8212;&lt;br/&gt;
    @@ -99,6 +99,13 @@ under the License.&lt;br/&gt;
     		&amp;lt;/dependency&amp;gt;&lt;/p&gt;

&lt;p&gt;     		&amp;lt;dependency&amp;gt;&lt;br/&gt;
    +			&amp;lt;groupId&amp;gt;org.apache.flink&amp;lt;/groupId&amp;gt;&lt;br/&gt;
    +			&amp;lt;artifactId&amp;gt;flink-s3-fs-hadoop&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
    +			&amp;lt;version&amp;gt;${project.version}&amp;lt;/version&amp;gt;&lt;br/&gt;
    +			&amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;&lt;br/&gt;
    +		&amp;lt;/dependency&amp;gt;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    We could...&lt;br/&gt;
    I wasn&apos;t putting this test there because in the project&apos;s pom.xml it says `There is a separate &quot;flink-yarn-tests&quot; package that expects the &quot;flink-dist&quot; package to be build before.` which we do not need for this test.&lt;/p&gt;

&lt;p&gt;    whatever you prefer...&lt;/p&gt;</comment>
                            <comment id="16240283" author="githubbot" created="Mon, 6 Nov 2017 13:27:26 +0000"  >&lt;p&gt;Github user NicoK commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939#discussion_r149077620&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939#discussion_r149077620&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-yarn/src/main/java/org/apache/flink/yarn/Utils.java &amp;#8212;&lt;br/&gt;
    @@ -117,27 +118,50 @@ public static void setupYarnClassPath(Configuration conf, Map&amp;lt;String, String&amp;gt; ap&lt;br/&gt;
     	}&lt;/p&gt;

&lt;p&gt;     	/**&lt;br/&gt;
    +	 * Copy a local file to a remote file system.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @param fs&lt;br/&gt;
    +	 * 		remote filesystem&lt;br/&gt;
    +	 * @param appId&lt;br/&gt;
    +	 * 		application ID&lt;br/&gt;
    +	 * @param localRsrcPath&lt;br/&gt;
    +	 * 		path to the local file&lt;br/&gt;
    +	 * @param homedir&lt;br/&gt;
    +	 * 		remote home directory base (will be extended)&lt;br/&gt;
    +	 * @param relativeTargetPath&lt;br/&gt;
    +	 * 		relative target path of the file (will be prefixed be the full home directory we set up)&lt;br/&gt;
    +	 *&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@return Path to remote file (usually hdfs)&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* @throws IOException&lt;br/&gt;
     	 */&lt;/li&gt;
	&lt;li&gt;public static Path setupLocalResource(&lt;/li&gt;
	&lt;li&gt;FileSystem fs,&lt;/li&gt;
	&lt;li&gt;String appId, Path localRsrcPath,&lt;/li&gt;
	&lt;li&gt;LocalResource appMasterJar,&lt;/li&gt;
	&lt;li&gt;Path homedir) throws IOException {&lt;br/&gt;
    +	static Tuple2&amp;lt;Path, LocalResource&amp;gt; setupLocalResource(&lt;br/&gt;
    +		FileSystem fs,&lt;br/&gt;
    +		String appId,&lt;br/&gt;
    +		Path localRsrcPath,&lt;br/&gt;
    +		Path homedir,&lt;br/&gt;
    +		String relativeTargetPath) throws IOException {&lt;br/&gt;
    +&lt;br/&gt;
    +		if (new File(localRsrcPath.toUri().getPath()).isDirectory()) 
{
    +			throw new IllegalArgumentException(&quot;File to copy must not be a directory: &quot; +
    +				localRsrcPath);
    +		}&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     		// copy resource to HDFS&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;String suffix = &quot;.flink/&quot; + appId + &quot;/&quot; + localRsrcPath.getName();&lt;br/&gt;
    +		String suffix = &quot;.flink/&quot; + appId + &quot;/&quot; + relativeTargetPath + &quot;/&quot; + localRsrcPath.getName();
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Yes, but apparently this is filtered out by `new Path(homedir, suffix);` and the right path is created. I did stumble upon this as well and double-checked the results. The alternative is, for example, using `null` if no relative path is desired and setting the `suffix` appropriately which is a bit more ugly but may also be cleaner/safer...&lt;br/&gt;
    Would you rather go this way?&lt;/p&gt;</comment>
                            <comment id="16240285" author="githubbot" created="Mon, 6 Nov 2017 13:27:55 +0000"  >&lt;p&gt;Github user NicoK commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939#discussion_r149077720&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939#discussion_r149077720&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-yarn/src/main/java/org/apache/flink/yarn/Utils.java &amp;#8212;&lt;br/&gt;
    @@ -117,27 +118,50 @@ public static void setupYarnClassPath(Configuration conf, Map&amp;lt;String, String&amp;gt; ap&lt;br/&gt;
     	}&lt;/p&gt;

&lt;p&gt;     	/**&lt;br/&gt;
    +	 * Copy a local file to a remote file system.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @param fs&lt;br/&gt;
    +	 * 		remote filesystem&lt;br/&gt;
    +	 * @param appId&lt;br/&gt;
    +	 * 		application ID&lt;br/&gt;
    +	 * @param localRsrcPath&lt;br/&gt;
    +	 * 		path to the local file&lt;br/&gt;
    +	 * @param homedir&lt;br/&gt;
    +	 * 		remote home directory base (will be extended)&lt;br/&gt;
    +	 * @param relativeTargetPath&lt;br/&gt;
    +	 * 		relative target path of the file (will be prefixed be the full home directory we set up)&lt;br/&gt;
    +	 *&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@return Path to remote file (usually hdfs)&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* @throws IOException&lt;br/&gt;
     	 */&lt;/li&gt;
	&lt;li&gt;public static Path setupLocalResource(&lt;/li&gt;
	&lt;li&gt;FileSystem fs,&lt;/li&gt;
	&lt;li&gt;String appId, Path localRsrcPath,&lt;/li&gt;
	&lt;li&gt;LocalResource appMasterJar,&lt;/li&gt;
	&lt;li&gt;Path homedir) throws IOException {&lt;br/&gt;
    +	static Tuple2&amp;lt;Path, LocalResource&amp;gt; setupLocalResource(&lt;br/&gt;
    +		FileSystem fs,&lt;br/&gt;
    +		String appId,&lt;br/&gt;
    +		Path localRsrcPath,&lt;br/&gt;
    +		Path homedir,&lt;br/&gt;
    +		String relativeTargetPath) throws IOException {&lt;br/&gt;
    +&lt;br/&gt;
    +		if (new File(localRsrcPath.toUri().getPath()).isDirectory()) 
{
    +			throw new IllegalArgumentException(&quot;File to copy must not be a directory: &quot; +
    +				localRsrcPath);
    +		}&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     		// copy resource to HDFS&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;String suffix = &quot;.flink/&quot; + appId + &quot;/&quot; + localRsrcPath.getName();&lt;br/&gt;
    +		String suffix = &quot;.flink/&quot; + appId + &quot;/&quot; + relativeTargetPath + &quot;/&quot; + localRsrcPath.getName();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     		Path dst = new Path(homedir, suffix);&lt;/p&gt;

&lt;p&gt;     		LOG.info(&quot;Copying from &quot; + localRsrcPath + &quot; to &quot; + dst);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;fs.copyFromLocalFile(localRsrcPath, dst);&lt;/li&gt;
	&lt;li&gt;registerLocalResource(fs, dst, appMasterJar);&lt;/li&gt;
	&lt;li&gt;return dst;&lt;br/&gt;
    +&lt;br/&gt;
    +		fs.copyFromLocalFile(false, true, localRsrcPath, dst);&lt;br/&gt;
    +&lt;br/&gt;
    +		// now create the resource instance&lt;br/&gt;
    +		LocalResource resource = Records.newRecord(LocalResource.class);&lt;br/&gt;
    +		registerLocalResource(fs, dst, resource);&lt;br/&gt;
    +		return Tuple2.of(dst, resource);&lt;br/&gt;
     	}&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static void registerLocalResource(FileSystem fs, Path remoteRsrcPath, LocalResource localResource) throws IOException {&lt;br/&gt;
    +	private static void registerLocalResource(
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    oh yes - totally...&lt;/p&gt;</comment>
                            <comment id="16240288" author="githubbot" created="Mon, 6 Nov 2017 13:29:49 +0000"  >&lt;p&gt;Github user NicoK commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939#discussion_r149078183&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939#discussion_r149078183&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-yarn/src/main/java/org/apache/flink/yarn/Utils.java &amp;#8212;&lt;br/&gt;
    @@ -408,10 +437,12 @@ static ContainerLaunchContext createTaskExecutorContext(&lt;br/&gt;
     		// prepare additional files to be shipped&lt;br/&gt;
     		for (String pathStr : shipListString.split(&quot;,&quot;)) {&lt;br/&gt;
     			if (!pathStr.isEmpty()) {&lt;br/&gt;
    +				String[] pathWithKey = pathStr.split(&quot;=&quot;);&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    yes, if you see this as the order of the arguments in the array - I&apos;ll adapt the name&lt;/p&gt;</comment>
                            <comment id="16240326" author="githubbot" created="Mon, 6 Nov 2017 13:54:25 +0000"  >&lt;p&gt;Github user tillrohrmann commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939#discussion_r149083811&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939#discussion_r149083811&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-yarn/src/main/java/org/apache/flink/yarn/Utils.java &amp;#8212;&lt;br/&gt;
    @@ -117,27 +118,50 @@ public static void setupYarnClassPath(Configuration conf, Map&amp;lt;String, String&amp;gt; ap&lt;br/&gt;
     	}&lt;/p&gt;

&lt;p&gt;     	/**&lt;br/&gt;
    +	 * Copy a local file to a remote file system.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @param fs&lt;br/&gt;
    +	 * 		remote filesystem&lt;br/&gt;
    +	 * @param appId&lt;br/&gt;
    +	 * 		application ID&lt;br/&gt;
    +	 * @param localRsrcPath&lt;br/&gt;
    +	 * 		path to the local file&lt;br/&gt;
    +	 * @param homedir&lt;br/&gt;
    +	 * 		remote home directory base (will be extended)&lt;br/&gt;
    +	 * @param relativeTargetPath&lt;br/&gt;
    +	 * 		relative target path of the file (will be prefixed be the full home directory we set up)&lt;br/&gt;
    +	 *&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@return Path to remote file (usually hdfs)&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* @throws IOException&lt;br/&gt;
     	 */&lt;/li&gt;
	&lt;li&gt;public static Path setupLocalResource(&lt;/li&gt;
	&lt;li&gt;FileSystem fs,&lt;/li&gt;
	&lt;li&gt;String appId, Path localRsrcPath,&lt;/li&gt;
	&lt;li&gt;LocalResource appMasterJar,&lt;/li&gt;
	&lt;li&gt;Path homedir) throws IOException {&lt;br/&gt;
    +	static Tuple2&amp;lt;Path, LocalResource&amp;gt; setupLocalResource(&lt;br/&gt;
    +		FileSystem fs,&lt;br/&gt;
    +		String appId,&lt;br/&gt;
    +		Path localRsrcPath,&lt;br/&gt;
    +		Path homedir,&lt;br/&gt;
    +		String relativeTargetPath) throws IOException {&lt;br/&gt;
    +&lt;br/&gt;
    +		if (new File(localRsrcPath.toUri().getPath()).isDirectory()) 
{
    +			throw new IllegalArgumentException(&quot;File to copy must not be a directory: &quot; +
    +				localRsrcPath);
    +		}&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     		// copy resource to HDFS&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;String suffix = &quot;.flink/&quot; + appId + &quot;/&quot; + localRsrcPath.getName();&lt;br/&gt;
    +		String suffix = &quot;.flink/&quot; + appId + &quot;/&quot; + relativeTargetPath + &quot;/&quot; + localRsrcPath.getName();
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    you could also check if the `relativeTargetPath` is empty. I would opt for the safe approach since I&apos;m not sure whether this behaviour is specific to a Hadoop version or not.&lt;/p&gt;</comment>
                            <comment id="16240330" author="githubbot" created="Mon, 6 Nov 2017 13:56:19 +0000"  >&lt;p&gt;Github user tillrohrmann commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939#discussion_r149084279&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939#discussion_r149084279&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-yarn/pom.xml &amp;#8212;&lt;br/&gt;
    @@ -99,6 +99,13 @@ under the License.&lt;br/&gt;
     		&amp;lt;/dependency&amp;gt;&lt;/p&gt;

&lt;p&gt;     		&amp;lt;dependency&amp;gt;&lt;br/&gt;
    +			&amp;lt;groupId&amp;gt;org.apache.flink&amp;lt;/groupId&amp;gt;&lt;br/&gt;
    +			&amp;lt;artifactId&amp;gt;flink-s3-fs-hadoop&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
    +			&amp;lt;version&amp;gt;${project.version}&amp;lt;/version&amp;gt;&lt;br/&gt;
    +			&amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;&lt;br/&gt;
    +		&amp;lt;/dependency&amp;gt;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I think it&apos;s also ok this way &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="16240749" author="githubbot" created="Mon, 6 Nov 2017 19:31:09 +0000"  >&lt;p&gt;Github user NicoK commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I addresses the PR comments in my latest commit but I will also have to adapt the S3 test to not use our S3 implementation and instead the one in the Hadoop version YARN uses. Stay tuned &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="16244254" author="githubbot" created="Wed, 8 Nov 2017 16:24:27 +0000"  >&lt;p&gt;Github user NicoK commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @tillrohrmann can you have a second look at the changes? (Travis already gave a green light on my branch with S3 tests enabled, the PR results will give the results without S3 tests due to missing credentials)&lt;/p&gt;</comment>
                            <comment id="16245767" author="githubbot" created="Thu, 9 Nov 2017 14:54:50 +0000"  >&lt;p&gt;Github user tillrohrmann commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939#discussion_r149980302&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939#discussion_r149980302&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-yarn/pom.xml &amp;#8212;&lt;br/&gt;
    @@ -153,6 +159,63 @@ under the License.&lt;br/&gt;
     				&amp;lt;/plugins&amp;gt;&lt;br/&gt;
     			&amp;lt;/build&amp;gt;&lt;br/&gt;
     		&amp;lt;/profile&amp;gt;&lt;br/&gt;
    +&lt;br/&gt;
    +		&amp;lt;profile&amp;gt;&lt;br/&gt;
    +			&amp;lt;!-- Hadoop &amp;gt;= 2.6 moved the S3 file systems from hadoop-common into hadoop-aws artifact&lt;br/&gt;
    +				(see &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-11074&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HADOOP-11074&lt;/a&gt;)&lt;br/&gt;
    +				We can add the (test) dependency per default once 2.6 is the minimum required version.&lt;br/&gt;
    +			--&amp;gt;&lt;br/&gt;
    +			&amp;lt;id&amp;gt;include_hadoop_aws&amp;lt;/id&amp;gt;&lt;br/&gt;
    +			&amp;lt;activation&amp;gt;&lt;br/&gt;
    +				&amp;lt;property&amp;gt;&lt;br/&gt;
    +					&amp;lt;name&amp;gt;include_hadoop_aws&amp;lt;/name&amp;gt;&lt;br/&gt;
    +				&amp;lt;/property&amp;gt;&lt;br/&gt;
    +			&amp;lt;/activation&amp;gt;&lt;br/&gt;
    +			&amp;lt;dependencies&amp;gt;&lt;br/&gt;
    +				&amp;lt;!-- for the S3 tests of YarnFileStageTestS3ITCase --&amp;gt;&lt;br/&gt;
    +				&amp;lt;dependency&amp;gt;&lt;br/&gt;
    +					&amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt;&lt;br/&gt;
    +					&amp;lt;artifactId&amp;gt;hadoop-aws&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
    +					&amp;lt;version&amp;gt;${hadoop.version}&amp;lt;/version&amp;gt;&lt;br/&gt;
    +					&amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;&lt;br/&gt;
    +					&amp;lt;exclusions&amp;gt;&lt;br/&gt;
    +						&amp;lt;exclusion&amp;gt;&lt;br/&gt;
    +							&amp;lt;groupId&amp;gt;org.apache.avro&amp;lt;/groupId&amp;gt;&lt;br/&gt;
    +							&amp;lt;artifactId&amp;gt;avro&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
    +						&amp;lt;/exclusion&amp;gt;&lt;br/&gt;
    +						&amp;lt;!-- The aws-java-sdk-core requires jackson 2.6, but&lt;br/&gt;
    +							hadoop pulls in 2.3 --&amp;gt;&lt;br/&gt;
    +						&amp;lt;exclusion&amp;gt;&lt;br/&gt;
    +							&amp;lt;groupId&amp;gt;com.fasterxml.jackson.core&amp;lt;/groupId&amp;gt;&lt;br/&gt;
    +							&amp;lt;artifactId&amp;gt;jackson-annotations&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
    +						&amp;lt;/exclusion&amp;gt;&lt;br/&gt;
    +						&amp;lt;exclusion&amp;gt;&lt;br/&gt;
    +							&amp;lt;groupId&amp;gt;com.fasterxml.jackson.core&amp;lt;/groupId&amp;gt;&lt;br/&gt;
    +							&amp;lt;artifactId&amp;gt;jackson-core&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
    +						&amp;lt;/exclusion&amp;gt;&lt;br/&gt;
    +						&amp;lt;exclusion&amp;gt;&lt;br/&gt;
    +							&amp;lt;groupId&amp;gt;com.fasterxml.jackson.core&amp;lt;/groupId&amp;gt;&lt;br/&gt;
    +							&amp;lt;artifactId&amp;gt;jackson-databind&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
    +						&amp;lt;/exclusion&amp;gt;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Can&apos;t we enforce jackson 2.6 via dependency management? I think this would be cleaner than excluding the dependencies here and assume that `aws-java-sdk-s3` pulls in the missing dependencies.&lt;/p&gt;</comment>
                            <comment id="16245768" author="githubbot" created="Thu, 9 Nov 2017 14:54:50 +0000"  >&lt;p&gt;Github user tillrohrmann commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939#discussion_r149983029&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939#discussion_r149983029&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-yarn/pom.xml &amp;#8212;&lt;br/&gt;
    @@ -153,6 +159,63 @@ under the License.&lt;br/&gt;
     				&amp;lt;/plugins&amp;gt;&lt;br/&gt;
     			&amp;lt;/build&amp;gt;&lt;br/&gt;
     		&amp;lt;/profile&amp;gt;&lt;br/&gt;
    +&lt;br/&gt;
    +		&amp;lt;profile&amp;gt;&lt;br/&gt;
    +			&amp;lt;!-- Hadoop &amp;gt;= 2.6 moved the S3 file systems from hadoop-common into hadoop-aws artifact&lt;br/&gt;
    +				(see &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-11074&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HADOOP-11074&lt;/a&gt;)&lt;br/&gt;
    +				We can add the (test) dependency per default once 2.6 is the minimum required version.&lt;br/&gt;
    +			--&amp;gt;&lt;br/&gt;
    +			&amp;lt;id&amp;gt;include_hadoop_aws&amp;lt;/id&amp;gt;&lt;br/&gt;
    +			&amp;lt;activation&amp;gt;&lt;br/&gt;
    +				&amp;lt;property&amp;gt;&lt;br/&gt;
    +					&amp;lt;name&amp;gt;include_hadoop_aws&amp;lt;/name&amp;gt;&lt;br/&gt;
    +				&amp;lt;/property&amp;gt;&lt;br/&gt;
    +			&amp;lt;/activation&amp;gt;&lt;br/&gt;
    +			&amp;lt;dependencies&amp;gt;&lt;br/&gt;
    +				&amp;lt;!-- for the S3 tests of YarnFileStageTestS3ITCase --&amp;gt;&lt;br/&gt;
    +				&amp;lt;dependency&amp;gt;&lt;br/&gt;
    +					&amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt;&lt;br/&gt;
    +					&amp;lt;artifactId&amp;gt;hadoop-aws&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
    +					&amp;lt;version&amp;gt;${hadoop.version}&amp;lt;/version&amp;gt;&lt;br/&gt;
    +					&amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;&lt;br/&gt;
    +					&amp;lt;exclusions&amp;gt;&lt;br/&gt;
    +						&amp;lt;exclusion&amp;gt;&lt;br/&gt;
    +							&amp;lt;groupId&amp;gt;org.apache.avro&amp;lt;/groupId&amp;gt;&lt;br/&gt;
    +							&amp;lt;artifactId&amp;gt;avro&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
    +						&amp;lt;/exclusion&amp;gt;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Shouldn&apos;t we override the avro version instead of excluding it?&lt;/p&gt;</comment>
                            <comment id="16245769" author="githubbot" created="Thu, 9 Nov 2017 14:54:50 +0000"  >&lt;p&gt;Github user tillrohrmann commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939#discussion_r149983025&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939#discussion_r149983025&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-yarn/pom.xml &amp;#8212;&lt;br/&gt;
    @@ -153,6 +159,63 @@ under the License.&lt;br/&gt;
     				&amp;lt;/plugins&amp;gt;&lt;br/&gt;
     			&amp;lt;/build&amp;gt;&lt;br/&gt;
     		&amp;lt;/profile&amp;gt;&lt;br/&gt;
    +&lt;br/&gt;
    +		&amp;lt;profile&amp;gt;&lt;br/&gt;
    +			&amp;lt;!-- Hadoop &amp;gt;= 2.6 moved the S3 file systems from hadoop-common into hadoop-aws artifact&lt;br/&gt;
    +				(see &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-11074&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HADOOP-11074&lt;/a&gt;)&lt;br/&gt;
    +				We can add the (test) dependency per default once 2.6 is the minimum required version.&lt;br/&gt;
    +			--&amp;gt;&lt;br/&gt;
    +			&amp;lt;id&amp;gt;include_hadoop_aws&amp;lt;/id&amp;gt;&lt;br/&gt;
    +			&amp;lt;activation&amp;gt;&lt;br/&gt;
    +				&amp;lt;property&amp;gt;&lt;br/&gt;
    +					&amp;lt;name&amp;gt;include_hadoop_aws&amp;lt;/name&amp;gt;&lt;br/&gt;
    +				&amp;lt;/property&amp;gt;&lt;br/&gt;
    +			&amp;lt;/activation&amp;gt;&lt;br/&gt;
    +			&amp;lt;dependencies&amp;gt;&lt;br/&gt;
    +				&amp;lt;!-- for the S3 tests of YarnFileStageTestS3ITCase --&amp;gt;&lt;br/&gt;
    +				&amp;lt;dependency&amp;gt;&lt;br/&gt;
    +					&amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt;&lt;br/&gt;
    +					&amp;lt;artifactId&amp;gt;hadoop-aws&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
    +					&amp;lt;version&amp;gt;${hadoop.version}&amp;lt;/version&amp;gt;&lt;br/&gt;
    +					&amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;&lt;br/&gt;
    +					&amp;lt;exclusions&amp;gt;&lt;br/&gt;
    +						&amp;lt;exclusion&amp;gt;&lt;br/&gt;
    +							&amp;lt;groupId&amp;gt;org.apache.avro&amp;lt;/groupId&amp;gt;&lt;br/&gt;
    +							&amp;lt;artifactId&amp;gt;avro&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
    +						&amp;lt;/exclusion&amp;gt;&lt;br/&gt;
    +						&amp;lt;!-- The aws-java-sdk-core requires jackson 2.6, but&lt;br/&gt;
    +							hadoop pulls in 2.3 --&amp;gt;&lt;br/&gt;
    +						&amp;lt;exclusion&amp;gt;&lt;br/&gt;
    +							&amp;lt;groupId&amp;gt;com.fasterxml.jackson.core&amp;lt;/groupId&amp;gt;&lt;br/&gt;
    +							&amp;lt;artifactId&amp;gt;jackson-annotations&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
    +						&amp;lt;/exclusion&amp;gt;&lt;br/&gt;
    +						&amp;lt;exclusion&amp;gt;&lt;br/&gt;
    +							&amp;lt;groupId&amp;gt;com.fasterxml.jackson.core&amp;lt;/groupId&amp;gt;&lt;br/&gt;
    +							&amp;lt;artifactId&amp;gt;jackson-core&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
    +						&amp;lt;/exclusion&amp;gt;&lt;br/&gt;
    +						&amp;lt;exclusion&amp;gt;&lt;br/&gt;
    +							&amp;lt;groupId&amp;gt;com.fasterxml.jackson.core&amp;lt;/groupId&amp;gt;&lt;br/&gt;
    +							&amp;lt;artifactId&amp;gt;jackson-databind&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
    +						&amp;lt;/exclusion&amp;gt;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    It might be necessary to add these dependencies then explicitly here.&lt;/p&gt;</comment>
                            <comment id="16247721" author="githubbot" created="Fri, 10 Nov 2017 16:12:57 +0000"  >&lt;p&gt;Github user NicoK commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    1) yes, I have tested the changes on a YARN cluster as described in the PR comment (before the changes on the unit tests)&lt;br/&gt;
    2) regarding the `pom.xml` changes: I actually took those from 991af3652479f85f732cbbade46bed7df1c5d819 and 36b663f4561458408ee68902a1db6a5cd539e1c2 and can only guess why @StephanEwen and @zentol used this way except for what you proposed. Maybe they can clarify.&lt;/p&gt;</comment>
                            <comment id="16247722" author="githubbot" created="Fri, 10 Nov 2017 16:14:12 +0000"  >&lt;p&gt;Github user NicoK commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939#discussion_r150275161&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939#discussion_r150275161&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-yarn/pom.xml &amp;#8212;&lt;br/&gt;
    @@ -153,6 +159,63 @@ under the License.&lt;br/&gt;
     				&amp;lt;/plugins&amp;gt;&lt;br/&gt;
     			&amp;lt;/build&amp;gt;&lt;br/&gt;
     		&amp;lt;/profile&amp;gt;&lt;br/&gt;
    +&lt;br/&gt;
    +		&amp;lt;profile&amp;gt;&lt;br/&gt;
    +			&amp;lt;!-- Hadoop &amp;gt;= 2.6 moved the S3 file systems from hadoop-common into hadoop-aws artifact&lt;br/&gt;
    +				(see &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-11074&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HADOOP-11074&lt;/a&gt;)&lt;br/&gt;
    +				We can add the (test) dependency per default once 2.6 is the minimum required version.&lt;br/&gt;
    +			--&amp;gt;&lt;br/&gt;
    +			&amp;lt;id&amp;gt;include_hadoop_aws&amp;lt;/id&amp;gt;&lt;br/&gt;
    +			&amp;lt;activation&amp;gt;&lt;br/&gt;
    +				&amp;lt;property&amp;gt;&lt;br/&gt;
    +					&amp;lt;name&amp;gt;include_hadoop_aws&amp;lt;/name&amp;gt;&lt;br/&gt;
    +				&amp;lt;/property&amp;gt;&lt;br/&gt;
    +			&amp;lt;/activation&amp;gt;&lt;br/&gt;
    +			&amp;lt;dependencies&amp;gt;&lt;br/&gt;
    +				&amp;lt;!-- for the S3 tests of YarnFileStageTestS3ITCase --&amp;gt;&lt;br/&gt;
    +				&amp;lt;dependency&amp;gt;&lt;br/&gt;
    +					&amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt;&lt;br/&gt;
    +					&amp;lt;artifactId&amp;gt;hadoop-aws&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
    +					&amp;lt;version&amp;gt;${hadoop.version}&amp;lt;/version&amp;gt;&lt;br/&gt;
    +					&amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;&lt;br/&gt;
    +					&amp;lt;exclusions&amp;gt;&lt;br/&gt;
    +						&amp;lt;exclusion&amp;gt;&lt;br/&gt;
    +							&amp;lt;groupId&amp;gt;org.apache.avro&amp;lt;/groupId&amp;gt;&lt;br/&gt;
    +							&amp;lt;artifactId&amp;gt;avro&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
    +						&amp;lt;/exclusion&amp;gt;&lt;br/&gt;
    +						&amp;lt;!-- The aws-java-sdk-core requires jackson 2.6, but&lt;br/&gt;
    +							hadoop pulls in 2.3 --&amp;gt;&lt;br/&gt;
    +						&amp;lt;exclusion&amp;gt;&lt;br/&gt;
    +							&amp;lt;groupId&amp;gt;com.fasterxml.jackson.core&amp;lt;/groupId&amp;gt;&lt;br/&gt;
    +							&amp;lt;artifactId&amp;gt;jackson-annotations&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
    +						&amp;lt;/exclusion&amp;gt;&lt;br/&gt;
    +						&amp;lt;exclusion&amp;gt;&lt;br/&gt;
    +							&amp;lt;groupId&amp;gt;com.fasterxml.jackson.core&amp;lt;/groupId&amp;gt;&lt;br/&gt;
    +							&amp;lt;artifactId&amp;gt;jackson-core&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
    +						&amp;lt;/exclusion&amp;gt;&lt;br/&gt;
    +						&amp;lt;exclusion&amp;gt;&lt;br/&gt;
    +							&amp;lt;groupId&amp;gt;com.fasterxml.jackson.core&amp;lt;/groupId&amp;gt;&lt;br/&gt;
    +							&amp;lt;artifactId&amp;gt;jackson-databind&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
    +						&amp;lt;/exclusion&amp;gt;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    this dependency is, however, somehow related to `com.amazonaws`, not our code...&lt;/p&gt;</comment>
                            <comment id="16247724" author="githubbot" created="Fri, 10 Nov 2017 16:15:08 +0000"  >&lt;p&gt;Github user NicoK commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939#discussion_r150275387&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939#discussion_r150275387&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-yarn/pom.xml &amp;#8212;&lt;br/&gt;
    @@ -153,6 +159,63 @@ under the License.&lt;br/&gt;
     				&amp;lt;/plugins&amp;gt;&lt;br/&gt;
     			&amp;lt;/build&amp;gt;&lt;br/&gt;
     		&amp;lt;/profile&amp;gt;&lt;br/&gt;
    +&lt;br/&gt;
    +		&amp;lt;profile&amp;gt;&lt;br/&gt;
    +			&amp;lt;!-- Hadoop &amp;gt;= 2.6 moved the S3 file systems from hadoop-common into hadoop-aws artifact&lt;br/&gt;
    +				(see &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-11074&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HADOOP-11074&lt;/a&gt;)&lt;br/&gt;
    +				We can add the (test) dependency per default once 2.6 is the minimum required version.&lt;br/&gt;
    +			--&amp;gt;&lt;br/&gt;
    +			&amp;lt;id&amp;gt;include_hadoop_aws&amp;lt;/id&amp;gt;&lt;br/&gt;
    +			&amp;lt;activation&amp;gt;&lt;br/&gt;
    +				&amp;lt;property&amp;gt;&lt;br/&gt;
    +					&amp;lt;name&amp;gt;include_hadoop_aws&amp;lt;/name&amp;gt;&lt;br/&gt;
    +				&amp;lt;/property&amp;gt;&lt;br/&gt;
    +			&amp;lt;/activation&amp;gt;&lt;br/&gt;
    +			&amp;lt;dependencies&amp;gt;&lt;br/&gt;
    +				&amp;lt;!-- for the S3 tests of YarnFileStageTestS3ITCase --&amp;gt;&lt;br/&gt;
    +				&amp;lt;dependency&amp;gt;&lt;br/&gt;
    +					&amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt;&lt;br/&gt;
    +					&amp;lt;artifactId&amp;gt;hadoop-aws&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
    +					&amp;lt;version&amp;gt;${hadoop.version}&amp;lt;/version&amp;gt;&lt;br/&gt;
    +					&amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;&lt;br/&gt;
    +					&amp;lt;exclusions&amp;gt;&lt;br/&gt;
    +						&amp;lt;exclusion&amp;gt;&lt;br/&gt;
    +							&amp;lt;groupId&amp;gt;org.apache.avro&amp;lt;/groupId&amp;gt;&lt;br/&gt;
    +							&amp;lt;artifactId&amp;gt;avro&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
    +						&amp;lt;/exclusion&amp;gt;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    makes sense, I guess&lt;/p&gt;</comment>
                            <comment id="16249213" author="githubbot" created="Mon, 13 Nov 2017 08:00:51 +0000"  >&lt;p&gt;Github user tillrohrmann commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I think excluding dependencies without declaring them explicitly has the disadvantage that this relies implicitly on another dependency to pull in the right versions. Once this dependency is removed or its version bumped the whole build might fail because it no longer pulls in the dependency.&lt;/p&gt;</comment>
                            <comment id="16249342" author="githubbot" created="Mon, 13 Nov 2017 10:19:23 +0000"  >&lt;p&gt;Github user zentol commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    At the same time, pinning a dependency version can cause problems if a dependency is upgraded and requires another jackson version.&lt;/p&gt;

&lt;p&gt;    There isn&apos;t a perfect solution. The current approach works as long as the aws dependency pulls in jackson, which is less likely to change compared to it bumping the version used.&lt;/p&gt;</comment>
                            <comment id="16249659" author="githubbot" created="Mon, 13 Nov 2017 14:27:02 +0000"  >&lt;p&gt;Github user tillrohrmann commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    This is true, however, I think that in case of doubt the more explicit solution should be preferred, because it is more predictable.&lt;/p&gt;</comment>
                            <comment id="16251029" author="githubbot" created="Tue, 14 Nov 2017 07:28:16 +0000"  >&lt;p&gt;Github user zentol commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    So we pin the version, update aws at some point (and forget to check the dependency), and now we got a sleeper failure because there may be a single code-path that fails due to a version mismatch.&lt;/p&gt;

&lt;p&gt;    On the other hand, a missing dependency fails on every single code path that uses it.&lt;/p&gt;</comment>
                            <comment id="16251097" author="githubbot" created="Tue, 14 Nov 2017 08:40:12 +0000"  >&lt;p&gt;Github user tillrohrmann commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Shouldn&apos;t we already have the problem of the sleeper failure by excluding Hadoop&apos;s dependency?&lt;/p&gt;

&lt;p&gt;    Agreed that a dependency management entry would require comments to explain it, but so does the exclusion, too.&lt;/p&gt;</comment>
                            <comment id="16251152" author="githubbot" created="Tue, 14 Nov 2017 09:41:02 +0000"  >&lt;p&gt;Github user zentol commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    yes there could be a sleeper failure in hadoop, but there&apos;s nothing we can do to fix that besides relocation jackson in either hadoop or aws, which is probably not possible.&lt;/p&gt;</comment>
                            <comment id="16253353" author="githubbot" created="Wed, 15 Nov 2017 12:18:18 +0000"  >&lt;p&gt;Github user zentol commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939#discussion_r151110083&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939#discussion_r151110083&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-yarn/pom.xml &amp;#8212;&lt;br/&gt;
    @@ -153,6 +159,63 @@ under the License.&lt;br/&gt;
     				&amp;lt;/plugins&amp;gt;&lt;br/&gt;
     			&amp;lt;/build&amp;gt;&lt;br/&gt;
     		&amp;lt;/profile&amp;gt;&lt;br/&gt;
    +&lt;br/&gt;
    +		&amp;lt;profile&amp;gt;&lt;br/&gt;
    +			&amp;lt;!-- Hadoop &amp;gt;= 2.6 moved the S3 file systems from hadoop-common into hadoop-aws artifact&lt;br/&gt;
    +				(see &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-11074&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HADOOP-11074&lt;/a&gt;)&lt;br/&gt;
    +				We can add the (test) dependency per default once 2.6 is the minimum required version.&lt;br/&gt;
    +			--&amp;gt;&lt;br/&gt;
    +			&amp;lt;id&amp;gt;include_hadoop_aws&amp;lt;/id&amp;gt;&lt;br/&gt;
    +			&amp;lt;activation&amp;gt;&lt;br/&gt;
    +				&amp;lt;property&amp;gt;&lt;br/&gt;
    +					&amp;lt;name&amp;gt;include_hadoop_aws&amp;lt;/name&amp;gt;&lt;br/&gt;
    +				&amp;lt;/property&amp;gt;&lt;br/&gt;
    +			&amp;lt;/activation&amp;gt;&lt;br/&gt;
    +			&amp;lt;dependencies&amp;gt;&lt;br/&gt;
    +				&amp;lt;!-- for the S3 tests of YarnFileStageTestS3ITCase --&amp;gt;&lt;br/&gt;
    +				&amp;lt;dependency&amp;gt;&lt;br/&gt;
    +					&amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt;&lt;br/&gt;
    +					&amp;lt;artifactId&amp;gt;hadoop-aws&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
    +					&amp;lt;version&amp;gt;${hadoop.version}&amp;lt;/version&amp;gt;&lt;br/&gt;
    +					&amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;&lt;br/&gt;
    +					&amp;lt;exclusions&amp;gt;&lt;br/&gt;
    +						&amp;lt;exclusion&amp;gt;&lt;br/&gt;
    +							&amp;lt;groupId&amp;gt;org.apache.avro&amp;lt;/groupId&amp;gt;&lt;br/&gt;
    +							&amp;lt;artifactId&amp;gt;avro&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
    +						&amp;lt;/exclusion&amp;gt;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    IIRC avro was excluded since it was just not necessary for the file-system, in which case it should stay.&lt;/p&gt;</comment>
                            <comment id="16256656" author="githubbot" created="Fri, 17 Nov 2017 08:38:42 +0000"  >&lt;p&gt;Github user NicoK commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I&apos;m actually with Chesnay on the part of using exclusions (and letting AWS pull them in with the right version) rather than adding explicit dependencies because it will be easier to debug than an error due to a version conflict.&lt;br/&gt;
    We can immediately spot those `ClassNotFoundException` instances in the failing unit tests (recall that those dependencies are all in `test` scope!) while as a version conflict may show up more subtle and be less easy to spot.&lt;/p&gt;</comment>
                            <comment id="16257100" author="githubbot" created="Fri, 17 Nov 2017 15:10:24 +0000"  >&lt;p&gt;Github user tillrohrmann commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Alright, then we&apos;ll do it as you&apos;ve proposed. I&apos;ll merge this PR. Thanks for your work @NicoK and the review @zentol and @StephanEwen.&lt;/p&gt;</comment>
                            <comment id="16258096" author="githubbot" created="Sat, 18 Nov 2017 14:35:51 +0000"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4939&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4939&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16258097" author="till.rohrmann" created="Sat, 18 Nov 2017 14:36:07 +0000"  >&lt;p&gt;Fixed&lt;br/&gt;
1.4.0: 36b807567d93c431c1498241a42c20221cb6a664&lt;br/&gt;
1.5.0: cf8504dba606ee758ac16867423e65dbf6afc23a&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i39aq3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>