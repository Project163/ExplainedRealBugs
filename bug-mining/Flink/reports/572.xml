<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 20:20:57 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-2617] ConcurrentModificationException when using HCatRecordReader to access a hive table</title>
                <link>https://issues.apache.org/jira/browse/FLINK-2617</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;p&gt;I don&apos;t know if it&apos;s a hcat or a flink problem, but when reading a hive table in a cluster with many slots (20 threads per container), I systematically run into a &lt;tt&gt;ConcurrentModificationException&lt;/tt&gt; in a copy method of a &lt;tt&gt;Configuration&lt;/tt&gt; object that change during the copy.&lt;/p&gt;

&lt;p&gt;From what I understand, this object comes from &lt;tt&gt;TaskAttemptContext.getConfiguration()&lt;/tt&gt; created by &lt;tt&gt;HadoopUtils.instantiateTaskAttemptContext(configuration, new TaskAttemptID());&lt;/tt&gt; &lt;/p&gt;

&lt;p&gt;Maybe the &lt;tt&gt;job.Configuration&lt;/tt&gt; object passed to the constructor of &lt;tt&gt;HadoopInputFormatBase&lt;/tt&gt; should be cloned somewhere?&lt;/p&gt;

&lt;p&gt;Stack trace is :&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;org.apache.flink.client.program.ProgramInvocationException: The program execution failed: Job execution failed.
org.apache.flink.client.program.Client.run(Client.java:413)
org.apache.flink.client.program.Client.run(Client.java:356)
org.apache.flink.client.program.Client.run(Client.java:349)
org.apache.flink.client.program.ContextEnvironment.execute(ContextEnvironment.java:63)
com.bouygtel.kuberasdk.main.ApplicationBatch.execCluster(ApplicationBatch.java:73)
com.bouygtel.kubera.main.segment.ApplicationGeoSegment.batchExec(ApplicationGeoSegment.java:69)
com.bouygtel.kuberasdk.main.ApplicationBatch.exec(ApplicationBatch.java:50)
com.bouygtel.kuberasdk.main.Application.mainMethod(Application.java:88)
com.bouygtel.kubera.main.segment.MainGeoSmooth.main(MainGeoSmooth.java:44)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:606)
org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:437)
org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:353)
org.apache.flink.client.program.Client.run(Client.java:315)
org.apache.flink.client.CliFrontend.executeProgram(CliFrontend.java:582)
org.apache.flink.client.CliFrontend.run(CliFrontend.java:288)
org.apache.flink.client.CliFrontend.parseParameters(CliFrontend.java:878)
org.apache.flink.client.CliFrontend.main(CliFrontend.java:920)

Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
org.apache.flink.runtime.jobmanager.JobManager$$anonfun$receiveWithLogMessages$1.applyOrElse(JobManager.scala:314)
scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33)
scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33)
scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25)
org.apache.flink.yarn.ApplicationMasterActor$$anonfun$receiveYarnMessages$1.applyOrElse(ApplicationMasterActor.scala:100)
scala.PartialFunction$OrElse.apply(PartialFunction.scala:162)
org.apache.flink.runtime.ActorLogMessages$$anon$1.apply(ActorLogMessages.scala:36)
org.apache.flink.runtime.ActorLogMessages$$anon$1.apply(ActorLogMessages.scala:29)
scala.PartialFunction$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;applyOrElse(PartialFunction.scala:118)
org.apache.flink.runtime.ActorLogMessages$$anon$1.applyOrElse(ActorLogMessages.scala:29)
akka.actor.Actor$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;aroundReceive(Actor.scala:465)
org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:92)
akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
akka.actor.ActorCell.invoke(ActorCell.scala:487)
akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254)
akka.dispatch.Mailbox.run(Mailbox.scala:221)
akka.dispatch.Mailbox.exec(Mailbox.scala:231)
scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

Caused by:  java.util.ConcurrentModificationException
java.util.HashMap$HashIterator.nextEntry(HashMap.java:926)
java.util.HashMap$KeyIterator.next(HashMap.java:960)
java.util.AbstractCollection.addAll(AbstractCollection.java:341)
java.util.HashSet.&amp;lt;init&amp;gt;(HashSet.java:117)
org.apache.hadoop.conf.Configuration.&amp;lt;init&amp;gt;(Configuration.java:554)
org.apache.hadoop.mapred.JobConf.&amp;lt;init&amp;gt;(JobConf.java:439)
org.apache.hive.hcatalog.common.HCatUtil.getJobConfFromContext(HCatUtil.java:637)
org.apache.hive.hcatalog.mapreduce.HCatRecordReader.createBaseRecordReader(HCatRecordReader.java:112)
org.apache.hive.hcatalog.mapreduce.HCatRecordReader.initialize(HCatRecordReader.java:91)
org.apache.flink.api.java.hadoop.mapreduce.HadoopInputFormatBase.open(HadoopInputFormatBase.java:182)
org.apache.flink.api.java.hadoop.mapreduce.HadoopInputFormatBase.open(HadoopInputFormatBase.java:56)
org.apache.flink.runtime.operators.DataSourceTask.invoke(DataSourceTask.java:151)
org.apache.flink.runtime.taskmanager.Task.run(Task.java:559)
java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:744)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Flink &quot;user&quot; code looks like:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.io.IOException;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.io.Serializable;

&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.flink.api.common.functions.MapFunction;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.flink.api.common.functions.RichFlatMapFunction;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.flink.api.common.functions.RichMapFunction;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.flink.api.common.io.FileOutputFormat;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.flink.api.java.DataSet;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.flink.api.java.ExecutionEnvironment;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.flink.api.java.hadoop.mapreduce.HadoopInputFormat;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.flink.api.java.tuple.Tuple2;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.flink.configuration.Configuration;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.flink.core.fs.FileSystem.WriteMode;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.flink.streaming.api.datastream.DataStream;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.flink.streaming.api.functions.sink.SinkFunction;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.flink.util.Collector;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.hadoop.io.NullWritable;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.hadoop.io.compress.CompressionCodec;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.hadoop.mapreduce.InputFormat;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.hadoop.mapreduce.Job;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.hive.hcatalog.data.DefaultHCatRecord;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.hive.hcatalog.data.schema.HCatSchema;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.hive.hcatalog.mapreduce.HCatInputFormat;


(...) 
        &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Job job = Job.getInstance();
        @SuppressWarnings({ &lt;span class=&quot;code-quote&quot;&gt;&quot;unchecked&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;rawtypes&quot;&lt;/span&gt; })
        &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; HadoopInputFormat&amp;lt;NullWritable, DefaultHCatRecord&amp;gt; inputFormat = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; HadoopInputFormat&amp;lt;NullWritable, 
        DefaultHCatRecord&amp;gt;(
            (InputFormat) HCatInputFormat.setInput(job, dbName, tableName, filter), &lt;span class=&quot;code-comment&quot;&gt;//
&lt;/span&gt;            NullWritable.class, &lt;span class=&quot;code-comment&quot;&gt;//
&lt;/span&gt;            DefaultHCatRecord.class, &lt;span class=&quot;code-comment&quot;&gt;//
&lt;/span&gt;            job);

        &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; HCatSchema inputSchema = HCatInputFormat.getTableSchema(job.getConfiguration());
        @SuppressWarnings(&lt;span class=&quot;code-quote&quot;&gt;&quot;serial&quot;&lt;/span&gt;)
        &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; DataSet&amp;lt;T&amp;gt; dataSet = cluster
            .createInput(inputFormat)
            .flatMap(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; FlatMapFunction&amp;lt;Tuple2&amp;lt;NullWritable, DefaultHCatRecord&amp;gt;, T&amp;gt;() {
                @Override
                &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void flatMap(Tuple2&amp;lt;NullWritable, DefaultHCatRecord&amp;gt; value, Collector&amp;lt;T&amp;gt; out) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; Exception { &lt;span class=&quot;code-comment&quot;&gt;// NOPMD
&lt;/span&gt;                    &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; T record = createBean(value.f1, inputSchema);
                    out.collect(record);
                }
            }).returns(beanClass);
(...)            
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
</description>
                <environment></environment>
        <key id="12861561">FLINK-2617</key>
            <summary>ConcurrentModificationException when using HCatRecordReader to access a hive table</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="fhueske">Fabian Hueske</assignee>
                                    <reporter username="ArnaudL">Arnaud Linz</reporter>
                        <labels>
                    </labels>
                <created>Thu, 3 Sep 2015 13:11:24 +0000</created>
                <updated>Thu, 16 Jun 2016 17:07:54 +0000</updated>
                            <resolved>Thu, 10 Sep 2015 12:36:09 +0000</resolved>
                                                    <fixVersion>0.9</fixVersion>
                    <fixVersion>0.10.0</fixVersion>
                                    <component>API / DataSet</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="14729289" author="arnaudl" created="Thu, 3 Sep 2015 15:44:48 +0000"  >&lt;p&gt;(I think that the subject is in fact critical because it happens really often in my cluster as soon as parallelism is &amp;gt; 250)&lt;/p&gt;</comment>
                            <comment id="14729302" author="stephanewen" created="Thu, 3 Sep 2015 15:55:38 +0000"  >&lt;p&gt;I agree, this is critical.&lt;/p&gt;

&lt;p&gt;The reason for this exception is that the Hadoop code assumes only one thread is ever working with the configuration. That is the MapReduce model where each task is in a dedicated JVM.&lt;br/&gt;
In Flink, multiple tasks can run in one JVM, which goes a bit against Hadoop&apos;s assumptions.&lt;/p&gt;

&lt;p&gt;We need to lock the &lt;tt&gt;open()&lt;/tt&gt; method of all MapReduce compatibility functions (on a static object) in order to prevent this.&lt;/p&gt;

&lt;p&gt;A short term mitigation may be to change your setup to run more TaskManagers, each with one slot - that way, you have one task per JVM as well.&lt;/p&gt;</comment>
                            <comment id="14730694" author="fhueske" created="Fri, 4 Sep 2015 12:08:07 +0000"  >&lt;p&gt;Yes, that could be a fix.&lt;br/&gt;
But I think it would be good to know, why that happens because it might bite us at other places as well. Maybe we need to guard all calls to Hadoop classes by locks.&lt;br/&gt;
I looked into the code and it is not obvious. All HadoopInputFormats (Flink&apos;s wrapper) are configured with different Configuration objects. &lt;/p&gt;</comment>
                            <comment id="14735098" author="rmetzger" created="Tue, 8 Sep 2015 16:32:09 +0000"  >&lt;p&gt;Which version of HCatalog are you using?&lt;br/&gt;
(I&apos;m asking because my argument list of the  HCatInputFormat.setInput() differs from yours)&lt;/p&gt;</comment>
                            <comment id="14736262" author="arnaudl" created="Wed, 9 Sep 2015 06:13:30 +0000"  >&lt;p&gt;hive-hcatalog-core-0.14.0.jar&lt;/p&gt;</comment>
                            <comment id="14736792" author="githubbot" created="Wed, 9 Sep 2015 12:54:58 +0000"  >&lt;p&gt;GitHub user fhueske opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1111&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1111&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-2617&quot; title=&quot;ConcurrentModificationException when using HCatRecordReader to access a hive table&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-2617&quot;&gt;&lt;del&gt;FLINK-2617&lt;/del&gt;&lt;/a&gt; Added static mutexes for configure, open, close HadoopIOFormats&lt;/p&gt;



&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/fhueske/flink&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/fhueske/flink&lt;/a&gt; hadoopConcurrent&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1111.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1111.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #1111&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit ff8fe027de2cac3c28fe5d81bd5351af3fcefb28&lt;br/&gt;
Author: Fabian Hueske &amp;lt;fhueske@apache.org&amp;gt;&lt;br/&gt;
Date:   2015-09-09T12:32:21Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-2617&quot; title=&quot;ConcurrentModificationException when using HCatRecordReader to access a hive table&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-2617&quot;&gt;&lt;del&gt;FLINK-2617&lt;/del&gt;&lt;/a&gt; Added static mutexes for configure, open, close HadoopFormats&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="14736800" author="fhueske" created="Wed, 9 Sep 2015 13:00:51 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ArnaudL&quot; class=&quot;user-hover&quot; rel=&quot;ArnaudL&quot;&gt;ArnaudL&lt;/a&gt;, I opened a PR against the 0.10 SNAPSHOT master that synchronizes open() calls in Flink&apos;s &lt;tt&gt;HadoopInputFormat&lt;/tt&gt;.&lt;br/&gt;
Can you check if that solves the problem with the HCatInputFormat? I can also port the fix to 0.9 if necessary.&lt;/p&gt;

&lt;p&gt;Btw. I did not find the reason for the exception, but I verified that Flink is separate config objects in its code. So I assume, the root cause is somewhere hidden in the HCat code.&lt;/p&gt;</comment>
                            <comment id="14736937" author="githubbot" created="Wed, 9 Sep 2015 14:32:55 +0000"  >&lt;p&gt;Github user rmetzger commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1111#issuecomment-138928137&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1111#issuecomment-138928137&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    +1 Looks good to merge&lt;/p&gt;</comment>
                            <comment id="14738522" author="githubbot" created="Thu, 10 Sep 2015 09:54:20 +0000"  >&lt;p&gt;Github user fhueske commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1111#issuecomment-139189798&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1111#issuecomment-139189798&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    will merge this&lt;/p&gt;</comment>
                            <comment id="14738652" author="githubbot" created="Thu, 10 Sep 2015 12:20:06 +0000"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1111&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1111&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14738669" author="fhueske" created="Thu, 10 Sep 2015 12:36:09 +0000"  >&lt;p&gt;Fixed for 0.10 with 8754352ff53cd1ab621d6c97f7e5baac369b5c28&lt;br/&gt;
Fixed for 0.9 with d656fc33a28dbcc286723b1f7413e297b1407add&lt;/p&gt;</comment>
                            <comment id="14740386" author="arnaudl" created="Fri, 11 Sep 2015 08:20:02 +0000"  >&lt;p&gt;Hi, today (11-09)  I&apos;ve tested the 0.10 nightly build with success, thank you very much.&lt;br/&gt;
However, I did not find the patch in the 0.9 snapshot ; is it normal?&lt;/p&gt;</comment>
                            <comment id="14740401" author="fhueske" created="Fri, 11 Sep 2015 08:36:04 +0000"  >&lt;p&gt;Hi Arnaud,&lt;/p&gt;

&lt;p&gt;Thanks for verifying the fix! Where did you look for the 0.9 patch? &lt;br/&gt;
We are pushing fixes for minor version 0.9 to the &lt;a href=&quot;https://github.com/apache/flink/tree/release-0.9&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;release-0.9&lt;/a&gt; branch and the patch for this issue is in.&lt;/p&gt;</comment>
                            <comment id="14740440" author="arnaudl" created="Fri, 11 Sep 2015 09:01:01 +0000"  >&lt;p&gt;Well, I&apos;ve searched apache snapshot repository for 0.9-SNAPSHOT and I&apos;ve looked at &lt;a href=&quot;http://stratosphere-bin.s3.amazonaws.com/flink-0.9-SNAPSHOT-bin-hadoop2.tgz&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://stratosphere-bin.s3.amazonaws.com/flink-0.9-SNAPSHOT-bin-hadoop2.tgz&lt;/a&gt;. I was trying to avoid building it from the sources until I can get the time to configure my env.&lt;/p&gt;</comment>
                            <comment id="14740495" author="fhueske" created="Fri, 11 Sep 2015 09:46:43 +0000"  >&lt;p&gt;I see. &lt;br/&gt;
SNAPSHOT builds are only uploaded if all tests pass. The last build failed due to some flaky tests. &lt;br/&gt;
I will push an empty commit to trigger another build. &lt;b&gt;fingersCrossed&lt;/b&gt; that this build passes.&lt;/p&gt;</comment>
                            <comment id="14740890" author="fhueske" created="Fri, 11 Sep 2015 14:20:44 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ArnaudL&quot; class=&quot;user-hover&quot; rel=&quot;ArnaudL&quot;&gt;ArnaudL&lt;/a&gt;, a new 0.9-SNAPSHOT build was just uploaded to S3.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 10 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2jqu7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>