<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 20:25:19 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-4035] Add Apache Kafka 0.10 connector</title>
                <link>https://issues.apache.org/jira/browse/FLINK-4035</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;p&gt;Kafka 0.10.0.0 introduced protocol changes related to the producer.  Published messages now include timestamps and compressed messages now include relative offsets.  As it is now, brokers must decompress publisher compressed messages, assign offset to them, and recompress them, which is wasteful and makes it less likely that compression will be used at all.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12977013">FLINK-4035</key>
            <summary>Add Apache Kafka 0.10 connector</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Minor</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="rmetzger">Robert Metzger</assignee>
                                    <reporter username="elevy">Elias Levy</reporter>
                        <labels>
                    </labels>
                <created>Wed, 8 Jun 2016 21:01:38 +0000</created>
                <updated>Tue, 11 Oct 2016 08:14:34 +0000</updated>
                            <resolved>Tue, 11 Oct 2016 08:06:09 +0000</resolved>
                                    <version>1.0.3</version>
                                    <fixVersion>1.2.0</fixVersion>
                                    <component>Connectors / Kafka</component>
                        <due></due>
                            <votes>4</votes>
                                    <watches>12</watches>
                                                                                                                <comments>
                            <comment id="15322417" author="aljoscha" created="Thu, 9 Jun 2016 12:23:19 +0000"  >&lt;p&gt;We&apos;ll probably need a new package for Kafka 0.10 consumers/producers, right &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rmetzger&quot; class=&quot;user-hover&quot; rel=&quot;rmetzger&quot;&gt;rmetzger&lt;/a&gt;. Similar to how we have it now for 0.8 and 0.9.&lt;/p&gt;</comment>
                            <comment id="15322421" author="rmetzger" created="Thu, 9 Jun 2016 12:29:24 +0000"  >&lt;p&gt;I haven&apos;t looked into the Kafka 0.10 changes in detail.&lt;/p&gt;

&lt;p&gt;If the API didn&apos;t change between 9 and 10, users can probably manually force the Kafka 0.9 connector to use the 0.10 code (by putting the kafka 0.10 client into their pom).&lt;/p&gt;</comment>
                            <comment id="15322741" author="elevy" created="Thu, 9 Jun 2016 15:49:38 +0000"  >&lt;p&gt;AFAIK there haven&apos;t been changes to the producer API in 0.10, just protocol changes.  I&apos;ll try dropping the 0.10 jars into my job to give it a try and report back.&lt;/p&gt;</comment>
                            <comment id="15373533" author="githubbot" created="Tue, 12 Jul 2016 19:28:18 +0000"  >&lt;p&gt;GitHub user radekg opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2231&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2231&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-4035&quot; title=&quot;Add Apache Kafka 0.10 connector&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-4035&quot;&gt;&lt;del&gt;FLINK-4035&lt;/del&gt;&lt;/a&gt; Bump Kafka producer in Kafka sink to Kafka 0.10.0.0&lt;/p&gt;

&lt;p&gt;    Hi everyone,&lt;/p&gt;

&lt;p&gt;    At The Weather Company we bumped into a problem while trying to use Flink with Kafka 0.10.x. This PR introduces the support for `FlinkKafkaConsumer010` and `FlinkKafkaProducer010`. Unit test coverage is provided and `mvn clean verify` passes.&lt;/p&gt;

&lt;p&gt;    The output is below:&lt;/p&gt;

&lt;p&gt;    ```&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; ------------------------------------------------------------------------&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Reactor Summary:&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; force-shading ...................................... SUCCESS [  1.311 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink .............................................. SUCCESS [  2.939 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-annotations .................................. SUCCESS [  1.476 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-shaded-hadoop ................................ SUCCESS [  0.152 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-shaded-hadoop2 ............................... SUCCESS [  7.065 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-shaded-include-yarn-tests .................... SUCCESS [  8.543 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-shaded-curator ............................... SUCCESS [  0.112 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-shaded-curator-recipes ....................... SUCCESS [  1.080 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-shaded-curator-test .......................... SUCCESS [  0.210 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-test-utils-parent ............................ SUCCESS [  0.126 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-test-utils-junit ............................. SUCCESS [  2.019 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-core ......................................... SUCCESS [ 34.501 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-java ......................................... SUCCESS [ 26.266 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-runtime ...................................... SUCCESS &lt;span class=&quot;error&quot;&gt;&amp;#91;04:57 min&amp;#93;&lt;/span&gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-optimizer .................................... SUCCESS [  7.914 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-clients ...................................... SUCCESS [  6.537 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-streaming-java ............................... SUCCESS [ 37.732 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-test-utils ................................... SUCCESS [  6.166 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-scala ........................................ SUCCESS [ 24.626 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-runtime-web .................................. SUCCESS [ 12.831 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-examples ..................................... SUCCESS [  1.123 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-examples-batch ............................... SUCCESS [ 11.919 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-contrib ...................................... SUCCESS [  0.096 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-statebackend-rocksdb ......................... SUCCESS [  7.770 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-tests ........................................ SUCCESS &lt;span class=&quot;error&quot;&gt;&amp;#91;06:22 min&amp;#93;&lt;/span&gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-streaming-scala .............................. SUCCESS [ 26.831 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-streaming-connectors ......................... SUCCESS [  0.100 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-connector-flume .............................. SUCCESS [  2.425 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-libraries .................................... SUCCESS [  0.084 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-table ........................................ SUCCESS &lt;span class=&quot;error&quot;&gt;&amp;#91;02:02 min&amp;#93;&lt;/span&gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-connector-kafka-base ......................... SUCCESS [  4.604 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-connector-kafka-0.8 .......................... SUCCESS &lt;span class=&quot;error&quot;&gt;&amp;#91;02:01 min&amp;#93;&lt;/span&gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-connector-kafka-0.9 .......................... SUCCESS &lt;span class=&quot;error&quot;&gt;&amp;#91;02:38 min&amp;#93;&lt;/span&gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-connector-kafka-0.10 ......................... SUCCESS &lt;span class=&quot;error&quot;&gt;&amp;#91;02:04 min&amp;#93;&lt;/span&gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-connector-elasticsearch ...................... SUCCESS [ 19.310 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-connector-elasticsearch2 ..................... SUCCESS [ 17.086 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-connector-rabbitmq ........................... SUCCESS [  2.885 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-connector-twitter ............................ SUCCESS [  2.649 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-connector-nifi ............................... SUCCESS [  1.339 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-connector-cassandra .......................... SUCCESS &lt;span class=&quot;error&quot;&gt;&amp;#91;01:21 min&amp;#93;&lt;/span&gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-connector-redis .............................. SUCCESS [  5.738 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-connector-filesystem ......................... SUCCESS [ 24.871 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-batch-connectors ............................. SUCCESS [  0.103 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-avro ......................................... SUCCESS [  9.788 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-jdbc ......................................... SUCCESS [  4.839 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-hadoop-compatibility ......................... SUCCESS [ 10.026 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-hbase ........................................ SUCCESS [  2.938 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-hcatalog ..................................... SUCCESS [  5.383 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-examples-streaming ........................... SUCCESS [ 24.339 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-gelly ........................................ SUCCESS [ 40.416 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-gelly-scala .................................. SUCCESS [ 29.050 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-gelly-examples ............................... SUCCESS [ 21.418 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-python ....................................... SUCCESS [ 58.016 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-ml ........................................... SUCCESS [ 57.270 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-cep .......................................... SUCCESS [  6.984 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-cep-scala .................................... SUCCESS [  8.521 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-scala-shell .................................. SUCCESS &lt;span class=&quot;error&quot;&gt;&amp;#91;03:28 min&amp;#93;&lt;/span&gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-quickstart ................................... SUCCESS [  1.248 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-quickstart-java .............................. SUCCESS [  0.610 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-quickstart-scala ............................. SUCCESS [  0.237 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-storm ........................................ SUCCESS [ 14.975 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-storm-examples ............................... SUCCESS [ 37.513 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-streaming-contrib ............................ SUCCESS [  8.452 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-tweet-inputformat ............................ SUCCESS [  3.075 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-operator-stats ............................... SUCCESS [  6.521 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-connector-wikiedits .......................... SUCCESS [ 18.022 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-yarn ......................................... SUCCESS [  7.539 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-dist ......................................... SUCCESS [ 11.453 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-metrics ...................................... SUCCESS [  0.101 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-metrics-dropwizard ........................... SUCCESS [  2.699 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-metrics-ganglia .............................. SUCCESS [  1.320 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-metrics-graphite ............................. SUCCESS [  1.188 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-metrics-statsd ............................... SUCCESS [  2.271 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-fs-tests ..................................... SUCCESS [ 27.916 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-java8 ........................................ SUCCESS [ 12.209 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; ------------------------------------------------------------------------&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; BUILD SUCCESS&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; ------------------------------------------------------------------------&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Total time: 37:24 min&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Finished at: 2016-07-12T15:18:39-04:00&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Final Memory: 234M/1833M&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; ------------------------------------------------------------------------&lt;br/&gt;
    ```&lt;/p&gt;

&lt;p&gt;    The only thing not provided right in this moment in time, is the documentation updates. Not sure how to take on that one, some guidance would be appreciated.&lt;/p&gt;

&lt;p&gt;    What would be the best way to proceed with the contribution?&lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/TheWeatherCompany/flink&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/TheWeatherCompany/flink&lt;/a&gt; kafka-0.10&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2231.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2231.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #2231&lt;/p&gt;

&lt;hr /&gt;

&lt;hr /&gt;</comment>
                            <comment id="15373536" author="radekg" created="Tue, 12 Jul 2016 19:30:19 +0000"  >&lt;p&gt;Hi everyone,&lt;/p&gt;

&lt;p&gt;I&apos;m the author of the above PR. Happy to hear what else would be necessary to make this contribution go into the main repo.&lt;/p&gt;</comment>
                            <comment id="15375102" author="radekg" created="Wed, 13 Jul 2016 14:28:50 +0000"  >&lt;p&gt;The tests are passing.&lt;/p&gt;</comment>
                            <comment id="15376410" author="githubbot" created="Thu, 14 Jul 2016 06:34:23 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2231&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2231&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Hi @radekg , thank you for opening a PR for this!&lt;br/&gt;
    From a first look it seems that there isn&apos;t much changes to the code of `flink-connector-kafka-0.9` and this PR. Also, from the original discussion / comments in the JIRA, the Kafka API doesn&apos;t seem to have changed between 0.9 and 0.10, so it might be possible to let the Kafka 0.9 connector use the 0.10 client by putting the Kafka 0.10 dependency into the user pom.&lt;/p&gt;

&lt;p&gt;    May I ask whether you have tried this approach out already? Also,&lt;br/&gt;
    &amp;gt; At The Weather Company we bumped into a problem while trying to use Flink with Kafka 0.10.x.&lt;br/&gt;
    What was the problem? If you can describe, it&apos;ll be helpful for deciding how we can proceed with this &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; There&apos;s another contributor who was trying this out, I&apos;ll also try to ask for his feedback on this in the JIRA.&lt;/p&gt;</comment>
                            <comment id="15376417" author="tzulitai" created="Thu, 14 Jul 2016 06:40:02 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=elevy&quot; class=&quot;user-hover&quot; rel=&quot;elevy&quot;&gt;elevy&lt;/a&gt;, do you have any feedback on the approach of forcing the 0.9 connector to use 0.10 jars? It&apos;ll be helpful to see how we should proceed with the PR &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15377036" author="githubbot" created="Thu, 14 Jul 2016 14:40:31 +0000"  >&lt;p&gt;Github user radekg commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2231&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2231&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Sure, the problems are the following:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&lt;a href=&quot;https://github.com/apache/flink/pull/2231/commits/06936d7c5acc0897348019161c9ced4596a0a4dd#diff-aba21cf86694f3f2cd85e2e5e9b04972R305&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2231/commits/06936d7c5acc0897348019161c9ced4596a0a4dd#diff-aba21cf86694f3f2cd85e2e5e9b04972R305&lt;/a&gt; in 0.9, `consumer.assign` (&lt;a href=&quot;https://github.com/apache/flink/pull/2231/commits/06936d7c5acc0897348019161c9ced4596a0a4dd#diff-aba21cf86694f3f2cd85e2e5e9b04972R180&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2231/commits/06936d7c5acc0897348019161c9ced4596a0a4dd#diff-aba21cf86694f3f2cd85e2e5e9b04972R180&lt;/a&gt;) takes a `List`, in 0.10 it takes `Collection`&lt;/li&gt;
	&lt;li&gt;for unit tests: &lt;a href=&quot;https://github.com/apache/flink/pull/2231/commits/06936d7c5acc0897348019161c9ced4596a0a4dd#diff-ab65f3156ed8820677f3420152b78908R130&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2231/commits/06936d7c5acc0897348019161c9ced4596a0a4dd#diff-ab65f3156ed8820677f3420152b78908R130&lt;/a&gt;, if we use 0.9 kafka version with 0.10 client, the concrete client tests fail as they catch wrong exception type in: &lt;a href=&quot;https://github.com/TheWeatherCompany/flink/blob/06936d7c5acc0897348019161c9ced4596a0a4dd/flink-streaming-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java#L185&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/TheWeatherCompany/flink/blob/06936d7c5acc0897348019161c9ced4596a0a4dd/flink-streaming-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java#L185&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Silly stuff. Everything else works just fine. Fell free to reuse this stuff.&lt;/p&gt;

&lt;p&gt;    FYI: I&apos;d be confused it I was to use a class indicating 0.9 when working with 0.10, that&apos;s the reason I assembled separate module. 0.9 is done and there&apos;s no future work required, it makes sense to have 0.10. Just my opinion.&lt;/p&gt;</comment>
                            <comment id="15381654" author="githubbot" created="Mon, 18 Jul 2016 03:15:29 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2231&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2231&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Thank you for the description, @radekg .&lt;/p&gt;

&lt;p&gt;    I think the problems you mentioned should be solvable by working on the 0.9 connector to be just a bit more general, then users can simply manually use the 0.10 jars. However, you also have a point on the possible confusion. IMHO, I think it is redundant to have two connector modules with almost the same code, and it doesn&apos;t also seem feasible for code maintainability to keep adding modules for new Kafka versions even if they don&apos;t have changes in the API.&lt;/p&gt;

&lt;p&gt;    I think we&apos;ll need to loop in @rmetzger and @aljoscha to decide on how we can proceed with this. The solutions I currently see is to work on the 0.9 connector on the above problems so it can be compatible with the 0.10 API, and either rename the module to be `flink-connecto-kafka-0.10` (doesn&apos;t seem good because it&apos;ll be breaking user&apos;s pom&apos;s), or add information to the documentation on how to work with Kafka 0.10. Either way, in the long-run, we&apos;ll probably still need to sort out a way to better manage the connector codes in situations of new external system versions like this.&lt;/p&gt;</comment>
                            <comment id="15381798" author="elevy" created="Mon, 18 Jul 2016 06:44:40 +0000"  >&lt;p&gt;Sorry. I am on a boat in the middle of Indonesia for a few weeks without&lt;br/&gt;
much access.&lt;/p&gt;

&lt;p&gt;On Thursday, July 14, 2016, Tzu-Li (Gordon) Tai (JIRA) &amp;lt;jira@apache.org&amp;gt;&lt;/p&gt;
</comment>
                            <comment id="15382242" author="githubbot" created="Mon, 18 Jul 2016 13:04:04 +0000"  >&lt;p&gt;Github user aljoscha commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2231&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2231&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I agree with @tzulitai that it would be nice if you could have minimum code duplication in the long-run but it might not be possible with the current design of the consumers.&lt;/p&gt;

&lt;p&gt;    What about the new timestamps that were introduced in Kafka 0.10? This is also something that wouldn&apos;t work with the 0.9 consumer and could only be implemented for the 0.10-specific consumer, correct?&lt;/p&gt;</comment>
                            <comment id="15385513" author="githubbot" created="Wed, 20 Jul 2016 07:55:31 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2231&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2231&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @aljoscha Ah right, for the new timestamps we will definitely need a 0.10-specific consumer. I think it makes sense to include a new module then.&lt;/p&gt;

&lt;p&gt;    This current PR does not add the new 0.10 timestamps ( @radekg correct me if I&apos;m wrong here ), but I think we can add this as a separate follow up JIRA / PR afterwards, because it&apos;ll probably require changing some code in the `flink-connector-kafka-base` and the user-facing deserialization schemas that will need more discussion.&lt;/p&gt;

&lt;p&gt;    I&apos;ll find time this week to give this PR a test.&lt;/p&gt;</comment>
                            <comment id="15385629" author="githubbot" created="Wed, 20 Jul 2016 09:49:38 +0000"  >&lt;p&gt;Github user radekg commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2231&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2231&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @tzulitai yes, this pr does not deal with 0.10 specific timestamps. It makes a simple consumer application work.&lt;/p&gt;</comment>
                            <comment id="15389060" author="githubbot" created="Fri, 22 Jul 2016 07:02:24 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2231&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2231&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Hi @radekg , &lt;br/&gt;
    There was a recent change to the connector code to how the Kafka metrics are reported, so right now the PR has conflicts and can&apos;t be built. Would you like to rebase this PR on the current master branch so we can start testing it?&lt;/p&gt;</comment>
                            <comment id="15390117" author="githubbot" created="Fri, 22 Jul 2016 20:06:07 +0000"  >&lt;p&gt;Github user radekg commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2231&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2231&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Merged with `upstream/master` and I&apos;m getting this when running `mvn clean verify`:&lt;/p&gt;

&lt;p&gt;    ```&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; -------------------------------------------------------------&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; COMPILATION ERROR :&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; -------------------------------------------------------------&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; /Users/rad/dev/twc/flink/flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java:&lt;span class=&quot;error&quot;&gt;&amp;#91;30,69&amp;#93;&lt;/span&gt; cannot find symbol&lt;br/&gt;
      symbol:   class DefaultKafkaMetricAccumulator&lt;br/&gt;
      location: package org.apache.flink.streaming.connectors.kafka.internals.metrics&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; /Users/rad/dev/twc/flink/flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java:&lt;span class=&quot;error&quot;&gt;&amp;#91;105,17&amp;#93;&lt;/span&gt; constructor AbstractFetcher in class org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher&amp;lt;T,KPH&amp;gt; cannot be applied to given types;&lt;br/&gt;
      required: org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext&amp;lt;T&amp;gt;,java.util.List&amp;lt;org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition&amp;gt;,org.apache.flink.util.SerializedValue&amp;lt;org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks&amp;lt;T&amp;gt;&amp;gt;,org.apache.flink.util.SerializedValue&amp;lt;org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks&amp;lt;T&amp;gt;&amp;gt;,org.apache.flink.streaming.api.operators.StreamingRuntimeContext,boolean&lt;br/&gt;
      found: org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext&amp;lt;T&amp;gt;,java.util.List&amp;lt;org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition&amp;gt;,org.apache.flink.util.SerializedValue&amp;lt;org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks&amp;lt;T&amp;gt;&amp;gt;,org.apache.flink.util.SerializedValue&amp;lt;org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks&amp;lt;T&amp;gt;&amp;gt;,org.apache.flink.streaming.api.operators.StreamingRuntimeContext&lt;br/&gt;
      reason: actual and formal argument lists differ in length&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; /Users/rad/dev/twc/flink/flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java:&lt;span class=&quot;error&quot;&gt;&amp;#91;192,49&amp;#93;&lt;/span&gt; cannot find symbol&lt;br/&gt;
      symbol:   class DefaultKafkaMetricAccumulator&lt;br/&gt;
      location: class org.apache.flink.streaming.connectors.kafka.internal.Kafka010Fetcher&amp;lt;T&amp;gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; /Users/rad/dev/twc/flink/flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java:&lt;span class=&quot;error&quot;&gt;&amp;#91;193,65&amp;#93;&lt;/span&gt; cannot find symbol&lt;br/&gt;
      symbol:   variable DefaultKafkaMetricAccumulator&lt;br/&gt;
      location: class org.apache.flink.streaming.connectors.kafka.internal.Kafka010Fetcher&amp;lt;T&amp;gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; 4 errors&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; -------------------------------------------------------------&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; ------------------------------------------------------------------------&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Reactor Summary:&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; force-shading ...................................... SUCCESS [  1.210 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink .............................................. SUCCESS [  4.416 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-annotations .................................. SUCCESS [  1.551 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-shaded-hadoop ................................ SUCCESS [  0.162 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-shaded-hadoop2 ............................... SUCCESS [  6.451 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-shaded-include-yarn-tests .................... SUCCESS [  7.929 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-shaded-curator ............................... SUCCESS [  0.110 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-shaded-curator-recipes ....................... SUCCESS [  0.986 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-shaded-curator-test .......................... SUCCESS [  0.200 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-test-utils-parent ............................ SUCCESS [  0.111 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-test-utils-junit ............................. SUCCESS [  2.417 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-core ......................................... SUCCESS [ 37.825 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-java ......................................... SUCCESS [ 23.620 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-runtime ...................................... SUCCESS &lt;span class=&quot;error&quot;&gt;&amp;#91;06:25 min&amp;#93;&lt;/span&gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-optimizer .................................... SUCCESS [ 12.698 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-clients ...................................... SUCCESS [  9.795 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-streaming-java ............................... SUCCESS [ 43.709 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-test-utils ................................... SUCCESS [  9.363 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-scala ........................................ SUCCESS [ 37.639 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-runtime-web .................................. SUCCESS [ 19.749 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-examples ..................................... SUCCESS [  1.006 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-examples-batch ............................... SUCCESS [ 14.276 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-contrib ...................................... SUCCESS [  0.104 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-statebackend-rocksdb ......................... SUCCESS [ 10.938 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-tests ........................................ SUCCESS &lt;span class=&quot;error&quot;&gt;&amp;#91;07:34 min&amp;#93;&lt;/span&gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-streaming-scala .............................. SUCCESS [ 33.365 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-streaming-connectors ......................... SUCCESS [  0.106 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-connector-flume .............................. SUCCESS [  5.626 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-libraries .................................... SUCCESS [  0.100 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-table ........................................ SUCCESS &lt;span class=&quot;error&quot;&gt;&amp;#91;02:31 min&amp;#93;&lt;/span&gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-connector-kafka-base ......................... SUCCESS [ 10.033 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-connector-kafka-0.8 .......................... SUCCESS &lt;span class=&quot;error&quot;&gt;&amp;#91;02:06 min&amp;#93;&lt;/span&gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-connector-kafka-0.9 .......................... SUCCESS &lt;span class=&quot;error&quot;&gt;&amp;#91;02:12 min&amp;#93;&lt;/span&gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-connector-kafka-0.10 ......................... FAILURE [  0.197 s]&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-connector-elasticsearch ...................... SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-connector-elasticsearch2 ..................... SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-connector-rabbitmq ........................... SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-connector-twitter ............................ SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-connector-nifi ............................... SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-connector-cassandra .......................... SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-connector-redis .............................. SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-connector-filesystem ......................... SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-batch-connectors ............................. SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-avro ......................................... SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-jdbc ......................................... SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-hadoop-compatibility ......................... SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-hbase ........................................ SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-hcatalog ..................................... SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-examples-streaming ........................... SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-gelly ........................................ SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-gelly-scala .................................. SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-gelly-examples ............................... SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-python ....................................... SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-ml ........................................... SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-cep .......................................... SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-cep-scala .................................... SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-scala-shell .................................. SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-quickstart ................................... SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-quickstart-java .............................. SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-quickstart-scala ............................. SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-storm ........................................ SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-storm-examples ............................... SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-streaming-contrib ............................ SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-tweet-inputformat ............................ SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-operator-stats ............................... SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-connector-wikiedits .......................... SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-yarn ......................................... SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-dist ......................................... SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-metrics ...................................... SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-metrics-dropwizard ........................... SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-metrics-ganglia .............................. SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-metrics-graphite ............................. SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-metrics-statsd ............................... SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-fs-tests ..................................... SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; flink-java8 ........................................ SKIPPED&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; ------------------------------------------------------------------------&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; BUILD FAILURE&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; ------------------------------------------------------------------------&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Total time: 25:46 min&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Finished at: 2016-07-22T21:52:55+02:00&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Final Memory: 159M/1763M&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; ------------------------------------------------------------------------&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project flink-connector-kafka-0.10_2.10: Compilation failure: Compilation failure:&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; /Users/rad/dev/twc/flink/flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java:&lt;span class=&quot;error&quot;&gt;&amp;#91;30,69&amp;#93;&lt;/span&gt; cannot find symbol&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; symbol:   class DefaultKafkaMetricAccumulator&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; location: package org.apache.flink.streaming.connectors.kafka.internals.metrics&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; /Users/rad/dev/twc/flink/flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java:&lt;span class=&quot;error&quot;&gt;&amp;#91;105,17&amp;#93;&lt;/span&gt; constructor AbstractFetcher in class org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher&amp;lt;T,KPH&amp;gt; cannot be applied to given types;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; required: org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext&amp;lt;T&amp;gt;,java.util.List&amp;lt;org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition&amp;gt;,org.apache.flink.util.SerializedValue&amp;lt;org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks&amp;lt;T&amp;gt;&amp;gt;,org.apache.flink.util.SerializedValue&amp;lt;org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks&amp;lt;T&amp;gt;&amp;gt;,org.apache.flink.streaming.api.operators.StreamingRuntimeContext,boolean&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; found: org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext&amp;lt;T&amp;gt;,java.util.List&amp;lt;org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition&amp;gt;,org.apache.flink.util.SerializedValue&amp;lt;org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks&amp;lt;T&amp;gt;&amp;gt;,org.apache.flink.util.SerializedValue&amp;lt;org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks&amp;lt;T&amp;gt;&amp;gt;,org.apache.flink.streaming.api.operators.StreamingRuntimeContext&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; reason: actual and formal argument lists differ in length&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; /Users/rad/dev/twc/flink/flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java:&lt;span class=&quot;error&quot;&gt;&amp;#91;192,49&amp;#93;&lt;/span&gt; cannot find symbol&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; symbol:   class DefaultKafkaMetricAccumulator&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; location: class org.apache.flink.streaming.connectors.kafka.internal.Kafka010Fetcher&amp;lt;T&amp;gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; /Users/rad/dev/twc/flink/flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java:&lt;span class=&quot;error&quot;&gt;&amp;#91;193,65&amp;#93;&lt;/span&gt; cannot find symbol&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; symbol:   variable DefaultKafkaMetricAccumulator&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; location: class org.apache.flink.streaming.connectors.kafka.internal.Kafka010Fetcher&amp;lt;T&amp;gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; -&amp;gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;Help 1&amp;#93;&lt;/span&gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; To see the full stack trace of the errors, re-run Maven with the -e switch.&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; Re-run Maven using the -X switch to enable full debug logging.&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; For more information about the errors and possible solutions, please read the following articles:&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;Help 1&amp;#93;&lt;/span&gt; &lt;a href=&quot;http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException&lt;/a&gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; After correcting the problems, you can resume the build with the command&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt;   mvn &amp;lt;goals&amp;gt; -rf :flink-connector-kafka-0.10_2.10&lt;br/&gt;
    ```&lt;/p&gt;

&lt;p&gt;    Any advice?&lt;/p&gt;</comment>
                            <comment id="15390130" author="githubbot" created="Fri, 22 Jul 2016 20:19:54 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2231&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2231&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    The errors are due to some of the changes to `AbstractFetcher` in &lt;a href=&quot;https://github.com/apache/flink/commit/41f58182289226850b23c61a32f01223485d4775&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/commit/41f58182289226850b23c61a32f01223485d4775&lt;/a&gt;. Some of the Kafka 0.9 connector code that has changed accordingly, so you&apos;ll probably need to reflect those changes in the Kafka 0.10 code too.&lt;/p&gt;</comment>
                            <comment id="15390177" author="githubbot" created="Fri, 22 Jul 2016 20:52:45 +0000"  >&lt;p&gt;Github user radekg commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2231&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2231&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Thanks, running `verify` again.&lt;/p&gt;</comment>
                            <comment id="15390225" author="githubbot" created="Fri, 22 Jul 2016 21:31:59 +0000"  >&lt;p&gt;Github user radekg commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2231&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2231&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Travis is going to run.&lt;/p&gt;</comment>
                            <comment id="15390505" author="githubbot" created="Sat, 23 Jul 2016 03:46:03 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2231&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2231&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @radekg Thank you for the quick fix. I hope to find time over the weekend to test + review this, if not than early next week &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15390636" author="githubbot" created="Sat, 23 Jul 2016 09:35:54 +0000"  >&lt;p&gt;Github user radekg commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2231&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2231&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Tests are failing for random setups on travis. Seems to be something scala related.&lt;/p&gt;</comment>
                            <comment id="15408921" author="githubbot" created="Fri, 5 Aug 2016 05:53:59 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2231&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2231&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Sorry for the delay on the review. Reviewing + testing now ...&lt;/p&gt;

&lt;p&gt;    @radekg Yup the failure is unrelated to the changes here. Some of the tests are currently a bit flaky.&lt;/p&gt;</comment>
                            <comment id="15408958" author="githubbot" created="Fri, 5 Aug 2016 06:21:48 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2231#discussion_r73646737&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2231#discussion_r73646737&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaTestEnvironmentImpl.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,331 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka;&lt;br/&gt;
    +&lt;br/&gt;
    +import kafka.admin.AdminUtils;&lt;br/&gt;
    +import kafka.common.KafkaException;&lt;br/&gt;
    +import kafka.network.SocketServer;&lt;br/&gt;
    +import kafka.server.KafkaConfig;&lt;br/&gt;
    +import kafka.server.KafkaServer;&lt;br/&gt;
    +import kafka.utils.SystemTime$;&lt;br/&gt;
    +import kafka.utils.ZkUtils;&lt;br/&gt;
    +import org.I0Itec.zkclient.ZkClient;&lt;br/&gt;
    +import org.apache.commons.io.FileUtils;&lt;br/&gt;
    +import org.apache.curator.test.TestingServer;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.testutils.ZooKeeperStringSerializer;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.KafkaPartitioner;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedDeserializationSchema;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchema;&lt;br/&gt;
    +import org.apache.flink.util.NetUtils;&lt;br/&gt;
    +import org.apache.kafka.common.protocol.SecurityProtocol;&lt;br/&gt;
    +import org.apache.kafka.common.requests.MetadataResponse;&lt;br/&gt;
    +import org.slf4j.Logger;&lt;br/&gt;
    +import org.slf4j.LoggerFactory;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.io.File;&lt;br/&gt;
    +import java.net.BindException;&lt;br/&gt;
    +import java.util.ArrayList;&lt;br/&gt;
    +import java.util.List;&lt;br/&gt;
    +import java.util.Properties;&lt;br/&gt;
    +import java.util.UUID;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.apache.flink.util.NetUtils.hostAndPortToUrlString;&lt;br/&gt;
    +import static org.junit.Assert.assertTrue;&lt;br/&gt;
    +import static org.junit.Assert.fail;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * An implementation of the KafkaServerProvider for Kafka 0.10&lt;br/&gt;
    + */&lt;br/&gt;
    +public class KafkaTestEnvironmentImpl extends KafkaTestEnvironment {&lt;br/&gt;
    +&lt;br/&gt;
    +	protected static final Logger LOG = LoggerFactory.getLogger(KafkaTestEnvironmentImpl.class);&lt;br/&gt;
    +	private File tmpZkDir;&lt;br/&gt;
    +	private File tmpKafkaParent;&lt;br/&gt;
    +	private List&amp;lt;File&amp;gt; tmpKafkaDirs;&lt;br/&gt;
    +	private List&amp;lt;KafkaServer&amp;gt; brokers;&lt;br/&gt;
    +	private TestingServer zookeeper;&lt;br/&gt;
    +	private String zookeeperConnectionString;&lt;br/&gt;
    +	private String brokerConnectionString = &quot;&quot;;&lt;br/&gt;
    +	private Properties standardProps;&lt;br/&gt;
    +	private Properties additionalServerProperties;&lt;br/&gt;
    +&lt;br/&gt;
    +	public String getBrokerConnectionString() &lt;/p&gt;
{
    +		return brokerConnectionString;
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public Properties getStandardProperties() &lt;/p&gt;
{
    +		return standardProps;
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public String getVersion() &lt;/p&gt;
{
    +		return &quot;0.10&quot;;
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public List&amp;lt;KafkaServer&amp;gt; getBrokers() &lt;/p&gt;
{
    +		return brokers;
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public &amp;lt;T&amp;gt; FlinkKafkaConsumerBase&amp;lt;T&amp;gt; getConsumer(List&amp;lt;String&amp;gt; topics, KeyedDeserializationSchema&amp;lt;T&amp;gt; readSchema, Properties props) &lt;/p&gt;
{
    +		return new FlinkKafkaConsumer010&amp;lt;&amp;gt;(topics, readSchema, props);
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public &amp;lt;T&amp;gt; FlinkKafkaProducerBase&amp;lt;T&amp;gt; getProducer(String topic, KeyedSerializationSchema&amp;lt;T&amp;gt; serSchema, Properties props, KafkaPartitioner&amp;lt;T&amp;gt; partitioner) &lt;/p&gt;
{
    +		FlinkKafkaProducer010&amp;lt;T&amp;gt; prod = new FlinkKafkaProducer010&amp;lt;&amp;gt;(topic, serSchema, props, partitioner);
    +		prod.setFlushOnCheckpoint(true);
    +		return prod;
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public void restartBroker(int leaderId) throws Exception &lt;/p&gt;
{
    +		brokers.set(leaderId, getKafkaServer(leaderId, tmpKafkaDirs.get(leaderId)));
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public int getLeaderToShutDown(String topic) throws Exception {&lt;br/&gt;
    +		ZkUtils zkUtils = getZkUtils();&lt;br/&gt;
    +		try {&lt;br/&gt;
    +			MetadataResponse.PartitionMetadata firstPart = null;&lt;br/&gt;
    +			do {&lt;br/&gt;
    +				if (firstPart != null) {&lt;br/&gt;
    +					LOG.info(&quot;Unable to find leader. error code {}&quot;, firstPart.error().code());&lt;br/&gt;
    +					// not the first try. Sleep a bit&lt;br/&gt;
    +					Thread.sleep(150);&lt;br/&gt;
    +				}&lt;br/&gt;
    +&lt;br/&gt;
    +				List&amp;lt;MetadataResponse.PartitionMetadata&amp;gt; partitionMetadata = AdminUtils.fetchTopicMetadataFromZk(topic, zkUtils).partitionMetadata();&lt;br/&gt;
    +				firstPart = partitionMetadata.get(0);&lt;br/&gt;
    +			}&lt;br/&gt;
    +			while (firstPart.error().code() != 0);&lt;br/&gt;
    +&lt;br/&gt;
    +			return firstPart.leader().id();&lt;br/&gt;
    +		} finally &lt;/p&gt;
{
    +			zkUtils.close();
    +		}
&lt;p&gt;    +	}&lt;br/&gt;
    +&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public int getBrokerId(KafkaServer server) &lt;/p&gt;
{
    +		return server.config().brokerId();
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public void prepare(int numKafkaServers, Properties additionalServerProperties) {&lt;br/&gt;
    +		this.additionalServerProperties = additionalServerProperties;&lt;br/&gt;
    +		File tempDir = new File(System.getProperty(&quot;java.io.tmpdir&quot;));&lt;br/&gt;
    +&lt;br/&gt;
    +		tmpZkDir = new File(tempDir, &quot;kafkaITcase-zk-dir-&quot; + (UUID.randomUUID().toString()));&lt;br/&gt;
    +		assertTrue(&quot;cannot create zookeeper temp dir&quot;, tmpZkDir.mkdirs());&lt;br/&gt;
    +&lt;br/&gt;
    +		tmpKafkaParent = new File(tempDir, &quot;kafkaITcase-kafka-dir*&quot; + (UUID.randomUUID().toString()));&lt;br/&gt;
    +		assertTrue(&quot;cannot create kafka temp dir&quot;, tmpKafkaParent.mkdirs());&lt;br/&gt;
    +&lt;br/&gt;
    +		tmpKafkaDirs = new ArrayList&amp;lt;&amp;gt;(numKafkaServers);&lt;br/&gt;
    +		for (int i = 0; i &amp;lt; numKafkaServers; i++) &lt;/p&gt;
{
    +			File tmpDir = new File(tmpKafkaParent, &quot;server-&quot; + i);
    +			assertTrue(&quot;cannot create kafka temp dir&quot;, tmpDir.mkdir());
    +			tmpKafkaDirs.add(tmpDir);
    +		}
&lt;p&gt;    +&lt;br/&gt;
    +		zookeeper = null;&lt;br/&gt;
    +		brokers = null;&lt;br/&gt;
    +&lt;br/&gt;
    +		try {&lt;br/&gt;
    +			LOG.info(&quot;Starting Zookeeper&quot;);&lt;br/&gt;
    +			zookeeper = new TestingServer(-1, tmpZkDir);&lt;br/&gt;
    +			zookeeperConnectionString = zookeeper.getConnectString();&lt;br/&gt;
    +&lt;br/&gt;
    +			LOG.info(&quot;Starting KafkaServer&quot;);&lt;br/&gt;
    +			brokers = new ArrayList&amp;lt;&amp;gt;(numKafkaServers);&lt;br/&gt;
    +&lt;br/&gt;
    +			for (int i = 0; i &amp;lt; numKafkaServers; i++) &lt;/p&gt;
{
    +				brokers.add(getKafkaServer(i, tmpKafkaDirs.get(i)));
    +
    +				SocketServer socketServer = brokers.get(i).socketServer();
    +				brokerConnectionString += hostAndPortToUrlString(KafkaTestEnvironment.KAFKA_HOST, brokers.get(i).socketServer().boundPort(SecurityProtocol.PLAINTEXT)) + &quot;,&quot;;
    +			}
&lt;p&gt;    +&lt;br/&gt;
    +			LOG.info(&quot;ZK and KafkaServer started.&quot;);&lt;br/&gt;
    +		}&lt;br/&gt;
    +		catch (Throwable t) &lt;/p&gt;
{
    +			t.printStackTrace();
    +			fail(&quot;Test setup failed: &quot; + t.getMessage());
    +		}
&lt;p&gt;    +&lt;br/&gt;
    +		standardProps = new Properties();&lt;br/&gt;
    +		standardProps.setProperty(&quot;zookeeper.connect&quot;, zookeeperConnectionString);&lt;br/&gt;
    +		standardProps.setProperty(&quot;bootstrap.servers&quot;, brokerConnectionString);&lt;br/&gt;
    +		standardProps.setProperty(&quot;group.id&quot;, &quot;flink-tests&quot;);&lt;br/&gt;
    +		standardProps.setProperty(&quot;auto.commit.enable&quot;, &quot;false&quot;);&lt;br/&gt;
    +		standardProps.setProperty(&quot;zookeeper.session.timeout.ms&quot;, &quot;30000&quot;); // 6 seconds is default. Seems to be too small for travis.&lt;br/&gt;
    +		standardProps.setProperty(&quot;zookeeper.connection.timeout.ms&quot;, &quot;30000&quot;);&lt;br/&gt;
    +		standardProps.setProperty(&quot;auto.offset.reset&quot;, &quot;earliest&quot;); // read from the beginning. (earliest is kafka 0.10 value)&lt;br/&gt;
    +		standardProps.setProperty(&quot;fetch.message.max.bytes&quot;, &quot;256&quot;); // make a lot of fetches (MESSAGES MUST BE SMALLER!)&lt;br/&gt;
    +	}&lt;br/&gt;
    +&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public void shutdown() {&lt;br/&gt;
    +		for (KafkaServer broker : brokers) {&lt;br/&gt;
    +			if (broker != null) &lt;/p&gt;
{
    +				broker.shutdown();
    +			}
&lt;p&gt;    +		}&lt;br/&gt;
    +		brokers.clear();&lt;br/&gt;
    +&lt;br/&gt;
    +		if (zookeeper != null) {&lt;br/&gt;
    +			try &lt;/p&gt;
{
    +				zookeeper.stop();
    +			}
&lt;p&gt;    +			catch (Exception e) &lt;/p&gt;
{
    +				LOG.warn(&quot;ZK.stop() failed&quot;, e);
    +			}
&lt;p&gt;    +			zookeeper = null;&lt;br/&gt;
    +		}&lt;br/&gt;
    +&lt;br/&gt;
    +		// clean up the temp spaces&lt;br/&gt;
    +&lt;br/&gt;
    +		if (tmpKafkaParent != null &amp;amp;&amp;amp; tmpKafkaParent.exists()) {&lt;br/&gt;
    +			try &lt;/p&gt;
{
    +				FileUtils.deleteDirectory(tmpKafkaParent);
    +			}
&lt;p&gt;    +			catch (Exception e) &lt;/p&gt;
{
    +				// ignore
    +			}&lt;br/&gt;
    +		}&lt;br/&gt;
    +		if (tmpZkDir != null &amp;amp;&amp;amp; tmpZkDir.exists()) {&lt;br/&gt;
    +			try {
    +				FileUtils.deleteDirectory(tmpZkDir);
    +			}&lt;br/&gt;
    +			catch (Exception e) {    +				// ignore    +			}
&lt;p&gt;    +		}&lt;br/&gt;
    +	}&lt;br/&gt;
    +&lt;br/&gt;
    +	public ZkUtils getZkUtils() &lt;/p&gt;
{
    +		ZkClient creator = new ZkClient(zookeeperConnectionString, Integer.valueOf(standardProps.getProperty(&quot;zookeeper.session.timeout.ms&quot;)),
    +				Integer.valueOf(standardProps.getProperty(&quot;zookeeper.connection.timeout.ms&quot;)), new ZooKeeperStringSerializer());
    +		return ZkUtils.apply(creator, false);
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public void createTestTopic(String topic, int numberOfPartitions, int replicationFactor, Properties topicConfig) {&lt;br/&gt;
    +		// create topic with one client&lt;br/&gt;
    +		LOG.info(&quot;Creating topic {}&quot;, topic);&lt;br/&gt;
    +&lt;br/&gt;
    +		ZkUtils zkUtils = getZkUtils();&lt;br/&gt;
    +		try {&lt;br/&gt;
    +			AdminUtils.createTopic(zkUtils, topic, numberOfPartitions, replicationFactor, topicConfig, new kafka.admin.RackAwareMode.Enforced$());&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    The proper usage of `RackAwareMode` here seems to be `kafka.admin.RackAwareMode.Enforced$.MODULE$` (this is how tests in Kafka use this). IntelliJ complains that `new kafka.admin.RackAwareMode.Enforced$()` has private access, I&apos;m not sure why the build is passing on this though ...&lt;/p&gt;</comment>
                            <comment id="15408993" author="githubbot" created="Fri, 5 Aug 2016 06:44:55 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2231#discussion_r73648473&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2231#discussion_r73648473&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java &amp;#8212;&lt;br/&gt;
    @@ -134,6 +134,10 @@&lt;br/&gt;
     	@Rule&lt;br/&gt;
     	public RetryRule retryRule = new RetryRule();&lt;/p&gt;

&lt;p&gt;    +	public String getExpectedKafkaVersion() &lt;/p&gt;
{
    +		return &quot;0.9&quot;;
    +	}
&lt;p&gt;    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    We should make this an abstract method in the abstract test base.&lt;/p&gt;</comment>
                            <comment id="15409063" author="githubbot" created="Fri, 5 Aug 2016 07:29:33 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2231#discussion_r73652372&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2231#discussion_r73652372&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java &amp;#8212;&lt;br/&gt;
    @@ -186,7 +190,7 @@ public void runFailOnNoBrokerTest() throws Exception &lt;/p&gt;
{
     			stream.print();
     			see.execute(&quot;No broker test&quot;);
     		}
&lt;p&gt; catch(RuntimeException re) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if(kafkaServer.getVersion().equals(&quot;0.9&quot;)) {&lt;br/&gt;
    +			if(kafkaServer.getVersion().equals(getExpectedKafkaVersion())) {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    I think the original intent of this assert here was that 0.8 connector throws different exception messages than 0.9.&lt;br/&gt;
    So adding the `getExpectedKafkaVersion()` is a bit confusing with respect to the test intent.&lt;br/&gt;
    Perhaps we should remove the new `getExpectedKafkaVersion()` and simply change this to `kafkaServer.getVersion().equals(&quot;0.9&quot;) || kafkaServer.getVersion().equals(&quot;0.10&quot;)`?&lt;/p&gt;</comment>
                            <comment id="15409138" author="githubbot" created="Fri, 5 Aug 2016 08:12:09 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2231#discussion_r73656446&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2231#discussion_r73656446&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumer010.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,259 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks;&lt;br/&gt;
    +import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks;&lt;br/&gt;
    +import org.apache.flink.streaming.api.operators.StreamingRuntimeContext;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internal.Kafka010Fetcher;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.DeserializationSchema;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedDeserializationSchema;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedDeserializationSchemaWrapper;&lt;br/&gt;
    +import org.apache.flink.util.SerializedValue;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.ConsumerConfig;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.KafkaConsumer;&lt;br/&gt;
    +import org.apache.kafka.common.PartitionInfo;&lt;br/&gt;
    +import org.apache.kafka.common.serialization.ByteArrayDeserializer;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.slf4j.Logger;&lt;br/&gt;
    +import org.slf4j.LoggerFactory;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util.ArrayList;&lt;br/&gt;
    +import java.util.Collections;&lt;br/&gt;
    +import java.util.List;&lt;br/&gt;
    +import java.util.Properties;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.apache.flink.util.Preconditions.checkNotNull;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * The Flink Kafka Consumer is a streaming data source that pulls a parallel data stream from&lt;br/&gt;
    + * Apache Kafka 0.10.x. The consumer can run in multiple parallel instances, each of which will pull&lt;br/&gt;
    + * data from one or more Kafka partitions. &lt;br/&gt;
    + * &lt;br/&gt;
    + * &amp;lt;p&amp;gt;The Flink Kafka Consumer participates in checkpointing and guarantees that no data is lost&lt;br/&gt;
    + * during a failure, and that the computation processes elements &quot;exactly once&quot;. &lt;br/&gt;
    + * (Note: These guarantees naturally assume that Kafka itself does not loose any data.)&amp;lt;/p&amp;gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * &amp;lt;p&amp;gt;Please note that Flink snapshots the offsets internally as part of its distributed checkpoints. The offsets&lt;br/&gt;
    + * committed to Kafka / ZooKeeper are only to bring the outside view of progress in sync with Flink&apos;s view&lt;br/&gt;
    + * of the progress. That way, monitoring and other jobs can get a view of how far the Flink Kafka consumer&lt;br/&gt;
    + * has consumed a topic.&amp;lt;/p&amp;gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * &amp;lt;p&amp;gt;Please refer to Kafka&apos;s documentation for the available configuration properties:&lt;br/&gt;
    + * &lt;a href=&quot;http://kafka.apache.org/documentation.html#newconsumerconfigs&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://kafka.apache.org/documentation.html#newconsumerconfigs&lt;/a&gt;&amp;lt;/p&amp;gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * &amp;lt;p&amp;gt;&amp;lt;b&amp;gt;NOTE:&amp;lt;/b&amp;gt; The implementation currently accesses partition metadata when the consumer&lt;br/&gt;
    + * is constructed. That means that the client that submits the program needs to be able to&lt;br/&gt;
    + * reach the Kafka brokers or ZooKeeper.&amp;lt;/p&amp;gt;&lt;br/&gt;
    + */&lt;br/&gt;
    +public class FlinkKafkaConsumer010&amp;lt;T&amp;gt; extends FlinkKafkaConsumerBase&amp;lt;T&amp;gt; {&lt;br/&gt;
    +&lt;br/&gt;
    +	private static final long serialVersionUID = 2324564345203409112L;&lt;br/&gt;
    +&lt;br/&gt;
    +	private static final Logger LOG = LoggerFactory.getLogger(FlinkKafkaConsumer010.class);&lt;br/&gt;
    +&lt;br/&gt;
    +	/**  Configuration key to change the polling timeout **/&lt;br/&gt;
    +	public static final String KEY_POLL_TIMEOUT = &quot;flink.poll-timeout&quot;;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** Boolean configuration key to disable metrics tracking **/&lt;br/&gt;
    +	public static final String KEY_DISABLE_METRICS = &quot;flink.disable-metrics&quot;;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    This is redundant. It&apos;s already declared in the `FlinkKafkaConsumerBase`.&lt;/p&gt;</comment>
                            <comment id="15409181" author="githubbot" created="Fri, 5 Aug 2016 08:50:02 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2231&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2231&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Finished first review of the code.&lt;/p&gt;

&lt;p&gt;    Let me summarize parts of the Kafka 0.10 API that requires us to have a separate module:&lt;/p&gt;

&lt;p&gt;     1. `ConsumerRecord` in 0.10 has a new `ConsumerRecord#timestamp()` method to retrieve Kafka server-side timestamps. If we want to attach this timestamp to the records as the default event time in the future, we&apos;d definitely need a separate module (0.10 timestamp feature not included in this PR).&lt;br/&gt;
     2. `PartitionMetaData` (used in `KafkaTestEnvironmentImpl`s) has a breaking change to the APIs for retrieving the info, so we can&apos;t simply bump the version either.&lt;/p&gt;

&lt;p&gt;    Other than the above, the rest of the code is the same between (or changes are irrelevant to Kafka API changes) the 0.9 connector.&lt;/p&gt;</comment>
                            <comment id="15409192" author="githubbot" created="Fri, 5 Aug 2016 08:58:56 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2231&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2231&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @radekg One thing that&apos;s missing is update to the docs. You can find the Kafka connector documentation at `flink/docs/apis/streaming/connectors/kafka.md`. We&apos;lll probably only need to update the Maven dependency table to include the 0.10 connector.&lt;/p&gt;</comment>
                            <comment id="15409194" author="githubbot" created="Fri, 5 Aug 2016 09:04:49 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2231#discussion_r73662520&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2231#discussion_r73662520&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.10/pom.xml &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,179 @@&lt;br/&gt;
    +&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;&lt;br/&gt;
    +&amp;lt;!--&lt;br/&gt;
    +Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    +or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    +distributed with this work for additional information&lt;br/&gt;
    +regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    +to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    +&quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    +with the License.  You may obtain a copy of the License at&lt;br/&gt;
    +&lt;br/&gt;
    +  &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    +&lt;br/&gt;
    +Unless required by applicable law or agreed to in writing,&lt;br/&gt;
    +software distributed under the License is distributed on an&lt;br/&gt;
    +&quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY&lt;br/&gt;
    +KIND, either express or implied.  See the License for the&lt;br/&gt;
    +specific language governing permissions and limitations&lt;br/&gt;
    +under the License.&lt;br/&gt;
    +--&amp;gt;&lt;br/&gt;
    +&amp;lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;&lt;br/&gt;
    +		 xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;&lt;br/&gt;
    +		 xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 &lt;a href=&quot;http://maven.apache.org/maven-v4_0_0.xsd&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://maven.apache.org/maven-v4_0_0.xsd&lt;/a&gt;&quot;&amp;gt;&lt;br/&gt;
    +&lt;br/&gt;
    +	&amp;lt;modelVersion&amp;gt;4.0.0&amp;lt;/modelVersion&amp;gt;&lt;br/&gt;
    +&lt;br/&gt;
    +	&amp;lt;parent&amp;gt;&lt;br/&gt;
    +		&amp;lt;groupId&amp;gt;org.apache.flink&amp;lt;/groupId&amp;gt;&lt;br/&gt;
    +		&amp;lt;artifactId&amp;gt;flink-streaming-connectors&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
    +		&amp;lt;version&amp;gt;1.1-SNAPSHOT&amp;lt;/version&amp;gt;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    We&apos;ve recently just bumped version to 1.2-SNAPSHOT.&lt;/p&gt;</comment>
                            <comment id="15413518" author="githubbot" created="Tue, 9 Aug 2016 13:33:50 +0000"  >&lt;p&gt;Github user rmetzger commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2231&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2231&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Sorry for joining this discussion late. I&apos;ve been on vacation.&lt;br/&gt;
    I also stumbled across the code duplicates. I&apos;ll check out the code from this pull request and see if there&apos;s a good way of re-using most of the 0.9 connector code.&lt;/p&gt;</comment>
                            <comment id="15415363" author="githubbot" created="Wed, 10 Aug 2016 14:38:56 +0000"  >&lt;p&gt;Github user rmetzger commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2231&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2231&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @radekg, are you okay with me using your pull request as a base for adding Kafka 0.10 to Flink?&lt;br/&gt;
    I&apos;ve started changing your code from the PR so that we don&apos;t need to copy so much code: &lt;a href=&quot;https://github.com/rmetzger/flink/commits/kafka-0.10&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/rmetzger/flink/commits/kafka-0.10&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15415365" author="githubbot" created="Wed, 10 Aug 2016 14:40:39 +0000"  >&lt;p&gt;Github user radekg commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2231&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2231&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @rmetzger it&apos;s absolutely fine to reuse the code. If I can help in any way, please let me know.&lt;/p&gt;</comment>
                            <comment id="15415368" author="githubbot" created="Wed, 10 Aug 2016 14:42:59 +0000"  >&lt;p&gt;Github user rmetzger commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2231&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2231&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Okay, cool. Thank you. I&apos;ll probably open a pull request with your and my changes. I&apos;ll let you know so that you can help reviewing it.&lt;/p&gt;
</comment>
                            <comment id="15417880" author="elevy" created="Thu, 11 Aug 2016 20:26:37 +0000"  >&lt;p&gt;FWIW I generated a flink-connector-kafka-0.9_2.11-1.1.1.jar that uses kaka-clients 0.10.0.1 (it required hacking around some issues in one of the tests which I largely ignore).  I&apos;ve tested it in a Flink 1.1.1 cluster against a 0.10.0.1 Kafka cluster without any issues.  Making use of the 0.10.0.1 clients dropped the CPU usage on the Kafka brokers from 100% to 2%, as previously the broker had to transcode the messages from the 0.10 format to the 0.9 format, whereas with the 0.10 client it can make use of zero copy from disk to the socket.&lt;/p&gt;

&lt;p&gt;It is really too bad that the Kafka clients are not backwards compatible with older brokers.  If they were, that would obviate the need to support multiple Kafka client version concurrently in Flink and similar system.  We&apos;d just have to keep up with the latest version of the client.&lt;/p&gt;</comment>
                            <comment id="15420438" author="githubbot" created="Sun, 14 Aug 2016 18:44:02 +0000"  >&lt;p&gt;GitHub user rmetzger opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-4035&quot; title=&quot;Add Apache Kafka 0.10 connector&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-4035&quot;&gt;&lt;del&gt;FLINK-4035&lt;/del&gt;&lt;/a&gt; Add a streaming connector for Apache Kafka 0.10.x&lt;/p&gt;

&lt;p&gt;    This pull request subsumes &lt;a href=&quot;https://github.com/apache/flink/pull/2231&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2231&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;    Compared to #2231, I&apos;ve based the connector on the existing 0.9 code (by extending it, reducing the amount of copied code), added a test case for the timestamp functionality and updated the documentation.&lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/rmetzger/flink&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/rmetzger/flink&lt;/a&gt; flink4035_rebased&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #2369&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit e3b2ede004b3c4ab6e37df1d6a268a52c1565316&lt;br/&gt;
Author: radekg &amp;lt;radek@gruchalski.com&amp;gt;&lt;br/&gt;
Date:   2016-07-12T17:19:01Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-4035&quot; title=&quot;Add Apache Kafka 0.10 connector&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-4035&quot;&gt;&lt;del&gt;FLINK-4035&lt;/del&gt;&lt;/a&gt; Add support for Kafka 0.10.x.&lt;/p&gt;

&lt;p&gt;commit 9d358a9ad5bcdd1ea644bc5b902240423586faf3&lt;br/&gt;
Author: Robert Metzger &amp;lt;rmetzger@apache.org&amp;gt;&lt;br/&gt;
Date:   2016-08-09T14:38:21Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-4035&quot; title=&quot;Add Apache Kafka 0.10 connector&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-4035&quot;&gt;&lt;del&gt;FLINK-4035&lt;/del&gt;&lt;/a&gt; Refactor the Kafka 0.10 connector to be based upon the 0.9 connector&lt;/p&gt;

&lt;p&gt;    Add a test case for Kafka&apos;s new timestamp functionality and update the documentation.&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="15421044" author="rmetzger" created="Mon, 15 Aug 2016 14:25:43 +0000"  >&lt;p&gt;Thank you for trying this out.&lt;br/&gt;
I&apos;ve opened a pull request yesterday that extends the kafka 0.9 connector, but uses the 0.10.0.0 Kafka dependency (I&apos;ll update to Kafka 0.10.0.1 before merging).&lt;/p&gt;

&lt;p&gt;The missing backwards compatibility in Kafka is indeed an issue for systems like ours.&lt;/p&gt;</comment>
                            <comment id="15421049" author="githubbot" created="Mon, 15 Aug 2016 14:29:21 +0000"  >&lt;p&gt;Github user rmetzger commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2231&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2231&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @radekg I&apos;ve opened a PR based on your code: &lt;a href=&quot;https://github.com/apache/flink/issues/2369&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/issues/2369&lt;/a&gt; feel free to review it.&lt;/p&gt;</comment>
                            <comment id="15422807" author="githubbot" created="Tue, 16 Aug 2016 14:36:32 +0000"  >&lt;p&gt;Github user StephanEwen commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Just looked over this briefly.&lt;br/&gt;
    What struck me first is that this again uses the dirty trick of adding a dependency to &quot;Flink Kafka 0.9&quot; and then transitively excluding &quot;Kafka 0.9&quot;. Is there a nicer way to solve this?&lt;/p&gt;</comment>
                            <comment id="15429127" author="githubbot" created="Sat, 20 Aug 2016 01:02:58 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369#discussion_r75569347&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369#discussion_r75569347&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/internals/AbstractFetcher.java &amp;#8212;&lt;br/&gt;
    @@ -207,15 +207,14 @@ public void restoreOffsets(HashMap&amp;lt;KafkaTopicPartition, Long&amp;gt; snapshotState) {&lt;br/&gt;
     	// ------------------------------------------------------------------------&lt;/p&gt;

&lt;p&gt;     	/**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;*&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;&amp;lt;p&amp;gt;Implementation Note: This method is kept brief to be JIT inlining friendly.&lt;/li&gt;
	&lt;li&gt;That makes the fast path efficient, the extended paths are called as separate methods.&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;*&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;@param record The record to emit&lt;/li&gt;
	&lt;li&gt;@param partitionState The state of the Kafka partition from which the record was fetched&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* @param offset The offset from which the record was fetched&lt;br/&gt;
    +	 * @param offset The offset of the record&lt;br/&gt;
    +	 * @param kafkaRecord The original Kafka record&lt;br/&gt;
     	 */&lt;/li&gt;
	&lt;li&gt;protected final void emitRecord(T record, KafkaTopicPartitionState&amp;lt;KPH&amp;gt; partitionState, long offset) {&lt;br/&gt;
    +	protected &amp;lt;R&amp;gt; void emitRecord(T record, KafkaTopicPartitionState&amp;lt;KPH&amp;gt; partitionState, long offset, R kafkaRecord) throws Exception {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Is there a reason we need to have an extra `kafkaRecord`? It doesn&apos;t seem to be used in the function.&lt;/p&gt;</comment>
                            <comment id="15429144" author="githubbot" created="Sat, 20 Aug 2016 01:25:56 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369#discussion_r75569873&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369#discussion_r75569873&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,91 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + * &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka.internal;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks;&lt;br/&gt;
    +import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks;&lt;br/&gt;
    +import org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext;&lt;br/&gt;
    +import org.apache.flink.streaming.api.operators.StreamingRuntimeContext;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionState;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedDeserializationSchema;&lt;br/&gt;
    +import org.apache.flink.util.SerializedValue;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.ConsumerRecord;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.KafkaConsumer;&lt;br/&gt;
    +import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util.List;&lt;br/&gt;
    +import java.util.Properties;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * A fetcher that fetches data from Kafka brokers via the Kafka 0.10 consumer API.&lt;br/&gt;
    + * &lt;br/&gt;
    + * @param &amp;lt;T&amp;gt; The type of elements produced by the fetcher.&lt;br/&gt;
    + */&lt;br/&gt;
    +public class Kafka010Fetcher&amp;lt;T&amp;gt; extends Kafka09Fetcher&amp;lt;T&amp;gt; {&lt;br/&gt;
    +&lt;br/&gt;
    +	public Kafka010Fetcher(&lt;br/&gt;
    +			SourceContext&amp;lt;T&amp;gt; sourceContext,&lt;br/&gt;
    +			List&amp;lt;KafkaTopicPartition&amp;gt; assignedPartitions,&lt;br/&gt;
    +			SerializedValue&amp;lt;AssignerWithPeriodicWatermarks&amp;lt;T&amp;gt;&amp;gt; watermarksPeriodic,&lt;br/&gt;
    +			SerializedValue&amp;lt;AssignerWithPunctuatedWatermarks&amp;lt;T&amp;gt;&amp;gt; watermarksPunctuated,&lt;br/&gt;
    +			StreamingRuntimeContext runtimeContext,&lt;br/&gt;
    +			KeyedDeserializationSchema&amp;lt;T&amp;gt; deserializer,&lt;br/&gt;
    +			Properties kafkaProperties,&lt;br/&gt;
    +			long pollTimeout,&lt;br/&gt;
    +			boolean useMetrics) throws Exception&lt;br/&gt;
    +	&lt;/p&gt;
{
    +		super(sourceContext, assignedPartitions, watermarksPeriodic, watermarksPunctuated, runtimeContext, deserializer, kafkaProperties, pollTimeout, useMetrics);
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	protected void assignPartitionsToConsumer(KafkaConsumer&amp;lt;byte[], byte[]&amp;gt; consumer, List&amp;lt;TopicPartition&amp;gt; topicPartitions) &lt;/p&gt;
{
    +		consumer.assign(topicPartitions);
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Emit record Kafka-timestamp aware.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	protected &amp;lt;R&amp;gt; void emitRecord(T record, KafkaTopicPartitionState&amp;lt;TopicPartition&amp;gt; partitionState, long offset, R kafkaRecord) throws Exception {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Just realized here the reason for the new `kafkaRecord`.&lt;/p&gt;

&lt;p&gt;    However, would it be better to add an `recordTimestamp` argument instead of passing the original Kafka record?&lt;br/&gt;
    If we don&apos;t have such a timestamp (for 0.8, 0.9), a `null` can be accepted. In `AbstractFetcher#emitRecord()`, we check if the value is `null` or not and correspondingly call `collect()` or `collectWithTimestamp`. The same goes for passing the `Long.MIN_VALUE` / timestamp to `emitRecordWithTimestampAndPeriodicWatermark` and `emitRecordWithTimestampAndPunctuatedWatermark `.&lt;/p&gt;

&lt;p&gt;    IMHO, I think this way the new code will be more meaningful and less confusing for the base `AbstractFetcher`, and also we won&apos;t need to override `emitRecord` in 0.10.&lt;/p&gt;</comment>
                            <comment id="15429179" author="githubbot" created="Sat, 20 Aug 2016 02:36:05 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369#discussion_r75571320&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369#discussion_r75571320&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer010.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,198 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.GenericTypeInfo;&lt;br/&gt;
    +import org.apache.flink.streaming.api.datastream.DataStream;&lt;br/&gt;
    +import org.apache.flink.streaming.api.datastream.DataStreamSink;&lt;br/&gt;
    +import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;&lt;br/&gt;
    +import org.apache.flink.streaming.api.operators.StreamSink;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.FixedPartitioner;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.KafkaPartitioner;&lt;br/&gt;
    +import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchema;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchemaWrapper;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.SerializationSchema;&lt;br/&gt;
    +import org.apache.kafka.clients.producer.ProducerRecord;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util.Properties;&lt;br/&gt;
    +&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * Flink Sink to produce data into a Kafka topic. This producer is compatible with Kafka 0.10.x&lt;br/&gt;
    + *&lt;br/&gt;
    + * Implementation note: This Producer wraps a Flink Kafka 0.9 Producer, overriding only&lt;br/&gt;
    + * the &quot;processElement&quot; / &quot;invoke&quot; method.&lt;br/&gt;
    + */&lt;br/&gt;
    +public class FlinkKafkaProducer010&amp;lt;T&amp;gt; extends StreamSink&amp;lt;T&amp;gt; {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Overall, I think the solution here seems a bit too &quot;hacky&quot; to me. Here&apos;s a few disadvantages I see: 1) the code in `processElement()` of this class is still quite a bit duplicate of the `invoke()` in `FlinkKafaProducerBase`. 2) with `writeToKafka()` we need to have a different usage style for the Kafka producer in 0.10, compared to previous versions.&lt;/p&gt;

&lt;p&gt;    If I&apos;m correct, I guess its due to the fact that we don&apos;t have access to the embedded record timestamp in the provided `next` in the usual `invoke`?&lt;/p&gt;

&lt;p&gt;    I&apos;m wondering whether or not a cleaner solution is to introduce a `serializeTimestamp(T element)` method in the `KeyedSerializationSchema`, and we can simply use that to make `FlinkKafkaProducerBase#invoke()` more general by replacing instantiation of `ProducerRecord`s with an abstract version-specific method. Then, `FlinkKafkaProducer010` can simply extend `FlinkKafkaProducer09`.&lt;/p&gt;

&lt;p&gt;    We&apos;ll need a migration plan though if we&apos;re going to change the serialization schema interface. If we&apos;re going for it, might as well migrate the `KeyedDeserializationSchema` to include the timestamp too (give `null` for 0.8, 0.9).&lt;br/&gt;
    Otherwise, I think the solution here is ok for a short term solution. What do you think?&lt;/p&gt;</comment>
                            <comment id="15429183" author="githubbot" created="Sat, 20 Aug 2016 02:49:24 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369#discussion_r75571478&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369#discussion_r75571478&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer010.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,198 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.GenericTypeInfo;&lt;br/&gt;
    +import org.apache.flink.streaming.api.datastream.DataStream;&lt;br/&gt;
    +import org.apache.flink.streaming.api.datastream.DataStreamSink;&lt;br/&gt;
    +import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;&lt;br/&gt;
    +import org.apache.flink.streaming.api.operators.StreamSink;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.FixedPartitioner;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.KafkaPartitioner;&lt;br/&gt;
    +import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchema;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchemaWrapper;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.SerializationSchema;&lt;br/&gt;
    +import org.apache.kafka.clients.producer.ProducerRecord;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util.Properties;&lt;br/&gt;
    +&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * Flink Sink to produce data into a Kafka topic. This producer is compatible with Kafka 0.10.x&lt;br/&gt;
    + *&lt;br/&gt;
    + * Implementation note: This Producer wraps a Flink Kafka 0.9 Producer, overriding only&lt;br/&gt;
    + * the &quot;processElement&quot; / &quot;invoke&quot; method.&lt;br/&gt;
    + */&lt;br/&gt;
    +public class FlinkKafkaProducer010&amp;lt;T&amp;gt; extends StreamSink&amp;lt;T&amp;gt; {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    But on the other hand, the downside for the approach above would be that users always have to explicitly extract the timestamp again (even if the record already has an embedded timestamp) to emit them to Kafka.&lt;/p&gt;</comment>
                            <comment id="15429201" author="githubbot" created="Sat, 20 Aug 2016 03:55:06 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369#discussion_r75572212&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369#discussion_r75572212&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer010.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,198 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.GenericTypeInfo;&lt;br/&gt;
    +import org.apache.flink.streaming.api.datastream.DataStream;&lt;br/&gt;
    +import org.apache.flink.streaming.api.datastream.DataStreamSink;&lt;br/&gt;
    +import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;&lt;br/&gt;
    +import org.apache.flink.streaming.api.operators.StreamSink;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.FixedPartitioner;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.KafkaPartitioner;&lt;br/&gt;
    +import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchema;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchemaWrapper;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.SerializationSchema;&lt;br/&gt;
    +import org.apache.kafka.clients.producer.ProducerRecord;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util.Properties;&lt;br/&gt;
    +&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * Flink Sink to produce data into a Kafka topic. This producer is compatible with Kafka 0.10.x&lt;br/&gt;
    + *&lt;br/&gt;
    + * Implementation note: This Producer wraps a Flink Kafka 0.9 Producer, overriding only&lt;br/&gt;
    + * the &quot;processElement&quot; / &quot;invoke&quot; method.&lt;br/&gt;
    + */&lt;br/&gt;
    +public class FlinkKafkaProducer010&amp;lt;T&amp;gt; extends StreamSink&amp;lt;T&amp;gt; {&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Flag controlling whether we are writing the Flink record&apos;s timestamp into Kafka.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	private boolean writeTimestampToKafka = false;&lt;br/&gt;
    +&lt;br/&gt;
    +	// ---------------------- &quot;Constructors&quot; for the producer ------------------ //&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to&lt;br/&gt;
    +	 * the topic.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @param inStream The stream to write to Kafka&lt;br/&gt;
    +	 * @param topicId ID of the Kafka topic.&lt;br/&gt;
    +	 * @param serializationSchema User defined serialization schema supporting key/value messages&lt;br/&gt;
    +	 * @param producerConfig Properties with the producer configuration.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public static &amp;lt;T&amp;gt; FlinkKafkaProducer010Configuration writeToKafka(DataStream&amp;lt;T&amp;gt; inStream,&lt;br/&gt;
    +										String topicId,&lt;br/&gt;
    +										KeyedSerializationSchema&amp;lt;T&amp;gt; serializationSchema,&lt;br/&gt;
    +										Properties producerConfig) &lt;/p&gt;
{
    +		return writeToKafka(inStream, topicId, serializationSchema, producerConfig, new FixedPartitioner&amp;lt;T&amp;gt;());
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to&lt;br/&gt;
    +	 * the topic.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @param inStream The stream to write to Kafka&lt;br/&gt;
    +	 * @param topicId ID of the Kafka topic.&lt;br/&gt;
    +	 * @param serializationSchema User defined (keyless) serialization schema.&lt;br/&gt;
    +	 * @param producerConfig Properties with the producer configuration.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public static &amp;lt;T&amp;gt; FlinkKafkaProducer010Configuration writeToKafka(DataStream&amp;lt;T&amp;gt; inStream,&lt;br/&gt;
    +										String topicId,&lt;br/&gt;
    +										SerializationSchema&amp;lt;T&amp;gt; serializationSchema,&lt;br/&gt;
    +										Properties producerConfig) &lt;/p&gt;
{
    +		return writeToKafka(inStream, topicId, new KeyedSerializationSchemaWrapper&amp;lt;&amp;gt;(serializationSchema), producerConfig, new FixedPartitioner&amp;lt;T&amp;gt;());
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to&lt;br/&gt;
    +	 * the topic.&lt;br/&gt;
    +	 *  @param inStream The stream to write to Kafka&lt;br/&gt;
    +	 *  @param topicId The name of the target topic&lt;br/&gt;
    +	 *  @param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages&lt;br/&gt;
    +	 *  @param producerConfig Configuration properties for the KafkaProducer. &apos;bootstrap.servers.&apos; is the only required argument.&lt;br/&gt;
    +	 *  @param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public static &amp;lt;T&amp;gt; FlinkKafkaProducer010Configuration&amp;lt;T&amp;gt; writeToKafka(DataStream&amp;lt;T&amp;gt; inStream,&lt;br/&gt;
    +																String topicId,&lt;br/&gt;
    +																KeyedSerializationSchema&amp;lt;T&amp;gt; serializationSchema,&lt;br/&gt;
    +																Properties producerConfig,&lt;br/&gt;
    +																KafkaPartitioner&amp;lt;T&amp;gt; customPartitioner) &lt;/p&gt;
{
    +		GenericTypeInfo&amp;lt;Object&amp;gt; objectTypeInfo = new GenericTypeInfo&amp;lt;&amp;gt;(Object.class);
    +		FlinkKafkaProducer010&amp;lt;T&amp;gt; kafkaProducer = new FlinkKafkaProducer010&amp;lt;&amp;gt;(topicId, serializationSchema, producerConfig, customPartitioner);
    +		SingleOutputStreamOperator&amp;lt;Object&amp;gt; transformation = inStream.transform(&quot;FlinKafkaProducer 0.10.x&quot;, objectTypeInfo, kafkaProducer);
    +		return new FlinkKafkaProducer010Configuration&amp;lt;&amp;gt;(transformation, kafkaProducer);
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Configuration object returned by the writeToKafka() call.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public static class FlinkKafkaProducer010Configuration&amp;lt;T&amp;gt; extends DataStreamSink&amp;lt;T&amp;gt; {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I wonder if we can move the config setter methods into `FlinkKafkaProducer010`, and let `writeToKafka` return the created 0.10 producer instead of instantiating a new `FlinkKafkaProducer010Configuration`? My reasoning is that I find it a bit strange to set these config values on a separate config object, but not &quot;applying&quot; it anywhere. I think users would normally expect they have to &quot;apply&quot; a completed config object somewhere to take effect. So, I have something like this in mind:&lt;/p&gt;

&lt;p&gt;    In constructor of `FlinkKafkaProducer010`&lt;br/&gt;
    ```&lt;br/&gt;
    private FlinkKafaProducer010(...) &lt;/p&gt;
{
      super(new FlinkKafkaProducer09&amp;lt;&amp;gt;(...));
      this.wrapped09Producer = (FlinkKafkaProducer09&amp;lt;T&amp;gt;) userFunction; // wrapped09Producer as FlinkKafkafProducer010&apos;s class field
    }
&lt;p&gt;    ```&lt;br/&gt;
    Then FlinkKafkaProducer010 can directly have the setter methods:&lt;br/&gt;
    ```&lt;br/&gt;
    public void setFlushOnCheckpoint(boolean flush) &lt;/p&gt;
{
        this.wrapped09producer.setFlushOnCheckpoint(flush);
    }

&lt;p&gt;    public void setLogFailuresOnly(boolean logFailuresOnly) &lt;/p&gt;
{
    	this.wrapped09producer.setLogFailuresOnly(logFailuresOnly);
    }

&lt;p&gt;    public void setWriteTimestampToKafka(boolean writeTimestampToKafka) &lt;/p&gt;
{
    	this.writeTimestampToKafka = writeTimestampToKafka;
    }
&lt;p&gt;    ```&lt;/p&gt;

&lt;p&gt;    Then users can use the producer like this:&lt;br/&gt;
    ```&lt;br/&gt;
    FlinkKafkaProducer010 producer = FlinkKafkaProducer010.writeToKafka(...)&lt;br/&gt;
    producer.setLogFailuresOnly(...)&lt;br/&gt;
    producer.setFlushOnCheckpoint(...)&lt;br/&gt;
    producer.setWriteTimestampToKafka(...)&lt;br/&gt;
    ```&lt;/p&gt;

&lt;p&gt;    Although still a bit different from the usage patterns of previous versions, it looks a bit more similar compared to what the PR has now. I&apos;m not sure if I may be missing something that lead to choosing to have a separate `FlinkKafkaProducer010Configuration`. Is there any particular reason?&lt;/p&gt;</comment>
                            <comment id="15429202" author="githubbot" created="Sat, 20 Aug 2016 04:01:33 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369#discussion_r75572263&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369#discussion_r75572263&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: docs/apis/streaming/connectors/kafka.md &amp;#8212;&lt;br/&gt;
    @@ -291,3 +301,35 @@ higher value.&lt;br/&gt;
     There is currently no transactional producer for Kafka, so Flink can not guarantee exactly-once delivery&lt;br/&gt;
     into a Kafka topic.&lt;/p&gt;

&lt;p&gt;    +### Using Kafka timestamps and Flink event time in Kafka 0.10&lt;br/&gt;
    +&lt;br/&gt;
    +Since Apache Kafka 0.10., Kafka&apos;s messages can carry &lt;span class=&quot;error&quot;&gt;&amp;#91;timestamps&amp;#93;&lt;/span&gt;(&lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-32+-+Add+timestamps+to+Kafka+message&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://cwiki.apache.org/confluence/display/KAFKA/KIP-32+-+Add+timestamps+to+Kafka+message&lt;/a&gt;), indicating&lt;br/&gt;
    +the time the event has occurred (see &lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;quot;event time&amp;quot; in Apache Flink&amp;#93;&lt;/span&gt;(../event_time.html)) or the time when the message&lt;br/&gt;
    +has been written to the Kafka broker.&lt;br/&gt;
    +&lt;br/&gt;
    +The `FlinkKafkaConsumer010` will emit records with the timestamp attached, if the time characteristic in Flink is &lt;br/&gt;
    +set to `TimeCharacteristic.EventTime` (`StreamExecutionEnvironment.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)`).&lt;br/&gt;
    +&lt;br/&gt;
    +The Kafka consumer does not emit watermarks. To emit watermarks, the same mechanisms as described above in &lt;br/&gt;
    +&quot;Kafka Consumers and Timestamp Extraction/Watermark Emission&quot;  using the `assignTimestampsAndWatermarks` method are applicable.&lt;br/&gt;
    +&lt;br/&gt;
    +There is no need to define a timestamp extractor when using the timestamps from Kafka. The `previousElementTimestamp` argument of &lt;br/&gt;
    +the `extractTimestamp()` method contains the timestamp carried by the Kafka message.&lt;br/&gt;
    +&lt;br/&gt;
    +A timestamp extractor for a Kafka consumer would look like this:&lt;br/&gt;
    +&lt;/p&gt;
{% highlight java %}&lt;br/&gt;
    +public long extractTimestamp(Long element, long previousElementTimestamp) {
    +    return previousElementTimestamp;
    +}&lt;br/&gt;
    +{% endhighlight %}&lt;br/&gt;
    +&lt;br/&gt;
    +&lt;br/&gt;
    +&lt;br/&gt;
    +The `FlinkKafkaProducer010` only emits the record timestamp, if `setWriteTimestampToKafka(true)` is set.&lt;br/&gt;
    +&lt;br/&gt;
    +{% highlight java %}
&lt;p&gt;    +FlinkKafkaProducer010.FlinkKafkaProducer010Configuration config = FlinkKafkaProducer010.writeToKafka(streamWithTimestamps, topic, new SimpleStringSchema(), standardProps);&lt;br/&gt;
    +config.setWriteTimestampToKafka(true);&lt;br/&gt;
    +&lt;/p&gt;
{% endhighlight %}
&lt;p&gt;    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I find the usage pattern of this a bit unfamiliar. I&apos;ve explained this in inline comments of the `FlinkKafkaProducer010` class.&lt;/p&gt;</comment>
                            <comment id="15429207" author="githubbot" created="Sat, 20 Aug 2016 04:10:18 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Left a few comments on some high-level design choices for a first review. Mostly on `FlinkKafkaProducer010`, I wonder if there are other better possibilities over there?&lt;/p&gt;</comment>
                            <comment id="15433144" author="githubbot" created="Tue, 23 Aug 2016 16:29:32 +0000"  >&lt;p&gt;Github user rmetzger commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @StephanEwen: The explicit exclude is actually not needed, because the kafka version defined in the connector has precedence over transitive kafka versions.&lt;br/&gt;
    ```&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; &amp;#8212; maven-dependency-plugin:2.8:tree (default-cli) @ flink-connector-kafka-0.10_2.10 &amp;#8212;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; org.apache.flink:flink-connector-kafka-0.10_2.10:jar:1.2-SNAPSHOT&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; +- org.apache.flink:flink-connector-kafka-0.9_2.10:jar:1.2-SNAPSHOT:compile&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; |  &amp;#45; org.apache.flink:flink-connector-kafka-base_2.10:jar:1.2-SNAPSHOT:compile&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; +- org.apache.kafka:kafka-clients:jar:0.10.0.1:compile&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; |  +- net.jpountz.lz4:lz4:jar:1.3.0:compile&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; |  &amp;#45; org.xerial.snappy:snappy-java:jar:1.1.2.6:compile&lt;br/&gt;
    ```&lt;/p&gt;
</comment>
                            <comment id="15433146" author="githubbot" created="Tue, 23 Aug 2016 16:30:46 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369#discussion_r75902757&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369#discussion_r75902757&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer010.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,198 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.GenericTypeInfo;&lt;br/&gt;
    +import org.apache.flink.streaming.api.datastream.DataStream;&lt;br/&gt;
    +import org.apache.flink.streaming.api.datastream.DataStreamSink;&lt;br/&gt;
    +import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;&lt;br/&gt;
    +import org.apache.flink.streaming.api.operators.StreamSink;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.FixedPartitioner;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.KafkaPartitioner;&lt;br/&gt;
    +import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchema;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchemaWrapper;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.SerializationSchema;&lt;br/&gt;
    +import org.apache.kafka.clients.producer.ProducerRecord;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util.Properties;&lt;br/&gt;
    +&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * Flink Sink to produce data into a Kafka topic. This producer is compatible with Kafka 0.10.x&lt;br/&gt;
    + *&lt;br/&gt;
    + * Implementation note: This Producer wraps a Flink Kafka 0.9 Producer, overriding only&lt;br/&gt;
    + * the &quot;processElement&quot; / &quot;invoke&quot; method.&lt;br/&gt;
    + */&lt;br/&gt;
    +public class FlinkKafkaProducer010&amp;lt;T&amp;gt; extends StreamSink&amp;lt;T&amp;gt; {&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Flag controlling whether we are writing the Flink record&apos;s timestamp into Kafka.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	private boolean writeTimestampToKafka = false;&lt;br/&gt;
    +&lt;br/&gt;
    +	// ---------------------- &quot;Constructors&quot; for the producer ------------------ //&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to&lt;br/&gt;
    +	 * the topic.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @param inStream The stream to write to Kafka&lt;br/&gt;
    +	 * @param topicId ID of the Kafka topic.&lt;br/&gt;
    +	 * @param serializationSchema User defined serialization schema supporting key/value messages&lt;br/&gt;
    +	 * @param producerConfig Properties with the producer configuration.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public static &amp;lt;T&amp;gt; FlinkKafkaProducer010Configuration writeToKafka(DataStream&amp;lt;T&amp;gt; inStream,&lt;br/&gt;
    +										String topicId,&lt;br/&gt;
    +										KeyedSerializationSchema&amp;lt;T&amp;gt; serializationSchema,&lt;br/&gt;
    +										Properties producerConfig) &lt;/p&gt;
{
    +		return writeToKafka(inStream, topicId, serializationSchema, producerConfig, new FixedPartitioner&amp;lt;T&amp;gt;());
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to&lt;br/&gt;
    +	 * the topic.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @param inStream The stream to write to Kafka&lt;br/&gt;
    +	 * @param topicId ID of the Kafka topic.&lt;br/&gt;
    +	 * @param serializationSchema User defined (keyless) serialization schema.&lt;br/&gt;
    +	 * @param producerConfig Properties with the producer configuration.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public static &amp;lt;T&amp;gt; FlinkKafkaProducer010Configuration writeToKafka(DataStream&amp;lt;T&amp;gt; inStream,&lt;br/&gt;
    +										String topicId,&lt;br/&gt;
    +										SerializationSchema&amp;lt;T&amp;gt; serializationSchema,&lt;br/&gt;
    +										Properties producerConfig) &lt;/p&gt;
{
    +		return writeToKafka(inStream, topicId, new KeyedSerializationSchemaWrapper&amp;lt;&amp;gt;(serializationSchema), producerConfig, new FixedPartitioner&amp;lt;T&amp;gt;());
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to&lt;br/&gt;
    +	 * the topic.&lt;br/&gt;
    +	 *  @param inStream The stream to write to Kafka&lt;br/&gt;
    +	 *  @param topicId The name of the target topic&lt;br/&gt;
    +	 *  @param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages&lt;br/&gt;
    +	 *  @param producerConfig Configuration properties for the KafkaProducer. &apos;bootstrap.servers.&apos; is the only required argument.&lt;br/&gt;
    +	 *  @param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public static &amp;lt;T&amp;gt; FlinkKafkaProducer010Configuration&amp;lt;T&amp;gt; writeToKafka(DataStream&amp;lt;T&amp;gt; inStream,&lt;br/&gt;
    +																String topicId,&lt;br/&gt;
    +																KeyedSerializationSchema&amp;lt;T&amp;gt; serializationSchema,&lt;br/&gt;
    +																Properties producerConfig,&lt;br/&gt;
    +																KafkaPartitioner&amp;lt;T&amp;gt; customPartitioner) &lt;/p&gt;
{
    +		GenericTypeInfo&amp;lt;Object&amp;gt; objectTypeInfo = new GenericTypeInfo&amp;lt;&amp;gt;(Object.class);
    +		FlinkKafkaProducer010&amp;lt;T&amp;gt; kafkaProducer = new FlinkKafkaProducer010&amp;lt;&amp;gt;(topicId, serializationSchema, producerConfig, customPartitioner);
    +		SingleOutputStreamOperator&amp;lt;Object&amp;gt; transformation = inStream.transform(&quot;FlinKafkaProducer 0.10.x&quot;, objectTypeInfo, kafkaProducer);
    +		return new FlinkKafkaProducer010Configuration&amp;lt;&amp;gt;(transformation, kafkaProducer);
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Configuration object returned by the writeToKafka() call.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public static class FlinkKafkaProducer010Configuration&amp;lt;T&amp;gt; extends DataStreamSink&amp;lt;T&amp;gt; {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I&apos;ll update it.&lt;/p&gt;</comment>
                            <comment id="15433148" author="githubbot" created="Tue, 23 Aug 2016 16:31:46 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369#discussion_r75902922&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369#discussion_r75902922&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer010.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,198 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.GenericTypeInfo;&lt;br/&gt;
    +import org.apache.flink.streaming.api.datastream.DataStream;&lt;br/&gt;
    +import org.apache.flink.streaming.api.datastream.DataStreamSink;&lt;br/&gt;
    +import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;&lt;br/&gt;
    +import org.apache.flink.streaming.api.operators.StreamSink;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.FixedPartitioner;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.KafkaPartitioner;&lt;br/&gt;
    +import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchema;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchemaWrapper;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.SerializationSchema;&lt;br/&gt;
    +import org.apache.kafka.clients.producer.ProducerRecord;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util.Properties;&lt;br/&gt;
    +&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * Flink Sink to produce data into a Kafka topic. This producer is compatible with Kafka 0.10.x&lt;br/&gt;
    + *&lt;br/&gt;
    + * Implementation note: This Producer wraps a Flink Kafka 0.9 Producer, overriding only&lt;br/&gt;
    + * the &quot;processElement&quot; / &quot;invoke&quot; method.&lt;br/&gt;
    + */&lt;br/&gt;
    +public class FlinkKafkaProducer010&amp;lt;T&amp;gt; extends StreamSink&amp;lt;T&amp;gt; {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Exactly. The problem is that there is no way of accessing the system&apos;s event time record.&lt;/p&gt;

&lt;p&gt;    Maybe I can try and build something like a hybrid producer that works with both invocation methods.&lt;/p&gt;</comment>
                            <comment id="15433151" author="githubbot" created="Tue, 23 Aug 2016 16:32:46 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369#discussion_r75903103&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369#discussion_r75903103&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,91 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + * &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka.internal;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks;&lt;br/&gt;
    +import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks;&lt;br/&gt;
    +import org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext;&lt;br/&gt;
    +import org.apache.flink.streaming.api.operators.StreamingRuntimeContext;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionState;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedDeserializationSchema;&lt;br/&gt;
    +import org.apache.flink.util.SerializedValue;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.ConsumerRecord;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.KafkaConsumer;&lt;br/&gt;
    +import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util.List;&lt;br/&gt;
    +import java.util.Properties;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * A fetcher that fetches data from Kafka brokers via the Kafka 0.10 consumer API.&lt;br/&gt;
    + * &lt;br/&gt;
    + * @param &amp;lt;T&amp;gt; The type of elements produced by the fetcher.&lt;br/&gt;
    + */&lt;br/&gt;
    +public class Kafka010Fetcher&amp;lt;T&amp;gt; extends Kafka09Fetcher&amp;lt;T&amp;gt; {&lt;br/&gt;
    +&lt;br/&gt;
    +	public Kafka010Fetcher(&lt;br/&gt;
    +			SourceContext&amp;lt;T&amp;gt; sourceContext,&lt;br/&gt;
    +			List&amp;lt;KafkaTopicPartition&amp;gt; assignedPartitions,&lt;br/&gt;
    +			SerializedValue&amp;lt;AssignerWithPeriodicWatermarks&amp;lt;T&amp;gt;&amp;gt; watermarksPeriodic,&lt;br/&gt;
    +			SerializedValue&amp;lt;AssignerWithPunctuatedWatermarks&amp;lt;T&amp;gt;&amp;gt; watermarksPunctuated,&lt;br/&gt;
    +			StreamingRuntimeContext runtimeContext,&lt;br/&gt;
    +			KeyedDeserializationSchema&amp;lt;T&amp;gt; deserializer,&lt;br/&gt;
    +			Properties kafkaProperties,&lt;br/&gt;
    +			long pollTimeout,&lt;br/&gt;
    +			boolean useMetrics) throws Exception&lt;br/&gt;
    +	&lt;/p&gt;
{
    +		super(sourceContext, assignedPartitions, watermarksPeriodic, watermarksPunctuated, runtimeContext, deserializer, kafkaProperties, pollTimeout, useMetrics);
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	protected void assignPartitionsToConsumer(KafkaConsumer&amp;lt;byte[], byte[]&amp;gt; consumer, List&amp;lt;TopicPartition&amp;gt; topicPartitions) &lt;/p&gt;
{
    +		consumer.assign(topicPartitions);
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Emit record Kafka-timestamp aware.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	protected &amp;lt;R&amp;gt; void emitRecord(T record, KafkaTopicPartitionState&amp;lt;TopicPartition&amp;gt; partitionState, long offset, R kafkaRecord) throws Exception {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I agree that this is confusing at the `AbstractFetcher` level. I&apos;ll look into that as well.&lt;/p&gt;</comment>
                            <comment id="15433152" author="githubbot" created="Tue, 23 Aug 2016 16:33:03 +0000"  >&lt;p&gt;Github user rmetzger commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Thank you for the review @tzulitai. I&apos;ll try to find some time soon to look into your comments in detail.&lt;/p&gt;</comment>
                            <comment id="15433195" author="githubbot" created="Tue, 23 Aug 2016 16:56:38 +0000"  >&lt;p&gt;Github user eliaslevy commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    This may be the wrong place to bring this up, but as you are discussing changes to the Kafka connector API, I think it is worth bring it up.  &lt;/p&gt;

&lt;p&gt;    As I&apos;ve pointed out elsewhere, the current connector API makes it difficult to make use of Kafka native serializer or deserializer (`org.apache.kafka.common.&lt;span class=&quot;error&quot;&gt;&amp;#91;Serializer, Deserializer&amp;#93;&lt;/span&gt;`), which can be configured via the Kafka client and producer configs.  &lt;/p&gt;

&lt;p&gt;    The connector code assumes that `ConsummerRecord`s and `ProducerRecord`s are both parametrized as `&amp;lt;byte[], byte[]&amp;gt;`, with the Flink serdes performing the conversion to/from `byte[]`.  This makes it difficult to make use of Confluent&apos;s `KafkaAvroSerializer` and `KafkaAvroDecoder`, which make use of their &lt;span class=&quot;error&quot;&gt;&amp;#91;schema registry&amp;#93;&lt;/span&gt;(&lt;a href=&quot;http://docs.confluent.io/3.0.0/schema-registry/docs/serializer-formatter.html#serializer&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://docs.confluent.io/3.0.0/schema-registry/docs/serializer-formatter.html#serializer&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;    If you are going to change the connector API, it would be good to tackle this issue at the same time to avoid future changes.  The connector should allow the type parametrization of the Kafka consumer and producer, and should make use of a pass through Flink serde by default.&lt;/p&gt;</comment>
                            <comment id="15434589" author="githubbot" created="Wed, 24 Aug 2016 09:31:57 +0000"  >&lt;p&gt;Github user rmetzger commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @eliaslevy, I assume you are referring to &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-4050&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/FLINK-4050&lt;/a&gt;. &lt;br/&gt;
    Its good that you are mentioning the issue again, so I can move it a bit up on my TODO list.&lt;/p&gt;

&lt;p&gt;    I would personally prefer to first add the Kafka 0.10 module in this pull request and then resolve &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-4050&quot; title=&quot;FlinkKafkaProducer API Refactor&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-4050&quot;&gt;FLINK-4050&lt;/a&gt; independently. I know that this might lead to a little bit of duplicate work on the Kafka 0.10 code, but on the other hand its easier to discuss one issue at a time &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15435284" author="githubbot" created="Wed, 24 Aug 2016 16:53:48 +0000"  >&lt;p&gt;Github user eliaslevy commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @rmetzger that&apos;s the one. NP.  I realize breaking it up makes things easier.  I just thought I&apos;d mention it.&lt;/p&gt;</comment>
                            <comment id="15436462" author="githubbot" created="Thu, 25 Aug 2016 08:19:28 +0000"  >&lt;p&gt;Github user rmetzger commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @tzulitai I&apos;ve addressed your comments.&lt;br/&gt;
    The Producer is now &quot;hybrid&quot;: you can use it with both invocation methods.&lt;br/&gt;
    The AbstractFetcher now accepts a long timestamp instead of a record.&lt;/p&gt;</comment>
                            <comment id="15437069" author="githubbot" created="Thu, 25 Aug 2016 15:21:41 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369#discussion_r76264614&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369#discussion_r76264614&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer010.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,399 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.api.common.functions.IterationRuntimeContext;&lt;br/&gt;
    +import org.apache.flink.api.common.functions.RichFunction;&lt;br/&gt;
    +import org.apache.flink.api.common.functions.RuntimeContext;&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.GenericTypeInfo;&lt;br/&gt;
    +import org.apache.flink.configuration.Configuration;&lt;br/&gt;
    +import org.apache.flink.streaming.api.datastream.DataStream;&lt;br/&gt;
    +import org.apache.flink.streaming.api.datastream.DataStreamSink;&lt;br/&gt;
    +import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;&lt;br/&gt;
    +import org.apache.flink.streaming.api.functions.sink.SinkFunction;&lt;br/&gt;
    +import org.apache.flink.streaming.api.operators.StreamSink;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.FixedPartitioner;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.KafkaPartitioner;&lt;br/&gt;
    +import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchema;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchemaWrapper;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.SerializationSchema;&lt;br/&gt;
    +import org.apache.kafka.clients.producer.ProducerRecord;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util.Properties;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBase.getPropertiesFromBrokerList;&lt;br/&gt;
    +&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * Flink Sink to produce data into a Kafka topic. This producer is compatible with Kafka 0.10.x&lt;br/&gt;
    + *&lt;br/&gt;
    + * Implementation note: This producer is a hybrid between a regular regular sink function (a)&lt;br/&gt;
    + * and a custom operator (b).&lt;br/&gt;
    + *&lt;br/&gt;
    + * For (a), the class implements the SinkFunction and RichFunction interfaces.&lt;br/&gt;
    + * For (b), it extends the StreamTask class.&lt;br/&gt;
    + *&lt;br/&gt;
    + * Details about approach (a):&lt;br/&gt;
    + *&lt;br/&gt;
    + *  Pre Kafka 0.10 producers only follow approach (a), allowing users to use the producer using the&lt;br/&gt;
    + *  DataStream.addSink() method.&lt;br/&gt;
    + *  Since the APIs exposed in that variant do not allow accessing the the timestamp attached to the record&lt;br/&gt;
    + *  the Kafka 0.10 producer has a section invocation option, approach (b).&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    &apos;section&apos; --&amp;gt; second?&lt;/p&gt;</comment>
                            <comment id="15437092" author="githubbot" created="Thu, 25 Aug 2016 15:40:01 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369#discussion_r76268268&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369#discussion_r76268268&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer010.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,399 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.api.common.functions.IterationRuntimeContext;&lt;br/&gt;
    +import org.apache.flink.api.common.functions.RichFunction;&lt;br/&gt;
    +import org.apache.flink.api.common.functions.RuntimeContext;&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.GenericTypeInfo;&lt;br/&gt;
    +import org.apache.flink.configuration.Configuration;&lt;br/&gt;
    +import org.apache.flink.streaming.api.datastream.DataStream;&lt;br/&gt;
    +import org.apache.flink.streaming.api.datastream.DataStreamSink;&lt;br/&gt;
    +import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;&lt;br/&gt;
    +import org.apache.flink.streaming.api.functions.sink.SinkFunction;&lt;br/&gt;
    +import org.apache.flink.streaming.api.operators.StreamSink;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.FixedPartitioner;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.KafkaPartitioner;&lt;br/&gt;
    +import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchema;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchemaWrapper;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.SerializationSchema;&lt;br/&gt;
    +import org.apache.kafka.clients.producer.ProducerRecord;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util.Properties;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBase.getPropertiesFromBrokerList;&lt;br/&gt;
    +&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * Flink Sink to produce data into a Kafka topic. This producer is compatible with Kafka 0.10.x&lt;br/&gt;
    + *&lt;br/&gt;
    + * Implementation note: This producer is a hybrid between a regular regular sink function (a)&lt;br/&gt;
    + * and a custom operator (b).&lt;br/&gt;
    + *&lt;br/&gt;
    + * For (a), the class implements the SinkFunction and RichFunction interfaces.&lt;br/&gt;
    + * For (b), it extends the StreamTask class.&lt;br/&gt;
    + *&lt;br/&gt;
    + * Details about approach (a):&lt;br/&gt;
    + *&lt;br/&gt;
    + *  Pre Kafka 0.10 producers only follow approach (a), allowing users to use the producer using the&lt;br/&gt;
    + *  DataStream.addSink() method.&lt;br/&gt;
    + *  Since the APIs exposed in that variant do not allow accessing the the timestamp attached to the record&lt;br/&gt;
    + *  the Kafka 0.10 producer has a section invocation option, approach (b).&lt;br/&gt;
    + *&lt;br/&gt;
    + * Details about approach (b):&lt;br/&gt;
    + *  Kafka 0.10 supports writing the timestamp attached to a record to Kafka. When adding the&lt;br/&gt;
    + *  FlinkKafkaProducer010 using the FlinkKafkaProducer010.writeToKafka() method, the Kafka producer&lt;br/&gt;
    + *  can access the internal record timestamp of the record and write it to Kafka.&lt;br/&gt;
    + *&lt;br/&gt;
    + * All methods and constructors in this class are marked with the approach they are needed for.&lt;br/&gt;
    + */&lt;br/&gt;
    +public class FlinkKafkaProducer010&amp;lt;T&amp;gt; extends StreamSink&amp;lt;T&amp;gt; implements SinkFunction&amp;lt;T&amp;gt;, RichFunction {&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Flag controlling whether we are writing the Flink record&apos;s timestamp into Kafka.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	private boolean writeTimestampToKafka = false;&lt;br/&gt;
    +&lt;br/&gt;
    +	// ---------------------- &quot;Constructors&quot; for timestamp writing ------------------&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to&lt;br/&gt;
    +	 * the topic.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @param inStream The stream to write to Kafka&lt;br/&gt;
    +	 * @param topicId ID of the Kafka topic.&lt;br/&gt;
    +	 * @param serializationSchema User defined serialization schema supporting key/value messages&lt;br/&gt;
    +	 * @param producerConfig Properties with the producer configuration.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public static &amp;lt;T&amp;gt; FlinkKafkaProducer010Configuration writeToKafka(DataStream&amp;lt;T&amp;gt; inStream,&lt;br/&gt;
    +										String topicId,&lt;br/&gt;
    +										KeyedSerializationSchema&amp;lt;T&amp;gt; serializationSchema,&lt;br/&gt;
    +										Properties producerConfig) &lt;/p&gt;
{
    +		return writeToKafka(inStream, topicId, serializationSchema, producerConfig, new FixedPartitioner&amp;lt;T&amp;gt;());
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to&lt;br/&gt;
    +	 * the topic.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @param inStream The stream to write to Kafka&lt;br/&gt;
    +	 * @param topicId ID of the Kafka topic.&lt;br/&gt;
    +	 * @param serializationSchema User defined (keyless) serialization schema.&lt;br/&gt;
    +	 * @param producerConfig Properties with the producer configuration.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public static &amp;lt;T&amp;gt; FlinkKafkaProducer010Configuration writeToKafka(DataStream&amp;lt;T&amp;gt; inStream,&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Now with hybrid invocation methods, would it be reasonable to name this as `writeToKafkaWithTimestamps` so that it&apos;s more meaningful and differentiable from the usual invocation?&lt;/p&gt;</comment>
                            <comment id="15437096" author="githubbot" created="Thu, 25 Aug 2016 15:42:17 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369#discussion_r76268687&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369#discussion_r76268687&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer010.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,399 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.api.common.functions.IterationRuntimeContext;&lt;br/&gt;
    +import org.apache.flink.api.common.functions.RichFunction;&lt;br/&gt;
    +import org.apache.flink.api.common.functions.RuntimeContext;&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.GenericTypeInfo;&lt;br/&gt;
    +import org.apache.flink.configuration.Configuration;&lt;br/&gt;
    +import org.apache.flink.streaming.api.datastream.DataStream;&lt;br/&gt;
    +import org.apache.flink.streaming.api.datastream.DataStreamSink;&lt;br/&gt;
    +import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;&lt;br/&gt;
    +import org.apache.flink.streaming.api.functions.sink.SinkFunction;&lt;br/&gt;
    +import org.apache.flink.streaming.api.operators.StreamSink;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.FixedPartitioner;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.KafkaPartitioner;&lt;br/&gt;
    +import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchema;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchemaWrapper;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.SerializationSchema;&lt;br/&gt;
    +import org.apache.kafka.clients.producer.ProducerRecord;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util.Properties;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBase.getPropertiesFromBrokerList;&lt;br/&gt;
    +&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * Flink Sink to produce data into a Kafka topic. This producer is compatible with Kafka 0.10.x&lt;br/&gt;
    + *&lt;br/&gt;
    + * Implementation note: This producer is a hybrid between a regular regular sink function (a)&lt;br/&gt;
    + * and a custom operator (b).&lt;br/&gt;
    + *&lt;br/&gt;
    + * For (a), the class implements the SinkFunction and RichFunction interfaces.&lt;br/&gt;
    + * For (b), it extends the StreamTask class.&lt;br/&gt;
    + *&lt;br/&gt;
    + * Details about approach (a):&lt;br/&gt;
    + *&lt;br/&gt;
    + *  Pre Kafka 0.10 producers only follow approach (a), allowing users to use the producer using the&lt;br/&gt;
    + *  DataStream.addSink() method.&lt;br/&gt;
    + *  Since the APIs exposed in that variant do not allow accessing the the timestamp attached to the record&lt;br/&gt;
    + *  the Kafka 0.10 producer has a section invocation option, approach (b).&lt;br/&gt;
    + *&lt;br/&gt;
    + * Details about approach (b):&lt;br/&gt;
    + *  Kafka 0.10 supports writing the timestamp attached to a record to Kafka. When adding the&lt;br/&gt;
    + *  FlinkKafkaProducer010 using the FlinkKafkaProducer010.writeToKafka() method, the Kafka producer&lt;br/&gt;
    + *  can access the internal record timestamp of the record and write it to Kafka.&lt;br/&gt;
    + *&lt;br/&gt;
    + * All methods and constructors in this class are marked with the approach they are needed for.&lt;br/&gt;
    + */&lt;br/&gt;
    +public class FlinkKafkaProducer010&amp;lt;T&amp;gt; extends StreamSink&amp;lt;T&amp;gt; implements SinkFunction&amp;lt;T&amp;gt;, RichFunction {&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Flag controlling whether we are writing the Flink record&apos;s timestamp into Kafka.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	private boolean writeTimestampToKafka = false;&lt;br/&gt;
    +&lt;br/&gt;
    +	// ---------------------- &quot;Constructors&quot; for timestamp writing ------------------&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to&lt;br/&gt;
    +	 * the topic.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @param inStream The stream to write to Kafka&lt;br/&gt;
    +	 * @param topicId ID of the Kafka topic.&lt;br/&gt;
    +	 * @param serializationSchema User defined serialization schema supporting key/value messages&lt;br/&gt;
    +	 * @param producerConfig Properties with the producer configuration.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public static &amp;lt;T&amp;gt; FlinkKafkaProducer010Configuration writeToKafka(DataStream&amp;lt;T&amp;gt; inStream,&lt;br/&gt;
    +										String topicId,&lt;br/&gt;
    +										KeyedSerializationSchema&amp;lt;T&amp;gt; serializationSchema,&lt;br/&gt;
    +										Properties producerConfig) &lt;/p&gt;
{
    +		return writeToKafka(inStream, topicId, serializationSchema, producerConfig, new FixedPartitioner&amp;lt;T&amp;gt;());
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to&lt;br/&gt;
    +	 * the topic.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @param inStream The stream to write to Kafka&lt;br/&gt;
    +	 * @param topicId ID of the Kafka topic.&lt;br/&gt;
    +	 * @param serializationSchema User defined (keyless) serialization schema.&lt;br/&gt;
    +	 * @param producerConfig Properties with the producer configuration.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public static &amp;lt;T&amp;gt; FlinkKafkaProducer010Configuration writeToKafka(DataStream&amp;lt;T&amp;gt; inStream,&lt;br/&gt;
    +										String topicId,&lt;br/&gt;
    +										SerializationSchema&amp;lt;T&amp;gt; serializationSchema,&lt;br/&gt;
    +										Properties producerConfig) &lt;/p&gt;
{
    +		return writeToKafka(inStream, topicId, new KeyedSerializationSchemaWrapper&amp;lt;&amp;gt;(serializationSchema), producerConfig, new FixedPartitioner&amp;lt;T&amp;gt;());
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to&lt;br/&gt;
    +	 * the topic.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 *  @param inStream The stream to write to Kafka&lt;br/&gt;
    +	 *  @param topicId The name of the target topic&lt;br/&gt;
    +	 *  @param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages&lt;br/&gt;
    +	 *  @param producerConfig Configuration properties for the KafkaProducer. &apos;bootstrap.servers.&apos; is the only required argument.&lt;br/&gt;
    +	 *  @param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public static &amp;lt;T&amp;gt; FlinkKafkaProducer010Configuration&amp;lt;T&amp;gt; writeToKafka(DataStream&amp;lt;T&amp;gt; inStream,&lt;br/&gt;
    +																String topicId,&lt;br/&gt;
    +																KeyedSerializationSchema&amp;lt;T&amp;gt; serializationSchema,&lt;br/&gt;
    +																Properties producerConfig,&lt;br/&gt;
    +																KafkaPartitioner&amp;lt;T&amp;gt; customPartitioner) &lt;/p&gt;
{
    +
    +		GenericTypeInfo&amp;lt;Object&amp;gt; objectTypeInfo = new GenericTypeInfo&amp;lt;&amp;gt;(Object.class);
    +		FlinkKafkaProducer010&amp;lt;T&amp;gt; kafkaProducer = new FlinkKafkaProducer010&amp;lt;&amp;gt;(topicId, serializationSchema, producerConfig, customPartitioner);
    +		SingleOutputStreamOperator&amp;lt;Object&amp;gt; transformation = inStream.transform(&quot;FlinKafkaProducer 0.10.x&quot;, objectTypeInfo, kafkaProducer);
    +		return new FlinkKafkaProducer010Configuration&amp;lt;&amp;gt;(transformation, kafkaProducer);
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	// ---------------------- Regular constructors w/o timestamp support  ------------------&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to&lt;br/&gt;
    +	 * the topic.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @param brokerList&lt;br/&gt;
    +	 *			Comma separated addresses of the brokers&lt;br/&gt;
    +	 * @param topicId&lt;br/&gt;
    +	 * 			ID of the Kafka topic.&lt;br/&gt;
    +	 * @param serializationSchema&lt;br/&gt;
    +	 * 			User defined (keyless) serialization schema.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public FlinkKafkaProducer010(String brokerList, String topicId, SerializationSchema&amp;lt;T&amp;gt; serializationSchema) &lt;/p&gt;
{
    +		this(topicId, new KeyedSerializationSchemaWrapper&amp;lt;&amp;gt;(serializationSchema), getPropertiesFromBrokerList(brokerList), new FixedPartitioner&amp;lt;T&amp;gt;());
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to&lt;br/&gt;
    +	 * the topic.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @param topicId&lt;br/&gt;
    +	 * 			ID of the Kafka topic.&lt;br/&gt;
    +	 * @param serializationSchema&lt;br/&gt;
    +	 * 			User defined (keyless) serialization schema.&lt;br/&gt;
    +	 * @param producerConfig&lt;br/&gt;
    +	 * 			Properties with the producer configuration.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public FlinkKafkaProducer010(String topicId, SerializationSchema&amp;lt;T&amp;gt; serializationSchema, Properties producerConfig) &lt;/p&gt;
{
    +		this(topicId, new KeyedSerializationSchemaWrapper&amp;lt;&amp;gt;(serializationSchema), producerConfig, new FixedPartitioner&amp;lt;T&amp;gt;());
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to&lt;br/&gt;
    +	 * the topic.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @param topicId The topic to write data to&lt;br/&gt;
    +	 * @param serializationSchema A (keyless) serializable serialization schema for turning user objects into a kafka-consumable byte[]&lt;br/&gt;
    +	 * @param producerConfig Configuration properties for the KafkaProducer. &apos;bootstrap.servers.&apos; is the only required argument.&lt;br/&gt;
    +	 * @param customPartitioner A serializable partitioner for assigning messages to Kafka partitions (when passing null, we&apos;ll use Kafka&apos;s partitioner)&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public FlinkKafkaProducer010(String topicId, SerializationSchema&amp;lt;T&amp;gt; serializationSchema, Properties producerConfig, KafkaPartitioner&amp;lt;T&amp;gt; customPartitioner) {&lt;br/&gt;
    +		this(topicId, new KeyedSerializationSchemaWrapper&amp;lt;&amp;gt;(serializationSchema), producerConfig, customPartitioner);&lt;br/&gt;
    +&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    nit: unnecessary empty line&lt;/p&gt;</comment>
                            <comment id="15437151" author="githubbot" created="Thu, 25 Aug 2016 16:13:40 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369#discussion_r76274826&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369#discussion_r76274826&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java &amp;#8212;&lt;br/&gt;
    @@ -60,12 +60,17 @@ protected void assignPartitionsToConsumer(KafkaConsumer&amp;lt;byte[], byte[]&amp;gt; consumer&lt;br/&gt;
     		consumer.assign(topicPartitions);&lt;br/&gt;
     	}&lt;/p&gt;

&lt;p&gt;    +	@Override&lt;br/&gt;
    +	protected void emitRecord(T record, KafkaTopicPartitionState&amp;lt;TopicPartition&amp;gt; partition, long offset, ConsumerRecord consumerRecord) throws Exception &lt;/p&gt;
{
    +		// pass timestamp
    +		super.emitRecord(record, partition, offset, consumerRecord.timestamp());
    +	}
&lt;p&gt;    +&lt;br/&gt;
     	/**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Emit record Kafka-timestamp aware.&lt;br/&gt;
     	 */&lt;br/&gt;
     	@Override&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;protected &amp;lt;R&amp;gt; void emitRecord(T record, KafkaTopicPartitionState&amp;lt;TopicPartition&amp;gt; partitionState, long offset, R kafkaRecord) throws Exception {&lt;/li&gt;
	&lt;li&gt;long timestamp = ((ConsumerRecord) kafkaRecord).timestamp();&lt;br/&gt;
    +	protected void emitRecord(T record, KafkaTopicPartitionState&amp;lt;TopicPartition&amp;gt; partitionState, long offset, long timestamp) throws Exception {&lt;br/&gt;
     		if (timestampWatermarkMode == NO_TIMESTAMPS_WATERMARKS) {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Is it possible to let `AbstractFetcher#emitRecord` determine whether to call `collectWithTimestamp` or `collect` depending on the provided timestamp (== `Long.MIN_VALUE`)? Then, we won&apos;t need to override the base `emitRecord` here, correct?&lt;/p&gt;</comment>
                            <comment id="15437159" author="githubbot" created="Thu, 25 Aug 2016 16:18:42 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369#discussion_r76275805&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369#discussion_r76275805&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.8/src/main/java/org/apache/flink/streaming/connectors/kafka/internals/SimpleConsumerThread.java &amp;#8212;&lt;br/&gt;
    @@ -376,7 +376,7 @@ else if (partitionsRemoved) &lt;/p&gt;
{
     								continue partitionsLoop;
     							}

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;owner.emitRecord(value, currentPartition, offset, msg);&lt;br/&gt;
    +							owner.emitRecord(value, currentPartition, offset, Long.MAX_VALUE);
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    I think this is supposed to give a `Long.MIN_VALUE` instead of MAX? &lt;/p&gt;</comment>
                            <comment id="15437160" author="githubbot" created="Thu, 25 Aug 2016 16:19:10 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369#discussion_r76275879&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369#discussion_r76275879&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09Fetcher.java &amp;#8212;&lt;br/&gt;
    @@ -262,6 +262,10 @@ public void run() {&lt;br/&gt;
     		}&lt;br/&gt;
     	}&lt;/p&gt;

&lt;p&gt;    +	// Kafka09Fetcher ignores the timestamp.&lt;br/&gt;
    +	protected void emitRecord(T record, KafkaTopicPartitionState&amp;lt;TopicPartition&amp;gt; partition, long offset, ConsumerRecord consumerRecord) throws Exception {&lt;br/&gt;
    +		emitRecord(record, partition, offset, Long.MAX_VALUE);&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Same here: I think this is supposed to give a `Long.MIN_VALUE` instead of MAX?&lt;/p&gt;</comment>
                            <comment id="15437170" author="githubbot" created="Thu, 25 Aug 2016 16:25:52 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369#discussion_r76277062&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369#discussion_r76277062&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/internals/AbstractFetcherTimestampsTest.java &amp;#8212;&lt;br/&gt;
    @@ -135,45 +135,45 @@ public void testPeriodicWatermarks() throws Exception {&lt;br/&gt;
     		// elements generate a watermark if the timestamp is a multiple of three&lt;/p&gt;

&lt;p&gt;     		// elements for partition 1&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;fetcher.emitRecord(1L, part1, 1L, new ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;(testTopic, 7, new byte[]
{0}, 1L));&lt;br/&gt;
    -		fetcher.emitRecord(2L, part1, 2L, new ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;(testTopic, 7, new byte[]{0}
&lt;p&gt;, 2L));&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;fetcher.emitRecord(3L, part1, 3L, new ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;(testTopic, 7, new byte[]
{0}, 3L));&lt;br/&gt;
    +		fetcher.emitRecord(1L, part1, 1L, Long.MAX_VALUE);&lt;br/&gt;
    +		fetcher.emitRecord(2L, part1, 2L, Long.MAX_VALUE);&lt;br/&gt;
    +		fetcher.emitRecord(3L, part1, 3L, Long.MAX_VALUE);&lt;br/&gt;
     		assertEquals(3L, sourceContext.getLatestElement().getValue().longValue());&lt;br/&gt;
     		assertEquals(3L, sourceContext.getLatestElement().getTimestamp());&lt;br/&gt;
     &lt;br/&gt;
     		// elements for partition 2&lt;br/&gt;
    -		fetcher.emitRecord(12L, part2, 1L, new ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;(testTopic, 13, new byte[]{0}
&lt;p&gt;, 1L));&lt;br/&gt;
    +		fetcher.emitRecord(12L, part2, 1L, Long.MAX_VALUE);&lt;br/&gt;
     		assertEquals(12L, sourceContext.getLatestElement().getValue().longValue());&lt;br/&gt;
     		assertEquals(12L, sourceContext.getLatestElement().getTimestamp());&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     		// elements for partition 3&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;fetcher.emitRecord(101L, part3, 1L, new ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;(testTopic, 21, new byte[]
{0}, 1L));&lt;br/&gt;
    -		fetcher.emitRecord(102L, part3, 2L, new ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;(testTopic, 21, new byte[]{0}
&lt;p&gt;, 2L));&lt;br/&gt;
    +		fetcher.emitRecord(101L, part3, 1L, Long.MAX_VALUE);&lt;br/&gt;
    +		fetcher.emitRecord(102L, part3, 2L, Long.MAX_VALUE);&lt;br/&gt;
     		assertEquals(102L, sourceContext.getLatestElement().getValue().longValue());&lt;br/&gt;
     		assertEquals(102L, sourceContext.getLatestElement().getTimestamp());&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     		// now, we should have a watermark (this blocks until the periodic thread emitted the watermark)&lt;br/&gt;
     		assertEquals(3L, sourceContext.getLatestWatermark().getTimestamp());&lt;/p&gt;

&lt;p&gt;     		// advance partition 3&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;fetcher.emitRecord(1003L, part3, 3L, new ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;(testTopic, 21, new byte[]
{0}, 3L));&lt;br/&gt;
    -		fetcher.emitRecord(1004L, part3, 4L, new ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;(testTopic, 21, new byte[]{0}
&lt;p&gt;, 4L));&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;fetcher.emitRecord(1005L, part3, 5L, new ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;(testTopic, 21, new byte[]
{0}, 5L));&lt;br/&gt;
    +		fetcher.emitRecord(1003L, part3, 3L, Long.MAX_VALUE);&lt;br/&gt;
    +		fetcher.emitRecord(1004L, part3, 4L, Long.MAX_VALUE);&lt;br/&gt;
    +		fetcher.emitRecord(1005L, part3, 5L, Long.MAX_VALUE);&lt;br/&gt;
     		assertEquals(1005L, sourceContext.getLatestElement().getValue().longValue());&lt;br/&gt;
     		assertEquals(1005L, sourceContext.getLatestElement().getTimestamp());&lt;br/&gt;
     &lt;br/&gt;
     		// advance partition 1 beyond partition 2 - this bumps the watermark&lt;br/&gt;
    -		fetcher.emitRecord(30L, part1, 4L, new ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;(testTopic, 7, new byte[]{0}
&lt;p&gt;, 4L));&lt;br/&gt;
    +		fetcher.emitRecord(30L, part1, 4L, Long.MAX_VALUE);&lt;br/&gt;
     		assertEquals(30L, sourceContext.getLatestElement().getValue().longValue());&lt;br/&gt;
     		assertEquals(30L, sourceContext.getLatestElement().getTimestamp());&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     		// this blocks until the periodic thread emitted the watermark&lt;br/&gt;
     		assertEquals(12L, sourceContext.getLatestWatermark().getTimestamp());&lt;/p&gt;

&lt;p&gt;     		// advance partition 2 again - this bumps the watermark&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;fetcher.emitRecord(13L, part2, 2L, new ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;(testTopic, 13, new byte[]
{0}, 2L));&lt;br/&gt;
    -		fetcher.emitRecord(14L, part2, 3L, new ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;(testTopic, 13, new byte[]{0}
&lt;p&gt;, 3L));&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;fetcher.emitRecord(15L, part2, 3L, new ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;(testTopic, 13, new byte[]
{0}
&lt;p&gt;, 3L));&lt;br/&gt;
    +		fetcher.emitRecord(13L, part2, 2L, Long.MAX_VALUE);&lt;br/&gt;
    +		fetcher.emitRecord(14L, part2, 3L, Long.MAX_VALUE);&lt;br/&gt;
    +		fetcher.emitRecord(15L, part2, 3L, Long.MAX_VALUE);&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Same here: I think these are supposed to give a `Long.MIN_VALUE` instead of MAX?&lt;/p&gt;</comment>
                            <comment id="15437224" author="githubbot" created="Thu, 25 Aug 2016 17:00:47 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Thanks Robert for addressing my comments &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;    Overall, I like the new hybrid producer approach. However, I&apos;m still curious whether or not it is possible / reasonable to drop the `FlinkKafkaProducer010Configuration` return type of invocation (b), and let both invocation methods return `FlinkKafkaProducer010` instead. So,&lt;/p&gt;

&lt;p&gt;    ```&lt;br/&gt;
    FlinkKafkaProducer010 kafka = new FlinkKafkaProducer010(...)&lt;br/&gt;
    // or FlinkKafkaProducer010 kafka = FlinkKafkaProducer010.writeToKafkaWithTimestamps(...) for timestamp support&lt;/p&gt;

&lt;p&gt;    // setter config methods directly done on the FlinkKafkaProducer010 instance regardless of (a) or (b)&lt;br/&gt;
    kafka.setLogFailuresOnly(true)&lt;br/&gt;
    kafka.setFlushOnCheckpoint(true)&lt;br/&gt;
    kafka.setWriteTimestampToKafka(true) // would not have effect if original invocation method (a) was used&lt;br/&gt;
    ```&lt;/p&gt;

&lt;p&gt;    But we&apos;ll need to be bit hacky in `invokeInternal(element, elementTimestamp)`, something like only letting the given `timestamp` to `ProducerRecord` be non-null if `writeTimestampToKafka &amp;amp;&amp;amp; elementTimestamp != Long.MIN_VALUE`.&lt;/p&gt;

&lt;p&gt;    What do you think?&lt;/p&gt;</comment>
                            <comment id="15501922" author="githubbot" created="Mon, 19 Sep 2016 00:48:14 +0000"  >&lt;p&gt;Github user nemccarthy commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    +1 for this pr&lt;/p&gt;</comment>
                            <comment id="15504506" author="githubbot" created="Mon, 19 Sep 2016 19:59:11 +0000"  >&lt;p&gt;Github user cjstehno commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Any thoughts on when this might make it into a release? We are having issues running Flink with Kafka 0.10 and would like to have an idea of whether we can/should wait for this or pull it and try building our own. Thanks.&lt;/p&gt;</comment>
                            <comment id="15507774" author="chobeat" created="Tue, 20 Sep 2016 20:57:20 +0000"  >&lt;p&gt;The PR for kerberos integration for Kafka 0.9 has been merged recently. I don&apos;t know how does it work but maybe the work to make it compatible with this component should be done in this PR so we don&apos;t have a partial support for Kerberos on Kafka. &lt;/p&gt;</comment>
                            <comment id="15508517" author="githubbot" created="Wed, 21 Sep 2016 02:52:16 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Looks like we need to rebase this PR on the recently merged Kerberos support.&lt;/p&gt;</comment>
                            <comment id="15508526" author="githubbot" created="Wed, 21 Sep 2016 02:57:08 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @cjstehno I would expect this to be in the 1.2.0 major release, which would probably be ~2 months from now according to Flink&apos;s past release cycle. The Flink community usually doesn&apos;t release major new features like this between minor bugfix releases.&lt;/p&gt;</comment>
                            <comment id="15523269" author="githubbot" created="Mon, 26 Sep 2016 14:46:05 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369#discussion_r80491475&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369#discussion_r80491475&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer010.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,399 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.api.common.functions.IterationRuntimeContext;&lt;br/&gt;
    +import org.apache.flink.api.common.functions.RichFunction;&lt;br/&gt;
    +import org.apache.flink.api.common.functions.RuntimeContext;&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.GenericTypeInfo;&lt;br/&gt;
    +import org.apache.flink.configuration.Configuration;&lt;br/&gt;
    +import org.apache.flink.streaming.api.datastream.DataStream;&lt;br/&gt;
    +import org.apache.flink.streaming.api.datastream.DataStreamSink;&lt;br/&gt;
    +import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;&lt;br/&gt;
    +import org.apache.flink.streaming.api.functions.sink.SinkFunction;&lt;br/&gt;
    +import org.apache.flink.streaming.api.operators.StreamSink;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.FixedPartitioner;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.KafkaPartitioner;&lt;br/&gt;
    +import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchema;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchemaWrapper;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.SerializationSchema;&lt;br/&gt;
    +import org.apache.kafka.clients.producer.ProducerRecord;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util.Properties;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBase.getPropertiesFromBrokerList;&lt;br/&gt;
    +&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * Flink Sink to produce data into a Kafka topic. This producer is compatible with Kafka 0.10.x&lt;br/&gt;
    + *&lt;br/&gt;
    + * Implementation note: This producer is a hybrid between a regular regular sink function (a)&lt;br/&gt;
    + * and a custom operator (b).&lt;br/&gt;
    + *&lt;br/&gt;
    + * For (a), the class implements the SinkFunction and RichFunction interfaces.&lt;br/&gt;
    + * For (b), it extends the StreamTask class.&lt;br/&gt;
    + *&lt;br/&gt;
    + * Details about approach (a):&lt;br/&gt;
    + *&lt;br/&gt;
    + *  Pre Kafka 0.10 producers only follow approach (a), allowing users to use the producer using the&lt;br/&gt;
    + *  DataStream.addSink() method.&lt;br/&gt;
    + *  Since the APIs exposed in that variant do not allow accessing the the timestamp attached to the record&lt;br/&gt;
    + *  the Kafka 0.10 producer has a section invocation option, approach (b).&lt;br/&gt;
    + *&lt;br/&gt;
    + * Details about approach (b):&lt;br/&gt;
    + *  Kafka 0.10 supports writing the timestamp attached to a record to Kafka. When adding the&lt;br/&gt;
    + *  FlinkKafkaProducer010 using the FlinkKafkaProducer010.writeToKafka() method, the Kafka producer&lt;br/&gt;
    + *  can access the internal record timestamp of the record and write it to Kafka.&lt;br/&gt;
    + *&lt;br/&gt;
    + * All methods and constructors in this class are marked with the approach they are needed for.&lt;br/&gt;
    + */&lt;br/&gt;
    +public class FlinkKafkaProducer010&amp;lt;T&amp;gt; extends StreamSink&amp;lt;T&amp;gt; implements SinkFunction&amp;lt;T&amp;gt;, RichFunction {&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Flag controlling whether we are writing the Flink record&apos;s timestamp into Kafka.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	private boolean writeTimestampToKafka = false;&lt;br/&gt;
    +&lt;br/&gt;
    +	// ---------------------- &quot;Constructors&quot; for timestamp writing ------------------&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to&lt;br/&gt;
    +	 * the topic.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @param inStream The stream to write to Kafka&lt;br/&gt;
    +	 * @param topicId ID of the Kafka topic.&lt;br/&gt;
    +	 * @param serializationSchema User defined serialization schema supporting key/value messages&lt;br/&gt;
    +	 * @param producerConfig Properties with the producer configuration.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public static &amp;lt;T&amp;gt; FlinkKafkaProducer010Configuration writeToKafka(DataStream&amp;lt;T&amp;gt; inStream,&lt;br/&gt;
    +										String topicId,&lt;br/&gt;
    +										KeyedSerializationSchema&amp;lt;T&amp;gt; serializationSchema,&lt;br/&gt;
    +										Properties producerConfig) &lt;/p&gt;
{
    +		return writeToKafka(inStream, topicId, serializationSchema, producerConfig, new FixedPartitioner&amp;lt;T&amp;gt;());
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to&lt;br/&gt;
    +	 * the topic.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @param inStream The stream to write to Kafka&lt;br/&gt;
    +	 * @param topicId ID of the Kafka topic.&lt;br/&gt;
    +	 * @param serializationSchema User defined (keyless) serialization schema.&lt;br/&gt;
    +	 * @param producerConfig Properties with the producer configuration.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public static &amp;lt;T&amp;gt; FlinkKafkaProducer010Configuration writeToKafka(DataStream&amp;lt;T&amp;gt; inStream,&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Its a good idea, I&apos;ll rename the methods&lt;/p&gt;</comment>
                            <comment id="15523327" author="githubbot" created="Mon, 26 Sep 2016 15:08:05 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369#discussion_r80496915&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369#discussion_r80496915&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java &amp;#8212;&lt;br/&gt;
    @@ -60,12 +60,17 @@ protected void assignPartitionsToConsumer(KafkaConsumer&amp;lt;byte[], byte[]&amp;gt; consumer&lt;br/&gt;
     		consumer.assign(topicPartitions);&lt;br/&gt;
     	}&lt;/p&gt;

&lt;p&gt;    +	@Override&lt;br/&gt;
    +	protected void emitRecord(T record, KafkaTopicPartitionState&amp;lt;TopicPartition&amp;gt; partition, long offset, ConsumerRecord consumerRecord) throws Exception &lt;/p&gt;
{
    +		// pass timestamp
    +		super.emitRecord(record, partition, offset, consumerRecord.timestamp());
    +	}
&lt;p&gt;    +&lt;br/&gt;
     	/**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Emit record Kafka-timestamp aware.&lt;br/&gt;
     	 */&lt;br/&gt;
     	@Override&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;protected &amp;lt;R&amp;gt; void emitRecord(T record, KafkaTopicPartitionState&amp;lt;TopicPartition&amp;gt; partitionState, long offset, R kafkaRecord) throws Exception {&lt;/li&gt;
	&lt;li&gt;long timestamp = ((ConsumerRecord) kafkaRecord).timestamp();&lt;br/&gt;
    +	protected void emitRecord(T record, KafkaTopicPartitionState&amp;lt;TopicPartition&amp;gt; partitionState, long offset, long timestamp) throws Exception {&lt;br/&gt;
     		if (timestampWatermarkMode == NO_TIMESTAMPS_WATERMARKS) {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Yes, I&apos;ll do that.&lt;/p&gt;</comment>
                            <comment id="15523334" author="githubbot" created="Mon, 26 Sep 2016 15:11:18 +0000"  >&lt;p&gt;Github user rmetzger commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I&apos;m currently working on rebasing the PR and addressing the comments.&lt;/p&gt;</comment>
                            <comment id="15523695" author="githubbot" created="Mon, 26 Sep 2016 17:34:45 +0000"  >&lt;p&gt;Github user rmetzger commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @tzulitai I addressed all your comments except the one relating `FlinkKafkaProducer010Configuration`: I had a quick offline discussion with @StephanEwen about the issue and he suggested to add the timestamp to the regular sink interface.&lt;br/&gt;
    But I would like to make that change separate from this one, and merge the Kafka 0.10. support as-is. This will make it easier for people to try it out now and provide us with feedback. Also, I think some other Kafka related pull requests are blocked on this one.&lt;/p&gt;

&lt;p&gt;    @tzulitai could you do a final pass over the changes. If you agree, I&apos;d like to merge it afterwards.&lt;/p&gt;</comment>
                            <comment id="15524939" author="githubbot" created="Tue, 27 Sep 2016 03:33:44 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @rmetzger Thanks for addressing the comments! Did a final pass, and the changes look good to me.&lt;br/&gt;
    I agree with merging the connector as is. Adding the timestamp to the regular sink interface seems like a good long term solution.&lt;/p&gt;

&lt;p&gt;    +1 to merge once travis turns green &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15525262" author="githubbot" created="Tue, 27 Sep 2016 06:44:16 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369#discussion_r80628475&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369#discussion_r80628475&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer010.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,398 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.api.common.functions.IterationRuntimeContext;&lt;br/&gt;
    +import org.apache.flink.api.common.functions.RichFunction;&lt;br/&gt;
    +import org.apache.flink.api.common.functions.RuntimeContext;&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.GenericTypeInfo;&lt;br/&gt;
    +import org.apache.flink.configuration.Configuration;&lt;br/&gt;
    +import org.apache.flink.streaming.api.datastream.DataStream;&lt;br/&gt;
    +import org.apache.flink.streaming.api.datastream.DataStreamSink;&lt;br/&gt;
    +import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;&lt;br/&gt;
    +import org.apache.flink.streaming.api.functions.sink.SinkFunction;&lt;br/&gt;
    +import org.apache.flink.streaming.api.operators.StreamSink;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.FixedPartitioner;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.KafkaPartitioner;&lt;br/&gt;
    +import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchema;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchemaWrapper;&lt;br/&gt;
    +import org.apache.flink.streaming.util.serialization.SerializationSchema;&lt;br/&gt;
    +import org.apache.kafka.clients.producer.ProducerRecord;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util.Properties;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBase.getPropertiesFromBrokerList;&lt;br/&gt;
    +&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * Flink Sink to produce data into a Kafka topic. This producer is compatible with Kafka 0.10.x&lt;br/&gt;
    + *&lt;br/&gt;
    + * Implementation note: This producer is a hybrid between a regular regular sink function (a)&lt;br/&gt;
    + * and a custom operator (b).&lt;br/&gt;
    + *&lt;br/&gt;
    + * For (a), the class implements the SinkFunction and RichFunction interfaces.&lt;br/&gt;
    + * For (b), it extends the StreamTask class.&lt;br/&gt;
    + *&lt;br/&gt;
    + * Details about approach (a):&lt;br/&gt;
    + *&lt;br/&gt;
    + *  Pre Kafka 0.10 producers only follow approach (a), allowing users to use the producer using the&lt;br/&gt;
    + *  DataStream.addSink() method.&lt;br/&gt;
    + *  Since the APIs exposed in that variant do not allow accessing the the timestamp attached to the record&lt;br/&gt;
    + *  the Kafka 0.10 producer has a second invocation option, approach (b).&lt;br/&gt;
    + *&lt;br/&gt;
    + * Details about approach (b):&lt;br/&gt;
    + *  Kafka 0.10 supports writing the timestamp attached to a record to Kafka. When adding the&lt;br/&gt;
    + *  FlinkKafkaProducer010 using the FlinkKafkaProducer010.writeToKafkaWithTimestamps() method, the Kafka producer&lt;br/&gt;
    + *  can access the internal record timestamp of the record and write it to Kafka.&lt;br/&gt;
    + *&lt;br/&gt;
    + * All methods and constructors in this class are marked with the approach they are needed for.&lt;br/&gt;
    + */&lt;br/&gt;
    +public class FlinkKafkaProducer010&amp;lt;T&amp;gt; extends StreamSink&amp;lt;T&amp;gt; implements SinkFunction&amp;lt;T&amp;gt;, RichFunction {&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Flag controlling whether we are writing the Flink record&apos;s timestamp into Kafka.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	private boolean writeTimestampToKafka = false;&lt;br/&gt;
    +&lt;br/&gt;
    +	// ---------------------- &quot;Constructors&quot; for timestamp writing ------------------&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to&lt;br/&gt;
    +	 * the topic.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @param inStream The stream to write to Kafka&lt;br/&gt;
    +	 * @param topicId ID of the Kafka topic.&lt;br/&gt;
    +	 * @param serializationSchema User defined serialization schema supporting key/value messages&lt;br/&gt;
    +	 * @param producerConfig Properties with the producer configuration.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public static &amp;lt;T&amp;gt; FlinkKafkaProducer010Configuration writeToKafkaWithTimestamps(DataStream&amp;lt;T&amp;gt; inStream,&lt;br/&gt;
    +																					String topicId,&lt;br/&gt;
    +																					KeyedSerializationSchema&amp;lt;T&amp;gt; serializationSchema,&lt;br/&gt;
    +																					Properties producerConfig) &lt;/p&gt;
{
    +		return writeToKafkaWithTimestamps(inStream, topicId, serializationSchema, producerConfig, new FixedPartitioner&amp;lt;T&amp;gt;());
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to&lt;br/&gt;
    +	 * the topic.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @param inStream The stream to write to Kafka&lt;br/&gt;
    +	 * @param topicId ID of the Kafka topic.&lt;br/&gt;
    +	 * @param serializationSchema User defined (keyless) serialization schema.&lt;br/&gt;
    +	 * @param producerConfig Properties with the producer configuration.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public static &amp;lt;T&amp;gt; FlinkKafkaProducer010Configuration writeToKafkaWithTimestamps(DataStream&amp;lt;T&amp;gt; inStream,&lt;br/&gt;
    +																					String topicId,&lt;br/&gt;
    +																					SerializationSchema&amp;lt;T&amp;gt; serializationSchema,&lt;br/&gt;
    +																					Properties producerConfig) &lt;/p&gt;
{
    +		return writeToKafkaWithTimestamps(inStream, topicId, new KeyedSerializationSchemaWrapper&amp;lt;&amp;gt;(serializationSchema), producerConfig, new FixedPartitioner&amp;lt;T&amp;gt;());
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to&lt;br/&gt;
    +	 * the topic.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 *  @param inStream The stream to write to Kafka&lt;br/&gt;
    +	 *  @param topicId The name of the target topic&lt;br/&gt;
    +	 *  @param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages&lt;br/&gt;
    +	 *  @param producerConfig Configuration properties for the KafkaProducer. &apos;bootstrap.servers.&apos; is the only required argument.&lt;br/&gt;
    +	 *  @param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public static &amp;lt;T&amp;gt; FlinkKafkaProducer010Configuration&amp;lt;T&amp;gt; writeToKafkaWithTimestamps(DataStream&amp;lt;T&amp;gt; inStream,&lt;br/&gt;
    +																					   String topicId,&lt;br/&gt;
    +																					   KeyedSerializationSchema&amp;lt;T&amp;gt; serializationSchema,&lt;br/&gt;
    +																					   Properties producerConfig,&lt;br/&gt;
    +																					   KafkaPartitioner&amp;lt;T&amp;gt; customPartitioner) {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Checkstyle failed for these few lines here, have leading spaces &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/tongue.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15525277" author="githubbot" created="Tue, 27 Sep 2016 06:51:14 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369#discussion_r80629075&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369#discussion_r80629075&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer010.java &amp;#8212;&lt;br/&gt;
    @@ -83,11 +83,11 @@&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@param serializationSchema User defined serialization schema supporting key/value messages&lt;/li&gt;
	&lt;li&gt;@param producerConfig Properties with the producer configuration.&lt;br/&gt;
     	 */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static &amp;lt;T&amp;gt; FlinkKafkaProducer010Configuration writeToKafka(DataStream&amp;lt;T&amp;gt; inStream,&lt;/li&gt;
	&lt;li&gt;String topicId,&lt;/li&gt;
	&lt;li&gt;KeyedSerializationSchema&amp;lt;T&amp;gt; serializationSchema,&lt;/li&gt;
	&lt;li&gt;Properties producerConfig) {&lt;/li&gt;
	&lt;li&gt;return writeToKafka(inStream, topicId, serializationSchema, producerConfig, new FixedPartitioner&amp;lt;T&amp;gt;());&lt;br/&gt;
    +	public static &amp;lt;T&amp;gt; FlinkKafkaProducer010Configuration writeToKafkaWithTimestamps(DataStream&amp;lt;T&amp;gt; inStream,
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Add the generic type parameter `T` to `FlinkKafkaProducer010Configuration` here too?&lt;/p&gt;</comment>
                            <comment id="15525278" author="githubbot" created="Tue, 27 Sep 2016 06:51:14 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369#discussion_r80629176&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369#discussion_r80629176&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer010.java &amp;#8212;&lt;br/&gt;
    @@ -102,11 +102,11 @@&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@param serializationSchema User defined (keyless) serialization schema.&lt;/li&gt;
	&lt;li&gt;@param producerConfig Properties with the producer configuration.&lt;br/&gt;
     	 */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static &amp;lt;T&amp;gt; FlinkKafkaProducer010Configuration writeToKafka(DataStream&amp;lt;T&amp;gt; inStream,&lt;/li&gt;
	&lt;li&gt;String topicId,&lt;/li&gt;
	&lt;li&gt;SerializationSchema&amp;lt;T&amp;gt; serializationSchema,&lt;/li&gt;
	&lt;li&gt;Properties producerConfig) {&lt;/li&gt;
	&lt;li&gt;return writeToKafka(inStream, topicId, new KeyedSerializationSchemaWrapper&amp;lt;&amp;gt;(serializationSchema), producerConfig, new FixedPartitioner&amp;lt;T&amp;gt;());&lt;br/&gt;
    +	public static &amp;lt;T&amp;gt; FlinkKafkaProducer010Configuration writeToKafkaWithTimestamps(DataStream&amp;lt;T&amp;gt; inStream,
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Add the generic type parameter `T` to `FlinkKafkaProducer010Configuration` here too?&lt;/p&gt;</comment>
                            <comment id="15525284" author="githubbot" created="Tue, 27 Sep 2016 06:53:12 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Just found some minor issues that can be fixed when merging.&lt;/p&gt;</comment>
                            <comment id="15562320" author="githubbot" created="Mon, 10 Oct 2016 13:34:51 +0000"  >&lt;p&gt;Github user rmetzger commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Thank you for the review. I&apos;ll address your comments, rebase again, test it, and if it turns green merge it &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15562537" author="githubbot" created="Mon, 10 Oct 2016 15:07:44 +0000"  >&lt;p&gt;Github user rmetzger commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Tests are running &lt;a href=&quot;https://travis-ci.org/rmetzger/flink/builds/166441047&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://travis-ci.org/rmetzger/flink/builds/166441047&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15564842" author="rmetzger" created="Tue, 11 Oct 2016 08:06:09 +0000"  >&lt;p&gt;Kafka 0.10 support added in &lt;a href=&quot;http://git-wip-us.apache.org/repos/asf/flink/commit/63859c64&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://git-wip-us.apache.org/repos/asf/flink/commit/63859c64&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15564843" author="githubbot" created="Tue, 11 Oct 2016 08:06:18 +0000"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2369&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2369&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15564844" author="githubbot" created="Tue, 11 Oct 2016 08:06:18 +0000"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2231&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2231&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15564862" author="githubbot" created="Tue, 11 Oct 2016 08:14:34 +0000"  >&lt;p&gt;Github user rmetzger commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2231&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2231&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @radekg I&apos;ve added Kafka 0.10 support to Flink, that&apos;s why I closed this pull request. My change preserved your commit from this pull request. Thank you for the contribution!&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="13005530">FLINK-4629</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 6 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2z6pj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>