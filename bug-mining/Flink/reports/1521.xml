<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 20:27:11 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-5487] Proper at-least-once support for ElasticsearchSink</title>
                <link>https://issues.apache.org/jira/browse/FLINK-5487</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;p&gt;Discussion in ML: &lt;a href=&quot;http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Fault-tolerance-guarantees-of-Elasticsearch-sink-in-flink-elasticsearch2-td10982.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Fault-tolerance-guarantees-of-Elasticsearch-sink-in-flink-elasticsearch2-td10982.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Currently, the Elasticsearch Sink actually doesn&apos;t offer any guarantees for message delivery.&lt;/p&gt;

&lt;p&gt;For proper support of at-least-once, the sink will need to participate in Flink&apos;s checkpointing: when snapshotting is triggered at the &lt;tt&gt;ElasticsearchSink&lt;/tt&gt;, we need to synchronize on the pending ES requests by flushing the internal bulk processor. For temporary ES failures (see &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-5122&quot; title=&quot;Elasticsearch Sink loses documents when cluster has high load&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-5122&quot;&gt;&lt;del&gt;FLINK-5122&lt;/del&gt;&lt;/a&gt;) that may happen on the flush, we should retry them before returning from snapshotting and acking the checkpoint. If there are non-temporary ES failures on the flush, the current snapshot should fail.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13034821">FLINK-5487</key>
            <summary>Proper at-least-once support for ElasticsearchSink</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="tzulitai">Tzu-Li (Gordon) Tai</assignee>
                                    <reporter username="tzulitai">Tzu-Li (Gordon) Tai</reporter>
                        <labels>
                    </labels>
                <created>Fri, 13 Jan 2017 21:11:05 +0000</created>
                <updated>Fri, 24 Feb 2017 15:40:55 +0000</updated>
                            <resolved>Fri, 24 Feb 2017 15:40:55 +0000</resolved>
                                                    <fixVersion>1.3.0</fixVersion>
                                    <component>Connectors / Common</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="15822356" author="tzulitai" created="Fri, 13 Jan 2017 21:15:29 +0000"  >&lt;p&gt;We should block this issue until &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-4988&quot; title=&quot;Elasticsearch 5.x support&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-4988&quot;&gt;&lt;del&gt;FLINK-4988&lt;/del&gt;&lt;/a&gt;, which includes restructuring for the ES connectors as part of the fix, is resolved so at-least-once support can be simultaneously included in all ES versions.&lt;/p&gt;</comment>
                            <comment id="15874253" author="githubbot" created="Mon, 20 Feb 2017 09:31:42 +0000"  >&lt;p&gt;GitHub user tzulitai opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-5487&quot; title=&quot;Proper at-least-once support for ElasticsearchSink&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-5487&quot;&gt;&lt;del&gt;FLINK-5487&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;elasticsearch&amp;#93;&lt;/span&gt; At-least-once ElasticsearchSink&lt;/p&gt;

&lt;p&gt;    This PR adds proper support for an at-least-once `ElasticsearchSink`. This is based on the pluggable error handling strategy functionality added in #3426, so only the last commit is relevant.&lt;/p&gt;

&lt;p&gt;    Like the Kafka producer, the way it works is that pending requests not yet acknowledged by Elasticsearch needs to be flushed before proceeding with the next record from upstream.&lt;br/&gt;
    Slight difference is that for the `ElasticsearchSink`, since we&apos;re allowing re-adding failed requests back to the internal `BulkProcessor` (as part of #3426), we&apos;ll also need to wait for the re-added requests. The docs warn that if requests are re-added, it may lead to longer checkpoints since we need to wait for those too.&lt;/p&gt;

&lt;p&gt;    Flushing is enabled by default, but we provide a `disableFlushOnCheckpoint` method to switch it off. The docs and Javadoc of the method warns the user how this would affect at-least-once delivery.&lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/tzulitai/flink&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/tzulitai/flink&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-5487&quot; title=&quot;Proper at-least-once support for ElasticsearchSink&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-5487&quot;&gt;&lt;del&gt;FLINK-5487&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #3358&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 6a826b8eb7a98e3d159999bc44d827df54c94fdd&lt;br/&gt;
Author: Max Kuklinski &amp;lt;max.kuklinski@live.de&amp;gt;&lt;br/&gt;
Date:   2016-11-23T16:54:11Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-5122&quot; title=&quot;Elasticsearch Sink loses documents when cluster has high load&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-5122&quot;&gt;&lt;del&gt;FLINK-5122&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;elasticsearch&amp;#93;&lt;/span&gt; Retry temporary Elasticsearch request errors.&lt;/p&gt;

&lt;p&gt;    Covered exceptions are: Timeouts, No Master, UnavailableShardsException, bulk queue on node full&lt;/p&gt;

&lt;p&gt;commit 9cb60c263fb0df9a8ccd82b33070e22085b5ab23&lt;br/&gt;
Author: Tzu-Li (Gordon) Tai &amp;lt;tzulitai@apache.org&amp;gt;&lt;br/&gt;
Date:   2017-01-30T05:55:26Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-5353&quot; title=&quot;Elasticsearch Sink loses well-formed documents when there are malformed documents&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-5353&quot;&gt;&lt;del&gt;FLINK-5353&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;elasticsearch&amp;#93;&lt;/span&gt; User-provided failure handler for ElasticsearchSink&lt;/p&gt;

&lt;p&gt;    This commit fixes both &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-5353&quot; title=&quot;Elasticsearch Sink loses well-formed documents when there are malformed documents&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-5353&quot;&gt;&lt;del&gt;FLINK-5353&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-5122&quot; title=&quot;Elasticsearch Sink loses documents when cluster has high load&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-5122&quot;&gt;&lt;del&gt;FLINK-5122&lt;/del&gt;&lt;/a&gt;. It allows users to implement a&lt;br/&gt;
    failure handler to control how failed action requests are dealt with.&lt;/p&gt;

&lt;p&gt;    The commit also includes general improvements to &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-5122&quot; title=&quot;Elasticsearch Sink loses documents when cluster has high load&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-5122&quot;&gt;&lt;del&gt;FLINK-5122&lt;/del&gt;&lt;/a&gt;:&lt;br/&gt;
    1. Use the built-in backoff functionality in the Elasticsearch BulkProcessor (not&lt;br/&gt;
    available for Elasticsearch 1.x)&lt;br/&gt;
    2. Migrate the `checkErrorAndRetryBulk` functionality to the new failure handler&lt;/p&gt;

&lt;p&gt;commit 1c448e3177c65ebc627bdd4ecfff76bbf209ddde&lt;br/&gt;
Author: Tzu-Li (Gordon) Tai &amp;lt;tzulitai@apache.org&amp;gt;&lt;br/&gt;
Date:   2017-02-20T08:50:19Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-5487&quot; title=&quot;Proper at-least-once support for ElasticsearchSink&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-5487&quot;&gt;&lt;del&gt;FLINK-5487&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;elasticsearch&amp;#93;&lt;/span&gt; At-least-once Elasticsearch Sink&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="15875780" author="githubbot" created="Tue, 21 Feb 2017 11:00:38 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102180098&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102180098&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java &amp;#8212;&lt;br/&gt;
    @@ -165,20 +286,36 @@ public void beforeBulk(long executionId, BulkRequest request) { }&lt;br/&gt;
     				@Override&lt;br/&gt;
     				public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {&lt;br/&gt;
     					if (response.hasFailures()) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (BulkItemResponse itemResp : response.getItems()) {&lt;/li&gt;
	&lt;li&gt;Throwable failure = callBridge.extractFailureCauseFromBulkItemResponse(itemResp);&lt;br/&gt;
    +						BulkItemResponse itemResponse;&lt;br/&gt;
    +						Throwable failure;&lt;br/&gt;
    +&lt;br/&gt;
    +						for (int i = 0; i &amp;lt; response.getItems().length; i++) {&lt;br/&gt;
    +							itemResponse = response.getItems()&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt;;&lt;br/&gt;
    +							failure = callBridge.extractFailureCauseFromBulkItemResponse(itemResponse);&lt;br/&gt;
     							if (failure != null) {&lt;/li&gt;
	&lt;li&gt;LOG.error(&quot;Failed Elasticsearch item request: {}&quot;, failure.getMessage(), failure);&lt;/li&gt;
	&lt;li&gt;failureThrowable.compareAndSet(null, failure);&lt;br/&gt;
    +								LOG.error(&quot;Failed Elasticsearch item request: {}&quot;, itemResponse.getFailureMessage(), failure);&lt;br/&gt;
    +&lt;br/&gt;
    +								if (failureHandler.onFailure(request.requests().get&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;, failure, requestIndexer)) {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    I wonder if it would be better if the `onFailure` method would not return a boolean but throw a Throwable?&lt;br/&gt;
    This way users have more flexibility in implementing their failure handler.&lt;/p&gt;

&lt;p&gt;    For example if a failure handler is doing three retries and fails afterwards, the original exception will be thrown. If the `onFailure()` method can throw their own exception, you can throw a custom exception that tells the user about the three retries.&lt;/p&gt;

&lt;p&gt;    We can definitively discuss this because this change is annoying to do (docs &amp;amp; javadocs need to be updated).&lt;/p&gt;</comment>
                            <comment id="15875782" author="githubbot" created="Tue, 21 Feb 2017 11:00:38 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102177573&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102177573&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ActionRequestFailureHandler.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,72 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.elasticsearch;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.elasticsearch.action.ActionRequest;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.io.Serializable;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * An implementation of &lt;/p&gt;
{@link ActionRequestFailureHandler}
&lt;p&gt; is provided by the user to define how failed&lt;br/&gt;
    + * &lt;/p&gt;
{@link ActionRequest ActionRequests}
&lt;p&gt; should be handled, ex. dropping them, reprocessing malformed documents, or&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I&apos;m not sure if the &quot;ex.&quot; is correct here: &lt;a href=&quot;http://english.stackexchange.com/questions/16197/whats-the-difference-between-e-g-and-ex&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://english.stackexchange.com/questions/16197/whats-the-difference-between-e-g-and-ex&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15875781" author="githubbot" created="Tue, 21 Feb 2017 11:00:38 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102177099&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102177099&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: docs/dev/connectors/elasticsearch.md &amp;#8212;&lt;br/&gt;
    @@ -272,9 +308,115 @@ input.addSink(new ElasticsearchSink(config, new ElasticsearchSinkFunction[String&lt;br/&gt;
     The difference is that now we do not need to provide a list of addresses&lt;br/&gt;
     of Elasticsearch nodes.&lt;/p&gt;

&lt;p&gt;    +### Handling Failing Elasticsearch Requests&lt;br/&gt;
    +&lt;br/&gt;
    +Elasticsearch action requests may fail due to a variety of reasons, including&lt;br/&gt;
    +temporarily saturated node queue capacity or malformed documents to be indexed.&lt;br/&gt;
    +The Flink Elasticsearch Sink allows the user to specify how request&lt;br/&gt;
    +failures are handled, by simply implementing an `ActionRequestFailureHandler` and&lt;br/&gt;
    +providing it to the constructor.&lt;br/&gt;
    +&lt;br/&gt;
    +Below is an example:&lt;br/&gt;
    +&lt;br/&gt;
    +&amp;lt;div class=&quot;codetabs&quot; markdown=&quot;1&quot;&amp;gt;&lt;br/&gt;
    +&amp;lt;div data-lang=&quot;java&quot; markdown=&quot;1&quot;&amp;gt;&lt;br/&gt;
    +&lt;/p&gt;
{% highlight java %}
&lt;p&gt;    +DataStream&amp;lt;String&amp;gt; input = ...;&lt;br/&gt;
    +&lt;br/&gt;
    +input.addSink(new ElasticsearchSink&amp;lt;&amp;gt;(&lt;br/&gt;
    +    config, transportAddresses,&lt;br/&gt;
    +    new ElasticsearchSinkFunction&amp;lt;String&amp;gt;() &lt;/p&gt;
{...},&lt;br/&gt;
    +    new ActionRequestFailureHandler() {&lt;br/&gt;
    +        @Override&lt;br/&gt;
    +        boolean onFailure(ActionRequest action, Throwable failure, RequestIndexer indexer) {&lt;br/&gt;
    +            // this example uses Apache Commons to search for nested exceptions&lt;br/&gt;
    +            &lt;br/&gt;
    +            if (ExceptionUtils.indexOfThrowable(failure, EsRejectedExecutionException.class) &amp;gt;= 0) {
    +                // full queue; re-add document for indexing
    +                indexer.add(action);
    +                return false;
    +            } else if (ExceptionUtils.indexOfThrowable(failure, ElasticsearchParseException.class) &amp;gt;= 0) {
    +                // malformed document; simply drop request without failing sink
    +                return false;
    +            } else {
    +                // for all other failures, fail the sink
    +                return true;
    +            }&lt;br/&gt;
    +        }&lt;br/&gt;
    +}));&lt;br/&gt;
    +{% endhighlight %}&lt;br/&gt;
    +&amp;lt;/div&amp;gt;&lt;br/&gt;
    +&amp;lt;div data-lang=&quot;scala&quot; markdown=&quot;1&quot;&amp;gt;&lt;br/&gt;
    +{% highlight scala %}&lt;br/&gt;
    +val input: DataStream&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt; = ...&lt;br/&gt;
    +&lt;br/&gt;
    +input.addSink(new ElasticsearchSink(&lt;br/&gt;
    +    config, transportAddresses,&lt;br/&gt;
    +    new ElasticsearchSinkFunction&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt; {...}
&lt;p&gt;,&lt;br/&gt;
    +    new ActionRequestFailureHandler {&lt;br/&gt;
    +        override def onFailure(ActionRequest action, Throwable failure, RequestIndexer indexer) {&lt;br/&gt;
    +            // this example uses Apache Commons to search for nested exceptions&lt;br/&gt;
    +&lt;br/&gt;
    +            if (ExceptionUtils.indexOfThrowable(failure, EsRejectedExecutionException.class) &amp;gt;= 0) &lt;/p&gt;
{
    +                // full queue; re-add document for indexing
    +                indexer.add(action)
    +                return false
    +            }
&lt;p&gt; else if (ExceptionUtils.indexOfThrowable(failure, ElasticsearchParseException.class) &lt;/p&gt;
{
    +                // malformed document; simply drop request without failing sink
    +                return false
    +            }
&lt;p&gt; else &lt;/p&gt;
{
    +                // for all other failures, fail the sink
    +                return true
    +            }
&lt;p&gt;    +        }&lt;br/&gt;
    +}))&lt;br/&gt;
    +&lt;/p&gt;
{% endhighlight %}
&lt;p&gt;    +&amp;lt;/div&amp;gt;&lt;br/&gt;
    +&amp;lt;/div&amp;gt;&lt;br/&gt;
    +&lt;br/&gt;
    +The above example will let the sink re-add requests that failed due to&lt;br/&gt;
    +queue capacity saturation and drop requests with malformed documents, without&lt;br/&gt;
    +failing the sink. For all other failures, the sink will fail. If a `ActionRequestFailureHandler`&lt;br/&gt;
    +is not provided to the constructor, the sink will fail for any kind of error.&lt;br/&gt;
    +&lt;br/&gt;
    +Note that `onFailure` is called for failures that still occur only after the&lt;br/&gt;
    +`BulkProcessor` internally finishes all backoff retry attempts.&lt;br/&gt;
    +By default, the `BulkProcessor` retries to a maximum of 8 attempts with&lt;br/&gt;
    +an exponential backoff. For more information on the behaviour of the&lt;br/&gt;
    +internal `BulkProcessor` and how to configure it, please see the following section.&lt;br/&gt;
    +&lt;br/&gt;
    +&amp;lt;p style=&quot;border-radius: 5px; padding: 5px&quot; class=&quot;bg-danger&quot;&amp;gt;&lt;br/&gt;
    +&amp;lt;b&amp;gt;IMPORTANT&amp;lt;/b&amp;gt;: Re-adding requests back to the internal &amp;lt;b&amp;gt;BulkProcessor&amp;lt;/b&amp;gt;&lt;br/&gt;
    +on failures will lead to longer checkpoints, as the sink will also&lt;br/&gt;
    +need to wait for the re-added requests to be flushed when checkpointing.&lt;br/&gt;
    +This also means that if re-added requests never succeed, the checkpoint will&lt;br/&gt;
    +never finish.&lt;br/&gt;
    +&amp;lt;/p&amp;gt;&lt;br/&gt;
    +&lt;br/&gt;
    + &lt;br/&gt;
    +### Configuring the Internal Bulk Processor&lt;br/&gt;
    +&lt;br/&gt;
    +The internal `BulkProcessor` can be further configured for its behaviour&lt;br/&gt;
    +on how buffered action requests are flushed, by setting the following values in&lt;br/&gt;
    +the provided `Map&amp;lt;String, String&amp;gt;`:&lt;br/&gt;
    +&lt;br/&gt;
    + * *&lt;b&gt;bulk.flush.max.actions&lt;/b&gt;*: Maximum amount of actions to buffer before flushing.&lt;br/&gt;
    + * *&lt;b&gt;bulk.flush.max.size.mb&lt;/b&gt;*: Maximum size of data (in megabytes) to buffer before flushing.&lt;br/&gt;
    + * *&lt;b&gt;bulk.flush.interval.ms&lt;/b&gt;*: Interval at which to flush regardless of the amount or size of buffered actions.&lt;br/&gt;
    + &lt;br/&gt;
    +For versions 2.x and above, configuring how temporary request errors are&lt;br/&gt;
    +retried is also supported:&lt;br/&gt;
    + &lt;br/&gt;
    + * *&lt;b&gt;bulk.flush.backoff.enable&lt;/b&gt;*: Whether or not to perform retries with backoff delay for a flush&lt;br/&gt;
    + if one or more of its actions failed due to a temporary `EsRejectedExecutionException`.&lt;br/&gt;
    + * *&lt;b&gt;bulk.flush.backoff.type&lt;/b&gt;*: The type of backoff delay, either `CONSTANT` or `EXPONENTIAL`&lt;br/&gt;
    + * *&lt;b&gt;bulk.flush.backoff.delay&lt;/b&gt;*: The amount of delay for backoff. For constant backoff, this&lt;br/&gt;
    + is simply the delay between each retry. For exponential backoff, this is the initial base delay.&lt;br/&gt;
    + * *&lt;b&gt;bulk.flush.backoff.retries&lt;/b&gt;*: The amount of backoff retries to attempt.&lt;br/&gt;
    +&lt;br/&gt;
     More information about Elasticsearch can be found &lt;span class=&quot;error&quot;&gt;&amp;#91;here&amp;#93;&lt;/span&gt;(&lt;a href=&quot;https://elastic.co&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://elastic.co&lt;/a&gt;).&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;
			&lt;ol&gt;
				&lt;li&gt;
				&lt;ol&gt;
					&lt;li&gt;Packaging the Elasticsearch Connector into an Uber-Jar&lt;br/&gt;
    +## Packaging the Elasticsearch Connector into an Uber-Jar&lt;/li&gt;
				&lt;/ol&gt;
				&lt;/li&gt;
			&lt;/ol&gt;
			&lt;/li&gt;
		&lt;/ol&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    I like the reworked documentation page a lot!&lt;/p&gt;</comment>
                            <comment id="15875783" author="githubbot" created="Tue, 21 Feb 2017 11:00:38 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102178460&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102178460&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java &amp;#8212;&lt;br/&gt;
    @@ -67,10 +73,56 @@&lt;br/&gt;
     	public static final String CONFIG_KEY_BULK_FLUSH_MAX_ACTIONS = &quot;bulk.flush.max.actions&quot;;&lt;br/&gt;
     	public static final String CONFIG_KEY_BULK_FLUSH_MAX_SIZE_MB = &quot;bulk.flush.max.size.mb&quot;;&lt;br/&gt;
     	public static final String CONFIG_KEY_BULK_FLUSH_INTERVAL_MS = &quot;bulk.flush.interval.ms&quot;;&lt;br/&gt;
    +	public static final String CONFIG_KEY_BULK_FLUSH_BACKOFF_ENABLE = &quot;bulk.flush.backoff.enable&quot;;&lt;br/&gt;
    +	public static final String CONFIG_KEY_BULK_FLUSH_BACKOFF_TYPE = &quot;bulk.flush.backoff.type&quot;;&lt;br/&gt;
    +	public static final String CONFIG_KEY_BULK_FLUSH_BACKOFF_RETRIES = &quot;bulk.flush.backoff.retries&quot;;&lt;br/&gt;
    +	public static final String CONFIG_KEY_BULK_FLUSH_BACKOFF_DELAY = &quot;bulk.flush.backoff.delay&quot;;&lt;br/&gt;
    +&lt;br/&gt;
    +	public enum FlushBackoffType &lt;/p&gt;
{
    +		CONSTANT,
    +		EXPONENTIAL
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	public class BulkFlushBackoffPolicy implements Serializable {&lt;br/&gt;
    +&lt;br/&gt;
    +		private static final long serialVersionUID = -6022851996101826049L;&lt;br/&gt;
    +&lt;br/&gt;
    +		// the default values follow the Elasticsearch default settings for BulkProcessor&lt;br/&gt;
    +		private FlushBackoffType backoffType = FlushBackoffType.EXPONENTIAL;&lt;br/&gt;
    +		private int maxRetryCount = 8;&lt;br/&gt;
    +		private long delayMillis = 50;&lt;br/&gt;
    +&lt;br/&gt;
    +		public FlushBackoffType getBackoffType() &lt;/p&gt;
{
    +			return backoffType;
    +		}
&lt;p&gt;    +&lt;br/&gt;
    +		public int getMaxRetryCount() &lt;/p&gt;
{
    +			return maxRetryCount;
    +		}
&lt;p&gt;    +&lt;br/&gt;
    +		public long getDelayMillis() &lt;/p&gt;
{
    +			return delayMillis;
    +		}
&lt;p&gt;    +&lt;br/&gt;
    +		public void setBackoffType(FlushBackoffType backoffType) &lt;/p&gt;
{
    +			this.backoffType = checkNotNull(backoffType);
    +		}
&lt;p&gt;    +&lt;br/&gt;
    +		public void setMaxRetryCount(int maxRetryCount) {&lt;br/&gt;
    +			checkArgument(maxRetryCount &amp;gt; 0);&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Isn&apos;t 0 also an acceptable value here? If users want to disable retries entirely?&lt;/p&gt;</comment>
                            <comment id="15875784" author="githubbot" created="Tue, 21 Feb 2017 11:00:38 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102179118&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102179118&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java &amp;#8212;&lt;br/&gt;
    @@ -122,10 +198,19 @@ public ElasticsearchSinkBase(&lt;br/&gt;
     				&quot;The object probably contains or references non serializable fields.&quot;);&lt;br/&gt;
     		}&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;checkNotNull(userConfig);&lt;br/&gt;
    +		try 
{
    +			InstantiationUtil.serializeObject(failureHandler);
    +		}
&lt;p&gt; catch (Exception e) &lt;/p&gt;
{
    +			throw new IllegalArgumentException(
    +				&quot;The implementation of the provided ActionRequestFailureHandler is not serializable. &quot; +
    +					&quot;The object probably contains or references non serializable fields.&quot;);
    +		}
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    This looks a bit like duplicate code. I think adding a utility into the `InstantiationUtil` that is called `isSerializable()` would be cleaner and save some LOC.&lt;/p&gt;</comment>
                            <comment id="15875785" author="githubbot" created="Tue, 21 Feb 2017 11:00:38 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102178621&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102178621&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java &amp;#8212;&lt;br/&gt;
    @@ -67,10 +73,56 @@&lt;br/&gt;
     	public static final String CONFIG_KEY_BULK_FLUSH_MAX_ACTIONS = &quot;bulk.flush.max.actions&quot;;&lt;br/&gt;
     	public static final String CONFIG_KEY_BULK_FLUSH_MAX_SIZE_MB = &quot;bulk.flush.max.size.mb&quot;;&lt;br/&gt;
     	public static final String CONFIG_KEY_BULK_FLUSH_INTERVAL_MS = &quot;bulk.flush.interval.ms&quot;;&lt;br/&gt;
    +	public static final String CONFIG_KEY_BULK_FLUSH_BACKOFF_ENABLE = &quot;bulk.flush.backoff.enable&quot;;&lt;br/&gt;
    +	public static final String CONFIG_KEY_BULK_FLUSH_BACKOFF_TYPE = &quot;bulk.flush.backoff.type&quot;;&lt;br/&gt;
    +	public static final String CONFIG_KEY_BULK_FLUSH_BACKOFF_RETRIES = &quot;bulk.flush.backoff.retries&quot;;&lt;br/&gt;
    +	public static final String CONFIG_KEY_BULK_FLUSH_BACKOFF_DELAY = &quot;bulk.flush.backoff.delay&quot;;&lt;br/&gt;
    +&lt;br/&gt;
    +	public enum FlushBackoffType &lt;/p&gt;
{
    +		CONSTANT,
    +		EXPONENTIAL
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	public class BulkFlushBackoffPolicy implements Serializable {&lt;br/&gt;
    +&lt;br/&gt;
    +		private static final long serialVersionUID = -6022851996101826049L;&lt;br/&gt;
    +&lt;br/&gt;
    +		// the default values follow the Elasticsearch default settings for BulkProcessor&lt;br/&gt;
    +		private FlushBackoffType backoffType = FlushBackoffType.EXPONENTIAL;&lt;br/&gt;
    +		private int maxRetryCount = 8;&lt;br/&gt;
    +		private long delayMillis = 50;&lt;br/&gt;
    +&lt;br/&gt;
    +		public FlushBackoffType getBackoffType() &lt;/p&gt;
{
    +			return backoffType;
    +		}
&lt;p&gt;    +&lt;br/&gt;
    +		public int getMaxRetryCount() &lt;/p&gt;
{
    +			return maxRetryCount;
    +		}
&lt;p&gt;    +&lt;br/&gt;
    +		public long getDelayMillis() &lt;/p&gt;
{
    +			return delayMillis;
    +		}
&lt;p&gt;    +&lt;br/&gt;
    +		public void setBackoffType(FlushBackoffType backoffType) &lt;/p&gt;
{
    +			this.backoffType = checkNotNull(backoffType);
    +		}
&lt;p&gt;    +&lt;br/&gt;
    +		public void setMaxRetryCount(int maxRetryCount) &lt;/p&gt;
{
    +			checkArgument(maxRetryCount &amp;gt; 0);
    +			this.maxRetryCount = maxRetryCount;
    +		}
&lt;p&gt;    +&lt;br/&gt;
    +		public void setDelayMillis(long delayMillis) {&lt;br/&gt;
    +			checkArgument(delayMillis &amp;gt; 0);&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    We should accept 0 here as well, if users want to retry immediately (for whatever reason &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; )&lt;/p&gt;</comment>
                            <comment id="15875786" author="githubbot" created="Tue, 21 Feb 2017 11:00:38 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102177012&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102177012&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: docs/dev/connectors/elasticsearch.md &amp;#8212;&lt;br/&gt;
    @@ -272,9 +308,115 @@ input.addSink(new ElasticsearchSink(config, new ElasticsearchSinkFunction[String&lt;br/&gt;
     The difference is that now we do not need to provide a list of addresses&lt;br/&gt;
     of Elasticsearch nodes.&lt;/p&gt;

&lt;p&gt;    +### Handling Failing Elasticsearch Requests&lt;br/&gt;
    +&lt;br/&gt;
    +Elasticsearch action requests may fail due to a variety of reasons, including&lt;br/&gt;
    +temporarily saturated node queue capacity or malformed documents to be indexed.&lt;br/&gt;
    +The Flink Elasticsearch Sink allows the user to specify how request&lt;br/&gt;
    +failures are handled, by simply implementing an `ActionRequestFailureHandler` and&lt;br/&gt;
    +providing it to the constructor.&lt;br/&gt;
    +&lt;br/&gt;
    +Below is an example:&lt;br/&gt;
    +&lt;br/&gt;
    +&amp;lt;div class=&quot;codetabs&quot; markdown=&quot;1&quot;&amp;gt;&lt;br/&gt;
    +&amp;lt;div data-lang=&quot;java&quot; markdown=&quot;1&quot;&amp;gt;&lt;br/&gt;
    +&lt;/p&gt;
{% highlight java %}
&lt;p&gt;    +DataStream&amp;lt;String&amp;gt; input = ...;&lt;br/&gt;
    +&lt;br/&gt;
    +input.addSink(new ElasticsearchSink&amp;lt;&amp;gt;(&lt;br/&gt;
    +    config, transportAddresses,&lt;br/&gt;
    +    new ElasticsearchSinkFunction&amp;lt;String&amp;gt;() &lt;/p&gt;
{...},&lt;br/&gt;
    +    new ActionRequestFailureHandler() {&lt;br/&gt;
    +        @Override&lt;br/&gt;
    +        boolean onFailure(ActionRequest action, Throwable failure, RequestIndexer indexer) {&lt;br/&gt;
    +            // this example uses Apache Commons to search for nested exceptions&lt;br/&gt;
    +            &lt;br/&gt;
    +            if (ExceptionUtils.indexOfThrowable(failure, EsRejectedExecutionException.class) &amp;gt;= 0) {
    +                // full queue; re-add document for indexing
    +                indexer.add(action);
    +                return false;
    +            } else if (ExceptionUtils.indexOfThrowable(failure, ElasticsearchParseException.class) &amp;gt;= 0) {
    +                // malformed document; simply drop request without failing sink
    +                return false;
    +            } else {
    +                // for all other failures, fail the sink
    +                return true;
    +            }&lt;br/&gt;
    +        }&lt;br/&gt;
    +}));&lt;br/&gt;
    +{% endhighlight %}&lt;br/&gt;
    +&amp;lt;/div&amp;gt;&lt;br/&gt;
    +&amp;lt;div data-lang=&quot;scala&quot; markdown=&quot;1&quot;&amp;gt;&lt;br/&gt;
    +{% highlight scala %}&lt;br/&gt;
    +val input: DataStream&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt; = ...&lt;br/&gt;
    +&lt;br/&gt;
    +input.addSink(new ElasticsearchSink(&lt;br/&gt;
    +    config, transportAddresses,&lt;br/&gt;
    +    new ElasticsearchSinkFunction&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt; {...}
&lt;p&gt;,&lt;br/&gt;
    +    new ActionRequestFailureHandler {&lt;br/&gt;
    +        override def onFailure(ActionRequest action, Throwable failure, RequestIndexer indexer) {&lt;br/&gt;
    +            // this example uses Apache Commons to search for nested exceptions&lt;br/&gt;
    +&lt;br/&gt;
    +            if (ExceptionUtils.indexOfThrowable(failure, EsRejectedExecutionException.class) &amp;gt;= 0) &lt;/p&gt;
{
    +                // full queue; re-add document for indexing
    +                indexer.add(action)
    +                return false
    +            }
&lt;p&gt; else if (ExceptionUtils.indexOfThrowable(failure, ElasticsearchParseException.class) &lt;/p&gt;
{
    +                // malformed document; simply drop request without failing sink
    +                return false
    +            }
&lt;p&gt; else &lt;/p&gt;
{
    +                // for all other failures, fail the sink
    +                return true
    +            }
&lt;p&gt;    +        }&lt;br/&gt;
    +}))&lt;br/&gt;
    +&lt;/p&gt;
{% endhighlight %}
&lt;p&gt;    +&amp;lt;/div&amp;gt;&lt;br/&gt;
    +&amp;lt;/div&amp;gt;&lt;br/&gt;
    +&lt;br/&gt;
    +The above example will let the sink re-add requests that failed due to&lt;br/&gt;
    +queue capacity saturation and drop requests with malformed documents, without&lt;br/&gt;
    +failing the sink. For all other failures, the sink will fail. If a `ActionRequestFailureHandler`&lt;br/&gt;
    +is not provided to the constructor, the sink will fail for any kind of error.&lt;br/&gt;
    +&lt;br/&gt;
    +Note that `onFailure` is called for failures that still occur only after the&lt;br/&gt;
    +`BulkProcessor` internally finishes all backoff retry attempts.&lt;br/&gt;
    +By default, the `BulkProcessor` retries to a maximum of 8 attempts with&lt;br/&gt;
    +an exponential backoff. For more information on the behaviour of the&lt;br/&gt;
    +internal `BulkProcessor` and how to configure it, please see the following section.&lt;br/&gt;
    +&lt;br/&gt;
    +&amp;lt;p style=&quot;border-radius: 5px; padding: 5px&quot; class=&quot;bg-danger&quot;&amp;gt;&lt;br/&gt;
    +&amp;lt;b&amp;gt;IMPORTANT&amp;lt;/b&amp;gt;: Re-adding requests back to the internal &amp;lt;b&amp;gt;BulkProcessor&amp;lt;/b&amp;gt;&lt;br/&gt;
    +on failures will lead to longer checkpoints, as the sink will also&lt;br/&gt;
    +need to wait for the re-added requests to be flushed when checkpointing.&lt;br/&gt;
    +This also means that if re-added requests never succeed, the checkpoint will&lt;br/&gt;
    +never finish.&lt;br/&gt;
    +&amp;lt;/p&amp;gt;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I think the docs should mention the `NoOpActionRequestFailureHandler`.&lt;/p&gt;

&lt;p&gt;    Also I wonder if we should offer a default `RetryActionRequestFailureHandler`. I suspect that many users will need that. What do you think?&lt;/p&gt;</comment>
                            <comment id="15875787" author="githubbot" created="Tue, 21 Feb 2017 11:00:38 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102181691&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102181691&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java &amp;#8212;&lt;br/&gt;
    @@ -165,20 +286,36 @@ public void beforeBulk(long executionId, BulkRequest request) { }&lt;br/&gt;
     				@Override&lt;br/&gt;
     				public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {&lt;br/&gt;
     					if (response.hasFailures()) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (BulkItemResponse itemResp : response.getItems()) {&lt;/li&gt;
	&lt;li&gt;Throwable failure = callBridge.extractFailureCauseFromBulkItemResponse(itemResp);&lt;br/&gt;
    +						BulkItemResponse itemResponse;&lt;br/&gt;
    +						Throwable failure;&lt;br/&gt;
    +&lt;br/&gt;
    +						for (int i = 0; i &amp;lt; response.getItems().length; i++) {&lt;br/&gt;
    +							itemResponse = response.getItems()&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt;;&lt;br/&gt;
    +							failure = callBridge.extractFailureCauseFromBulkItemResponse(itemResponse);&lt;br/&gt;
     							if (failure != null) {&lt;/li&gt;
	&lt;li&gt;LOG.error(&quot;Failed Elasticsearch item request: {}&quot;, failure.getMessage(), failure);&lt;/li&gt;
	&lt;li&gt;failureThrowable.compareAndSet(null, failure);&lt;br/&gt;
    +								LOG.error(&quot;Failed Elasticsearch item request: {}&quot;, itemResponse.getFailureMessage(), failure);&lt;br/&gt;
    +&lt;br/&gt;
    +								if (failureHandler.onFailure(request.requests().get&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;, failure, requestIndexer)) 
{
    +									failureThrowable.compareAndSet(null, failure);
    +								}
&lt;p&gt;     							}&lt;br/&gt;
     						}&lt;br/&gt;
     					}&lt;br/&gt;
    +&lt;br/&gt;
    +					numPendingRequests.getAndAdd(-request.numberOfActions());&lt;br/&gt;
     				}&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     				@Override&lt;br/&gt;
     				public void afterBulk(long executionId, BulkRequest request, Throwable failure) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;LOG.error(&quot;Failed Elasticsearch bulk request: {}&quot;, failure.getMessage(), failure);&lt;/li&gt;
	&lt;li&gt;failureThrowable.compareAndSet(null, failure);&lt;br/&gt;
    +					LOG.error(&quot;Failed Elasticsearch bulk request: {}&quot;, failure.getMessage(), failure.getCause());&lt;br/&gt;
    +&lt;br/&gt;
    +					// whole bulk request failures are usually just temporary timeouts on&lt;br/&gt;
    +					// the Elasticsearch side; simply retry all action requests in the bulk&lt;br/&gt;
    +					for (ActionRequest action : request.requests()) 
{
    +						requestIndexer.add(action);
    +					}
&lt;p&gt;    +&lt;br/&gt;
    +					numPendingRequests.getAndAdd(-request.numberOfActions());&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Lets say a bulk with 500 actions fails, so we re-add the bulk again, but subtract 500 actions from the pending requests.&lt;/p&gt;

&lt;p&gt;    Now the bulk succeeds and we subtract 500 actions again. Which would make the num pending requests negative? and void the at least once guarantees?&lt;/p&gt;

&lt;p&gt;    Am I overseeing something here?&lt;/p&gt;</comment>
                            <comment id="15875788" author="githubbot" created="Tue, 21 Feb 2017 11:00:38 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102181294&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102181294&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java &amp;#8212;&lt;br/&gt;
    @@ -165,20 +286,36 @@ public void beforeBulk(long executionId, BulkRequest request) { }&lt;br/&gt;
     				@Override&lt;br/&gt;
     				public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {&lt;br/&gt;
     					if (response.hasFailures()) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (BulkItemResponse itemResp : response.getItems()) {&lt;/li&gt;
	&lt;li&gt;Throwable failure = callBridge.extractFailureCauseFromBulkItemResponse(itemResp);&lt;br/&gt;
    +						BulkItemResponse itemResponse;&lt;br/&gt;
    +						Throwable failure;&lt;br/&gt;
    +&lt;br/&gt;
    +						for (int i = 0; i &amp;lt; response.getItems().length; i++) {&lt;br/&gt;
    +							itemResponse = response.getItems()&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt;;&lt;br/&gt;
    +							failure = callBridge.extractFailureCauseFromBulkItemResponse(itemResponse);&lt;br/&gt;
     							if (failure != null) {&lt;/li&gt;
	&lt;li&gt;LOG.error(&quot;Failed Elasticsearch item request: {}&quot;, failure.getMessage(), failure);&lt;/li&gt;
	&lt;li&gt;failureThrowable.compareAndSet(null, failure);&lt;br/&gt;
    +								LOG.error(&quot;Failed Elasticsearch item request: {}&quot;, itemResponse.getFailureMessage(), failure);&lt;br/&gt;
    +&lt;br/&gt;
    +								if (failureHandler.onFailure(request.requests().get&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;, failure, requestIndexer)) 
{
    +									failureThrowable.compareAndSet(null, failure);
    +								}
&lt;p&gt;     							}&lt;br/&gt;
     						}&lt;br/&gt;
     					}&lt;br/&gt;
    +&lt;br/&gt;
    +					numPendingRequests.getAndAdd(-request.numberOfActions());&lt;br/&gt;
     				}&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     				@Override&lt;br/&gt;
     				public void afterBulk(long executionId, BulkRequest request, Throwable failure) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;LOG.error(&quot;Failed Elasticsearch bulk request: {}&quot;, failure.getMessage(), failure);&lt;/li&gt;
	&lt;li&gt;failureThrowable.compareAndSet(null, failure);&lt;br/&gt;
    +					LOG.error(&quot;Failed Elasticsearch bulk request: {}&quot;, failure.getMessage(), failure.getCause());&lt;br/&gt;
    +&lt;br/&gt;
    +					// whole bulk request failures are usually just temporary timeouts on&lt;br/&gt;
    +					// the Elasticsearch side; simply retry all action requests in the bulk&lt;br/&gt;
    +					for (ActionRequest action : request.requests()) 
{
    +						requestIndexer.add(action);
    +					}
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    I wonder if we should provide a custom, pluggable retry logic here as well. If you are sure that only connection issues cause this, we can leave it as is.&lt;/p&gt;</comment>
                            <comment id="15875797" author="githubbot" created="Tue, 21 Feb 2017 11:09:21 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102183423&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102183423&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: docs/dev/connectors/elasticsearch.md &amp;#8212;&lt;br/&gt;
    @@ -272,9 +308,115 @@ input.addSink(new ElasticsearchSink(config, new ElasticsearchSinkFunction[String&lt;br/&gt;
     The difference is that now we do not need to provide a list of addresses&lt;br/&gt;
     of Elasticsearch nodes.&lt;/p&gt;

&lt;p&gt;    +### Handling Failing Elasticsearch Requests&lt;br/&gt;
    +&lt;br/&gt;
    +Elasticsearch action requests may fail due to a variety of reasons, including&lt;br/&gt;
    +temporarily saturated node queue capacity or malformed documents to be indexed.&lt;br/&gt;
    +The Flink Elasticsearch Sink allows the user to specify how request&lt;br/&gt;
    +failures are handled, by simply implementing an `ActionRequestFailureHandler` and&lt;br/&gt;
    +providing it to the constructor.&lt;br/&gt;
    +&lt;br/&gt;
    +Below is an example:&lt;br/&gt;
    +&lt;br/&gt;
    +&amp;lt;div class=&quot;codetabs&quot; markdown=&quot;1&quot;&amp;gt;&lt;br/&gt;
    +&amp;lt;div data-lang=&quot;java&quot; markdown=&quot;1&quot;&amp;gt;&lt;br/&gt;
    +&lt;/p&gt;
{% highlight java %}
&lt;p&gt;    +DataStream&amp;lt;String&amp;gt; input = ...;&lt;br/&gt;
    +&lt;br/&gt;
    +input.addSink(new ElasticsearchSink&amp;lt;&amp;gt;(&lt;br/&gt;
    +    config, transportAddresses,&lt;br/&gt;
    +    new ElasticsearchSinkFunction&amp;lt;String&amp;gt;() &lt;/p&gt;
{...},&lt;br/&gt;
    +    new ActionRequestFailureHandler() {&lt;br/&gt;
    +        @Override&lt;br/&gt;
    +        boolean onFailure(ActionRequest action, Throwable failure, RequestIndexer indexer) {&lt;br/&gt;
    +            // this example uses Apache Commons to search for nested exceptions&lt;br/&gt;
    +            &lt;br/&gt;
    +            if (ExceptionUtils.indexOfThrowable(failure, EsRejectedExecutionException.class) &amp;gt;= 0) {
    +                // full queue; re-add document for indexing
    +                indexer.add(action);
    +                return false;
    +            } else if (ExceptionUtils.indexOfThrowable(failure, ElasticsearchParseException.class) &amp;gt;= 0) {
    +                // malformed document; simply drop request without failing sink
    +                return false;
    +            } else {
    +                // for all other failures, fail the sink
    +                return true;
    +            }&lt;br/&gt;
    +        }&lt;br/&gt;
    +}));&lt;br/&gt;
    +{% endhighlight %}&lt;br/&gt;
    +&amp;lt;/div&amp;gt;&lt;br/&gt;
    +&amp;lt;div data-lang=&quot;scala&quot; markdown=&quot;1&quot;&amp;gt;&lt;br/&gt;
    +{% highlight scala %}&lt;br/&gt;
    +val input: DataStream&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt; = ...&lt;br/&gt;
    +&lt;br/&gt;
    +input.addSink(new ElasticsearchSink(&lt;br/&gt;
    +    config, transportAddresses,&lt;br/&gt;
    +    new ElasticsearchSinkFunction&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt; {...}
&lt;p&gt;,&lt;br/&gt;
    +    new ActionRequestFailureHandler {&lt;br/&gt;
    +        override def onFailure(ActionRequest action, Throwable failure, RequestIndexer indexer) {&lt;br/&gt;
    +            // this example uses Apache Commons to search for nested exceptions&lt;br/&gt;
    +&lt;br/&gt;
    +            if (ExceptionUtils.indexOfThrowable(failure, EsRejectedExecutionException.class) &amp;gt;= 0) &lt;/p&gt;
{
    +                // full queue; re-add document for indexing
    +                indexer.add(action)
    +                return false
    +            }
&lt;p&gt; else if (ExceptionUtils.indexOfThrowable(failure, ElasticsearchParseException.class) &lt;/p&gt;
{
    +                // malformed document; simply drop request without failing sink
    +                return false
    +            }
&lt;p&gt; else &lt;/p&gt;
{
    +                // for all other failures, fail the sink
    +                return true
    +            }
&lt;p&gt;    +        }&lt;br/&gt;
    +}))&lt;br/&gt;
    +&lt;/p&gt;
{% endhighlight %}
&lt;p&gt;    +&amp;lt;/div&amp;gt;&lt;br/&gt;
    +&amp;lt;/div&amp;gt;&lt;br/&gt;
    +&lt;br/&gt;
    +The above example will let the sink re-add requests that failed due to&lt;br/&gt;
    +queue capacity saturation and drop requests with malformed documents, without&lt;br/&gt;
    +failing the sink. For all other failures, the sink will fail. If a `ActionRequestFailureHandler`&lt;br/&gt;
    +is not provided to the constructor, the sink will fail for any kind of error.&lt;br/&gt;
    +&lt;br/&gt;
    +Note that `onFailure` is called for failures that still occur only after the&lt;br/&gt;
    +`BulkProcessor` internally finishes all backoff retry attempts.&lt;br/&gt;
    +By default, the `BulkProcessor` retries to a maximum of 8 attempts with&lt;br/&gt;
    +an exponential backoff. For more information on the behaviour of the&lt;br/&gt;
    +internal `BulkProcessor` and how to configure it, please see the following section.&lt;br/&gt;
    +&lt;br/&gt;
    +&amp;lt;p style=&quot;border-radius: 5px; padding: 5px&quot; class=&quot;bg-danger&quot;&amp;gt;&lt;br/&gt;
    +&amp;lt;b&amp;gt;IMPORTANT&amp;lt;/b&amp;gt;: Re-adding requests back to the internal &amp;lt;b&amp;gt;BulkProcessor&amp;lt;/b&amp;gt;&lt;br/&gt;
    +on failures will lead to longer checkpoints, as the sink will also&lt;br/&gt;
    +need to wait for the re-added requests to be flushed when checkpointing.&lt;br/&gt;
    +This also means that if re-added requests never succeed, the checkpoint will&lt;br/&gt;
    +never finish.&lt;br/&gt;
    +&amp;lt;/p&amp;gt;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Good point. I&apos;ll add to mention that by default, the sink uses the `NoOpActionRequestFailureHandler `.&lt;/p&gt;</comment>
                            <comment id="15875802" author="githubbot" created="Tue, 21 Feb 2017 11:10:50 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102183643&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102183643&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: docs/dev/connectors/elasticsearch.md &amp;#8212;&lt;br/&gt;
    @@ -272,9 +308,115 @@ input.addSink(new ElasticsearchSink(config, new ElasticsearchSinkFunction[String&lt;br/&gt;
     The difference is that now we do not need to provide a list of addresses&lt;br/&gt;
     of Elasticsearch nodes.&lt;/p&gt;

&lt;p&gt;    +### Handling Failing Elasticsearch Requests&lt;br/&gt;
    +&lt;br/&gt;
    +Elasticsearch action requests may fail due to a variety of reasons, including&lt;br/&gt;
    +temporarily saturated node queue capacity or malformed documents to be indexed.&lt;br/&gt;
    +The Flink Elasticsearch Sink allows the user to specify how request&lt;br/&gt;
    +failures are handled, by simply implementing an `ActionRequestFailureHandler` and&lt;br/&gt;
    +providing it to the constructor.&lt;br/&gt;
    +&lt;br/&gt;
    +Below is an example:&lt;br/&gt;
    +&lt;br/&gt;
    +&amp;lt;div class=&quot;codetabs&quot; markdown=&quot;1&quot;&amp;gt;&lt;br/&gt;
    +&amp;lt;div data-lang=&quot;java&quot; markdown=&quot;1&quot;&amp;gt;&lt;br/&gt;
    +&lt;/p&gt;
{% highlight java %}
&lt;p&gt;    +DataStream&amp;lt;String&amp;gt; input = ...;&lt;br/&gt;
    +&lt;br/&gt;
    +input.addSink(new ElasticsearchSink&amp;lt;&amp;gt;(&lt;br/&gt;
    +    config, transportAddresses,&lt;br/&gt;
    +    new ElasticsearchSinkFunction&amp;lt;String&amp;gt;() &lt;/p&gt;
{...},&lt;br/&gt;
    +    new ActionRequestFailureHandler() {&lt;br/&gt;
    +        @Override&lt;br/&gt;
    +        boolean onFailure(ActionRequest action, Throwable failure, RequestIndexer indexer) {&lt;br/&gt;
    +            // this example uses Apache Commons to search for nested exceptions&lt;br/&gt;
    +            &lt;br/&gt;
    +            if (ExceptionUtils.indexOfThrowable(failure, EsRejectedExecutionException.class) &amp;gt;= 0) {
    +                // full queue; re-add document for indexing
    +                indexer.add(action);
    +                return false;
    +            } else if (ExceptionUtils.indexOfThrowable(failure, ElasticsearchParseException.class) &amp;gt;= 0) {
    +                // malformed document; simply drop request without failing sink
    +                return false;
    +            } else {
    +                // for all other failures, fail the sink
    +                return true;
    +            }&lt;br/&gt;
    +        }&lt;br/&gt;
    +}));&lt;br/&gt;
    +{% endhighlight %}&lt;br/&gt;
    +&amp;lt;/div&amp;gt;&lt;br/&gt;
    +&amp;lt;div data-lang=&quot;scala&quot; markdown=&quot;1&quot;&amp;gt;&lt;br/&gt;
    +{% highlight scala %}&lt;br/&gt;
    +val input: DataStream&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt; = ...&lt;br/&gt;
    +&lt;br/&gt;
    +input.addSink(new ElasticsearchSink(&lt;br/&gt;
    +    config, transportAddresses,&lt;br/&gt;
    +    new ElasticsearchSinkFunction&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt; {...}
&lt;p&gt;,&lt;br/&gt;
    +    new ActionRequestFailureHandler {&lt;br/&gt;
    +        override def onFailure(ActionRequest action, Throwable failure, RequestIndexer indexer) {&lt;br/&gt;
    +            // this example uses Apache Commons to search for nested exceptions&lt;br/&gt;
    +&lt;br/&gt;
    +            if (ExceptionUtils.indexOfThrowable(failure, EsRejectedExecutionException.class) &amp;gt;= 0) &lt;/p&gt;
{
    +                // full queue; re-add document for indexing
    +                indexer.add(action)
    +                return false
    +            }
&lt;p&gt; else if (ExceptionUtils.indexOfThrowable(failure, ElasticsearchParseException.class) &lt;/p&gt;
{
    +                // malformed document; simply drop request without failing sink
    +                return false
    +            }
&lt;p&gt; else &lt;/p&gt;
{
    +                // for all other failures, fail the sink
    +                return true
    +            }
&lt;p&gt;    +        }&lt;br/&gt;
    +}))&lt;br/&gt;
    +&lt;/p&gt;
{% endhighlight %}
&lt;p&gt;    +&amp;lt;/div&amp;gt;&lt;br/&gt;
    +&amp;lt;/div&amp;gt;&lt;br/&gt;
    +&lt;br/&gt;
    +The above example will let the sink re-add requests that failed due to&lt;br/&gt;
    +queue capacity saturation and drop requests with malformed documents, without&lt;br/&gt;
    +failing the sink. For all other failures, the sink will fail. If a `ActionRequestFailureHandler`&lt;br/&gt;
    +is not provided to the constructor, the sink will fail for any kind of error.&lt;br/&gt;
    +&lt;br/&gt;
    +Note that `onFailure` is called for failures that still occur only after the&lt;br/&gt;
    +`BulkProcessor` internally finishes all backoff retry attempts.&lt;br/&gt;
    +By default, the `BulkProcessor` retries to a maximum of 8 attempts with&lt;br/&gt;
    +an exponential backoff. For more information on the behaviour of the&lt;br/&gt;
    +internal `BulkProcessor` and how to configure it, please see the following section.&lt;br/&gt;
    +&lt;br/&gt;
    +&amp;lt;p style=&quot;border-radius: 5px; padding: 5px&quot; class=&quot;bg-danger&quot;&amp;gt;&lt;br/&gt;
    +&amp;lt;b&amp;gt;IMPORTANT&amp;lt;/b&amp;gt;: Re-adding requests back to the internal &amp;lt;b&amp;gt;BulkProcessor&amp;lt;/b&amp;gt;&lt;br/&gt;
    +on failures will lead to longer checkpoints, as the sink will also&lt;br/&gt;
    +need to wait for the re-added requests to be flushed when checkpointing.&lt;br/&gt;
    +This also means that if re-added requests never succeed, the checkpoint will&lt;br/&gt;
    +never finish.&lt;br/&gt;
    +&amp;lt;/p&amp;gt;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I think a pre-implemented `ActionRequestFailureHandler` that re-adds requests for full queue exceptions will be nice, and useful out-of-the box for a large portion of users. Great idea!&lt;/p&gt;</comment>
                            <comment id="15875804" author="githubbot" created="Tue, 21 Feb 2017 11:11:27 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102183754&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102183754&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java &amp;#8212;&lt;br/&gt;
    @@ -67,10 +73,56 @@&lt;br/&gt;
     	public static final String CONFIG_KEY_BULK_FLUSH_MAX_ACTIONS = &quot;bulk.flush.max.actions&quot;;&lt;br/&gt;
     	public static final String CONFIG_KEY_BULK_FLUSH_MAX_SIZE_MB = &quot;bulk.flush.max.size.mb&quot;;&lt;br/&gt;
     	public static final String CONFIG_KEY_BULK_FLUSH_INTERVAL_MS = &quot;bulk.flush.interval.ms&quot;;&lt;br/&gt;
    +	public static final String CONFIG_KEY_BULK_FLUSH_BACKOFF_ENABLE = &quot;bulk.flush.backoff.enable&quot;;&lt;br/&gt;
    +	public static final String CONFIG_KEY_BULK_FLUSH_BACKOFF_TYPE = &quot;bulk.flush.backoff.type&quot;;&lt;br/&gt;
    +	public static final String CONFIG_KEY_BULK_FLUSH_BACKOFF_RETRIES = &quot;bulk.flush.backoff.retries&quot;;&lt;br/&gt;
    +	public static final String CONFIG_KEY_BULK_FLUSH_BACKOFF_DELAY = &quot;bulk.flush.backoff.delay&quot;;&lt;br/&gt;
    +&lt;br/&gt;
    +	public enum FlushBackoffType &lt;/p&gt;
{
    +		CONSTANT,
    +		EXPONENTIAL
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	public class BulkFlushBackoffPolicy implements Serializable {&lt;br/&gt;
    +&lt;br/&gt;
    +		private static final long serialVersionUID = -6022851996101826049L;&lt;br/&gt;
    +&lt;br/&gt;
    +		// the default values follow the Elasticsearch default settings for BulkProcessor&lt;br/&gt;
    +		private FlushBackoffType backoffType = FlushBackoffType.EXPONENTIAL;&lt;br/&gt;
    +		private int maxRetryCount = 8;&lt;br/&gt;
    +		private long delayMillis = 50;&lt;br/&gt;
    +&lt;br/&gt;
    +		public FlushBackoffType getBackoffType() &lt;/p&gt;
{
    +			return backoffType;
    +		}
&lt;p&gt;    +&lt;br/&gt;
    +		public int getMaxRetryCount() &lt;/p&gt;
{
    +			return maxRetryCount;
    +		}
&lt;p&gt;    +&lt;br/&gt;
    +		public long getDelayMillis() &lt;/p&gt;
{
    +			return delayMillis;
    +		}
&lt;p&gt;    +&lt;br/&gt;
    +		public void setBackoffType(FlushBackoffType backoffType) &lt;/p&gt;
{
    +			this.backoffType = checkNotNull(backoffType);
    +		}
&lt;p&gt;    +&lt;br/&gt;
    +		public void setMaxRetryCount(int maxRetryCount) {&lt;br/&gt;
    +			checkArgument(maxRetryCount &amp;gt; 0);&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Yup, 0 should be acceptable.&lt;/p&gt;</comment>
                            <comment id="15875805" author="githubbot" created="Tue, 21 Feb 2017 11:11:53 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102183824&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102183824&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java &amp;#8212;&lt;br/&gt;
    @@ -67,10 +73,56 @@&lt;br/&gt;
     	public static final String CONFIG_KEY_BULK_FLUSH_MAX_ACTIONS = &quot;bulk.flush.max.actions&quot;;&lt;br/&gt;
     	public static final String CONFIG_KEY_BULK_FLUSH_MAX_SIZE_MB = &quot;bulk.flush.max.size.mb&quot;;&lt;br/&gt;
     	public static final String CONFIG_KEY_BULK_FLUSH_INTERVAL_MS = &quot;bulk.flush.interval.ms&quot;;&lt;br/&gt;
    +	public static final String CONFIG_KEY_BULK_FLUSH_BACKOFF_ENABLE = &quot;bulk.flush.backoff.enable&quot;;&lt;br/&gt;
    +	public static final String CONFIG_KEY_BULK_FLUSH_BACKOFF_TYPE = &quot;bulk.flush.backoff.type&quot;;&lt;br/&gt;
    +	public static final String CONFIG_KEY_BULK_FLUSH_BACKOFF_RETRIES = &quot;bulk.flush.backoff.retries&quot;;&lt;br/&gt;
    +	public static final String CONFIG_KEY_BULK_FLUSH_BACKOFF_DELAY = &quot;bulk.flush.backoff.delay&quot;;&lt;br/&gt;
    +&lt;br/&gt;
    +	public enum FlushBackoffType &lt;/p&gt;
{
    +		CONSTANT,
    +		EXPONENTIAL
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	public class BulkFlushBackoffPolicy implements Serializable {&lt;br/&gt;
    +&lt;br/&gt;
    +		private static final long serialVersionUID = -6022851996101826049L;&lt;br/&gt;
    +&lt;br/&gt;
    +		// the default values follow the Elasticsearch default settings for BulkProcessor&lt;br/&gt;
    +		private FlushBackoffType backoffType = FlushBackoffType.EXPONENTIAL;&lt;br/&gt;
    +		private int maxRetryCount = 8;&lt;br/&gt;
    +		private long delayMillis = 50;&lt;br/&gt;
    +&lt;br/&gt;
    +		public FlushBackoffType getBackoffType() &lt;/p&gt;
{
    +			return backoffType;
    +		}
&lt;p&gt;    +&lt;br/&gt;
    +		public int getMaxRetryCount() &lt;/p&gt;
{
    +			return maxRetryCount;
    +		}
&lt;p&gt;    +&lt;br/&gt;
    +		public long getDelayMillis() &lt;/p&gt;
{
    +			return delayMillis;
    +		}
&lt;p&gt;    +&lt;br/&gt;
    +		public void setBackoffType(FlushBackoffType backoffType) &lt;/p&gt;
{
    +			this.backoffType = checkNotNull(backoffType);
    +		}
&lt;p&gt;    +&lt;br/&gt;
    +		public void setMaxRetryCount(int maxRetryCount) &lt;/p&gt;
{
    +			checkArgument(maxRetryCount &amp;gt; 0);
    +			this.maxRetryCount = maxRetryCount;
    +		}
&lt;p&gt;    +&lt;br/&gt;
    +		public void setDelayMillis(long delayMillis) {&lt;br/&gt;
    +			checkArgument(delayMillis &amp;gt; 0);&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    True, 0 should be acceptable. Nice catches.&lt;/p&gt;</comment>
                            <comment id="15875807" author="githubbot" created="Tue, 21 Feb 2017 11:14:10 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102184175&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102184175&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java &amp;#8212;&lt;br/&gt;
    @@ -165,20 +286,36 @@ public void beforeBulk(long executionId, BulkRequest request) { }&lt;br/&gt;
     				@Override&lt;br/&gt;
     				public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {&lt;br/&gt;
     					if (response.hasFailures()) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (BulkItemResponse itemResp : response.getItems()) {&lt;/li&gt;
	&lt;li&gt;Throwable failure = callBridge.extractFailureCauseFromBulkItemResponse(itemResp);&lt;br/&gt;
    +						BulkItemResponse itemResponse;&lt;br/&gt;
    +						Throwable failure;&lt;br/&gt;
    +&lt;br/&gt;
    +						for (int i = 0; i &amp;lt; response.getItems().length; i++) {&lt;br/&gt;
    +							itemResponse = response.getItems()&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt;;&lt;br/&gt;
    +							failure = callBridge.extractFailureCauseFromBulkItemResponse(itemResponse);&lt;br/&gt;
     							if (failure != null) {&lt;/li&gt;
	&lt;li&gt;LOG.error(&quot;Failed Elasticsearch item request: {}&quot;, failure.getMessage(), failure);&lt;/li&gt;
	&lt;li&gt;failureThrowable.compareAndSet(null, failure);&lt;br/&gt;
    +								LOG.error(&quot;Failed Elasticsearch item request: {}&quot;, itemResponse.getFailureMessage(), failure);&lt;br/&gt;
    +&lt;br/&gt;
    +								if (failureHandler.onFailure(request.requests().get&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;, failure, requestIndexer)) 
{
    +									failureThrowable.compareAndSet(null, failure);
    +								}
&lt;p&gt;     							}&lt;br/&gt;
     						}&lt;br/&gt;
     					}&lt;br/&gt;
    +&lt;br/&gt;
    +					numPendingRequests.getAndAdd(-request.numberOfActions());&lt;br/&gt;
     				}&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     				@Override&lt;br/&gt;
     				public void afterBulk(long executionId, BulkRequest request, Throwable failure) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;LOG.error(&quot;Failed Elasticsearch bulk request: {}&quot;, failure.getMessage(), failure);&lt;/li&gt;
	&lt;li&gt;failureThrowable.compareAndSet(null, failure);&lt;br/&gt;
    +					LOG.error(&quot;Failed Elasticsearch bulk request: {}&quot;, failure.getMessage(), failure.getCause());&lt;br/&gt;
    +&lt;br/&gt;
    +					// whole bulk request failures are usually just temporary timeouts on&lt;br/&gt;
    +					// the Elasticsearch side; simply retry all action requests in the bulk&lt;br/&gt;
    +					for (ActionRequest action : request.requests()) 
{
    +						requestIndexer.add(action);
    +					}
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    I&apos;ll need to double check this. The ES documents don&apos;t say much about this.&lt;/p&gt;</comment>
                            <comment id="15875810" author="githubbot" created="Tue, 21 Feb 2017 11:17:14 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102184622&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102184622&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java &amp;#8212;&lt;br/&gt;
    @@ -165,20 +286,36 @@ public void beforeBulk(long executionId, BulkRequest request) { }&lt;br/&gt;
     				@Override&lt;br/&gt;
     				public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {&lt;br/&gt;
     					if (response.hasFailures()) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (BulkItemResponse itemResp : response.getItems()) {&lt;/li&gt;
	&lt;li&gt;Throwable failure = callBridge.extractFailureCauseFromBulkItemResponse(itemResp);&lt;br/&gt;
    +						BulkItemResponse itemResponse;&lt;br/&gt;
    +						Throwable failure;&lt;br/&gt;
    +&lt;br/&gt;
    +						for (int i = 0; i &amp;lt; response.getItems().length; i++) {&lt;br/&gt;
    +							itemResponse = response.getItems()&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt;;&lt;br/&gt;
    +							failure = callBridge.extractFailureCauseFromBulkItemResponse(itemResponse);&lt;br/&gt;
     							if (failure != null) {&lt;/li&gt;
	&lt;li&gt;LOG.error(&quot;Failed Elasticsearch item request: {}&quot;, failure.getMessage(), failure);&lt;/li&gt;
	&lt;li&gt;failureThrowable.compareAndSet(null, failure);&lt;br/&gt;
    +								LOG.error(&quot;Failed Elasticsearch item request: {}&quot;, itemResponse.getFailureMessage(), failure);&lt;br/&gt;
    +&lt;br/&gt;
    +								if (failureHandler.onFailure(request.requests().get&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;, failure, requestIndexer)) 
{
    +									failureThrowable.compareAndSet(null, failure);
    +								}
&lt;p&gt;     							}&lt;br/&gt;
     						}&lt;br/&gt;
     					}&lt;br/&gt;
    +&lt;br/&gt;
    +					numPendingRequests.getAndAdd(-request.numberOfActions());&lt;br/&gt;
     				}&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     				@Override&lt;br/&gt;
     				public void afterBulk(long executionId, BulkRequest request, Throwable failure) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;LOG.error(&quot;Failed Elasticsearch bulk request: {}&quot;, failure.getMessage(), failure);&lt;/li&gt;
	&lt;li&gt;failureThrowable.compareAndSet(null, failure);&lt;br/&gt;
    +					LOG.error(&quot;Failed Elasticsearch bulk request: {}&quot;, failure.getMessage(), failure.getCause());&lt;br/&gt;
    +&lt;br/&gt;
    +					// whole bulk request failures are usually just temporary timeouts on&lt;br/&gt;
    +					// the Elasticsearch side; simply retry all action requests in the bulk&lt;br/&gt;
    +					for (ActionRequest action : request.requests()) 
{
    +						requestIndexer.add(action);
    +					}
&lt;p&gt;    +&lt;br/&gt;
    +					numPendingRequests.getAndAdd(-request.numberOfActions());&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    The `BulkProcessorIndexer` will increment `numPendingRequests` whenever the user calls `add(ActionRequest)`. So, in your description, when the user re-adds the 500 requests, `numPendingRequests` first becomes `500+500=1000`. Then, we consider the failed 500 requests to have completed when this line is reached, so `numPendingRequests` becomes `1000-500=500`.&lt;/p&gt;</comment>
                            <comment id="15875819" author="githubbot" created="Tue, 21 Feb 2017 11:22:46 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102185551&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102185551&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java &amp;#8212;&lt;br/&gt;
    @@ -165,20 +286,36 @@ public void beforeBulk(long executionId, BulkRequest request) { }&lt;br/&gt;
     				@Override&lt;br/&gt;
     				public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {&lt;br/&gt;
     					if (response.hasFailures()) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (BulkItemResponse itemResp : response.getItems()) {&lt;/li&gt;
	&lt;li&gt;Throwable failure = callBridge.extractFailureCauseFromBulkItemResponse(itemResp);&lt;br/&gt;
    +						BulkItemResponse itemResponse;&lt;br/&gt;
    +						Throwable failure;&lt;br/&gt;
    +&lt;br/&gt;
    +						for (int i = 0; i &amp;lt; response.getItems().length; i++) {&lt;br/&gt;
    +							itemResponse = response.getItems()&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt;;&lt;br/&gt;
    +							failure = callBridge.extractFailureCauseFromBulkItemResponse(itemResponse);&lt;br/&gt;
     							if (failure != null) {&lt;/li&gt;
	&lt;li&gt;LOG.error(&quot;Failed Elasticsearch item request: {}&quot;, failure.getMessage(), failure);&lt;/li&gt;
	&lt;li&gt;failureThrowable.compareAndSet(null, failure);&lt;br/&gt;
    +								LOG.error(&quot;Failed Elasticsearch item request: {}&quot;, itemResponse.getFailureMessage(), failure);&lt;br/&gt;
    +&lt;br/&gt;
    +								if (failureHandler.onFailure(request.requests().get&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;, failure, requestIndexer)) {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    That&apos;s actually a good idea, I didn&apos;t think it that way. I would like to change it to throw a `Throwable` instead.&lt;/p&gt;</comment>
                            <comment id="15875822" author="githubbot" created="Tue, 21 Feb 2017 11:24:21 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102185790&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102185790&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java &amp;#8212;&lt;br/&gt;
    @@ -165,20 +286,36 @@ public void beforeBulk(long executionId, BulkRequest request) { }&lt;br/&gt;
     				@Override&lt;br/&gt;
     				public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {&lt;br/&gt;
     					if (response.hasFailures()) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (BulkItemResponse itemResp : response.getItems()) {&lt;/li&gt;
	&lt;li&gt;Throwable failure = callBridge.extractFailureCauseFromBulkItemResponse(itemResp);&lt;br/&gt;
    +						BulkItemResponse itemResponse;&lt;br/&gt;
    +						Throwable failure;&lt;br/&gt;
    +&lt;br/&gt;
    +						for (int i = 0; i &amp;lt; response.getItems().length; i++) {&lt;br/&gt;
    +							itemResponse = response.getItems()&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt;;&lt;br/&gt;
    +							failure = callBridge.extractFailureCauseFromBulkItemResponse(itemResponse);&lt;br/&gt;
     							if (failure != null) {&lt;/li&gt;
	&lt;li&gt;LOG.error(&quot;Failed Elasticsearch item request: {}&quot;, failure.getMessage(), failure);&lt;/li&gt;
	&lt;li&gt;failureThrowable.compareAndSet(null, failure);&lt;br/&gt;
    +								LOG.error(&quot;Failed Elasticsearch item request: {}&quot;, itemResponse.getFailureMessage(), failure);&lt;br/&gt;
    +&lt;br/&gt;
    +								if (failureHandler.onFailure(request.requests().get&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;, failure, requestIndexer)) {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    For the use case you mentioned, that would mean the user implements a stateful `ActionRequestFailureHandler`, with its state being the number of failures so far, correct?&lt;/p&gt;

&lt;p&gt;    I didn&apos;t think about this too much, but I guess there shouldn&apos;t be a problem for this.&lt;/p&gt;</comment>
                            <comment id="15875825" author="githubbot" created="Tue, 21 Feb 2017 11:26:29 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102186137&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102186137&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: docs/dev/connectors/elasticsearch.md &amp;#8212;&lt;br/&gt;
    @@ -272,9 +308,115 @@ input.addSink(new ElasticsearchSink(config, new ElasticsearchSinkFunction[String&lt;br/&gt;
     The difference is that now we do not need to provide a list of addresses&lt;br/&gt;
     of Elasticsearch nodes.&lt;/p&gt;

&lt;p&gt;    +### Handling Failing Elasticsearch Requests&lt;br/&gt;
    +&lt;br/&gt;
    +Elasticsearch action requests may fail due to a variety of reasons, including&lt;br/&gt;
    +temporarily saturated node queue capacity or malformed documents to be indexed.&lt;br/&gt;
    +The Flink Elasticsearch Sink allows the user to specify how request&lt;br/&gt;
    +failures are handled, by simply implementing an `ActionRequestFailureHandler` and&lt;br/&gt;
    +providing it to the constructor.&lt;br/&gt;
    +&lt;br/&gt;
    +Below is an example:&lt;br/&gt;
    +&lt;br/&gt;
    +&amp;lt;div class=&quot;codetabs&quot; markdown=&quot;1&quot;&amp;gt;&lt;br/&gt;
    +&amp;lt;div data-lang=&quot;java&quot; markdown=&quot;1&quot;&amp;gt;&lt;br/&gt;
    +&lt;/p&gt;
{% highlight java %}
&lt;p&gt;    +DataStream&amp;lt;String&amp;gt; input = ...;&lt;br/&gt;
    +&lt;br/&gt;
    +input.addSink(new ElasticsearchSink&amp;lt;&amp;gt;(&lt;br/&gt;
    +    config, transportAddresses,&lt;br/&gt;
    +    new ElasticsearchSinkFunction&amp;lt;String&amp;gt;() &lt;/p&gt;
{...},&lt;br/&gt;
    +    new ActionRequestFailureHandler() {&lt;br/&gt;
    +        @Override&lt;br/&gt;
    +        boolean onFailure(ActionRequest action, Throwable failure, RequestIndexer indexer) {&lt;br/&gt;
    +            // this example uses Apache Commons to search for nested exceptions&lt;br/&gt;
    +            &lt;br/&gt;
    +            if (ExceptionUtils.indexOfThrowable(failure, EsRejectedExecutionException.class) &amp;gt;= 0) {
    +                // full queue; re-add document for indexing
    +                indexer.add(action);
    +                return false;
    +            } else if (ExceptionUtils.indexOfThrowable(failure, ElasticsearchParseException.class) &amp;gt;= 0) {
    +                // malformed document; simply drop request without failing sink
    +                return false;
    +            } else {
    +                // for all other failures, fail the sink
    +                return true;
    +            }&lt;br/&gt;
    +        }&lt;br/&gt;
    +}));&lt;br/&gt;
    +{% endhighlight %}&lt;br/&gt;
    +&amp;lt;/div&amp;gt;&lt;br/&gt;
    +&amp;lt;div data-lang=&quot;scala&quot; markdown=&quot;1&quot;&amp;gt;&lt;br/&gt;
    +{% highlight scala %}&lt;br/&gt;
    +val input: DataStream&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt; = ...&lt;br/&gt;
    +&lt;br/&gt;
    +input.addSink(new ElasticsearchSink(&lt;br/&gt;
    +    config, transportAddresses,&lt;br/&gt;
    +    new ElasticsearchSinkFunction&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt; {...}
&lt;p&gt;,&lt;br/&gt;
    +    new ActionRequestFailureHandler {&lt;br/&gt;
    +        override def onFailure(ActionRequest action, Throwable failure, RequestIndexer indexer) {&lt;br/&gt;
    +            // this example uses Apache Commons to search for nested exceptions&lt;br/&gt;
    +&lt;br/&gt;
    +            if (ExceptionUtils.indexOfThrowable(failure, EsRejectedExecutionException.class) &amp;gt;= 0) &lt;/p&gt;
{
    +                // full queue; re-add document for indexing
    +                indexer.add(action)
    +                return false
    +            }
&lt;p&gt; else if (ExceptionUtils.indexOfThrowable(failure, ElasticsearchParseException.class) &lt;/p&gt;
{
    +                // malformed document; simply drop request without failing sink
    +                return false
    +            }
&lt;p&gt; else &lt;/p&gt;
{
    +                // for all other failures, fail the sink
    +                return true
    +            }
&lt;p&gt;    +        }&lt;br/&gt;
    +}))&lt;br/&gt;
    +&lt;/p&gt;
{% endhighlight %}
&lt;p&gt;    +&amp;lt;/div&amp;gt;&lt;br/&gt;
    +&amp;lt;/div&amp;gt;&lt;br/&gt;
    +&lt;br/&gt;
    +The above example will let the sink re-add requests that failed due to&lt;br/&gt;
    +queue capacity saturation and drop requests with malformed documents, without&lt;br/&gt;
    +failing the sink. For all other failures, the sink will fail. If a `ActionRequestFailureHandler`&lt;br/&gt;
    +is not provided to the constructor, the sink will fail for any kind of error.&lt;br/&gt;
    +&lt;br/&gt;
    +Note that `onFailure` is called for failures that still occur only after the&lt;br/&gt;
    +`BulkProcessor` internally finishes all backoff retry attempts.&lt;br/&gt;
    +By default, the `BulkProcessor` retries to a maximum of 8 attempts with&lt;br/&gt;
    +an exponential backoff. For more information on the behaviour of the&lt;br/&gt;
    +internal `BulkProcessor` and how to configure it, please see the following section.&lt;br/&gt;
    +&lt;br/&gt;
    +&amp;lt;p style=&quot;border-radius: 5px; padding: 5px&quot; class=&quot;bg-danger&quot;&amp;gt;&lt;br/&gt;
    +&amp;lt;b&amp;gt;IMPORTANT&amp;lt;/b&amp;gt;: Re-adding requests back to the internal &amp;lt;b&amp;gt;BulkProcessor&amp;lt;/b&amp;gt;&lt;br/&gt;
    +on failures will lead to longer checkpoints, as the sink will also&lt;br/&gt;
    +need to wait for the re-added requests to be flushed when checkpointing.&lt;br/&gt;
    +This also means that if re-added requests never succeed, the checkpoint will&lt;br/&gt;
    +never finish.&lt;br/&gt;
    +&amp;lt;/p&amp;gt;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I wouldn&apos;t suggest adding a `ActionRequestFailureHandler` that out-of-the-box retries for all exceptions, though. That could let users easily overlook some exceptions that simply cannot be retried without custom logic (for example, malformed documents with wrong field types).&lt;/p&gt;</comment>
                            <comment id="15875944" author="githubbot" created="Tue, 21 Feb 2017 13:15:02 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102203605&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102203605&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: docs/dev/connectors/elasticsearch.md &amp;#8212;&lt;br/&gt;
    @@ -272,9 +308,115 @@ input.addSink(new ElasticsearchSink(config, new ElasticsearchSinkFunction[String&lt;br/&gt;
     The difference is that now we do not need to provide a list of addresses&lt;br/&gt;
     of Elasticsearch nodes.&lt;/p&gt;

&lt;p&gt;    +### Handling Failing Elasticsearch Requests&lt;br/&gt;
    +&lt;br/&gt;
    +Elasticsearch action requests may fail due to a variety of reasons, including&lt;br/&gt;
    +temporarily saturated node queue capacity or malformed documents to be indexed.&lt;br/&gt;
    +The Flink Elasticsearch Sink allows the user to specify how request&lt;br/&gt;
    +failures are handled, by simply implementing an `ActionRequestFailureHandler` and&lt;br/&gt;
    +providing it to the constructor.&lt;br/&gt;
    +&lt;br/&gt;
    +Below is an example:&lt;br/&gt;
    +&lt;br/&gt;
    +&amp;lt;div class=&quot;codetabs&quot; markdown=&quot;1&quot;&amp;gt;&lt;br/&gt;
    +&amp;lt;div data-lang=&quot;java&quot; markdown=&quot;1&quot;&amp;gt;&lt;br/&gt;
    +&lt;/p&gt;
{% highlight java %}
&lt;p&gt;    +DataStream&amp;lt;String&amp;gt; input = ...;&lt;br/&gt;
    +&lt;br/&gt;
    +input.addSink(new ElasticsearchSink&amp;lt;&amp;gt;(&lt;br/&gt;
    +    config, transportAddresses,&lt;br/&gt;
    +    new ElasticsearchSinkFunction&amp;lt;String&amp;gt;() &lt;/p&gt;
{...},&lt;br/&gt;
    +    new ActionRequestFailureHandler() {&lt;br/&gt;
    +        @Override&lt;br/&gt;
    +        boolean onFailure(ActionRequest action, Throwable failure, RequestIndexer indexer) {&lt;br/&gt;
    +            // this example uses Apache Commons to search for nested exceptions&lt;br/&gt;
    +            &lt;br/&gt;
    +            if (ExceptionUtils.indexOfThrowable(failure, EsRejectedExecutionException.class) &amp;gt;= 0) {
    +                // full queue; re-add document for indexing
    +                indexer.add(action);
    +                return false;
    +            } else if (ExceptionUtils.indexOfThrowable(failure, ElasticsearchParseException.class) &amp;gt;= 0) {
    +                // malformed document; simply drop request without failing sink
    +                return false;
    +            } else {
    +                // for all other failures, fail the sink
    +                return true;
    +            }&lt;br/&gt;
    +        }&lt;br/&gt;
    +}));&lt;br/&gt;
    +{% endhighlight %}&lt;br/&gt;
    +&amp;lt;/div&amp;gt;&lt;br/&gt;
    +&amp;lt;div data-lang=&quot;scala&quot; markdown=&quot;1&quot;&amp;gt;&lt;br/&gt;
    +{% highlight scala %}&lt;br/&gt;
    +val input: DataStream&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt; = ...&lt;br/&gt;
    +&lt;br/&gt;
    +input.addSink(new ElasticsearchSink(&lt;br/&gt;
    +    config, transportAddresses,&lt;br/&gt;
    +    new ElasticsearchSinkFunction&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt; {...}
&lt;p&gt;,&lt;br/&gt;
    +    new ActionRequestFailureHandler {&lt;br/&gt;
    +        override def onFailure(ActionRequest action, Throwable failure, RequestIndexer indexer) {&lt;br/&gt;
    +            // this example uses Apache Commons to search for nested exceptions&lt;br/&gt;
    +&lt;br/&gt;
    +            if (ExceptionUtils.indexOfThrowable(failure, EsRejectedExecutionException.class) &amp;gt;= 0) &lt;/p&gt;
{
    +                // full queue; re-add document for indexing
    +                indexer.add(action)
    +                return false
    +            }
&lt;p&gt; else if (ExceptionUtils.indexOfThrowable(failure, ElasticsearchParseException.class) &lt;/p&gt;
{
    +                // malformed document; simply drop request without failing sink
    +                return false
    +            }
&lt;p&gt; else &lt;/p&gt;
{
    +                // for all other failures, fail the sink
    +                return true
    +            }
&lt;p&gt;    +        }&lt;br/&gt;
    +}))&lt;br/&gt;
    +&lt;/p&gt;
{% endhighlight %}
&lt;p&gt;    +&amp;lt;/div&amp;gt;&lt;br/&gt;
    +&amp;lt;/div&amp;gt;&lt;br/&gt;
    +&lt;br/&gt;
    +The above example will let the sink re-add requests that failed due to&lt;br/&gt;
    +queue capacity saturation and drop requests with malformed documents, without&lt;br/&gt;
    +failing the sink. For all other failures, the sink will fail. If a `ActionRequestFailureHandler`&lt;br/&gt;
    +is not provided to the constructor, the sink will fail for any kind of error.&lt;br/&gt;
    +&lt;br/&gt;
    +Note that `onFailure` is called for failures that still occur only after the&lt;br/&gt;
    +`BulkProcessor` internally finishes all backoff retry attempts.&lt;br/&gt;
    +By default, the `BulkProcessor` retries to a maximum of 8 attempts with&lt;br/&gt;
    +an exponential backoff. For more information on the behaviour of the&lt;br/&gt;
    +internal `BulkProcessor` and how to configure it, please see the following section.&lt;br/&gt;
    +&lt;br/&gt;
    +&amp;lt;p style=&quot;border-radius: 5px; padding: 5px&quot; class=&quot;bg-danger&quot;&amp;gt;&lt;br/&gt;
    +&amp;lt;b&amp;gt;IMPORTANT&amp;lt;/b&amp;gt;: Re-adding requests back to the internal &amp;lt;b&amp;gt;BulkProcessor&amp;lt;/b&amp;gt;&lt;br/&gt;
    +on failures will lead to longer checkpoints, as the sink will also&lt;br/&gt;
    +need to wait for the re-added requests to be flushed when checkpointing.&lt;br/&gt;
    +This also means that if re-added requests never succeed, the checkpoint will&lt;br/&gt;
    +never finish.&lt;br/&gt;
    +&amp;lt;/p&amp;gt;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I agree that we should provide a reasonable default behavior, instead of just retrying.&lt;/p&gt;</comment>
                            <comment id="15875948" author="githubbot" created="Tue, 21 Feb 2017 13:19:34 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102204385&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102204385&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java &amp;#8212;&lt;br/&gt;
    @@ -165,20 +286,36 @@ public void beforeBulk(long executionId, BulkRequest request) { }&lt;br/&gt;
     				@Override&lt;br/&gt;
     				public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {&lt;br/&gt;
     					if (response.hasFailures()) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (BulkItemResponse itemResp : response.getItems()) {&lt;/li&gt;
	&lt;li&gt;Throwable failure = callBridge.extractFailureCauseFromBulkItemResponse(itemResp);&lt;br/&gt;
    +						BulkItemResponse itemResponse;&lt;br/&gt;
    +						Throwable failure;&lt;br/&gt;
    +&lt;br/&gt;
    +						for (int i = 0; i &amp;lt; response.getItems().length; i++) {&lt;br/&gt;
    +							itemResponse = response.getItems()&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt;;&lt;br/&gt;
    +							failure = callBridge.extractFailureCauseFromBulkItemResponse(itemResponse);&lt;br/&gt;
     							if (failure != null) {&lt;/li&gt;
	&lt;li&gt;LOG.error(&quot;Failed Elasticsearch item request: {}&quot;, failure.getMessage(), failure);&lt;/li&gt;
	&lt;li&gt;failureThrowable.compareAndSet(null, failure);&lt;br/&gt;
    +								LOG.error(&quot;Failed Elasticsearch item request: {}&quot;, itemResponse.getFailureMessage(), failure);&lt;br/&gt;
    +&lt;br/&gt;
    +								if (failureHandler.onFailure(request.requests().get&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;, failure, requestIndexer)) {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Mh. I don&apos;t know if the use case I&apos;ve mentioned makes a lot of sense. Probably most of the users just want to use a custom logic to decide how to do the retries / discards.&lt;/p&gt;

&lt;p&gt;    I think we shouldn&apos;t do complicated things like checkpointing the state of the failure handler. Its good enough if the user keeps it locally (and loses it on failure)&lt;/p&gt;
</comment>
                            <comment id="15875951" author="githubbot" created="Tue, 21 Feb 2017 13:20:40 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102204579&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102204579&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java &amp;#8212;&lt;br/&gt;
    @@ -165,20 +286,36 @@ public void beforeBulk(long executionId, BulkRequest request) { }&lt;br/&gt;
     				@Override&lt;br/&gt;
     				public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {&lt;br/&gt;
     					if (response.hasFailures()) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (BulkItemResponse itemResp : response.getItems()) {&lt;/li&gt;
	&lt;li&gt;Throwable failure = callBridge.extractFailureCauseFromBulkItemResponse(itemResp);&lt;br/&gt;
    +						BulkItemResponse itemResponse;&lt;br/&gt;
    +						Throwable failure;&lt;br/&gt;
    +&lt;br/&gt;
    +						for (int i = 0; i &amp;lt; response.getItems().length; i++) {&lt;br/&gt;
    +							itemResponse = response.getItems()&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt;;&lt;br/&gt;
    +							failure = callBridge.extractFailureCauseFromBulkItemResponse(itemResponse);&lt;br/&gt;
     							if (failure != null) {&lt;/li&gt;
	&lt;li&gt;LOG.error(&quot;Failed Elasticsearch item request: {}&quot;, failure.getMessage(), failure);&lt;/li&gt;
	&lt;li&gt;failureThrowable.compareAndSet(null, failure);&lt;br/&gt;
    +								LOG.error(&quot;Failed Elasticsearch item request: {}&quot;, itemResponse.getFailureMessage(), failure);&lt;br/&gt;
    +&lt;br/&gt;
    +								if (failureHandler.onFailure(request.requests().get&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;, failure, requestIndexer)) 
{
    +									failureThrowable.compareAndSet(null, failure);
    +								}
&lt;p&gt;     							}&lt;br/&gt;
     						}&lt;br/&gt;
     					}&lt;br/&gt;
    +&lt;br/&gt;
    +					numPendingRequests.getAndAdd(-request.numberOfActions());&lt;br/&gt;
     				}&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     				@Override&lt;br/&gt;
     				public void afterBulk(long executionId, BulkRequest request, Throwable failure) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;LOG.error(&quot;Failed Elasticsearch bulk request: {}&quot;, failure.getMessage(), failure);&lt;/li&gt;
	&lt;li&gt;failureThrowable.compareAndSet(null, failure);&lt;br/&gt;
    +					LOG.error(&quot;Failed Elasticsearch bulk request: {}&quot;, failure.getMessage(), failure.getCause());&lt;br/&gt;
    +&lt;br/&gt;
    +					// whole bulk request failures are usually just temporary timeouts on&lt;br/&gt;
    +					// the Elasticsearch side; simply retry all action requests in the bulk&lt;br/&gt;
    +					for (ActionRequest action : request.requests()) 
{
    +						requestIndexer.add(action);
    +					}
&lt;p&gt;    +&lt;br/&gt;
    +					numPendingRequests.getAndAdd(-request.numberOfActions());&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Puh, that&apos;s good &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; Thx for the explanation.&lt;br/&gt;
    I didn&apos;t look close enough on your changes.&lt;/p&gt;</comment>
                            <comment id="15875954" author="githubbot" created="Tue, 21 Feb 2017 13:22:03 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102204839&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102204839&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java &amp;#8212;&lt;br/&gt;
    @@ -165,20 +286,36 @@ public void beforeBulk(long executionId, BulkRequest request) { }&lt;br/&gt;
     				@Override&lt;br/&gt;
     				public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {&lt;br/&gt;
     					if (response.hasFailures()) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (BulkItemResponse itemResp : response.getItems()) {&lt;/li&gt;
	&lt;li&gt;Throwable failure = callBridge.extractFailureCauseFromBulkItemResponse(itemResp);&lt;br/&gt;
    +						BulkItemResponse itemResponse;&lt;br/&gt;
    +						Throwable failure;&lt;br/&gt;
    +&lt;br/&gt;
    +						for (int i = 0; i &amp;lt; response.getItems().length; i++) {&lt;br/&gt;
    +							itemResponse = response.getItems()&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt;;&lt;br/&gt;
    +							failure = callBridge.extractFailureCauseFromBulkItemResponse(itemResponse);&lt;br/&gt;
     							if (failure != null) {&lt;/li&gt;
	&lt;li&gt;LOG.error(&quot;Failed Elasticsearch item request: {}&quot;, failure.getMessage(), failure);&lt;/li&gt;
	&lt;li&gt;failureThrowable.compareAndSet(null, failure);&lt;br/&gt;
    +								LOG.error(&quot;Failed Elasticsearch item request: {}&quot;, itemResponse.getFailureMessage(), failure);&lt;br/&gt;
    +&lt;br/&gt;
    +								if (failureHandler.onFailure(request.requests().get&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;, failure, requestIndexer)) 
{
    +									failureThrowable.compareAndSet(null, failure);
    +								}
&lt;p&gt;     							}&lt;br/&gt;
     						}&lt;br/&gt;
     					}&lt;br/&gt;
    +&lt;br/&gt;
    +					numPendingRequests.getAndAdd(-request.numberOfActions());&lt;br/&gt;
     				}&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     				@Override&lt;br/&gt;
     				public void afterBulk(long executionId, BulkRequest request, Throwable failure) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;LOG.error(&quot;Failed Elasticsearch bulk request: {}&quot;, failure.getMessage(), failure);&lt;/li&gt;
	&lt;li&gt;failureThrowable.compareAndSet(null, failure);&lt;br/&gt;
    +					LOG.error(&quot;Failed Elasticsearch bulk request: {}&quot;, failure.getMessage(), failure.getCause());&lt;br/&gt;
    +&lt;br/&gt;
    +					// whole bulk request failures are usually just temporary timeouts on&lt;br/&gt;
    +					// the Elasticsearch side; simply retry all action requests in the bulk&lt;br/&gt;
    +					for (ActionRequest action : request.requests()) 
{
    +						requestIndexer.add(action);
    +					}
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Thank you. I&apos;m undecided if we want to add this here or not.&lt;br/&gt;
    Just based on my experience with the Kafka connector, at some point there is a user who wants to have a very specific custom behavior &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; But we can also keep it as is and fix it if a user needs it (worst case: they have to override our implementation)&lt;/p&gt;</comment>
                            <comment id="15875979" author="githubbot" created="Tue, 21 Feb 2017 13:40:40 +0000"  >&lt;p&gt;Github user rmetzger commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I finished checking the code.&lt;br/&gt;
    The only thing I&apos;m missing from the change is a test case ensuring that the implementation works.&lt;/p&gt;

&lt;p&gt;    I think we can build a test similar to what we did with Kafka. (With a mock producer)&lt;/p&gt;</comment>
                            <comment id="15876148" author="githubbot" created="Tue, 21 Feb 2017 15:26:02 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102233292&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102233292&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java &amp;#8212;&lt;br/&gt;
    @@ -165,20 +286,36 @@ public void beforeBulk(long executionId, BulkRequest request) { }&lt;br/&gt;
     				@Override&lt;br/&gt;
     				public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {&lt;br/&gt;
     					if (response.hasFailures()) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (BulkItemResponse itemResp : response.getItems()) {&lt;/li&gt;
	&lt;li&gt;Throwable failure = callBridge.extractFailureCauseFromBulkItemResponse(itemResp);&lt;br/&gt;
    +						BulkItemResponse itemResponse;&lt;br/&gt;
    +						Throwable failure;&lt;br/&gt;
    +&lt;br/&gt;
    +						for (int i = 0; i &amp;lt; response.getItems().length; i++) {&lt;br/&gt;
    +							itemResponse = response.getItems()&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt;;&lt;br/&gt;
    +							failure = callBridge.extractFailureCauseFromBulkItemResponse(itemResponse);&lt;br/&gt;
     							if (failure != null) {&lt;/li&gt;
	&lt;li&gt;LOG.error(&quot;Failed Elasticsearch item request: {}&quot;, failure.getMessage(), failure);&lt;/li&gt;
	&lt;li&gt;failureThrowable.compareAndSet(null, failure);&lt;br/&gt;
    +								LOG.error(&quot;Failed Elasticsearch item request: {}&quot;, itemResponse.getFailureMessage(), failure);&lt;br/&gt;
    +&lt;br/&gt;
    +								if (failureHandler.onFailure(request.requests().get&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;, failure, requestIndexer)) {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Alright, then I&apos;ll simply just change the `boolean` return usage to throwing a `Throwable`, and add some Javadoc that any state in the failure handler is volatile.&lt;/p&gt;</comment>
                            <comment id="15876150" author="githubbot" created="Tue, 21 Feb 2017 15:28:36 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Yes, I overlooked adding tests for this.&lt;/p&gt;

&lt;p&gt;    Thanks a lot for the reviews @rmetzger! I&apos;ll address your comments and tests for the additional features.&lt;br/&gt;
    Will ping you once it&apos;s ready for another review.&lt;/p&gt;</comment>
                            <comment id="15877698" author="githubbot" created="Wed, 22 Feb 2017 07:46:08 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102400283&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102400283&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java &amp;#8212;&lt;br/&gt;
    @@ -165,20 +286,36 @@ public void beforeBulk(long executionId, BulkRequest request) { }&lt;br/&gt;
     				@Override&lt;br/&gt;
     				public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {&lt;br/&gt;
     					if (response.hasFailures()) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (BulkItemResponse itemResp : response.getItems()) {&lt;/li&gt;
	&lt;li&gt;Throwable failure = callBridge.extractFailureCauseFromBulkItemResponse(itemResp);&lt;br/&gt;
    +						BulkItemResponse itemResponse;&lt;br/&gt;
    +						Throwable failure;&lt;br/&gt;
    +&lt;br/&gt;
    +						for (int i = 0; i &amp;lt; response.getItems().length; i++) {&lt;br/&gt;
    +							itemResponse = response.getItems()&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt;;&lt;br/&gt;
    +							failure = callBridge.extractFailureCauseFromBulkItemResponse(itemResponse);&lt;br/&gt;
     							if (failure != null) {&lt;/li&gt;
	&lt;li&gt;LOG.error(&quot;Failed Elasticsearch item request: {}&quot;, failure.getMessage(), failure);&lt;/li&gt;
	&lt;li&gt;failureThrowable.compareAndSet(null, failure);&lt;br/&gt;
    +								LOG.error(&quot;Failed Elasticsearch item request: {}&quot;, itemResponse.getFailureMessage(), failure);&lt;br/&gt;
    +&lt;br/&gt;
    +								if (failureHandler.onFailure(request.requests().get&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;, failure, requestIndexer)) 
{
    +									failureThrowable.compareAndSet(null, failure);
    +								}
&lt;p&gt;     							}&lt;br/&gt;
     						}&lt;br/&gt;
     					}&lt;br/&gt;
    +&lt;br/&gt;
    +					numPendingRequests.getAndAdd(-request.numberOfActions());&lt;br/&gt;
     				}&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     				@Override&lt;br/&gt;
     				public void afterBulk(long executionId, BulkRequest request, Throwable failure) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;LOG.error(&quot;Failed Elasticsearch bulk request: {}&quot;, failure.getMessage(), failure);&lt;/li&gt;
	&lt;li&gt;failureThrowable.compareAndSet(null, failure);&lt;br/&gt;
    +					LOG.error(&quot;Failed Elasticsearch bulk request: {}&quot;, failure.getMessage(), failure.getCause());&lt;br/&gt;
    +&lt;br/&gt;
    +					// whole bulk request failures are usually just temporary timeouts on&lt;br/&gt;
    +					// the Elasticsearch side; simply retry all action requests in the bulk&lt;br/&gt;
    +					for (ActionRequest action : request.requests()) 
{
    +						requestIndexer.add(action);
    +					}
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    It seems like we will need to use the failure handler too.&lt;br/&gt;
    Any exception that the Elasticsearch `Client` throws while issuing the bulk request can appear here too. So, exceptions like unreachable node can pop out here as well, and I don&apos;t think we should implicitly treat them as temporary.&lt;/p&gt;</comment>
                            <comment id="15878744" author="githubbot" created="Wed, 22 Feb 2017 17:13:29 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @rmetzger All of your comments have been addressed, and tests for the new features have been added (in `ElasticsearchSinkBaseTest`). Can you take another look? Thanks a lot!&lt;/p&gt;

&lt;p&gt;    Some notes on changes I made that weren&apos;t previously discussed:&lt;br/&gt;
    1. Renamed `NoOpActionRequestFailureHandler` to just `NoOpActionFailureHandler` - less of a mouthful &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
    2. I added the responsed REST status code through the failure handler&apos;s `onFailure(...)` callback. The reason for this is explained in the doc / Javadoc changes of the last follow-up commit (c594523).&lt;/p&gt;</comment>
                            <comment id="15879027" author="githubbot" created="Wed, 22 Feb 2017 19:28:00 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102545769&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102545769&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java &amp;#8212;&lt;br/&gt;
    @@ -211,6 +283,23 @@ public void invoke(T value) throws Exception {&lt;br/&gt;
     	}&lt;/p&gt;

&lt;p&gt;     	@Override&lt;br/&gt;
    +	public void initializeState(FunctionInitializationContext context) throws Exception &lt;/p&gt;
{
    +		// no initialization needed
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public void snapshotState(FunctionSnapshotContext context) throws Exception {&lt;br/&gt;
    +		checkErrorAndRethrow();&lt;br/&gt;
    +&lt;br/&gt;
    +		if (flushOnCheckpoint) {&lt;br/&gt;
    +			do {&lt;br/&gt;
    +				bulkProcessor.flush();&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    This flush() might be a noop if bulkRequest.numberOfActions() == 0 in the bulkProcessor implementation.&lt;br/&gt;
    If so, this loop turns into a busy loop wasting CPU cycles.&lt;br/&gt;
    I wonder if we should wait on the numPendingRequests and notify on it once we update it?&lt;/p&gt;

&lt;p&gt;    (Sorry that I bring this up in the second review)&lt;/p&gt;</comment>
                            <comment id="15879028" author="githubbot" created="Wed, 22 Feb 2017 19:28:00 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102546521&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102546521&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/util/RetryRejectedExecutionFailureHandler.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,46 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.elasticsearch.util;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.elasticsearch.ActionRequestFailureHandler;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.elasticsearch.RequestIndexer;&lt;br/&gt;
    +import org.apache.flink.util.ExceptionUtils;&lt;br/&gt;
    +import org.elasticsearch.action.ActionRequest;&lt;br/&gt;
    +import org.elasticsearch.common.util.concurrent.EsRejectedExecutionException;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * An &lt;/p&gt;
{@link ActionRequestFailureHandler}
&lt;p&gt; that re-adds requests that failed due to temporary&lt;br/&gt;
    + * &lt;/p&gt;
{@link EsRejectedExecutionException}
&lt;p&gt;s (which means that Elasticsearch node queues are currently full),&lt;br/&gt;
    + * and fails for all other failures.&lt;br/&gt;
    + */&lt;br/&gt;
    +public class RetryRejectedExecutionFailureHandler implements ActionRequestFailureHandler {&lt;br/&gt;
    +&lt;br/&gt;
    +	private static final long serialVersionUID = -7423562912824511906L;&lt;br/&gt;
    +&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public void onFailure(ActionRequest action, Throwable failure, int restStatusCode, RequestIndexer indexer) throws Throwable {&lt;br/&gt;
    +		if (ExceptionUtils.containsThrowable(failure, EsRejectedExecutionException.class)) {&lt;br/&gt;
    +			indexer.add(action);&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Do you think this is worth a LOG.debug statement?&lt;br/&gt;
    Or will it happen too often / is too uninformative?&lt;/p&gt;

&lt;p&gt;    I wonder if we could use the metrics system for exposing stuff like error rate, retry rate etc. (Maybe we should file a JIRA for the ElasticSearch connectors to &quot;metricify&quot; them)&lt;/p&gt;</comment>
                            <comment id="15879029" author="githubbot" created="Wed, 22 Feb 2017 19:28:00 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102544239&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102544239&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java &amp;#8212;&lt;br/&gt;
    @@ -92,40 +152,59 @@&lt;br/&gt;
     	/** Call bridge for different version-specfic */&lt;br/&gt;
     	private final ElasticsearchApiCallBridge callBridge;&lt;/p&gt;

&lt;p&gt;    +	/**&lt;br/&gt;
    +	 * Number of pending action requests not yet acknowledged by Elasticsearch.&lt;br/&gt;
    +	 * This value is maintained only if &lt;/p&gt;
{@link ElasticsearchSinkBase#flushOnCheckpoint}
&lt;p&gt; is &lt;/p&gt;
{@code true}
&lt;p&gt;.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * This is incremented whenever the user adds (or re-adds through the &lt;/p&gt;
{@link ActionRequestFailureHandler}
&lt;p&gt;) requests&lt;br/&gt;
    +	 * to the &lt;/p&gt;
{@link RequestIndexer}
&lt;p&gt;. It is decremented for each completed request of a bulk request, in&lt;br/&gt;
    +	 * &lt;/p&gt;
{@link BulkProcessor.Listener#afterBulk(long, BulkRequest, BulkResponse)}
&lt;p&gt; and&lt;br/&gt;
    +	 * &lt;/p&gt;
{@link BulkProcessor.Listener#afterBulk(long, BulkRequest, Throwable)}
&lt;p&gt;.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	private AtomicLong numPendingRequests = new AtomicLong(0);&lt;br/&gt;
    +&lt;br/&gt;
     	/** Elasticsearch client created using the call bridge. */&lt;br/&gt;
     	private transient Client client;&lt;/p&gt;

&lt;p&gt;     	/** Bulk processor to buffer and send requests to Elasticsearch, created using the client. */&lt;br/&gt;
     	private transient BulkProcessor bulkProcessor;&lt;/p&gt;

&lt;p&gt;     	/**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* This is set from inside the 
{@link BulkProcessor.Listener} if a {@link Throwable} was thrown in callbacks.&lt;br/&gt;
    +	 * This is set from inside the {@link BulkProcessor.Listener}
&lt;p&gt; if a &lt;/p&gt;
{@link Throwable}
&lt;p&gt; was thrown in callbacks and&lt;br/&gt;
    +	 * the user considered it should fail the sink via the&lt;br/&gt;
    +	 * &lt;/p&gt;
{@link ActionRequestFailureHandler#onFailure(ActionRequest, Throwable, int, RequestIndexer)}
&lt;p&gt; method.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * Errors will be checked and rethrown before processing each input element, and when the sink is closed.&lt;br/&gt;
     	 */&lt;br/&gt;
     	private final AtomicReference&amp;lt;Throwable&amp;gt; failureThrowable = new AtomicReference&amp;lt;&amp;gt;();&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     	public ElasticsearchSinkBase(&lt;br/&gt;
     		ElasticsearchApiCallBridge callBridge,&lt;br/&gt;
     		Map&amp;lt;String, String&amp;gt; userConfig,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;ElasticsearchSinkFunction&amp;lt;T&amp;gt; elasticsearchSinkFunction) {&lt;br/&gt;
    +		ElasticsearchSinkFunction&amp;lt;T&amp;gt; elasticsearchSinkFunction,&lt;br/&gt;
    +		ActionRequestFailureHandler failureHandler) {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     		this.callBridge = checkNotNull(callBridge);&lt;br/&gt;
     		this.elasticsearchSinkFunction = checkNotNull(elasticsearchSinkFunction);&lt;br/&gt;
    +		this.failureHandler = checkNotNull(failureHandler);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// we eagerly check if the user-provided sink function is serializable;&lt;/li&gt;
	&lt;li&gt;// otherwise, if it isn&apos;t serializable, users will merely get a non-informative error message&lt;br/&gt;
    +		// we eagerly check if the user-provided sink function and failure handler is serializable;&lt;br/&gt;
    +		// otherwise, if they aren&apos;t serializable, users will merely get a non-informative error message&lt;br/&gt;
     		// &quot;ElasticsearchSinkBase is not serializable&quot;&lt;/li&gt;
	&lt;li&gt;try 
{
    -			InstantiationUtil.serializeObject(elasticsearchSinkFunction);
    -		}
&lt;p&gt; catch (Exception e) &lt;/p&gt;
{
    -			throw new IllegalArgumentException(
    -				&quot;The implementation of the provided ElasticsearchSinkFunction is not serializable. &quot; +
    -				&quot;The object probably contains or references non serializable fields.&quot;);
    -		}&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;checkNotNull(userConfig);&lt;br/&gt;
    +		checkArgument(InstantiationUtil.isSerializable(elasticsearchSinkFunction),&lt;br/&gt;
    +			&quot;The implementation of the provided ElasticsearchSinkFunction is not serializable. &quot; +&lt;br/&gt;
    +				&quot;The object probably contains or references non-serializable fields.&quot;);&lt;br/&gt;
    +&lt;br/&gt;
    +		checkArgument(InstantiationUtil.isSerializable(failureHandler),&lt;br/&gt;
    +			&quot;The implementation of the provided ActionRequestFailureHandler is not serializable. &quot; +&lt;br/&gt;
    +				&quot;The object probably contains or references non-serializable fields.&quot;);
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    That&apos;s so much nicer now &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15879974" author="githubbot" created="Thu, 23 Feb 2017 06:37:33 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102647565&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102647565&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/util/RetryRejectedExecutionFailureHandler.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,46 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.elasticsearch.util;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.elasticsearch.ActionRequestFailureHandler;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.elasticsearch.RequestIndexer;&lt;br/&gt;
    +import org.apache.flink.util.ExceptionUtils;&lt;br/&gt;
    +import org.elasticsearch.action.ActionRequest;&lt;br/&gt;
    +import org.elasticsearch.common.util.concurrent.EsRejectedExecutionException;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * An &lt;/p&gt;
{@link ActionRequestFailureHandler}
&lt;p&gt; that re-adds requests that failed due to temporary&lt;br/&gt;
    + * &lt;/p&gt;
{@link EsRejectedExecutionException}
&lt;p&gt;s (which means that Elasticsearch node queues are currently full),&lt;br/&gt;
    + * and fails for all other failures.&lt;br/&gt;
    + */&lt;br/&gt;
    +public class RetryRejectedExecutionFailureHandler implements ActionRequestFailureHandler {&lt;br/&gt;
    +&lt;br/&gt;
    +	private static final long serialVersionUID = -7423562912824511906L;&lt;br/&gt;
    +&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public void onFailure(ActionRequest action, Throwable failure, int restStatusCode, RequestIndexer indexer) throws Throwable {&lt;br/&gt;
    +		if (ExceptionUtils.containsThrowable(failure, EsRejectedExecutionException.class)) {&lt;br/&gt;
    +			indexer.add(action);&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    The `BulkProcessor` listener actually logs them as LOG.error before they are processed by the failure handler (line 171 and line 180). So, these failures are always logged regardless of whether the failure handler chooses to log them. Do you think that&apos;s ok?&lt;/p&gt;</comment>
                            <comment id="15879988" author="githubbot" created="Thu, 23 Feb 2017 06:41:14 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102647903&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102647903&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/util/RetryRejectedExecutionFailureHandler.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,46 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.elasticsearch.util;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.elasticsearch.ActionRequestFailureHandler;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.elasticsearch.RequestIndexer;&lt;br/&gt;
    +import org.apache.flink.util.ExceptionUtils;&lt;br/&gt;
    +import org.elasticsearch.action.ActionRequest;&lt;br/&gt;
    +import org.elasticsearch.common.util.concurrent.EsRejectedExecutionException;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * An &lt;/p&gt;
{@link ActionRequestFailureHandler}
&lt;p&gt; that re-adds requests that failed due to temporary&lt;br/&gt;
    + * &lt;/p&gt;
{@link EsRejectedExecutionException}
&lt;p&gt;s (which means that Elasticsearch node queues are currently full),&lt;br/&gt;
    + * and fails for all other failures.&lt;br/&gt;
    + */&lt;br/&gt;
    +public class RetryRejectedExecutionFailureHandler implements ActionRequestFailureHandler {&lt;br/&gt;
    +&lt;br/&gt;
    +	private static final long serialVersionUID = -7423562912824511906L;&lt;br/&gt;
    +&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public void onFailure(ActionRequest action, Throwable failure, int restStatusCode, RequestIndexer indexer) throws Throwable {&lt;br/&gt;
    +		if (ExceptionUtils.containsThrowable(failure, EsRejectedExecutionException.class)) {&lt;br/&gt;
    +			indexer.add(action);&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Regarding the frequency of `EsRejectedExecutionException`, from my experience with ES before, they pop up a lot with under-resourced / configured ES clusters.&lt;/p&gt;

&lt;p&gt;    It can flood logs if it isn&apos;t treated accordingly, but not logging them can be bad too because you&apos;ll know nothing about it, unless the sink eventually fails with it.&lt;/p&gt;

&lt;p&gt;    We could also remove the failure logging from the `ElasticsearchSinkBase` and let the user be responsible for that, but I&apos;m a bit undecided here. Open to suggestions for this!&lt;/p&gt;</comment>
                            <comment id="15879990" author="githubbot" created="Thu, 23 Feb 2017 06:42:15 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102648005&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102648005&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/util/RetryRejectedExecutionFailureHandler.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,46 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.elasticsearch.util;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.elasticsearch.ActionRequestFailureHandler;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.elasticsearch.RequestIndexer;&lt;br/&gt;
    +import org.apache.flink.util.ExceptionUtils;&lt;br/&gt;
    +import org.elasticsearch.action.ActionRequest;&lt;br/&gt;
    +import org.elasticsearch.common.util.concurrent.EsRejectedExecutionException;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * An &lt;/p&gt;
{@link ActionRequestFailureHandler}
&lt;p&gt; that re-adds requests that failed due to temporary&lt;br/&gt;
    + * &lt;/p&gt;
{@link EsRejectedExecutionException}
&lt;p&gt;s (which means that Elasticsearch node queues are currently full),&lt;br/&gt;
    + * and fails for all other failures.&lt;br/&gt;
    + */&lt;br/&gt;
    +public class RetryRejectedExecutionFailureHandler implements ActionRequestFailureHandler {&lt;br/&gt;
    +&lt;br/&gt;
    +	private static final long serialVersionUID = -7423562912824511906L;&lt;br/&gt;
    +&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public void onFailure(ActionRequest action, Throwable failure, int restStatusCode, RequestIndexer indexer) throws Throwable {&lt;br/&gt;
    +		if (ExceptionUtils.containsThrowable(failure, EsRejectedExecutionException.class)) {&lt;br/&gt;
    +			indexer.add(action);&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Metricifing the ES connectors seems like a good idea, especially with its growing popularity. I&apos;ll think about it and file a JIRA with some initial proposals.&lt;/p&gt;</comment>
                            <comment id="15880057" author="githubbot" created="Thu, 23 Feb 2017 07:39:58 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102654126&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102654126&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java &amp;#8212;&lt;br/&gt;
    @@ -211,6 +283,23 @@ public void invoke(T value) throws Exception {&lt;br/&gt;
     	}&lt;/p&gt;

&lt;p&gt;     	@Override&lt;br/&gt;
    +	public void initializeState(FunctionInitializationContext context) throws Exception &lt;/p&gt;
{
    +		// no initialization needed
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public void snapshotState(FunctionSnapshotContext context) throws Exception {&lt;br/&gt;
    +		checkErrorAndRethrow();&lt;br/&gt;
    +&lt;br/&gt;
    +		if (flushOnCheckpoint) {&lt;br/&gt;
    +			do {&lt;br/&gt;
    +				bulkProcessor.flush();&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Ah, I see the problem here ...&lt;br/&gt;
    The bulk processor&apos;s internal `bulkRequest.numberOfActions() == 0` will become `true` as soon as it starts executing the flush, and not after `afterBulk` is invoked.&lt;/p&gt;

&lt;p&gt;    So, since our `numPendingRequests` implementation relies on the `afterBulk` callback, we might have busy loops on `bulkProcessor.flush()` while we wait for `numPendingRequests` to become 0.&lt;/p&gt;

&lt;p&gt;    This is quite a nice catch actually! So no worries on bringing it up now.&lt;/p&gt;</comment>
                            <comment id="15880059" author="githubbot" created="Thu, 23 Feb 2017 07:41:15 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102654252&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102654252&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java &amp;#8212;&lt;br/&gt;
    @@ -211,6 +283,23 @@ public void invoke(T value) throws Exception {&lt;br/&gt;
     	}&lt;/p&gt;

&lt;p&gt;     	@Override&lt;br/&gt;
    +	public void initializeState(FunctionInitializationContext context) throws Exception &lt;/p&gt;
{
    +		// no initialization needed
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public void snapshotState(FunctionSnapshotContext context) throws Exception {&lt;br/&gt;
    +		checkErrorAndRethrow();&lt;br/&gt;
    +&lt;br/&gt;
    +		if (flushOnCheckpoint) {&lt;br/&gt;
    +			do {&lt;br/&gt;
    +				bulkProcessor.flush();&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Waiting on `numPendingRequests` makes sense, I&apos;ll try and see if it works out.&lt;/p&gt;</comment>
                            <comment id="15880075" author="githubbot" created="Thu, 23 Feb 2017 08:00:27 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102656903&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102656903&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java &amp;#8212;&lt;br/&gt;
    @@ -211,6 +283,23 @@ public void invoke(T value) throws Exception {&lt;br/&gt;
     	}&lt;/p&gt;

&lt;p&gt;     	@Override&lt;br/&gt;
    +	public void initializeState(FunctionInitializationContext context) throws Exception &lt;/p&gt;
{
    +		// no initialization needed
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public void snapshotState(FunctionSnapshotContext context) throws Exception {&lt;br/&gt;
    +		checkErrorAndRethrow();&lt;br/&gt;
    +&lt;br/&gt;
    +		if (flushOnCheckpoint) {&lt;br/&gt;
    +			do {&lt;br/&gt;
    +				bulkProcessor.flush();&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    On a second look, I think my previous statement is incorrect.&lt;/p&gt;

&lt;p&gt;    To elaborate, this is the way the `BulkProcessor`&apos;s `flush` is implemented:&lt;br/&gt;
    ```&lt;br/&gt;
    if(this.bulkRequest.numberOfActions() &amp;gt; 0) &lt;/p&gt;
{
        this.execute();
    }
&lt;p&gt;    ```&lt;/p&gt;

&lt;p&gt;    `execute()` doesn&apos;t return until `afterBulk` is called on the listener. Since we can re-add requests to the bulk processor within `afterBulk`, the `bulkRequest.numberOfActions() &amp;gt; 0` will be true again and enters the loop.&lt;/p&gt;

&lt;p&gt;    Therefore, the `bulkProcessor.flush()` can actually just be called once, and will work with our failure-handler re-adding strategy so that the flush also waits for re-added requests before returning. We can just check once on `numPendingRequests` after the flush to make sure the flush works as expected.&lt;/p&gt;</comment>
                            <comment id="15880120" author="githubbot" created="Thu, 23 Feb 2017 08:38:08 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102662136&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102662136&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java &amp;#8212;&lt;br/&gt;
    @@ -211,6 +283,23 @@ public void invoke(T value) throws Exception {&lt;br/&gt;
     	}&lt;/p&gt;

&lt;p&gt;     	@Override&lt;br/&gt;
    +	public void initializeState(FunctionInitializationContext context) throws Exception &lt;/p&gt;
{
    +		// no initialization needed
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public void snapshotState(FunctionSnapshotContext context) throws Exception {&lt;br/&gt;
    +		checkErrorAndRethrow();&lt;br/&gt;
    +&lt;br/&gt;
    +		if (flushOnCheckpoint) {&lt;br/&gt;
    +			do {&lt;br/&gt;
    +				bulkProcessor.flush();&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Following my arguments above, I think the busy loop you mentioned shouldn&apos;t happen, because bulk processor&apos;s internal `bulkRequest.numberOfActions()` should always be synced with our `numPendingRecords`. (i.e., it should not occur that `bulkRequest.numberOfActions() == 0` but our own `numPendingRecords != 0`).&lt;/p&gt;

&lt;p&gt;    So in that case, if `bulkRequest.numberOfActions() == 0` then my original loop implementation just fallbacks to a single pass with 2 condition checks.&lt;/p&gt;

&lt;p&gt;    To a certain extent, I think it might be better to stick to the original loop implementation, so that we&apos;re not locked-in with how the `BulkProcessor`&apos;s flush is implemented. As you can see from a commit I just pushed (2956f99) which modifies the mock bulk processor in tests to correctly mimic the flushing behaviour I described above, the loop implementation still pass the tests.&lt;/p&gt;</comment>
                            <comment id="15882301" author="githubbot" created="Fri, 24 Feb 2017 09:23:49 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358#discussion_r102905100&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358#discussion_r102905100&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java &amp;#8212;&lt;br/&gt;
    @@ -211,6 +283,23 @@ public void invoke(T value) throws Exception {&lt;br/&gt;
     	}&lt;/p&gt;

&lt;p&gt;     	@Override&lt;br/&gt;
    +	public void initializeState(FunctionInitializationContext context) throws Exception &lt;/p&gt;
{
    +		// no initialization needed
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public void snapshotState(FunctionSnapshotContext context) throws Exception {&lt;br/&gt;
    +		checkErrorAndRethrow();&lt;br/&gt;
    +&lt;br/&gt;
    +		if (flushOnCheckpoint) {&lt;br/&gt;
    +			do {&lt;br/&gt;
    +				bulkProcessor.flush();&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Thanks a lot for looking into this in detail.&lt;br/&gt;
    I think calling flush() this way is okay then.&lt;/p&gt;</comment>
                            <comment id="15882302" author="githubbot" created="Fri, 24 Feb 2017 09:24:30 +0000"  >&lt;p&gt;Github user rmetzger commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    +1 to merge.&lt;/p&gt;

&lt;p&gt;    Thank you for answering all my comments so detailed!&lt;/p&gt;</comment>
                            <comment id="15882763" author="githubbot" created="Fri, 24 Feb 2017 14:14:45 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Merging this to `master` &#127881;  ..&lt;/p&gt;</comment>
                            <comment id="15882842" author="githubbot" created="Fri, 24 Feb 2017 15:06:42 +0000"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3358&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3358&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15882900" author="tzulitai" created="Fri, 24 Feb 2017 15:40:55 +0000"  >&lt;p&gt;Resolved in &lt;tt&gt;master&lt;/tt&gt; via &lt;a href=&quot;http://git-wip-us.apache.org/repos/asf/flink/2437da6&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://git-wip-us.apache.org/repos/asf/flink/2437da6&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                                                <inwardlinks description="is blocked by">
                                        <issuelink>
            <issuekey id="13016790">FLINK-4988</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13022319">FLINK-5122</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 38 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i38p0n:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>