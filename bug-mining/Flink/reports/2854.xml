<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 20:38:28 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-11187] StreamingFileSink with S3 backend transient socket timeout issues </title>
                <link>https://issues.apache.org/jira/browse/FLINK-11187</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;p&gt;When using the StreamingFileSink with S3A backend, occasionally, errors like this will occur:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Caused by: org.apache.flink.fs.s3base.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed. (Service: Amazon S3; Status Code: 400; Error Code: RequestTimeout; Request ID: xxx; S3 Extended Request ID: xxx, S3 Extended Request ID: xxx
&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.flink.fs.s3base.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1639)
&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.flink.fs.s3base.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1304)
&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.flink.fs.s3base.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1056)&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This causes a restart of flink job, which is often able to recover from, but under heavy load, this can become very frequent.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Turning on debug logs you can find the following relevant stack trace:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;2018-12-17 05:55:46,546 DEBUG org.apache.flink.fs.s3base.shaded.com.amazonaws.http.AmazonHttpClient&#160; - FYI: failed to reset content inputstream before throwing up
java.io.IOException: Resetting to invalid mark
&#160; at java.io.BufferedInputStream.reset(BufferedInputStream.java:448)
&#160; at org.apache.flink.fs.s3base.shaded.com.amazonaws.internal.SdkBufferedInputStream.reset(SdkBufferedInputStream.java:106)
&#160; at org.apache.flink.fs.s3base.shaded.com.amazonaws.internal.SdkFilterInputStream.reset(SdkFilterInputStream.java:112)
&#160; at org.apache.flink.fs.s3base.shaded.com.amazonaws.event.ProgressInputStream.reset(ProgressInputStream.java:168)
&#160; at org.apache.flink.fs.s3base.shaded.com.amazonaws.internal.SdkFilterInputStream.reset(SdkFilterInputStream.java:112)
&#160; at org.apache.flink.fs.s3base.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.lastReset(AmazonHttpClient.java:1145)
&#160; at org.apache.flink.fs.s3base.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1070)
&#160; at org.apache.flink.fs.s3base.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:743)
&#160; at org.apache.flink.fs.s3base.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:717)
&#160; at org.apache.flink.fs.s3base.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:699)
&#160; at org.apache.flink.fs.s3base.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:667)
&#160; at org.apache.flink.fs.s3base.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:649)
&#160; at org.apache.flink.fs.s3base.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:513)
&#160; at org.apache.flink.fs.s3base.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4325)
&#160; at org.apache.flink.fs.s3base.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4272)
&#160; at org.apache.flink.fs.s3base.shaded.com.amazonaws.services.s3.AmazonS3Client.doUploadPart(AmazonS3Client.java:3306)
&#160; at org.apache.flink.fs.s3base.shaded.com.amazonaws.services.s3.AmazonS3Client.uploadPart(AmazonS3Client.java:3291)
&#160; at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.S3AFileSystem.uploadPart(S3AFileSystem.java:1576)
&#160; at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.WriteOperationHelper.lambda$uploadPart$8(WriteOperationHelper.java:474)
&#160; at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)
&#160; at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:260)
&#160; at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:317)
&#160; at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:256)
&#160; at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:231)
&#160; at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.WriteOperationHelper.retry(WriteOperationHelper.java:123)
&#160; at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.WriteOperationHelper.uploadPart(WriteOperationHelper.java:471)
&#160; at org.apache.flink.fs.s3hadoop.HadoopS3AccessHelper.uploadPart(HadoopS3AccessHelper.java:74)
&#160; at org.apache.flink.fs.s3.common.writer.RecoverableMultiPartUploadImpl$UploadTask.run(RecoverableMultiPartUploadImpl.java:319)
&#160; at org.apache.flink.fs.s3.common.utils.BackPressuringExecutor$SemaphoreReleasingRunnable.run(BackPressuringExecutor.java:92)
&#160; at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
&#160; at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
&#160; at java.lang.Thread.run(Thread.java:748)&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This error occurs because of a transient failure in writing a multipart chunk fails and the underlying InputStream cannot be reset. This ResetException should be thrown to the client (as documented here:&#160;&lt;a href=&quot;https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/best-practices.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/best-practices.html&lt;/a&gt;) but for some reason is not, instead, the client is retrying the request, but now with a fully consumed InputStream. Because this InputStream is empty/smaller we can&apos;t fill up the Content-Length that the multipart upload is expecting, so the socket hangs to eventually be timed out.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;This failure happens roughly ~20 times before the AWS client retry logic finally fails the request and the socket time out exception is thrown.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;As mentioned in the best practice AWS doc, the best fix for this is to use a File or FileInputStream object or to use the setReadLimit. I tried to use a global SDK property (com.amazonaws.sdk.s3.defaultStreamBufferSize) to set this value, but that did not fix the problem, which I believe is because the InputStream is not mark-able and the AWS client doesn&apos;t wrap the stream.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;What is confirmed to work is the following patch: &lt;a href=&quot;https://gist.github.com/addisonj/00fc28f1f8f189380d8e53fdc887fae6&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://gist.github.com/addisonj/00fc28f1f8f189380d8e53fdc887fae6&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;That is obviously not ideal, but it may suffice to just make that configurable.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;The other option is to instead expose the S3A WriteHelper option to pass a file to the S3AccessHelper and change the other relevant classes (RefCountedFSOutputStream) to expose the File object and directly hand that to the S3A WriteHelper&lt;/p&gt;</description>
                <environment></environment>
        <key id="13204998">FLINK-11187</key>
            <summary>StreamingFileSink with S3 backend transient socket timeout issues </summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="addisonj@gmail.com">Addison Higham</assignee>
                                    <reporter username="addisonj@gmail.com">Addison Higham</reporter>
                        <labels>
                            <label>pull-request-available</label>
                    </labels>
                <created>Mon, 17 Dec 2018 21:49:18 +0000</created>
                <updated>Wed, 16 Jan 2019 12:35:27 +0000</updated>
                            <resolved>Wed, 16 Jan 2019 12:35:15 +0000</resolved>
                                    <version>1.7.0</version>
                    <version>1.7.1</version>
                                    <fixVersion>1.7.2</fixVersion>
                    <fixVersion>1.8.0</fixVersion>
                                    <component>Connectors / Common</component>
                    <component>FileSystems</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>7</watches>
                                                    <progress percentage="100">
                                    <originalProgress>
                                                    <row percentage="0" backgroundColor="#89afd7"/>
                                                    <row percentage="100" backgroundColor="transparent"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="100" backgroundColor="#51a825"/>
                                                    <row percentage="0" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </progress>
                                    <aggregateprogress percentage="100">
                                    <originalProgress>
                                                    <row percentage="0" backgroundColor="#89afd7"/>
                                                    <row percentage="100" backgroundColor="transparent"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="100" backgroundColor="#51a825"/>
                                                    <row percentage="0" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </aggregateprogress>
                                            <timeestimate seconds="0">0h</timeestimate>
                            <timespent seconds="600">10m</timespent>
                                <comments>
                            <comment id="16723402" author="addisonj@gmail.com" created="Mon, 17 Dec 2018 21:50:46 +0000"  >&lt;p&gt;See &lt;a href=&quot;https://lists.apache.org/thread.html/87eb8e16abad09232da5fbc6999c19c4fba0f16d641c565e62096564@%3Cuser.flink.apache.org%3E&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://lists.apache.org/thread.html/87eb8e16abad09232da5fbc6999c19c4fba0f16d641c565e62096564@%3Cuser.flink.apache.org%3E&lt;/a&gt;&#160;for the first discussion of this on the mailing list.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16725942" author="stevel@apache.org" created="Thu, 20 Dec 2018 15:35:54 +0000"  >&lt;p&gt;There&apos;s limits to how well you can buffer...if the post fails it goes back to byte 0 and retries, which is done with a reset() to the marker set when uploading that part began.&lt;/p&gt;

&lt;p&gt;Better to serve up directly from a file, if that&apos;s the source, or, if its memory, having an input stream which &lt;a href=&quot;https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ADataBlocks.java#L640&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;supports mark/reset to anywhere in the entire block&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16735999" author="kkl0u" created="Mon, 7 Jan 2019 15:59:17 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=addisonj%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;addisonj@gmail.com&quot;&gt;addisonj@gmail.com&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stevel%40apache.org&quot; class=&quot;user-hover&quot; rel=&quot;stevel@apache.org&quot;&gt;stevel@apache.org&lt;/a&gt;. Thanks for looking into this and for digging into the code.&lt;/p&gt;

&lt;p&gt;From what you describe, I would lean towards the solution that exposes the &lt;tt&gt;File&lt;/tt&gt; in the &lt;tt&gt;RefCountedFSOutputStream&lt;/tt&gt; and go for the second solution proposed by &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=addisonj%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;addisonj@gmail.com&quot;&gt;addisonj@gmail.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For uniformity, also the &lt;tt&gt;putObject()&lt;/tt&gt; should change to take a &lt;tt&gt;File&lt;/tt&gt; as an argument (and get rid of the &lt;tt&gt;length&lt;/tt&gt; argument. So effectively, the two methods in the &lt;tt&gt;HadoopS3AccessHelper&lt;/tt&gt; will become:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
@Override
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; UploadPartResult uploadPart(&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; key, &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; uploadId, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; partNumber, File inputFile, &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; length) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
	&lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; UploadPartRequest uploadRequest = s3accessHelper.newUploadPartRequest(
			key, uploadId, partNumber, MathUtils.checkedDownCast(length), &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;, inputFile, 0L);
	&lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; s3accessHelper.uploadPart(uploadRequest);
}

@Override
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; PutObjectResult putObject(&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; key, File inputFile) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
	&lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; PutObjectRequest putRequest = s3accessHelper.createPutObjectRequest(key, inputFile);
	&lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; s3accessHelper.putObject(putRequest);
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=addisonj%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;addisonj@gmail.com&quot;&gt;addisonj@gmail.com&lt;/a&gt; Do you have a minimal example that verifies that this solution solves the problem?&lt;/p&gt;

</comment>
                            <comment id="16736855" author="sthm" created="Tue, 8 Jan 2019 08:00:05 +0000"  >&lt;p&gt;Thanks for looking into this, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=kkl0u&quot; class=&quot;user-hover&quot; rel=&quot;kkl0u&quot;&gt;kkl0u&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=addisonj%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;addisonj@gmail.com&quot;&gt;addisonj@gmail.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I&apos;ve applied the changes proposed by Kostas on top of the 1.7.1 release. I ran the same job mentioned in the email thread for five hours and haven&apos;t seen the error during that time. This is a very promising sing, but I&apos;ll keep my job running to verify that the error doesn&apos;t occur at a later point and let you know what I find.&lt;/p&gt;</comment>
                            <comment id="16736889" author="kkl0u" created="Tue, 8 Jan 2019 08:59:12 +0000"  >&lt;p&gt;Thanks for checking this &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sthm&quot; class=&quot;user-hover&quot; rel=&quot;sthm&quot;&gt;sthm&lt;/a&gt; ! &lt;br/&gt;
It would be really helpful if you could also provide a minimal example that we can use for a test.&lt;br/&gt;
And of course, let us know if these changes solve the problem or not.&lt;/p&gt;</comment>
                            <comment id="16737665" author="addisonj@gmail.com" created="Tue, 8 Jan 2019 23:57:39 +0000"  >&lt;p&gt;Hey &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=kkl0u&quot; class=&quot;user-hover&quot; rel=&quot;kkl0u&quot;&gt;kkl0u&lt;/a&gt;, I took a crack at making the relevant changes here: &lt;a href=&quot;https://github.com/instructure/flink/tree/s3_file&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/instructure/flink/tree/s3_file&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Additionally, I don&apos;t think it should take anything special to cause a repro, but things that should help based on what I understand of the bug:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Creating larger files so you have s3 multipart parts (i.e. don&apos;t create new flink parts as often)&lt;/li&gt;
	&lt;li&gt;hitting it from quite a few servers, we only saw this behavior once we had&#160; 20+ servers, parallelism of 100+ and even then it was only once every few hours&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;As far as I can tell, this is just a transient s3 error, so it appears to be pretty rare but does happen more often with increased load.&lt;/p&gt;


&lt;p&gt;I made some tweaks to the the flink-streaming-file-sink e2e test (here: &lt;a href=&quot;https://github.com/instructure/flink/tree/s3_repro)&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/instructure/flink/tree/s3_repro)&lt;/a&gt;&#160;and was trying to force an s3 failure by using tcpkill to kill a connection,&#160;but haven&apos;t been able to repro it yet. I think a better approach might be to using something like minio (an alternate s3 implementation) and kill it temporarily to force a failed part upload.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16738029" author="kkl0u" created="Wed, 9 Jan 2019 09:47:34 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=addisonj%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;addisonj@gmail.com&quot;&gt;addisonj@gmail.com&lt;/a&gt;, thanks for working on that.&lt;/p&gt;

&lt;p&gt;I understand that reproducing transient network failures between yours and a system you cannot control (s3) is difficult.&lt;br/&gt;
This is why I asked if you have an idea about how to add a minimum example that is reproducible and always fails.&lt;/p&gt;

&lt;p&gt;Using minio was also discussed internally when we were writing the s3 connector, but we decided to go with using s3 instead.&lt;br/&gt;
Maybe we should reconsider, at least for some tests like this.&lt;/p&gt;

&lt;p&gt;In any case, feel free to open a PR with your changes (even without a test for now) and I will review it as soon as possible.&lt;/p&gt;</comment>
                            <comment id="16738131" author="stevel@apache.org" created="Wed, 9 Jan 2019 11:34:49 +0000"  >&lt;blockquote&gt;&lt;p&gt;As far as I can tell, this is just a transient s3 error, so it appears to be pretty rare but does happen more often with increased load.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;its believed to be a network error: upload of a single block fails, transfer manager wants to retry but the block buffering is such that it can&apos;t repost that block.&lt;/p&gt;

&lt;p&gt;AFAIK. no easy way to test this other than a long-haul upload over many hours.Not unless you can insert an unreliable proxy or sneak in an httpclient which fails (possible if you have your own modified apache httpclient, run the SDK unshaded, etc...)&lt;/p&gt;

&lt;p&gt;BTW: do make sure that final POST does retries too; some of the AWS SDKs have under-retried there, and you don&apos;t want a 4h upload to fail at the final step.&lt;/p&gt;</comment>
                            <comment id="16738224" author="aljoscha" created="Wed, 9 Jan 2019 13:18:41 +0000"  >&lt;p&gt;After the fix, does this even need the stream nature of &lt;tt&gt;RefCountedBufferingFileStream&lt;/tt&gt; anymore?&lt;/p&gt;</comment>
                            <comment id="16739662" author="addisonj@gmail.com" created="Thu, 10 Jan 2019 18:31:19 +0000"  >&lt;p&gt;PR opened here: &lt;a href=&quot;https://github.com/apache/flink/pull/7460&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/7460&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aljoscha&quot; class=&quot;user-hover&quot; rel=&quot;aljoscha&quot;&gt;aljoscha&lt;/a&gt; not in runtime code, but it is still used in unit tests to read back objects written. But yes, the name is now a bit of a misnomer, but IDK how extensive this refactor should be...&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16743755" author="sthm" created="Wed, 16 Jan 2019 08:42:04 +0000"  >&lt;p&gt;I&apos;ve kept the job running for several days and haven&apos;t seen the exception since. So I&apos;m pretty sure that the fix properly addresses the bug.&lt;/p&gt;

&lt;p&gt;But as previously mentioned, providing a minimal example is quite challenging, as for my workload, the exception occurred roughly every 4-8 hours.&lt;/p&gt;</comment>
                            <comment id="16743769" author="kkl0u" created="Wed, 16 Jan 2019 08:58:58 +0000"  >&lt;p&gt;Thanks a lot for the help &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sthm&quot; class=&quot;user-hover&quot; rel=&quot;sthm&quot;&gt;sthm&lt;/a&gt; and for the work of course &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=addisonj%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;addisonj@gmail.com&quot;&gt;addisonj@gmail.com&lt;/a&gt;! We will merge probably today.&lt;/p&gt;</comment>
                            <comment id="16743882" author="aljoscha" created="Wed, 16 Jan 2019 11:14:17 +0000"  >&lt;p&gt;Fixed on master in&lt;br/&gt;
9c07822acedfd7e0582481de620e5af205f4aef3&lt;/p&gt;</comment>
                            <comment id="16743973" author="aljoscha" created="Wed, 16 Jan 2019 12:35:05 +0000"  >&lt;p&gt;Fixed on release-1.7 in&lt;br/&gt;
d5506b9191f0af0346e10561df783815e7cd0565&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 43 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|u002ew:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>