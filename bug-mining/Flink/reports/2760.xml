<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 20:36:11 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-10674] Fix handling of retractions after clean up</title>
                <link>https://issues.apache.org/jira/browse/FLINK-10674</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;p&gt;Our online Flink Job run about a week&#65292;job contain sql &#65306;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
select&#160; `time`,&#160; 
        lower(trim(os_type)) as os_type, 
        count(distinct feed_id) as feed_total_view&#160; 
from&#160; my_table&#160;
group by `time`, lower(trim(os_type))&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&#160;then&#160;occur NPE:&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
java.lang.NullPointerException

at scala.Predef$.Long2long(Predef.scala:363)

at org.apache.flink.table.functions.aggfunctions.DistinctAccumulator.remove(DistinctAccumulator.scala:109)

at NonWindowedAggregationHelper$894.retract(Unknown Source)

at org.apache.flink.table.runtime.aggregate.GroupAggProcessFunction.processElement(GroupAggProcessFunction.scala:124)

at org.apache.flink.table.runtime.aggregate.GroupAggProcessFunction.processElement(GroupAggProcessFunction.scala:39)

at org.apache.flink.streaming.api.operators.LegacyKeyedProcessOperator.processElement(LegacyKeyedProcessOperator.java:88)

at org.apache.flink.streaming.runtime.io.StreamInputProcessor.processInput(StreamInputProcessor.java:202)

at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.run(OneInputStreamTask.java:105)

at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:300)

at org.apache.flink.runtime.taskmanager.Task.run(Task.java:711)

at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;View&#160;DistinctAccumulator.remove&lt;br/&gt;
 &lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/12945552/12945552_image-2018-10-25-14-46-03-373.png&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;this NPE should&#160;currentCnt = null lead to, so we simple handle like :&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
def remove(params: Row): &lt;span class=&quot;code-object&quot;&gt;Boolean&lt;/span&gt; = {
  &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt;(!distinctValueMap.contains(params)){
    &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;
  }&lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt;{
    val currentCnt = distinctValueMap.get(params)
    &lt;span class=&quot;code-comment&quot;&gt;// 
&lt;/span&gt;    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (currentCnt == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; || currentCnt == 1) {
      distinctValueMap.remove(params)
      &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;
    } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
      &lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt; value = currentCnt - 1L
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt;(value &amp;lt; 0){
        value = 1
      }
      distinctValueMap.put(params, value)
      &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;
    }
  }
}&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Update:&lt;/p&gt;

&lt;p&gt;Because state clean up happens in processing time, it might be&lt;br/&gt;
 the case that retractions are arriving after the state has&lt;br/&gt;
 been cleaned up. Before these changes, a new accumulator was&lt;br/&gt;
 created and invalid retraction messages were emitted. This&lt;br/&gt;
 change drops retraction messages for which no accumulator&lt;br/&gt;
 exists.&#160;&lt;/p&gt;</description>
                <environment>&lt;p&gt;Flink 1.6.0&lt;/p&gt;</environment>
        <key id="13194067">FLINK-10674</key>
            <summary>Fix handling of retractions after clean up</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Minor</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="twalthr">Timo Walther</assignee>
                                    <reporter username="ambition">ambition</reporter>
                        <labels>
                            <label>pull-request-available</label>
                    </labels>
                <created>Thu, 25 Oct 2018 06:52:33 +0000</created>
                <updated>Mon, 27 Jul 2020 12:58:38 +0000</updated>
                            <resolved>Thu, 22 Nov 2018 15:22:52 +0000</resolved>
                                    <version>1.6.1</version>
                                    <fixVersion>1.5.6</fixVersion>
                    <fixVersion>1.6.3</fixVersion>
                    <fixVersion>1.7.0</fixVersion>
                                    <component>Table SQL / API</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>7</watches>
                                                                                                                <comments>
                            <comment id="16678335" author="twalthr" created="Wed, 7 Nov 2018 14:53:55 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=winipanda&quot; class=&quot;user-hover&quot; rel=&quot;winipanda&quot;&gt;winipanda&lt;/a&gt; have you started working on this already? It would be great if we could fix this for Flink 1.7. If not, I would be happy to fix this bug this week.&lt;/p&gt;</comment>
                            <comment id="16679246" author="ambition" created="Thu, 8 Nov 2018 03:17:26 +0000"  >&lt;p&gt;Our Online Flink on Yarn environment operation&#160; job been occured exception, use modify remove function&#160;not happened.&lt;/p&gt;</comment>
                            <comment id="16679335" author="winipanda" created="Thu, 8 Nov 2018 06:16:58 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=twalthr&quot; class=&quot;user-hover&quot; rel=&quot;twalthr&quot;&gt;twalthr&lt;/a&gt;&#160;I have already modified the DistinctAccumulator.remove(params: Row), but I&apos;m not sure if it&apos;s the only cause, I am trying to reproduce the problem to exam it.&lt;/p&gt;</comment>
                            <comment id="16679401" author="twalthr" created="Thu, 8 Nov 2018 08:01:46 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=winipanda&quot; class=&quot;user-hover&quot; rel=&quot;winipanda&quot;&gt;winipanda&lt;/a&gt; let me know if I can help you.&lt;/p&gt;</comment>
                            <comment id="16681238" author="githubbot" created="Fri, 9 Nov 2018 10:42:36 +0000"  >&lt;p&gt;ambition119 commented on issue #7067: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10674&quot; title=&quot;Fix handling of retractions after clean up&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10674&quot;&gt;&lt;del&gt;FLINK-10674&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;table&amp;#93;&lt;/span&gt; DistinctAccumulator.remove occur NPE&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/7067#issuecomment-437321281&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/7067#issuecomment-437321281&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Complex Event Processing (CEP) SQL Realization is new pull request, is a new feature. Use statement :&lt;br/&gt;
   ```sql &lt;br/&gt;
   SELECT [ ALL | DISTINCT ]&lt;/p&gt;
   { * | projectItem [, projectItem ]* }
&lt;p&gt;   FROM tableExpression&lt;br/&gt;
   [MATCH_RECOGNIZE (&lt;br/&gt;
   [PARTITION BY &lt;/p&gt;
{partitionItem [, partitionItem]*}
&lt;p&gt;]&lt;br/&gt;
   [ORDER BY &lt;/p&gt;
{orderItem [, orderItem]*}
&lt;p&gt;]&lt;br/&gt;
   [MEASURES &lt;/p&gt;
{measureItem AS col [, measureItem AS col]*}
&lt;p&gt;]&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;ONE ROW PER MATCH|ALL ROWS PER MATCH|ONE ROW PER MATCH WITH TIMEOUT ROWS|ALL ROWS PER MATCH WITH TIMEOUT ROWS&amp;#93;&lt;/span&gt;&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;AFTER MATCH SKIP&amp;#93;&lt;/span&gt;&lt;br/&gt;
   PATTERN (patternVariable&lt;span class=&quot;error&quot;&gt;&amp;#91;quantifier&amp;#93;&lt;/span&gt; [ patternVariable&lt;span class=&quot;error&quot;&gt;&amp;#91;quantifier&amp;#93;&lt;/span&gt;]*) WITHIN intervalExpression&lt;br/&gt;
   DEFINE &lt;/p&gt;
{patternVariable AS patternDefinationExpression [, patternVariable AS patternDefinationExpression]*}
&lt;p&gt;   )];&lt;/p&gt;

&lt;p&gt;   ```&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16681242" author="githubbot" created="Fri, 9 Nov 2018 10:44:19 +0000"  >&lt;p&gt;ambition119 edited a comment on issue #7067: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10674&quot; title=&quot;Fix handling of retractions after clean up&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10674&quot;&gt;&lt;del&gt;FLINK-10674&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;table&amp;#93;&lt;/span&gt; DistinctAccumulator.remove occur NPE&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/7067#issuecomment-437321281&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/7067#issuecomment-437321281&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Complex Event Processing (CEP) SQL Realization is new pull request, is a new feature. Use statement :&lt;br/&gt;
   ```sql &lt;br/&gt;
   SELECT [ ALL | DISTINCT ]&lt;/p&gt;
   { * | projectItem [, projectItem ]* }
&lt;p&gt;   FROM tableExpression&lt;br/&gt;
   [MATCH_RECOGNIZE (&lt;br/&gt;
   [PARTITION BY &lt;/p&gt;
{partitionItem [, partitionItem]*}
&lt;p&gt;]&lt;br/&gt;
   [ORDER BY &lt;/p&gt;
{orderItem [, orderItem]*}
&lt;p&gt;]&lt;br/&gt;
   [MEASURES &lt;/p&gt;
{measureItem AS col [, measureItem AS col]*}
&lt;p&gt;]&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;ONE ROW PER MATCH|ALL ROWS PER MATCH|ONE ROW PER MATCH WITH TIMEOUT ROWS|ALL ROWS PER MATCH WITH TIMEOUT ROWS&amp;#93;&lt;/span&gt;&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;AFTER MATCH SKIP&amp;#93;&lt;/span&gt;&lt;br/&gt;
   PATTERN (patternVariable&lt;span class=&quot;error&quot;&gt;&amp;#91;quantifier&amp;#93;&lt;/span&gt; [ patternVariable&lt;span class=&quot;error&quot;&gt;&amp;#91;quantifier&amp;#93;&lt;/span&gt;]*) WITHIN intervalExpression&lt;br/&gt;
   DEFINE &lt;/p&gt;
{patternVariable AS patternDefinationExpression [, patternVariable AS patternDefinationExpression]*}
&lt;p&gt;   )];&lt;/p&gt;

&lt;p&gt;   ```&lt;br/&gt;
   I cann&apos;t create new pull request, sorry&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16681288" author="githubbot" created="Fri, 9 Nov 2018 11:19:02 +0000"  >&lt;p&gt;ambition119 edited a comment on issue #7067: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10674&quot; title=&quot;Fix handling of retractions after clean up&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10674&quot;&gt;&lt;del&gt;FLINK-10674&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;table&amp;#93;&lt;/span&gt; DistinctAccumulator.remove occur NPE&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/7067#issuecomment-437321281&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/7067#issuecomment-437321281&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Complex Event Processing (CEP) SQL Realization  is a new feature. Use statement :&lt;br/&gt;
   ```sql &lt;br/&gt;
   SELECT [ ALL | DISTINCT ]&lt;/p&gt;
   { * | projectItem [, projectItem ]* }
&lt;p&gt;   FROM tableExpression&lt;br/&gt;
   [MATCH_RECOGNIZE (&lt;br/&gt;
   [PARTITION BY &lt;/p&gt;
{partitionItem [, partitionItem]*}
&lt;p&gt;]&lt;br/&gt;
   [ORDER BY &lt;/p&gt;
{orderItem [, orderItem]*}
&lt;p&gt;]&lt;br/&gt;
   [MEASURES &lt;/p&gt;
{measureItem AS col [, measureItem AS col]*}
&lt;p&gt;]&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;ONE ROW PER MATCH|ALL ROWS PER MATCH|ONE ROW PER MATCH WITH TIMEOUT ROWS|ALL ROWS PER MATCH WITH TIMEOUT ROWS&amp;#93;&lt;/span&gt;&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;AFTER MATCH SKIP&amp;#93;&lt;/span&gt;&lt;br/&gt;
   PATTERN (patternVariable&lt;span class=&quot;error&quot;&gt;&amp;#91;quantifier&amp;#93;&lt;/span&gt; [ patternVariable&lt;span class=&quot;error&quot;&gt;&amp;#91;quantifier&amp;#93;&lt;/span&gt;]*) WITHIN intervalExpression&lt;br/&gt;
   DEFINE &lt;/p&gt;
{patternVariable AS patternDefinationExpression [, patternVariable AS patternDefinationExpression]*}
&lt;p&gt;   )];&lt;/p&gt;

&lt;p&gt;   ```&lt;br/&gt;
   I cann&apos;t create new pull request, sorry&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16681419" author="githubbot" created="Fri, 9 Nov 2018 13:22:52 +0000"  >&lt;p&gt;ambition119 removed a comment on issue #7067: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10674&quot; title=&quot;Fix handling of retractions after clean up&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10674&quot;&gt;&lt;del&gt;FLINK-10674&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;table&amp;#93;&lt;/span&gt; DistinctAccumulator.remove occur NPE&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/7067#issuecomment-437321281&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/7067#issuecomment-437321281&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Complex Event Processing (CEP) SQL Realization  is a new feature. Use statement :&lt;br/&gt;
   ```sql &lt;br/&gt;
   SELECT [ ALL | DISTINCT ]&lt;/p&gt;
   { * | projectItem [, projectItem ]* }
&lt;p&gt;   FROM tableExpression&lt;br/&gt;
   [MATCH_RECOGNIZE (&lt;br/&gt;
   [PARTITION BY &lt;/p&gt;
{partitionItem [, partitionItem]*}
&lt;p&gt;]&lt;br/&gt;
   [ORDER BY &lt;/p&gt;
{orderItem [, orderItem]*}
&lt;p&gt;]&lt;br/&gt;
   [MEASURES &lt;/p&gt;
{measureItem AS col [, measureItem AS col]*}
&lt;p&gt;]&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;ONE ROW PER MATCH|ALL ROWS PER MATCH|ONE ROW PER MATCH WITH TIMEOUT ROWS|ALL ROWS PER MATCH WITH TIMEOUT ROWS&amp;#93;&lt;/span&gt;&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;AFTER MATCH SKIP&amp;#93;&lt;/span&gt;&lt;br/&gt;
   PATTERN (patternVariable&lt;span class=&quot;error&quot;&gt;&amp;#91;quantifier&amp;#93;&lt;/span&gt; [ patternVariable&lt;span class=&quot;error&quot;&gt;&amp;#91;quantifier&amp;#93;&lt;/span&gt;]*) WITHIN intervalExpression&lt;br/&gt;
   DEFINE &lt;/p&gt;
{patternVariable AS patternDefinationExpression [, patternVariable AS patternDefinationExpression]*}
&lt;p&gt;   )];&lt;/p&gt;

&lt;p&gt;   ```&lt;br/&gt;
   I cann&apos;t create new pull request, sorry&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16681456" author="githubbot" created="Fri, 9 Nov 2018 13:56:18 +0000"  >&lt;p&gt;ambition119 commented on issue #7067: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10674&quot; title=&quot;Fix handling of retractions after clean up&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10674&quot;&gt;&lt;del&gt;FLINK-10674&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;table&amp;#93;&lt;/span&gt; DistinctAccumulator.remove occur NPE&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/7067#issuecomment-437366592&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/7067#issuecomment-437366592&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   sorry, Complex Event Processing (CEP) SQL Realization this commit I can&apos;t remove&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16681497" author="githubbot" created="Fri, 9 Nov 2018 14:20:25 +0000"  >&lt;p&gt;wenhuitang opened a new pull request #7076: &lt;span class=&quot;error&quot;&gt;&amp;#91;Patch&amp;#93;&lt;/span&gt;:Fix &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10674&quot; title=&quot;Fix handling of retractions after clean up&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10674&quot;&gt;&lt;del&gt;FLINK-10674&lt;/del&gt;&lt;/a&gt;DistinctAccumulator.remove lead to NPE.&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/7076&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/7076&lt;/a&gt;&lt;/p&gt;



&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;What is the purpose of the change&lt;br/&gt;
   This pull request fixes the problem reported by &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10674&quot; title=&quot;Fix handling of retractions after clean up&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10674&quot;&gt;&lt;del&gt;FLINK-10674&lt;/del&gt;&lt;/a&gt; DistinctAccumulator.remove lead to NPE.&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;



&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Brief change log&lt;br/&gt;
   DistinctAccumulator.remove deals with the situation that there is no corresponding instance of the parameters in the distinct map.&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Verifying this change&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;   &lt;b&gt;(Please pick either of the following options)&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;   This change is a trivial rework / code cleanup without any test coverage.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Does this pull request potentially affect one of the following parts:&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Dependencies (does it add or upgrade a dependency): (yes / no) no&lt;/li&gt;
	&lt;li&gt;The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no) no&lt;/li&gt;
	&lt;li&gt;The serializers: (yes / no / don&apos;t know) no&lt;/li&gt;
	&lt;li&gt;The runtime per-record code paths (performance sensitive): (yes / no / don&apos;t know) no&lt;/li&gt;
	&lt;li&gt;Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don&apos;t know) no&lt;/li&gt;
	&lt;li&gt;The S3 file system connector: (yes / no / don&apos;t know) no&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Documentation&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Does this pull request introduce a new feature? (yes / no)&lt;br/&gt;
       no&lt;/li&gt;
	&lt;li&gt;If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16683156" author="winipanda" created="Mon, 12 Nov 2018 03:14:02 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=twalthr&quot; class=&quot;user-hover&quot; rel=&quot;twalthr&quot;&gt;twalthr&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ambition&quot; class=&quot;user-hover&quot; rel=&quot;ambition&quot;&gt;ambition&lt;/a&gt;&#160;Sorry, I haven&apos;t noticed before, so I created a pull request too.&#160;&lt;/p&gt;

&lt;p&gt;Our PR might be little different, I don&apos;t think when value = (currentCnt - 1L)&amp;lt;0, we should change it to 1.&#160; I think that when (currentCnt - 1L)&amp;lt;0, the corresponding instance in distinctValueMap should be remove.&lt;/p&gt;</comment>
                            <comment id="16684785" author="githubbot" created="Tue, 13 Nov 2018 06:45:13 +0000"  >&lt;p&gt;wenhuitang closed pull request #7076: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10674&quot; title=&quot;Fix handling of retractions after clean up&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10674&quot;&gt;&lt;del&gt;FLINK-10674&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;table&amp;#93;&lt;/span&gt; Fix DistinctAccumulator.remove lead to NPE.&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/7076&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/7076&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/docs/build_docs.sh b/docs/build_docs.sh&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/docs/check_links.sh b/docs/check_links.sh&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/docs/docker/run.sh b/docs/docker/run.sh&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/docs/fig/ssl_internal_external.svg b/docs/fig/ssl_internal_external.svg&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/flink-container/docker/build.sh b/flink-container/docker/build.sh&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/flink-container/docker/docker-entrypoint.sh b/flink-container/docker/docker-entrypoint.sh&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/flink-contrib/docker-flink/bluemix-docker-compose.sh b/flink-contrib/docker-flink/bluemix-docker-compose.sh&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/flink-contrib/docker-flink/build.sh b/flink-contrib/docker-flink/build.sh&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/flink-contrib/docker-flink/create-docker-swarm-service.sh b/flink-contrib/docker-flink/create-docker-swarm-service.sh&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/flink-contrib/docker-flink/docker-entrypoint.sh b/flink-contrib/docker-flink/docker-entrypoint.sh&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/flink-contrib/docker-flink/remove-docker-swarm-service.sh b/flink-contrib/docker-flink/remove-docker-swarm-service.sh&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/flink-dist/src/main/flink-bin/bin/config.sh b/flink-dist/src/main/flink-bin/bin/config.sh&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/flink-dist/src/main/flink-bin/bin/jobmanager.sh b/flink-dist/src/main/flink-bin/bin/jobmanager.sh&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/flink-dist/src/main/flink-bin/bin/start-cluster.sh b/flink-dist/src/main/flink-bin/bin/start-cluster.sh&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/flink-dist/src/main/flink-bin/bin/start-zookeeper-quorum.sh b/flink-dist/src/main/flink-bin/bin/start-zookeeper-quorum.sh&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/flink-dist/src/main/flink-bin/bin/stop-cluster.sh b/flink-dist/src/main/flink-bin/bin/stop-cluster.sh&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/flink-dist/src/main/flink-bin/bin/stop-zookeeper-quorum.sh b/flink-dist/src/main/flink-bin/bin/stop-zookeeper-quorum.sh&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/flink-dist/src/main/flink-bin/bin/taskmanager.sh b/flink-dist/src/main/flink-bin/bin/taskmanager.sh&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/flink-dist/src/main/flink-bin/bin/zookeeper.sh b/flink-dist/src/main/flink-bin/bin/zookeeper.sh&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/flink-dist/src/main/flink-bin/mesos-bin/mesos-appmaster-job.sh b/flink-dist/src/main/flink-bin/mesos-bin/mesos-appmaster-job.sh&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/flink-dist/src/main/flink-bin/mesos-bin/mesos-appmaster.sh b/flink-dist/src/main/flink-bin/mesos-bin/mesos-appmaster.sh&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/flink-dist/src/main/flink-bin/mesos-bin/mesos-taskmanager.sh b/flink-dist/src/main/flink-bin/mesos-bin/mesos-taskmanager.sh&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/flink-dist/src/main/flink-bin/yarn-bin/yarn-session.sh b/flink-dist/src/main/flink-bin/yarn-bin/yarn-session.sh&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/flink-end-to-end-tests/flink-parent-child-classloading-test/src/main/resources/.version.properties b/flink-end-to-end-tests/flink-parent-child-classloading-test/src/main/resources/.version.properties&lt;br/&gt;
deleted file mode 100644&lt;br/&gt;
index 76f9c5aa859..00000000000&lt;br/&gt;
&amp;#8212; a/flink-end-to-end-tests/flink-parent-child-classloading-test/src/main/resources/.version.properties&lt;br/&gt;
+++ /dev/null&lt;br/&gt;
@@ -1 +0,0 @@&lt;br/&gt;
-git.commit.id.abbrev=hello-there-42&lt;br/&gt;
diff --git a/flink-end-to-end-tests/test-scripts/test_resume_externalized_checkpoints.sh b/flink-end-to-end-tests/test-scripts/test_resume_externalized_checkpoints.sh&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/flink-end-to-end-tests/test-scripts/test_resume_savepoint.sh b/flink-end-to-end-tests/test-scripts/test_resume_savepoint.sh&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/flink-end-to-end-tests/test-scripts/test_yarn_kerberos_docker.sh b/flink-end-to-end-tests/test-scripts/test_yarn_kerberos_docker.sh&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/flink-jepsen/docker/up.sh b/flink-jepsen/docker/up.sh&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/functions/aggfunctions/DistinctAccumulator.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/functions/aggfunctions/DistinctAccumulator.scala&lt;br/&gt;
index 3427c9c96f7..91c2f923b72 100644&lt;br/&gt;
&amp;#8212; a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/functions/aggfunctions/DistinctAccumulator.scala&lt;br/&gt;
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/functions/aggfunctions/DistinctAccumulator.scala&lt;br/&gt;
@@ -101,13 +101,18 @@ class DistinctAccumulator&lt;span class=&quot;error&quot;&gt;&amp;#91;ACC&amp;#93;&lt;/span&gt;(&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@return true if no instances of the parameters remain in the map, false otherwise.&lt;br/&gt;
     */&lt;br/&gt;
   def remove(params: Row): Boolean = {&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val currentCnt = distinctValueMap.get(params)&lt;/li&gt;
	&lt;li&gt;if (currentCnt == 1) {&lt;/li&gt;
	&lt;li&gt;distinctValueMap.remove(params)&lt;br/&gt;
+    if (!distinctValueMap.contains(params)) 
{
       true
     }
&lt;p&gt; else {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;distinctValueMap.put(params, currentCnt - 1L)&lt;/li&gt;
	&lt;li&gt;false&lt;br/&gt;
+      val currentCnt = distinctValueMap.get(params)&lt;br/&gt;
+&lt;br/&gt;
+      if (currentCnt == null || currentCnt &amp;lt;= 1) 
{
+        distinctValueMap.remove(params)
+        true
+      }
&lt;p&gt; else &lt;/p&gt;
{
+        distinctValueMap.put(params, currentCnt - 1L)
+        false
+      }
&lt;p&gt;     }&lt;br/&gt;
   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/flink-mesos/src/main/java/org/apache/flink/mesos/entrypoint/MesosEntrypointUtils.java b/flink-mesos/src/main/java/org/apache/flink/mesos/entrypoint/MesosEntrypointUtils.java&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/flink-mesos/src/main/java/org/apache/flink/mesos/entrypoint/MesosJobClusterEntrypoint.java b/flink-mesos/src/main/java/org/apache/flink/mesos/entrypoint/MesosJobClusterEntrypoint.java&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/flink-mesos/src/main/java/org/apache/flink/mesos/entrypoint/MesosSessionClusterEntrypoint.java b/flink-mesos/src/main/java/org/apache/flink/mesos/entrypoint/MesosSessionClusterEntrypoint.java&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosApplicationMasterRunner.java b/flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosApplicationMasterRunner.java&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/entrypoint/ClusterEntrypoint.java b/flink-runtime/src/main/java/org/apache/flink/runtime/entrypoint/ClusterEntrypoint.java&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/tools/list_deps.py b/tools/list_deps.py&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/tools/merge_flink_pr.py b/tools/merge_flink_pr.py&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/tools/merge_pull_request.sh.template b/tools/merge_pull_request.sh.template&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/tools/releasing/create_binary_release.sh b/tools/releasing/create_binary_release.sh&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/tools/releasing/create_release_branch.sh b/tools/releasing/create_release_branch.sh&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/tools/releasing/create_source_release.sh b/tools/releasing/create_source_release.sh&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/tools/releasing/deploy_staging_jars.sh b/tools/releasing/deploy_staging_jars.sh&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;br/&gt;
diff --git a/tools/releasing/update_branch_version.sh b/tools/releasing/update_branch_version.sh&lt;br/&gt;
old mode 100755&lt;br/&gt;
new mode 100644&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16684786" author="githubbot" created="Tue, 13 Nov 2018 06:45:20 +0000"  >&lt;p&gt;wenhuitang opened a new pull request #7076: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10674&quot; title=&quot;Fix handling of retractions after clean up&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10674&quot;&gt;&lt;del&gt;FLINK-10674&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;table&amp;#93;&lt;/span&gt; Fix DistinctAccumulator.remove lead to NPE.&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/7076&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/7076&lt;/a&gt;&lt;/p&gt;



&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;What is the purpose of the change&lt;br/&gt;
   This pull request fixes the problem reported by &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10674&quot; title=&quot;Fix handling of retractions after clean up&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10674&quot;&gt;&lt;del&gt;FLINK-10674&lt;/del&gt;&lt;/a&gt; DistinctAccumulator.remove lead to NPE.&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;



&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Brief change log&lt;br/&gt;
   DistinctAccumulator.remove deals with the situation that there is no corresponding instance of the parameters in the distinct map.&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Verifying this change&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;   &lt;b&gt;(Please pick either of the following options)&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;   This change is a trivial rework / code cleanup without any test coverage.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Does this pull request potentially affect one of the following parts:&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Dependencies (does it add or upgrade a dependency): (yes / no) no&lt;/li&gt;
	&lt;li&gt;The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no) no&lt;/li&gt;
	&lt;li&gt;The serializers: (yes / no / don&apos;t know) no&lt;/li&gt;
	&lt;li&gt;The runtime per-record code paths (performance sensitive): (yes / no / don&apos;t know) no&lt;/li&gt;
	&lt;li&gt;Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don&apos;t know) no&lt;/li&gt;
	&lt;li&gt;The S3 file system connector: (yes / no / don&apos;t know) no&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Documentation&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Does this pull request introduce a new feature? (yes / no)&lt;br/&gt;
       no&lt;/li&gt;
	&lt;li&gt;If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16692994" author="githubbot" created="Tue, 20 Nov 2018 10:06:01 +0000"  >&lt;p&gt;twalthr opened a new pull request #7147: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10674&quot; title=&quot;Fix handling of retractions after clean up&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10674&quot;&gt;&lt;del&gt;FLINK-10674&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;table&amp;#93;&lt;/span&gt; Fix handling of retractions after clean up&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/7147&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/7147&lt;/a&gt;&lt;/p&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;What is the purpose of the change&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;   Because state clean up happens in processing time, it might be the case that retractions are arriving after the state has been cleaned up. Before these changes, a new accumulator was created and invalid retraction messages were emitted. This change drops retraction messages for which no accumulator exists. It also refactors the tests and adds more test cases and explanations.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Brief change log&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Fix of `org.apache.flink.table.runtime.aggregate.GroupAggProcessFunction`&lt;/li&gt;
	&lt;li&gt;Improvements to the harness tests&lt;/li&gt;
&lt;/ul&gt;



&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Verifying this change&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;   Additional harness tests and one ITCase for nested distinct aggregates.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Does this pull request potentially affect one of the following parts:&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Dependencies (does it add or upgrade a dependency): no&lt;/li&gt;
	&lt;li&gt;The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no&lt;/li&gt;
	&lt;li&gt;The serializers: no&lt;/li&gt;
	&lt;li&gt;The runtime per-record code paths (performance sensitive): yes&lt;/li&gt;
	&lt;li&gt;Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no&lt;/li&gt;
	&lt;li&gt;The S3 file system connector: no&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Documentation&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Does this pull request introduce a new feature? no&lt;/li&gt;
	&lt;li&gt;If yes, how is the feature documented? not applicable&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16694650" author="githubbot" created="Wed, 21 Nov 2018 12:24:43 +0000"  >&lt;p&gt;yanghua commented on issue #7076: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10674&quot; title=&quot;Fix handling of retractions after clean up&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10674&quot;&gt;&lt;del&gt;FLINK-10674&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;table&amp;#93;&lt;/span&gt; Fix DistinctAccumulator.remove lead to NPE.&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/7076#issuecomment-440644786&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/7076#issuecomment-440644786&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   @twalthr &lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16694681" author="githubbot" created="Wed, 21 Nov 2018 13:05:20 +0000"  >&lt;p&gt;twalthr commented on issue #7076: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10674&quot; title=&quot;Fix handling of retractions after clean up&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10674&quot;&gt;&lt;del&gt;FLINK-10674&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;table&amp;#93;&lt;/span&gt; Fix DistinctAccumulator.remove lead to NPE.&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/7076#issuecomment-440655504&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/7076#issuecomment-440655504&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   I looked into the root cause of this NPE and opened #7147 for it. It includes proper tests. Feel free to try it out.&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16694682" author="githubbot" created="Wed, 21 Nov 2018 13:05:20 +0000"  >&lt;p&gt;twalthr closed pull request #7076: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10674&quot; title=&quot;Fix handling of retractions after clean up&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10674&quot;&gt;&lt;del&gt;FLINK-10674&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;table&amp;#93;&lt;/span&gt; Fix DistinctAccumulator.remove lead to NPE.&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/7076&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/7076&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/functions/aggfunctions/DistinctAccumulator.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/functions/aggfunctions/DistinctAccumulator.scala&lt;br/&gt;
index 3427c9c96f7..91c2f923b72 100644&lt;br/&gt;
&amp;#8212; a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/functions/aggfunctions/DistinctAccumulator.scala&lt;br/&gt;
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/functions/aggfunctions/DistinctAccumulator.scala&lt;br/&gt;
@@ -101,13 +101,18 @@ class DistinctAccumulator&lt;span class=&quot;error&quot;&gt;&amp;#91;ACC&amp;#93;&lt;/span&gt;(&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@return true if no instances of the parameters remain in the map, false otherwise.&lt;br/&gt;
     */&lt;br/&gt;
   def remove(params: Row): Boolean = {&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val currentCnt = distinctValueMap.get(params)&lt;/li&gt;
	&lt;li&gt;if (currentCnt == 1) {&lt;/li&gt;
	&lt;li&gt;distinctValueMap.remove(params)&lt;br/&gt;
+    if (!distinctValueMap.contains(params)) 
{
       true
     }
&lt;p&gt; else {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;distinctValueMap.put(params, currentCnt - 1L)&lt;/li&gt;
	&lt;li&gt;false&lt;br/&gt;
+      val currentCnt = distinctValueMap.get(params)&lt;br/&gt;
+&lt;br/&gt;
+      if (currentCnt == null || currentCnt &amp;lt;= 1) 
{
+        distinctValueMap.remove(params)
+        true
+      }
&lt;p&gt; else &lt;/p&gt;
{
+        distinctValueMap.put(params, currentCnt - 1L)
+        false
+      }
&lt;p&gt;     }&lt;br/&gt;
   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;






&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16694683" author="githubbot" created="Wed, 21 Nov 2018 13:05:45 +0000"  >&lt;p&gt;twalthr commented on issue #7067: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10674&quot; title=&quot;Fix handling of retractions after clean up&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10674&quot;&gt;&lt;del&gt;FLINK-10674&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;table&amp;#93;&lt;/span&gt; DistinctAccumulator.remove occur NPE&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/7067#issuecomment-440655617&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/7067#issuecomment-440655617&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   I looked into the root cause of this NPE and opened #7147 for it. It includes proper tests. Feel free to try it out.&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16694684" author="githubbot" created="Wed, 21 Nov 2018 13:05:46 +0000"  >&lt;p&gt;twalthr closed pull request #7067: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10674&quot; title=&quot;Fix handling of retractions after clean up&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10674&quot;&gt;&lt;del&gt;FLINK-10674&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;table&amp;#93;&lt;/span&gt; DistinctAccumulator.remove occur NPE&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/7067&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/7067&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/flink-libraries/flink-table/pom.xml b/flink-libraries/flink-table/pom.xml&lt;br/&gt;
index 77d365066ab..ba1e5ff76f3 100644&lt;br/&gt;
&amp;#8212; a/flink-libraries/flink-table/pom.xml&lt;br/&gt;
+++ b/flink-libraries/flink-table/pom.xml&lt;br/&gt;
@@ -66,6 +66,14 @@ under the License.&lt;br/&gt;
 			&amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt;&lt;br/&gt;
 		&amp;lt;/dependency&amp;gt;&lt;/p&gt;

&lt;p&gt;+		&amp;lt;!-- Used for code generation --&amp;gt;&lt;br/&gt;
+		&amp;lt;dependency&amp;gt;&lt;br/&gt;
+			&amp;lt;groupId&amp;gt;org.apache.flink&amp;lt;/groupId&amp;gt;&lt;br/&gt;
+			&amp;lt;artifactId&amp;gt;flink-cep_${scala.binary.version}&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
+			&amp;lt;version&amp;gt;${project.version}&amp;lt;/version&amp;gt;&lt;br/&gt;
+			&amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt;&lt;br/&gt;
+		&amp;lt;/dependency&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
 		&amp;lt;dependency&amp;gt;&lt;br/&gt;
 			&amp;lt;groupId&amp;gt;org.apache.flink&amp;lt;/groupId&amp;gt;&lt;br/&gt;
 			&amp;lt;artifactId&amp;gt;flink-shaded-jackson&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/calcite/RelTimeIndicatorConverter.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/calcite/RelTimeIndicatorConverter.scala&lt;br/&gt;
index 4f3fbaa8ede..3636c657580 100644&lt;br/&gt;
&amp;#8212; a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/calcite/RelTimeIndicatorConverter.scala&lt;br/&gt;
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/calcite/RelTimeIndicatorConverter.scala&lt;br/&gt;
@@ -34,6 +34,8 @@ import org.apache.flink.table.validate.BasicOperatorTable&lt;/p&gt;

&lt;p&gt; import scala.collection.JavaConversions._&lt;br/&gt;
 import scala.collection.mutable&lt;br/&gt;
+import java.util.Map;&lt;br/&gt;
+import java.util.TreeSet;&lt;/p&gt;

&lt;p&gt; /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Traverses a [&lt;span class=&quot;error&quot;&gt;&amp;#91;RelNode&amp;#93;&lt;/span&gt;] tree and converts fields with [&lt;span class=&quot;error&quot;&gt;&amp;#91;TimeIndicatorRelDataType&amp;#93;&lt;/span&gt;] type. If a&lt;br/&gt;
@@ -99,8 +101,52 @@ class RelTimeIndicatorConverter(rexBuilder: RexBuilder) extends RelShuttle 
{
     LogicalSort.create(input, sort.collation, sort.offset, sort.fetch)
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;override def visit(`match`: LogicalMatch): RelNode =&lt;/li&gt;
	&lt;li&gt;throw new TableException(&quot;Logical match in a stream environment is not supported yet.&quot;)&lt;br/&gt;
+  override def visit(`match`: LogicalMatch): RelNode = 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+    // input row type+    val rowType = `match`.getInput.getRowType+    val rexTimeIndicatorMaterializer = new RexTimeIndicatorMaterializer(+      rexBuilder,+      rowType.getFieldList.map(_.getType)+    )++    // pattern definition+    val patternDefinitions =+      `match`.getPatternDefinitions.foldLeft(mutable.Map[String, RexNode]()) {
+        case (m, (k, v)) =&amp;gt; m += k -&amp;gt; v.accept(rexTimeIndicatorMaterializer)
+      }++    val measures = `match`.getMeasures.foldLeft(mutable.Map[String, RexNode]()) {
+      case (m, (k, v)) =&amp;gt; m += k -&amp;gt; v.accept(rexTimeIndicatorMaterializer)
+    }++    // output types+    val outputTypeBuilder = rexBuilder+      .getTypeFactory+      .asInstanceOf[FlinkTypeFactory]+      .builder()++    `match`.getRowType.getFieldList+      .foreach(x =&amp;gt; measures.get(x.getName) match {
+        case Some(measure) =&amp;gt; outputTypeBuilder.add(x.getName, measure.getType)
+        case None =&amp;gt; outputTypeBuilder.add(x)
+      })++    // creates a LogicalMatch+    LogicalMatch.create(+      `match`.getInput,+      outputTypeBuilder.build(),+      `match`.getPattern,+      `match`.isStrictStart,+      `match`.isStrictEnd,+      patternDefinitions,+      measures,+      `match`.getAfter,+      `match`.getSubsets.asInstanceOf[Map[String, TreeSet[String]]],+      `match`.isAllRows,+      `match`.getPartitionKeys,+      `match`.getOrderKeys,+      `match`.getInterval)+  }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   override def visit(other: RelNode): RelNode = other match &lt;/p&gt;
{
 
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/codegen/CodeGenerator.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/codegen/CodeGenerator.scala
index 6cabe212213..680cede106e 100644
--- a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/codegen/CodeGenerator.scala
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/codegen/CodeGenerator.scala
@@ -756,6 +756,13 @@ abstract class CodeGenerator(
       case (o@_, _) =&amp;gt;
         o.accept(this)
     }
&lt;p&gt;+    generateCallExpression(call, operands, resultType)&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  def generateCallExpression(&lt;br/&gt;
+    call: RexCall,&lt;br/&gt;
+    operands: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;GeneratedExpression&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+    resultType: TypeInformation&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;): GeneratedExpression = {&lt;/p&gt;

&lt;p&gt;     call.getOperator match &lt;/p&gt;
{
       // arithmetic
@@ -1193,7 +1200,7 @@ abstract class CodeGenerator(
     GeneratedExpression(resultTerm, nullTerm, inputCheckCode, fieldType)
   }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private def generateFieldAccess(&lt;br/&gt;
+   def generateFieldAccess(&lt;br/&gt;
       inputType: TypeInformation&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;,&lt;br/&gt;
       inputTerm: String,&lt;br/&gt;
       index: Int)&lt;br/&gt;
@@ -1969,4 +1976,15 @@ abstract class CodeGenerator(&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     fieldTerm&lt;br/&gt;
   }&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+    * MATCH_RECOGNIZE PATTERN() patternVariable&lt;br/&gt;
+    */&lt;br/&gt;
+  def addReusableInitStatement(initStatement: String): Unit = &lt;/p&gt;
{
+    reusableInitStatements.add(initStatement)
+  }
&lt;p&gt;+&lt;br/&gt;
+  def addReusableMemberStatement(memberStatement: String): Unit = &lt;/p&gt;
{
+    reusableMemberStatements.add(memberStatement)
+  }
&lt;p&gt; }&lt;br/&gt;
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/codegen/MatchCodeGenerator.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/codegen/MatchCodeGenerator.scala&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..0ea8ef1ebb7&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/codegen/MatchCodeGenerator.scala&lt;br/&gt;
@@ -0,0 +1,594 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
+ * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
+ * distributed with this work for additional information&lt;br/&gt;
+ * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
+ * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
+ * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
+ * with the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package org.apache.flink.table.codegen&lt;br/&gt;
+&lt;br/&gt;
+import java.math.&lt;/p&gt;
{BigDecimal =&amp;gt; JBigDecimal}&lt;br/&gt;
+import java.util&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.calcite.rel.RelCollation&lt;br/&gt;
+import org.apache.calcite.rex._&lt;br/&gt;
+import org.apache.calcite.sql.fun.SqlStdOperatorTable.{CLASSIFIER, FINAL, FIRST, LAST, MATCH_NUMBER, NEXT, PREV, RUNNING}&lt;br/&gt;
+import org.apache.flink.api.common.typeinfo.TypeInformation&lt;br/&gt;
+import org.apache.flink.cep.{PatternFlatSelectFunction, PatternSelectFunction}&lt;br/&gt;
+import org.apache.flink.cep.pattern.conditions.IterativeCondition&lt;br/&gt;
+import org.apache.flink.table.api.TableConfig&lt;br/&gt;
+import org.apache.flink.table.calcite.FlinkTypeFactory&lt;br/&gt;
+import org.apache.flink.table.codegen.Indenter.toISC&lt;br/&gt;
+import org.apache.flink.table.codegen.CodeGenUtils.{boxedTypeTermForTypeInfo, newName, primitiveDefaultValue}&lt;br/&gt;
+import org.apache.flink.table.plan.schema.RowSchema&lt;br/&gt;
+import org.apache.flink.types.Row&lt;br/&gt;
+&lt;br/&gt;
+import scala.collection.JavaConverters._&lt;br/&gt;
+import scala.collection.mutable&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+  * A code generator for generating CEP related functions.&lt;br/&gt;
+  *&lt;br/&gt;
+  * @param config configuration that determines runtime behavior&lt;br/&gt;
+  * @param nullableInput input(s) can be null.&lt;br/&gt;
+  * @param input type information about the first input of the Function&lt;br/&gt;
+  * @param patternNames the names of patterns&lt;br/&gt;
+  * @param generateCondition whether the code generator is generating [&lt;span class=&quot;error&quot;&gt;&amp;#91;IterativeCondition&amp;#93;&lt;/span&gt;]&lt;br/&gt;
+  * @param patternName the name of current pattern&lt;br/&gt;
+  */&lt;br/&gt;
+class MatchCodeGenerator(&lt;br/&gt;
+    config: TableConfig,&lt;br/&gt;
+    nullableInput: Boolean,&lt;br/&gt;
+    input: TypeInformation&lt;span class=&quot;error&quot;&gt;&amp;#91;_ &amp;lt;: Any&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+    patternNames: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+    generateCondition: Boolean,&lt;br/&gt;
+    patternName: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt; = None)&lt;br/&gt;
+  extends CodeGenerator(config, nullableInput, input) {&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+    * @return term of pattern names&lt;br/&gt;
+    */&lt;br/&gt;
+  private val patternNameListTerm = newName(&quot;patternNameList&quot;)&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+    * @return term of current pattern which is processing&lt;br/&gt;
+    */&lt;br/&gt;
+  private val currPatternTerm = newName(&quot;currPattern&quot;)&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+    * @return term of current event which is processing&lt;br/&gt;
+    */&lt;br/&gt;
+  private val currEventTerm = newName(&quot;currEvent&quot;)&lt;br/&gt;
+&lt;br/&gt;
+  private val buildPatternNameList: String = {
+    for (patternName &amp;lt;- patternNames) yield
+      s&quot;&quot;&quot;
+         |$patternNameListTerm.add(&quot;$patternName&quot;);
+         |&quot;&quot;&quot;.stripMargin
+  }.mkString(&quot;\n&quot;)&lt;br/&gt;
+&lt;br/&gt;
+  def addReusableStatements(): Unit = {
+    val eventTypeTerm = boxedTypeTermForTypeInfo(input)
+    val memberStatement =
+      s&quot;&quot;&quot;
+         |$eventTypeTerm $currEventTerm = null;
+         |String $currPatternTerm = null;
+         |java.util.List&amp;lt;String&amp;gt; $patternNameListTerm = new java.util.ArrayList();
+         |&quot;&quot;&quot;.stripMargin
+
+    addReusableMemberStatement(memberStatement)
+
+    addReusableInitStatement(buildPatternNameList)
+  }&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+    * Generates a [&lt;span class=&quot;error&quot;&gt;&amp;#91;IterativeCondition&amp;#93;&lt;/span&gt;] that can be passed to Java compiler.&lt;br/&gt;
+    *&lt;br/&gt;
+    * @param name     Class name of the function. Must not be unique but has to be a&lt;br/&gt;
+    *                 valid Java class identifier.&lt;br/&gt;
+    * @param bodyCode body code for the function&lt;br/&gt;
+    * @return a GeneratedIterativeCondition&lt;br/&gt;
+    */&lt;br/&gt;
+  def generateIterativeCondition(&lt;br/&gt;
+                                  name: String,&lt;br/&gt;
+                                  bodyCode: String)&lt;br/&gt;
+  : GeneratedIterativeCondition = {&lt;br/&gt;
+&lt;br/&gt;
+    val funcName = newName(name)&lt;br/&gt;
+    val inputTypeTerm = boxedTypeTermForTypeInfo(input)&lt;br/&gt;
+&lt;br/&gt;
+    val funcCode =&lt;br/&gt;
+      j&quot;&quot;&quot;&lt;br/&gt;
+      public class $funcName&lt;br/&gt;
+          extends ${classOf[IterativeCondition&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;].getCanonicalName} {&lt;br/&gt;
+&lt;br/&gt;
+        ${reuseMemberCode()}&lt;br/&gt;
+&lt;br/&gt;
+        public $funcName() throws Exception {&lt;br/&gt;
+          ${reuseInitCode()}&lt;br/&gt;
+        }&lt;br/&gt;
+&lt;br/&gt;
+        @Override&lt;br/&gt;
+        public boolean filter(&lt;br/&gt;
+          Object _in1, ${classOf[IterativeCondition.Context&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;].getCanonicalName} $contextTerm)&lt;br/&gt;
+          throws Exception {&lt;br/&gt;
+&lt;br/&gt;
+          $inputTypeTerm $input1Term = ($inputTypeTerm) _in1;&lt;br/&gt;
+          ${reusePerRecordCode()}&lt;br/&gt;
+          ${reuseInputUnboxingCode()}&lt;br/&gt;
+          $bodyCode&lt;br/&gt;
+        }&lt;br/&gt;
+      }&lt;br/&gt;
+    &quot;&quot;&quot;.stripMargin&lt;br/&gt;
+&lt;br/&gt;
+    GeneratedIterativeCondition(funcName, funcCode)&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+    * Generates a [&lt;span class=&quot;error&quot;&gt;&amp;#91;PatternSelectFunction&amp;#93;&lt;/span&gt;] that can be passed to Java compiler.&lt;br/&gt;
+    *&lt;br/&gt;
+    * @param name     Class name of the function. Must not be unique but has to be a&lt;br/&gt;
+    *                 valid Java class identifier.&lt;br/&gt;
+    * @param bodyCode body code for the function&lt;br/&gt;
+    * @return a GeneratedPatternSelectFunction&lt;br/&gt;
+    */&lt;br/&gt;
+  def generatePatternSelectFunction(&lt;br/&gt;
+                                     name: String,&lt;br/&gt;
+                                     bodyCode: String)&lt;br/&gt;
+  : GeneratedPatternSelectFunction = {&lt;br/&gt;
+&lt;br/&gt;
+    val funcName = newName(name)&lt;br/&gt;
+    val inputTypeTerm =&lt;br/&gt;
+      classOf[java.util.Map[java.lang.String, java.util.List&lt;span class=&quot;error&quot;&gt;&amp;#91;Row&amp;#93;&lt;/span&gt;]].getCanonicalName&lt;br/&gt;
+&lt;br/&gt;
+    val funcCode =&lt;br/&gt;
+      j&quot;&quot;&quot;&lt;br/&gt;
+      public class $funcName&lt;br/&gt;
+          implements ${classOf[PatternSelectFunction&lt;span class=&quot;error&quot;&gt;&amp;#91;_, _&amp;#93;&lt;/span&gt;].getCanonicalName} {&lt;br/&gt;
+&lt;br/&gt;
+        ${reuseMemberCode()}&lt;br/&gt;
+&lt;br/&gt;
+        public $funcName() throws Exception {&lt;br/&gt;
+          ${reuseInitCode()}&lt;br/&gt;
+        }&lt;br/&gt;
+&lt;br/&gt;
+        @Override&lt;br/&gt;
+        public Object select(java.util.Map&amp;lt;String, java.util.List&amp;lt;Object&amp;gt;&amp;gt; _in1)&lt;br/&gt;
+          throws Exception {&lt;br/&gt;
+&lt;br/&gt;
+          $inputTypeTerm $input1Term = ($inputTypeTerm) _in1;&lt;br/&gt;
+          ${reusePerRecordCode()}&lt;br/&gt;
+          ${reuseInputUnboxingCode()}&lt;br/&gt;
+          $bodyCode&lt;br/&gt;
+        }&lt;br/&gt;
+      }&lt;br/&gt;
+    &quot;&quot;&quot;.stripMargin&lt;br/&gt;
+&lt;br/&gt;
+    GeneratedPatternSelectFunction(funcName, funcCode)&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+    * Generates a [&lt;span class=&quot;error&quot;&gt;&amp;#91;PatternFlatSelectFunction&amp;#93;&lt;/span&gt;] that can be passed to Java compiler.&lt;br/&gt;
+    *&lt;br/&gt;
+    * @param name     Class name of the function. Must not be unique but has to be a&lt;br/&gt;
+    *                 valid Java class identifier.&lt;br/&gt;
+    * @param bodyCode body code for the function&lt;br/&gt;
+    * @return a GeneratedPatternFlatSelectFunction&lt;br/&gt;
+    */&lt;br/&gt;
+  def generatePatternFlatSelectFunction(&lt;br/&gt;
+                                         name: String,&lt;br/&gt;
+                                         bodyCode: String)&lt;br/&gt;
+  : GeneratedPatternFlatSelectFunction = {&lt;br/&gt;
+&lt;br/&gt;
+    val funcName = newName(name)&lt;br/&gt;
+    val inputTypeTerm =&lt;br/&gt;
+      classOf[java.util.Map[java.lang.String, java.util.List&lt;span class=&quot;error&quot;&gt;&amp;#91;Row&amp;#93;&lt;/span&gt;]].getCanonicalName&lt;br/&gt;
+&lt;br/&gt;
+    val funcCode =&lt;br/&gt;
+      j&quot;&quot;&quot;&lt;br/&gt;
+      public class $funcName&lt;br/&gt;
+          implements ${classOf[PatternFlatSelectFunction&lt;span class=&quot;error&quot;&gt;&amp;#91;_, _&amp;#93;&lt;/span&gt;].getCanonicalName} {&lt;br/&gt;
+&lt;br/&gt;
+        ${reuseMemberCode()}&lt;br/&gt;
+&lt;br/&gt;
+        public $funcName() throws Exception {&lt;br/&gt;
+          ${reuseInitCode()}&lt;br/&gt;
+        }&lt;br/&gt;
+&lt;br/&gt;
+        @Override&lt;br/&gt;
+        public void flatSelect(java.util.Map&amp;lt;String, java.util.List&amp;lt;Object&amp;gt;&amp;gt; _in1,&lt;br/&gt;
+            org.apache.flink.util.Collector $collectorTerm)&lt;br/&gt;
+          throws Exception {&lt;br/&gt;
+&lt;br/&gt;
+          $inputTypeTerm $input1Term = ($inputTypeTerm) _in1;&lt;br/&gt;
+          ${reusePerRecordCode()}&lt;br/&gt;
+          ${reuseInputUnboxingCode()}&lt;br/&gt;
+          $bodyCode&lt;br/&gt;
+        }&lt;br/&gt;
+      }&lt;br/&gt;
+    &quot;&quot;&quot;.stripMargin&lt;br/&gt;
+&lt;br/&gt;
+    GeneratedPatternFlatSelectFunction(funcName, funcCode)&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  def generateSelectOutputExpression(&lt;br/&gt;
+                                      partitionKeys: util.List&lt;span class=&quot;error&quot;&gt;&amp;#91;RexNode&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+                                      measures: util.Map&lt;span class=&quot;error&quot;&gt;&amp;#91;String, RexNode&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+                                      returnType: RowSchema)&lt;br/&gt;
+  : GeneratedExpression = {&lt;br/&gt;
+&lt;br/&gt;
+    val eventNameTerm = newName(&quot;event&quot;)&lt;br/&gt;
+    val eventTypeTerm = boxedTypeTermForTypeInfo(input)&lt;br/&gt;
+&lt;br/&gt;
+    // For &quot;ONE ROW PER MATCH&quot;, the output columns include:&lt;br/&gt;
+    // 1) the partition columns;&lt;br/&gt;
+    // 2) the columns defined in the measures clause.&lt;br/&gt;
+    val resultExprs =&lt;br/&gt;
+    partitionKeys.asScala.map { case inputRef: RexInputRef =&amp;gt;
+      generateFieldAccess(input, eventNameTerm, inputRef.getIndex)
+    } ++ returnType.fieldNames.filter(measures.containsKey(_)).map { fieldName =&amp;gt;
+      generateExpression(measures.get(fieldName))
+    }&lt;br/&gt;
+&lt;br/&gt;
+    val resultExpression = generateResultExpression(&lt;br/&gt;
+      resultExprs,&lt;br/&gt;
+      returnType.typeInfo,&lt;br/&gt;
+      returnType.fieldNames)&lt;br/&gt;
+&lt;br/&gt;
+    val resultCode =&lt;br/&gt;
+      s&quot;&quot;&quot;&lt;br/&gt;
+         |$eventTypeTerm $eventNameTerm = null;&lt;br/&gt;
+         |if (${partitionKeys.size()} &amp;gt; 0) {&lt;br/&gt;
+         |  for (java.util.Map.Entry entry : $input1Term.entrySet()) {&lt;br/&gt;
+         |    java.util.List&amp;lt;Row&amp;gt; value = (java.util.List&amp;lt;Row&amp;gt;) entry.getValue();&lt;br/&gt;
+         |    if (value != null &amp;amp;&amp;amp; value.size() &amp;gt; 0) {
+         |      $eventNameTerm = ($eventTypeTerm) value.get(0);
+         |      break;
+         |    }&lt;br/&gt;
+         |  }&lt;br/&gt;
+         |}&lt;br/&gt;
+         |&lt;br/&gt;
+        |${resultExpression.code}&lt;br/&gt;
+         |&quot;&quot;&quot;.stripMargin&lt;br/&gt;
+&lt;br/&gt;
+    resultExpression.copy(code = resultCode)&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  def generateFlatSelectOutputExpression(&lt;br/&gt;
+                                          partitionKeys: util.List&lt;span class=&quot;error&quot;&gt;&amp;#91;RexNode&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+                                          orderKeys: RelCollation,&lt;br/&gt;
+                                          measures: util.Map&lt;span class=&quot;error&quot;&gt;&amp;#91;String, RexNode&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+                                          returnType: RowSchema)&lt;br/&gt;
+  : GeneratedExpression = {&lt;br/&gt;
+&lt;br/&gt;
+    val patternNameTerm = newName(&quot;patternName&quot;)&lt;br/&gt;
+    val eventNameTerm = newName(&quot;event&quot;)&lt;br/&gt;
+    val eventNameListTerm = newName(&quot;eventList&quot;)&lt;br/&gt;
+    val eventTypeTerm = boxedTypeTermForTypeInfo(input)&lt;br/&gt;
+    val listTypeTerm = classOf[java.util.List&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;].getCanonicalName&lt;br/&gt;
+&lt;br/&gt;
+    // For &quot;ALL ROWS PER MATCH&quot;, the output columns include:&lt;br/&gt;
+    // 1) the partition columns;&lt;br/&gt;
+    // 2) the ordering columns;&lt;br/&gt;
+    // 3) the columns defined in the measures clause;&lt;br/&gt;
+    // 4) any remaining columns defined of the input.&lt;br/&gt;
+    val fieldsAccessed = mutable.Set&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;()&lt;br/&gt;
+    val resultExprs =&lt;br/&gt;
+      partitionKeys.asScala.map { case inputRef: RexInputRef =&amp;gt;
+        fieldsAccessed += inputRef.getIndex
+        generateFieldAccess(input, eventNameTerm, inputRef.getIndex)
+      } ++ orderKeys.getFieldCollations.asScala.map { fieldCollation =&amp;gt;
+        fieldsAccessed += fieldCollation.getFieldIndex
+        generateFieldAccess(input, eventNameTerm, fieldCollation.getFieldIndex)
+      } ++ (0 until input.getArity).filterNot(fieldsAccessed.contains).map { idx =&amp;gt;
+        generateFieldAccess(input, eventNameTerm, idx)
+      } ++ returnType.fieldNames.filter(measures.containsKey(_)).map { fieldName =&amp;gt;
+        generateExpression(measures.get(fieldName))
+      }&lt;br/&gt;
+&lt;br/&gt;
+    val resultExpression = generateResultExpression(&lt;br/&gt;
+      resultExprs,&lt;br/&gt;
+      returnType.typeInfo,&lt;br/&gt;
+      returnType.fieldNames)&lt;br/&gt;
+&lt;br/&gt;
+    val resultCode =&lt;br/&gt;
+      s&quot;&quot;&quot;&lt;br/&gt;
+         |for (String $patternNameTerm : $patternNameListTerm) {&lt;br/&gt;
+         |  $currPatternTerm = $patternNameTerm;&lt;br/&gt;
+         |  $listTypeTerm $eventNameListTerm = ($listTypeTerm) $input1Term.get($patternNameTerm);&lt;br/&gt;
+         |  if ($eventNameListTerm != null) {&lt;br/&gt;
+         |    for ($eventTypeTerm $eventNameTerm : $eventNameListTerm) {&lt;br/&gt;
+         |      $currEventTerm = $eventNameTerm;&lt;br/&gt;
+         |      ${resultExpression.code}&lt;br/&gt;
+         |      $collectorTerm.collect(${resultExpression.resultTerm});&lt;br/&gt;
+         |    }&lt;br/&gt;
+         |  }&lt;br/&gt;
+         |}&lt;br/&gt;
+         |$currPatternTerm = null;&lt;br/&gt;
+         |$currEventTerm = null;&lt;br/&gt;
+         |&quot;&quot;&quot;.stripMargin&lt;br/&gt;
+&lt;br/&gt;
+    GeneratedExpression(&quot;&quot;, &quot;false&quot;, resultCode, null)&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  override def visitCall(call: RexCall): GeneratedExpression = {&lt;br/&gt;
+    val resultType = FlinkTypeFactory.toTypeInfo(call.getType)&lt;br/&gt;
+    call.getOperator match {
+      case PREV =&amp;gt;
+        val countLiteral = call.operands.get(1).asInstanceOf[RexLiteral]
+        val count = countLiteral.getValue3.asInstanceOf[JBigDecimal].intValue()
+        generatePrev(
+          call.operands.get(0),
+          count,
+          resultType)
+
+      case NEXT | CLASSIFIER | MATCH_NUMBER =&amp;gt;
+        throw new CodeGenException(s&quot;Unsupported call: $call&quot;)
+
+      case FIRST | LAST =&amp;gt;
+        val countLiteral = call.operands.get(1).asInstanceOf[RexLiteral]
+        val count = countLiteral.getValue3.asInstanceOf[JBigDecimal].intValue()
+        generateFirstLast(
+          call.operands.get(0),
+          count,
+          resultType,
+          running = true,
+          call.getOperator == FIRST)
+
+      case RUNNING | FINAL =&amp;gt;
+        generateRunningFinal(
+          call.operands.get(0),
+          resultType,
+          call.getOperator == RUNNING)
+
+      case _ =&amp;gt; super.visitCall(call)
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  private def generatePrev(&lt;br/&gt;
+    rexNode: RexNode,&lt;br/&gt;
+    count: Int,&lt;br/&gt;
+    resultType: TypeInformation&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;): GeneratedExpression = {&lt;br/&gt;
+    rexNode match {&lt;br/&gt;
+      case patternFieldRef: RexPatternFieldRef =&amp;gt;&lt;br/&gt;
+        if (count == 0 &amp;amp;&amp;amp; patternFieldRef.getAlpha == patternName.get) {
+          // return current one
+          return visitInputRef(patternFieldRef)
+        }&lt;br/&gt;
+&lt;br/&gt;
+        val listName = newName(&quot;patternEvents&quot;)&lt;br/&gt;
+        val resultTerm = newName(&quot;result&quot;)&lt;br/&gt;
+        val nullTerm = newName(&quot;isNull&quot;)&lt;br/&gt;
+        val indexTerm = newName(&quot;eventIndex&quot;)&lt;br/&gt;
+        val visitedEventNumberTerm = newName(&quot;visitedEventNumber&quot;)&lt;br/&gt;
+        val eventTerm = newName(&quot;event&quot;)&lt;br/&gt;
+        val resultTypeTerm = boxedTypeTermForTypeInfo(resultType)&lt;br/&gt;
+        val defaultValue = primitiveDefaultValue(resultType)&lt;br/&gt;
+&lt;br/&gt;
+        val eventTypeTerm = boxedTypeTermForTypeInfo(input)&lt;br/&gt;
+&lt;br/&gt;
+        val patternNamesToVisit = patternNames&lt;br/&gt;
+          .take(patternNames.indexOf(patternFieldRef.getAlpha) + 1)&lt;br/&gt;
+          .reverse&lt;br/&gt;
+&lt;br/&gt;
+        def findEventByPhysicalPosition: String = {&lt;br/&gt;
+          val init: String =&lt;br/&gt;
+            s&quot;&quot;&quot;&lt;br/&gt;
+               |java.util.List $listName = new java.util.ArrayList();&lt;br/&gt;
+               |&quot;&quot;&quot;.stripMargin&lt;br/&gt;
+&lt;br/&gt;
+          val getResult: String = {&lt;br/&gt;
+            for (tmpPatternName &amp;lt;- patternNamesToVisit) yield&lt;br/&gt;
+              s&quot;&quot;&quot;&lt;br/&gt;
+                 |for ($eventTypeTerm $eventTerm : $contextTerm&lt;br/&gt;
+                 |    .getEventsForPattern(&quot;$tmpPatternName&quot;)) {
+                 |  $listName.add($eventTerm);
+                 |}&lt;br/&gt;
+                 |&lt;br/&gt;
+                |$indexTerm = $listName.size() - ($count - $visitedEventNumberTerm);&lt;br/&gt;
+                 |if ($indexTerm &amp;gt;= 0) {&lt;br/&gt;
+                 |  $resultTerm = ($resultTypeTerm) (($eventTypeTerm) $listName.get($indexTerm))&lt;br/&gt;
+                 |    .getField(${patternFieldRef.getIndex});&lt;br/&gt;
+                 |  $nullTerm = false;&lt;br/&gt;
+                 |  break;&lt;br/&gt;
+                 |}&lt;br/&gt;
+                 |&lt;br/&gt;
+                |$visitedEventNumberTerm += $listName.size();&lt;br/&gt;
+                 |$listName.clear();&lt;br/&gt;
+                 |&quot;&quot;&quot;.stripMargin&lt;br/&gt;
+          }.mkString(&quot;\n&quot;)&lt;br/&gt;
+&lt;br/&gt;
+          s&quot;&quot;&quot;&lt;br/&gt;
+             |$init&lt;br/&gt;
+             |$getResult&lt;br/&gt;
+             |&quot;&quot;&quot;.stripMargin&lt;br/&gt;
+        }&lt;br/&gt;
+&lt;br/&gt;
+        val resultCode =&lt;br/&gt;
+          s&quot;&quot;&quot;&lt;br/&gt;
+             |int $visitedEventNumberTerm = 0;&lt;br/&gt;
+             |int $indexTerm;&lt;br/&gt;
+             |$resultTypeTerm $resultTerm = $defaultValue;&lt;br/&gt;
+             |boolean $nullTerm = true;&lt;br/&gt;
+             |do {
+             |  $findEventByPhysicalPosition
+             |} while (false);&lt;br/&gt;
+             |&quot;&quot;&quot;.stripMargin&lt;br/&gt;
+&lt;br/&gt;
+        GeneratedExpression(resultTerm, nullTerm, resultCode, resultType)&lt;br/&gt;
+&lt;br/&gt;
+      case rexCall: RexCall =&amp;gt;&lt;br/&gt;
+        val operands = rexCall.operands.asScala.map {
+          operand =&amp;gt;
+            generatePrev(
+              operand,
+              count,
+              FlinkTypeFactory.toTypeInfo(operand.getType))
+        }&lt;br/&gt;
+&lt;br/&gt;
+        generateCallExpression(rexCall, operands, resultType)&lt;br/&gt;
+&lt;br/&gt;
+      case _ =&amp;gt;&lt;br/&gt;
+        generateExpression(rexNode)&lt;br/&gt;
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  private def generateFirstLast(&lt;br/&gt;
+   rexNode: RexNode,&lt;br/&gt;
+   count: Int,&lt;br/&gt;
+   resultType: TypeInformation&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+   running: Boolean,&lt;br/&gt;
+   first: Boolean): GeneratedExpression = {&lt;br/&gt;
+   rexNode match {&lt;br/&gt;
+     case patternFieldRef: RexPatternFieldRef =&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+        val eventNameTerm = newName(&quot;event&quot;)&lt;br/&gt;
+        val resultTerm = newName(&quot;result&quot;)&lt;br/&gt;
+        val listName = newName(&quot;patternEvents&quot;)&lt;br/&gt;
+        val nullTerm = newName(&quot;isNull&quot;)&lt;br/&gt;
+        val patternNameTerm = newName(&quot;patternName&quot;)&lt;br/&gt;
+        val eventNameListTerm = newName(&quot;eventNameList&quot;)&lt;br/&gt;
+        val resultTypeTerm = boxedTypeTermForTypeInfo(resultType)&lt;br/&gt;
+        val defaultValue = primitiveDefaultValue(resultType)&lt;br/&gt;
+&lt;br/&gt;
+        val eventTypeTerm = boxedTypeTermForTypeInfo(input)&lt;br/&gt;
+        val listTypeTerm = classOf[java.util.List&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;].getCanonicalName&lt;br/&gt;
+&lt;br/&gt;
+        def findEventByLogicalPosition: String = {&lt;br/&gt;
+          val init =&lt;br/&gt;
+            s&quot;&quot;&quot;&lt;br/&gt;
+               |java.util.List $listName = new java.util.ArrayList();&lt;br/&gt;
+               |&quot;&quot;&quot;.stripMargin&lt;br/&gt;
+&lt;br/&gt;
+          val findEventsByPatterName = if (generateCondition) {&lt;br/&gt;
+            s&quot;&quot;&quot;&lt;br/&gt;
+               |for ($eventTypeTerm $eventNameTerm : $contextTerm&lt;br/&gt;
+               |    .getEventsForPattern(&quot;${patternFieldRef.getAlpha}&quot;)) {
+               |  $listName.add($eventNameTerm);
+               |}&lt;br/&gt;
+               |&quot;&quot;&quot;.stripMargin&lt;br/&gt;
+          } else {&lt;br/&gt;
+            s&quot;&quot;&quot;&lt;br/&gt;
+               |for (String $patternNameTerm : $patternNameListTerm) {&lt;br/&gt;
+               |  if ($patternNameTerm.equals(&quot;${patternFieldRef.getAlpha}&quot;) ||&lt;br/&gt;
+               |      ${patternFieldRef.getAlpha.equals(&quot;*&quot;)}) {&lt;br/&gt;
+               |    boolean skipLoop = false;&lt;br/&gt;
+               |    $listTypeTerm $eventNameListTerm =&lt;br/&gt;
+               |      ($listTypeTerm) $input1Term.get($patternNameTerm);&lt;br/&gt;
+               |    if ($eventNameListTerm != null) {&lt;br/&gt;
+               |      for ($eventTypeTerm $eventNameTerm : $eventNameListTerm) {&lt;br/&gt;
+               |        $listName.add($eventNameTerm);&lt;br/&gt;
+               |        if ($running &amp;amp;&amp;amp; $eventNameTerm == $currEventTerm) {
+               |          skipLoop = true;
+               |          break;
+               |        }&lt;br/&gt;
+               |      }&lt;br/&gt;
+               |    }&lt;br/&gt;
+               |&lt;br/&gt;
+              |    if (skipLoop) {
+               |      break;
+               |    }&lt;br/&gt;
+               |  }&lt;br/&gt;
+               |&lt;br/&gt;
+              |  if ($running &amp;amp;&amp;amp; $patternNameTerm.equals($currPatternTerm)) {
+               |    break;
+               |  }&lt;br/&gt;
+               |}&lt;br/&gt;
+               |&quot;&quot;&quot;.stripMargin&lt;br/&gt;
+          }&lt;br/&gt;
+&lt;br/&gt;
+          val getResult =&lt;br/&gt;
+            s&quot;&quot;&quot;&lt;br/&gt;
+               |if ($listName.size() &amp;gt; $count) {&lt;br/&gt;
+               |  if ($first) {&lt;br/&gt;
+               |    $resultTerm = ($resultTypeTerm) (($eventTypeTerm)&lt;br/&gt;
+               |      $listName.get($count))&lt;br/&gt;
+               |        .getField(${patternFieldRef.getIndex});&lt;br/&gt;
+               |  } else {&lt;br/&gt;
+               |    $resultTerm = ($resultTypeTerm) (($eventTypeTerm)&lt;br/&gt;
+               |      $listName.get($listName.size() - $count - 1))&lt;br/&gt;
+               |        .getField(${patternFieldRef.getIndex});&lt;br/&gt;
+               |  }&lt;br/&gt;
+               |  $nullTerm = false;&lt;br/&gt;
+               |}&lt;br/&gt;
+               |&quot;&quot;&quot;.stripMargin&lt;br/&gt;
+&lt;br/&gt;
+          s&quot;&quot;&quot;&lt;br/&gt;
+             |$init&lt;br/&gt;
+             |$findEventsByPatterName&lt;br/&gt;
+             |$getResult&lt;br/&gt;
+             |&quot;&quot;&quot;.stripMargin&lt;br/&gt;
+        }&lt;br/&gt;
+&lt;br/&gt;
+        val resultCode =&lt;br/&gt;
+          s&quot;&quot;&quot;&lt;br/&gt;
+             |$resultTypeTerm $resultTerm = $defaultValue;&lt;br/&gt;
+             |boolean $nullTerm = true;&lt;br/&gt;
+             |$findEventByLogicalPosition&lt;br/&gt;
+             |&quot;&quot;&quot;.stripMargin&lt;br/&gt;
+&lt;br/&gt;
+        GeneratedExpression(resultTerm, nullTerm, resultCode, resultType)&lt;br/&gt;
+&lt;br/&gt;
+      case rexCall: RexCall =&amp;gt;&lt;br/&gt;
+        val operands = rexCall.operands.asScala.map {
+          operand =&amp;gt;
+            generateFirstLast(
+              operand,
+              count,
+              FlinkTypeFactory.toTypeInfo(operand.getType),
+              running,
+              first)
+        }&lt;br/&gt;
+&lt;br/&gt;
+        generateCallExpression(rexCall, operands, resultType)&lt;br/&gt;
+&lt;br/&gt;
+      case _ =&amp;gt;&lt;br/&gt;
+        generateExpression(rexNode)&lt;br/&gt;
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  private def generateRunningFinal(&lt;br/&gt;
+    rexNode: RexNode,&lt;br/&gt;
+    resultType: TypeInformation&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+    running: Boolean): GeneratedExpression = {&lt;br/&gt;
+    rexNode match {&lt;br/&gt;
+      case _: RexPatternFieldRef =&amp;gt;&lt;br/&gt;
+        generateFirstLast(rexNode, 0, resultType, running, first = false)&lt;br/&gt;
+&lt;br/&gt;
+      case rexCall: RexCall if rexCall.getOperator == FIRST || rexCall.getOperator == LAST =&amp;gt;&lt;br/&gt;
+        val countLiteral = rexCall.operands.get(1).asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;RexLiteral&amp;#93;&lt;/span&gt;&lt;br/&gt;
+        val count = countLiteral.getValue3.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;JBigDecimal&amp;#93;&lt;/span&gt;.intValue()&lt;br/&gt;
+        generateFirstLast(&lt;br/&gt;
+          rexCall.operands.get(0),&lt;br/&gt;
+          count,&lt;br/&gt;
+          resultType,&lt;br/&gt;
+          running,&lt;br/&gt;
+          rexCall.getOperator == FIRST)&lt;br/&gt;
+&lt;br/&gt;
+      case rexCall: RexCall =&amp;gt;&lt;br/&gt;
+        val operands = rexCall.operands.asScala.map {
+          operand =&amp;gt;
+            generateRunningFinal(
+              operand,
+              FlinkTypeFactory.toTypeInfo(operand.getType),
+              running)
+        }&lt;br/&gt;
+&lt;br/&gt;
+        generateCallExpression(rexCall, operands, resultType)&lt;br/&gt;
+&lt;br/&gt;
+      case _ =&amp;gt;&lt;br/&gt;
+        generateExpression(rexNode)&lt;br/&gt;
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/codegen/generated.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/codegen/generated.scala&lt;br/&gt;
index c6d722a59a8..9b43b141c28 100644&lt;br/&gt;
&amp;#8212; a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/codegen/generated.scala&lt;br/&gt;
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/codegen/generated.scala&lt;br/&gt;
@@ -92,3 +92,27 @@ case class GeneratedInput[F &amp;lt;: InputFormat&lt;span class=&quot;error&quot;&gt;&amp;#91;_, _&amp;#93;&lt;/span&gt;, T &amp;lt;: Any](&lt;br/&gt;
   * @param code code of the generated Collector.&lt;br/&gt;
   */&lt;br/&gt;
 case class GeneratedCollector(name: String, code: String)&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+  * Describes a generated [&lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.flink.cep.pattern.conditions.IterativeCondition&amp;#93;&lt;/span&gt;].&lt;br/&gt;
+  *&lt;br/&gt;
+  * @param name class name of the generated IterativeCondition.&lt;br/&gt;
+  * @param code code of the generated IterativeCondition.&lt;br/&gt;
+  */&lt;br/&gt;
+case class GeneratedIterativeCondition(name: String, code: String)&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+  * Describes a generated [&lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.flink.cep.PatternSelectFunction&amp;#93;&lt;/span&gt;].&lt;br/&gt;
+  *&lt;br/&gt;
+  * @param name class name of the generated PatternSelectFunction.&lt;br/&gt;
+  * @param code code of the generated PatternSelectFunction.&lt;br/&gt;
+  */&lt;br/&gt;
+case class GeneratedPatternSelectFunction(name: String, code: String)&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+  * Describes a generated [&lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.flink.cep.PatternFlatSelectFunction&amp;#93;&lt;/span&gt;].&lt;br/&gt;
+  *&lt;br/&gt;
+  * @param name class name of the generated PatternFlatSelectFunction.&lt;br/&gt;
+  * @param code code of the generated PatternFlatSelectFunction.&lt;br/&gt;
+  */&lt;br/&gt;
+case class GeneratedPatternFlatSelectFunction(name: String, code: String)&lt;br/&gt;
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/functions/aggfunctions/DistinctAccumulator.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/functions/aggfunctions/DistinctAccumulator.scala&lt;br/&gt;
index 3427c9c96f7..f058b713a72 100644&lt;br/&gt;
&amp;#8212; a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/functions/aggfunctions/DistinctAccumulator.scala&lt;br/&gt;
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/functions/aggfunctions/DistinctAccumulator.scala&lt;br/&gt;
@@ -101,13 +101,21 @@ class DistinctAccumulator&lt;span class=&quot;error&quot;&gt;&amp;#91;ACC&amp;#93;&lt;/span&gt;(&lt;br/&gt;
     * @return true if no instances of the parameters remain in the map, false otherwise.&lt;br/&gt;
     */&lt;br/&gt;
   def remove(params: Row): Boolean = {&lt;br/&gt;
-    val currentCnt = distinctValueMap.get(params)&lt;br/&gt;
-    if (currentCnt == 1) {&lt;br/&gt;
-      distinctValueMap.remove(params)&lt;br/&gt;
+    if(!distinctValueMap.contains(params)){
       true
-    } else {
-      distinctValueMap.put(params, currentCnt - 1L)
-      false
+    }else{&lt;br/&gt;
+      val currentCnt = distinctValueMap.get(params)&lt;br/&gt;
+      if (currentCnt == null || currentCnt == 1) {
+        distinctValueMap.remove(params)
+        true
+      } else {&lt;br/&gt;
+        var value = currentCnt - 1L&lt;br/&gt;
+        if(value &amp;lt; 0){
+          value = 1
+        }&lt;br/&gt;
+        distinctValueMap.put(params, value)&lt;br/&gt;
+        false&lt;br/&gt;
+      }&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamMatch.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamMatch.scala&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..9ddff6a2a5e&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamMatch.scala&lt;br/&gt;
@@ -0,0 +1,344 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
+ * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
+ * distributed with this work for additional information&lt;br/&gt;
+ * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
+ * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
+ * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
+ * with the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package org.apache.flink.table.plan.nodes.datastream&lt;br/&gt;
+&lt;br/&gt;
+import java.math.{BigDecimal =&amp;gt; JBigDecimal}
&lt;p&gt;+import java.util&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.calcite.plan.&lt;/p&gt;
{RelOptCluster, RelTraitSet}
&lt;p&gt;+import org.apache.calcite.rel.&lt;/p&gt;
{RelCollation, RelNode, RelWriter, SingleRel}
&lt;p&gt;+import org.apache.calcite.rel.`type`.RelDataType&lt;br/&gt;
+import org.apache.calcite.rex.&lt;/p&gt;
{RexCall, RexLiteral, RexNode}
&lt;p&gt;+import org.apache.calcite.sql.`type`.SqlTypeName.&lt;/p&gt;
{INTERVAL_MONTH, INTERVAL_YEAR, INTERVAL_YEAR_MONTH}
&lt;p&gt;+import org.apache.calcite.sql.fun.SqlStdOperatorTable._&lt;br/&gt;
+import org.apache.flink.api.common.functions.MapFunction&lt;br/&gt;
+import org.apache.flink.cep.&lt;/p&gt;
{CEP, PatternStream}
&lt;p&gt;+import org.apache.flink.cep.pattern.Pattern&lt;br/&gt;
+import org.apache.flink.streaming.api.datastream.DataStream&lt;br/&gt;
+import org.apache.flink.streaming.api.windowing.time.Time&lt;br/&gt;
+import org.apache.flink.table.api.&lt;/p&gt;
{StreamQueryConfig, StreamTableEnvironment, TableException}
&lt;p&gt;+import org.apache.flink.table.calcite.FlinkTypeFactory&lt;br/&gt;
+import org.apache.flink.table.plan.schema.RowSchema&lt;br/&gt;
+import org.apache.flink.table.runtime.RowtimeProcessFunction&lt;br/&gt;
+import org.apache.flink.table.runtime.`match`.MatchUtil&lt;br/&gt;
+import org.apache.flink.table.runtime.types.&lt;/p&gt;
{CRow, CRowTypeInfo}
&lt;p&gt;+import org.apache.flink.types.Row&lt;br/&gt;
+&lt;br/&gt;
+import scala.collection.JavaConverters._&lt;br/&gt;
+import scala.collection.mutable.ListBuffer&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+  * Flink RelNode which matches along with LogicalMatch.&lt;br/&gt;
+  */&lt;br/&gt;
+class DataStreamMatch(&lt;br/&gt;
+    cluster: RelOptCluster,&lt;br/&gt;
+    traitSet: RelTraitSet,&lt;br/&gt;
+    input: RelNode,&lt;br/&gt;
+    pattern: RexNode,&lt;br/&gt;
+    strictStart: Boolean,&lt;br/&gt;
+    strictEnd: Boolean,&lt;br/&gt;
+    patternDefinitions: util.Map&lt;span class=&quot;error&quot;&gt;&amp;#91;String, RexNode&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+    measures: util.Map&lt;span class=&quot;error&quot;&gt;&amp;#91;String, RexNode&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+    after: RexNode,&lt;br/&gt;
+    subsets: util.Map[String, util.SortedSet&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;],&lt;br/&gt;
+    allRows: Boolean,&lt;br/&gt;
+    partitionKeys: util.List&lt;span class=&quot;error&quot;&gt;&amp;#91;RexNode&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+    orderKeys: RelCollation,&lt;br/&gt;
+    interval: RexNode,&lt;br/&gt;
+    schema: RowSchema,&lt;br/&gt;
+    inputSchema: RowSchema)&lt;br/&gt;
+  extends SingleRel(cluster, traitSet, input)&lt;br/&gt;
+  with DataStreamRel {&lt;br/&gt;
+&lt;br/&gt;
+  override def deriveRowType(): RelDataType = schema.relDataType&lt;br/&gt;
+&lt;br/&gt;
+  override def copy(traitSet: RelTraitSet, inputs: util.List&lt;span class=&quot;error&quot;&gt;&amp;#91;RelNode&amp;#93;&lt;/span&gt;): RelNode = &lt;/p&gt;
{
+    new DataStreamMatch(
+      cluster,
+      traitSet,
+      inputs.get(0),
+      pattern,
+      strictStart,
+      strictEnd,
+      patternDefinitions,
+      measures,
+      after,
+      subsets,
+      allRows,
+      partitionKeys,
+      orderKeys,
+      interval,
+      schema,
+      inputSchema)
+  }
&lt;p&gt;+&lt;br/&gt;
+  override def toString: String = {&lt;br/&gt;
+    s&quot;Match(${&lt;br/&gt;
+      if (!partitionKeys.isEmpty) {&lt;br/&gt;
+        s&quot;PARTITION BY: ${partitionKeys.toArray.map(_.toString).mkString(&quot;, &quot;)}, &quot;&lt;br/&gt;
+      } else &lt;/p&gt;
{
+        &quot;&quot;
+      }&lt;br/&gt;
+    }${&lt;br/&gt;
+      if (!orderKeys.getFieldCollations.isEmpty) {&lt;br/&gt;
+        s&quot;ORDER BY: ${orderKeys.getFieldCollations.asScala.map {
+          x =&amp;gt; inputSchema.relDataType.getFieldList.get(x.getFieldIndex).getName
+        }.mkString(&quot;, &quot;)}, &quot;&lt;br/&gt;
+      } else {+        &quot;&quot;+      }
&lt;p&gt;+    }${&lt;br/&gt;
+      if (!measures.isEmpty) {&lt;br/&gt;
+        s&quot;MEASURES: ${measures.asScala.map {&lt;br/&gt;
+          case (k, v) =&amp;gt; s&quot;${v.toString} AS $k&quot;&lt;br/&gt;
+        }.mkString(&quot;, &quot;)}, &quot;&lt;br/&gt;
+      } else &lt;/p&gt;
{
+        &quot;&quot;
+      }&lt;br/&gt;
+    }${&lt;br/&gt;
+      if (allRows) {
+        s&quot;ALL ROWS PER MATCH, &quot;
+      } else {
+        s&quot;ONE ROW PER MATCH, &quot;
+      }&lt;br/&gt;
+    }${&lt;br/&gt;
+      s&quot;${after.toString}, &quot;&lt;br/&gt;
+    }${&lt;br/&gt;
+      s&quot;PATTERN: (${pattern.toString})&quot;&lt;br/&gt;
+    }${&lt;br/&gt;
+      if (interval != null) {
+        s&quot;WITHIN INTERVAL: $interval, &quot;
+      } else {
+        s&quot;, &quot;
+      }&lt;br/&gt;
+    }${&lt;br/&gt;
+      if (!subsets.isEmpty) {&lt;br/&gt;
+        s&quot;SUBSET: ${subsets.asScala.map {&lt;br/&gt;
+          case (k, v) =&amp;gt; s&quot;$k = (${v.toArray.mkString(&quot;, &quot;)})&quot;&lt;br/&gt;
+        }.mkString(&quot;, &quot;)}, &quot;&lt;br/&gt;
+      } else {+        &quot;&quot;+      }
&lt;p&gt;+    }${&lt;br/&gt;
+      s&quot;DEFINE: ${patternDefinitions.asScala.map {&lt;br/&gt;
+        case (k, v) =&amp;gt; s&quot;$k AS ${v.toString}&quot;&lt;br/&gt;
+      }.mkString(&quot;, &quot;)}&quot;&lt;br/&gt;
+    })&quot;&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  override def explainTerms(pw: RelWriter): RelWriter = {&lt;br/&gt;
+    pw.input(&quot;input&quot;, getInput())&lt;br/&gt;
+      .itemIf(&quot;partitionBy&quot;,&lt;br/&gt;
+        partitionKeys.toArray.map(_.toString).mkString(&quot;, &quot;),&lt;br/&gt;
+        !partitionKeys.isEmpty)&lt;br/&gt;
+      .itemIf(&quot;orderBy&quot;,&lt;br/&gt;
+        orderKeys.getFieldCollations.asScala.map &lt;/p&gt;
{
+          x =&amp;gt; inputSchema.relDataType.getFieldList.get(x.getFieldIndex).getName
+        }
&lt;p&gt;.mkString(&quot;, &quot;),&lt;br/&gt;
+        !orderKeys.getFieldCollations.isEmpty)&lt;br/&gt;
+      .itemIf(&quot;measures&quot;,&lt;br/&gt;
+        measures.asScala.map { case (k, v) =&amp;gt; s&quot;${v.toString} AS $k&quot;}.mkString(&quot;, &quot;),&lt;br/&gt;
+        !measures.isEmpty)&lt;br/&gt;
+      .item(&quot;allrows&quot;, allRows)&lt;br/&gt;
+      .item(&quot;after&quot;, after.toString)&lt;br/&gt;
+      .item(&quot;pattern&quot;, pattern.toString)&lt;br/&gt;
+      .itemIf(&quot;within interval&quot;,&lt;br/&gt;
+        if (interval != null) &lt;/p&gt;
{
+          interval.toString
+        }
&lt;p&gt; else &lt;/p&gt;
{
+          null
+        }
&lt;p&gt;,&lt;br/&gt;
+        interval != null)&lt;br/&gt;
+      .itemIf(&quot;subset&quot;,&lt;br/&gt;
+        subsets.asScala.map { case (k, v) =&amp;gt; s&quot;$k = (${v.toArray.mkString(&quot;, &quot;)})&quot;}.mkString(&quot;, &quot;),&lt;br/&gt;
+        !subsets.isEmpty)&lt;br/&gt;
+      .item(&quot;define&quot;,&lt;br/&gt;
+        patternDefinitions.asScala.map { case (k, v) =&amp;gt; s&quot;$k AS ${v.toString}&quot;}.mkString(&quot;, &quot;))&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  override def translateToPlan(&lt;br/&gt;
+                                tableEnv: StreamTableEnvironment,&lt;br/&gt;
+                                queryConfig: StreamQueryConfig): DataStream&lt;span class=&quot;error&quot;&gt;&amp;#91;CRow&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+&lt;br/&gt;
+    val config = tableEnv.config&lt;br/&gt;
+    val inputTypeInfo = inputSchema.typeInfo&lt;br/&gt;
+&lt;br/&gt;
+    val crowInput: DataStream&lt;span class=&quot;error&quot;&gt;&amp;#91;CRow&amp;#93;&lt;/span&gt; = getInput&lt;br/&gt;
+      .asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;DataStreamRel&amp;#93;&lt;/span&gt;&lt;br/&gt;
+      .translateToPlan(tableEnv, queryConfig)&lt;br/&gt;
+&lt;br/&gt;
+    val rowtimeFields = inputSchema.relDataType&lt;br/&gt;
+      .getFieldList.asScala&lt;br/&gt;
+      .filter(f =&amp;gt; FlinkTypeFactory.isRowtimeIndicatorType(f.getType))&lt;br/&gt;
+&lt;br/&gt;
+    val timestampedInput = if (rowtimeFields.nonEmpty) {&lt;br/&gt;
+      // copy the rowtime field into the StreamRecord timestamp field&lt;br/&gt;
+      val timeIdx = rowtimeFields.head.getIndex&lt;br/&gt;
+&lt;br/&gt;
+      crowInput&lt;br/&gt;
+        .process(new RowtimeProcessFunction(timeIdx, CRowTypeInfo(inputTypeInfo)))&lt;br/&gt;
+        .setParallelism(crowInput.getParallelism)&lt;br/&gt;
+        .name(s&quot;rowtime field: (${rowtimeFields.head})&quot;)&lt;br/&gt;
+    } else &lt;/p&gt;
{
+      crowInput
+    }
&lt;p&gt;+&lt;br/&gt;
+    //transform DataStream&lt;br/&gt;
+    val inputDS: DataStream&lt;span class=&quot;error&quot;&gt;&amp;#91;Row&amp;#93;&lt;/span&gt; = timestampedInput&lt;br/&gt;
+      .map(new ConvertToRow)&lt;br/&gt;
+      .setParallelism(timestampedInput.getParallelism)&lt;br/&gt;
+      .name(&quot;ConvertToRow&quot;)&lt;br/&gt;
+      .returns(inputTypeInfo)&lt;br/&gt;
+&lt;br/&gt;
+    def translatePattern(&lt;br/&gt;
+                          rexNode: RexNode,&lt;br/&gt;
+                          currentPattern: Pattern&lt;span class=&quot;error&quot;&gt;&amp;#91;Row, Row&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+                          patternNames: ListBuffer&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;): Pattern&lt;span class=&quot;error&quot;&gt;&amp;#91;Row, Row&amp;#93;&lt;/span&gt; = rexNode match {&lt;br/&gt;
+      //Conditions&lt;br/&gt;
+      case literal: RexLiteral =&amp;gt;&lt;br/&gt;
+        val patternName = literal.getValue3.toString&lt;br/&gt;
+        patternNames += patternName&lt;br/&gt;
+        val newPattern = next(currentPattern, patternName)&lt;br/&gt;
+&lt;br/&gt;
+        val patternDefinition = patternDefinitions.get(patternName)&lt;br/&gt;
+        if (patternDefinition != null) &lt;/p&gt;
{
+          //condition generate
+          val condition = MatchUtil.generateIterativeCondition(
+            config,
+            inputSchema,
+            patternName,
+            patternNames,
+            patternDefinition,
+            inputTypeInfo)
+
+          newPattern.where(condition)
+        }
&lt;p&gt; else &lt;/p&gt;
{
+          newPattern
+        }
&lt;p&gt;+&lt;br/&gt;
+      case call: RexCall =&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+        call.getOperator match {&lt;br/&gt;
+          case PATTERN_CONCAT =&amp;gt;&lt;br/&gt;
+            val left = call.operands.get(0)&lt;br/&gt;
+            val right = call.operands.get(1)&lt;br/&gt;
+            translatePattern(right,&lt;br/&gt;
+              translatePattern(left, currentPattern, patternNames),&lt;br/&gt;
+              patternNames)&lt;br/&gt;
+          // Quantifiers&lt;br/&gt;
+          case PATTERN_QUANTIFIER =&amp;gt;&lt;br/&gt;
+            val name = call.operands.get(0).asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;RexLiteral&amp;#93;&lt;/span&gt;&lt;br/&gt;
+            val newPattern = translatePattern(name, currentPattern, patternNames)&lt;br/&gt;
+&lt;br/&gt;
+            val startNum = call.operands.get(1).asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;RexLiteral&amp;#93;&lt;/span&gt;&lt;br/&gt;
+              .getValue3.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;JBigDecimal&amp;#93;&lt;/span&gt;.intValue()&lt;br/&gt;
+            val endNum = call.operands.get(2).asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;RexLiteral&amp;#93;&lt;/span&gt;&lt;br/&gt;
+              .getValue3.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;JBigDecimal&amp;#93;&lt;/span&gt;.intValue()&lt;br/&gt;
+            // zero or more&lt;br/&gt;
+            if (startNum == 0 &amp;amp;&amp;amp; endNum == -1) &lt;/p&gt;
{
+              newPattern.oneOrMore().optional().consecutive()
+            // one or more
+            }
&lt;p&gt; else if (startNum == 1 &amp;amp;&amp;amp; endNum == -1) &lt;/p&gt;
{
+              newPattern.oneOrMore().consecutive()
+            // optional
+            }
&lt;p&gt; else if (startNum == 0 &amp;amp;&amp;amp; endNum == 1) &lt;/p&gt;
{
+              newPattern.optional()
+            // times
+            }
&lt;p&gt; else if (endNum != -1) &lt;/p&gt;
{
+              newPattern.times(startNum, endNum).consecutive()
+            // times or more
+            }
&lt;p&gt; else &lt;/p&gt;
{
+              newPattern.timesOrMore(startNum).consecutive()
+            }
&lt;p&gt;+&lt;br/&gt;
+          case PATTERN_ALTER =&amp;gt;&lt;br/&gt;
+            throw TableException(&quot;Currently, CEP doesn&apos;t support branching patterns.&quot;)&lt;br/&gt;
+&lt;br/&gt;
+          case PATTERN_PERMUTE =&amp;gt;&lt;br/&gt;
+            throw TableException(&quot;Currently, CEP doesn&apos;t support PERMUTE patterns.&quot;)&lt;br/&gt;
+&lt;br/&gt;
+          case PATTERN_EXCLUDE =&amp;gt;&lt;br/&gt;
+            throw TableException(&quot;Currently, CEP doesn&apos;t support &apos;{&lt;del&gt;&apos; &apos;&lt;/del&gt;}&apos; patterns.&quot;)&lt;br/&gt;
+        }&lt;br/&gt;
+&lt;br/&gt;
+      case _ =&amp;gt;&lt;br/&gt;
+        throw TableException(&quot;&quot;)&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    val patternNames: ListBuffer&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt; = ListBuffer()&lt;br/&gt;
+    val cepPattern = translatePattern(pattern, null, patternNames)&lt;br/&gt;
+    if (interval != null) {&lt;br/&gt;
+      val intervalLiteral = interval.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;RexLiteral&amp;#93;&lt;/span&gt;&lt;br/&gt;
+      val intervalValue = interval.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;RexLiteral&amp;#93;&lt;/span&gt;.getValueAs(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;java.lang.Long&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+      val intervalMs: Long = intervalLiteral.getTypeName match &lt;/p&gt;
{
+        case INTERVAL_YEAR | INTERVAL_YEAR_MONTH | INTERVAL_MONTH =&amp;gt;
+          // convert from months to milliseconds, suppose 1 month = 30 days
+          intervalValue * 30L * 24 * 3600 * 1000
+        case _ =&amp;gt; intervalValue
+      }
&lt;p&gt;+&lt;br/&gt;
+      cepPattern.within(Time.milliseconds(intervalMs))&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    // use CEP.Pattern, generate PatternStream&lt;br/&gt;
+    val patternStream: PatternStream&lt;span class=&quot;error&quot;&gt;&amp;#91;Row&amp;#93;&lt;/span&gt; = CEP.pattern&lt;span class=&quot;error&quot;&gt;&amp;#91;Row&amp;#93;&lt;/span&gt;(inputDS, cepPattern)&lt;br/&gt;
+&lt;br/&gt;
+    val outTypeInfo = CRowTypeInfo(schema.typeInfo)&lt;br/&gt;
+    if (allRows) &lt;/p&gt;
{
+      val patternFlatSelectFunction =
+        MatchUtil.generatePatternFlatSelectFunction(
+          config,
+          schema,
+          patternNames,
+          partitionKeys,
+          orderKeys,
+          measures,
+          inputTypeInfo)
+      patternStream.flatSelect[CRow](patternFlatSelectFunction, outTypeInfo)
+    }
&lt;p&gt; else &lt;/p&gt;
{
+      val patternSelectFunction =
+        MatchUtil.generatePatternSelectFunction(
+          config,
+          schema,
+          patternNames,
+          partitionKeys,
+          measures,
+          inputTypeInfo)
+      patternStream.select[CRow](patternSelectFunction, outTypeInfo)
+    }
&lt;p&gt;+  }&lt;br/&gt;
+&lt;br/&gt;
+  // pattern start or add&lt;br/&gt;
+  private def next(currentPattern: Pattern&lt;span class=&quot;error&quot;&gt;&amp;#91;Row, Row&amp;#93;&lt;/span&gt;, patternName: String): Pattern&lt;span class=&quot;error&quot;&gt;&amp;#91;Row, Row&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+    if (currentPattern == null) &lt;/p&gt;
{
+      Pattern.begin(patternName)
+    }
&lt;p&gt; else &lt;/p&gt;
{
+      currentPattern.next(patternName)
+    }
&lt;p&gt;+  }&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+  * MapFunction convert CRow to Row.&lt;br/&gt;
+  */&lt;br/&gt;
+class ConvertToRow extends MapFunction&lt;span class=&quot;error&quot;&gt;&amp;#91;CRow, Row&amp;#93;&lt;/span&gt; {&lt;br/&gt;
+  override def map(value: CRow): Row = &lt;/p&gt;
{
+    value.row
+  }
&lt;p&gt;+}&lt;br/&gt;
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/logical/FlinkLogicalMatch.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/logical/FlinkLogicalMatch.scala&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..09970657b54&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/logical/FlinkLogicalMatch.scala&lt;br/&gt;
@@ -0,0 +1,132 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
+ * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
+ * distributed with this work for additional information&lt;br/&gt;
+ * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
+ * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
+ * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
+ * with the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package org.apache.flink.table.plan.nodes.logical&lt;br/&gt;
+&lt;br/&gt;
+import java.util&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.calcite.plan._&lt;br/&gt;
+import org.apache.calcite.rel.`type`.RelDataType&lt;br/&gt;
+import org.apache.calcite.rel.convert.ConverterRule&lt;br/&gt;
+import org.apache.calcite.rel.core.Match&lt;br/&gt;
+import org.apache.calcite.rel.logical.LogicalMatch&lt;br/&gt;
+import org.apache.calcite.rel.&lt;/p&gt;
{RelCollation, RelNode}
&lt;p&gt;+import org.apache.calcite.rex.RexNode&lt;br/&gt;
+import org.apache.flink.table.plan.nodes.FlinkConventions&lt;br/&gt;
+&lt;br/&gt;
+&lt;br/&gt;
+class FlinkLogicalMatch(&lt;br/&gt;
+    cluster: RelOptCluster,&lt;br/&gt;
+    traitSet: RelTraitSet,&lt;br/&gt;
+    input: RelNode,&lt;br/&gt;
+    rowType: RelDataType,&lt;br/&gt;
+    pattern: RexNode,&lt;br/&gt;
+    strictStart: Boolean,&lt;br/&gt;
+    strictEnd: Boolean,&lt;br/&gt;
+    patternDefinitions: util.Map&lt;span class=&quot;error&quot;&gt;&amp;#91;String, RexNode&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+    measures: util.Map&lt;span class=&quot;error&quot;&gt;&amp;#91;String, RexNode&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+    after: RexNode,&lt;br/&gt;
+    subsets: util.Map[String, _ &amp;lt;: util.SortedSet&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;],&lt;br/&gt;
+    allRows: Boolean,&lt;br/&gt;
+    partitionKeys: util.List&lt;span class=&quot;error&quot;&gt;&amp;#91;RexNode&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+    orderKeys: RelCollation,&lt;br/&gt;
+    interval: RexNode)&lt;br/&gt;
+  extends Match(&lt;br/&gt;
+    cluster,&lt;br/&gt;
+    traitSet,&lt;br/&gt;
+    input,&lt;br/&gt;
+    rowType,&lt;br/&gt;
+    pattern,&lt;br/&gt;
+    strictStart,&lt;br/&gt;
+    strictEnd,&lt;br/&gt;
+    patternDefinitions,&lt;br/&gt;
+    measures,&lt;br/&gt;
+    after,&lt;br/&gt;
+    subsets,&lt;br/&gt;
+    allRows,&lt;br/&gt;
+    partitionKeys,&lt;br/&gt;
+    orderKeys,&lt;br/&gt;
+    interval)&lt;br/&gt;
+  with FlinkLogicalRel {&lt;br/&gt;
+    override def copy(&lt;br/&gt;
+      input: RelNode,&lt;br/&gt;
+      rowType: RelDataType,&lt;br/&gt;
+      pattern: RexNode,&lt;br/&gt;
+      strictStart: Boolean,&lt;br/&gt;
+      strictEnd: Boolean,&lt;br/&gt;
+      patternDefinitions: util.Map&lt;span class=&quot;error&quot;&gt;&amp;#91;String, RexNode&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+      measures: util.Map&lt;span class=&quot;error&quot;&gt;&amp;#91;String, RexNode&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+      after: RexNode,&lt;br/&gt;
+      subsets: util.Map[String, _ &amp;lt;: util.SortedSet&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;],&lt;br/&gt;
+      allRows: Boolean,&lt;br/&gt;
+      partitionKeys: util.List&lt;span class=&quot;error&quot;&gt;&amp;#91;RexNode&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+      orderKeys: RelCollation,&lt;br/&gt;
+      interval: RexNode): Match = &lt;/p&gt;
{
+    new FlinkLogicalMatch(
+      cluster,
+      traitSet,
+      input,
+      rowType,
+      pattern,
+      strictStart,
+      strictEnd,
+      patternDefinitions,
+      measures,
+      after,
+      subsets,
+      allRows,
+      partitionKeys,
+      orderKeys,
+      interval)
+  }
&lt;p&gt;+}&lt;br/&gt;
+&lt;br/&gt;
+private class FlinkLogicalMatchConverter&lt;br/&gt;
+  extends ConverterRule(&lt;br/&gt;
+    classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LogicalMatch&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+    Convention.NONE,&lt;br/&gt;
+    FlinkConventions.LOGICAL,&lt;br/&gt;
+    &quot;FlinkLogicalMatchConverter&quot;) {&lt;br/&gt;
+&lt;br/&gt;
+  override def convert(rel: RelNode): RelNode = &lt;/p&gt;
{
+    val logicalMatch = rel.asInstanceOf[LogicalMatch]
+    val traitSet = rel.getTraitSet.replace(FlinkConventions.LOGICAL)
+    val newInput = RelOptRule.convert(logicalMatch.getInput, FlinkConventions.LOGICAL)
+
+    new FlinkLogicalMatch(
+      rel.getCluster,
+      traitSet,
+      newInput,
+      logicalMatch.getRowType,
+      logicalMatch.getPattern,
+      logicalMatch.isStrictStart,
+      logicalMatch.isStrictEnd,
+      logicalMatch.getPatternDefinitions,
+      logicalMatch.getMeasures,
+      logicalMatch.getAfter,
+      logicalMatch.getSubsets,
+      logicalMatch.isAllRows,
+      logicalMatch.getPartitionKeys,
+      logicalMatch.getOrderKeys,
+      logicalMatch.getInterval)
+  }
&lt;p&gt;+}&lt;br/&gt;
+&lt;br/&gt;
+object FlinkLogicalMatch &lt;/p&gt;
{
+  val CONVERTER: ConverterRule = new FlinkLogicalMatchConverter()
+}
&lt;p&gt;diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/rules/FlinkRuleSets.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/rules/FlinkRuleSets.scala&lt;br/&gt;
index 52dab8b3379..b56a2b2fdfd 100644&lt;br/&gt;
&amp;#8212; a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/rules/FlinkRuleSets.scala&lt;br/&gt;
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/rules/FlinkRuleSets.scala&lt;br/&gt;
@@ -25,7 +25,7 @@ import org.apache.flink.table.plan.rules.common._&lt;br/&gt;
 import org.apache.flink.table.plan.rules.logical._&lt;br/&gt;
 import org.apache.flink.table.plan.rules.dataSet._&lt;br/&gt;
 import org.apache.flink.table.plan.rules.datastream._&lt;br/&gt;
-import org.apache.flink.table.plan.nodes.logical._&lt;br/&gt;
+import org.apache.flink.table.plan.nodes.logical.&lt;/p&gt;
{FlinkLogicalMatch, _}

&lt;p&gt; object FlinkRuleSets {&lt;/p&gt;

&lt;p&gt;@@ -133,7 +133,10 @@ object FlinkRuleSets {&lt;br/&gt;
     FlinkLogicalValues.CONVERTER,&lt;br/&gt;
     FlinkLogicalTableSourceScan.CONVERTER,&lt;br/&gt;
     FlinkLogicalTableFunctionScan.CONVERTER,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;FlinkLogicalNativeTableScan.CONVERTER&lt;br/&gt;
+    FlinkLogicalNativeTableScan.CONVERTER,&lt;br/&gt;
+&lt;br/&gt;
+    // MATCH_RECOGNIZE logical&lt;br/&gt;
+    FlinkLogicalMatch.CONVERTER&lt;br/&gt;
   )&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   /**&lt;br/&gt;
@@ -211,7 +214,10 @@ object FlinkRuleSets {&lt;br/&gt;
     DataStreamCorrelateRule.INSTANCE,&lt;br/&gt;
     DataStreamWindowJoinRule.INSTANCE,&lt;br/&gt;
     DataStreamJoinRule.INSTANCE,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;StreamTableSourceScanRule.INSTANCE&lt;br/&gt;
+    StreamTableSourceScanRule.INSTANCE,&lt;br/&gt;
+&lt;br/&gt;
+    // MATCH_RECOGNIZE DataStream&lt;br/&gt;
+    DataStreamMatchRule.INSTANCE&lt;br/&gt;
   )&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   /**&lt;br/&gt;
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/rules/datastream/DataStreamMatchRule.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/rules/datastream/DataStreamMatchRule.scala&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..f3c57e5f0d3&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/rules/datastream/DataStreamMatchRule.scala&lt;br/&gt;
@@ -0,0 +1,64 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
+ * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
+ * distributed with this work for additional information&lt;br/&gt;
+ * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
+ * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
+ * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
+ * with the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package org.apache.flink.table.plan.rules.datastream&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.calcite.plan.&lt;/p&gt;
{RelOptRule, RelTraitSet}
&lt;p&gt;+import org.apache.calcite.rel.RelNode&lt;br/&gt;
+import org.apache.calcite.rel.convert.ConverterRule&lt;br/&gt;
+import org.apache.flink.table.plan.nodes.FlinkConventions&lt;br/&gt;
+import org.apache.flink.table.plan.nodes.datastream.DataStreamMatch&lt;br/&gt;
+import org.apache.flink.table.plan.nodes.logical.FlinkLogicalMatch&lt;br/&gt;
+import org.apache.flink.table.plan.schema.RowSchema&lt;br/&gt;
+&lt;br/&gt;
+class DataStreamMatchRule&lt;br/&gt;
+  extends ConverterRule(&lt;br/&gt;
+    classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;FlinkLogicalMatch&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+    FlinkConventions.LOGICAL,&lt;br/&gt;
+    FlinkConventions.DATASTREAM,&lt;br/&gt;
+    &quot;DataStreamMatchRule&quot;) {&lt;br/&gt;
+&lt;br/&gt;
+  override def convert(rel: RelNode): RelNode = &lt;/p&gt;
{
+    val logicalMatch: FlinkLogicalMatch = rel.asInstanceOf[FlinkLogicalMatch]
+    val traitSet: RelTraitSet = rel.getTraitSet.replace(FlinkConventions.DATASTREAM)
+    val convertInput: RelNode =
+      RelOptRule.convert(logicalMatch.getInput, FlinkConventions.DATASTREAM)
+
+    new DataStreamMatch(
+      rel.getCluster,
+      traitSet,
+      convertInput,
+      logicalMatch.getPattern,
+      logicalMatch.isStrictStart,
+      logicalMatch.isStrictEnd,
+      logicalMatch.getPatternDefinitions,
+      logicalMatch.getMeasures,
+      logicalMatch.getAfter,
+      logicalMatch.getSubsets,
+      logicalMatch.isAllRows,
+      logicalMatch.getPartitionKeys,
+      logicalMatch.getOrderKeys,
+      logicalMatch.getInterval,
+      new RowSchema(logicalMatch.getRowType),
+      new RowSchema(logicalMatch.getInput.getRowType))
+  }
&lt;p&gt;+}&lt;br/&gt;
+&lt;br/&gt;
+object DataStreamMatchRule &lt;/p&gt;
{
+  val INSTANCE: RelOptRule = new DataStreamMatchRule
+}
&lt;p&gt;diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/match/IterativeConditionRunner.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/match/IterativeConditionRunner.scala&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..80c3dd7e245&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/match/IterativeConditionRunner.scala&lt;br/&gt;
@@ -0,0 +1,58 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
+ * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
+ * distributed with this work for additional information&lt;br/&gt;
+ * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
+ * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
+ * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
+ * with the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package org.apache.flink.table.runtime.`match`&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.flink.cep.pattern.conditions.IterativeCondition&lt;br/&gt;
+import org.apache.flink.table.codegen.Compiler&lt;br/&gt;
+import org.apache.flink.types.Row&lt;br/&gt;
+import org.slf4j.LoggerFactory&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+  * IterativeConditionRunner with [&lt;span class=&quot;error&quot;&gt;&amp;#91;Row&amp;#93;&lt;/span&gt;] value.&lt;br/&gt;
+  */&lt;br/&gt;
+class IterativeConditionRunner(&lt;br/&gt;
+  name: String,&lt;br/&gt;
+  code: String)&lt;br/&gt;
+  extends IterativeCondition&lt;span class=&quot;error&quot;&gt;&amp;#91;Row&amp;#93;&lt;/span&gt;&lt;br/&gt;
+  with Compiler[IterativeCondition&lt;span class=&quot;error&quot;&gt;&amp;#91;Row&amp;#93;&lt;/span&gt;]{&lt;br/&gt;
+&lt;br/&gt;
+  val LOG = LoggerFactory.getLogger(this.getClass)&lt;br/&gt;
+&lt;br/&gt;
+  // IterativeCondition will be serialized as part of state,&lt;br/&gt;
+  // so make function as transient to avoid ClassNotFoundException when restore state,&lt;br/&gt;
+  // see &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-6939&quot; title=&quot;Not store IterativeCondition with NFA state&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-6939&quot;&gt;&lt;del&gt;FLINK-6939&lt;/del&gt;&lt;/a&gt; for details&lt;br/&gt;
+  @transient private var function: IterativeCondition&lt;span class=&quot;error&quot;&gt;&amp;#91;Row&amp;#93;&lt;/span&gt; = _&lt;br/&gt;
+&lt;br/&gt;
+  def init(): Unit = &lt;/p&gt;
{
+    LOG.debug(s&quot;Compiling IterativeCondition: $name \n\n Code:\n$code&quot;)
+    // We cannot get user&apos;s classloader currently, see FLINK-6938 for details
+    val clazz = compile(Thread.currentThread().getContextClassLoader, name, code)
+    LOG.debug(&quot;Instantiating IterativeCondition.&quot;)
+    function = clazz.newInstance()
+  }
&lt;p&gt;+&lt;br/&gt;
+  override def filter(value: Row, ctx: IterativeCondition.Context&lt;span class=&quot;error&quot;&gt;&amp;#91;Row&amp;#93;&lt;/span&gt;): Boolean = {&lt;br/&gt;
+&lt;br/&gt;
+    if (function == null) &lt;/p&gt;
{
+      init()
+    }&lt;br/&gt;
+&lt;br/&gt;
+    function.filter(value, ctx)&lt;br/&gt;
+  }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/match/MatchUtil.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/match/MatchUtil.scala&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..1f0f2bc9a4f&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/match/MatchUtil.scala&lt;br/&gt;
@@ -0,0 +1,117 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
+ * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
+ * distributed with this work for additional information&lt;br/&gt;
+ * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
+ * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
+ * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
+ * with the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package org.apache.flink.table.runtime.`match`&lt;br/&gt;
+&lt;br/&gt;
+import java.util&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.calcite.rel.RelCollation&lt;br/&gt;
+import org.apache.calcite.rex.RexNode&lt;br/&gt;
+import org.apache.flink.api.common.typeinfo.TypeInformation&lt;br/&gt;
+import org.apache.flink.cep.{PatternFlatSelectFunction, PatternSelectFunction}&lt;br/&gt;
+import org.apache.flink.cep.pattern.conditions.IterativeCondition&lt;br/&gt;
+import org.apache.flink.table.api.TableConfig&lt;br/&gt;
+import org.apache.flink.table.codegen.MatchCodeGenerator&lt;br/&gt;
+import org.apache.flink.table.plan.schema.RowSchema&lt;br/&gt;
+import org.apache.flink.table.runtime.types.CRow&lt;br/&gt;
+import org.apache.flink.types.Row&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+  * An util class to generate match functions.&lt;br/&gt;
+  * 1&#12290;IterativeCondition&lt;br/&gt;
+  * 2&#12290;PatternSelectFunction&lt;br/&gt;
+  * 3&#12290;PatternFlatSelectFunction&lt;br/&gt;
+  */&lt;br/&gt;
+object MatchUtil {&lt;br/&gt;
+&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;flink&amp;#93;&lt;/span&gt; def generateIterativeCondition(&lt;br/&gt;
+    config: TableConfig,&lt;br/&gt;
+    inputType: RowSchema,&lt;br/&gt;
+    patternName: String,&lt;br/&gt;
+    patternNames: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+    patternDefinition: RexNode,&lt;br/&gt;
+    inputTypeInfo: TypeInformation&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;): IterativeCondition&lt;span class=&quot;error&quot;&gt;&amp;#91;Row&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+&lt;br/&gt;
+    val generator = new MatchCodeGenerator(&lt;br/&gt;
+      config, false, inputTypeInfo, patternNames, true, Some(patternName))&lt;br/&gt;
+    val condition = generator.generateExpression(patternDefinition)&lt;br/&gt;
+    val body =&lt;br/&gt;
+      s&quot;&quot;&quot;&lt;br/&gt;
+         |${condition.code}&lt;br/&gt;
+         |return ${condition.resultTerm};&lt;br/&gt;
+         |&quot;&quot;&quot;.stripMargin&lt;br/&gt;
+&lt;br/&gt;
+    val genCondition = generator.generateIterativeCondition(&quot;MatchRecognizeCondition&quot;, body)&lt;br/&gt;
+    new IterativeConditionRunner(genCondition.name, genCondition.code)&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;flink&amp;#93;&lt;/span&gt; def generatePatternSelectFunction(&lt;br/&gt;
+    config: TableConfig,&lt;br/&gt;
+    returnType: RowSchema,&lt;br/&gt;
+    patternNames: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+    partitionKeys: util.List&lt;span class=&quot;error&quot;&gt;&amp;#91;RexNode&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+    measures: util.Map&lt;span class=&quot;error&quot;&gt;&amp;#91;String, RexNode&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+    inputTypeInfo: TypeInformation&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;): PatternSelectFunction&lt;span class=&quot;error&quot;&gt;&amp;#91;Row, CRow&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+&lt;br/&gt;
+    val generator = new MatchCodeGenerator(config, false, inputTypeInfo, patternNames, false)&lt;br/&gt;
+&lt;br/&gt;
+    val resultExpression = generator.generateSelectOutputExpression(&lt;br/&gt;
+      partitionKeys,&lt;br/&gt;
+      measures,&lt;br/&gt;
+      returnType)&lt;br/&gt;
+    val body =&lt;br/&gt;
+      s&quot;&quot;&quot;&lt;br/&gt;
+         |${resultExpression.code}&lt;br/&gt;
+         |return ${resultExpression.resultTerm};&lt;br/&gt;
+         |&quot;&quot;&quot;.stripMargin&lt;br/&gt;
+&lt;br/&gt;
+    generator.addReusableStatements()&lt;br/&gt;
+    val genFunction = generator.generatePatternSelectFunction(&lt;br/&gt;
+      &quot;MatchRecognizePatternSelectFunction&quot;,&lt;br/&gt;
+      body)&lt;br/&gt;
+    new PatternSelectFunctionRunner(genFunction.name, genFunction.code)&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;flink&amp;#93;&lt;/span&gt; def generatePatternFlatSelectFunction(&lt;br/&gt;
+    config: TableConfig,&lt;br/&gt;
+    returnType: RowSchema,&lt;br/&gt;
+    patternNames: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+    partitionKeys: util.List&lt;span class=&quot;error&quot;&gt;&amp;#91;RexNode&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+    orderKeys: RelCollation,&lt;br/&gt;
+    measures: util.Map&lt;span class=&quot;error&quot;&gt;&amp;#91;String, RexNode&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+    inputTypeInfo: TypeInformation&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;): PatternFlatSelectFunction&lt;span class=&quot;error&quot;&gt;&amp;#91;Row, CRow&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+&lt;br/&gt;
+    val generator = new MatchCodeGenerator(config, false, inputTypeInfo, patternNames, false)&lt;br/&gt;
+&lt;br/&gt;
+    val resultExpression = generator.generateFlatSelectOutputExpression(&lt;br/&gt;
+      partitionKeys,&lt;br/&gt;
+      orderKeys,&lt;br/&gt;
+      measures,&lt;br/&gt;
+      returnType)&lt;br/&gt;
+    val body =&lt;br/&gt;
+      s&quot;&quot;&quot;&lt;br/&gt;
+         |${resultExpression.code}&lt;br/&gt;
+         |&quot;&quot;&quot;.stripMargin&lt;br/&gt;
+&lt;br/&gt;
+    generator.addReusableStatements()&lt;br/&gt;
+    val genFunction = generator.generatePatternFlatSelectFunction(&lt;br/&gt;
+      &quot;MatchRecognizePatternFlatSelectFunction&quot;,&lt;br/&gt;
+      body)&lt;br/&gt;
+    new PatternFlatSelectFunctionRunner(genFunction.name, genFunction.code)&lt;br/&gt;
+  }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/match/PatternFlatSelectFunctionRunner.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/match/PatternFlatSelectFunctionRunner.scala&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..986fff3a563&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/match/PatternFlatSelectFunctionRunner.scala&lt;br/&gt;
@@ -0,0 +1,65 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
+ * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
+ * distributed with this work for additional information&lt;br/&gt;
+ * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
+ * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
+ * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
+ * with the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package org.apache.flink.table.runtime.`match`&lt;br/&gt;
+&lt;br/&gt;
+import java.util&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.flink.cep.PatternFlatSelectFunction&lt;br/&gt;
+import org.apache.flink.table.codegen.Compiler&lt;br/&gt;
+import org.apache.flink.table.runtime.CRowWrappingCollector&lt;br/&gt;
+import org.apache.flink.table.runtime.types.CRow&lt;br/&gt;
+import org.apache.flink.types.Row&lt;br/&gt;
+import org.apache.flink.util.Collector&lt;br/&gt;
+import org.slf4j.LoggerFactory&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+  * PatternFlatSelectFunctionRunner with [&lt;span class=&quot;error&quot;&gt;&amp;#91;Row&amp;#93;&lt;/span&gt;] input and [&lt;span class=&quot;error&quot;&gt;&amp;#91;CRow&amp;#93;&lt;/span&gt;] output.&lt;br/&gt;
+  */&lt;br/&gt;
+class PatternFlatSelectFunctionRunner(&lt;br/&gt;
+  name: String,&lt;br/&gt;
+  code: String)&lt;br/&gt;
+  extends PatternFlatSelectFunction&lt;span class=&quot;error&quot;&gt;&amp;#91;Row, CRow&amp;#93;&lt;/span&gt;&lt;br/&gt;
+  with Compiler[PatternFlatSelectFunction&lt;span class=&quot;error&quot;&gt;&amp;#91;Row, Row&amp;#93;&lt;/span&gt;] {&lt;br/&gt;
+&lt;br/&gt;
+  val LOG = LoggerFactory.getLogger(this.getClass)&lt;br/&gt;
+&lt;br/&gt;
+  private var cRowWrapper: CRowWrappingCollector = _&lt;br/&gt;
+&lt;br/&gt;
+  private var function: PatternFlatSelectFunction&lt;span class=&quot;error&quot;&gt;&amp;#91;Row, Row&amp;#93;&lt;/span&gt; = _&lt;br/&gt;
+&lt;br/&gt;
+  def init(): Unit = {
+    LOG.debug(s&quot;Compiling PatternFlatSelectFunction: $name \n\n Code:\n$code&quot;)
+    val clazz = compile(Thread.currentThread().getContextClassLoader, name, code)
+    LOG.debug(&quot;Instantiating PatternFlatSelectFunction.&quot;)
+    function = clazz.newInstance()
+
+    this.cRowWrapper = new CRowWrappingCollector()
+  }&lt;br/&gt;
+&lt;br/&gt;
+  override def flatSelect(&lt;br/&gt;
+    pattern: util.Map[String, util.List&lt;span class=&quot;error&quot;&gt;&amp;#91;Row&amp;#93;&lt;/span&gt;],&lt;br/&gt;
+    out: Collector&lt;span class=&quot;error&quot;&gt;&amp;#91;CRow&amp;#93;&lt;/span&gt;): Unit = {&lt;br/&gt;
+    if (function == null) {+      init()+    }
&lt;p&gt;+&lt;br/&gt;
+    cRowWrapper.out = out&lt;br/&gt;
+    function.flatSelect(pattern, cRowWrapper)&lt;br/&gt;
+  }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/match/PatternSelectFunctionRunner.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/match/PatternSelectFunctionRunner.scala&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..de789af5acb&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/match/PatternSelectFunctionRunner.scala&lt;br/&gt;
@@ -0,0 +1,63 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
+ * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
+ * distributed with this work for additional information&lt;br/&gt;
+ * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
+ * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
+ * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
+ * with the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package org.apache.flink.table.runtime.`match`&lt;br/&gt;
+&lt;br/&gt;
+import java.util&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.flink.cep.PatternSelectFunction&lt;br/&gt;
+import org.apache.flink.table.codegen.Compiler&lt;br/&gt;
+import org.apache.flink.table.runtime.types.CRow&lt;br/&gt;
+import org.apache.flink.types.Row&lt;br/&gt;
+import org.slf4j.LoggerFactory&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+  * PatternSelectFunctionRunner with [&lt;span class=&quot;error&quot;&gt;&amp;#91;Row&amp;#93;&lt;/span&gt;] input and [&lt;span class=&quot;error&quot;&gt;&amp;#91;CRow&amp;#93;&lt;/span&gt;] output.&lt;br/&gt;
+  */&lt;br/&gt;
+class PatternSelectFunctionRunner(&lt;br/&gt;
+  name: String,&lt;br/&gt;
+  code: String)&lt;br/&gt;
+  extends PatternSelectFunction&lt;span class=&quot;error&quot;&gt;&amp;#91;Row, CRow&amp;#93;&lt;/span&gt;&lt;br/&gt;
+  with Compiler[PatternSelectFunction&lt;span class=&quot;error&quot;&gt;&amp;#91;Row, Row&amp;#93;&lt;/span&gt;] {&lt;br/&gt;
+&lt;br/&gt;
+  val LOG = LoggerFactory.getLogger(this.getClass)&lt;br/&gt;
+&lt;br/&gt;
+  private var outCRow: CRow = _&lt;br/&gt;
+&lt;br/&gt;
+  private var function: PatternSelectFunction&lt;span class=&quot;error&quot;&gt;&amp;#91;Row, Row&amp;#93;&lt;/span&gt; = _&lt;br/&gt;
+&lt;br/&gt;
+  def init(): Unit = &lt;/p&gt;
{
+    LOG.debug(s&quot;Compiling PatternSelectFunction: $name \n\n Code:\n$code&quot;)
+    val clazz = compile(Thread.currentThread().getContextClassLoader, name, code)
+    LOG.debug(&quot;Instantiating PatternSelectFunction.&quot;)
+    function = clazz.newInstance()
+  }
&lt;p&gt;+&lt;br/&gt;
+  override def select(pattern: util.Map[String, util.List&lt;span class=&quot;error&quot;&gt;&amp;#91;Row&amp;#93;&lt;/span&gt;]): CRow = {&lt;br/&gt;
+    if (outCRow == null) &lt;/p&gt;
{
+      outCRow = new CRow(null, true)
+    }
&lt;p&gt;+&lt;br/&gt;
+    if (function == null) &lt;/p&gt;
{
+      init()
+    }
&lt;p&gt;+&lt;br/&gt;
+    outCRow.row = function.select(pattern)&lt;br/&gt;
+    outCRow&lt;br/&gt;
+  }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/validate/FunctionCatalog.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/validate/FunctionCatalog.scala&lt;br/&gt;
index 3184e0001ea..b2a715aafe2 100644&lt;br/&gt;
&amp;#8212; a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/validate/FunctionCatalog.scala&lt;br/&gt;
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/validate/FunctionCatalog.scala&lt;br/&gt;
@@ -458,7 +458,17 @@ class BasicOperatorTable extends ReflectiveSqlOperatorTable {&lt;br/&gt;
     BasicOperatorTable.HOP_PROCTIME,&lt;br/&gt;
     BasicOperatorTable.HOP_ROWTIME,&lt;br/&gt;
     BasicOperatorTable.SESSION_PROCTIME,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;BasicOperatorTable.SESSION_ROWTIME&lt;br/&gt;
+    BasicOperatorTable.SESSION_ROWTIME,&lt;br/&gt;
+&lt;br/&gt;
+    // MATCH_RECOGNIZE&lt;br/&gt;
+    SqlStdOperatorTable.FIRST,&lt;br/&gt;
+    SqlStdOperatorTable.LAST,&lt;br/&gt;
+    SqlStdOperatorTable.PREV,&lt;br/&gt;
+    SqlStdOperatorTable.NEXT,&lt;br/&gt;
+    SqlStdOperatorTable.CLASSIFIER,&lt;br/&gt;
+    SqlStdOperatorTable.MATCH_NUMBER,&lt;br/&gt;
+    SqlStdOperatorTable.FINAL,&lt;br/&gt;
+    SqlStdOperatorTable.RUNNING&lt;br/&gt;
   )&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   builtInSqlOperators.foreach(register)&lt;br/&gt;
diff --git a/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/stream/sql/CepITCase.scala b/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/stream/sql/CepITCase.scala&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..1715800474c&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/stream/sql/CepITCase.scala&lt;br/&gt;
@@ -0,0 +1,410 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
+ * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
+ * distributed with this work for additional information&lt;br/&gt;
+ * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
+ * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
+ * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
+ * with the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package org.apache.flink.table.runtime.stream.sql&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.flink.api.scala._&lt;br/&gt;
+import org.apache.flink.streaming.api.TimeCharacteristic&lt;br/&gt;
+import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment&lt;br/&gt;
+import org.apache.flink.table.api.TableEnvironment&lt;br/&gt;
+import org.apache.flink.table.api.scala._&lt;br/&gt;
+import org.apache.flink.table.runtime.utils.TimeTestUtil.EventTimeSourceFunction&lt;br/&gt;
+import org.apache.flink.table.runtime.utils.&lt;/p&gt;
{StreamITCase, StreamingWithStateTestBase}
&lt;p&gt;+import org.apache.flink.types.Row&lt;br/&gt;
+import org.junit.Assert.assertEquals&lt;br/&gt;
+import org.junit.Test&lt;br/&gt;
+&lt;br/&gt;
+import scala.collection.mutable&lt;br/&gt;
+&lt;br/&gt;
+class CepITCase extends StreamingWithStateTestBase {&lt;br/&gt;
+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testSimpleCEP() = &lt;/p&gt;
{
+    val env = StreamExecutionEnvironment.getExecutionEnvironment
+    env.setParallelism(1)
+    val tEnv = TableEnvironment.getTableEnvironment(env)
+    StreamITCase.clear
+
+    val data = new mutable.MutableList[(Int, String)]
+    data.+=((1, &quot;a&quot;))
+    data.+=((2, &quot;z&quot;))
+    data.+=((3, &quot;b&quot;))
+    data.+=((4, &quot;c&quot;))
+    data.+=((5, &quot;d&quot;))
+    data.+=((6, &quot;a&quot;))
+    data.+=((7, &quot;b&quot;))
+    data.+=((8, &quot;c&quot;))
+    data.+=((9, &quot;h&quot;))
+
+    val t = env.fromCollection(data).toTable(tEnv).as(&apos;id, &apos;name)
+    tEnv.registerTable(&quot;MyTable&quot;, t)
+
+    val sqlQuery =
+      s&quot;&quot;&quot;
+         |SELECT T.aid, T.bid, T.cid
+         |FROM MyTable
+         |MATCH_RECOGNIZE (
+         |  MEASURES
+         |    A.id AS aid,
+         |    B.id AS bid,
+         |    C.id AS cid
+         |  PATTERN (A B C)
+         |  DEFINE
+         |    A AS A.name = &apos;a&apos;,
+         |    B AS B.name = &apos;b&apos;,
+         |    C AS C.name = &apos;c&apos;
+         |) AS T
+         |&quot;&quot;&quot;.stripMargin
+
+    val result = tEnv.sqlQuery(sqlQuery).toAppendStream[Row]
+    result.addSink(new StreamITCase.StringSink[Row])
+    env.execute()
+
+    val expected = mutable.MutableList(&quot;6,7,8&quot;)
+    assertEquals(expected.sorted, StreamITCase.testResults.sorted)
+  }
&lt;p&gt;+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testAllRowsPerMatch() = &lt;/p&gt;
{
+    val env = StreamExecutionEnvironment.getExecutionEnvironment
+    env.setParallelism(1)
+    val tEnv = TableEnvironment.getTableEnvironment(env)
+    StreamITCase.clear
+
+    val data = new mutable.MutableList[(Int, String)]
+    data.+=((1, &quot;a&quot;))
+    data.+=((2, &quot;z&quot;))
+    data.+=((3, &quot;b&quot;))
+    data.+=((4, &quot;c&quot;))
+    data.+=((5, &quot;d&quot;))
+    data.+=((6, &quot;a&quot;))
+    data.+=((7, &quot;b&quot;))
+    data.+=((8, &quot;c&quot;))
+    data.+=((9, &quot;h&quot;))
+
+    val t = env.fromCollection(data).toTable(tEnv).as(&apos;id, &apos;name)
+    tEnv.registerTable(&quot;MyTable&quot;, t)
+
+    val sqlQuery =
+      s&quot;&quot;&quot;
+         |SELECT *
+         |FROM MyTable
+         |MATCH_RECOGNIZE (
+         |  MEASURES
+         |    A.id AS aid,
+         |    B.id AS bid,
+         |    C.id AS cid
+         |  ALL ROWS PER MATCH
+         |  PATTERN (A B C)
+         |  DEFINE
+         |    A AS A.name = &apos;a&apos;,
+         |    B AS B.name = &apos;b&apos;,
+         |    C AS C.name = &apos;c&apos;
+         |) AS T
+         |&quot;&quot;&quot;.stripMargin
+
+    val result = tEnv.sql(sqlQuery).toAppendStream[Row]
+    result.addSink(new StreamITCase.StringSink[Row])
+    env.execute()
+
+    val expected = mutable.MutableList(&quot;6,a,6,null,null&quot;, &quot;7,b,6,7,null&quot;, &quot;8,c,6,7,8&quot;)
+    assertEquals(expected.sorted, StreamITCase.testResults.sorted)
+  }
&lt;p&gt;+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testFinalFirst() = &lt;/p&gt;
{
+    val env = StreamExecutionEnvironment.getExecutionEnvironment
+    env.setParallelism(1)
+    val tEnv = TableEnvironment.getTableEnvironment(env)
+    StreamITCase.clear
+
+    val data = new mutable.MutableList[(String, Long, Int, Int)]
+    data.+=((&quot;ACME&quot;, 1L, 12, 1))
+    data.+=((&quot;ACME&quot;, 2L, 17, 2))
+    data.+=((&quot;ACME&quot;, 3L, 13, 3))
+    data.+=((&quot;ACME&quot;, 4L, 15, 4))
+    data.+=((&quot;ACME&quot;, 5L, 20, 5))
+    data.+=((&quot;ACME&quot;, 6L, 24, 6))
+    data.+=((&quot;ACME&quot;, 7L, 25, 7))
+    data.+=((&quot;ACME&quot;, 8L, 19, 8))
+
+    val t = env.fromCollection(data).toTable(tEnv).as(&apos;symbol, &apos;tstamp, &apos;price, &apos;tax)
+    tEnv.registerTable(&quot;Ticker&quot;, t)
+
+    val sqlQuery =
+      s&quot;&quot;&quot;
+         |SELECT *
+         |FROM Ticker
+         |MATCH_RECOGNIZE (
+         |  MEASURES
+         |    STRT.tstamp AS start_tstamp,
+         |    FIRST(DOWN.tstamp) AS bottom_tstamp,
+         |    FIRST(UP.tstamp) AS end_tstamp,
+         |    FIRST(DOWN.price + DOWN.tax + 1) AS bottom_total,
+         |    FIRST(UP.price + UP.tax) AS end_total
+         |  ONE ROW PER MATCH
+         |  PATTERN (STRT DOWN+ UP+)
+         |  DEFINE
+         |    DOWN AS DOWN.price &amp;lt; PREV(DOWN.price),
+         |    UP AS UP.price &amp;gt; PREV(UP.price)
+         |) AS T
+         |&quot;&quot;&quot;.stripMargin
+
+    val result = tEnv.sql(sqlQuery).toAppendStream[Row]
+    result.addSink(new StreamITCase.StringSink[Row])
+    env.execute()
+
+    val expected = List(&quot;2,3,4,17,19&quot;, &quot;2,3,4,17,19&quot;, &quot;2,3,4,17,19&quot;, &quot;2,3,4,17,19&quot;)
+    assertEquals(expected.sorted, StreamITCase.testResults.sorted)
+  }
&lt;p&gt;+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testFinalLast() = &lt;/p&gt;
{
+    val env = StreamExecutionEnvironment.getExecutionEnvironment
+    env.setParallelism(1)
+    val tEnv = TableEnvironment.getTableEnvironment(env)
+    StreamITCase.clear
+
+    val data = new mutable.MutableList[(String, Long, Int, Int)]
+    data.+=((&quot;ACME&quot;, 1L, 12, 1))
+    data.+=((&quot;ACME&quot;, 2L, 17, 2))
+    data.+=((&quot;ACME&quot;, 3L, 13, 3))
+    data.+=((&quot;ACME&quot;, 4L, 15, 4))
+    data.+=((&quot;ACME&quot;, 5L, 20, 5))
+    data.+=((&quot;ACME&quot;, 6L, 24, 6))
+    data.+=((&quot;ACME&quot;, 7L, 25, 7))
+    data.+=((&quot;ACME&quot;, 8L, 19, 8))
+
+    val t = env.fromCollection(data).toTable(tEnv).as(&apos;symbol, &apos;tstamp, &apos;price, &apos;tax)
+    tEnv.registerTable(&quot;Ticker&quot;, t)
+
+    val sqlQuery =
+      s&quot;&quot;&quot;
+         |SELECT *
+         |FROM Ticker
+         |MATCH_RECOGNIZE (
+         |  MEASURES
+         |    STRT.tstamp AS start_tstamp,
+         |    LAST(DOWN.tstamp) AS bottom_tstamp,
+         |    LAST(UP.tstamp) AS end_tstamp,
+         |    LAST(DOWN.price + DOWN.tax) AS bottom_total,
+         |    LAST(UP.price + UP.tax + 1) AS end_total
+         |  ONE ROW PER MATCH
+         |  PATTERN (STRT DOWN+ UP+)
+         |  DEFINE
+         |    DOWN AS DOWN.price &amp;lt; PREV(DOWN.price),
+         |    UP AS UP.price &amp;gt; PREV(UP.price)
+         |) AS T
+         |&quot;&quot;&quot;.stripMargin
+
+    val result = tEnv.sql(sqlQuery).toAppendStream[Row]
+    result.addSink(new StreamITCase.StringSink[Row])
+    env.execute()
+
+    val expected = List(&quot;2,3,4,16,20&quot;, &quot;2,3,5,16,26&quot;, &quot;2,3,6,16,31&quot;, &quot;2,3,7,16,33&quot;)
+    assertEquals(expected.sorted, StreamITCase.testResults.sorted)
+  }
&lt;p&gt;+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testPrev() = &lt;/p&gt;
{
+    val env = StreamExecutionEnvironment.getExecutionEnvironment
+    env.setParallelism(1)
+    val tEnv = TableEnvironment.getTableEnvironment(env)
+    StreamITCase.clear
+
+    val data = new mutable.MutableList[(String, Long, Int)]
+    data.+=((&quot;ACME&quot;, 1L, 12))
+    data.+=((&quot;ACME&quot;, 2L, 17))
+    data.+=((&quot;ACME&quot;, 3L, 13))
+    data.+=((&quot;ACME&quot;, 4L, 11))
+    data.+=((&quot;ACME&quot;, 5L, 14))
+    data.+=((&quot;ACME&quot;, 6L, 12))
+    data.+=((&quot;ACME&quot;, 7L, 13))
+    data.+=((&quot;ACME&quot;, 8L, 19))
+
+    val t = env.fromCollection(data).toTable(tEnv).as(&apos;symbol, &apos;tstamp, &apos;price)
+    tEnv.registerTable(&quot;Ticker&quot;, t)
+
+    val sqlQuery =
+      s&quot;&quot;&quot;
+         |SELECT *
+         |FROM Ticker
+         |MATCH_RECOGNIZE (
+         |  MEASURES
+         |    STRT.tstamp AS start_tstamp,
+         |    LAST(DOWN.tstamp) AS up_days,
+         |    LAST(UP.tstamp) AS total_days
+         |  PATTERN (STRT DOWN+ UP+)
+         |  DEFINE
+         |    DOWN AS DOWN.price &amp;lt; PREV(DOWN.price),
+         |    UP AS UP.price &amp;gt; PREV(UP.price, 2)
+         |) AS T
+         |&quot;&quot;&quot;.stripMargin
+
+    val result = tEnv.sql(sqlQuery).toAppendStream[Row]
+    result.addSink(new StreamITCase.StringSink[Row])
+    env.execute()
+
+    val expected = List(&quot;2,4,5&quot;, &quot;2,4,6&quot;, &quot;3,4,5&quot;, &quot;3,4,6&quot;)
+    assertEquals(expected.sorted, StreamITCase.testResults.sorted)
+  }
&lt;p&gt;+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testRunningFirst() = &lt;/p&gt;
{
+    val env = StreamExecutionEnvironment.getExecutionEnvironment
+    env.setParallelism(1)
+    val tEnv = TableEnvironment.getTableEnvironment(env)
+    StreamITCase.clear
+
+    val data = new mutable.MutableList[(String, Long, Int, Int)]
+    data.+=((&quot;ACME&quot;, 1L, 12, 1))
+    data.+=((&quot;ACME&quot;, 2L, 17, 2))
+    data.+=((&quot;ACME&quot;, 3L, 13, 4))
+    data.+=((&quot;ACME&quot;, 4L, 11, 3))
+    data.+=((&quot;ACME&quot;, 5L, 20, 5))
+    data.+=((&quot;ACME&quot;, 6L, 24, 4))
+    data.+=((&quot;ACME&quot;, 7L, 25, 3))
+    data.+=((&quot;ACME&quot;, 8L, 19, 8))
+
+    val t = env.fromCollection(data).toTable(tEnv).as(&apos;symbol, &apos;tstamp, &apos;price, &apos;tax)
+    tEnv.registerTable(&quot;Ticker&quot;, t)
+
+    val sqlQuery =
+      s&quot;&quot;&quot;
+         |SELECT *
+         |FROM Ticker
+         |MATCH_RECOGNIZE (
+         |  MEASURES
+         |    STRT.tstamp AS start_tstamp,
+         |    LAST(DOWN.tstamp) AS bottom_tstamp,
+         |    LAST(UP.tstamp) AS end_tstamp
+         |  ONE ROW PER MATCH
+         |  PATTERN (STRT DOWN+ UP+)
+         |  DEFINE
+         |    DOWN AS DOWN.price &amp;lt; PREV(DOWN.price),
+         |    UP AS UP.price &amp;gt; PREV(UP.price) AND UP.tax &amp;gt; FIRST(DOWN.tax)
+         |) AS T
+         |&quot;&quot;&quot;.stripMargin
+
+    val result = tEnv.sql(sqlQuery).toAppendStream[Row]
+    result.addSink(new StreamITCase.StringSink[Row])
+    env.execute()
+
+    val expected = List(&quot;2,4,5&quot;, &quot;3,4,5&quot;, &quot;3,4,6&quot;)
+    assertEquals(expected.sorted, StreamITCase.testResults.sorted)
+  }
&lt;p&gt;+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testRunningLast() = &lt;/p&gt;
{
+    val env = StreamExecutionEnvironment.getExecutionEnvironment
+    env.setParallelism(1)
+    val tEnv = TableEnvironment.getTableEnvironment(env)
+    StreamITCase.clear
+
+    val data = new mutable.MutableList[(String, Long, Int, Int)]
+    data.+=((&quot;ACME&quot;, 1L, 12, 1))
+    data.+=((&quot;ACME&quot;, 2L, 17, 2))
+    data.+=((&quot;ACME&quot;, 3L, 13, 4))
+    data.+=((&quot;ACME&quot;, 4L, 11, 3))
+    data.+=((&quot;ACME&quot;, 5L, 20, 4))
+    data.+=((&quot;ACME&quot;, 6L, 24, 4))
+    data.+=((&quot;ACME&quot;, 7L, 25, 3))
+    data.+=((&quot;ACME&quot;, 8L, 19, 8))
+
+    val t = env.fromCollection(data).toTable(tEnv).as(&apos;symbol, &apos;tstamp, &apos;price, &apos;tax)
+    tEnv.registerTable(&quot;Ticker&quot;, t)
+
+    val sqlQuery =
+      s&quot;&quot;&quot;
+         |SELECT *
+         |FROM Ticker
+         |MATCH_RECOGNIZE (
+         |  MEASURES
+         |    STRT.tstamp AS start_tstamp,
+         |    LAST(DOWN.tstamp) AS bottom_tstamp,
+         |    LAST(UP.tstamp) AS end_tstamp
+         |  ONE ROW PER MATCH
+         |  PATTERN (STRT DOWN+ UP+)
+         |  DEFINE
+         |    DOWN AS DOWN.price &amp;lt; PREV(DOWN.price),
+         |    UP AS UP.price &amp;gt; PREV(UP.price) AND UP.tax &amp;gt; LAST(DOWN.tax)
+         |) AS T
+         |&quot;&quot;&quot;.stripMargin
+
+    val result = tEnv.sql(sqlQuery).toAppendStream[Row]
+    result.addSink(new StreamITCase.StringSink[Row])
+    env.execute()
+
+    val expected = List(&quot;2,4,5&quot;, &quot;2,4,6&quot;, &quot;3,4,5&quot;, &quot;3,4,6&quot;)
+    assertEquals(expected.sorted, StreamITCase.testResults.sorted)
+  }
&lt;p&gt;+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testWithinEventTime() = &lt;/p&gt;
{
+    val env = StreamExecutionEnvironment.getExecutionEnvironment
+    env.setParallelism(1)
+    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
+    val tEnv = TableEnvironment.getTableEnvironment(env)
+    StreamITCase.clear
+
+    val data = new mutable.MutableList[Either[(Long, (String, Int, Int)), Long]]
+    data.+=(Left((3000L, (&quot;ACME&quot;, 17, 2))))
+    data.+=(Left((1000L, (&quot;ACME&quot;, 12, 1))))
+    data.+=(Right(4000L))
+    data.+=(Left((5000L, (&quot;ACME&quot;, 13, 3))))
+    data.+=(Left((7000L, (&quot;ACME&quot;, 15, 4))))
+    data.+=(Right(8000L))
+    data.+=(Left((9000L, (&quot;ACME&quot;, 20, 5))))
+    data.+=(Right(13000L))
+    data.+=(Left((15000L, (&quot;ACME&quot;, 19, 8))))
+    data.+=(Right(16000L))
+
+    val t = env.addSource(new EventTimeSourceFunction[(String, Int, Int)](data))
+      .toTable(tEnv, &apos;symbol, &apos;price, &apos;tax, &apos;tstamp.rowtime)
+    tEnv.registerTable(&quot;Ticker&quot;, t)
+
+    val sqlQuery =
+      s&quot;&quot;&quot;
+         |SELECT *
+         |FROM Ticker
+         |MATCH_RECOGNIZE (
+         |  PARTITION BY symbol
+         |  ORDER BY tstamp
+         |  MEASURES
+         |    STRT.tstamp AS start_tstamp,
+         |    FIRST(DOWN.tstamp) AS bottom_tstamp,
+         |    FIRST(UP.tstamp) AS end_tstamp,
+         |    FIRST(DOWN.price + DOWN.tax + 1) AS bottom_total,
+         |    FIRST(UP.price + UP.tax) AS end_total
+         |  ONE ROW PER MATCH
+         |  PATTERN (STRT DOWN+ UP+) within interval &apos;5&apos; second
+         |  DEFINE
+         |    DOWN AS DOWN.price &amp;lt; PREV(DOWN.price),
+         |    UP AS UP.price &amp;gt; PREV(UP.price)
+         |) AS T
+         |&quot;&quot;&quot;.stripMargin
+
+    val result = tEnv.sql(sqlQuery).toAppendStream[Row]
+    result.addSink(new StreamITCase.StringSink[Row])
+    env.execute()
+
+    val expected = List(
+      &quot;ACME,1970-01-01 00:00:03.0,1970-01-01 00:00:05.0,1970-01-01 00:00:07.0,17,19&quot;)
+    assertEquals(expected.sorted, StreamITCase.testResults.sorted)
+  }
&lt;p&gt;+}&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16695894" author="githubbot" created="Thu, 22 Nov 2018 13:24:05 +0000"  >&lt;p&gt;fhueske commented on a change in pull request #7147: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10674&quot; title=&quot;Fix handling of retractions after clean up&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10674&quot;&gt;&lt;del&gt;FLINK-10674&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;table&amp;#93;&lt;/span&gt; Fix handling of retractions after clean up&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/7147#discussion_r235723629&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/7147#discussion_r235723629&lt;/a&gt;&lt;/p&gt;



&lt;p&gt; ##########&lt;br/&gt;
 File path: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/aggregate/GroupAggProcessFunction.scala&lt;br/&gt;
 ##########&lt;br/&gt;
 @@ -95,6 +95,12 @@ class GroupAggProcessFunction(&lt;br/&gt;
     var inputCnt = cntState.value()&lt;/p&gt;

&lt;p&gt;     if (null == accumulators) {&lt;br/&gt;
+      // don&apos;t create a new accumulator for unknown retractions&lt;/p&gt;

&lt;p&gt; Review comment:&lt;br/&gt;
   The retraction is not unknown, IMO. &lt;br/&gt;
   I&apos;d rephrase comment: &quot;Don&apos;t create a new accumulator for a retraction message. This might happen if the retraction message is the first message for the key or after a state clean up.&quot;&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16695908" author="githubbot" created="Thu, 22 Nov 2018 13:32:28 +0000"  >&lt;p&gt;twalthr commented on issue #7147: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10674&quot; title=&quot;Fix handling of retractions after clean up&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10674&quot;&gt;&lt;del&gt;FLINK-10674&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;table&amp;#93;&lt;/span&gt; Fix handling of retractions after clean up&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/7147#issuecomment-441032066&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/7147#issuecomment-441032066&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Thanks @fhueske. I will merge this.&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16695922" author="githubbot" created="Thu, 22 Nov 2018 13:54:31 +0000"  >&lt;p&gt;asfgit closed pull request #7147: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10674&quot; title=&quot;Fix handling of retractions after clean up&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10674&quot;&gt;&lt;del&gt;FLINK-10674&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;table&amp;#93;&lt;/span&gt; Fix handling of retractions after clean up&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/7147&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/7147&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/flink-libraries/flink-table-common/src/main/java/org/apache/flink/table/utils/EncodingUtils.java b/flink-libraries/flink-table-common/src/main/java/org/apache/flink/table/utils/EncodingUtils.java&lt;br/&gt;
index 47aac25e897..5531082611d 100644&lt;br/&gt;
&amp;#8212; a/flink-libraries/flink-table-common/src/main/java/org/apache/flink/table/utils/EncodingUtils.java&lt;br/&gt;
+++ b/flink-libraries/flink-table-common/src/main/java/org/apache/flink/table/utils/EncodingUtils.java&lt;br/&gt;
@@ -76,7 +76,7 @@ public static String encodeObjectToString(Serializable obj) &lt;/p&gt;
{
 			return instance;
 		}
&lt;p&gt; catch (Exception e) &lt;/p&gt;
{
 			throw new ValidationException(
-				&quot;Unable to deserialize string &apos;&quot; + base64String + &quot;&apos; of base class &apos;&quot; + baseClass.getName() + &quot;&apos;.&quot;);
+				&quot;Unable to deserialize string &apos;&quot; + base64String + &quot;&apos; of base class &apos;&quot; + baseClass.getName() + &quot;&apos;.&quot;, e);
 		}
&lt;p&gt; 	}&lt;/p&gt;

&lt;p&gt;diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/aggregate/GroupAggProcessFunction.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/aggregate/GroupAggProcessFunction.scala&lt;br/&gt;
index 397032003ec..f591c4f2299 100644&lt;br/&gt;
&amp;#8212; a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/aggregate/GroupAggProcessFunction.scala&lt;br/&gt;
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/aggregate/GroupAggProcessFunction.scala&lt;br/&gt;
@@ -95,6 +95,12 @@ class GroupAggProcessFunction(&lt;br/&gt;
     var inputCnt = cntState.value()&lt;/p&gt;

&lt;p&gt;     if (null == accumulators) {&lt;br/&gt;
+      // don&apos;t create a new accumulator for unknown retractions&lt;br/&gt;
+      // e.g. retractions that come in right after state clean up&lt;br/&gt;
+      if (!inputC.change) &lt;/p&gt;
{
+        return
+      }
&lt;p&gt;+      // first accumulate message&lt;br/&gt;
       firstRow = true&lt;br/&gt;
       accumulators = function.createAccumulators()&lt;br/&gt;
     } else {&lt;br/&gt;
diff --git a/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/harness/NonWindowHarnessTest.scala b/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/harness/GroupAggregateHarnessTest.scala&lt;br/&gt;
similarity index 65%&lt;br/&gt;
rename from flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/harness/NonWindowHarnessTest.scala&lt;br/&gt;
rename to flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/harness/GroupAggregateHarnessTest.scala&lt;br/&gt;
index 7c4f5430328..1dce9946877 100644&lt;br/&gt;
&amp;#8212; a/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/harness/NonWindowHarnessTest.scala&lt;br/&gt;
+++ b/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/harness/GroupAggregateHarnessTest.scala&lt;br/&gt;
@@ -24,20 +24,18 @@ import org.apache.flink.api.common.time.Time&lt;br/&gt;
 import org.apache.flink.api.common.typeinfo.BasicTypeInfo&lt;br/&gt;
 import org.apache.flink.streaming.api.operators.LegacyKeyedProcessOperator&lt;br/&gt;
 import org.apache.flink.streaming.runtime.streamrecord.StreamRecord&lt;br/&gt;
-import org.apache.flink.table.api.StreamQueryConfig&lt;br/&gt;
 import org.apache.flink.table.runtime.aggregate._&lt;br/&gt;
 import org.apache.flink.table.runtime.harness.HarnessTestBase._&lt;br/&gt;
 import org.apache.flink.table.runtime.types.CRow&lt;br/&gt;
-import org.apache.flink.types.Row&lt;br/&gt;
 import org.junit.Test&lt;/p&gt;

&lt;p&gt;-class NonWindowHarnessTest extends HarnessTestBase {&lt;br/&gt;
+class GroupAggregateHarnessTest extends HarnessTestBase {&lt;/p&gt;

&lt;p&gt;   protected var queryConfig =&lt;br/&gt;
     new TestStreamQueryConfig(Time.seconds(2), Time.seconds(3))&lt;/p&gt;

&lt;p&gt;   @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def testNonWindow(): Unit = {&lt;br/&gt;
+  def testAggregate(): Unit = {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     val processFunction = new LegacyKeyedProcessOperator&lt;span class=&quot;error&quot;&gt;&amp;#91;String, CRow, CRow&amp;#93;&lt;/span&gt;(&lt;br/&gt;
       new GroupAggProcessFunction(&lt;br/&gt;
@@ -54,50 +52,49 @@ class NonWindowHarnessTest extends HarnessTestBase &lt;/p&gt;
{
 
     testHarness.open()
 
+    val expectedOutput = new ConcurrentLinkedQueue[Object]()
+
     // register cleanup timer with 3001
     testHarness.setProcessingTime(1)
 
     testHarness.processElement(new StreamRecord(CRow(1L: JLong, 1: JInt, &quot;aaa&quot;), 1))
+    expectedOutput.add(new StreamRecord(CRow(1L: JLong, 1: JInt), 1))
     testHarness.processElement(new StreamRecord(CRow(2L: JLong, 1: JInt, &quot;bbb&quot;), 1))
+    expectedOutput.add(new StreamRecord(CRow(2L: JLong, 1: JInt), 1))
     // reuse timer 3001
     testHarness.setProcessingTime(1000)
     testHarness.processElement(new StreamRecord(CRow(3L: JLong, 2: JInt, &quot;aaa&quot;), 1))
+    expectedOutput.add(new StreamRecord(CRow(3L: JLong, 3: JInt), 1))
     testHarness.processElement(new StreamRecord(CRow(4L: JLong, 3: JInt, &quot;aaa&quot;), 1))
+    expectedOutput.add(new StreamRecord(CRow(4L: JLong, 6: JInt), 1))
 
     // register cleanup timer with 4002
     testHarness.setProcessingTime(1002)
     testHarness.processElement(new StreamRecord(CRow(5L: JLong, 4: JInt, &quot;aaa&quot;), 1))
+    expectedOutput.add(new StreamRecord(CRow(5L: JLong, 10: JInt), 1))
     testHarness.processElement(new StreamRecord(CRow(6L: JLong, 2: JInt, &quot;bbb&quot;), 1))
+    expectedOutput.add(new StreamRecord(CRow(6L: JLong, 3: JInt), 1))
 
     // trigger cleanup timer and register cleanup timer with 7003
     testHarness.setProcessingTime(4003)
     testHarness.processElement(new StreamRecord(CRow(7L: JLong, 5: JInt, &quot;aaa&quot;), 1))
+    expectedOutput.add(new StreamRecord(CRow(7L: JLong, 5: JInt), 1))
     testHarness.processElement(new StreamRecord(CRow(8L: JLong, 6: JInt, &quot;aaa&quot;), 1))
+    expectedOutput.add(new StreamRecord(CRow(8L: JLong, 11: JInt), 1))
     testHarness.processElement(new StreamRecord(CRow(9L: JLong, 7: JInt, &quot;aaa&quot;), 1))
+    expectedOutput.add(new StreamRecord(CRow(9L: JLong, 18: JInt), 1))
     testHarness.processElement(new StreamRecord(CRow(10L: JLong, 3: JInt, &quot;bbb&quot;), 1))
+    expectedOutput.add(new StreamRecord(CRow(10L: JLong, 3: JInt), 1))
 
     val result = testHarness.getOutput
 
-    val expectedOutput = new ConcurrentLinkedQueue[Object]()
-
-    expectedOutput.add(new StreamRecord(CRow(1L: JLong, 1: JInt), 1))
-    expectedOutput.add(new StreamRecord(CRow(2L: JLong, 1: JInt), 1))
-    expectedOutput.add(new StreamRecord(CRow(3L: JLong, 3: JInt), 1))
-    expectedOutput.add(new StreamRecord(CRow(4L: JLong, 6: JInt), 1))
-    expectedOutput.add(new StreamRecord(CRow(5L: JLong, 10: JInt), 1))
-    expectedOutput.add(new StreamRecord(CRow(6L: JLong, 3: JInt), 1))
-    expectedOutput.add(new StreamRecord(CRow(7L: JLong, 5: JInt), 1))
-    expectedOutput.add(new StreamRecord(CRow(8L: JLong, 11: JInt), 1))
-    expectedOutput.add(new StreamRecord(CRow(9L: JLong, 18: JInt), 1))
-    expectedOutput.add(new StreamRecord(CRow(10L: JLong, 3: JInt), 1))
-
     verify(expectedOutput, result)
 
     testHarness.close()
   }

&lt;p&gt;   @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def testNonWindowWithRetract(): Unit = {&lt;br/&gt;
+  def testAggregateWithRetract(): Unit = {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     val processFunction = new LegacyKeyedProcessOperator&lt;span class=&quot;error&quot;&gt;&amp;#91;String, CRow, CRow&amp;#93;&lt;/span&gt;(&lt;br/&gt;
       new GroupAggProcessFunction(&lt;br/&gt;
@@ -114,42 +111,136 @@ class NonWindowHarnessTest extends HarnessTestBase &lt;/p&gt;
{
 
     testHarness.open()
 
+    val expectedOutput = new ConcurrentLinkedQueue[Object]()
+
     // register cleanup timer with 3001
     testHarness.setProcessingTime(1)
 
+    // accumulate
     testHarness.processElement(new StreamRecord(CRow(1L: JLong, 1: JInt, &quot;aaa&quot;), 1))
+    expectedOutput.add(new StreamRecord(CRow(1L: JLong, 1: JInt), 1))
+
+    // accumulate
     testHarness.processElement(new StreamRecord(CRow(2L: JLong, 1: JInt, &quot;bbb&quot;), 2))
+    expectedOutput.add(new StreamRecord(CRow(2L: JLong, 1: JInt), 2))
+
+    // retract for insertion
     testHarness.processElement(new StreamRecord(CRow(3L: JLong, 2: JInt, &quot;aaa&quot;), 3))
+    expectedOutput.add(new StreamRecord(CRow(false, 3L: JLong, 1: JInt), 3))
+    expectedOutput.add(new StreamRecord(CRow(3L: JLong, 3: JInt), 3))
+
+    // retract for deletion
+    testHarness.processElement(new StreamRecord(CRow(false, 3L: JLong, 2: JInt, &quot;aaa&quot;), 3))
+    expectedOutput.add(new StreamRecord(CRow(false, 3L: JLong, 3: JInt), 3))
+    expectedOutput.add(new StreamRecord(CRow(3L: JLong, 1: JInt), 3))
+
+    // accumulate
     testHarness.processElement(new StreamRecord(CRow(4L: JLong, 3: JInt, &quot;ccc&quot;), 4))
+    expectedOutput.add(new StreamRecord(CRow(4L: JLong, 3: JInt), 4))
 
     // trigger cleanup timer and register cleanup timer with 6002
     testHarness.setProcessingTime(3002)
-    testHarness.processElement(new StreamRecord(CRow(5L: JLong, 4: JInt, &quot;aaa&quot;), 5))
-    testHarness.processElement(new StreamRecord(CRow(6L: JLong, 2: JInt, &quot;bbb&quot;), 6))
-    testHarness.processElement(new StreamRecord(CRow(7L: JLong, 5: JInt, &quot;aaa&quot;), 7))
-    testHarness.processElement(new StreamRecord(CRow(8L: JLong, 6: JInt, &quot;eee&quot;), 8))
-    testHarness.processElement(new StreamRecord(CRow(9L: JLong, 7: JInt, &quot;aaa&quot;), 9))
-    testHarness.processElement(new StreamRecord(CRow(10L: JLong, 3: JInt, &quot;bbb&quot;), 10))
-
-    val result = testHarness.getOutput
 
-    val expectedOutput = new ConcurrentLinkedQueue[Object]()
+    // retract after clean up
+    testHarness.processElement(new StreamRecord(CRow(false, 4L: JLong, 3: JInt, &quot;ccc&quot;), 4))
 
-    expectedOutput.add(new StreamRecord(CRow(1L: JLong, 1: JInt), 1))
-    expectedOutput.add(new StreamRecord(CRow(2L: JLong, 1: JInt), 2))
-    expectedOutput.add(new StreamRecord(CRow(false, 3L: JLong, 1: JInt), 3))
-    expectedOutput.add(new StreamRecord(CRow(3L: JLong, 3: JInt), 3))
-    expectedOutput.add(new StreamRecord(CRow(4L: JLong, 3: JInt), 4))
+    // accumulate
+    testHarness.processElement(new StreamRecord(CRow(5L: JLong, 4: JInt, &quot;aaa&quot;), 5))
     expectedOutput.add(new StreamRecord(CRow(5L: JLong, 4: JInt), 5))
+    testHarness.processElement(new StreamRecord(CRow(6L: JLong, 2: JInt, &quot;bbb&quot;), 6))
     expectedOutput.add(new StreamRecord(CRow(6L: JLong, 2: JInt), 6))
+
+    // retract
+    testHarness.processElement(new StreamRecord(CRow(7L: JLong, 5: JInt, &quot;aaa&quot;), 7))
     expectedOutput.add(new StreamRecord(CRow(false, 7L: JLong, 4: JInt), 7))
     expectedOutput.add(new StreamRecord(CRow(7L: JLong, 9: JInt), 7))
+
+    // accumulate
+    testHarness.processElement(new StreamRecord(CRow(8L: JLong, 6: JInt, &quot;eee&quot;), 8))
     expectedOutput.add(new StreamRecord(CRow(8L: JLong, 6: JInt), 8))
+
+    // retract
+    testHarness.processElement(new StreamRecord(CRow(9L: JLong, 7: JInt, &quot;aaa&quot;), 9))
     expectedOutput.add(new StreamRecord(CRow(false, 9L: JLong, 9: JInt), 9))
     expectedOutput.add(new StreamRecord(CRow(9L: JLong, 16: JInt), 9))
+    testHarness.processElement(new StreamRecord(CRow(10L: JLong, 3: JInt, &quot;bbb&quot;), 10))
     expectedOutput.add(new StreamRecord(CRow(false, 10L: JLong, 2: JInt), 10))
     expectedOutput.add(new StreamRecord(CRow(10L: JLong, 5: JInt), 10))
 
+    val result = testHarness.getOutput
+
+    verify(expectedOutput, result)
+
+    testHarness.close()
+  }
&lt;p&gt;+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testDistinctAggregateWithRetract(): Unit = {&lt;br/&gt;
+&lt;br/&gt;
+    val processFunction = new LegacyKeyedProcessOperator&lt;span class=&quot;error&quot;&gt;&amp;#91;String, CRow, CRow&amp;#93;&lt;/span&gt;(&lt;br/&gt;
+      new GroupAggProcessFunction(&lt;br/&gt;
+        genDistinctCountAggFunction,&lt;br/&gt;
+        distinctCountAggregationStateType,&lt;br/&gt;
+        true,&lt;br/&gt;
+        queryConfig))&lt;br/&gt;
+&lt;br/&gt;
+    val testHarness =&lt;br/&gt;
+      createHarnessTester(&lt;br/&gt;
+        processFunction,&lt;br/&gt;
+        new TupleRowKeySelector&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;(2),&lt;br/&gt;
+        BasicTypeInfo.STRING_TYPE_INFO)&lt;br/&gt;
+&lt;br/&gt;
+    testHarness.open()&lt;br/&gt;
+&lt;br/&gt;
+    val expectedOutput = new ConcurrentLinkedQueue&lt;span class=&quot;error&quot;&gt;&amp;#91;Object&amp;#93;&lt;/span&gt;()&lt;br/&gt;
+&lt;br/&gt;
+    // register cleanup timer with 3001&lt;br/&gt;
+    testHarness.setProcessingTime(1)&lt;br/&gt;
+&lt;br/&gt;
+    // insert&lt;br/&gt;
+    testHarness.processElement(new StreamRecord(CRow(1L: JLong, 1: JInt, &quot;aaa&quot;)))&lt;br/&gt;
+    expectedOutput.add(new StreamRecord(CRow(1L: JLong, 1L: JLong)))&lt;br/&gt;
+    testHarness.processElement(new StreamRecord(CRow(2L: JLong, 1: JInt, &quot;bbb&quot;)))&lt;br/&gt;
+    expectedOutput.add(new StreamRecord(CRow(2L: JLong, 1L: JLong)))&lt;br/&gt;
+&lt;br/&gt;
+    // distinct count retract then accumulate for downstream operators&lt;br/&gt;
+    testHarness.processElement(new StreamRecord(CRow(2L: JLong, 1: JInt, &quot;bbb&quot;)))&lt;br/&gt;
+    expectedOutput.add(new StreamRecord(CRow(false, 2L: JLong, 1L: JLong)))&lt;br/&gt;
+    expectedOutput.add(new StreamRecord(CRow(2L: JLong, 1L: JLong)))&lt;br/&gt;
+&lt;br/&gt;
+    // update count for accumulate&lt;br/&gt;
+    testHarness.processElement(new StreamRecord(CRow(1L: JLong, 2: JInt, &quot;aaa&quot;)))&lt;br/&gt;
+    expectedOutput.add(new StreamRecord(CRow(false, 1L: JLong, 1L: JLong)))&lt;br/&gt;
+    expectedOutput.add(new StreamRecord(CRow(1L: JLong, 2L: JLong)))&lt;br/&gt;
+&lt;br/&gt;
+    // update count for retraction&lt;br/&gt;
+    testHarness.processElement(new StreamRecord(CRow(false, 1L: JLong, 2: JInt, &quot;aaa&quot;)))&lt;br/&gt;
+    expectedOutput.add(new StreamRecord(CRow(false, 1L: JLong, 2L: JLong)))&lt;br/&gt;
+    expectedOutput.add(new StreamRecord(CRow(1L: JLong, 1L: JLong)))&lt;br/&gt;
+&lt;br/&gt;
+    // insert&lt;br/&gt;
+    testHarness.processElement(new StreamRecord(CRow(4L: JLong, 3: JInt, &quot;ccc&quot;)))&lt;br/&gt;
+    expectedOutput.add(new StreamRecord(CRow(4L: JLong, 1L: JLong)))&lt;br/&gt;
+&lt;br/&gt;
+    // retract entirely&lt;br/&gt;
+    testHarness.processElement(new StreamRecord(CRow(false, 4L: JLong, 3: JInt, &quot;ccc&quot;)))&lt;br/&gt;
+    expectedOutput.add(new StreamRecord(CRow(false, 4L: JLong, 1L: JLong)))&lt;br/&gt;
+&lt;br/&gt;
+    // trigger cleanup timer and register cleanup timer with 6002&lt;br/&gt;
+    testHarness.setProcessingTime(3002)&lt;br/&gt;
+&lt;br/&gt;
+    // insert&lt;br/&gt;
+    testHarness.processElement(new StreamRecord(CRow(1L: JLong, 1: JInt, &quot;aaa&quot;)))&lt;br/&gt;
+    expectedOutput.add(new StreamRecord(CRow(1L: JLong, 1L: JLong)))&lt;br/&gt;
+&lt;br/&gt;
+    // trigger cleanup timer and register cleanup timer with 9002&lt;br/&gt;
+    testHarness.setProcessingTime(6002)&lt;br/&gt;
+&lt;br/&gt;
+    // retract after cleanup&lt;br/&gt;
+    testHarness.processElement(new StreamRecord(CRow(false, 1L: JLong, 1: JInt, &quot;aaa&quot;)))&lt;br/&gt;
+&lt;br/&gt;
+    val result = testHarness.getOutput&lt;br/&gt;
+&lt;br/&gt;
     verify(expectedOutput, result)&lt;/p&gt;

&lt;p&gt;     testHarness.close()&lt;br/&gt;
diff --git a/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/harness/HarnessTestBase.scala b/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/harness/HarnessTestBase.scala&lt;br/&gt;
index f70d991e50b..e5cceecc560 100644&lt;br/&gt;
&amp;#8212; a/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/harness/HarnessTestBase.scala&lt;br/&gt;
+++ b/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/harness/HarnessTestBase.scala&lt;br/&gt;
@@ -19,8 +19,9 @@ package org.apache.flink.table.runtime.harness&lt;/p&gt;

&lt;p&gt; import java.util.&lt;/p&gt;
{Comparator, Queue =&amp;gt; JQueue}

&lt;p&gt;+import org.apache.flink.api.common.state.&lt;/p&gt;
{MapStateDescriptor, StateDescriptor}
&lt;p&gt; import org.apache.flink.api.common.time.Time&lt;br/&gt;
-import org.apache.flink.api.common.typeinfo.BasicTypeInfo.&lt;/p&gt;
{INT_TYPE_INFO, LONG_TYPE_INFO, STRING_TYPE_INFO}
&lt;p&gt;+import org.apache.flink.api.common.typeinfo.BasicTypeInfo.&lt;/p&gt;
{LONG_TYPE_INFO, STRING_TYPE_INFO}
&lt;p&gt; import org.apache.flink.api.common.typeinfo.TypeInformation&lt;br/&gt;
 import org.apache.flink.api.java.functions.KeySelector&lt;br/&gt;
 import org.apache.flink.api.java.typeutils.RowTypeInfo&lt;br/&gt;
@@ -28,11 +29,11 @@ import org.apache.flink.streaming.api.operators.OneInputStreamOperator&lt;br/&gt;
 import org.apache.flink.streaming.api.watermark.Watermark&lt;br/&gt;
 import org.apache.flink.streaming.runtime.streamrecord.StreamRecord&lt;br/&gt;
 import org.apache.flink.streaming.util.&lt;/p&gt;
{KeyedOneInputStreamOperatorTestHarness, TestHarnessUtil}
&lt;p&gt;-import org.apache.flink.table.api.StreamQueryConfig&lt;br/&gt;
+import org.apache.flink.table.api.&lt;/p&gt;
{StreamQueryConfig, Types}
&lt;p&gt; import org.apache.flink.table.codegen.GeneratedAggregationsFunction&lt;br/&gt;
-import org.apache.flink.table.functions.&lt;/p&gt;
{AggregateFunction, UserDefinedFunction}&lt;br/&gt;
-import org.apache.flink.table.functions.aggfunctions.{IntSumWithRetractAggFunction, LongMaxWithRetractAggFunction, LongMinWithRetractAggFunction}&lt;br/&gt;
+import org.apache.flink.table.functions.aggfunctions.{CountAggFunction, IntSumWithRetractAggFunction, LongMaxWithRetractAggFunction, LongMinWithRetractAggFunction}&lt;br/&gt;
 import org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.getAccumulatorTypeOfAggregateFunction&lt;br/&gt;
+import org.apache.flink.table.functions.{AggregateFunction, UserDefinedFunction}
&lt;p&gt; import org.apache.flink.table.runtime.harness.HarnessTestBase.&lt;/p&gt;
{RowResultSortComparator, RowResultSortComparatorWithWatermarks}
&lt;p&gt; import org.apache.flink.table.runtime.types.&lt;/p&gt;
{CRow, CRowTypeInfo}
&lt;p&gt; import org.apache.flink.table.utils.EncodingUtils&lt;br/&gt;
@@ -55,20 +56,16 @@ class HarnessTestBase {&lt;br/&gt;
   val intSumWithRetractAggFunction: String =&lt;br/&gt;
     EncodingUtils.encodeObjectToString(new IntSumWithRetractAggFunction)&lt;/p&gt;

&lt;p&gt;+  val distinctCountAggFunction: String =&lt;br/&gt;
+    EncodingUtils.encodeObjectToString(new CountAggFunction())&lt;br/&gt;
+&lt;br/&gt;
   protected val MinMaxRowType = new RowTypeInfo(Array[TypeInformation&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;](&lt;br/&gt;
     LONG_TYPE_INFO,&lt;br/&gt;
     STRING_TYPE_INFO,&lt;br/&gt;
     LONG_TYPE_INFO),&lt;br/&gt;
     Array(&quot;rowtime&quot;, &quot;a&quot;, &quot;b&quot;))&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;protected val SumRowType = new RowTypeInfo(Array[TypeInformation&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;](&lt;/li&gt;
	&lt;li&gt;LONG_TYPE_INFO,&lt;/li&gt;
	&lt;li&gt;INT_TYPE_INFO,&lt;/li&gt;
	&lt;li&gt;STRING_TYPE_INFO),&lt;/li&gt;
	&lt;li&gt;Array(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;))&lt;br/&gt;
-&lt;br/&gt;
   protected val minMaxCRowType = new CRowTypeInfo(MinMaxRowType)&lt;/li&gt;
	&lt;li&gt;protected val sumCRowType = new CRowTypeInfo(SumRowType)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   protected val minMaxAggregates: Array[AggregateFunction&lt;span class=&quot;error&quot;&gt;&amp;#91;_, _&amp;#93;&lt;/span&gt;] =&lt;br/&gt;
     Array(new LongMinWithRetractAggFunction,&lt;br/&gt;
@@ -77,15 +74,28 @@ class HarnessTestBase {&lt;br/&gt;
   protected val sumAggregates: Array[AggregateFunction&lt;span class=&quot;error&quot;&gt;&amp;#91;_, _&amp;#93;&lt;/span&gt;] =&lt;br/&gt;
     Array(new IntSumWithRetractAggFunction).asInstanceOf[Array[AggregateFunction&lt;span class=&quot;error&quot;&gt;&amp;#91;_, _&amp;#93;&lt;/span&gt;]]&lt;/p&gt;

&lt;p&gt;+  protected val distinctCountAggregates: Array[AggregateFunction&lt;span class=&quot;error&quot;&gt;&amp;#91;_, _&amp;#93;&lt;/span&gt;] =&lt;br/&gt;
+    Array(new CountAggFunction).asInstanceOf[Array[AggregateFunction&lt;span class=&quot;error&quot;&gt;&amp;#91;_, _&amp;#93;&lt;/span&gt;]]&lt;br/&gt;
+&lt;br/&gt;
   protected val minMaxAggregationStateType: RowTypeInfo =&lt;br/&gt;
     new RowTypeInfo(minMaxAggregates.map(getAccumulatorTypeOfAggregateFunction(_)): _*)&lt;/p&gt;

&lt;p&gt;   protected val sumAggregationStateType: RowTypeInfo =&lt;br/&gt;
     new RowTypeInfo(sumAggregates.map(getAccumulatorTypeOfAggregateFunction(_)): _*)&lt;/p&gt;

&lt;p&gt;+  protected val distinctCountAggregationStateType: RowTypeInfo =&lt;br/&gt;
+    new RowTypeInfo(distinctCountAggregates.map(getAccumulatorTypeOfAggregateFunction(_)): _*)&lt;br/&gt;
+&lt;br/&gt;
+  protected val distinctCountDescriptor: String = EncodingUtils.encodeObjectToString(&lt;br/&gt;
+    new MapStateDescriptor(&quot;distinctAgg0&quot;, distinctCountAggregationStateType, Types.LONG))&lt;br/&gt;
+&lt;br/&gt;
+  protected val minMaxFuncName = &quot;MinMaxAggregateHelper&quot;&lt;br/&gt;
+  protected val sumFuncName = &quot;SumAggregationHelper&quot;&lt;br/&gt;
+  protected val distinctCountFuncName = &quot;DistinctCountAggregationHelper&quot;&lt;br/&gt;
+&lt;br/&gt;
   val minMaxCode: String =&lt;br/&gt;
     s&quot;&quot;&quot;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;public class MinMaxAggregateHelper&lt;br/&gt;
+      &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;public class $minMaxFuncName&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  extends org.apache.flink.table.runtime.aggregate.GeneratedAggregations {&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  transient org.apache.flink.table.functions.aggfunctions.LongMinWithRetractAggFunction&lt;br/&gt;
@@ -94,7 +104,7 @@ class HarnessTestBase {&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  transient org.apache.flink.table.functions.aggfunctions.LongMaxWithRetractAggFunction&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;    fmax = null;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/li&gt;
	&lt;li&gt;&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  public MinMaxAggregateHelper() throws Exception {&lt;br/&gt;
+      &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  public $minMaxFuncName() throws Exception {&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;    fmin = (org.apache.flink.table.functions.aggfunctions.LongMinWithRetractAggFunction)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;    ${classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;EncodingUtils&amp;#93;&lt;/span&gt;.getCanonicalName}.decodeStringToObject(&lt;br/&gt;
@@ -207,25 +217,25 @@ class HarnessTestBase {&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   val sumAggCode: String =&lt;br/&gt;
     s&quot;&quot;&quot;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;public final class SumAggregationHelper&lt;br/&gt;
+      &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;public final class $sumFuncName&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  extends org.apache.flink.table.runtime.aggregate.GeneratedAggregations {&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/li&gt;
	&lt;li&gt;&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;transient org.apache.flink.table.functions.aggfunctions.IntSumWithRetractAggFunction&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/li&gt;
	&lt;li&gt;&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;sum = null;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/li&gt;
	&lt;li&gt;&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;private final org.apache.flink.table.runtime.aggregate.SingleElementIterable&amp;lt;org.apache&lt;br/&gt;
+      &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  transient org.apache.flink.table.functions.aggfunctions.IntSumWithRetractAggFunction&lt;br/&gt;
+      &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  sum = null;&lt;br/&gt;
+      &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  private final org.apache.flink.table.runtime.aggregate.SingleElementIterable&amp;lt;org.apache&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;    .flink.table.functions.aggfunctions.SumWithRetractAccumulator&amp;gt; accIt0 =&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;      new org.apache.flink.table.runtime.aggregate.SingleElementIterable&amp;lt;org.apache.flink&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;      .table&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;      .functions.aggfunctions.SumWithRetractAccumulator&amp;gt;();&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/li&gt;
	&lt;li&gt;&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  public SumAggregationHelper() throws Exception {&lt;br/&gt;
+      &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  public $sumFuncName() throws Exception {&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/li&gt;
	&lt;li&gt;&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;sum = (org.apache.flink.table.functions.aggfunctions.IntSumWithRetractAggFunction)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/li&gt;
	&lt;li&gt;&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;${classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;EncodingUtils&amp;#93;&lt;/span&gt;.getCanonicalName}.decodeStringToObject(&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/li&gt;
	&lt;li&gt;&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  &quot;$intSumWithRetractAggFunction&quot;,&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/li&gt;
	&lt;li&gt;&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  ${classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;UserDefinedFunction&amp;#93;&lt;/span&gt;.getCanonicalName}.class);&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/li&gt;
	&lt;li&gt;&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;}&lt;br/&gt;
+      &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;    sum = (org.apache.flink.table.functions.aggfunctions.IntSumWithRetractAggFunction)&lt;br/&gt;
+      &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;      ${classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;EncodingUtils&amp;#93;&lt;/span&gt;.getCanonicalName}.decodeStringToObject(&lt;br/&gt;
+      &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;        &quot;$intSumWithRetractAggFunction&quot;,&lt;br/&gt;
+      &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;        ${classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;UserDefinedFunction&amp;#93;&lt;/span&gt;.getCanonicalName}.class);&lt;br/&gt;
+      &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  }&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  public final void setAggregationResults(&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;    org.apache.flink.types.Row accs,&lt;br/&gt;
@@ -256,6 +266,12 @@ class HarnessTestBase 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {       |  public final void retract(       |    org.apache.flink.types.Row accs,       |    org.apache.flink.types.Row input) {
+      |
+      |    sum.retract(
+      |      ((org.apache.flink.table.functions.aggfunctions.SumWithRetractAccumulator) accs
+      |      .getField
+      |      (0)),
+      |      (java.lang.Integer) input.getField(1));
       |  }       |       |  public final org.apache.flink.types.Row createAccumulators()@@ -281,6 +297,162 @@ class HarnessTestBase {
       |      input.getField(0));
       |  }       |+      |  public final org.apache.flink.types.Row createOutputRow() {
+      |    return new org.apache.flink.types.Row(2);
+      |  }+      |+      |+      |  public final org.apache.flink.types.Row mergeAccumulatorsPair(+      |    org.apache.flink.types.Row a,+      |    org.apache.flink.types.Row b)+      |            {
+      |
+      |      return a;
+      |
+      |  }+      |+      |  public final void resetAccumulator(+      |    org.apache.flink.types.Row accs) {
+      |  }&lt;br/&gt;
+      |&lt;br/&gt;
+      |  public void open(org.apache.flink.api.common.functions.RuntimeContext ctx) {+      |  }+      |+      |  public void cleanup() {
+      |  }&lt;br/&gt;
+      |&lt;br/&gt;
+      |  public void close() {+      |  }+      |}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+      &lt;/p&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&quot;&quot;&quot;.stripMargin&lt;br/&gt;
+&lt;br/&gt;
+    val distinctCountAggCode: String =&lt;br/&gt;
+    s&quot;&quot;&quot;&lt;br/&gt;
+      &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;public final class $distinctCountFuncName&lt;br/&gt;
+      &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  extends org.apache.flink.table.runtime.aggregate.GeneratedAggregations {&lt;br/&gt;
+      &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;+      |  final org.apache.flink.table.functions.aggfunctions.CountAggFunction count;&lt;br/&gt;
+      |&lt;br/&gt;
+      |  final org.apache.flink.table.api.dataview.MapView acc0_distinctValueMap_dataview;&lt;br/&gt;
+      |&lt;br/&gt;
+      |  final java.lang.reflect.Field distinctValueMap =&lt;br/&gt;
+      |      org.apache.flink.api.java.typeutils.TypeExtractor.getDeclaredField(&lt;br/&gt;
+      |        org.apache.flink.table.functions.aggfunctions.DistinctAccumulator.class,&lt;br/&gt;
+      |        &quot;distinctValueMap&quot;);&lt;br/&gt;
+      |&lt;br/&gt;
+      |&lt;br/&gt;
+      |  private final org.apache.flink.table.runtime.aggregate.SingleElementIterable&amp;lt;org.apache&lt;br/&gt;
+      |    .flink.table.functions.aggfunctions.CountAccumulator&amp;gt; accIt0 =&lt;br/&gt;
+      |      new org.apache.flink.table.runtime.aggregate.SingleElementIterable&amp;lt;org.apache.flink&lt;br/&gt;
+      |      .table&lt;br/&gt;
+      |      .functions.aggfunctions.CountAccumulator&amp;gt;();&lt;br/&gt;
+      |&lt;br/&gt;
+      |  public $distinctCountFuncName() throws Exception {&lt;br/&gt;
+      |&lt;br/&gt;
+      |    count = (org.apache.flink.table.functions.aggfunctions.CountAggFunction)&lt;br/&gt;
+      |      ${classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;EncodingUtils&amp;#93;&lt;/span&gt;.getCanonicalName}.decodeStringToObject(&lt;br/&gt;
+      |      &quot;$distinctCountAggFunction&quot;,&lt;br/&gt;
+      |      ${classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;UserDefinedFunction&amp;#93;&lt;/span&gt;.getCanonicalName}.class);&lt;br/&gt;
+      |&lt;br/&gt;
+      |    distinctValueMap.setAccessible(true);&lt;br/&gt;
+      |  }&lt;br/&gt;
+      |&lt;br/&gt;
+      |  public void open(org.apache.flink.api.common.functions.RuntimeContext ctx) {&lt;br/&gt;
+      |    org.apache.flink.api.common.state.StateDescriptor acc0_distinctValueMap_dataview_desc =&lt;br/&gt;
+      |      (org.apache.flink.api.common.state.StateDescriptor)&lt;br/&gt;
+      |      ${classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;EncodingUtils&amp;#93;&lt;/span&gt;.getCanonicalName}.decodeStringToObject(&lt;br/&gt;
+      |      &quot;$distinctCountDescriptor&quot;,&lt;br/&gt;
+      |      ${classOf[StateDescriptor&lt;span class=&quot;error&quot;&gt;&amp;#91;_, _&amp;#93;&lt;/span&gt;].getCanonicalName}.class,&lt;br/&gt;
+      |      ctx.getUserCodeClassLoader());&lt;br/&gt;
+      |    acc0_distinctValueMap_dataview = new org.apache.flink.table.dataview.StateMapView(&lt;br/&gt;
+      |          ctx.getMapState((org.apache.flink.api.common.state.MapStateDescriptor)&lt;br/&gt;
+      |          acc0_distinctValueMap_dataview_desc));&lt;br/&gt;
+      |  }&lt;br/&gt;
+      |&lt;br/&gt;
+      |  public final void setAggregationResults(&lt;br/&gt;
+      |    org.apache.flink.types.Row accs,&lt;br/&gt;
+      |    org.apache.flink.types.Row output) &lt;/p&gt;
{
+      |
+      |    org.apache.flink.table.functions.AggregateFunction baseClass0 =
+      |      (org.apache.flink.table.functions.AggregateFunction)
+      |      count;
+      |
+      |    org.apache.flink.table.functions.aggfunctions.DistinctAccumulator distinctAcc0 =
+      |      (org.apache.flink.table.functions.aggfunctions.DistinctAccumulator) accs.getField(0);
+      |    org.apache.flink.table.functions.aggfunctions.CountAccumulator acc0 =
+      |      (org.apache.flink.table.functions.aggfunctions.CountAccumulator)
+      |      distinctAcc0.getRealAcc();
+      |
+      |    output.setField(1, baseClass0.getValue(acc0));
+      |  }
&lt;p&gt;+      |&lt;br/&gt;
+      |  public final void accumulate(&lt;br/&gt;
+      |    org.apache.flink.types.Row accs,&lt;br/&gt;
+      |    org.apache.flink.types.Row input) throws Exception &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+      |+      |    org.apache.flink.table.functions.aggfunctions.DistinctAccumulator distinctAcc0 =+      |      (org.apache.flink.table.functions.aggfunctions.DistinctAccumulator) accs.getField(0);+      |+      |    distinctValueMap.set(distinctAcc0, acc0_distinctValueMap_dataview);+      |+      |    if (distinctAcc0.add(+      |          org.apache.flink.types.Row.of((java.lang.Integer) input.getField(1)))) {
+      |        org.apache.flink.table.functions.aggfunctions.CountAccumulator acc0 =
+      |          (org.apache.flink.table.functions.aggfunctions.CountAccumulator)
+      |          distinctAcc0.getRealAcc();
+      |
+      |
+      |        count.accumulate(acc0, (java.lang.Integer) input.getField(1));
+      |    }+      |  }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+      |&lt;br/&gt;
+      |  public final void retract(&lt;br/&gt;
+      |    org.apache.flink.types.Row accs,&lt;br/&gt;
+      |    org.apache.flink.types.Row input) throws Exception &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+      |+      |    org.apache.flink.table.functions.aggfunctions.DistinctAccumulator distinctAcc0 =+      |      (org.apache.flink.table.functions.aggfunctions.DistinctAccumulator) accs.getField(0);+      |+      |    distinctValueMap.set(distinctAcc0, acc0_distinctValueMap_dataview);+      |+      |    if (distinctAcc0.remove(+      |          org.apache.flink.types.Row.of((java.lang.Integer) input.getField(1)))) {
+      |        org.apache.flink.table.functions.aggfunctions.CountAccumulator acc0 =
+      |          (org.apache.flink.table.functions.aggfunctions.CountAccumulator)
+      |            distinctAcc0.getRealAcc();
+      |
+      |        count.retract(acc0 , (java.lang.Integer) input.getField(1));
+      |      }+      |  }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+      |&lt;br/&gt;
+      |  public final org.apache.flink.types.Row createAccumulators()&lt;br/&gt;
+      |     &lt;/p&gt;
{
+      |
+      |      org.apache.flink.types.Row accs = new org.apache.flink.types.Row(1);
+      |
+      |      org.apache.flink.table.functions.aggfunctions.CountAccumulator acc0 =
+      |        (org.apache.flink.table.functions.aggfunctions.CountAccumulator)
+      |        count.createAccumulator();
+      |      org.apache.flink.table.functions.aggfunctions.DistinctAccumulator distinctAcc0 =
+      |        (org.apache.flink.table.functions.aggfunctions.DistinctAccumulator)
+      |        new org.apache.flink.table.functions.aggfunctions.DistinctAccumulator (acc0);
+      |      accs.setField(
+      |        0,
+      |        distinctAcc0);
+      |
+      |        return accs;
+      |  }
&lt;p&gt;+      |&lt;br/&gt;
+      |  public final void setForwardedFields(&lt;br/&gt;
+      |    org.apache.flink.types.Row input,&lt;br/&gt;
+      |    org.apache.flink.types.Row output)&lt;br/&gt;
+      |     &lt;/p&gt;
{
+      |
+      |    output.setField(
+      |      0,
+      |      input.getField(0));
+      |  }
&lt;p&gt;+      |&lt;/p&gt;
&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  public final void setConstantFlags(org.apache.flink.types.Row output)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;     {&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;@@ -304,10 +476,8 @@ class HarnessTestBase {&lt;/p&gt;
&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;    org.apache.flink.types.Row accs) 
{
       |  }&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/li&gt;
	&lt;li&gt;&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  public void open(org.apache.flink.api.common.functions.RuntimeContext ctx) 
{
-      |  }&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/li&gt;
	&lt;li&gt;&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  public void cleanup() 
{
+      |    acc0_distinctValueMap_dataview.clear();
       |  }&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  public void close() {&lt;br/&gt;
@@ -315,12 +485,11 @@ class HarnessTestBase 
{
       |}&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&quot;&quot;&quot;.stripMargin&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;protected val minMaxFuncName = &quot;MinMaxAggregateHelper&quot;&lt;/li&gt;
	&lt;li&gt;protected val sumFuncName = &quot;SumAggregationHelper&quot;&lt;br/&gt;
-&lt;br/&gt;
   protected val genMinMaxAggFunction = GeneratedAggregationsFunction(minMaxFuncName, minMaxCode)&lt;br/&gt;
   protected val genSumAggFunction = GeneratedAggregationsFunction(sumFuncName, sumAggCode)&lt;br/&gt;
+  protected val genDistinctCountAggFunction = GeneratedAggregationsFunction(&lt;br/&gt;
+    distinctCountFuncName,&lt;br/&gt;
+    distinctCountAggCode)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   def createHarnessTester&lt;span class=&quot;error&quot;&gt;&amp;#91;IN, OUT, KEY&amp;#93;&lt;/span&gt;(&lt;br/&gt;
     operator: OneInputStreamOperator&lt;span class=&quot;error&quot;&gt;&amp;#91;IN, OUT&amp;#93;&lt;/span&gt;,&lt;br/&gt;
diff --git a/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/stream/sql/SqlITCase.scala b/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/stream/sql/SqlITCase.scala&lt;br/&gt;
index c3da65f887a..46dde8e0225 100644&lt;br/&gt;
&amp;#8212; a/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/stream/sql/SqlITCase.scala&lt;br/&gt;
+++ b/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/stream/sql/SqlITCase.scala&lt;br/&gt;
@@ -263,6 +263,43 @@ class SqlITCase extends StreamingWithStateTestBase &lt;/p&gt;
{
     assertEquals(expected.sorted, StreamITCase.retractedResults.sorted)
   }

&lt;p&gt;+  @Test&lt;br/&gt;
+  def testDistinctWithRetraction(): Unit = &lt;/p&gt;
{
+
+    val env = StreamExecutionEnvironment.getExecutionEnvironment
+    val tEnv = TableEnvironment.getTableEnvironment(env)
+    StreamITCase.clear
+
+    val data = new mutable.MutableList[(Int, Long, String)]
+    data.+=((1, 1L, &quot;Hi&quot;))
+    data.+=((1, 1L, &quot;Hi World&quot;))
+    data.+=((1, 1L, &quot;Test&quot;))
+    data.+=((2, 1L, &quot;Hi World&quot;))
+    data.+=((2, 1L, &quot;Test&quot;))
+    data.+=((3, 1L, &quot;Hi World&quot;))
+    data.+=((3, 1L, &quot;Hi World&quot;))
+    data.+=((3, 1L, &quot;Hi World&quot;))
+    data.+=((4, 1L, &quot;Hi World&quot;))
+    data.+=((4, 1L, &quot;Test&quot;))
+
+    val t = env.fromCollection(data).toTable(tEnv).as(&apos;a, &apos;b, &apos;c)
+    tEnv.registerTable(&quot;MyTable&quot;, t)
+
+    // &quot;1,1,3&quot;, &quot;2,1,2&quot;, &quot;3,1,1&quot;, &quot;4,1,2&quot;
+    val distinct = &quot;SELECT a, COUNT(DISTINCT b) AS distinct_b, COUNT(DISTINCT c) AS distinct_c &quot; +
+      &quot;FROM MyTable GROUP BY a&quot;
+    val nestedDistinct = s&quot;SELECT distinct_b, COUNT(DISTINCT distinct_c) &quot; +
+      s&quot;FROM ($distinct) GROUP BY distinct_b&quot;
+
+    val result = tEnv.sqlQuery(nestedDistinct).toRetractStream[Row]
+    result.addSink(new StreamITCase.RetractingSink).setParallelism(1)
+
+    env.execute()
+
+    val expected = List(&quot;1,3&quot;)
+    assertEquals(expected.sorted, StreamITCase.retractedResults.sorted)
+  }
&lt;p&gt;+&lt;br/&gt;
   @Test&lt;br/&gt;
   def testUnboundedGroupByCollect(): Unit = {&lt;/p&gt;





&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16696014" author="twalthr" created="Thu, 22 Nov 2018 15:22:52 +0000"  >&lt;p&gt;Fixed in master: 1878e1168267755356bb332df326625e3049a6c8, 7e86a80b714dac6dc0ae8fda0155d655f623d31e, b480971a1b5d70b9de692f38b69b5578b8ad2355&lt;br/&gt;
Fixed in 1.7.0: 3cb24911b9a3fcc081493df735ed1b02a15de3f9, 7e86a80b714dac6dc0ae8fda0155d655f623d31e, b480971a1b5d70b9de692f38b69b5578b8ad2355&lt;br/&gt;
Fixed in 1.6.3: d01dbe3a2adb042957355f4d80a77126412fd52d&lt;br/&gt;
Fixed in 1.5.6: 5a533e227254d0142181cb1b231da4a9c3d09dfe&lt;/p&gt;</comment>
                            <comment id="17165401" author="dixingxing@yeah.net" created="Mon, 27 Jul 2020 03:21:12 +0000"  >&lt;p&gt;Hi Timo,  I saw this issue has been fixed in 1.7.0 but today we still got the exception under flink 1.7.2, should we reopen this issue?&lt;br/&gt;
 &lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/13008456/13008456_screenshot-1.png&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt; &lt;/p&gt;</comment>
                            <comment id="17165688" author="twalthr" created="Mon, 27 Jul 2020 12:58:38 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dixingxing%40yeah.net&quot; class=&quot;user-hover&quot; rel=&quot;dixingxing@yeah.net&quot;&gt;dixingxing@yeah.net&lt;/a&gt;, this is very strange. Could you debug this problem? 1.7 is a very old version and I doubt that we will release another minor version. The cause was:&lt;/p&gt;

&lt;p&gt;Because state clean up happens in processing time, it might be&lt;br/&gt;
the case that retractions are arriving after the state has&lt;br/&gt;
been cleaned up. Before these changes, a new accumulator was&lt;br/&gt;
created and invalid retraction messages were emitted. This&lt;br/&gt;
change drops retraction messages for which no accumulator&lt;br/&gt;
exists. &lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12945552" name="image-2018-10-25-14-46-03-373.png" size="63577" author="ambition" created="Thu, 25 Oct 2018 06:46:10 +0000"/>
                            <attachment id="13008456" name="screenshot-1.png" size="68391" author="dixingxing@yeah.net" created="Mon, 27 Jul 2020 03:18:22 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            5 years, 16 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3zmdb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>