diff --git a/flink-table/flink-table-api-java-bridge/src/main/java/org/apache/flink/table/connector/sink/DataStreamSinkProvider.java b/flink-table/flink-table-api-java-bridge/src/main/java/org/apache/flink/table/connector/sink/DataStreamSinkProvider.java
index 8a6af08b009..bbb0bd9a418 100644
--- a/flink-table/flink-table-api-java-bridge/src/main/java/org/apache/flink/table/connector/sink/DataStreamSinkProvider.java
+++ b/flink-table/flink-table-api-java-bridge/src/main/java/org/apache/flink/table/connector/sink/DataStreamSinkProvider.java
@@ -21,8 +21,11 @@ package org.apache.flink.table.connector.sink;
 import org.apache.flink.annotation.PublicEvolving;
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.datastream.DataStreamSink;
+import org.apache.flink.table.connector.ParallelismProvider;
 import org.apache.flink.table.data.RowData;
 
+import java.util.Optional;
+
 /**
  * Provider that consumes a Java {@link DataStream} as a runtime implementation for {@link
  * DynamicTableSink}.
@@ -33,11 +36,24 @@ import org.apache.flink.table.data.RowData;
  * attention to how changes are shuffled to not mess up the changelog per parallel subtask.
  */
 @PublicEvolving
-public interface DataStreamSinkProvider extends DynamicTableSink.SinkRuntimeProvider {
+public interface DataStreamSinkProvider
+        extends DynamicTableSink.SinkRuntimeProvider, ParallelismProvider {
 
     /**
      * Consumes the given Java {@link DataStream} and returns the sink transformation {@link
      * DataStreamSink}.
      */
     DataStreamSink<?> consumeDataStream(DataStream<RowData> dataStream);
+
+    /**
+     * {@inheritDoc}
+     *
+     * <p>Note: If a custom parallelism is returned and {@link #consumeDataStream(DataStream)}
+     * applies multiple transformations, make sure to set the same custom parallelism to each
+     * operator to not mess up the changelog.
+     */
+    @Override
+    default Optional<Integer> getParallelism() {
+        return Optional.empty();
+    }
 }
diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/connector/ParallelismProvider.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/connector/ParallelismProvider.java
index 81a725b2b76..f9c4684383a 100644
--- a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/connector/ParallelismProvider.java
+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/connector/ParallelismProvider.java
@@ -19,7 +19,7 @@
 package org.apache.flink.table.connector;
 
 import org.apache.flink.annotation.PublicEvolving;
-import org.apache.flink.table.connector.sink.OutputFormatProvider;
+import org.apache.flink.table.connector.sink.DynamicTableSink.SinkRuntimeProvider;
 
 import java.util.Optional;
 
@@ -27,9 +27,7 @@ import java.util.Optional;
  * Parallelism provider for other connector providers. It allows to express a custom parallelism for
  * the connector runtime implementation. Otherwise the parallelism is determined by the planner.
  *
- * <p>Note: Currently, this interface can only work with {@code
- * org.apache.flink.table.connector.sink.SinkFunctionProvider} in {@code
- * flink-table-api-java-bridge} module and {@link OutputFormatProvider}.
+ * <p>Note: Currently, this interface only works with {@link SinkRuntimeProvider}.
  */
 @PublicEvolving
 public interface ParallelismProvider {
@@ -40,6 +38,10 @@ public interface ParallelismProvider {
      * <p>The parallelism denotes how many parallel instances of a source or sink will be spawned
      * during the execution.
      *
+     * <p>Enforcing a different parallelism for sinks might mess up the changelog if the input is
+     * not {@link ChangelogMode#insertOnly()}. Therefore, a primary key is required by which the
+     * input will be shuffled before records enter the {@link SinkRuntimeProvider} implementation.
+     *
      * @return empty if the connector does not provide a custom parallelism, then the planner will
      *     decide the number of parallel instances by itself.
      */
diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/common/CommonExecSink.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/common/CommonExecSink.java
index 545bf96da98..b0f34906421 100644
--- a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/common/CommonExecSink.java
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/common/CommonExecSink.java
@@ -37,11 +37,11 @@ import org.apache.flink.table.api.TableConfig;
 import org.apache.flink.table.api.TableException;
 import org.apache.flink.table.api.config.ExecutionConfigOptions;
 import org.apache.flink.table.catalog.ResolvedSchema;
-import org.apache.flink.table.catalog.UniqueConstraint;
 import org.apache.flink.table.connector.ChangelogMode;
 import org.apache.flink.table.connector.ParallelismProvider;
 import org.apache.flink.table.connector.sink.DataStreamSinkProvider;
 import org.apache.flink.table.connector.sink.DynamicTableSink;
+import org.apache.flink.table.connector.sink.DynamicTableSink.SinkRuntimeProvider;
 import org.apache.flink.table.connector.sink.OutputFormatProvider;
 import org.apache.flink.table.connector.sink.SinkFunctionProvider;
 import org.apache.flink.table.connector.sink.SinkProvider;
@@ -60,6 +60,7 @@ import org.apache.flink.table.runtime.keyselector.RowDataKeySelector;
 import org.apache.flink.table.runtime.operators.sink.SinkNotNullEnforcer;
 import org.apache.flink.table.runtime.operators.sink.SinkOperator;
 import org.apache.flink.table.runtime.operators.sink.SinkUpsertMaterializer;
+import org.apache.flink.table.runtime.typeutils.InternalSerializers;
 import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.runtime.util.StateConfigUtil;
 import org.apache.flink.table.types.logical.LogicalType;
@@ -69,16 +70,11 @@ import org.apache.flink.types.RowKind;
 import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.annotation.JsonIgnore;
 import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.annotation.JsonProperty;
 
-import javax.annotation.Nullable;
-
 import java.util.Arrays;
 import java.util.List;
-import java.util.Optional;
 import java.util.stream.Collectors;
 import java.util.stream.IntStream;
 
-import static org.apache.flink.util.Preconditions.checkArgument;
-
 /**
  * Base {@link ExecNode} to write data to an external sink defined by a {@link DynamicTableSink}.
  */
@@ -119,107 +115,42 @@ public abstract class CommonExecSink extends ExecNodeBase<Object>
             int rowtimeFieldIndex,
             boolean upsertMaterialize) {
         final DynamicTableSink tableSink = tableSinkSpec.getTableSink();
-        final DynamicTableSink.SinkRuntimeProvider runtimeProvider =
-                tableSink.getSinkRuntimeProvider(new SinkRuntimeProviderContext(isBounded));
         final ResolvedSchema schema = tableSinkSpec.getCatalogTable().getResolvedSchema();
+
+        final SinkRuntimeProvider runtimeProvider =
+                tableSink.getSinkRuntimeProvider(new SinkRuntimeProviderContext(isBounded));
+
         final RowType physicalRowType = getPhysicalRowType(schema);
-        inputTransform = applyNotNullEnforcer(tableConfig, physicalRowType, inputTransform);
 
-        if (runtimeProvider instanceof DataStreamSinkProvider) {
-            if (runtimeProvider instanceof ParallelismProvider) {
-                throw new TableException(
-                        "`DataStreamSinkProvider` is not allowed to work with"
-                                + " `ParallelismProvider`, "
-                                + "please see document of `ParallelismProvider`");
-            }
+        final int[] primaryKeys = getPrimaryKeyIndices(physicalRowType, schema);
 
-            final DataStream<RowData> dataStream = new DataStream<>(env, inputTransform);
-            final DataStreamSinkProvider provider = (DataStreamSinkProvider) runtimeProvider;
-            return provider.consumeDataStream(dataStream).getTransformation();
-        } else if (runtimeProvider instanceof TransformationSinkProvider) {
-            final TransformationSinkProvider provider =
-                    (TransformationSinkProvider) runtimeProvider;
-            return (Transformation<Object>)
-                    provider.createTransformation(
-                            TransformationSinkProvider.Context.of(
-                                    inputTransform, rowtimeFieldIndex));
-        } else {
-            checkArgument(
-                    runtimeProvider instanceof ParallelismProvider,
-                    "%s should implement ParallelismProvider interface.",
-                    runtimeProvider.getClass().getName());
-            final int inputParallelism = inputTransform.getParallelism();
-            final int sinkParallelism =
-                    deriveSinkParallelism((ParallelismProvider) runtimeProvider, inputParallelism);
-
-            // apply keyBy partition transformation if needed
-            inputTransform =
-                    applyKeyByIfNeeded(
-                            physicalRowType,
-                            schema.getPrimaryKey().orElse(null),
-                            inputTransform,
-                            inputParallelism,
-                            sinkParallelism,
-                            upsertMaterialize);
-
-            if (upsertMaterialize) {
-                GeneratedRecordEqualiser equaliser =
-                        new EqualiserCodeGenerator(physicalRowType)
-                                .generateRecordEqualiser("SinkMaterializeEqualiser");
-                SinkUpsertMaterializer operator =
-                        new SinkUpsertMaterializer(
-                                StateConfigUtil.createTtlConfig(
-                                        tableConfig.getIdleStateRetention().toMillis()),
-                                InternalTypeInfo.of(physicalRowType).toSerializer(),
-                                equaliser);
-                OneInputTransformation<RowData, RowData> materializeTransform =
-                        new OneInputTransformation<>(
-                                inputTransform,
-                                "SinkMaterializer",
-                                operator,
-                                inputTransform.getOutputType(),
-                                sinkParallelism);
-                int[] pkIndices =
-                        getPrimaryKeyIndices(physicalRowType, schema.getPrimaryKey().get());
-                RowDataKeySelector keySelector =
-                        KeySelectorUtil.getRowDataSelector(
-                                pkIndices, InternalTypeInfo.of(physicalRowType));
-                materializeTransform.setStateKeySelector(keySelector);
-                materializeTransform.setStateKeyType(keySelector.getProducedType());
-                inputTransform = materializeTransform;
-            }
-
-            final SinkFunction<RowData> sinkFunction;
-            if (runtimeProvider instanceof SinkFunctionProvider) {
-                sinkFunction = ((SinkFunctionProvider) runtimeProvider).createSinkFunction();
-                return createSinkFunctionTransformation(
-                        sinkFunction, env, inputTransform, rowtimeFieldIndex, sinkParallelism);
-
-            } else if (runtimeProvider instanceof OutputFormatProvider) {
-                OutputFormat<RowData> outputFormat =
-                        ((OutputFormatProvider) runtimeProvider).createOutputFormat();
-                sinkFunction = new OutputFormatSinkFunction<>(outputFormat);
-                return createSinkFunctionTransformation(
-                        sinkFunction, env, inputTransform, rowtimeFieldIndex, sinkParallelism);
-
-            } else if (runtimeProvider instanceof SinkProvider) {
-                return new SinkTransformation<>(
-                        inputTransform,
-                        ((SinkProvider) runtimeProvider).createSink(),
-                        getDescription(),
-                        sinkParallelism);
+        final int sinkParallelism = deriveSinkParallelism(inputTransform, runtimeProvider);
+
+        Transformation<RowData> sinkTransform =
+                applyNotNullEnforcer(inputTransform, tableConfig, physicalRowType);
 
-            } else {
-                throw new TableException("This should not happen.");
-            }
+        sinkTransform = applyKeyBy(sinkTransform, primaryKeys, sinkParallelism, upsertMaterialize);
+
+        if (upsertMaterialize) {
+            sinkTransform =
+                    applyUpsertMaterialize(
+                            sinkTransform,
+                            primaryKeys,
+                            sinkParallelism,
+                            tableConfig,
+                            physicalRowType);
         }
+
+        return (Transformation<Object>)
+                applySinkProvider(
+                        sinkTransform, env, runtimeProvider, rowtimeFieldIndex, sinkParallelism);
     }
 
     /**
      * Apply an operator to filter or report error to process not-null values for not-null fields.
      */
     private Transformation<RowData> applyNotNullEnforcer(
-            TableConfig config, RowType physicalRowType, Transformation<RowData> inputTransform) {
+            Transformation<RowData> inputTransform, TableConfig config, RowType physicalRowType) {
         final ExecutionConfigOptions.NotNullEnforcer notNullEnforcer =
                 config.getConfiguration()
                         .get(ExecutionConfigOptions.TABLE_EXEC_SINK_NOT_NULL_ENFORCER);
@@ -260,76 +191,135 @@ public abstract class CommonExecSink extends ExecNodeBase<Object>
      * the parallelism is provided, otherwise it uses parallelism of input transformation.
      */
     private int deriveSinkParallelism(
-            ParallelismProvider parallelismProvider, int inputParallelism) {
-        final Optional<Integer> parallelismOptional = parallelismProvider.getParallelism();
-        if (parallelismOptional.isPresent()) {
-            int sinkParallelism = parallelismOptional.get();
-            if (sinkParallelism <= 0) {
-                throw new TableException(
-                        String.format(
-                                "Table: %s configured sink parallelism: "
-                                        + "%s should not be less than zero or equal to zero",
-                                tableSinkSpec.getObjectIdentifier().asSummaryString(),
-                                sinkParallelism));
-            }
-            return sinkParallelism;
-        } else {
-            // use input parallelism if not specified
+            Transformation<RowData> inputTransform, SinkRuntimeProvider runtimeProvider) {
+        final int inputParallelism = inputTransform.getParallelism();
+        if (!(runtimeProvider instanceof ParallelismProvider)) {
             return inputParallelism;
         }
+        final ParallelismProvider parallelismProvider = (ParallelismProvider) runtimeProvider;
+        return parallelismProvider
+                .getParallelism()
+                .map(
+                        sinkParallelism -> {
+                            if (sinkParallelism <= 0) {
+                                throw new TableException(
+                                        String.format(
+                                                "Invalid configured parallelism %s for table '%s'.",
+                                                sinkParallelism,
+                                                tableSinkSpec
+                                                        .getObjectIdentifier()
+                                                        .asSummaryString()));
+                            }
+                            return sinkParallelism;
+                        })
+                .orElse(inputParallelism);
     }
 
     /**
-     * Apply a keyBy partition transformation if the parallelism of sink operator and input operator
-     * is different and sink changelog-mode is not insert-only or requireMaterialize. This is used
-     * to guarantee the strict ordering of changelog messages.
+     * Apply a primary key partition transformation to guarantee the strict ordering of changelog
+     * messages.
      */
-    private Transformation<RowData> applyKeyByIfNeeded(
-            RowType sinkRowType,
-            @Nullable UniqueConstraint primaryKey,
+    private Transformation<RowData> applyKeyBy(
             Transformation<RowData> inputTransform,
-            int inputParallelism,
+            int[] primaryKeys,
             int sinkParallelism,
             boolean upsertMaterialize) {
-        final int[] primaryKeys = getPrimaryKeyIndices(sinkRowType, primaryKey);
+        final int inputParallelism = inputTransform.getParallelism();
         if ((inputParallelism == sinkParallelism || changelogMode.containsOnly(RowKind.INSERT))
                 && !upsertMaterialize) {
-            // if the inputParallelism is equals to the parallelism or insert-only mode, do nothing.
             return inputTransform;
-        } else if (primaryKeys.length == 0) {
+        }
+        if (primaryKeys.length == 0) {
             throw new TableException(
                     String.format(
-                            "Table: %s configured sink parallelism is: %s, while the input parallelism is: "
-                                    + "%s. Since configured parallelism is different from input parallelism and the changelog mode "
-                                    + "contains [%s], which is not INSERT_ONLY mode, primary key is required but no primary key is found",
+                            "The sink for table '%s' has a configured parallelism of %s, while the input parallelism is %s. "
+                                    + "Since the configured parallelism is different from the input's parallelism and "
+                                    + "the changelog mode is not insert-only, a primary key is required but could not "
+                                    + "be found.",
                             tableSinkSpec.getObjectIdentifier().asSummaryString(),
                             sinkParallelism,
-                            inputParallelism,
-                            changelogMode.getContainedKinds().stream()
-                                    .map(Enum::toString)
-                                    .collect(Collectors.joining(","))));
-        } else {
-            // keyBy before sink
-            final RowDataKeySelector selector =
-                    KeySelectorUtil.getRowDataSelector(primaryKeys, getInputTypeInfo());
-            final KeyGroupStreamPartitioner<RowData, RowData> partitioner =
-                    new KeyGroupStreamPartitioner<>(
-                            selector, KeyGroupRangeAssignment.DEFAULT_LOWER_BOUND_MAX_PARALLELISM);
-            Transformation<RowData> partitionedTransform =
-                    new PartitionTransformation<>(inputTransform, partitioner);
-            partitionedTransform.setParallelism(sinkParallelism);
-            return partitionedTransform;
+                            inputParallelism));
         }
+
+        final RowDataKeySelector selector =
+                KeySelectorUtil.getRowDataSelector(primaryKeys, getInputTypeInfo());
+        final KeyGroupStreamPartitioner<RowData, RowData> partitioner =
+                new KeyGroupStreamPartitioner<>(
+                        selector, KeyGroupRangeAssignment.DEFAULT_LOWER_BOUND_MAX_PARALLELISM);
+        Transformation<RowData> partitionedTransform =
+                new PartitionTransformation<>(inputTransform, partitioner);
+        partitionedTransform.setParallelism(sinkParallelism);
+        return partitionedTransform;
+    }
+
+    private Transformation<RowData> applyUpsertMaterialize(
+            Transformation<RowData> inputTransform,
+            int[] primaryKeys,
+            int sinkParallelism,
+            TableConfig tableConfig,
+            RowType physicalRowType) {
+        GeneratedRecordEqualiser equaliser =
+                new EqualiserCodeGenerator(physicalRowType)
+                        .generateRecordEqualiser("SinkMaterializeEqualiser");
+        SinkUpsertMaterializer operator =
+                new SinkUpsertMaterializer(
+                        StateConfigUtil.createTtlConfig(
+                                tableConfig.getIdleStateRetention().toMillis()),
+                        InternalSerializers.create(physicalRowType),
+                        equaliser);
+        OneInputTransformation<RowData, RowData> materializeTransform =
+                new OneInputTransformation<>(
+                        inputTransform,
+                        "SinkMaterializer",
+                        operator,
+                        inputTransform.getOutputType(),
+                        sinkParallelism);
+        RowDataKeySelector keySelector =
+                KeySelectorUtil.getRowDataSelector(
+                        primaryKeys, InternalTypeInfo.of(physicalRowType));
+        materializeTransform.setStateKeySelector(keySelector);
+        materializeTransform.setStateKeyType(keySelector.getProducedType());
+        return materializeTransform;
     }
 
-    private int[] getPrimaryKeyIndices(RowType sinkRowType, @Nullable UniqueConstraint primaryKey) {
-        if (primaryKey == null) {
-            return new int[0];
+    private Transformation<?> applySinkProvider(
+            Transformation<RowData> inputTransform,
+            StreamExecutionEnvironment env,
+            SinkRuntimeProvider runtimeProvider,
+            int rowtimeFieldIndex,
+            int sinkParallelism) {
+        if (runtimeProvider instanceof DataStreamSinkProvider) {
+            final DataStream<RowData> dataStream = new DataStream<>(env, inputTransform);
+            final DataStreamSinkProvider provider = (DataStreamSinkProvider) runtimeProvider;
+            return provider.consumeDataStream(dataStream).getTransformation();
+        } else if (runtimeProvider instanceof TransformationSinkProvider) {
+            final TransformationSinkProvider provider =
+                    (TransformationSinkProvider) runtimeProvider;
+            return provider.createTransformation(
+                    TransformationSinkProvider.Context.of(inputTransform, rowtimeFieldIndex));
+        } else if (runtimeProvider instanceof SinkFunctionProvider) {
+            final SinkFunction<RowData> sinkFunction =
+                    ((SinkFunctionProvider) runtimeProvider).createSinkFunction();
+            return createSinkFunctionTransformation(
+                    sinkFunction, env, inputTransform, rowtimeFieldIndex, sinkParallelism);
+        } else if (runtimeProvider instanceof OutputFormatProvider) {
+            OutputFormat<RowData> outputFormat =
+                    ((OutputFormatProvider) runtimeProvider).createOutputFormat();
+            final SinkFunction<RowData> sinkFunction = new OutputFormatSinkFunction<>(outputFormat);
+            return createSinkFunctionTransformation(
+                    sinkFunction, env, inputTransform, rowtimeFieldIndex, sinkParallelism);
+        } else if (runtimeProvider instanceof SinkProvider) {
+            return new SinkTransformation<>(
+                    inputTransform,
+                    ((SinkProvider) runtimeProvider).createSink(),
+                    getDescription(),
+                    sinkParallelism);
+        } else {
+            throw new TableException("Unsupported sink runtime provider.");
         }
-        return primaryKey.getColumns().stream().mapToInt(sinkRowType::getFieldIndex).toArray();
     }
 
-    private Transformation<Object> createSinkFunctionTransformation(
+    private Transformation<?> createSinkFunctionTransformation(
             SinkFunction<RowData> sinkFunction,
             StreamExecutionEnvironment env,
             Transformation<RowData> inputTransformation,
@@ -353,6 +343,12 @@ public abstract class CommonExecSink extends ExecNodeBase<Object>
         return InternalTypeInfo.of(getInputEdges().get(0).getOutputType());
     }
 
+    private int[] getPrimaryKeyIndices(RowType sinkRowType, ResolvedSchema schema) {
+        return schema.getPrimaryKey()
+                .map(k -> k.getColumns().stream().mapToInt(sinkRowType::getFieldIndex).toArray())
+                .orElse(new int[0]);
+    }
+
     private RowType getPhysicalRowType(ResolvedSchema schema) {
         return (RowType) schema.toPhysicalRowDataType().getLogicalType();
     }
diff --git a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/factories/TestValuesRuntimeFunctions.java b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/factories/TestValuesRuntimeFunctions.java
index 68169c2d849..7d53f456301 100644
--- a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/factories/TestValuesRuntimeFunctions.java
+++ b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/factories/TestValuesRuntimeFunctions.java
@@ -34,12 +34,8 @@ import org.apache.flink.core.memory.DataOutputViewStreamWrapper;
 import org.apache.flink.runtime.state.FunctionInitializationContext;
 import org.apache.flink.runtime.state.FunctionSnapshotContext;
 import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;
-import org.apache.flink.streaming.api.datastream.DataStream;
-import org.apache.flink.streaming.api.datastream.DataStreamSink;
 import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;
 import org.apache.flink.streaming.api.functions.source.SourceFunction;
-import org.apache.flink.table.connector.ParallelismProvider;
-import org.apache.flink.table.connector.sink.DataStreamSinkProvider;
 import org.apache.flink.table.connector.sink.DynamicTableSink.DataStructureConverter;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.data.TimestampData;
@@ -62,7 +58,6 @@ import java.util.HashMap;
 import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
-import java.util.Optional;
 import java.util.concurrent.CompletableFuture;
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.Executors;
@@ -149,29 +144,6 @@ final class TestValuesRuntimeFunctions {
         }
     }
 
-    // ------------------------------------------------------------------------------------------
-    // Specialized test provider implementations
-    // ------------------------------------------------------------------------------------------
-    static class InternalDataStreamSinkProviderWithParallelism
-            implements DataStreamSinkProvider, ParallelismProvider {
-
-        private final Integer parallelism;
-
-        public InternalDataStreamSinkProviderWithParallelism(Integer parallelism) {
-            this.parallelism = parallelism;
-        }
-
-        @Override
-        public DataStreamSink<?> consumeDataStream(DataStream<RowData> dataStream) {
-            throw new UnsupportedOperationException("should not be called");
-        }
-
-        @Override
-        public Optional<Integer> getParallelism() {
-            return Optional.ofNullable(parallelism);
-        }
-    }
-
     // ------------------------------------------------------------------------------------------
     // Source Function implementations
     // ------------------------------------------------------------------------------------------
diff --git a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java
index 29f1c633755..d71787621b1 100644
--- a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java
+++ b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java
@@ -28,6 +28,7 @@ import org.apache.flink.api.java.io.CollectionInputFormat;
 import org.apache.flink.configuration.ConfigOption;
 import org.apache.flink.configuration.ConfigOptions;
 import org.apache.flink.streaming.api.datastream.DataStream;
+import org.apache.flink.streaming.api.datastream.DataStreamSink;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 import org.apache.flink.streaming.api.functions.sink.SinkFunction;
 import org.apache.flink.streaming.api.functions.source.FromElementsFunction;
@@ -1359,14 +1360,21 @@ public final class TestValuesTableFactory
                             }
                         };
                     case "DataStream":
-                        return (DataStreamSinkProvider)
-                                dataStream ->
-                                        dataStream.addSink(
-                                                new AppendingSinkFunction(
-                                                        tableName, converter, rowtimeIndex));
-                    case "DataStreamWithParallelism":
-                        return new TestValuesRuntimeFunctions
-                                .InternalDataStreamSinkProviderWithParallelism(1);
+                        return new DataStreamSinkProvider() {
+                            @Override
+                            public DataStreamSink<?> consumeDataStream(
+                                    DataStream<RowData> dataStream) {
+                                return dataStream.addSink(
+                                        new AppendingSinkFunction(
+                                                tableName, converter, rowtimeIndex));
+                            }
+
+                            @Override
+                            public Optional<Integer> getParallelism() {
+                                return parallelismOption;
+                            }
+                        };
+
                     default:
                         throw new IllegalArgumentException(
                                 "Unsupported runtime sink class: " + runtimeSink);
diff --git a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/runtime/stream/sql/DataStreamJavaITCase.java b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/runtime/stream/sql/DataStreamJavaITCase.java
index 17f488cdfa9..623c0c0ecee 100644
--- a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/runtime/stream/sql/DataStreamJavaITCase.java
+++ b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/runtime/stream/sql/DataStreamJavaITCase.java
@@ -71,6 +71,7 @@ import java.time.DayOfWeek;
 import java.time.LocalDateTime;
 import java.time.ZoneId;
 import java.time.ZoneOffset;
+import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.List;
@@ -638,6 +639,80 @@ public class DataStreamJavaITCase extends AbstractTestBase {
                 containsInAnyOrder("+I[1]", "+I[2]", "+I[3]"));
     }
 
+    @Test
+    public void testMultiChangelogStreamUpsert() throws Exception {
+        final StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);
+
+        createTableFromElements(
+                tableEnv,
+                "T1",
+                ChangelogMode.insertOnly(),
+                Schema.newBuilder()
+                        .column("pk", "INT NOT NULL")
+                        .column("x", "STRING NOT NULL")
+                        .primaryKey("pk")
+                        .build(),
+                Arrays.asList(Types.INT, Types.STRING),
+                Row.ofKind(RowKind.INSERT, 1, "1"),
+                Row.ofKind(RowKind.INSERT, 2, "2"));
+
+        createTableFromElements(
+                tableEnv,
+                "T2",
+                ChangelogMode.upsert(),
+                Schema.newBuilder()
+                        .column("pk", "INT NOT NULL")
+                        .column("y", "STRING NOT NULL")
+                        .column("some_value", "DOUBLE NOT NULL")
+                        .primaryKey("pk")
+                        .build(),
+                Arrays.asList(Types.INT, Types.STRING, Types.DOUBLE),
+                Row.ofKind(RowKind.INSERT, 1, "A", 1.0),
+                Row.ofKind(RowKind.INSERT, 2, "B", 2.0),
+                Row.ofKind(RowKind.UPDATE_AFTER, 1, "A", 1.1),
+                Row.ofKind(RowKind.UPDATE_AFTER, 2, "B", 2.1));
+
+        createTableFromElements(
+                tableEnv,
+                "T3",
+                ChangelogMode.insertOnly(),
+                Schema.newBuilder()
+                        .column("pk1", "STRING NOT NULL")
+                        .column("pk2", "STRING NOT NULL")
+                        .column("some_other_value", "DOUBLE NOT NULL")
+                        .primaryKey("pk1", "pk2")
+                        .build(),
+                Arrays.asList(Types.STRING, Types.STRING, Types.DOUBLE),
+                Row.ofKind(RowKind.INSERT, "1", "A", 10.0),
+                Row.ofKind(RowKind.INSERT, "1", "B", 11.0));
+
+        final Table resultTable =
+                tableEnv.sqlQuery(
+                        "SELECT\n"
+                                + "T1.pk,\n"
+                                + "T2.some_value * T3.some_other_value,\n"
+                                + "T3.pk1,\n"
+                                + "T3.pk2\n"
+                                + "FROM T1\n"
+                                + "LEFT JOIN T2 on T1.pk = T2.pk\n"
+                                + "LEFT JOIN T3 ON T1.x = T3.pk1 AND T2.y = T3.pk2");
+
+        final DataStream<Row> resultStream =
+                tableEnv.toChangelogStream(
+                        resultTable,
+                        Schema.newBuilder()
+                                .column("pk", "INT NOT NULL")
+                                .column("some_calculated_value", "DOUBLE")
+                                .column("pk1", "STRING")
+                                .column("pk2", "STRING")
+                                .primaryKey("pk")
+                                .build(),
+                        ChangelogMode.upsert());
+
+        testMaterializedResult(
+                resultStream, 0, Row.of(2, null, null, null), Row.of(1, 11.0, "1", "A"));
+    }
+
     // --------------------------------------------------------------------------------------------
     // Helper methods
     // --------------------------------------------------------------------------------------------
@@ -731,6 +806,24 @@ public class DataStreamJavaITCase extends AbstractTestBase {
                 .toArray(Row[]::new);
     }
 
+    private void createTableFromElements(
+            StreamTableEnvironment tableEnv,
+            String name,
+            ChangelogMode changelogMode,
+            Schema schema,
+            List<TypeInformation<?>> fieldTypeInfo,
+            Row... elements) {
+        final String[] fieldNames =
+                schema.getColumns().stream()
+                        .map(Schema.UnresolvedColumn::getName)
+                        .toArray(String[]::new);
+        final TypeInformation<?>[] fieldTypes = fieldTypeInfo.toArray(new TypeInformation[0]);
+        final DataStream<Row> dataStream =
+                env.fromElements(elements).returns(Types.ROW_NAMED(fieldNames, fieldTypes));
+        final Table table = tableEnv.fromChangelogStream(dataStream, schema, changelogMode);
+        tableEnv.createTemporaryView(name, table);
+    }
+
     private static void testSchema(Table table, Column... expectedColumns) {
         assertEquals(ResolvedSchema.of(expectedColumns), table.getResolvedSchema());
     }
@@ -757,6 +850,36 @@ public class DataStreamJavaITCase extends AbstractTestBase {
         }
     }
 
+    private static void testMaterializedResult(
+            DataStream<Row> dataStream, int primaryKeyPos, Row... expectedResult) throws Exception {
+        try (CloseableIterator<Row> iterator = dataStream.executeAndCollect()) {
+            final List<Row> materializedResult = new ArrayList<>();
+            iterator.forEachRemaining(
+                    row -> {
+                        final RowKind kind = row.getKind();
+                        row.setKind(RowKind.INSERT);
+                        switch (kind) {
+                            case UPDATE_BEFORE:
+                                materializedResult.remove(row);
+                                break;
+                            case INSERT: // temporary solution for FLINK-24054
+                            case UPDATE_AFTER:
+                                final Object primaryKeyValue = row.getField(primaryKeyPos);
+                                assert primaryKeyValue != null;
+                                materializedResult.removeIf(
+                                        r -> primaryKeyValue.equals(r.getField(primaryKeyPos)));
+                                materializedResult.add(row);
+                                break;
+                            case DELETE:
+                                row.setKind(RowKind.INSERT);
+                                materializedResult.remove(row);
+                                break;
+                        }
+                    });
+            assertThat(materializedResult, containsInAnyOrder(expectedResult));
+        }
+    }
+
     // --------------------------------------------------------------------------------------------
     // Helper classes
     // --------------------------------------------------------------------------------------------
diff --git a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/runtime/stream/table/TableSinkITCase.scala b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/runtime/stream/table/TableSinkITCase.scala
index cd7b6462b97..6399c29c578 100644
--- a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/runtime/stream/table/TableSinkITCase.scala
+++ b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/runtime/stream/table/TableSinkITCase.scala
@@ -680,24 +680,6 @@ class TableSinkITCase extends StreamingTestBase {
     assertEquals(expected.sorted, result.sorted)
   }
 
-
-  @Test
-  def testParallelismWithDataStream(): Unit = {
-    Try(innerTestSetParallelism(
-      "DataStreamWithParallelism",
-      1,
-      index = 1)) match {
-      case Success(_) => fail("this should not happen")
-      case Failure(t) => {
-        val exception = ExceptionUtils.findThrowableWithMessage(
-          t,
-          "`DataStreamSinkProvider` is not allowed to work with `ParallelismProvider`," +
-            " please see document of `ParallelismProvider`")
-        assertTrue(exception.isPresent)
-      }
-    }
-  }
-
   @Test
   def testParallelismWithSinkFunction(): Unit = {
     val negativeParallelism = -1
@@ -710,12 +692,11 @@ class TableSinkITCase extends StreamingTestBase {
       index = index.getAndIncrement))
     match {
       case Success(_) => fail("this should not happen")
-      case Failure(t) => {
+      case Failure(t) =>
         val exception = ExceptionUtils.findThrowableWithMessage(
           t,
-          s"should not be less than zero or equal to zero")
+          s"Invalid configured parallelism")
         assertTrue(exception.isPresent)
-      }
     }
 
     assertTrue(Try(innerTestSetParallelism(
@@ -724,33 +705,6 @@ class TableSinkITCase extends StreamingTestBase {
       index = index.getAndIncrement)).isSuccess)
   }
 
-  @Test
-  def testParallelismWithOutputFormat(): Unit = {
-    val negativeParallelism = -1
-    val validParallelism = 1
-    val index = new AtomicInteger(1)
-
-    Try(innerTestSetParallelism(
-      "OutputFormat",
-      negativeParallelism,
-      index = index.getAndIncrement))
-    match {
-      case Success(_) => fail("this should not happen")
-      case Failure(t) => {
-        val exception = ExceptionUtils.findThrowableWithMessage(
-          t,
-          s"should not be less than zero or equal to zero")
-        assertTrue(exception.isPresent)
-      }
-    }
-
-    assertTrue(Try(innerTestSetParallelism(
-      "SinkFunction",
-      validParallelism,
-      index = index.getAndIncrement))
-      .isSuccess)
-  }
-
   @Test
   def testParallelismOnChangelogMode():Unit = {
     val dataId = TestValuesTableFactory.registerData(data1)
@@ -791,7 +745,7 @@ class TableSinkITCase extends StreamingTestBase {
         val exception = ExceptionUtils
           .findThrowableWithMessage(
             e,
-            "primary key is required but no primary key is found")
+            "a primary key is required")
         assertTrue(exception.isPresent)
     }
 
