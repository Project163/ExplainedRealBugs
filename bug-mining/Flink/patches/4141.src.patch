diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java
index 35e355c4fd4..a4e30123207 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java
@@ -182,7 +182,7 @@ public class HiveTableSink implements AppendStreamTableSink, PartitionableTableS
 				TableBucketAssigner assigner = new TableBucketAssigner(partComputer);
 				TableRollingPolicy rollingPolicy = new TableRollingPolicy(
 						true,
-						conf.get(SINK_ROLLING_POLICY_FILE_SIZE),
+						conf.get(SINK_ROLLING_POLICY_FILE_SIZE).getBytes(),
 						conf.get(SINK_ROLLING_POLICY_TIME_INTERVAL).toMillis());
 
 				Optional<BulkWriter.Factory<RowData>> bulkFactory = createBulkWriterFactory(partitionColumns, sd);
diff --git a/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvFilesystemStreamITCase.java b/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvFilesystemStreamITCase.java
index 5f1069fe2ee..976e70e6ab0 100644
--- a/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvFilesystemStreamITCase.java
+++ b/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvFilesystemStreamITCase.java
@@ -32,7 +32,7 @@ public class CsvFilesystemStreamITCase extends FsStreamingSinkITCaseBase {
 		List<String> ret = new ArrayList<>();
 		ret.add("'format'='csv'");
 		// for test purpose
-		ret.add("'sink.rolling-policy.file-size'='1'");
+		ret.add("'sink.rolling-policy.file-size'='1b'");
 		return ret.toArray(new String[0]);
 	}
 }
diff --git a/flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/JsonFsStreamSinkITCase.java b/flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/JsonFsStreamSinkITCase.java
index 7690299cd3f..f2cd6d4d06b 100644
--- a/flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/JsonFsStreamSinkITCase.java
+++ b/flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/JsonFsStreamSinkITCase.java
@@ -33,7 +33,7 @@ public class JsonFsStreamSinkITCase extends FsStreamingSinkITCaseBase {
 		List<String> ret = new ArrayList<>();
 		ret.add("'format'='json'");
 		// for test purpose
-		ret.add("'sink.rolling-policy.file-size'='1'");
+		ret.add("'sink.rolling-policy.file-size'='1b'");
 		return ret.toArray(new String[0]);
 	}
 }
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/FsStreamingSinkITCaseBase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/FsStreamingSinkITCaseBase.scala
index 02b70b45cad..8564e4b2b38 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/FsStreamingSinkITCaseBase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/FsStreamingSinkITCaseBase.scala
@@ -79,12 +79,12 @@ abstract class FsStreamingSinkITCaseBase extends StreamingTestBase {
 
   @Test
   def testNonPart(): Unit = {
-    test(false)
+    test(partition = false)
   }
 
   @Test
   def testPart(): Unit = {
-    test(true)
+    test(partition = true)
     val basePath = new File(new URI(resultPath).getPath, "d=2020-05-03")
     Assert.assertEquals(5, basePath.list().length)
     Assert.assertTrue(new File(new File(basePath, "e=7"), "_MY_SUCCESS").exists())
@@ -94,7 +94,7 @@ abstract class FsStreamingSinkITCaseBase extends StreamingTestBase {
     Assert.assertTrue(new File(new File(basePath, "e=11"), "_MY_SUCCESS").exists())
   }
 
-  private def test(partition: Boolean): Unit = {
+  private def test(partition: Boolean, policy: String = "success-file"): Unit = {
     val dollar = '$'
     val ddl = s"""
                  |create table sink_table (
@@ -111,7 +111,7 @@ abstract class FsStreamingSinkITCaseBase extends StreamingTestBase {
                  |  '${PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN.key()}' =
                  |      '${dollar}d ${dollar}e:00:00',
                  |  '${SINK_PARTITION_COMMIT_DELAY.key()}' = '1h',
-                 |  '${SINK_PARTITION_COMMIT_POLICY_KIND.key()}' = 'success-file',
+                 |  '${SINK_PARTITION_COMMIT_POLICY_KIND.key()}' = '$policy',
                  |  '${SINK_PARTITION_COMMIT_SUCCESS_FILE_NAME.key()}' = '_MY_SUCCESS',
                  |  ${additionalProperties().mkString(",\n")}
                  |)
@@ -129,6 +129,14 @@ abstract class FsStreamingSinkITCaseBase extends StreamingTestBase {
       data)
   }
 
+  @Test
+  def testMetastorePolicy(): Unit = {
+    thrown.expectMessage(
+      "Can not configure a 'metastore' partition commit policy for a file system table." +
+          " You can only configure 'metastore' partition commit policy for a hive table.")
+    test(partition = true, "metastore")
+  }
+
   def check(ddl: String, sqlQuery: String, expectedResult: Seq[Row]): Unit = {
     val setting = EnvironmentSettings.newInstance().useBlinkPlanner().inBatchMode().build()
     val tEnv = TableEnvironment.create(setting)
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/FsStreamingSinkTestCsvITCase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/FsStreamingSinkTestCsvITCase.scala
index 62d0f95ef84..42ec923af8b 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/FsStreamingSinkTestCsvITCase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/FsStreamingSinkTestCsvITCase.scala
@@ -36,7 +36,7 @@ class FsStreamingSinkTestCsvITCase(useBulkWriter: Boolean) extends FsStreamingSi
         Seq(
           "'format' = 'testcsv'",
           s"'testcsv.use-bulk-writer' = '$useBulkWriter'") ++
-        (if (useBulkWriter) Seq() else Seq("'sink.rolling-policy.file-size' = '1'"))
+        (if (useBulkWriter) Seq() else Seq("'sink.rolling-policy.file-size' = '1b'"))
   }
 }
 
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/EmptyMetaStoreFactory.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/EmptyMetaStoreFactory.java
new file mode 100644
index 00000000000..6eed415cf9c
--- /dev/null
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/EmptyMetaStoreFactory.java
@@ -0,0 +1,65 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.filesystem;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.core.fs.Path;
+
+import java.util.LinkedHashMap;
+import java.util.Optional;
+
+/**
+ * Empty implementation {@link TableMetaStoreFactory}.
+ */
+@Internal
+public class EmptyMetaStoreFactory implements TableMetaStoreFactory {
+
+	private final Path path;
+
+	public EmptyMetaStoreFactory(Path path) {
+		this.path = path;
+	}
+
+	@Override
+	public TableMetaStore createTableMetaStore() {
+		return new TableMetaStore() {
+
+			@Override
+			public Path getLocationPath() {
+				return path;
+			}
+
+			@Override
+			public Optional<Path> getPartition(LinkedHashMap<String, String> partitionSpec) {
+				return Optional.empty();
+			}
+
+			@Override
+			public void createOrAlterPartition(
+					LinkedHashMap<String, String> partitionSpec,
+					Path partitionPath) {
+			}
+
+			@Override
+			public void close() {
+
+			}
+		};
+	}
+}
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java
index 0d9739fcb4e..eecdfe2dabf 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java
@@ -19,6 +19,7 @@
 package org.apache.flink.table.filesystem;
 
 import org.apache.flink.configuration.ConfigOption;
+import org.apache.flink.configuration.MemorySize;
 
 import java.time.Duration;
 
@@ -40,9 +41,9 @@ public class FileSystemOptions {
 			.withDescription("The default partition name in case the dynamic partition" +
 					" column value is null/empty string");
 
-	public static final ConfigOption<Long> SINK_ROLLING_POLICY_FILE_SIZE = key("sink.rolling-policy.file-size")
-			.longType()
-			.defaultValue(1024L * 1024L * 128L)
+	public static final ConfigOption<MemorySize> SINK_ROLLING_POLICY_FILE_SIZE = key("sink.rolling-policy.file-size")
+			.memoryType()
+			.defaultValue(MemorySize.ofMebiBytes(128))
 			.withDescription("The maximum part file size before rolling (by default 128MB).");
 
 	public static final ConfigOption<Duration> SINK_ROLLING_POLICY_TIME_INTERVAL = key("sink.rolling-policy.time-interval")
@@ -132,12 +133,14 @@ public class FileSystemOptions {
 	public static final ConfigOption<String> SINK_PARTITION_COMMIT_TRIGGER =
 			key("sink.partition-commit.trigger")
 					.stringType()
-					.defaultValue("partition-time")
-					.withDescription("Trigger type for partition commit:" +
-							" 'partition-time': extract time from partition," +
-							" if 'watermark' > 'partition-time' + 'delay', will commit the partition." +
-							" 'process-time': use processing time, if 'current processing time' > " +
-							"'partition directory creation time' + 'delay', will commit the partition.");
+					.defaultValue("process-time")
+					.withDescription("Trigger type for partition commit:\n" +
+							" 'process-time': based on the time of the machine, it neither requires" +
+							" partition time extraction nor watermark generation. Commit partition" +
+							" once the 'current system time' passes 'partition creation system time' plus 'delay'.\n" +
+							" 'partition-time': based on the time that extracted from partition values," +
+							" it requires watermark generation. Commit partition once the 'watermark'" +
+							" passes 'time extracted from partition values' plus 'delay'.");
 
 	public static final ConfigOption<Duration> SINK_PARTITION_COMMIT_DELAY =
 			key("sink.partition-commit.delay")
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java
index 64081454dde..977aa3f9234 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java
@@ -125,7 +125,7 @@ public class FileSystemTableSink implements
 				schema.getFieldDataTypes(),
 				partitionKeys.toArray(new String[0]));
 
-		TableMetaStoreFactory metaStoreFactory = createTableMetaStoreFactory(path);
+		EmptyMetaStoreFactory metaStoreFactory = new EmptyMetaStoreFactory(path);
 
 		if (isBounded) {
 			FileSystemOutputFormat.Builder<RowData> builder = new FileSystemOutputFormat.Builder<>();
@@ -146,7 +146,7 @@ public class FileSystemTableSink implements
 			TableBucketAssigner assigner = new TableBucketAssigner(computer);
 			TableRollingPolicy rollingPolicy = new TableRollingPolicy(
 					!(writer instanceof Encoder),
-					conf.get(SINK_ROLLING_POLICY_FILE_SIZE),
+					conf.get(SINK_ROLLING_POLICY_FILE_SIZE).getBytes(),
 					conf.get(SINK_ROLLING_POLICY_TIME_INTERVAL).toMillis());
 
 			BucketsBuilder<RowData, String, ? extends BucketsBuilder<RowData, ?, ?>> bucketsBuilder;
@@ -331,29 +331,6 @@ public class FileSystemTableSink implements
 		};
 	}
 
-	private static TableMetaStoreFactory createTableMetaStoreFactory(Path path) {
-		return (TableMetaStoreFactory) () -> new TableMetaStoreFactory.TableMetaStore() {
-
-			@Override
-			public Path getLocationPath() {
-				return path;
-			}
-
-			@Override
-			public Optional<Path> getPartition(LinkedHashMap<String, String> partitionSpec) {
-				return Optional.empty();
-			}
-
-			@Override
-			public void createOrAlterPartition(LinkedHashMap<String, String> partitionSpec, Path partitionPath) throws Exception {
-			}
-
-			@Override
-			public void close() {
-			}
-		};
-	}
-
 	@Override
 	public FileSystemTableSink configure(String[] fieldNames, TypeInformation<?>[] fieldTypes) {
 		return this;
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/PartitionCommitPolicy.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/PartitionCommitPolicy.java
index 93e076a1d85..9074b7f5990 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/PartitionCommitPolicy.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/PartitionCommitPolicy.java
@@ -21,6 +21,7 @@ package org.apache.flink.table.filesystem;
 import org.apache.flink.annotation.Experimental;
 import org.apache.flink.core.fs.FileSystem;
 import org.apache.flink.core.fs.Path;
+import org.apache.flink.table.api.ValidationException;
 
 import java.util.Arrays;
 import java.util.Collections;
@@ -104,7 +105,7 @@ public interface PartitionCommitPolicy {
 		}
 		String[] policyStrings = policyKind.split(",");
 		return Arrays.stream(policyStrings).map(name -> {
-			switch (name) {
+			switch (name.toLowerCase()) {
 				case METASTORE:
 					return new MetastoreCommitPolicy();
 				case SUCCESS_FILE:
@@ -121,4 +122,20 @@ public interface PartitionCommitPolicy {
 			}
 		}).collect(Collectors.toList());
 	}
+
+	/**
+	 * Validate commit policy.
+	 */
+	static void validatePolicyChain(boolean isEmptyMetastore, String policyKind) {
+		if (policyKind != null) {
+			String[] policyStrings = policyKind.split(",");
+			for (String policy : policyStrings) {
+				if (isEmptyMetastore && METASTORE.equalsIgnoreCase(policy)) {
+					throw new ValidationException("Can not configure a 'metastore' partition commit" +
+							" policy for a file system table. You can only configure 'metastore'" +
+							" partition commit policy for a hive table.");
+				}
+			}
+		}
+	}
 }
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/TableMetaStoreFactory.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/TableMetaStoreFactory.java
index a23c19227ee..8af28b5e3b8 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/TableMetaStoreFactory.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/TableMetaStoreFactory.java
@@ -47,6 +47,7 @@ public interface TableMetaStoreFactory extends Serializable {
 		/**
 		 * Get base location path of this table.
 		 */
+		@Deprecated
 		Path getLocationPath();
 
 		/**
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/StreamingFileCommitter.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/StreamingFileCommitter.java
index 2ae18298724..d8aa2831ce8 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/StreamingFileCommitter.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/StreamingFileCommitter.java
@@ -28,6 +28,7 @@ import org.apache.flink.streaming.api.operators.OneInputStreamOperator;
 import org.apache.flink.streaming.api.watermark.Watermark;
 import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
 import org.apache.flink.table.catalog.ObjectIdentifier;
+import org.apache.flink.table.filesystem.EmptyMetaStoreFactory;
 import org.apache.flink.table.filesystem.MetastoreCommitPolicy;
 import org.apache.flink.table.filesystem.PartitionCommitPolicy;
 import org.apache.flink.table.filesystem.TableMetaStoreFactory;
@@ -93,6 +94,9 @@ public class StreamingFileCommitter extends AbstractStreamOperator<Void>
 		this.partitionKeys = partitionKeys;
 		this.metaStoreFactory = metaStoreFactory;
 		this.conf = conf;
+		PartitionCommitPolicy.validatePolicyChain(
+				metaStoreFactory instanceof EmptyMetaStoreFactory,
+				conf.get(SINK_PARTITION_COMMIT_POLICY_KIND));
 	}
 
 	@Override
@@ -144,7 +148,7 @@ public class StreamingFileCommitter extends AbstractStreamOperator<Void>
 		try (TableMetaStoreFactory.TableMetaStore metaStore = metaStoreFactory.createTableMetaStore()) {
 			for (String partition : partitions) {
 				LinkedHashMap<String, String> partSpec = extractPartitionSpecFromPath(new Path(partition));
-				Path path = new Path(metaStore.getLocationPath(), generatePartitionPath(partSpec));
+				Path path = new Path(locationPath, generatePartitionPath(partSpec));
 				PartitionCommitPolicy.Context context = new PolicyContext(
 						new ArrayList<>(partSpec.values()), path);
 				for (PartitionCommitPolicy policy : policies) {
