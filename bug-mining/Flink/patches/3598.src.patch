diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShim.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShim.java
index 4a8ff3e4a03..cd6bc0bc14b 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShim.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShim.java
@@ -218,7 +218,7 @@ public interface HiveShim extends Serializable {
 	/**
 	 * Converts a Flink timestamp instance to what's expected by Hive.
 	 */
-	Object toHiveTimestamp(Object flinkTimestamp);
+	@Nullable Object toHiveTimestamp(@Nullable Object flinkTimestamp);
 
 	/**
 	 * Converts a hive timestamp instance to LocalDateTime which is expected by DataFormatConverter.
@@ -228,7 +228,7 @@ public interface HiveShim extends Serializable {
 	/**
 	 * Converts a Flink date instance to what's expected by Hive.
 	 */
-	Object toHiveDate(Object flinkDate);
+	@Nullable Object toHiveDate(@Nullable Object flinkDate);
 
 	/**
 	 * Converts a hive date instance to LocalDate which is expected by DataFormatConverter.
diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShimV100.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShimV100.java
index cf290346f87..d1d7c5568b9 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShimV100.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShimV100.java
@@ -383,6 +383,9 @@ public class HiveShimV100 implements HiveShim {
 
 	@Override
 	public Object toHiveTimestamp(Object flinkTimestamp) {
+		if (flinkTimestamp == null) {
+			return null;
+		}
 		ensureSupportedFlinkTimestamp(flinkTimestamp);
 		return flinkTimestamp instanceof Timestamp ? flinkTimestamp : Timestamp.valueOf((LocalDateTime) flinkTimestamp);
 	}
@@ -397,6 +400,9 @@ public class HiveShimV100 implements HiveShim {
 
 	@Override
 	public Object toHiveDate(Object flinkDate) {
+		if (flinkDate == null) {
+			return null;
+		}
 		ensureSupportedFlinkDate(flinkDate);
 		return flinkDate instanceof Date ? flinkDate : Date.valueOf((LocalDate) flinkDate);
 	}
diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShimV310.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShimV310.java
index 7da558afd67..43cc79a852d 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShimV310.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShimV310.java
@@ -190,6 +190,9 @@ public class HiveShimV310 extends HiveShimV235 {
 
 	@Override
 	public Object toHiveTimestamp(Object flinkTimestamp) {
+		if (flinkTimestamp == null) {
+			return null;
+		}
 		ensureSupportedFlinkTimestamp(flinkTimestamp);
 		initDateTimeClasses();
 		if (flinkTimestamp instanceof Timestamp) {
@@ -217,6 +220,9 @@ public class HiveShimV310 extends HiveShimV235 {
 
 	@Override
 	public Object toHiveDate(Object flinkDate) {
+		if (flinkDate == null) {
+			return null;
+		}
 		ensureSupportedFlinkDate(flinkDate);
 		initDateTimeClasses();
 		if (flinkDate instanceof Date) {
diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/functions/hive/conversion/HiveInspectors.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/functions/hive/conversion/HiveInspectors.java
index fb93870cc50..1e1a9ccb3ac 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/functions/hive/conversion/HiveInspectors.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/functions/hive/conversion/HiveInspectors.java
@@ -145,14 +145,16 @@ public class HiveInspectors {
 			} else if (inspector instanceof TimestampObjectInspector) {
 				conversion = hiveShim::toHiveTimestamp;
 			} else if (inspector instanceof HiveCharObjectInspector) {
-				conversion = o -> new HiveChar((String) o, ((CharType) dataType).getLength());
+				conversion = o -> o == null ? null : new HiveChar((String) o, ((CharType) dataType).getLength());
 			} else if (inspector instanceof HiveVarcharObjectInspector) {
-				conversion = o -> new HiveVarchar((String) o, ((VarCharType) dataType).getLength());
+				conversion = o -> o == null ? null : new HiveVarchar((String) o, ((VarCharType) dataType).getLength());
 			} else if (inspector instanceof HiveDecimalObjectInspector) {
 				conversion = o -> o == null ? null : HiveDecimal.create((BigDecimal) o);
 			} else {
 				throw new FlinkHiveUDFException("Unsupported primitive object inspector " + inspector.getClass().getName());
 			}
+			// if the object inspector prefers Writable objects, we should add an extra conversion for that
+			// currently this happens for constant arguments for UDFs
 			if (((PrimitiveObjectInspector) inspector).preferWritable()) {
 				conversion = new WritableHiveObjectConversion(conversion, hiveShim);
 			}
@@ -164,6 +166,9 @@ public class HiveInspectors {
 				((ListObjectInspector) inspector).getListElementObjectInspector(),
 				((ArrayType) dataType).getElementType(), hiveShim);
 			return o -> {
+				if (o == null) {
+					return null;
+				}
 				Object[] array = (Object[]) o;
 				List<Object> result = new ArrayList<>();
 
@@ -184,6 +189,9 @@ public class HiveInspectors {
 				getConversion(mapInspector.getMapValueObjectInspector(), kvType.getValueType(), hiveShim);
 
 			return o -> {
+				if (o == null) {
+					return null;
+				}
 				Map<Object, Object> map = (Map) o;
 				Map<Object, Object> result = new HashMap<>(map.size());
 
@@ -209,6 +217,9 @@ public class HiveInspectors {
 			}
 
 			return o -> {
+				if (o == null) {
+					return null;
+				}
 				Row row = (Row) o;
 				List<Object> result = new ArrayList<>(row.getArity());
 				for (int i = 0; i < row.getArity(); i++) {
diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/functions/hive/conversion/HiveObjectConversion.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/functions/hive/conversion/HiveObjectConversion.java
index 6058b921031..500c14678e2 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/functions/hive/conversion/HiveObjectConversion.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/functions/hive/conversion/HiveObjectConversion.java
@@ -20,6 +20,8 @@ package org.apache.flink.table.functions.hive.conversion;
 
 import org.apache.flink.annotation.Internal;
 
+import javax.annotation.Nullable;
+
 import java.io.Serializable;
 
 /**
@@ -29,5 +31,5 @@ import java.io.Serializable;
 @Internal
 public interface HiveObjectConversion extends Serializable {
 
-	Object toHiveObject(Object o);
+	@Nullable Object toHiveObject(@Nullable Object o);
 }
diff --git a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSinkTest.java b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSinkTest.java
index 6978d548cee..718a499d822 100644
--- a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSinkTest.java
+++ b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSinkTest.java
@@ -187,6 +187,35 @@ public class HiveTableSinkTest {
 		hiveCatalog.dropTable(tablePath, false);
 	}
 
+	@Test
+	public void testWriteNullValues() throws Exception {
+		hiveShell.execute("create database db1");
+		try {
+			// 17 data types
+			hiveShell.execute("create table db1.src" +
+					"(t tinyint,s smallint,i int,b bigint,f float,d double,de decimal(10,5),ts timestamp,dt date," +
+					"str string,ch char(5),vch varchar(8),bl boolean,bin binary,arr array<int>,mp map<int,string>,strt struct<f1:int,f2:string>)");
+			HiveTestUtils.createTextTableInserter(hiveShell, "db1", "src")
+					.addRow(new Object[]{null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null})
+					.commit();
+			hiveShell.execute("create table db1.dest like db1.src");
+			TableEnvironment tableEnv = HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode();
+			tableEnv.registerCatalog(hiveCatalog.getName(), hiveCatalog);
+			tableEnv.useCatalog(hiveCatalog.getName());
+
+			tableEnv.sqlUpdate("insert into db1.dest select * from db1.src");
+			tableEnv.execute("write to dest");
+			List<String> results = hiveShell.executeQuery("select * from db1.dest");
+			assertEquals(1, results.size());
+			String[] cols = results.get(0).split("\t");
+			assertEquals(17, cols.length);
+			assertEquals("NULL", cols[0]);
+			assertEquals(1, new HashSet<>(Arrays.asList(cols)).size());
+		} finally {
+			hiveShell.execute("drop database db1 cascade");
+		}
+	}
+
 	private RowTypeInfo createHiveDestTable(String dbName, String tblName, TableSchema tableSchema, int numPartCols) throws Exception {
 		CatalogTable catalogTable = createHiveCatalogTable(tableSchema, numPartCols);
 		hiveCatalog.createTable(new ObjectPath(dbName, tblName), catalogTable, false);
diff --git a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/catalog/hive/HiveTestUtils.java b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/catalog/hive/HiveTestUtils.java
index d187cd519ed..88184fc9d05 100644
--- a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/catalog/hive/HiveTestUtils.java
+++ b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/catalog/hive/HiveTestUtils.java
@@ -159,6 +159,8 @@ public class HiveTestUtils {
 						}
 						writer.write(toText(rows.get(i)));
 					}
+					// new line at the end of file
+					writer.newLine();
 				}
 				String load = String.format("load data local inpath '%s' into table %s.%s", file.getAbsolutePath(), dbName, tableName);
 				if (partitionSpec != null) {
@@ -178,7 +180,7 @@ public class HiveTestUtils {
 				}
 				String colStr = toText(col);
 				if (colStr != null) {
-					builder.append(toText(col));
+					builder.append(colStr);
 				}
 			}
 			return builder.toString();
