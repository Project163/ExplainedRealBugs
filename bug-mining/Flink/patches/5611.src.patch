diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java b/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java
index f46f1ae797c..4f484ac9bdf 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java
@@ -969,19 +969,11 @@ public class Execution
     }
 
     private void finishPartitionsAndUpdateConsumers() {
-        final List<IntermediateResultPartition> newlyFinishedResults =
+        final List<IntermediateResultPartition> finishedPartitions =
                 getVertex().finishAllBlockingPartitions();
-        if (newlyFinishedResults.isEmpty()) {
-            return;
-        }
 
-        for (IntermediateResultPartition finishedPartition : newlyFinishedResults) {
-            final IntermediateResultPartition[] allPartitionsOfNewlyFinishedResults =
-                    finishedPartition.getIntermediateResult().getPartitions();
-
-            for (IntermediateResultPartition partition : allPartitionsOfNewlyFinishedResults) {
-                updatePartitionConsumers(partition);
-            }
+        for (IntermediateResultPartition partition : finishedPartitions) {
+            updatePartitionConsumers(partition);
         }
     }
 
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionVertex.java b/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionVertex.java
index 032e3d0b772..2c1e16096e9 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionVertex.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionVertex.java
@@ -491,13 +491,17 @@ public class ExecutionVertex
     }
 
     /** Returns all blocking result partitions whose receivers can be scheduled/updated. */
-    List<IntermediateResultPartition> finishAllBlockingPartitions() {
+    @VisibleForTesting
+    public List<IntermediateResultPartition> finishAllBlockingPartitions() {
         List<IntermediateResultPartition> finishedBlockingPartitions = null;
 
         for (IntermediateResultPartition partition : resultPartitions.values()) {
-            if (partition.getResultType().isBlocking() && partition.markFinished()) {
+            if (partition.getResultType().isBlocking()) {
+
+                partition.markFinished();
+
                 if (finishedBlockingPartitions == null) {
-                    finishedBlockingPartitions = new LinkedList<IntermediateResultPartition>();
+                    finishedBlockingPartitions = new LinkedList<>();
                 }
 
                 finishedBlockingPartitions.add(partition);
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/IntermediateResult.java b/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/IntermediateResult.java
index 988dfd87e1d..da35ec9e3c6 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/IntermediateResult.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/IntermediateResult.java
@@ -24,7 +24,6 @@ import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;
 import org.apache.flink.runtime.jobgraph.IntermediateResultPartitionID;
 
 import java.util.HashMap;
-import java.util.concurrent.atomic.AtomicInteger;
 
 import static org.apache.flink.util.Preconditions.checkArgument;
 import static org.apache.flink.util.Preconditions.checkNotNull;
@@ -47,8 +46,6 @@ public class IntermediateResult {
 
     private final int numParallelProducers;
 
-    private final AtomicInteger numberOfRunningProducers;
-
     private int partitionsAssigned;
 
     private final int connectionIndex;
@@ -69,8 +66,6 @@ public class IntermediateResult {
 
         this.partitions = new IntermediateResultPartition[numParallelProducers];
 
-        this.numberOfRunningProducers = new AtomicInteger(numParallelProducers);
-
         // we do not set the intermediate result partitions here, because we let them be initialized
         // by
         // the execution vertex that produces them
@@ -154,23 +149,6 @@ public class IntermediateResult {
         }
     }
 
-    @VisibleForTesting
-    int getNumberOfRunningProducers() {
-        return numberOfRunningProducers.get();
-    }
-
-    int incrementNumberOfRunningProducersAndGetRemaining() {
-        return numberOfRunningProducers.incrementAndGet();
-    }
-
-    int decrementNumberOfRunningProducersAndGetRemaining() {
-        return numberOfRunningProducers.decrementAndGet();
-    }
-
-    boolean areAllPartitionsFinished() {
-        return numberOfRunningProducers.get() == 0;
-    }
-
     @Override
     public String toString() {
         return "IntermediateResult " + id.toString();
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/IntermediateResultPartition.java b/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/IntermediateResultPartition.java
index 8f1216b94d0..9b995a09756 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/IntermediateResultPartition.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/IntermediateResultPartition.java
@@ -82,18 +82,16 @@ public class IntermediateResultPartition {
     }
 
     public boolean isConsumable() {
-        if (getResultType().isPipelined()) {
-            return hasDataProduced;
-        } else {
-            return totalResult.areAllPartitionsFinished();
-        }
+        return hasDataProduced;
     }
 
     void resetForNewExecution() {
         if (getResultType().isBlocking() && hasDataProduced) {
             // A BLOCKING result partition with data produced means it is finished
             // Need to add the running producer count of the result on resetting it
-            totalResult.incrementNumberOfRunningProducersAndGetRemaining();
+            for (ConsumedPartitionGroup consumedPartitionGroup : getConsumedPartitionGroups()) {
+                consumedPartitionGroup.partitionUnfinished();
+            }
         }
         hasDataProduced = false;
     }
@@ -106,7 +104,7 @@ public class IntermediateResultPartition {
         return edgeManager;
     }
 
-    boolean markFinished() {
+    void markFinished() {
         // Sanity check that this is only called on blocking partitions.
         if (!getResultType().isBlocking()) {
             throw new IllegalStateException(
@@ -121,17 +119,8 @@ public class IntermediateResultPartition {
 
         hasDataProduced = true;
 
-        final int refCnt = totalResult.decrementNumberOfRunningProducersAndGetRemaining();
-
-        if (refCnt == 0) {
-            return true;
-        } else if (refCnt < 0) {
-            throw new IllegalStateException(
-                    "Decremented number of unfinished producers below 0. "
-                            + "This is most likely a bug in the execution state/intermediate result "
-                            + "partition management.");
+        for (ConsumedPartitionGroup consumedPartitionGroup : getConsumedPartitionGroups()) {
+            consumedPartitionGroup.partitionFinished();
         }
-
-        return false;
     }
 }
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/strategy/ConsumedPartitionGroup.java b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/strategy/ConsumedPartitionGroup.java
index 3eedaaebb81..ef0690d5a18 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/strategy/ConsumedPartitionGroup.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/strategy/ConsumedPartitionGroup.java
@@ -18,18 +18,24 @@
 
 package org.apache.flink.runtime.scheduler.strategy;
 
+import org.apache.flink.annotation.VisibleForTesting;
 import org.apache.flink.runtime.jobgraph.IntermediateResultPartitionID;
 
 import java.util.Collections;
 import java.util.Iterator;
 import java.util.List;
+import java.util.concurrent.atomic.AtomicInteger;
 
 /** Group of consumed {@link IntermediateResultPartitionID}s. */
 public class ConsumedPartitionGroup implements Iterable<IntermediateResultPartitionID> {
+
     private final List<IntermediateResultPartitionID> resultPartitions;
 
+    private final AtomicInteger unfinishedPartitions;
+
     private ConsumedPartitionGroup(List<IntermediateResultPartitionID> resultPartitions) {
         this.resultPartitions = resultPartitions;
+        this.unfinishedPartitions = new AtomicInteger(resultPartitions.size());
     }
 
     public static ConsumedPartitionGroup fromMultiplePartitions(
@@ -58,4 +64,21 @@ public class ConsumedPartitionGroup implements Iterable<IntermediateResultPartit
     public IntermediateResultPartitionID getFirst() {
         return iterator().next();
     }
+
+    public int partitionUnfinished() {
+        return unfinishedPartitions.incrementAndGet();
+    }
+
+    public int partitionFinished() {
+        return unfinishedPartitions.decrementAndGet();
+    }
+
+    @VisibleForTesting
+    public int getNumberOfUnfinishedPartitions() {
+        return unfinishedPartitions.get();
+    }
+
+    public boolean areAllPartitionsFinished() {
+        return unfinishedPartitions.get() == 0;
+    }
 }
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/strategy/PipelinedRegionSchedulingStrategy.java b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/strategy/PipelinedRegionSchedulingStrategy.java
index 1e04d01f8a6..bf7fe27c21e 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/strategy/PipelinedRegionSchedulingStrategy.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/strategy/PipelinedRegionSchedulingStrategy.java
@@ -20,7 +20,6 @@ package org.apache.flink.runtime.scheduler.strategy;
 
 import org.apache.flink.annotation.VisibleForTesting;
 import org.apache.flink.runtime.execution.ExecutionState;
-import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;
 import org.apache.flink.runtime.jobgraph.IntermediateResultPartitionID;
 import org.apache.flink.runtime.scheduler.DeploymentOption;
 import org.apache.flink.runtime.scheduler.ExecutionVertexDeploymentOption;
@@ -51,10 +50,6 @@ public class PipelinedRegionSchedulingStrategy implements SchedulingStrategy {
 
     private final DeploymentOption deploymentOption = new DeploymentOption(false);
 
-    /** ConsumedPartitionGroups are correlated if they have the same result id. */
-    private final Map<IntermediateDataSetID, Set<ConsumedPartitionGroup>>
-            correlatedResultPartitionGroups = new HashMap<>();
-
     /** External consumer regions of each ConsumedPartitionGroup. */
     private final Map<ConsumedPartitionGroup, Set<SchedulingPipelinedRegion>>
             partitionGroupConsumerRegions = new IdentityHashMap<>();
@@ -82,8 +77,6 @@ public class PipelinedRegionSchedulingStrategy implements SchedulingStrategy {
 
         initPartitionGroupConsumerRegions();
 
-        initCorrelatedResultPartitionGroups();
-
         for (SchedulingExecutionVertex vertex : schedulingTopology.getVertices()) {
             final SchedulingPipelinedRegion region =
                     schedulingTopology.getPipelinedRegionOfVertex(vertex.getId());
@@ -148,18 +141,6 @@ public class PipelinedRegionSchedulingStrategy implements SchedulingStrategy {
         }
     }
 
-    private void initCorrelatedResultPartitionGroups() {
-        for (ConsumedPartitionGroup consumedPartitionGroup :
-                partitionGroupConsumerRegions.keySet()) {
-            for (IntermediateResultPartitionID partitionId : consumedPartitionGroup) {
-                correlatedResultPartitionGroups
-                        .computeIfAbsent(
-                                partitionId.getIntermediateDataSetID(), id -> new HashSet<>())
-                        .add(consumedPartitionGroup);
-            }
-        }
-    }
-
     @Override
     public void startScheduling() {
         final Set<SchedulingPipelinedRegion> sourceRegions =
@@ -201,26 +182,21 @@ public class PipelinedRegionSchedulingStrategy implements SchedulingStrategy {
                             .filter(
                                     partition ->
                                             partition.getState() == ResultPartitionState.CONSUMABLE)
-                            .flatMap(
-                                    partition ->
-                                            correlatedResultPartitionGroups
-                                                    .getOrDefault(
-                                                            partition.getResultId(),
-                                                            Collections.emptySet())
-                                                    .stream())
+                            .flatMap(partition -> partition.getConsumedPartitionGroups().stream())
+                            .filter(
+                                    group ->
+                                            crossRegionConsumedPartitionGroups.contains(group)
+                                                    || group.areAllPartitionsFinished())
                             .collect(Collectors.toSet());
 
-            // for POINTWISE consumers of a BLOCKING partition, it's possible that some of the
-            // consumers are not affected by the restarting of sibling vertices so they are still in
-            // SCHEDULED/DEPLOYING/RUNNING/FINISHED. We should skip rescheduling these vertices.
             final Set<SchedulingPipelinedRegion> consumerRegions =
                     finishedConsumedPartitionGroups.stream()
                             .flatMap(
                                     partitionGroup ->
-                                            partitionGroupConsumerRegions.get(partitionGroup)
+                                            partitionGroupConsumerRegions
+                                                    .getOrDefault(
+                                                            partitionGroup, Collections.emptySet())
                                                     .stream())
-                            .distinct()
-                            .filter(this::areRegionVerticesAllInCreatedState)
                             .collect(Collectors.toSet());
 
             maybeScheduleRegions(consumerRegions);
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/IntermediateResultPartitionTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/IntermediateResultPartitionTest.java
index 3875f3d04d4..59813538fb4 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/IntermediateResultPartitionTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/IntermediateResultPartitionTest.java
@@ -18,15 +18,23 @@
 
 package org.apache.flink.runtime.executiongraph;
 
+import org.apache.flink.runtime.concurrent.ComponentMainThreadExecutorServiceAdapter;
 import org.apache.flink.runtime.io.network.partition.ResultPartitionType;
+import org.apache.flink.runtime.jobgraph.DistributionPattern;
+import org.apache.flink.runtime.jobgraph.JobGraph;
+import org.apache.flink.runtime.jobgraph.JobGraphTestUtils;
 import org.apache.flink.runtime.jobgraph.JobVertex;
+import org.apache.flink.runtime.scheduler.SchedulerBase;
+import org.apache.flink.runtime.scheduler.SchedulerTestingUtils;
+import org.apache.flink.runtime.scheduler.strategy.ConsumedPartitionGroup;
 import org.apache.flink.runtime.testtasks.NoOpInvokable;
 import org.apache.flink.runtime.testutils.DirectScheduledExecutorService;
 import org.apache.flink.util.TestLogger;
 
 import org.junit.Test;
 
-import static org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.getExecutionJobVertex;
+import java.util.concurrent.ScheduledExecutorService;
+
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertTrue;
@@ -61,27 +69,31 @@ public class IntermediateResultPartitionTest extends TestLogger {
         IntermediateResultPartition partition1 = result.getPartitions()[0];
         IntermediateResultPartition partition2 = result.getPartitions()[1];
 
+        ConsumedPartitionGroup consumedPartitionGroup =
+                partition1.getConsumedPartitionGroups().get(0);
+
         // Not consumable on init
         assertFalse(partition1.isConsumable());
         assertFalse(partition2.isConsumable());
+        assertFalse(consumedPartitionGroup.areAllPartitionsFinished());
 
         // Not consumable if only one partition is FINISHED
         partition1.markFinished();
-        assertFalse(partition1.isConsumable());
+        assertTrue(partition1.isConsumable());
         assertFalse(partition2.isConsumable());
-        assertFalse(result.areAllPartitionsFinished());
+        assertFalse(consumedPartitionGroup.areAllPartitionsFinished());
 
         // Consumable after all partitions are FINISHED
         partition2.markFinished();
         assertTrue(partition1.isConsumable());
         assertTrue(partition2.isConsumable());
-        assertTrue(result.areAllPartitionsFinished());
+        assertTrue(consumedPartitionGroup.areAllPartitionsFinished());
 
         // Not consumable if failover happens
         result.resetForNewExecution();
         assertFalse(partition1.isConsumable());
         assertFalse(partition2.isConsumable());
-        assertFalse(result.areAllPartitionsFinished());
+        assertFalse(consumedPartitionGroup.areAllPartitionsFinished());
     }
 
     @Test
@@ -90,53 +102,70 @@ public class IntermediateResultPartitionTest extends TestLogger {
         IntermediateResultPartition partition1 = result.getPartitions()[0];
         IntermediateResultPartition partition2 = result.getPartitions()[1];
 
+        ConsumedPartitionGroup consumedPartitionGroup =
+                partition1.getConsumedPartitionGroups().get(0);
+
         // Not consumable on init
         assertFalse(partition1.isConsumable());
         assertFalse(partition2.isConsumable());
 
         // Not consumable if partition1 is FINISHED
         partition1.markFinished();
-        assertEquals(1, result.getNumberOfRunningProducers());
-        assertFalse(partition1.isConsumable());
+        assertEquals(1, consumedPartitionGroup.getNumberOfUnfinishedPartitions());
+        assertTrue(partition1.isConsumable());
         assertFalse(partition2.isConsumable());
-        assertFalse(result.areAllPartitionsFinished());
+        assertFalse(consumedPartitionGroup.areAllPartitionsFinished());
 
         // Reset the result and mark partition2 FINISHED, the result should still not be consumable
         result.resetForNewExecution();
-        assertEquals(2, result.getNumberOfRunningProducers());
+        assertEquals(2, consumedPartitionGroup.getNumberOfUnfinishedPartitions());
         partition2.markFinished();
-        assertEquals(1, result.getNumberOfRunningProducers());
+        assertEquals(1, consumedPartitionGroup.getNumberOfUnfinishedPartitions());
         assertFalse(partition1.isConsumable());
-        assertFalse(partition2.isConsumable());
-        assertFalse(result.areAllPartitionsFinished());
+        assertTrue(partition2.isConsumable());
+        assertFalse(consumedPartitionGroup.areAllPartitionsFinished());
 
         // Consumable after all partitions are FINISHED
         partition1.markFinished();
-        assertEquals(0, result.getNumberOfRunningProducers());
+        assertEquals(0, consumedPartitionGroup.getNumberOfUnfinishedPartitions());
         assertTrue(partition1.isConsumable());
         assertTrue(partition2.isConsumable());
-        assertTrue(result.areAllPartitionsFinished());
+        assertTrue(consumedPartitionGroup.areAllPartitionsFinished());
 
         // Not consumable again if failover happens
         result.resetForNewExecution();
-        assertEquals(2, result.getNumberOfRunningProducers());
+        assertEquals(2, consumedPartitionGroup.getNumberOfUnfinishedPartitions());
         assertFalse(partition1.isConsumable());
         assertFalse(partition2.isConsumable());
-        assertFalse(result.areAllPartitionsFinished());
+        assertFalse(consumedPartitionGroup.areAllPartitionsFinished());
     }
 
     private static IntermediateResult createResult(
-            ResultPartitionType resultPartitionType, int producerCount) throws Exception {
+            ResultPartitionType resultPartitionType, int parallelism) throws Exception {
+
+        JobVertex source = new JobVertex("v1");
+        source.setInvokableClass(NoOpInvokable.class);
+        source.setParallelism(parallelism);
+
+        JobVertex sink = new JobVertex("v2");
+        sink.setInvokableClass(NoOpInvokable.class);
+        sink.setParallelism(parallelism);
+
+        sink.connectNewDataSetAsInput(source, DistributionPattern.ALL_TO_ALL, resultPartitionType);
+
+        ScheduledExecutorService executorService = new DirectScheduledExecutorService();
+
+        JobGraph jobGraph = JobGraphTestUtils.batchJobGraph(source, sink);
 
-        JobVertex jobVertex = new JobVertex("v1");
-        jobVertex.setInvokableClass(NoOpInvokable.class);
-        jobVertex.setParallelism(producerCount);
-        jobVertex.createAndAddResultDataSet(resultPartitionType);
+        SchedulerBase scheduler =
+                SchedulerTestingUtils.newSchedulerBuilder(
+                                jobGraph, ComponentMainThreadExecutorServiceAdapter.forMainThread())
+                        .setIoExecutor(executorService)
+                        .setFutureExecutor(executorService)
+                        .build();
 
-        ExecutionJobVertex ejv =
-                getExecutionJobVertex(jobVertex, new DirectScheduledExecutorService());
-        IntermediateResult result = ejv.getProducedDataSets()[0];
+        ExecutionJobVertex ejv = scheduler.getExecutionJobVertex(source.getID());
 
-        return result;
+        return ejv.getProducedDataSets()[0];
     }
 }
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/benchmark/scheduling/SchedulingDownstreamTasksInBatchJobBenchmark.java b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/benchmark/scheduling/SchedulingDownstreamTasksInBatchJobBenchmark.java
index 60eda9998fc..29e17ce0d1d 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/benchmark/scheduling/SchedulingDownstreamTasksInBatchJobBenchmark.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/benchmark/scheduling/SchedulingDownstreamTasksInBatchJobBenchmark.java
@@ -19,14 +19,11 @@
 package org.apache.flink.runtime.scheduler.benchmark.scheduling;
 
 import org.apache.flink.runtime.execution.ExecutionState;
-import org.apache.flink.runtime.executiongraph.IntermediateResult;
+import org.apache.flink.runtime.executiongraph.ExecutionVertex;
 import org.apache.flink.runtime.scheduler.benchmark.JobConfiguration;
 import org.apache.flink.runtime.scheduler.strategy.ExecutionVertexID;
 import org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy;
 
-import java.lang.reflect.Field;
-import java.util.concurrent.atomic.AtomicInteger;
-
 /**
  * The benchmark of scheduling downstream task in a BATCH job. The related method is {@link
  * PipelinedRegionSchedulingStrategy#onExecutionStateChange}.
@@ -43,25 +40,15 @@ public class SchedulingDownstreamTasksInBatchJobBenchmark extends SchedulingBenc
         schedulingStrategy =
                 new PipelinedRegionSchedulingStrategy(schedulerOperations, schedulingTopology);
 
-        // When we trying to scheduling downstream tasks via
-        // onExecutionStateChange(ExecutionState.FINISHED),
-        // the result partitions of upstream tasks need to be CONSUMABLE.
-        // The CONSUMABLE status is determined by the variable "numberOfRunningProducers" of the
-        // IntermediateResult.
-        // Its value cannot be changed by any public methods.
-        // So here we use reflections to modify this value and then schedule the downstream tasks.
-        for (IntermediateResult result : executionGraph.getAllIntermediateResults().values()) {
-            Field f = result.getClass().getDeclaredField("numberOfRunningProducers");
-            f.setAccessible(true);
-            AtomicInteger numberOfRunningProducers = (AtomicInteger) f.get(result);
-            numberOfRunningProducers.set(0);
-        }
-
         executionVertexID =
                 executionGraph
                         .getJobVertex(jobVertices.get(0).getID())
                         .getTaskVertices()[0]
                         .getID();
+        for (ExecutionVertex vertex :
+                executionGraph.getJobVertex(jobVertices.get(0).getID()).getTaskVertices()) {
+            vertex.finishAllBlockingPartitions();
+        }
     }
 
     public void schedulingDownstreamTasks() {
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/strategy/PipelinedRegionSchedulingStrategyTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/strategy/PipelinedRegionSchedulingStrategyTest.java
index a4336a53eab..05764f27274 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/strategy/PipelinedRegionSchedulingStrategyTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/strategy/PipelinedRegionSchedulingStrategyTest.java
@@ -36,7 +36,6 @@ import org.junit.Test;
 
 import java.util.ArrayList;
 import java.util.Arrays;
-import java.util.Collections;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Set;
@@ -139,14 +138,14 @@ public class PipelinedRegionSchedulingStrategyTest extends TestLogger {
                 startScheduling(testingSchedulingTopology);
 
         final TestingSchedulingExecutionVertex map1 = map.get(0);
-        map1.getProducedResults().iterator().next().setState(ResultPartitionState.CONSUMABLE);
+        map1.getProducedResults().iterator().next().markFinished();
         schedulingStrategy.onExecutionStateChange(map1.getId(), ExecutionState.FINISHED);
 
         // sinks' inputs are not all consumable yet so they are not scheduled
         assertThat(testingSchedulerOperation.getScheduledVertices(), hasSize(2));
 
         final TestingSchedulingExecutionVertex map2 = map.get(1);
-        map2.getProducedResults().iterator().next().setState(ResultPartitionState.CONSUMABLE);
+        map2.getProducedResults().iterator().next().markFinished();
         schedulingStrategy.onExecutionStateChange(map2.getId(), ExecutionState.FINISHED);
 
         assertThat(testingSchedulerOperation.getScheduledVertices(), hasSize(4));
@@ -158,36 +157,6 @@ public class PipelinedRegionSchedulingStrategyTest extends TestLogger {
         assertLatestScheduledVerticesAreEqualTo(expectedScheduledVertices);
     }
 
-    @Test
-    public void testFinishedBlockingResultPartitionProducerDoNotScheduleNonCreatedRegions() {
-        final TestingSchedulingTopology topology = new TestingSchedulingTopology();
-
-        final List<TestingSchedulingExecutionVertex> producer =
-                topology.addExecutionVertices().withParallelism(2).finish();
-        final List<TestingSchedulingExecutionVertex> consumer =
-                topology.addExecutionVertices().withParallelism(2).finish();
-
-        topology.connectPointwise(producer, consumer)
-                .withResultPartitionState(ResultPartitionState.CONSUMABLE)
-                .withResultPartitionType(ResultPartitionType.BLOCKING)
-                .finish();
-
-        final PipelinedRegionSchedulingStrategy schedulingStrategy = startScheduling(topology);
-
-        consumer.get(0).setState(ExecutionState.SCHEDULED);
-        schedulingStrategy.onExecutionStateChange(producer.get(0).getId(), ExecutionState.FINISHED);
-
-        // non-CREATED regions should not be re-scheduled
-        assertThat(testingSchedulerOperation.getScheduledVertices(), hasSize(3));
-
-        final List<List<TestingSchedulingExecutionVertex>> expectedScheduledVertices =
-                new ArrayList<>();
-        expectedScheduledVertices.add(Collections.singletonList(producer.get(0)));
-        expectedScheduledVertices.add(Collections.singletonList(producer.get(1)));
-        expectedScheduledVertices.add(Collections.singletonList(consumer.get(1)));
-        assertLatestScheduledVerticesAreEqualTo(expectedScheduledVertices);
-    }
-
     @Test
     public void testSchedulingTopologyWithPersistentBlockingEdges() {
         final TestingSchedulingTopology topology = new TestingSchedulingTopology();
@@ -316,17 +285,59 @@ public class PipelinedRegionSchedulingStrategyTest extends TestLogger {
                 .forEach(vertex -> region2.add(vertex.getId()));
         assertEquals(4, region2.size());
 
-        // Test whether the correct region is scheduled correctly
-        startScheduling(schedulingTopology);
+        // Test whether region 1 is scheduled correctly
+        PipelinedRegionSchedulingStrategy schedulingStrategy = startScheduling(schedulingTopology);
 
         assertEquals(1, testingSchedulerOperation.getScheduledVertices().size());
-        final List<ExecutionVertexDeploymentOption> deploymentOptions =
+        final List<ExecutionVertexDeploymentOption> deploymentOptions1 =
                 testingSchedulerOperation.getScheduledVertices().get(0);
-        assertEquals(5, deploymentOptions.size());
+        assertEquals(5, deploymentOptions1.size());
 
-        for (ExecutionVertexDeploymentOption deploymentOption : deploymentOptions) {
+        for (ExecutionVertexDeploymentOption deploymentOption : deploymentOptions1) {
             assertTrue(region1.contains(deploymentOption.getExecutionVertexId()));
         }
+
+        // Test whether the region 2 is scheduled correctly when region 1 is finished
+        final ExecutionVertex v22 = executionGraph.getJobVertex(v2.getID()).getTaskVertices()[1];
+        v22.finishAllBlockingPartitions();
+
+        schedulingStrategy.onExecutionStateChange(v22.getID(), ExecutionState.FINISHED);
+        assertEquals(2, testingSchedulerOperation.getScheduledVertices().size());
+        final List<ExecutionVertexDeploymentOption> deploymentOptions2 =
+                testingSchedulerOperation.getScheduledVertices().get(1);
+        assertEquals(4, deploymentOptions2.size());
+
+        for (ExecutionVertexDeploymentOption deploymentOption : deploymentOptions2) {
+            assertTrue(region2.contains(deploymentOption.getExecutionVertexId()));
+        }
+    }
+
+    @Test
+    public void testScheduleBlockingDownstreamTaskIndividually() throws Exception {
+        final JobVertex v1 = createJobVertex("v1", 2);
+        final JobVertex v2 = createJobVertex("v2", 2);
+
+        v2.connectNewDataSetAsInput(
+                v1, DistributionPattern.POINTWISE, ResultPartitionType.BLOCKING);
+
+        final List<JobVertex> ordered = new ArrayList<>(Arrays.asList(v1, v2));
+        final JobGraph jobGraph =
+                JobGraphBuilder.newBatchJobGraphBuilder().addJobVertices(ordered).build();
+        final ExecutionGraph executionGraph =
+                TestingDefaultExecutionGraphBuilder.newBuilder().setJobGraph(jobGraph).build();
+
+        final SchedulingTopology schedulingTopology = executionGraph.getSchedulingTopology();
+
+        final PipelinedRegionSchedulingStrategy schedulingStrategy =
+                startScheduling(schedulingTopology);
+
+        assertEquals(2, testingSchedulerOperation.getScheduledVertices().size());
+
+        final ExecutionVertex v11 = executionGraph.getJobVertex(v1.getID()).getTaskVertices()[0];
+        v11.finishAllBlockingPartitions();
+
+        schedulingStrategy.onExecutionStateChange(v11.getID(), ExecutionState.FINISHED);
+        assertEquals(3, testingSchedulerOperation.getScheduledVertices().size());
     }
 
     private static JobVertex createJobVertex(String vertexName, int parallelism) {
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/strategy/TestingSchedulingExecutionVertex.java b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/strategy/TestingSchedulingExecutionVertex.java
index a429c460b91..32c81b6768b 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/strategy/TestingSchedulingExecutionVertex.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/strategy/TestingSchedulingExecutionVertex.java
@@ -94,6 +94,9 @@ public class TestingSchedulingExecutionVertex implements SchedulingExecutionVert
                 ConsumedPartitionGroup.fromSinglePartition(consumedPartition.getId());
 
         consumedPartition.registerConsumedPartitionGroup(consumedPartitionGroup);
+        if (consumedPartition.getState() == ResultPartitionState.CONSUMABLE) {
+            consumedPartitionGroup.partitionFinished();
+        }
 
         this.consumedPartitionGroups.add(consumedPartitionGroup);
         this.resultPartitionsById.putIfAbsent(consumedPartition.getId(), consumedPartition);
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/strategy/TestingSchedulingResultPartition.java b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/strategy/TestingSchedulingResultPartition.java
index f1701035fd1..188853198da 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/strategy/TestingSchedulingResultPartition.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/strategy/TestingSchedulingResultPartition.java
@@ -129,6 +129,13 @@ public class TestingSchedulingResultPartition implements SchedulingResultPartiti
         this.producer = checkNotNull(producer);
     }
 
+    void markFinished() {
+        for (ConsumedPartitionGroup consumedPartitionGroup : consumedPartitionGroups) {
+            consumedPartitionGroup.partitionFinished();
+        }
+        setState(ResultPartitionState.CONSUMABLE);
+    }
+
     void setState(ResultPartitionState state) {
         this.state = state;
     }
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/strategy/TestingSchedulingTopology.java b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/strategy/TestingSchedulingTopology.java
index db1514725ae..4ac523b664a 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/strategy/TestingSchedulingTopology.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/strategy/TestingSchedulingTopology.java
@@ -374,6 +374,9 @@ public class TestingSchedulingTopology implements SchedulingTopology {
 
             for (TestingSchedulingResultPartition resultPartition : resultPartitions) {
                 resultPartition.registerConsumedPartitionGroup(consumedPartitionGroup);
+                if (resultPartition.getState() == ResultPartitionState.CONSUMABLE) {
+                    consumedPartitionGroup.partitionFinished();
+                }
             }
 
             return resultPartitions;
