diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/executor/BufferReduceStatementExecutor.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/executor/BufferReduceStatementExecutor.java
new file mode 100644
index 00000000000..39d0959f4ce
--- /dev/null
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/executor/BufferReduceStatementExecutor.java
@@ -0,0 +1,108 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connector.jdbc.internal.executor;
+
+import org.apache.flink.api.java.tuple.Tuple2;
+import org.apache.flink.table.data.RowData;
+import org.apache.flink.types.RowKind;
+
+import java.sql.Connection;
+import java.sql.SQLException;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.function.Function;
+
+/**
+ * Currently, this statement executor is only used for table/sql to buffer insert/update/delete
+ * events, and reduce them in buffer before submit to external database.
+ */
+public class BufferReduceStatementExecutor implements JdbcBatchStatementExecutor<RowData> {
+
+	private final JdbcBatchStatementExecutor<RowData> upsertExecutor;
+	private final JdbcBatchStatementExecutor<RowData> deleteExecutor;
+	private final Function<RowData, RowData> keyExtractor;
+	private final Function<RowData, RowData> valueTransform;
+	// the mapping is [KEY, <+/-, VALUE>]
+	private final Map<RowData, Tuple2<Boolean, RowData>> reduceBuffer = new HashMap<>();
+
+	public BufferReduceStatementExecutor(
+			JdbcBatchStatementExecutor<RowData> upsertExecutor,
+			JdbcBatchStatementExecutor<RowData> deleteExecutor,
+			Function<RowData, RowData> keyExtractor,
+			Function<RowData, RowData> valueTransform) {
+		this.upsertExecutor = upsertExecutor;
+		this.deleteExecutor = deleteExecutor;
+		this.keyExtractor = keyExtractor;
+		this.valueTransform = valueTransform;
+	}
+
+	@Override
+	public void prepareStatements(Connection connection) throws SQLException {
+		upsertExecutor.prepareStatements(connection);
+		deleteExecutor.prepareStatements(connection);
+	}
+
+	@Override
+	public void addToBatch(RowData record) throws SQLException {
+		RowData key = keyExtractor.apply(record);
+		boolean flag = changeFlag(record.getRowKind());
+		RowData value = valueTransform.apply(record); // copy or not
+		reduceBuffer.put(key, Tuple2.of(flag, value));
+	}
+
+	/**
+	 * Returns true if the row kind is INSERT or UPDATE_AFTER,
+	 * returns false if the row kind is DELETE or UPDATE_BEFORE.
+	 */
+	private boolean changeFlag(RowKind rowKind) {
+		switch (rowKind) {
+			case INSERT:
+			case UPDATE_AFTER:
+				return true;
+			case DELETE:
+			case UPDATE_BEFORE:
+				return false;
+			default:
+				throw new UnsupportedOperationException(
+					String.format("Unknown row kind, the supported row kinds is: INSERT, UPDATE_BEFORE, UPDATE_AFTER," +
+						" DELETE, but get: %s.", rowKind));
+		}
+	}
+
+	@Override
+	public void executeBatch() throws SQLException {
+		// TODO: reuse the extracted key and avoid value copy in the future
+		for (Tuple2<Boolean, RowData> tuple : reduceBuffer.values()) {
+			if (tuple.f0) {
+				upsertExecutor.addToBatch(tuple.f1);
+			} else {
+				deleteExecutor.addToBatch(tuple.f1);
+			}
+		}
+		upsertExecutor.executeBatch();
+		deleteExecutor.executeBatch();
+		reduceBuffer.clear();
+	}
+
+	@Override
+	public void closeStatements() throws SQLException {
+		upsertExecutor.closeStatements();
+		deleteExecutor.closeStatements();
+	}
+}
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicOutputFormatBuilder.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicOutputFormatBuilder.java
new file mode 100644
index 00000000000..3b31d3941a9
--- /dev/null
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicOutputFormatBuilder.java
@@ -0,0 +1,220 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connector.jdbc.table;
+
+import org.apache.flink.api.common.functions.RuntimeContext;
+import org.apache.flink.api.common.typeinfo.TypeInformation;
+import org.apache.flink.api.common.typeutils.TypeSerializer;
+import org.apache.flink.connector.jdbc.JdbcExecutionOptions;
+import org.apache.flink.connector.jdbc.JdbcStatementBuilder;
+import org.apache.flink.connector.jdbc.dialect.JdbcDialect;
+import org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat;
+import org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider;
+import org.apache.flink.connector.jdbc.internal.converter.JdbcRowConverter;
+import org.apache.flink.connector.jdbc.internal.executor.BufferReduceStatementExecutor;
+import org.apache.flink.connector.jdbc.internal.executor.InsertOrUpdateJdbcExecutor;
+import org.apache.flink.connector.jdbc.internal.executor.JdbcBatchStatementExecutor;
+import org.apache.flink.connector.jdbc.internal.options.JdbcDmlOptions;
+import org.apache.flink.connector.jdbc.internal.options.JdbcOptions;
+import org.apache.flink.connector.jdbc.utils.JdbcUtils;
+import org.apache.flink.table.data.GenericRowData;
+import org.apache.flink.table.data.RowData;
+import org.apache.flink.table.types.DataType;
+import org.apache.flink.table.types.logical.LogicalType;
+import org.apache.flink.table.types.logical.RowType;
+
+import java.io.Serializable;
+import java.util.Arrays;
+import java.util.function.Function;
+
+import static org.apache.flink.table.data.RowData.createFieldGetter;
+import static org.apache.flink.util.Preconditions.checkArgument;
+import static org.apache.flink.util.Preconditions.checkNotNull;
+
+/**
+ * Builder for {@link JdbcBatchingOutputFormat} for Table/SQL.
+ */
+public class JdbcDynamicOutputFormatBuilder implements Serializable {
+
+	private JdbcOptions jdbcOptions;
+	private JdbcExecutionOptions executionOptions;
+	private JdbcDmlOptions dmlOptions;
+	private TypeInformation<RowData> rowDataTypeInformation;
+	private DataType[] fieldDataTypes;
+
+	public JdbcDynamicOutputFormatBuilder() {
+	}
+
+	public JdbcDynamicOutputFormatBuilder setJdbcOptions(JdbcOptions jdbcOptions) {
+		this.jdbcOptions = jdbcOptions;
+		return this;
+	}
+
+	public JdbcDynamicOutputFormatBuilder setJdbcExecutionOptions(JdbcExecutionOptions executionOptions) {
+		this.executionOptions = executionOptions;
+		return this;
+	}
+
+	public JdbcDynamicOutputFormatBuilder setJdbcDmlOptions(JdbcDmlOptions dmlOptions) {
+		this.dmlOptions = dmlOptions;
+		return this;
+	}
+
+	public JdbcDynamicOutputFormatBuilder setRowDataTypeInfo(TypeInformation<RowData> rowDataTypeInfo) {
+		this.rowDataTypeInformation = rowDataTypeInfo;
+		return this;
+	}
+
+	public JdbcDynamicOutputFormatBuilder setFieldDataTypes(DataType[] fieldDataTypes) {
+		this.fieldDataTypes = fieldDataTypes;
+		return this;
+	}
+
+	public JdbcBatchingOutputFormat<RowData, ?, ?> build() {
+		checkNotNull(jdbcOptions, "jdbc options can not be null");
+		checkNotNull(dmlOptions, "jdbc dml options can not be null");
+		checkNotNull(executionOptions, "jdbc execution options can not be null");
+
+		final LogicalType[] logicalTypes = Arrays.stream(fieldDataTypes)
+			.map(DataType::getLogicalType)
+			.toArray(LogicalType[]::new);
+		if (dmlOptions.getKeyFields().isPresent() && dmlOptions.getKeyFields().get().length > 0) {
+			//upsert query
+			return new JdbcBatchingOutputFormat<>(
+				new SimpleJdbcConnectionProvider(jdbcOptions),
+				executionOptions,
+				ctx -> createBufferReduceExecutor(dmlOptions, ctx, rowDataTypeInformation, logicalTypes),
+				JdbcBatchingOutputFormat.RecordExtractor.identity());
+		} else {
+			// append only query
+			final String sql = dmlOptions
+				.getDialect()
+				.getInsertIntoStatement(dmlOptions.getTableName(), dmlOptions.getFieldNames());
+			return new JdbcBatchingOutputFormat<>(
+				new SimpleJdbcConnectionProvider(jdbcOptions),
+				executionOptions,
+				ctx -> createSimpleRowDataExecutor(dmlOptions.getDialect(), sql, logicalTypes, ctx, rowDataTypeInformation),
+				JdbcBatchingOutputFormat.RecordExtractor.identity());
+		}
+	}
+
+	private static JdbcBatchStatementExecutor<RowData> createKeyedRowExecutor(
+			JdbcDialect dialect,
+			int[] pkFields,
+			LogicalType[] pkTypes,
+			String sql,
+			LogicalType[] logicalTypes) {
+		final JdbcRowConverter rowConverter = dialect.getRowConverter(RowType.of(pkTypes));
+		final Function<RowData, RowData> keyExtractor = createRowKeyExtractor(logicalTypes, pkFields);
+		return JdbcBatchStatementExecutor.keyed(
+			sql,
+			keyExtractor,
+			(st, record) -> rowConverter.toExternal(keyExtractor.apply(record), st));
+	}
+
+	private static JdbcBatchStatementExecutor<RowData> createBufferReduceExecutor(
+			JdbcDmlOptions opt,
+			RuntimeContext ctx,
+			TypeInformation<RowData> rowDataTypeInfo,
+			LogicalType[] fieldTypes) {
+		checkArgument(opt.getKeyFields().isPresent());
+		int[] pkFields = Arrays.stream(opt.getKeyFields().get()).mapToInt(Arrays.asList(opt.getFieldNames())::indexOf).toArray();
+		LogicalType[] pkTypes = Arrays.stream(pkFields).mapToObj(f -> fieldTypes[f]).toArray(LogicalType[]::new);
+		final TypeSerializer<RowData> typeSerializer = rowDataTypeInfo.createSerializer(ctx.getExecutionConfig());
+		final Function<RowData, RowData> valueTransform = ctx.getExecutionConfig().isObjectReuseEnabled() ? typeSerializer::copy : Function.identity();
+
+		JdbcBatchStatementExecutor<RowData> upsertExecutor = createUpsertRowExecutor(opt, ctx, rowDataTypeInfo, pkFields, pkTypes, fieldTypes, valueTransform);
+		JdbcBatchStatementExecutor<RowData> deleteExecutor = createDeleteExecutor(opt, pkFields, pkTypes, fieldTypes);
+
+		return new BufferReduceStatementExecutor(
+			upsertExecutor,
+			deleteExecutor,
+			createRowKeyExtractor(fieldTypes, pkFields),
+			valueTransform);
+	}
+
+	private static JdbcBatchStatementExecutor<RowData> createUpsertRowExecutor(
+			JdbcDmlOptions opt,
+			RuntimeContext ctx,
+			TypeInformation<RowData> rowDataTypeInfo,
+			int[] pkFields,
+			LogicalType[] pkTypes,
+			LogicalType[] fieldTypes,
+			Function<RowData, RowData> valueTransform) {
+		checkArgument(opt.getKeyFields().isPresent());
+		JdbcDialect dialect = opt.getDialect();
+		return opt.getDialect()
+			.getUpsertStatement(opt.getTableName(), opt.getFieldNames(), opt.getKeyFields().get())
+			.map(sql -> createSimpleRowDataExecutor(dialect, sql, fieldTypes, ctx, rowDataTypeInfo))
+			.orElseGet(() ->
+				new InsertOrUpdateJdbcExecutor<>(
+					opt.getDialect().getRowExistsStatement(opt.getTableName(), opt.getKeyFields().get()),
+					opt.getDialect().getInsertIntoStatement(opt.getTableName(), opt.getFieldNames()),
+					opt.getDialect().getUpdateStatement(opt.getTableName(), opt.getFieldNames(), opt.getKeyFields().get()),
+					createRowDataJdbcStatementBuilder(dialect, pkTypes),
+					createRowDataJdbcStatementBuilder(dialect, fieldTypes),
+					createRowDataJdbcStatementBuilder(dialect, fieldTypes),
+					createRowKeyExtractor(fieldTypes, pkFields),
+					valueTransform));
+	}
+
+	private static JdbcBatchStatementExecutor<RowData> createDeleteExecutor(
+			JdbcDmlOptions dmlOptions,
+			int[] pkFields,
+			LogicalType[] pkTypes,
+			LogicalType[] fieldTypes) {
+		checkArgument(dmlOptions.getKeyFields().isPresent());
+		String[] pkNames = Arrays.stream(pkFields).mapToObj(k -> dmlOptions.getFieldNames()[k]).toArray(String[]::new);
+		String deleteSql = dmlOptions.getDialect().getDeleteStatement(dmlOptions.getTableName(), pkNames);
+		return createKeyedRowExecutor(dmlOptions.getDialect(), pkFields, pkTypes, deleteSql, fieldTypes);
+	}
+
+	private static Function<RowData, RowData> createRowKeyExtractor(LogicalType[] logicalTypes, int[] pkFields) {
+		final RowData.FieldGetter[] fieldGetters = new RowData.FieldGetter[pkFields.length];
+		for (int i = 0; i < pkFields.length; i++) {
+			fieldGetters[i] = createFieldGetter(logicalTypes[pkFields[i]], pkFields[i]);
+		}
+		return row -> getPrimaryKey(row, fieldGetters);
+	}
+
+	private static JdbcBatchStatementExecutor<RowData> createSimpleRowDataExecutor(JdbcDialect dialect, String sql, LogicalType[] fieldTypes, RuntimeContext ctx, TypeInformation<RowData> rowDataTypeInfo) {
+		final TypeSerializer<RowData> typeSerializer = rowDataTypeInfo.createSerializer(ctx.getExecutionConfig());
+		return JdbcBatchStatementExecutor.simple(
+			sql,
+			createRowDataJdbcStatementBuilder(dialect, fieldTypes),
+			ctx.getExecutionConfig().isObjectReuseEnabled() ? typeSerializer::copy : Function.identity());
+	}
+
+	/**
+	 * Creates a {@link JdbcStatementBuilder} for {@link RowData} using the provided SQL types array.
+	 * Uses {@link JdbcUtils#setRecordToStatement}
+	 */
+	private static JdbcStatementBuilder<RowData> createRowDataJdbcStatementBuilder(JdbcDialect dialect, LogicalType[] types) {
+		final JdbcRowConverter converter = dialect.getRowConverter(RowType.of(types));
+		return (st, record) -> converter.toExternal(record, st);
+	}
+
+	private static RowData getPrimaryKey(RowData row, RowData.FieldGetter[] fieldGetters) {
+		GenericRowData pkRow = new GenericRowData(fieldGetters.length);
+		for (int i = 0; i < fieldGetters.length; i++) {
+			pkRow.setField(i, fieldGetters[i].getFieldOrNull(row));
+		}
+		return pkRow;
+	}
+}
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSink.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSink.java
index 7b002241661..3e09e1c66f4 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSink.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSink.java
@@ -32,7 +32,6 @@ import org.apache.flink.types.RowKind;
 
 import java.util.Objects;
 
-import static org.apache.flink.connector.jdbc.table.JdbcRowDataOutputFormat.DynamicOutputFormatBuilder;
 import static org.apache.flink.util.Preconditions.checkState;
 
 /**
@@ -79,7 +78,7 @@ public class JdbcDynamicTableSink implements DynamicTableSink {
 	public SinkRuntimeProvider getSinkRuntimeProvider(Context context) {
 		final TypeInformation<RowData> rowDataTypeInformation = (TypeInformation<RowData>) context
 			.createTypeInformation(tableSchema.toRowDataType());
-		final DynamicOutputFormatBuilder builder = JdbcRowDataOutputFormat.dynamicOutputFormatBuilder();
+		final JdbcDynamicOutputFormatBuilder builder = new JdbcDynamicOutputFormatBuilder();
 
 		builder.setJdbcOptions(jdbcOptions);
 		builder.setJdbcDmlOptions(dmlOptions);
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcRowDataOutputFormat.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcRowDataOutputFormat.java
deleted file mode 100644
index 4ce940f8b50..00000000000
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcRowDataOutputFormat.java
+++ /dev/null
@@ -1,296 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.connector.jdbc.table;
-
-import org.apache.flink.annotation.Internal;
-import org.apache.flink.api.common.functions.RuntimeContext;
-import org.apache.flink.api.common.typeinfo.TypeInformation;
-import org.apache.flink.api.common.typeutils.TypeSerializer;
-import org.apache.flink.connector.jdbc.JdbcExecutionOptions;
-import org.apache.flink.connector.jdbc.JdbcStatementBuilder;
-import org.apache.flink.connector.jdbc.dialect.JdbcDialect;
-import org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat;
-import org.apache.flink.connector.jdbc.internal.connection.JdbcConnectionProvider;
-import org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider;
-import org.apache.flink.connector.jdbc.internal.converter.JdbcRowConverter;
-import org.apache.flink.connector.jdbc.internal.executor.InsertOrUpdateJdbcExecutor;
-import org.apache.flink.connector.jdbc.internal.executor.JdbcBatchStatementExecutor;
-import org.apache.flink.connector.jdbc.internal.options.JdbcDmlOptions;
-import org.apache.flink.connector.jdbc.internal.options.JdbcOptions;
-import org.apache.flink.connector.jdbc.utils.JdbcUtils;
-import org.apache.flink.table.data.GenericRowData;
-import org.apache.flink.table.data.RowData;
-import org.apache.flink.table.data.RowData.FieldGetter;
-import org.apache.flink.table.types.DataType;
-import org.apache.flink.table.types.logical.LogicalType;
-import org.apache.flink.table.types.logical.RowType;
-import org.apache.flink.types.Row;
-
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.io.IOException;
-import java.sql.SQLException;
-import java.util.Arrays;
-import java.util.function.Function;
-
-import static org.apache.flink.table.data.RowData.createFieldGetter;
-import static org.apache.flink.util.Preconditions.checkArgument;
-import static org.apache.flink.util.Preconditions.checkNotNull;
-
-/**
- * OutputFormat for {@link JdbcDynamicTableSource}.
- */
-@Internal
-public class JdbcRowDataOutputFormat extends JdbcBatchingOutputFormat<RowData, RowData, JdbcBatchStatementExecutor<RowData>> {
-
-	private static final long serialVersionUID = 1L;
-	private static final Logger LOG = LoggerFactory.getLogger(JdbcRowDataOutputFormat.class);
-
-	private JdbcBatchStatementExecutor<RowData> deleteExecutor;
-	private final JdbcDmlOptions dmlOptions;
-	private final LogicalType[] logicalTypes;
-
-	private JdbcRowDataOutputFormat(
-			JdbcConnectionProvider connectionProvider,
-			JdbcDmlOptions dmlOptions,
-			JdbcExecutionOptions batchOptions,
-			TypeInformation<RowData> rowDataTypeInfo,
-			LogicalType[] logicalTypes) {
-		super(
-			connectionProvider,
-			batchOptions,
-			ctx -> createUpsertRowExecutor(dmlOptions, ctx, rowDataTypeInfo, logicalTypes),
-			RecordExtractor.identity());
-		this.dmlOptions = dmlOptions;
-		this.logicalTypes = logicalTypes;
-	}
-
-	private JdbcRowDataOutputFormat(
-			JdbcConnectionProvider connectionProvider,
-			JdbcDmlOptions dmlOptions,
-			JdbcExecutionOptions batchOptions,
-			TypeInformation<RowData> rowDataTypeInfo,
-			LogicalType[] logicalTypes,
-			String sql) {
-		super(connectionProvider,
-			batchOptions,
-			ctx -> createSimpleRowDataExecutor(dmlOptions.getDialect(), sql, logicalTypes, ctx, rowDataTypeInfo),
-			RecordExtractor.identity());
-		this.dmlOptions = dmlOptions;
-		this.logicalTypes = logicalTypes;
-	}
-
-	@Override
-	public void open(int taskNumber, int numTasks) throws IOException {
-		deleteExecutor = createDeleteExecutor();
-		super.open(taskNumber, numTasks);
-		try {
-			deleteExecutor.prepareStatements(connection);
-		} catch (SQLException e) {
-			throw new IOException(e);
-		}
-	}
-
-	private JdbcBatchStatementExecutor<RowData> createDeleteExecutor() {
-		int[] pkFields = Arrays.stream(dmlOptions.getFieldNames()).mapToInt(Arrays.asList(dmlOptions.getFieldNames())::indexOf).toArray();
-		LogicalType[] pkTypes = Arrays.stream(pkFields).mapToObj(f -> logicalTypes[f]).toArray(LogicalType[]::new);
-		String deleteSql = dmlOptions.getDialect().getDeleteStatement(dmlOptions.getTableName(), dmlOptions.getFieldNames());
-		return createKeyedRowExecutor(dmlOptions.getDialect(), pkFields, pkTypes, deleteSql, logicalTypes);
-	}
-
-	@Override
-	protected void addToBatch(RowData original, RowData extracted) throws SQLException {
-		switch (original.getRowKind()) {
-			case INSERT:
-			case UPDATE_AFTER:
-				super.addToBatch(original, extracted);
-				break;
-			case DELETE:
-			case UPDATE_BEFORE:
-				deleteExecutor.addToBatch(extracted);
-				break;
-			default:
-				throw new UnsupportedOperationException(
-					String.format("unknown row kind, the supported row kinds is: INSERT, UPDATE_BEFORE, UPDATE_AFTER," +
-						" DELETE, but get: %s.", original.getRowKind()));
-		}
-	}
-
-	@Override
-	public synchronized void close() {
-		try {
-			super.close();
-		} finally {
-			try {
-				if (deleteExecutor != null) {
-					deleteExecutor.closeStatements();
-				}
-			} catch (SQLException e) {
-				LOG.warn("unable to close delete statement runner", e);
-			}
-		}
-	}
-
-	@Override
-	protected void attemptFlush() throws SQLException {
-		super.attemptFlush();
-		deleteExecutor.executeBatch();
-	}
-
-	private static JdbcBatchStatementExecutor<RowData> createKeyedRowExecutor(JdbcDialect dialect, int[] pkFields, LogicalType[] pkTypes, String sql, LogicalType[] logicalTypes) {
-		final JdbcRowConverter rowConverter = dialect.getRowConverter(RowType.of(logicalTypes));
-		final Function<RowData, RowData>  keyExtractor = createRowKeyExtractor(logicalTypes, pkFields);
-		return JdbcBatchStatementExecutor.keyed(
-			sql,
-			keyExtractor,
-			(st, record) -> rowConverter
-				.toExternal(keyExtractor.apply(record), st));
-	}
-
-	private static JdbcBatchStatementExecutor<RowData> createUpsertRowExecutor(JdbcDmlOptions opt, RuntimeContext ctx, TypeInformation<RowData> rowDataTypeInfo, LogicalType[] logicalTypes) {
-		checkArgument(opt.getKeyFields().isPresent());
-
-		int[] pkFields = Arrays.stream(opt.getKeyFields().get()).mapToInt(Arrays.asList(opt.getFieldNames())::indexOf).toArray();
-		LogicalType[] pkTypes = Arrays.stream(pkFields).mapToObj(f -> logicalTypes[f]).toArray(LogicalType[]::new);
-		JdbcDialect dialect = opt.getDialect();
-		final TypeSerializer<RowData> typeSerializer = rowDataTypeInfo.createSerializer(ctx.getExecutionConfig());
-		return opt.getDialect()
-			.getUpsertStatement(opt.getTableName(), opt.getFieldNames(), opt.getKeyFields().get())
-			.map(sql -> createSimpleRowDataExecutor(dialect, sql, logicalTypes, ctx, rowDataTypeInfo))
-			.orElseGet(() ->
-				new InsertOrUpdateJdbcExecutor<>(
-					opt.getDialect().getRowExistsStatement(opt.getTableName(), opt.getKeyFields().get()),
-					opt.getDialect().getInsertIntoStatement(opt.getTableName(), opt.getFieldNames()),
-					opt.getDialect().getUpdateStatement(opt.getTableName(), opt.getFieldNames(), opt.getKeyFields().get()),
-					createRowDataJdbcStatementBuilder(dialect, pkTypes),
-					createRowDataJdbcStatementBuilder(dialect, logicalTypes),
-					createRowDataJdbcStatementBuilder(dialect, logicalTypes),
-					createRowKeyExtractor(logicalTypes, pkFields),
-					ctx.getExecutionConfig().isObjectReuseEnabled() ? typeSerializer::copy : r -> r));
-	}
-
-	private static Function<RowData, RowData> createRowKeyExtractor(LogicalType[] logicalTypes, int[] pkFields) {
-		final FieldGetter[] fieldGetters = new FieldGetter[pkFields.length];
-		for (int i = 0; i < pkFields.length; i++) {
-			fieldGetters[i] = createFieldGetter(logicalTypes[pkFields[i]], pkFields[i]);
-		}
-		return row -> getPrimaryKey(row, fieldGetters);
-	}
-
-	private static JdbcBatchStatementExecutor<RowData> createSimpleRowDataExecutor(JdbcDialect dialect, String sql, LogicalType[] fieldTypes, RuntimeContext ctx, TypeInformation<RowData> rowDataTypeInfo) {
-		final TypeSerializer<RowData> typeSerializer = rowDataTypeInfo.createSerializer(ctx.getExecutionConfig());
-		return JdbcBatchStatementExecutor.simple(
-			sql,
-			createRowDataJdbcStatementBuilder(dialect, fieldTypes),
-			ctx.getExecutionConfig().isObjectReuseEnabled() ? typeSerializer::copy : Function.identity());
-	}
-
-	/**
-	 * Creates a {@link JdbcStatementBuilder} for {@link Row} using the provided SQL types array.
-	 * Uses {@link JdbcUtils#setRecordToStatement}
-	 */
-	private static JdbcStatementBuilder<RowData> createRowDataJdbcStatementBuilder(JdbcDialect dialect, LogicalType[] types) {
-		final JdbcRowConverter converter = dialect.getRowConverter(RowType.of(types));
-		return (st, record) -> converter.toExternal(record, st);
-	}
-
-	private static RowData getPrimaryKey(RowData row, FieldGetter[] fieldGetters) {
-		GenericRowData pkRow = new GenericRowData(fieldGetters.length);
-		for (int i = 0; i < fieldGetters.length; i++) {
-			pkRow.setField(i, fieldGetters[i].getFieldOrNull(row));
-		}
-		return pkRow;
-	}
-
-	public static DynamicOutputFormatBuilder dynamicOutputFormatBuilder() {
-		return new DynamicOutputFormatBuilder();
-	}
-
-	/**
-	 * Builder for {@link JdbcRowDataOutputFormat}.
-	 */
-	public static class DynamicOutputFormatBuilder {
-		private JdbcOptions jdbcOptions;
-		private JdbcExecutionOptions executionOptions;
-		private JdbcDmlOptions dmlOptions;
-		private TypeInformation<RowData> rowDataTypeInformation;
-		private DataType[] fieldDataTypes;
-
-		private DynamicOutputFormatBuilder() {
-		}
-
-		public DynamicOutputFormatBuilder setJdbcOptions(JdbcOptions jdbcOptions) {
-			this.jdbcOptions = jdbcOptions;
-			return this;
-		}
-
-		public DynamicOutputFormatBuilder setJdbcExecutionOptions(JdbcExecutionOptions executionOptions) {
-			this.executionOptions = executionOptions;
-			return this;
-		}
-
-		public DynamicOutputFormatBuilder setJdbcDmlOptions(JdbcDmlOptions dmlOptions) {
-			this.dmlOptions = dmlOptions;
-			return this;
-		}
-
-		public DynamicOutputFormatBuilder setRowDataTypeInfo(TypeInformation<RowData> rowDataTypeInfo) {
-			this.rowDataTypeInformation = rowDataTypeInfo;
-			return this;
-		}
-
-		public DynamicOutputFormatBuilder setFieldDataTypes(DataType[] fieldDataTypes) {
-			this.fieldDataTypes = fieldDataTypes;
-			return this;
-		}
-
-		public JdbcRowDataOutputFormat build() {
-			checkNotNull(jdbcOptions, "jdbc options can not be null");
-			checkNotNull(dmlOptions, "jdbc dml options can not be null");
-			checkNotNull(executionOptions, "jdbc execution options can not be null");
-
-			final LogicalType[] logicalTypes = Arrays.stream(fieldDataTypes)
-				.map(DataType::getLogicalType)
-				.toArray(LogicalType[]::new);
-			if (dmlOptions.getKeyFields().isPresent() && dmlOptions.getKeyFields().get().length > 0) {
-				//upsert query
-				return new JdbcRowDataOutputFormat(
-					new SimpleJdbcConnectionProvider(jdbcOptions),
-					dmlOptions,
-					executionOptions,
-					rowDataTypeInformation,
-					logicalTypes);
-			} else {
-				// append only query
-				final String sql = dmlOptions
-					.getDialect()
-					.getInsertIntoStatement(dmlOptions.getTableName(), dmlOptions.getFieldNames());
-				return new JdbcRowDataOutputFormat(
-					new SimpleJdbcConnectionProvider(jdbcOptions),
-					dmlOptions,
-					executionOptions,
-					rowDataTypeInformation,
-					logicalTypes,
-					sql);
-			}
-		}
-	}
-}
-
diff --git a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcRowDataOutputFormatTest.java b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicOutputFormatTest.java
similarity index 95%
rename from flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcRowDataOutputFormatTest.java
rename to flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicOutputFormatTest.java
index 4b3de6810ba..24cf8c60691 100644
--- a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcRowDataOutputFormatTest.java
+++ b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicOutputFormatTest.java
@@ -20,6 +20,7 @@ package org.apache.flink.connector.jdbc.table;
 
 import org.apache.flink.connector.jdbc.JdbcDataTestBase;
 import org.apache.flink.connector.jdbc.JdbcExecutionOptions;
+import org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat;
 import org.apache.flink.connector.jdbc.internal.options.JdbcDmlOptions;
 import org.apache.flink.connector.jdbc.internal.options.JdbcOptions;
 import org.apache.flink.table.api.DataTypes;
@@ -58,11 +59,11 @@ import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertTrue;
 
 /**
- * Test suite for {@link JdbcRowDataOutputFormat}.
+ * Test suite for {@link JdbcDynamicOutputFormatBuilder}.
  */
-public class JdbcRowDataOutputFormatTest extends JdbcDataTestBase {
+public class JdbcDynamicOutputFormatTest extends JdbcDataTestBase {
 
-	private static JdbcRowDataOutputFormat outputFormat;
+	private static JdbcBatchingOutputFormat<RowData, ?, ?> outputFormat;
 	private static String[] fieldNames = new String[] {"id", "title", "author", "price", "qty"};
 	private static DataType[] fieldDataTypes = new DataType[]{
 		DataTypes.INT(),
@@ -100,7 +101,7 @@ public class JdbcRowDataOutputFormatTest extends JdbcDataTestBase {
 				.withFieldNames(fieldNames)
 				.build();
 
-			outputFormat = JdbcRowDataOutputFormat.dynamicOutputFormatBuilder()
+			outputFormat = new JdbcDynamicOutputFormatBuilder()
 				.setJdbcOptions(jdbcOptions)
 				.setFieldDataTypes(fieldDataTypes)
 				.setJdbcDmlOptions(dmlOptions)
@@ -128,7 +129,7 @@ public class JdbcRowDataOutputFormatTest extends JdbcDataTestBase {
 				.withFieldNames(fieldNames)
 				.build();
 
-			outputFormat = JdbcRowDataOutputFormat.dynamicOutputFormatBuilder()
+			outputFormat = new JdbcDynamicOutputFormatBuilder()
 				.setJdbcOptions(jdbcOptions)
 				.setFieldDataTypes(fieldDataTypes)
 				.setJdbcDmlOptions(dmlOptions)
@@ -155,7 +156,7 @@ public class JdbcRowDataOutputFormatTest extends JdbcDataTestBase {
 				.withFieldNames(fieldNames)
 				.build();
 
-			outputFormat = JdbcRowDataOutputFormat.dynamicOutputFormatBuilder()
+			outputFormat = new JdbcDynamicOutputFormatBuilder()
 				.setJdbcOptions(jdbcOptions)
 				.setFieldDataTypes(fieldDataTypes)
 				.setJdbcDmlOptions(dmlOptions)
@@ -188,7 +189,7 @@ public class JdbcRowDataOutputFormatTest extends JdbcDataTestBase {
 				.withFieldNames(fieldNames)
 				.build();
 
-			outputFormat = JdbcRowDataOutputFormat.dynamicOutputFormatBuilder()
+			outputFormat = new JdbcDynamicOutputFormatBuilder()
 				.setJdbcOptions(jdbcOptions)
 				.setFieldDataTypes(fieldDataTypes)
 				.setJdbcDmlOptions(dmlOptions)
@@ -222,7 +223,7 @@ public class JdbcRowDataOutputFormatTest extends JdbcDataTestBase {
 				.withFieldNames(fieldNames)
 				.build();
 
-			outputFormat = JdbcRowDataOutputFormat.dynamicOutputFormatBuilder()
+			outputFormat = new JdbcDynamicOutputFormatBuilder()
 				.setJdbcOptions(jdbcOptions)
 				.setFieldDataTypes(fieldDataTypes)
 				.setJdbcDmlOptions(dmlOptions)
@@ -258,7 +259,7 @@ public class JdbcRowDataOutputFormatTest extends JdbcDataTestBase {
 			.withFieldNames(fieldNames)
 			.build();
 
-		outputFormat = JdbcRowDataOutputFormat.dynamicOutputFormatBuilder()
+		outputFormat = new JdbcDynamicOutputFormatBuilder()
 			.setJdbcOptions(jdbcOptions)
 			.setFieldDataTypes(fieldDataTypes)
 			.setJdbcDmlOptions(dmlOptions)
@@ -312,7 +313,7 @@ public class JdbcRowDataOutputFormatTest extends JdbcDataTestBase {
 			.withBatchSize(3)
 			.build();
 
-		outputFormat = JdbcRowDataOutputFormat.dynamicOutputFormatBuilder()
+		outputFormat = new JdbcDynamicOutputFormatBuilder()
 			.setJdbcOptions(jdbcOptions)
 			.setFieldDataTypes(fieldDataTypes)
 			.setJdbcDmlOptions(dmlOptions)
@@ -374,7 +375,7 @@ public class JdbcRowDataOutputFormatTest extends JdbcDataTestBase {
 			.withFieldNames(fieldNames)
 			.build();
 
-		outputFormat = JdbcRowDataOutputFormat.dynamicOutputFormatBuilder()
+		outputFormat = new JdbcDynamicOutputFormatBuilder()
 			.setJdbcOptions(jdbcOptions)
 			.setFieldDataTypes(fieldDataTypes)
 			.setJdbcDmlOptions(dmlOptions)
diff --git a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSinkITCase.java b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSinkITCase.java
index 76d3cf6ad02..2d988b080cd 100644
--- a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSinkITCase.java
+++ b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSinkITCase.java
@@ -29,6 +29,9 @@ import org.apache.flink.table.api.Table;
 import org.apache.flink.table.api.TableEnvironment;
 import org.apache.flink.table.api.TableResult;
 import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
+import org.apache.flink.table.planner.factories.TestValuesTableFactory;
+import org.apache.flink.table.planner.runtime.utils.TableEnvUtil;
+import org.apache.flink.table.planner.runtime.utils.TestData;
 import org.apache.flink.test.util.AbstractTestBase;
 import org.apache.flink.types.Row;
 
@@ -36,6 +39,7 @@ import org.junit.After;
 import org.junit.Before;
 import org.junit.Test;
 
+import java.math.BigDecimal;
 import java.sql.Connection;
 import java.sql.DriverManager;
 import java.sql.SQLException;
@@ -59,6 +63,7 @@ public class JdbcDynamicTableSinkITCase extends AbstractTestBase {
 	public static final String OUTPUT_TABLE2 = "dynamicSinkForAppend";
 	public static final String OUTPUT_TABLE3 = "dynamicSinkForBatch";
 	public static final String OUTPUT_TABLE4 = "REAL_TABLE";
+	public static final String USER_TABLE = "USER_TABLE";
 
 	@Before
 	public void before() throws ClassNotFoundException, SQLException {
@@ -85,11 +90,20 @@ public class JdbcDynamicTableSinkITCase extends AbstractTestBase {
 				"SCORE BIGINT NOT NULL DEFAULT 0)");
 
 			stat.executeUpdate("CREATE TABLE " + OUTPUT_TABLE4 + " (real_data REAL)");
+
+			stat.executeUpdate("CREATE TABLE " + USER_TABLE + " (" +
+				"user_id VARCHAR(20) NOT NULL," +
+				"user_name VARCHAR(20) NOT NULL," +
+				"email VARCHAR(255)," +
+				"balance DECIMAL(18,2)," +
+				"balance2 DECIMAL(18,2)," +
+				"PRIMARY KEY (user_id))");
 		}
 	}
 
 	@After
 	public void clearOutputTable() throws Exception {
+		TestValuesTableFactory.clearAllData();
 		Class.forName(DERBY_EBOOKSHOP_DB.getDriverClass());
 		try (
 			Connection conn = DriverManager.getConnection(DB_URL);
@@ -98,6 +112,7 @@ public class JdbcDynamicTableSinkITCase extends AbstractTestBase {
 			stat.execute("DROP TABLE " + OUTPUT_TABLE2);
 			stat.execute("DROP TABLE " + OUTPUT_TABLE3);
 			stat.execute("DROP TABLE " + OUTPUT_TABLE4);
+			stat.execute("DROP TABLE " + USER_TABLE);
 		}
 	}
 
@@ -273,4 +288,43 @@ public class JdbcDynamicTableSinkITCase extends AbstractTestBase {
 			Row.of("Bob", 1)
 		}, DB_URL, OUTPUT_TABLE3, new String[]{"NAME", "SCORE"});
 	}
+
+	@Test
+	public void testReadingFromChangelogSource() throws SQLException {
+		TableEnvironment tEnv = TableEnvironment.create(EnvironmentSettings.newInstance().build());
+		String dataId = TestValuesTableFactory.registerData(TestData.userChangelog());
+		tEnv.executeSql("CREATE TABLE user_logs (\n" +
+				"  user_id STRING,\n" +
+				"  user_name STRING,\n" +
+				"  email STRING,\n" +
+				"  balance DECIMAL(18,2),\n" +
+				"  balance2 AS balance * 2\n" +
+				") WITH (\n" +
+				" 'connector' = 'values',\n" +
+				" 'data-id' = '" + dataId + "',\n" +
+				" 'changelog-mode' = 'I,UA,UB,D'\n" +
+				")");
+		tEnv.executeSql("CREATE TABLE user_sink (\n" +
+			"  user_id STRING PRIMARY KEY NOT ENFORCED,\n" +
+			"  user_name STRING,\n" +
+			"  email STRING,\n" +
+			"  balance DECIMAL(18,2),\n" +
+			"  balance2 DECIMAL(18,2)\n" +
+			") WITH (\n" +
+			"  'connector' = 'jdbc'," +
+			"  'url'='" + DB_URL + "'," +
+			"  'table-name' = '" + USER_TABLE + "'," +
+			"  'sink.buffer-flush.max-rows' = '100'," +
+			"  'sink.buffer-flush.interval' = '0'" + // disable async flush
+			")");
+		TableEnvUtil.execInsertSqlAndWaitResult(
+			tEnv,
+			"INSERT INTO user_sink SELECT * FROM user_logs");
+
+		check(new Row[] {
+			Row.of("user1", "Tom", "tom123@gmail.com", new BigDecimal("8.10"), new BigDecimal("16.20")),
+			Row.of("user3", "Bailey", "bailey@qq.com", new BigDecimal("9.99"), new BigDecimal("19.98")),
+			Row.of("user4", "Tina", "tina@gmail.com", new BigDecimal("11.30"), new BigDecimal("22.60"))
+		}, DB_URL, USER_TABLE, new String[]{"user_id", "user_name", "email", "balance", "balance2"});
+	}
 }
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/optimize/program/FlinkChangelogModeInferenceProgram.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/optimize/program/FlinkChangelogModeInferenceProgram.scala
index 6b8625d963a..f08bb96a24c 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/optimize/program/FlinkChangelogModeInferenceProgram.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/optimize/program/FlinkChangelogModeInferenceProgram.scala
@@ -562,10 +562,8 @@ class FlinkChangelogModeInferenceProgram extends FlinkOptimizeProgram[StreamOpti
                   case (UpdateKind.NONE, r: UpdateKind) => r
                   case (l: UpdateKind, UpdateKind.NONE) => l
                   case (l: UpdateKind, r: UpdateKind) if l == r => l
-                  case (_, _) =>
-                    throw new UnsupportedOperationException(
-                      "UNION doesn't support to union ONLY_UPDATE_AFTER input " +
-                        "and BEFORE_AND_AFTER input.")
+                  // UNION doesn't support to union ONLY_UPDATE_AFTER and BEFORE_AND_AFTER inputs
+                  case (_, _) => return None
                 }
               }
             new UpdateKindTrait(merged)
@@ -618,12 +616,10 @@ class FlinkChangelogModeInferenceProgram extends FlinkOptimizeProgram[StreamOpti
             return None
           case Some(newChild) =>
             val providedTrait = newChild.getTraitSet.getTrait(UpdateKindTraitDef.INSTANCE)
-            val childDescription = newChild.getRelDetailedDescription
             if (!providedTrait.satisfies(requiredChildrenTrait)) {
-              throw new TableException(s"Provided trait $providedTrait can't satisfy " +
-                s"required trait $requiredChildrenTrait. " +
-                s"This is a bug in planner, please file an issue. \n" +
-                s"Current node is $childDescription")
+              // the provided trait can't satisfy required trait, thus we should return None.
+              // for example, the changelog source can't provide ONLY_UPDATE_AFTER.
+              return None
             }
             newChild
         }
diff --git a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/TableScanTest.xml b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/TableScanTest.xml
index 3503d13560b..2c63ad1d3b9 100644
--- a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/TableScanTest.xml
+++ b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/TableScanTest.xml
@@ -16,28 +16,6 @@ See the License for the specific language governing permissions and
 limitations under the License.
 -->
 <Root>
-  <TestCase name="testDDLWithComputedColumnReferRowtime">
-    <Resource name="sql">
-      <![CDATA[SELECT * FROM src WHERE a > 1]]>
-    </Resource>
-    <Resource name="planBefore">
-      <![CDATA[
-LogicalProject(ts=[$0], a=[$1], b=[$2], my_ts=[$3], proc=[$4])
-+- LogicalFilter(condition=[>($1, 1)])
-   +- LogicalWatermarkAssigner(rowtime=[ts], watermark=[-($0, 1:INTERVAL SECOND)])
-      +- LogicalProject(ts=[$0], a=[$1], b=[$2], my_ts=[-($0, 1:INTERVAL SECOND)], proc=[PROCTIME()])
-         +- LogicalTableScan(table=[[default_catalog, default_database, src]])
-]]>
-    </Resource>
-    <Resource name="planAfter">
-      <![CDATA[
-Calc(select=[ts, a, b, my_ts, PROCTIME_MATERIALIZE(proc) AS proc], where=[>(a, 1)])
-+- WatermarkAssigner(rowtime=[ts], watermark=[-(ts, 1:INTERVAL SECOND)])
-   +- Calc(select=[ts, a, b, -(ts, 1:INTERVAL SECOND) AS my_ts, PROCTIME() AS proc])
-      +- TableSourceScan(table=[[default_catalog, default_database, src]], fields=[ts, a, b])
-]]>
-    </Resource>
-  </TestCase>
   <TestCase name="testAggregateOnChangelogSource">
     <Resource name="sql">
       <![CDATA[SELECT COUNT(*) FROM src WHERE a > 1]]>
@@ -95,51 +73,66 @@ Calc(select=[ts, a, b], where=[>(a, 1)])
 ]]>
     </Resource>
   </TestCase>
-  <TestCase name="testJoinOnChangelogSource">
+  <TestCase name="testDDLWithComputedColumn">
     <Resource name="sql">
+      <![CDATA[SELECT * FROM t1]]>
+    </Resource>
+    <Resource name="planBefore">
       <![CDATA[
-SELECT o.currency, o.amount, r.rate, o.amount * r.rate
-FROM orders AS o JOIN rates_history AS r
-ON o.currency = r.currency
+LogicalProject(a=[$0], b=[$1], c=[+($0, 1)], d=[TO_TIMESTAMP($1)], e=[my_udf($0)])
++- LogicalTableScan(table=[[default_catalog, default_database, t1]])
 ]]>
     </Resource>
+    <Resource name="planAfter">
+      <![CDATA[
+Calc(select=[a, b, +(a, 1) AS c, TO_TIMESTAMP(b) AS d, my_udf(a) AS e])
++- TableSourceScan(table=[[default_catalog, default_database, t1]], fields=[a, b])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testDDLWithComputedColumnReferRowtime">
+    <Resource name="sql">
+      <![CDATA[SELECT * FROM src WHERE a > 1]]>
+    </Resource>
     <Resource name="planBefore">
       <![CDATA[
-LogicalProject(currency=[$1], amount=[$0], rate=[$3], EXPR$3=[*($0, $3)])
-+- LogicalJoin(condition=[=($1, $2)], joinType=[inner])
-   :- LogicalTableScan(table=[[default_catalog, default_database, orders]])
-   +- LogicalTableScan(table=[[default_catalog, default_database, rates_history]])
+LogicalProject(ts=[$0], a=[$1], b=[$2], my_ts=[$3], proc=[$4])
++- LogicalFilter(condition=[>($1, 1)])
+   +- LogicalWatermarkAssigner(rowtime=[ts], watermark=[-($0, 1:INTERVAL SECOND)])
+      +- LogicalProject(ts=[$0], a=[$1], b=[$2], my_ts=[-($0, 1:INTERVAL SECOND)], proc=[PROCTIME()])
+         +- LogicalTableScan(table=[[default_catalog, default_database, src]])
 ]]>
     </Resource>
     <Resource name="planAfter">
       <![CDATA[
-Calc(select=[currency, amount, rate, *(amount, rate) AS EXPR$3], changelogMode=[I,UA])
-+- Join(joinType=[InnerJoin], where=[=(currency, currency0)], select=[amount, currency, currency0, rate], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey], changelogMode=[I,UA])
-   :- Exchange(distribution=[hash[currency]], changelogMode=[I])
-   :  +- TableSourceScan(table=[[default_catalog, default_database, orders]], fields=[amount, currency], changelogMode=[I])
-   +- Exchange(distribution=[hash[currency]], changelogMode=[I,UB,UA])
-      +- TableSourceScan(table=[[default_catalog, default_database, rates_history]], fields=[currency, rate], changelogMode=[I,UB,UA])
+Calc(select=[ts, a, b, my_ts, PROCTIME_MATERIALIZE(proc) AS proc], where=[>(a, 1)])
++- WatermarkAssigner(rowtime=[ts], watermark=[-(ts, 1:INTERVAL SECOND)])
+   +- Calc(select=[ts, a, b, -(ts, 1:INTERVAL SECOND) AS my_ts, PROCTIME() AS proc])
+      +- TableSourceScan(table=[[default_catalog, default_database, src]], fields=[ts, a, b])
 ]]>
     </Resource>
   </TestCase>
-  <TestCase name="testDDLWithComputedColumn">
+  <TestCase name="testDDLWithWatermarkComputedColumn">
     <Resource name="sql">
       <![CDATA[SELECT * FROM t1]]>
     </Resource>
     <Resource name="planBefore">
       <![CDATA[
-LogicalProject(a=[$0], b=[$1], c=[+($0, 1)], d=[TO_TIMESTAMP($1)], e=[my_udf($0)])
-+- LogicalTableScan(table=[[default_catalog, default_database, t1]])
+LogicalProject(a=[$0], b=[$1], c=[$2], d=[$3], e=[$4])
++- LogicalWatermarkAssigner(rowtime=[d], watermark=[-($3, 1:INTERVAL SECOND)])
+   +- LogicalProject(a=[$0], b=[$1], c=[+($0, 1)], d=[TO_TIMESTAMP($1)], e=[my_udf($0)])
+      +- LogicalTableScan(table=[[default_catalog, default_database, t1]])
 ]]>
     </Resource>
     <Resource name="planAfter">
       <![CDATA[
-Calc(select=[a, b, +(a, 1) AS c, TO_TIMESTAMP(b) AS d, my_udf(a) AS e])
-+- TableSourceScan(table=[[default_catalog, default_database, t1]], fields=[a, b])
+WatermarkAssigner(rowtime=[d], watermark=[-(d, 1:INTERVAL SECOND)])
++- Calc(select=[a, b, +(a, 1) AS c, TO_TIMESTAMP(b) AS d, my_udf(a) AS e])
+   +- TableSourceScan(table=[[default_catalog, default_database, t1]], fields=[a, b])
 ]]>
     </Resource>
   </TestCase>
-  <TestCase name="testScanOnChangelogSource">
+  <TestCase name="testFilterOnChangelogSource">
     <Resource name="sql">
       <![CDATA[SELECT * FROM src WHERE a > 1]]>
     </Resource>
@@ -157,43 +150,81 @@ Calc(select=[ts, a, b], where=[>(a, 1)], changelogMode=[I,UB,UA,D])
 ]]>
     </Resource>
   </TestCase>
-  <TestCase name="testDDLWithWatermarkComputedColumn">
+  <TestCase name="testUnionChangelogSourceAndAggregation">
     <Resource name="sql">
-      <![CDATA[SELECT * FROM t1]]>
+      <![CDATA[
+SELECT b, ts, a
+FROM (
+  SELECT * FROM changelog_src
+  UNION ALL
+  SELECT MAX(ts) as t, a, MAX(b) as b FROM append_src GROUP BY a
+)
+]]>
     </Resource>
     <Resource name="planBefore">
       <![CDATA[
-LogicalProject(a=[$0], b=[$1], c=[$2], d=[$3], e=[$4])
-+- LogicalWatermarkAssigner(rowtime=[d], watermark=[-($3, 1:INTERVAL SECOND)])
-   +- LogicalProject(a=[$0], b=[$1], c=[+($0, 1)], d=[TO_TIMESTAMP($1)], e=[my_udf($0)])
-      +- LogicalTableScan(table=[[default_catalog, default_database, t1]])
+LogicalProject(b=[$2], ts=[$0], a=[$1])
++- LogicalUnion(all=[true])
+   :- LogicalProject(ts=[$0], a=[$1], b=[$2])
+   :  +- LogicalTableScan(table=[[default_catalog, default_database, changelog_src]])
+   +- LogicalProject(t=[$1], a=[$0], b=[$2])
+      +- LogicalAggregate(group=[{0}], t=[MAX($1)], b=[MAX($2)])
+         +- LogicalProject(a=[$1], ts=[$0], b=[$2])
+            +- LogicalTableScan(table=[[default_catalog, default_database, append_src]])
 ]]>
     </Resource>
     <Resource name="planAfter">
       <![CDATA[
-WatermarkAssigner(rowtime=[d], watermark=[-(d, 1:INTERVAL SECOND)])
-+- Calc(select=[a, b, +(a, 1) AS c, TO_TIMESTAMP(b) AS d, my_udf(a) AS e])
-   +- TableSourceScan(table=[[default_catalog, default_database, t1]], fields=[a, b])
+Union(all=[true], union=[b, ts, a], changelogMode=[I,UB,UA,D])
+:- Calc(select=[b, ts, a], changelogMode=[I,UB,UA,D])
+:  +- TableSourceScan(table=[[default_catalog, default_database, changelog_src]], fields=[ts, a, b], changelogMode=[I,UB,UA,D])
++- Calc(select=[b, t AS ts, a], changelogMode=[I,UB,UA])
+   +- GroupAggregate(groupBy=[a], select=[a, MAX(ts) AS t, MAX(b) AS b], changelogMode=[I,UB,UA])
+      +- Exchange(distribution=[hash[a]], changelogMode=[I])
+         +- TableSourceScan(table=[[default_catalog, default_database, append_src]], fields=[ts, a, b], changelogMode=[I])
 ]]>
     </Resource>
   </TestCase>
-  <TestCase name="testKeywordsWithWatermarkComputedColumn">
+  <TestCase name="testJoinOnChangelogSource">
     <Resource name="sql">
-      <![CDATA[SELECT * FROM t1]]>
+      <![CDATA[
+SELECT o.currency, o.amount, r.rate, o.amount * r.rate
+FROM orders AS o JOIN rates_history AS r
+ON o.currency = r.currency
+]]>
     </Resource>
     <Resource name="planBefore">
       <![CDATA[
-LogicalProject(a=[$0], b=[$1], time=[$2], mytime=[$3], current_time=[$4], json_row=[$5], timestamp=[$6])
-+- LogicalWatermarkAssigner(rowtime=[timestamp], watermark=[$6])
-   +- LogicalProject(a=[$0], b=[$1], time=[$2], mytime=[$2], current_time=[CURRENT_TIME], json_row=[$3], timestamp=[$3.timestamp])
-      +- LogicalTableScan(table=[[default_catalog, default_database, t1]])
+LogicalProject(currency=[$1], amount=[$0], rate=[$3], EXPR$3=[*($0, $3)])
++- LogicalJoin(condition=[=($1, $2)], joinType=[inner])
+   :- LogicalTableScan(table=[[default_catalog, default_database, orders]])
+   +- LogicalTableScan(table=[[default_catalog, default_database, rates_history]])
 ]]>
     </Resource>
     <Resource name="planAfter">
       <![CDATA[
-WatermarkAssigner(rowtime=[timestamp], watermark=[timestamp])
-+- Calc(select=[a, b, time, time AS mytime, CURRENT_TIME() AS current_time, json_row, json_row.timestamp AS timestamp])
-   +- TableSourceScan(table=[[default_catalog, default_database, t1]], fields=[a, b, time, json_row])
+Calc(select=[currency, amount, rate, *(amount, rate) AS EXPR$3], changelogMode=[I,UA])
++- Join(joinType=[InnerJoin], where=[=(currency, currency0)], select=[amount, currency, currency0, rate], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey], changelogMode=[I,UA])
+   :- Exchange(distribution=[hash[currency]], changelogMode=[I])
+   :  +- TableSourceScan(table=[[default_catalog, default_database, orders]], fields=[amount, currency], changelogMode=[I])
+   +- Exchange(distribution=[hash[currency]], changelogMode=[I,UB,UA])
+      +- TableSourceScan(table=[[default_catalog, default_database, rates_history]], fields=[currency, rate], changelogMode=[I,UB,UA])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testLegacyTableSourceScan">
+    <Resource name="sql">
+      <![CDATA[SELECT * FROM MyTable]]>
+    </Resource>
+    <Resource name="planBefore">
+      <![CDATA[
+LogicalProject(a=[$0], b=[$1], c=[$2])
++- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]])
+]]>
+    </Resource>
+    <Resource name="planAfter">
+      <![CDATA[
+LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
 ]]>
     </Resource>
   </TestCase>
@@ -215,19 +246,40 @@ Calc(select=[ts, a, b], where=[>(a, 1)], changelogMode=[I])
 ]]>
     </Resource>
   </TestCase>
-  <TestCase name="testLegacyTableSourceScan">
+  <TestCase name="testScanOnChangelogSource">
     <Resource name="sql">
-      <![CDATA[SELECT * FROM MyTable]]>
+      <![CDATA[SELECT b,a,ts FROM src]]>
     </Resource>
     <Resource name="planBefore">
       <![CDATA[
-LogicalProject(a=[$0], b=[$1], c=[$2])
-+- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]])
+LogicalProject(b=[$2], a=[$1], ts=[$0])
++- LogicalTableScan(table=[[default_catalog, default_database, src]])
 ]]>
     </Resource>
     <Resource name="planAfter">
       <![CDATA[
-LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
+Calc(select=[b, a, ts], changelogMode=[I,UB,UA,D])
++- TableSourceScan(table=[[default_catalog, default_database, src]], fields=[ts, a, b], changelogMode=[I,UB,UA,D])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testKeywordsWithWatermarkComputedColumn">
+    <Resource name="sql">
+      <![CDATA[SELECT * FROM t1]]>
+    </Resource>
+    <Resource name="planBefore">
+      <![CDATA[
+LogicalProject(a=[$0], b=[$1], time=[$2], mytime=[$3], current_time=[$4], json_row=[$5], timestamp=[$6])
++- LogicalWatermarkAssigner(rowtime=[timestamp], watermark=[$6])
+   +- LogicalProject(a=[$0], b=[$1], time=[$2], mytime=[$2], current_time=[CURRENT_TIME], json_row=[$3], timestamp=[$3.timestamp])
+      +- LogicalTableScan(table=[[default_catalog, default_database, t1]])
+]]>
+    </Resource>
+    <Resource name="planAfter">
+      <![CDATA[
+WatermarkAssigner(rowtime=[timestamp], watermark=[timestamp])
++- Calc(select=[a, b, time, time AS mytime, CURRENT_TIME() AS current_time, json_row, json_row.timestamp AS timestamp])
+   +- TableSourceScan(table=[[default_catalog, default_database, t1]], fields=[a, b, time, json_row])
 ]]>
     </Resource>
   </TestCase>
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/stream/sql/TableScanTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/stream/sql/TableScanTest.scala
index 37c9464b6cf..bff33c62734 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/stream/sql/TableScanTest.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/stream/sql/TableScanTest.scala
@@ -155,7 +155,7 @@ class TableScanTest extends TableTestBase {
   }
 
   @Test
-  def testScanOnChangelogSource(): Unit = {
+  def testFilterOnChangelogSource(): Unit = {
     util.addTable(
       """
         |CREATE TABLE src (
@@ -170,6 +170,58 @@ class TableScanTest extends TableTestBase {
     util.verifyPlan("SELECT * FROM src WHERE a > 1", ExplainDetail.CHANGELOG_MODE)
   }
 
+  @Test
+  def testScanOnChangelogSource(): Unit = {
+    util.addTable(
+      """
+        |CREATE TABLE src (
+        |  ts TIMESTAMP(3),
+        |  a INT,
+        |  b DOUBLE
+        |) WITH (
+        |  'connector' = 'values',
+        |  'changelog-mode' = 'I,UA,UB,D'
+        |)
+      """.stripMargin)
+    util.verifyPlan("SELECT b,a,ts FROM src", ExplainDetail.CHANGELOG_MODE)
+  }
+
+  @Test
+  def testUnionChangelogSourceAndAggregation(): Unit = {
+    util.addTable(
+      """
+        |CREATE TABLE changelog_src (
+        |  ts TIMESTAMP(3),
+        |  a INT,
+        |  b DOUBLE
+        |) WITH (
+        |  'connector' = 'values',
+        |  'changelog-mode' = 'I,UA,UB,D'
+        |)
+      """.stripMargin)
+    util.addTable(
+      """
+        |CREATE TABLE append_src (
+        |  ts TIMESTAMP(3),
+        |  a INT,
+        |  b DOUBLE
+        |) WITH (
+        |  'connector' = 'values',
+        |  'changelog-mode' = 'I'
+        |)
+      """.stripMargin)
+
+    val query = """
+      |SELECT b, ts, a
+      |FROM (
+      |  SELECT * FROM changelog_src
+      |  UNION ALL
+      |  SELECT MAX(ts) as t, a, MAX(b) as b FROM append_src GROUP BY a
+      |)
+      |""".stripMargin
+    util.verifyPlan(query, ExplainDetail.CHANGELOG_MODE)
+  }
+
   @Test
   def testAggregateOnChangelogSource(): Unit = {
     util.addTable(
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/AggregateITCase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/AggregateITCase.scala
index ccf76dbd805..d3c6b775bbb 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/AggregateITCase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/AggregateITCase.scala
@@ -38,7 +38,7 @@ import org.apache.flink.table.planner.runtime.utils.UserDefinedFunctionTestUtils
 import org.apache.flink.table.planner.runtime.utils._
 import org.apache.flink.table.planner.utils.DateTimeTestUtil.{localDate, localDateTime, localTime => mLocalTime}
 import org.apache.flink.table.runtime.typeutils.BigDecimalTypeInfo
-import org.apache.flink.types.{Row, RowKind}
+import org.apache.flink.types.Row
 
 import org.junit.Assert.assertEquals
 import org.junit._
@@ -1255,86 +1255,6 @@ class AggregateITCase(
     assertEquals(expected.sorted, tableSink.getUpsertResults.sorted)
   }
 
-  @Test
-  def testAggregateOnChangelogSource(): Unit = {
-    val dataId = TestValuesTableFactory.registerData(TestData.userChangelog)
-    val ddl =
-      s"""
-         |CREATE TABLE user_logs (
-         |  user_id STRING,
-         |  user_name STRING,
-         |  email STRING,
-         |  balance DECIMAL(18,2)
-         |) WITH (
-         | 'connector' = 'values',
-         | 'data-id' = '$dataId',
-         | 'changelog-mode' = 'I,UA,UB,D'
-         |)
-         |""".stripMargin
-    tEnv.executeSql(ddl)
-
-    val query =
-      s"""
-         |SELECT count(*), sum(balance), max(email)
-         |FROM user_logs
-         |""".stripMargin
-
-    val result = tEnv.sqlQuery(query).toRetractStream[Row]
-    val sink = new TestingRetractSink()
-    result.addSink(sink).setParallelism(result.parallelism)
-    env.execute()
-
-    val expected = Seq("3,29.39,tom123@gmail.com")
-    assertEquals(expected.sorted, sink.getRetractResults.sorted)
-  }
-
-  @Test
-  def testAggregateOnInsertDeleteChangelogSource(): Unit = {
-    // only contains INSERT and DELETE
-    val userChangelog = TestData.userChangelog.map { row =>
-      row.getKind match {
-        case RowKind.INSERT | RowKind.DELETE => row
-        case RowKind.UPDATE_BEFORE =>
-          val ret = Row.copy(row)
-          ret.setKind(RowKind.DELETE)
-          ret
-        case RowKind.UPDATE_AFTER =>
-          val ret = Row.copy(row)
-          ret.setKind(RowKind.INSERT)
-          ret
-      }
-    }
-    val dataId = TestValuesTableFactory.registerData(userChangelog)
-    val ddl =
-      s"""
-         |CREATE TABLE user_logs (
-         |  user_id STRING,
-         |  user_name STRING,
-         |  email STRING,
-         |  balance DECIMAL(18,2)
-         |) WITH (
-         | 'connector' = 'values',
-         | 'data-id' = '$dataId',
-         | 'changelog-mode' = 'I,D'
-         |)
-         |""".stripMargin
-    tEnv.executeSql(ddl)
-
-    val query =
-      s"""
-         |SELECT count(*), sum(balance), max(email)
-         |FROM user_logs
-         |""".stripMargin
-
-    val result = tEnv.sqlQuery(query).toRetractStream[Row]
-    val sink = new TestingRetractSink()
-    result.addSink(sink).setParallelism(result.parallelism)
-    env.execute()
-
-    val expected = Seq("3,29.39,tom123@gmail.com")
-    assertEquals(expected.sorted, sink.getRetractResults.sorted)
-  }
-
   @Test
   def testAggregationCodeSplit(): Unit = {
 
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/ChangelogSourceITCase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/ChangelogSourceITCase.scala
new file mode 100644
index 00000000000..e2bf869a522
--- /dev/null
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/ChangelogSourceITCase.scala
@@ -0,0 +1,196 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.planner.runtime.stream.sql
+
+import org.apache.flink.api.scala._
+import org.apache.flink.table.planner.factories.TestValuesTableFactory
+import org.apache.flink.table.planner.runtime.utils.{StreamingWithStateTestBase, TestData, TestingRetractSink}
+import org.apache.flink.table.planner.runtime.utils.StreamingWithStateTestBase.StateBackendMode
+import org.apache.flink.table.api.bridge.scala._
+import org.apache.flink.types.{Row, RowKind}
+import org.junit.Assert.assertEquals
+import org.junit.{Before, Test}
+import org.junit.runner.RunWith
+import org.junit.runners.Parameterized
+
+import scala.collection.JavaConversions._
+import scala.collection.Seq
+
+@RunWith(classOf[Parameterized])
+class ChangelogSourceITCase(state: StateBackendMode) extends StreamingWithStateTestBase(state) {
+
+  val dataId: String = TestValuesTableFactory.registerData(TestData.userChangelog)
+
+  @Before
+  override def before(): Unit = {
+    super.before()
+    val ddl =
+      s"""
+         |CREATE TABLE user_logs (
+         |  user_id STRING,
+         |  user_name STRING,
+         |  email STRING,
+         |  balance DECIMAL(18,2),
+         |  balance2 AS balance * 2
+         |) WITH (
+         | 'connector' = 'values',
+         | 'data-id' = '$dataId',
+         | 'changelog-mode' = 'I,UA,UB,D'
+         |)
+         |""".stripMargin
+    tEnv.executeSql(ddl)
+  }
+
+  @Test
+  def testChangelogSourceAndToRetractStream(): Unit = {
+    val result = tEnv.sqlQuery("SELECT * FROM user_logs").toRetractStream[Row]
+    val sink = new TestingRetractSink()
+    result.addSink(sink).setParallelism(result.parallelism)
+    env.execute()
+
+    val expected = Seq(
+      "user1,Tom,tom123@gmail.com,8.10,16.20",
+      "user3,Bailey,bailey@qq.com,9.99,19.98",
+      "user4,Tina,tina@gmail.com,11.30,22.60")
+    assertEquals(expected.sorted, sink.getRetractResults.sorted)
+  }
+
+  @Test
+  def testChangelogSourceAndUpsertSink(): Unit = {
+    val sinkDDL =
+      s"""
+         |CREATE TABLE user_sink (
+         |  user_id STRING PRIMARY KEY NOT ENFORCED,
+         |  user_name STRING,
+         |  email STRING,
+         |  balance DECIMAL(18,2),
+         |  balance2 DECIMAL(18,2)
+         |) WITH (
+         | 'connector' = 'values',
+         | 'sink-insert-only' = 'false'
+         |)
+         |""".stripMargin
+    val dml =
+      s"""
+         |INSERT INTO user_sink
+         |SELECT * FROM user_logs
+         |""".stripMargin
+    tEnv.executeSql(sinkDDL)
+    execInsertSqlAndWaitResult(dml)
+
+    val expected = Seq(
+      "user1,Tom,tom123@gmail.com,8.10,16.20",
+      "user3,Bailey,bailey@qq.com,9.99,19.98",
+      "user4,Tina,tina@gmail.com,11.30,22.60")
+    assertEquals(expected.sorted, TestValuesTableFactory.getResults("user_sink").sorted)
+  }
+
+  @Test
+  def testAggregateOnChangelogSource(): Unit = {
+    val query =
+      s"""
+         |SELECT count(*), sum(balance), max(email)
+         |FROM user_logs
+         |""".stripMargin
+
+    val result = tEnv.sqlQuery(query).toRetractStream[Row]
+    val sink = new TestingRetractSink()
+    result.addSink(sink).setParallelism(result.parallelism)
+    env.execute()
+
+    val expected = Seq("3,29.39,tom123@gmail.com")
+    assertEquals(expected.sorted, sink.getRetractResults.sorted)
+  }
+
+  @Test
+  def testAggregateOnChangelogSourceAndUpsertSink(): Unit = {
+    val sinkDDL =
+      s"""
+         |CREATE TABLE user_sink (
+         |  `scope` STRING,
+         |  cnt BIGINT,
+         |  sum_balance DECIMAL(18,2),
+         |  max_email STRING,
+         |  PRIMARY KEY (`scope`) NOT ENFORCED
+         |) WITH (
+         | 'connector' = 'values',
+         | 'sink-insert-only' = 'false'
+         |)
+         |""".stripMargin
+    val dml =
+      s"""
+         |INSERT INTO user_sink
+         |SELECT 'ALL', count(*), sum(balance), max(email)
+         |FROM user_logs
+         |GROUP BY 'ALL'
+         |""".stripMargin
+    tEnv.executeSql(sinkDDL)
+    execInsertSqlAndWaitResult(dml)
+
+    val expected = Seq("ALL,3,29.39,tom123@gmail.com")
+    assertEquals(expected.sorted, TestValuesTableFactory.getResults("user_sink").sorted)
+  }
+
+  @Test
+  def testAggregateOnInsertDeleteChangelogSource(): Unit = {
+    // only contains INSERT and DELETE
+    val userChangelog = TestData.userChangelog.map { row =>
+      row.getKind match {
+        case RowKind.INSERT | RowKind.DELETE => row
+        case RowKind.UPDATE_BEFORE =>
+          val ret = Row.copy(row)
+          ret.setKind(RowKind.DELETE)
+          ret
+        case RowKind.UPDATE_AFTER =>
+          val ret = Row.copy(row)
+          ret.setKind(RowKind.INSERT)
+          ret
+      }
+    }
+    val dataId = TestValuesTableFactory.registerData(userChangelog)
+    val ddl =
+      s"""
+         |CREATE TABLE user_logs2 (
+         |  user_id STRING,
+         |  user_name STRING,
+         |  email STRING,
+         |  balance DECIMAL(18,2)
+         |) WITH (
+         | 'connector' = 'values',
+         | 'data-id' = '$dataId',
+         | 'changelog-mode' = 'I,D'
+         |)
+         |""".stripMargin
+    tEnv.executeSql(ddl)
+
+    val query =
+      s"""
+         |SELECT count(*), sum(balance), max(email)
+         |FROM user_logs2
+         |""".stripMargin
+
+    val result = tEnv.sqlQuery(query).toRetractStream[Row]
+    val sink = new TestingRetractSink()
+    result.addSink(sink).setParallelism(result.parallelism)
+    env.execute()
+
+    val expected = Seq("3,29.39,tom123@gmail.com")
+    assertEquals(expected.sorted, sink.getRetractResults.sorted)
+  }
+}
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/TableSourceITCase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/TableSourceITCase.scala
index d98b4336c78..1243ab89640 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/TableSourceITCase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/TableSourceITCase.scala
@@ -262,35 +262,4 @@ class TableSourceITCase extends StreamingTestBase {
     )
     assertEquals(expected.sorted.mkString("\n"), sink.getAppendResults.sorted.mkString("\n"))
   }
-
-  @Test
-  def testChangelogSource(): Unit = {
-    val dataId = TestValuesTableFactory.registerData(TestData.userChangelog)
-    val ddl =
-      s"""
-         |CREATE TABLE user_logs (
-         |  user_id STRING,
-         |  user_name STRING,
-         |  email STRING,
-         |  balance DECIMAL(18,2),
-         |  balance2 AS balance * 2
-         |) WITH (
-         | 'connector' = 'values',
-         | 'data-id' = '$dataId',
-         | 'changelog-mode' = 'I,UA,UB,D'
-         |)
-         |""".stripMargin
-    tEnv.executeSql(ddl)
-
-    val result = tEnv.sqlQuery("SELECT * FROM user_logs").toRetractStream[Row]
-    val sink = new TestingRetractSink()
-    result.addSink(sink).setParallelism(result.parallelism)
-    env.execute()
-
-    val expected = Seq(
-      "user1,Tom,tom123@gmail.com,8.10,16.20",
-      "user3,Bailey,bailey@qq.com,9.99,19.98",
-      "user4,Tina,tina@gmail.com,11.30,22.60")
-    assertEquals(expected.sorted, sink.getRetractResults.sorted)
-  }
 }
