diff --git a/flink-staging/flink-scala-shell/pom.xml b/flink-staging/flink-scala-shell/pom.xml
index c4b5bd6b8a6..371257b7e5c 100644
--- a/flink-staging/flink-scala-shell/pom.xml
+++ b/flink-staging/flink-scala-shell/pom.xml
@@ -194,39 +194,6 @@ under the License.
 				</executions>
 			</plugin>
 
-			<plugin>
-				<groupId>org.scalatest</groupId>
-				<artifactId>scalatest-maven-plugin</artifactId>
-				<version>1.0</version>
-				<configuration>
-					<reportsDirectory>${project.build.directory}/surefire-reports</reportsDirectory>
-					<stdout>W</stdout> <!-- Skip coloring output -->
-				</configuration>
-				<executions>
-					<execution>
-						<id>scala-test</id>
-						<goals>
-							<goal>test</goal>
-						</goals>
-						<configuration>
-							<suffixes>(?&lt;!(IT|Integration))(Test|Suite|Case)</suffixes>
-							<argLine>-Xms256m -Xmx800m -Dlog4j.configuration=${log4j.configuration} -Dlog.dir=${log.dir} -Dmvn.forkNumber=1 -XX:-UseGCOverheadLimit</argLine>
-						</configuration>
-					</execution>
-					<execution>
-						<id>integration-test</id>
-						<phase>integration-test</phase>
-						<goals>
-							<goal>test</goal>
-						</goals>
-						<configuration>
-							<suffixes>(IT|Integration)(Test|Suite|Case)</suffixes>
-							<argLine>-Xms256m -Xmx800m -Dlog4j.configuration=${log4j.configuration} -Dlog.dir=${log.dir} -Dmvn.forkNumber=1 -XX:-UseGCOverheadLimit</argLine>
-						</configuration>
-					</execution>
-				</executions>
-			</plugin>
-
 			<plugin>
 				<groupId>org.scalastyle</groupId>
 				<artifactId>scalastyle-maven-plugin</artifactId>
diff --git a/flink-staging/flink-scala-shell/src/main/scala/org/apache/flink/api/scala/FlinkILoop.scala b/flink-staging/flink-scala-shell/src/main/scala/org/apache/flink/api/scala/FlinkILoop.scala
index 9fb45a83fc3..a26ac2ecdc4 100644
--- a/flink-staging/flink-scala-shell/src/main/scala/org/apache/flink/api/scala/FlinkILoop.scala
+++ b/flink-staging/flink-scala-shell/src/main/scala/org/apache/flink/api/scala/FlinkILoop.scala
@@ -233,6 +233,5 @@ HINT: You can use print() on a DataSet to print the contents to this shell.
   }
 
   def getExternalJars(): Array[String] = externalJars.getOrElse(Array.empty[String])
-
 }
 
diff --git a/flink-staging/flink-scala-shell/src/main/scala/org/apache/flink/api/scala/FlinkShell.scala b/flink-staging/flink-scala-shell/src/main/scala/org/apache/flink/api/scala/FlinkShell.scala
index 54bbf80eff5..eb7f816ef38 100644
--- a/flink-staging/flink-scala-shell/src/main/scala/org/apache/flink/api/scala/FlinkShell.scala
+++ b/flink-staging/flink-scala-shell/src/main/scala/org/apache/flink/api/scala/FlinkShell.scala
@@ -22,7 +22,7 @@ import java.io.{StringWriter, BufferedReader}
 
 import org.apache.flink.api.common.ExecutionMode
 
-import org.apache.flink.configuration.Configuration
+import org.apache.flink.configuration.{ConfigConstants, Configuration}
 import org.apache.flink.runtime.minicluster.LocalFlinkMiniCluster
 
 import scala.tools.nsc.Settings
@@ -86,7 +86,7 @@ object FlinkShell {
           config.flinkShellExecutionMode,
           config.externalJars)
 
-      case _ => println("Could not parse program arguments")
+      case _ => System.out.println("Could not parse program arguments")
     }
   }
 
@@ -97,36 +97,41 @@ object FlinkShell {
       executionMode: ExecutionMode.Value,
       externalJars: Option[Array[String]] = None): Unit ={
     
-    println("Starting Flink Shell:")
+    System.out.println("Starting Flink Shell:")
 
     // either port or userhost not specified by user, create new minicluster
     val (host: String, port: Int, cluster: Option[LocalFlinkMiniCluster]) =
       executionMode match {
         case ExecutionMode.LOCAL =>
-          val miniCluster = new LocalFlinkMiniCluster(new Configuration, false)
+          val config = new Configuration()
+          config.setInteger(ConfigConstants.JOB_MANAGER_IPC_PORT_KEY, 0)
+          val miniCluster = new LocalFlinkMiniCluster(config, false)
           miniCluster.start()
           val port = miniCluster.getLeaderRPCPort
-          println(s"\nStarting local Flink cluster (host: localhost, port: $port).\n")
+          System.out.println(s"\nStarting local Flink cluster (host: localhost, port: $port).\n")
           ("localhost", port, Some(miniCluster))
 
         case ExecutionMode.REMOTE =>
           if (userHost == "none" || userPort == -1) {
-            println("Error: <host> or <port> not specified!")
+            System.out.println("Error: <host> or <port> not specified!")
             return
           } else {
-            println(s"\nConnecting to Flink cluster (host: $userHost, port: $userPort).\n")
+            System.out.println(
+              s"\nConnecting to Flink cluster (host: $userHost, port: $userPort).\n")
             (userHost, userPort, None)
           }
 
         case ExecutionMode.UNDEFINED =>
-          println("Error: please specify execution mode:")
-          println("[local | remote <host> <port>]")
+          System.out.println("Error: please specify execution mode:")
+          System.out.println("[local | remote <host> <port>]")
           return
       }
 
+    var repl: Option[FlinkILoop] = None
+
     try {
       // custom shell
-      val repl: FlinkILoop =
+      repl = Some(
         bufferedReader match {
 
           case Some(br) =>
@@ -135,21 +140,20 @@ object FlinkShell {
 
           case None =>
             new FlinkILoop(host, port, externalJars)
-        }
+        })
 
       val settings = new Settings()
 
       settings.usejavacp.value = true
+      settings.Yreplsync.value = true
 
       // start scala interpreter shell
-      repl.process(settings)
+      repl.foreach(_.process(settings))
     } finally {
-      cluster match {
-        case Some(c) => c.stop()
-        case None =>
-      }
+      repl.foreach(_.closeInterpreter())
+      cluster.foreach(_.stop())
     }
 
-    println(" good bye ..")
+    System.out.println(" good bye ..")
   }
 }
diff --git a/flink-staging/flink-scala-shell/src/test/scala/org/apache/flink/api/scala/ScalaShellITSuite.scala b/flink-staging/flink-scala-shell/src/test/scala/org/apache/flink/api/scala/ScalaShellITCase.scala
similarity index 60%
rename from flink-staging/flink-scala-shell/src/test/scala/org/apache/flink/api/scala/ScalaShellITSuite.scala
rename to flink-staging/flink-scala-shell/src/test/scala/org/apache/flink/api/scala/ScalaShellITCase.scala
index c8b1990d862..de2f3ec65ea 100644
--- a/flink-staging/flink-scala-shell/src/test/scala/org/apache/flink/api/scala/ScalaShellITSuite.scala
+++ b/flink-staging/flink-scala-shell/src/test/scala/org/apache/flink/api/scala/ScalaShellITCase.scala
@@ -22,21 +22,20 @@ import java.io._
 import java.util.concurrent.TimeUnit
 
 import org.apache.flink.runtime.StreamingMode
-import org.apache.flink.test.util.{TestEnvironment, ForkableFlinkMiniCluster, TestBaseUtils}
-import org.junit.runner.RunWith
-import org.scalatest.junit.JUnitRunner
-import org.scalatest.{BeforeAndAfterAll, FunSuite, Matchers}
+import org.apache.flink.test.util.{ForkableFlinkMiniCluster, TestBaseUtils}
+import org.apache.flink.util.TestLogger
+import org.junit.{AfterClass, BeforeClass, Test, Assert}
 
 import scala.concurrent.duration.FiniteDuration
 import scala.tools.nsc.Settings
 
-@RunWith(classOf[JUnitRunner])
-class ScalaShellITSuite extends FunSuite with Matchers with BeforeAndAfterAll {
+class ScalaShellITCase extends TestLogger {
 
-  var cluster: Option[ForkableFlinkMiniCluster] = None
-  val parallelism = 4
+  import ScalaShellITCase._
 
-  test("Prevent re-creation of environment") {
+  /** Prevent re-creation of environment */
+  @Test
+  def testPreventRecreation(): Unit = {
 
     val input: String =
       """
@@ -45,11 +44,14 @@ class ScalaShellITSuite extends FunSuite with Matchers with BeforeAndAfterAll {
 
     val output: String = processInShell(input)
 
-    output should include("UnsupportedOperationException: Execution Environment is already " +
-      "defined for this shell")
+    Assert.assertTrue(output.contains(
+      "UnsupportedOperationException: Execution Environment is already " +
+      "defined for this shell"))
   }
 
-  test("Iteration test with iterative Pi example") {
+  /** Iteration test with iterative Pi example */
+  @Test
+  def testIterativePI(): Unit = {
 
     val input: String =
       """
@@ -68,12 +70,14 @@ class ScalaShellITSuite extends FunSuite with Matchers with BeforeAndAfterAll {
 
     val output: String = processInShell(input)
 
-    output should not include "failed"
-    output should not include "error"
-    output should not include "Exception"
+    Assert.assertFalse(output.contains("failed"))
+    Assert.assertFalse(output.contains("error"))
+    Assert.assertFalse(output.contains("Exception"))
   }
 
-  test("WordCount in Shell") {
+  /** WordCount in Shell */
+  @Test
+  def testWordCount(): Unit = {
     val input =
       """
         val text = env.fromElements("To be, or not to be,--that is the question:--",
@@ -86,18 +90,20 @@ class ScalaShellITSuite extends FunSuite with Matchers with BeforeAndAfterAll {
 
     val output = processInShell(input)
 
-    output should not include "failed"
-    output should not include "error"
-    output should not include "Exception"
+    Assert.assertFalse(output.contains("failed"))
+    Assert.assertFalse(output.contains("error"))
+    Assert.assertFalse(output.contains("Exception"))
 
     // some of the words that should be included
-    output should include("(a,1)")
-    output should include("(whether,1)")
-    output should include("(to,4)")
-    output should include("(arrows,1)")
+    Assert.assertTrue(output.contains("(a,1)"))
+    Assert.assertTrue(output.contains("(whether,1)"))
+    Assert.assertTrue(output.contains("(to,4)"))
+    Assert.assertTrue(output.contains("(arrows,1)"))
   }
 
-  test("Sum 1..10, should be 55") {
+  /** Sum 1..10, should be 55 */
+  @Test
+  def testSum: Unit = {
     val input =
       """
         val input: DataSet[Int] = env.fromElements(0,1,2,3,4,5,6,7,8,9,10)
@@ -107,14 +113,16 @@ class ScalaShellITSuite extends FunSuite with Matchers with BeforeAndAfterAll {
 
     val output = processInShell(input)
 
-    output should not include "failed"
-    output should not include "error"
-    output should not include "Exception"
+    Assert.assertFalse(output.contains("failed"))
+    Assert.assertFalse(output.contains("error"))
+    Assert.assertFalse(output.contains("Exception"))
 
-    output should include("55")
+    Assert.assertTrue(output.contains("55"))
   }
 
-  test("WordCount in Shell with custom case class") {
+  /** WordCount in Shell with custom case class */
+  @Test
+  def testWordCountWithCustomCaseClass: Unit = {
     val input =
       """
       case class WC(word: String, count: Int)
@@ -128,15 +136,17 @@ class ScalaShellITSuite extends FunSuite with Matchers with BeforeAndAfterAll {
 
     val output = processInShell(input)
 
-    output should not include "failed"
-    output should not include "error"
-    output should not include "Exception"
+    Assert.assertFalse(output.contains("failed"))
+    Assert.assertFalse(output.contains("error"))
+    Assert.assertFalse(output.contains("Exception"))
 
-    output should include("WC(hello,1)")
-    output should include("WC(world,10)")
+    Assert.assertTrue(output.contains("WC(hello,1)"))
+    Assert.assertTrue(output.contains("WC(world,10)"))
   }
 
-  test("Submit external library") {
+  /** Submit external library */
+  @Test
+  def testSubmissionOfExternalLibrary: Unit = {
     val input =
       """
         import org.apache.flink.ml.math._
@@ -153,7 +163,6 @@ class ScalaShellITSuite extends FunSuite with Matchers with BeforeAndAfterAll {
       val filename: String = listOfFiles(i).getName
       if (!filename.contains("test") && !filename.contains("original") && filename.contains(
         ".jar")) {
-        println("ive found file:" + listOfFiles(i).getAbsolutePath)
         externalJar = listOfFiles(i).getAbsolutePath
       }
     }
@@ -162,11 +171,109 @@ class ScalaShellITSuite extends FunSuite with Matchers with BeforeAndAfterAll {
 
     val output: String = processInShell(input, Option(externalJar))
 
-    output should not include "failed"
-    output should not include "error"
-    output should not include "Exception"
+    Assert.assertFalse(output.contains("failed"))
+    Assert.assertFalse(output.contains("error"))
+    Assert.assertFalse(output.contains("Exception"))
+
+    Assert.assertTrue(output.contains("\nDenseVector(1.0, 2.0, 3.0)"))
+  }
+
+
+  /**
+   * tests flink shell startup with remote cluster (starts cluster internally)
+   */
+  @Test
+  def testRemoteCluster: Unit = {
+
+    val input: String =
+      """
+        |import org.apache.flink.api.common.functions.RichMapFunction
+        |import org.apache.flink.api.java.io.PrintingOutputFormat
+        |import org.apache.flink.api.common.accumulators.IntCounter
+        |import org.apache.flink.configuration.Configuration
+        |
+        |val els = env.fromElements("foobar","barfoo")
+        |val mapped = els.map{
+        | new RichMapFunction[String, String]() {
+        |   var intCounter: IntCounter = _
+        |   override def open(conf: Configuration): Unit = {
+        |     intCounter = getRuntimeContext.getIntCounter("intCounter")
+        |   }
+        |
+        |   def map(element: String): String = {
+        |     intCounter.add(1)
+        |     element
+        |   }
+        | }
+        |}
+        |mapped.output(new PrintingOutputFormat())
+        |val executionResult = env.execute("Test Job")
+        |System.out.println("IntCounter: " + executionResult.getIntCounterResult("intCounter"))
+        |
+        |:q
+      """.stripMargin
+
+    val in: BufferedReader = new BufferedReader(
+      new StringReader(
+        input + "\n"))
+    val out: StringWriter = new StringWriter
+
+    val baos: ByteArrayOutputStream = new ByteArrayOutputStream
+    val oldOut: PrintStream = System.out
+    System.setOut(new PrintStream(baos))
+
+    val (c, args) = cluster match{
+      case Some(cl) =>
+        val arg = Array("remote",
+          cl.hostname,
+          Integer.toString(cl.getLeaderRPCPort))
+        (cl, arg)
+      case None =>
+        throw new AssertionError("Cluster creation failed.")
+    }
+
+    //start scala shell with initialized
+    // buffered reader for testing
+    FlinkShell.bufferedReader = Some(in)
+    FlinkShell.main(args)
+    baos.flush()
+
+    val output: String = baos.toString
+    System.setOut(oldOut)
+
+    Assert.assertTrue(output.contains("IntCounter: 2"))
+    Assert.assertTrue(output.contains("foobar"))
+    Assert.assertTrue(output.contains("barfoo"))
+
+    Assert.assertFalse(output.contains("failed"))
+    Assert.assertFalse(output.contains("Error"))
+    Assert.assertFalse(output.contains("ERROR"))
+    Assert.assertFalse(output.contains("Exception"))
+  }
+}
+
+object ScalaShellITCase {
+  var cluster: Option[ForkableFlinkMiniCluster] = None
+  val parallelism = 4
+
+  @BeforeClass
+  def beforeAll(): Unit = {
+    val cl = TestBaseUtils.startCluster(
+      1,
+      parallelism,
+      StreamingMode.BATCH_ONLY,
+      false,
+      false,
+      false)
+
+    cluster = Some(cl)
+  }
 
-    output should include("\nDenseVector(1.0, 2.0, 3.0)")
+  @AfterClass
+  def afterAll(): Unit = {
+    // The Scala interpreter somehow changes the class loader. Therfore, we have to reset it
+    Thread.currentThread().setContextClassLoader(classOf[ScalaShellITCase].getClassLoader)
+    cluster.foreach(c => TestBaseUtils.stopCluster(c, new FiniteDuration(1000, TimeUnit.SECONDS)))
   }
 
   /**
@@ -222,65 +329,4 @@ class ScalaShellITSuite extends FunSuite with Matchers with BeforeAndAfterAll {
 
     out.toString + stdout
   }
-
-  /**
-   * tests flink shell startup with remote cluster (starts cluster internally)
-   */
-  test("start flink scala shell with remote cluster") {
-
-    val input: String = "val els = env.fromElements(\"a\",\"b\");\n" +
-      "els.print\nError\n:q\n"
-
-    val in: BufferedReader = new BufferedReader(
-      new StringReader(
-        input + "\n"))
-    val out: StringWriter = new StringWriter
-
-    val baos: ByteArrayOutputStream = new ByteArrayOutputStream
-    val oldOut: PrintStream = System.out
-    System.setOut(new PrintStream(baos))
-
-    val (c, args) = cluster match{
-      case Some(cl) =>
-        val arg = Array("remote",
-          cl.hostname,
-          Integer.toString(cl.getLeaderRPCPort))
-        (cl, arg)
-      case None =>
-        fail("Cluster creation failed!")
-    }
-
-    //start scala shell with initialized
-    // buffered reader for testing
-    FlinkShell.bufferedReader = Some(in)
-    FlinkShell.main(args)
-    baos.flush()
-
-    val output: String = baos.toString
-    System.setOut(oldOut)
-
-    output should include("Job execution switched to status FINISHED.")
-    output should include("a\nb")
-
-    output should not include "Error"
-    output should not include "ERROR"
-    output should not include "Exception"
-    output should not include "failed"
-  }
-
-  override def beforeAll(): Unit = {
-    val cl = TestBaseUtils.startCluster(
-      1,
-      parallelism,
-      StreamingMode.BATCH_ONLY,
-      false,
-      false,
-      false)
-
-    cluster = Some(cl)
-  }
-
-  override def afterAll(): Unit = {
-    cluster.foreach(c => TestBaseUtils.stopCluster(c, new FiniteDuration(1000, TimeUnit.SECONDS)))
-  }
 }
diff --git a/flink-staging/flink-scala-shell/src/test/scala/org/apache/flink/api/scala/ScalaShellLocalStartupITCase.scala b/flink-staging/flink-scala-shell/src/test/scala/org/apache/flink/api/scala/ScalaShellLocalStartupITCase.scala
index 60da09eee25..57bbd9b13de 100644
--- a/flink-staging/flink-scala-shell/src/test/scala/org/apache/flink/api/scala/ScalaShellLocalStartupITCase.scala
+++ b/flink-staging/flink-scala-shell/src/test/scala/org/apache/flink/api/scala/ScalaShellLocalStartupITCase.scala
@@ -20,20 +20,44 @@ package org.apache.flink.api.scala
 
 import java.io._
 
-import org.junit.runner.RunWith
-import org.scalatest.{Matchers, FunSuite}
-import org.scalatest.junit.JUnitRunner
+import org.apache.flink.util.TestLogger
+import org.junit.Test
+import org.junit.Assert
 
+class ScalaShellLocalStartupITCase extends TestLogger {
 
-@RunWith(classOf[JUnitRunner])
-class ScalaShellLocalStartupITCase extends FunSuite with Matchers {
-
-    /**
-     * tests flink shell with local setup through startup script in bin folder
-     */
-    test("start flink scala shell with local cluster") {
-
-      val input: String = "val els = env.fromElements(\"a\",\"b\");\n" + "els.print\nError\n:q\n"
+  /**
+   * tests flink shell with local setup through startup script in bin folder
+   */
+  @Test
+  def testLocalCluster: Unit = {
+    val input: String =
+      """
+        |import org.apache.flink.api.common.functions.RichMapFunction
+        |import org.apache.flink.api.java.io.PrintingOutputFormat
+        |import org.apache.flink.api.common.accumulators.IntCounter
+        |import org.apache.flink.configuration.Configuration
+        |
+        |val els = env.fromElements("foobar","barfoo")
+        |val mapped = els.map{
+        | new RichMapFunction[String, String]() {
+        |   var intCounter: IntCounter = _
+        |   override def open(conf: Configuration): Unit = {
+        |     intCounter = getRuntimeContext.getIntCounter("intCounter")
+        |   }
+        |
+        |   def map(element: String): String = {
+        |     intCounter.add(1)
+        |     element
+        |   }
+        | }
+        |}
+        |mapped.output(new PrintingOutputFormat())
+        |val executionResult = env.execute("Test Job")
+        |System.out.println("IntCounter: " + executionResult.getIntCounterResult("intCounter"))
+        |
+        |:q
+      """.stripMargin
       val in: BufferedReader = new BufferedReader(new StringReader(input + "\n"))
       val out: StringWriter = new StringWriter
       val baos: ByteArrayOutputStream = new ByteArrayOutputStream
@@ -41,20 +65,21 @@ class ScalaShellLocalStartupITCase extends FunSuite with Matchers {
       System.setOut(new PrintStream(baos))
       val args: Array[String] = Array("local")
 
-      //start flink scala shell
-      FlinkShell.bufferedReader = Some(in);
-      FlinkShell.main(args)
+    //start flink scala shell
+    FlinkShell.bufferedReader = Some(in);
+    FlinkShell.main(args)
 
-      baos.flush()
-      val output: String = baos.toString
-      System.setOut(oldOut)
+    baos.flush()
+    val output: String = baos.toString
+    System.setOut(oldOut)
 
-      output should include("Job execution switched to status FINISHED.")
-      output should include("a\nb")
+    Assert.assertTrue(output.contains("IntCounter: 2"))
+    Assert.assertTrue(output.contains("foobar"))
+    Assert.assertTrue(output.contains("barfoo"))
 
-      output should not include "Error"
-      output should not include "ERROR"
-      output should not include "Exception"
-      output should not include "failed"
-    }
+    Assert.assertFalse(output.contains("failed"))
+    Assert.assertFalse(output.contains("Error"))
+    Assert.assertFalse(output.contains("ERROR"))
+    Assert.assertFalse(output.contains("Exception"))
+  }
 }
diff --git a/tools/log4j-travis.properties b/tools/log4j-travis.properties
index d55209e30f6..53379b4ed30 100644
--- a/tools/log4j-travis.properties
+++ b/tools/log4j-travis.properties
@@ -40,7 +40,6 @@ log4j.logger.org.apache.zookeeper=ERROR
 log4j.logger.org.apache.zookeeper.server.quorum.QuorumCnxManager=OFF
 log4j.logger.org.apache.flink.runtime.leaderelection=DEBUG
 log4j.logger.org.apache.flink.runtime.leaderretrieval=DEBUG
-log4j.logger.org.apache.flink.runtime.executiongraph=DEBUG
 
 # Log a bit when running the flink-yarn-tests to avoid running into the 5 minutes timeout for
 # the tests
