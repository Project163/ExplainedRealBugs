diff --git a/flink-contrib/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/AbstractRocksDBState.java b/flink-contrib/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/AbstractRocksDBState.java
index 3cf9ed3c692..ef58c795635 100644
--- a/flink-contrib/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/AbstractRocksDBState.java
+++ b/flink-contrib/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/AbstractRocksDBState.java
@@ -31,7 +31,6 @@ import org.apache.flink.runtime.state.KvStateSnapshot;
 
 import org.apache.flink.streaming.util.HDFSCopyFromLocal;
 import org.apache.flink.streaming.util.HDFSCopyToLocal;
-import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 
@@ -47,9 +46,7 @@ import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 import java.io.ByteArrayOutputStream;
-import java.io.DataOutputStream;
 import java.io.File;
-import java.io.FileOutputStream;
 import java.io.IOException;
 import java.net.URI;
 import java.util.UUID;
@@ -76,8 +73,6 @@ import static java.util.Objects.requireNonNull;
 public abstract class AbstractRocksDBState<K, N, S extends State, SD extends StateDescriptor<S, ?>>
 	implements KvState<K, N, S, SD, RocksDBStateBackend>, State {
 
-	private static final String HADOOP_CONF_NAME = "hadoop-conf.binary";
-
 	private static final Logger LOG = LoggerFactory.getLogger(AbstractRocksDBState.class);
 
 	/** Serializer for the keys */
@@ -101,12 +96,6 @@ public abstract class AbstractRocksDBState<K, N, S extends State, SD extends Sta
 	/** Directory in "basePath" where the actual RocksDB data base instance stores its files */
 	protected final File rocksDbPath;
 
-	/**
-	 * File where we store a serialized Hadoop Configuration for use by the external process
-	 * HDFS copy utilities.
-	 */
-	protected File hadoopConfPath;
-
 	/** Our RocksDB instance */
 	protected final RocksDB db;
 
@@ -125,7 +114,6 @@ public abstract class AbstractRocksDBState<K, N, S extends State, SD extends Sta
 			Options options) {
 
 		rocksDbPath = new File(basePath, "db" + UUID.randomUUID().toString());
-		hadoopConfPath = new File(basePath, HADOOP_CONF_NAME);
 
 		this.keySerializer = requireNonNull(keySerializer);
 		this.namespaceSerializer = namespaceSerializer;
@@ -155,8 +143,6 @@ public abstract class AbstractRocksDBState<K, N, S extends State, SD extends Sta
 		} catch (RocksDBException e) {
 			throw new RuntimeException("Error while opening RocksDB instance.", e);
 		}
-
-		writeHadoopConfig(hadoopConfPath);
 	}
 
 	/**
@@ -177,7 +163,6 @@ public abstract class AbstractRocksDBState<K, N, S extends State, SD extends Sta
 			Options options) {
 
 		rocksDbPath = new File(basePath, "db" + UUID.randomUUID().toString());
-		hadoopConfPath = new File(basePath, HADOOP_CONF_NAME);
 
 		RocksDB.loadLibrary();
 
@@ -219,23 +204,6 @@ public abstract class AbstractRocksDBState<K, N, S extends State, SD extends Sta
 		} catch (RocksDBException e) {
 			throw new RuntimeException("Error while opening RocksDB instance.", e);
 		}
-
-		writeHadoopConfig(hadoopConfPath);
-	}
-
-	private static void writeHadoopConfig(File hadoopConfPath) {
-		Configuration conf = HadoopFileSystem.getHadoopConfiguration();
-
-		if (hadoopConfPath.exists()) {
-			if (!hadoopConfPath.delete()) {
-				throw new RuntimeException("Error deleting existing Hadoop configuration: " + hadoopConfPath);
-			}
-		}
-		try (DataOutputStream out = new DataOutputStream(new FileOutputStream(hadoopConfPath))) {
-			conf.write(out);
-		} catch (IOException e) {
-			LOG.error("Error writing Hadoop Configuration.", e);
-		}
 	}
 
 	// ------------------------------------------------------------------------
@@ -408,9 +376,7 @@ public abstract class AbstractRocksDBState<K, N, S extends State, SD extends Sta
 				}
 			}
 
-			writeHadoopConfig(new File(basePath, HADOOP_CONF_NAME));
-
-			HDFSCopyToLocal.copyToLocal(new File(basePath, HADOOP_CONF_NAME), backupUri, basePath);
+			HDFSCopyToLocal.copyToLocal(backupUri, basePath);
 			return createRocksDBState(keySerializer, namespaceSerializer, stateDesc, basePath,
 					checkpointPath, localBackupPath.getAbsolutePath(), stateBackend.getRocksDBOptions());
 		}
@@ -453,7 +419,7 @@ public abstract class AbstractRocksDBState<K, N, S extends State, SD extends Sta
 		@Override
 		public KvStateSnapshot<K, N, S, SD, RocksDBStateBackend> materialize() throws Exception {
 			try {
-				HDFSCopyFromLocal.copyFromLocal(state.hadoopConfPath, localBackupPath, backupUri);
+				HDFSCopyFromLocal.copyFromLocal(localBackupPath, backupUri);
 				return state.createRocksDBSnapshot(backupUri, checkpointId);
 			} catch (Exception e) {
 				FileSystem fs = FileSystem.get(backupUri, HadoopFileSystem.getHadoopConfiguration());
diff --git a/flink-contrib/flink-statebackend-rocksdb/src/test/java/org/apache/flink/contrib/streaming/state/RocksDBAsyncKVSnapshotTest.java b/flink-contrib/flink-statebackend-rocksdb/src/test/java/org/apache/flink/contrib/streaming/state/RocksDBAsyncKVSnapshotTest.java
index 260819e4cb1..8806c7fc649 100644
--- a/flink-contrib/flink-statebackend-rocksdb/src/test/java/org/apache/flink/contrib/streaming/state/RocksDBAsyncKVSnapshotTest.java
+++ b/flink-contrib/flink-statebackend-rocksdb/src/test/java/org/apache/flink/contrib/streaming/state/RocksDBAsyncKVSnapshotTest.java
@@ -19,7 +19,6 @@
 package org.apache.flink.contrib.streaming.state;
 
 import org.apache.flink.api.common.functions.MapFunction;
-import org.apache.flink.api.common.state.ReducingStateDescriptor;
 import org.apache.flink.api.common.state.ValueState;
 import org.apache.flink.api.common.state.ValueStateDescriptor;
 import org.apache.flink.api.common.typeinfo.BasicTypeInfo;
@@ -29,7 +28,6 @@ import org.apache.flink.api.java.functions.KeySelector;
 import org.apache.flink.configuration.ConfigConstants;
 import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;
 import org.apache.flink.runtime.operators.testutils.MockInputSplitProvider;
-import org.apache.flink.runtime.state.AsynchronousStateHandle;
 import org.apache.flink.runtime.state.StateHandle;
 import org.apache.flink.runtime.state.memory.MemoryStateBackend;
 import org.apache.flink.runtime.taskmanager.OneShotLatch;
@@ -44,13 +42,18 @@ import org.apache.flink.streaming.runtime.tasks.StreamMockEnvironment;
 import org.apache.flink.streaming.runtime.tasks.StreamTask;
 import org.apache.flink.streaming.runtime.tasks.StreamTaskState;
 import org.apache.flink.streaming.runtime.tasks.StreamTaskStateList;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.LocalFileSystem;
 import org.junit.Test;
 import org.junit.runner.RunWith;
+import org.powermock.api.mockito.PowerMockito;
 import org.powermock.core.classloader.annotations.PrepareForTest;
 import org.powermock.modules.junit4.PowerMockRunner;
 
 import java.io.File;
 import java.lang.reflect.Field;
+import java.net.URI;
 import java.util.UUID;
 
 import static org.junit.Assert.assertEquals;
@@ -60,7 +63,7 @@ import static org.junit.Assert.assertTrue;
  * Tests for asynchronous RocksDB Key/Value state checkpoints.
  */
 @RunWith(PowerMockRunner.class)
-@PrepareForTest(ResultPartitionWriter.class)
+@PrepareForTest({ResultPartitionWriter.class, FileSystem.class})
 @SuppressWarnings("serial")
 public class RocksDBAsyncKVSnapshotTest {
 
@@ -73,6 +76,10 @@ public class RocksDBAsyncKVSnapshotTest {
 	 */
 	@Test
 	public void testAsyncCheckpoints() throws Exception {
+		LocalFileSystem localFS = new LocalFileSystem();
+		localFS.initialize(new URI("file:///"), new Configuration());
+		PowerMockito.stub(PowerMockito.method(FileSystem.class, "get", URI.class, Configuration.class)).toReturn(localFS);
+
 		final OneShotLatch delayCheckpointLatch = new OneShotLatch();
 		final OneShotLatch ensureCheckpointLatch = new OneShotLatch();
 
diff --git a/flink-core/src/main/java/org/apache/flink/util/ExternalProcessRunner.java b/flink-core/src/main/java/org/apache/flink/util/ExternalProcessRunner.java
deleted file mode 100644
index a9ce6b382b9..00000000000
--- a/flink-core/src/main/java/org/apache/flink/util/ExternalProcessRunner.java
+++ /dev/null
@@ -1,238 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.flink.util;
-
-import org.apache.flink.annotation.Internal;
-
-import java.io.File;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.StringWriter;
-import java.lang.management.ManagementFactory;
-import java.lang.management.RuntimeMXBean;
-import java.lang.reflect.Method;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-
-/**
- * Utility class for running a class in an external process. This will try to find the java
- * executable in common places and will use the classpath of the current process as the classpath
- * of the new process.
- *
- * <p>Attention: The entry point class must be in the classpath of the currently running process,
- * otherwise the newly spawned process will not find it and fail.
- */
-@Internal
-public class ExternalProcessRunner {
-	private final Process process;
-
-	private final Thread pipeForwarder;
-
-	final StringWriter errorOutput = new StringWriter();
-
-	/**
-	 * Creates a new {@code ProcessRunner} that runs the given class with the given parameters.
-	 * The class must have a "main" method.
-	 */
-	public ExternalProcessRunner(String entryPointClassName, String[] parameters) throws IOException {
-		String javaCommand = getJavaCommandPath();
-
-		List<String> commandList = new ArrayList<>();
-
-		commandList.add(javaCommand);
-		commandList.add("-classpath");
-		commandList.add(getCurrentClasspath());
-		commandList.add(entryPointClassName);
-
-		Collections.addAll(commandList, parameters);
-
-		process = new ProcessBuilder(commandList).start();
-
-		pipeForwarder = new PipeForwarder(process.getErrorStream(), errorOutput);
-	}
-
-	/**
-	 * Get the stderr stream of the process.
-	 */
-	public StringWriter getErrorOutput() {
-		return errorOutput;
-	}
-
-	/**
-	 * Start the external process, wait for it to finish and return the exit code of that process.
-	 *
-	 * <p>If this method is interrupted it will destroy the external process and forward the
-	 * {@code InterruptedException}.
-	 */
-	public int run() throws Exception {
-		try {
-			int returnCode = process.waitFor();
-
-			// wait to finish copying standard error stream
-			pipeForwarder.join();
-
-			if (returnCode != 0) {
-
-				final String errorOutput = getErrorOutput().toString();
-				if (!errorOutput.isEmpty()) {
-					throw new RuntimeException(errorOutput);
-				}
-
-			}
-			return returnCode;
-		} catch (InterruptedException e) {
-			try {
-				Class<?> processClass = process.getClass();
-				Method destroyForcibly = processClass.getMethod("destroyForcibly");
-				destroyForcibly.setAccessible(true);
-				destroyForcibly.invoke(process);
-			} catch (NoSuchMethodException ex) {
-				// we don't have destroyForcibly
-				process.destroy();
-			}
-			throw new InterruptedException("Interrupted while waiting for external process.");
-		}
-	}
-
-	/**
-	 * Tries to get the java executable command with which the current JVM was started.
-	 * Returns null, if the command could not be found.
-	 *
-	 * @return The java executable command.
-	 */
-	public static String getJavaCommandPath() {
-
-		try {
-			ProcessBuilder bld = new ProcessBuilder("java", "-version");
-			Process process = bld.start();
-			if (process.waitFor() == 0) {
-				return "java";
-			}
-		}
-		catch (Throwable t) {
-			// ignore and try the second path
-		}
-
-		try {
-			ProcessBuilder bld = new ProcessBuilder("java.exe", "-version");
-			Process process = bld.start();
-			if (process.waitFor() == 0) {
-				return "java.exe";
-			}
-		}
-		catch (Throwable t) {
-			// ignore and try the second path
-		}
-
-		File javaHome = new File(System.getProperty("java.home"));
-
-		String path1 = new File(javaHome, "java").getAbsolutePath();
-		String path2 = new File(new File(javaHome, "bin"), "java").getAbsolutePath();
-
-		try {
-			ProcessBuilder bld = new ProcessBuilder(path1, "-version");
-			Process process = bld.start();
-			if (process.waitFor() == 0) {
-				return path1;
-			}
-		}
-		catch (Throwable t) {
-			// ignore and try the second path
-		}
-
-		try {
-			ProcessBuilder bld = new ProcessBuilder(path2, "-version");
-			Process process = bld.start();
-			if (process.waitFor() == 0) {
-				return path2;
-			}
-		}
-		catch (Throwable tt) {
-			// no luck
-		}
-
-		String path3 = new File(javaHome, "java.exe").getAbsolutePath();
-		String path4 = new File(new File(javaHome, "bin"), "java.exe").getAbsolutePath();
-
-		try {
-			ProcessBuilder bld = new ProcessBuilder(path3, "-version");
-			Process process = bld.start();
-			if (process.waitFor() == 0) {
-				return path3;
-			}
-		}
-		catch (Throwable t) {
-			// ignore and try the second path
-		}
-
-		try {
-			ProcessBuilder bld = new ProcessBuilder(path4, "-version");
-			Process process = bld.start();
-			if (process.waitFor() == 0) {
-				return path4;
-			}
-		}
-		catch (Throwable tt) {
-			// no luck
-		}
-		return null;
-	}
-
-	/**
-	 * Gets the classpath with which the current JVM was started.
-	 *
-	 * @return The classpath with which the current JVM was started.
-	 */
-	public static String getCurrentClasspath() {
-		RuntimeMXBean bean = ManagementFactory.getRuntimeMXBean();
-		return bean.getClassPath();
-	}
-
-	/**
-	 * Utility class to read the output of a process stream and forward it into a StringWriter.
-	 */
-	public static class PipeForwarder extends Thread {
-
-		private final StringWriter target;
-		private final InputStream source;
-
-		public PipeForwarder(InputStream source, StringWriter target) {
-			super("Pipe Forwarder");
-			setDaemon(true);
-
-			this.source = source;
-			this.target = target;
-
-			start();
-		}
-
-		@Override
-		public void run() {
-			try {
-				int next;
-				while ((next = source.read()) != -1) {
-					target.write(next);
-				}
-			}
-			catch (IOException e) {
-				// terminate
-			}
-		}
-	}
-}
diff --git a/flink-core/src/test/java/org/apache/flink/util/ExternalProcessRunnerTest.java b/flink-core/src/test/java/org/apache/flink/util/ExternalProcessRunnerTest.java
deleted file mode 100644
index f179cbf4942..00000000000
--- a/flink-core/src/test/java/org/apache/flink/util/ExternalProcessRunnerTest.java
+++ /dev/null
@@ -1,98 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.flink.util;
-
-import org.junit.Test;
-
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.fail;
-
-public class ExternalProcessRunnerTest {
-
-	@Test(expected = ClassNotFoundException.class)
-	public void testClassNotFound() throws Exception {
-		final String nonExistingClassName = "MyClassThatDoesNotExist";
-		ExternalProcessRunner runner = new ExternalProcessRunner(nonExistingClassName, new String[]{});
-		try {
-			runner.run();
-		} catch (final Exception e) {
-			if (e.getMessage().contains(nonExistingClassName)) {
-				throw new ClassNotFoundException();
-			}
-		}
-	}
-
-	@Test
-	public void testInterrupting() throws Exception {
-
-		final ExternalProcessRunner runner = new ExternalProcessRunner(InfiniteLoop.class.getName(), new String[]{});
-
-		Thread thread = new Thread() {
-			@Override
-			public void run() {
-				try {
-					runner.run();
-				} catch (InterruptedException e) {
-					// this is expected
-				} catch (Exception e) {
-					fail("Other exception received " + e);
-				}
-			}
-		};
-
-		thread.start();
-		thread.interrupt();
-		thread.join();
-	}
-
-	@Test
-	public void testPrintToErr() throws Exception {
-		final ExternalProcessRunner runner = new ExternalProcessRunner(PrintToError.class.getName(), new String[]{"hello42"});
-
-		int result = runner.run();
-
-		assertEquals(0, result);
-		assertEquals(runner.getErrorOutput().toString(), "Hello process hello42" + System.lineSeparator());
-	}
-
-	@Test(expected = RuntimeException.class)
-	public void testFailing() throws Exception {
-		final ExternalProcessRunner runner = new ExternalProcessRunner(Failing.class.getName(), new String[]{});
-		runner.run();
-	}
-
-	public static class InfiniteLoop {
-		public static void main(String[] args) {
-			while (true) {
-			}
-		}
-	}
-
-	public static class PrintToError {
-		public static void main(String[] args) {
-			System.err.println("Hello process " + args[0]);
-		}
-	}
-
-	public static class Failing {
-		public static void main(String[] args) {
-			throw new RuntimeException("HEHE, I'm failing.");
-		}
-	}
-
-}
diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/util/HDFSCopyFromLocal.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/util/HDFSCopyFromLocal.java
index be9304c3816..65b278a0d39 100644
--- a/flink-streaming-java/src/main/java/org/apache/flink/streaming/util/HDFSCopyFromLocal.java
+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/util/HDFSCopyFromLocal.java
@@ -17,41 +17,49 @@
  */
 package org.apache.flink.streaming.util;
 
-import org.apache.flink.util.ExternalProcessRunner;
+import org.apache.flink.api.java.tuple.Tuple1;
+import org.apache.flink.runtime.fs.hdfs.HadoopFileSystem;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 
-import java.io.DataInputStream;
 import java.io.File;
-import java.io.FileInputStream;
 import java.net.URI;
 
 /**
- * Utility for copying from local file system to a HDFS {@link FileSystem} in an external process.
- * This is required since {@code FileSystem.copyFromLocalFile} does not like being interrupted.
+ * Utility for copying from local file system to a HDFS {@link FileSystem}.
  */
 public class HDFSCopyFromLocal {
-	public static void main(String[] args) throws Exception {
-		String hadoopConfPath = args[0];
-		String localBackupPath = args[1];
-		String backupUri = args[2];
-
-		Configuration hadoopConf = new Configuration();
-		try (DataInputStream in = new DataInputStream(new FileInputStream(hadoopConfPath))) {
-			hadoopConf.readFields(in);
-		}
 
-		FileSystem fs = FileSystem.get(new URI(backupUri), hadoopConf);
+	public static void copyFromLocal(final File localPath,
+			final URI remotePath) throws Exception {
+		// Do it in another Thread because HDFS can deadlock if being interrupted while copying
+		String threadName = "HDFS Copy from " + localPath + " to " + remotePath;
 
-		fs.copyFromLocalFile(new Path(localBackupPath), new Path(backupUri));
-	}
+		final Tuple1<Exception> asyncException = Tuple1.of(null);
+
+		Thread copyThread = new Thread(threadName) {
+			@Override
+			public void run() {
+				try {
+					Configuration hadoopConf = HadoopFileSystem.getHadoopConfiguration();
+
+					FileSystem fs = FileSystem.get(remotePath, hadoopConf);
+
+					fs.copyFromLocalFile(new Path(localPath.getAbsolutePath()),
+							new Path(remotePath));
+				} catch (Exception t) {
+					asyncException.f0 = t;
+				}
+			}
+		};
+
+		copyThread.setDaemon(true);
+		copyThread.start();
+		copyThread.join();
 
-	public static void copyFromLocal(File hadoopConfPath, File localPath, URI remotePath) throws Exception {
-		ExternalProcessRunner processRunner = new ExternalProcessRunner(HDFSCopyFromLocal.class.getName(),
-			new String[]{hadoopConfPath.getAbsolutePath(), localPath.getAbsolutePath(), remotePath.toString()});
-		if (processRunner.run() != 0) {
-			throw new  RuntimeException("Error while copying to remote FileSystem: " + processRunner.getErrorOutput());
+		if (asyncException.f0 != null) {
+			throw asyncException.f0;
 		}
 	}
 }
diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/util/HDFSCopyToLocal.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/util/HDFSCopyToLocal.java
index 9379af88a31..7dfe7272b25 100644
--- a/flink-streaming-java/src/main/java/org/apache/flink/streaming/util/HDFSCopyToLocal.java
+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/util/HDFSCopyToLocal.java
@@ -17,42 +17,47 @@
  */
 package org.apache.flink.streaming.util;
 
-import org.apache.flink.util.ExternalProcessRunner;
+import org.apache.flink.api.java.tuple.Tuple1;
+import org.apache.flink.runtime.fs.hdfs.HadoopFileSystem;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 
-import java.io.DataInputStream;
 import java.io.File;
-import java.io.FileInputStream;
 import java.net.URI;
 
 /**
- * Utility for copying from a HDFS {@link FileSystem} to the local file system in an external
- * process. This is required since {@code FileSystem.copyToLocalFile} does not like being
- * interrupted.
+ * Utility for copying from a HDFS {@link FileSystem} to the local file system.
  */
 public class HDFSCopyToLocal {
-	public static void main(String[] args) throws Exception {
-		String hadoopConfPath = args[0];
-		String backupUri = args[1];
-		String dbPath = args[2];
-
-		Configuration hadoopConf = new Configuration();
-		try (DataInputStream in = new DataInputStream(new FileInputStream(hadoopConfPath))) {
-			hadoopConf.readFields(in);
-		}
 
-		FileSystem fs = FileSystem.get(new URI(backupUri), hadoopConf);
+	public static void copyToLocal(final URI remotePath,
+			final File localPath) throws Exception {
+		// Do it in another Thread because HDFS can deadlock if being interrupted while copying
+		String threadName = "HDFS Copy from " + remotePath + " to " + localPath;
 
-		fs.copyToLocalFile(new Path(backupUri), new Path(dbPath));
-	}
+		final Tuple1<Exception> asyncException = Tuple1.of(null);
+
+		Thread copyThread = new Thread(threadName) {
+			@Override
+			public void run() {
+				try {
+					Configuration hadoopConf = HadoopFileSystem.getHadoopConfiguration();
+
+					FileSystem fs = FileSystem.get(remotePath, hadoopConf);
+					fs.copyToLocalFile(new Path(remotePath), new Path(localPath.getAbsolutePath()));
+				} catch (Exception t) {
+					asyncException.f0 = t;
+				}
+			}
+		};
+
+		copyThread.setDaemon(true);
+		copyThread.start();
+		copyThread.join();
 
-	public static void copyToLocal(File hadoopConfPath, URI remotePath, File localPath) throws Exception {
-		ExternalProcessRunner processRunner = new ExternalProcessRunner(HDFSCopyToLocal.class.getName(),
-			new String[]{hadoopConfPath.getAbsolutePath(), remotePath.toString(), localPath.getAbsolutePath()});
-		if (processRunner.run() != 0) {
-			throw new  RuntimeException("Error while copying from remote FileSystem: " + processRunner.getErrorOutput());
+		if (asyncException.f0 != null) {
+			throw asyncException.f0;
 		}
 	}
 }
diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/util/HDFSCopyUtilitiesTest.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/util/HDFSCopyUtilitiesTest.java
index 5dde783e8be..07e1e651ed2 100644
--- a/flink-streaming-java/src/test/java/org/apache/flink/streaming/util/HDFSCopyUtilitiesTest.java
+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/util/HDFSCopyUtilitiesTest.java
@@ -17,7 +17,6 @@
  */
 package org.apache.flink.streaming.util;
 
-import org.apache.hadoop.conf.Configuration;
 import org.junit.Rule;
 import org.junit.Test;
 import org.junit.rules.TemporaryFolder;
@@ -43,14 +42,8 @@ public class HDFSCopyUtilitiesTest {
 	 */
 	@Test
 	public void testCopyFromLocal() throws Exception {
-		Configuration config = new Configuration();
-		config.set("fs.default.name", "magic-1337:///");
 
 		File testFolder = tempFolder.newFolder();
-		File hadoopConfPath = new File(testFolder, "hadoop-conf.binary");
-		try (DataOutputStream out = new DataOutputStream(new FileOutputStream(hadoopConfPath))) {
-			config.write(out);
-		}
 
 		File originalFile = new File(testFolder, "original");
 		File copyFile = new File(testFolder, "copy");
@@ -59,25 +52,7 @@ public class HDFSCopyUtilitiesTest {
 			out.writeUTF("Hello there, 42!");
 		}
 
-		try {
-			HDFSCopyFromLocal.copyFromLocal(hadoopConfPath,
-					originalFile,
-					new URI(copyFile.getAbsolutePath()));
-		} catch (Exception e) {
-			// The copying will try to write to filesystem "magic-1337" for which there is no
-			// implementation, the error message will contain the name of the file system and
-			// we check for that.
-			assertTrue(e.getMessage().contains("magic-1337"));
-		}
-
-		config.set("fs.default.name", "file:///");
-
-		hadoopConfPath.delete();
-		try (DataOutputStream out = new DataOutputStream(new FileOutputStream(hadoopConfPath))) {
-			config.write(out);
-		}
-
-		HDFSCopyFromLocal.copyFromLocal(hadoopConfPath,
+		HDFSCopyFromLocal.copyFromLocal(
 				originalFile,
 				new URI(copyFile.getAbsolutePath()));
 
@@ -93,14 +68,8 @@ public class HDFSCopyUtilitiesTest {
 	 */
 	@Test
 	public void testCopyToLocal() throws Exception {
-		Configuration config = new Configuration();
-		config.set("fs.default.name", "magic-1337:///");
 
 		File testFolder = tempFolder.newFolder();
-		File hadoopConfPath = new File(testFolder, "hadoop-conf.binary");
-		try (DataOutputStream out = new DataOutputStream(new FileOutputStream(hadoopConfPath))) {
-			config.write(out);
-		}
 
 		File originalFile = new File(testFolder, "original");
 		File copyFile = new File(testFolder, "copy");
@@ -109,25 +78,7 @@ public class HDFSCopyUtilitiesTest {
 			out.writeUTF("Hello there, 42!");
 		}
 
-		try {
-			HDFSCopyToLocal.copyToLocal(hadoopConfPath,
-					new URI(originalFile.getAbsolutePath()),
-					copyFile);
-		} catch (Exception e) {
-			// The copying will try to write to filesystem "magic-1337" for which there is no
-			// implementation, the error message will contain the name of the file system and
-			// we check for that.
-			assertTrue(e.getMessage().contains("magic-1337"));
-		}
-
-		config.set("fs.default.name", "file:///");
-
-		hadoopConfPath.delete();
-		try (DataOutputStream out = new DataOutputStream(new FileOutputStream(hadoopConfPath))) {
-			config.write(out);
-		}
-
-		HDFSCopyToLocal.copyToLocal(hadoopConfPath,
+		HDFSCopyToLocal.copyToLocal(
 				new URI(originalFile.getAbsolutePath()),
 				copyFile);
 
