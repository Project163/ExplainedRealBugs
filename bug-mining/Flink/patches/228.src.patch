diff --git a/flink-clients/src/main/java/org/apache/flink/client/FlinkYarnSessionCli.java b/flink-clients/src/main/java/org/apache/flink/client/FlinkYarnSessionCli.java
index 8117ec3d025..8d5a3c5a15f 100644
--- a/flink-clients/src/main/java/org/apache/flink/client/FlinkYarnSessionCli.java
+++ b/flink-clients/src/main/java/org/apache/flink/client/FlinkYarnSessionCli.java
@@ -407,8 +407,10 @@ public class FlinkYarnSessionCli {
 
 			runInteractiveCli(yarnCluster);
 
-			LOG.info("Command Line Interface requested session shutdown");
-			yarnCluster.shutdown();
+			if(!yarnCluster.hasBeenStopped()) {
+				LOG.info("Command Line Interface requested session shutdown");
+				yarnCluster.shutdown();
+			}
 
 			try {
 				yarnPropertiesFile.delete();
diff --git a/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManager.scala b/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManager.scala
index 4fe0ea69cb2..b490b326382 100644
--- a/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManager.scala
+++ b/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManager.scala
@@ -115,7 +115,7 @@ class JobManager(val configuration: Configuration,
     log.info(s"Stopping job manager ${self.path}.")
 
     archive ! PoisonPill
-    profiler.map( ref => ref ! PoisonPill )
+    profiler.foreach( ref => ref ! PoisonPill )
 
     for((e,_) <- currentJobs.values){
       e.fail(new Exception("The JobManager is shutting down."))
diff --git a/flink-yarn/src/main/java/org/apache/flink/yarn/FlinkYarnCluster.java b/flink-yarn/src/main/java/org/apache/flink/yarn/FlinkYarnCluster.java
index 6fedd9a7778..2f652a121c6 100644
--- a/flink-yarn/src/main/java/org/apache/flink/yarn/FlinkYarnCluster.java
+++ b/flink-yarn/src/main/java/org/apache/flink/yarn/FlinkYarnCluster.java
@@ -103,7 +103,7 @@ public class FlinkYarnCluster extends AbstractFlinkYarnCluster {
 		// start application client
 		LOG.info("Start application client.");
 
-		applicationClient = actorSystem.actorOf(Props.create(ApplicationClient.class));
+		applicationClient = actorSystem.actorOf(Props.create(ApplicationClient.class), "applicationClient");
 
 		// instruct ApplicationClient to start a periodical status polling
 		applicationClient.tell(new Messages.LocalRegisterClient(this.jobManagerAddress), applicationClient);
diff --git a/flink-yarn/src/main/scala/org/apache/flink/yarn/ApplicationClient.scala b/flink-yarn/src/main/scala/org/apache/flink/yarn/ApplicationClient.scala
index 684990bb86d..dba634438b5 100644
--- a/flink-yarn/src/main/scala/org/apache/flink/yarn/ApplicationClient.scala
+++ b/flink-yarn/src/main/scala/org/apache/flink/yarn/ApplicationClient.scala
@@ -21,6 +21,7 @@ package org.apache.flink.yarn
 import java.net.InetSocketAddress
 
 import akka.actor._
+import akka.pattern.ask
 import org.apache.flink.configuration.GlobalConfiguration
 import org.apache.flink.runtime.ActorLogMessages
 import org.apache.flink.runtime.akka.AkkaUtils
@@ -61,6 +62,9 @@ class ApplicationClient extends Actor with ActorLogMessages with ActorLogging {
     }
 
     pollingTimer = None
+
+    // Terminate the whole actor system because there is only the application client running
+    context.system.shutdown()
   }
 
   override def receiveWithLogMessages: Receive = {
@@ -75,6 +79,8 @@ class ApplicationClient extends Actor with ActorLogMessages with ActorLogging {
         case Failure(t) =>
           log.error(t, "Registration at JobManager/ApplicationMaster failed. Shutting " +
             "ApplicationClient down.")
+
+          // we could not connect to the job manager --> poison ourselves
           self ! PoisonPill
       }
 
@@ -84,7 +90,11 @@ class ApplicationClient extends Actor with ActorLogMessages with ActorLogging {
       // the message came from the FlinkYarnCluster. We send the message to the JobManager.
       // it is important not to forward the message because the JobManager is storing the
       // sender as the Application Client (this class).
-      jm ! RegisterClient
+      (jm ? RegisterClient(self))(timeout).onFailure{
+        case t: Throwable =>
+          log.error(t, "Could not register at the job manager.")
+          self ! PoisonPill
+      }
 
       // schedule a periodic status report from the JobManager
       // request the number of task managers and slots from the job manager
@@ -93,7 +103,7 @@ class ApplicationClient extends Actor with ActorLogMessages with ActorLogging {
 
     case msg: StopYarnSession =>
       log.info("Stop yarn session.")
-      stopMessageReceiver = Some(sender())
+      stopMessageReceiver = Some(sender)
       yarnJobManager foreach {
         _ forward msg
       }
@@ -104,9 +114,8 @@ class ApplicationClient extends Actor with ActorLogMessages with ActorLogging {
       stopMessageReceiver foreach {
         _ ! JobManagerStopped
       }
-      // stop ourselves
-      context.system.shutdown()
-
+      // poison ourselves
+      self ! PoisonPill
 
     // handle the responses from the PollYarnClusterStatus messages to the yarn job mgr
     case status: FlinkYarnClusterStatus =>
diff --git a/flink-yarn/src/main/scala/org/apache/flink/yarn/Messages.scala b/flink-yarn/src/main/scala/org/apache/flink/yarn/Messages.scala
index e880fdf35aa..4ae1a3a12ef 100644
--- a/flink-yarn/src/main/scala/org/apache/flink/yarn/Messages.scala
+++ b/flink-yarn/src/main/scala/org/apache/flink/yarn/Messages.scala
@@ -29,7 +29,7 @@ object Messages {
 
   case class YarnMessage(message: String, date: Date = new Date())
   case class ApplicationMasterStatus(numTaskManagers: Int, numSlots: Int)
-  case object RegisterClient
+  case class RegisterClient(client: ActorRef)
 
   case class StopYarnSession(status: FinalApplicationStatus)
 
diff --git a/flink-yarn/src/main/scala/org/apache/flink/yarn/YarnJobManager.scala b/flink-yarn/src/main/scala/org/apache/flink/yarn/YarnJobManager.scala
index a37c8b4ffa4..81bdc3b3f6d 100644
--- a/flink-yarn/src/main/scala/org/apache/flink/yarn/YarnJobManager.scala
+++ b/flink-yarn/src/main/scala/org/apache/flink/yarn/YarnJobManager.scala
@@ -26,6 +26,7 @@ import akka.actor.ActorRef
 import org.apache.flink.configuration.ConfigConstants
 import org.apache.flink.runtime.ActorLogMessages
 import org.apache.flink.runtime.jobmanager.JobManager
+import org.apache.flink.runtime.messages.Messages.Acknowledge
 import org.apache.flink.runtime.yarn.FlinkYarnClusterStatus
 import org.apache.flink.yarn.Messages._
 import org.apache.flink.yarn.appMaster.YarnTaskManagerRunner
@@ -42,6 +43,7 @@ import org.apache.hadoop.yarn.util.Records
 
 import scala.concurrent.duration._
 import scala.language.postfixOps
+import scala.util.Try
 
 
 trait YarnJobManager extends ActorLogMessages {
@@ -69,7 +71,7 @@ trait YarnJobManager extends ActorLogMessages {
 
   def receiveYarnMessages: Receive = {
     case StopYarnSession(status) =>
-      log.info("Stopping YARN Session.")
+      log.info("Stopping YARN JobManager with status {}.", status)
 
       instanceManager.getAllRegisteredInstances.asScala foreach {
         instance =>
@@ -78,30 +80,96 @@ trait YarnJobManager extends ActorLogMessages {
 
       rmClientOption foreach {
         rmClient =>
-          rmClient.unregisterApplicationMaster(status, "", "")
-          rmClient.close()
+          Try(rmClient.unregisterApplicationMaster(status, "", "")).recover{
+            case t: Throwable => log.error(t, "Could not unregister the application master.")
+          }
+
+          Try(rmClient.close()).recover{
+            case t:Throwable => log.error(t, "Could not close the AMRMClient.")
+          }
       }
 
       rmClientOption = None
 
       nmClientOption foreach {
-        _.close()
+        nmClient =>
+        Try(nmClient.close()).recover{
+          case t: Throwable => log.error(t, "Could not close the NMClient.")
+        }
       }
 
       nmClientOption = None
       messageListener foreach {
-        _ ! JobManagerStopped
+          _ ! JobManagerStopped
       }
+
       context.system.shutdown()
 
-    case RegisterClient =>
-      messageListener = Some(sender())
+    case RegisterClient(client) =>
+      log.info("Register {} as client.", client.path)
+      messageListener = Some(client)
+      sender ! Acknowledge
 
     case PollYarnClusterStatus =>
       sender() ! new FlinkYarnClusterStatus(instanceManager.getNumberOfRegisteredTaskManagers,
         instanceManager.getTotalNumberOfSlots)
 
-    case StartYarnSession(conf, actorSystemPort, webServerport) =>
+    case StartYarnSession(conf, actorSystemPort, webServerPort) => startYarnSession(conf, actorSystemPort, webServerPort)
+
+    case PollContainerCompletion =>
+      rmClientOption match {
+        case Some(rmClient) =>
+          val response = rmClient.allocate(completedContainers.toFloat / numTaskManager)
+
+          for (container <- response.getAllocatedContainers.asScala) {
+            log.info(s"Got new container for TM ${container.getId} on host ${
+              container.getNodeId.getHost
+            }")
+
+            allocatedContainers += 1
+
+            log.info(s"Launching container #$allocatedContainers.")
+            nmClientOption match {
+              case Some(nmClient) =>
+                containerLaunchContext match {
+                  case Some(ctx) => nmClient.startContainer(container, ctx)
+                  case None =>
+                    log.error("The ContainerLaunchContext was not set.")
+                    self ! StopYarnSession(FinalApplicationStatus.FAILED)
+                }
+              case None =>
+                log.error("The NMClient was not set.")
+                self ! StopYarnSession(FinalApplicationStatus.FAILED)
+            }
+          }
+
+          for (status <- response.getCompletedContainersStatuses.asScala) {
+            completedContainers += 1
+            log.info(s"Completed container ${status.getContainerId}. Total completed " +
+              s"$completedContainers.")
+            log.info(s"Diagnostics ${status.getDiagnostics}.")
+
+            messageListener foreach {
+              _ ! YarnMessage(s"Diagnostics for containerID=${status.getContainerId} in " +
+                s"state=${status.getState}.\n${status.getDiagnostics}")
+            }
+          }
+
+          if (allocatedContainers < numTaskManager) {
+            context.system.scheduler.scheduleOnce(ALLOCATION_DELAY, self, PollContainerCompletion)
+          } else if (completedContainers < numTaskManager) {
+            context.system.scheduler.scheduleOnce(COMPLETION_DELAY, self, PollContainerCompletion)
+          } else {
+            self ! StopYarnSession(FinalApplicationStatus.FAILED)
+          }
+        case None =>
+          log.error("The AMRMClient was not set.")
+          self ! StopYarnSession(FinalApplicationStatus.FAILED)
+      }
+  }
+
+  private def startYarnSession(conf: Configuration, actorSystemPort: Int, webServerPort: Int): Unit = {
+    Try {
       log.info("Start yarn session.")
       val memoryPerTaskManager = env.get(FlinkYarnClient.ENV_TM_MEMORY).toInt
       val heapLimit = Utils.calculateHeapSize(memoryPerTaskManager)
@@ -134,7 +202,7 @@ trait YarnJobManager extends ActorLogMessages {
       nmClientOption = Some(nm)
 
       // Register with ResourceManager
-      val url = s"http://$applicationMasterHost:$webServerport"
+      val url = s"http://$applicationMasterHost:$webServerPort"
       log.info(s"Registering ApplicationMaster with tracking url $url.")
       rm.registerApplicationMaster(applicationMasterHost, actorSystemPort, url)
 
@@ -146,7 +214,7 @@ trait YarnJobManager extends ActorLogMessages {
       // Resource requirements for worker containers
       val capability = Records.newRecord(classOf[Resource])
       capability.setMemory(memoryPerTaskManager)
-      capability.setVirtualCores(1) // hard-code that number (YARN is not accunting for CPUs)
+      capability.setVirtualCores(1) // hard-code that number (YARN is not accounting for CPUs)
 
       // Make container requests to ResourceManager
       for (i <- 0 until numTaskManager) {
@@ -193,60 +261,14 @@ trait YarnJobManager extends ActorLogMessages {
         yarnClientUsername, conf, taskManagerLocalResources))
 
       context.system.scheduler.scheduleOnce(ALLOCATION_DELAY, self, PollContainerCompletion)
-
-    case PollContainerCompletion =>
-      rmClientOption match {
-        case Some(rmClient) =>
-          val response = rmClient.allocate(completedContainers.toFloat / numTaskManager)
-
-          for (container <- response.getAllocatedContainers.asScala) {
-            log.info(s"Got new container for TM ${container.getId} on host ${
-              container.getNodeId.getHost
-            }")
-
-            allocatedContainers += 1
-
-            log.info(s"Launching container #$allocatedContainers.")
-            nmClientOption match {
-              case Some(nmClient) =>
-                containerLaunchContext match {
-                  case Some(ctx) => nmClient.startContainer(container, ctx)
-                  case None =>
-                    log.error("The ContainerLaunchContext was not set.")
-                    self ! StopYarnSession(FinalApplicationStatus.FAILED)
-                }
-              case None =>
-                log.error("The NMClient was not set.")
-                self ! StopYarnSession(FinalApplicationStatus.FAILED)
-            }
-          }
-
-          for (status <- response.getCompletedContainersStatuses.asScala) {
-            completedContainers += 1
-            log.info(s"Completed container ${status.getContainerId}. Total completed " +
-              s"$completedContainers.")
-            log.info(s"Diagnostics ${status.getDiagnostics}.")
-
-            messageListener foreach {
-              _ ! YarnMessage(s"Diagnostics for containerID=${status.getContainerId} in " +
-                s"state=${status.getState}.\n${status.getDiagnostics}")
-            }
-          }
-
-          if (allocatedContainers < numTaskManager) {
-            context.system.scheduler.scheduleOnce(ALLOCATION_DELAY, self, PollContainerCompletion)
-          } else if (completedContainers < numTaskManager) {
-            context.system.scheduler.scheduleOnce(COMPLETION_DELAY, self, PollContainerCompletion)
-          } else {
-            self ! StopYarnSession(FinalApplicationStatus.FAILED)
-          }
-        case None =>
-          log.error("The AMRMClient was not set.")
-          self ! StopYarnSession(FinalApplicationStatus.FAILED)
-      }
+    } recover {
+      case t: Throwable =>
+        log.error(t, "Could not start yarn session.")
+        self ! StopYarnSession(FinalApplicationStatus.FAILED)
+    }
   }
 
-  def createContainerLaunchContext(heapLimit: Int, hasLogback: Boolean, hasLog4j: Boolean,
+  private def createContainerLaunchContext(heapLimit: Int, hasLogback: Boolean, hasLog4j: Boolean,
                                    yarnClientUsername: String, yarnConf: Configuration,
                                    taskManagerLocalResources: Map[String, LocalResource]):
   ContainerLaunchContext = {
@@ -294,12 +316,12 @@ trait YarnJobManager extends ActorLogMessages {
       val securityTokens = ByteBuffer.wrap(dob.getData, 0, dob.getLength)
       ctx.setTokens(securityTokens)
     } catch {
-      case e: IOException =>
-        log.warning("Getting current user info failed when trying to launch the container", e)
+      case t: Throwable =>
+        log.error(t, "Getting current user info failed when trying to launch the container")
     }
 
     ctx
   }
 
-  def env = System.getenv()
+  private def env = System.getenv()
 }
