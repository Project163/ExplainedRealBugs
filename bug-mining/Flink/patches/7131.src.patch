diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/connectors/DynamicSourceUtils.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/connectors/DynamicSourceUtils.java
index db5b4090877..af58b5e8132 100644
--- a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/connectors/DynamicSourceUtils.java
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/connectors/DynamicSourceUtils.java
@@ -483,8 +483,11 @@ public final class DynamicSourceUtils {
             ChangelogMode changelogMode,
             ReadableConfig config) {
         // sanity check for produced ChangelogMode
-        final boolean hasUpdateBefore = changelogMode.contains(RowKind.UPDATE_BEFORE);
-        final boolean hasUpdateAfter = changelogMode.contains(RowKind.UPDATE_AFTER);
+        final boolean hasChangelogMode = changelogMode != null;
+        final boolean hasUpdateBefore =
+                hasChangelogMode && changelogMode.contains(RowKind.UPDATE_BEFORE);
+        final boolean hasUpdateAfter =
+                hasChangelogMode && changelogMode.contains(RowKind.UPDATE_AFTER);
         if (!hasUpdateBefore && hasUpdateAfter) {
             // only UPDATE_AFTER
             if (!schema.getPrimaryKey().isPresent()) {
@@ -503,7 +506,7 @@ public final class DynamicSourceUtils {
                             tableDebugName,
                             ScanTableSource.class.getSimpleName(),
                             scanSource.getClass().getName()));
-        } else if (!changelogMode.containsOnly(RowKind.INSERT)) {
+        } else if (hasChangelogMode && !changelogMode.containsOnly(RowKind.INSERT)) {
             // CDC mode (non-upsert mode and non-insert-only mode)
             final boolean changeEventsDuplicate =
                     config.get(ExecutionConfigOptions.TABLE_EXEC_SOURCE_CDC_EVENTS_DUPLICATE);
diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/spec/DynamicTableSourceSpec.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/spec/DynamicTableSourceSpec.java
index 2f5fdd08953..6d5be2d813b 100644
--- a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/spec/DynamicTableSourceSpec.java
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/spec/DynamicTableSourceSpec.java
@@ -20,6 +20,7 @@ package org.apache.flink.table.planner.plan.nodes.exec.spec;
 
 import org.apache.flink.table.api.TableException;
 import org.apache.flink.table.catalog.ContextResolvedTable;
+import org.apache.flink.table.catalog.ResolvedCatalogTable;
 import org.apache.flink.table.connector.source.DynamicTableSource;
 import org.apache.flink.table.connector.source.LookupTableSource;
 import org.apache.flink.table.connector.source.ScanTableSource;
@@ -28,6 +29,7 @@ import org.apache.flink.table.factories.FactoryUtil;
 import org.apache.flink.table.module.Module;
 import org.apache.flink.table.planner.calcite.FlinkContext;
 import org.apache.flink.table.planner.calcite.FlinkTypeFactory;
+import org.apache.flink.table.planner.connectors.DynamicSourceUtils;
 import org.apache.flink.table.planner.plan.abilities.source.SourceAbilityContext;
 import org.apache.flink.table.planner.plan.abilities.source.SourceAbilitySpec;
 import org.apache.flink.table.types.logical.RowType;
@@ -74,23 +76,34 @@ public class DynamicTableSourceSpec extends DynamicTableSpecBase {
                             .getFactory(Module::getTableSourceFactory)
                             .orElse(null);
 
+            ResolvedCatalogTable resolvedCatalogTable = contextResolvedTable.getResolvedTable();
             tableSource =
                     FactoryUtil.createDynamicTableSource(
                             factory,
                             contextResolvedTable.getIdentifier(),
-                            contextResolvedTable.getResolvedTable(),
+                            resolvedCatalogTable,
                             loadOptionsFromCatalogTable(contextResolvedTable, context),
                             context.getTableConfig(),
                             context.getClassLoader(),
                             contextResolvedTable.isTemporary());
+            // validate DynamicSource and apply Metadata
+            DynamicSourceUtils.prepareDynamicSource(
+                    contextResolvedTable.getIdentifier().toString(),
+                    resolvedCatalogTable,
+                    tableSource,
+                    false,
+                    context.getTableConfig().getConfiguration());
 
             if (sourceAbilities != null) {
+                //  Note: use DynamicSourceUtils.createProducedType to produce the type info so that
+                //  keep consistent with sql2Rel phase which also called the method producing
+                //  deterministic format (PHYSICAL COLUMNS + METADATA COLUMNS) when converts a given
+                //  DynamicTableSource to a RelNode.
+                // TODO should do a refactor(e.g., add serialized input type info into each
+                //  SourceAbilitySpec so as to avoid this implicit logic dependency)
                 RowType newProducedType =
-                        (RowType)
-                                contextResolvedTable
-                                        .getResolvedSchema()
-                                        .toSourceRowDataType()
-                                        .getLogicalType();
+                        DynamicSourceUtils.createProducedType(
+                                contextResolvedTable.getResolvedSchema(), tableSource);
                 for (SourceAbilitySpec spec : sourceAbilities) {
                     SourceAbilityContext sourceAbilityContext =
                             new SourceAbilityContext(context, typeFactory, newProducedType);
diff --git a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/nodes/exec/stream/CalcJsonPlanTest.java b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/nodes/exec/stream/CalcJsonPlanTest.java
index b763d92cdec..0eea911954f 100644
--- a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/nodes/exec/stream/CalcJsonPlanTest.java
+++ b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/nodes/exec/stream/CalcJsonPlanTest.java
@@ -128,4 +128,20 @@ public class CalcJsonPlanTest extends TableTestBase {
         String sql = "insert into MySink SELECT a from MyTable where a = 1 or a = 2 or a is null";
         util.verifyJsonPlan(sql);
     }
+
+    @Test
+    public void testProjectPushDown() {
+        // ensure PartitionPushDownSpec was added to exec plan
+        String sinkTableDdl =
+                "CREATE TABLE MySink (\n"
+                        + "  a int,\n"
+                        + "  b bigint,\n"
+                        + "  c varchar\n"
+                        + ") with (\n"
+                        + "  'connector' = 'values',\n"
+                        + "  'table-sink-class' = 'DEFAULT')";
+        tEnv.executeSql(sinkTableDdl);
+        util.verifyJsonPlan(
+                "insert into MySink SELECT b, a, cast(a as varchar) FROM MyTable WHERE b > 1");
+    }
 }
diff --git a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/runtime/stream/jsonplan/CalcJsonPlanITCase.java b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/runtime/stream/jsonplan/CalcJsonPlanITCase.java
index 980fa3b1bd2..8d03a24dd14 100644
--- a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/runtime/stream/jsonplan/CalcJsonPlanITCase.java
+++ b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/runtime/stream/jsonplan/CalcJsonPlanITCase.java
@@ -84,4 +84,21 @@ public class CalcJsonPlanITCase extends JsonPlanTestBase {
         assertResult(
                 Arrays.asList("2,2,2,Hello2,$Hello", "3,3,2,Hello world3,$Hello wo"), sinkPath);
     }
+
+    @Test
+    public void testProjectPushDown() throws Exception {
+        List<String> data = Arrays.asList("1,1,hi", "2,1,hello", "3,2,hello world");
+        createTestCsvSourceTable("MyTable", data, "a bigint", "b int not null", "c varchar");
+        File sinkPath = createTestCsvSinkTable("MySink", "b int", "a bigint", "a1 varchar");
+
+        compileSqlAndExecutePlan(
+                        "insert into MySink select "
+                                + "b, "
+                                + "a, "
+                                + "cast(a as varchar) as a1 "
+                                + "from MyTable where b > 1")
+                .await();
+
+        assertResult(Collections.singletonList("2,3,3"), sinkPath);
+    }
 }
diff --git a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/runtime/stream/jsonplan/WatermarkAssignerJsonPlanITCase.java b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/runtime/stream/jsonplan/WatermarkAssignerJsonPlanITCase.java
index 80b2c15b788..4ac0a181860 100644
--- a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/runtime/stream/jsonplan/WatermarkAssignerJsonPlanITCase.java
+++ b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/runtime/stream/jsonplan/WatermarkAssignerJsonPlanITCase.java
@@ -63,4 +63,73 @@ public class WatermarkAssignerJsonPlanITCase extends JsonPlanTestBase {
                         "6,3," + toLocalDateTime(6000L)),
                 sinkPath);
     }
+
+    @Test
+    public void testWatermarkPushDownWithMetadata() throws Exception {
+        // to verify FLINK-30598: the case declares metadata field first, without the fix it'll get
+        // wrong code generated by WatermarkGeneratorCodeGenerator which reference the incorrect
+        // varchar column as the watermark field.
+        createTestValuesSourceTable(
+                "MyTable",
+                JavaScalaConversionUtil.toJava(TestData.data3WithTimestamp()),
+                new String[] {
+                    "ts timestamp(3) metadata",
+                    "a int",
+                    "b bigint",
+                    "c varchar",
+                    "watermark for ts as ts - interval '5' second"
+                },
+                new HashMap<String, String>() {
+                    {
+                        put("enable-watermark-push-down", "true");
+                        put("readable-metadata", "ts:timestamp(3)");
+                    }
+                });
+
+        File sinkPath =
+                createTestCsvSinkTable(
+                        "MySink", "a int", "b bigint", "c varchar", "ts timestamp(3)");
+
+        compileSqlAndExecutePlan("insert into MySink select a, b, c, ts from MyTable where b = 3")
+                .await();
+
+        assertResult(
+                Arrays.asList(
+                        "4,3,Hello world, how are you?," + toLocalDateTime(4000L),
+                        "5,3,I am fine.," + toLocalDateTime(5000L),
+                        "6,3,Luke Skywalker," + toLocalDateTime(6000L)),
+                sinkPath);
+    }
+
+    @Test
+    public void testWatermarkAndProjectPushDownWithMetadata() throws Exception {
+        createTestValuesSourceTable(
+                "MyTable",
+                JavaScalaConversionUtil.toJava(TestData.data3WithTimestamp()),
+                new String[] {
+                    "ts timestamp(3) metadata",
+                    "a int",
+                    "b bigint",
+                    "c varchar",
+                    "watermark for ts as ts - interval '5' second"
+                },
+                new HashMap<String, String>() {
+                    {
+                        put("enable-watermark-push-down", "true");
+                        put("readable-metadata", "ts:timestamp(3)");
+                    }
+                });
+
+        File sinkPath = createTestCsvSinkTable("MySink", "c varchar", "ts timestamp(3)");
+
+        compileSqlAndExecutePlan("insert into MySink select c, ts from MyTable where b = 3")
+                .await();
+
+        assertResult(
+                Arrays.asList(
+                        "Hello world, how are you?," + toLocalDateTime(4000L),
+                        "I am fine.," + toLocalDateTime(5000L),
+                        "Luke Skywalker," + toLocalDateTime(6000L)),
+                sinkPath);
+    }
 }
diff --git a/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/nodes/exec/stream/CalcJsonPlanTest_jsonplan/testProjectPushDown.out b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/nodes/exec/stream/CalcJsonPlanTest_jsonplan/testProjectPushDown.out
new file mode 100644
index 00000000000..6e53219eeb8
--- /dev/null
+++ b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/nodes/exec/stream/CalcJsonPlanTest_jsonplan/testProjectPushDown.out
@@ -0,0 +1,155 @@
+{
+  "flinkVersion" : "",
+  "nodes" : [ {
+    "id" : 1,
+    "type" : "stream-exec-table-source-scan_1",
+    "scanTableSource" : {
+      "table" : {
+        "identifier" : "`default_catalog`.`default_database`.`MyTable`",
+        "resolvedTable" : {
+          "schema" : {
+            "columns" : [ {
+              "name" : "a",
+              "dataType" : "BIGINT"
+            }, {
+              "name" : "b",
+              "dataType" : "INT NOT NULL"
+            }, {
+              "name" : "c",
+              "dataType" : "VARCHAR(2147483647)"
+            }, {
+              "name" : "d",
+              "dataType" : "TIMESTAMP(3)"
+            } ],
+            "watermarkSpecs" : [ ]
+          },
+          "partitionKeys" : [ ],
+          "options" : {
+            "connector" : "values",
+            "bounded" : "false"
+          }
+        }
+      },
+      "abilities" : [ {
+        "type" : "FilterPushDown",
+        "predicates" : [ ]
+      }, {
+        "type" : "ProjectPushDown",
+        "projectedFields" : [ [ 0 ], [ 1 ] ],
+        "producedType" : "ROW<`a` BIGINT, `b` INT NOT NULL> NOT NULL"
+      }, {
+        "type" : "ReadingMetadata",
+        "metadataKeys" : [ ],
+        "producedType" : "ROW<`a` BIGINT, `b` INT NOT NULL> NOT NULL"
+      } ]
+    },
+    "outputType" : "ROW<`a` BIGINT, `b` INT NOT NULL>",
+    "description" : "TableSourceScan(table=[[default_catalog, default_database, MyTable, filter=[], project=[a, b], metadata=[]]], fields=[a, b])",
+    "inputProperties" : [ ]
+  }, {
+    "id" : 2,
+    "type" : "stream-exec-calc_1",
+    "projection" : [ {
+      "kind" : "INPUT_REF",
+      "inputIndex" : 1,
+      "type" : "INT NOT NULL"
+    }, {
+      "kind" : "INPUT_REF",
+      "inputIndex" : 0,
+      "type" : "BIGINT"
+    }, {
+      "kind" : "CALL",
+      "syntax" : "SPECIAL",
+      "internalName" : "$CAST$1",
+      "operands" : [ {
+        "kind" : "INPUT_REF",
+        "inputIndex" : 0,
+        "type" : "BIGINT"
+      } ],
+      "type" : "VARCHAR(2147483647)"
+    } ],
+    "condition" : {
+      "kind" : "CALL",
+      "syntax" : "BINARY",
+      "internalName" : "$>$1",
+      "operands" : [ {
+        "kind" : "INPUT_REF",
+        "inputIndex" : 1,
+        "type" : "INT NOT NULL"
+      }, {
+        "kind" : "LITERAL",
+        "value" : 1,
+        "type" : "INT NOT NULL"
+      } ],
+      "type" : "BOOLEAN NOT NULL"
+    },
+    "inputProperties" : [ {
+      "requiredDistribution" : {
+        "type" : "UNKNOWN"
+      },
+      "damBehavior" : "PIPELINED",
+      "priority" : 0
+    } ],
+    "outputType" : "ROW<`b` INT NOT NULL, `a` BIGINT, `EXPR$2` VARCHAR(2147483647)>",
+    "description" : "Calc(select=[b, a, CAST(a AS VARCHAR(2147483647)) AS EXPR$2], where=[(b > 1)])"
+  }, {
+    "id" : 3,
+    "type" : "stream-exec-sink_2",
+    "configuration" : {
+      "table.exec.sink.keyed-shuffle" : "AUTO",
+      "table.exec.sink.not-null-enforcer" : "ERROR",
+      "table.exec.sink.type-length-enforcer" : "IGNORE",
+      "table.exec.sink.upsert-materialize" : "AUTO"
+    },
+    "dynamicTableSink" : {
+      "table" : {
+        "identifier" : "`default_catalog`.`default_database`.`MySink`",
+        "resolvedTable" : {
+          "schema" : {
+            "columns" : [ {
+              "name" : "a",
+              "dataType" : "INT"
+            }, {
+              "name" : "b",
+              "dataType" : "BIGINT"
+            }, {
+              "name" : "c",
+              "dataType" : "VARCHAR(2147483647)"
+            } ],
+            "watermarkSpecs" : [ ]
+          },
+          "partitionKeys" : [ ],
+          "options" : {
+            "connector" : "values",
+            "table-sink-class" : "DEFAULT"
+          }
+        }
+      }
+    },
+    "inputChangelogMode" : [ "INSERT" ],
+    "inputProperties" : [ {
+      "requiredDistribution" : {
+        "type" : "UNKNOWN"
+      },
+      "damBehavior" : "PIPELINED",
+      "priority" : 0
+    } ],
+    "outputType" : "ROW<`b` INT NOT NULL, `a` BIGINT, `EXPR$2` VARCHAR(2147483647)>",
+    "description" : "Sink(table=[default_catalog.default_database.MySink], fields=[b, a, EXPR$2])"
+  } ],
+  "edges" : [ {
+    "source" : 1,
+    "target" : 2,
+    "shuffle" : {
+      "type" : "FORWARD"
+    },
+    "shuffleMode" : "PIPELINED"
+  }, {
+    "source" : 2,
+    "target" : 3,
+    "shuffle" : {
+      "type" : "FORWARD"
+    },
+    "shuffleMode" : "PIPELINED"
+  } ]
+}
\ No newline at end of file
