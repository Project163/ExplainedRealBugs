diff --git a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/FlinkKafkaInternalProducer.java b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/FlinkKafkaInternalProducer.java
index 65616d2fb79..a023cdd144d 100644
--- a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/FlinkKafkaInternalProducer.java
+++ b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/FlinkKafkaInternalProducer.java
@@ -85,16 +85,16 @@ class FlinkKafkaInternalProducer<K, V> extends KafkaProducer<K, V> {
     public void abortTransaction() throws ProducerFencedException {
         LOG.debug("abortTransaction {}", transactionalId);
         checkState(inTransaction, "Transaction was not started");
-        super.abortTransaction();
         inTransaction = false;
+        super.abortTransaction();
     }
 
     @Override
     public void commitTransaction() throws ProducerFencedException {
         LOG.debug("commitTransaction {}", transactionalId);
         checkState(inTransaction, "Transaction was not started");
-        super.commitTransaction();
         inTransaction = false;
+        super.commitTransaction();
     }
 
     public boolean isInTransaction() {
@@ -152,7 +152,9 @@ class FlinkKafkaInternalProducer<K, V> extends KafkaProducer<K, V> {
 
     public void setTransactionId(String transactionalId) {
         if (!transactionalId.equals(this.transactionalId)) {
-            checkState(!inTransaction);
+            checkState(
+                    !inTransaction,
+                    String.format("Another transaction %s is still open.", transactionalId));
             LOG.debug("Change transaction id from {} to {}", this.transactionalId, transactionalId);
             Object transactionManager = getTransactionManager();
             synchronized (transactionManager) {
diff --git a/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/sink/FlinkKafkaInternalProducerITCase.java b/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/sink/FlinkKafkaInternalProducerITCase.java
index 0a684331582..ca2c6b7e19f 100644
--- a/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/sink/FlinkKafkaInternalProducerITCase.java
+++ b/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/sink/FlinkKafkaInternalProducerITCase.java
@@ -19,6 +19,8 @@ package org.apache.flink.connector.kafka.sink;
 
 import org.apache.flink.util.TestLogger;
 
+import org.apache.flink.shaded.guava30.com.google.common.collect.Lists;
+
 import org.apache.kafka.clients.CommonClientConfigs;
 import org.apache.kafka.clients.consumer.ConsumerConfig;
 import org.apache.kafka.clients.consumer.ConsumerRecords;
@@ -26,9 +28,12 @@ import org.apache.kafka.clients.consumer.KafkaConsumer;
 import org.apache.kafka.clients.producer.ProducerConfig;
 import org.apache.kafka.clients.producer.ProducerRecord;
 import org.apache.kafka.common.TopicPartition;
+import org.apache.kafka.common.errors.ProducerFencedException;
 import org.apache.kafka.common.serialization.StringDeserializer;
 import org.apache.kafka.common.serialization.StringSerializer;
 import org.junit.jupiter.api.Test;
+import org.junit.jupiter.params.ParameterizedTest;
+import org.junit.jupiter.params.provider.MethodSource;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import org.testcontainers.containers.KafkaContainer;
@@ -38,18 +43,17 @@ import org.testcontainers.junit.jupiter.Testcontainers;
 import java.time.Duration;
 import java.util.List;
 import java.util.Properties;
+import java.util.function.Consumer;
 import java.util.stream.Collectors;
 
 import static org.apache.flink.connector.kafka.sink.KafkaUtil.createKafkaContainer;
 import static org.apache.flink.util.DockerImageVersions.KAFKA;
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.MatcherAssert.assertThat;
-import static org.hamcrest.Matchers.hasSize;
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Assertions.assertThatThrownBy;
 
 @Testcontainers
 class FlinkKafkaInternalProducerITCase extends TestLogger {
 
-    private static final String TEST_TOPIC = "test-topic";
     private static final Logger LOG =
             LoggerFactory.getLogger(FlinkKafkaInternalProducerITCase.class);
 
@@ -59,28 +63,16 @@ class FlinkKafkaInternalProducerITCase extends TestLogger {
 
     private static final String TRANSACTION_PREFIX = "test-transaction-";
 
-    Properties getProperties() {
-        Properties properties = new Properties();
-        properties.put(
-                CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG,
-                KAFKA_CONTAINER.getBootstrapServers());
-        properties.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, "true");
-        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
-        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
-        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
-        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
-        return properties;
-    }
-
     @Test
     void testInitTransactionId() {
+        final String topic = "test-init-transactions";
         try (FlinkKafkaInternalProducer<String, String> reuse =
                 new FlinkKafkaInternalProducer<>(getProperties(), "dummy")) {
             int numTransactions = 20;
             for (int i = 1; i <= numTransactions; i++) {
                 reuse.initTransactionId(TRANSACTION_PREFIX + i);
                 reuse.beginTransaction();
-                reuse.send(new ProducerRecord<>(TEST_TOPIC, "test-value-" + i));
+                reuse.send(new ProducerRecord<>(topic, "test-value-" + i));
                 if (i % 2 == 0) {
                     reuse.commitTransaction();
                 } else {
@@ -88,20 +80,64 @@ class FlinkKafkaInternalProducerITCase extends TestLogger {
                     reuse.abortTransaction();
                 }
                 assertNumTransactions(i);
-                assertThat(readRecords(TEST_TOPIC).count(), equalTo(i / 2));
+                assertThat(readRecords(topic).count()).isEqualTo(i / 2);
+            }
+        }
+    }
+
+    @ParameterizedTest
+    @MethodSource("provideTransactionsFinalizer")
+    void testResetInnerTransactionIfFinalizingTransactionFailed(
+            Consumer<FlinkKafkaInternalProducer<?, ?>> transactionFinalizer) {
+        final String topic = "reset-producer-internal-state";
+        try (FlinkKafkaInternalProducer<String, String> fenced =
+                new FlinkKafkaInternalProducer<>(getProperties(), "dummy")) {
+            fenced.initTransactions();
+            fenced.beginTransaction();
+            fenced.send(new ProducerRecord<>(topic, "test-value"));
+            // Start a second producer that fences the first one
+            try (FlinkKafkaInternalProducer<String, String> producer =
+                    new FlinkKafkaInternalProducer<>(getProperties(), "dummy")) {
+                producer.initTransactions();
+                producer.beginTransaction();
+                producer.send(new ProducerRecord<>(topic, "test-value"));
+                producer.commitTransaction();
             }
+            assertThatThrownBy(() -> transactionFinalizer.accept(fenced))
+                    .isInstanceOf(ProducerFencedException.class);
+            // Internal transaction should be reset and setting a new transactional id is possible
+            fenced.setTransactionId("dummy2");
         }
     }
 
+    private static Properties getProperties() {
+        Properties properties = new Properties();
+        properties.put(
+                CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG,
+                KAFKA_CONTAINER.getBootstrapServers());
+        properties.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, "true");
+        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
+        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
+        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
+        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
+        return properties;
+    }
+
+    private static List<Consumer<FlinkKafkaInternalProducer<?, ?>>> provideTransactionsFinalizer() {
+        return Lists.newArrayList(
+                FlinkKafkaInternalProducer::commitTransaction,
+                FlinkKafkaInternalProducer::abortTransaction);
+    }
+
     private void assertNumTransactions(int numTransactions) {
         List<KafkaTransactionLog.TransactionRecord> transactions =
                 new KafkaTransactionLog(getProperties())
                         .getTransactions(id -> id.startsWith(TRANSACTION_PREFIX));
         assertThat(
-                transactions.stream()
-                        .map(KafkaTransactionLog.TransactionRecord::getTransactionId)
-                        .collect(Collectors.toSet()),
-                hasSize(numTransactions));
+                        transactions.stream()
+                                .map(KafkaTransactionLog.TransactionRecord::getTransactionId)
+                                .collect(Collectors.toSet()))
+                .hasSize(numTransactions);
     }
 
     private ConsumerRecords<String, String> readRecords(String topic) {
