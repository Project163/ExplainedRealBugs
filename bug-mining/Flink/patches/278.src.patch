diff --git a/flink-compiler/src/main/java/org/apache/flink/compiler/PactCompiler.java b/flink-compiler/src/main/java/org/apache/flink/compiler/PactCompiler.java
index d5cbf684605..0ea87249979 100644
--- a/flink-compiler/src/main/java/org/apache/flink/compiler/PactCompiler.java
+++ b/flink-compiler/src/main/java/org/apache/flink/compiler/PactCompiler.java
@@ -28,10 +28,12 @@ import java.util.List;
 import java.util.Map;
 import java.util.Set;
 
-import org.apache.flink.api.common.operators.base.SortPartitionOperatorBase;
-import org.apache.flink.compiler.dag.SortPartitionNode;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+
+import org.apache.flink.api.common.ExecutionMode;
+import org.apache.flink.api.common.operators.base.SortPartitionOperatorBase;
+import org.apache.flink.compiler.dag.SortPartitionNode;
 import org.apache.flink.api.common.InvalidProgramException;
 import org.apache.flink.api.common.Plan;
 import org.apache.flink.api.common.operators.GenericDataSinkBase;
@@ -487,14 +489,14 @@ public class PactCompiler {
 			LOG.debug("Beginning compilation of program '" + program.getJobName() + '\'');
 		}
 
-		// set the default degree of parallelism
-		int defaultParallelism = program.getDefaultParallelism() > 0 ?
+		final ExecutionMode defaultDataExchangeMode = program.getExecutionConfig().getExecutionMode();
+
+		final int defaultParallelism = program.getDefaultParallelism() > 0 ?
 			program.getDefaultParallelism() : this.defaultDegreeOfParallelism;
 
-		// log the output
-		if (LOG.isDebugEnabled()) {
-			LOG.debug("Using a default degree of parallelism of " + defaultParallelism + '.');
-		}
+		// log the default settings
+		LOG.debug("Using a default parallelism of {}",  defaultParallelism);
+		LOG.debug("Using default data exchange mode {}", defaultDataExchangeMode);
 
 		// the first step in the compilation is to create the optimizer plan representation
 		// this step does the following:
@@ -505,7 +507,7 @@ public class PactCompiler {
 		// 4) It makes estimates about the data volume of the data sources and
 		// propagates those estimates through the plan
 
-		GraphCreatingVisitor graphCreator = new GraphCreatingVisitor(defaultParallelism);
+		GraphCreatingVisitor graphCreator = new GraphCreatingVisitor(defaultParallelism, defaultDataExchangeMode);
 		program.accept(graphCreator);
 
 		// if we have a plan with multiple data sinks, add logical optimizer nodes that have two data-sinks as children
@@ -527,21 +529,17 @@ public class PactCompiler {
 
 		// now that we have all nodes created and recorded which ones consume memory, tell the nodes their minimal
 		// guaranteed memory, for further cost estimations. we assume an equal distribution of memory among consumer tasks
-		
 		rootNode.accept(new IdAndEstimatesVisitor(this.statistics));
-		
-		// Now that the previous step is done, the next step is to traverse the graph again for the two
-		// steps that cannot directly be performed during the plan enumeration, because we are dealing with DAGs
-		// rather than a trees. That requires us to deviate at some points from the classical DB optimizer algorithms.
-		//
-		// 1) propagate the interesting properties top-down through the graph
-		// 2) Track information about nodes with multiple outputs that are later on reconnected in a node with
-		// multiple inputs.
-		InterestingPropertyVisitor propsVisitor = new InterestingPropertyVisitor(this.costEstimator);
-		rootNode.accept(propsVisitor);
-		
+
+		// We are dealing with operator DAGs, rather than operator trees.
+		// That requires us to deviate at some points from the classical DB optimizer algorithms.
+		// This step build some auxiliary structures to help track branches and joins in the DAG
 		BranchesVisitor branchingVisitor = new BranchesVisitor();
 		rootNode.accept(branchingVisitor);
+
+		// Propagate the interesting properties top-down through the graph
+		InterestingPropertyVisitor propsVisitor = new InterestingPropertyVisitor(this.costEstimator);
+		rootNode.accept(propsVisitor);
 		
 		// perform a sanity check: the root may not have any unclosed branches
 		if (rootNode.getOpenBranches() != null && rootNode.getOpenBranches().size() > 0) {
@@ -590,7 +588,7 @@ public class PactCompiler {
 	 *         from the plan can be traversed.
 	 */
 	public static List<DataSinkNode> createPreOptimizedPlan(Plan program) {
-		GraphCreatingVisitor graphCreator = new GraphCreatingVisitor(1);
+		GraphCreatingVisitor graphCreator = new GraphCreatingVisitor(1, null);
 		program.accept(graphCreator);
 		return graphCreator.sinks;
 	}
@@ -609,40 +607,45 @@ public class PactCompiler {
 	 * estimation and the awareness for optimizer hints, the sizes will be properly estimated and the translated plan
 	 * already respects all optimizer hints.
 	 */
-	private static final class GraphCreatingVisitor implements Visitor<Operator<?>> {
+	public static final class GraphCreatingVisitor implements Visitor<Operator<?>> {
 		
 		private final Map<Operator<?>, OptimizerNode> con2node; // map from the operator objects to their
 																// corresponding optimizer nodes
 
-		private final List<DataSourceNode> sources; // all data source nodes in the optimizer plan
-
 		private final List<DataSinkNode> sinks; // all data sink nodes in the optimizer plan
 
 		private final int defaultParallelism; // the default degree of parallelism
 		
 		private final GraphCreatingVisitor parent;	// reference to enclosing creator, in case of a recursive translation
-		
+
+		private final ExecutionMode defaultDataExchangeMode;
+
 		private final boolean forceDOP;
 
 		
-		private GraphCreatingVisitor(int defaultParallelism) {
-			this(null, false, defaultParallelism, null);
+		public GraphCreatingVisitor(int defaultParallelism, ExecutionMode defaultDataExchangeMode) {
+			this(null, false, defaultParallelism, defaultDataExchangeMode, null);
 		}
 
-		private GraphCreatingVisitor(GraphCreatingVisitor parent, boolean forceDOP,
-									int defaultParallelism, HashMap<Operator<?>, OptimizerNode> closure) {
+		private GraphCreatingVisitor(GraphCreatingVisitor parent, boolean forceDOP, int defaultParallelism,
+									ExecutionMode dataExchangeMode, HashMap<Operator<?>, OptimizerNode> closure) {
 			if (closure == null){
 				con2node = new HashMap<Operator<?>, OptimizerNode>();
 			} else {
 				con2node = closure;
 			}
-			this.sources = new ArrayList<DataSourceNode>(4);
+
 			this.sinks = new ArrayList<DataSinkNode>(2);
 			this.defaultParallelism = defaultParallelism;
 			this.parent = parent;
+			this.defaultDataExchangeMode = dataExchangeMode;
 			this.forceDOP = forceDOP;
 		}
 
+		public List<DataSinkNode> getSinks() {
+			return sinks;
+		}
+
 		@SuppressWarnings("deprecation")
 		@Override
 		public boolean preVisit(Operator<?> c) {
@@ -660,9 +663,7 @@ public class PactCompiler {
 				n = dsn;
 			}
 			else if (c instanceof GenericDataSourceBase) {
-				DataSourceNode dsn = new DataSourceNode((GenericDataSourceBase<?, ?>) c);
-				this.sources.add(dsn);
-				n = dsn;
+				n = new DataSourceNode((GenericDataSourceBase<?, ?>) c);
 			}
 			else if (c instanceof MapOperatorBase) {
 				n = new MapNode((MapOperatorBase<?, ?, ?>) c);
@@ -768,8 +769,8 @@ public class PactCompiler {
 				if (par > 0) {
 					if (this.forceDOP && par != this.defaultParallelism) {
 						par = this.defaultParallelism;
-						LOG.warn("The degree-of-parallelism of nested Dataflows (such as step functions in iterations) is " +
-							"currently fixed to the degree-of-parallelism of the surrounding operator (the iteration).");
+						LOG.warn("The parallelism of nested dataflows (such as step functions in iterations) is " +
+							"currently fixed to the parallelism of the surrounding operator (the iteration).");
 					}
 				} else {
 					par = this.defaultParallelism;
@@ -786,8 +787,8 @@ public class PactCompiler {
 			OptimizerNode n = this.con2node.get(c);
 
 			// first connect to the predecessors
-			n.setInput(this.con2node);
-			n.setBroadcastInputs(this.con2node);
+			n.setInput(this.con2node, this.defaultDataExchangeMode);
+			n.setBroadcastInputs(this.con2node, this.defaultDataExchangeMode);
 			
 			// if the node represents a bulk iteration, we recursively translate the data flow now
 			if (n instanceof BulkIterationNode) {
@@ -800,9 +801,9 @@ public class PactCompiler {
 
 				// first, recursively build the data flow for the step function
 				final GraphCreatingVisitor recursiveCreator = new GraphCreatingVisitor(this, true,
-					iterNode.getDegreeOfParallelism(), closure);
+					iterNode.getDegreeOfParallelism(), defaultDataExchangeMode, closure);
 				
-				BulkPartialSolutionNode partialSolution = null;
+				BulkPartialSolutionNode partialSolution;
 				
 				iter.getNextPartialSolution().accept(recursiveCreator);
 				
@@ -836,13 +837,14 @@ public class PactCompiler {
 				final WorksetIterationNode iterNode = (WorksetIterationNode) n;
 				final DeltaIterationBase<?, ?> iter = iterNode.getIterationContract();
 
-				// we need to ensure that both the next-workset and the solution-set-delta depend on the workset. One check is for free
-				// during the translation, we do the other check here as a pre-condition
+				// we need to ensure that both the next-workset and the solution-set-delta depend on the workset.
+				// One check is for free during the translation, we do the other check here as a pre-condition
 				{
 					StepFunctionValidator wsf = new StepFunctionValidator();
 					iter.getNextWorkset().accept(wsf);
 					if (!wsf.foundWorkset) {
-						throw new CompilerException("In the given program, the next workset does not depend on the workset. This is a prerequisite in delta iterations.");
+						throw new CompilerException("In the given program, the next workset does not depend on the workset. " +
+															"This is a prerequisite in delta iterations.");
 					}
 				}
 				
@@ -850,7 +852,8 @@ public class PactCompiler {
 				HashMap<Operator<?>, OptimizerNode> closure = new HashMap<Operator<?>, OptimizerNode>(con2node);
 
 				// first, recursively build the data flow for the step function
-				final GraphCreatingVisitor recursiveCreator = new GraphCreatingVisitor(this, true, iterNode.getDegreeOfParallelism(), closure);
+				final GraphCreatingVisitor recursiveCreator = new GraphCreatingVisitor(
+						this, true, iterNode.getDegreeOfParallelism(), defaultDataExchangeMode, closure);
 				
 				// descend from the solution set delta. check that it depends on both the workset
 				// and the solution set. If it does depend on both, this descend should create both nodes
@@ -859,7 +862,8 @@ public class PactCompiler {
 				final WorksetNode worksetNode = (WorksetNode) recursiveCreator.con2node.get(iter.getWorkset());
 				
 				if (worksetNode == null) {
-					throw new CompilerException("In the given program, the solution set delta does not depend on the workset. This is a prerequisite in delta iterations.");
+					throw new CompilerException("In the given program, the solution set delta does not depend on the workset." +
+														"This is a prerequisite in delta iterations.");
 				}
 				
 				iter.getNextWorkset().accept(recursiveCreator);
@@ -895,7 +899,8 @@ public class PactCompiler {
 							}
 						}
 						else {
-							throw new InvalidProgramException("Error: The only operations allowed on the solution set are Join and CoGroup.");
+							throw new InvalidProgramException(
+									"Error: The only operations allowed on the solution set are Join and CoGroup.");
 						}
 					}
 				}
@@ -905,14 +910,14 @@ public class PactCompiler {
 				
 				// set the step function nodes to the iteration node
 				iterNode.setPartialSolution(solutionSetNode, worksetNode);
-				iterNode.setNextPartialSolution(solutionSetDeltaNode, nextWorksetNode);
+				iterNode.setNextPartialSolution(solutionSetDeltaNode, nextWorksetNode, defaultDataExchangeMode);
 				
 				// go over the contained data flow and mark the dynamic path nodes
 				StaticDynamicPathIdentifier pathIdentifier = new StaticDynamicPathIdentifier(iterNode.getCostWeight());
 				iterNode.acceptForStepFunction(pathIdentifier);
 			}
 		}
-	};
+	}
 	
 	private static final class StaticDynamicPathIdentifier implements Visitor<OptimizerNode> {
 		
@@ -944,28 +949,21 @@ public class PactCompiler {
 	 * Simple visitor that sets the minimal guaranteed memory per task based on the amount of available memory,
 	 * the number of memory consumers, and on the task's degree of parallelism.
 	 */
-	private static final class IdAndEstimatesVisitor implements Visitor<OptimizerNode> {
+	public static final class IdAndEstimatesVisitor implements Visitor<OptimizerNode> {
 		
 		private final DataStatistics statistics;
 
 		private int id = 1;
 		
-		private IdAndEstimatesVisitor(DataStatistics statistics) {
+		public IdAndEstimatesVisitor(DataStatistics statistics) {
 			this.statistics = statistics;
 		}
 
-
 		@Override
 		public boolean preVisit(OptimizerNode visitable) {
-			if (visitable.getId() != -1) {
-				// been here before
-				return false;
-			}
-			
-			return true;
+			return visitable.getId() == -1;
 		}
 
-
 		@Override
 		public void postVisit(OptimizerNode visitable) {
 			// the node ids
@@ -1031,7 +1029,7 @@ public class PactCompiler {
 	 * that are not a minimally connected DAG (Such plans are not trees, but at least one node feeds its
 	 * output into more than one other node).
 	 */
-	private static final class BranchesVisitor implements Visitor<OptimizerNode> {
+	public static final class BranchesVisitor implements Visitor<OptimizerNode> {
 		
 		@Override
 		public boolean preVisit(OptimizerNode node) {
diff --git a/flink-compiler/src/main/java/org/apache/flink/compiler/dag/AbstractPartialSolutionNode.java b/flink-compiler/src/main/java/org/apache/flink/compiler/dag/AbstractPartialSolutionNode.java
index d996fe915ec..957dd1afa6a 100644
--- a/flink-compiler/src/main/java/org/apache/flink/compiler/dag/AbstractPartialSolutionNode.java
+++ b/flink-compiler/src/main/java/org/apache/flink/compiler/dag/AbstractPartialSolutionNode.java
@@ -23,6 +23,7 @@ import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 
+import org.apache.flink.api.common.ExecutionMode;
 import org.apache.flink.api.common.operators.Operator;
 import org.apache.flink.api.common.operators.SemanticProperties;
 import org.apache.flink.api.common.operators.SemanticProperties.EmptySemanticProperties;
@@ -66,7 +67,7 @@ public abstract class AbstractPartialSolutionNode extends OptimizerNode {
 	}
 
 	@Override
-	public void setInput(Map<Operator<?>, OptimizerNode> contractToNode) {}
+	public void setInput(Map<Operator<?>, OptimizerNode> contractToNode, ExecutionMode dataExchangeMode) {}
 
 	@Override
 	protected void computeOperatorSpecificDefaultEstimates(DataStatistics statistics) {
diff --git a/flink-compiler/src/main/java/org/apache/flink/compiler/dag/BinaryUnionNode.java b/flink-compiler/src/main/java/org/apache/flink/compiler/dag/BinaryUnionNode.java
index ddc776e4c6f..b0db6238550 100644
--- a/flink-compiler/src/main/java/org/apache/flink/compiler/dag/BinaryUnionNode.java
+++ b/flink-compiler/src/main/java/org/apache/flink/compiler/dag/BinaryUnionNode.java
@@ -20,10 +20,10 @@ package org.apache.flink.compiler.dag;
 
 import java.util.ArrayList;
 import java.util.Collections;
-import java.util.HashSet;
 import java.util.List;
 import java.util.Set;
 
+import org.apache.flink.api.common.ExecutionMode;
 import org.apache.flink.api.common.operators.SemanticProperties;
 import org.apache.flink.api.common.operators.Union;
 import org.apache.flink.api.common.operators.util.FieldSet;
@@ -39,6 +39,7 @@ import org.apache.flink.compiler.operators.OperatorDescriptorDual;
 import org.apache.flink.compiler.plan.Channel;
 import org.apache.flink.compiler.plan.NamedChannel;
 import org.apache.flink.compiler.plan.PlanNode;
+import org.apache.flink.runtime.io.network.DataExchangeMode;
 import org.apache.flink.runtime.operators.shipping.ShipStrategyType;
 
 /**
@@ -105,30 +106,22 @@ public class BinaryUnionNode extends TwoInputNode {
 		// step down to all producer nodes and calculate alternative plans
 		final List<? extends PlanNode> subPlans1 = getFirstPredecessorNode().getAlternativePlans(estimator);
 		final List<? extends PlanNode> subPlans2 = getSecondPredecessorNode().getAlternativePlans(estimator);
-		
-		// calculate alternative sub-plans for broadcast inputs
-		final List<Set<? extends NamedChannel>> broadcastPlanChannels = new ArrayList<Set<? extends NamedChannel>>();
+
 		List<PactConnection> broadcastConnections = getBroadcastConnections();
-		List<String> broadcastConnectionNames = getBroadcastConnectionNames();
-		for (int i = 0; i < broadcastConnections.size(); i++ ) {
-			PactConnection broadcastConnection = broadcastConnections.get(i);
-			String broadcastConnectionName = broadcastConnectionNames.get(i);
-			List<PlanNode> broadcastPlanCandidates = broadcastConnection.getSource().getAlternativePlans(estimator);
-			// wrap the plan candidates in named channels 
-			HashSet<NamedChannel> broadcastChannels = new HashSet<NamedChannel>(broadcastPlanCandidates.size());
-			for (PlanNode plan: broadcastPlanCandidates) {
-				final NamedChannel c = new NamedChannel(broadcastConnectionName, plan);
-				c.setShipStrategy(ShipStrategyType.BROADCAST);
-				broadcastChannels.add(c);
-			}
-			broadcastPlanChannels.add(broadcastChannels);
+		if (broadcastConnections != null && broadcastConnections.size() > 0) {
+			throw new CompilerException("Found BroadcastVariables on a Union operation");
 		}
 		
 		final ArrayList<PlanNode> outputPlans = new ArrayList<PlanNode>();
-		
+
+		final List<Set<? extends NamedChannel>> broadcastPlanChannels = Collections.emptyList();
+
 		final BinaryUnionOpDescriptor operator = new BinaryUnionOpDescriptor();
 		final RequestedLocalProperties noLocalProps = new RequestedLocalProperties();
-		
+
+		final ExecutionMode input1Mode = this.input1.getDataExchangeMode();
+		final ExecutionMode input2Mode = this.input2.getDataExchangeMode();
+
 		final int dop = getDegreeOfParallelism();
 		final int inDop1 = getFirstPredecessorNode().getDegreeOfParallelism();
 		final int inDop2 = getSecondPredecessorNode().getDegreeOfParallelism();
@@ -136,6 +129,9 @@ public class BinaryUnionNode extends TwoInputNode {
 		final boolean dopChange1 = dop != inDop1;
 		final boolean dopChange2 = dop != inDop2;
 
+		final boolean input1breakPipeline = this.input1.isBreakingPipeline();
+		final boolean input2breakPipeline = this.input2.isBreakingPipeline();
+
 		// enumerate all pairwise combination of the children's plans together with
 		// all possible operator strategy combination
 		
@@ -154,19 +150,22 @@ public class BinaryUnionNode extends TwoInputNode {
 					Channel c1 = new Channel(child1, this.input1.getMaterializationMode());
 					if (this.input1.getShipStrategy() == null) {
 						// free to choose the ship strategy
-						igps.parameterizeChannel(c1, dopChange1);
+						igps.parameterizeChannel(c1, dopChange1, input1Mode, input1breakPipeline);
 						
 						// if the DOP changed, make sure that we cancel out properties, unless the
 						// ship strategy preserves/establishes them even under changing DOPs
 						if (dopChange1 && !c1.getShipStrategy().isNetworkStrategy()) {
 							c1.getGlobalProperties().reset();
 						}
-					} else {
+					}
+					else {
 						// ship strategy fixed by compiler hint
+						ShipStrategyType shipStrategy = this.input1.getShipStrategy();
+						DataExchangeMode exMode = DataExchangeMode.select(input1Mode, shipStrategy, input1breakPipeline);
 						if (this.keys1 != null) {
-							c1.setShipStrategy(this.input1.getShipStrategy(), this.keys1.toFieldList());
+							c1.setShipStrategy(this.input1.getShipStrategy(), this.keys1.toFieldList(), exMode);
 						} else {
-							c1.setShipStrategy(this.input1.getShipStrategy());
+							c1.setShipStrategy(this.input1.getShipStrategy(), exMode);
 						}
 						
 						if (dopChange1) {
@@ -178,7 +177,7 @@ public class BinaryUnionNode extends TwoInputNode {
 					Channel c2 = new Channel(child2, this.input2.getMaterializationMode());
 					if (this.input2.getShipStrategy() == null) {
 						// free to choose the ship strategy
-						igps.parameterizeChannel(c2, dopChange2);
+						igps.parameterizeChannel(c2, dopChange2, input2Mode, input2breakPipeline);
 						
 						// if the DOP changed, make sure that we cancel out properties, unless the
 						// ship strategy preserves/establishes them even under changing DOPs
@@ -187,10 +186,12 @@ public class BinaryUnionNode extends TwoInputNode {
 						}
 					} else {
 						// ship strategy fixed by compiler hint
+						ShipStrategyType shipStrategy = this.input2.getShipStrategy();
+						DataExchangeMode exMode = DataExchangeMode.select(input2Mode, shipStrategy, input2breakPipeline);
 						if (this.keys2 != null) {
-							c2.setShipStrategy(this.input2.getShipStrategy(), this.keys2.toFieldList());
+							c2.setShipStrategy(this.input2.getShipStrategy(), this.keys2.toFieldList(), exMode);
 						} else {
-							c2.setShipStrategy(this.input2.getShipStrategy());
+							c2.setShipStrategy(this.input2.getShipStrategy(), exMode);
 						}
 						
 						if (dopChange2) {
@@ -204,7 +205,7 @@ public class BinaryUnionNode extends TwoInputNode {
 					p1.clearUniqueFieldCombinations();
 					p2.clearUniqueFieldCombinations();
 					
-					// adjust the partitionings, if they exist but are not equal. this may happen when both channels have a
+					// adjust the partitioning, if they exist but are not equal. this may happen when both channels have a
 					// partitioning that fulfills the requirements, but both are incompatible. For example may a property requirement
 					// be ANY_PARTITIONING on fields (0) and one channel is range partitioned on that field, the other is hash
 					// partitioned on that field. 
@@ -212,20 +213,22 @@ public class BinaryUnionNode extends TwoInputNode {
 						if (c1.getShipStrategy() == ShipStrategyType.FORWARD && c2.getShipStrategy() != ShipStrategyType.FORWARD) {
 							// adjust c2 to c1
 							c2 = c2.clone();
-							p1.parameterizeChannel(c2,dopChange2);
-						} else if (c2.getShipStrategy() == ShipStrategyType.FORWARD && c1.getShipStrategy() != ShipStrategyType.FORWARD) {
+							p1.parameterizeChannel(c2, dopChange2, input2Mode, input2breakPipeline);
+						}
+						else if (c2.getShipStrategy() == ShipStrategyType.FORWARD && c1.getShipStrategy() != ShipStrategyType.FORWARD) {
 							// adjust c1 to c2
 							c1 = c1.clone();
-							p2.parameterizeChannel(c1,dopChange1);
-						} else if (c1.getShipStrategy() == ShipStrategyType.FORWARD && c2.getShipStrategy() == ShipStrategyType.FORWARD) {
+							p2.parameterizeChannel(c1,dopChange1, input1Mode, input1breakPipeline);
+						}
+						else if (c1.getShipStrategy() == ShipStrategyType.FORWARD && c2.getShipStrategy() == ShipStrategyType.FORWARD) {
 							boolean adjustC1 = c1.getEstimatedOutputSize() <= 0 || c2.getEstimatedOutputSize() <= 0 ||
 									c1.getEstimatedOutputSize() <= c2.getEstimatedOutputSize();
 							if (adjustC1) {
 								c2 = c2.clone();
-								p1.parameterizeChannel(c2, dopChange2);
+								p1.parameterizeChannel(c2, dopChange2, input2Mode, input2breakPipeline);
 							} else {
 								c1 = c1.clone();
-								p2.parameterizeChannel(c1, dopChange1);
+								p2.parameterizeChannel(c1, dopChange1, input1Mode, input1breakPipeline);
 							}
 						} else {
 							// this should never happen, as it implies both realize a different strategy, which is
@@ -233,7 +236,8 @@ public class BinaryUnionNode extends TwoInputNode {
 							throw new CompilerException("Bug in Plan Enumeration for Union Node.");
 						}
 					}
-					
+
+
 					instantiate(operator, c1, c2, broadcastPlanChannels, outputPlans, estimator, igps, igps, noLocalProps, noLocalProps);
 				}
 			}
diff --git a/flink-compiler/src/main/java/org/apache/flink/compiler/dag/BulkIterationNode.java b/flink-compiler/src/main/java/org/apache/flink/compiler/dag/BulkIterationNode.java
index 43b579986fe..ac94ef34137 100644
--- a/flink-compiler/src/main/java/org/apache/flink/compiler/dag/BulkIterationNode.java
+++ b/flink-compiler/src/main/java/org/apache/flink/compiler/dag/BulkIterationNode.java
@@ -24,6 +24,7 @@ import java.util.Iterator;
 import java.util.List;
 import java.util.Set;
 
+import org.apache.flink.api.common.ExecutionMode;
 import org.apache.flink.api.common.operators.SemanticProperties;
 import org.apache.flink.api.common.operators.SemanticProperties.EmptySemanticProperties;
 import org.apache.flink.api.common.operators.base.BulkIterationBase;
@@ -139,7 +140,7 @@ public class BulkIterationNode extends SingleInputNode implements IterationNode
 			NoOpNode noop = new NoOpNode();
 			noop.setDegreeOfParallelism(getDegreeOfParallelism());
 
-			PactConnection noOpConn = new PactConnection(nextPartialSolution, noop);
+			PactConnection noOpConn = new PactConnection(nextPartialSolution, noop, ExecutionMode.PIPELINED);
 			noop.setIncomingConnection(noOpConn);
 			nextPartialSolution.addOutgoingConnection(noOpConn);
 			
@@ -151,13 +152,15 @@ public class BulkIterationNode extends SingleInputNode implements IterationNode
 		
 		if (terminationCriterion == null) {
 			this.singleRoot = nextPartialSolution;
-			this.rootConnection = new PactConnection(nextPartialSolution);
+			this.rootConnection = new PactConnection(nextPartialSolution, ExecutionMode.PIPELINED);
 		}
 		else {
 			// we have a termination criterion
 			SingleRootJoiner singleRootJoiner = new SingleRootJoiner();
-			this.rootConnection = new PactConnection(nextPartialSolution, singleRootJoiner);
-			this.terminationCriterionRootConnection = new PactConnection(terminationCriterion, singleRootJoiner);
+			this.rootConnection = new PactConnection(nextPartialSolution, singleRootJoiner, ExecutionMode.PIPELINED);
+			this.terminationCriterionRootConnection = new PactConnection(terminationCriterion, singleRootJoiner,
+																		ExecutionMode.PIPELINED);
+
 			singleRootJoiner.setInputs(this.rootConnection, this.terminationCriterionRootConnection);
 			
 			this.singleRoot = singleRootJoiner;
@@ -316,7 +319,7 @@ public class BulkIterationNode extends SingleInputNode implements IterationNode
 				else if (report == FeedbackPropertiesMeetRequirementsReport.NOT_MET) {
 					// attach a no-op node through which we create the properties of the original input
 					Channel toNoOp = new Channel(candidate);
-					globPropsReq.parameterizeChannel(toNoOp, false);
+					globPropsReq.parameterizeChannel(toNoOp, false, rootConnection.getDataExchangeMode(), false);
 					locPropsReq.parameterizeChannel(toNoOp);
 					
 					UnaryOperatorNode rebuildPropertiesNode = new UnaryOperatorNode("Rebuild Partial Solution Properties", FieldList.EMPTY_LIST);
diff --git a/flink-compiler/src/main/java/org/apache/flink/compiler/dag/DataSinkNode.java b/flink-compiler/src/main/java/org/apache/flink/compiler/dag/DataSinkNode.java
index aa80451bcca..2a0ef2b01d3 100644
--- a/flink-compiler/src/main/java/org/apache/flink/compiler/dag/DataSinkNode.java
+++ b/flink-compiler/src/main/java/org/apache/flink/compiler/dag/DataSinkNode.java
@@ -16,7 +16,6 @@
  * limitations under the License.
  */
 
-
 package org.apache.flink.compiler.dag;
 
 import java.util.ArrayList;
@@ -24,6 +23,7 @@ import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 
+import org.apache.flink.api.common.ExecutionMode;
 import org.apache.flink.api.common.distributions.DataDistribution;
 import org.apache.flink.api.common.operators.GenericDataSinkBase;
 import org.apache.flink.api.common.operators.Operator;
@@ -69,7 +69,9 @@ public class DataSinkNode extends OptimizerNode {
 	}
 	
 	/**
-	 * 
+	 * Gets the predecessor of this node.
+	 *
+	 * @return The predecessor, or null, if no predecessor has been set.
 	 */
 	public OptimizerNode getPredecessorNode() {
 		if(this.input != null) {
@@ -80,9 +82,9 @@ public class DataSinkNode extends OptimizerNode {
 	}
 
 	/**
-	 * Gets the contract object for this data source node.
+	 * Gets the operator for which this optimizer sink node was created.
 	 * 
-	 * @return The contract.
+	 * @return The node's underlying operator.
 	 */
 	@Override
 	public GenericDataSinkBase<?> getPactContract() {
@@ -99,19 +101,25 @@ public class DataSinkNode extends OptimizerNode {
 		return Collections.singletonList(this.input);
 	}
 
+	/**
+	 * Gets all outgoing connections, which is an empty set for the data sink.
+	 *
+	 * @return An empty list.
+	 */
+	@Override
 	public List<PactConnection> getOutgoingConnections() {
 		return Collections.emptyList();
 	}
 
 	@Override
-	public void setInput(Map<Operator<?>, OptimizerNode> contractToNode) {
+	public void setInput(Map<Operator<?>, OptimizerNode> contractToNode, ExecutionMode defaultExchangeMode) {
 		Operator<?> children = getPactContract().getInput();
 
 		final OptimizerNode pred;
 		final PactConnection conn;
 		
 		pred = contractToNode.get(children);
-		conn = new PactConnection(pred, this);
+		conn = new PactConnection(pred, this, defaultExchangeMode);
 			
 		// create the connection and add it
 		this.input = conn;
@@ -170,7 +178,7 @@ public class DataSinkNode extends OptimizerNode {
 		}
 
 		// we need to track open branches even in the sinks, because they get "closed" when
-		// we build a single "roor" for the data flow plan
+		// we build a single "root" for the data flow plan
 		addClosedBranches(getPredecessorNode().closedBranchingNodes);
 		this.openBranches = getPredecessorNode().getBranchesForParent(this.input);
 	}
@@ -199,14 +207,16 @@ public class DataSinkNode extends OptimizerNode {
 		final int dop = getDegreeOfParallelism();
 		final int inDop = getPredecessorNode().getDegreeOfParallelism();
 
+		final ExecutionMode executionMode = this.input.getDataExchangeMode();
 		final boolean dopChange = dop != inDop;
+		final boolean breakPipeline = this.input.isBreakingPipeline();
 
 		InterestingProperties ips = this.input.getInterestingProperties();
 		for (PlanNode p : subPlans) {
 			for (RequestedGlobalProperties gp : ips.getGlobalProperties()) {
 				for (RequestedLocalProperties lp : ips.getLocalProperties()) {
 					Channel c = new Channel(p);
-					gp.parameterizeChannel(c, dopChange);
+					gp.parameterizeChannel(c, dopChange, executionMode, breakPipeline);
 					lp.parameterizeChannel(c);
 					c.setRequiredLocalProps(lp);
 					c.setRequiredGlobalProps(gp);
@@ -214,7 +224,7 @@ public class DataSinkNode extends OptimizerNode {
 					// no need to check whether the created properties meet what we need in case
 					// of ordering or global ordering, because the only interesting properties we have
 					// are what we require
-					outputPlans.add(new SinkPlanNode(this, "DataSink("+this.getPactContract().getName()+")" ,c));
+					outputPlans.add(new SinkPlanNode(this, "DataSink ("+this.getPactContract().getName()+")" ,c));
 				}
 			}
 		}
diff --git a/flink-compiler/src/main/java/org/apache/flink/compiler/dag/DataSourceNode.java b/flink-compiler/src/main/java/org/apache/flink/compiler/dag/DataSourceNode.java
index 49946e07ac8..3454db0c8ed 100644
--- a/flink-compiler/src/main/java/org/apache/flink/compiler/dag/DataSourceNode.java
+++ b/flink-compiler/src/main/java/org/apache/flink/compiler/dag/DataSourceNode.java
@@ -16,7 +16,6 @@
  * limitations under the License.
  */
 
-
 package org.apache.flink.compiler.dag;
 
 import java.util.ArrayList;
@@ -24,6 +23,7 @@ import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 
+import org.apache.flink.api.common.ExecutionMode;
 import org.apache.flink.api.common.functions.Partitioner;
 import org.apache.flink.api.common.io.FileInputFormat;
 import org.apache.flink.api.common.io.InputFormat;
@@ -81,11 +81,8 @@ public class DataSourceNode extends OptimizerNode {
 			this.sequentialInput = false;
 		}
 
-		if (pactContract.getUserCodeWrapper().getUserCodeObject() instanceof ReplicatingInputFormat) {
-			this.replicatedInput = true;
-		} else {
-			this.replicatedInput = false;
-		}
+		this.replicatedInput = ReplicatingInputFormat.class.isAssignableFrom(
+														pactContract.getUserCodeWrapper().getUserCodeClass());
 
 		this.gprops = new GlobalProperties();
 		this.lprops = new LocalProperties();
@@ -119,7 +116,7 @@ public class DataSourceNode extends OptimizerNode {
 
 	@Override
 	public void setDegreeOfParallelism(int degreeOfParallelism) {
-		// if unsplittable, DOP remains at 1
+		// if unsplittable, parallelism remains at 1
 		if (!this.sequentialInput) {
 			super.setDegreeOfParallelism(degreeOfParallelism);
 		}
@@ -131,14 +128,14 @@ public class DataSourceNode extends OptimizerNode {
 	}
 
 	@Override
-	public void setInput(Map<Operator<?>, OptimizerNode> contractToNode) {}
+	public void setInput(Map<Operator<?>, OptimizerNode> contractToNode, ExecutionMode defaultDataExchangeMode) {}
 
 	@Override
 	protected void computeOperatorSpecificDefaultEstimates(DataStatistics statistics) {
 		// see, if we have a statistics object that can tell us a bit about the file
 		if (statistics != null) {
 			// instantiate the input format, as this is needed by the statistics 
-			InputFormat<?, ?> format = null;
+			InputFormat<?, ?> format;
 			String inFormatDescription = "<unknown>";
 			
 			try {
@@ -156,7 +153,9 @@ public class DataSourceNode extends OptimizerNode {
 			try {
 				inFormatDescription = format.toString();
 			}
-			catch (Throwable t) {}
+			catch (Throwable t) {
+				// we can ignore this error, as it only prevents us to use a cosmetic string
+			}
 			
 			// first of all, get the statistics from the cache
 			final String statisticsKey = getPactContract().getStatisticsKey();
diff --git a/flink-compiler/src/main/java/org/apache/flink/compiler/dag/OptimizerNode.java b/flink-compiler/src/main/java/org/apache/flink/compiler/dag/OptimizerNode.java
index 4e4ef282d91..16c60a41515 100644
--- a/flink-compiler/src/main/java/org/apache/flink/compiler/dag/OptimizerNode.java
+++ b/flink-compiler/src/main/java/org/apache/flink/compiler/dag/OptimizerNode.java
@@ -28,6 +28,7 @@ import java.util.List;
 import java.util.Map;
 import java.util.Set;
 
+import org.apache.flink.api.common.ExecutionMode;
 import org.apache.flink.api.common.operators.AbstractUdfOperator;
 import org.apache.flink.api.common.operators.CompilerHints;
 import org.apache.flink.api.common.operators.Operator;
@@ -133,42 +134,39 @@ public abstract class OptimizerNode implements Visitable<OptimizerNode>, Estimat
 	}
 
 	// ------------------------------------------------------------------------
-	//      Abstract methods that implement node specific behavior
-	//        and the pact type specific optimization methods.
+	//  Methods specific to unary- / binary- / special nodes
 	// ------------------------------------------------------------------------
 
 	/**
-	 * Gets the name of this node. This returns either the name of the PACT, or
-	 * a string marking the node as a data source or a data sink.
+	 * Gets the name of this node, which is the name of the function/operator, or
+	 * data source / data sink.
 	 * 
 	 * @return The node name.
 	 */
 	public abstract String getName();
 
 	/**
-	 * This function is for plan translation purposes. Upon invocation, the implementing subclasses should
-	 * examine its contained contract and look at the contracts that feed their data into that contract.
-	 * The method should then create a <tt>PactConnection</tt> for each of those inputs.
-	 * <p>
-	 * In addition, the nodes must set the shipping strategy of the connection, if a suitable optimizer hint is found.
-	 * 
-	 * @param contractToNode
-	 *        The map to translate the contracts to their corresponding optimizer nodes.
+	 * This function connects the predecessors to this operator.
+	 *
+	 * @param operatorToNode The map from program operators to optimizer nodes.
+	 * @param defaultExchangeMode The data exchange mode to use, if the operator does not
+	 *                            specify one.
 	 */
-	public abstract void setInput(Map<Operator<?>, OptimizerNode> contractToNode);
+	public abstract void setInput(Map<Operator<?>, OptimizerNode> operatorToNode,
+									ExecutionMode defaultExchangeMode);
 
 	/**
-	 * This function is for plan translation purposes. Upon invocation, this method creates a {@link PactConnection}
-	 * for each one of the broadcast inputs associated with the {@code Operator} referenced by this node.
-	 * <p>
-	 * The {@code PactConnections} must set its shipping strategy type to BROADCAST.
-	 * 
-	 * @param operatorToNode
-	 *        The map associating operators with their corresponding optimizer nodes.
+	 * This function connects the predecessors to this operator.
+	 *
+	 * @param operatorToNode The map from program operators to optimizer nodes.
+	 * @param defaultExchangeMode The data exchange mode to use, if the operator does not
+	 *                            specify one.
+	 *
 	 * @throws CompilerException
 	 */
-	public void setBroadcastInputs(Map<Operator<?>, OptimizerNode> operatorToNode) throws CompilerException {
-
+	public void setBroadcastInputs(Map<Operator<?>, OptimizerNode> operatorToNode, ExecutionMode defaultExchangeMode)
+			throws CompilerException
+	{
 		// skip for Operators that don't support broadcast variables 
 		if (!(getPactContract() instanceof AbstractUdfOperator<?, ?>)) {
 			return;
@@ -180,7 +178,8 @@ public abstract class OptimizerNode implements Visitable<OptimizerNode>, Estimat
 		// create connections and add them
 		for (Map.Entry<String, Operator<?>> input : operator.getBroadcastInputs().entrySet()) {
 			OptimizerNode predecessor = operatorToNode.get(input.getValue());
-			PactConnection connection = new PactConnection(predecessor, this, ShipStrategyType.BROADCAST);
+			PactConnection connection = new PactConnection(predecessor, this,
+															ShipStrategyType.BROADCAST, defaultExchangeMode);
 			addBroadcastConnection(input.getKey(), connection);
 			predecessor.addOutgoingConnection(connection);
 		}
@@ -189,7 +188,7 @@ public abstract class OptimizerNode implements Visitable<OptimizerNode>, Estimat
 	/**
 	 * This method needs to be overridden by subclasses to return the children.
 	 * 
-	 * @return The list of incoming links.
+	 * @return The list of incoming connections.
 	 */
 	public abstract List<PactConnection> getIncomingConnections();
 
@@ -198,7 +197,7 @@ public abstract class OptimizerNode implements Visitable<OptimizerNode>, Estimat
 	 * for the node itself must have been computed before.
 	 * The node must then see how many of interesting properties it preserves and add its own.
 	 * 
-	 * @param estimator	The {@code CostEstimator} instance to use for plan cost estimation.
+	 * @param estimator The {@code CostEstimator} instance to use for plan cost estimation.
 	 */
 	public abstract void computeInterestingPropertiesForInputs(CostEstimator estimator);
 
@@ -212,7 +211,9 @@ public abstract class OptimizerNode implements Visitable<OptimizerNode>, Estimat
 	public abstract void computeUnclosedBranchStack();
 	
 	
-	protected List<UnclosedBranchDescriptor> computeUnclosedBranchStackForBroadcastInputs(List<UnclosedBranchDescriptor> branchesSoFar) {
+	protected List<UnclosedBranchDescriptor> computeUnclosedBranchStackForBroadcastInputs(
+															List<UnclosedBranchDescriptor> branchesSoFar)
+	{
 		// handle the data flow branching for the broadcast inputs
 		for (PactConnection broadcastInput : getBroadcastConnections()) {
 			OptimizerNode bcSource = broadcastInput.getSource();
@@ -221,7 +222,7 @@ public abstract class OptimizerNode implements Visitable<OptimizerNode>, Estimat
 			List<UnclosedBranchDescriptor> bcBranches = bcSource.getBranchesForParent(broadcastInput);
 			
 			ArrayList<UnclosedBranchDescriptor> mergedBranches = new ArrayList<UnclosedBranchDescriptor>();
-			mergeLists(branchesSoFar, bcBranches, mergedBranches);
+			mergeLists(branchesSoFar, bcBranches, mergedBranches, true);
 			branchesSoFar = mergedBranches.isEmpty() ? Collections.<UnclosedBranchDescriptor>emptyList() : mergedBranches;
 		}
 		
@@ -261,9 +262,9 @@ public abstract class OptimizerNode implements Visitable<OptimizerNode>, Estimat
 	@Override
 	public Iterable<OptimizerNode> getPredecessors() {
 		List<OptimizerNode> allPredecessors = new ArrayList<OptimizerNode>();
-		
-		for (Iterator<PactConnection> inputs = getIncomingConnections().iterator(); inputs.hasNext(); ){
-			allPredecessors.add(inputs.next().getSource());
+
+		for (PactConnection pactConnection : getIncomingConnections()) {
+			allPredecessors.add(pactConnection.getSource());
 		}
 		
 		for (PactConnection conn : getBroadcastConnections()) {
@@ -303,8 +304,7 @@ public abstract class OptimizerNode implements Visitable<OptimizerNode>, Estimat
 	/**
 	 * Adds the broadcast connection identified by the given {@code name} to this node.
 	 * 
-	 * @param broadcastConnection
-	 *        The connection to add.
+	 * @param broadcastConnection The connection to add.
 	 */
 	public void addBroadcastConnection(String name, PactConnection broadcastConnection) {
 		this.broadcastConnectionNames.add(name);
@@ -353,42 +353,40 @@ public abstract class OptimizerNode implements Visitable<OptimizerNode>, Estimat
 	}
 
 	/**
-	 * Gets the object that specifically describes the contract of this node.
+	 * Gets the operator represented by this optimizer node.
 	 * 
-	 * @return This node's contract.
+	 * @return This node's operator.
 	 */
 	public Operator<?> getPactContract() {
 		return this.pactContract;
 	}
 
 	/**
-	 * Gets the degree of parallelism for the contract represented by this optimizer node.
-	 * The degree of parallelism denotes how many parallel instances of the user function will be
+	 * Gets the parallelism for the operator represented by this optimizer node.
+	 * The parallelism denotes how many parallel instances of the operator on will be
 	 * spawned during the execution. If this value is <code>-1</code>, then the system will take
 	 * the default number of parallel instances.
 	 * 
-	 * @return The degree of parallelism.
+	 * @return The parallelism of the operator.
 	 */
 	public int getDegreeOfParallelism() {
 		return this.degreeOfParallelism;
 	}
 
 	/**
-	 * Sets the degree of parallelism for the contract represented by this optimizer node.
-	 * The degree of parallelism denotes how many parallel instances of the user function will be
+	 * Sets the parallelism for this optimizer node.
+	 * The parallelism denotes how many parallel instances of the operator will be
 	 * spawned during the execution. If this value is set to <code>-1</code>, then the system will take
 	 * the default number of parallel instances.
 	 * 
-	 * @param degreeOfParallelism
-	 *        The degree of parallelism to set.
-	 * @throws IllegalArgumentException
-	 *         If the degree of parallelism is smaller than one and not -1.
+	 * @param parallelism The parallelism to set.
+	 * @throws IllegalArgumentException If the parallelism is smaller than one and not -1.
 	 */
-	public void setDegreeOfParallelism(int degreeOfParallelism) {
-		if (degreeOfParallelism < 1) {
-			throw new IllegalArgumentException("Degree of parallelism of " + degreeOfParallelism + " is invalid.");
+	public void setDegreeOfParallelism(int parallelism) {
+		if (parallelism < 1 && parallelism != -1) {
+			throw new IllegalArgumentException("Degree of parallelism of " + parallelism + " is invalid.");
 		}
-		this.degreeOfParallelism = degreeOfParallelism;
+		this.degreeOfParallelism = parallelism;
 	}
 	
 	/**
@@ -498,6 +496,15 @@ public abstract class OptimizerNode implements Visitable<OptimizerNode>, Estimat
 		return getOutgoingConnections() != null && getOutgoingConnections().size() > 1;
 	}
 
+	public void markAllOutgoingConnectionsAsPipelineBreaking() {
+		if (this.outgoingConnections == null) {
+			throw new IllegalStateException("The outgoing connections have not yet been initialized.");
+		}
+		for (PactConnection conn : getOutgoingConnections()) {
+			conn.markBreaksPipeline();
+		}
+	}
+
 	// ------------------------------------------------------------------------
 	//                              Miscellaneous
 	// ------------------------------------------------------------------------
@@ -971,10 +978,15 @@ public abstract class OptimizerNode implements Visitable<OptimizerNode>, Estimat
 	}
 	
 	/**
-	 * The node IDs are assigned in graph-traversal order (pre-order), hence, each list is sorted by ID in ascending order and
-	 * all consecutive lists start with IDs in ascending order.
+	 * The node IDs are assigned in graph-traversal order (pre-order), hence, each list is
+	 * sorted by ID in ascending order and all consecutive lists start with IDs in ascending order.
+	 *
+	 * @param markJoinedBranchesAsPipelineBreaking True, if the
 	 */
-	protected final boolean mergeLists(List<UnclosedBranchDescriptor> child1open, List<UnclosedBranchDescriptor> child2open, List<UnclosedBranchDescriptor> result) {
+	protected final boolean mergeLists(List<UnclosedBranchDescriptor> child1open,
+										List<UnclosedBranchDescriptor> child2open,
+										List<UnclosedBranchDescriptor> result,
+										boolean markJoinedBranchesAsPipelineBreaking) {
 
 		//remove branches which have already been closed
 		removeClosedBranches(child1open);
@@ -1033,7 +1045,15 @@ public abstract class OptimizerNode implements Visitable<OptimizerNode>, Estimat
 				// if it is the same, add it only once, otherwise process the join of the paths
 				if (vector1 == vector2) {
 					result.add(child1open.get(index1));
-				} else {
+				}
+				else {
+					// we merge (re-join) a branch
+
+					// mark the branch as a point where we break the pipeline
+					if (markJoinedBranchesAsPipelineBreaking) {
+						currBanchingNode.markAllOutgoingConnectionsAsPipelineBreaking();
+					}
+
 					if (this.hereJoinedBranches == null) {
 						this.hereJoinedBranches = new ArrayList<OptimizerNode>(2);
 					}
diff --git a/flink-compiler/src/main/java/org/apache/flink/compiler/dag/PactConnection.java b/flink-compiler/src/main/java/org/apache/flink/compiler/dag/PactConnection.java
index 41369c9c1d7..c215ae07043 100644
--- a/flink-compiler/src/main/java/org/apache/flink/compiler/dag/PactConnection.java
+++ b/flink-compiler/src/main/java/org/apache/flink/compiler/dag/PactConnection.java
@@ -51,6 +51,8 @@ public class PactConnection implements EstimateProvider, DumpableConnection<Opti
 	
 	private int maxDepth = -1;
 
+	private boolean breakPipeline;  // whether this connection should break the pipeline due to potential deadlocks
+
 	/**
 	 * Creates a new Connection between two nodes. The shipping strategy is by default <tt>NONE</tt>.
 	 * The temp mode is by default <tt>NONE</tt>.
@@ -74,10 +76,10 @@ public class PactConnection implements EstimateProvider, DumpableConnection<Opti
 	 * @param shipStrategy
 	 *        The shipping strategy.
 	 * @param exchangeMode
-	 *        The data exchange mode (pipelined / batch)
+	 *        The data exchange mode (pipelined / batch / batch only for shuffles / ... )
 	 */
 	public PactConnection(OptimizerNode source, OptimizerNode target,
-						  ShipStrategyType shipStrategy, ExecutionMode exchangeMode)
+							ShipStrategyType shipStrategy, ExecutionMode exchangeMode)
 	{
 		if (source == null || target == null) {
 			throw new NullPointerException("Source and target must not be null.");
@@ -95,14 +97,14 @@ public class PactConnection implements EstimateProvider, DumpableConnection<Opti
 	 * @param source
 	 *        The source node.
 	 */
-	public PactConnection(OptimizerNode source) {
+	public PactConnection(OptimizerNode source, ExecutionMode exchangeMode) {
 		if (source == null) {
 			throw new NullPointerException("Source and target must not be null.");
 		}
 		this.source = source;
 		this.target = null;
 		this.shipStrategy = ShipStrategyType.NONE;
-		this.dataExchangeMode = null;
+		this.dataExchangeMode = exchangeMode;
 	}
 
 	/**
@@ -149,11 +151,29 @@ public class PactConnection implements EstimateProvider, DumpableConnection<Opti
 	 */
 	public ExecutionMode getDataExchangeMode() {
 		if (dataExchangeMode == null) {
-			throw new IllegalStateException("This connection does not have a data exchange");
+			throw new IllegalStateException("This connection does not have the data exchange mode set");
 		}
 		return dataExchangeMode;
 	}
 
+	/**
+	 * Marks that this connection should do a decoupled data exchange (such as batched)
+	 * rather then pipeline data. Connections are marked as pipeline breakers to avoid
+	 * deadlock situations.
+	 */
+	public void markBreaksPipeline() {
+		this.breakPipeline = true;
+	}
+
+	/**
+	 * Checks whether this connection is marked to break the pipeline.
+	 *
+	 * @return True, if this connection is marked to break the pipeline, false otherwise.
+	 */
+	public boolean isBreakingPipeline() {
+		return this.breakPipeline;
+	}
+
 	/**
 	 * Gets the interesting properties object for this pact connection.
 	 * If the interesting properties for this connections have not yet been set,
@@ -199,6 +219,8 @@ public class PactConnection implements EstimateProvider, DumpableConnection<Opti
 		}
 	}
 
+	// --------------------------------------------------------------------------------------------
+	//  Estimates
 	// --------------------------------------------------------------------------------------------
 
 	@Override
diff --git a/flink-compiler/src/main/java/org/apache/flink/compiler/dag/SingleInputNode.java b/flink-compiler/src/main/java/org/apache/flink/compiler/dag/SingleInputNode.java
index 70c42919086..a23f0e5b113 100644
--- a/flink-compiler/src/main/java/org/apache/flink/compiler/dag/SingleInputNode.java
+++ b/flink-compiler/src/main/java/org/apache/flink/compiler/dag/SingleInputNode.java
@@ -30,6 +30,7 @@ import java.util.List;
 import java.util.Map;
 import java.util.Set;
 
+import org.apache.flink.api.common.ExecutionMode;
 import org.apache.flink.api.common.operators.Operator;
 import org.apache.flink.api.common.operators.SemanticProperties;
 import org.apache.flink.api.common.operators.SingleInputOperator;
@@ -50,17 +51,18 @@ import org.apache.flink.compiler.plan.SingleInputPlanNode;
 import org.apache.flink.compiler.plan.PlanNode.SourceAndDamReport;
 import org.apache.flink.compiler.util.NoOpUnaryUdfOp;
 import org.apache.flink.configuration.Configuration;
+import org.apache.flink.runtime.io.network.DataExchangeMode;
 import org.apache.flink.runtime.operators.shipping.ShipStrategyType;
 import org.apache.flink.util.Visitor;
 
 import com.google.common.collect.Sets;
 
 /**
- * A node in the optimizer's program representation for a PACT with a single input.
+ * A node in the optimizer's program representation for an operation with a single input.
  * 
- * This class contains all the generic logic for branch handling, interesting properties,
- * and candidate plan enumeration. The subclasses for specific operators simply add logic
- * for cost estimates and specify possible strategies for their realization.
+ * This class contains all the generic logic for handling branching flows, as well as to
+ * enumerate candidate execution plans. The subclasses for specific operators simply add logic
+ * for cost estimates and specify possible strategies for their execution.
  */
 public abstract class SingleInputNode extends OptimizerNode {
 	
@@ -73,12 +75,12 @@ public abstract class SingleInputNode extends OptimizerNode {
 	/**
 	 * Creates a new node with a single input for the optimizer plan.
 	 * 
-	 * @param pactContract The PACT that the node represents.
+	 * @param programOperator The PACT that the node represents.
 	 */
-	protected SingleInputNode(SingleInputOperator<?, ?, ?> pactContract) {
-		super(pactContract);
+	protected SingleInputNode(SingleInputOperator<?, ?, ?> programOperator) {
+		super(programOperator);
 		
-		int[] k = pactContract.getKeyColumns(0);
+		int[] k = programOperator.getKeyColumns(0);
 		this.keys = k == null || k.length == 0 ? null : new FieldSet(k);
 	}
 	
@@ -115,7 +117,7 @@ public abstract class SingleInputNode extends OptimizerNode {
 	}
 
 	/**
-	 * Sets the <tt>PactConnection</tt> through which this node receives its input.
+	 * Sets the connection through which this node receives its input.
 	 * 
 	 * @param inConn The input connection to set.
 	 */
@@ -136,7 +138,6 @@ public abstract class SingleInputNode extends OptimizerNode {
 		}
 	}
 
-
 	@Override
 	public List<PactConnection> getIncomingConnections() {
 		return Collections.singletonList(this.inConn);
@@ -145,12 +146,14 @@ public abstract class SingleInputNode extends OptimizerNode {
 
 	@Override
 	public SemanticProperties getSemanticProperties() {
-		return ((SingleInputOperator<?,?,?>) getPactContract()).getSemanticProperties();
+		return getPactContract().getSemanticProperties();
 	}
 	
 
 	@Override
-	public void setInput(Map<Operator<?>, OptimizerNode> contractToNode) throws CompilerException {
+	public void setInput(Map<Operator<?>, OptimizerNode> contractToNode, ExecutionMode defaultExchangeMode)
+			throws CompilerException
+	{
 		// see if an internal hint dictates the strategy to use
 		final Configuration conf = getPactContract().getParameters();
 		final String shipStrategy = conf.getString(PactCompiler.HINT_SHIP_STRATEGY, null);
@@ -181,7 +184,7 @@ public abstract class SingleInputNode extends OptimizerNode {
 			throw new CompilerException("Error: Node for '" + getPactContract().getName() + "' has no input.");
 		} else {
 			pred = contractToNode.get(children);
-			conn = new PactConnection(pred, this);
+			conn = new PactConnection(pred, this, defaultExchangeMode);
 			if (preSet != null) {
 				conn.setShipStrategy(preSet);
 			}
@@ -250,15 +253,19 @@ public abstract class SingleInputNode extends OptimizerNode {
 		final List<Set<? extends NamedChannel>> broadcastPlanChannels = new ArrayList<Set<? extends NamedChannel>>();
 		List<PactConnection> broadcastConnections = getBroadcastConnections();
 		List<String> broadcastConnectionNames = getBroadcastConnectionNames();
+
 		for (int i = 0; i < broadcastConnections.size(); i++ ) {
 			PactConnection broadcastConnection = broadcastConnections.get(i);
 			String broadcastConnectionName = broadcastConnectionNames.get(i);
 			List<PlanNode> broadcastPlanCandidates = broadcastConnection.getSource().getAlternativePlans(estimator);
-			// wrap the plan candidates in named channels 
+
+			// wrap the plan candidates in named channels
 			HashSet<NamedChannel> broadcastChannels = new HashSet<NamedChannel>(broadcastPlanCandidates.size());
 			for (PlanNode plan: broadcastPlanCandidates) {
-				final NamedChannel c = new NamedChannel(broadcastConnectionName, plan);
-				c.setShipStrategy(ShipStrategyType.BROADCAST);
+				NamedChannel c = new NamedChannel(broadcastConnectionName, plan);
+				DataExchangeMode exMode = DataExchangeMode.select(broadcastConnection.getDataExchangeMode(),
+										ShipStrategyType.BROADCAST, broadcastConnection.isBreakingPipeline());
+				c.setShipStrategy(ShipStrategyType.BROADCAST, exMode);
 				broadcastChannels.add(c);
 			}
 			broadcastPlanChannels.add(broadcastChannels);
@@ -270,21 +277,24 @@ public abstract class SingleInputNode extends OptimizerNode {
 			for (OperatorDescriptorSingle ods : getPossibleProperties()) {
 				pairs.addAll(ods.getPossibleGlobalProperties());
 			}
-			allValidGlobals = (RequestedGlobalProperties[]) pairs.toArray(new RequestedGlobalProperties[pairs.size()]);
+			allValidGlobals = pairs.toArray(new RequestedGlobalProperties[pairs.size()]);
 		}
 		final ArrayList<PlanNode> outputPlans = new ArrayList<PlanNode>();
-		
+
+		final ExecutionMode executionMode = this.inConn.getDataExchangeMode();
+
 		final int dop = getDegreeOfParallelism();
 		final int inDop = getPredecessorNode().getDegreeOfParallelism();
-
 		final boolean dopChange = inDop != dop;
 
+		final boolean breaksPipeline = this.inConn.isBreakingPipeline();
+
 		// create all candidates
 		for (PlanNode child : subPlans) {
 
-			if(child.getGlobalProperties().isFullyReplicated()) {
+			if (child.getGlobalProperties().isFullyReplicated()) {
 				// fully replicated input is always locally forwarded if DOP is not changed
-				if(dopChange) {
+				if (dopChange) {
 					// can not continue with this child
 					childrenSkippedDueToReplicatedInput = true;
 					continue;
@@ -297,7 +307,7 @@ public abstract class SingleInputNode extends OptimizerNode {
 				// pick the strategy ourselves
 				for (RequestedGlobalProperties igps: intGlobal) {
 					final Channel c = new Channel(child, this.inConn.getMaterializationMode());
-					igps.parameterizeChannel(c, dopChange);
+					igps.parameterizeChannel(c, dopChange, executionMode, breaksPipeline);
 					
 					// if the DOP changed, make sure that we cancel out properties, unless the
 					// ship strategy preserves/establishes them even under changing DOPs
@@ -320,10 +330,13 @@ public abstract class SingleInputNode extends OptimizerNode {
 			} else {
 				// hint fixed the strategy
 				final Channel c = new Channel(child, this.inConn.getMaterializationMode());
+				final ShipStrategyType shipStrategy = this.inConn.getShipStrategy();
+				final DataExchangeMode exMode = DataExchangeMode.select(executionMode, shipStrategy, breaksPipeline);
+
 				if (this.keys != null) {
-					c.setShipStrategy(this.inConn.getShipStrategy(), this.keys.toFieldList());
+					c.setShipStrategy(shipStrategy, this.keys.toFieldList(), exMode);
 				} else {
-					c.setShipStrategy(this.inConn.getShipStrategy());
+					c.setShipStrategy(shipStrategy, exMode);
 				}
 				
 				if (dopChange) {
diff --git a/flink-compiler/src/main/java/org/apache/flink/compiler/dag/SinkJoiner.java b/flink-compiler/src/main/java/org/apache/flink/compiler/dag/SinkJoiner.java
index c15307882c6..088a0f3230f 100644
--- a/flink-compiler/src/main/java/org/apache/flink/compiler/dag/SinkJoiner.java
+++ b/flink-compiler/src/main/java/org/apache/flink/compiler/dag/SinkJoiner.java
@@ -22,6 +22,7 @@ import java.util.ArrayList;
 import java.util.Collections;
 import java.util.List;
 
+import org.apache.flink.api.common.ExecutionMode;
 import org.apache.flink.api.common.typeinfo.NothingTypeInfo;
 import org.apache.flink.compiler.DataStatistics;
 import org.apache.flink.compiler.operators.OperatorDescriptorDual;
@@ -39,8 +40,8 @@ public class SinkJoiner extends TwoInputNode {
 	public SinkJoiner(OptimizerNode input1, OptimizerNode input2) {
 		super(new NoOpBinaryUdfOp<Nothing>(new NothingTypeInfo()));
 
-		PactConnection conn1 = new PactConnection(input1, this);
-		PactConnection conn2 = new PactConnection(input2, this);
+		PactConnection conn1 = new PactConnection(input1, this, null, ExecutionMode.PIPELINED);
+		PactConnection conn2 = new PactConnection(input2, this, null, ExecutionMode.PIPELINED);
 		
 		this.input1 = conn1;
 		this.input2 = conn2;
@@ -87,7 +88,7 @@ public class SinkJoiner extends TwoInputNode {
 			List<UnclosedBranchDescriptor> result2 = new ArrayList<UnclosedBranchDescriptor>(pred2branches);
 			
 			ArrayList<UnclosedBranchDescriptor> result = new ArrayList<UnclosedBranchDescriptor>();
-			mergeLists(result1, result2, result);
+			mergeLists(result1, result2, result, false);
 			
 			this.openBranches = result.isEmpty() ? Collections.<UnclosedBranchDescriptor>emptyList() : result;
 		}
diff --git a/flink-compiler/src/main/java/org/apache/flink/compiler/dag/TwoInputNode.java b/flink-compiler/src/main/java/org/apache/flink/compiler/dag/TwoInputNode.java
index b39189019d8..6faceaa7726 100644
--- a/flink-compiler/src/main/java/org/apache/flink/compiler/dag/TwoInputNode.java
+++ b/flink-compiler/src/main/java/org/apache/flink/compiler/dag/TwoInputNode.java
@@ -30,6 +30,7 @@ import java.util.List;
 import java.util.Map;
 import java.util.Set;
 
+import org.apache.flink.api.common.ExecutionMode;
 import org.apache.flink.api.common.operators.DualInputOperator;
 import org.apache.flink.api.common.operators.Operator;
 import org.apache.flink.api.common.operators.SemanticProperties;
@@ -51,6 +52,7 @@ import org.apache.flink.compiler.plan.NamedChannel;
 import org.apache.flink.compiler.plan.PlanNode;
 import org.apache.flink.compiler.plan.PlanNode.SourceAndDamReport;
 import org.apache.flink.configuration.Configuration;
+import org.apache.flink.runtime.io.network.DataExchangeMode;
 import org.apache.flink.runtime.operators.DamBehavior;
 import org.apache.flink.runtime.operators.DriverStrategy;
 import org.apache.flink.runtime.operators.shipping.ShipStrategyType;
@@ -155,7 +157,7 @@ public abstract class TwoInputNode extends OptimizerNode {
 
 
 	@Override
-	public void setInput(Map<Operator<?>, OptimizerNode> contractToNode) {
+	public void setInput(Map<Operator<?>, OptimizerNode> contractToNode, ExecutionMode defaultExecutionMode) {
 		// see if there is a hint that dictates which shipping strategy to use for BOTH inputs
 		final Configuration conf = getPactContract().getParameters();
 		ShipStrategyType preSet1 = null;
@@ -215,7 +217,7 @@ public abstract class TwoInputNode extends OptimizerNode {
 		}
 		
 		// get the predecessors
-		DualInputOperator<?, ?, ?, ?> contr = (DualInputOperator<?, ?, ?, ?>) getPactContract();
+		DualInputOperator<?, ?, ?, ?> contr = getPactContract();
 		
 		Operator<?> leftPred = contr.getFirstInput();
 		Operator<?> rightPred = contr.getSecondInput();
@@ -226,7 +228,7 @@ public abstract class TwoInputNode extends OptimizerNode {
 			throw new CompilerException("Error: Node for '" + getPactContract().getName() + "' has no input set for first input.");
 		} else {
 			pred1 = contractToNode.get(leftPred);
-			conn1 = new PactConnection(pred1, this);
+			conn1 = new PactConnection(pred1, this, defaultExecutionMode);
 			if (preSet1 != null) {
 				conn1.setShipStrategy(preSet1);
 			}
@@ -242,7 +244,7 @@ public abstract class TwoInputNode extends OptimizerNode {
 			throw new CompilerException("Error: Node for '" + getPactContract().getName() + "' has no input set for second input.");
 		} else {
 			pred2 = contractToNode.get(rightPred);
-			conn2 = new PactConnection(pred2, this);
+			conn2 = new PactConnection(pred2, this, defaultExecutionMode);
 			if (preSet2 != null) {
 				conn2.setShipStrategy(preSet2);
 			}
@@ -314,15 +316,19 @@ public abstract class TwoInputNode extends OptimizerNode {
 		final List<Set<? extends NamedChannel>> broadcastPlanChannels = new ArrayList<Set<? extends NamedChannel>>();
 		List<PactConnection> broadcastConnections = getBroadcastConnections();
 		List<String> broadcastConnectionNames = getBroadcastConnectionNames();
+
 		for (int i = 0; i < broadcastConnections.size(); i++ ) {
 			PactConnection broadcastConnection = broadcastConnections.get(i);
 			String broadcastConnectionName = broadcastConnectionNames.get(i);
 			List<PlanNode> broadcastPlanCandidates = broadcastConnection.getSource().getAlternativePlans(estimator);
-			// wrap the plan candidates in named channels 
+
+			// wrap the plan candidates in named channels
 			HashSet<NamedChannel> broadcastChannels = new HashSet<NamedChannel>(broadcastPlanCandidates.size());
 			for (PlanNode plan: broadcastPlanCandidates) {
 				final NamedChannel c = new NamedChannel(broadcastConnectionName, plan);
-				c.setShipStrategy(ShipStrategyType.BROADCAST);
+				DataExchangeMode exMode = DataExchangeMode.select(broadcastConnection.getDataExchangeMode(),
+											ShipStrategyType.BROADCAST, broadcastConnection.isBreakingPipeline());
+				c.setShipStrategy(ShipStrategyType.BROADCAST, exMode);
 				broadcastChannels.add(c);
 			}
 			broadcastPlanChannels.add(broadcastChannels);
@@ -337,12 +343,15 @@ public abstract class TwoInputNode extends OptimizerNode {
 				pairsGlob.addAll(ods.getPossibleGlobalProperties());
 				pairsLoc.addAll(ods.getPossibleLocalProperties());
 			}
-			allGlobalPairs = (GlobalPropertiesPair[]) pairsGlob.toArray(new GlobalPropertiesPair[pairsGlob.size()]);
-			allLocalPairs = (LocalPropertiesPair[]) pairsLoc.toArray(new LocalPropertiesPair[pairsLoc.size()]);
+			allGlobalPairs = pairsGlob.toArray(new GlobalPropertiesPair[pairsGlob.size()]);
+			allLocalPairs = pairsLoc.toArray(new LocalPropertiesPair[pairsLoc.size()]);
 		}
 		
 		final ArrayList<PlanNode> outputPlans = new ArrayList<PlanNode>();
-		
+
+		final ExecutionMode input1Mode = this.input1.getDataExchangeMode();
+		final ExecutionMode input2Mode = this.input2.getDataExchangeMode();
+
 		final int dop = getDegreeOfParallelism();
 		final int inDop1 = getFirstPredecessorNode().getDegreeOfParallelism();
 		final int inDop2 = getSecondPredecessorNode().getDegreeOfParallelism();
@@ -350,15 +359,18 @@ public abstract class TwoInputNode extends OptimizerNode {
 		final boolean dopChange1 = dop != inDop1;
 		final boolean dopChange2 = dop != inDop2;
 
+		final boolean input1breaksPipeline = this.input1.isBreakingPipeline();
+		final boolean input2breaksPipeline = this.input2.isBreakingPipeline();
+
 		// enumerate all pairwise combination of the children's plans together with
 		// all possible operator strategy combination
 		
 		// create all candidates
 		for (PlanNode child1 : subPlans1) {
 
-			if(child1.getGlobalProperties().isFullyReplicated()) {
+			if (child1.getGlobalProperties().isFullyReplicated()) {
 				// fully replicated input is always locally forwarded if DOP is not changed
-				if(dopChange1) {
+				if (dopChange1) {
 					// can not continue with this child
 					childrenSkippedDueToReplicatedInput = true;
 					continue;
@@ -369,9 +381,9 @@ public abstract class TwoInputNode extends OptimizerNode {
 
 			for (PlanNode child2 : subPlans2) {
 
-				if(child2.getGlobalProperties().isFullyReplicated()) {
+				if (child2.getGlobalProperties().isFullyReplicated()) {
 					// fully replicated input is always locally forwarded if DOP is not changed
-					if(dopChange2) {
+					if (dopChange2) {
 						// can not continue with this child
 						childrenSkippedDueToReplicatedInput = true;
 						continue;
@@ -391,19 +403,23 @@ public abstract class TwoInputNode extends OptimizerNode {
 					final Channel c1 = new Channel(child1, this.input1.getMaterializationMode());
 					if (this.input1.getShipStrategy() == null) {
 						// free to choose the ship strategy
-						igps1.parameterizeChannel(c1, dopChange1);
+						igps1.parameterizeChannel(c1, dopChange1, input1Mode, input1breaksPipeline);
 						
 						// if the DOP changed, make sure that we cancel out properties, unless the
 						// ship strategy preserves/establishes them even under changing DOPs
 						if (dopChange1 && !c1.getShipStrategy().isNetworkStrategy()) {
 							c1.getGlobalProperties().reset();
 						}
-					} else {
+					}
+					else {
 						// ship strategy fixed by compiler hint
+						ShipStrategyType shipType = this.input1.getShipStrategy();
+						DataExchangeMode exMode = DataExchangeMode.select(input1Mode, shipType, input1breaksPipeline);
 						if (this.keys1 != null) {
-							c1.setShipStrategy(this.input1.getShipStrategy(), this.keys1.toFieldList());
-						} else {
-							c1.setShipStrategy(this.input1.getShipStrategy());
+							c1.setShipStrategy(shipType, this.keys1.toFieldList(), exMode);
+						}
+						else {
+							c1.setShipStrategy(shipType, exMode);
 						}
 						
 						if (dopChange1) {
@@ -416,7 +432,7 @@ public abstract class TwoInputNode extends OptimizerNode {
 						final Channel c2 = new Channel(child2, this.input2.getMaterializationMode());
 						if (this.input2.getShipStrategy() == null) {
 							// free to choose the ship strategy
-							igps2.parameterizeChannel(c2, dopChange2);
+							igps2.parameterizeChannel(c2, dopChange2, input2Mode, input2breaksPipeline);
 							
 							// if the DOP changed, make sure that we cancel out properties, unless the
 							// ship strategy preserves/establishes them even under changing DOPs
@@ -425,10 +441,12 @@ public abstract class TwoInputNode extends OptimizerNode {
 							}
 						} else {
 							// ship strategy fixed by compiler hint
+							ShipStrategyType shipType = this.input2.getShipStrategy();
+							DataExchangeMode exMode = DataExchangeMode.select(input2Mode, shipType, input2breaksPipeline);
 							if (this.keys2 != null) {
-								c2.setShipStrategy(this.input2.getShipStrategy(), this.keys2.toFieldList());
+								c2.setShipStrategy(shipType, this.keys2.toFieldList(), exMode);
 							} else {
-								c2.setShipStrategy(this.input2.getShipStrategy());
+								c2.setShipStrategy(shipType, exMode);
 							}
 							
 							if (dopChange2) {
@@ -437,7 +455,7 @@ public abstract class TwoInputNode extends OptimizerNode {
 						}
 						
 						/* ********************************************************************
-						 * NOTE: Depending on how we proceed with different partitionings,
+						 * NOTE: Depending on how we proceed with different partitioning,
 						 *       we might at some point need a compatibility check between
 						 *       the pairs of global properties.
 						 * *******************************************************************/
@@ -457,7 +475,8 @@ public abstract class TwoInputNode extends OptimizerNode {
 										
 										// we form a valid combination, so create the local candidates
 										// for this
-										addLocalCandidates(c1Clone, c2, broadcastPlanChannels, igps1, igps2, outputPlans, allLocalPairs, estimator);
+										addLocalCandidates(c1Clone, c2, broadcastPlanChannels, igps1, igps2,
+																			outputPlans, allLocalPairs, estimator);
 										break outer;
 									}
 								}
@@ -484,9 +503,11 @@ public abstract class TwoInputNode extends OptimizerNode {
 
 		if(outputPlans.isEmpty()) {
 			if(childrenSkippedDueToReplicatedInput) {
-				throw new CompilerException("No plan meeting the requirements could be created @ " + this + ". Most likely reason: Invalid use of replicated input.");
+				throw new CompilerException("No plan meeting the requirements could be created @ " + this
+											+ ". Most likely reason: Invalid use of replicated input.");
 			} else {
-				throw new CompilerException("No plan meeting the requirements could be created @ " + this + ". Most likely reason: Too restrictive plan hints.");
+				throw new CompilerException("No plan meeting the requirements could be created @ " + this
+											+ ". Most likely reason: Too restrictive plan hints.");
 			}
 		}
 
@@ -535,9 +556,8 @@ public abstract class TwoInputNode extends OptimizerNode {
 								// all right, co compatible
 								instantiate(dps, in1Copy, in2Copy, broadcastPlanChannels, target, estimator, rgps1, rgps2, ilp1, ilp2);
 								break;
-							} else {
-								// cannot use this pair, fall through the loop and try the next one
 							}
+							// else cannot use this pair, fall through the loop and try the next one
 						}
 					}
 				}
@@ -675,17 +695,6 @@ public abstract class TwoInputNode extends OptimizerNode {
 			}
 		}
 	}
-	
-	/**
-	 * Checks if the subPlan has a valid outputSize estimation.
-	 * 
-	 * @param subPlan The subPlan to check.
-	 * 
-	 * @return {@code True}, if all values are valid, {@code false} otherwise
-	 */
-	protected boolean haveValidOutputEstimates(OptimizerNode subPlan) {
-		return subPlan.getEstimatedOutputSize() != -1;
-	}
 
 	@Override
 	public void computeUnclosedBranchStack() {
@@ -701,7 +710,7 @@ public abstract class TwoInputNode extends OptimizerNode {
 		List<UnclosedBranchDescriptor> result2 = getSecondPredecessorNode().getBranchesForParent(getSecondIncomingConnection());
 
 		ArrayList<UnclosedBranchDescriptor> inputsMerged = new ArrayList<UnclosedBranchDescriptor>();
-		mergeLists(result1, result2, inputsMerged);
+		mergeLists(result1, result2, inputsMerged, true);
 		
 		// handle the data flow branching for the broadcast inputs
 		List<UnclosedBranchDescriptor> result = computeUnclosedBranchStackForBroadcastInputs(inputsMerged);
@@ -709,23 +718,9 @@ public abstract class TwoInputNode extends OptimizerNode {
 		this.openBranches = (result == null || result.isEmpty()) ? Collections.<UnclosedBranchDescriptor>emptyList() : result;
 	}
 
-	/**
-	 * Returns the key fields of the given input.
-	 * 
-	 * @param input The input for which key fields must be returned.
-	 * @return the key fields of the given input.
-	 */
-	public FieldList getInputKeySet(int input) {
-		switch(input) {
-			case 0: return keys1;
-			case 1: return keys2;
-			default: throw new IndexOutOfBoundsException();
-		}
-	}
-
 	@Override
 	public SemanticProperties getSemanticProperties() {
-		return ((DualInputOperator<?, ?, ?, ?>) getPactContract()).getSemanticProperties();
+		return getPactContract().getSemanticProperties();
 	}
 	
 	// --------------------------------------------------------------------------------------------
diff --git a/flink-compiler/src/main/java/org/apache/flink/compiler/dag/WorksetIterationNode.java b/flink-compiler/src/main/java/org/apache/flink/compiler/dag/WorksetIterationNode.java
index 0557633fa15..3ae4976a851 100644
--- a/flink-compiler/src/main/java/org/apache/flink/compiler/dag/WorksetIterationNode.java
+++ b/flink-compiler/src/main/java/org/apache/flink/compiler/dag/WorksetIterationNode.java
@@ -24,6 +24,7 @@ import java.util.Iterator;
 import java.util.List;
 import java.util.Set;
 
+import org.apache.flink.api.common.ExecutionMode;
 import org.apache.flink.api.common.operators.SemanticProperties;
 import org.apache.flink.api.common.operators.SemanticProperties.EmptySemanticProperties;
 import org.apache.flink.api.common.operators.base.DeltaIterationBase;
@@ -148,7 +149,9 @@ public class WorksetIterationNode extends TwoInputNode implements IterationNode
 		this.worksetNode = worksetNode;
 	}
 	
-	public void setNextPartialSolution(OptimizerNode solutionSetDelta, OptimizerNode nextWorkset) {
+	public void setNextPartialSolution(OptimizerNode solutionSetDelta, OptimizerNode nextWorkset,
+										ExecutionMode executionMode) {
+
 		// check whether the next partial solution is itself the join with
 		// the partial solution (so we can potentially do direct updates)
 		if (solutionSetDelta instanceof TwoInputNode) {
@@ -166,7 +169,7 @@ public class WorksetIterationNode extends TwoInputNode implements IterationNode
 			NoOpNode noop = new NoOpNode();
 			noop.setDegreeOfParallelism(getDegreeOfParallelism());
 
-			PactConnection noOpConn = new PactConnection(nextWorkset, noop);
+			PactConnection noOpConn = new PactConnection(nextWorkset, noop, executionMode);
 			noop.setIncomingConnection(noOpConn);
 			nextWorkset.addOutgoingConnection(noOpConn);
 			
@@ -178,7 +181,7 @@ public class WorksetIterationNode extends TwoInputNode implements IterationNode
 				new SolutionSetDeltaOperator(getSolutionSetKeyFields()));
 		solutionSetDeltaUpdateAux.setDegreeOfParallelism(getDegreeOfParallelism());
 
-		PactConnection conn = new PactConnection(solutionSetDelta, solutionSetDeltaUpdateAux);
+		PactConnection conn = new PactConnection(solutionSetDelta, solutionSetDeltaUpdateAux, executionMode);
 		solutionSetDeltaUpdateAux.setIncomingConnection(conn);
 		solutionSetDelta.addOutgoingConnection(conn);
 		
@@ -186,8 +189,10 @@ public class WorksetIterationNode extends TwoInputNode implements IterationNode
 		this.nextWorkset = nextWorkset;
 		
 		this.singleRoot = new SingleRootJoiner();
-		this.solutionSetDeltaRootConnection = new PactConnection(solutionSetDeltaUpdateAux, this.singleRoot);
-		this.nextWorksetRootConnection = new PactConnection(nextWorkset, this.singleRoot);
+		this.solutionSetDeltaRootConnection = new PactConnection(solutionSetDeltaUpdateAux,
+													this.singleRoot, executionMode);
+
+		this.nextWorksetRootConnection = new PactConnection(nextWorkset, this.singleRoot, executionMode);
 		this.singleRoot.setInputs(this.solutionSetDeltaRootConnection, this.nextWorksetRootConnection);
 		
 		solutionSetDeltaUpdateAux.addOutgoingConnection(this.solutionSetDeltaRootConnection);
@@ -350,30 +355,37 @@ public class WorksetIterationNode extends TwoInputNode implements IterationNode
 				GlobalProperties atEndGlobal = candidate.getGlobalProperties();
 				LocalProperties atEndLocal = candidate.getLocalProperties();
 				
-				FeedbackPropertiesMeetRequirementsReport report = candidate.checkPartialSolutionPropertiesMet(wspn, atEndGlobal, atEndLocal);
+				FeedbackPropertiesMeetRequirementsReport report = candidate.checkPartialSolutionPropertiesMet(wspn,
+																							atEndGlobal, atEndLocal);
+
 				if (report == FeedbackPropertiesMeetRequirementsReport.NO_PARTIAL_SOLUTION) {
 					; // depends only through broadcast variable on the workset solution
 				}
 				else if (report == FeedbackPropertiesMeetRequirementsReport.NOT_MET) {
 					// attach a no-op node through which we create the properties of the original input
 					Channel toNoOp = new Channel(candidate);
-					globPropsReqWorkset.parameterizeChannel(toNoOp, false);
+					globPropsReqWorkset.parameterizeChannel(toNoOp, false,
+															nextWorksetRootConnection.getDataExchangeMode(), false);
 					locPropsReqWorkset.parameterizeChannel(toNoOp);
 					
-					UnaryOperatorNode rebuildWorksetPropertiesNode = new UnaryOperatorNode("Rebuild Workset Properties", FieldList.EMPTY_LIST);
+					UnaryOperatorNode rebuildWorksetPropertiesNode = new UnaryOperatorNode("Rebuild Workset Properties",
+																							FieldList.EMPTY_LIST);
 					
 					rebuildWorksetPropertiesNode.setDegreeOfParallelism(candidate.getDegreeOfParallelism());
 					
-					SingleInputPlanNode rebuildWorksetPropertiesPlanNode = new SingleInputPlanNode(rebuildWorksetPropertiesNode, "Rebuild Workset Properties", toNoOp, DriverStrategy.UNARY_NO_OP);
-					rebuildWorksetPropertiesPlanNode.initProperties(toNoOp.getGlobalProperties(), toNoOp.getLocalProperties());
+					SingleInputPlanNode rebuildWorksetPropertiesPlanNode = new SingleInputPlanNode(
+												rebuildWorksetPropertiesNode, "Rebuild Workset Properties",
+												toNoOp, DriverStrategy.UNARY_NO_OP);
+					rebuildWorksetPropertiesPlanNode.initProperties(toNoOp.getGlobalProperties(),
+																	toNoOp.getLocalProperties());
 					estimator.costOperator(rebuildWorksetPropertiesPlanNode);
 						
 					GlobalProperties atEndGlobalModified = rebuildWorksetPropertiesPlanNode.getGlobalProperties();
 					LocalProperties atEndLocalModified = rebuildWorksetPropertiesPlanNode.getLocalProperties();
 						
 					if (!(atEndGlobalModified.equals(atEndGlobal) && atEndLocalModified.equals(atEndLocal))) {
-						FeedbackPropertiesMeetRequirementsReport report2 = candidate.checkPartialSolutionPropertiesMet(wspn, atEndGlobalModified, atEndLocalModified);
-						
+						FeedbackPropertiesMeetRequirementsReport report2 = candidate.checkPartialSolutionPropertiesMet(
+																		wspn, atEndGlobalModified, atEndLocalModified);
 						if (report2 != FeedbackPropertiesMeetRequirementsReport.NOT_MET) {
 							newCandidates.add(rebuildWorksetPropertiesPlanNode);
 						}
@@ -393,13 +405,12 @@ public class WorksetIterationNode extends TwoInputNode implements IterationNode
 		}
 		
 		// sanity check the solution set delta
-		for (Iterator<PlanNode> deltaPlans = solutionSetDeltaCandidates.iterator(); deltaPlans.hasNext(); ) {
-			SingleInputPlanNode candidate = (SingleInputPlanNode) deltaPlans.next();
+		for (PlanNode solutionSetDeltaCandidate : solutionSetDeltaCandidates) {
+			SingleInputPlanNode candidate = (SingleInputPlanNode) solutionSetDeltaCandidate;
 			GlobalProperties gp = candidate.getGlobalProperties();
-			
+
 			if (gp.getPartitioning() != PartitioningProperty.HASH_PARTITIONED || gp.getPartitioningFields() == null ||
-					!gp.getPartitioningFields().equals(this.solutionSetKeyFields))
-			{
+					!gp.getPartitioningFields().equals(this.solutionSetKeyFields)) {
 				throw new CompilerException("Bug: The solution set delta is not partitioned.");
 			}
 		}
@@ -422,10 +433,14 @@ public class WorksetIterationNode extends TwoInputNode implements IterationNode
 					boolean immediateDeltaUpdate;
 					
 					// check whether we need a dedicated solution set delta operator, or whether we can update on the fly
-					if (siSolutionDeltaCandidate.getInput().getShipStrategy() == ShipStrategyType.FORWARD && this.solutionDeltaImmediatelyAfterSolutionJoin) {
+					if (siSolutionDeltaCandidate.getInput().getShipStrategy() == ShipStrategyType.FORWARD &&
+							this.solutionDeltaImmediatelyAfterSolutionJoin)
+					{
 						// we do not need this extra node. we can make the predecessor the delta
 						// sanity check the node and connection
-						if (siSolutionDeltaCandidate.getDriverStrategy() != DriverStrategy.UNARY_NO_OP || siSolutionDeltaCandidate.getInput().getLocalStrategy() != LocalStrategy.NONE) {
+						if (siSolutionDeltaCandidate.getDriverStrategy() != DriverStrategy.UNARY_NO_OP ||
+								siSolutionDeltaCandidate.getInput().getLocalStrategy() != LocalStrategy.NONE)
+						{
 							throw new CompilerException("Invalid Solution set delta node.");
 						}
 						
@@ -438,8 +453,9 @@ public class WorksetIterationNode extends TwoInputNode implements IterationNode
 						immediateDeltaUpdate = false;
 					}
 					
-					WorksetIterationPlanNode wsNode = new WorksetIterationPlanNode(
-						this, "WorksetIteration ("+this.getPactContract().getName()+")", solutionSetIn, worksetIn, sspn, wspn, worksetCandidate, solutionSetCandidate);
+					WorksetIterationPlanNode wsNode = new WorksetIterationPlanNode(this,
+							"WorksetIteration ("+this.getPactContract().getName()+")", solutionSetIn,
+							worksetIn, sspn, wspn, worksetCandidate, solutionSetCandidate);
 					wsNode.setImmediateSolutionSetUpdate(immediateDeltaUpdate);
 					wsNode.initProperties(gp, lp);
 					target.add(wsNode);
@@ -453,7 +469,6 @@ public class WorksetIterationNode extends TwoInputNode implements IterationNode
 		if (this.openBranches != null) {
 			return;
 		}
-
 		
 		// IMPORTANT: First compute closed branches from the two inputs
 		// we need to do this because the runtime iteration head effectively joins
@@ -464,13 +479,13 @@ public class WorksetIterationNode extends TwoInputNode implements IterationNode
 		List<UnclosedBranchDescriptor> result2 = getSecondPredecessorNode().getBranchesForParent(getSecondIncomingConnection());
 
 		ArrayList<UnclosedBranchDescriptor> inputsMerged1 = new ArrayList<UnclosedBranchDescriptor>();
-		mergeLists(result1, result2, inputsMerged1); // this method also sets which branches are joined here (in the head)
+		mergeLists(result1, result2, inputsMerged1, true); // this method also sets which branches are joined here (in the head)
 		
 		addClosedBranches(getSingleRootOfStepFunction().closedBranchingNodes);
 
 		ArrayList<UnclosedBranchDescriptor> inputsMerged2 = new ArrayList<UnclosedBranchDescriptor>();
 		List<UnclosedBranchDescriptor> result3 = getSingleRootOfStepFunction().openBranches;
-		mergeLists(inputsMerged1, result3, inputsMerged2);
+		mergeLists(inputsMerged1, result3, inputsMerged2, true);
 
 		// handle the data flow branching for the broadcast inputs
 		List<UnclosedBranchDescriptor> result = computeUnclosedBranchStackForBroadcastInputs(inputsMerged2);
diff --git a/flink-compiler/src/main/java/org/apache/flink/compiler/dataproperties/GlobalProperties.java b/flink-compiler/src/main/java/org/apache/flink/compiler/dataproperties/GlobalProperties.java
index 31e13ae8fa6..76702771048 100644
--- a/flink-compiler/src/main/java/org/apache/flink/compiler/dataproperties/GlobalProperties.java
+++ b/flink-compiler/src/main/java/org/apache/flink/compiler/dataproperties/GlobalProperties.java
@@ -21,6 +21,7 @@ package org.apache.flink.compiler.dataproperties;
 import java.util.HashSet;
 import java.util.Set;
 
+import org.apache.flink.api.common.ExecutionMode;
 import org.apache.flink.api.common.functions.Partitioner;
 import org.apache.flink.api.common.operators.Order;
 import org.apache.flink.api.common.operators.Ordering;
@@ -30,16 +31,15 @@ import org.apache.flink.api.common.operators.util.FieldSet;
 import org.apache.flink.compiler.CompilerException;
 import org.apache.flink.compiler.plan.Channel;
 import org.apache.flink.compiler.util.Utils;
+import org.apache.flink.runtime.io.network.DataExchangeMode;
 import org.apache.flink.runtime.operators.shipping.ShipStrategyType;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 /**
  * This class represents global properties of the data at a certain point in the plan.
- * Global properties are properties that describe data across different partitions.
- * <p>
- * Currently, the properties are the following: A partitioning type (ANY, HASH, RANGE), and EITHER an ordering (for range partitioning)
- * or an FieldSet with the hash partitioning columns.
+ * Global properties are properties that describe data across different partitions, such as
+ * whether the data is hash partitioned, range partitioned, replicated, etc.
  */
 public class GlobalProperties implements Cloneable {
 
@@ -67,9 +67,9 @@ public class GlobalProperties implements Cloneable {
 	// --------------------------------------------------------------------------------------------
 	
 	/**
-	 * Sets the partitioning property for the global properties.
+	 * Sets this global properties to represent a hash partitioning.
 	 * 
-	 * @param partitionedFields 
+	 * @param partitionedFields The key fields on which the data is hash partitioned.
 	 */
 	public void setHashPartitioned(FieldList partitionedFields) {
 		if (partitionedFields == null) {
@@ -355,30 +355,64 @@ public class GlobalProperties implements Cloneable {
 	}
 
 
-	public void parameterizeChannel(Channel channel, boolean globalDopChange) {
+	public void parameterizeChannel(Channel channel, boolean globalDopChange,
+									ExecutionMode exchangeMode, boolean breakPipeline) {
+
+		ShipStrategyType shipType;
+		FieldList partitionKeys;
+		boolean[] sortDirection;
+		Partitioner<?> partitioner;
+
 		switch (this.partitioning) {
 			case RANDOM_PARTITIONED:
-				channel.setShipStrategy(globalDopChange ? ShipStrategyType.PARTITION_RANDOM : ShipStrategyType.FORWARD);
+				shipType = globalDopChange ? ShipStrategyType.PARTITION_RANDOM : ShipStrategyType.FORWARD;
+				partitionKeys = null;
+				sortDirection = null;
+				partitioner = null;
 				break;
+
 			case FULL_REPLICATION:
-				channel.setShipStrategy(ShipStrategyType.BROADCAST);
+				shipType = ShipStrategyType.BROADCAST;
+				partitionKeys = null;
+				sortDirection = null;
+				partitioner = null;
 				break;
+
 			case ANY_PARTITIONING:
 			case HASH_PARTITIONED:
-				channel.setShipStrategy(ShipStrategyType.PARTITION_HASH, Utils.createOrderedFromSet(this.partitioningFields));
+				shipType = ShipStrategyType.PARTITION_HASH;
+				partitionKeys = Utils.createOrderedFromSet(this.partitioningFields);
+				sortDirection = null;
+				partitioner = null;
 				break;
+
 			case RANGE_PARTITIONED:
-				channel.setShipStrategy(ShipStrategyType.PARTITION_RANGE, this.ordering.getInvolvedIndexes(), this.ordering.getFieldSortDirections());
+				shipType = ShipStrategyType.PARTITION_RANGE;
+				partitionKeys = this.ordering.getInvolvedIndexes();
+				sortDirection = this.ordering.getFieldSortDirections();
+				partitioner = null;
 				break;
+
 			case FORCED_REBALANCED:
-				channel.setShipStrategy(ShipStrategyType.PARTITION_RANDOM);
+				shipType = ShipStrategyType.PARTITION_RANDOM;
+				partitionKeys = null;
+				sortDirection = null;
+				partitioner = null;
 				break;
+
 			case CUSTOM_PARTITIONING:
-				channel.setShipStrategy(ShipStrategyType.PARTITION_CUSTOM, this.partitioningFields, this.customPartitioner);
+				shipType = ShipStrategyType.PARTITION_CUSTOM;
+				partitionKeys = this.partitioningFields;
+				sortDirection = null;
+				partitioner = this.customPartitioner;
 				break;
+
 			default:
 				throw new CompilerException("Unsupported partitioning strategy");
 		}
+
+		DataExchangeMode exMode = DataExchangeMode.select(exchangeMode, shipType, breakPipeline);
+		channel.setShipStrategy(shipType, partitionKeys, sortDirection, partitioner, exMode);
 	}
 
 	// ------------------------------------------------------------------------
@@ -438,7 +472,7 @@ public class GlobalProperties implements Cloneable {
 	
 	// --------------------------------------------------------------------------------------------
 	
-	public static final GlobalProperties combine(GlobalProperties gp1, GlobalProperties gp2) {
+	public static GlobalProperties combine(GlobalProperties gp1, GlobalProperties gp2) {
 		if (gp1.isFullyReplicated()) {
 			if (gp2.isFullyReplicated()) {
 				return new GlobalProperties();
@@ -448,7 +482,7 @@ public class GlobalProperties implements Cloneable {
 		} else if (gp2.isFullyReplicated()) {
 			return gp1;
 		} else if (gp1.ordering != null) {
-			return gp1; 
+			return gp1;
 		} else if (gp2.ordering != null) {
 			return gp2;
 		} else if (gp1.partitioningFields != null) {
diff --git a/flink-compiler/src/main/java/org/apache/flink/compiler/dataproperties/RequestedGlobalProperties.java b/flink-compiler/src/main/java/org/apache/flink/compiler/dataproperties/RequestedGlobalProperties.java
index f4334fff3ed..10c1248dbe5 100644
--- a/flink-compiler/src/main/java/org/apache/flink/compiler/dataproperties/RequestedGlobalProperties.java
+++ b/flink-compiler/src/main/java/org/apache/flink/compiler/dataproperties/RequestedGlobalProperties.java
@@ -18,6 +18,7 @@
 
 package org.apache.flink.compiler.dataproperties;
 
+import org.apache.flink.api.common.ExecutionMode;
 import org.apache.flink.api.common.distributions.DataDistribution;
 import org.apache.flink.api.common.functions.Partitioner;
 import org.apache.flink.api.common.operators.Ordering;
@@ -27,14 +28,20 @@ import org.apache.flink.api.common.operators.util.FieldSet;
 import org.apache.flink.compiler.CompilerException;
 import org.apache.flink.compiler.plan.Channel;
 import org.apache.flink.compiler.util.Utils;
+import org.apache.flink.runtime.io.network.DataExchangeMode;
 import org.apache.flink.runtime.operators.shipping.ShipStrategyType;
 
 /**
- * This class represents global properties of the data that an operator is interested in, because it needs those
- * properties for its contract.
- * <p>
- * Currently, the properties are the following: A partitioning type (ANY, HASH, RANGE), and EITHER an ordering (for range partitioning)
- * or an FieldSet with the hash partitioning columns.
+ * This class represents the global properties of the data that are requested by an operator.
+ * Operators request the global properties they need for correct execution. This list is an example of global
+ * properties requested by certain operators:
+ * <ul>
+ *     <li>"groupBy/reduce" will request the data to be partitioned in some way after the key fields.</li>
+ *     <li>"map" will request the data to be in an arbitrary distribution - it has no prerequisites</li>
+ *     <li>"join" will request certain properties for each input. This class represents the properties
+ *         on an input alone. The properties may be partitioning on the key fields, or a combination of
+ *         replication on one input and anything-but-replication on the other input.</li>
+ * </ul>
  */
 public final class RequestedGlobalProperties implements Cloneable {
 	
@@ -60,11 +67,13 @@ public final class RequestedGlobalProperties implements Cloneable {
 	// --------------------------------------------------------------------------------------------
 	
 	/**
-	 * Sets the partitioning property for the global properties.
-	 * If the partitionFields are provided as {@link FieldSet} also subsets are valid,
-	 * if provided as {@link FieldList} partitioning fields must exactly match incl. order.
+	 * Sets these properties to request a hash partitioning on the given fields.
 	 *
-	 * @param partitionedFields
+	 * If the fields are provided as {@link FieldSet}, then any permutation of the fields is a
+	 * valid partitioning, including subsets. If the fields are given as a {@link FieldList},
+	 * then only an exact partitioning on the fields matches this requested partitioning.
+	 *
+	 * @param partitionedFields The key fields for the partitioning.
 	 */
 	public void setHashPartitioned(FieldSet partitionedFields) {
 		if (partitionedFields == null) {
@@ -91,11 +100,14 @@ public final class RequestedGlobalProperties implements Cloneable {
 	}
 
 	/**
-	 * Sets the partitioning property for the global properties.
-	 * If the partitionFields are provided as {@link FieldSet} also subsets are valid,
-	 * if provided as {@link FieldList} partitioning fields must exactly match incl. order.
+	 * Sets these properties to request some partitioning on the given fields. This will allow
+	 * both hash partitioning and range partitioning to match.
+	 *
+	 * If the fields are provided as {@link FieldSet}, then any permutation of the fields is a
+	 * valid partitioning, including subsets. If the fields are given as a {@link FieldList},
+	 * then only an exact partitioning on the fields matches this requested partitioning.
 	 *
-	 * @param partitionedFields
+	 * @param partitionedFields The key fields for the partitioning.
 	 */
 	public void setAnyPartitioning(FieldSet partitionedFields) {
 		if (partitionedFields == null) {
@@ -131,11 +143,13 @@ public final class RequestedGlobalProperties implements Cloneable {
 	}
 
 	/**
-	 * Sets the partitioning property for the global properties.
-	 * If the partitionFields are provided as {@link FieldSet} also subsets are valid,
-	 * if provided as {@link FieldList} partitioning fields must exactly match incl. order.
+	 * Sets these properties to request a custom partitioning with the given {@link Partitioner} instance.
 	 *
-	 * @param partitionedFields
+	 * If the fields are provided as {@link FieldSet}, then any permutation of the fields is a
+	 * valid partitioning, including subsets. If the fields are given as a {@link FieldList},
+	 * then only an exact partitioning on the fields matches this requested partitioning.
+	 *
+	 * @param partitionedFields The key fields for the partitioning.
 	 */
 	public void setCustomPartitioned(FieldSet partitionedFields, Partitioner<?> partitioner) {
 		if (partitionedFields == null || partitioner == null) {
@@ -322,63 +336,102 @@ public final class RequestedGlobalProperties implements Cloneable {
 	}
 
 	/**
-	 * Parameterizes the ship strategy fields of a channel such that the channel produces the desired global properties.
+	 * Parametrizes the ship strategy fields of a channel such that the channel produces
+	 * the desired global properties.
 	 * 
-	 * @param channel The channel to parameterize.
-	 * @param globalDopChange
+	 * @param channel The channel to parametrize.
+	 * @param globalDopChange Flag indicating whether the degree of parallelism changes
+	 *                        between sender and receiver.
+	 * @param exchangeMode The mode of data exchange (pipelined, always batch,
+	 *                     batch only on shuffle, ...)
+	 * @param breakPipeline Indicates whether this data exchange should break
+	 *                      pipelines (unless pipelines are forced).
 	 */
-	public void parameterizeChannel(Channel channel, boolean globalDopChange) {
+	public void parameterizeChannel(Channel channel, boolean globalDopChange,
+									ExecutionMode exchangeMode, boolean breakPipeline) {
 
 		// safety check. Fully replicated input must be preserved.
-		if(channel.getSource().getGlobalProperties().isFullyReplicated() &&
+		if (channel.getSource().getGlobalProperties().isFullyReplicated() &&
 				!(this.partitioning == PartitioningProperty.FULL_REPLICATION ||
-					this.partitioning == PartitioningProperty.ANY_DISTRIBUTION)) {
-			throw new CompilerException("Fully replicated input must be preserved and may not be converted into another global property.");
+					this.partitioning == PartitioningProperty.ANY_DISTRIBUTION))
+		{
+			throw new CompilerException("Fully replicated input must be preserved " +
+					"and may not be converted into another global property.");
 		}
 
 		// if we request nothing, then we need no special strategy. forward, if the number of instances remains
 		// the same, randomly repartition otherwise
 		if (isTrivial() || this.partitioning == PartitioningProperty.ANY_DISTRIBUTION) {
-			channel.setShipStrategy(globalDopChange ? ShipStrategyType.PARTITION_RANDOM : ShipStrategyType.FORWARD);
+			ShipStrategyType shipStrategy = globalDopChange ? ShipStrategyType.PARTITION_RANDOM :
+																ShipStrategyType.FORWARD;
+
+			DataExchangeMode em = DataExchangeMode.select(exchangeMode, shipStrategy, breakPipeline);
+			channel.setShipStrategy(shipStrategy, em);
 			return;
 		}
 		
 		final GlobalProperties inGlobals = channel.getSource().getGlobalProperties();
 		// if we have no global parallelism change, check if we have already compatible global properties
 		if (!globalDopChange && isMetBy(inGlobals)) {
-			channel.setShipStrategy(ShipStrategyType.FORWARD);
+			DataExchangeMode em = DataExchangeMode.select(exchangeMode, ShipStrategyType.FORWARD, breakPipeline);
+			channel.setShipStrategy(ShipStrategyType.FORWARD, em);
 			return;
 		}
 		
 		// if we fall through the conditions until here, we need to re-establish
+		ShipStrategyType shipType;
+		FieldList partitionKeys;
+		boolean[] sortDirection;
+		Partitioner<?> partitioner;
+
 		switch (this.partitioning) {
 			case FULL_REPLICATION:
-				channel.setShipStrategy(ShipStrategyType.BROADCAST);
+				shipType = ShipStrategyType.BROADCAST;
+				partitionKeys = null;
+				sortDirection = null;
+				partitioner = null;
 				break;
-			
+
 			case ANY_PARTITIONING:
 			case HASH_PARTITIONED:
-				channel.setShipStrategy(ShipStrategyType.PARTITION_HASH, Utils.createOrderedFromSet(this.partitioningFields));
+				shipType = ShipStrategyType.PARTITION_HASH;
+				partitionKeys = Utils.createOrderedFromSet(this.partitioningFields);
+				sortDirection = null;
+				partitioner = null;
 				break;
 			
 			case RANGE_PARTITIONED:
-				channel.setShipStrategy(ShipStrategyType.PARTITION_RANGE, this.ordering.getInvolvedIndexes(), this.ordering.getFieldSortDirections());
-				if(this.dataDistribution != null) {
+				shipType = ShipStrategyType.PARTITION_RANGE;
+				partitionKeys = this.ordering.getInvolvedIndexes();
+				sortDirection = this.ordering.getFieldSortDirections();
+				partitioner = null;
+
+				if (this.dataDistribution != null) {
 					channel.setDataDistribution(this.dataDistribution);
 				}
 				break;
-			
+
 			case FORCED_REBALANCED:
-				channel.setShipStrategy(ShipStrategyType.PARTITION_FORCED_REBALANCE);
+				shipType = ShipStrategyType.PARTITION_FORCED_REBALANCE;
+				partitionKeys = null;
+				sortDirection = null;
+				partitioner = null;
 				break;
-				
+
 			case CUSTOM_PARTITIONING:
-				channel.setShipStrategy(ShipStrategyType.PARTITION_CUSTOM, Utils.createOrderedFromSet(this.partitioningFields), this.customPartitioner);
+				shipType = ShipStrategyType.PARTITION_CUSTOM;
+				partitionKeys = Utils.createOrderedFromSet(this.partitioningFields);
+				sortDirection = null;
+				partitioner = this.customPartitioner;
 				break;
-				
+
 			default:
-				throw new CompilerException("Invalid partitioning to create through a data exchange: " + this.partitioning.name());
+				throw new CompilerException("Invalid partitioning to create through a data exchange: "
+											+ this.partitioning.name());
 		}
+
+		DataExchangeMode exMode = DataExchangeMode.select(exchangeMode, shipType, breakPipeline);
+		channel.setShipStrategy(shipType, partitionKeys, sortDirection, partitioner, exMode);
 	}
 
 	// ------------------------------------------------------------------------
diff --git a/flink-compiler/src/main/java/org/apache/flink/compiler/operators/AllGroupWithPartialPreGroupProperties.java b/flink-compiler/src/main/java/org/apache/flink/compiler/operators/AllGroupWithPartialPreGroupProperties.java
index 54885a72a2b..ec38b4741da 100644
--- a/flink-compiler/src/main/java/org/apache/flink/compiler/operators/AllGroupWithPartialPreGroupProperties.java
+++ b/flink-compiler/src/main/java/org/apache/flink/compiler/operators/AllGroupWithPartialPreGroupProperties.java
@@ -31,6 +31,7 @@ import org.apache.flink.compiler.dataproperties.RequestedGlobalProperties;
 import org.apache.flink.compiler.dataproperties.RequestedLocalProperties;
 import org.apache.flink.compiler.plan.Channel;
 import org.apache.flink.compiler.plan.SingleInputPlanNode;
+import org.apache.flink.runtime.io.network.DataExchangeMode;
 import org.apache.flink.runtime.operators.DriverStrategy;
 import org.apache.flink.runtime.operators.shipping.ShipStrategyType;
 
@@ -45,24 +46,29 @@ public final class AllGroupWithPartialPreGroupProperties extends OperatorDescrip
 	public SingleInputPlanNode instantiate(Channel in, SingleInputNode node) {
 		if (in.getShipStrategy() == ShipStrategyType.FORWARD) {
 			// locally connected, directly instantiate
-			return new SingleInputPlanNode(node, "GroupReduce ("+node.getPactContract().getName()+")", in, DriverStrategy.ALL_GROUP_REDUCE);
+			return new SingleInputPlanNode(node, "GroupReduce ("+node.getPactContract().getName()+")",
+											in, DriverStrategy.ALL_GROUP_REDUCE);
 		} else {
 			// non forward case.plug in a combiner
 			Channel toCombiner = new Channel(in.getSource());
-			toCombiner.setShipStrategy(ShipStrategyType.FORWARD);
+			toCombiner.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 			
 			// create an input node for combine with same DOP as input node
 			GroupReduceNode combinerNode = ((GroupReduceNode) node).getCombinerUtilityNode();
 			combinerNode.setDegreeOfParallelism(in.getSource().getDegreeOfParallelism());
 
-			SingleInputPlanNode combiner = new SingleInputPlanNode(combinerNode, "Combine ("+node.getPactContract().getName()+")", toCombiner, DriverStrategy.ALL_GROUP_COMBINE);
+			SingleInputPlanNode combiner = new SingleInputPlanNode(combinerNode,
+					"Combine ("+node.getPactContract().getName()+")", toCombiner, DriverStrategy.ALL_GROUP_COMBINE);
 			combiner.setCosts(new Costs(0, 0));
 			combiner.initProperties(toCombiner.getGlobalProperties(), toCombiner.getLocalProperties());
 			
 			Channel toReducer = new Channel(combiner);
-			toReducer.setShipStrategy(in.getShipStrategy(), in.getShipStrategyKeys(), in.getShipStrategySortOrder());
+			toReducer.setShipStrategy(in.getShipStrategy(), in.getShipStrategyKeys(),
+										in.getShipStrategySortOrder(), in.getDataExchangeMode());
+
 			toReducer.setLocalStrategy(in.getLocalStrategy(), in.getLocalStrategyKeys(), in.getLocalStrategySortOrder());
-			return new SingleInputPlanNode(node, "GroupReduce ("+node.getPactContract().getName()+")", toReducer, DriverStrategy.ALL_GROUP_REDUCE);
+			return new SingleInputPlanNode(node, "GroupReduce ("+node.getPactContract().getName()+")",
+											toReducer, DriverStrategy.ALL_GROUP_REDUCE);
 		}
 	}
 
diff --git a/flink-compiler/src/main/java/org/apache/flink/compiler/operators/AllReduceProperties.java b/flink-compiler/src/main/java/org/apache/flink/compiler/operators/AllReduceProperties.java
index 2bf757e01f1..17fa318cf87 100644
--- a/flink-compiler/src/main/java/org/apache/flink/compiler/operators/AllReduceProperties.java
+++ b/flink-compiler/src/main/java/org/apache/flink/compiler/operators/AllReduceProperties.java
@@ -30,11 +30,11 @@ import org.apache.flink.compiler.dataproperties.RequestedGlobalProperties;
 import org.apache.flink.compiler.dataproperties.RequestedLocalProperties;
 import org.apache.flink.compiler.plan.Channel;
 import org.apache.flink.compiler.plan.SingleInputPlanNode;
+import org.apache.flink.runtime.io.network.DataExchangeMode;
 import org.apache.flink.runtime.operators.DriverStrategy;
 import org.apache.flink.runtime.operators.shipping.ShipStrategyType;
 
-public final class AllReduceProperties extends OperatorDescriptorSingle
-{
+public final class AllReduceProperties extends OperatorDescriptorSingle {
 
 	@Override
 	public DriverStrategy getStrategy() {
@@ -45,24 +45,30 @@ public final class AllReduceProperties extends OperatorDescriptorSingle
 	public SingleInputPlanNode instantiate(Channel in, SingleInputNode node) {
 		if (in.getShipStrategy() == ShipStrategyType.FORWARD) {
 			// locally connected, directly instantiate
-			return new SingleInputPlanNode(node, "Reduce ("+node.getPactContract().getName()+")", in, DriverStrategy.ALL_REDUCE);
+			return new SingleInputPlanNode(node, "Reduce ("+node.getPactContract().getName()+")",
+											in, DriverStrategy.ALL_REDUCE);
 		} else {
 			// non forward case.plug in a combiner
 			Channel toCombiner = new Channel(in.getSource());
-			toCombiner.setShipStrategy(ShipStrategyType.FORWARD);
+			toCombiner.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 			
 			// create an input node for combine with same DOP as input node
 			ReduceNode combinerNode = ((ReduceNode) node).getCombinerUtilityNode();
 			combinerNode.setDegreeOfParallelism(in.getSource().getDegreeOfParallelism());
 
-			SingleInputPlanNode combiner = new SingleInputPlanNode(combinerNode, "Combine ("+node.getPactContract().getName()+")", toCombiner, DriverStrategy.ALL_REDUCE);
+			SingleInputPlanNode combiner = new SingleInputPlanNode(combinerNode,
+					"Combine ("+node.getPactContract().getName()+")", toCombiner, DriverStrategy.ALL_REDUCE);
 			combiner.setCosts(new Costs(0, 0));
 			combiner.initProperties(toCombiner.getGlobalProperties(), toCombiner.getLocalProperties());
 			
 			Channel toReducer = new Channel(combiner);
-			toReducer.setShipStrategy(in.getShipStrategy(), in.getShipStrategyKeys(), in.getShipStrategySortOrder());
-			toReducer.setLocalStrategy(in.getLocalStrategy(), in.getLocalStrategyKeys(), in.getLocalStrategySortOrder());
-			return new SingleInputPlanNode(node, "Reduce("+node.getPactContract().getName()+")", toReducer, DriverStrategy.ALL_REDUCE);
+			toReducer.setShipStrategy(in.getShipStrategy(), in.getShipStrategyKeys(),
+										in.getShipStrategySortOrder(), in.getDataExchangeMode());
+			toReducer.setLocalStrategy(in.getLocalStrategy(), in.getLocalStrategyKeys(),
+										in.getLocalStrategySortOrder());
+
+			return new SingleInputPlanNode(node, "Reduce ("+node.getPactContract().getName()+")",
+											toReducer, DriverStrategy.ALL_REDUCE);
 		}
 	}
 	
diff --git a/flink-compiler/src/main/java/org/apache/flink/compiler/operators/GroupReduceWithCombineProperties.java b/flink-compiler/src/main/java/org/apache/flink/compiler/operators/GroupReduceWithCombineProperties.java
index fd263e6e347..7180845382b 100644
--- a/flink-compiler/src/main/java/org/apache/flink/compiler/operators/GroupReduceWithCombineProperties.java
+++ b/flink-compiler/src/main/java/org/apache/flink/compiler/operators/GroupReduceWithCombineProperties.java
@@ -35,6 +35,7 @@ import org.apache.flink.compiler.dataproperties.RequestedGlobalProperties;
 import org.apache.flink.compiler.dataproperties.RequestedLocalProperties;
 import org.apache.flink.compiler.plan.Channel;
 import org.apache.flink.compiler.plan.SingleInputPlanNode;
+import org.apache.flink.runtime.io.network.DataExchangeMode;
 import org.apache.flink.runtime.operators.DriverStrategy;
 import org.apache.flink.runtime.operators.shipping.ShipStrategyType;
 import org.apache.flink.runtime.operators.util.LocalStrategy;
@@ -94,13 +95,16 @@ public final class GroupReduceWithCombineProperties extends OperatorDescriptorSi
 				if (!in.getLocalStrategyKeys().isValidUnorderedPrefix(this.keys)) {
 					throw new RuntimeException("Bug: Inconsistent sort for group strategy.");
 				}
-				in.setLocalStrategy(LocalStrategy.COMBININGSORT, in.getLocalStrategyKeys(), in.getLocalStrategySortOrder());
+				in.setLocalStrategy(LocalStrategy.COMBININGSORT, in.getLocalStrategyKeys(),
+									in.getLocalStrategySortOrder());
 			}
-			return new SingleInputPlanNode(node, "Reduce("+node.getPactContract().getName()+")", in, DriverStrategy.SORTED_GROUP_REDUCE, this.keyList);
+			return new SingleInputPlanNode(node, "Reduce("+node.getPactContract().getName()+")", in,
+											DriverStrategy.SORTED_GROUP_REDUCE, this.keyList);
 		} else {
 			// non forward case. all local properties are killed anyways, so we can safely plug in a combiner
 			Channel toCombiner = new Channel(in.getSource());
-			toCombiner.setShipStrategy(ShipStrategyType.FORWARD);
+			toCombiner.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
+
 			// create an input node for combine with same DOP as input node
 			GroupReduceNode combinerNode = ((GroupReduceNode) node).getCombinerUtilityNode();
 			combinerNode.setDegreeOfParallelism(in.getSource().getDegreeOfParallelism());
@@ -115,9 +119,13 @@ public final class GroupReduceWithCombineProperties extends OperatorDescriptorSi
 			combiner.setDriverKeyInfo(this.keyList, 1);
 			
 			Channel toReducer = new Channel(combiner);
-			toReducer.setShipStrategy(in.getShipStrategy(), in.getShipStrategyKeys(), in.getShipStrategySortOrder());
-			toReducer.setLocalStrategy(LocalStrategy.COMBININGSORT, in.getLocalStrategyKeys(), in.getLocalStrategySortOrder());
-			return new SingleInputPlanNode(node, "Reduce ("+node.getPactContract().getName()+")", toReducer, DriverStrategy.SORTED_GROUP_REDUCE, this.keyList);
+			toReducer.setShipStrategy(in.getShipStrategy(), in.getShipStrategyKeys(),
+									in.getShipStrategySortOrder(), in.getDataExchangeMode());
+			toReducer.setLocalStrategy(LocalStrategy.COMBININGSORT, in.getLocalStrategyKeys(),
+										in.getLocalStrategySortOrder());
+
+			return new SingleInputPlanNode(node, "Reduce ("+node.getPactContract().getName()+")",
+											toReducer, DriverStrategy.SORTED_GROUP_REDUCE, this.keyList);
 		}
 	}
 
diff --git a/flink-compiler/src/main/java/org/apache/flink/compiler/operators/ReduceProperties.java b/flink-compiler/src/main/java/org/apache/flink/compiler/operators/ReduceProperties.java
index 000079d69ae..3a054ff9bec 100644
--- a/flink-compiler/src/main/java/org/apache/flink/compiler/operators/ReduceProperties.java
+++ b/flink-compiler/src/main/java/org/apache/flink/compiler/operators/ReduceProperties.java
@@ -33,6 +33,7 @@ import org.apache.flink.compiler.dataproperties.RequestedGlobalProperties;
 import org.apache.flink.compiler.dataproperties.RequestedLocalProperties;
 import org.apache.flink.compiler.plan.Channel;
 import org.apache.flink.compiler.plan.SingleInputPlanNode;
+import org.apache.flink.runtime.io.network.DataExchangeMode;
 import org.apache.flink.runtime.operators.DriverStrategy;
 import org.apache.flink.runtime.operators.shipping.ShipStrategyType;
 import org.apache.flink.runtime.operators.util.LocalStrategy;
@@ -60,25 +61,32 @@ public final class ReduceProperties extends OperatorDescriptorSingle {
 		if (in.getShipStrategy() == ShipStrategyType.FORWARD ||
 				(node.getBroadcastConnections() != null && !node.getBroadcastConnections().isEmpty()))
 		{
-			return new SingleInputPlanNode(node, "Reduce ("+node.getPactContract().getName()+")", in, DriverStrategy.SORTED_REDUCE, this.keyList);
+			return new SingleInputPlanNode(node, "Reduce ("+node.getPactContract().getName()+")", in,
+											DriverStrategy.SORTED_REDUCE, this.keyList);
 		}
 		else {
 			// non forward case. all local properties are killed anyways, so we can safely plug in a combiner
 			Channel toCombiner = new Channel(in.getSource());
-			toCombiner.setShipStrategy(ShipStrategyType.FORWARD);
+			toCombiner.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 			
 			// create an input node for combine with same DOP as input node
 			ReduceNode combinerNode = ((ReduceNode) node).getCombinerUtilityNode();
 			combinerNode.setDegreeOfParallelism(in.getSource().getDegreeOfParallelism());
 
-			SingleInputPlanNode combiner = new SingleInputPlanNode(combinerNode, "Combine ("+node.getPactContract().getName()+")", toCombiner, DriverStrategy.SORTED_PARTIAL_REDUCE, this.keyList);
+			SingleInputPlanNode combiner = new SingleInputPlanNode(combinerNode,
+								"Combine ("+node.getPactContract().getName()+")", toCombiner,
+								DriverStrategy.SORTED_PARTIAL_REDUCE, this.keyList);
+
 			combiner.setCosts(new Costs(0, 0));
 			combiner.initProperties(toCombiner.getGlobalProperties(), toCombiner.getLocalProperties());
 			
 			Channel toReducer = new Channel(combiner);
-			toReducer.setShipStrategy(in.getShipStrategy(), in.getShipStrategyKeys(), in.getShipStrategySortOrder());
+			toReducer.setShipStrategy(in.getShipStrategy(), in.getShipStrategyKeys(),
+										in.getShipStrategySortOrder(), in.getDataExchangeMode());
 			toReducer.setLocalStrategy(LocalStrategy.SORT, in.getLocalStrategyKeys(), in.getLocalStrategySortOrder());
-			return new SingleInputPlanNode(node, "Reduce("+node.getPactContract().getName()+")", toReducer, DriverStrategy.SORTED_REDUCE, this.keyList);
+
+			return new SingleInputPlanNode(node, "Reduce("+node.getPactContract().getName()+")", toReducer,
+											DriverStrategy.SORTED_REDUCE, this.keyList);
 		}
 	}
 
diff --git a/flink-compiler/src/main/java/org/apache/flink/compiler/plan/Channel.java b/flink-compiler/src/main/java/org/apache/flink/compiler/plan/Channel.java
index e159481e7c2..3903c84f675 100644
--- a/flink-compiler/src/main/java/org/apache/flink/compiler/plan/Channel.java
+++ b/flink-compiler/src/main/java/org/apache/flink/compiler/plan/Channel.java
@@ -32,6 +32,7 @@ import org.apache.flink.compiler.dataproperties.RequestedGlobalProperties;
 import org.apache.flink.compiler.dataproperties.RequestedLocalProperties;
 import org.apache.flink.compiler.plandump.DumpableConnection;
 import org.apache.flink.compiler.util.Utils;
+import org.apache.flink.runtime.io.network.DataExchangeMode;
 import org.apache.flink.runtime.operators.shipping.ShipStrategyType;
 import org.apache.flink.runtime.operators.util.LocalStrategy;
 
@@ -43,8 +44,10 @@ public class Channel implements EstimateProvider, Cloneable, DumpableConnection<
 	private PlanNode source;
 	
 	private PlanNode target;
-	
+
 	private ShipStrategyType shipStrategy = ShipStrategyType.NONE;
+
+	private DataExchangeMode dataExchangeMode;
 	
 	private LocalStrategy localStrategy = LocalStrategy.NONE;
 	
@@ -78,8 +81,6 @@ public class Channel implements EstimateProvider, Cloneable, DumpableConnection<
 	
 	private double relativeTempMemory;
 	
-	private double relativeMemoryGlobalStrategy;
-	
 	private double relativeMemoryLocalStrategy;
 	
 	private int replicationFactor = 1;
@@ -125,33 +126,46 @@ public class Channel implements EstimateProvider, Cloneable, DumpableConnection<
 	public PlanNode getTarget() {
 		return this.target;
 	}
-	
-	public void setShipStrategy(ShipStrategyType strategy) {
-		setShipStrategy(strategy, null, null, null);
+
+	public void setShipStrategy(ShipStrategyType strategy, DataExchangeMode dataExchangeMode) {
+		setShipStrategy(strategy, null, null, null, dataExchangeMode);
 	}
 	
-	public void setShipStrategy(ShipStrategyType strategy, FieldList keys) {
-		setShipStrategy(strategy, keys, null, null);
+	public void setShipStrategy(ShipStrategyType strategy, FieldList keys, DataExchangeMode dataExchangeMode) {
+		setShipStrategy(strategy, keys, null, null, dataExchangeMode);
 	}
 	
-	public void setShipStrategy(ShipStrategyType strategy, FieldList keys, boolean[] sortDirection) {
-		setShipStrategy(strategy, keys, sortDirection, null);
+	public void setShipStrategy(ShipStrategyType strategy, FieldList keys,
+								boolean[] sortDirection, DataExchangeMode dataExchangeMode) {
+		setShipStrategy(strategy, keys, sortDirection, null, dataExchangeMode);
 	}
 	
-	public void setShipStrategy(ShipStrategyType strategy, FieldList keys, Partitioner<?> partitioner) {
-		setShipStrategy(strategy, keys, null, partitioner);
+	public void setShipStrategy(ShipStrategyType strategy, FieldList keys,
+								Partitioner<?> partitioner, DataExchangeMode dataExchangeMode) {
+		setShipStrategy(strategy, keys, null, partitioner, dataExchangeMode);
 	}
 	
-	public void setShipStrategy(ShipStrategyType strategy, FieldList keys, boolean[] sortDirection, Partitioner<?> partitioner) {
+	public void setShipStrategy(ShipStrategyType strategy, FieldList keys,
+								boolean[] sortDirection, Partitioner<?> partitioner,
+								DataExchangeMode dataExchangeMode) {
 		this.shipStrategy = strategy;
 		this.shipKeys = keys;
 		this.shipSortOrder = sortDirection;
 		this.partitioner = partitioner;
-		
+		this.dataExchangeMode = dataExchangeMode;
 		this.globalProps = null;		// reset the global properties
 	}
-	
-	
+
+	/**
+	 * Gets the data exchange mode (batch / streaming) to use for the data
+	 * exchange of this channel.
+	 *
+	 * @return The data exchange mode of this channel.
+	 */
+	public DataExchangeMode getDataExchangeMode() {
+		return dataExchangeMode;
+	}
+
 	public ShipStrategyType getShipStrategy() {
 		return this.shipStrategy;
 	}
@@ -168,10 +182,6 @@ public class Channel implements EstimateProvider, Cloneable, DumpableConnection<
 		setLocalStrategy(strategy, null, null);
 	}
 	
-	public void setLocalStrategy(LocalStrategy strategy, FieldList keys) {
-		setLocalStrategy(strategy, keys, null);
-	}
-	
 	public void setLocalStrategy(LocalStrategy strategy, FieldList keys, boolean[] sortDirection) {
 		this.localStrategy = strategy;
 		this.localKeys = keys;
@@ -307,14 +317,6 @@ public class Channel implements EstimateProvider, Cloneable, DumpableConnection<
 		this.localStrategyComparator = localStrategyComparator;
 	}
 	
-	public double getRelativeMemoryGlobalStrategy() {
-		return relativeMemoryGlobalStrategy;
-	}
-	
-	public void setRelativeMemoryGlobalStrategy(double relativeMemoryGlobalStrategy) {
-		this.relativeMemoryGlobalStrategy = relativeMemoryGlobalStrategy;
-	}
-	
 	public double getRelativeMemoryLocalStrategy() {
 		return relativeMemoryLocalStrategy;
 	}
@@ -477,8 +479,6 @@ public class Channel implements EstimateProvider, Cloneable, DumpableConnection<
 	
 	/**
 	 * Utility method used while swapping binary union nodes for n-ary union nodes.
-	 * 
-	 * @param newUnionNode
 	 */
 	public void swapUnionNodes(PlanNode newUnionNode) {
 		if (!(this.source instanceof BinaryUnionPlanNode)) {
@@ -493,16 +493,17 @@ public class Channel implements EstimateProvider, Cloneable, DumpableConnection<
 	public int getMaxDepth() {
 		return this.source.getOptimizerNode().getMaxDepth() + 1;
 	}
-	// --------------------------------------------------------------------------------------------
 
-	
+	// --------------------------------------------------------------------------------------------
 
+	@Override
 	public String toString() {
 		return "Channel (" + this.source + (this.target == null ? ')' : ") -> (" + this.target + ')') +
 				'[' + this.shipStrategy + "] [" + this.localStrategy + "] " +
 				(this.tempMode == null || this.tempMode == TempMode.NONE ? "{NO-TEMP}" : this.tempMode);
 	}
-	
+
+	@Override
 	public Channel clone() {
 		try {
 			return (Channel) super.clone();
diff --git a/flink-compiler/src/main/java/org/apache/flink/compiler/plan/OptimizedPlan.java b/flink-compiler/src/main/java/org/apache/flink/compiler/plan/OptimizedPlan.java
index 00eb287f648..51d65e1fcca 100644
--- a/flink-compiler/src/main/java/org/apache/flink/compiler/plan/OptimizedPlan.java
+++ b/flink-compiler/src/main/java/org/apache/flink/compiler/plan/OptimizedPlan.java
@@ -16,7 +16,6 @@
  * limitations under the License.
  */
 
-
 package org.apache.flink.compiler.plan;
 
 import java.util.Collection;
@@ -26,64 +25,46 @@ import org.apache.flink.util.Visitable;
 import org.apache.flink.util.Visitor;
 
 /**
- * The optimizer representation of a plan. The optimizer creates this from the user defined PACT job plan.
- * It works on this representation during its optimization. Finally, this plan is translated to a schedule
- * for the nephele runtime system.
+ * The execution plan generated by the Optimizer. It contains {@link PlanNode}s
+ * and {@link Channel}s that describe exactly how the program should be executed.
+ * It defines all ship strategies (local pipe, shuffle, broadcast, rebalance), all
+ * operator strategies (sorting-merge join, hash join, sorted grouping, ...),
+ * and the data exchange modes (batched, pipelined).
  */
 public class OptimizedPlan implements FlinkPlan, Visitable<PlanNode>  {
 	
-	/**
-	 * The data sources in the plan.
-	 */
+	/** The data sources in the plan. */
 	private final Collection<SourcePlanNode> dataSources;
 
-	/**
-	 * The data sinks in the plan.
-	 */
+	/** The data sinks in the plan. */
 	private final Collection<SinkPlanNode> dataSinks;
 
-	/**
-	 * All nodes in the optimizer plan.
-	 */
+	/** All nodes in the optimizer plan. */
 	private final Collection<PlanNode> allNodes;
 	
-	/**
-	 * The original pact plan.
-	 */
-	private final Plan pactPlan;
+	/** The original program. */
+	private final Plan originalProgram;
 
-	/**
-	 * Name of the PACT job
-	 */
+	/** Name of the job */
 	private final String jobName;
 
-	/**
-	 * The name of the instance type that is to be used.
-	 */
-	private String instanceTypeName;
-	
-	
 	/**
 	 * Creates a new instance of this optimizer plan container. The plan is given and fully
 	 * described by the data sources, sinks and the collection of all nodes.
 	 * 
-	 * @param sources
-	 *        The nodes describing the data sources.
-	 * @param sinks
-	 *        The nodes describing the data sinks.
-	 * @param allNodes
-	 *        A collection containing all nodes in the plan.
-	 * @param jobName
-	 *        The name of the PACT job
+	 * @param sources The data sources.
+	 * @param sinks The data sinks.
+	 * @param allNodes A collection containing all nodes in the plan.
+	 * @param jobName The name of the program
 	 */
 	public OptimizedPlan(Collection<SourcePlanNode> sources, Collection<SinkPlanNode> sinks,
-			Collection<PlanNode> allNodes, String jobName, Plan pactPlan)
+			Collection<PlanNode> allNodes, String jobName, Plan programPlan)
 	{
 		this.dataSources = sources;
 		this.dataSinks = sinks;
 		this.allNodes = allNodes;
 		this.jobName = jobName;
-		this.pactPlan = pactPlan;
+		this.originalProgram = programPlan;
 	}
 
 	/**
@@ -114,46 +95,27 @@ public class OptimizedPlan implements FlinkPlan, Visitable<PlanNode>  {
 	}
 
 	/**
-	 * Returns the name of the optimized PACT job.
+	 * Returns the name of the program.
 	 * 
-	 * @return The name of the optimized PACT job.
+	 * @return The name of the program.
 	 */
 	public String getJobName() {
 		return this.jobName;
 	}
 	
 	/**
-	 * Gets the original pact plan from which this optimized plan was created.
+	 * Gets the original program plan from which this optimized plan was created.
 	 * 
-	 * @return The original pact plan.
+	 * @return The original program plan.
 	 */
 	public Plan getOriginalPactPlan() {
-		return this.pactPlan;
-	}
-
-	/**
-	 * Gets the name of the instance type that should be used for this PACT job.
-	 * 
-	 * @return The instance-type name.
-	 */
-	public String getInstanceTypeName() {
-		return instanceTypeName;
-	}
-
-	/**
-	 * Sets the name of the instance type that should be used for this PACT job.
-	 * 
-	 * @param instanceTypeName
-	 *        The name of the instance type.
-	 */
-	public void setInstanceTypeName(String instanceTypeName) {
-		this.instanceTypeName = instanceTypeName;
+		return this.originalProgram;
 	}
 
 	// ------------------------------------------------------------------------
 
 	/**
-	 * Takes the given visitor and applies it top down to all nodes, starting at the sinks.
+	 * Applies the given visitor top down to all nodes, starting at the sinks.
 	 * 
 	 * @param visitor
 	 *        The visitor to apply to the nodes in this plan.
@@ -165,5 +127,4 @@ public class OptimizedPlan implements FlinkPlan, Visitable<PlanNode>  {
 			node.accept(visitor);
 		}
 	}
-
 }
diff --git a/flink-compiler/src/test/java/org/apache/flink/compiler/AdditionalOperatorsTest.java b/flink-compiler/src/test/java/org/apache/flink/compiler/AdditionalOperatorsTest.java
index cf32126b993..07fc972009a 100644
--- a/flink-compiler/src/test/java/org/apache/flink/compiler/AdditionalOperatorsTest.java
+++ b/flink-compiler/src/test/java/org/apache/flink/compiler/AdditionalOperatorsTest.java
@@ -16,7 +16,6 @@
  * limitations under the License.
  */
 
-
 package org.apache.flink.compiler;
 
 import static org.junit.Assert.assertEquals;
@@ -28,7 +27,6 @@ import org.apache.flink.api.java.record.operators.CrossWithLargeOperator;
 import org.apache.flink.api.java.record.operators.CrossWithSmallOperator;
 import org.apache.flink.api.java.record.operators.FileDataSink;
 import org.apache.flink.api.java.record.operators.FileDataSource;
-import org.apache.flink.compiler.CompilerException;
 import org.apache.flink.compiler.plan.Channel;
 import org.apache.flink.compiler.plan.DualInputPlanNode;
 import org.apache.flink.compiler.plan.OptimizedPlan;
@@ -38,7 +36,6 @@ import org.apache.flink.compiler.util.DummyOutputFormat;
 import org.apache.flink.runtime.operators.shipping.ShipStrategyType;
 import org.junit.Test;
 
-
 /**
 * Tests that validate optimizer choices when using operators that are requesting certain specific execution
 * strategies.
diff --git a/flink-compiler/src/test/java/org/apache/flink/compiler/CompilerTestBase.java b/flink-compiler/src/test/java/org/apache/flink/compiler/CompilerTestBase.java
index c6a9b55c517..beea0b9d5b0 100644
--- a/flink-compiler/src/test/java/org/apache/flink/compiler/CompilerTestBase.java
+++ b/flink-compiler/src/test/java/org/apache/flink/compiler/CompilerTestBase.java
@@ -20,19 +20,16 @@ package org.apache.flink.compiler;
 
 import java.util.ArrayList;
 import java.util.HashMap;
-import java.util.HashSet;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
-import java.util.Set;
 
 import org.apache.flink.api.common.Plan;
 import org.apache.flink.api.common.functions.Function;
 import org.apache.flink.api.common.io.FileInputFormat.FileBaseStatistics;
 import org.apache.flink.api.common.operators.GenericDataSourceBase;
 import org.apache.flink.api.common.operators.Operator;
-import org.apache.flink.api.java.record.operators.BulkIteration;
-import org.apache.flink.api.java.record.operators.DeltaIteration;
+import org.apache.flink.api.common.operators.base.BulkIterationBase;
 import org.apache.flink.compiler.costs.DefaultCostEstimator;
 import org.apache.flink.compiler.plan.OptimizedPlan;
 import org.apache.flink.compiler.plan.PlanNode;
@@ -42,9 +39,10 @@ import org.apache.flink.util.Visitor;
 import org.junit.Before;
 
 /**
- *
+ * Base class for Optimizer tests. Offers utility methods to trigger optimization
+ * of a program and to fetch the nodes in an optimizer plan that correspond
+ * the the node in the program plan.
  */
-@SuppressWarnings("deprecation")
 public abstract class CompilerTestBase implements java.io.Serializable {
 	
 	private static final long serialVersionUID = 1L;
@@ -55,8 +53,6 @@ public abstract class CompilerTestBase implements java.io.Serializable {
 	
 	protected static final int DEFAULT_PARALLELISM = 8;
 	
-	protected static final String DEFAULT_PARALLELISM_STRING = String.valueOf(DEFAULT_PARALLELISM);
-	
 	private static final String CACHE_KEY = "cachekey";
 	
 	// ------------------------------------------------------------------------
@@ -102,16 +98,11 @@ public abstract class CompilerTestBase implements java.io.Serializable {
 	}
 	
 	// ------------------------------------------------------------------------
-	public static OperatorResolver getContractResolver(Plan plan) {
-		return new OperatorResolver(plan);
-	}
 	
 	public static OptimizerPlanNodeResolver getOptimizerPlanNodeResolver(OptimizedPlan plan) {
 		return new OptimizerPlanNodeResolver(plan);
 	}
 	
-	// ------------------------------------------------------------------------
-	
 	public static final class OptimizerPlanNodeResolver {
 		
 		private final Map<String, ArrayList<PlanNode>> map;
@@ -205,97 +196,6 @@ public abstract class CompilerTestBase implements java.io.Serializable {
 			}
 		}
 	}
-	
-	// ------------------------------------------------------------------------
-	
-	public static final class OperatorResolver implements Visitor<Operator<?>> {
-		
-		private final Map<String, List<Operator<?>>> map;
-		private Set<Operator<?>> seen;
-		
-		OperatorResolver(Plan p) {
-			this.map = new HashMap<String, List<Operator<?>>>();
-			this.seen = new HashSet<Operator<?>>();
-			
-			p.accept(this);
-			this.seen = null;
-		}
-		
-		
-		@SuppressWarnings("unchecked")
-		public <T extends Operator<?>> T getNode(String name) {
-			List<Operator<?>> nodes = this.map.get(name);
-			if (nodes == null || nodes.isEmpty()) {
-				throw new RuntimeException("No nodes found with the given name.");
-			} else if (nodes.size() != 1) {
-				throw new RuntimeException("Multiple nodes found with the given name.");
-			} else {
-				return (T) nodes.get(0);
-			}
-		}
-		
-		@SuppressWarnings("unchecked")
-		public <T extends Operator<?>> T getNode(String name, Class<? extends Function> stubClass) {
-			List<Operator<?>> nodes = this.map.get(name);
-			if (nodes == null || nodes.isEmpty()) {
-				throw new RuntimeException("No node found with the given name and stub class.");
-			} else {
-				Operator<?> found = null;
-				for (Operator<?> node : nodes) {
-					if (node.getClass() == stubClass) {
-						if (found == null) {
-							found = node;
-						} else {
-							throw new RuntimeException("Multiple nodes found with the given name and stub class.");
-						}
-					}
-				}
-				if (found == null) {
-					throw new RuntimeException("No node found with the given name and stub class.");
-				} else {
-					return (T) found;
-				}
-			}
-		}
-		
-		public List<Operator<?>> getNodes(String name) {
-			List<Operator<?>> nodes = this.map.get(name);
-			if (nodes == null || nodes.isEmpty()) {
-				throw new RuntimeException("No node found with the given name.");
-			} else {
-				return new ArrayList<Operator<?>>(nodes);
-			}
-		}
-
-		@Override
-		public boolean preVisit(Operator<?> visitable) {
-			if (this.seen.add(visitable)) {
-				// add to  the map
-				final String name = visitable.getName();
-				List<Operator<?>> list = this.map.get(name);
-				if (list == null) {
-					list = new ArrayList<Operator<?>>(2);
-					this.map.put(name, list);
-				}
-				list.add(visitable);
-				
-				// recurse into bulk iterations
-				if (visitable instanceof BulkIteration) {
-					((BulkIteration) visitable).getNextPartialSolution().accept(this);
-				} else if (visitable instanceof DeltaIteration) {
-					((DeltaIteration) visitable).getSolutionSetDelta().accept(this);
-					((DeltaIteration) visitable).getNextWorkset().accept(this);
-				}
-				
-				return true;
-			} else {
-				return false;
-			}
-		}
-
-		@Override
-		public void postVisit(Operator<?> visitable) {}
-	}
 
 	/**
 	 * Collects all DataSources of a plan to add statistics
@@ -311,8 +211,8 @@ public abstract class CompilerTestBase implements java.io.Serializable {
 			if(visitable instanceof GenericDataSourceBase) {
 				sources.add((GenericDataSourceBase<?, ?>) visitable);
 			}
-			else if(visitable instanceof BulkIteration) {
-				((BulkIteration) visitable).getNextPartialSolution().accept(this);
+			else if(visitable instanceof BulkIterationBase) {
+				((BulkIterationBase<?>) visitable).getNextPartialSolution().accept(this);
 			}
 			
 			return true;
diff --git a/flink-compiler/src/test/java/org/apache/flink/compiler/FeedbackPropertiesMatchTest.java b/flink-compiler/src/test/java/org/apache/flink/compiler/FeedbackPropertiesMatchTest.java
index 677d9be55ad..b80027937fa 100644
--- a/flink-compiler/src/test/java/org/apache/flink/compiler/FeedbackPropertiesMatchTest.java
+++ b/flink-compiler/src/test/java/org/apache/flink/compiler/FeedbackPropertiesMatchTest.java
@@ -50,6 +50,7 @@ import org.apache.flink.compiler.plan.PlanNode.FeedbackPropertiesMeetRequirement
 import org.apache.flink.compiler.testfunctions.DummyFlatJoinFunction;
 import org.apache.flink.compiler.testfunctions.IdentityMapper;
 import org.apache.flink.core.fs.Path;
+import org.apache.flink.runtime.io.network.DataExchangeMode;
 import org.apache.flink.runtime.operators.DriverStrategy;
 import org.apache.flink.runtime.operators.shipping.ShipStrategyType;
 import org.apache.flink.runtime.operators.util.LocalStrategy;
@@ -67,12 +68,12 @@ public class FeedbackPropertiesMatchTest {
 			SourcePlanNode otherTarget = new SourcePlanNode(getSourceNode(), "Source");
 			
 			Channel toMap1 = new Channel(target);
-			toMap1.setShipStrategy(ShipStrategyType.FORWARD);
+			toMap1.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 			toMap1.setLocalStrategy(LocalStrategy.NONE);
 			SingleInputPlanNode map1 = new SingleInputPlanNode(getMapNode(), "Mapper 1", toMap1, DriverStrategy.MAP);
 			
 			Channel toMap2 = new Channel(map1);
-			toMap2.setShipStrategy(ShipStrategyType.FORWARD);
+			toMap2.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 			toMap2.setLocalStrategy(LocalStrategy.NONE);
 			SingleInputPlanNode map2 = new SingleInputPlanNode(getMapNode(), "Mapper 2", toMap2, DriverStrategy.MAP);
 			
@@ -96,12 +97,12 @@ public class FeedbackPropertiesMatchTest {
 			SourcePlanNode target = new SourcePlanNode(getSourceNode(), "Source");
 			
 			Channel toMap1 = new Channel(target);
-			toMap1.setShipStrategy(ShipStrategyType.FORWARD);
+			toMap1.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 			toMap1.setLocalStrategy(LocalStrategy.NONE);
 			SingleInputPlanNode map1 = new SingleInputPlanNode(getMapNode(), "Mapper 1", toMap1, DriverStrategy.MAP);
 			
 			Channel toMap2 = new Channel(map1);
-			toMap2.setShipStrategy(ShipStrategyType.FORWARD);
+			toMap2.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 			toMap2.setLocalStrategy(LocalStrategy.NONE);
 			SingleInputPlanNode map2 = new SingleInputPlanNode(getMapNode(), "Mapper 2", toMap2, DriverStrategy.MAP);
 			
@@ -674,10 +675,10 @@ public class FeedbackPropertiesMatchTest {
 				RequestedGlobalProperties reqGp = new RequestedGlobalProperties();
 				reqGp.setAnyPartitioning(new FieldSet(2, 5));
 				
-				toMap1.setShipStrategy(ShipStrategyType.PARTITION_HASH, new FieldList(2, 5));
+				toMap1.setShipStrategy(ShipStrategyType.PARTITION_HASH, new FieldList(2, 5), DataExchangeMode.PIPELINED);
 				toMap1.setLocalStrategy(LocalStrategy.NONE);
 				
-				toMap2.setShipStrategy(ShipStrategyType.FORWARD);
+				toMap2.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 				toMap2.setLocalStrategy(LocalStrategy.NONE);
 				
 				
@@ -700,10 +701,10 @@ public class FeedbackPropertiesMatchTest {
 				RequestedGlobalProperties reqGp = new RequestedGlobalProperties();
 				reqGp.setAnyPartitioning(new FieldSet(2, 5));
 				
-				toMap1.setShipStrategy(ShipStrategyType.FORWARD);
+				toMap1.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 				toMap1.setLocalStrategy(LocalStrategy.NONE);
 				
-				toMap2.setShipStrategy(ShipStrategyType.PARTITION_HASH, new FieldList(2, 5));
+				toMap2.setShipStrategy(ShipStrategyType.PARTITION_HASH, new FieldList(2, 5), DataExchangeMode.PIPELINED);
 				toMap2.setLocalStrategy(LocalStrategy.NONE);
 				
 				
@@ -726,10 +727,10 @@ public class FeedbackPropertiesMatchTest {
 				RequestedLocalProperties reqLp = new RequestedLocalProperties();
 				reqLp.setGroupedFields(new FieldList(4, 1));
 				
-				toMap1.setShipStrategy(ShipStrategyType.FORWARD);
+				toMap1.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 				toMap1.setLocalStrategy(LocalStrategy.SORT, new FieldList(5, 7), new boolean[] {false, false});
 				
-				toMap2.setShipStrategy(ShipStrategyType.FORWARD);
+				toMap2.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 				toMap2.setLocalStrategy(LocalStrategy.NONE);
 				
 				toMap1.setRequiredGlobalProps(null);
@@ -751,10 +752,10 @@ public class FeedbackPropertiesMatchTest {
 				RequestedLocalProperties reqLp = new RequestedLocalProperties();
 				reqLp.setGroupedFields(new FieldList(4, 1));
 				
-				toMap1.setShipStrategy(ShipStrategyType.FORWARD);
+				toMap1.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 				toMap1.setLocalStrategy(LocalStrategy.NONE);
 				
-				toMap2.setShipStrategy(ShipStrategyType.FORWARD);
+				toMap2.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 				toMap2.setLocalStrategy(LocalStrategy.SORT, new FieldList(5, 7), new boolean[] {false, false});
 				
 				
@@ -780,10 +781,10 @@ public class FeedbackPropertiesMatchTest {
 				RequestedLocalProperties reqLp = new RequestedLocalProperties();
 				reqLp.setGroupedFields(new FieldList(5, 7));
 				
-				toMap1.setShipStrategy(ShipStrategyType.PARTITION_HASH, new FieldList(5, 7));
+				toMap1.setShipStrategy(ShipStrategyType.PARTITION_HASH, new FieldList(5, 7), DataExchangeMode.PIPELINED);
 				toMap1.setLocalStrategy(LocalStrategy.SORT, new FieldList(5, 7), new boolean[] {false, false});
 				
-				toMap2.setShipStrategy(ShipStrategyType.FORWARD);
+				toMap2.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 				toMap2.setLocalStrategy(LocalStrategy.NONE);
 				
 				toMap1.setRequiredGlobalProps(reqGp);
@@ -824,13 +825,13 @@ public class FeedbackPropertiesMatchTest {
 				RequestedLocalProperties reqLp = new RequestedLocalProperties();
 				reqLp.setGroupedFields(new FieldList(4, 1));
 				
-				toMap1.setShipStrategy(ShipStrategyType.FORWARD);
+				toMap1.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 				toMap1.setLocalStrategy(LocalStrategy.SORT, new FieldList(5, 7), new boolean[] {false, false});
 				
-				toMap2.setShipStrategy(ShipStrategyType.FORWARD);
+				toMap2.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 				toMap2.setLocalStrategy(LocalStrategy.NONE);
 				
-				toMap3.setShipStrategy(ShipStrategyType.FORWARD);
+				toMap3.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 				toMap3.setLocalStrategy(LocalStrategy.NONE);
 				
 				toMap1.setRequiredGlobalProps(null);
@@ -855,13 +856,13 @@ public class FeedbackPropertiesMatchTest {
 				RequestedGlobalProperties reqGp = new RequestedGlobalProperties();
 				reqGp.setAnyPartitioning(new FieldSet(2, 3));
 				
-				toMap1.setShipStrategy(ShipStrategyType.PARTITION_HASH, new FieldList(1, 2));
+				toMap1.setShipStrategy(ShipStrategyType.PARTITION_HASH, new FieldList(1, 2), DataExchangeMode.PIPELINED);
 				toMap1.setLocalStrategy(LocalStrategy.NONE);
 				
-				toMap2.setShipStrategy(ShipStrategyType.FORWARD);
+				toMap2.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 				toMap2.setLocalStrategy(LocalStrategy.NONE);
 				
-				toMap3.setShipStrategy(ShipStrategyType.FORWARD);
+				toMap3.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 				toMap3.setLocalStrategy(LocalStrategy.NONE);
 				
 				toMap1.setRequiredGlobalProps(null);
@@ -892,21 +893,21 @@ public class FeedbackPropertiesMatchTest {
 			SourcePlanNode source2 = new SourcePlanNode(getSourceNode(), "Source 2");
 			
 			Channel toMap1 = new Channel(source1);
-			toMap1.setShipStrategy(ShipStrategyType.FORWARD);
+			toMap1.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 			toMap1.setLocalStrategy(LocalStrategy.NONE);
 			SingleInputPlanNode map1 = new SingleInputPlanNode(getMapNode(), "Mapper 1", toMap1, DriverStrategy.MAP);
 			
 			Channel toMap2 = new Channel(source2);
-			toMap2.setShipStrategy(ShipStrategyType.FORWARD);
+			toMap2.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 			toMap2.setLocalStrategy(LocalStrategy.NONE);
 			SingleInputPlanNode map2 = new SingleInputPlanNode(getMapNode(), "Mapper 2", toMap2, DriverStrategy.MAP);
 			
 			Channel toJoin1 = new Channel(map1);
 			Channel toJoin2 = new Channel(map2);
 			
-			toJoin1.setShipStrategy(ShipStrategyType.FORWARD);
+			toJoin1.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 			toJoin1.setLocalStrategy(LocalStrategy.NONE);
-			toJoin2.setShipStrategy(ShipStrategyType.FORWARD);
+			toJoin2.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 			toJoin2.setLocalStrategy(LocalStrategy.NONE);
 			
 			DualInputPlanNode join = new DualInputPlanNode(getJoinNode(), "Join", toJoin1, toJoin2, DriverStrategy.HYBRIDHASH_BUILD_FIRST);
@@ -927,12 +928,12 @@ public class FeedbackPropertiesMatchTest {
 			SourcePlanNode source = new SourcePlanNode(getSourceNode(), "Other Source");
 			
 			Channel toMap1 = new Channel(target);
-			toMap1.setShipStrategy(ShipStrategyType.FORWARD);
+			toMap1.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 			toMap1.setLocalStrategy(LocalStrategy.NONE);
 			SingleInputPlanNode map1 = new SingleInputPlanNode(getMapNode(), "Mapper 1", toMap1, DriverStrategy.MAP);
 			
 			Channel toMap2 = new Channel(source);
-			toMap2.setShipStrategy(ShipStrategyType.FORWARD);
+			toMap2.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 			toMap2.setLocalStrategy(LocalStrategy.NONE);
 			SingleInputPlanNode map2 = new SingleInputPlanNode(getMapNode(), "Mapper 2", toMap2, DriverStrategy.MAP);
 			
@@ -942,13 +943,13 @@ public class FeedbackPropertiesMatchTest {
 			DualInputPlanNode join = new DualInputPlanNode(getJoinNode(), "Join", toJoin1, toJoin2, DriverStrategy.HYBRIDHASH_BUILD_FIRST);
 			
 			Channel toAfterJoin = new Channel(join);
-			toAfterJoin.setShipStrategy(ShipStrategyType.FORWARD);
+			toAfterJoin.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 			toAfterJoin.setLocalStrategy(LocalStrategy.NONE);
 			SingleInputPlanNode afterJoin = new SingleInputPlanNode(getMapNode(), "After Join Mapper", toAfterJoin, DriverStrategy.MAP);
 			
 			// attach some properties to the non-relevant input
 			{
-				toMap2.setShipStrategy(ShipStrategyType.BROADCAST);
+				toMap2.setShipStrategy(ShipStrategyType.BROADCAST, DataExchangeMode.PIPELINED);
 				toMap2.setLocalStrategy(LocalStrategy.SORT, new FieldList(2, 7), new boolean[] {true, true});
 				
 				RequestedGlobalProperties joinGp = new RequestedGlobalProperties();
@@ -957,7 +958,7 @@ public class FeedbackPropertiesMatchTest {
 				RequestedLocalProperties joinLp = new RequestedLocalProperties();
 				joinLp.setOrdering(new Ordering(2, null, Order.ASCENDING).appendOrdering(7, null, Order.ASCENDING));
 				
-				toJoin2.setShipStrategy(ShipStrategyType.FORWARD);
+				toJoin2.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 				toJoin2.setLocalStrategy(LocalStrategy.NONE);
 				toJoin2.setRequiredGlobalProps(joinGp);
 				toJoin2.setRequiredLocalProps(joinLp);
@@ -967,7 +968,7 @@ public class FeedbackPropertiesMatchTest {
 			
 			// no properties from the partial solution, no required properties
 			{
-				toJoin1.setShipStrategy(ShipStrategyType.FORWARD);
+				toJoin1.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 				toJoin1.setLocalStrategy(LocalStrategy.NONE);
 				
 				GlobalProperties gp = new GlobalProperties();
@@ -979,7 +980,7 @@ public class FeedbackPropertiesMatchTest {
 			
 			// some properties from the partial solution, no required properties
 			{
-				toJoin1.setShipStrategy(ShipStrategyType.FORWARD);
+				toJoin1.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 				toJoin1.setLocalStrategy(LocalStrategy.NONE);
 				
 				GlobalProperties gp = new GlobalProperties();
@@ -1006,7 +1007,7 @@ public class FeedbackPropertiesMatchTest {
 				toJoin1.setRequiredGlobalProps(rgp);
 				toJoin1.setRequiredLocalProps(rlp);
 				
-				toJoin1.setShipStrategy(ShipStrategyType.FORWARD);
+				toJoin1.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 				toJoin1.setLocalStrategy(LocalStrategy.NONE);
 				
 				FeedbackPropertiesMeetRequirementsReport report = join.checkPartialSolutionPropertiesMet(target, gp, lp);
@@ -1028,7 +1029,7 @@ public class FeedbackPropertiesMatchTest {
 				toJoin1.setRequiredGlobalProps(rgp);
 				toJoin1.setRequiredLocalProps(rlp);
 				
-				toJoin1.setShipStrategy(ShipStrategyType.FORWARD);
+				toJoin1.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 				toJoin1.setLocalStrategy(LocalStrategy.NONE);
 				
 				FeedbackPropertiesMeetRequirementsReport report = join.checkPartialSolutionPropertiesMet(target, gp, lp);
@@ -1053,7 +1054,7 @@ public class FeedbackPropertiesMatchTest {
 				toJoin1.setRequiredGlobalProps(null);
 				toJoin1.setRequiredLocalProps(null);
 				
-				toJoin1.setShipStrategy(ShipStrategyType.PARTITION_HASH, new FieldList(2, 1));
+				toJoin1.setShipStrategy(ShipStrategyType.PARTITION_HASH, new FieldList(2, 1), DataExchangeMode.PIPELINED);
 				toJoin1.setLocalStrategy(LocalStrategy.SORT, new FieldList(7, 3), new boolean[] {true, false});
 				
 				FeedbackPropertiesMeetRequirementsReport report = join.checkPartialSolutionPropertiesMet(target, gp, lp);
@@ -1075,13 +1076,13 @@ public class FeedbackPropertiesMatchTest {
 				toMap1.setRequiredGlobalProps(null);
 				toMap1.setRequiredLocalProps(null);
 				
-				toJoin1.setShipStrategy(ShipStrategyType.FORWARD);
+				toJoin1.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 				toJoin1.setLocalStrategy(LocalStrategy.NONE);
 				
 				toJoin1.setRequiredGlobalProps(rgp);
 				toJoin1.setRequiredLocalProps(rlp);
 			
-				toAfterJoin.setShipStrategy(ShipStrategyType.FORWARD);
+				toAfterJoin.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 				toAfterJoin.setLocalStrategy(LocalStrategy.NONE);
 				
 				toAfterJoin.setRequiredGlobalProps(rgp);
@@ -1112,7 +1113,7 @@ public class FeedbackPropertiesMatchTest {
 				toJoin1.setRequiredGlobalProps(rgp1);
 				toJoin1.setRequiredLocalProps(rlp1);
 			
-				toAfterJoin.setShipStrategy(ShipStrategyType.FORWARD);
+				toAfterJoin.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 				toAfterJoin.setLocalStrategy(LocalStrategy.NONE);
 				
 				toAfterJoin.setRequiredGlobalProps(rgp2);
@@ -1137,7 +1138,7 @@ public class FeedbackPropertiesMatchTest {
 				toJoin1.setRequiredGlobalProps(null);
 				toJoin1.setRequiredLocalProps(null);
 				
-				toJoin1.setShipStrategy(ShipStrategyType.PARTITION_HASH, new FieldList(2, 1));
+				toJoin1.setShipStrategy(ShipStrategyType.PARTITION_HASH, new FieldList(2, 1), DataExchangeMode.PIPELINED);
 				toJoin1.setLocalStrategy(LocalStrategy.SORT, new FieldList(7, 3), new boolean[] {true, false});
 				
 				toAfterJoin.setRequiredGlobalProps(rgp);
@@ -1159,7 +1160,7 @@ public class FeedbackPropertiesMatchTest {
 				toJoin1.setRequiredGlobalProps(null);
 				toJoin1.setRequiredLocalProps(null);
 				
-				toJoin1.setShipStrategy(ShipStrategyType.FORWARD);
+				toJoin1.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 				toJoin1.setLocalStrategy(LocalStrategy.SORT, new FieldList(7, 3), new boolean[] {true, false});
 				
 				toAfterJoin.setRequiredGlobalProps(null);
@@ -1184,7 +1185,7 @@ public class FeedbackPropertiesMatchTest {
 				toJoin1.setRequiredGlobalProps(null);
 				toJoin1.setRequiredLocalProps(null);
 				
-				toJoin1.setShipStrategy(ShipStrategyType.FORWARD);
+				toJoin1.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 				toJoin1.setLocalStrategy(LocalStrategy.SORT, new FieldList(7, 3), new boolean[] {true, false});
 				
 				toAfterJoin.setRequiredGlobalProps(rgp);
@@ -1206,27 +1207,27 @@ public class FeedbackPropertiesMatchTest {
 			SourcePlanNode target = new SourcePlanNode(getSourceNode(), "Partial Solution");
 			
 			Channel toMap1 = new Channel(target);
-			toMap1.setShipStrategy(ShipStrategyType.FORWARD);
+			toMap1.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 			toMap1.setLocalStrategy(LocalStrategy.NONE);
 			SingleInputPlanNode map1 = new SingleInputPlanNode(getMapNode(), "Mapper 1", toMap1, DriverStrategy.MAP);
 			
 			Channel toMap2 = new Channel(target);
-			toMap2.setShipStrategy(ShipStrategyType.FORWARD);
+			toMap2.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 			toMap2.setLocalStrategy(LocalStrategy.NONE);
 			SingleInputPlanNode map2 = new SingleInputPlanNode(getMapNode(), "Mapper 2", toMap2, DriverStrategy.MAP);
 			
 			Channel toJoin1 = new Channel(map1);
-			toJoin1.setShipStrategy(ShipStrategyType.FORWARD);
+			toJoin1.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 			toJoin1.setLocalStrategy(LocalStrategy.NONE);
 			
 			Channel toJoin2 = new Channel(map2);
-			toJoin2.setShipStrategy(ShipStrategyType.FORWARD);
+			toJoin2.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 			toJoin2.setLocalStrategy(LocalStrategy.NONE);
 			
 			DualInputPlanNode join = new DualInputPlanNode(getJoinNode(), "Join", toJoin1, toJoin2, DriverStrategy.HYBRIDHASH_BUILD_FIRST);
 			
 			Channel toAfterJoin = new Channel(join);
-			toAfterJoin.setShipStrategy(ShipStrategyType.FORWARD);
+			toAfterJoin.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 			toAfterJoin.setLocalStrategy(LocalStrategy.NONE);
 			SingleInputPlanNode afterJoin = new SingleInputPlanNode(getMapNode(), "After Join Mapper", toAfterJoin, DriverStrategy.MAP);
 			
@@ -1336,8 +1337,8 @@ public class FeedbackPropertiesMatchTest {
 				toJoin2.setRequiredGlobalProps(null);
 				toJoin2.setRequiredLocalProps(null);
 				
-				toJoin1.setShipStrategy(ShipStrategyType.PARTITION_HASH, new FieldList(88));
-				toJoin2.setShipStrategy(ShipStrategyType.BROADCAST);
+				toJoin1.setShipStrategy(ShipStrategyType.PARTITION_HASH, new FieldList(88), DataExchangeMode.PIPELINED);
+				toJoin2.setShipStrategy(ShipStrategyType.BROADCAST, DataExchangeMode.PIPELINED);
 				
 				toAfterJoin.setRequiredGlobalProps(rgp);
 				toAfterJoin.setRequiredLocalProps(rlp);
@@ -1358,8 +1359,8 @@ public class FeedbackPropertiesMatchTest {
 				RequestedLocalProperties rlp = new RequestedLocalProperties();
 				rlp.setGroupedFields(new FieldList(2, 1));
 				
-				toJoin1.setShipStrategy(ShipStrategyType.PARTITION_HASH, new FieldList(88));
-				toJoin2.setShipStrategy(ShipStrategyType.FORWARD);
+				toJoin1.setShipStrategy(ShipStrategyType.PARTITION_HASH, new FieldList(88), DataExchangeMode.PIPELINED);
+				toJoin2.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 				
 				toAfterJoin.setRequiredGlobalProps(rgp);
 				toAfterJoin.setRequiredLocalProps(rlp);
@@ -1380,8 +1381,8 @@ public class FeedbackPropertiesMatchTest {
 				RequestedLocalProperties rlp = new RequestedLocalProperties();
 				rlp.setGroupedFields(new FieldList(77, 69));
 				
-				toJoin1.setShipStrategy(ShipStrategyType.PARTITION_HASH, new FieldList(88));
-				toJoin2.setShipStrategy(ShipStrategyType.FORWARD);
+				toJoin1.setShipStrategy(ShipStrategyType.PARTITION_HASH, new FieldList(88), DataExchangeMode.PIPELINED);
+				toJoin2.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 				
 				toAfterJoin.setRequiredGlobalProps(rgp);
 				toAfterJoin.setRequiredLocalProps(rlp);
@@ -1400,10 +1401,10 @@ public class FeedbackPropertiesMatchTest {
 				rgp.setHashPartitioned(new FieldList(3));
 				
 				
-				toJoin1.setShipStrategy(ShipStrategyType.FORWARD);
+				toJoin1.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 				toJoin1.setLocalStrategy(LocalStrategy.SORT, new FieldList(3), new boolean[] { false });
 				
-				toJoin2.setShipStrategy(ShipStrategyType.FORWARD);
+				toJoin2.setShipStrategy(ShipStrategyType.FORWARD, DataExchangeMode.PIPELINED);
 				toJoin1.setLocalStrategy(LocalStrategy.NONE);
 				
 				toAfterJoin.setRequiredGlobalProps(rgp);
@@ -1421,15 +1422,15 @@ public class FeedbackPropertiesMatchTest {
 	
 	// --------------------------------------------------------------------------------------------
 	
-	private static final DataSourceNode getSourceNode() {
+	private static DataSourceNode getSourceNode() {
 		return new DataSourceNode(new GenericDataSourceBase<String, TextInputFormat>(new TextInputFormat(new Path("/")), new OperatorInformation<String>(BasicTypeInfo.STRING_TYPE_INFO)));
 	}
 	
-	private static final MapNode getMapNode() {
+	private static MapNode getMapNode() {
 		return new MapNode(new MapOperatorBase<String, String, MapFunction<String,String>>(new IdentityMapper<String>(), new UnaryOperatorInformation<String, String>(BasicTypeInfo.STRING_TYPE_INFO, BasicTypeInfo.STRING_TYPE_INFO), "map op"));
 	}
 	
-	private static final JoinNode getJoinNode() {
+	private static JoinNode getJoinNode() {
 		return new JoinNode(new JoinOperatorBase<String, String, String, FlatJoinFunction<String, String, String>>(new DummyFlatJoinFunction<String>(), new BinaryOperatorInformation<String, String, String>(BasicTypeInfo.STRING_TYPE_INFO, BasicTypeInfo.STRING_TYPE_INFO, BasicTypeInfo.STRING_TYPE_INFO), new int[] {1}, new int[] {2}, "join op"));
 	}
 }
diff --git a/flink-compiler/src/test/java/org/apache/flink/compiler/costs/DefaultCostEstimatorTest.java b/flink-compiler/src/test/java/org/apache/flink/compiler/costs/DefaultCostEstimatorTest.java
index 3c28a3aa416..01404ace2ec 100644
--- a/flink-compiler/src/test/java/org/apache/flink/compiler/costs/DefaultCostEstimatorTest.java
+++ b/flink-compiler/src/test/java/org/apache/flink/compiler/costs/DefaultCostEstimatorTest.java
@@ -21,9 +21,6 @@ package org.apache.flink.compiler.costs;
 
 import static org.junit.Assert.*;
 
-import org.apache.flink.compiler.costs.CostEstimator;
-import org.apache.flink.compiler.costs.Costs;
-import org.apache.flink.compiler.costs.DefaultCostEstimator;
 import org.apache.flink.compiler.dag.EstimateProvider;
 import org.junit.Test;
 
diff --git a/flink-compiler/src/test/java/org/apache/flink/compiler/dataexchange/DataExchangeModeClosedBranchingTest.java b/flink-compiler/src/test/java/org/apache/flink/compiler/dataexchange/DataExchangeModeClosedBranchingTest.java
new file mode 100644
index 00000000000..a78336c3005
--- /dev/null
+++ b/flink-compiler/src/test/java/org/apache/flink/compiler/dataexchange/DataExchangeModeClosedBranchingTest.java
@@ -0,0 +1,257 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.compiler.dataexchange;
+
+import org.apache.flink.api.common.ExecutionMode;
+import org.apache.flink.api.common.functions.FilterFunction;
+import org.apache.flink.api.common.functions.MapFunction;
+import org.apache.flink.api.java.DataSet;
+import org.apache.flink.api.java.ExecutionEnvironment;
+import org.apache.flink.api.java.io.DiscardingOutputFormat;
+import org.apache.flink.api.java.tuple.Tuple2;
+import org.apache.flink.compiler.CompilerTestBase;
+import org.apache.flink.compiler.plan.DualInputPlanNode;
+import org.apache.flink.compiler.plan.OptimizedPlan;
+import org.apache.flink.compiler.plan.SingleInputPlanNode;
+import org.apache.flink.compiler.plan.SinkPlanNode;
+import org.apache.flink.compiler.testfunctions.DummyCoGroupFunction;
+import org.apache.flink.compiler.testfunctions.DummyFlatJoinFunction;
+import org.apache.flink.compiler.testfunctions.IdentityFlatMapper;
+import org.apache.flink.compiler.testfunctions.SelectOneReducer;
+import org.apache.flink.compiler.testfunctions.Top1GroupReducer;
+import org.apache.flink.runtime.io.network.DataExchangeMode;
+import org.junit.Test;
+
+import java.util.Collection;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.fail;
+
+/**
+ * This test checks the correct assignment of the DataExchangeMode to
+ * connections for programs that branch, and re-join those branches.
+ *
+ * <pre>
+ *                                         /-> (sink)
+ *                                        /
+ *                         /-> (reduce) -+          /-> (flatmap) -> (sink)
+ *                        /               \        /
+ *     (source) -> (map) -                (join) -+-----\
+ *                        \               /              \
+ *                         \-> (filter) -+                \
+ *                                       \                (co group) -> (sink)
+ *                                        \                /
+ *                                         \-> (reduce) - /
+ * </pre>
+ */
+@SuppressWarnings("serial")
+public class DataExchangeModeClosedBranchingTest extends CompilerTestBase {
+
+	@Test
+	public void testPipelinedForced() {
+		// PIPELINED_FORCED should result in pipelining all the way
+		verifyBranchingJoiningPlan(ExecutionMode.PIPELINED_FORCED,
+				DataExchangeMode.PIPELINED, DataExchangeMode.PIPELINED,
+				DataExchangeMode.PIPELINED, DataExchangeMode.PIPELINED,
+				DataExchangeMode.PIPELINED, DataExchangeMode.PIPELINED,
+				DataExchangeMode.PIPELINED, DataExchangeMode.PIPELINED,
+				DataExchangeMode.PIPELINED, DataExchangeMode.PIPELINED,
+				DataExchangeMode.PIPELINED, DataExchangeMode.PIPELINED,
+				DataExchangeMode.PIPELINED, DataExchangeMode.PIPELINED);
+	}
+
+	@Test
+	public void testPipelined() {
+		// PIPELINED should result in pipelining all the way
+		verifyBranchingJoiningPlan(ExecutionMode.PIPELINED,
+				DataExchangeMode.PIPELINED,   // to map
+				DataExchangeMode.PIPELINED,   // to combiner connections are pipelined
+				DataExchangeMode.BATCH,       // to reduce
+				DataExchangeMode.BATCH,       // to filter
+				DataExchangeMode.PIPELINED,   // to sink after reduce
+				DataExchangeMode.PIPELINED,   // to join (first input)
+				DataExchangeMode.BATCH,       // to join (second input)
+				DataExchangeMode.PIPELINED,   // combiner connections are pipelined
+				DataExchangeMode.BATCH,       // to other reducer
+				DataExchangeMode.PIPELINED,   // to flatMap
+				DataExchangeMode.PIPELINED,   // to sink after flatMap
+				DataExchangeMode.PIPELINED,   // to coGroup (first input)
+				DataExchangeMode.PIPELINED,   // to coGroup (second input)
+				DataExchangeMode.PIPELINED    // to sink after coGroup
+		);
+	}
+
+	@Test
+	public void testBatch() {
+		// BATCH should result in batching the shuffle all the way
+		verifyBranchingJoiningPlan(ExecutionMode.BATCH,
+				DataExchangeMode.PIPELINED,   // to map
+				DataExchangeMode.PIPELINED,   // to combiner connections are pipelined
+				DataExchangeMode.BATCH,       // to reduce
+				DataExchangeMode.BATCH,       // to filter
+				DataExchangeMode.PIPELINED,   // to sink after reduce
+				DataExchangeMode.BATCH,       // to join (first input)
+				DataExchangeMode.BATCH,       // to join (second input)
+				DataExchangeMode.PIPELINED,   // combiner connections are pipelined
+				DataExchangeMode.BATCH,       // to other reducer
+				DataExchangeMode.PIPELINED,   // to flatMap
+				DataExchangeMode.PIPELINED,   // to sink after flatMap
+				DataExchangeMode.BATCH,       // to coGroup (first input)
+				DataExchangeMode.BATCH,       // to coGroup (second input)
+				DataExchangeMode.PIPELINED    // to sink after coGroup
+		);
+	}
+
+	@Test
+	public void testBatchForced() {
+		// BATCH_FORCED should result in batching all the way
+		verifyBranchingJoiningPlan(ExecutionMode.BATCH_FORCED,
+				DataExchangeMode.BATCH,       // to map
+				DataExchangeMode.PIPELINED,   // to combiner connections are pipelined
+				DataExchangeMode.BATCH,       // to reduce
+				DataExchangeMode.BATCH,       // to filter
+				DataExchangeMode.BATCH,       // to sink after reduce
+				DataExchangeMode.BATCH,       // to join (first input)
+				DataExchangeMode.BATCH,       // to join (second input)
+				DataExchangeMode.PIPELINED,   // combiner connections are pipelined
+				DataExchangeMode.BATCH,       // to other reducer
+				DataExchangeMode.BATCH,       // to flatMap
+				DataExchangeMode.BATCH,       // to sink after flatMap
+				DataExchangeMode.BATCH,       // to coGroup (first input)
+				DataExchangeMode.BATCH,       // to coGroup (second input)
+				DataExchangeMode.BATCH        // to sink after coGroup
+		);
+	}
+
+	private void verifyBranchingJoiningPlan(ExecutionMode execMode,
+											DataExchangeMode toMap,
+											DataExchangeMode toReduceCombiner,
+											DataExchangeMode toReduce,
+											DataExchangeMode toFilter,
+											DataExchangeMode toReduceSink,
+											DataExchangeMode toJoin1,
+											DataExchangeMode toJoin2,
+											DataExchangeMode toOtherReduceCombiner,
+											DataExchangeMode toOtherReduce,
+											DataExchangeMode toFlatMap,
+											DataExchangeMode toFlatMapSink,
+											DataExchangeMode toCoGroup1,
+											DataExchangeMode toCoGroup2,
+											DataExchangeMode toCoGroupSink)
+	{
+		try {
+			ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
+			env.getConfig().setExecutionMode(execMode);
+
+			DataSet<Tuple2<Long, Long>> data = env.fromElements(33L, 44L)
+					.map(new MapFunction<Long, Tuple2<Long, Long>>() {
+						@Override
+						public Tuple2<Long, Long> map(Long value) {
+							return new Tuple2<Long, Long>(value, value);
+						}
+					});
+
+			DataSet<Tuple2<Long, Long>> reduced = data.groupBy(0).reduce(new SelectOneReducer<Tuple2<Long, Long>>());
+			reduced.output(new DiscardingOutputFormat<Tuple2<Long, Long>>()).name("reduceSink");
+
+			DataSet<Tuple2<Long, Long>> filtered = data.filter(new FilterFunction<Tuple2<Long, Long>>() {
+				@Override
+				public boolean filter(Tuple2<Long, Long> value) throws Exception {
+					return false;
+				}
+			});
+
+			DataSet<Tuple2<Long, Long>> joined = reduced.join(filtered)
+					.where(1).equalTo(1)
+					.with(new DummyFlatJoinFunction<Tuple2<Long, Long>>());
+
+			joined.flatMap(new IdentityFlatMapper<Tuple2<Long, Long>>())
+					.output(new DiscardingOutputFormat<Tuple2<Long, Long>>()).name("flatMapSink");
+
+			joined.coGroup(filtered.groupBy(1).reduceGroup(new Top1GroupReducer<Tuple2<Long, Long>>()))
+					.where(0).equalTo(0)
+					.with(new DummyCoGroupFunction<Tuple2<Long, Long>, Tuple2<Long, Long>>())
+					.output(new DiscardingOutputFormat<Tuple2<Tuple2<Long, Long>, Tuple2<Long, Long>>>()).name("cgSink");
+
+
+			OptimizedPlan optPlan = compileNoStats(env.createProgramPlan());
+
+			SinkPlanNode reduceSink = findSink(optPlan.getDataSinks(), "reduceSink");
+			SinkPlanNode flatMapSink = findSink(optPlan.getDataSinks(), "flatMapSink");
+			SinkPlanNode cgSink = findSink(optPlan.getDataSinks(), "cgSink");
+
+			DualInputPlanNode coGroupNode = (DualInputPlanNode) cgSink.getPredecessor();
+
+			DualInputPlanNode joinNode = (DualInputPlanNode) coGroupNode.getInput1().getSource();
+			SingleInputPlanNode otherReduceNode = (SingleInputPlanNode) coGroupNode.getInput2().getSource();
+			SingleInputPlanNode otherReduceCombinerNode = (SingleInputPlanNode) otherReduceNode.getPredecessor();
+
+			SingleInputPlanNode reduceNode = (SingleInputPlanNode) joinNode.getInput1().getSource();
+			SingleInputPlanNode reduceCombinerNode = (SingleInputPlanNode) reduceNode.getPredecessor();
+			assertEquals(reduceNode, reduceSink.getPredecessor());
+
+			SingleInputPlanNode filterNode = (SingleInputPlanNode) joinNode.getInput2().getSource();
+			assertEquals(filterNode, otherReduceCombinerNode.getPredecessor());
+
+			SingleInputPlanNode mapNode = (SingleInputPlanNode) filterNode.getPredecessor();
+			assertEquals(mapNode, reduceCombinerNode.getPredecessor());
+
+			SingleInputPlanNode flatMapNode = (SingleInputPlanNode) flatMapSink.getPredecessor();
+			assertEquals(joinNode, flatMapNode.getPredecessor());
+
+			// verify the data exchange modes
+
+			assertEquals(toReduceSink, reduceSink.getInput().getDataExchangeMode());
+			assertEquals(toFlatMapSink, flatMapSink.getInput().getDataExchangeMode());
+			assertEquals(toCoGroupSink, cgSink.getInput().getDataExchangeMode());
+
+			assertEquals(toCoGroup1, coGroupNode.getInput1().getDataExchangeMode());
+			assertEquals(toCoGroup2, coGroupNode.getInput2().getDataExchangeMode());
+
+			assertEquals(toJoin1, joinNode.getInput1().getDataExchangeMode());
+			assertEquals(toJoin2, joinNode.getInput2().getDataExchangeMode());
+
+			assertEquals(toOtherReduce, otherReduceNode.getInput().getDataExchangeMode());
+			assertEquals(toOtherReduceCombiner, otherReduceCombinerNode.getInput().getDataExchangeMode());
+
+			assertEquals(toFlatMap, flatMapNode.getInput().getDataExchangeMode());
+
+			assertEquals(toFilter, filterNode.getInput().getDataExchangeMode());
+			assertEquals(toReduce, reduceNode.getInput().getDataExchangeMode());
+			assertEquals(toReduceCombiner, reduceCombinerNode.getInput().getDataExchangeMode());
+
+			assertEquals(toMap, mapNode.getInput().getDataExchangeMode());
+		}
+		catch (Exception e) {
+			e.printStackTrace();
+			fail(e.getMessage());
+		}
+	}
+
+	private SinkPlanNode findSink(Collection<SinkPlanNode> collection, String name) {
+		for (SinkPlanNode node : collection) {
+			String nodeName = node.getOptimizerNode().getPactContract().getName();
+			if (nodeName != null && nodeName.equals(name)) {
+				return node;
+			}
+		}
+
+		throw new IllegalArgumentException("No node with that name was found.");
+	}
+}
diff --git a/flink-compiler/src/test/java/org/apache/flink/compiler/dataexchange/DataExchangeModeForwardTest.java b/flink-compiler/src/test/java/org/apache/flink/compiler/dataexchange/DataExchangeModeForwardTest.java
new file mode 100644
index 00000000000..e550749e95a
--- /dev/null
+++ b/flink-compiler/src/test/java/org/apache/flink/compiler/dataexchange/DataExchangeModeForwardTest.java
@@ -0,0 +1,139 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.compiler.dataexchange;
+
+import org.apache.flink.api.common.ExecutionMode;
+import org.apache.flink.api.common.functions.FilterFunction;
+import org.apache.flink.api.common.functions.MapFunction;
+import org.apache.flink.api.java.DataSet;
+import org.apache.flink.api.java.ExecutionEnvironment;
+import org.apache.flink.api.java.io.DiscardingOutputFormat;
+import org.apache.flink.compiler.CompilerTestBase;
+import org.apache.flink.compiler.plan.OptimizedPlan;
+import org.apache.flink.compiler.plan.SingleInputPlanNode;
+import org.apache.flink.compiler.plan.SinkPlanNode;
+import org.apache.flink.compiler.testfunctions.IdentityKeyExtractor;
+import org.apache.flink.compiler.testfunctions.Top1GroupReducer;
+import org.apache.flink.runtime.io.network.DataExchangeMode;
+import org.junit.Test;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.fail;
+
+/**
+ * This test verifies that the optimizer assigns the correct
+ * data exchange mode to a simple forward / shuffle plan.
+ *
+ * <pre>
+ *     (source) -> (map) -> (filter) -> (groupBy / reduce)
+ * </pre>
+ */
+@SuppressWarnings("serial")
+public class DataExchangeModeForwardTest extends CompilerTestBase {
+
+
+	@Test
+	public void testPipelinedForced() {
+		// PIPELINED_FORCED should result in pipelining all the way
+		verifySimpleForwardPlan(ExecutionMode.PIPELINED_FORCED,
+				DataExchangeMode.PIPELINED, DataExchangeMode.PIPELINED,
+				DataExchangeMode.PIPELINED, DataExchangeMode.PIPELINED,
+				DataExchangeMode.PIPELINED, DataExchangeMode.PIPELINED);
+	}
+
+	@Test
+	public void testPipelined() {
+		// PIPELINED should result in pipelining all the way
+		verifySimpleForwardPlan(ExecutionMode.PIPELINED,
+				DataExchangeMode.PIPELINED, DataExchangeMode.PIPELINED,
+				DataExchangeMode.PIPELINED, DataExchangeMode.PIPELINED,
+				DataExchangeMode.PIPELINED, DataExchangeMode.PIPELINED);
+	}
+
+	@Test
+	public void testBatch() {
+		// BATCH should result in batching the shuffle all the way
+		verifySimpleForwardPlan(ExecutionMode.BATCH,
+				DataExchangeMode.PIPELINED, DataExchangeMode.PIPELINED,
+				DataExchangeMode.PIPELINED, DataExchangeMode.PIPELINED,
+				DataExchangeMode.BATCH, DataExchangeMode.PIPELINED);
+	}
+
+	@Test
+	public void testBatchForced() {
+		// BATCH_FORCED should result in batching all the way
+		verifySimpleForwardPlan(ExecutionMode.BATCH_FORCED,
+				DataExchangeMode.BATCH, DataExchangeMode.BATCH,
+				DataExchangeMode.BATCH, DataExchangeMode.PIPELINED,
+				DataExchangeMode.BATCH, DataExchangeMode.BATCH);
+	}
+
+	private void verifySimpleForwardPlan(ExecutionMode execMode,
+										DataExchangeMode toMap,
+										DataExchangeMode toFilter,
+										DataExchangeMode toKeyExtractor,
+										DataExchangeMode toCombiner,
+										DataExchangeMode toReduce,
+										DataExchangeMode toSink)
+	{
+		try {
+			ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
+			env.getConfig().setExecutionMode(execMode);
+
+			DataSet<String> dataSet = env.readTextFile("/never/accessed");
+			dataSet
+				.map(new MapFunction<String, Integer>() {
+					@Override
+					public Integer map(String value) {
+						return 0;
+					}
+				})
+				.filter(new FilterFunction<Integer>() {
+					@Override
+					public boolean filter(Integer value) {
+						return false;
+					}
+				})
+				.groupBy(new IdentityKeyExtractor<Integer>())
+				.reduceGroup(new Top1GroupReducer<Integer>())
+				.output(new DiscardingOutputFormat<Integer>());
+
+			OptimizedPlan optPlan = compileNoStats(env.createProgramPlan());
+			SinkPlanNode sinkNode = optPlan.getDataSinks().iterator().next();
+
+			SingleInputPlanNode reduceNode = (SingleInputPlanNode) sinkNode.getPredecessor();
+			SingleInputPlanNode combineNode = (SingleInputPlanNode) reduceNode.getPredecessor();
+			SingleInputPlanNode keyExtractorNode = (SingleInputPlanNode) combineNode.getPredecessor();
+
+			SingleInputPlanNode filterNode = (SingleInputPlanNode) keyExtractorNode.getPredecessor();
+			SingleInputPlanNode mapNode = (SingleInputPlanNode) filterNode.getPredecessor();
+
+			assertEquals(toMap, mapNode.getInput().getDataExchangeMode());
+			assertEquals(toFilter, filterNode.getInput().getDataExchangeMode());
+			assertEquals(toKeyExtractor, keyExtractorNode.getInput().getDataExchangeMode());
+			assertEquals(toCombiner, combineNode.getInput().getDataExchangeMode());
+			assertEquals(toReduce, reduceNode.getInput().getDataExchangeMode());
+			assertEquals(toSink, sinkNode.getInput().getDataExchangeMode());
+		}
+		catch (Exception e) {
+			e.printStackTrace();
+			fail(e.getMessage());
+		}
+	}
+}
diff --git a/flink-compiler/src/test/java/org/apache/flink/compiler/dataexchange/DataExchangeModeOpenBranchingTest.java b/flink-compiler/src/test/java/org/apache/flink/compiler/dataexchange/DataExchangeModeOpenBranchingTest.java
new file mode 100644
index 00000000000..e4e0cba4bc1
--- /dev/null
+++ b/flink-compiler/src/test/java/org/apache/flink/compiler/dataexchange/DataExchangeModeOpenBranchingTest.java
@@ -0,0 +1,182 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.compiler.dataexchange;
+
+import org.apache.flink.api.common.ExecutionMode;
+import org.apache.flink.api.common.functions.FilterFunction;
+import org.apache.flink.api.common.functions.MapFunction;
+import org.apache.flink.api.java.DataSet;
+import org.apache.flink.api.java.ExecutionEnvironment;
+import org.apache.flink.api.java.io.DiscardingOutputFormat;
+import org.apache.flink.api.java.tuple.Tuple2;
+import org.apache.flink.compiler.CompilerTestBase;
+import org.apache.flink.compiler.plan.DualInputPlanNode;
+import org.apache.flink.compiler.plan.OptimizedPlan;
+import org.apache.flink.compiler.plan.SingleInputPlanNode;
+import org.apache.flink.compiler.plan.SinkPlanNode;
+import org.apache.flink.runtime.io.network.DataExchangeMode;
+import org.junit.Test;
+
+import java.util.Collection;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.fail;
+
+/**
+ * This test checks the correct assignment of the DataExchangeMode to
+ * connections for programs that branch, but do not re-join the branches.
+ *
+ * <pre>
+ *                      /---> (filter) -> (sink)
+ *                     /
+ *                    /
+ * (source) -> (map) -----------------\
+ *                    \               (join) -> (sink)
+ *                     \   (source) --/
+ *                      \
+ *                       \
+ *                        \-> (sink)
+ * </pre>
+ */
+@SuppressWarnings({"serial", "unchecked"})
+public class DataExchangeModeOpenBranchingTest extends CompilerTestBase {
+
+	@Test
+	public void testPipelinedForced() {
+		// PIPELINED_FORCED should result in pipelining all the way
+		verifyBranchigPlan(ExecutionMode.PIPELINED_FORCED,
+				DataExchangeMode.PIPELINED, DataExchangeMode.PIPELINED,
+				DataExchangeMode.PIPELINED, DataExchangeMode.PIPELINED,
+				DataExchangeMode.PIPELINED, DataExchangeMode.PIPELINED,
+				DataExchangeMode.PIPELINED);
+	}
+
+	@Test
+	public void testPipelined() {
+		// PIPELINED should result in pipelining all the way
+		verifyBranchigPlan(ExecutionMode.PIPELINED,
+				DataExchangeMode.PIPELINED, DataExchangeMode.PIPELINED,
+				DataExchangeMode.PIPELINED, DataExchangeMode.PIPELINED,
+				DataExchangeMode.PIPELINED, DataExchangeMode.PIPELINED,
+				DataExchangeMode.PIPELINED);
+	}
+
+	@Test
+	public void testBatch() {
+		// BATCH should result in batching the shuffle all the way
+		verifyBranchigPlan(ExecutionMode.BATCH,
+				DataExchangeMode.PIPELINED, DataExchangeMode.PIPELINED,
+				DataExchangeMode.PIPELINED, DataExchangeMode.BATCH,
+				DataExchangeMode.BATCH, DataExchangeMode.PIPELINED,
+				DataExchangeMode.PIPELINED);
+	}
+
+	@Test
+	public void testBatchForced() {
+		// BATCH_FORCED should result in batching all the way
+		verifyBranchigPlan(ExecutionMode.BATCH_FORCED,
+				DataExchangeMode.BATCH, DataExchangeMode.BATCH,
+				DataExchangeMode.BATCH, DataExchangeMode.BATCH,
+				DataExchangeMode.BATCH, DataExchangeMode.BATCH,
+				DataExchangeMode.BATCH);
+	}
+
+	private void verifyBranchigPlan(ExecutionMode execMode,
+									DataExchangeMode toMap,
+									DataExchangeMode toFilter,
+									DataExchangeMode toFilterSink,
+									DataExchangeMode toJoin1,
+									DataExchangeMode toJoin2,
+									DataExchangeMode toJoinSink,
+									DataExchangeMode toDirectSink)
+	{
+		try {
+			ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
+			env.getConfig().setExecutionMode(execMode);
+
+			DataSet<Tuple2<Long, Long>> data = env.generateSequence(1, 100000)
+					.map(new MapFunction<Long, Tuple2<Long, Long>>() {
+						@Override
+						public Tuple2<Long, Long> map(Long value) {
+							return new Tuple2<Long, Long>(value, value);
+						}
+					});
+
+			// output 1
+			data
+					.filter(new FilterFunction<Tuple2<Long, Long>>() {
+						@Override
+						public boolean filter(Tuple2<Long, Long> value) {
+							return false;
+						}
+					})
+					.output(new DiscardingOutputFormat<Tuple2<Long, Long>>()).name("sink1");
+
+			// output 2 does a join before a join
+			data
+					.join(env.fromElements(new Tuple2<Long, Long>(1L, 2L)))
+					.where(1)
+					.equalTo(0)
+					.output(new DiscardingOutputFormat<Tuple2<Tuple2<Long, Long>, Tuple2<Long, Long>>>()).name("sink2");
+
+			// output 3 is direct
+			data
+					.output(new DiscardingOutputFormat<Tuple2<Long, Long>>()).name("sink3");
+
+			OptimizedPlan optPlan = compileNoStats(env.createProgramPlan());
+
+			SinkPlanNode filterSink = findSink(optPlan.getDataSinks(), "sink1");
+			SinkPlanNode joinSink = findSink(optPlan.getDataSinks(), "sink2");
+			SinkPlanNode directSink = findSink(optPlan.getDataSinks(), "sink3");
+
+			SingleInputPlanNode filterNode = (SingleInputPlanNode) filterSink.getPredecessor();
+			SingleInputPlanNode mapNode = (SingleInputPlanNode) filterNode.getPredecessor();
+
+			DualInputPlanNode joinNode = (DualInputPlanNode) joinSink.getPredecessor();
+			assertEquals(mapNode, joinNode.getInput1().getSource());
+
+			assertEquals(mapNode, directSink.getPredecessor());
+
+			assertEquals(toFilterSink, filterSink.getInput().getDataExchangeMode());
+			assertEquals(toJoinSink, joinSink.getInput().getDataExchangeMode());
+			assertEquals(toDirectSink, directSink.getInput().getDataExchangeMode());
+
+			assertEquals(toMap, mapNode.getInput().getDataExchangeMode());
+			assertEquals(toFilter, filterNode.getInput().getDataExchangeMode());
+
+			assertEquals(toJoin1, joinNode.getInput1().getDataExchangeMode());
+			assertEquals(toJoin2, joinNode.getInput2().getDataExchangeMode());
+		}
+		catch (Exception e) {
+			e.printStackTrace();
+			fail(e.getMessage());
+		}
+	}
+
+	private SinkPlanNode findSink(Collection<SinkPlanNode> collection, String name) {
+		for (SinkPlanNode node : collection) {
+			String nodeName = node.getOptimizerNode().getPactContract().getName();
+			if (nodeName != null && nodeName.equals(name)) {
+				return node;
+			}
+		}
+
+		throw new IllegalArgumentException("No node with that name was found.");
+	}
+}
diff --git a/flink-compiler/src/test/java/org/apache/flink/compiler/dataexchange/PipelineBreakingTest.java b/flink-compiler/src/test/java/org/apache/flink/compiler/dataexchange/PipelineBreakingTest.java
new file mode 100644
index 00000000000..f6a77c6f68d
--- /dev/null
+++ b/flink-compiler/src/test/java/org/apache/flink/compiler/dataexchange/PipelineBreakingTest.java
@@ -0,0 +1,320 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.compiler.dataexchange;
+
+import org.apache.flink.api.common.Plan;
+import org.apache.flink.api.common.functions.FilterFunction;
+import org.apache.flink.api.common.functions.MapFunction;
+import org.apache.flink.api.java.DataSet;
+import org.apache.flink.api.java.ExecutionEnvironment;
+import org.apache.flink.api.java.io.DiscardingOutputFormat;
+import org.apache.flink.api.java.tuple.Tuple2;
+import org.apache.flink.compiler.PactCompiler;
+import org.apache.flink.compiler.dag.DataSinkNode;
+import org.apache.flink.compiler.dag.OptimizerNode;
+import org.apache.flink.compiler.dag.SingleInputNode;
+import org.apache.flink.compiler.dag.SinkJoiner;
+import org.apache.flink.compiler.dag.TwoInputNode;
+import org.apache.flink.compiler.testfunctions.DummyCoGroupFunction;
+import org.apache.flink.compiler.testfunctions.DummyFlatJoinFunction;
+import org.apache.flink.compiler.testfunctions.IdentityFlatMapper;
+import org.apache.flink.compiler.testfunctions.IdentityKeyExtractor;
+import org.apache.flink.compiler.testfunctions.SelectOneReducer;
+import org.apache.flink.compiler.testfunctions.Top1GroupReducer;
+import org.junit.Test;
+
+import java.util.Iterator;
+import java.util.List;
+
+import static org.junit.Assert.*;
+
+/**
+ * This test checks whether connections are correctly marked as pipelined breaking.
+ */
+@SuppressWarnings("serial")
+public class PipelineBreakingTest {
+
+	/**
+	 * Tests that no pipeline breakers are inserted into a simple forward
+	 * pipeline.
+	 *
+	 * <pre>
+	 *     (source) -> (map) -> (filter) -> (groupBy / reduce)
+	 * </pre>
+	 */
+	@Test
+	public void testSimpleForwardPlan() {
+		try {
+			ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
+
+			DataSet<String> dataSet = env.readTextFile("/never/accessed");
+			dataSet
+				.map(new MapFunction<String, Integer>() {
+					@Override
+					public Integer map(String value) {
+						return 0;
+					}
+				})
+				.filter(new FilterFunction<Integer>() {
+					@Override
+					public boolean filter(Integer value) {
+						return false;
+					}
+				})
+				.groupBy(new IdentityKeyExtractor<Integer>())
+				.reduceGroup(new Top1GroupReducer<Integer>())
+				.output(new DiscardingOutputFormat<Integer>());
+
+			DataSinkNode sinkNode = convertPlan(env.createProgramPlan()).get(0);
+
+			SingleInputNode reduceNode = (SingleInputNode) sinkNode.getPredecessorNode();
+			SingleInputNode keyExtractorNode = (SingleInputNode) reduceNode.getPredecessorNode();
+
+			SingleInputNode filterNode = (SingleInputNode) keyExtractorNode.getPredecessorNode();
+			SingleInputNode mapNode = (SingleInputNode) filterNode.getPredecessorNode();
+
+			assertFalse(sinkNode.getInputConnection().isBreakingPipeline());
+			assertFalse(reduceNode.getIncomingConnection().isBreakingPipeline());
+			assertFalse(keyExtractorNode.getIncomingConnection().isBreakingPipeline());
+			assertFalse(filterNode.getIncomingConnection().isBreakingPipeline());
+			assertFalse(mapNode.getIncomingConnection().isBreakingPipeline());
+		}
+		catch (Exception e) {
+			e.printStackTrace();
+			fail(e.getMessage());
+		}
+	}
+
+	/**
+	 * Tests that branching plans, where the branches are not re-joined,
+	 * do not place pipeline breakers.
+	 * 
+	 * <pre>
+	 *                      /---> (filter) -> (sink)
+	 *                     /
+	 *                    /
+	 * (source) -> (map) -----------------\
+	 *                    \               (join) -> (sink)
+	 *                     \   (source) --/
+	 *                      \
+	 *                       \
+	 *                        \-> (sink)
+	 * </pre>
+	 */
+	@Test
+	public void testBranchingPlanNotReJoined() {
+		try {
+			ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
+
+			DataSet<Integer> data = env.readTextFile("/never/accessed")
+				.map(new MapFunction<String, Integer>() {
+					@Override
+					public Integer map(String value) {
+						return 0;
+					}
+				});
+
+			// output 1
+			data
+				.filter(new FilterFunction<Integer>() {
+					@Override
+					public boolean filter(Integer value) {
+						return false;
+					}
+				})
+				.output(new DiscardingOutputFormat<Integer>());
+
+			// output 2 does a join before a join
+			data
+				.join(env.fromElements(1, 2, 3, 4))
+					.where(new IdentityKeyExtractor<Integer>())
+					.equalTo(new IdentityKeyExtractor<Integer>())
+				.output(new DiscardingOutputFormat<Tuple2<Integer, Integer>>());
+
+			// output 3 is direct
+			data
+				.output(new DiscardingOutputFormat<Integer>());
+
+			List<DataSinkNode> sinks = convertPlan(env.createProgramPlan());
+
+			// gather the optimizer DAG nodes
+
+			DataSinkNode sinkAfterFilter = sinks.get(0);
+			DataSinkNode sinkAfterJoin = sinks.get(1);
+			DataSinkNode sinkDirect = sinks.get(2);
+
+			SingleInputNode filterNode = (SingleInputNode) sinkAfterFilter.getPredecessorNode();
+			SingleInputNode mapNode = (SingleInputNode) filterNode.getPredecessorNode();
+
+			TwoInputNode joinNode = (TwoInputNode) sinkAfterJoin.getPredecessorNode();
+			SingleInputNode joinInput = (SingleInputNode) joinNode.getSecondPredecessorNode();
+
+			// verify the non-pipeline breaking status
+
+			assertFalse(sinkAfterFilter.getInputConnection().isBreakingPipeline());
+			assertFalse(sinkAfterJoin.getInputConnection().isBreakingPipeline());
+			assertFalse(sinkDirect.getInputConnection().isBreakingPipeline());
+
+			assertFalse(filterNode.getIncomingConnection().isBreakingPipeline());
+			assertFalse(mapNode.getIncomingConnection().isBreakingPipeline());
+
+			assertFalse(joinNode.getFirstIncomingConnection().isBreakingPipeline());
+			assertFalse(joinNode.getSecondIncomingConnection().isBreakingPipeline());
+			assertFalse(joinInput.getIncomingConnection().isBreakingPipeline());
+
+			// some other sanity checks on the plan construction (cannot hurt)
+
+			assertEquals(mapNode, ((SingleInputNode) joinNode.getFirstPredecessorNode()).getPredecessorNode());
+			assertEquals(mapNode, sinkDirect.getPredecessorNode());
+		}
+		catch (Exception e) {
+			e.printStackTrace();
+			fail(e.getMessage());
+		}
+	}
+
+	/**
+	 * Tests that branches that are re-joined have place pipeline breakers.
+	 * 
+	 * <pre>
+	 *                                         /-> (sink)
+	 *                                        /
+	 *                         /-> (reduce) -+          /-> (flatmap) -> (sink)
+	 *                        /               \        /
+	 *     (source) -> (map) -                (join) -+-----\
+	 *                        \               /              \
+	 *                         \-> (filter) -+                \
+	 *                                       \                (co group) -> (sink)
+	 *                                        \                /
+	 *                                         \-> (reduce) - /
+	 * </pre>
+	 */
+	@Test
+	public void testReJoinedBranches() {
+		try {
+			// build a test program
+
+			ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
+
+			DataSet<Tuple2<Long, Long>> data = env.fromElements(33L, 44L)
+					.map(new MapFunction<Long, Tuple2<Long, Long>>() {
+						@Override
+						public Tuple2<Long, Long> map(Long value) {
+							return new Tuple2<Long, Long>(value, value);
+						}
+					});
+
+			DataSet<Tuple2<Long, Long>> reduced = data.groupBy(0).reduce(new SelectOneReducer<Tuple2<Long, Long>>());
+			reduced.output(new DiscardingOutputFormat<Tuple2<Long, Long>>());
+			
+			DataSet<Tuple2<Long, Long>> filtered = data.filter(new FilterFunction<Tuple2<Long, Long>>() {
+				@Override
+				public boolean filter(Tuple2<Long, Long> value) throws Exception {
+					return false;
+				}
+			});
+			
+			DataSet<Tuple2<Long, Long>> joined = reduced.join(filtered)
+					.where(1).equalTo(1)
+					.with(new DummyFlatJoinFunction<Tuple2<Long, Long>>());
+			
+			joined.flatMap(new IdentityFlatMapper<Tuple2<Long, Long>>())
+					.output(new DiscardingOutputFormat<Tuple2<Long, Long>>());
+
+			joined.coGroup(filtered.groupBy(1).reduceGroup(new Top1GroupReducer<Tuple2<Long, Long>>()))
+					.where(0).equalTo(0)
+					.with(new DummyCoGroupFunction<Tuple2<Long, Long>, Tuple2<Long, Long>>())
+					.output(new DiscardingOutputFormat<Tuple2<Tuple2<Long, Long>, Tuple2<Long, Long>>>());
+
+			List<DataSinkNode> sinks = convertPlan(env.createProgramPlan());
+
+			// gather the optimizer DAG nodes
+
+			DataSinkNode sinkAfterReduce = sinks.get(0);
+			DataSinkNode sinkAfterFlatMap = sinks.get(1);
+			DataSinkNode sinkAfterCoGroup = sinks.get(2);
+
+			SingleInputNode reduceNode = (SingleInputNode) sinkAfterReduce.getPredecessorNode();
+			SingleInputNode mapNode = (SingleInputNode) reduceNode.getPredecessorNode();
+
+			SingleInputNode flatMapNode = (SingleInputNode) sinkAfterFlatMap.getPredecessorNode();
+			TwoInputNode joinNode = (TwoInputNode) flatMapNode.getPredecessorNode();
+			SingleInputNode filterNode = (SingleInputNode) joinNode.getSecondPredecessorNode();
+
+			TwoInputNode coGroupNode = (TwoInputNode) sinkAfterCoGroup.getPredecessorNode();
+			SingleInputNode otherReduceNode = (SingleInputNode) coGroupNode.getSecondPredecessorNode();
+
+			// test sanity checks (that we constructed the DAG correctly)
+
+			assertEquals(reduceNode, joinNode.getFirstPredecessorNode());
+			assertEquals(mapNode, filterNode.getPredecessorNode());
+			assertEquals(joinNode, coGroupNode.getFirstPredecessorNode());
+			assertEquals(filterNode, otherReduceNode.getPredecessorNode());
+
+			// verify the pipeline breaking status
+
+			assertFalse(sinkAfterReduce.getInputConnection().isBreakingPipeline());
+			assertFalse(sinkAfterFlatMap.getInputConnection().isBreakingPipeline());
+			assertFalse(sinkAfterCoGroup.getInputConnection().isBreakingPipeline());
+
+			assertFalse(mapNode.getIncomingConnection().isBreakingPipeline());
+			assertFalse(flatMapNode.getIncomingConnection().isBreakingPipeline());
+			assertFalse(joinNode.getFirstIncomingConnection().isBreakingPipeline());
+			assertFalse(coGroupNode.getFirstIncomingConnection().isBreakingPipeline());
+			assertFalse(coGroupNode.getSecondIncomingConnection().isBreakingPipeline());
+
+			// these should be pipeline breakers
+			assertTrue(reduceNode.getIncomingConnection().isBreakingPipeline());
+			assertTrue(filterNode.getIncomingConnection().isBreakingPipeline());
+			assertTrue(otherReduceNode.getIncomingConnection().isBreakingPipeline());
+			assertTrue(joinNode.getSecondIncomingConnection().isBreakingPipeline());
+		}
+		catch (Exception e) {
+			e.printStackTrace();
+			fail(e.getMessage());
+		}
+	}
+
+	private static List<DataSinkNode> convertPlan(Plan p) {
+		PactCompiler.GraphCreatingVisitor dagCreator =
+				new PactCompiler.GraphCreatingVisitor(17, p.getExecutionConfig().getExecutionMode());
+
+		// create the DAG
+		p.accept(dagCreator);
+		List<DataSinkNode> sinks = dagCreator.getSinks();
+
+		// build a single root and run the branch tracking logic
+		OptimizerNode rootNode;
+		if (sinks.size() == 1) {
+			rootNode = sinks.get(0);
+		}
+		else {
+			Iterator<DataSinkNode> iter = sinks.iterator();
+			rootNode = iter.next();
+
+			while (iter.hasNext()) {
+				rootNode = new SinkJoiner(rootNode, iter.next());
+			}
+		}
+		rootNode.accept(new PactCompiler.IdAndEstimatesVisitor(null));
+		rootNode.accept(new PactCompiler.BranchesVisitor());
+
+		return sinks;
+	}
+}
diff --git a/flink-compiler/src/test/java/org/apache/flink/compiler/testfunctions/IdentityFlatMapper.java b/flink-compiler/src/test/java/org/apache/flink/compiler/testfunctions/IdentityFlatMapper.java
new file mode 100644
index 00000000000..5fe32b43ddc
--- /dev/null
+++ b/flink-compiler/src/test/java/org/apache/flink/compiler/testfunctions/IdentityFlatMapper.java
@@ -0,0 +1,30 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.compiler.testfunctions;
+
+import org.apache.flink.api.common.functions.FlatMapFunction;
+import org.apache.flink.util.Collector;
+
+public class IdentityFlatMapper<T> implements FlatMapFunction<T, T> {
+
+	private static final long serialVersionUID = 1L;
+
+	@Override
+	public void flatMap(T value, Collector<T> out) {}
+}
diff --git a/flink-core/src/main/java/org/apache/flink/api/common/operators/Ordering.java b/flink-core/src/main/java/org/apache/flink/api/common/operators/Ordering.java
index e99cac7bbff..f75b7977439 100644
--- a/flink-core/src/main/java/org/apache/flink/api/common/operators/Ordering.java
+++ b/flink-core/src/main/java/org/apache/flink/api/common/operators/Ordering.java
@@ -16,7 +16,6 @@
  * limitations under the License.
  */
 
-
 package org.apache.flink.api.common.operators;
 
 import java.util.ArrayList;
@@ -26,9 +25,10 @@ import org.apache.flink.api.common.operators.util.FieldSet;
 import org.apache.flink.types.Key;
 
 /**
- *
+ * This class represents an ordering on a set of fields. It specifies the fields and order direction
+ * (ascending, descending).
  */
-public class Ordering {
+public class Ordering implements Cloneable {
 	
 	protected FieldList indexes = new FieldList();
 	
@@ -212,9 +212,6 @@ public class Ordering {
 	}
 	
 	// --------------------------------------------------------------------------------------------
-	
-	
-	
 
 	public Ordering clone() {
 		Ordering newOrdering = new Ordering();
@@ -223,7 +220,6 @@ public class Ordering {
 		newOrdering.orders.addAll(this.orders);
 		return newOrdering;
 	}
-	
 
 	@Override
 	public int hashCode() {
@@ -235,7 +231,6 @@ public class Ordering {
 		return result;
 	}
 
-
 	@Override
 	public boolean equals(Object obj) {
 		if (this == obj) {
@@ -273,7 +268,7 @@ public class Ordering {
 	}
 
 	public String toString() {
-		final StringBuffer buf = new StringBuffer("[");
+		final StringBuilder buf = new StringBuilder("[");
 		for (int i = 0; i < indexes.size(); i++) {
 			if (i != 0) {
 				buf.append(",");
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/DataExchangeMode.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/DataExchangeMode.java
new file mode 100644
index 00000000000..7810c3ed7ad
--- /dev/null
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/DataExchangeMode.java
@@ -0,0 +1,128 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.io.network;
+
+import org.apache.flink.api.common.ExecutionMode;
+import org.apache.flink.runtime.operators.shipping.ShipStrategyType;
+
+/**
+ * Defines how the data exchange between two specific operators happens.
+ */
+public enum DataExchangeMode {
+
+	/**
+	 * The data exchange is streamed, sender and receiver are online at the same time,
+	 * and the receiver back-pressures the sender.
+	 */
+	PIPELINED,
+
+	/**
+	 * The data exchange is decoupled. The sender first produces its entire result and finishes.
+	 * After that, the receiver is stated and may consume the data.
+	 */
+	BATCH,
+
+	/**
+	 * The data exchange starts like ine {@link #PIPELINED} and falls back to {@link #BATCH}
+	 * for recovery runs.
+	 */
+	PIPELINE_WITH_BATCH_FALLBACK;
+
+	// ------------------------------------------------------------------------
+
+	public static DataExchangeMode getForForwardExchange(ExecutionMode mode) {
+		return FORWARD[mode.ordinal()];
+	}
+
+	public static DataExchangeMode getForShuffleOrBroadcast(ExecutionMode mode) {
+		return SHUFFLE[mode.ordinal()];
+	}
+
+	public static DataExchangeMode getPipelineBreakingExchange(ExecutionMode mode) {
+		return BREAKING[mode.ordinal()];
+	}
+
+	/**
+	 * Computes the mode of data exchange to be used for a, given an execution mode and ship strategy.
+	 * The type of the data exchange depends also on whether this connection has been identified to require
+	 * pipeline breaking for deadlock avoidance.
+	 * <ul>
+	 *     <li>If the connection is set to be pipeline breaking, this returns the pipeline breaking variant
+	 *         of the execution mode
+	 *         {@link org.apache.flink.runtime.io.network.DataExchangeMode#getPipelineBreakingExchange(org.apache.flink.api.common.ExecutionMode)}.
+	 *     </li>
+	 *     <li>If the data exchange is a simple FORWARD (one-to-one communication), this returns
+	 *         {@link org.apache.flink.runtime.io.network.DataExchangeMode#getForForwardExchange(org.apache.flink.api.common.ExecutionMode)}.
+	 *     </li>
+	 *     <li>If otherwise, this returns
+	 *         {@link org.apache.flink.runtime.io.network.DataExchangeMode#getForShuffleOrBroadcast(org.apache.flink.api.common.ExecutionMode)}.
+	 *     </li>
+	 * </ul>
+	 *
+	 * @param shipStrategy The ship strategy (FORWARD, PARTITION, BROADCAST, ...) of the runtime data exchange.
+	 * @return The data exchange mode for the connection, given the concrete ship strategy.
+	 */
+	public static DataExchangeMode select(ExecutionMode executionMode, ShipStrategyType shipStrategy,
+													boolean breakPipeline) {
+
+		if (shipStrategy == null || shipStrategy == ShipStrategyType.NONE) {
+			throw new IllegalArgumentException("shipStrategy may not be null or NONE");
+		}
+		if (executionMode == null) {
+			throw new IllegalArgumentException("executionMode may not mbe null");
+		}
+
+		if (breakPipeline) {
+			return getPipelineBreakingExchange(executionMode);
+		}
+		else if (shipStrategy == ShipStrategyType.FORWARD) {
+			return getForForwardExchange(executionMode);
+		}
+		else {
+			return getForShuffleOrBroadcast(executionMode);
+		}
+	}
+
+	// ------------------------------------------------------------------------
+
+	private static final DataExchangeMode[] FORWARD = new DataExchangeMode[ExecutionMode.values().length];
+
+	private static final DataExchangeMode[] SHUFFLE = new DataExchangeMode[ExecutionMode.values().length];
+
+	private static final DataExchangeMode[] BREAKING = new DataExchangeMode[ExecutionMode.values().length];
+
+	// initialize the map between execution modes and exchange modes in
+	static {
+		FORWARD[ExecutionMode.PIPELINED_FORCED.ordinal()] = PIPELINED;
+		SHUFFLE[ExecutionMode.PIPELINED_FORCED.ordinal()] = PIPELINED;
+		BREAKING[ExecutionMode.PIPELINED_FORCED.ordinal()] = PIPELINED;
+
+		FORWARD[ExecutionMode.PIPELINED.ordinal()] = PIPELINED;
+		SHUFFLE[ExecutionMode.PIPELINED.ordinal()] = PIPELINED;
+		BREAKING[ExecutionMode.PIPELINED.ordinal()] = BATCH;
+
+		FORWARD[ExecutionMode.BATCH.ordinal()] = PIPELINED;
+		SHUFFLE[ExecutionMode.BATCH.ordinal()] = BATCH;
+		BREAKING[ExecutionMode.BATCH.ordinal()] = BATCH;
+
+		FORWARD[ExecutionMode.BATCH_FORCED.ordinal()] = BATCH;
+		SHUFFLE[ExecutionMode.BATCH_FORCED.ordinal()] = BATCH;
+		BREAKING[ExecutionMode.BATCH_FORCED.ordinal()] = BATCH;
+	}
+}
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/DataExchangeModeTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/DataExchangeModeTest.java
new file mode 100644
index 00000000000..cae80e864f0
--- /dev/null
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/DataExchangeModeTest.java
@@ -0,0 +1,51 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.io.network;
+
+import org.apache.flink.api.common.ExecutionMode;
+import org.junit.Test;
+
+import static org.junit.Assert.assertNotNull;
+
+/**
+ * This test verifies that the data exchange modes are defined for every execution mode.
+ */
+public class DataExchangeModeTest {
+
+	@Test
+	public void testForward() {
+		for (ExecutionMode mode : ExecutionMode.values()) {
+			assertNotNull(DataExchangeMode.getForForwardExchange(mode));
+		}
+	}
+
+	@Test
+	public void testShuffleAndBroadcast() {
+		for (ExecutionMode mode : ExecutionMode.values()) {
+			assertNotNull(DataExchangeMode.getForShuffleOrBroadcast(mode));
+		}
+	}
+
+	@Test
+	public void testPipelineBreaking() {
+		for (ExecutionMode mode : ExecutionMode.values()) {
+			assertNotNull(DataExchangeMode.getPipelineBreakingExchange(mode));
+		}
+	}
+}
