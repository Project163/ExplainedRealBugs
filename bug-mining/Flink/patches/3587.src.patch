diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/FlinkHiveException.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/FlinkHiveException.java
index 6a4e2fd9d1c..7289dbdb9df 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/FlinkHiveException.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/FlinkHiveException.java
@@ -25,6 +25,10 @@ import org.apache.flink.annotation.PublicEvolving;
 @PublicEvolving
 public class FlinkHiveException extends RuntimeException {
 
+	public FlinkHiveException(String message) {
+		super(message);
+	}
+
 	public FlinkHiveException(Throwable cause) {
 		super(cause);
 	}
diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShim.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShim.java
index 6b235a6b141..4a8ff3e4a03 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShim.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShim.java
@@ -43,6 +43,8 @@ import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.thrift.TException;
 
+import javax.annotation.Nullable;
+
 import java.io.IOException;
 import java.io.Serializable;
 import java.time.LocalDate;
@@ -232,4 +234,9 @@ public interface HiveShim extends Serializable {
 	 * Converts a hive date instance to LocalDate which is expected by DataFormatConverter.
 	 */
 	LocalDate toFlinkDate(Object hiveDate);
+
+	/**
+	 * Converts a Hive primitive java object to corresponding Writable object.
+	 */
+	@Nullable Writable hivePrimitiveToWritable(@Nullable Object value);
 }
diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShimV100.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShimV100.java
index 2b96efc9a5d..cf290346f87 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShimV100.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShimV100.java
@@ -18,12 +18,12 @@
 
 package org.apache.flink.table.catalog.hive.client;
 
+import org.apache.flink.connectors.hive.FlinkHiveException;
 import org.apache.flink.table.api.constraints.UniqueConstraint;
 import org.apache.flink.table.catalog.exceptions.CatalogException;
 import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;
 import org.apache.flink.table.catalog.stats.CatalogColumnStatisticsDataDate;
 import org.apache.flink.table.functions.hive.FlinkHiveUDFException;
-import org.apache.flink.table.functions.hive.conversion.HiveInspectors;
 import org.apache.flink.util.Preconditions;
 
 import org.apache.hadoop.conf.Configuration;
@@ -32,6 +32,9 @@ import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.common.FileUtils;
 import org.apache.hadoop.hive.common.HiveStatsUtils;
+import org.apache.hadoop.hive.common.type.HiveChar;
+import org.apache.hadoop.hive.common.type.HiveDecimal;
+import org.apache.hadoop.hive.common.type.HiveVarchar;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;
 import org.apache.hadoop.hive.metastore.IMetaStoreClient;
@@ -49,6 +52,14 @@ import org.apache.hadoop.hive.ql.io.HiveFileFormatUtils;
 import org.apache.hadoop.hive.ql.io.HiveOutputFormat;
 import org.apache.hadoop.hive.ql.udf.generic.SimpleGenericUDAFParameterInfo;
 import org.apache.hadoop.hive.serde2.Deserializer;
+import org.apache.hadoop.hive.serde2.io.ByteWritable;
+import org.apache.hadoop.hive.serde2.io.DateWritable;
+import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.HiveCharWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
+import org.apache.hadoop.hive.serde2.io.HiveVarcharWritable;
+import org.apache.hadoop.hive.serde2.io.ShortWritable;
+import org.apache.hadoop.hive.serde2.io.TimestampWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantBinaryObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantBooleanObjectInspector;
@@ -65,17 +76,27 @@ import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantS
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantStringObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantTimestampObjectInspector;
 import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo;
+import org.apache.hadoop.io.BooleanWritable;
+import org.apache.hadoop.io.BytesWritable;
+import org.apache.hadoop.io.FloatWritable;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.Reporter;
 import org.apache.thrift.TException;
 
+import javax.annotation.Nonnull;
+
 import java.io.IOException;
 import java.lang.reflect.Constructor;
 import java.lang.reflect.InvocationTargetException;
 import java.lang.reflect.Method;
+import java.math.BigDecimal;
 import java.sql.Date;
 import java.sql.Timestamp;
 import java.time.LocalDate;
@@ -213,7 +234,7 @@ public class HiveShimV100 implements HiveShim {
 	@Override
 	public ObjectInspector getObjectInspectorForConstant(PrimitiveTypeInfo primitiveTypeInfo, Object value) {
 		String className;
-		value = HiveInspectors.hivePrimitiveToWritable(value);
+		value = hivePrimitiveToWritable(value);
 		// Java constant object inspectors are not available until 1.2.0 -- https://issues.apache.org/jira/browse/HIVE-9766
 		// So we have to use writable constant object inspectors for 1.1.x
 		switch (primitiveTypeInfo.getPrimitiveCategory()) {
@@ -242,18 +263,20 @@ public class HiveShimV100 implements HiveShim {
 				className = WritableConstantStringObjectInspector.class.getName();
 				return HiveReflectionUtils.createConstantObjectInspector(className, value);
 			case CHAR:
-				className = WritableConstantHiveCharObjectInspector.class.getName();
 				try {
-					return (ObjectInspector) Class.forName(className).getDeclaredConstructor(
-							CharTypeInfo.class, value.getClass()).newInstance(primitiveTypeInfo, value);
+					Constructor<WritableConstantHiveCharObjectInspector> constructor =
+							WritableConstantHiveCharObjectInspector.class.getDeclaredConstructor(CharTypeInfo.class, value.getClass());
+					constructor.setAccessible(true);
+					return constructor.newInstance(primitiveTypeInfo, value);
 				} catch (Exception e) {
 					throw new FlinkHiveUDFException("Failed to create writable constant object inspector", e);
 				}
 			case VARCHAR:
-				className = WritableConstantHiveVarcharObjectInspector.class.getName();
 				try {
-					return (ObjectInspector) Class.forName(className).getDeclaredConstructor(
-							VarcharTypeInfo.class, value.getClass()).newInstance(primitiveTypeInfo, value);
+					Constructor<WritableConstantHiveVarcharObjectInspector> constructor =
+							WritableConstantHiveVarcharObjectInspector.class.getDeclaredConstructor(VarcharTypeInfo.class, value.getClass());
+					constructor.setAccessible(true);
+					return constructor.newInstance(primitiveTypeInfo, value);
 				} catch (Exception e) {
 					throw new FlinkHiveUDFException("Failed to create writable constant object inspector", e);
 				}
@@ -264,8 +287,14 @@ public class HiveShimV100 implements HiveShim {
 				className = WritableConstantTimestampObjectInspector.class.getName();
 				return HiveReflectionUtils.createConstantObjectInspector(className, value);
 			case DECIMAL:
-				className = WritableConstantHiveDecimalObjectInspector.class.getName();
-				return HiveReflectionUtils.createConstantObjectInspector(className, value);
+				try {
+					Constructor<WritableConstantHiveDecimalObjectInspector> constructor =
+							WritableConstantHiveDecimalObjectInspector.class.getDeclaredConstructor(DecimalTypeInfo.class, value.getClass());
+					constructor.setAccessible(true);
+					return constructor.newInstance(primitiveTypeInfo, value);
+				} catch (Exception e) {
+					throw new FlinkHiveUDFException("Failed to create writable constant object inspector", e);
+				}
 			case BINARY:
 				className = WritableConstantBinaryObjectInspector.class.getName();
 				return HiveReflectionUtils.createConstantObjectInspector(className, value);
@@ -380,6 +409,55 @@ public class HiveShimV100 implements HiveShim {
 		return ((Date) hiveDate).toLocalDate();
 	}
 
+	@Override
+	public Writable hivePrimitiveToWritable(Object value) {
+		if (value == null) {
+			return null;
+		}
+		Optional<Writable> optional = javaToWritable(value);
+		return optional.orElseThrow(() -> new FlinkHiveException("Unsupported primitive java value of class " + value.getClass().getName()));
+	}
+
+	Optional<Writable> javaToWritable(@Nonnull Object value) {
+		Writable writable = null;
+		// in case value is already a Writable
+		if (value instanceof Writable) {
+			writable = (Writable) value;
+		} else if (value instanceof Boolean) {
+			writable = new BooleanWritable((Boolean) value);
+		} else if (value instanceof Byte) {
+			writable = new ByteWritable((Byte) value);
+		} else if (value instanceof Short) {
+			writable = new ShortWritable((Short) value);
+		} else if (value instanceof Integer) {
+			writable = new IntWritable((Integer) value);
+		} else if (value instanceof Long) {
+			writable = new LongWritable((Long) value);
+		} else if (value instanceof Float) {
+			writable = new FloatWritable((Float) value);
+		} else if (value instanceof Double) {
+			writable = new DoubleWritable((Double) value);
+		} else if (value instanceof String) {
+			writable = new Text((String) value);
+		} else if (value instanceof HiveChar) {
+			writable = new HiveCharWritable((HiveChar) value);
+		} else if (value instanceof HiveVarchar) {
+			writable = new HiveVarcharWritable((HiveVarchar) value);
+		} else if (value instanceof HiveDecimal) {
+			writable = new HiveDecimalWritable((HiveDecimal) value);
+		} else if (value instanceof Date) {
+			writable = new DateWritable((Date) value);
+		} else if (value instanceof Timestamp) {
+			writable = new TimestampWritable((Timestamp) value);
+		} else if (value instanceof BigDecimal) {
+			HiveDecimal hiveDecimal = HiveDecimal.create((BigDecimal) value);
+			writable = new HiveDecimalWritable(hiveDecimal);
+		} else if (value instanceof byte[]) {
+			writable = new BytesWritable((byte[]) value);
+		}
+		return Optional.ofNullable(writable);
+	}
+
 	void ensureSupportedFlinkTimestamp(Object flinkTimestamp) {
 		Preconditions.checkArgument(flinkTimestamp instanceof Timestamp || flinkTimestamp instanceof LocalDateTime,
 				"Only support converting %s or %s to Hive timestamp, but not %s",
diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShimV120.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShimV120.java
index 3074681d4ed..872621e7768 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShimV120.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShimV120.java
@@ -20,10 +20,8 @@ package org.apache.flink.table.catalog.hive.client;
 
 import org.apache.flink.connectors.hive.FlinkHiveException;
 import org.apache.flink.table.catalog.exceptions.CatalogException;
-import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;
 import org.apache.flink.table.catalog.stats.CatalogColumnStatisticsDataDate;
 import org.apache.flink.table.catalog.stats.Date;
-import org.apache.flink.table.functions.hive.FlinkHiveUDFException;
 
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.IMetaStoreClient;
@@ -35,8 +33,6 @@ import org.apache.hadoop.hive.metastore.api.Table;
 import org.apache.hadoop.hive.ql.exec.FunctionInfo;
 import org.apache.hadoop.hive.ql.exec.FunctionRegistry;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;
 import org.apache.thrift.TException;
 
 import java.lang.reflect.Constructor;
@@ -71,63 +67,6 @@ public class HiveShimV120 extends HiveShimV111 {
 		client.alter_table(databaseName, tableName, table);
 	}
 
-	@Override
-	public ObjectInspector getObjectInspectorForConstant(PrimitiveTypeInfo primitiveTypeInfo, Object value) {
-		String className;
-		switch (primitiveTypeInfo.getPrimitiveCategory()) {
-			case BOOLEAN:
-				className = "org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaConstantBooleanObjectInspector";
-				return HiveReflectionUtils.createConstantObjectInspector(className, value);
-			case BYTE:
-				className = "org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaConstantByteObjectInspector";
-				return HiveReflectionUtils.createConstantObjectInspector(className, value);
-			case SHORT:
-				className = "org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaConstantShortObjectInspector";
-				return HiveReflectionUtils.createConstantObjectInspector(className, value);
-			case INT:
-				className = "org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaConstantIntObjectInspector";
-				return HiveReflectionUtils.createConstantObjectInspector(className, value);
-			case LONG:
-				className = "org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaConstantLongObjectInspector";
-				return HiveReflectionUtils.createConstantObjectInspector(className, value);
-			case FLOAT:
-				className = "org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaConstantFloatObjectInspector";
-				return HiveReflectionUtils.createConstantObjectInspector(className, value);
-			case DOUBLE:
-				className = "org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaConstantDoubleObjectInspector";
-				return HiveReflectionUtils.createConstantObjectInspector(className, value);
-			case STRING:
-				className = "org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaConstantStringObjectInspector";
-				return HiveReflectionUtils.createConstantObjectInspector(className, value);
-			case CHAR:
-				className = "org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaConstantHiveCharObjectInspector";
-				return HiveReflectionUtils.createConstantObjectInspector(className, value);
-			case VARCHAR:
-				className = "org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaConstantHiveVarcharObjectInspector";
-				return HiveReflectionUtils.createConstantObjectInspector(className, value);
-			case DATE:
-				className = "org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaConstantDateObjectInspector";
-				return HiveReflectionUtils.createConstantObjectInspector(className, value);
-			case TIMESTAMP:
-				className = "org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaConstantTimestampObjectInspector";
-				return HiveReflectionUtils.createConstantObjectInspector(className, value);
-			case DECIMAL:
-				className = "org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaConstantHiveDecimalObjectInspector";
-				return HiveReflectionUtils.createConstantObjectInspector(className, value);
-			case BINARY:
-				className = "org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaConstantBinaryObjectInspector";
-				return HiveReflectionUtils.createConstantObjectInspector(className, value);
-			case UNKNOWN:
-			case VOID:
-				// If type is null, we use the Java Constant String to replace
-				className = "org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaConstantStringObjectInspector";
-				return HiveReflectionUtils.createConstantObjectInspector(className, value.toString());
-			default:
-				throw new FlinkHiveUDFException(
-					String.format("Cannot find ConstantObjectInspector for %s", primitiveTypeInfo));
-		}
-	}
-
 	@Override
 	public ColumnStatisticsData toHiveDateColStats(CatalogColumnStatisticsDataDate flinkDateColStats) {
 		try {
diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShimV310.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShimV310.java
index 86c702f5d6f..7da558afd67 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShimV310.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShimV310.java
@@ -32,6 +32,7 @@ import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.IMetaStoreClient;
 import org.apache.hadoop.hive.metastore.RetryingMetaStoreClient;
 import org.apache.hadoop.hive.metastore.Warehouse;
+import org.apache.hadoop.io.Writable;
 
 import java.io.IOException;
 import java.lang.reflect.Constructor;
@@ -45,6 +46,7 @@ import java.time.LocalDateTime;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
+import java.util.Optional;
 import java.util.Set;
 
 /**
@@ -56,11 +58,13 @@ public class HiveShimV310 extends HiveShimV235 {
 	private static Class hiveTimestampClz;
 	private static Constructor hiveTimestampConstructor;
 	private static Field hiveTimestampLocalDateTime;
+	private static Constructor timestampWritableConstructor;
 
 	// date classes
 	private static Class hiveDateClz;
 	private static Constructor hiveDateConstructor;
 	private static Field hiveDateLocalDate;
+	private static Constructor dateWritableConstructor;
 
 	private static boolean hiveClassesInited;
 
@@ -74,12 +78,16 @@ public class HiveShimV310 extends HiveShimV235 {
 						hiveTimestampConstructor.setAccessible(true);
 						hiveTimestampLocalDateTime = hiveTimestampClz.getDeclaredField("localDateTime");
 						hiveTimestampLocalDateTime.setAccessible(true);
+						timestampWritableConstructor = Class.forName("org.apache.hadoop.hive.serde2.io.TimestampWritableV2")
+								.getDeclaredConstructor(hiveTimestampClz);
 
 						hiveDateClz = Class.forName("org.apache.hadoop.hive.common.type.Date");
 						hiveDateConstructor = hiveDateClz.getDeclaredConstructor(LocalDate.class);
 						hiveDateConstructor.setAccessible(true);
 						hiveDateLocalDate = hiveDateClz.getDeclaredField("localDate");
 						hiveDateLocalDate.setAccessible(true);
+						dateWritableConstructor = Class.forName("org.apache.hadoop.hive.serde2.io.DateWritableV2")
+								.getDeclaredConstructor(hiveDateClz);
 					} catch (ClassNotFoundException | NoSuchMethodException | NoSuchFieldException e) {
 						throw new FlinkHiveException("Failed to get Hive timestamp class and constructor", e);
 					}
@@ -233,4 +241,26 @@ public class HiveShimV310 extends HiveShimV235 {
 			throw new FlinkHiveException("Failed to convert to Flink date", e);
 		}
 	}
+
+	@Override
+	public Writable hivePrimitiveToWritable(Object value) {
+		if (value == null) {
+			return null;
+		}
+		Optional<Writable> optional = javaToWritable(value);
+		if (optional.isPresent()) {
+			return optional.get();
+		}
+		try {
+			if (getDateDataTypeClass().isInstance(value)) {
+				return (Writable) dateWritableConstructor.newInstance(value);
+			}
+			if (getTimestampDataTypeClass().isInstance(value)) {
+				return (Writable) timestampWritableConstructor.newInstance(value);
+			}
+		} catch (IllegalAccessException | InstantiationException | InvocationTargetException e) {
+			throw new FlinkHiveException("Failed to create writable objects", e);
+		}
+		throw new FlinkHiveException("Unsupported primitive java value of class " + value.getClass().getName());
+	}
 }
diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveReflectionUtils.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveReflectionUtils.java
index 401a750d29e..7b4093bb879 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveReflectionUtils.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveReflectionUtils.java
@@ -69,7 +69,7 @@ public class HiveReflectionUtils {
 			return (ObjectInspector) method.newInstance(value);
 		} catch (ClassNotFoundException | NoSuchMethodException | InstantiationException | IllegalAccessException
 				| InvocationTargetException e) {
-			throw new FlinkHiveUDFException("Failed to instantiate JavaConstantDateObjectInspector", e);
+			throw new FlinkHiveUDFException("Failed to instantiate java constant object inspector", e);
 		}
 	}
 
diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/functions/hive/conversion/HiveInspectors.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/functions/hive/conversion/HiveInspectors.java
index ec3407c1f36..fb93870cc50 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/functions/hive/conversion/HiveInspectors.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/functions/hive/conversion/HiveInspectors.java
@@ -80,12 +80,9 @@ import org.apache.hadoop.io.FloatWritable;
 import org.apache.hadoop.io.IntWritable;
 import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.Writable;
 
 import java.lang.reflect.Array;
 import java.math.BigDecimal;
-import java.sql.Date;
-import java.sql.Timestamp;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.List;
@@ -118,6 +115,8 @@ public class HiveInspectors {
 						HiveTypeUtil.toHiveTypeInfo(argTypes[i], false));
 			} else {
 				PrimitiveTypeInfo primitiveTypeInfo = (PrimitiveTypeInfo) HiveTypeUtil.toHiveTypeInfo(argTypes[i], false);
+				constant = getConversion(getObjectInspector(primitiveTypeInfo), argTypes[i].getLogicalType(), hiveShim)
+						.toHiveObject(constant);
 				argumentInspectors[i] = hiveShim.getObjectInspectorForConstant(primitiveTypeInfo, constant);
 			}
 		}
@@ -155,7 +154,7 @@ public class HiveInspectors {
 				throw new FlinkHiveUDFException("Unsupported primitive object inspector " + inspector.getClass().getName());
 			}
 			if (((PrimitiveObjectInspector) inspector).preferWritable()) {
-				conversion = new WritableHiveObjectConversion(conversion);
+				conversion = new WritableHiveObjectConversion(conversion, hiveShim);
 			}
 			return conversion;
 		}
@@ -406,47 +405,4 @@ public class HiveInspectors {
 				throw new CatalogException("Unsupported Hive type category " + type.getCategory());
 		}
 	}
-
-	/**
-	 * Converts a Hive primitive java object to corresponding Writable object.
-	 */
-	public static Writable hivePrimitiveToWritable(Object value) {
-		Writable writable;
-		// in case value is already a Writable
-		if (value instanceof Writable) {
-			writable = (Writable) value;
-		} else if (value instanceof Boolean) {
-			writable = new BooleanWritable((Boolean) value);
-		} else if (value instanceof Byte) {
-			writable = new ByteWritable((Byte) value);
-		} else if (value instanceof Short) {
-			writable = new ShortWritable((Short) value);
-		} else if (value instanceof Integer) {
-			writable = new IntWritable((Integer) value);
-		} else if (value instanceof Long) {
-			writable = new LongWritable((Long) value);
-		} else if (value instanceof Float) {
-			writable = new FloatWritable((Float) value);
-		} else if (value instanceof Double) {
-			writable = new DoubleWritable((Double) value);
-		} else if (value instanceof String) {
-			writable = new Text((String) value);
-		} else if (value instanceof HiveChar) {
-			writable = new HiveCharWritable((HiveChar) value);
-		} else if (value instanceof HiveVarchar) {
-			writable = new HiveVarcharWritable((HiveVarchar) value);
-		} else if (value instanceof Date) {
-			writable = new DateWritable((Date) value);
-		} else if (value instanceof Timestamp) {
-			writable = new TimestampWritable((Timestamp) value);
-		} else if (value instanceof BigDecimal) {
-			HiveDecimal hiveDecimal = HiveDecimal.create((BigDecimal) value);
-			writable = new HiveDecimalWritable(hiveDecimal);
-		} else if (value instanceof byte[]) {
-			writable = new BytesWritable((byte[]) value);
-		} else {
-			throw new CatalogException("Unsupported primitive java value of class " + value.getClass().getName());
-		}
-		return writable;
-	}
 }
diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/functions/hive/conversion/WritableHiveObjectConversion.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/functions/hive/conversion/WritableHiveObjectConversion.java
index eadd4d140d4..059dc36d4cf 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/functions/hive/conversion/WritableHiveObjectConversion.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/functions/hive/conversion/WritableHiveObjectConversion.java
@@ -18,6 +18,8 @@
 
 package org.apache.flink.table.functions.hive.conversion;
 
+import org.apache.flink.table.catalog.hive.client.HiveShim;
+
 /**
  * A HiveObjectConversion that converts Flink objects to Hive Writable objects.
  */
@@ -26,13 +28,15 @@ public class WritableHiveObjectConversion implements HiveObjectConversion {
 	private static final long serialVersionUID = 1L;
 
 	private final HiveObjectConversion flinkToJavaConversion;
+	private final HiveShim hiveShim;
 
-	WritableHiveObjectConversion(HiveObjectConversion flinkToJavaConversion) {
+	WritableHiveObjectConversion(HiveObjectConversion flinkToJavaConversion, HiveShim hiveShim) {
 		this.flinkToJavaConversion = flinkToJavaConversion;
+		this.hiveShim = hiveShim;
 	}
 
 	@Override
 	public Object toHiveObject(Object o) {
-		return HiveInspectors.hivePrimitiveToWritable(flinkToJavaConversion.toHiveObject(o));
+		return hiveShim.hivePrimitiveToWritable(flinkToJavaConversion.toHiveObject(o));
 	}
 }
diff --git a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/module/hive/HiveModuleTest.java b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/module/hive/HiveModuleTest.java
index cea529f1414..19d2f661ecd 100644
--- a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/module/hive/HiveModuleTest.java
+++ b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/module/hive/HiveModuleTest.java
@@ -18,16 +18,22 @@
 package org.apache.flink.table.module.hive;
 
 import org.apache.flink.table.api.DataTypes;
+import org.apache.flink.table.api.TableEnvironment;
+import org.apache.flink.table.api.TableUtils;
+import org.apache.flink.table.catalog.hive.HiveTestUtils;
 import org.apache.flink.table.catalog.hive.client.HiveShimLoader;
 import org.apache.flink.table.functions.FunctionDefinition;
 import org.apache.flink.table.functions.ScalarFunction;
 import org.apache.flink.table.functions.ScalarFunctionDefinition;
 import org.apache.flink.table.functions.hive.HiveSimpleUDF;
 import org.apache.flink.table.types.DataType;
+import org.apache.flink.types.Row;
 
 import org.junit.BeforeClass;
 import org.junit.Test;
 
+import java.util.List;
+
 import static org.apache.flink.table.HiveVersionTestUtil.HIVE_120_OR_LATER;
 import static org.apache.flink.table.catalog.hive.client.HiveShimLoader.HIVE_VERSION_V1_2_0;
 import static org.apache.flink.table.catalog.hive.client.HiveShimLoader.HIVE_VERSION_V2_0_0;
@@ -88,4 +94,28 @@ public class HiveModuleTest {
 	public void testNonExistFunction() {
 		assertFalse(new HiveModule(HiveShimLoader.getHiveVersion()).getFunctionDefinition("nonexist").isPresent());
 	}
+
+	@Test
+	public void testConstantArguments() throws Exception {
+		TableEnvironment tEnv = HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode();
+
+		tEnv.unloadModule("core");
+		tEnv.loadModule("hive", new HiveModule(HiveShimLoader.getHiveVersion()));
+
+		List<Row> results = TableUtils.collectToList(tEnv.sqlQuery("select concat('an', 'bn')"));
+		assertEquals("[anbn]", results.toString());
+
+		results = TableUtils.collectToList(tEnv.sqlQuery("select concat('ab', cast('cdefghi' as varchar(5)))"));
+		assertEquals("[abcdefg]", results.toString());
+
+		results = TableUtils.collectToList(tEnv.sqlQuery("select concat('ab',cast(12.34 as decimal(10,5)))"));
+		assertEquals("[ab12.34]", results.toString());
+
+		results = TableUtils.collectToList(tEnv.sqlQuery("select concat(cast('2018-01-19' as date),cast('2019-12-27 17:58:23.385' as timestamp))"));
+		assertEquals("[2018-01-192019-12-27 17:58:23.385]", results.toString());
+
+		// TODO: null cannot be a constant argument at the moment. This test will make more sense when that changes.
+		results = TableUtils.collectToList(tEnv.sqlQuery("select concat('ab',cast(null as int))"));
+		assertEquals("[null]", results.toString());
+	}
 }
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/GenerateUtils.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/GenerateUtils.scala
index 8fc89a6d4a0..fb32f58c5b8 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/GenerateUtils.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/GenerateUtils.scala
@@ -375,7 +375,7 @@ object GenerateUtils {
              |  $SQL_TIMESTAMP.fromEpochMillis(${ts.getMillisecond}L, ${ts.getNanoOfMillisecond});
            """.stripMargin
         ctx.addReusableMember(fieldTimestamp)
-        generateNonNullLiteral(literalType, fieldTerm, literalType)
+        generateNonNullLiteral(literalType, fieldTerm, ts)
 
       case TIMESTAMP_WITH_LOCAL_TIME_ZONE =>
         val fieldTerm = newName("timestampWithLocalZone")
