diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/PartitionPruner.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/PartitionPruner.scala
index c01896d8139..5018dc7283d 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/PartitionPruner.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/PartitionPruner.scala
@@ -22,15 +22,17 @@ import org.apache.flink.api.common.functions.util.ListCollector
 import org.apache.flink.api.common.functions.{MapFunction, RichMapFunction}
 import org.apache.flink.configuration.Configuration
 import org.apache.flink.table.api.{TableConfig, TableException}
-import org.apache.flink.table.dataformat.{BinaryString, Decimal, GenericRow}
+import org.apache.flink.table.dataformat.{BinaryString, Decimal, GenericRow, SqlTimestamp}
 import org.apache.flink.table.planner.codegen.CodeGenUtils.DEFAULT_COLLECTOR_TERM
 import org.apache.flink.table.planner.codegen.{ConstantCodeGeneratorContext, ExprCodeGenerator, FunctionCodeGenerator}
+import org.apache.flink.table.runtime.functions.SqlDateTimeUtils
 import org.apache.flink.table.runtime.typeutils.BaseRowTypeInfo
 import org.apache.flink.table.types.logical.LogicalTypeRoot._
 import org.apache.flink.table.types.logical.{BooleanType, DecimalType, LogicalType}
 
 import org.apache.calcite.rex.RexNode
 
+import java.time.ZoneId
 import java.util.{ArrayList => JArrayList, List => JList, Map => JMap}
 
 import scala.collection.JavaConversions._
@@ -45,7 +47,20 @@ object PartitionPruner {
 
   // current supports partition field type
   val supportedPartitionFieldTypes = Array(
-    VARCHAR, CHAR, BOOLEAN, TINYINT, SMALLINT, INTEGER, BIGINT, FLOAT, DOUBLE, DECIMAL
+    VARCHAR,
+    CHAR,
+    BOOLEAN,
+    TINYINT,
+    SMALLINT,
+    INTEGER,
+    BIGINT,
+    FLOAT,
+    DOUBLE,
+    DECIMAL,
+    DATE,
+    TIME_WITHOUT_TIME_ZONE,
+    TIMESTAMP_WITHOUT_TIME_ZONE,
+    TIMESTAMP_WITH_LOCAL_TIME_ZONE
   )
 
   /**
@@ -112,7 +127,8 @@ object PartitionPruner {
       richMapFunction.open(parameters)
       // do filter against all partitions
       allPartitions.foreach { partition =>
-        val row = convertPartitionToRow(partitionFieldNames, partitionFieldTypes, partition)
+        val row = convertPartitionToRow(
+          config.getLocalTimeZone, partitionFieldNames, partitionFieldTypes, partition)
         collector.collect(richMapFunction.map(row))
       }
     } finally {
@@ -122,47 +138,52 @@ object PartitionPruner {
     // get pruned partitions
     allPartitions.zipWithIndex.filter {
       case (_, index) => results.get(index)
-    }.unzip._1
+    }.map(_._1)
   }
 
   /**
     * create new Row from partition, set partition values to corresponding positions of row.
     */
   private def convertPartitionToRow(
+      timeZone: ZoneId,
       partitionFieldNames: Array[String],
       partitionFieldTypes: Array[LogicalType],
       partition: JMap[String, String]): GenericRow = {
     val row = new GenericRow(partitionFieldNames.length)
     partitionFieldNames.zip(partitionFieldTypes).zipWithIndex.foreach {
       case ((fieldName, fieldType), index) =>
-        val value = convertPartitionFieldValue(partition(fieldName), fieldType)
+        val value = convertPartitionFieldValue(timeZone, partition(fieldName), fieldType)
         row.setField(index, value)
     }
     row
   }
 
   private def convertPartitionFieldValue(
-      partitionFieldValue: String,
-      partitionFieldType: LogicalType): Any = {
-    partitionFieldValue match {
-      case null => null
+      timeZone: ZoneId,
+      v: String,
+      t: LogicalType): Any = {
+    if (v == null) {
+      return null
+    }
+    t.getTypeRoot match {
+      case VARCHAR | CHAR => BinaryString.fromString(v)
+      case BOOLEAN => Boolean
+      case TINYINT => v.toByte
+      case SMALLINT => v.toShort
+      case INTEGER => v.toInt
+      case BIGINT => v.toLong
+      case FLOAT => v.toFloat
+      case DOUBLE => v.toDouble
+      case DECIMAL =>
+        val decimalType = t.asInstanceOf[DecimalType]
+        Decimal.castFrom(v, decimalType.getPrecision, decimalType.getScale)
+      case DATE => SqlDateTimeUtils.dateStringToUnixDate(v)
+      case TIME_WITHOUT_TIME_ZONE => SqlDateTimeUtils.timeStringToUnixDate(v)
+      case TIMESTAMP_WITHOUT_TIME_ZONE => SqlDateTimeUtils.toSqlTimestamp(v)
+      case TIMESTAMP_WITH_LOCAL_TIME_ZONE => SqlTimestamp.fromInstant(
+        SqlDateTimeUtils.toSqlTimestamp(v).toLocalDateTime.atZone(timeZone).toInstant)
       case _ =>
-        val v = partitionFieldValue
-        partitionFieldType.getTypeRoot match {
-          case VARCHAR | CHAR => BinaryString.fromString(v)
-          case BOOLEAN => Boolean
-          case TINYINT => v.toByte
-          case SMALLINT => v.toShort
-          case INTEGER => v.toInt
-          case BIGINT => v.toLong
-          case FLOAT => v.toFloat
-          case DOUBLE => v.toDouble
-          case DECIMAL =>
-            val decimalType = partitionFieldType.asInstanceOf[DecimalType]
-            Decimal.castFrom(v, decimalType.getPrecision, decimalType.getScale)
-          case _ =>
-            throw new TableException(s"$partitionFieldType is not supported in PartitionPruner")
-        }
+        throw new TableException(s"$t is not supported in PartitionPruner")
     }
   }
 
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/utils/PartitionPrunerTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/utils/PartitionPrunerTest.scala
index 13ee3fe66f5..bdcdec0748e 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/utils/PartitionPrunerTest.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/utils/PartitionPrunerTest.scala
@@ -22,13 +22,18 @@ import org.apache.flink.table.functions.FunctionIdentifier
 import org.apache.flink.table.planner.expressions.utils.Func1
 import org.apache.flink.table.planner.functions.utils.ScalarSqlFunction
 
+import org.apache.calcite.rex.RexUtil
+import org.apache.calcite.sql.`type`.SqlTypeName
+import org.apache.calcite.sql.`type`.SqlTypeName.DATE
 import org.apache.calcite.sql.fun.SqlStdOperatorTable
+import org.apache.calcite.util.{DateString, TimeString, TimestampString}
 import org.junit.Assert.assertEquals
 import org.junit.Test
 
 import java.math.BigDecimal
 import java.util.{List => JList, Map => JMap}
 
+import scala.collection.JavaConversions._
 import scala.collection.JavaConverters._
 
 /**
@@ -114,4 +119,73 @@ class PartitionPrunerTest extends RexNodeTestBase {
     assertEquals("200", prunedPartitions.get(1).get("amount"))
   }
 
+  @Test
+  def testTimePrunePartitions(): Unit = {
+    val f0 = rexBuilder.makeInputRef(typeFactory.createSqlType(SqlTypeName.DATE), 0)
+    val f1 = rexBuilder.makeInputRef(typeFactory.createSqlType(SqlTypeName.TIME, 0), 1)
+    val f2 = rexBuilder.makeInputRef(typeFactory.createSqlType(SqlTypeName.TIMESTAMP, 3), 2)
+    val f3 = rexBuilder.makeInputRef(
+      typeFactory.createSqlType(SqlTypeName.TIMESTAMP_WITH_LOCAL_TIME_ZONE, 3), 3)
+
+    val c0 = rexBuilder.makeCall(
+      SqlStdOperatorTable.GREATER_THAN,
+      f0,
+      rexBuilder.makeDateLiteral(new DateString("2018-08-06")))
+
+    val c1 = rexBuilder.makeCall(
+      SqlStdOperatorTable.GREATER_THAN,
+      f1,
+      rexBuilder.makeTimeLiteral(new TimeString("12:08:06"), 0))
+
+    val c2 = rexBuilder.makeCall(
+      SqlStdOperatorTable.GREATER_THAN,
+      f2,
+      rexBuilder.makeTimestampLiteral(new TimestampString("2018-08-06 12:08:06.123"), 3))
+
+    val c3 = rexBuilder.makeCall(
+      SqlStdOperatorTable.GREATER_THAN,
+      f3,
+      rexBuilder.makeTimestampWithLocalTimeZoneLiteral(
+        new TimestampString("2018-08-06 12:08:06.123"), 3))
+
+    val condition = RexUtil.composeConjunction(rexBuilder, Seq(c0, c1, c2, c3))
+
+    val partitionFieldNames = Array("f0", "f1", "f2", "f3")
+    val partitionFieldTypes = Array(
+      DataTypes.DATE().getLogicalType,
+      DataTypes.TIME(0).getLogicalType,
+      DataTypes.TIMESTAMP(3).getLogicalType,
+      DataTypes.TIMESTAMP_WITH_LOCAL_TIME_ZONE(3).getLogicalType)
+
+    val allPartitions: JList[JMap[String, String]] = List(
+      Map(
+        "f0" -> "2018-08-05",
+        "f1" -> "12:08:07",
+        "f2" -> "2018-08-06 12:08:06.124",
+        "f3" -> "2018-08-06 12:08:06.124").asJava,
+      Map(
+        "f0" -> "2018-08-07",
+        "f1" -> "12:08:05",
+        "f2" -> "2018-08-06 12:08:06.124",
+        "f3" -> "2018-08-06 12:08:06.124").asJava,
+      Map(
+        "f0" -> "2018-08-07",
+        "f1" -> "12:08:07",
+        "f2" -> "2018-08-06 12:08:06.124",
+        "f3" -> "2018-08-06 12:08:06.124").asJava
+    ).asJava
+
+    val config = new TableConfig
+    val prunedPartitions = PartitionPruner.prunePartitions(
+      config,
+      partitionFieldNames,
+      partitionFieldTypes,
+      allPartitions,
+      condition
+    )
+
+    assertEquals(1, prunedPartitions.size())
+    assertEquals(allPartitions(2), prunedPartitions(0))
+  }
+
 }
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/utils/RexNodeExtractorTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/utils/RexNodeExtractorTest.scala
index 62c8e9f3e7f..28e057654a1 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/utils/RexNodeExtractorTest.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/utils/RexNodeExtractorTest.scala
@@ -798,15 +798,15 @@ class RexNodeExtractorTest extends RexNodeTestBase {
   }
 
   @Test
-  def testExtractPartitionPredicates_UnsupportedType(): Unit = {
+  def testExtractPartitionPredicatesDate(): Unit = {
     // amount
-    val t0 = rexBuilder.makeInputRef(allFieldTypes.get(2), 2)
+    val t0 = rexBuilder.makeInputRef(allFieldTypes.get(2), 1)
     // 100
     val t1 = rexBuilder.makeExactLiteral(BigDecimal.valueOf(100L))
     val c1 = rexBuilder.makeCall(SqlStdOperatorTable.GREATER_THAN, t0, t1)
     // date
     val t2 = rexBuilder.makeInputRef(
-      typeFactory.createFieldTypeFromLogicalType(DataTypes.DATE().getLogicalType), 1)
+      typeFactory.createFieldTypeFromLogicalType(DataTypes.DATE().getLogicalType), 0)
     // 2019-04-14
     val t3 = rexBuilder.makeDateLiteral(DateString.fromDaysSinceEpoch(18000))
     val c2 = rexBuilder.makeCall(SqlStdOperatorTable.EQUALS, t2, t3)
@@ -820,10 +820,9 @@ class RexNodeExtractorTest extends RexNodeTestBase {
         rexBuilder,
         Array("date")
       )
-    assertTrue(partitionPredicate1.isAlwaysTrue)
-    assertEquals(c3, nonPartitionPredicate1)
+    assertEquals(c2, partitionPredicate1)
+    assertEquals(c1, nonPartitionPredicate1)
 
-    // date is not supported
     val (partitionPredicate2, nonPartitionPredicate2) =
       RexNodeExtractor.extractPartitionPredicates(
         c3,
@@ -832,8 +831,8 @@ class RexNodeExtractorTest extends RexNodeTestBase {
         rexBuilder,
         Array("date", "amount")
       )
-    assertTrue(partitionPredicate2.isAlwaysTrue)
-    assertEquals(c3, nonPartitionPredicate2)
+    assertEquals(c3, partitionPredicate2)
+    assertTrue(nonPartitionPredicate2.isAlwaysTrue)
 
     val (partitionPredicate3, nonPartitionPredicate3) =
       RexNodeExtractor.extractPartitionPredicates(
