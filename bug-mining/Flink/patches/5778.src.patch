diff --git a/flink-connectors/flink-connector-kafka/pom.xml b/flink-connectors/flink-connector-kafka/pom.xml
index 3e5f95a4930..191fd14b1ba 100644
--- a/flink-connectors/flink-connector-kafka/pom.xml
+++ b/flink-connectors/flink-connector-kafka/pom.xml
@@ -161,14 +161,6 @@ under the License.
 			<scope>test</scope>
 		</dependency>
 
-		<dependency>
-			<groupId>org.apache.flink</groupId>
-			<artifactId>flink-table-runtime_${scala.binary.version}</artifactId>
-			<version>${project.version}</version>
-			<type>test-jar</type>
-			<scope>test</scope>
-		</dependency>
-
 		<!-- Kafka table factory testing -->
 		<dependency>
 			<groupId>org.apache.flink</groupId>
diff --git a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicSink.java b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicSink.java
index b7aa1420ab0..1fd9cafd07a 100644
--- a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicSink.java
+++ b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicSink.java
@@ -213,7 +213,6 @@ public class KafkaDynamicSink implements DynamicTableSink, SupportsWritingMetada
                                         getMetadataPositions(physicalChildren),
                                         upsertMode))
                         .build();
-
         if (flushMode.isEnabled() && upsertMode) {
             return (DataStreamSinkProvider)
                     dataStream -> {
@@ -222,18 +221,17 @@ public class KafkaDynamicSink implements DynamicTableSink, SupportsWritingMetada
                                         .getExecutionEnvironment()
                                         .getConfig()
                                         .isObjectReuseEnabled();
-                        final TypeSerializer<RowData> rowDataTypeSerializer =
-                                createRowDataTypeSerializer(
-                                        context, dataStream.getExecutionConfig());
                         final ReducingUpsertSink<?> sink =
                                 new ReducingUpsertSink<>(
                                         kafkaSink,
                                         physicalDataType,
                                         keyProjection,
                                         flushMode,
-                                        rowDataTypeSerializer,
                                         objectReuse
-                                                ? rowDataTypeSerializer::copy
+                                                ? createRowDataTypeSerializer(
+                                                                context,
+                                                                dataStream.getExecutionConfig())
+                                                        ::copy
                                                 : Function.identity());
                         final DataStreamSink<RowData> end = dataStream.sinkTo(sink);
                         if (parallelism != null) {
diff --git a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/ReducingUpsertSink.java b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/ReducingUpsertSink.java
index 0ce0a131b66..1cb723102f3 100644
--- a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/ReducingUpsertSink.java
+++ b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/ReducingUpsertSink.java
@@ -17,7 +17,6 @@
 
 package org.apache.flink.streaming.connectors.kafka.table;
 
-import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.api.connector.sink.Committer;
 import org.apache.flink.api.connector.sink.GlobalCommitter;
 import org.apache.flink.api.connector.sink.Sink;
@@ -27,13 +26,9 @@ import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.types.DataType;
 
 import java.io.IOException;
-import java.util.Collection;
 import java.util.List;
 import java.util.Optional;
 import java.util.function.Function;
-import java.util.stream.Collectors;
-
-import static org.apache.flink.util.Preconditions.checkNotNull;
 
 /**
  * A wrapper of a {@link Sink}. It will buffer the data emitted by the wrapper {@link SinkWriter}
@@ -42,14 +37,12 @@ import static org.apache.flink.util.Preconditions.checkNotNull;
  * <p>The sink provides eventual consistency guarantees without the need of a two-phase protocol
  * because the updates are idempotent therefore duplicates have no effect.
  */
-class ReducingUpsertSink<WriterState>
-        implements Sink<RowData, Void, ReducingUpsertWriterState<WriterState>, Void> {
+class ReducingUpsertSink<WriterState> implements Sink<RowData, Void, WriterState, Void> {
 
     private final Sink<RowData, ?, WriterState, ?> wrappedSink;
     private final DataType physicalDataType;
     private final int[] keyProjection;
     private final SinkBufferFlushMode bufferFlushMode;
-    private final TypeSerializer<RowData> rowDataTypeSerializer;
     private final Function<RowData, RowData> valueCopyFunction;
 
     ReducingUpsertSink(
@@ -57,27 +50,19 @@ class ReducingUpsertSink<WriterState>
             DataType physicalDataType,
             int[] keyProjection,
             SinkBufferFlushMode bufferFlushMode,
-            TypeSerializer<RowData> rowDataTypeSerializer,
             Function<RowData, RowData> valueCopyFunction) {
-        this.wrappedSink = checkNotNull(wrappedSink);
-        this.physicalDataType = checkNotNull(physicalDataType);
-        this.keyProjection = checkNotNull(keyProjection);
-        this.bufferFlushMode = checkNotNull(bufferFlushMode);
-        this.rowDataTypeSerializer = checkNotNull(rowDataTypeSerializer);
-        this.valueCopyFunction = checkNotNull(valueCopyFunction);
+        this.wrappedSink = wrappedSink;
+        this.physicalDataType = physicalDataType;
+        this.keyProjection = keyProjection;
+        this.bufferFlushMode = bufferFlushMode;
+        this.valueCopyFunction = valueCopyFunction;
     }
 
     @Override
-    public SinkWriter<RowData, Void, ReducingUpsertWriterState<WriterState>> createWriter(
-            InitContext context, List<ReducingUpsertWriterState<WriterState>> states)
-            throws IOException {
+    public SinkWriter<RowData, Void, WriterState> createWriter(
+            InitContext context, List<WriterState> states) throws IOException {
         final SinkWriter<RowData, ?, WriterState> wrapperWriter =
-                wrappedSink.createWriter(
-                        context,
-                        states.stream()
-                                .map(ReducingUpsertWriterState::getWrappedStates)
-                                .flatMap(Collection::stream)
-                                .collect(Collectors.toList()));
+                wrappedSink.createWriter(context, states);
         return new ReducingUpsertWriter<>(
                 wrapperWriter,
                 physicalDataType,
@@ -88,12 +73,8 @@ class ReducingUpsertSink<WriterState>
     }
 
     @Override
-    public Optional<SimpleVersionedSerializer<ReducingUpsertWriterState<WriterState>>>
-            getWriterStateSerializer() {
-        return Optional.of(
-                new ReducingUpsertWriterStateSerializer<>(
-                        rowDataTypeSerializer,
-                        wrappedSink.getWriterStateSerializer().orElse(null)));
+    public Optional<SimpleVersionedSerializer<WriterState>> getWriterStateSerializer() {
+        return wrappedSink.getWriterStateSerializer();
     }
 
     @Override
diff --git a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/ReducingUpsertWriter.java b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/ReducingUpsertWriter.java
index 3e5e78ac86e..aa178af1bdc 100644
--- a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/ReducingUpsertWriter.java
+++ b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/ReducingUpsertWriter.java
@@ -25,8 +25,6 @@ import org.apache.flink.table.types.DataType;
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.types.RowKind;
 
-import org.apache.flink.shaded.guava30.com.google.common.collect.ImmutableList;
-
 import java.io.IOException;
 import java.util.Arrays;
 import java.util.Collections;
@@ -41,8 +39,7 @@ import static org.apache.flink.types.RowKind.UPDATE_AFTER;
 import static org.apache.flink.util.Preconditions.checkArgument;
 import static org.apache.flink.util.Preconditions.checkNotNull;
 
-class ReducingUpsertWriter<WriterState>
-        implements SinkWriter<RowData, Void, ReducingUpsertWriterState<WriterState>> {
+class ReducingUpsertWriter<WriterState> implements SinkWriter<RowData, Void, WriterState> {
 
     private final SinkWriter<RowData, ?, WriterState> wrappedWriter;
     private final WrappedContext wrappedContext = new WrappedContext();
@@ -94,9 +91,8 @@ class ReducingUpsertWriter<WriterState>
     }
 
     @Override
-    public List<ReducingUpsertWriterState<WriterState>> snapshotState() throws IOException {
-        return ImmutableList.of(
-                new ReducingUpsertWriterState<>(reduceBuffer, wrappedWriter.snapshotState()));
+    public List<WriterState> snapshotState() throws IOException {
+        return wrappedWriter.snapshotState();
     }
 
     @Override
diff --git a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/ReducingUpsertWriterState.java b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/ReducingUpsertWriterState.java
deleted file mode 100644
index 79b841cc43f..00000000000
--- a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/ReducingUpsertWriterState.java
+++ /dev/null
@@ -1,68 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.streaming.connectors.kafka.table;
-
-import org.apache.flink.api.java.tuple.Tuple2;
-import org.apache.flink.table.data.RowData;
-
-import javax.annotation.Nullable;
-
-import java.util.List;
-import java.util.Map;
-import java.util.Objects;
-
-import static org.apache.flink.util.Preconditions.checkNotNull;
-
-class ReducingUpsertWriterState<WriterState> {
-
-    private final List<WriterState> wrappedStates;
-    private final Map<RowData, Tuple2<RowData, Long>> reduceBuffer;
-
-    ReducingUpsertWriterState(
-            Map<RowData, Tuple2<RowData, Long>> reduceBuffer,
-            @Nullable List<WriterState> wrappedStates) {
-        this.wrappedStates = wrappedStates;
-        this.reduceBuffer = checkNotNull(reduceBuffer);
-    }
-
-    public List<WriterState> getWrappedStates() {
-        return wrappedStates;
-    }
-
-    public Map<RowData, Tuple2<RowData, Long>> getReduceBuffer() {
-        return reduceBuffer;
-    }
-
-    @Override
-    public boolean equals(Object o) {
-        if (this == o) {
-            return true;
-        }
-        if (o == null || getClass() != o.getClass()) {
-            return false;
-        }
-        ReducingUpsertWriterState<?> that = (ReducingUpsertWriterState<?>) o;
-        return Objects.equals(wrappedStates, that.wrappedStates)
-                && Objects.equals(reduceBuffer, that.reduceBuffer);
-    }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(wrappedStates, reduceBuffer);
-    }
-}
diff --git a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/ReducingUpsertWriterStateSerializer.java b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/ReducingUpsertWriterStateSerializer.java
deleted file mode 100644
index fa961c5a3be..00000000000
--- a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/ReducingUpsertWriterStateSerializer.java
+++ /dev/null
@@ -1,99 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.streaming.connectors.kafka.table;
-
-import org.apache.flink.api.common.typeutils.TypeSerializer;
-import org.apache.flink.api.java.tuple.Tuple2;
-import org.apache.flink.core.io.SimpleVersionedSerializer;
-import org.apache.flink.core.memory.DataInputViewStreamWrapper;
-import org.apache.flink.core.memory.DataOutputViewStreamWrapper;
-import org.apache.flink.table.data.RowData;
-
-import javax.annotation.Nullable;
-
-import java.io.ByteArrayInputStream;
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import static org.apache.flink.core.io.SimpleVersionedSerialization.readVersionAndDeserializeList;
-import static org.apache.flink.core.io.SimpleVersionedSerialization.writeVersionAndSerializeList;
-import static org.apache.flink.util.Preconditions.checkNotNull;
-
-class ReducingUpsertWriterStateSerializer<WriterState>
-        implements SimpleVersionedSerializer<ReducingUpsertWriterState<WriterState>> {
-
-    private final TypeSerializer<RowData> rowDataTypeSerializer;
-    @Nullable private final SimpleVersionedSerializer<WriterState> wrappedStateSerializer;
-
-    ReducingUpsertWriterStateSerializer(
-            TypeSerializer<RowData> rowDataTypeSerializer,
-            @Nullable SimpleVersionedSerializer<WriterState> wrappedStateSerializer) {
-        this.wrappedStateSerializer = wrappedStateSerializer;
-        this.rowDataTypeSerializer = checkNotNull(rowDataTypeSerializer);
-    }
-
-    @Override
-    public int getVersion() {
-        return 1;
-    }
-
-    @Override
-    public byte[] serialize(ReducingUpsertWriterState<WriterState> state) throws IOException {
-        try (final ByteArrayOutputStream baos = new ByteArrayOutputStream();
-                final DataOutputViewStreamWrapper out = new DataOutputViewStreamWrapper(baos)) {
-            final List<WriterState> wrappedStates = state.getWrappedStates();
-            if (wrappedStateSerializer != null) {
-                writeVersionAndSerializeList(wrappedStateSerializer, wrappedStates, out);
-            }
-
-            final Map<RowData, Tuple2<RowData, Long>> reduceBuffer = state.getReduceBuffer();
-            out.writeInt(reduceBuffer.size());
-            for (final Map.Entry<RowData, Tuple2<RowData, Long>> entry : reduceBuffer.entrySet()) {
-                rowDataTypeSerializer.serialize(entry.getKey(), out);
-                rowDataTypeSerializer.serialize(entry.getValue().f0, out);
-                out.writeLong(entry.getValue().f1);
-            }
-            return baos.toByteArray();
-        }
-    }
-
-    @Override
-    public ReducingUpsertWriterState<WriterState> deserialize(int version, byte[] serialized)
-            throws IOException {
-        try (final ByteArrayInputStream bais = new ByteArrayInputStream(serialized);
-                final DataInputViewStreamWrapper in = new DataInputViewStreamWrapper(bais); ) {
-            List<WriterState> wrappedStates = null;
-            if (wrappedStateSerializer != null) {
-                wrappedStates = readVersionAndDeserializeList(wrappedStateSerializer, in);
-            }
-
-            final int reduceBufferSize = in.readInt();
-            final Map<RowData, Tuple2<RowData, Long>> reduceBuffer = new HashMap<>();
-            for (int ignored = 0; ignored < reduceBufferSize; ignored++) {
-                final RowData key = rowDataTypeSerializer.deserialize(in);
-                final RowData changed = rowDataTypeSerializer.deserialize(in);
-                final long timestamp = in.readLong();
-                reduceBuffer.put(key, new Tuple2<>(changed, timestamp));
-            }
-            return new ReducingUpsertWriterState<>(reduceBuffer, wrappedStates);
-        }
-    }
-}
diff --git a/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/ReducingUpsertWriterStateSerializerTest.java b/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/ReducingUpsertWriterStateSerializerTest.java
deleted file mode 100644
index f84504f2851..00000000000
--- a/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/ReducingUpsertWriterStateSerializerTest.java
+++ /dev/null
@@ -1,99 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.streaming.connectors.kafka.table;
-
-import org.apache.flink.api.java.tuple.Tuple2;
-import org.apache.flink.core.io.SimpleVersionedSerializer;
-import org.apache.flink.table.api.DataTypes;
-import org.apache.flink.table.runtime.typeutils.RowDataSerializer;
-
-import org.apache.flink.shaded.guava30.com.google.common.collect.ImmutableList;
-import org.apache.flink.shaded.guava30.com.google.common.collect.ImmutableMap;
-
-import org.junit.jupiter.params.ParameterizedTest;
-import org.junit.jupiter.params.provider.Arguments;
-import org.junit.jupiter.params.provider.MethodSource;
-
-import java.io.ByteArrayInputStream;
-import java.io.ByteArrayOutputStream;
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.util.List;
-
-import static org.apache.flink.table.runtime.util.StreamRecordUtils.binaryrow;
-import static org.junit.Assert.assertEquals;
-
-/** Tests for {@link ReducingUpsertWriterStateSerializer}. */
-public class ReducingUpsertWriterStateSerializerTest {
-
-    private static final int VERSION = 1;
-
-    @ParameterizedTest
-    @MethodSource("getArguments")
-    public void testSerDe(
-            List<Integer> wrappedState, SimpleVersionedSerializer<Integer> wrappedStateSerializer)
-            throws IOException {
-        final ReducingUpsertWriterState<Integer> state =
-                new ReducingUpsertWriterState<>(
-                        ImmutableMap.of(
-                                binaryrow("row1", 1), Tuple2.of(binaryrow("row1", 2), 42L),
-                                binaryrow("row2", 2), Tuple2.of(binaryrow("row2", 3), 43L)),
-                        wrappedState);
-        final ReducingUpsertWriterStateSerializer<Integer> serializer =
-                new ReducingUpsertWriterStateSerializer<>(
-                        new RowDataSerializer(
-                                DataTypes.STRING().getLogicalType(),
-                                DataTypes.INT().getLogicalType()),
-                        wrappedStateSerializer);
-        final ReducingUpsertWriterState<Integer> actual =
-                serializer.deserialize(VERSION, serializer.serialize(state));
-        assertEquals(state, actual);
-    }
-
-    private static List<Arguments> getArguments() {
-        return ImmutableList.of(
-                Arguments.of(null, null),
-                Arguments.of(ImmutableList.of(5, 4), new WrappedSerializer()));
-    }
-
-    private static class WrappedSerializer implements SimpleVersionedSerializer<Integer> {
-
-        @Override
-        public int getVersion() {
-            return 1;
-        }
-
-        @Override
-        public byte[] serialize(Integer obj) throws IOException {
-            try (final ByteArrayOutputStream baos = new ByteArrayOutputStream();
-                    final DataOutputStream out = new DataOutputStream(baos)) {
-                out.writeInt(obj);
-                return baos.toByteArray();
-            }
-        }
-
-        @Override
-        public Integer deserialize(int version, byte[] serialized) throws IOException {
-            try (final ByteArrayInputStream bais = new ByteArrayInputStream(serialized);
-                    final DataInputStream in = new DataInputStream(bais)) {
-                return in.readInt();
-            }
-        }
-    }
-}
diff --git a/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/ReducingUpsertWriterTest.java b/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/ReducingUpsertWriterTest.java
index b170616081b..a3181891f79 100644
--- a/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/ReducingUpsertWriterTest.java
+++ b/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/ReducingUpsertWriterTest.java
@@ -32,8 +32,6 @@ import org.apache.flink.table.runtime.connector.sink.SinkRuntimeProviderContext;
 import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.runtime.typeutils.RowDataSerializer;
 
-import org.apache.flink.shaded.guava30.com.google.common.collect.ImmutableList;
-
 import org.junit.Test;
 import org.junit.runner.RunWith;
 import org.junit.runners.Parameterized;
@@ -48,13 +46,10 @@ import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 import java.util.function.Function;
-import java.util.stream.Collectors;
 
 import static org.apache.flink.types.RowKind.DELETE;
 import static org.apache.flink.types.RowKind.INSERT;
 import static org.apache.flink.types.RowKind.UPDATE_AFTER;
-import static org.hamcrest.MatcherAssert.assertThat;
-import static org.hamcrest.Matchers.containsInAnyOrder;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertTrue;
 
@@ -267,40 +262,6 @@ public class ReducingUpsertWriterTest {
         compareCompactedResult(expected, writer.rowDataCollectors);
     }
 
-    @Test
-    public void testSnapshotReduceBuffer() throws Exception {
-        final MockedSinkWriter writer = new MockedSinkWriter();
-        final ReducingUpsertWriter<?> bufferedWriter = createBufferedWriter(writer);
-        final ImmutableList<RowData> bufferedData =
-                ImmutableList.<RowData>of(
-                        GenericRowData.ofKind(
-                                UPDATE_AFTER,
-                                1001,
-                                StringData.fromString("Java public for dummies"),
-                                StringData.fromString("Tan Ah Teck"),
-                                11.11,
-                                11,
-                                TimestampData.fromInstant(Instant.parse("2021-03-30T15:00:00Z"))),
-                        GenericRowData.ofKind(
-                                UPDATE_AFTER,
-                                1002,
-                                StringData.fromString("More Java for dummies"),
-                                StringData.fromString("Tan Ah Teck"),
-                                22.22,
-                                22,
-                                TimestampData.fromInstant(Instant.parse("2021-03-30T16:00:00Z"))));
-        writeData(bufferedWriter, bufferedData.iterator());
-
-        final List<? extends ReducingUpsertWriterState<?>> snapshots =
-                bufferedWriter.snapshotState();
-        assertEquals(1, snapshots.size());
-        assertThat(
-                snapshots.get(0).getReduceBuffer().values().stream()
-                        .map(t -> t.f0)
-                        .collect(Collectors.toList()),
-                containsInAnyOrder(bufferedData.get(0), bufferedData.get(1)));
-    }
-
     private void compareCompactedResult(
             Map<Integer, List<RowData>> expected, List<RowData> actual) {
         Map<Integer, List<RowData>> actualMap = new HashMap<>();
