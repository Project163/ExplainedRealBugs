diff --git a/flink-connectors/flink-connector-hive/pom.xml b/flink-connectors/flink-connector-hive/pom.xml
index d4164ed7195..8b38b977dff 100644
--- a/flink-connectors/flink-connector-hive/pom.xml
+++ b/flink-connectors/flink-connector-hive/pom.xml
@@ -963,7 +963,7 @@ under the License.
 				<dependency>
 					<groupId>org.apache.orc</groupId>
 					<artifactId>orc-core</artifactId>
-					<version>${hive.connector.orc.version}</version>
+					<version>${orc.version}</version>
 					<classifier>nohive</classifier>
 					<exclusions>
 						<exclusion>
@@ -991,7 +991,7 @@ under the License.
 				<dependency>
 					<groupId>org.apache.orc</groupId>
 					<artifactId>orc-core</artifactId>
-					<version>${hive.connector.orc.version}</version>
+					<version>${orc.version}</version>
 					<classifier>nohive</classifier>
 					<exclusions>
 						<exclusion>
@@ -1017,7 +1017,7 @@ under the License.
 				<dependency>
 					<groupId>org.apache.orc</groupId>
 					<artifactId>orc-core</artifactId>
-					<version>${hive.connector.orc.version}</version>
+					<version>${orc.version}</version>
 					<classifier>nohive</classifier>
 					<exclusions>
 						<exclusion>
@@ -1053,7 +1053,7 @@ under the License.
 				<dependency>
 					<groupId>org.apache.orc</groupId>
 					<artifactId>orc-core</artifactId>
-					<version>${hive.connector.orc.version}</version>
+					<version>${hive-2.2.0-orc-version}</version>
 					<exclusions>
 						<exclusion>
 							<groupId>org.apache.hadoop</groupId>
@@ -1068,6 +1068,8 @@ under the License.
 			<properties>
 				<hive.version>3.1.1</hive.version>
 				<derby.version>10.14.1.0</derby.version>
+                <!-- need a hadoop version that fixes HADOOP-14683 -->
+				<hivemetastore.hadoop.version>2.8.2</hivemetastore.hadoop.version>
 			</properties>
 
 			<dependencyManagement>
diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java
index 03f21df2515..2ffb79210be 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java
@@ -189,23 +189,24 @@ public class HiveTableSink implements AppendStreamTableSink, PartitionableTableS
 						conf.get(SINK_ROLLING_POLICY_FILE_SIZE).getBytes(),
 						conf.get(SINK_ROLLING_POLICY_ROLLOVER_INTERVAL).toMillis());
 
-				Optional<BulkWriter.Factory<RowData>> bulkFactory = createBulkWriterFactory(partitionColumns, sd);
 				BucketsBuilder<RowData, String, ? extends BucketsBuilder<RowData, ?, ?>> builder;
-				if (userMrWriter || !bulkFactory.isPresent()) {
-					HiveBulkWriterFactory hadoopBulkFactory = new HiveBulkWriterFactory(recordWriterFactory);
-					builder = new HadoopPathBasedBulkFormatBuilder<>(
-							new Path(sd.getLocation()), hadoopBulkFactory, jobConf, assigner)
-							.withRollingPolicy(rollingPolicy)
-							.withOutputFileConfig(outputFileConfig);
+				if (userMrWriter) {
+					builder = bucketsBuilderForMRWriter(recordWriterFactory, sd, assigner, rollingPolicy, outputFileConfig);
 					LOG.info("Hive streaming sink: Use MapReduce RecordWriter writer.");
 				} else {
-					builder = StreamingFileSink.forBulkFormat(
-							new org.apache.flink.core.fs.Path(sd.getLocation()),
-							new FileSystemTableSink.ProjectionBulkFactory(bulkFactory.get(), partComputer))
-							.withBucketAssigner(assigner)
-							.withRollingPolicy(rollingPolicy)
-							.withOutputFileConfig(outputFileConfig);
-					LOG.info("Hive streaming sink: Use native parquet&orc writer.");
+					Optional<BulkWriter.Factory<RowData>> bulkFactory = createBulkWriterFactory(partitionColumns, sd);
+					if (bulkFactory.isPresent()) {
+						builder = StreamingFileSink.forBulkFormat(
+								new org.apache.flink.core.fs.Path(sd.getLocation()),
+								new FileSystemTableSink.ProjectionBulkFactory(bulkFactory.get(), partComputer))
+								.withBucketAssigner(assigner)
+								.withRollingPolicy(rollingPolicy)
+								.withOutputFileConfig(outputFileConfig);
+						LOG.info("Hive streaming sink: Use native parquet&orc writer.");
+					} else {
+						builder = bucketsBuilderForMRWriter(recordWriterFactory, sd, assigner, rollingPolicy, outputFileConfig);
+						LOG.info("Hive streaming sink: Use MapReduce RecordWriter writer because BulkWriter Factory not available.");
+					}
 				}
 				return FileSystemTableSink.createStreamingSink(
 						conf,
@@ -230,6 +231,19 @@ public class HiveTableSink implements AppendStreamTableSink, PartitionableTableS
 		}
 	}
 
+	private BucketsBuilder<RowData, String, ? extends BucketsBuilder<RowData, ?, ?>> bucketsBuilderForMRWriter(
+			HiveWriterFactory recordWriterFactory,
+			StorageDescriptor sd,
+			TableBucketAssigner assigner,
+			TableRollingPolicy rollingPolicy,
+			OutputFileConfig outputFileConfig) {
+		HiveBulkWriterFactory hadoopBulkFactory = new HiveBulkWriterFactory(recordWriterFactory);
+		return new HadoopPathBasedBulkFormatBuilder<>(
+				new Path(sd.getLocation()), hadoopBulkFactory, jobConf, assigner)
+				.withRollingPolicy(rollingPolicy)
+				.withOutputFileConfig(outputFileConfig);
+	}
+
 	private Optional<BulkWriter.Factory<RowData>> createBulkWriterFactory(String[] partitionColumns,
 			StorageDescriptor sd) {
 		String serLib = sd.getSerdeInfo().getSerializationLib().toLowerCase();
diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/write/HiveBulkWriterFactory.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/write/HiveBulkWriterFactory.java
index 9a22d5662e6..1fb71937a9c 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/write/HiveBulkWriterFactory.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/write/HiveBulkWriterFactory.java
@@ -51,7 +51,8 @@ public class HiveBulkWriterFactory implements HadoopPathBasedBulkWriter.Factory<
 
 			@Override
 			public long getSize() throws IOException {
-				return fs.getFileStatus(inProgressPath).getLen();
+				// it's possible the in-progress file hasn't yet been created, due to writer lazy init or data buffering
+				return fs.exists(inProgressPath) ? fs.getFileStatus(inProgressPath).getLen() : 0;
 			}
 
 			@Override
diff --git a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSinkITCase.java b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSinkITCase.java
index 7592a33b7d9..d91624029b8 100644
--- a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSinkITCase.java
+++ b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSinkITCase.java
@@ -252,32 +252,48 @@ public class HiveTableSinkITCase {
 
 	@Test(timeout = 120000)
 	public void testDefaultSerPartStreamingWrite() throws Exception {
-		testStreamingWrite(true, false, true, this::checkSuccessFiles);
+		testStreamingWrite(true, false, "textfile", this::checkSuccessFiles);
 	}
 
 	@Test(timeout = 120000)
 	public void testPartStreamingWrite() throws Exception {
-		testStreamingWrite(true, false, false, this::checkSuccessFiles);
+		testStreamingWrite(true, false, "parquet", this::checkSuccessFiles);
+		// disable vector orc writer test for hive 2.x due to dependency conflict
+		if (!hiveCatalog.getHiveVersion().startsWith("2.")) {
+			testStreamingWrite(true, false, "orc", this::checkSuccessFiles);
+		}
 	}
 
 	@Test(timeout = 120000)
 	public void testNonPartStreamingWrite() throws Exception {
-		testStreamingWrite(false, false, false, (p) -> {});
+		testStreamingWrite(false, false, "parquet", (p) -> {});
+		// disable vector orc writer test for hive 2.x due to dependency conflict
+		if (!hiveCatalog.getHiveVersion().startsWith("2.")) {
+			testStreamingWrite(false, false, "orc", (p) -> {});
+		}
 	}
 
 	@Test(timeout = 120000)
 	public void testPartStreamingMrWrite() throws Exception {
-		testStreamingWrite(true, true, false, this::checkSuccessFiles);
+		testStreamingWrite(true, true, "parquet", this::checkSuccessFiles);
+		// doesn't support writer 2.0 orc table
+		if (!hiveCatalog.getHiveVersion().startsWith("2.0")) {
+			testStreamingWrite(true, true, "orc", this::checkSuccessFiles);
+		}
 	}
 
 	@Test(timeout = 120000)
 	public void testNonPartStreamingMrWrite() throws Exception {
-		testStreamingWrite(false, true, false, (p) -> {});
+		testStreamingWrite(false, true, "parquet", (p) -> {});
+		// doesn't support writer 2.0 orc table
+		if (!hiveCatalog.getHiveVersion().startsWith("2.0")) {
+			testStreamingWrite(false, true, "orc", (p) -> {});
+		}
 	}
 
 	@Test(timeout = 120000)
 	public void testStreamingAppend() throws Exception {
-		testStreamingWrite(false, false, false, (p) -> {
+		testStreamingWrite(false, false, "parquet", (p) -> {
 			StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
 			env.setParallelism(1);
 			StreamTableEnvironment tEnv = HiveTestUtils.createTableEnvWithBlinkPlannerStreamMode(env);
@@ -316,7 +332,7 @@ public class HiveTableSinkITCase {
 	private void testStreamingWrite(
 			boolean part,
 			boolean useMr,
-			boolean defaultSer,
+			String format,
 			Consumer<String> pathConsumer) throws Exception {
 		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
 		env.setParallelism(1);
@@ -355,7 +371,7 @@ public class HiveTableSinkITCase {
 					(part ? "" : ",d string,e string") +
 					") " +
 					(part ? "partitioned by (d string,e string) " : "") +
-					(defaultSer ? "" : " stored as parquet") +
+					" stored as " + format +
 					" TBLPROPERTIES (" +
 					"'" + PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN.key() + "'='$d $e:00:00'," +
 					"'" + SINK_PARTITION_COMMIT_DELAY.key() + "'='1h'," +
diff --git a/flink-connectors/flink-sql-connector-hive-1.2.2/pom.xml b/flink-connectors/flink-sql-connector-hive-1.2.2/pom.xml
index 3c9d9005352..67ee6a44dc0 100644
--- a/flink-connectors/flink-sql-connector-hive-1.2.2/pom.xml
+++ b/flink-connectors/flink-sql-connector-hive-1.2.2/pom.xml
@@ -87,7 +87,7 @@ under the License.
 		<dependency>
 			<groupId>org.apache.orc</groupId>
 			<artifactId>orc-core</artifactId>
-			<version>${hive.connector.orc.version}</version>
+			<version>${orc.version}</version>
 			<classifier>nohive</classifier>
 			<exclusions>
 				<exclusion>
@@ -106,6 +106,12 @@ under the License.
 			</exclusions>
 		</dependency>
 
+		<dependency>
+			<groupId>org.apache.orc</groupId>
+			<artifactId>orc-shims</artifactId>
+			<version>${orc.version}</version>
+		</dependency>
+
 		<dependency>
 			<groupId>io.airlift</groupId>
 			<artifactId>aircompressor</artifactId>
@@ -133,6 +139,7 @@ under the License.
 									<include>org.apache.hive:hive-metastore</include>
 									<include>org.apache.thrift:libfb303</include>
 									<include>org.apache.orc:orc-core</include>
+									<include>org.apache.orc:orc-shims</include>
 									<include>io.airlift:aircompressor</include>
 								</includes>
 							</artifactSet>
diff --git a/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE b/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE
index d12de924217..0f0cf1247bb 100644
--- a/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE
+++ b/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE
@@ -9,7 +9,8 @@ This project bundles the following dependencies under the Apache Software Licens
 - io.airlift:aircompressor:0.8
 - org.apache.hive:hive-exec:1.2.2
 - org.apache.hive:hive-metastore:1.2.2
-- org.apache.orc:orc-core:nohive:1.4.3
+- org.apache.orc:orc-core:nohive:1.5.6
+- org.apache.orc:orc-shims:1.5.6
 - org.apache.thrift:libfb303:0.9.2
 
 The bundled Apache Hive dependencies bundle the following dependencies under the Apache Software License 2.0 (http://www.apache.org/licenses/LICENSE-2.0.txt)
diff --git a/flink-connectors/flink-sql-connector-hive-2.2.0/pom.xml b/flink-connectors/flink-sql-connector-hive-2.2.0/pom.xml
index 3088e271377..d6ec76fe29a 100644
--- a/flink-connectors/flink-sql-connector-hive-2.2.0/pom.xml
+++ b/flink-connectors/flink-sql-connector-hive-2.2.0/pom.xml
@@ -65,7 +65,7 @@ under the License.
 		<dependency>
 			<groupId>org.apache.orc</groupId>
 			<artifactId>orc-core</artifactId>
-			<version>${hive.connector.orc.version}</version>
+			<version>${hive-2.2.0-orc-version}</version>
 			<exclusions>
 				<exclusion>
 					<groupId>log4j</groupId>
diff --git a/flink-formats/flink-orc-nohive/src/main/java/org/apache/flink/orc/nohive/OrcNoHiveBulkWriterFactory.java b/flink-formats/flink-orc-nohive/src/main/java/org/apache/flink/orc/nohive/OrcNoHiveBulkWriterFactory.java
index 0ec69c7f8f4..920f7cd8913 100644
--- a/flink-formats/flink-orc-nohive/src/main/java/org/apache/flink/orc/nohive/OrcNoHiveBulkWriterFactory.java
+++ b/flink-formats/flink-orc-nohive/src/main/java/org/apache/flink/orc/nohive/OrcNoHiveBulkWriterFactory.java
@@ -20,7 +20,7 @@ package org.apache.flink.orc.nohive;
 
 import org.apache.flink.api.common.serialization.BulkWriter;
 import org.apache.flink.core.fs.FSDataOutputStream;
-import org.apache.flink.orc.writer.PhysicalWriterImpl;
+import org.apache.flink.orc.nohive.writer.NoHivePhysicalWriterImpl;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.types.logical.DecimalType;
 import org.apache.flink.table.types.logical.LocalZonedTimestampType;
@@ -42,6 +42,8 @@ import org.apache.orc.storage.ql.exec.vector.TimestampColumnVector;
 import org.apache.orc.storage.ql.exec.vector.VectorizedRowBatch;
 
 import java.io.IOException;
+import java.io.ObjectInputStream;
+import java.io.ObjectOutputStream;
 import java.sql.Timestamp;
 import java.util.Properties;
 
@@ -50,9 +52,9 @@ import java.util.Properties;
  */
 public class OrcNoHiveBulkWriterFactory implements BulkWriter.Factory<RowData> {
 
-	private final Configuration conf;
-	private final String schema;
-	private final LogicalType[] fieldTypes;
+	private Configuration conf;
+	private String schema;
+	private LogicalType[] fieldTypes;
 
 	public OrcNoHiveBulkWriterFactory(Configuration conf, String schema, LogicalType[] fieldTypes) {
 		this.conf = conf;
@@ -65,7 +67,7 @@ public class OrcNoHiveBulkWriterFactory implements BulkWriter.Factory<RowData> {
 		OrcFile.WriterOptions opts = OrcFile.writerOptions(new Properties(), conf);
 		TypeDescription description = TypeDescription.fromString(schema);
 		opts.setSchema(description);
-		opts.physicalWriter(new PhysicalWriterImpl(out, opts));
+		opts.physicalWriter(new NoHivePhysicalWriterImpl(out, opts));
 		WriterImpl writer = new WriterImpl(null, new Path("."), opts);
 
 		VectorizedRowBatch rowBatch = description.createRowBatch();
@@ -98,6 +100,20 @@ public class OrcNoHiveBulkWriterFactory implements BulkWriter.Factory<RowData> {
 		};
 	}
 
+	// Custom serialization methods
+	private void writeObject(ObjectOutputStream out) throws IOException {
+		conf.write(out);
+		out.writeObject(schema);
+		out.writeObject(fieldTypes);
+	}
+
+	private void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {
+		conf = new Configuration(false);
+		conf.readFields(in);
+		schema = (String) in.readObject();
+		fieldTypes = (LogicalType[]) in.readObject();
+	}
+
 	private static void setColumn(
 			int rowId, ColumnVector column, LogicalType type, RowData row, int columnId) {
 		if (row.isNullAt(columnId)) {
diff --git a/flink-formats/flink-orc-nohive/src/main/java/org/apache/flink/orc/nohive/writer/NoHivePhysicalWriterImpl.java b/flink-formats/flink-orc-nohive/src/main/java/org/apache/flink/orc/nohive/writer/NoHivePhysicalWriterImpl.java
new file mode 100644
index 00000000000..b168cd51532
--- /dev/null
+++ b/flink-formats/flink-orc-nohive/src/main/java/org/apache/flink/orc/nohive/writer/NoHivePhysicalWriterImpl.java
@@ -0,0 +1,65 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.orc.nohive.writer;
+
+import org.apache.flink.core.fs.FSDataOutputStream;
+import org.apache.flink.orc.writer.PhysicalWriterImpl;
+
+import com.google.protobuf25.CodedOutputStream;
+import org.apache.orc.OrcFile;
+import org.apache.orc.OrcProto;
+
+import java.io.IOException;
+
+/**
+ * Protobuf is relocated in orc-core-nohive, therefore method calls involving PB classes need to use the relocated
+ * class names here.
+ */
+public class NoHivePhysicalWriterImpl extends PhysicalWriterImpl {
+
+	// relocated PB class in orc-core-nohive
+	private final CodedOutputStream noHiveProtobufWriter;
+
+	public NoHivePhysicalWriterImpl(FSDataOutputStream out, OrcFile.WriterOptions opts) throws IOException {
+		super(out, opts);
+		noHiveProtobufWriter = CodedOutputStream.newInstance(writer);
+	}
+
+	@Override
+	protected void writeMetadata(OrcProto.Metadata metadata) throws IOException {
+		metadata.writeTo(noHiveProtobufWriter);
+		noHiveProtobufWriter.flush();
+		writer.flush();
+	}
+
+	@Override
+	protected void writeFileFooter(OrcProto.Footer footer) throws IOException {
+		footer.writeTo(noHiveProtobufWriter);
+		noHiveProtobufWriter.flush();
+		writer.flush();
+	}
+
+	@Override
+	protected void writeStripeFooter(OrcProto.StripeFooter footer) throws IOException {
+		footer.writeTo(noHiveProtobufWriter);
+		noHiveProtobufWriter.flush();
+		writer.flush();
+	}
+
+}
diff --git a/flink-formats/flink-orc/src/main/java/org/apache/flink/orc/writer/PhysicalWriterImpl.java b/flink-formats/flink-orc/src/main/java/org/apache/flink/orc/writer/PhysicalWriterImpl.java
index 3558b998dd9..f6255a47cc9 100644
--- a/flink-formats/flink-orc/src/main/java/org/apache/flink/orc/writer/PhysicalWriterImpl.java
+++ b/flink-formats/flink-orc/src/main/java/org/apache/flink/orc/writer/PhysicalWriterImpl.java
@@ -61,7 +61,7 @@ public class PhysicalWriterImpl implements PhysicalWriter {
 	private static final byte[] ZEROS = new byte[64 * 1024];
 	private static final int HDFS_BUFFER_SIZE = 256 * 1024;
 
-	private final OutStream writer;
+	protected final OutStream writer;
 	private final CodedOutputStream protobufWriter;
 	private final CompressionKind compress;
 	private final Map<StreamName, BufferedStream> streams;
@@ -175,9 +175,7 @@ public class PhysicalWriterImpl implements PhysicalWriter {
 	public void writeFileMetadata(OrcProto.Metadata.Builder builder) throws IOException {
 		long startPosition = out.getPos();
 		OrcProto.Metadata metadata = builder.build();
-		metadata.writeTo(protobufWriter);
-		protobufWriter.flush();
-		writer.flush();
+		writeMetadata(metadata);
 		this.metadataLength = (int) (out.getPos() - startPosition);
 	}
 
@@ -188,9 +186,7 @@ public class PhysicalWriterImpl implements PhysicalWriter {
 		builder.setHeaderLength(headerLength);
 		long startPosition = out.getPos();
 		OrcProto.Footer footer = builder.build();
-		footer.writeTo(protobufWriter);
-		protobufWriter.flush();
-		writer.flush();
+		writeFileFooter(footer);
 		this.footerLength = (int) (out.getPos() - startPosition);
 	}
 
@@ -300,12 +296,28 @@ public class PhysicalWriterImpl implements PhysicalWriter {
 
 	private void writeStripeFooter(OrcProto.StripeFooter footer, long dataSize,
 									long indexSize, OrcProto.StripeInformation.Builder dirEntry) throws IOException {
+		writeStripeFooter(footer);
+
+		dirEntry.setOffset(stripeStart);
+		dirEntry.setFooterLength(out.getPos() - stripeStart - dataSize - indexSize);
+	}
+
+	protected void writeMetadata(OrcProto.Metadata metadata) throws IOException {
+		metadata.writeTo(protobufWriter);
+		protobufWriter.flush();
+		writer.flush();
+	}
+
+	protected void writeFileFooter(OrcProto.Footer footer) throws IOException {
 		footer.writeTo(protobufWriter);
 		protobufWriter.flush();
 		writer.flush();
+	}
 
-		dirEntry.setOffset(stripeStart);
-		dirEntry.setFooterLength(out.getPos() - stripeStart - dataSize - indexSize);
+	protected void writeStripeFooter(OrcProto.StripeFooter footer) throws IOException {
+		footer.writeTo(protobufWriter);
+		protobufWriter.flush();
+		writer.flush();
 	}
 
 	private static void writeZeros(OutputStream output, long remaining) throws IOException {
diff --git a/pom.xml b/pom.xml
index 703a7689e32..7beedf4b71f 100644
--- a/pom.xml
+++ b/pom.xml
@@ -147,7 +147,7 @@ under the License.
 		<minikdc.version>3.2.0</minikdc.version>
 		<generated.docs.dir>./docs/_includes/generated</generated.docs.dir>
 		<hive.version>2.3.4</hive.version>
-		<hive.connector.orc.version>1.4.3</hive.connector.orc.version>
+		<hive-2.2.0-orc-version>1.4.3</hive-2.2.0-orc-version>
 		<orc.version>1.5.6</orc.version>
 		<!--
 			Hive 2.3.4 relies on Hadoop 2.7.2 and later versions.
