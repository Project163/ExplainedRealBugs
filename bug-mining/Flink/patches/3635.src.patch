diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/stream/sql/ModifiedMonotonicityTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/stream/sql/ModifiedMonotonicityTest.scala
index fcca13666ee..68c50bbebd3 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/stream/sql/ModifiedMonotonicityTest.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/stream/sql/ModifiedMonotonicityTest.scala
@@ -29,7 +29,7 @@ import org.apache.flink.table.planner.utils.{StreamTableTestUtil, TableTestBase,
 
 import org.apache.calcite.sql.validate.SqlMonotonicity.{CONSTANT, DECREASING, INCREASING, NOT_MONOTONIC}
 import org.junit.Assert.assertEquals
-import org.junit.{Ignore, Test}
+import org.junit.Test
 
 class ModifiedMonotonicityTest extends TableTestBase {
 
@@ -95,9 +95,7 @@ class ModifiedMonotonicityTest extends TableTestBase {
     util.verifyPlanWithTrait(query)
   }
 
-  @Ignore
   @Test
-  // TODO remove ignore after window aggregate supported
   def testTumbleFunAndRegularAggFunInGroupBy(): Unit = {
     val sql = "SELECT b, d, weightedAvg(c, a) FROM " +
       " (SELECT a, b, c, count(*) d," +
@@ -108,9 +106,7 @@ class ModifiedMonotonicityTest extends TableTestBase {
     verifyMonotonicity(sql, new RelModifiedMonotonicity(Array(CONSTANT, CONSTANT, NOT_MONOTONIC)))
   }
 
-  @Ignore
   @Test
-  // TODO remove ignore after anti-join supported
   def testAntiJoin(): Unit = {
     val sql = "SELECT * FROM AA WHERE NOT EXISTS (SELECT b1 from BB WHERE a1 = b1)"
     verifyMonotonicity(sql, null)
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/CalcITCase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/CalcITCase.scala
index 1b633fc7625..afcb621657d 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/CalcITCase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/CalcITCase.scala
@@ -456,7 +456,6 @@ class CalcITCase extends BatchTestBase {
       ))
   }
 
-  @Ignore // TODO support agg
   @Test
   def testExternalTypeFunc2(): Unit = {
     registerFunction("func1", RowFunc)
@@ -605,7 +604,6 @@ class CalcITCase extends BatchTestBase {
       Seq(row("Hello"), row("Hello world")))
   }
 
-  @Ignore // TODO support substring
   @Test
   def testComplexNotInLargeValues(): Unit = {
     checkResult(
@@ -802,7 +800,6 @@ class CalcITCase extends BatchTestBase {
     )
   }
 
-  @Ignore //TODO support cast string to bigint.
   @Test
   def testSelectStarFromNestedValues2(): Unit = {
     val table = BatchTableEnvUtil.fromCollection(tEnv, Seq(
@@ -1077,7 +1074,6 @@ class CalcITCase extends BatchTestBase {
     * T[h]h:[m]m:[s]s.[ms][ms][ms][us][us][us]-[h]h:[m]m
     * T[h]h:[m]m:[s]s.[ms][ms][ms][us][us][us]+[h]h:[m]m
     */
-  @Ignore
   @Test
   def testTimestampCompareWithDateString(): Unit = {
     //j 2015-05-20 10:00:00.887
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/CorrelateITCase2.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/CorrelateITCase2.scala
index 877119dc5c5..961948d17f3 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/CorrelateITCase2.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/CorrelateITCase2.scala
@@ -25,7 +25,7 @@ import org.apache.flink.table.planner.runtime.utils.BatchTestBase._
 import org.apache.flink.table.planner.runtime.utils.JavaUserDefinedTableFunctions.StringSplit
 import org.apache.flink.table.planner.runtime.utils.TestData._
 
-import org.junit.{Before, Ignore, Test}
+import org.junit.{Before, Test}
 
 import scala.collection.Seq
 
@@ -87,7 +87,6 @@ class CorrelateITCase2 extends BatchTestBase {
     )
   }
 
-  @Ignore
   @Test
   def testConstantTableFunc2(): Unit = {
     registerFunction("str_split", new StringSplit())
@@ -121,7 +120,6 @@ class CorrelateITCase2 extends BatchTestBase {
       ))
   }
 
-  @Ignore // TODO substring
   @Test
   def testConstantTableFunc3(): Unit = {
     registerFunction("str_split", new StringSplit())
@@ -133,7 +131,6 @@ class CorrelateITCase2 extends BatchTestBase {
     )
   }
 
-  @Ignore
   @Test
   def testConstantTableFuncWithSubString(): Unit = {
     registerFunction("str_split", new StringSplit())
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/MiscITCase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/MiscITCase.scala
index 2f389ce9ed9..8f93ff1b116 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/MiscITCase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/MiscITCase.scala
@@ -317,7 +317,6 @@ class MiscITCase extends BatchTestBase {
     )
   }
 
-  @Ignore // TODO support lazy from source
   @Test
   def testExcept(): Unit = {
     checkQuery2(
@@ -376,7 +375,6 @@ class MiscITCase extends BatchTestBase {
     )
   }
 
-  @Ignore // TODO support lazy from source
   @Test
   def testIntersect(): Unit = {
     checkQuery(
@@ -512,7 +510,6 @@ class MiscITCase extends BatchTestBase {
     )
   }
 
-  @Ignore // TODO support lazy from source
   @Test
   def testCompareFunctionWithSubquery(): Unit = {
     checkResult("SELECT " +
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/agg/AggregateITCaseBase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/agg/AggregateITCaseBase.scala
index 6bf042c5f59..2732fed3613 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/agg/AggregateITCaseBase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/agg/AggregateITCaseBase.scala
@@ -28,7 +28,7 @@ import org.apache.flink.table.planner.runtime.utils.BatchTestBase.row
 import org.apache.flink.table.planner.runtime.utils.TestData._
 import org.apache.flink.types.Row
 
-import org.junit.{Before, Ignore, Test}
+import org.junit.{Before, Test}
 
 import scala.collection.Seq
 
@@ -362,10 +362,9 @@ abstract class AggregateITCaseBase(testName: String) extends BatchTestBase {
     )
   }
 
-  @Ignore
   @Test
   def testGroupByRegexp(): Unit = {
-    val expr = "regexp_extract(f0, '([a-z]+)\\\\[', 1)"
+    val expr = "regexp_extract(f0, '([a-z]+)\\[', 1)"
     checkQuery(
       Seq(("some[thing]", "random-string")),
       s"select $expr, count(*) from TableName group by $expr",
@@ -447,7 +446,6 @@ abstract class AggregateITCaseBase(testName: String) extends BatchTestBase {
     )
   }
 
-  @Ignore
   @Test
   def testGroupingInsideWindowFunction(): Unit = {
     checkQuery(
@@ -792,7 +790,6 @@ abstract class AggregateITCaseBase(testName: String) extends BatchTestBase {
   // NOTE: select from values -- supported by Spark, but not Blink
   //       "select sum(a) over () from values 1.0, 2.0, 3.0 T(a)"
 
-  @Ignore
   @Test
   def testDecimalSumAvgOverWindow(): Unit = {
     checkQuery(
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/agg/SortAggITCase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/agg/SortAggITCase.scala
index eed0d3eeb0f..62d90abbe52 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/agg/SortAggITCase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/agg/SortAggITCase.scala
@@ -29,7 +29,7 @@ import org.apache.flink.table.planner.runtime.utils.BatchTestBase.row
 import org.apache.flink.table.planner.runtime.utils.UserDefinedFunctionTestUtils.{MyPojo, MyToPojoFunc}
 import org.apache.flink.table.planner.utils.{CountAccumulator, CountAggFunction, IntSumAggFunction}
 
-import org.junit.{Ignore, Test}
+import org.junit.Test
 
 import java.lang
 import java.lang.{Iterable => JIterable}
@@ -71,8 +71,7 @@ class SortAggITCase
       "SELECT simplePrimitiveArrayUdaf(id) FROM RangeT",
       Seq(row(499999500000L)))
   }
-
-  @Ignore
+  
   @Test
   def testMultiSetAggBufferGroupBy(): Unit = {
     checkResult(
@@ -249,7 +248,6 @@ class SortAggITCase
     )
   }
 
-  @Ignore
   @Test
   def testFirstValueOnString(): Unit = {
     checkResult(
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/join/JoinITCase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/join/JoinITCase.scala
index 8268521c84e..662f77fc63c 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/join/JoinITCase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/join/JoinITCase.scala
@@ -22,22 +22,24 @@ import org.apache.flink.api.common.ExecutionConfig
 import org.apache.flink.api.common.typeinfo.BasicTypeInfo.{DOUBLE_TYPE_INFO, INT_TYPE_INFO, LONG_TYPE_INFO}
 import org.apache.flink.api.common.typeutils.TypeComparator
 import org.apache.flink.api.java.typeutils.{GenericTypeInfo, RowTypeInfo}
-import org.apache.flink.table.api.Types
-import org.apache.flink.table.api.config.ExecutionConfigOptions
 import org.apache.flink.table.planner.expressions.utils.FuncWithOpen
 import org.apache.flink.table.planner.runtime.batch.sql.join.JoinType.{BroadcastHashJoin, HashJoin, JoinType, NestedLoopJoin, SortMergeJoin}
 import org.apache.flink.table.planner.runtime.utils.BatchTestBase
 import org.apache.flink.table.planner.runtime.utils.BatchTestBase.row
 import org.apache.flink.table.planner.runtime.utils.TestData._
-import org.apache.flink.table.planner.sinks.CollectRowTableSink
-import org.apache.flink.table.runtime.operators.CodeGenOperatorFactory
-
 import org.junit.runner.RunWith
 import org.junit.runners.Parameterized
-import org.junit.{Assert, Before, Ignore, Test}
-
+import org.junit.{Assert, Before, Test}
 import java.util
 
+import org.apache.flink.api.common.typeinfo.Types
+import org.apache.flink.api.dag.Transformation
+import org.apache.flink.streaming.api.transformations.{OneInputTransformation, SinkTransformation, TwoInputTransformation}
+import org.apache.flink.table.planner.delegation.PlannerBase
+import org.apache.flink.table.planner.sinks.CollectRowTableSink
+import org.apache.flink.table.planner.utils.TestingTableEnvironment
+import org.apache.flink.table.runtime.operators.CodeGenOperatorFactory
+
 import scala.collection.JavaConversions._
 import scala.collection.Seq
 
@@ -94,28 +96,39 @@ class JoinITCase(expectedJoinType: JoinType) extends BatchTestBase {
       ))
   }
 
-  @Ignore // TODO support lazy from source
   @Test
   def testLongHashJoinGenerator(): Unit = {
     if (expectedJoinType == HashJoin) {
       val sink = (new CollectRowTableSink).configure(Array("c"), Array(Types.STRING))
-      tEnv.registerTableSink("collect", sink)
-      tEnv.insertInto("collect", tEnv.sqlQuery("SELECT c FROM SmallTable3, Table5 WHERE b = e"))
+      tEnv.registerTableSink("outputTable", sink)
+      tEnv.insertInto("outputTable", tEnv.sqlQuery("SELECT c FROM SmallTable3, Table5 WHERE b = e"))
+      val testingTEnv = tEnv.asInstanceOf[TestingTableEnvironment]
+      val transforms = testingTEnv.getPlanner.asInstanceOf[PlannerBase]
+        .translate(testingTEnv.getBufferedOperations)
       var haveTwoOp = false
-      env.getStreamGraph.getAllOperatorFactory.foreach(o =>
-        o.f1 match {
+
+      @scala.annotation.tailrec
+      def findTwoInputTransform(t: Transformation[_]): TwoInputTransformation[_, _, _] = {
+        t match {
+          case sink: SinkTransformation[_] => findTwoInputTransform(sink.getInput)
+          case one: OneInputTransformation[_, _] => findTwoInputTransform(one.getInput)
+          case two: TwoInputTransformation[_, _, _] => two
+        }
+      }
+
+      transforms.map(findTwoInputTransform).foreach { transform =>
+        transform.getOperatorFactory match {
           case factory: CodeGenOperatorFactory[_] =>
             if (factory.getGeneratedClass.getCode.contains("LongHashJoinOperator")) {
               haveTwoOp = true
             }
           case _ =>
         }
-      )
+      }
       Assert.assertTrue(haveTwoOp)
     }
   }
 
-  @Ignore
   @Test
   def testOneSideSmjFieldError(): Unit = {
     if (expectedJoinType == SortMergeJoin) {
@@ -445,7 +458,6 @@ class JoinITCase(expectedJoinType: JoinType) extends BatchTestBase {
     }
   }
 
-  @Ignore // TODO not support same source until set lazy_from_source
   @Test
   def testFullOuterJoinWithoutEqualCond(): Unit = {
     if (expectedJoinType == NestedLoopJoin) {
@@ -458,7 +470,6 @@ class JoinITCase(expectedJoinType: JoinType) extends BatchTestBase {
     }
   }
 
-  @Ignore // TODO not support same source until set lazy_from_source
   @Test
   def testSingleRowFullOuterJoinWithoutEqualCond(): Unit = {
     if (expectedJoinType == NestedLoopJoin) {
@@ -471,7 +482,6 @@ class JoinITCase(expectedJoinType: JoinType) extends BatchTestBase {
     }
   }
 
-  @Ignore // TODO not support same source until set lazy_from_source
   @Test
   def testSingleRowFullOuterJoinWithoutEqualCondNoMatch(): Unit = {
     if (expectedJoinType == NestedLoopJoin) {
@@ -514,7 +524,6 @@ class JoinITCase(expectedJoinType: JoinType) extends BatchTestBase {
       ))
   }
 
-  // join with agg
   @Test
   def testJoinWithAggregation(): Unit = {
     checkResult(
@@ -522,7 +531,6 @@ class JoinITCase(expectedJoinType: JoinType) extends BatchTestBase {
       Seq(row(6L, 6L)))
   }
 
-  @Ignore
   @Test
   def testJoinConditionNeedSimplify(): Unit = {
     checkResult(
@@ -530,7 +538,6 @@ class JoinITCase(expectedJoinType: JoinType) extends BatchTestBase {
       Seq(row(1), row(3), row(3), row(3)))
   }
 
-  @Ignore
   @Test
   def testJoinConditionDerivedFromCorrelatedSubQueryNeedSimplify(): Unit = {
     checkResult(
@@ -539,7 +546,6 @@ class JoinITCase(expectedJoinType: JoinType) extends BatchTestBase {
       Seq(row(1), row(2)))
   }
 
-  @Ignore
   @Test
   def testSimple(): Unit = {
     checkResult(
@@ -567,7 +573,6 @@ class JoinITCase(expectedJoinType: JoinType) extends BatchTestBase {
     }
   }
 
-  @Ignore
   @Test
   def testCorrelatedExist(): Unit = {
     checkResult(
@@ -579,7 +584,6 @@ class JoinITCase(expectedJoinType: JoinType) extends BatchTestBase {
       Seq(row(2, 1.0), row(2, 1.0)))
   }
 
-  @Ignore
   @Test
   def testCorrelatedNotExist(): Unit = {
     checkResult(
@@ -587,7 +591,6 @@ class JoinITCase(expectedJoinType: JoinType) extends BatchTestBase {
       Seq(row(1, 2.0), row(1, 2.0), row(6, null), row(null, 5.0), row(null, null)))
   }
 
-  @Ignore
   @Test
   def testUncorrelatedScalar(): Unit = {
     checkResult(
@@ -603,7 +606,6 @@ class JoinITCase(expectedJoinType: JoinType) extends BatchTestBase {
       Seq(row(1)))
   }
 
-  @Ignore
   @Test
   def testEqualWithAggScalar(): Unit = {
     checkResult(
@@ -611,7 +613,6 @@ class JoinITCase(expectedJoinType: JoinType) extends BatchTestBase {
       Seq(row(2, 1.0), row(2, 1.0)))
   }
 
-  @Ignore
   @Test
   def testComparisonsScalar(): Unit = {
     if (expectedJoinType == NestedLoopJoin) {
@@ -628,7 +629,6 @@ class JoinITCase(expectedJoinType: JoinType) extends BatchTestBase {
       row(2, 1.0) :: row(2, 1.0) :: Nil)
   }
 
-  @Ignore // TODO not support same source until set lazy_from_source
   @Test
   def testJoinWithNull(): Unit = {
     // TODO enable all
@@ -670,7 +670,6 @@ class JoinITCase(expectedJoinType: JoinType) extends BatchTestBase {
     }
   }
 
-  @Ignore // TODO not support same source until set lazy_from_source
   @Test
   def testSingleRowJoin(): Unit = {
     if (expectedJoinType == NestedLoopJoin) {
@@ -712,7 +711,6 @@ class JoinITCase(expectedJoinType: JoinType) extends BatchTestBase {
     }
   }
 
-  @Ignore // TODO not support same source until set lazy_from_source
   @Test
   def testNonEmptyTableJoinEmptyTable(): Unit = {
     if (expectedJoinType == NestedLoopJoin) {
@@ -775,7 +773,6 @@ class JoinITCase(expectedJoinType: JoinType) extends BatchTestBase {
     }
   }
 
-  @Ignore
   @Test
   def testJoinCollation(): Unit = {
     checkResult(
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/AggregateITCase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/AggregateITCase.scala
index 0d3417af3d8..cf8df05ccc6 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/AggregateITCase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/AggregateITCase.scala
@@ -641,7 +641,6 @@ class AggregateITCase(
     assertMapStrEquals(expected.sorted.toString, sink.getRetractResults.sorted.toString)
   }
 
-  @Ignore("[FLINK-12088]: JOIN is not supported")
   @Test
   def testGroupBySingleValue(): Unit = {
     val data = new mutable.MutableList[(Int, Long, String)]
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/table/GroupWindowTableAggregateITCase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/table/GroupWindowTableAggregateITCase.scala
index 31dd9aec923..456fa3276f9 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/table/GroupWindowTableAggregateITCase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/table/GroupWindowTableAggregateITCase.scala
@@ -31,11 +31,10 @@ import org.apache.flink.table.planner.utils.Top3
 import org.apache.flink.types.Row
 import org.apache.flink.table.planner.runtime.utils.TestData._
 import org.junit.Assert._
-import org.junit.{Ignore, Test}
+import org.junit.Test
 import org.junit.runner.RunWith
 import org.junit.runners.Parameterized
 
-@Ignore("Remove this ignore when FLINK-13740 is solved.")
 @RunWith(classOf[Parameterized])
 class GroupWindowTableAggregateITCase(mode: StateBackendMode)
   extends StreamingWithStateTestBase(mode) {
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/table/TableAggregateITCase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/table/TableAggregateITCase.scala
index e53e1bc5f8d..7b18b9673e9 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/table/TableAggregateITCase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/table/TableAggregateITCase.scala
@@ -30,12 +30,11 @@ import org.apache.flink.types.Row
 import org.junit.Assert.assertEquals
 import org.junit.runner.RunWith
 import org.junit.runners.Parameterized
-import org.junit.{Before, Ignore, Test}
+import org.junit.{Before, Test}
 
 /**
   * Tests of groupby (without window) table aggregations
   */
-@Ignore("Remove this ignore when FLINK-13740 is solved.")
 @RunWith(classOf[Parameterized])
 class TableAggregateITCase(mode: StateBackendMode) extends StreamingWithStateTestBase(mode) {
 
diff --git a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/api/batch/sql/SetOperatorsTest.scala b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/api/batch/sql/SetOperatorsTest.scala
index ce33da0833e..1b3daaa529f 100644
--- a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/api/batch/sql/SetOperatorsTest.scala
+++ b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/api/batch/sql/SetOperatorsTest.scala
@@ -25,7 +25,7 @@ import org.apache.flink.table.api.scala._
 import org.apache.flink.table.runtime.utils.CommonTestData.NonPojo
 import org.apache.flink.table.utils.TableTestBase
 import org.apache.flink.table.utils.TableTestUtil._
-import org.junit.{Ignore, Test}
+import org.junit.Test
 
 class SetOperatorsTest extends TableTestBase {
 
@@ -159,13 +159,57 @@ class SetOperatorsTest extends TableTestBase {
   }
 
   @Test
-  @Ignore // Calcite bug
   def testNotInWithFilter(): Unit = {
     val util = batchTestUtil()
-    util.addTable[(Int, Long, String)]("A", 'a, 'b, 'c)
-    util.addTable[(Int, Long, Int, String, Long)]("B", 'a, 'b, 'c, 'd, 'e)
+    val tableA = util.addTable[(Int, Long, String)]("A", 'a, 'b, 'c)
+    val tableB = util.addTable[(Int, Long, Int, String, Long)]("B", 'a, 'b, 'c, 'd, 'e)
 
-    val expected = "FAIL"
+    val expected = unaryNode(
+      "DataSetCalc",
+      binaryNode(
+        "DataSetJoin",
+        unaryNode(
+          "DataSetCalc",
+          binaryNode(
+            "DataSetSingleRowJoin",
+            unaryNode(
+              "DataSetCalc",
+              batchTableNode(tableB),
+              term("select", "d"),
+              term("where", "<(CAST(d), 5:BIGINT)")
+            ),
+            unaryNode(
+              "DataSetAggregate",
+              unaryNode(
+                "DataSetCalc",
+                batchTableNode(tableA),
+                term("select", "a")
+              ),
+              term("select", "COUNT(*) AS $f0, COUNT(a) AS $f1")
+            ),
+            term("where", "true"),
+            term("join", "d, $f0, $f1"),
+            term("joinType", "NestedLoopInnerJoin")
+          ),
+          term("select", "d, $f0, $f1, d AS d0")
+        ),
+        unaryNode(
+          "DataSetAggregate",
+          unaryNode(
+            "DataSetCalc",
+            batchTableNode(tableA),
+            term("select", "a, true AS $f1")
+          ),
+          term("groupBy", "a"),
+          term("select", "a, MIN($f1) AS $f1")
+        ),
+        term("where", "=(d0, a)"),
+        term("join", "d, $f0, $f1, d0, a, $f10"),
+        term("joinType", "LeftOuterJoin")
+      ),
+      term("select", "d"),
+      term("where", "OR(=($f0, 0:BIGINT), AND(IS NULL($f10), >=($f1, $f0), IS NOT NULL(d0)))")
+    )
 
     util.verifySql(
       "SELECT d FROM B WHERE d NOT IN (SELECT a FROM A) AND d < 5",
diff --git a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/plan/ExpressionReductionRulesTest.scala b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/plan/ExpressionReductionRulesTest.scala
index 1b409aab468..080da5d7c42 100644
--- a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/plan/ExpressionReductionRulesTest.scala
+++ b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/plan/ExpressionReductionRulesTest.scala
@@ -19,7 +19,6 @@
 package org.apache.flink.table.plan
 
 import org.apache.flink.api.scala._
-import org.apache.flink.configuration.PipelineOptions
 import org.apache.flink.table.api.Types
 import org.apache.flink.table.api.scala._
 import org.apache.flink.table.expressions.utils.{Func1, RichFunc1}
@@ -28,9 +27,8 @@ import org.apache.flink.table.functions.python.{PythonEnv, PythonFunction}
 import org.apache.flink.table.utils.TableTestBase
 import org.apache.flink.table.utils.TableTestUtil._
 
-import org.junit.{Ignore, Test}
+import org.junit.Test
 
-import scala.collection.JavaConverters._
 
 class ExpressionReductionRulesTest extends TableTestBase {
 
@@ -469,19 +467,20 @@ class ExpressionReductionRulesTest extends TableTestBase {
     util.verifySql(sqlQuery, expected)
   }
 
-  // TODO this NPE is caused by Calcite, it shall pass when [CALCITE-1860] is fixed
-  @Ignore
+  @Test
   def testReduceDeterministicUDF(): Unit = {
     val util = streamTestUtil()
     val table = util.addTable[(Int, Long, String)]("MyTable", 'a, 'b, 'c)
-
-    // if isDeterministic = true, will cause a Calcite NPE, which will be fixed in [CALCITE-1860]
     val result = table
       .select('a, 'b, 'c, DeterministicNullFunc() as 'd)
       .where("d.isNull")
       .select('a, 'b, 'c)
 
-    val expected: String = streamTableNode(table)
+    val expected: String = unaryNode("DataStreamCalc",
+      streamTableNode(table),
+      term("select", "a", "b", "c"),
+      term("where", s"IS NULL(null:VARCHAR(65536))")
+    )
 
     util.verifyTable(result, expected)
   }
diff --git a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/runtime/batch/sql/SetOperatorsITCase.scala b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/runtime/batch/sql/SetOperatorsITCase.scala
index 08bc2ebef1a..014380f7d30 100644
--- a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/runtime/batch/sql/SetOperatorsITCase.scala
+++ b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/runtime/batch/sql/SetOperatorsITCase.scala
@@ -158,8 +158,6 @@ class SetOperatorsITCase(
   }
 
   @Test
-  @Ignore
-  // calcite sql parser doesn't support EXCEPT ALL
   def testExceptAll(): Unit = {
     val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment
     val tEnv = BatchTableEnvironment.create(env, config)
