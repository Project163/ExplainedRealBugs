diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/ExecNodeBase.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/ExecNodeBase.java
index 4540b574fed..c16cf1772a8 100644
--- a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/ExecNodeBase.java
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/ExecNodeBase.java
@@ -280,10 +280,6 @@ public abstract class ExecNodeBase<T> implements ExecNode<T> {
         return detailName;
     }
 
-    public void resetTransformation() {
-        this.transformation = null;
-    }
-
     @VisibleForTesting
     @JsonIgnore
     public Transformation<T> getTransformation() {
diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/BatchExecExecutionOrderEnforcer.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/BatchExecExecutionOrderEnforcer.java
new file mode 100644
index 00000000000..c0ba1fd9335
--- /dev/null
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/BatchExecExecutionOrderEnforcer.java
@@ -0,0 +1,123 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.planner.plan.nodes.exec.batch;
+
+import org.apache.flink.api.dag.Transformation;
+import org.apache.flink.configuration.ReadableConfig;
+import org.apache.flink.streaming.api.transformations.OneInputTransformation;
+import org.apache.flink.streaming.api.transformations.SourceTransformation;
+import org.apache.flink.table.data.RowData;
+import org.apache.flink.table.planner.codegen.CodeGeneratorContext;
+import org.apache.flink.table.planner.codegen.dynamicfiltering.ExecutionOrderEnforcerCodeGenerator;
+import org.apache.flink.table.planner.delegation.PlannerBase;
+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;
+import org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase;
+import org.apache.flink.table.planner.plan.nodes.exec.ExecNodeConfig;
+import org.apache.flink.table.planner.plan.nodes.exec.ExecNodeContext;
+import org.apache.flink.table.planner.plan.nodes.exec.InputProperty;
+import org.apache.flink.table.planner.plan.nodes.exec.utils.ExecNodeUtil;
+import org.apache.flink.table.runtime.operators.CodeGenOperatorFactory;
+import org.apache.flink.table.runtime.operators.dynamicfiltering.DynamicFilteringDataCollectorOperatorFactory;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
+import org.apache.flink.table.types.logical.LogicalType;
+import org.apache.flink.table.types.logical.RowType;
+
+import java.util.List;
+
+/**
+ * Batch {@link ExecNode} for ExecutionOrderEnforcer.
+ *
+ * <p>ExecutionOrderEnforcer has two inputs, one of which is a source, and the other is the
+ * dependent upstream. It enforces that the input source is executed after the dependent input is
+ * finished. Everything passed from the inputs is forwarded to the output, though typically the
+ * dependent input should not send anything.
+ *
+ * <p>The ExecutionOrderEnforcer should generally be chained with the source. If chaining is
+ * explicitly disabled, the enforcer can not work as expected.
+ *
+ * <p>The operator is used only for dynamic filtering at present.
+ */
+public class BatchExecExecutionOrderEnforcer extends ExecNodeBase<RowData>
+        implements BatchExecNode<RowData> {
+    public BatchExecExecutionOrderEnforcer(
+            ReadableConfig tableConfig,
+            List<InputProperty> inputProperties,
+            LogicalType outputType,
+            String description) {
+        super(
+                ExecNodeContext.newNodeId(),
+                ExecNodeContext.newContext(BatchExecExecutionOrderEnforcer.class),
+                ExecNodeContext.newPersistedConfig(
+                        BatchExecExecutionOrderEnforcer.class, tableConfig),
+                inputProperties,
+                outputType,
+                description);
+    }
+
+    @Override
+    @SuppressWarnings("unchecked")
+    protected Transformation<RowData> translateToPlanInternal(
+            PlannerBase planner, ExecNodeConfig config) {
+        Transformation<RowData> dynamicFilteringInputTransform =
+                (Transformation<RowData>) getInputEdges().get(0).translateToPlan(planner);
+        Transformation<RowData> sourceTransform =
+                (Transformation<RowData>) getInputEdges().get(1).translateToPlan(planner);
+
+        // set dynamic filtering data listener id
+        BatchExecDynamicFilteringDataCollector dynamicFilteringDataCollector =
+                (BatchExecDynamicFilteringDataCollector)
+                        ignoreExchange(getInputEdges().get(0).getSource());
+        BatchExecTableSourceScan tableSourceScan =
+                (BatchExecTableSourceScan) getInputEdges().get(1).getSource();
+        ((SourceTransformation<?, ?, ?>) sourceTransform)
+                .setCoordinatorListeningID(tableSourceScan.getDynamicFilteringDataListenerID());
+        ((DynamicFilteringDataCollectorOperatorFactory)
+                        ((OneInputTransformation<?, ?>)
+                                        dynamicFilteringDataCollector.translateToPlan(planner))
+                                .getOperatorFactory())
+                .registerDynamicFilteringDataListenerID(
+                        tableSourceScan.getDynamicFilteringDataListenerID());
+
+        final CodeGenOperatorFactory<RowData> operatorFactory =
+                ExecutionOrderEnforcerCodeGenerator.gen(
+                        new CodeGeneratorContext(
+                                config, planner.getFlinkContext().getClassLoader()),
+                        (RowType) getInputEdges().get(0).getOutputType(),
+                        (RowType) getInputEdges().get(1).getOutputType());
+
+        return ExecNodeUtil.createTwoInputTransformation(
+                dynamicFilteringInputTransform,
+                sourceTransform,
+                createTransformationName(config),
+                createTransformationDescription(config),
+                operatorFactory,
+                InternalTypeInfo.of(getOutputType()),
+                sourceTransform.getParallelism(),
+                0,
+                false);
+    }
+
+    private static ExecNode<?> ignoreExchange(ExecNode<?> execNode) {
+        if (execNode instanceof BatchExecExchange) {
+            return execNode.getInputEdges().get(0).getSource();
+        } else {
+            return execNode;
+        }
+    }
+}
diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/BatchExecMultipleInput.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/BatchExecMultipleInput.java
index 8ffbcbc7fc9..e87807839e4 100644
--- a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/BatchExecMultipleInput.java
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/BatchExecMultipleInput.java
@@ -41,7 +41,6 @@ import org.apache.flink.table.planner.plan.nodes.exec.ExecNodeContext;
 import org.apache.flink.table.planner.plan.nodes.exec.InputProperty;
 import org.apache.flink.table.planner.plan.nodes.exec.SingleTransformationTranslator;
 import org.apache.flink.table.planner.plan.nodes.exec.utils.ExecNodeUtil;
-import org.apache.flink.table.planner.plan.nodes.exec.visitor.AbstractExecNodeExactlyOnceVisitor;
 import org.apache.flink.table.runtime.operators.fusion.OperatorFusionCodegenFactory;
 import org.apache.flink.table.runtime.operators.multipleinput.BatchMultipleInputStreamOperatorFactory;
 import org.apache.flink.table.runtime.operators.multipleinput.TableOperatorWrapperGenerator;
@@ -250,23 +249,6 @@ public class BatchExecMultipleInput extends ExecNodeBase<RowData>
         return multipleInputTransform;
     }
 
-    @Override
-    public void resetTransformation() {
-        super.resetTransformation();
-        // For BatchExecMultipleInput, we also need to reset transformation for
-        // rootNode in BatchExecMultipleInput.
-        AbstractExecNodeExactlyOnceVisitor visitor =
-                new AbstractExecNodeExactlyOnceVisitor() {
-
-                    @Override
-                    protected void visitNode(ExecNode<?> node) {
-                        ((ExecNodeBase<?>) node).resetTransformation();
-                        visitInputs(node);
-                    }
-                };
-        rootNode.accept(visitor);
-    }
-
     public List<ExecEdge> getOriginalEdges() {
         return originalEdges;
     }
diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/BatchExecTableSourceScan.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/BatchExecTableSourceScan.java
index 72a3a201fd2..e982477f7ab 100644
--- a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/BatchExecTableSourceScan.java
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/BatchExecTableSourceScan.java
@@ -23,9 +23,6 @@ import org.apache.flink.api.dag.Transformation;
 import org.apache.flink.configuration.ReadableConfig;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 import org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction;
-import org.apache.flink.streaming.api.transformations.MultipleInputTransformation;
-import org.apache.flink.streaming.api.transformations.OneInputTransformation;
-import org.apache.flink.streaming.api.transformations.SourceTransformation;
 import org.apache.flink.table.connector.source.ScanTableSource;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.planner.delegation.PlannerBase;
@@ -36,8 +33,6 @@ import org.apache.flink.table.planner.plan.nodes.exec.InputProperty;
 import org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan;
 import org.apache.flink.table.planner.plan.nodes.exec.spec.DynamicTableSourceSpec;
 import org.apache.flink.table.planner.plan.nodes.exec.utils.ExecNodeUtil;
-import org.apache.flink.table.runtime.operators.dynamicfiltering.DynamicFilteringDataCollectorOperatorFactory;
-import org.apache.flink.table.runtime.operators.dynamicfiltering.ExecutionOrderEnforcerOperatorFactory;
 import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.util.Preconditions;
@@ -54,8 +49,7 @@ public class BatchExecTableSourceScan extends CommonExecTableSourceScan
 
     // Avoids creating different ids if translated multiple times
     private final String dynamicFilteringDataListenerID = UUID.randomUUID().toString();
-
-    private boolean needDynamicFilteringDependency;
+    private final ReadableConfig tableConfig;
 
     // This constructor can be used only when table source scan has
     // BatchExecDynamicFilteringDataCollector input
@@ -73,6 +67,7 @@ public class BatchExecTableSourceScan extends CommonExecTableSourceScan
                 Collections.singletonList(inputProperty),
                 outputType,
                 description);
+        this.tableConfig = tableConfig;
     }
 
     public BatchExecTableSourceScan(
@@ -88,10 +83,11 @@ public class BatchExecTableSourceScan extends CommonExecTableSourceScan
                 Collections.emptyList(),
                 outputType,
                 description);
+        this.tableConfig = tableConfig;
     }
 
-    public void setNeedDynamicFilteringDependency(boolean needDynamicFilteringDependency) {
-        this.needDynamicFilteringDependency = needDynamicFilteringDependency;
+    public String getDynamicFilteringDataListenerID() {
+        return dynamicFilteringDataListenerID;
     }
 
     @Override
@@ -102,50 +98,10 @@ public class BatchExecTableSourceScan extends CommonExecTableSourceScan
         // the boundedness has been checked via the runtime provider already, so we can safely
         // declare all legacy transformations as bounded to make the stream graph generator happy
         ExecNodeUtil.makeLegacySourceTransformationsBounded(transformation);
-
-        // no dynamic filtering applied
-        if (getInputEdges().isEmpty() || !(transformation instanceof SourceTransformation)) {
-            return transformation;
-        }
-
-        // handle dynamic filtering
-        BatchExecDynamicFilteringDataCollector dynamicFilteringDataCollector =
-                getDynamicFilteringDataCollector(this);
-
-        // Set the dynamic filtering data listener ids for both sides. Must use translateToPlan to
-        // avoid duplication.
-        Transformation<Object> dynamicFilteringTransform =
-                dynamicFilteringDataCollector.translateToPlan(planner);
-        ((SourceTransformation<?, ?, ?>) transformation)
-                .setCoordinatorListeningID(dynamicFilteringDataListenerID);
-        ((DynamicFilteringDataCollectorOperatorFactory)
-                        ((OneInputTransformation<?, ?>) dynamicFilteringTransform)
-                                .getOperatorFactory())
-                .registerDynamicFilteringDataListenerID(dynamicFilteringDataListenerID);
-
-        // Translate the predecessor nodes. Must use translateToPlan to avoid duplication.
-        BatchExecNode<?> input = (BatchExecNode<?>) getInputEdges().get(0).getSource();
-        Transformation<?> dynamicFilteringInputTransform = input.translateToPlan(planner);
-
-        if (!needDynamicFilteringDependency) {
-            planner.addExtraTransformation(dynamicFilteringInputTransform);
-            return transformation;
-        } else {
-            MultipleInputTransformation<RowData> multipleInputTransformation =
-                    new MultipleInputTransformation<>(
-                            "Order-Enforcer",
-                            new ExecutionOrderEnforcerOperatorFactory<>(),
-                            transformation.getOutputType(),
-                            transformation.getParallelism(),
-                            false);
-            multipleInputTransformation.addInput(dynamicFilteringInputTransform);
-            multipleInputTransformation.addInput(transformation);
-
-            return multipleInputTransformation;
-        }
+        return transformation;
     }
 
-    private BatchExecDynamicFilteringDataCollector getDynamicFilteringDataCollector(
+    public static BatchExecDynamicFilteringDataCollector getDynamicFilteringDataCollector(
             BatchExecNode<?> node) {
         Preconditions.checkState(
                 node.getInputEdges().size() == 1,
@@ -177,4 +133,15 @@ public class BatchExecTableSourceScan extends CommonExecTableSourceScan
                 new InputFormatSourceFunction<>(inputFormat, outputTypeInfo);
         return env.addSource(function, operatorName, outputTypeInfo).getTransformation();
     }
+
+    public BatchExecTableSourceScan copyAndRemoveInputs() {
+        BatchExecTableSourceScan tableSourceScan =
+                new BatchExecTableSourceScan(
+                        tableConfig,
+                        getTableSourceSpec(),
+                        (RowType) getOutputType(),
+                        getDescription());
+        tableSourceScan.setInputEdges(Collections.emptyList());
+        return tableSourceScan;
+    }
 }
diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/processor/DynamicFilteringDependencyProcessor.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/processor/DynamicFilteringDependencyProcessor.java
index c5061676f3d..8570532a21b 100644
--- a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/processor/DynamicFilteringDependencyProcessor.java
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/processor/DynamicFilteringDependencyProcessor.java
@@ -28,13 +28,14 @@ import org.apache.flink.table.planner.plan.nodes.exec.ExecNodeGraph;
 import org.apache.flink.table.planner.plan.nodes.exec.InputProperty;
 import org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecDynamicFilteringDataCollector;
 import org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecExchange;
-import org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecMultipleInput;
+import org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecExecutionOrderEnforcer;
 import org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecNode;
 import org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecTableSourceScan;
 import org.apache.flink.table.planner.plan.nodes.exec.visitor.AbstractExecNodeExactlyOnceVisitor;
 import org.apache.flink.table.types.logical.RowType;
 
 import java.util.ArrayList;
+import java.util.Arrays;
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.HashSet;
@@ -42,6 +43,8 @@ import java.util.List;
 import java.util.Map;
 import java.util.Set;
 
+import static org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecTableSourceScan.getDynamicFilteringDataCollector;
+
 /**
  * This processor future checks each dynamic filter source to see if it is chained with a multiple
  * input operator. If so, we'll set the dependency flag.
@@ -52,50 +55,88 @@ public class DynamicFilteringDependencyProcessor implements ExecNodeGraphProcess
 
     @Override
     public ExecNodeGraph process(ExecNodeGraph execGraph, ProcessorContext context) {
-        ExecNodeGraph factSideProcessedGraph = checkIfFactSourceNeedEnforceDependency(execGraph);
+        ExecNodeGraph factSideProcessedGraph = addOrderEnforcer(execGraph, context);
         return enforceDimSideBlockingExchange(factSideProcessedGraph, context);
     }
 
-    private ExecNodeGraph checkIfFactSourceNeedEnforceDependency(ExecNodeGraph execGraph) {
-        Map<BatchExecTableSourceScan, List<ExecNode<?>>> dynamicFilteringScanDescendants =
+    private ExecNodeGraph addOrderEnforcer(ExecNodeGraph execGraph, ProcessorContext context) {
+        Map<BatchExecTableSourceScan, List<DescendantInfo>> dynamicFilteringScanDescendants =
                 new HashMap<>();
 
         AbstractExecNodeExactlyOnceVisitor dynamicFilteringScanCollector =
                 new AbstractExecNodeExactlyOnceVisitor() {
                     @Override
                     protected void visitNode(ExecNode<?> node) {
-                        node.getInputEdges().stream()
-                                .map(ExecEdge::getSource)
-                                .forEach(
-                                        input -> {
-                                            // The character of the dynamic filter scan is that it
-                                            // has an input.
-                                            if (input instanceof BatchExecTableSourceScan
-                                                    && input.getInputEdges().size() > 0) {
-                                                dynamicFilteringScanDescendants
-                                                        .computeIfAbsent(
-                                                                (BatchExecTableSourceScan) input,
-                                                                ignored -> new ArrayList<>())
-                                                        .add(node);
-                                            }
-                                        });
+                        for (int i = 0; i < node.getInputEdges().size(); ++i) {
+                            ExecEdge edge = node.getInputEdges().get(i);
+                            ExecNode<?> input = edge.getSource();
+
+                            // The character of the dynamic filter scan is that it
+                            // has an input.
+                            if (input instanceof BatchExecTableSourceScan
+                                    && input.getInputEdges().size() > 0) {
+                                dynamicFilteringScanDescendants
+                                        .computeIfAbsent(
+                                                (BatchExecTableSourceScan) input,
+                                                ignored -> new ArrayList<>())
+                                        .add(new DescendantInfo(node, i));
+                            }
+                        }
 
                         visitInputs(node);
                     }
                 };
         execGraph.getRootNodes().forEach(node -> node.accept(dynamicFilteringScanCollector));
 
-        for (Map.Entry<BatchExecTableSourceScan, List<ExecNode<?>>> entry :
+        for (Map.Entry<BatchExecTableSourceScan, List<DescendantInfo>> entry :
                 dynamicFilteringScanDescendants.entrySet()) {
-            if (entry.getValue().size() == 1) {
-                ExecNode<?> next = entry.getValue().get(0);
-                if (next instanceof BatchExecMultipleInput) {
-                    // the source can be chained with BatchExecMultipleInput
-                    continue;
-                }
-            }
-            // otherwise we need dependencies
-            entry.getKey().setNeedDynamicFilteringDependency(true);
+            BatchExecTableSourceScan oldTableSourceScan = entry.getKey();
+            BatchExecDynamicFilteringDataCollector dynamicFilteringDataCollector =
+                    getDynamicFilteringDataCollector(oldTableSourceScan);
+
+            // we clear the input of tableSourceScan to avoid cycle in exec plan
+            BatchExecTableSourceScan newTableSourceScan = oldTableSourceScan.copyAndRemoveInputs();
+
+            // Add exchange between collector and enforcer
+            BatchExecExchange exchange =
+                    new BatchExecExchange(
+                            context.getPlanner().getTableConfig(),
+                            InputProperty.builder()
+                                    .requiredDistribution(InputProperty.ANY_DISTRIBUTION)
+                                    .damBehavior(InputProperty.DamBehavior.BLOCKING)
+                                    .build(),
+                            (RowType) dynamicFilteringDataCollector.getOutputType(),
+                            "Exchange");
+            exchange.setRequiredExchangeMode(StreamExchangeMode.BATCH);
+            exchange.setInputEdges(
+                    Collections.singletonList(
+                            ExecEdge.builder()
+                                    .source(dynamicFilteringDataCollector)
+                                    .target(exchange)
+                                    .build()));
+
+            // set enforcer inputs
+            BatchExecExecutionOrderEnforcer enforcer =
+                    new BatchExecExecutionOrderEnforcer(
+                            context.getPlanner().getTableConfig(),
+                            Arrays.asList(
+                                    exchange.getInputProperties().get(0), InputProperty.DEFAULT),
+                            newTableSourceScan.getOutputType(),
+                            "OrderEnforcer");
+            ExecEdge edge1 = ExecEdge.builder().source(exchange).target(enforcer).build();
+            ExecEdge edge2 = ExecEdge.builder().source(newTableSourceScan).target(enforcer).build();
+            enforcer.setInputEdges(Arrays.asList(edge1, edge2));
+
+            // set enforcer's output
+            entry.getValue()
+                    .forEach(
+                            descendantInfo ->
+                                    descendantInfo.descendant.replaceInputEdge(
+                                            descendantInfo.inputId,
+                                            ExecEdge.builder()
+                                                    .source(enforcer)
+                                                    .target(descendantInfo.descendant)
+                                                    .build()));
         }
 
         return execGraph;
@@ -204,4 +245,16 @@ public class DynamicFilteringDependencyProcessor implements ExecNodeGraphProcess
 
         return exchange;
     }
+
+    private static class DescendantInfo {
+        /** The DynamicFilteringScan is the {@code inputId}th input of current descendant . */
+        final int inputId;
+
+        final ExecNode<?> descendant;
+
+        DescendantInfo(ExecNode<?> descendant, int inputId) {
+            this.descendant = descendant;
+            this.inputId = inputId;
+        }
+    }
 }
diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/processor/ResetTransformationProcessor.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/processor/ResetTransformationProcessor.java
deleted file mode 100644
index 4c090786922..00000000000
--- a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/processor/ResetTransformationProcessor.java
+++ /dev/null
@@ -1,46 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.table.planner.plan.nodes.exec.processor;
-
-import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;
-import org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase;
-import org.apache.flink.table.planner.plan.nodes.exec.ExecNodeGraph;
-import org.apache.flink.table.planner.plan.nodes.exec.visitor.AbstractExecNodeExactlyOnceVisitor;
-
-/**
- * A {@link ExecNodeGraphProcessor} that reset the Transformation to null generated by other
- * processors, such as: {@link MultipleInputNodeCreationProcessor}.
- */
-public class ResetTransformationProcessor implements ExecNodeGraphProcessor {
-
-    @Override
-    public ExecNodeGraph process(ExecNodeGraph execGraph, ProcessorContext context) {
-        AbstractExecNodeExactlyOnceVisitor visitor =
-                new AbstractExecNodeExactlyOnceVisitor() {
-
-                    @Override
-                    protected void visitNode(ExecNode<?> node) {
-                        ((ExecNodeBase<?>) node).resetTransformation();
-                        visitInputs(node);
-                    }
-                };
-        execGraph.getRootNodes().forEach(r -> r.accept(visitor));
-        return execGraph;
-    }
-}
diff --git a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/codegen/dynamicfiltering/ExecutionOrderEnforcerCodeGenerator.scala b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/codegen/dynamicfiltering/ExecutionOrderEnforcerCodeGenerator.scala
new file mode 100644
index 00000000000..9b3ae87003b
--- /dev/null
+++ b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/codegen/dynamicfiltering/ExecutionOrderEnforcerCodeGenerator.scala
@@ -0,0 +1,57 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.flink.table.planner.codegen.dynamicfiltering
+
+import org.apache.flink.table.data.RowData
+import org.apache.flink.table.planner.codegen.{CodeGeneratorContext, OperatorCodeGenerator}
+import org.apache.flink.table.planner.codegen.CodeGenUtils.{DEFAULT_INPUT1_TERM, DEFAULT_INPUT2_TERM}
+import org.apache.flink.table.planner.codegen.OperatorCodeGenerator.generateCollect
+import org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecExecutionOrderEnforcer
+import org.apache.flink.table.runtime.operators.CodeGenOperatorFactory
+import org.apache.flink.table.types.logical.RowType
+
+/**
+ * Operator code generator for ExecutionOrderEnforcer operator. Input1 is the dependent upstream,
+ * input2 is the source. see [[BatchExecExecutionOrderEnforcer]] for details.
+ */
+object ExecutionOrderEnforcerCodeGenerator {
+  def gen(
+      ctx: CodeGeneratorContext,
+      input1Type: RowType,
+      input2Type: RowType): CodeGenOperatorFactory[RowData] = {
+
+    new CodeGenOperatorFactory[RowData](
+      OperatorCodeGenerator.generateTwoInputStreamOperator(
+        ctx,
+        "ExecutionOrderEnforcerOperator",
+        "",
+        s"""
+           |${generateCollect(s"$DEFAULT_INPUT2_TERM")}
+           |""".stripMargin,
+        input1Type,
+        input2Type,
+        DEFAULT_INPUT1_TERM,
+        DEFAULT_INPUT2_TERM,
+        None,
+        // we cannot pass None or use default here, because this operator must implement BoundedMultiInput
+        Some(""),
+        Some("")
+      ))
+  }
+
+}
diff --git a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/delegation/BatchPlanner.scala b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/delegation/BatchPlanner.scala
index a3e0b37cae0..bb4c1b75a28 100644
--- a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/delegation/BatchPlanner.scala
+++ b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/delegation/BatchPlanner.scala
@@ -29,7 +29,7 @@ import org.apache.flink.table.operations.{ModifyOperation, Operation}
 import org.apache.flink.table.planner.plan.`trait`.FlinkRelDistributionTraitDef
 import org.apache.flink.table.planner.plan.nodes.exec.ExecNodeGraph
 import org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecNode
-import org.apache.flink.table.planner.plan.nodes.exec.processor.{DeadlockBreakupProcessor, DynamicFilteringDependencyProcessor, ExecNodeGraphProcessor, ForwardHashExchangeProcessor, MultipleInputNodeCreationProcessor, ResetTransformationProcessor}
+import org.apache.flink.table.planner.plan.nodes.exec.processor.{DeadlockBreakupProcessor, DynamicFilteringDependencyProcessor, ExecNodeGraphProcessor, ForwardHashExchangeProcessor, MultipleInputNodeCreationProcessor}
 import org.apache.flink.table.planner.plan.nodes.exec.utils.ExecNodePlanDumper
 import org.apache.flink.table.planner.plan.optimize.{BatchCommonSubGraphBasedOptimizer, Optimizer}
 import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil
@@ -73,15 +73,14 @@ class BatchPlanner(
     val processors = new util.ArrayList[ExecNodeGraphProcessor]()
     // deadlock breakup
     processors.add(new DeadlockBreakupProcessor())
+    if (getTableConfig.get(OptimizerConfigOptions.TABLE_OPTIMIZER_DYNAMIC_FILTERING_ENABLED)) {
+      processors.add(new DynamicFilteringDependencyProcessor)
+    }
     // multiple input creation
     if (getTableConfig.get(OptimizerConfigOptions.TABLE_OPTIMIZER_MULTIPLE_INPUT_ENABLED)) {
       processors.add(new MultipleInputNodeCreationProcessor(false))
     }
     processors.add(new ForwardHashExchangeProcessor)
-    if (getTableConfig.get(OptimizerConfigOptions.TABLE_OPTIMIZER_DYNAMIC_FILTERING_ENABLED)) {
-      processors.add(new DynamicFilteringDependencyProcessor)
-      processors.add(new ResetTransformationProcessor)
-    }
     processors
   }
 
diff --git a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/nodes/exec/processor/ResetTransformationProcessorTest.java b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/nodes/exec/processor/ResetTransformationProcessorTest.java
deleted file mode 100644
index 0a5a00d3bf9..00000000000
--- a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/nodes/exec/processor/ResetTransformationProcessorTest.java
+++ /dev/null
@@ -1,143 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.table.planner.plan.nodes.exec.processor;
-
-import org.apache.flink.table.api.TableConfig;
-import org.apache.flink.table.api.TableEnvironment;
-import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;
-import org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase;
-import org.apache.flink.table.planner.plan.nodes.exec.ExecNodeGraph;
-import org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecMultipleInput;
-import org.apache.flink.table.planner.plan.nodes.exec.visitor.AbstractExecNodeExactlyOnceVisitor;
-import org.apache.flink.table.planner.utils.BatchTableTestUtil;
-import org.apache.flink.table.planner.utils.TableTestBase;
-import org.apache.flink.table.planner.utils.TableTestUtil;
-
-import org.junit.Before;
-import org.junit.Test;
-
-import static org.junit.Assert.assertNotNull;
-import static org.junit.Assert.assertNull;
-
-/** Tests for {@link ResetTransformationProcessor}. */
-public class ResetTransformationProcessorTest extends TableTestBase {
-
-    private BatchTableTestUtil util;
-    private TableEnvironment tEnv;
-
-    @Before
-    public void begin() {
-        util = batchTestUtil(TableConfig.getDefault());
-        tEnv = util.tableEnv();
-
-        tEnv.executeSql(
-                "CREATE TABLE Source1 (\n"
-                        + "  a BIGINT,\n"
-                        + "  b BIGINT,\n"
-                        + "  c VARCHAR,\n"
-                        + "  d BIGINT\n"
-                        + ") WITH (\n"
-                        + " 'connector' = 'values',\n"
-                        + " 'bounded' = 'true'\n"
-                        + ")");
-
-        tEnv.executeSql(
-                "CREATE TABLE Source2 (\n"
-                        + "  a BIGINT,\n"
-                        + "  b BIGINT,\n"
-                        + "  c VARCHAR,\n"
-                        + "  d BIGINT\n"
-                        + ") WITH (\n"
-                        + " 'connector' = 'values',\n"
-                        + " 'bounded' = 'true'\n"
-                        + ")");
-    }
-
-    @Test
-    public void testResetTransformation() {
-        String query = "SELECT * FROM Source1 WHERE a > 100";
-        ExecNodeGraph execNodeGraph = TableTestUtil.toExecNodeGraph(tEnv, query);
-        execNodeGraph.getRootNodes().forEach(node -> node.translateToPlan(util.getPlanner()));
-        assertAllTransformationsIsNotNull(execNodeGraph);
-
-        ResetTransformationProcessor resetTransformationProcessor =
-                new ResetTransformationProcessor();
-        resetTransformationProcessor.process(
-                execNodeGraph, new ProcessorContext(util.getPlanner()));
-        assertAllTransformationsIsNull(execNodeGraph);
-    }
-
-    @Test
-    public void testResetTransformationWithExecMultipleInputInExecGraph() {
-        String query =
-                "SELECT Source1.a FROM Source1, Source2 "
-                        + "WHERE Source1.a = Source2.a AND Source2.a = (SELECT Source2.a FROM Source2 WHERE b > 100)";
-        ExecNodeGraph execNodeGraph = TableTestUtil.toExecNodeGraph(tEnv, query);
-
-        MultipleInputNodeCreationProcessor multipleInputNodeCreationProcessor =
-                new MultipleInputNodeCreationProcessor(false);
-        multipleInputNodeCreationProcessor.process(
-                execNodeGraph, new ProcessorContext(util.getPlanner()));
-        execNodeGraph.getRootNodes().forEach(node -> node.translateToPlan(util.getPlanner()));
-        assertAllTransformationsIsNotNull(execNodeGraph);
-
-        // If execNodeGraph include batchExecMultipleInput node. The ResetTransformationProcessor
-        // need to both set the transformation in each execNode and the transformation in
-        // batchExecMultipleInput.rootNode to null.
-        ResetTransformationProcessor resetTransformationProcessor =
-                new ResetTransformationProcessor();
-        resetTransformationProcessor.process(
-                execNodeGraph, new ProcessorContext(util.getPlanner()));
-        assertAllTransformationsIsNull(execNodeGraph);
-    }
-
-    private void assertAllTransformationsIsNotNull(ExecNodeGraph execNodeGraph) {
-        AbstractExecNodeExactlyOnceVisitor visitor =
-                new AbstractExecNodeExactlyOnceVisitor() {
-                    @Override
-                    protected void visitNode(ExecNode<?> node) {
-                        assertNotNull(((ExecNodeBase<?>) node).getTransformation());
-                        visitInputs(node);
-                        if (node instanceof BatchExecMultipleInput) {
-                            ExecNode<?> rootNode = ((BatchExecMultipleInput) node).getRootNode();
-                            assertNotNull(((ExecNodeBase<?>) node).getTransformation());
-                            visitInputs(rootNode);
-                        }
-                    }
-                };
-        execNodeGraph.getRootNodes().forEach(r -> r.accept(visitor));
-    }
-
-    private void assertAllTransformationsIsNull(ExecNodeGraph execNodeGraph) {
-        AbstractExecNodeExactlyOnceVisitor visitor =
-                new AbstractExecNodeExactlyOnceVisitor() {
-                    @Override
-                    protected void visitNode(ExecNode<?> node) {
-                        assertNull(((ExecNodeBase<?>) node).getTransformation());
-                        visitInputs(node);
-                        if (node instanceof BatchExecMultipleInput) {
-                            ExecNode<?> rootNode = ((BatchExecMultipleInput) node).getRootNode();
-                            assertNull(((ExecNodeBase<?>) node).getTransformation());
-                            visitInputs(rootNode);
-                        }
-                    }
-                };
-        execNodeGraph.getRootNodes().forEach(r -> r.accept(visitor));
-    }
-}
diff --git a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/runtime/batch/sql/DynamicFilteringITCase.java b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/runtime/batch/sql/DynamicFilteringITCase.java
index 639133c9633..c5ca3a83107 100644
--- a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/runtime/batch/sql/DynamicFilteringITCase.java
+++ b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/runtime/batch/sql/DynamicFilteringITCase.java
@@ -74,6 +74,7 @@ public class DynamicFilteringITCase extends BatchTestBase {
                                 + "  z BIGINT\n"
                                 + ")  WITH (\n"
                                 + " 'connector' = 'values',\n"
+                                + "  'runtime-source' = 'NewSource',\n"
                                 + " 'disable-lookup' = 'true',\n"
                                 + " 'data-id' = '%s',\n"
                                 + " 'bounded' = 'true'\n"
diff --git a/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/DynamicFilteringTest.xml b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/DynamicFilteringTest.xml
index 0731970c05a..539954a475e 100644
--- a/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/DynamicFilteringTest.xml
+++ b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/DynamicFilteringTest.xml
@@ -39,10 +39,12 @@ HashJoin(joinType=[InnerJoin], where=[=(p1, p)], select=[a1, b1, c1, p1, x, y, z
 == Optimized Execution Plan ==
 HashJoin(joinType=[InnerJoin], where=[(p1 = p)], select=[a1, b1, c1, p1, x, y, z, p], build=[right])
 :- Exchange(distribution=[hash[p1]])
-:  +- DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1, partitions=[{p1=2}, {p1=3}]]], fields=[a1, b1, c1, p1])
-:     +- DynamicFilteringDataCollector(fields=[p])
-:        +- Calc(select=[x, y, z, p], where=[((x > 10) AND (CAST(p AS BIGINT) > 1))])(reuse_id=[1])
-:           +- TableSourceScan(table=[[default_catalog, default_database, dim, filter=[]]], fields=[x, y, z, p])
+:  +- MultipleInput(members=[\nOrderEnforcer\n:- [#1] Exchange(distribution=[any], shuffle_mode=[BATCH])\n+- [#2] DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1, partitions=[{p1=2}, {p1=3}]]], fields=[a1, b1, c1, p1])\n])
+:     :- Exchange(distribution=[any], shuffle_mode=[BATCH])
+:     :  +- DynamicFilteringDataCollector(fields=[p])
+:     :     +- Calc(select=[x, y, z, p], where=[((x > 10) AND (CAST(p AS BIGINT) > 1))])(reuse_id=[1])
+:     :        +- TableSourceScan(table=[[default_catalog, default_database, dim, filter=[]]], fields=[x, y, z, p])
+:     +- DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1, partitions=[{p1=2}, {p1=3}]]], fields=[a1, b1, c1, p1])
 +- Exchange(distribution=[hash[p]])
    +- Reused(reference_id=[1])
 
@@ -84,9 +86,9 @@ HashJoin(joinType=[InnerJoin], where=[(p1 = p)], select=[a1, b1, c1, p1, x, y, z
     } ]
   }, {
     "id" : ,
-    "type" : "Order-Enforcer",
+    "type" : "MultipleInput[]",
     "pact" : "Operator",
-    "contents" : "Order-Enforcer",
+    "contents" : "[]:MultipleInput(members=[\\nOrderEnforcer\\n:- [#1] Exchange(distribution=[any], shuffle_mode=[BATCH])\\n+- [#2] DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1, partitions=[{p1=2}, {p1=3}]]], fields=[a1, b1, c1, p1])\\n])",
     "parallelism" : 10,
     "predecessors" : [ {
       "id" : ,
@@ -153,10 +155,12 @@ Calc(select=[a1, b1, c1, p1, x, y, z, p, a10, b10, c10, p10])
    :- Exchange(distribution=[hash[y]])
    :  +- HashJoin(joinType=[InnerJoin], where=[(p1 = p)], select=[a1, b1, c1, p1, x, y, z, p], build=[right])
    :     :- Exchange(distribution=[hash[p1]])
-   :     :  +- DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])
-   :     :     +- DynamicFilteringDataCollector(fields=[p])
-   :     :        +- Calc(select=[x, y, z, p], where=[(x > 10)])(reuse_id=[1])
-   :     :           +- TableSourceScan(table=[[default_catalog, default_database, dim, filter=[]]], fields=[x, y, z, p])
+   :     :  +- MultipleInput(members=[\nOrderEnforcer\n:- [#1] Exchange(distribution=[any], shuffle_mode=[BATCH])\n+- [#2] DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])\n])
+   :     :     :- Exchange(distribution=[any], shuffle_mode=[BATCH])
+   :     :     :  +- DynamicFilteringDataCollector(fields=[p])
+   :     :     :     +- Calc(select=[x, y, z, p], where=[(x > 10)])(reuse_id=[1])
+   :     :     :        +- TableSourceScan(table=[[default_catalog, default_database, dim, filter=[]]], fields=[x, y, z, p])
+   :     :     +- DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])
    :     +- Exchange(distribution=[hash[p]])
    :        +- Reused(reference_id=[1])
    +- Exchange(distribution=[hash[b10]])
@@ -201,9 +205,9 @@ Calc(select=[a1, b1, c1, p1, x, y, z, p, a10, b10, c10, p10])
     } ]
   }, {
     "id" : ,
-    "type" : "Order-Enforcer",
+    "type" : "MultipleInput[]",
     "pact" : "Operator",
-    "contents" : "Order-Enforcer",
+    "contents" : "[]:MultipleInput(members=[\\nOrderEnforcer\\n:- [#1] Exchange(distribution=[any], shuffle_mode=[BATCH])\\n+- [#2] DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])\\n])",
     "parallelism" : 10,
     "predecessors" : [ {
       "id" : ,
@@ -313,11 +317,12 @@ Calc(select=[a1, b1, c1, p1, x, y, z, p, a10, b10, c10, p10])
    :- Exchange(distribution=[hash[y]])
    :  +- HashJoin(joinType=[InnerJoin], where=[(p1 = p)], select=[a1, b1, c1, p1, x, y, z, p], build=[right])
    :     :- Exchange(distribution=[hash[p1]])
-   :     :  +- DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])
-   :     :     +- Exchange(distribution=[any], shuffle_mode=[BATCH])
-   :     :        +- DynamicFilteringDataCollector(fields=[p])
-   :     :           +- Calc(select=[x, y, z, p], where=[(x > 10)])(reuse_id=[1])
-   :     :              +- TableSourceScan(table=[[default_catalog, default_database, dim, filter=[]]], fields=[x, y, z, p])
+   :     :  +- MultipleInput(members=[\nOrderEnforcer\n:- [#1] Exchange(distribution=[any], shuffle_mode=[BATCH])\n+- [#2] DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])\n])
+   :     :     :- Exchange(distribution=[any], shuffle_mode=[BATCH])
+   :     :     :  +- DynamicFilteringDataCollector(fields=[p])
+   :     :     :     +- Calc(select=[x, y, z, p], where=[(x > 10)])(reuse_id=[1])
+   :     :     :        +- TableSourceScan(table=[[default_catalog, default_database, dim, filter=[]]], fields=[x, y, z, p])
+   :     :     +- DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])
    :     +- Exchange(distribution=[hash[p]], shuffle_mode=[BATCH])
    :        +- Reused(reference_id=[1])
    +- Exchange(distribution=[hash[b10]])
@@ -362,9 +367,9 @@ Calc(select=[a1, b1, c1, p1, x, y, z, p, a10, b10, c10, p10])
     } ]
   }, {
     "id" : ,
-    "type" : "Order-Enforcer",
+    "type" : "MultipleInput[]",
     "pact" : "Operator",
-    "contents" : "Order-Enforcer",
+    "contents" : "[]:MultipleInput(members=[\\nOrderEnforcer\\n:- [#1] Exchange(distribution=[any], shuffle_mode=[BATCH])\\n+- [#2] DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])\\n])",
     "parallelism" : 10,
     "predecessors" : [ {
       "id" : ,
@@ -467,10 +472,12 @@ MultipleInput(readOrder=[2,1,0], members=[\nHashJoin(joinType=[InnerJoin], where
 :- Exchange(distribution=[hash[p2]])
 :  +- TableSourceScan(table=[[default_catalog, default_database, fact2]], fields=[a2, b2, c2, p2])
 :- Exchange(distribution=[hash[p1]])
-:  +- DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])
-:     +- DynamicFilteringDataCollector(fields=[p])
-:        +- Calc(select=[x, y, z, p], where=[(x > 10)])(reuse_id=[1])
-:           +- TableSourceScan(table=[[default_catalog, default_database, dim, filter=[]]], fields=[x, y, z, p])
+:  +- MultipleInput(members=[\nOrderEnforcer\n:- [#1] Exchange(distribution=[any], shuffle_mode=[BATCH])\n+- [#2] DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])\n])
+:     :- Exchange(distribution=[any], shuffle_mode=[BATCH])
+:     :  +- DynamicFilteringDataCollector(fields=[p])
+:     :     +- Calc(select=[x, y, z, p], where=[(x > 10)])(reuse_id=[1])
+:     :        +- TableSourceScan(table=[[default_catalog, default_database, dim, filter=[]]], fields=[x, y, z, p])
+:     +- DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])
 +- Exchange(distribution=[hash[p]])
    +- Reused(reference_id=[1])
 
@@ -478,15 +485,15 @@ MultipleInput(readOrder=[2,1,0], members=[\nHashJoin(joinType=[InnerJoin], where
 {
   "nodes" : [ {
     "id" : ,
-    "type" : "Source: fact2[]",
+    "type" : "Source: fact1[]",
     "pact" : "Data Source",
-    "contents" : "[]:TableSourceScan(table=[[default_catalog, default_database, fact2]], fields=[a2, b2, c2, p2])",
+    "contents" : "[]:DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])",
     "parallelism" : 10
   }, {
     "id" : ,
-    "type" : "Source: fact1[]",
+    "type" : "Source: fact2[]",
     "pact" : "Data Source",
-    "contents" : "[]:DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])",
+    "contents" : "[]:TableSourceScan(table=[[default_catalog, default_database, fact2]], fields=[a2, b2, c2, p2])",
     "parallelism" : 10
   }, {
     "id" : ,
@@ -518,9 +525,9 @@ MultipleInput(readOrder=[2,1,0], members=[\nHashJoin(joinType=[InnerJoin], where
     } ]
   }, {
     "id" : ,
-    "type" : "Order-Enforcer",
+    "type" : "MultipleInput[]",
     "pact" : "Operator",
-    "contents" : "Order-Enforcer",
+    "contents" : "[]:MultipleInput(members=[\\nOrderEnforcer\\n:- [#1] Exchange(distribution=[any], shuffle_mode=[BATCH])\\n+- [#2] DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])\\n])",
     "parallelism" : 10,
     "predecessors" : [ {
       "id" : ,
@@ -580,15 +587,16 @@ HashJoin(joinType=[InnerJoin], where=[=(p1, p2)], select=[a1, b1, c1, p1, x, y,
    +- TableSourceScan(table=[[default_catalog, default_database, fact2]], fields=[a2, b2, c2, p2])
 
 == Optimized Execution Plan ==
-MultipleInput(readOrder=[2,1,0], members=[\nHashJoin(joinType=[InnerJoin], where=[(p1 = p2)], select=[a1, b1, c1, p1, x, y, z, p, a2, b2, c2, p2], build=[left])\n:- HashJoin(joinType=[InnerJoin], where=[(p1 = p)], select=[a1, b1, c1, p1, x, y, z, p], build=[right])\n:  :- [#2] Exchange(distribution=[hash[p1]])\n:  +- [#3] Exchange(distribution=[hash[p]])\n+- [#1] Exchange(distribution=[hash[p2]])\n])
+MultipleInput(readOrder=[2,1,0], members=[\nHashJoin(joinType=[InnerJoin], where=[(p1 = p2)], select=[a1, b1, c1, p1, x, y, z, p, a2, b2, c2, p2], build=[left])\n:- HashJoin(joinType=[InnerJoin], where=[(p1 = p)], select=[a1, b1, c1, p1, x, y, z, p], build=[right])\n:  :- [#2] Exchange(distribution=[hash[p1]])\n:  +- [#3] Exchange(distribution=[hash[p]], shuffle_mode=[BATCH])\n+- [#1] Exchange(distribution=[hash[p2]])\n])
 :- Exchange(distribution=[hash[p2]])
 :  +- TableSourceScan(table=[[default_catalog, default_database, fact2]], fields=[a2, b2, c2, p2])
 :- Exchange(distribution=[hash[p1]])
-:  +- DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])
-:     +- Exchange(distribution=[any], shuffle_mode=[BATCH])
-:        +- DynamicFilteringDataCollector(fields=[p])
-:           +- Calc(select=[x, y, z, p], where=[(x > 10)])(reuse_id=[1])
-:              +- TableSourceScan(table=[[default_catalog, default_database, dim, filter=[]]], fields=[x, y, z, p])
+:  +- MultipleInput(members=[\nOrderEnforcer\n:- [#1] Exchange(distribution=[any], shuffle_mode=[BATCH])\n+- [#2] DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])\n])
+:     :- Exchange(distribution=[any], shuffle_mode=[BATCH])
+:     :  +- DynamicFilteringDataCollector(fields=[p])
+:     :     +- Calc(select=[x, y, z, p], where=[(x > 10)])(reuse_id=[1])
+:     :        +- TableSourceScan(table=[[default_catalog, default_database, dim, filter=[]]], fields=[x, y, z, p])
+:     +- DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])
 +- Exchange(distribution=[hash[p]], shuffle_mode=[BATCH])
    +- Reused(reference_id=[1])
 
@@ -596,15 +604,15 @@ MultipleInput(readOrder=[2,1,0], members=[\nHashJoin(joinType=[InnerJoin], where
 {
   "nodes" : [ {
     "id" : ,
-    "type" : "Source: fact2[]",
+    "type" : "Source: fact1[]",
     "pact" : "Data Source",
-    "contents" : "[]:TableSourceScan(table=[[default_catalog, default_database, fact2]], fields=[a2, b2, c2, p2])",
+    "contents" : "[]:DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])",
     "parallelism" : 10
   }, {
     "id" : ,
-    "type" : "Source: fact1[]",
+    "type" : "Source: fact2[]",
     "pact" : "Data Source",
-    "contents" : "[]:DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])",
+    "contents" : "[]:TableSourceScan(table=[[default_catalog, default_database, fact2]], fields=[a2, b2, c2, p2])",
     "parallelism" : 10
   }, {
     "id" : ,
@@ -636,9 +644,9 @@ MultipleInput(readOrder=[2,1,0], members=[\nHashJoin(joinType=[InnerJoin], where
     } ]
   }, {
     "id" : ,
-    "type" : "Order-Enforcer",
+    "type" : "MultipleInput[]",
     "pact" : "Operator",
-    "contents" : "Order-Enforcer",
+    "contents" : "[]:MultipleInput(members=[\\nOrderEnforcer\\n:- [#1] Exchange(distribution=[any], shuffle_mode=[BATCH])\\n+- [#2] DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])\\n])",
     "parallelism" : 10,
     "predecessors" : [ {
       "id" : ,
@@ -653,7 +661,7 @@ MultipleInput(readOrder=[2,1,0], members=[\nHashJoin(joinType=[InnerJoin], where
     "id" : ,
     "type" : "MultipleInput[]",
     "pact" : "Operator",
-    "contents" : "[]:MultipleInput(readOrder=[2,1,0], members=[\\nHashJoin(joinType=[InnerJoin], where=[(p1 = p2)], select=[a1, b1, c1, p1, x, y, z, p, a2, b2, c2, p2], build=[left])\\n:- HashJoin(joinType=[InnerJoin], where=[(p1 = p)], select=[a1, b1, c1, p1, x, y, z, p], build=[right])\\n:  :- [#2] Exchange(distribution=[hash[p1]])\\n:  +- [#3] Exchange(distribution=[hash[p]])\\n+- [#1] Exchange(distribution=[hash[p2]])\\n])",
+    "contents" : "[]:MultipleInput(readOrder=[2,1,0], members=[\\nHashJoin(joinType=[InnerJoin], where=[(p1 = p2)], select=[a1, b1, c1, p1, x, y, z, p, a2, b2, c2, p2], build=[left])\\n:- HashJoin(joinType=[InnerJoin], where=[(p1 = p)], select=[a1, b1, c1, p1, x, y, z, p], build=[right])\\n:  :- [#2] Exchange(distribution=[hash[p1]])\\n:  +- [#3] Exchange(distribution=[hash[p]], shuffle_mode=[BATCH])\\n+- [#1] Exchange(distribution=[hash[p2]])\\n])",
     "parallelism" : 10,
     "predecessors" : [ {
       "id" : ,
@@ -695,11 +703,12 @@ HashJoin(joinType=[InnerJoin], where=[=(p1, p)], select=[a1, b1, c1, p1, x, y, z
 == Optimized Execution Plan ==
 HashJoin(joinType=[InnerJoin], where=[(p1 = p)], select=[a1, b1, c1, p1, x, y, z, p], build=[right])
 :- Exchange(distribution=[hash[p1]])
-:  +- DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1, partitions=[{p1=2}, {p1=3}]]], fields=[a1, b1, c1, p1])
-:     +- Exchange(distribution=[any], shuffle_mode=[BATCH])
-:        +- DynamicFilteringDataCollector(fields=[p])
-:           +- Calc(select=[x, y, z, p], where=[((x > 10) AND (CAST(p AS BIGINT) > 1))])(reuse_id=[1])
-:              +- TableSourceScan(table=[[default_catalog, default_database, dim, filter=[]]], fields=[x, y, z, p])
+:  +- MultipleInput(members=[\nOrderEnforcer\n:- [#1] Exchange(distribution=[any], shuffle_mode=[BATCH])\n+- [#2] DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1, partitions=[{p1=2}, {p1=3}]]], fields=[a1, b1, c1, p1])\n])
+:     :- Exchange(distribution=[any], shuffle_mode=[BATCH])
+:     :  +- DynamicFilteringDataCollector(fields=[p])
+:     :     +- Calc(select=[x, y, z, p], where=[((x > 10) AND (CAST(p AS BIGINT) > 1))])(reuse_id=[1])
+:     :        +- TableSourceScan(table=[[default_catalog, default_database, dim, filter=[]]], fields=[x, y, z, p])
+:     +- DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1, partitions=[{p1=2}, {p1=3}]]], fields=[a1, b1, c1, p1])
 +- Exchange(distribution=[hash[p]], shuffle_mode=[BATCH])
    +- Reused(reference_id=[1])
 
@@ -741,9 +750,9 @@ HashJoin(joinType=[InnerJoin], where=[(p1 = p)], select=[a1, b1, c1, p1, x, y, z
     } ]
   }, {
     "id" : ,
-    "type" : "Order-Enforcer",
+    "type" : "MultipleInput[]",
     "pact" : "Operator",
-    "contents" : "Order-Enforcer",
+    "contents" : "[]:MultipleInput(members=[\\nOrderEnforcer\\n:- [#1] Exchange(distribution=[any], shuffle_mode=[BATCH])\\n+- [#2] DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1, partitions=[{p1=2}, {p1=3}]]], fields=[a1, b1, c1, p1])\\n])",
     "parallelism" : 10,
     "predecessors" : [ {
       "id" : ,
@@ -934,10 +943,12 @@ HashJoin(joinType=[InnerJoin], where=[=(p1, p)], select=[a1, b1, c1, p1, x, y, z
 == Optimized Execution Plan ==
 HashJoin(joinType=[InnerJoin], where=[(p1 = p)], select=[a1, b1, c1, p1, x, y, z, p], build=[right])
 :- Exchange(distribution=[hash[p1]])
-:  +- DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])
-:     +- DynamicFilteringDataCollector(fields=[p])
-:        +- Calc(select=[x, y, z, p], where=[(x > 10)])(reuse_id=[1])
-:           +- TableSourceScan(table=[[default_catalog, default_database, dim, filter=[]]], fields=[x, y, z, p])
+:  +- MultipleInput(members=[\nOrderEnforcer\n:- [#1] Exchange(distribution=[any], shuffle_mode=[BATCH])\n+- [#2] DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])\n])
+:     :- Exchange(distribution=[any], shuffle_mode=[BATCH])
+:     :  +- DynamicFilteringDataCollector(fields=[p])
+:     :     +- Calc(select=[x, y, z, p], where=[(x > 10)])(reuse_id=[1])
+:     :        +- TableSourceScan(table=[[default_catalog, default_database, dim, filter=[]]], fields=[x, y, z, p])
+:     +- DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])
 +- Exchange(distribution=[hash[p]])
    +- Reused(reference_id=[1])
 
@@ -979,9 +990,9 @@ HashJoin(joinType=[InnerJoin], where=[(p1 = p)], select=[a1, b1, c1, p1, x, y, z
     } ]
   }, {
     "id" : ,
-    "type" : "Order-Enforcer",
+    "type" : "MultipleInput[]",
     "pact" : "Operator",
-    "contents" : "Order-Enforcer",
+    "contents" : "[]:MultipleInput(members=[\\nOrderEnforcer\\n:- [#1] Exchange(distribution=[any], shuffle_mode=[BATCH])\\n+- [#2] DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])\\n])",
     "parallelism" : 10,
     "predecessors" : [ {
       "id" : ,
@@ -1050,15 +1061,19 @@ Union(all=[true], union=[a1, b1, c1, p1, x, y, z, p])
 == Optimized Execution Plan ==
 MultipleInput(readOrder=[1,0,1,0], members=[\nUnion(all=[true], union=[a1, b1, c1, p1, x, y, z, p])\n:- HashJoin(joinType=[InnerJoin], where=[(p1 = p)], select=[a1, b1, c1, p1, x, y, z, p], build=[right])\n:  :- [#1] Exchange(distribution=[hash[p1]])\n:  +- [#2] Exchange(distribution=[hash[p]])\n+- HashJoin(joinType=[InnerJoin], where=[(p2 = p)], select=[a2, b2, c2, p2, x, y, z, p], build=[right])\n   :- [#3] Exchange(distribution=[hash[p2]])\n   +- [#2] Exchange(distribution=[hash[p]])\n])
 :- Exchange(distribution=[hash[p1]])
-:  +- DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])
-:     +- DynamicFilteringDataCollector(fields=[p])(reuse_id=[2])
-:        +- Calc(select=[x, y, z, p], where=[(x > 10)])(reuse_id=[1])
-:           +- TableSourceScan(table=[[default_catalog, default_database, dim, filter=[]]], fields=[x, y, z, p])
+:  +- MultipleInput(members=[\nOrderEnforcer\n:- [#1] Exchange(distribution=[any], shuffle_mode=[BATCH])\n+- [#2] DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])\n])
+:     :- Exchange(distribution=[any], shuffle_mode=[BATCH])
+:     :  +- DynamicFilteringDataCollector(fields=[p])(reuse_id=[2])
+:     :     +- Calc(select=[x, y, z, p], where=[(x > 10)])(reuse_id=[1])
+:     :        +- TableSourceScan(table=[[default_catalog, default_database, dim, filter=[]]], fields=[x, y, z, p])
+:     +- DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])
 :- Exchange(distribution=[hash[p]])(reuse_id=[3])
 :  +- Reused(reference_id=[1])
 :- Exchange(distribution=[hash[p2]])
-:  +- DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact2]], fields=[a2, b2, c2, p2])
-:     +- Reused(reference_id=[2])
+:  +- MultipleInput(members=[\nOrderEnforcer\n:- [#1] Exchange(distribution=[any], shuffle_mode=[BATCH])\n+- [#2] DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact2]], fields=[a2, b2, c2, p2])\n])
+:     :- Exchange(distribution=[any], shuffle_mode=[BATCH])
+:     :  +- Reused(reference_id=[2])
+:     +- DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact2]], fields=[a2, b2, c2, p2])
 +- Reused(reference_id=[3])
 
 == Physical Execution Plan ==
@@ -1069,6 +1084,12 @@ MultipleInput(readOrder=[1,0,1,0], members=[\nUnion(all=[true], union=[a1, b1, c
     "pact" : "Data Source",
     "contents" : "[]:DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])",
     "parallelism" : 10
+  }, {
+    "id" : ,
+    "type" : "Source: fact2[]",
+    "pact" : "Data Source",
+    "contents" : "[]:DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact2]], fields=[a2, b2, c2, p2])",
+    "parallelism" : 10
   }, {
     "id" : ,
     "type" : "Source: dim[]",
@@ -1099,9 +1120,9 @@ MultipleInput(readOrder=[1,0,1,0], members=[\nUnion(all=[true], union=[a1, b1, c
     } ]
   }, {
     "id" : ,
-    "type" : "Order-Enforcer",
+    "type" : "MultipleInput[]",
     "pact" : "Operator",
-    "contents" : "Order-Enforcer",
+    "contents" : "[]:MultipleInput(members=[\\nOrderEnforcer\\n:- [#1] Exchange(distribution=[any], shuffle_mode=[BATCH])\\n+- [#2] DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])\\n])",
     "parallelism" : 10,
     "predecessors" : [ {
       "id" : ,
@@ -1114,15 +1135,9 @@ MultipleInput(readOrder=[1,0,1,0], members=[\nUnion(all=[true], union=[a1, b1, c
     } ]
   }, {
     "id" : ,
-    "type" : "Source: fact2[]",
-    "pact" : "Data Source",
-    "contents" : "[]:DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact2]], fields=[a2, b2, c2, p2])",
-    "parallelism" : 10
-  }, {
-    "id" : ,
-    "type" : "Order-Enforcer",
+    "type" : "MultipleInput[]",
     "pact" : "Operator",
-    "contents" : "Order-Enforcer",
+    "contents" : "[]:MultipleInput(members=[\\nOrderEnforcer\\n:- [#1] Exchange(distribution=[any], shuffle_mode=[BATCH])\\n+- [#2] DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact2]], fields=[a2, b2, c2, p2])\\n])",
     "parallelism" : 10,
     "predecessors" : [ {
       "id" : ,
@@ -1183,11 +1198,12 @@ HashJoin(joinType=[InnerJoin], where=[=(p1, p)], select=[a1, b1, c1, p1, x, y, z
 == Optimized Execution Plan ==
 HashJoin(joinType=[InnerJoin], where=[(p1 = p)], select=[a1, b1, c1, p1, x, y, z, p], build=[right])
 :- Exchange(distribution=[hash[p1]])
-:  +- DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])
-:     +- Exchange(distribution=[any], shuffle_mode=[BATCH])
-:        +- DynamicFilteringDataCollector(fields=[p])
-:           +- Calc(select=[x, y, z, p], where=[(x > 10)])(reuse_id=[1])
-:              +- TableSourceScan(table=[[default_catalog, default_database, dim, filter=[]]], fields=[x, y, z, p])
+:  +- MultipleInput(members=[\nOrderEnforcer\n:- [#1] Exchange(distribution=[any], shuffle_mode=[BATCH])\n+- [#2] DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])\n])
+:     :- Exchange(distribution=[any], shuffle_mode=[BATCH])
+:     :  +- DynamicFilteringDataCollector(fields=[p])
+:     :     +- Calc(select=[x, y, z, p], where=[(x > 10)])(reuse_id=[1])
+:     :        +- TableSourceScan(table=[[default_catalog, default_database, dim, filter=[]]], fields=[x, y, z, p])
+:     +- DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])
 +- Exchange(distribution=[hash[p]], shuffle_mode=[BATCH])
    +- Reused(reference_id=[1])
 
@@ -1229,9 +1245,9 @@ HashJoin(joinType=[InnerJoin], where=[(p1 = p)], select=[a1, b1, c1, p1, x, y, z
     } ]
   }, {
     "id" : ,
-    "type" : "Order-Enforcer",
+    "type" : "MultipleInput[]",
     "pact" : "Operator",
-    "contents" : "Order-Enforcer",
+    "contents" : "[]:MultipleInput(members=[\\nOrderEnforcer\\n:- [#1] Exchange(distribution=[any], shuffle_mode=[BATCH])\\n+- [#2] DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])\\n])",
     "parallelism" : 10,
     "predecessors" : [ {
       "id" : ,
@@ -1298,19 +1314,21 @@ Union(all=[true], union=[a1, b1, c1, p1, x, y, z, p])
          +- TableSourceScan(table=[[default_catalog, default_database, dim, filter=[]]], fields=[x, y, z, p])
 
 == Optimized Execution Plan ==
-MultipleInput(readOrder=[1,0,1,0], members=[\nUnion(all=[true], union=[a1, b1, c1, p1, x, y, z, p])\n:- HashJoin(joinType=[InnerJoin], where=[(p1 = p)], select=[a1, b1, c1, p1, x, y, z, p], build=[right])\n:  :- [#1] Exchange(distribution=[hash[p1]])\n:  +- [#2] Exchange(distribution=[hash[p]])\n+- HashJoin(joinType=[InnerJoin], where=[(p2 = p)], select=[a2, b2, c2, p2, x, y, z, p], build=[right])\n   :- [#3] Exchange(distribution=[hash[p2]])\n   +- [#2] Exchange(distribution=[hash[p]])\n])
+MultipleInput(readOrder=[1,0,1,0], members=[\nUnion(all=[true], union=[a1, b1, c1, p1, x, y, z, p])\n:- HashJoin(joinType=[InnerJoin], where=[(p1 = p)], select=[a1, b1, c1, p1, x, y, z, p], build=[right])\n:  :- [#1] Exchange(distribution=[hash[p1]])\n:  +- [#2] Exchange(distribution=[hash[p]], shuffle_mode=[BATCH])\n+- HashJoin(joinType=[InnerJoin], where=[(p2 = p)], select=[a2, b2, c2, p2, x, y, z, p], build=[right])\n   :- [#3] Exchange(distribution=[hash[p2]])\n   +- [#2] Exchange(distribution=[hash[p]], shuffle_mode=[BATCH])\n])
 :- Exchange(distribution=[hash[p1]])
-:  +- DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])
-:     +- Exchange(distribution=[any], shuffle_mode=[BATCH])
-:        +- DynamicFilteringDataCollector(fields=[p])(reuse_id=[2])
-:           +- Calc(select=[x, y, z, p], where=[(x > 10)])(reuse_id=[1])
-:              +- TableSourceScan(table=[[default_catalog, default_database, dim, filter=[]]], fields=[x, y, z, p])
+:  +- MultipleInput(members=[\nOrderEnforcer\n:- [#1] Exchange(distribution=[any], shuffle_mode=[BATCH])\n+- [#2] DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])\n])
+:     :- Exchange(distribution=[any], shuffle_mode=[BATCH])
+:     :  +- DynamicFilteringDataCollector(fields=[p])(reuse_id=[2])
+:     :     +- Calc(select=[x, y, z, p], where=[(x > 10)])(reuse_id=[1])
+:     :        +- TableSourceScan(table=[[default_catalog, default_database, dim, filter=[]]], fields=[x, y, z, p])
+:     +- DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])
 :- Exchange(distribution=[hash[p]], shuffle_mode=[BATCH])(reuse_id=[3])
 :  +- Reused(reference_id=[1])
 :- Exchange(distribution=[hash[p2]])
-:  +- DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact2]], fields=[a2, b2, c2, p2])
-:     +- Exchange(distribution=[any], shuffle_mode=[BATCH])
-:        +- Reused(reference_id=[2])
+:  +- MultipleInput(members=[\nOrderEnforcer\n:- [#1] Exchange(distribution=[any], shuffle_mode=[BATCH])\n+- [#2] DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact2]], fields=[a2, b2, c2, p2])\n])
+:     :- Exchange(distribution=[any], shuffle_mode=[BATCH])
+:     :  +- Reused(reference_id=[2])
+:     +- DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact2]], fields=[a2, b2, c2, p2])
 +- Reused(reference_id=[3])
 
 == Physical Execution Plan ==
@@ -1321,6 +1339,12 @@ MultipleInput(readOrder=[1,0,1,0], members=[\nUnion(all=[true], union=[a1, b1, c
     "pact" : "Data Source",
     "contents" : "[]:DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])",
     "parallelism" : 10
+  }, {
+    "id" : ,
+    "type" : "Source: fact2[]",
+    "pact" : "Data Source",
+    "contents" : "[]:DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact2]], fields=[a2, b2, c2, p2])",
+    "parallelism" : 10
   }, {
     "id" : ,
     "type" : "Source: dim[]",
@@ -1351,9 +1375,9 @@ MultipleInput(readOrder=[1,0,1,0], members=[\nUnion(all=[true], union=[a1, b1, c
     } ]
   }, {
     "id" : ,
-    "type" : "Order-Enforcer",
+    "type" : "MultipleInput[]",
     "pact" : "Operator",
-    "contents" : "Order-Enforcer",
+    "contents" : "[]:MultipleInput(members=[\\nOrderEnforcer\\n:- [#1] Exchange(distribution=[any], shuffle_mode=[BATCH])\\n+- [#2] DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact1]], fields=[a1, b1, c1, p1])\\n])",
     "parallelism" : 10,
     "predecessors" : [ {
       "id" : ,
@@ -1366,15 +1390,9 @@ MultipleInput(readOrder=[1,0,1,0], members=[\nUnion(all=[true], union=[a1, b1, c
     } ]
   }, {
     "id" : ,
-    "type" : "Source: fact2[]",
-    "pact" : "Data Source",
-    "contents" : "[]:DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact2]], fields=[a2, b2, c2, p2])",
-    "parallelism" : 10
-  }, {
-    "id" : ,
-    "type" : "Order-Enforcer",
+    "type" : "MultipleInput[]",
     "pact" : "Operator",
-    "contents" : "Order-Enforcer",
+    "contents" : "[]:MultipleInput(members=[\\nOrderEnforcer\\n:- [#1] Exchange(distribution=[any], shuffle_mode=[BATCH])\\n+- [#2] DynamicFilteringTableSourceScan(table=[[default_catalog, default_database, fact2]], fields=[a2, b2, c2, p2])\\n])",
     "parallelism" : 10,
     "predecessors" : [ {
       "id" : ,
@@ -1389,7 +1407,7 @@ MultipleInput(readOrder=[1,0,1,0], members=[\nUnion(all=[true], union=[a1, b1, c
     "id" : ,
     "type" : "MultipleInput[]",
     "pact" : "Operator",
-    "contents" : "[]:MultipleInput(readOrder=[1,0,1,0], members=[\\nUnion(all=[true], union=[a1, b1, c1, p1, x, y, z, p])\\n:- HashJoin(joinType=[InnerJoin], where=[(p1 = p)], select=[a1, b1, c1, p1, x, y, z, p], build=[right])\\n:  :- [#1] Exchange(distribution=[hash[p1]])\\n:  +- [#2] Exchange(distribution=[hash[p]])\\n+- HashJoin(joinType=[InnerJoin], where=[(p2 = p)], select=[a2, b2, c2, p2, x, y, z, p], build=[right])\\n   :- [#3] Exchange(distribution=[hash[p2]])\\n   +- [#2] Exchange(distribution=[hash[p]])\\n])",
+    "contents" : "[]:MultipleInput(readOrder=[1,0,1,0], members=[\\nUnion(all=[true], union=[a1, b1, c1, p1, x, y, z, p])\\n:- HashJoin(joinType=[InnerJoin], where=[(p1 = p)], select=[a1, b1, c1, p1, x, y, z, p], build=[right])\\n:  :- [#1] Exchange(distribution=[hash[p1]])\\n:  +- [#2] Exchange(distribution=[hash[p]], shuffle_mode=[BATCH])\\n+- HashJoin(joinType=[InnerJoin], where=[(p2 = p)], select=[a2, b2, c2, p2, x, y, z, p], build=[right])\\n   :- [#3] Exchange(distribution=[hash[p2]])\\n   +- [#2] Exchange(distribution=[hash[p]], shuffle_mode=[BATCH])\\n])",
     "parallelism" : 10,
     "predecessors" : [ {
       "id" : ,
diff --git a/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/nodes/exec/operator/BatchOperatorNameTest.xml b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/nodes/exec/operator/BatchOperatorNameTest.xml
index 13183114bc6..bfa8c9deee9 100644
--- a/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/nodes/exec/operator/BatchOperatorNameTest.xml
+++ b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/nodes/exec/operator/BatchOperatorNameTest.xml
@@ -1241,15 +1241,15 @@ NestedLoopJoin(joinType=[InnerJoin], where=[(a = d0)], select=[a, b, c, d, a0, b
 {
   "nodes" : [ {
     "id" : ,
-    "type" : "Source: TableSourceScan(table=[[default_catalog, default_database, A]], fields=[a, b, c, d])",
+    "type" : "Source: TableSourceScan(table=[[default_catalog, default_database, B]], fields=[a, b, c, d])",
     "pact" : "Data Source",
-    "contents" : "TableSourceScan(table=[[default_catalog, default_database, A]], fields=[a, b, c, d])",
+    "contents" : "TableSourceScan(table=[[default_catalog, default_database, B]], fields=[a, b, c, d])",
     "parallelism" : 1
   }, {
     "id" : ,
-    "type" : "Source: TableSourceScan(table=[[default_catalog, default_database, B]], fields=[a, b, c, d])",
+    "type" : "Source: TableSourceScan(table=[[default_catalog, default_database, A]], fields=[a, b, c, d])",
     "pact" : "Data Source",
-    "contents" : "TableSourceScan(table=[[default_catalog, default_database, B]], fields=[a, b, c, d])",
+    "contents" : "TableSourceScan(table=[[default_catalog, default_database, A]], fields=[a, b, c, d])",
     "parallelism" : 1
   }, {
     "id" : ,
@@ -1295,15 +1295,15 @@ NestedLoopJoin(joinType=[InnerJoin], where=[(a = d0)], select=[a, b, c, d, a0, b
 {
   "nodes" : [ {
     "id" : ,
-    "type" : "Source: A[]",
+    "type" : "Source: B[]",
     "pact" : "Data Source",
-    "contents" : "[]:TableSourceScan(table=[[default_catalog, default_database, A]], fields=[a, b, c, d])",
+    "contents" : "[]:TableSourceScan(table=[[default_catalog, default_database, B]], fields=[a, b, c, d])",
     "parallelism" : 1
   }, {
     "id" : ,
-    "type" : "Source: B[]",
+    "type" : "Source: A[]",
     "pact" : "Data Source",
-    "contents" : "[]:TableSourceScan(table=[[default_catalog, default_database, B]], fields=[a, b, c, d])",
+    "contents" : "[]:TableSourceScan(table=[[default_catalog, default_database, A]], fields=[a, b, c, d])",
     "parallelism" : 1
   }, {
     "id" : ,
diff --git a/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/optimize/program/DynamicPartitionPruningProgramTest.xml b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/optimize/program/DynamicPartitionPruningProgramTest.xml
index 4ba1f656e7e..9f726d1ee30 100644
--- a/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/optimize/program/DynamicPartitionPruningProgramTest.xml
+++ b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/optimize/program/DynamicPartitionPruningProgramTest.xml
@@ -170,18 +170,20 @@ LogicalProject(id=[$0], price=[$1], amount=[$2])
     <Resource name="optimized exec plan">
       <![CDATA[
 Calc(select=[id, price, amount])
-+- MultipleInput(readOrder=[0,0,1,1], members=[\nHashJoin(joinType=[InnerJoin], where=[(amount = id0)], select=[id, price, amount, id0], isBroadcast=[true], build=[right])\n:- Calc(select=[id, price, amount])\n:  +- HashJoin(joinType=[InnerJoin], where=[(fact_date_sk = dim_date_sk)], select=[id, price, amount, fact_date_sk, dim_date_sk], isBroadcast=[true], build=[right])\n:     :- Union(all=[true], union=[id, price, amount, fact_date_sk])\n:     :  :- [#3] DynamicFilteringTableSourceScan(table=[[testCatalog, test_database, fact_part, project=[id, price, amount, fact_date_sk], metadata=[]]], fields=[id, price, amount, fact_date_sk])\n:     :  +- [#4] DynamicFilteringTableSourceScan(table=[[testCatalog, test_database, fact_part2, project=[id, price, amount, fact_date_sk], metadata=[]]], fields=[id, price, amount, fact_date_sk])\n:     +- [#2] Exchange(distribution=[broadcast])\n+- [#1] Exchange(distribution=[broadcast])\n])
++- MultipleInput(readOrder=[0,0,0,1,0,1], members=[\nHashJoin(joinType=[InnerJoin], where=[(amount = id0)], select=[id, price, amount, id0], isBroadcast=[true], build=[right])\n:- Calc(select=[id, price, amount])\n:  +- HashJoin(joinType=[InnerJoin], where=[(fact_date_sk = dim_date_sk)], select=[id, price, amount, fact_date_sk, dim_date_sk], isBroadcast=[true], build=[right])\n:     :- Union(all=[true], union=[id, price, amount, fact_date_sk])\n:     :  :- OrderEnforcer\n:     :  :  :- [#3] Exchange(distribution=[any], shuffle_mode=[BATCH])\n:     :  :  +- [#4] DynamicFilteringTableSourceScan(table=[[testCatalog, test_database, fact_part, project=[id, price, amount, fact_date_sk], metadata=[]]], fields=[id, price, amount, fact_date_sk])\n:     :  +- OrderEnforcer\n:     :     :- [#5] Exchange(distribution=[any], shuffle_mode=[BATCH])\n:     :     +- [#6] DynamicFilteringTableSourceScan(table=[[testCatalog, test_database, fact_part2, project=[id, price, amount, fact_date_sk], metadata=[]]], fields=[id, price, amount, fact_date_sk])\n:     +- [#2] Exchange(distribution=[broadcast])\n+- [#1] Exchange(distribution=[broadcast])\n])
    :- Exchange(distribution=[broadcast])
    :  +- Calc(select=[id], where=[(amount < 10)])(reuse_id=[2])
    :     +- TableSourceScan(table=[[testCatalog, test_database, dim, filter=[], project=[id, amount, price, dim_date_sk], metadata=[]]], fields=[id, amount, price, dim_date_sk])(reuse_id=[1])
    :- Exchange(distribution=[broadcast])
    :  +- Calc(select=[dim_date_sk], where=[SEARCH(price, Sarg[(300..500)])])
    :     +- Reused(reference_id=[1])
-   :- DynamicFilteringTableSourceScan(table=[[testCatalog, test_database, fact_part, project=[id, price, amount, fact_date_sk], metadata=[]]], fields=[id, price, amount, fact_date_sk])
+   :- Exchange(distribution=[any], shuffle_mode=[BATCH])
    :  +- DynamicFilteringDataCollector(fields=[id])(reuse_id=[3])
    :     +- Reused(reference_id=[2])
+   :- DynamicFilteringTableSourceScan(table=[[testCatalog, test_database, fact_part, project=[id, price, amount, fact_date_sk], metadata=[]]], fields=[id, price, amount, fact_date_sk])
+   :- Exchange(distribution=[any], shuffle_mode=[BATCH])
+   :  +- Reused(reference_id=[3])
    +- DynamicFilteringTableSourceScan(table=[[testCatalog, test_database, fact_part2, project=[id, price, amount, fact_date_sk], metadata=[]]], fields=[id, price, amount, fact_date_sk])
-      +- Reused(reference_id=[3])
 ]]>
     </Resource>
   </TestCase>
@@ -311,10 +313,12 @@ Union(all=[true], union=[id, price, amount])
 :- Calc(select=[id, price, amount])
 :  +- HashJoin(joinType=[InnerJoin], where=[(fact_date_sk = dim_date_sk)], select=[id, amount, price, fact_date_sk, dim_date_sk], build=[right])
 :     :- Exchange(distribution=[hash[fact_date_sk]])
-:     :  +- DynamicFilteringTableSourceScan(table=[[testCatalog, test_database, fact_part, project=[id, amount, price, fact_date_sk], metadata=[]]], fields=[id, amount, price, fact_date_sk])
-:     :     +- DynamicFilteringDataCollector(fields=[dim_date_sk])
-:     :        +- Calc(select=[dim_date_sk], where=[(price < 500)])(reuse_id=[1])
-:     :           +- TableSourceScan(table=[[testCatalog, test_database, dim, filter=[], project=[price, dim_date_sk], metadata=[]]], fields=[price, dim_date_sk])
+:     :  +- MultipleInput(members=[\nOrderEnforcer\n:- [#1] Exchange(distribution=[any], shuffle_mode=[BATCH])\n+- [#2] DynamicFilteringTableSourceScan(table=[[testCatalog, test_database, fact_part, project=[id, amount, price, fact_date_sk], metadata=[]]], fields=[id, amount, price, fact_date_sk])\n])
+:     :     :- Exchange(distribution=[any], shuffle_mode=[BATCH])
+:     :     :  +- DynamicFilteringDataCollector(fields=[dim_date_sk])
+:     :     :     +- Calc(select=[dim_date_sk], where=[(price < 500)])(reuse_id=[1])
+:     :     :        +- TableSourceScan(table=[[testCatalog, test_database, dim, filter=[], project=[price, dim_date_sk], metadata=[]]], fields=[price, dim_date_sk])
+:     :     +- DynamicFilteringTableSourceScan(table=[[testCatalog, test_database, fact_part, project=[id, amount, price, fact_date_sk], metadata=[]]], fields=[id, amount, price, fact_date_sk])
 :     +- Exchange(distribution=[hash[dim_date_sk]])
 :        +- Reused(reference_id=[1])
 +- TableSourceScan(table=[[testCatalog, test_database, fact_part, project=[id, price, amount], metadata=[]]], fields=[id, price, amount])
diff --git a/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/optimize/program/FlinkRuntimeFilterProgramTest.xml b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/optimize/program/FlinkRuntimeFilterProgramTest.xml
index 205ac0d1467..f3eaf8b9b0d 100644
--- a/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/optimize/program/FlinkRuntimeFilterProgramTest.xml
+++ b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/optimize/program/FlinkRuntimeFilterProgramTest.xml
@@ -1146,9 +1146,11 @@ HashJoin(joinType=[InnerJoin], where=[(fact_date_sk = dim_date_sk)], select=[id,
 :  +- Calc(select=[id, male, amount, price, dim_date_sk], where=[(price < 500)])(reuse_id=[1])
 :     +- TableSourceScan(table=[[testCatalog, test_database, dim, filter=[]]], fields=[id, male, amount, price, dim_date_sk])
 +- Exchange(distribution=[hash[fact_date_sk]])
-   +- DynamicFilteringTableSourceScan(table=[[testCatalog, test_database, fact_part]], fields=[id, name, amount, price, fact_date_sk])
-      +- DynamicFilteringDataCollector(fields=[dim_date_sk])
-         +- Reused(reference_id=[1])
+   +- MultipleInput(members=[\nOrderEnforcer\n:- [#1] Exchange(distribution=[any], shuffle_mode=[BATCH])\n+- [#2] DynamicFilteringTableSourceScan(table=[[testCatalog, test_database, fact_part]], fields=[id, name, amount, price, fact_date_sk])\n])
+      :- Exchange(distribution=[any], shuffle_mode=[BATCH])
+      :  +- DynamicFilteringDataCollector(fields=[dim_date_sk])
+      :     +- Reused(reference_id=[1])
+      +- DynamicFilteringTableSourceScan(table=[[testCatalog, test_database, fact_part]], fields=[id, name, amount, price, fact_date_sk])
 ]]>
 		</Resource>
 	</TestCase>
diff --git a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/dynamicfiltering/ExecutionOrderEnforcerOperator.java b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/dynamicfiltering/ExecutionOrderEnforcerOperator.java
deleted file mode 100644
index caa74ed3713..00000000000
--- a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/dynamicfiltering/ExecutionOrderEnforcerOperator.java
+++ /dev/null
@@ -1,71 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.table.runtime.operators.dynamicfiltering;
-
-import org.apache.flink.streaming.api.operators.AbstractInput;
-import org.apache.flink.streaming.api.operators.AbstractStreamOperatorV2;
-import org.apache.flink.streaming.api.operators.Input;
-import org.apache.flink.streaming.api.operators.MultipleInputStreamOperator;
-import org.apache.flink.streaming.api.operators.Output;
-import org.apache.flink.streaming.api.operators.StreamOperatorParameters;
-import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
-
-import java.util.Arrays;
-import java.util.List;
-
-/**
- * ExecutionOrderEnforcerOperator has two inputs, one of which is a source, and the other is the
- * dependent upstream. It enforces that the input source is executed after the dependent input is
- * finished. Everything passed from the inputs is forwarded to the output, though typically the
- * dependent input should not send anything.
- *
- * <p>The operator must be chained with the source, which is generally ensured by the {@link
- * ExecutionOrderEnforcerOperatorFactory}. If chaining is explicitly disabled, the enforcer can not
- * work as expected.
- *
- * <p>The operator is used only for dynamic filtering at present.
- */
-public class ExecutionOrderEnforcerOperator<IN> extends AbstractStreamOperatorV2<IN>
-        implements MultipleInputStreamOperator<IN> {
-
-    public ExecutionOrderEnforcerOperator(StreamOperatorParameters<IN> parameters) {
-        super(parameters, 2);
-    }
-
-    @Override
-    public List<Input> getInputs() {
-        return Arrays.asList(
-                new ForwardingInput<>(this, 1, output), new ForwardingInput<>(this, 2, output));
-    }
-
-    private static class ForwardingInput<IN> extends AbstractInput<IN, IN> {
-        private final Output<StreamRecord<IN>> output;
-
-        public ForwardingInput(
-                AbstractStreamOperatorV2<IN> owner, int inputId, Output<StreamRecord<IN>> output) {
-            super(owner, inputId);
-            this.output = output;
-        }
-
-        @Override
-        public void processElement(StreamRecord<IN> element) throws Exception {
-            output.collect(element);
-        }
-    }
-}
diff --git a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/dynamicfiltering/ExecutionOrderEnforcerOperatorFactory.java b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/dynamicfiltering/ExecutionOrderEnforcerOperatorFactory.java
deleted file mode 100644
index 291d16d2a46..00000000000
--- a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/dynamicfiltering/ExecutionOrderEnforcerOperatorFactory.java
+++ /dev/null
@@ -1,47 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.table.runtime.operators.dynamicfiltering;
-
-import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;
-import org.apache.flink.streaming.api.operators.ChainingStrategy;
-import org.apache.flink.streaming.api.operators.StreamOperator;
-import org.apache.flink.streaming.api.operators.StreamOperatorParameters;
-
-/**
- * The factory class for {@link ExecutionOrderEnforcerOperator}. This is a simple operator factory
- * whose chaining strategy is always ChainingStrategy.HEAD_WITH_SOURCES.
- */
-public class ExecutionOrderEnforcerOperatorFactory<IN> extends AbstractStreamOperatorFactory<IN> {
-
-    @Override
-    public <T extends StreamOperator<IN>> T createStreamOperator(
-            StreamOperatorParameters<IN> parameters) {
-        return (T) new ExecutionOrderEnforcerOperator<>(parameters);
-    }
-
-    @Override
-    public ChainingStrategy getChainingStrategy() {
-        return ChainingStrategy.HEAD_WITH_SOURCES;
-    }
-
-    @Override
-    public Class<? extends StreamOperator> getStreamOperatorClass(ClassLoader classLoader) {
-        return ExecutionOrderEnforcerOperator.class;
-    }
-}
