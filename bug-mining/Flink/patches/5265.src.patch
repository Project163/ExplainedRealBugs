diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/NestedProjectionUtil.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/NestedProjectionUtil.scala
index 17bde486630..bdb3856e215 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/NestedProjectionUtil.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/NestedProjectionUtil.scala
@@ -259,6 +259,14 @@ private class NestedSchemaRewriter(schema: NestedSchema, builder: RexBuilder) ex
             (Option.empty, child)
           }
         }
+      case expr =>
+        // rewrite operands of the expression
+        val newExpr = expr.accept(this)
+        // rebuild FieldAccess
+        (
+          Some(
+            builder.makeFieldAccess(newExpr, fieldAccess.getField.getName, true)),
+          null)
     }
   }
 }
@@ -269,18 +277,25 @@ private class NestedSchemaRewriter(schema: NestedSchema, builder: RexBuilder) ex
 private class NestedSchemaExtractor(schema: NestedSchema) extends RexVisitorImpl[Unit](true) {
 
   override def visitFieldAccess(fieldAccess: RexFieldAccess): Unit = {
-    def internalVisit(fieldAccess: RexFieldAccess): (Int, List[String]) = {
+    def internalVisit(fieldAccess: RexFieldAccess): (Boolean, Int, List[String]) = {
       fieldAccess.getReferenceExpr match {
         case ref: RexInputRef =>
-          (ref.getIndex, List(ref.getName, fieldAccess.getField.getName))
+          (true, ref.getIndex, List(ref.getName, fieldAccess.getField.getName))
         case fac: RexFieldAccess =>
-          val (i, n) = internalVisit(fac)
-          (i, n :+ fieldAccess.getField.getName)
+          val (success, i, n) = internalVisit(fac)
+          (success, i, if (success) n :+ fieldAccess.getField.getName else null)
+        case expr =>
+          // only extract operands of the expression
+          expr.accept(this)
+          (false, -1, null)
       }
     }
 
     // extract the info
-    val (index, names) = internalVisit(fieldAccess)
+    val (success, index, names) = internalVisit(fieldAccess)
+    if (!success) {
+      return
+    }
 
     val topLevelNodeName = schema.inputRowType.getFieldNames.get(index)
     val topLevelNode = if (!schema.columns.contains(topLevelNodeName)) {
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeExtractor.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeExtractor.scala
index abc313042a2..40bc9c9984e 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeExtractor.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeExtractor.scala
@@ -326,17 +326,23 @@ class RefFieldAccessorVisitor(usedFields: Array[Int]) extends RexVisitorImpl[Uni
   }
 
   override def visitFieldAccess(fieldAccess: RexFieldAccess): Unit = {
-    def internalVisit(fieldAccess: RexFieldAccess): (Int, List[String]) = {
+    def internalVisit(fieldAccess: RexFieldAccess): (Boolean, Int, List[String]) = {
       fieldAccess.getReferenceExpr match {
         case ref: RexInputRef =>
-          (ref.getIndex, List(fieldAccess.getField.getName))
+          (true, ref.getIndex, List(fieldAccess.getField.getName))
         case fac: RexFieldAccess =>
-          val (i, n) = internalVisit(fac)
-          (i, n :+ fieldAccess.getField.getName)
+          val (success, i, n) = internalVisit(fac)
+          (success, i, if (success) n :+ fieldAccess.getField.getName else null)
+        case expr =>
+          expr.accept(this)
+          (false, -1, null)
       }
     }
 
-    val (index, fullName) = internalVisit(fieldAccess)
+    val (success, index, fullName) = internalVisit(fieldAccess)
+    if (!success) {
+      return
+    }
     val outputIndex = order.getOrElse(index, -1)
     val fields: List[List[String]] = projectedFields(outputIndex)
     projectedFields(outputIndex) = fields :+ fullName
diff --git a/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRuleTest.java b/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRuleTest.java
index 6d92c450c9e..007f012e9b1 100644
--- a/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRuleTest.java
+++ b/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRuleTest.java
@@ -116,6 +116,40 @@ public class PushProjectIntoTableSourceScanRuleTest
                         + " 'readable-metadata' = 'metadata_1:INT, metadata_2:STRING, metadata_3:BIGINT'"
                         + ")";
         util().tableEnv().executeSql(ddl5);
+
+        String ddl6 =
+                "CREATE TABLE NestedItemTable (\n"
+                        + "  `ID` INT,\n"
+                        + "  `Timestamp` TIMESTAMP(3),\n"
+                        + "  `Result` ROW<\n"
+                        + "    `Mid` ROW<"
+                        + "      `data_arr` ROW<`value` BIGINT> ARRAY,\n"
+                        + "      `data_map` MAP<STRING, ROW<`value` BIGINT>>"
+                        + "     >"
+                        + "   >,\n"
+                        + "   WATERMARK FOR `Timestamp` AS `Timestamp`\n"
+                        + ") WITH (\n"
+                        + " 'connector' = 'values',\n"
+                        + " 'nested-projection-supported' = 'true',"
+                        + " 'bounded' = 'true'\n"
+                        + ")";
+        util().tableEnv().executeSql(ddl6);
+
+        String ddl7 =
+                "CREATE TABLE ItemTable (\n"
+                        + "  `ID` INT,\n"
+                        + "  `Timestamp` TIMESTAMP(3),\n"
+                        + "  `Result` ROW<\n"
+                        + "    `data_arr` ROW<`value` BIGINT> ARRAY,\n"
+                        + "    `data_map` MAP<STRING, ROW<`value` BIGINT>>>,\n"
+                        + "  `outer_array` ARRAY<INT>,\n"
+                        + "  `outer_map` MAP<STRING, STRING>,\n"
+                        + "   WATERMARK FOR `Timestamp` AS `Timestamp`\n"
+                        + ") WITH (\n"
+                        + " 'connector' = 'values',\n"
+                        + " 'bounded' = 'true'\n"
+                        + ")";
+        util().tableEnv().executeSql(ddl7);
     }
 
     @Test
@@ -168,4 +202,45 @@ public class PushProjectIntoTableSourceScanRuleTest
 
         util().verifyRelPlan(sqlQuery);
     }
+
+    @Test
+    public void testNestedProjectFieldAccessWithITEM() {
+        util().verifyRelPlan(
+                        "SELECT "
+                                + "`Result`.`Mid`.data_arr[ID].`value`, "
+                                + "`Result`.`Mid`.data_map['item'].`value` "
+                                + "FROM NestedItemTable");
+    }
+
+    @Test
+    public void testNestedProjectFieldAccessWithITEMWithConstantIndex() {
+        util().verifyRelPlan(
+                        "SELECT "
+                                + "`Result`.`Mid`.data_arr[2].`value`, "
+                                + "`Result`.`Mid`.data_arr "
+                                + "FROM NestedItemTable");
+    }
+
+    @Test
+    public void testNestedProjectFieldAccessWithITEMContainsTopLevelAccess() {
+        util().verifyRelPlan(
+                        "SELECT "
+                                + "`Result`.`Mid`.data_arr[2].`value`, "
+                                + "`Result`.`Mid`.data_arr[ID].`value`, "
+                                + "`Result`.`Mid`.data_map['item'].`value`, "
+                                + "`Result`.`Mid` "
+                                + "FROM NestedItemTable");
+    }
+
+    @Test
+    public void testProjectFieldAccessWithITEM() {
+        util().verifyRelPlan(
+                        "SELECT "
+                                + "`Result`.data_arr[ID].`value`, "
+                                + "`Result`.data_map['item'].`value`, "
+                                + "`outer_array`[1], "
+                                + "`outer_array`[ID], "
+                                + "`outer_map`['item'] "
+                                + "FROM ItemTable");
+    }
 }
diff --git a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/TableSourceTest.xml b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/TableSourceTest.xml
index 1a6d96e2a0c..a6db72296d9 100644
--- a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/TableSourceTest.xml
+++ b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/TableSourceTest.xml
@@ -40,58 +40,63 @@ Calc(select=[id, deepNested_nested1_name AS nestedName, nested_value AS nestedVa
 ]]>
     </Resource>
   </TestCase>
-  <TestCase name="testNestProjectWithMetadata">
+  <TestCase name="testSimpleProject">
     <Resource name="sql">
-      <![CDATA[
-SELECT id,
-       deepNested.nested1 AS nested1,
-       deepNested.nested1.`value` + deepNested.nested2.num + metadata_1 as results
-FROM T
-]]>
+      <![CDATA[SELECT a, c FROM ProjectableTable]]>
     </Resource>
     <Resource name="ast">
       <![CDATA[
-LogicalProject(id=[$0], nested1=[$1.nested1], results=[+(+($1.nested1.value, $1.nested2.num), $2)])
-+- LogicalTableScan(table=[[default_catalog, default_database, T]])
+LogicalProject(a=[$0], c=[$2])
++- LogicalTableScan(table=[[default_catalog, default_database, ProjectableTable]])
 ]]>
     </Resource>
     <Resource name="optimized exec plan">
       <![CDATA[
-Calc(select=[id, deepNested_nested1 AS nested1, ((deepNested_nested1.value + deepNested_nested2_num) + metadata_1) AS results])
-+- TableSourceScan(table=[[default_catalog, default_database, T, project=[id, deepNested_nested1, deepNested_nested2_num, metadata_1]]], fields=[id, deepNested_nested1, deepNested_nested2_num, metadata_1])
+TableSourceScan(table=[[default_catalog, default_database, ProjectableTable, project=[a, c]]], fields=[a, c])
 ]]>
     </Resource>
   </TestCase>
-  <TestCase name="testSimpleProject">
+  <TestCase name="testNestedProjectFieldWithITEM">
     <Resource name="sql">
-      <![CDATA[SELECT a, c FROM ProjectableTable]]>
+      <![CDATA[
+SELECT
+  `result`.`data_arr`[`id`].`value`,
+  `result`.`data_map`['item'].`value`
+FROM NestedItemTable
+]]>
     </Resource>
     <Resource name="ast">
       <![CDATA[
-LogicalProject(a=[$0], c=[$2])
-+- LogicalTableScan(table=[[default_catalog, default_database, ProjectableTable]])
+LogicalProject(EXPR$0=[ITEM($2.data_arr, $0).value], EXPR$1=[ITEM($2.data_map, _UTF-16LE'item').value])
++- LogicalTableScan(table=[[default_catalog, default_database, NestedItemTable]])
 ]]>
     </Resource>
     <Resource name="optimized exec plan">
       <![CDATA[
-TableSourceScan(table=[[default_catalog, default_database, ProjectableTable, project=[a, c]]], fields=[a, c])
+Calc(select=[ITEM(result_data_arr, id).value AS EXPR$0, ITEM(result_data_map, _UTF-16LE'item').value AS EXPR$1])
++- TableSourceScan(table=[[default_catalog, default_database, NestedItemTable, project=[result_data_arr, result_data_map, id]]], fields=[result_data_arr, result_data_map, id])
 ]]>
     </Resource>
   </TestCase>
-  <TestCase name="testSimpleProjectWithProctime">
+  <TestCase name="testNestProjectWithMetadata">
     <Resource name="sql">
-      <![CDATA[SELECT a, c, PROCTIME() FROM ProjectableTable]]>
+      <![CDATA[
+SELECT id,
+       deepNested.nested1 AS nested1,
+       deepNested.nested1.`value` + deepNested.nested2.num + metadata_1 as results
+FROM T
+]]>
     </Resource>
     <Resource name="ast">
       <![CDATA[
-LogicalProject(a=[$0], c=[$2], EXPR$2=[PROCTIME()])
-+- LogicalTableScan(table=[[default_catalog, default_database, ProjectableTable]])
+LogicalProject(id=[$0], nested1=[$1.nested1], results=[+(+($1.nested1.value, $1.nested2.num), $2)])
++- LogicalTableScan(table=[[default_catalog, default_database, T]])
 ]]>
     </Resource>
     <Resource name="optimized exec plan">
       <![CDATA[
-Calc(select=[a, c, PROCTIME_MATERIALIZE(PROCTIME()) AS EXPR$2])
-+- TableSourceScan(table=[[default_catalog, default_database, ProjectableTable, project=[a, c]]], fields=[a, c])
+Calc(select=[id, deepNested_nested1 AS nested1, ((deepNested_nested1.value + deepNested_nested2_num) + metadata_1) AS results])
++- TableSourceScan(table=[[default_catalog, default_database, T, project=[id, deepNested_nested1, deepNested_nested2_num, metadata_1]]], fields=[id, deepNested_nested1, deepNested_nested2_num, metadata_1])
 ]]>
     </Resource>
   </TestCase>
@@ -112,6 +117,23 @@ HashAggregate(isMerge=[true], select=[Final_COUNT(count1$0) AS EXPR$0])
 +- Exchange(distribution=[single])
    +- LocalHashAggregate(select=[Partial_COUNT(*) AS count1$0])
       +- TableSourceScan(table=[[default_catalog, default_database, ProjectableTable, project=[]]], fields=[])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testSimpleProjectWithProctime">
+    <Resource name="sql">
+      <![CDATA[SELECT a, c, PROCTIME() FROM ProjectableTable]]>
+    </Resource>
+    <Resource name="ast">
+      <![CDATA[
+LogicalProject(a=[$0], c=[$2], EXPR$2=[PROCTIME()])
++- LogicalTableScan(table=[[default_catalog, default_database, ProjectableTable]])
+]]>
+    </Resource>
+    <Resource name="optimized exec plan">
+      <![CDATA[
+Calc(select=[a, c, PROCTIME_MATERIALIZE(PROCTIME()) AS EXPR$2])
++- TableSourceScan(table=[[default_catalog, default_database, ProjectableTable, project=[a, c]]], fields=[a, c])
 ]]>
     </Resource>
   </TestCase>
diff --git a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRuleTest.xml b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRuleTest.xml
index 80ab456edfd..6dfaa2e9639 100644
--- a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRuleTest.xml
+++ b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRuleTest.xml
@@ -47,6 +47,23 @@ LogicalProject(a=[$0], c=[$2], d=[+($0, 1)], EXPR$3=[+($1, 1)])
       <![CDATA[
 LogicalProject(a=[$0], c=[$2], d=[+($0, 1)], EXPR$3=[+($1, 1)])
 +- LogicalTableScan(table=[[default_catalog, default_database, VirtualTable]])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testNestedProjectFieldAccessWithITEMWithConstantIndex">
+    <Resource name="sql">
+      <![CDATA[SELECT `Result`.`Mid`.data_arr[2].`value`, `Result`.`Mid`.data_arr FROM NestedItemTable]]>
+    </Resource>
+    <Resource name="ast">
+      <![CDATA[
+LogicalProject(EXPR$0=[ITEM($2.Mid.data_arr, 2).value], data_arr=[$2.Mid.data_arr])
++- LogicalTableScan(table=[[default_catalog, default_database, NestedItemTable]])
+]]>
+    </Resource>
+    <Resource name="optimized rel plan">
+      <![CDATA[
+LogicalProject(EXPR$0=[ITEM($0, 2).value], data_arr=[$0])
++- LogicalTableScan(table=[[default_catalog, default_database, NestedItemTable, project=[Result_Mid_data_arr]]])
 ]]>
     </Resource>
   </TestCase>
@@ -69,7 +86,63 @@ LogicalProject(id=[$0], nestedName=[$1], nestedSum=[+($2, $3)])
 ]]>
     </Resource>
   </TestCase>
-  <TestCase name="testNestProjectWithUpsertSource">
+  <TestCase name="testNestedProject">
+    <Resource name="sql">
+      <![CDATA[SELECT id,
+    deepNested.nested1.name AS nestedName,
+    nested.`value` AS nestedValue,
+    deepNested.nested2.flag AS nestedFlag,
+    deepNested.nested2.num AS nestedNum
+FROM NestedTable]]>
+    </Resource>
+    <Resource name="ast">
+      <![CDATA[
+LogicalProject(id=[$0], nestedName=[$1.nested1.name], nestedValue=[$2.value], nestedFlag=[$1.nested2.flag], nestedNum=[$1.nested2.num])
++- LogicalTableScan(table=[[default_catalog, default_database, NestedTable]])
+]]>
+    </Resource>
+    <Resource name="optimized rel plan">
+      <![CDATA[
+LogicalProject(id=[$0], nestedName=[$1], nestedValue=[$4], nestedFlag=[$2], nestedNum=[$3])
++- LogicalTableScan(table=[[default_catalog, default_database, NestedTable, project=[id, deepNested_nested1_name, deepNested_nested2_flag, deepNested_nested2_num, nested_value]]])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testNestedProjectFieldAccessWithITEM">
+    <Resource name="sql">
+      <![CDATA[SELECT `Result`.`Mid`.data_arr[ID].`value`, `Result`.`Mid`.data_map['item'].`value` FROM NestedItemTable]]>
+    </Resource>
+    <Resource name="ast">
+      <![CDATA[
+LogicalProject(EXPR$0=[ITEM($2.Mid.data_arr, $0).value], EXPR$1=[ITEM($2.Mid.data_map, _UTF-16LE'item').value])
++- LogicalTableScan(table=[[default_catalog, default_database, NestedItemTable]])
+]]>
+    </Resource>
+    <Resource name="optimized rel plan">
+      <![CDATA[
+LogicalProject(EXPR$0=[ITEM($0, $2).value], EXPR$1=[ITEM($1, _UTF-16LE'item').value])
++- LogicalTableScan(table=[[default_catalog, default_database, NestedItemTable, project=[Result_Mid_data_arr, Result_Mid_data_map, ID]]])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testNestedProjectFieldAccessWithITEMContainsTopLevelAccess">
+    <Resource name="sql">
+      <![CDATA[SELECT `Result`.`Mid`.data_arr[2].`value`, `Result`.`Mid`.data_arr[ID].`value`, `Result`.`Mid`.data_map['item'].`value`, `Result`.`Mid` FROM NestedItemTable]]>
+    </Resource>
+    <Resource name="ast">
+      <![CDATA[
+LogicalProject(EXPR$0=[ITEM($2.Mid.data_arr, 2).value], EXPR$1=[ITEM($2.Mid.data_arr, $0).value], EXPR$2=[ITEM($2.Mid.data_map, _UTF-16LE'item').value], Mid=[$2.Mid])
++- LogicalTableScan(table=[[default_catalog, default_database, NestedItemTable]])
+]]>
+    </Resource>
+    <Resource name="optimized rel plan">
+      <![CDATA[
+LogicalProject(EXPR$0=[ITEM($0.data_arr, 2).value], EXPR$1=[ITEM($0.data_arr, $1).value], EXPR$2=[ITEM($0.data_map, _UTF-16LE'item').value], Mid=[$0])
++- LogicalTableScan(table=[[default_catalog, default_database, NestedItemTable, project=[Result_Mid, ID]]])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testNestProjectWithMetadata">
     <Resource name="sql">
       <![CDATA[SELECT id,    deepNested.nested1 AS nested1,
     deepNested.nested1.`value` + deepNested.nested2.num + metadata_1 as results
@@ -88,25 +161,22 @@ LogicalProject(id=[$0], nested1=[$1], results=[+(+($1.value, $2), $3)])
 ]]>
     </Resource>
   </TestCase>
-  <TestCase name="testNestedProject">
+  <TestCase name="testNestProjectWithUpsertSource">
     <Resource name="sql">
-      <![CDATA[SELECT id,
-    deepNested.nested1.name AS nestedName,
-    nested.`value` AS nestedValue,
-    deepNested.nested2.flag AS nestedFlag,
-    deepNested.nested2.num AS nestedNum
-FROM NestedTable]]>
+      <![CDATA[SELECT id,    deepNested.nested1 AS nested1,
+    deepNested.nested1.`value` + deepNested.nested2.num + metadata_1 as results
+FROM MetadataTable]]>
     </Resource>
     <Resource name="ast">
       <![CDATA[
-LogicalProject(id=[$0], nestedName=[$1.nested1.name], nestedValue=[$2.value], nestedFlag=[$1.nested2.flag], nestedNum=[$1.nested2.num])
-+- LogicalTableScan(table=[[default_catalog, default_database, NestedTable]])
+LogicalProject(id=[$0], nested1=[$1.nested1], results=[+(+($1.nested1.value, $1.nested2.num), $2)])
++- LogicalTableScan(table=[[default_catalog, default_database, MetadataTable]])
 ]]>
     </Resource>
     <Resource name="optimized rel plan">
       <![CDATA[
-LogicalProject(id=[$0], nestedName=[$1], nestedValue=[$4], nestedFlag=[$2], nestedNum=[$3])
-+- LogicalTableScan(table=[[default_catalog, default_database, NestedTable, project=[id, deepNested_nested1_name, deepNested_nested2_flag, deepNested_nested2_num, nested_value]]])
+LogicalProject(id=[$0], nested1=[$1], results=[+(+($1.value, $2), $3)])
++- LogicalTableScan(table=[[default_catalog, default_database, MetadataTable, project=[id, deepNested_nested1, deepNested_nested2_num, metadata_1]]])
 ]]>
     </Resource>
   </TestCase>
@@ -128,22 +198,20 @@ LogicalProject(id=[$0], EXPR$1=[ITEM($1, _UTF-16LE'e')])
 ]]>
     </Resource>
   </TestCase>
-  <TestCase name="testNestProjectWithMetadata">
+  <TestCase name="testProjectFieldAccessWithITEM">
     <Resource name="sql">
-      <![CDATA[SELECT id,    deepNested.nested1 AS nested1,
-    deepNested.nested1.`value` + deepNested.nested2.num + metadata_1 as results
-FROM MetadataTable]]>
+      <![CDATA[SELECT `Result`.data_arr[ID].`value`, `Result`.data_map['item'].`value`, `outer_array`[1], `outer_array`[ID], `outer_map`['item'] FROM ItemTable]]>
     </Resource>
     <Resource name="ast">
       <![CDATA[
-LogicalProject(id=[$0], nested1=[$1.nested1], results=[+(+($1.nested1.value, $1.nested2.num), $2)])
-+- LogicalTableScan(table=[[default_catalog, default_database, MetadataTable]])
+LogicalProject(EXPR$0=[ITEM($2.data_arr, $0).value], EXPR$1=[ITEM($2.data_map, _UTF-16LE'item').value], EXPR$2=[ITEM($3, 1)], EXPR$3=[ITEM($3, $0)], EXPR$4=[ITEM($4, _UTF-16LE'item')])
++- LogicalTableScan(table=[[default_catalog, default_database, ItemTable]])
 ]]>
     </Resource>
     <Resource name="optimized rel plan">
       <![CDATA[
-LogicalProject(id=[$0], nested1=[$1], results=[+(+($1.value, $2), $3)])
-+- LogicalTableScan(table=[[default_catalog, default_database, MetadataTable, project=[id, deepNested_nested1, deepNested_nested2_num, metadata_1]]])
+LogicalProject(EXPR$0=[ITEM($0.data_arr, $1).value], EXPR$1=[ITEM($0.data_map, _UTF-16LE'item').value], EXPR$2=[ITEM($2, 1)], EXPR$3=[ITEM($2, $1)], EXPR$4=[ITEM($3, _UTF-16LE'item')])
++- LogicalTableScan(table=[[default_catalog, default_database, ItemTable, project=[Result, ID, outer_array, outer_map]]])
 ]]>
     </Resource>
   </TestCase>
diff --git a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/TableSourceTest.xml b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/TableSourceTest.xml
index 917cc323c79..767ac764b46 100644
--- a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/TableSourceTest.xml
+++ b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/TableSourceTest.xml
@@ -37,6 +37,28 @@ LogicalProject(id=[$0], nestedName=[$1.nested1.name], nestedValue=[$2.value], ne
       <![CDATA[
 Calc(select=[id, deepNested_nested1_name AS nestedName, nested_value AS nestedValue, deepNested_nested2_flag AS nestedFlag, deepNested_nested2_num AS nestedNum])
 +- TableSourceScan(table=[[default_catalog, default_database, T, project=[id, deepNested_nested1_name, deepNested_nested2_flag, deepNested_nested2_num, nested_value]]], fields=[id, deepNested_nested1_name, deepNested_nested2_flag, deepNested_nested2_num, nested_value])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testNestedProjectWithItem">
+    <Resource name="sql">
+      <![CDATA[
+SELECT
+  `result`.`data_arr`[`id`].`value`,
+  `result`.`data_map`['item'].`value`
+FROM NestedItemTable
+]]>
+    </Resource>
+    <Resource name="ast">
+      <![CDATA[
+LogicalProject(EXPR$0=[ITEM($2.data_arr, $0).value], EXPR$1=[ITEM($2.data_map, _UTF-16LE'item').value])
++- LogicalTableScan(table=[[default_catalog, default_database, NestedItemTable]])
+]]>
+    </Resource>
+    <Resource name="optimized exec plan">
+      <![CDATA[
+Calc(select=[ITEM(result_data_arr, id).value AS EXPR$0, ITEM(result_data_map, _UTF-16LE'item').value AS EXPR$1])
++- TableSourceScan(table=[[default_catalog, default_database, NestedItemTable, project=[result_data_arr, result_data_map, id]]], fields=[result_data_arr, result_data_map, id])
 ]]>
     </Resource>
   </TestCase>
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/batch/sql/TableSourceTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/batch/sql/TableSourceTest.scala
index fb5b8d2235e..4a0841df8d0 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/batch/sql/TableSourceTest.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/batch/sql/TableSourceTest.scala
@@ -73,6 +73,22 @@ class TableSourceTest extends TableTestBase {
          |)
          |""".stripMargin
     util.tableEnv.executeSql(ddl3)
+    val ddl4 =
+      s"""
+         |CREATE TABLE NestedItemTable (
+         |  `id` INT,
+         |  `name` STRING,
+         |  `result` ROW<
+         |     `data_arr` ROW<`value` BIGINT> ARRAY,
+         |     `data_map` MAP<STRING, ROW<`value` BIGINT>>>,
+         |  `extra` STRING
+         |  ) WITH (
+         |    'connector' = 'values',
+         |    'nested-projection-supported' = 'true',
+         |    'bounded' = 'true'
+         |)
+         |""".stripMargin
+    util.tableEnv.executeSql(ddl4)
   }
 
   @Test
@@ -115,4 +131,17 @@ class TableSourceTest extends TableTestBase {
         |""".stripMargin
     util.verifyExecPlan(sqlQuery)
   }
+
+  @Test
+  def testNestedProjectFieldWithITEM(): Unit = {
+    //TODO: always push projection into table source in FLINK-22118
+    util.verifyExecPlan(
+      s"""
+         |SELECT
+         |  `result`.`data_arr`[`id`].`value`,
+         |  `result`.`data_map`['item'].`value`
+         |FROM NestedItemTable
+         |""".stripMargin
+    )
+  }
 }
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/stream/sql/TableSourceTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/stream/sql/TableSourceTest.scala
index 436ccb59cab..3a97553e26f 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/stream/sql/TableSourceTest.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/stream/sql/TableSourceTest.scala
@@ -260,4 +260,34 @@ class TableSourceTest extends TableTestBase {
         |""".stripMargin
     util.verifyExecPlan(sqlQuery)
   }
+
+  @Test
+  def testNestedProjectWithItem(): Unit = {
+    util.tableEnv.executeSql(
+      s"""
+         |CREATE TABLE NestedItemTable (
+         |  `id` INT,
+         |  `name` STRING,
+         |  `result` ROW<
+         |     `data_arr` ROW<`value` BIGINT> ARRAY,
+         |     `data_map` MAP<STRING, ROW<`value` BIGINT>>>,
+         |  `extra` STRING
+         |  ) WITH (
+         |    'connector' = 'values',
+         |    'nested-projection-supported' = 'true',
+         |    'bounded' = 'true'
+         |)
+         |""".stripMargin
+    )
+
+    //TODO: always push projection into table source in FLINK-22118
+    util.verifyExecPlan(
+      s"""
+         |SELECT
+         |  `result`.`data_arr`[`id`].`value`,
+         |  `result`.`data_map`['item'].`value`
+         |FROM NestedItemTable
+         |""".stripMargin
+    )
+  }
 }
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/utils/NestedProjectionUtilTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/utils/NestedProjectionUtilTest.scala
index 4454830490a..b8d0d9af69f 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/utils/NestedProjectionUtilTest.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/utils/NestedProjectionUtilTest.scala
@@ -70,7 +70,9 @@ class NestedProjectionUtilTest extends RexNodeTestBase{
       Array(1, 1),
       Array(0),
       Array(2, 0, 0, 0),
-      Array(2, 0, 1, 0)
+      Array(2, 0, 1, 0),
+      Array(3, 1, 0),
+      Array(3, 0)
     )
 
     assertArray(actual, expected)
@@ -153,12 +155,18 @@ class NestedProjectionUtilTest extends RexNodeTestBase{
     // $1 = payment ROW<id BIGINT, amount INT>
     // $2 = field ROW<with ROW<deeper ROW<entry ROW<inside ROW<entry VARCHAR>>>,
     //                         deep ROW<entry VARCHAR>>>
+    // $3 = items ROW<outer INT,
+    //                inner ROW<deep_array
+    //                              ROW<MAP<VARCHAR, ROW<val_inner VARCHAR, val_entry VARCHAR>>>
+    //                          ARRAY>>
 
     // new schema:
     // $0 = payment.amount INT
     // $1 = persons ROW<name VARCHAR, age INT, passport ROW<id VARCHAR, status VARCHAR>>
     // $2 = field.with.deep.entry VARCHAR
     // $3 = field.with.deeper.entry ROW<inside ROW<entry VARCHAR>>
+    // $4 = items.outer
+    // $5 = items.inner.deep_array
 
     // mapping
     // $1.amount -> $0
@@ -167,6 +175,7 @@ class NestedProjectionUtilTest extends RexNodeTestBase{
     // $2.with.deeper.entry.inside.entry -> $3.inside.entry
     // $2.with.deeper.entry -> $3
     // $0 -> $1
+    // $3.inner.deep_array[$3.outer].deep_map['item']
 
     val (exprs, rowType) = buildExprsWithDeepNesting()
     assertTrue(exprs.asScala.map(_.toString) == wrapRefArray(Array(
@@ -175,7 +184,8 @@ class NestedProjectionUtilTest extends RexNodeTestBase{
       "$2.with.deep.entry",
       "$2.with.deeper.entry.inside.entry",
       "$2.with.deeper.entry",
-      "$0"
+      "$0",
+      "ITEM(ITEM($3.inner.deep_array, $3.outer).deep_map, _UTF-16LE'item')"
     )))
     val nestedFields = NestedProjectionUtil.build(exprs, rowType)
     val paths = NestedProjectionUtil.convertToIndexArray(nestedFields)
@@ -183,7 +193,9 @@ class NestedProjectionUtilTest extends RexNodeTestBase{
       Array(1, 1),
       Array(0),
       Array(2, 0, 0, 0),
-      Array(2, 0, 1, 0)
+      Array(2, 0, 1, 0),
+      Array(3, 1, 0),
+      Array(3, 0)
     )
     assertArray(paths, orderedPaths)
     val newExprs =
@@ -195,6 +207,7 @@ class NestedProjectionUtilTest extends RexNodeTestBase{
       "$2",
       "$3.inside.entry",
       "$3",
-      "$1")))
+      "$1",
+      "ITEM(ITEM($4, $5).deep_map, _UTF-16LE'item')")))
   }
 }
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/utils/RexNodeExtractorTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/utils/RexNodeExtractorTest.scala
index 24193d1dd93..7d9400d01d0 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/utils/RexNodeExtractorTest.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/utils/RexNodeExtractorTest.scala
@@ -100,9 +100,10 @@ class RexNodeExtractorTest extends RexNodeTestBase {
     val expected = Array(
       Array(Arrays.asList("amount")),
       Array(Arrays.asList("*")),
-      Array(Arrays.asList("with", "deeper", "entry"), Arrays.asList("with", "deep", "entry")))
+      Array(Arrays.asList("with", "deeper", "entry"), Arrays.asList("with", "deep", "entry")),
+      Array(Arrays.asList("outer"), Arrays.asList("inner", "deep_array")))
 
-    assertThat(usedFields, is(Array(1, 0, 2)))
+    assertThat(usedFields, is(Array(1, 0, 2, 3)))
     assertThat(usedNestedFields, is(expected))
   }
 
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/utils/RexNodeTestBase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/utils/RexNodeTestBase.scala
index ab5042d08b4..9984bfa30a0 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/utils/RexNodeTestBase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/utils/RexNodeTestBase.scala
@@ -131,11 +131,31 @@ abstract class RexNodeTestBase {
       .nestedField("with", withRowType)
       .build
 
+    // deep field access with array/map
+    val valueInnerType = inputOf(typeFactory)
+      .field("val_inner", VARCHAR)
+      .field("val_entry", VARCHAR)
+      .build
+
+    val mapType = inputOf(typeFactory)
+      .nestedField("deep_map",
+        typeFactory.createMapType(typeFactory.createSqlType(INTEGER), valueInnerType))
+      .build
+
+    val arrayInnerType = inputOf(typeFactory)
+      .nestedField("deep_array", typeFactory.createArrayType(mapType, -1))
+      .build
+
+    val deepItems = inputOf(typeFactory)
+      .field("outer", INTEGER)
+      .nestedField("inner", arrayInnerType)
+      .build
+
     val rowType = typeFactory.createStructType(
       Seq(
-      personRow, paymentRow, fieldRowType),
+      personRow, paymentRow, fieldRowType, deepItems),
       Seq(
-        "persons", "payments", "field"
+        "persons", "payments", "field", "items"
       )
     )
 
@@ -146,12 +166,18 @@ abstract class RexNodeTestBase {
     //   field:    [ with: [ deep: [ entry: VARCHAR ],
     //                       deeper: [ entry: [ inside: [entry: VARCHAR ] ] ]
     //             ] ]
+    //   items:    [ outer: INT,
+    //               inner: [ deep_array:
+    //               [ deep_map: key INT - [val_inner VARCHAR, val_entry VARCHAR ] ] ]
+    //             ]
     // ]
 
     val t0 = rexBuilder.makeInputRef(personRow, 0)
     val t1 = rexBuilder.makeInputRef(paymentRow, 1)
     val t2 = rexBuilder.makeInputRef(fieldRowType, 2)
-    val t3 = rexBuilder.makeExactLiteral(BigDecimal.valueOf(10L))
+    val t3 = rexBuilder.makeInputRef(deepItems, 3)
+    val t4 = rexBuilder.makeExactLiteral(BigDecimal.valueOf(10L))
+    val t5 = rexBuilder.makeLiteral("item")
 
     // person
     val person$pass = rexBuilder.makeFieldAccess(t0, "passport", false)
@@ -159,7 +185,7 @@ abstract class RexNodeTestBase {
 
     // payment
     val pay$amount = rexBuilder.makeFieldAccess(t1, "amount", false)
-    val multiplyAmount = rexBuilder.makeCall(SqlStdOperatorTable.MULTIPLY, pay$amount, t3)
+    val multiplyAmount = rexBuilder.makeCall(SqlStdOperatorTable.MULTIPLY, pay$amount, t4)
 
     // field
     val field$with = rexBuilder.makeFieldAccess(t2, "with", false)
@@ -172,6 +198,21 @@ abstract class RexNodeTestBase {
     val field$with$deeper$entry$inside$entry = rexBuilder
       .makeFieldAccess(field$with$deeper$entry$inside, "entry", false)
 
+    // items
+    val items$outer = rexBuilder.makeFieldAccess(t3, "outer", false)
+    val items$inner = rexBuilder.makeFieldAccess(t3, "inner", false)
+    val items$inner$deep_array =
+      rexBuilder.makeCall(
+        SqlStdOperatorTable.ITEM,
+        rexBuilder.makeFieldAccess(items$inner, "deep_array", false),
+        items$outer)
+    val items$inner$deep_array$deep_map =
+      rexBuilder.makeCall(
+        SqlStdOperatorTable.ITEM,
+        rexBuilder.makeFieldAccess(items$inner$deep_array, "deep_map",
+          false),
+        t5)
+
     // Program
     // (
     //   payments.amount * 10),
@@ -182,7 +223,8 @@ abstract class RexNodeTestBase {
     //   persons
     // )
     (List(multiplyAmount, person$pass$stat, field$with$deep$entry,
-      field$with$deeper$entry$inside$entry, field$with$deeper$entry, t0).asJava,
+      field$with$deeper$entry$inside$entry, field$with$deeper$entry, t0,
+      items$inner$deep_array$deep_map).asJava,
       rowType)
   }
 
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/TableSourceITCase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/TableSourceITCase.scala
index 5409e6fec3b..631a3ca7898 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/TableSourceITCase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/TableSourceITCase.scala
@@ -70,11 +70,11 @@ class TableSourceITCase extends BatchTestBase {
          |  id BIGINT,
          |  deepNested ROW<
          |     nested1 ROW<name STRING, `value.` INT>,
-         |     `nested2.` ROW<num INT, flag BOOLEAN>
-         |   >,
-         |   nested ROW<name STRING, `value` INT>,
-         |   name STRING,
-         |   lower_name AS LOWER(name)
+         |     `nested2.` ROW<num INT, flag BOOLEAN>>,
+         |  nested ROW<name STRING, `value` INT>,
+         |  name STRING,
+         |  nestedItem ROW<deepArray ROW<`value` INT> ARRAY, deepMap MAP<STRING, INT>>,
+         |  lower_name AS LOWER(name)
          |) WITH (
          |  'connector' = 'values',
          |  'nested-projection-supported' = 'true',
@@ -135,6 +135,16 @@ class TableSourceITCase extends BatchTestBase {
     )
   }
 
+  @Test
+  def testNestedProjectWithItem(): Unit = {
+    checkResult(
+      """
+        |SELECT nestedItem.deepArray[nestedItem.deepMap['Monday']] FROM  NestedTable
+        |""".stripMargin,
+      Seq(row(row(1)), row(row(1)), row(row(1)))
+    )
+  }
+
   @Test
   def testTableSourceWithFilterable(): Unit = {
     checkResult(
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/TableSourceITCase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/TableSourceITCase.scala
index 6475d3aec5d..7ff10b2ccf8 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/TableSourceITCase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/TableSourceITCase.scala
@@ -94,15 +94,16 @@ class TableSourceITCase extends StreamingTestBase {
          |  id BIGINT,
          |  deepNested ROW<
          |     nested1 ROW<name STRING, `value.` INT>,
-         |     `nested2.` ROW<num INT, flag BOOLEAN>
-         |   >,
-         |   nested ROW<name STRING, `value` INT>,
-         |   name STRING,
-         |   lower_name AS LOWER(name)
+         |     `nested2.` ROW<num INT, flag BOOLEAN>>,
+         |  nested ROW<name STRING, `value` INT>,
+         |  name STRING,
+         |  nestedItem ROW<deepArray ROW<`value` INT> ARRAY, deepMap MAP<STRING, INT>>,
+         |  lower_name AS LOWER(name)
          |) WITH (
          |  'connector' = 'values',
          |  'nested-projection-supported' = 'true',
-         |  'data-id' = '$nestedTableDataId'
+         |  'data-id' = '$nestedTableDataId',
+         |  'bounded' = 'true'
          |)
          |""".stripMargin
     )
@@ -158,6 +159,22 @@ class TableSourceITCase extends StreamingTestBase {
     assertEquals(expected.sorted, sink.getAppendResults.sorted)
   }
 
+  @Test
+  def testNestedProjectWithItem(): Unit = {
+    val query =
+      """
+        |SELECT nestedItem.deepArray[nestedItem.deepMap['Monday']] FROM  NestedTable
+        |""".stripMargin
+    val result = tEnv.sqlQuery(query).toAppendStream[Row]
+    val sink = new TestingAppendSink
+    result.addSink(sink)
+    env.execute()
+
+    val expected = Seq("1", "1", "1")
+    assertEquals(expected.sorted, sink.getAppendResults.sorted)
+  }
+
+
   @Test
   def testTableSourceWithFilterable(): Unit = {
     val query = "SELECT id, amount, name FROM FilterableTable WHERE amount > 4 AND price < 9"
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/utils/TestData.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/utils/TestData.scala
index c756677b9e5..0ee5c85cd71 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/utils/TestData.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/utils/TestData.scala
@@ -289,6 +289,14 @@ object TestData {
     data
   }
 
+  lazy val arrayRows = Array(
+    Row.of(new JInt(1)),
+    Row.of(new JInt(2)),
+    Row.of(new JInt(3)),
+    Row.of(new JInt(4)))
+
+  lazy val mapRows = map(("Monday", 1), ("Tuesday", 2), ("Wednesday", 3))
+
   lazy val deepNestedRow: Seq[Row] = {
     Seq(
       Row.of(new JLong(1),
@@ -297,21 +305,24 @@ object TestData {
           Row.of(new JInt(1000), new JBool(true))
         ),
         Row.of("Peter", new JInt(10000)),
-        "Mary"),
+        "Mary",
+        Row.of(arrayRows, mapRows)),
       Row.of(new JLong(2),
         Row.of(
           Row.of("Rob", new JInt(200)),
           Row.of(new JInt(2000), new JBool(false))
         ),
         Row.of("Lucy", new JInt(20000)),
-        "Bob"),
+        "Bob",
+        Row.of(arrayRows, mapRows)),
       Row.of(new JLong(3),
         Row.of(
           Row.of("Mike", new JInt(300)),
           Row.of(new JInt(3000), new JBool(true))
         ),
         Row.of("Betty", new JInt(30000)),
-        "Liz"))
+        "Liz",
+        Row.of(arrayRows, mapRows)))
   }
 
   lazy val tupleData5: Seq[(Int, Long, Int, String, Long)] = {
