diff --git a/flink-connectors/flink-connector-jdbc/pom.xml b/flink-connectors/flink-connector-jdbc/pom.xml
index be107af4deb..cb4d92389c7 100644
--- a/flink-connectors/flink-connector-jdbc/pom.xml
+++ b/flink-connectors/flink-connector-jdbc/pom.xml
@@ -116,6 +116,20 @@ under the License.
 			<scope>test</scope>
 		</dependency>
 
+		<!-- MySQL test dependencies -->
+		<dependency>
+			<groupId>ch.vorburger.mariaDB4j</groupId>
+			<artifactId>mariaDB4j</artifactId>
+			<version>2.4.0</version>
+			<scope>test</scope>
+		</dependency>
+		<dependency>
+			<groupId>mysql</groupId>
+			<artifactId>mysql-connector-java</artifactId>
+			<version>8.0.20</version>
+			<scope>test</scope>
+		</dependency>
+
 		<!-- Derby test dependencies -->
 
 		<dependency>
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/AbstractJdbcRowConverter.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/AbstractJdbcRowConverter.java
index be2b6ede28d..a622e6c87fd 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/AbstractJdbcRowConverter.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/AbstractJdbcRowConverter.java
@@ -33,6 +33,7 @@ import org.apache.flink.table.types.utils.TypeConversions;
 
 import java.io.Serializable;
 import java.math.BigDecimal;
+import java.math.BigInteger;
 import java.sql.Date;
 import java.sql.PreparedStatement;
 import java.sql.ResultSet;
@@ -73,7 +74,8 @@ public abstract class AbstractJdbcRowConverter implements JdbcRowConverter {
 	public RowData toInternal(ResultSet resultSet) throws SQLException {
 		GenericRowData genericRowData = new GenericRowData(rowType.getFieldCount());
 		for (int pos = 0; pos < rowType.getFieldCount(); pos++) {
-			genericRowData.setField(pos, toInternalConverters[pos].deserialize(resultSet.getObject(pos + 1)));
+			Object field = resultSet.getObject(pos + 1);
+			genericRowData.setField(pos, toInternalConverters[pos].deserialize(field));
 		}
 		return genericRowData;
 	}
@@ -114,11 +116,11 @@ public abstract class AbstractJdbcRowConverter implements JdbcRowConverter {
 	}
 
 	protected JdbcDeserializationConverter wrapIntoNullableInternalConverter(JdbcDeserializationConverter jdbcDeserializationConverter) {
-		return v -> {
-			if (v == null) {
+		return val -> {
+			if (val == null) {
 				return null;
 			} else {
-				return jdbcDeserializationConverter.deserialize(v);
+				return jdbcDeserializationConverter.deserialize(val);
 			}
 		};
 	}
@@ -126,37 +128,44 @@ public abstract class AbstractJdbcRowConverter implements JdbcRowConverter {
 	protected JdbcDeserializationConverter createInternalConverter(LogicalType type) {
 		switch (type.getTypeRoot()) {
 			case NULL:
-				return v -> null;
+				return val -> null;
 			case BOOLEAN:
-			case TINYINT:
 			case FLOAT:
 			case DOUBLE:
-			case INTEGER:
 			case INTERVAL_YEAR_MONTH:
-			case BIGINT:
 			case INTERVAL_DAY_TIME:
-				return v -> v;
+				return val -> val;
+			case TINYINT:
+				return val -> ((Integer) val).byteValue();
 			case SMALLINT:
 				// Converter for small type that casts value to int and then return short value, since
 				// JDBC 1.0 use int type for small values.
-				return v -> (Integer.valueOf(v.toString())).shortValue();
+				return val -> val instanceof Integer ? ((Integer) val).shortValue() : val;
+			case INTEGER:
+				return val -> val;
+			case BIGINT:
+				return val -> val;
+			case DECIMAL:
+				final int precision = ((DecimalType) type).getPrecision();
+				final int scale = ((DecimalType) type).getScale();
+				// using decimal(20, 0) to support db type bigint unsigned, user should define decimal(20, 0) in SQL,
+				// but other precision like decimal(30, 0) can work too from lenient consideration.
+				return val -> val instanceof BigInteger ?
+					DecimalData.fromBigDecimal(new BigDecimal((BigInteger) val, 0), precision, scale) :
+					DecimalData.fromBigDecimal((BigDecimal) val, precision, scale);
 			case DATE:
-				return v -> (int) (((Date) v).toLocalDate().toEpochDay());
+				return val -> (int) (((Date) val).toLocalDate().toEpochDay());
 			case TIME_WITHOUT_TIME_ZONE:
-				return v -> (int) (((Time) v).toLocalTime().toNanoOfDay() / 1_000_000L);
+				return val -> (int) (((Time) val).toLocalTime().toNanoOfDay() / 1_000_000L);
 			case TIMESTAMP_WITH_TIME_ZONE:
 			case TIMESTAMP_WITHOUT_TIME_ZONE:
-				return v -> TimestampData.fromTimestamp((Timestamp) v);
+				return val -> TimestampData.fromTimestamp((Timestamp) val);
 			case CHAR:
 			case VARCHAR:
-				return v -> StringData.fromString((String) v);
+				return val -> StringData.fromString((String) val);
 			case BINARY:
 			case VARBINARY:
-				return v -> (byte[]) v;
-			case DECIMAL:
-				final int precision = ((DecimalType) type).getPrecision();
-				final int scale = ((DecimalType) type).getScale();
-				return v -> DecimalData.fromBigDecimal((BigDecimal) v, precision, scale);
+				return val -> (byte[]) val;
 			case ARRAY:
 			case ROW:
 			case MAP:
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/PostgresRowConverter.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/PostgresRowConverter.java
index 106464a801e..2b3f30624dd 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/PostgresRowConverter.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/PostgresRowConverter.java
@@ -80,8 +80,8 @@ public class PostgresRowConverter extends AbstractJdbcRowConverter {
 			final Class<?> elementClass = LogicalTypeUtils.toInternalConversionClass(arrayType.getElementType());
 			final JdbcDeserializationConverter elementConverter = createNullableInternalConverter(arrayType.getElementType());
 
-			return v -> {
-				PgArray pgArray = (PgArray) v;
+			return val -> {
+				PgArray pgArray = (PgArray) val;
 				Object[] in = (Object[]) pgArray.getArray();
 				final Object[] array = (Object[]) Array.newInstance(elementClass, in.length);
 				for (int i = 0; i < in.length; i++) {
@@ -92,8 +92,8 @@ public class PostgresRowConverter extends AbstractJdbcRowConverter {
 		} else {
 			final Class<?> elementClass = LogicalTypeUtils.toInternalConversionClass(arrayType.getElementType());
 			final JdbcDeserializationConverter elementConverter = createNullableInternalConverter(arrayType.getElementType());
-			return v -> {
-				PgArray pgArray = (PgArray) v;
+			return val -> {
+				PgArray pgArray = (PgArray) val;
 				Object[] in = (Object[]) pgArray.getArray();
 				final Object[] array = (Object[]) Array.newInstance(elementClass, in.length);
 				for (int i = 0; i < in.length; i++) {
diff --git a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/UnsignedTypeConversionITCase.java b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/UnsignedTypeConversionITCase.java
new file mode 100644
index 00000000000..906d5a0a890
--- /dev/null
+++ b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/UnsignedTypeConversionITCase.java
@@ -0,0 +1,184 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connector.jdbc.table;
+
+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
+import org.apache.flink.table.api.DataTypes;
+import org.apache.flink.table.api.Table;
+import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
+import org.apache.flink.table.planner.runtime.utils.TableEnvUtil;
+import org.apache.flink.test.util.AbstractTestBase;
+import org.apache.flink.types.Row;
+
+import org.apache.flink.shaded.guava18.com.google.common.collect.Lists;
+
+import ch.vorburger.mariadb4j.DBConfigurationBuilder;
+import ch.vorburger.mariadb4j.junit.MariaDB4jRule;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.ClassRule;
+import org.junit.Test;
+
+import java.math.BigDecimal;
+import java.math.BigInteger;
+import java.sql.Connection;
+import java.sql.DriverManager;
+import java.sql.PreparedStatement;
+import java.sql.ResultSet;
+import java.sql.SQLException;
+import java.util.Collections;
+import java.util.Iterator;
+import java.util.List;
+import java.util.stream.Collectors;
+
+import static org.apache.flink.table.api.Expressions.row;
+import static org.junit.Assert.assertEquals;
+
+/**
+ * Test unsigned type conversion between Flink and JDBC driver mysql, the test underlying use
+ * MariaDB to mock a DB which use mysql driver too.
+ */
+public class UnsignedTypeConversionITCase extends AbstractTestBase {
+
+	private static final String DEFAULT_DB_NAME = "test";
+	private static final String TABLE_NAME = "unsigned_test";
+
+	private StreamTableEnvironment tEnv;
+	private String dbUrl;
+	private Connection connection;
+
+	@ClassRule
+	public static MariaDB4jRule db4jRule = new MariaDB4jRule(
+		DBConfigurationBuilder.newBuilder().build(),
+		DEFAULT_DB_NAME,
+		null);
+
+	@Before
+	public void setUp() throws SQLException, ClassNotFoundException {
+		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
+		tEnv = StreamTableEnvironment.create(env);
+		//dbUrl: jdbc:mysql://localhost:3306/test
+		dbUrl = db4jRule.getURL();
+		connection = DriverManager.getConnection(dbUrl);
+		createMysqlTable();
+		createFlinkTable();
+		prepareData();
+	}
+
+	@Test
+	public void testUnsignedType() throws Exception {
+		// write data to db
+		TableEnvUtil.execInsertSqlAndWaitResult(tEnv, "insert into jdbc_sink select" +
+			" tiny_c," +
+			" tiny_un_c," +
+			" small_c," +
+			" small_un_c ," +
+			" int_c," +
+			" int_un_c," +
+			" big_c ," +
+			" big_un_c from data");
+
+		// read data from db using jdbc connection and compare
+		PreparedStatement query = connection.prepareStatement(String.format("select tiny_c, tiny_un_c, small_c, small_un_c," +
+			" int_c, int_un_c, big_c, big_un_c from %s", TABLE_NAME));
+		ResultSet resultSet = query.executeQuery();
+		while (resultSet.next()) {
+			assertEquals(Integer.valueOf(127), resultSet.getObject("tiny_c"));
+			assertEquals(Integer.valueOf(255), resultSet.getObject("tiny_un_c"));
+			assertEquals(Integer.valueOf(32767), resultSet.getObject("small_c"));
+			assertEquals(Integer.valueOf(65535), resultSet.getObject("small_un_c"));
+			assertEquals(Integer.valueOf(2147483647), resultSet.getObject("int_c"));
+			assertEquals(Long.valueOf(4294967295L), resultSet.getObject("int_un_c"));
+			assertEquals(Long.valueOf(9223372036854775807L), resultSet.getObject("big_c"));
+			assertEquals(new BigInteger("18446744073709551615"), resultSet.getObject("big_un_c"));
+		}
+
+		// read data from db using flink and compare
+		Iterator<Row> collected = tEnv.executeSql("select tiny_c, tiny_un_c, small_c, small_un_c," +
+			" int_c, int_un_c, big_c, big_un_c from jdbc_source")
+			.collect();
+		List<String> result = Lists.newArrayList(collected).stream()
+			.map(Row::toString)
+			.sorted()
+			.collect(Collectors.toList());
+		List<String> expected = Collections.singletonList(
+			"127,255,32767,65535,2147483647,4294967295,9223372036854775807,18446744073709551615");
+		assertEquals(expected, result);
+	}
+
+	private void createMysqlTable() throws SQLException {
+		PreparedStatement ddlStatement = connection.prepareStatement("create table " + TABLE_NAME + " (" +
+			" tiny_c TINYINT," +
+			" tiny_un_c TINYINT UNSIGNED," +
+			" small_c SMALLINT," +
+			" small_un_c SMALLINT UNSIGNED," +
+			" int_c INTEGER ," +
+			" int_un_c INTEGER UNSIGNED," +
+			" big_c BIGINT," +
+			" big_un_c BIGINT UNSIGNED);");
+		ddlStatement.execute();
+	}
+
+	private void createFlinkTable() {
+		String commonDDL = "create table %s (" +
+			"tiny_c TINYINT," +
+			"tiny_un_c SMALLINT," +
+			"small_c SMALLINT," +
+			"small_un_c INT," +
+			"int_c INT," +
+			"int_un_c BIGINT," +
+			"big_c BIGINT," +
+			"big_un_c DECIMAL(20, 0)) with(" +
+			" 'connector' = 'jdbc'," +
+			" 'url' = '" + dbUrl + "'," +
+			" 'table-name' = '" + TABLE_NAME + "'" +
+			")";
+		tEnv.executeSql(String.format(commonDDL, "jdbc_source"));
+		tEnv.executeSql(String.format(commonDDL, "jdbc_sink"));
+	}
+
+	private void prepareData() {
+		Table dataTable = tEnv.fromValues(
+			DataTypes.ROW(
+				DataTypes.FIELD("tiny_c", DataTypes.TINYINT().notNull()),
+				DataTypes.FIELD("tiny_un_c", DataTypes.SMALLINT().notNull()),
+				DataTypes.FIELD("small_c", DataTypes.SMALLINT().notNull()),
+				DataTypes.FIELD("small_un_c", DataTypes.INT().notNull()),
+				DataTypes.FIELD("int_c", DataTypes.INT().notNull()),
+				DataTypes.FIELD("int_un_c", DataTypes.BIGINT().notNull()),
+				DataTypes.FIELD("big_c", DataTypes.BIGINT().notNull()),
+				DataTypes.FIELD("big_un_c", DataTypes.DECIMAL(20, 0).notNull())),
+			row(
+				new Integer(127).byteValue(),
+				new Integer(255).shortValue(),
+				new Integer(32767).shortValue(),
+				Integer.valueOf(65535),
+				Integer.valueOf(2147483647),
+				Long.valueOf(4294967295L),
+				Long.valueOf(9223372036854775807L),
+				new BigDecimal(new BigInteger("18446744073709551615"), 0)));
+		tEnv.createTemporaryView("data", dataTable);
+	}
+
+	@After
+	public void cleanup() throws Exception {
+		PreparedStatement preparedStatement = connection.prepareStatement(String.format("drop table %s", TABLE_NAME));
+		preparedStatement.execute();
+	}
+}
