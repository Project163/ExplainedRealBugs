diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/EdgeManager.java b/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/EdgeManager.java
index 42ccfa3686a..a9e3fcc7de6 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/EdgeManager.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/EdgeManager.java
@@ -41,6 +41,9 @@ public class EdgeManager {
     private final Map<ExecutionVertexID, List<ConsumedPartitionGroup>> vertexConsumedPartitions =
             new HashMap<>();
 
+    private final Map<IntermediateResultPartitionID, List<ConsumedPartitionGroup>>
+            consumedPartitionsById = new HashMap<>();
+
     public void connectPartitionWithConsumerVertexGroup(
             IntermediateResultPartitionID resultPartitionId,
             ConsumerVertexGroup consumerVertexGroup) {
@@ -89,4 +92,23 @@ public class EdgeManager {
         return Collections.unmodifiableList(
                 getConsumedPartitionGroupsForVertexInternal(executionVertexId));
     }
+
+    public void registerConsumedPartitionGroup(ConsumedPartitionGroup group) {
+        for (IntermediateResultPartitionID partitionId : group) {
+            consumedPartitionsById
+                    .computeIfAbsent(partitionId, ignore -> new ArrayList<>())
+                    .add(group);
+        }
+    }
+
+    private List<ConsumedPartitionGroup> getConsumedPartitionGroupsByIdInternal(
+            IntermediateResultPartitionID resultPartitionId) {
+        return consumedPartitionsById.computeIfAbsent(resultPartitionId, id -> new ArrayList<>());
+    }
+
+    public List<ConsumedPartitionGroup> getConsumedPartitionGroupsById(
+            IntermediateResultPartitionID resultPartitionId) {
+        return Collections.unmodifiableList(
+                getConsumedPartitionGroupsByIdInternal(resultPartitionId));
+    }
 }
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/EdgeManagerBuildUtil.java b/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/EdgeManagerBuildUtil.java
index 47222073243..9ba16aa8eb9 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/EdgeManagerBuildUtil.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/EdgeManagerBuildUtil.java
@@ -83,22 +83,25 @@ public class EdgeManagerBuildUtil {
     private static void connectAllToAll(
             ExecutionVertex[] taskVertices, IntermediateResult intermediateResult) {
 
-        ConsumedPartitionGroup consumedPartitions =
-                ConsumedPartitionGroup.fromMultiplePartitions(
-                        Arrays.stream(intermediateResult.getPartitions())
-                                .map(IntermediateResultPartition::getPartitionId)
-                                .collect(Collectors.toList()));
+        List<IntermediateResultPartitionID> consumedPartitions =
+                Arrays.stream(intermediateResult.getPartitions())
+                        .map(IntermediateResultPartition::getPartitionId)
+                        .collect(Collectors.toList());
+        ConsumedPartitionGroup consumedPartitionGroup =
+                createAndRegisterConsumedPartitionGroupToEdgeManager(
+                        consumedPartitions, intermediateResult);
         for (ExecutionVertex ev : taskVertices) {
-            ev.addConsumedPartitionGroup(consumedPartitions);
+            ev.addConsumedPartitionGroup(consumedPartitionGroup);
         }
 
-        ConsumerVertexGroup vertices =
-                ConsumerVertexGroup.fromMultipleVertices(
-                        Arrays.stream(taskVertices)
-                                .map(ExecutionVertex::getID)
-                                .collect(Collectors.toList()));
+        List<ExecutionVertexID> consumerVertices =
+                Arrays.stream(taskVertices)
+                        .map(ExecutionVertex::getID)
+                        .collect(Collectors.toList());
+        ConsumerVertexGroup consumerVertexGroup =
+                ConsumerVertexGroup.fromMultipleVertices(consumerVertices);
         for (IntermediateResultPartition partition : intermediateResult.getPartitions()) {
-            partition.addConsumers(vertices);
+            partition.addConsumers(consumerVertexGroup);
         }
     }
 
@@ -118,7 +121,8 @@ public class EdgeManagerBuildUtil {
                 partition.addConsumers(consumerVertexGroup);
 
                 ConsumedPartitionGroup consumedPartitionGroup =
-                        ConsumedPartitionGroup.fromSinglePartition(partition.getPartitionId());
+                        createAndRegisterConsumedPartitionGroupToEdgeManager(
+                                partition.getPartitionId(), intermediateResult);
                 executionVertex.addConsumedPartitionGroup(consumedPartitionGroup);
             }
         } else if (sourceCount > targetCount) {
@@ -142,7 +146,8 @@ public class EdgeManagerBuildUtil {
                 }
 
                 ConsumedPartitionGroup consumedPartitionGroup =
-                        ConsumedPartitionGroup.fromMultiplePartitions(consumedPartitions);
+                        createAndRegisterConsumedPartitionGroupToEdgeManager(
+                                consumedPartitions, intermediateResult);
                 executionVertex.addConsumedPartitionGroup(consumedPartitionGroup);
             }
         } else {
@@ -150,8 +155,9 @@ public class EdgeManagerBuildUtil {
 
                 IntermediateResultPartition partition =
                         intermediateResult.getPartitions()[partitionNum];
-                ConsumedPartitionGroup consumerPartitionGroup =
-                        ConsumedPartitionGroup.fromSinglePartition(partition.getPartitionId());
+                ConsumedPartitionGroup consumedPartitionGroup =
+                        createAndRegisterConsumedPartitionGroupToEdgeManager(
+                                partition.getPartitionId(), intermediateResult);
 
                 int start = (partitionNum * targetCount + sourceCount - 1) / sourceCount;
                 int end = ((partitionNum + 1) * targetCount + sourceCount - 1) / sourceCount;
@@ -160,7 +166,7 @@ public class EdgeManagerBuildUtil {
 
                 for (int i = start; i < end; i++) {
                     ExecutionVertex executionVertex = taskVertices[i];
-                    executionVertex.addConsumedPartitionGroup(consumerPartitionGroup);
+                    executionVertex.addConsumedPartitionGroup(consumedPartitionGroup);
 
                     consumers.add(executionVertex.getID());
                 }
@@ -171,4 +177,31 @@ public class EdgeManagerBuildUtil {
             }
         }
     }
+
+    private static ConsumedPartitionGroup createAndRegisterConsumedPartitionGroupToEdgeManager(
+            IntermediateResultPartitionID consumedPartitionId,
+            IntermediateResult intermediateResult) {
+        ConsumedPartitionGroup consumedPartitionGroup =
+                ConsumedPartitionGroup.fromSinglePartition(consumedPartitionId);
+        registerConsumedPartitionGroupToEdgeManager(consumedPartitionGroup, intermediateResult);
+        return consumedPartitionGroup;
+    }
+
+    private static ConsumedPartitionGroup createAndRegisterConsumedPartitionGroupToEdgeManager(
+            List<IntermediateResultPartitionID> consumedPartitions,
+            IntermediateResult intermediateResult) {
+        ConsumedPartitionGroup consumedPartitionGroup =
+                ConsumedPartitionGroup.fromMultiplePartitions(consumedPartitions);
+        registerConsumedPartitionGroupToEdgeManager(consumedPartitionGroup, intermediateResult);
+        return consumedPartitionGroup;
+    }
+
+    private static void registerConsumedPartitionGroupToEdgeManager(
+            ConsumedPartitionGroup consumedPartitionGroup, IntermediateResult intermediateResult) {
+        intermediateResult
+                .getProducer()
+                .getGraph()
+                .getEdgeManager()
+                .registerConsumedPartitionGroup(consumedPartitionGroup);
+    }
 }
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/IntermediateResultPartition.java b/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/IntermediateResultPartition.java
index 7b1c89196ec..8f1216b94d0 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/IntermediateResultPartition.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/IntermediateResultPartition.java
@@ -20,6 +20,7 @@ package org.apache.flink.runtime.executiongraph;
 
 import org.apache.flink.runtime.io.network.partition.ResultPartitionType;
 import org.apache.flink.runtime.jobgraph.IntermediateResultPartitionID;
+import org.apache.flink.runtime.scheduler.strategy.ConsumedPartitionGroup;
 import org.apache.flink.runtime.scheduler.strategy.ConsumerVertexGroup;
 
 import java.util.List;
@@ -72,6 +73,10 @@ public class IntermediateResultPartition {
         return getEdgeManager().getConsumerVertexGroupsForPartition(partitionId);
     }
 
+    public List<ConsumedPartitionGroup> getConsumedPartitionGroups() {
+        return getEdgeManager().getConsumedPartitionGroupsById(partitionId);
+    }
+
     public void markDataProduced() {
         hasDataProduced = true;
     }
@@ -108,6 +113,12 @@ public class IntermediateResultPartition {
                     "Tried to mark a non-blocking result partition as finished");
         }
 
+        // Sanity check to make sure a result partition cannot be marked as finished twice.
+        if (hasDataProduced) {
+            throw new IllegalStateException(
+                    "Tried to mark a finished result partition as finished.");
+        }
+
         hasDataProduced = true;
 
         final int refCnt = totalResult.decrementNumberOfRunningProducersAndGetRemaining();
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adapter/DefaultExecutionTopology.java b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adapter/DefaultExecutionTopology.java
index e5e5544521a..5a73da7c8c8 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adapter/DefaultExecutionTopology.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adapter/DefaultExecutionTopology.java
@@ -216,7 +216,8 @@ public class DefaultExecutionTopology implements SchedulingTopology {
                                                                 : ResultPartitionState.CREATED,
                                                 partitionConsumerVertexGroups.apply(
                                                         irp.getPartitionId()),
-                                                executionVertexRetriever)));
+                                                executionVertexRetriever,
+                                                irp::getConsumedPartitionGroups)));
 
         return producedSchedulingPartitions;
     }
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adapter/DefaultResultPartition.java b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adapter/DefaultResultPartition.java
index 9861aaedfdc..1b5dc08ff5d 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adapter/DefaultResultPartition.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adapter/DefaultResultPartition.java
@@ -22,6 +22,7 @@ import org.apache.flink.annotation.VisibleForTesting;
 import org.apache.flink.runtime.io.network.partition.ResultPartitionType;
 import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;
 import org.apache.flink.runtime.jobgraph.IntermediateResultPartitionID;
+import org.apache.flink.runtime.scheduler.strategy.ConsumedPartitionGroup;
 import org.apache.flink.runtime.scheduler.strategy.ConsumerVertexGroup;
 import org.apache.flink.runtime.scheduler.strategy.ExecutionVertexID;
 import org.apache.flink.runtime.scheduler.strategy.ResultPartitionState;
@@ -51,19 +52,23 @@ class DefaultResultPartition implements SchedulingResultPartition {
 
     private final Function<ExecutionVertexID, DefaultExecutionVertex> executionVertexRetriever;
 
+    private final Supplier<List<ConsumedPartitionGroup>> consumerPartitionGroupSupplier;
+
     DefaultResultPartition(
             IntermediateResultPartitionID partitionId,
             IntermediateDataSetID intermediateDataSetId,
             ResultPartitionType partitionType,
             Supplier<ResultPartitionState> resultPartitionStateSupplier,
             List<ConsumerVertexGroup> consumerVertexGroups,
-            Function<ExecutionVertexID, DefaultExecutionVertex> executionVertexRetriever) {
+            Function<ExecutionVertexID, DefaultExecutionVertex> executionVertexRetriever,
+            Supplier<List<ConsumedPartitionGroup>> consumerPartitionGroupSupplier) {
         this.resultPartitionId = checkNotNull(partitionId);
         this.intermediateDataSetId = checkNotNull(intermediateDataSetId);
         this.partitionType = checkNotNull(partitionType);
         this.resultPartitionStateSupplier = checkNotNull(resultPartitionStateSupplier);
         this.consumerVertexGroups = consumerVertexGroups;
         this.executionVertexRetriever = executionVertexRetriever;
+        this.consumerPartitionGroupSupplier = consumerPartitionGroupSupplier;
     }
 
     @VisibleForTesting
@@ -78,6 +83,7 @@ class DefaultResultPartition implements SchedulingResultPartition {
                 partitionType,
                 resultPartitionStateSupplier,
                 null,
+                null,
                 null);
     }
 
@@ -116,6 +122,11 @@ class DefaultResultPartition implements SchedulingResultPartition {
         return consumerVertexGroups;
     }
 
+    @Override
+    public List<ConsumedPartitionGroup> getConsumedPartitionGroups() {
+        return consumerPartitionGroupSupplier.get();
+    }
+
     void setProducer(DefaultExecutionVertex vertex) {
         producer = checkNotNull(vertex);
     }
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/strategy/SchedulingResultPartition.java b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/strategy/SchedulingResultPartition.java
index 1d94d4e9c95..86cafebac6b 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/strategy/SchedulingResultPartition.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/strategy/SchedulingResultPartition.java
@@ -53,4 +53,11 @@ public interface SchedulingResultPartition
      * @return list of {@link ConsumerVertexGroup}s
      */
     List<ConsumerVertexGroup> getConsumerVertexGroups();
+
+    /**
+     * Gets the {@link ConsumedPartitionGroup}s this partition belongs to.
+     *
+     * @return list of {@link ConsumedPartitionGroup}s
+     */
+    List<ConsumedPartitionGroup> getConsumedPartitionGroups();
 }
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/DefaultExecutionGraphConstructionTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/DefaultExecutionGraphConstructionTest.java
index 31e9473a91a..f070a98d9c8 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/DefaultExecutionGraphConstructionTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/DefaultExecutionGraphConstructionTest.java
@@ -28,10 +28,12 @@ import org.apache.flink.runtime.io.network.partition.ResultPartitionType;
 import org.apache.flink.runtime.jobgraph.DistributionPattern;
 import org.apache.flink.runtime.jobgraph.IntermediateDataSet;
 import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;
+import org.apache.flink.runtime.jobgraph.IntermediateResultPartitionID;
 import org.apache.flink.runtime.jobgraph.JobGraph;
 import org.apache.flink.runtime.jobgraph.JobVertex;
 import org.apache.flink.runtime.jobgraph.tasks.AbstractInvokable;
 import org.apache.flink.runtime.scheduler.SchedulerBase;
+import org.apache.flink.runtime.scheduler.strategy.ConsumedPartitionGroup;
 
 import org.apache.flink.shaded.guava18.com.google.common.collect.Sets;
 
@@ -41,14 +43,18 @@ import org.mockito.Matchers;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
+import java.util.HashSet;
 import java.util.List;
+import java.util.Objects;
+import java.util.Set;
 import java.util.stream.Collectors;
 import java.util.stream.Stream;
 
+import static org.hamcrest.MatcherAssert.assertThat;
+import static org.hamcrest.Matchers.containsInAnyOrder;
 import static org.hamcrest.Matchers.empty;
 import static org.hamcrest.Matchers.is;
 import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertThat;
 import static org.junit.Assert.fail;
 import static org.mockito.Mockito.mock;
 import static org.mockito.Mockito.when;
@@ -485,4 +491,40 @@ public class DefaultExecutionGraphConstructionTest {
             fail(e.getMessage());
         }
     }
+
+    @Test
+    public void testRegisterConsumedPartitionGroupToEdgeManager() throws Exception {
+        JobVertex v1 = new JobVertex("source");
+        JobVertex v2 = new JobVertex("sink");
+
+        v1.setParallelism(2);
+        v2.setParallelism(2);
+
+        v2.connectNewDataSetAsInput(
+                v1, DistributionPattern.ALL_TO_ALL, ResultPartitionType.BLOCKING);
+
+        List<JobVertex> ordered = new ArrayList<>(Arrays.asList(v1, v2));
+        ExecutionGraph eg = createDefaultExecutionGraph(ordered);
+        eg.attachJobGraph(ordered);
+
+        IntermediateResult result =
+                Objects.requireNonNull(eg.getJobVertex(v1.getID())).getProducedDataSets()[0];
+
+        IntermediateResultPartition partition1 = result.getPartitions()[0];
+        IntermediateResultPartition partition2 = result.getPartitions()[1];
+
+        assertEquals(
+                partition1.getConsumedPartitionGroups().get(0),
+                partition2.getConsumedPartitionGroups().get(0));
+
+        ConsumedPartitionGroup consumedPartitionGroup =
+                partition1.getConsumedPartitionGroups().get(0);
+        Set<IntermediateResultPartitionID> partitionIds = new HashSet<>();
+        for (IntermediateResultPartitionID partitionId : consumedPartitionGroup) {
+            partitionIds.add(partitionId);
+        }
+        assertThat(
+                partitionIds,
+                containsInAnyOrder(partition1.getPartitionId(), partition2.getPartitionId()));
+    }
 }
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/EdgeManagerTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/EdgeManagerTest.java
new file mode 100644
index 00000000000..39582b3716f
--- /dev/null
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/EdgeManagerTest.java
@@ -0,0 +1,87 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License
+ */
+
+package org.apache.flink.runtime.executiongraph;
+
+import org.apache.flink.runtime.concurrent.ComponentMainThreadExecutorServiceAdapter;
+import org.apache.flink.runtime.io.network.partition.ResultPartitionType;
+import org.apache.flink.runtime.jobgraph.DistributionPattern;
+import org.apache.flink.runtime.jobgraph.JobGraph;
+import org.apache.flink.runtime.jobgraph.JobGraphTestUtils;
+import org.apache.flink.runtime.jobgraph.JobVertex;
+import org.apache.flink.runtime.scheduler.SchedulerBase;
+import org.apache.flink.runtime.scheduler.SchedulerTestingUtils;
+import org.apache.flink.runtime.scheduler.strategy.ConsumedPartitionGroup;
+import org.apache.flink.runtime.testtasks.NoOpInvokable;
+
+import org.junit.Test;
+
+import java.util.Objects;
+
+import static org.junit.Assert.assertEquals;
+
+/** Tests for {@link EdgeManager}. */
+public class EdgeManagerTest {
+
+    @Test
+    public void testGetConsumedPartitionGroup() throws Exception {
+        JobVertex v1 = new JobVertex("source");
+        JobVertex v2 = new JobVertex("sink");
+
+        v1.setParallelism(2);
+        v2.setParallelism(2);
+
+        v1.setInvokableClass(NoOpInvokable.class);
+        v2.setInvokableClass(NoOpInvokable.class);
+
+        v2.connectNewDataSetAsInput(
+                v1, DistributionPattern.ALL_TO_ALL, ResultPartitionType.BLOCKING);
+
+        JobGraph jobGraph = JobGraphTestUtils.batchJobGraph(v1, v2);
+        SchedulerBase scheduler =
+                SchedulerTestingUtils.createScheduler(
+                        jobGraph, ComponentMainThreadExecutorServiceAdapter.forMainThread());
+        ExecutionGraph eg = scheduler.getExecutionGraph();
+
+        ConsumedPartitionGroup groupRetrievedByDownstreamVertex =
+                Objects.requireNonNull(eg.getJobVertex(v2.getID()))
+                        .getTaskVertices()[0]
+                        .getAllConsumedPartitionGroups()
+                        .get(0);
+
+        IntermediateResultPartition consumedPartition =
+                Objects.requireNonNull(eg.getJobVertex(v1.getID()))
+                        .getProducedDataSets()[0]
+                        .getPartitions()[0];
+
+        ConsumedPartitionGroup groupRetrievedByIntermediateResultPartition =
+                consumedPartition.getConsumedPartitionGroups().get(0);
+
+        assertEquals(groupRetrievedByDownstreamVertex, groupRetrievedByIntermediateResultPartition);
+
+        ConsumedPartitionGroup groupRetrievedByScheduledResultPartition =
+                scheduler
+                        .getExecutionGraph()
+                        .getSchedulingTopology()
+                        .getResultPartition(consumedPartition.getPartitionId())
+                        .getConsumedPartitionGroups()
+                        .get(0);
+
+        assertEquals(groupRetrievedByDownstreamVertex, groupRetrievedByScheduledResultPartition);
+    }
+}
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/strategy/TestingSchedulingExecutionVertex.java b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/strategy/TestingSchedulingExecutionVertex.java
index a414c7675e9..a429c460b91 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/strategy/TestingSchedulingExecutionVertex.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/strategy/TestingSchedulingExecutionVertex.java
@@ -90,8 +90,12 @@ public class TestingSchedulingExecutionVertex implements SchedulingExecutionVert
     }
 
     void addConsumedPartition(TestingSchedulingResultPartition consumedPartition) {
-        this.consumedPartitionGroups.add(
-                ConsumedPartitionGroup.fromSinglePartition(consumedPartition.getId()));
+        final ConsumedPartitionGroup consumedPartitionGroup =
+                ConsumedPartitionGroup.fromSinglePartition(consumedPartition.getId());
+
+        consumedPartition.registerConsumedPartitionGroup(consumedPartitionGroup);
+
+        this.consumedPartitionGroups.add(consumedPartitionGroup);
         this.resultPartitionsById.putIfAbsent(consumedPartition.getId(), consumedPartition);
     }
 
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/strategy/TestingSchedulingResultPartition.java b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/strategy/TestingSchedulingResultPartition.java
index 831f9b11b5f..f1701035fd1 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/strategy/TestingSchedulingResultPartition.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/strategy/TestingSchedulingResultPartition.java
@@ -24,6 +24,7 @@ import org.apache.flink.runtime.jobgraph.IntermediateResultPartitionID;
 import org.apache.flink.util.IterableUtils;
 
 import java.util.ArrayList;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -43,6 +44,8 @@ public class TestingSchedulingResultPartition implements SchedulingResultPartiti
 
     private final List<ConsumerVertexGroup> consumerVertexGroups;
 
+    private final List<ConsumedPartitionGroup> consumedPartitionGroups;
+
     private final Map<ExecutionVertexID, TestingSchedulingExecutionVertex> executionVerticesById;
 
     private ResultPartitionState state;
@@ -58,6 +61,7 @@ public class TestingSchedulingResultPartition implements SchedulingResultPartiti
         this.intermediateResultPartitionID =
                 new IntermediateResultPartitionID(dataSetID, partitionNum);
         this.consumerVertexGroups = new ArrayList<>();
+        this.consumedPartitionGroups = new ArrayList<>();
         this.executionVerticesById = new HashMap<>();
     }
 
@@ -96,6 +100,11 @@ public class TestingSchedulingResultPartition implements SchedulingResultPartiti
         return consumerVertexGroups;
     }
 
+    @Override
+    public List<ConsumedPartitionGroup> getConsumedPartitionGroups() {
+        return Collections.unmodifiableList(consumedPartitionGroups);
+    }
+
     void addConsumer(TestingSchedulingExecutionVertex consumer) {
         this.consumerVertexGroups.add(ConsumerVertexGroup.fromSingleVertex(consumer.getId()));
         this.executionVerticesById.putIfAbsent(consumer.getId(), consumer);
@@ -108,6 +117,14 @@ public class TestingSchedulingResultPartition implements SchedulingResultPartiti
         this.executionVerticesById.putAll(consumerVertexById);
     }
 
+    void registerConsumedPartitionGroup(ConsumedPartitionGroup consumedPartitionGroup) {
+        consumedPartitionGroups.add(consumedPartitionGroup);
+
+        if (getState() == ResultPartitionState.CONSUMABLE) {
+            consumedPartitionGroup.partitionFinished();
+        }
+    }
+
     void setProducer(TestingSchedulingExecutionVertex producer) {
         this.producer = checkNotNull(producer);
     }
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/strategy/TestingSchedulingTopology.java b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/strategy/TestingSchedulingTopology.java
index 1f14606bdad..db1514725ae 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/strategy/TestingSchedulingTopology.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/strategy/TestingSchedulingTopology.java
@@ -372,6 +372,10 @@ public class TestingSchedulingTopology implements SchedulingTopology {
                 consumer.addConsumedPartitionGroup(consumedPartitionGroup, consumedPartitionById);
             }
 
+            for (TestingSchedulingResultPartition resultPartition : resultPartitions) {
+                resultPartition.registerConsumedPartitionGroup(consumedPartitionGroup);
+            }
+
             return resultPartitions;
         }
     }
