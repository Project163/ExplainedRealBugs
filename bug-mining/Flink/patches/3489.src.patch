diff --git a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCAppendTableSinkTest.java b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCAppendTableSinkTest.java
index 95305c8ab01..c35dd395e15 100644
--- a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCAppendTableSinkTest.java
+++ b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCAppendTableSinkTest.java
@@ -61,11 +61,16 @@ public class JDBCAppendTableSinkTest {
 		DataStream<Row> ds = env.fromCollection(Collections.singleton(Row.of("foo")), ROW_TYPE);
 		sink.emitDataStream(ds);
 
-		Collection<Integer> sinkIds = env.getStreamGraph().getSinkIDs();
+		Collection<Integer> sinkIds = env
+				.getStreamGraph(StreamExecutionEnvironment.DEFAULT_JOB_NAME, false)
+				.getSinkIDs();
 		assertEquals(1, sinkIds.size());
 		int sinkId = sinkIds.iterator().next();
 
-		StreamSink planSink = (StreamSink) env.getStreamGraph().getStreamNode(sinkId).getOperator();
+		StreamSink planSink = (StreamSink) env
+				.getStreamGraph(StreamExecutionEnvironment.DEFAULT_JOB_NAME, false)
+				.getStreamNode(sinkId)
+				.getOperator();
 		assertTrue(planSink.getUserFunction() instanceof JDBCSinkFunction);
 
 		JDBCSinkFunction sinkFunction = (JDBCSinkFunction) planSink.getUserFunction();
diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/LocalStreamEnvironment.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/LocalStreamEnvironment.java
index e20c61ffa47..f7fee912bf9 100644
--- a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/LocalStreamEnvironment.java
+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/LocalStreamEnvironment.java
@@ -19,11 +19,9 @@ package org.apache.flink.streaming.api.environment;
 
 import org.apache.flink.annotation.Public;
 import org.apache.flink.api.common.InvalidProgramException;
-import org.apache.flink.api.common.JobExecutionResult;
 import org.apache.flink.api.java.ExecutionEnvironment;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.configuration.DeploymentOptions;
-import org.apache.flink.streaming.api.graph.StreamGraph;
 
 import javax.annotation.Nonnull;
 
@@ -68,19 +66,4 @@ public class LocalStreamEnvironment extends StreamExecutionEnvironment {
 		effectiveConfiguration.set(DeploymentOptions.ATTACHED, true);
 		return effectiveConfiguration;
 	}
-
-	/**
-	 * Executes the JobGraph of the on a mini cluster of ClusterUtil with a user
-	 * specified name.
-	 *
-	 * @return The result of the job execution, containing elapsed time and accumulators.
-	 */
-	@Override
-	public JobExecutionResult execute(StreamGraph streamGraph) throws Exception {
-		try {
-			return super.execute(streamGraph);
-		} finally {
-			transformations.clear();
-		}
-	}
 }
diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/RemoteStreamEnvironment.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/RemoteStreamEnvironment.java
index e89e257dc54..a698dd4ba06 100644
--- a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/RemoteStreamEnvironment.java
+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/RemoteStreamEnvironment.java
@@ -215,7 +215,6 @@ public class RemoteStreamEnvironment extends StreamExecutionEnvironment {
 
 	@Override
 	public JobExecutionResult execute(StreamGraph streamGraph) throws Exception {
-		transformations.clear();
 		try {
 			return super.execute(streamGraph);
 		}
diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamContextEnvironment.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamContextEnvironment.java
index ba772e0bd4d..8d67496ee19 100644
--- a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamContextEnvironment.java
+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamContextEnvironment.java
@@ -60,8 +60,6 @@ public class StreamContextEnvironment extends StreamExecutionEnvironment {
 
 	@Override
 	public JobExecutionResult execute(StreamGraph streamGraph) throws Exception {
-		transformations.clear();
-
 		JobClient jobClient = executeAsync(streamGraph);
 
 		JobExecutionResult jobExecutionResult;
diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.java
index dc1053f7f44..5536558aa21 100644
--- a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.java
+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.java
@@ -1760,24 +1760,45 @@ public class StreamExecutionEnvironment {
 	}
 
 	/**
-	 * Getter of the {@link org.apache.flink.streaming.api.graph.StreamGraph} of the streaming job.
+	 * Getter of the {@link org.apache.flink.streaming.api.graph.StreamGraph} of the streaming job. This call
+	 * clears previously registered {@link Transformation transformations}.
 	 *
 	 * @return The streamgraph representing the transformations
 	 */
 	@Internal
 	public StreamGraph getStreamGraph() {
-		return getStreamGraphGenerator().generate();
+		return getStreamGraph(DEFAULT_JOB_NAME);
 	}
 
 	/**
-	 * Getter of the {@link org.apache.flink.streaming.api.graph.StreamGraph} of the streaming job.
+	 * Getter of the {@link org.apache.flink.streaming.api.graph.StreamGraph} of the streaming job. This call
+	 * clears previously registered {@link Transformation transformations}.
 	 *
 	 * @param jobName Desired name of the job
 	 * @return The streamgraph representing the transformations
 	 */
 	@Internal
 	public StreamGraph getStreamGraph(String jobName) {
-		return getStreamGraphGenerator().setJobName(jobName).generate();
+		return getStreamGraph(jobName, true);
+	}
+
+	/**
+	 * Getter of the {@link org.apache.flink.streaming.api.graph.StreamGraph StreamGraph} of the streaming job
+	 * with the option to clear previously registered {@link Transformation transformations}. Clearing the
+	 * transformations allows, for example, to not re-execute the same operations when calling
+	 * {@link #execute()} multiple times.
+	 *
+	 * @param jobName Desired name of the job
+	 * @param clearTransformations Whether or not to clear previously registered transformations
+	 * @return The streamgraph representing the transformations
+	 */
+	@Internal
+	public StreamGraph getStreamGraph(String jobName, boolean clearTransformations) {
+		StreamGraph streamGraph = getStreamGraphGenerator().setJobName(jobName).generate();
+		if (clearTransformations) {
+			this.transformations.clear();
+		}
+		return streamGraph;
 	}
 
 	private StreamGraphGenerator getStreamGraphGenerator() {
@@ -1801,7 +1822,7 @@ public class StreamExecutionEnvironment {
 	 * @return The execution plan of the program, as a JSON String.
 	 */
 	public String getExecutionPlan() {
-		return getStreamGraph().getStreamingPlanAsJSON();
+		return getStreamGraph(DEFAULT_JOB_NAME, false).getStreamingPlanAsJSON();
 	}
 
 	/**
diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamPlanEnvironment.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamPlanEnvironment.java
index 89ede4f5962..0a2de7f8879 100644
--- a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamPlanEnvironment.java
+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamPlanEnvironment.java
@@ -49,8 +49,6 @@ public class StreamPlanEnvironment extends StreamExecutionEnvironment {
 
 	@Override
 	public JobExecutionResult execute(StreamGraph streamGraph) throws Exception {
-		transformations.clear();
-
 		if (env instanceof OptimizerPlanEnvironment) {
 			((OptimizerPlanEnvironment) env).setPipeline(streamGraph);
 		}
diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/DataStreamTest.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/DataStreamTest.java
index a8b7be1fe41..4c226bed8f0 100644
--- a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/DataStreamTest.java
+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/DataStreamTest.java
@@ -195,7 +195,7 @@ public class DataStreamTest extends TestLogger {
 			}
 		}).setParallelism(4);
 
-		StreamGraph streamGraph = env.getStreamGraph();
+		StreamGraph streamGraph = getStreamGraph(env);
 
 		// verify self union
 		assertTrue(streamGraph.getStreamNode(selfUnion.getId()).getInEdges().size() == 2);
@@ -326,10 +326,10 @@ public class DataStreamTest extends TestLogger {
 		int id3 = createDownStreamId(group3);
 		int id4 = createDownStreamId(group4);
 
-		assertTrue(isPartitioned(env.getStreamGraph().getStreamEdges(src1.getId(), id1)));
-		assertTrue(isPartitioned(env.getStreamGraph().getStreamEdges(src1.getId(), id2)));
-		assertTrue(isPartitioned(env.getStreamGraph().getStreamEdges(src1.getId(), id3)));
-		assertTrue(isPartitioned(env.getStreamGraph().getStreamEdges(src1.getId(), id4)));
+		assertTrue(isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId(), id1)));
+		assertTrue(isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId(), id2)));
+		assertTrue(isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId(), id3)));
+		assertTrue(isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId(), id4)));
 
 		assertTrue(isKeyed(group1));
 		assertTrue(isKeyed(group2));
@@ -347,10 +347,10 @@ public class DataStreamTest extends TestLogger {
 		int pid3 = createDownStreamId(partition3);
 		int pid4 = createDownStreamId(partition4);
 
-		assertTrue(isPartitioned(env.getStreamGraph().getStreamEdges(src1.getId(), pid1)));
-		assertTrue(isPartitioned(env.getStreamGraph().getStreamEdges(src1.getId(), pid2)));
-		assertTrue(isPartitioned(env.getStreamGraph().getStreamEdges(src1.getId(), pid3)));
-		assertTrue(isPartitioned(env.getStreamGraph().getStreamEdges(src1.getId(), pid4)));
+		assertTrue(isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId(), pid1)));
+		assertTrue(isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId(), pid2)));
+		assertTrue(isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId(), pid3)));
+		assertTrue(isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId(), pid4)));
 
 		assertTrue(isKeyed(partition1));
 		assertTrue(isKeyed(partition3));
@@ -373,9 +373,9 @@ public class DataStreamTest extends TestLogger {
 		int cid2 = createDownStreamId(customPartition3);
 		int cid3 = createDownStreamId(customPartition4);
 
-		assertTrue(isCustomPartitioned(env.getStreamGraph().getStreamEdges(src1.getId(), cid1)));
-		assertTrue(isCustomPartitioned(env.getStreamGraph().getStreamEdges(src1.getId(), cid2)));
-		assertTrue(isCustomPartitioned(env.getStreamGraph().getStreamEdges(src1.getId(), cid3)));
+		assertTrue(isCustomPartitioned(getStreamGraph(env).getStreamEdges(src1.getId(), cid1)));
+		assertTrue(isCustomPartitioned(getStreamGraph(env).getStreamEdges(src1.getId(), cid2)));
+		assertTrue(isCustomPartitioned(getStreamGraph(env).getStreamEdges(src1.getId(), cid3)));
 
 		assertFalse(isKeyed(customPartition1));
 		assertFalse(isKeyed(customPartition3));
@@ -397,20 +397,20 @@ public class DataStreamTest extends TestLogger {
 		ConnectedStreams<Tuple2<Long, Long>, Tuple2<Long, Long>> connectedGroup5 = connected.keyBy(new FirstSelector(), new FirstSelector());
 		Integer downStreamId5 = createDownStreamId(connectedGroup5);
 
-		assertTrue(isPartitioned(env.getStreamGraph().getStreamEdges(src1.getId(), downStreamId1)));
-		assertTrue(isPartitioned(env.getStreamGraph().getStreamEdges(src2.getId(), downStreamId1)));
+		assertTrue(isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId(), downStreamId1)));
+		assertTrue(isPartitioned(getStreamGraph(env).getStreamEdges(src2.getId(), downStreamId1)));
 
-		assertTrue(isPartitioned(env.getStreamGraph().getStreamEdges(src1.getId(), downStreamId2)));
-		assertTrue(isPartitioned(env.getStreamGraph().getStreamEdges(src2.getId(), downStreamId2)));
+		assertTrue(isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId(), downStreamId2)));
+		assertTrue(isPartitioned(getStreamGraph(env).getStreamEdges(src2.getId(), downStreamId2)));
 
-		assertTrue(isPartitioned(env.getStreamGraph().getStreamEdges(src1.getId(), downStreamId3)));
-		assertTrue(isPartitioned(env.getStreamGraph().getStreamEdges(src2.getId(), downStreamId3)));
+		assertTrue(isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId(), downStreamId3)));
+		assertTrue(isPartitioned(getStreamGraph(env).getStreamEdges(src2.getId(), downStreamId3)));
 
-		assertTrue(isPartitioned(env.getStreamGraph().getStreamEdges(src1.getId(), downStreamId4)));
-		assertTrue(isPartitioned(env.getStreamGraph().getStreamEdges(src2.getId(), downStreamId4)));
+		assertTrue(isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId(), downStreamId4)));
+		assertTrue(isPartitioned(getStreamGraph(env).getStreamEdges(src2.getId(), downStreamId4)));
 
-		assertTrue(isPartitioned(env.getStreamGraph().getStreamEdges(src1.getId(), downStreamId5)));
-		assertTrue(isPartitioned(env.getStreamGraph().getStreamEdges(src2.getId(), downStreamId5)));
+		assertTrue(isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId(), downStreamId5)));
+		assertTrue(isPartitioned(getStreamGraph(env).getStreamEdges(src2.getId(), downStreamId5)));
 
 		assertTrue(isKeyed(connectedGroup1));
 		assertTrue(isKeyed(connectedGroup2));
@@ -434,29 +434,29 @@ public class DataStreamTest extends TestLogger {
 		ConnectedStreams<Tuple2<Long, Long>, Tuple2<Long, Long>> connectedPartition5 = connected.keyBy(new FirstSelector(), new FirstSelector());
 		Integer connectDownStreamId5 = createDownStreamId(connectedPartition5);
 
-		assertTrue(isPartitioned(env.getStreamGraph().getStreamEdges(src1.getId(),
+		assertTrue(isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId(),
 				connectDownStreamId1)));
-		assertTrue(isPartitioned(env.getStreamGraph().getStreamEdges(src2.getId(),
+		assertTrue(isPartitioned(getStreamGraph(env).getStreamEdges(src2.getId(),
 				connectDownStreamId1)));
 
-		assertTrue(isPartitioned(env.getStreamGraph().getStreamEdges(src1.getId(),
+		assertTrue(isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId(),
 				connectDownStreamId2)));
-		assertTrue(isPartitioned(env.getStreamGraph().getStreamEdges(src2.getId(),
+		assertTrue(isPartitioned(getStreamGraph(env).getStreamEdges(src2.getId(),
 				connectDownStreamId2)));
 
-		assertTrue(isPartitioned(env.getStreamGraph().getStreamEdges(src1.getId(),
+		assertTrue(isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId(),
 				connectDownStreamId3)));
-		assertTrue(isPartitioned(env.getStreamGraph().getStreamEdges(src2.getId(),
+		assertTrue(isPartitioned(getStreamGraph(env).getStreamEdges(src2.getId(),
 				connectDownStreamId3)));
 
-		assertTrue(isPartitioned(env.getStreamGraph().getStreamEdges(src1.getId(),
+		assertTrue(isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId(),
 				connectDownStreamId4)));
-		assertTrue(isPartitioned(env.getStreamGraph().getStreamEdges(src2.getId(),
+		assertTrue(isPartitioned(getStreamGraph(env).getStreamEdges(src2.getId(),
 				connectDownStreamId4)));
 
-		assertTrue(isPartitioned(env.getStreamGraph().getStreamEdges(src1.getId(),
+		assertTrue(isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId(),
 				connectDownStreamId5)));
-		assertTrue(isPartitioned(env.getStreamGraph().getStreamEdges(src2.getId(),
+		assertTrue(isPartitioned(getStreamGraph(env).getStreamEdges(src2.getId(),
 				connectDownStreamId5)));
 
 		assertTrue(isKeyed(connectedPartition1));
@@ -503,21 +503,21 @@ public class DataStreamTest extends TestLogger {
 			}
 		});
 
-		assertEquals(1, env.getStreamGraph().getStreamNode(src.getId()).getParallelism());
-		assertEquals(10, env.getStreamGraph().getStreamNode(map.getId()).getParallelism());
-		assertEquals(1, env.getStreamGraph().getStreamNode(windowed.getId()).getParallelism());
+		assertEquals(1, getStreamGraph(env).getStreamNode(src.getId()).getParallelism());
+		assertEquals(10, getStreamGraph(env).getStreamNode(map.getId()).getParallelism());
+		assertEquals(1, getStreamGraph(env).getStreamNode(windowed.getId()).getParallelism());
 		assertEquals(10,
-				env.getStreamGraph().getStreamNode(sink.getTransformation().getId()).getParallelism());
+				getStreamGraph(env).getStreamNode(sink.getTransformation().getId()).getParallelism());
 
 		env.setParallelism(7);
 
 		// Some parts, such as windowing rely on the fact that previous operators have a parallelism
 		// set when instantiating the Discretizer. This would break if we dynamically changed
 		// the parallelism of operations when changing the setting on the Execution Environment.
-		assertEquals(1, env.getStreamGraph().getStreamNode(src.getId()).getParallelism());
-		assertEquals(10, env.getStreamGraph().getStreamNode(map.getId()).getParallelism());
-		assertEquals(1, env.getStreamGraph().getStreamNode(windowed.getId()).getParallelism());
-		assertEquals(10, env.getStreamGraph().getStreamNode(sink.getTransformation().getId()).getParallelism());
+		assertEquals(1, getStreamGraph(env).getStreamNode(src.getId()).getParallelism());
+		assertEquals(10, getStreamGraph(env).getStreamNode(map.getId()).getParallelism());
+		assertEquals(1, getStreamGraph(env).getStreamNode(windowed.getId()).getParallelism());
+		assertEquals(10, getStreamGraph(env).getStreamNode(sink.getTransformation().getId()).getParallelism());
 
 		try {
 			src.setParallelism(3);
@@ -528,16 +528,16 @@ public class DataStreamTest extends TestLogger {
 
 		DataStreamSource<Long> parallelSource = env.generateSequence(0, 0);
 		parallelSource.addSink(new DiscardingSink<Long>());
-		assertEquals(7, env.getStreamGraph().getStreamNode(parallelSource.getId()).getParallelism());
+		assertEquals(7, getStreamGraph(env).getStreamNode(parallelSource.getId()).getParallelism());
 
 		parallelSource.setParallelism(3);
-		assertEquals(3, env.getStreamGraph().getStreamNode(parallelSource.getId()).getParallelism());
+		assertEquals(3, getStreamGraph(env).getStreamNode(parallelSource.getId()).getParallelism());
 
 		map.setParallelism(2);
-		assertEquals(2, env.getStreamGraph().getStreamNode(map.getId()).getParallelism());
+		assertEquals(2, getStreamGraph(env).getStreamNode(map.getId()).getParallelism());
 
 		sink.setParallelism(4);
-		assertEquals(4, env.getStreamGraph().getStreamNode(sink.getTransformation().getId()).getParallelism());
+		assertEquals(4, getStreamGraph(env).getStreamNode(sink.getTransformation().getId()).getParallelism());
 	}
 
 	/**
@@ -624,26 +624,26 @@ public class DataStreamTest extends TestLogger {
 		DataStreamSink<Long> sink = windowed.print();
 		sinkMethod.invoke(sink, minResource7, preferredResource7);
 
-		assertEquals(minResource1, env.getStreamGraph().getStreamNode(source1.getId()).getMinResources());
-		assertEquals(preferredResource1, env.getStreamGraph().getStreamNode(source1.getId()).getPreferredResources());
+		assertEquals(minResource1, getStreamGraph(env).getStreamNode(source1.getId()).getMinResources());
+		assertEquals(preferredResource1, getStreamGraph(env).getStreamNode(source1.getId()).getPreferredResources());
 
-		assertEquals(minResource2, env.getStreamGraph().getStreamNode(map1.getId()).getMinResources());
-		assertEquals(preferredResource2, env.getStreamGraph().getStreamNode(map1.getId()).getPreferredResources());
+		assertEquals(minResource2, getStreamGraph(env).getStreamNode(map1.getId()).getMinResources());
+		assertEquals(preferredResource2, getStreamGraph(env).getStreamNode(map1.getId()).getPreferredResources());
 
-		assertEquals(minResource3, env.getStreamGraph().getStreamNode(source2.getId()).getMinResources());
-		assertEquals(preferredResource3, env.getStreamGraph().getStreamNode(source2.getId()).getPreferredResources());
+		assertEquals(minResource3, getStreamGraph(env).getStreamNode(source2.getId()).getMinResources());
+		assertEquals(preferredResource3, getStreamGraph(env).getStreamNode(source2.getId()).getPreferredResources());
 
-		assertEquals(minResource4, env.getStreamGraph().getStreamNode(map2.getId()).getMinResources());
-		assertEquals(preferredResource4, env.getStreamGraph().getStreamNode(map2.getId()).getPreferredResources());
+		assertEquals(minResource4, getStreamGraph(env).getStreamNode(map2.getId()).getMinResources());
+		assertEquals(preferredResource4, getStreamGraph(env).getStreamNode(map2.getId()).getPreferredResources());
 
-		assertEquals(minResource5, env.getStreamGraph().getStreamNode(connected.getId()).getMinResources());
-		assertEquals(preferredResource5, env.getStreamGraph().getStreamNode(connected.getId()).getPreferredResources());
+		assertEquals(minResource5, getStreamGraph(env).getStreamNode(connected.getId()).getMinResources());
+		assertEquals(preferredResource5, getStreamGraph(env).getStreamNode(connected.getId()).getPreferredResources());
 
-		assertEquals(minResource6, env.getStreamGraph().getStreamNode(windowed.getId()).getMinResources());
-		assertEquals(preferredResource6, env.getStreamGraph().getStreamNode(windowed.getId()).getPreferredResources());
+		assertEquals(minResource6, getStreamGraph(env).getStreamNode(windowed.getId()).getMinResources());
+		assertEquals(preferredResource6, getStreamGraph(env).getStreamNode(windowed.getId()).getPreferredResources());
 
-		assertEquals(minResource7, env.getStreamGraph().getStreamNode(sink.getTransformation().getId()).getMinResources());
-		assertEquals(preferredResource7, env.getStreamGraph().getStreamNode(sink.getTransformation().getId()).getPreferredResources());
+		assertEquals(minResource7, getStreamGraph(env).getStreamNode(sink.getTransformation().getId()).getMinResources());
+		assertEquals(preferredResource7, getStreamGraph(env).getStreamNode(sink.getTransformation().getId()).getPreferredResources());
 	}
 
 	@Test
@@ -942,13 +942,13 @@ public class DataStreamTest extends TestLogger {
 		assertEquals(filterFunction, getFunctionForDataStream(unionFilter));
 
 		try {
-			env.getStreamGraph().getStreamEdges(map.getId(), unionFilter.getId());
+			getStreamGraph(env).getStreamEdges(map.getId(), unionFilter.getId());
 		} catch (RuntimeException e) {
 			fail(e.getMessage());
 		}
 
 		try {
-			env.getStreamGraph().getStreamEdges(flatMap.getId(), unionFilter.getId());
+			getStreamGraph(env).getStreamEdges(flatMap.getId(), unionFilter.getId());
 		} catch (RuntimeException e) {
 			fail(e.getMessage());
 		}
@@ -957,18 +957,18 @@ public class DataStreamTest extends TestLogger {
 
 		SplitStream<Integer> split = unionFilter.split(outputSelector);
 		split.select("dummy").addSink(new DiscardingSink<Integer>());
-		List<OutputSelector<?>> outputSelectors = env.getStreamGraph().getStreamNode(unionFilter.getId()).getOutputSelectors();
+		List<OutputSelector<?>> outputSelectors = getStreamGraph(env).getStreamNode(unionFilter.getId()).getOutputSelectors();
 		assertEquals(1, outputSelectors.size());
 		assertEquals(outputSelector, outputSelectors.get(0));
 
 		DataStream<Integer> select = split.select("a");
 		DataStreamSink<Integer> sink = select.print();
 
-		StreamEdge splitEdge = env.getStreamGraph().getStreamEdges(unionFilter.getId(), sink.getTransformation().getId()).get(0);
+		StreamEdge splitEdge = getStreamGraph(env).getStreamEdges(unionFilter.getId(), sink.getTransformation().getId()).get(0);
 		assertEquals("a", splitEdge.getSelectedNames().get(0));
 
 		DataStreamSink<Integer> sinkWithIdentifier = select.print("identifier");
-		StreamEdge newSplitEdge = env.getStreamGraph().getStreamEdges(unionFilter.getId(), sinkWithIdentifier.getTransformation().getId()).get(0);
+		StreamEdge newSplitEdge = getStreamGraph(env).getStreamEdges(unionFilter.getId(), sinkWithIdentifier.getTransformation().getId()).get(0);
 		assertEquals("a", newSplitEdge.getSelectedNames().get(0));
 
 		ConnectedStreams<Integer, Integer> connect = map.connect(flatMap);
@@ -990,13 +990,13 @@ public class DataStreamTest extends TestLogger {
 		assertEquals(coMapper, getFunctionForDataStream(coMap));
 
 		try {
-			env.getStreamGraph().getStreamEdges(map.getId(), coMap.getId());
+			getStreamGraph(env).getStreamEdges(map.getId(), coMap.getId());
 		} catch (RuntimeException e) {
 			fail(e.getMessage());
 		}
 
 		try {
-			env.getStreamGraph().getStreamEdges(flatMap.getId(), coMap.getId());
+			getStreamGraph(env).getStreamEdges(flatMap.getId(), coMap.getId());
 		} catch (RuntimeException e) {
 			fail(e.getMessage());
 		}
@@ -1007,8 +1007,8 @@ public class DataStreamTest extends TestLogger {
 		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
 
 		DataStreamSink<Long> sink = env.generateSequence(1, 100).print();
-		assertTrue(env.getStreamGraph().getStreamNode(sink.getTransformation().getId()).getStatePartitioner1() == null);
-		assertTrue(env.getStreamGraph().getStreamNode(sink.getTransformation().getId()).getInEdges().get(0).getPartitioner() instanceof ForwardPartitioner);
+		assertTrue(getStreamGraph(env).getStreamNode(sink.getTransformation().getId()).getStatePartitioner1() == null);
+		assertTrue(getStreamGraph(env).getStreamNode(sink.getTransformation().getId()).getInEdges().get(0).getPartitioner() instanceof ForwardPartitioner);
 
 		KeySelector<Long, Long> key1 = new KeySelector<Long, Long>() {
 
@@ -1022,11 +1022,11 @@ public class DataStreamTest extends TestLogger {
 
 		DataStreamSink<Long> sink2 = env.generateSequence(1, 100).keyBy(key1).print();
 
-		assertNotNull(env.getStreamGraph().getStreamNode(sink2.getTransformation().getId()).getStatePartitioner1());
-		assertNotNull(env.getStreamGraph().getStreamNode(sink2.getTransformation().getId()).getStateKeySerializer());
-		assertNotNull(env.getStreamGraph().getStreamNode(sink2.getTransformation().getId()).getStateKeySerializer());
-		assertEquals(key1, env.getStreamGraph().getStreamNode(sink2.getTransformation().getId()).getStatePartitioner1());
-		assertTrue(env.getStreamGraph().getStreamNode(sink2.getTransformation().getId()).getInEdges().get(0).getPartitioner() instanceof KeyGroupStreamPartitioner);
+		assertNotNull(getStreamGraph(env).getStreamNode(sink2.getTransformation().getId()).getStatePartitioner1());
+		assertNotNull(getStreamGraph(env).getStreamNode(sink2.getTransformation().getId()).getStateKeySerializer());
+		assertNotNull(getStreamGraph(env).getStreamNode(sink2.getTransformation().getId()).getStateKeySerializer());
+		assertEquals(key1, getStreamGraph(env).getStreamNode(sink2.getTransformation().getId()).getStatePartitioner1());
+		assertTrue(getStreamGraph(env).getStreamNode(sink2.getTransformation().getId()).getInEdges().get(0).getPartitioner() instanceof KeyGroupStreamPartitioner);
 
 		KeySelector<Long, Long> key2 = new KeySelector<Long, Long>() {
 
@@ -1040,9 +1040,9 @@ public class DataStreamTest extends TestLogger {
 
 		DataStreamSink<Long> sink3 = env.generateSequence(1, 100).keyBy(key2).print();
 
-		assertTrue(env.getStreamGraph().getStreamNode(sink3.getTransformation().getId()).getStatePartitioner1() != null);
-		assertEquals(key2, env.getStreamGraph().getStreamNode(sink3.getTransformation().getId()).getStatePartitioner1());
-		assertTrue(env.getStreamGraph().getStreamNode(sink3.getTransformation().getId()).getInEdges().get(0).getPartitioner() instanceof KeyGroupStreamPartitioner);
+		assertTrue(getStreamGraph(env).getStreamNode(sink3.getTransformation().getId()).getStatePartitioner1() != null);
+		assertEquals(key2, getStreamGraph(env).getStreamNode(sink3.getTransformation().getId()).getStatePartitioner1());
+		assertTrue(getStreamGraph(env).getStreamNode(sink3.getTransformation().getId()).getInEdges().get(0).getPartitioner() instanceof KeyGroupStreamPartitioner);
 	}
 
 	@Test
@@ -1054,35 +1054,35 @@ public class DataStreamTest extends TestLogger {
 		DataStream<Long> broadcast = src.broadcast();
 		DataStreamSink<Long> broadcastSink = broadcast.print();
 		StreamPartitioner<?> broadcastPartitioner =
-				env.getStreamGraph().getStreamEdges(src.getId(),
+				getStreamGraph(env).getStreamEdges(src.getId(),
 						broadcastSink.getTransformation().getId()).get(0).getPartitioner();
 		assertTrue(broadcastPartitioner instanceof BroadcastPartitioner);
 
 		DataStream<Long> shuffle = src.shuffle();
 		DataStreamSink<Long> shuffleSink = shuffle.print();
 		StreamPartitioner<?> shufflePartitioner =
-				env.getStreamGraph().getStreamEdges(src.getId(),
+				getStreamGraph(env).getStreamEdges(src.getId(),
 						shuffleSink.getTransformation().getId()).get(0).getPartitioner();
 		assertTrue(shufflePartitioner instanceof ShufflePartitioner);
 
 		DataStream<Long> forward = src.forward();
 		DataStreamSink<Long> forwardSink = forward.print();
 		StreamPartitioner<?> forwardPartitioner =
-				env.getStreamGraph().getStreamEdges(src.getId(),
+				getStreamGraph(env).getStreamEdges(src.getId(),
 						forwardSink.getTransformation().getId()).get(0).getPartitioner();
 		assertTrue(forwardPartitioner instanceof ForwardPartitioner);
 
 		DataStream<Long> rebalance = src.rebalance();
 		DataStreamSink<Long> rebalanceSink = rebalance.print();
 		StreamPartitioner<?> rebalancePartitioner =
-				env.getStreamGraph().getStreamEdges(src.getId(),
+				getStreamGraph(env).getStreamEdges(src.getId(),
 						rebalanceSink.getTransformation().getId()).get(0).getPartitioner();
 		assertTrue(rebalancePartitioner instanceof RebalancePartitioner);
 
 		DataStream<Long> global = src.global();
 		DataStreamSink<Long> globalSink = global.print();
 		StreamPartitioner<?> globalPartitioner =
-				env.getStreamGraph().getStreamEdges(src.getId(),
+				getStreamGraph(env).getStreamEdges(src.getId(),
 						globalSink.getTransformation().getId()).get(0).getPartitioner();
 		assertTrue(globalPartitioner instanceof GlobalPartitioner);
 	}
@@ -1104,7 +1104,7 @@ public class DataStreamTest extends TestLogger {
 		expectedException.expect(IllegalStateException.class);
 		expectedException.expectMessage("Consecutive multiple splits are not supported. Splits are deprecated. Please use side-outputs.");
 
-		env.getStreamGraph();
+		getStreamGraph(env);
 	}
 
 	@Test
@@ -1121,7 +1121,7 @@ public class DataStreamTest extends TestLogger {
 		expectedException.expect(IllegalStateException.class);
 		expectedException.expectMessage("Split after side-outputs are not supported. Splits are deprecated. Please use side-outputs.");
 
-		env.getStreamGraph();
+		getStreamGraph(env);
 	}
 
 	@Test
@@ -1137,7 +1137,7 @@ public class DataStreamTest extends TestLogger {
 		expectedException.expect(IllegalStateException.class);
 		expectedException.expectMessage("Consecutive multiple splits are not supported. Splits are deprecated. Please use side-outputs.");
 
-		env.getStreamGraph();
+		getStreamGraph(env);
 	}
 
 	@Test
@@ -1153,7 +1153,7 @@ public class DataStreamTest extends TestLogger {
 		expectedException.expect(IllegalStateException.class);
 		expectedException.expectMessage("Consecutive multiple splits are not supported. Splits are deprecated. Please use side-outputs.");
 
-		env.getStreamGraph();
+		getStreamGraph(env);
 	}
 
 	@Test
@@ -1169,7 +1169,7 @@ public class DataStreamTest extends TestLogger {
 		expectedException.expect(IllegalStateException.class);
 		expectedException.expectMessage("Consecutive multiple splits are not supported. Splits are deprecated. Please use side-outputs.");
 
-		env.getStreamGraph();
+		getStreamGraph(env);
 	}
 
 	/////////////////////////////////////////////////////////////
@@ -1406,7 +1406,7 @@ public class DataStreamTest extends TestLogger {
 
 	private static StreamOperator<?> getOperatorForDataStream(DataStream<?> dataStream) {
 		StreamExecutionEnvironment env = dataStream.getExecutionEnvironment();
-		StreamGraph streamGraph = env.getStreamGraph();
+		StreamGraph streamGraph = getStreamGraph(env);
 		return streamGraph.getStreamNode(dataStream.getId()).getOperator();
 	}
 
@@ -1416,6 +1416,11 @@ public class DataStreamTest extends TestLogger {
 		return operator.getUserFunction();
 	}
 
+	/** Returns the StreamGraph without clearing the transformations. */
+	private static StreamGraph getStreamGraph(StreamExecutionEnvironment sEnv) {
+		return sEnv.getStreamGraph(StreamExecutionEnvironment.DEFAULT_JOB_NAME, false);
+	}
+
 	private static Integer createDownStreamId(DataStream<?> dataStream) {
 		return dataStream.print().getTransformation().getId();
 	}
diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/StreamExecutionEnvironmentTest.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/StreamExecutionEnvironmentTest.java
index 91cbe135b6e..65ca0ae9256 100644
--- a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/StreamExecutionEnvironmentTest.java
+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/StreamExecutionEnvironmentTest.java
@@ -87,12 +87,12 @@ public class StreamExecutionEnvironmentTest {
 
 			dataStream2.addSink(new DiscardingSink<Integer>());
 
-			env.getExecutionPlan();
+			final StreamGraph streamGraph = env.getStreamGraph();
+			streamGraph.getStreamingPlanAsJSON();
 
-			assertEquals("Parallelism of collection source must be 1.", 1, env.getStreamGraph().getStreamNode(dataStream1.getId()).getParallelism());
+			assertEquals("Parallelism of collection source must be 1.", 1, streamGraph.getStreamNode(dataStream1.getId()).getParallelism());
 			assertEquals("Parallelism of parallel collection source must be 4.",
-					4,
-					env.getStreamGraph().getStreamNode(dataStream2.getId()).getParallelism());
+					4, streamGraph.getStreamNode(dataStream2.getId()).getParallelism());
 		}
 		catch (Exception e) {
 			e.printStackTrace();
@@ -177,12 +177,12 @@ public class StreamExecutionEnvironmentTest {
 		Assert.assertEquals(1 << 15, operator.getParallelism());
 
 		// default value after generating
-		env.getStreamGraph().getJobGraph();
+		env.getStreamGraph(StreamExecutionEnvironment.DEFAULT_JOB_NAME, false).getJobGraph();
 		Assert.assertEquals(-1, operator.getTransformation().getMaxParallelism());
 
 		// configured value after generating
 		env.setMaxParallelism(42);
-		env.getStreamGraph().getJobGraph();
+		env.getStreamGraph(StreamExecutionEnvironment.DEFAULT_JOB_NAME, false).getJobGraph();
 		Assert.assertEquals(42, operator.getTransformation().getMaxParallelism());
 
 		// bounds configured parallelism 1
@@ -222,10 +222,37 @@ public class StreamExecutionEnvironmentTest {
 		Assert.assertEquals(1 << 15, operator.getTransformation().getMaxParallelism());
 
 		// override config
-		env.getStreamGraph().getJobGraph();
+		env.getStreamGraph(StreamExecutionEnvironment.DEFAULT_JOB_NAME, false).getJobGraph();
 		Assert.assertEquals(1 << 15 , operator.getTransformation().getMaxParallelism());
 	}
 
+	@Test
+	public void testGetStreamGraph() {
+		try {
+			TypeInformation<Integer> typeInfo = BasicTypeInfo.INT_TYPE_INFO;
+			StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
+
+			DataStreamSource<Integer> dataStream1 = env.fromCollection(new DummySplittableIterator<Integer>(), typeInfo);
+			dataStream1.addSink(new DiscardingSink<Integer>());
+			assertEquals(2, env.getStreamGraph().getStreamNodes().size());
+
+			DataStreamSource<Integer> dataStream2 = env.fromCollection(new DummySplittableIterator<Integer>(), typeInfo);
+			dataStream2.addSink(new DiscardingSink<Integer>());
+			assertEquals(2, env.getStreamGraph().getStreamNodes().size());
+
+			DataStreamSource<Integer> dataStream3 = env.fromCollection(new DummySplittableIterator<Integer>(), typeInfo);
+			dataStream3.addSink(new DiscardingSink<Integer>());
+			// Does not clear the transformations.
+			env.getExecutionPlan();
+			DataStreamSource<Integer> dataStream4 = env.fromCollection(new DummySplittableIterator<Integer>(), typeInfo);
+			dataStream4.addSink(new DiscardingSink<Integer>());
+			assertEquals(4, env.getStreamGraph("TestJob").getStreamNodes().size());
+		} catch (Exception e) {
+			e.printStackTrace();
+			fail(e.getMessage());
+		}
+	}
+
 	/////////////////////////////////////////////////////////////
 	// Utilities
 	/////////////////////////////////////////////////////////////
diff --git a/flink-streaming-scala/src/main/scala/org/apache/flink/streaming/api/scala/StreamExecutionEnvironment.scala b/flink-streaming-scala/src/main/scala/org/apache/flink/streaming/api/scala/StreamExecutionEnvironment.scala
index 9eb0e3339b0..4a4033deb35 100644
--- a/flink-streaming-scala/src/main/scala/org/apache/flink/streaming/api/scala/StreamExecutionEnvironment.scala
+++ b/flink-streaming-scala/src/main/scala/org/apache/flink/streaming/api/scala/StreamExecutionEnvironment.scala
@@ -18,8 +18,6 @@
 
 package org.apache.flink.streaming.api.scala
 
-import java.util.concurrent.CompletableFuture
-
 import com.esotericsoftware.kryo.Serializer
 import org.apache.flink.annotation.{Internal, Public, PublicEvolving}
 import org.apache.flink.api.common.io.{FileInputFormat, FilePathFilter, InputFormat}
@@ -36,7 +34,6 @@ import org.apache.flink.streaming.api.environment.{StreamExecutionEnvironment =>
 import org.apache.flink.streaming.api.functions.source._
 import org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext
 import org.apache.flink.streaming.api.{CheckpointingMode, TimeCharacteristic}
-import org.apache.flink.util.Preconditions.checkNotNull
 import org.apache.flink.util.SplittableIterator
 
 import scala.collection.JavaConverters._
@@ -743,12 +740,40 @@ class StreamExecutionEnvironment(javaEnv: JavaEnv) {
 
   /**
    * Getter of the [[org.apache.flink.streaming.api.graph.StreamGraph]] of the streaming job.
+   * This call clears previously registered
+   * [[org.apache.flink.api.dag.Transformation transformations]].
    *
    * @return The StreamGraph representing the transformations
    */
   @Internal
   def getStreamGraph = javaEnv.getStreamGraph
 
+  /**
+   * Getter of the [[org.apache.flink.streaming.api.graph.StreamGraph]] of the streaming job.
+   * This call clears previously registered
+   * [[org.apache.flink.api.dag.Transformation transformations]].
+   *
+   * @param jobName Desired name of the job
+   * @return The StreamGraph representing the transformations
+   */
+  @Internal
+  def getStreamGraph(jobName: String) = javaEnv.getStreamGraph(jobName)
+
+  /**
+   * Getter of the [[org.apache.flink.streaming.api.graph.StreamGraph]] of the streaming job
+   * with the option to clear previously registered
+   * [[org.apache.flink.api.dag.Transformation transformations]]. Clearing the transformations
+   * allows, for example, to not re-execute the same operations when calling
+   * [[execute()]] multiple times.
+   *
+   * @param jobName Desired name of the job
+   * @param clearTransformations Whether or not to clear previously registered transformations
+   * @return The StreamGraph representing the transformations
+   */
+  @Internal
+  def getStreamGraph(jobName: String, clearTransformations: Boolean) =
+    javaEnv.getStreamGraph(jobName, clearTransformations)
+
   /**
    * Getter of the wrapped [[org.apache.flink.streaming.api.environment.StreamExecutionEnvironment]]
  *
diff --git a/flink-streaming-scala/src/test/scala/org/apache/flink/streaming/api/scala/DataStreamTest.scala b/flink-streaming-scala/src/test/scala/org/apache/flink/streaming/api/scala/DataStreamTest.scala
index f65caa49608..4efe55521c0 100644
--- a/flink-streaming-scala/src/test/scala/org/apache/flink/streaming/api/scala/DataStreamTest.scala
+++ b/flink-streaming-scala/src/test/scala/org/apache/flink/streaming/api/scala/DataStreamTest.scala
@@ -19,11 +19,11 @@
 package org.apache.flink.streaming.api.scala
 
 import java.lang
-
 import org.apache.flink.api.common.functions._
 import org.apache.flink.api.java.io.ParallelIteratorInputFormat
 import org.apache.flink.api.java.typeutils.TypeExtractor
 import org.apache.flink.streaming.api.collector.selector.OutputSelector
+import org.apache.flink.streaming.api.environment.{StreamExecutionEnvironment => JStreamExecutionEnvironment}
 import org.apache.flink.streaming.api.functions.{KeyedProcessFunction, ProcessFunction}
 import org.apache.flink.streaming.api.functions.co.CoMapFunction
 import org.apache.flink.streaming.api.graph.{StreamEdge, StreamGraph}
@@ -34,6 +34,7 @@ import org.apache.flink.streaming.api.windowing.windows.GlobalWindow
 import org.apache.flink.streaming.runtime.partitioner._
 import org.apache.flink.test.util.AbstractTestBase
 import org.apache.flink.util.Collector
+
 import org.junit.Assert._
 import org.junit.rules.ExpectedException
 import org.junit.{Rule, Test}
@@ -116,10 +117,10 @@ class DataStreamTest extends AbstractTestBase {
     val gid2 = createDownStreamId(group2)
     val gid3 = createDownStreamId(group3)
     val gid4 = createDownStreamId(group4)
-    assert(isPartitioned(env.getStreamGraph.getStreamEdges(src1.getId, gid1)))
-    assert(isPartitioned(env.getStreamGraph.getStreamEdges(src1.getId, gid2)))
-    assert(isPartitioned(env.getStreamGraph.getStreamEdges(src1.getId, gid3)))
-    assert(isPartitioned(env.getStreamGraph.getStreamEdges(src1.getId, gid4)))
+    assert(isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId, gid1)))
+    assert(isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId, gid2)))
+    assert(isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId, gid3)))
+    assert(isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId, gid4)))
 
     //Testing DataStream partitioning
     val partition1: DataStream[_] = src1.keyBy(0)
@@ -132,10 +133,10 @@ class DataStreamTest extends AbstractTestBase {
     val pid3 = createDownStreamId(partition3)
     val pid4 = createDownStreamId(partition4)
 
-    assert(isPartitioned(env.getStreamGraph.getStreamEdges(src1.getId, pid1)))
-    assert(isPartitioned(env.getStreamGraph.getStreamEdges(src1.getId, pid2)))
-    assert(isPartitioned(env.getStreamGraph.getStreamEdges(src1.getId, pid3)))
-    assert(isPartitioned(env.getStreamGraph.getStreamEdges(src1.getId, pid4)))
+    assert(isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId, pid1)))
+    assert(isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId, pid2)))
+    assert(isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId, pid3)))
+    assert(isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId, pid4)))
 
     // Testing DataStream custom partitioning
     val longPartitioner: Partitioner[Long] = new Partitioner[Long] {
@@ -152,9 +153,9 @@ class DataStreamTest extends AbstractTestBase {
     val cpid1 = createDownStreamId(customPartition1)
     val cpid2 = createDownStreamId(customPartition3)
     val cpid3 = createDownStreamId(customPartition4)
-    assert(isCustomPartitioned(env.getStreamGraph.getStreamEdges(src1.getId, cpid1)))
-    assert(isCustomPartitioned(env.getStreamGraph.getStreamEdges(src1.getId, cpid2)))
-    assert(isCustomPartitioned(env.getStreamGraph.getStreamEdges(src1.getId, cpid3)))
+    assert(isCustomPartitioned(getStreamGraph(env).getStreamEdges(src1.getId, cpid1)))
+    assert(isCustomPartitioned(getStreamGraph(env).getStreamEdges(src1.getId, cpid2)))
+    assert(isCustomPartitioned(getStreamGraph(env).getStreamEdges(src1.getId, cpid3)))
 
     //Testing ConnectedStreams grouping
     val connectedGroup1: ConnectedStreams[_, _] = connected.keyBy(0, 0)
@@ -173,20 +174,20 @@ class DataStreamTest extends AbstractTestBase {
     val connectedGroup5: ConnectedStreams[_, _] = connected.keyBy(x => x._1, x => x._1)
     val downStreamId5: Integer = createDownStreamId(connectedGroup5)
 
-    assert(isPartitioned(env.getStreamGraph.getStreamEdges(src1.getId, downStreamId1)))
-    assert(isPartitioned(env.getStreamGraph.getStreamEdges(src2.getId, downStreamId1)))
+    assert(isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId, downStreamId1)))
+    assert(isPartitioned(getStreamGraph(env).getStreamEdges(src2.getId, downStreamId1)))
 
-    assert(isPartitioned(env.getStreamGraph.getStreamEdges(src1.getId, downStreamId2)))
-    assert(isPartitioned(env.getStreamGraph.getStreamEdges(src2.getId, downStreamId2)))
+    assert(isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId, downStreamId2)))
+    assert(isPartitioned(getStreamGraph(env).getStreamEdges(src2.getId, downStreamId2)))
 
-    assert(isPartitioned(env.getStreamGraph.getStreamEdges(src1.getId, downStreamId3)))
-    assert(isPartitioned(env.getStreamGraph.getStreamEdges(src2.getId, downStreamId3)))
+    assert(isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId, downStreamId3)))
+    assert(isPartitioned(getStreamGraph(env).getStreamEdges(src2.getId, downStreamId3)))
 
-    assert(isPartitioned(env.getStreamGraph.getStreamEdges(src1.getId, downStreamId4)))
-    assert(isPartitioned(env.getStreamGraph.getStreamEdges(src2.getId, downStreamId4)))
+    assert(isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId, downStreamId4)))
+    assert(isPartitioned(getStreamGraph(env).getStreamEdges(src2.getId, downStreamId4)))
 
-    assert(isPartitioned(env.getStreamGraph.getStreamEdges(src1.getId, downStreamId5)))
-    assert(isPartitioned(env.getStreamGraph.getStreamEdges(src2.getId, downStreamId5)))
+    assert(isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId, downStreamId5)))
+    assert(isPartitioned(getStreamGraph(env).getStreamEdges(src2.getId, downStreamId5)))
 
     //Testing ConnectedStreams partitioning
     val connectedPartition1: ConnectedStreams[_, _] = connected.keyBy(0, 0)
@@ -208,38 +209,38 @@ class DataStreamTest extends AbstractTestBase {
     val connectDownStreamId5: Integer = createDownStreamId(connectedPartition5)
 
     assert(
-      isPartitioned(env.getStreamGraph.getStreamEdges(src1.getId, connectDownStreamId1))
+      isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId, connectDownStreamId1))
     )
     assert(
-      isPartitioned(env.getStreamGraph.getStreamEdges(src2.getId, connectDownStreamId1))
+      isPartitioned(getStreamGraph(env).getStreamEdges(src2.getId, connectDownStreamId1))
     )
 
     assert(
-      isPartitioned(env.getStreamGraph.getStreamEdges(src1.getId, connectDownStreamId2))
+      isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId, connectDownStreamId2))
     )
     assert(
-      isPartitioned(env.getStreamGraph.getStreamEdges(src2.getId, connectDownStreamId2))
+      isPartitioned(getStreamGraph(env).getStreamEdges(src2.getId, connectDownStreamId2))
     )
 
     assert(
-      isPartitioned(env.getStreamGraph.getStreamEdges(src1.getId, connectDownStreamId3))
+      isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId, connectDownStreamId3))
     )
     assert(
-      isPartitioned(env.getStreamGraph.getStreamEdges(src2.getId, connectDownStreamId3))
+      isPartitioned(getStreamGraph(env).getStreamEdges(src2.getId, connectDownStreamId3))
     )
 
     assert(
-      isPartitioned(env.getStreamGraph.getStreamEdges(src1.getId, connectDownStreamId4))
+      isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId, connectDownStreamId4))
     )
     assert(
-      isPartitioned(env.getStreamGraph.getStreamEdges(src2.getId, connectDownStreamId4))
+      isPartitioned(getStreamGraph(env).getStreamEdges(src2.getId, connectDownStreamId4))
     )
 
     assert(
-      isPartitioned(env.getStreamGraph.getStreamEdges(src1.getId, connectDownStreamId5))
+      isPartitioned(getStreamGraph(env).getStreamEdges(src1.getId, connectDownStreamId5))
     )
     assert(
-      isPartitioned(env.getStreamGraph.getStreamEdges(src2.getId, connectDownStreamId5))
+      isPartitioned(getStreamGraph(env).getStreamEdges(src2.getId, connectDownStreamId5))
     )
   }
 
@@ -261,11 +262,10 @@ class DataStreamTest extends AbstractTestBase {
     windowed.print()
     val sink = map.addSink(x => {})
 
-    assert(1 == env.getStreamGraph.getStreamNode(src.getId).getParallelism)
-    assert(parallelism == env.getStreamGraph.getStreamNode(map.getId).getParallelism)
-    assert(1 == env.getStreamGraph.getStreamNode(windowed.getId).getParallelism)
-    assert(parallelism == env
-      .getStreamGraph
+    assert(1 == getStreamGraph(env).getStreamNode(src.getId).getParallelism)
+    assert(parallelism == getStreamGraph(env).getStreamNode(map.getId).getParallelism)
+    assert(1 == getStreamGraph(env).getStreamNode(windowed.getId).getParallelism)
+    assert(parallelism == getStreamGraph(env)
       .getStreamNode(sink.getTransformation.getId)
       .getParallelism)
 
@@ -283,27 +283,26 @@ class DataStreamTest extends AbstractTestBase {
     env.setParallelism(newParallelism)
     // the parallelism does not change since some windowing code takes the parallelism from
     // input operations and that cannot change dynamically
-    assert(1 == env.getStreamGraph.getStreamNode(src.getId).getParallelism)
-    assert(parallelism == env.getStreamGraph.getStreamNode(map.getId).getParallelism)
-    assert(1 == env.getStreamGraph.getStreamNode(windowed.getId).getParallelism)
-    assert(parallelism == env
-      .getStreamGraph
+    assert(1 == getStreamGraph(env).getStreamNode(src.getId).getParallelism)
+    assert(parallelism == getStreamGraph(env).getStreamNode(map.getId).getParallelism)
+    assert(1 == getStreamGraph(env).getStreamNode(windowed.getId).getParallelism)
+    assert(parallelism == getStreamGraph(env)
       .getStreamNode(sink.getTransformation.getId)
       .getParallelism)
 
     val parallelSource = env.generateSequence(0, 0)
     parallelSource.print()
 
-    assert(newParallelism == env.getStreamGraph.getStreamNode(parallelSource.getId).getParallelism)
+    assert(newParallelism == getStreamGraph(env).getStreamNode(parallelSource.getId).getParallelism)
 
     parallelSource.setParallelism(3)
-    assert(3 == env.getStreamGraph.getStreamNode(parallelSource.getId).getParallelism)
+    assert(3 == getStreamGraph(env).getStreamNode(parallelSource.getId).getParallelism)
 
     map.setParallelism(2)
-    assert(2 == env.getStreamGraph.getStreamNode(map.getId).getParallelism)
+    assert(2 == getStreamGraph(env).getStreamNode(map.getId).getParallelism)
 
     sink.setParallelism(4)
-    assert(4 == env.getStreamGraph.getStreamNode(sink.getTransformation.getId).getParallelism)
+    assert(4 == getStreamGraph(env).getStreamNode(sink.getTransformation.getId).getParallelism)
   }
 
 
@@ -372,33 +371,33 @@ class DataStreamTest extends AbstractTestBase {
 
     val plan = env.getExecutionPlan
 
-    assertEquals(minResource1, env.getStreamGraph.getStreamNode(source1.getId).
+    assertEquals(minResource1, getStreamGraph(env).getStreamNode(source1.getId).
       getMinResource)
-    assertEquals(preferredResource1, env.getStreamGraph.getStreamNode(source1.getId).
+    assertEquals(preferredResource1, getStreamGraph(env).getStreamNode(source1.getId).
       getPreferredResource)
-    assertEquals(minResource2, env.getStreamGraph.getStreamNode(map1.getId).
+    assertEquals(minResource2, getStreamGraph(env).getStreamNode(map1.getId).
       getMinResource)
-    assertEquals(preferredResource2, env.getStreamGraph.getStreamNode(map1.getId).
+    assertEquals(preferredResource2, getStreamGraph(env).getStreamNode(map1.getId).
       getPreferredResource)
-    assertEquals(minResource3, env.getStreamGraph.getStreamNode(source2.getId).
+    assertEquals(minResource3, getStreamGraph(env).getStreamNode(source2.getId).
       getMinResource)
-    assertEquals(preferredResource3, env.getStreamGraph.getStreamNode(source2.getId).
+    assertEquals(preferredResource3, getStreamGraph(env).getStreamNode(source2.getId).
       getPreferredResource)
-    assertEquals(minResource4, env.getStreamGraph.getStreamNode(map2.getId).
+    assertEquals(minResource4, getStreamGraph(env).getStreamNode(map2.getId).
       getMinResource)
-    assertEquals(preferredResource4, env.getStreamGraph.getStreamNode(map2.getId).
+    assertEquals(preferredResource4, getStreamGraph(env).getStreamNode(map2.getId).
       getPreferredResource)
-    assertEquals(minResource5, env.getStreamGraph.getStreamNode(connected.getId).
+    assertEquals(minResource5, getStreamGraph(env).getStreamNode(connected.getId).
       getMinResource)
-    assertEquals(preferredResource5, env.getStreamGraph.getStreamNode(connected.getId).
+    assertEquals(preferredResource5, getStreamGraph(env).getStreamNode(connected.getId).
       getPreferredResource)
-    assertEquals(minResource6, env.getStreamGraph.getStreamNode(windowed.getId).
+    assertEquals(minResource6, getStreamGraph(env).getStreamNode(windowed.getId).
       getMinResource)
-    assertEquals(preferredResource6, env.getStreamGraph.getStreamNode(windowed.getId).
+    assertEquals(preferredResource6, getStreamGraph(env).getStreamNode(windowed.getId).
       getPreferredResource)
-    assertEquals(minResource7, env.getStreamGraph.getStreamNode(
+    assertEquals(minResource7, getStreamGraph(env).getStreamNode(
       sink.getPreferredResource.getId).getMinResource)
-    assertEquals(preferredResource7, env.getStreamGraph.getStreamNode(
+    assertEquals(preferredResource7, getStreamGraph(env).getStreamNode(
       sink.getPreferredResource.getId).getPreferredResource)
   }*/
 
@@ -542,7 +541,7 @@ class DataStreamTest extends AbstractTestBase {
         (in, state: Option[Long]) => (false, None))
    
     try {
-      env.getStreamGraph.getStreamEdges(map.getId, unionFilter.getId)
+      getStreamGraph(env).getStreamEdges(map.getId, unionFilter.getId)
     }
     catch {
       case e: Throwable => {
@@ -551,7 +550,7 @@ class DataStreamTest extends AbstractTestBase {
     }
 
     try {
-      env.getStreamGraph.getStreamEdges(flatMap.getId, unionFilter.getId)
+      getStreamGraph(env).getStreamEdges(flatMap.getId, unionFilter.getId)
     }
     catch {
       case e: Throwable => {
@@ -565,22 +564,24 @@ class DataStreamTest extends AbstractTestBase {
 
     val split = unionFilter.split(outputSelector)
     split.print()
-    val outputSelectors = env.getStreamGraph.getStreamNode(unionFilter.getId).getOutputSelectors
+    val outputSelectors = getStreamGraph(env).getStreamNode(unionFilter.getId).getOutputSelectors
     assert(1 == outputSelectors.size)
     assert(outputSelector == outputSelectors.get(0))
 
     unionFilter.split(x => List("a")).print()
-    val moreOutputSelectors = env.getStreamGraph.getStreamNode(unionFilter.getId).getOutputSelectors
+    val moreOutputSelectors = getStreamGraph(env)
+      .getStreamNode(unionFilter.getId)
+      .getOutputSelectors
     assert(2 == moreOutputSelectors.size)
 
     val select = split.select("a")
     val sink = select.print()
     val splitEdge =
-      env.getStreamGraph.getStreamEdges(unionFilter.getId, sink.getTransformation.getId)
+      getStreamGraph(env).getStreamEdges(unionFilter.getId, sink.getTransformation.getId)
     assert("a" == splitEdge.get(0).getSelectedNames.get(0))
 
     val sinkWithIdentifier = select.print("identifier")
-    val newSplitEdge = env.getStreamGraph.getStreamEdges(
+    val newSplitEdge = getStreamGraph(env).getStreamEdges(
       unionFilter.getId,
       sinkWithIdentifier.getTransformation.getId)
     assert("a" == newSplitEdge.get(0).getSelectedNames.get(0))
@@ -607,7 +608,7 @@ class DataStreamTest extends AbstractTestBase {
     assert(coMapFunction == getFunctionForDataStream(coMap))
 
     try {
-      env.getStreamGraph.getStreamEdges(fold.getId, coMap.getId)
+      getStreamGraph(env).getStreamEdges(fold.getId, coMap.getId)
     }
     catch {
       case e: Throwable => {
@@ -615,7 +616,7 @@ class DataStreamTest extends AbstractTestBase {
       }
     }
     try {
-      env.getStreamGraph.getStreamEdges(flatMap.getId, coMap.getId)
+      getStreamGraph(env).getStreamEdges(flatMap.getId, coMap.getId)
     }
     catch {
       case e: Throwable => {
@@ -632,31 +633,31 @@ class DataStreamTest extends AbstractTestBase {
 
     val broadcast = src.broadcast
     val broadcastSink = broadcast.print()
-    val broadcastPartitioner = env.getStreamGraph
+    val broadcastPartitioner = getStreamGraph(env)
       .getStreamEdges(src.getId, broadcastSink.getTransformation.getId).get(0).getPartitioner
     assert(broadcastPartitioner.isInstanceOf[BroadcastPartitioner[_]])
 
     val shuffle: DataStream[Long] = src.shuffle
     val shuffleSink = shuffle.print()
-    val shufflePartitioner = env.getStreamGraph
+    val shufflePartitioner = getStreamGraph(env)
       .getStreamEdges(src.getId, shuffleSink.getTransformation.getId).get(0).getPartitioner
     assert(shufflePartitioner.isInstanceOf[ShufflePartitioner[_]])
 
     val forward: DataStream[Long] = src.forward
     val forwardSink = forward.print()
-    val forwardPartitioner = env.getStreamGraph
+    val forwardPartitioner = getStreamGraph(env)
       .getStreamEdges(src.getId, forwardSink.getTransformation.getId).get(0).getPartitioner
     assert(forwardPartitioner.isInstanceOf[ForwardPartitioner[_]])
 
     val rebalance: DataStream[Long] = src.rebalance
     val rebalanceSink = rebalance.print()
-    val rebalancePartitioner = env.getStreamGraph
+    val rebalancePartitioner = getStreamGraph(env)
       .getStreamEdges(src.getId, rebalanceSink.getTransformation.getId).get(0).getPartitioner
     assert(rebalancePartitioner.isInstanceOf[RebalancePartitioner[_]])
 
     val global: DataStream[Long] = src.global
     val globalSink = global.print()
-    val globalPartitioner = env.getStreamGraph
+    val globalPartitioner = getStreamGraph(env)
       .getStreamEdges(src.getId, globalSink.getTransformation.getId).get(0).getPartitioner
     assert(globalPartitioner.isInstanceOf[GlobalPartitioner[_]])
   }
@@ -675,7 +676,7 @@ class DataStreamTest extends AbstractTestBase {
     val iterated2 = source.iterate((input: DataStream[Int]) => 
       (input.map(_ + 1), input.map(_.toString)), 2000)
 
-    val sg = env.getStreamGraph
+    val sg = getStreamGraph(env)
 
     assert(sg.getIterationSourceSinkPairs.size() == 2)
   }
@@ -700,11 +701,17 @@ class DataStreamTest extends AbstractTestBase {
   private def getOperatorForDataStream(dataStream: DataStream[_]): StreamOperator[_] = {
     dataStream.print()
     val env = dataStream.javaStream.getExecutionEnvironment
-    val streamGraph: StreamGraph = env.getStreamGraph
+    val streamGraph: StreamGraph =
+      env.getStreamGraph(JStreamExecutionEnvironment.DEFAULT_JOB_NAME, false)
     streamGraph.getStreamNode(dataStream.getId).getOperator
   }
 
-  private def isPartitioned(edges: java.util.List[StreamEdge]): Boolean = {
+  /** Returns the StreamGraph without clearing the transformations. */
+  private def getStreamGraph(sEnv: StreamExecutionEnvironment): StreamGraph = {
+    sEnv.getStreamGraph(JStreamExecutionEnvironment.DEFAULT_JOB_NAME, clearTransformations = false)
+  }
+
+  private def isPartitioned(edges: java.util.List[StreamEdge]) = {
     import scala.collection.JavaConverters._
     edges.asScala.forall( _.getPartitioner.isInstanceOf[KeyGroupStreamPartitioner[_, _]])
   }
diff --git a/flink-test-utils-parent/flink-test-utils/src/main/java/org/apache/flink/streaming/util/TestStreamEnvironment.java b/flink-test-utils-parent/flink-test-utils/src/main/java/org/apache/flink/streaming/util/TestStreamEnvironment.java
index 9a073c9bad3..ef345e06be7 100644
--- a/flink-test-utils-parent/flink-test-utils/src/main/java/org/apache/flink/streaming/util/TestStreamEnvironment.java
+++ b/flink-test-utils-parent/flink-test-utils/src/main/java/org/apache/flink/streaming/util/TestStreamEnvironment.java
@@ -67,7 +67,6 @@ public class TestStreamEnvironment extends StreamExecutionEnvironment {
 	@Override
 	public JobExecutionResult execute(StreamGraph streamGraph) throws Exception {
 		final JobGraph jobGraph = streamGraph.getJobGraph();
-		transformations.clear();
 
 		for (Path jarFile : jarFiles) {
 			jobGraph.addJar(jarFile);
diff --git a/flink-tests/src/test/java/org/apache/flink/test/streaming/runtime/IterateITCase.java b/flink-tests/src/test/java/org/apache/flink/test/streaming/runtime/IterateITCase.java
index 168f11d9fde..fbe66161e41 100644
--- a/flink-tests/src/test/java/org/apache/flink/test/streaming/runtime/IterateITCase.java
+++ b/flink-tests/src/test/java/org/apache/flink/test/streaming/runtime/IterateITCase.java
@@ -485,7 +485,11 @@ public class IterateITCase extends AbstractTestBase {
 
 				head.addSink(new TestSink()).setParallelism(1);
 
-				assertEquals(1, env.getStreamGraph().getIterationSourceSinkPairs().size());
+				assertEquals(
+						1,
+						env.getStreamGraph(StreamExecutionEnvironment.DEFAULT_JOB_NAME, false)
+								.getIterationSourceSinkPairs()
+								.size());
 
 				env.execute();
 
@@ -589,16 +593,8 @@ public class IterateITCase extends AbstractTestBase {
 			try {
 				StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
 
-				env.enableCheckpointing();
-
-				DataStream<Boolean> source = env.fromCollection(Collections.nCopies(parallelism * 2, false))
-						.map(noOpBoolMap).name("ParallelizeMap");
-
-				IterativeStream<Boolean> iteration = source.iterate(3000 * timeoutScale);
-
-				iteration.closeWith(iteration.flatMap(new IterationHead())).addSink(new ReceiveCheckNoOpSink<Boolean>());
-
 				try {
+					createIteration(env, timeoutScale);
 					env.execute();
 
 					// this statement should never be reached
@@ -610,6 +606,7 @@ public class IterateITCase extends AbstractTestBase {
 				// Test force checkpointing
 
 				try {
+					createIteration(env, timeoutScale);
 					env.enableCheckpointing(
 						CheckpointCoordinatorConfiguration.MINIMAL_CHECKPOINT_TIME,
 						CheckpointingMode.EXACTLY_ONCE,
@@ -622,6 +619,7 @@ public class IterateITCase extends AbstractTestBase {
 					// expected behaviour
 				}
 
+				createIteration(env, timeoutScale);
 				env.enableCheckpointing(
 					CheckpointCoordinatorConfiguration.MINIMAL_CHECKPOINT_TIME,
 					CheckpointingMode.EXACTLY_ONCE,
@@ -641,6 +639,17 @@ public class IterateITCase extends AbstractTestBase {
 		}
 	}
 
+	private void createIteration(StreamExecutionEnvironment env, int timeoutScale) {
+		env.enableCheckpointing();
+
+		DataStream<Boolean> source = env.fromCollection(Collections.nCopies(parallelism * 2, false))
+				.map(noOpBoolMap).name("ParallelizeMap");
+
+		IterativeStream<Boolean> iteration = source.iterate(3000 * timeoutScale);
+
+		iteration.closeWith(iteration.flatMap(new IterationHead())).addSink(new ReceiveCheckNoOpSink<Boolean>());
+	}
+
 	private static final class IterationHead extends RichFlatMapFunction<Boolean, Boolean> {
 		public void flatMap(Boolean value, Collector<Boolean> out) throws Exception {
 			int indx = getRuntimeContext().getIndexOfThisSubtask();
