diff --git a/flink-connectors/flink-connector-base/src/test/java/org/apache/flink/connector/base/source/reader/CoordinatedSourceITCase.java b/flink-connectors/flink-connector-base/src/test/java/org/apache/flink/connector/base/source/reader/CoordinatedSourceITCase.java
index 635a1886b90..158ec9436cf 100644
--- a/flink-connectors/flink-connector-base/src/test/java/org/apache/flink/connector/base/source/reader/CoordinatedSourceITCase.java
+++ b/flink-connectors/flink-connector-base/src/test/java/org/apache/flink/connector/base/source/reader/CoordinatedSourceITCase.java
@@ -19,207 +19,25 @@
 package org.apache.flink.connector.base.source.reader;
 
 import org.apache.flink.api.common.accumulators.ListAccumulator;
-import org.apache.flink.api.common.eventtime.BoundedOutOfOrdernessWatermarks;
-import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner;
-import org.apache.flink.api.common.eventtime.WatermarkOutput;
 import org.apache.flink.api.common.eventtime.WatermarkStrategy;
 import org.apache.flink.api.connector.source.Boundedness;
 import org.apache.flink.api.connector.source.Source;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.connector.base.source.reader.mocks.MockBaseSource;
-import org.apache.flink.connector.base.source.reader.mocks.MockRecordEmitter;
-import org.apache.flink.core.execution.JobClient;
-import org.apache.flink.metrics.Gauge;
-import org.apache.flink.metrics.Metric;
-import org.apache.flink.metrics.groups.OperatorMetricGroup;
-import org.apache.flink.runtime.metrics.MetricNames;
-import org.apache.flink.runtime.testutils.InMemoryReporterRule;
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
-import org.apache.flink.streaming.api.functions.sink.DiscardingSink;
 import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;
 import org.apache.flink.test.util.AbstractTestBase;
-import org.apache.flink.testutils.junit.SharedObjects;
-import org.apache.flink.testutils.junit.SharedReference;
 
-import org.hamcrest.Matcher;
-import org.junit.Rule;
 import org.junit.Test;
 
-import java.time.Duration;
 import java.util.Collections;
 import java.util.List;
-import java.util.Map;
-import java.util.concurrent.CyclicBarrier;
 
-import static org.apache.flink.metrics.testutils.MetricMatchers.isCounter;
-import static org.apache.flink.metrics.testutils.MetricMatchers.isGauge;
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.nullValue;
-import static org.hamcrest.Matchers.both;
-import static org.hamcrest.Matchers.greaterThan;
-import static org.hamcrest.Matchers.hasSize;
-import static org.hamcrest.Matchers.lessThan;
 import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertThat;
 
 /** IT case for the {@link Source} with a coordinator. */
 public class CoordinatedSourceITCase extends AbstractTestBase {
-    // since integration tests depend on wall clock time, use huge lags
-    private static final long EVENTTIME_LAG = Duration.ofDays(100).toMillis();
-    private static final long WATERMARK_LAG = Duration.ofDays(1).toMillis();
-    private static final long EVENTTIME_EPSILON = Duration.ofDays(20).toMillis();
-    // this basically is the time a build is allowed to be frozen before the test fails
-    private static final long WATERMARK_EPSILON = Duration.ofHours(6).toMillis();
-    @Rule public final SharedObjects sharedObjects = SharedObjects.create();
-
-    @Rule
-    public final InMemoryReporterRule inMemoryReporter =
-            InMemoryReporterRule.fromMiniCluster(miniClusterResource);
-
-    @Test
-    public void testMetricsWithTimestamp() throws Exception {
-        long baseTime = System.currentTimeMillis() - EVENTTIME_LAG;
-        WatermarkStrategy<Integer> strategy =
-                WatermarkStrategy.forGenerator(
-                                context -> new EagerBoundedOutOfOrdernessWatermarks())
-                        .withTimestampAssigner(new LaggingTimestampAssigner(baseTime));
-
-        testMetrics(strategy, true);
-    }
-
-    @Test
-    public void testMetricsWithoutTimestamp() throws Exception {
-        testMetrics(WatermarkStrategy.noWatermarks(), false);
-    }
-
-    private void testMetrics(WatermarkStrategy<Integer> strategy, boolean hasTimestamps)
-            throws Exception {
-        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
-        int numSplits = Math.max(1, env.getParallelism() - 2);
-        env.getConfig().setAutoWatermarkInterval(1L);
-
-        int numRecordsPerSplit = 10;
-        MockBaseSource source =
-                new MockBaseSource(numSplits, numRecordsPerSplit, Boundedness.BOUNDED);
-
-        // make sure all parallel instances have processed the same amount of records before
-        // validating metrics
-        SharedReference<CyclicBarrier> beforeBarrier =
-                sharedObjects.add(new CyclicBarrier(numSplits + 1));
-        SharedReference<CyclicBarrier> afterBarrier =
-                sharedObjects.add(new CyclicBarrier(numSplits + 1));
-        int stopAtRecord1 = 3;
-        int stopAtRecord2 = numRecordsPerSplit - 1;
-        DataStream<Integer> stream =
-                env.fromSource(source, strategy, "MetricTestingSource")
-                        .map(
-                                i -> {
-                                    if (i % numRecordsPerSplit == stopAtRecord1
-                                            || i % numRecordsPerSplit == stopAtRecord2) {
-                                        beforeBarrier.get().await();
-                                        afterBarrier.get().await();
-                                    }
-                                    return i;
-                                });
-        stream.addSink(new DiscardingSink<>());
-        JobClient jobClient = env.executeAsync();
-
-        beforeBarrier.get().await();
-        assertSourceMetrics(
-                stopAtRecord1 + 1,
-                numRecordsPerSplit,
-                env.getParallelism(),
-                numSplits,
-                hasTimestamps);
-        afterBarrier.get().await();
-
-        beforeBarrier.get().await();
-        assertSourceMetrics(
-                stopAtRecord2 + 1,
-                numRecordsPerSplit,
-                env.getParallelism(),
-                numSplits,
-                hasTimestamps);
-        afterBarrier.get().await();
-
-        jobClient.getJobExecutionResult().get();
-    }
-
-    private void assertSourceMetrics(
-            long processedRecordsPerSubtask,
-            long numTotalPerSubtask,
-            int parallelism,
-            int numSplits,
-            boolean hasTimestamps) {
-        List<OperatorMetricGroup> groups =
-                inMemoryReporter.getReporter().findOperatorMetricGroups("MetricTestingSource");
-        assertThat(groups, hasSize(parallelism));
-
-        int subtaskWithMetrics = 0;
-        for (OperatorMetricGroup group : groups) {
-            Map<String, Metric> metrics = inMemoryReporter.getReporter().getMetricsByGroup(group);
-            // there are only 2 splits assigned; so two groups will not update metrics
-            if (group.getIOMetricGroup().getNumRecordsInCounter().getCount() == 0) {
-                // assert that optional metrics are not initialized when no split assigned
-                assertThat(metrics.get(MetricNames.CURRENT_EMIT_EVENT_TIME_LAG), nullValue());
-                assertThat(metrics.get(MetricNames.WATERMARK_LAG), nullValue());
-                continue;
-            }
-            subtaskWithMetrics++;
-            // I/O metrics
-            assertThat(
-                    group.getIOMetricGroup().getNumRecordsInCounter(),
-                    isCounter(equalTo(processedRecordsPerSubtask)));
-            assertThat(
-                    group.getIOMetricGroup().getNumBytesInCounter(),
-                    isCounter(
-                            equalTo(
-                                    processedRecordsPerSubtask
-                                            * MockRecordEmitter.RECORD_SIZE_IN_BYTES)));
-            // MockRecordEmitter is just incrementing errors every even record
-            assertThat(
-                    metrics.get(MetricNames.NUM_RECORDS_IN_ERRORS),
-                    isCounter(equalTo(processedRecordsPerSubtask / 2)));
-            if (hasTimestamps) {
-                // Timestamp assigner subtracting EVENTTIME_LAG from wall clock
-                assertThat(
-                        metrics.get(MetricNames.CURRENT_EMIT_EVENT_TIME_LAG),
-                        isGauge(isCloseTo(EVENTTIME_LAG, EVENTTIME_EPSILON)));
-                // Watermark is derived from timestamp, so it has to be in the same order of
-                // magnitude
-                assertThat(
-                        metrics.get(MetricNames.WATERMARK_LAG),
-                        isGauge(isCloseTo(EVENTTIME_LAG, EVENTTIME_EPSILON)));
-                // Calculate the additional watermark lag (on top of event time lag)
-                Long watermarkLag =
-                        ((Gauge<Long>) metrics.get(MetricNames.WATERMARK_LAG)).getValue()
-                                - ((Gauge<Long>)
-                                                metrics.get(
-                                                        MetricNames.CURRENT_EMIT_EVENT_TIME_LAG))
-                                        .getValue();
-                // That should correspond to the out-of-order boundedness
-                assertThat(watermarkLag, isCloseTo(WATERMARK_LAG, WATERMARK_EPSILON));
-            } else {
-                // assert that optional metrics are not initialized when no timestamp assigned
-                assertThat(metrics.get(MetricNames.CURRENT_EMIT_EVENT_TIME_LAG), nullValue());
-                assertThat(metrics.get(MetricNames.WATERMARK_LAG), nullValue());
-            }
-
-            long pendingRecords = numTotalPerSubtask - processedRecordsPerSubtask;
-            assertThat(metrics.get(MetricNames.PENDING_RECORDS), isGauge(equalTo(pendingRecords)));
-            assertThat(
-                    metrics.get(MetricNames.PENDING_BYTES),
-                    isGauge(equalTo(pendingRecords * MockRecordEmitter.RECORD_SIZE_IN_BYTES)));
-            // test is keeping source idle time metric busy with the barrier
-            assertThat(metrics.get(MetricNames.SOURCE_IDLE_TIME), isGauge(equalTo(0L)));
-        }
-        assertThat(subtaskWithMetrics, equalTo(numSplits));
-    }
-
-    private Matcher<Long> isCloseTo(long value, long epsilon) {
-        return both(greaterThan(value - epsilon)).and(lessThan(value + epsilon));
-    }
 
     @Test
     public void testEnumeratorReaderCommunication() throws Exception {
@@ -265,32 +83,4 @@ public class CoordinatedSourceITCase extends AbstractTestBase {
         assertEquals(0, (int) result.get(0));
         assertEquals(numRecords - 1, (int) result.get(result.size() - 1));
     }
-
-    private static class LaggingTimestampAssigner
-            implements SerializableTimestampAssigner<Integer> {
-        private final long baseTime;
-
-        public LaggingTimestampAssigner(long baseTime) {
-            this.baseTime = baseTime;
-        }
-
-        @Override
-        public long extractTimestamp(Integer i, long ts) {
-            return baseTime + i;
-        }
-    }
-
-    /** Emits watermarks on each record. */
-    private static class EagerBoundedOutOfOrdernessWatermarks
-            extends BoundedOutOfOrdernessWatermarks<Integer> {
-        public EagerBoundedOutOfOrdernessWatermarks() {
-            super(Duration.ofMillis(CoordinatedSourceITCase.WATERMARK_LAG));
-        }
-
-        @Override
-        public void onEvent(Integer event, long eventTimestamp, WatermarkOutput output) {
-            super.onEvent(event, eventTimestamp, output);
-            onPeriodicEmit(output);
-        }
-    }
 }
diff --git a/flink-connectors/flink-connector-base/src/test/java/org/apache/flink/connector/base/source/reader/SourceMetricsITCase.java b/flink-connectors/flink-connector-base/src/test/java/org/apache/flink/connector/base/source/reader/SourceMetricsITCase.java
new file mode 100644
index 00000000000..399fe96a21f
--- /dev/null
+++ b/flink-connectors/flink-connector-base/src/test/java/org/apache/flink/connector/base/source/reader/SourceMetricsITCase.java
@@ -0,0 +1,273 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connector.base.source.reader;
+
+import org.apache.flink.api.common.eventtime.BoundedOutOfOrdernessWatermarks;
+import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner;
+import org.apache.flink.api.common.eventtime.WatermarkOutput;
+import org.apache.flink.api.common.eventtime.WatermarkStrategy;
+import org.apache.flink.api.connector.source.Boundedness;
+import org.apache.flink.api.connector.source.Source;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.connector.base.source.reader.mocks.MockBaseSource;
+import org.apache.flink.connector.base.source.reader.mocks.MockRecordEmitter;
+import org.apache.flink.core.execution.JobClient;
+import org.apache.flink.metrics.Gauge;
+import org.apache.flink.metrics.Metric;
+import org.apache.flink.metrics.groups.OperatorMetricGroup;
+import org.apache.flink.runtime.metrics.MetricNames;
+import org.apache.flink.runtime.testutils.InMemoryReporter;
+import org.apache.flink.runtime.testutils.MiniClusterResourceConfiguration;
+import org.apache.flink.streaming.api.datastream.DataStream;
+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
+import org.apache.flink.streaming.api.functions.sink.DiscardingSink;
+import org.apache.flink.test.util.MiniClusterWithClientResource;
+import org.apache.flink.testutils.junit.SharedObjects;
+import org.apache.flink.testutils.junit.SharedReference;
+import org.apache.flink.util.TestLogger;
+
+import org.hamcrest.Matcher;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Rule;
+import org.junit.Test;
+
+import java.time.Duration;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.CyclicBarrier;
+
+import static org.apache.flink.metrics.testutils.MetricMatchers.isCounter;
+import static org.apache.flink.metrics.testutils.MetricMatchers.isGauge;
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.CoreMatchers.nullValue;
+import static org.hamcrest.Matchers.both;
+import static org.hamcrest.Matchers.greaterThan;
+import static org.hamcrest.Matchers.hasSize;
+import static org.hamcrest.Matchers.lessThan;
+import static org.junit.Assert.assertThat;
+
+/** Tests whether all provided metrics of a {@link Source} are of the expected values (FLIP-33). */
+public class SourceMetricsITCase extends TestLogger {
+    private static final int DEFAULT_PARALLELISM = 4;
+    // since integration tests depend on wall clock time, use huge lags
+    private static final long EVENTTIME_LAG = Duration.ofDays(100).toMillis();
+    private static final long WATERMARK_LAG = Duration.ofDays(1).toMillis();
+    private static final long EVENTTIME_EPSILON = Duration.ofDays(20).toMillis();
+    // this basically is the time a build is allowed to be frozen before the test fails
+    private static final long WATERMARK_EPSILON = Duration.ofHours(6).toMillis();
+    @Rule public final SharedObjects sharedObjects = SharedObjects.create();
+    private InMemoryReporter reporter;
+
+    private MiniClusterWithClientResource miniClusterResource;
+
+    @Before
+    public void setup() throws Exception {
+        reporter = InMemoryReporter.createWithRetainedMetrics();
+        Configuration configuration = new Configuration();
+        reporter.addToConfiguration(configuration);
+        miniClusterResource =
+                new MiniClusterWithClientResource(
+                        new MiniClusterResourceConfiguration.Builder()
+                                .setNumberTaskManagers(1)
+                                .setNumberSlotsPerTaskManager(DEFAULT_PARALLELISM)
+                                .setConfiguration(configuration)
+                                .build());
+        miniClusterResource.before();
+    }
+
+    @After
+    public void teardown() {
+        miniClusterResource.after();
+    }
+
+    @Test
+    public void testMetricsWithTimestamp() throws Exception {
+        long baseTime = System.currentTimeMillis() - EVENTTIME_LAG;
+        WatermarkStrategy<Integer> strategy =
+                WatermarkStrategy.forGenerator(
+                                context -> new EagerBoundedOutOfOrdernessWatermarks())
+                        .withTimestampAssigner(new LaggingTimestampAssigner(baseTime));
+
+        testMetrics(strategy, true);
+    }
+
+    @Test
+    public void testMetricsWithoutTimestamp() throws Exception {
+        testMetrics(WatermarkStrategy.noWatermarks(), false);
+    }
+
+    private void testMetrics(WatermarkStrategy<Integer> strategy, boolean hasTimestamps)
+            throws Exception {
+        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
+        int numSplits = Math.max(1, env.getParallelism() - 2);
+        env.getConfig().setAutoWatermarkInterval(1L);
+
+        int numRecordsPerSplit = 10;
+        MockBaseSource source =
+                new MockBaseSource(numSplits, numRecordsPerSplit, Boundedness.BOUNDED);
+
+        // make sure all parallel instances have processed the same amount of records before
+        // validating metrics
+        SharedReference<CyclicBarrier> beforeBarrier =
+                sharedObjects.add(new CyclicBarrier(numSplits + 1));
+        SharedReference<CyclicBarrier> afterBarrier =
+                sharedObjects.add(new CyclicBarrier(numSplits + 1));
+        int stopAtRecord1 = 3;
+        int stopAtRecord2 = numRecordsPerSplit - 1;
+        DataStream<Integer> stream =
+                env.fromSource(source, strategy, "MetricTestingSource")
+                        .map(
+                                i -> {
+                                    if (i % numRecordsPerSplit == stopAtRecord1
+                                            || i % numRecordsPerSplit == stopAtRecord2) {
+                                        beforeBarrier.get().await();
+                                        afterBarrier.get().await();
+                                    }
+                                    return i;
+                                });
+        stream.addSink(new DiscardingSink<>());
+        JobClient jobClient = env.executeAsync();
+
+        beforeBarrier.get().await();
+        assertSourceMetrics(
+                reporter,
+                stopAtRecord1 + 1,
+                numRecordsPerSplit,
+                env.getParallelism(),
+                numSplits,
+                hasTimestamps);
+        afterBarrier.get().await();
+
+        beforeBarrier.get().await();
+        assertSourceMetrics(
+                reporter,
+                stopAtRecord2 + 1,
+                numRecordsPerSplit,
+                env.getParallelism(),
+                numSplits,
+                hasTimestamps);
+        afterBarrier.get().await();
+
+        jobClient.getJobExecutionResult().get();
+    }
+
+    private void assertSourceMetrics(
+            InMemoryReporter reporter,
+            long processedRecordsPerSubtask,
+            long numTotalPerSubtask,
+            int parallelism,
+            int numSplits,
+            boolean hasTimestamps) {
+        List<OperatorMetricGroup> groups = reporter.findOperatorMetricGroups("MetricTestingSource");
+        assertThat(groups, hasSize(parallelism));
+
+        int subtaskWithMetrics = 0;
+        for (OperatorMetricGroup group : groups) {
+            Map<String, Metric> metrics = reporter.getMetricsByGroup(group);
+            // there are only 2 splits assigned; so two groups will not update metrics
+            if (group.getIOMetricGroup().getNumRecordsInCounter().getCount() == 0) {
+                // assert that optional metrics are not initialized when no split assigned
+                assertThat(metrics.get(MetricNames.CURRENT_EMIT_EVENT_TIME_LAG), nullValue());
+                assertThat(metrics.get(MetricNames.WATERMARK_LAG), nullValue());
+                continue;
+            }
+            subtaskWithMetrics++;
+            // I/O metrics
+            assertThat(
+                    group.getIOMetricGroup().getNumRecordsInCounter(),
+                    isCounter(equalTo(processedRecordsPerSubtask)));
+            assertThat(
+                    group.getIOMetricGroup().getNumBytesInCounter(),
+                    isCounter(
+                            equalTo(
+                                    processedRecordsPerSubtask
+                                            * MockRecordEmitter.RECORD_SIZE_IN_BYTES)));
+            // MockRecordEmitter is just incrementing errors every even record
+            assertThat(
+                    metrics.get(MetricNames.NUM_RECORDS_IN_ERRORS),
+                    isCounter(equalTo(processedRecordsPerSubtask / 2)));
+            if (hasTimestamps) {
+                // Timestamp assigner subtracting EVENTTIME_LAG from wall clock
+                assertThat(
+                        metrics.get(MetricNames.CURRENT_EMIT_EVENT_TIME_LAG),
+                        isGauge(isCloseTo(EVENTTIME_LAG, EVENTTIME_EPSILON)));
+                // Watermark is derived from timestamp, so it has to be in the same order of
+                // magnitude
+                assertThat(
+                        metrics.get(MetricNames.WATERMARK_LAG),
+                        isGauge(isCloseTo(EVENTTIME_LAG, EVENTTIME_EPSILON)));
+                // Calculate the additional watermark lag (on top of event time lag)
+                Long watermarkLag =
+                        ((Gauge<Long>) metrics.get(MetricNames.WATERMARK_LAG)).getValue()
+                                - ((Gauge<Long>)
+                                                metrics.get(
+                                                        MetricNames.CURRENT_EMIT_EVENT_TIME_LAG))
+                                        .getValue();
+                // That should correspond to the out-of-order boundedness
+                assertThat(watermarkLag, isCloseTo(WATERMARK_LAG, WATERMARK_EPSILON));
+            } else {
+                // assert that optional metrics are not initialized when no timestamp assigned
+                assertThat(metrics.get(MetricNames.CURRENT_EMIT_EVENT_TIME_LAG), nullValue());
+                assertThat(metrics.get(MetricNames.WATERMARK_LAG), nullValue());
+            }
+
+            long pendingRecords = numTotalPerSubtask - processedRecordsPerSubtask;
+            assertThat(metrics.get(MetricNames.PENDING_RECORDS), isGauge(equalTo(pendingRecords)));
+            assertThat(
+                    metrics.get(MetricNames.PENDING_BYTES),
+                    isGauge(equalTo(pendingRecords * MockRecordEmitter.RECORD_SIZE_IN_BYTES)));
+            // test is keeping source idle time metric busy with the barrier
+            assertThat(metrics.get(MetricNames.SOURCE_IDLE_TIME), isGauge(equalTo(0L)));
+        }
+        assertThat(subtaskWithMetrics, equalTo(numSplits));
+    }
+
+    private Matcher<Long> isCloseTo(long value, long epsilon) {
+        return both(greaterThan(value - epsilon)).and(lessThan(value + epsilon));
+    }
+
+    private static class LaggingTimestampAssigner
+            implements SerializableTimestampAssigner<Integer> {
+        private final long baseTime;
+
+        public LaggingTimestampAssigner(long baseTime) {
+            this.baseTime = baseTime;
+        }
+
+        @Override
+        public long extractTimestamp(Integer i, long ts) {
+            return baseTime + i;
+        }
+    }
+
+    /** Emits watermarks on each record. */
+    private static class EagerBoundedOutOfOrdernessWatermarks
+            extends BoundedOutOfOrdernessWatermarks<Integer> {
+        public EagerBoundedOutOfOrdernessWatermarks() {
+            super(Duration.ofMillis(SourceMetricsITCase.WATERMARK_LAG));
+        }
+
+        @Override
+        public void onEvent(Integer event, long eventTimestamp, WatermarkOutput output) {
+            super.onEvent(event, eventTimestamp, output);
+            onPeriodicEmit(output);
+        }
+    }
+}
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/testutils/InMemoryReporter.java b/flink-runtime/src/test/java/org/apache/flink/runtime/testutils/InMemoryReporter.java
index 39f935a8b28..22548411c66 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/testutils/InMemoryReporter.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/testutils/InMemoryReporter.java
@@ -35,9 +35,9 @@ import javax.annotation.concurrent.ThreadSafe;
 
 import java.util.AbstractMap.SimpleEntry;
 import java.util.Arrays;
+import java.util.Collections;
 import java.util.Comparator;
 import java.util.HashMap;
-import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
 import java.util.Map.Entry;
@@ -50,12 +50,15 @@ import java.util.regex.Pattern;
 import java.util.stream.Collectors;
 import java.util.stream.Stream;
 
+import static org.apache.flink.util.Preconditions.checkState;
+
 /**
  * A {@link MetricReporter} implementation that makes all reported metrics available for tests.
- * Metrics remain registered even after the job finishes for easier assertions.
  *
- * <p>This class can only be accessed through {@link InMemoryReporterRule#getReporter()} to ensure
- * that test cases are properly isolated.
+ * <p>By default, metrics in the {@link InMemoryReporter} follow the general life-cycle of metrics
+ * in a {@link org.apache.flink.runtime.metrics.util.TestReporter}; that is, task metrics will be
+ * removed as soon as the task finishes etc. By using {@link #createWithRetainedMetrics()}, these
+ * metrics will only be retained until the cluster is closed.
  *
  * <p>Note that at this time, there is not a strong guarantee that metrics from one job in test case
  * A cannot spill over to a job from test case B if both test cases use the same minicluster - even
@@ -70,14 +73,24 @@ public class InMemoryReporter implements MetricReporter {
     private static final Map<UUID, InMemoryReporter> REPORTERS = new ConcurrentHashMap<>();
 
     private final Map<MetricGroup, Map<String, Metric>> metrics = new HashMap<>();
-    private final Set<MetricGroup> removedGroups = new HashSet<>();
     private final UUID id;
 
-    InMemoryReporter() {
+    private final boolean retainMetrics;
+
+    InMemoryReporter(boolean retainMetrics) {
+        this.retainMetrics = retainMetrics;
         this.id = UUID.randomUUID();
         REPORTERS.put(id, this);
     }
 
+    public static InMemoryReporter create() {
+        return new InMemoryReporter(false);
+    }
+
+    public static InMemoryReporter createWithRetainedMetrics() {
+        return new InMemoryReporter(true);
+    }
+
     @Override
     public void open(MetricConfig config) {}
 
@@ -89,13 +102,6 @@ public class InMemoryReporter implements MetricReporter {
         }
     }
 
-    void applyRemovals() {
-        synchronized (this) {
-            metrics.keySet().removeAll(removedGroups);
-            removedGroups.clear();
-        }
-    }
-
     public Map<String, Metric> getMetricsByIdentifiers() {
         synchronized (this) {
             return getMetricStream().collect(Collectors.toMap(Entry::getKey, Entry::getValue));
@@ -113,7 +119,7 @@ public class InMemoryReporter implements MetricReporter {
     public Map<String, Metric> getMetricsByGroup(MetricGroup metricGroup) {
         synchronized (this) {
             // create a copy of the inner Map to avoid concurrent modifications
-            return new HashMap<>(metrics.get(metricGroup));
+            return new HashMap<>(metrics.getOrDefault(metricGroup, Collections.emptyMap()));
         }
     }
 
@@ -149,19 +155,21 @@ public class InMemoryReporter implements MetricReporter {
                     .filter(
                             g ->
                                     g instanceof OperatorMetricGroup
-                                            && pattern.matcher(
-                                                            g.getScopeComponents()[
-                                                                    g.getScopeComponents().length
-                                                                            - 2])
-                                                    .find())
+                                            && pattern.matcher(getOperatorName(g)).find())
                     .map(OperatorMetricGroup.class::cast)
-                    .sorted(
-                            Comparator.comparing(
-                                    g -> g.getScopeComponents()[g.getScopeComponents().length - 1]))
+                    .sorted(Comparator.comparing(this::getSubtaskId))
                     .collect(Collectors.toList());
         }
     }
 
+    private String getSubtaskId(OperatorMetricGroup g) {
+        return g.getScopeComponents()[g.getScopeComponents().length - 1];
+    }
+
+    private String getOperatorName(MetricGroup g) {
+        return g.getScopeComponents()[g.getScopeComponents().length - 2];
+    }
+
     @Override
     public void notifyOfAddedMetric(Metric metric, String metricName, MetricGroup group) {
         MetricGroup metricGroup = unwrap(group);
@@ -173,8 +181,17 @@ public class InMemoryReporter implements MetricReporter {
 
     @Override
     public void notifyOfRemovedMetric(Metric metric, String metricName, MetricGroup group) {
-        synchronized (this) {
-            removedGroups.add(unwrap(group));
+        if (!retainMetrics) {
+            synchronized (this) {
+                MetricGroup metricGroup = unwrap(group);
+                Map<String, Metric> registeredMetrics = metrics.get(metricGroup);
+                if (registeredMetrics != null) {
+                    registeredMetrics.remove(metricName);
+                    if (registeredMetrics.isEmpty()) {
+                        metrics.remove(metricGroup);
+                    }
+                }
+            }
         }
     }
 
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/testutils/InMemoryReporterRule.java b/flink-runtime/src/test/java/org/apache/flink/runtime/testutils/InMemoryReporterRule.java
deleted file mode 100644
index 878a7cd7841..00000000000
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/testutils/InMemoryReporterRule.java
+++ /dev/null
@@ -1,68 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.runtime.testutils;
-
-import org.apache.flink.annotation.Experimental;
-import org.apache.flink.util.ExternalResource;
-
-/**
- * A JUnit rule that encapsulates {@link InMemoryReporter} properly with test cases.
- *
- * <pre>
- * public static class Test {
- *     &#064;ClassRule
- *     public MiniClusterResource miniCluster = new MiniClusterResource(...);
- *     // ensures that job metrics from different tests don't interfere with each other
- *     &#064;Rule
- *     public InMemoryReporterRule inMemoryReporter = InMemoryReporterRule.create();
- *
- *     &#064;Test
- *     public void test() {
- *         // all task/job + cluster metrics
- *         inMemoryReporter.getReporter().getMetricsByIdentifiers()
- *     }
- * }
- * </pre>
- */
-@Experimental
-public class InMemoryReporterRule implements ExternalResource {
-    private final InMemoryReporter inMemoryReporter;
-
-    public static InMemoryReporterRule fromMiniCluster(MiniClusterResource miniClusterResource) {
-        return new InMemoryReporterRule(miniClusterResource.getReporter());
-    }
-
-    private InMemoryReporterRule(InMemoryReporter inMemoryReporter) {
-        this.inMemoryReporter = inMemoryReporter;
-    }
-
-    public InMemoryReporter getReporter() {
-        return inMemoryReporter;
-    }
-
-    @Override
-    public void before() {
-        // FLINK-23785: some metrics may be deregistered even after the test case.
-        inMemoryReporter.applyRemovals();
-    }
-
-    @Override
-    public void afterTestSuccess() {
-        inMemoryReporter.applyRemovals();
-    }
-}
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/testutils/MiniClusterResource.java b/flink-runtime/src/test/java/org/apache/flink/runtime/testutils/MiniClusterResource.java
index e3ca1325eb4..d61a6663e7c 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/testutils/MiniClusterResource.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/testutils/MiniClusterResource.java
@@ -62,8 +62,6 @@ public class MiniClusterResource extends ExternalResource {
 
     private UnmodifiableConfiguration restClusterClientConfig;
 
-    private final InMemoryReporter reporter = new InMemoryReporter();
-
     public MiniClusterResource(
             final MiniClusterResourceConfiguration miniClusterResourceConfiguration) {
         this.miniClusterResourceConfiguration =
@@ -86,10 +84,6 @@ public class MiniClusterResource extends ExternalResource {
         return miniCluster.getRestAddress().join();
     }
 
-    InMemoryReporter getReporter() {
-        return reporter;
-    }
-
     @Override
     public void before() throws Exception {
         temporaryFolder.create();
@@ -177,7 +171,6 @@ public class MiniClusterResource extends ExternalResource {
                 new Configuration(miniClusterResourceConfiguration.getConfiguration());
         configuration.setString(
                 CoreOptions.TMP_DIRS, temporaryFolder.newFolder().getAbsolutePath());
-        reporter.addToConfiguration(configuration);
 
         // we need to set this since a lot of test expect this because TestBaseUtils.startCluster()
         // enabled this by default
diff --git a/flink-tests/src/test/java/org/apache/flink/test/streaming/runtime/SinkITCase.java b/flink-tests/src/test/java/org/apache/flink/test/streaming/runtime/SinkITCase.java
index 270ccef8de6..c4f85c41b84 100644
--- a/flink-tests/src/test/java/org/apache/flink/test/streaming/runtime/SinkITCase.java
+++ b/flink-tests/src/test/java/org/apache/flink/test/streaming/runtime/SinkITCase.java
@@ -18,62 +18,35 @@
 package org.apache.flink.test.streaming.runtime;
 
 import org.apache.flink.api.common.RuntimeExecutionMode;
-import org.apache.flink.api.common.typeinfo.BasicTypeInfo;
 import org.apache.flink.api.common.typeinfo.IntegerTypeInfo;
-import org.apache.flink.api.connector.sink.Sink;
 import org.apache.flink.api.java.tuple.Tuple3;
-import org.apache.flink.core.execution.JobClient;
-import org.apache.flink.metrics.Metric;
-import org.apache.flink.metrics.groups.OperatorMetricGroup;
-import org.apache.flink.metrics.groups.SinkWriterMetricGroup;
-import org.apache.flink.runtime.metrics.MetricNames;
-import org.apache.flink.runtime.testutils.InMemoryReporterRule;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 import org.apache.flink.streaming.runtime.operators.sink.TestSink;
 import org.apache.flink.streaming.util.FiniteTestSource;
 import org.apache.flink.test.util.AbstractTestBase;
-import org.apache.flink.testutils.junit.SharedObjects;
-import org.apache.flink.testutils.junit.SharedReference;
 
 import org.junit.Before;
-import org.junit.Rule;
 import org.junit.Test;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
 
 import java.io.Serializable;
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.List;
-import java.util.Map;
 import java.util.Queue;
 import java.util.concurrent.ConcurrentLinkedQueue;
-import java.util.concurrent.CyclicBarrier;
 import java.util.function.BooleanSupplier;
 import java.util.function.Supplier;
 import java.util.stream.Collectors;
-import java.util.stream.LongStream;
 
 import static java.util.stream.Collectors.joining;
-import static org.apache.flink.metrics.testutils.MetricMatchers.isCounter;
-import static org.apache.flink.metrics.testutils.MetricMatchers.isGauge;
 import static org.apache.flink.streaming.runtime.operators.sink.TestSink.END_OF_INPUT_STR;
-import static org.hamcrest.CoreMatchers.equalTo;
 import static org.hamcrest.MatcherAssert.assertThat;
 import static org.hamcrest.Matchers.containsInAnyOrder;
-import static org.hamcrest.Matchers.hasSize;
 
 /**
  * Integration test for {@link org.apache.flink.api.connector.sink.Sink} run time implementation.
  */
 public class SinkITCase extends AbstractTestBase {
-    private static final Logger LOG = LoggerFactory.getLogger(SinkITCase.class);
-    @Rule public final SharedObjects sharedObjects = SharedObjects.create();
-
-    @Rule
-    public final InMemoryReporterRule inMemoryReporter =
-            InMemoryReporterRule.fromMiniCluster(miniClusterResource);
-
     static final List<Integer> SOURCE_DATA =
             Arrays.asList(
                     895, 127, 148, 161, 148, 662, 822, 491, 275, 122, 850, 630, 682, 765, 434, 970,
@@ -282,90 +255,6 @@ public class SinkITCase extends AbstractTestBase {
                 containsInAnyOrder(EXPECTED_GLOBAL_COMMITTED_DATA_IN_BATCH_MODE.toArray()));
     }
 
-    @Test
-    public void testMetrics() throws Exception {
-        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
-        int numSplits = Math.max(1, env.getParallelism() - 2);
-
-        int numRecordsPerSplit = 10;
-
-        // make sure all parallel instances have processed the same amount of records before
-        // validating metrics
-        SharedReference<CyclicBarrier> beforeBarrier =
-                sharedObjects.add(new CyclicBarrier(numSplits + 1));
-        SharedReference<CyclicBarrier> afterBarrier =
-                sharedObjects.add(new CyclicBarrier(numSplits + 1));
-        int stopAtRecord1 = 4;
-        int stopAtRecord2 = numRecordsPerSplit - 1;
-
-        env.fromSequence(0, numSplits - 1)
-                .<Long>flatMap(
-                        (split, collector) ->
-                                LongStream.range(0, numRecordsPerSplit).forEach(collector::collect))
-                .returns(BasicTypeInfo.LONG_TYPE_INFO)
-                .map(
-                        i -> {
-                            if (i % numRecordsPerSplit == stopAtRecord1
-                                    || i % numRecordsPerSplit == stopAtRecord2) {
-                                beforeBarrier.get().await();
-                                afterBarrier.get().await();
-                            }
-                            return i;
-                        })
-                .sinkTo(TestSink.newBuilder().setWriter(new MetricWriter()).build())
-                .name("MetricTestSink");
-        JobClient jobClient = env.executeAsync();
-
-        beforeBarrier.get().await();
-        assertSinkMetrics(stopAtRecord1, env.getParallelism(), numSplits);
-        afterBarrier.get().await();
-
-        beforeBarrier.get().await();
-        assertSinkMetrics(stopAtRecord2, env.getParallelism(), numSplits);
-        afterBarrier.get().await();
-
-        jobClient.getJobExecutionResult().get();
-    }
-
-    private void assertSinkMetrics(
-            long processedRecordsPerSubtask, int parallelism, int numSplits) {
-        List<OperatorMetricGroup> groups =
-                inMemoryReporter.getReporter().findOperatorMetricGroups("MetricTestSink");
-        assertThat(groups, hasSize(parallelism));
-
-        int subtaskWithMetrics = 0;
-        for (OperatorMetricGroup group : groups) {
-            Map<String, Metric> metrics = inMemoryReporter.getReporter().getMetricsByGroup(group);
-            // there are only 2 splits assigned; so two groups will not update metrics
-            if (group.getIOMetricGroup().getNumRecordsOutCounter().getCount() == 0) {
-                continue;
-            }
-            subtaskWithMetrics++;
-            // I/O metrics
-            assertThat(
-                    group.getIOMetricGroup().getNumRecordsOutCounter(),
-                    isCounter(equalTo(processedRecordsPerSubtask)));
-            assertThat(
-                    group.getIOMetricGroup().getNumBytesOutCounter(),
-                    isCounter(
-                            equalTo(
-                                    processedRecordsPerSubtask
-                                            * MetricWriter.RECORD_SIZE_IN_BYTES)));
-            // MetricWriter is just incrementing errors every even record
-            assertThat(
-                    metrics.get(MetricNames.NUM_RECORDS_OUT_ERRORS),
-                    isCounter(equalTo((processedRecordsPerSubtask + 1) / 2)));
-            // check if the latest send time is fetched
-            assertThat(
-                    metrics.get(MetricNames.CURRENT_SEND_TIME),
-                    isGauge(
-                            equalTo(
-                                    (processedRecordsPerSubtask - 1)
-                                            * MetricWriter.BASE_SEND_TIME)));
-        }
-        assertThat(subtaskWithMetrics, equalTo(numSplits));
-    }
-
     private static List<String> getSplittedGlobalCommittedData() {
         return GLOBAL_COMMIT_QUEUE.stream()
                 .flatMap(x -> Arrays.stream(x.split("\\+")))
@@ -384,27 +273,4 @@ public class SinkITCase extends AbstractTestBase {
         env.setRuntimeMode(RuntimeExecutionMode.BATCH);
         return env;
     }
-
-    private static class MetricWriter extends TestSink.DefaultSinkWriter<Long> {
-        static final long BASE_SEND_TIME = 100;
-        static final long RECORD_SIZE_IN_BYTES = 10;
-        private SinkWriterMetricGroup metricGroup;
-        private long sendTime;
-
-        @Override
-        public void init(Sink.InitContext context) {
-            this.metricGroup = context.metricGroup();
-            metricGroup.setCurrentSendTimeGauge(() -> sendTime);
-        }
-
-        @Override
-        public void write(Long element, Context context) {
-            super.write(element, context);
-            sendTime = element * BASE_SEND_TIME;
-            if (element % 2 == 0) {
-                metricGroup.getNumRecordsOutErrorsCounter().inc();
-            }
-            metricGroup.getIOMetricGroup().getNumBytesOutCounter().inc(RECORD_SIZE_IN_BYTES);
-        }
-    }
 }
diff --git a/flink-tests/src/test/java/org/apache/flink/test/streaming/runtime/SinkMetricsITCase.java b/flink-tests/src/test/java/org/apache/flink/test/streaming/runtime/SinkMetricsITCase.java
new file mode 100644
index 00000000000..7ad504f6518
--- /dev/null
+++ b/flink-tests/src/test/java/org/apache/flink/test/streaming/runtime/SinkMetricsITCase.java
@@ -0,0 +1,186 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.test.streaming.runtime;
+
+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;
+import org.apache.flink.api.connector.sink.Sink;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.core.execution.JobClient;
+import org.apache.flink.metrics.Metric;
+import org.apache.flink.metrics.groups.OperatorMetricGroup;
+import org.apache.flink.metrics.groups.SinkWriterMetricGroup;
+import org.apache.flink.runtime.metrics.MetricNames;
+import org.apache.flink.runtime.testutils.InMemoryReporter;
+import org.apache.flink.runtime.testutils.MiniClusterResourceConfiguration;
+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
+import org.apache.flink.streaming.runtime.operators.sink.TestSink;
+import org.apache.flink.test.util.MiniClusterWithClientResource;
+import org.apache.flink.testutils.junit.SharedObjects;
+import org.apache.flink.testutils.junit.SharedReference;
+import org.apache.flink.util.TestLogger;
+
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Rule;
+import org.junit.Test;
+
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.CyclicBarrier;
+import java.util.stream.LongStream;
+
+import static org.apache.flink.metrics.testutils.MetricMatchers.isCounter;
+import static org.apache.flink.metrics.testutils.MetricMatchers.isGauge;
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.MatcherAssert.assertThat;
+import static org.hamcrest.Matchers.hasSize;
+
+/** Tests whether all provided metrics of a {@link Sink} are of the expected values (FLIP-33). */
+public class SinkMetricsITCase extends TestLogger {
+    private static final int DEFAULT_PARALLELISM = 4;
+    @Rule public final SharedObjects sharedObjects = SharedObjects.create();
+    private InMemoryReporter reporter;
+
+    private MiniClusterWithClientResource miniClusterResource;
+
+    @Before
+    public void setup() throws Exception {
+        reporter = InMemoryReporter.createWithRetainedMetrics();
+        Configuration configuration = new Configuration();
+        reporter.addToConfiguration(configuration);
+        miniClusterResource =
+                new MiniClusterWithClientResource(
+                        new MiniClusterResourceConfiguration.Builder()
+                                .setNumberTaskManagers(1)
+                                .setNumberSlotsPerTaskManager(DEFAULT_PARALLELISM)
+                                .setConfiguration(configuration)
+                                .build());
+        miniClusterResource.before();
+    }
+
+    @After
+    public void teardown() {
+        miniClusterResource.after();
+    }
+
+    @Test
+    public void testMetrics() throws Exception {
+        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
+        int numSplits = Math.max(1, env.getParallelism() - 2);
+
+        int numRecordsPerSplit = 10;
+
+        // make sure all parallel instances have processed the same amount of records before
+        // validating metrics
+        SharedReference<CyclicBarrier> beforeBarrier =
+                sharedObjects.add(new CyclicBarrier(numSplits + 1));
+        SharedReference<CyclicBarrier> afterBarrier =
+                sharedObjects.add(new CyclicBarrier(numSplits + 1));
+        int stopAtRecord1 = 4;
+        int stopAtRecord2 = numRecordsPerSplit - 1;
+
+        env.fromSequence(0, numSplits - 1)
+                .<Long>flatMap(
+                        (split, collector) ->
+                                LongStream.range(0, numRecordsPerSplit).forEach(collector::collect))
+                .returns(BasicTypeInfo.LONG_TYPE_INFO)
+                .map(
+                        i -> {
+                            if (i % numRecordsPerSplit == stopAtRecord1
+                                    || i % numRecordsPerSplit == stopAtRecord2) {
+                                beforeBarrier.get().await();
+                                afterBarrier.get().await();
+                            }
+                            return i;
+                        })
+                .sinkTo(TestSink.newBuilder().setWriter(new MetricWriter()).build())
+                .name("MetricTestSink");
+        JobClient jobClient = env.executeAsync();
+
+        beforeBarrier.get().await();
+        assertSinkMetrics(stopAtRecord1, env.getParallelism(), numSplits);
+        afterBarrier.get().await();
+
+        beforeBarrier.get().await();
+        assertSinkMetrics(stopAtRecord2, env.getParallelism(), numSplits);
+        afterBarrier.get().await();
+
+        jobClient.getJobExecutionResult().get();
+    }
+
+    private void assertSinkMetrics(
+            long processedRecordsPerSubtask, int parallelism, int numSplits) {
+        List<OperatorMetricGroup> groups = reporter.findOperatorMetricGroups("MetricTestSink");
+        assertThat(groups, hasSize(parallelism));
+
+        int subtaskWithMetrics = 0;
+        for (OperatorMetricGroup group : groups) {
+            Map<String, Metric> metrics = reporter.getMetricsByGroup(group);
+            // there are only 2 splits assigned; so two groups will not update metrics
+            if (group.getIOMetricGroup().getNumRecordsOutCounter().getCount() == 0) {
+                continue;
+            }
+            subtaskWithMetrics++;
+            // I/O metrics
+            assertThat(
+                    group.getIOMetricGroup().getNumRecordsOutCounter(),
+                    isCounter(equalTo(processedRecordsPerSubtask)));
+            assertThat(
+                    group.getIOMetricGroup().getNumBytesOutCounter(),
+                    isCounter(
+                            equalTo(
+                                    processedRecordsPerSubtask
+                                            * MetricWriter.RECORD_SIZE_IN_BYTES)));
+            // MetricWriter is just incrementing errors every even record
+            assertThat(
+                    metrics.get(MetricNames.NUM_RECORDS_OUT_ERRORS),
+                    isCounter(equalTo((processedRecordsPerSubtask + 1) / 2)));
+            // check if the latest send time is fetched
+            assertThat(
+                    metrics.get(MetricNames.CURRENT_SEND_TIME),
+                    isGauge(
+                            equalTo(
+                                    (processedRecordsPerSubtask - 1)
+                                            * MetricWriter.BASE_SEND_TIME)));
+        }
+        assertThat(subtaskWithMetrics, equalTo(numSplits));
+    }
+
+    private static class MetricWriter extends TestSink.DefaultSinkWriter<Long> {
+        static final long BASE_SEND_TIME = 100;
+        static final long RECORD_SIZE_IN_BYTES = 10;
+        private SinkWriterMetricGroup metricGroup;
+        private long sendTime;
+
+        @Override
+        public void init(Sink.InitContext context) {
+            this.metricGroup = context.metricGroup();
+            metricGroup.setCurrentSendTimeGauge(() -> sendTime);
+        }
+
+        @Override
+        public void write(Long element, Context context) {
+            super.write(element, context);
+            sendTime = element * BASE_SEND_TIME;
+            if (element % 2 == 0) {
+                metricGroup.getNumRecordsOutErrorsCounter().inc();
+            }
+            metricGroup.getIOMetricGroup().getNumBytesOutCounter().inc(RECORD_SIZE_IN_BYTES);
+        }
+    }
+}
