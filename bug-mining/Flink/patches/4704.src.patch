diff --git a/flink-connectors/flink-connector-hbase-base/src/main/java/org/apache/flink/connector/hbase/util/HBaseTableSchema.java b/flink-connectors/flink-connector-hbase-base/src/main/java/org/apache/flink/connector/hbase/util/HBaseTableSchema.java
index 116b1ae374f..6cccc4139d7 100644
--- a/flink-connectors/flink-connector-hbase-base/src/main/java/org/apache/flink/connector/hbase/util/HBaseTableSchema.java
+++ b/flink-connectors/flink-connector-hbase-base/src/main/java/org/apache/flink/connector/hbase/util/HBaseTableSchema.java
@@ -21,6 +21,7 @@ package org.apache.flink.connector.hbase.util;
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.api.java.typeutils.TypeExtractor;
+import org.apache.flink.table.api.DataTypes;
 import org.apache.flink.table.api.TableSchema;
 import org.apache.flink.table.types.DataType;
 import org.apache.flink.table.types.logical.LogicalType;
@@ -315,10 +316,9 @@ public class HBaseTableSchema implements Serializable {
 					int familyIndex = i < rowKeyInfo.rowKeyIndex ? i : i - 1;
 					String family = familyNames[familyIndex];
 					fieldNames[i] = family;
-					fieldTypes[i] = TableSchema.builder()
-						.fields(getQualifierNames(family), getQualifierDataTypes(family))
-						.build()
-						.toRowDataType();
+					fieldTypes[i] = getRowDataType(
+						getQualifierNames(family),
+						getQualifierDataTypes(family));
 				}
 			}
 			return TableSchema.builder().fields(fieldNames, fieldTypes).build();
@@ -328,15 +328,30 @@ public class HBaseTableSchema implements Serializable {
 			for (int i = 0; i < fieldNames.length; i++) {
 				String family = familyNames[i];
 				fieldNames[i] = family;
-				fieldTypes[i] = TableSchema.builder()
-					.fields(getQualifierNames(family), getQualifierDataTypes(family))
-					.build()
-					.toRowDataType();
+				fieldTypes[i] = getRowDataType(
+					getQualifierNames(family),
+					getQualifierDataTypes(family));
 			}
 			return TableSchema.builder().fields(fieldNames, fieldTypes).build();
 		}
 	}
 
+	/**
+	 * Returns row data type with given field names {@code fieldNames}
+	 * and data types {@code fieldTypes}.
+	 *
+	 * @param fieldNames the field names
+	 * @param fieldTypes the field types
+	 * @return nullable row type
+	 */
+	private static DataType getRowDataType(String[] fieldNames, DataType[] fieldTypes) {
+		final DataTypes.Field[] fields = new DataTypes.Field[fieldNames.length];
+		for (int j = 0; j < fieldNames.length; j++) {
+			fields[j] = DataTypes.FIELD(fieldNames[j], fieldTypes[j]);
+		}
+		return DataTypes.ROW(fields);
+	}
+
 	/**
 	 * Construct a {@link HBaseTableSchema} from a {@link TableSchema}.
 	 */
diff --git a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSinkITCase.java b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSinkITCase.java
index 8e309153ea0..243e3a49919 100644
--- a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSinkITCase.java
+++ b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSinkITCase.java
@@ -511,7 +511,7 @@ public class HiveTableSinkITCase {
 
 		@Override
 		public DataType getProducedDataType() {
-			return TypeConversions.fromLegacyInfoToDataType(rowTypeInfo);
+			return TypeConversions.fromLegacyInfoToDataType(rowTypeInfo).notNull();
 		}
 
 		@Override
diff --git a/flink-formats/flink-avro-confluent-registry/src/test/java/org/apache/flink/formats/avro/registry/confluent/RegistryAvroRowDataSeDeSchemaTest.java b/flink-formats/flink-avro-confluent-registry/src/test/java/org/apache/flink/formats/avro/registry/confluent/RegistryAvroRowDataSeDeSchemaTest.java
index ce3d630b536..69378f04089 100644
--- a/flink-formats/flink-avro-confluent-registry/src/test/java/org/apache/flink/formats/avro/registry/confluent/RegistryAvroRowDataSeDeSchemaTest.java
+++ b/flink-formats/flink-avro-confluent-registry/src/test/java/org/apache/flink/formats/avro/registry/confluent/RegistryAvroRowDataSeDeSchemaTest.java
@@ -139,7 +139,8 @@ public class RegistryAvroRowDataSeDeSchemaTest {
 		RowType rowType = (RowType) dataType.getLogicalType();
 
 		AvroRowDataSerializationSchema serializer = getSerializationSchema(rowType, schema);
-		Schema writeSchema = AvroSchemaConverter.convertToSchema(dataType.getLogicalType());
+		Schema writeSchema = AvroSchemaConverter
+			.convertToSchema(dataType.getLogicalType());
 		AvroRowDataDeserializationSchema deserializer =
 				getDeserializationSchema(rowType, writeSchema);
 
diff --git a/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverter.java b/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverter.java
index 924fa55b5c6..bf44dd64f0f 100644
--- a/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverter.java
+++ b/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverter.java
@@ -265,7 +265,7 @@ public class AvroSchemaConverter {
 			if (logicalType == LogicalTypes.date()) {
 				return DataTypes.DATE().notNull();
 			} else if (logicalType == LogicalTypes.timeMillis()) {
-				return DataTypes.TIME().notNull();
+				return DataTypes.TIME(3).notNull();
 			}
 			return DataTypes.INT().notNull();
 		case LONG:
@@ -274,6 +274,8 @@ public class AvroSchemaConverter {
 				return DataTypes.TIMESTAMP(3).notNull();
 			} else if (schema.getLogicalType() == LogicalTypes.timestampMicros()) {
 				return DataTypes.TIMESTAMP(6).notNull();
+			} else if (schema.getLogicalType() == LogicalTypes.timeMillis()) {
+				return DataTypes.TIME(3).notNull();
 			} else if (schema.getLogicalType() == LogicalTypes.timeMicros()) {
 				return DataTypes.TIME(6).notNull();
 			}
@@ -293,16 +295,23 @@ public class AvroSchemaConverter {
 	/**
 	 * Converts Flink SQL {@link LogicalType} (can be nested) into an Avro schema.
 	 *
-	 * @param logicalType logical type
+	 * <p>Use "record" as the type name.
+	 *
+	 * @param schema the schema type, usually it should be the top level record type,
+	 *               e.g. not a nested type
 	 * @return Avro's {@link Schema} matching this logical type.
 	 */
-	public static Schema convertToSchema(LogicalType logicalType) {
-		return convertToSchema(logicalType, "record");
+	public static Schema convertToSchema(LogicalType schema) {
+		return convertToSchema(schema, "record");
 	}
 
 	/**
 	 * Converts Flink SQL {@link LogicalType} (can be nested) into an Avro schema.
 	 *
+	 * <p>The "{rowName}_" is used as the nested row type name prefix in order to
+	 * generate the right schema. Nested record type that only differs with type name
+	 * is still compatible.
+	 *
 	 * @param logicalType logical type
 	 * @param rowName     the record name
 	 * @return Avro's {@link Schema} matching this logical type.
@@ -349,10 +358,12 @@ public class AvroSchemaConverter {
 					throw new IllegalArgumentException("Avro does not support TIMESTAMP type " +
 						"with precision: " + precision + ", it only supports precision less than 3.");
 				}
-				return avroLogicalType.addToSchema(SchemaBuilder.builder().longType());
+				Schema timestamp = avroLogicalType.addToSchema(SchemaBuilder.builder().longType());
+				return nullable ? nullableSchema(timestamp) : timestamp;
 			case DATE:
 				// use int to represents Date
-				return LogicalTypes.date().addToSchema(SchemaBuilder.builder().intType());
+				Schema date = LogicalTypes.date().addToSchema(SchemaBuilder.builder().intType());
+				return nullable ? nullableSchema(date) : date;
 			case TIME_WITHOUT_TIME_ZONE:
 				precision = ((TimeType) logicalType).getPrecision();
 				if (precision > 3) {
@@ -361,13 +372,16 @@ public class AvroSchemaConverter {
 						", it only supports precision less than 3.");
 				}
 				// use int to represents Time, we only support millisecond when deserialization
-				return LogicalTypes.timeMillis().addToSchema(SchemaBuilder.builder().intType());
+				Schema time = LogicalTypes.timeMillis()
+						.addToSchema(SchemaBuilder.builder().intType());
+				return nullable ? nullableSchema(time) : time;
 			case DECIMAL:
 				DecimalType decimalType = (DecimalType) logicalType;
 				// store BigDecimal as byte[]
-				return LogicalTypes
-					.decimal(decimalType.getPrecision(), decimalType.getScale())
-					.addToSchema(SchemaBuilder.builder().bytesType());
+				Schema decimal = LogicalTypes
+						.decimal(decimalType.getPrecision(), decimalType.getScale())
+						.addToSchema(SchemaBuilder.builder().bytesType());
+				return nullable ? nullableSchema(decimal) : decimal;
 			case ROW:
 				RowType rowType = (RowType) logicalType;
 				List<String> fieldNames = rowType.getFieldNames();
@@ -386,18 +400,16 @@ public class AvroSchemaConverter {
 				return nullable ? nullableSchema(record) : record;
 			case MULTISET:
 			case MAP:
-				return SchemaBuilder
-					.builder()
-					.nullable()
-					.map()
-					.values(convertToSchema(extractValueTypeToAvroMap(logicalType), rowName));
+				Schema map = SchemaBuilder.builder()
+						.map()
+						.values(convertToSchema(extractValueTypeToAvroMap(logicalType), rowName));
+				return nullable ? nullableSchema(map) : map;
 			case ARRAY:
 				ArrayType arrayType = (ArrayType) logicalType;
-				return SchemaBuilder
-					.builder()
-					.nullable()
-					.array()
-					.items(convertToSchema(arrayType.getElementType(), rowName));
+				Schema array = SchemaBuilder.builder()
+						.array()
+						.items(convertToSchema(arrayType.getElementType(), rowName));
+				return nullable ? nullableSchema(array) : array;
 			case RAW:
 			case TIMESTAMP_WITH_LOCAL_TIME_ZONE:
 			default:
diff --git a/flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/AvroRowDataDeSerializationSchemaTest.java b/flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/AvroRowDataDeSerializationSchemaTest.java
index 206f8a671ed..8d297067aae 100644
--- a/flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/AvroRowDataDeSerializationSchemaTest.java
+++ b/flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/AvroRowDataDeSerializationSchemaTest.java
@@ -95,7 +95,8 @@ public class AvroRowDataDeSerializationSchemaTest {
 			FIELD("map", MAP(STRING(), BIGINT())),
 			FIELD("map2map", MAP(STRING(), MAP(STRING(), INT()))),
 			FIELD("map2array", MAP(STRING(), ARRAY(INT()))),
-			FIELD("nullEntryMap", MAP(STRING(), STRING())));
+			FIELD("nullEntryMap", MAP(STRING(), STRING())))
+			.notNull();
 		final RowType rowType = (RowType) dataType.getLogicalType();
 		final TypeInformation<RowData> typeInfo = InternalTypeInfo.of(rowType);
 
@@ -182,9 +183,10 @@ public class AvroRowDataDeSerializationSchemaTest {
 		byte[] input = byteArrayOutputStream.toByteArray();
 
 		DataType dataType = ROW(
-				FIELD("type_timestamp_millis", TIMESTAMP(3)),
-				FIELD("type_date", DATE()),
-				FIELD("type_time_millis", TIME(3)));
+				FIELD("type_timestamp_millis", TIMESTAMP(3).notNull()),
+				FIELD("type_date", DATE().notNull()),
+				FIELD("type_time_millis", TIME(3).notNull()))
+			.notNull();
 		final RowType rowType = (RowType) dataType.getLogicalType();
 		final TypeInformation<RowData> typeInfo = InternalTypeInfo.of(rowType);
 		AvroRowDataSerializationSchema serializationSchema = new AvroRowDataSerializationSchema(rowType);
diff --git a/flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverterTest.java b/flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverterTest.java
index f673a6a9680..2e63cbb4f26 100644
--- a/flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverterTest.java
+++ b/flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverterTest.java
@@ -104,80 +104,79 @@ public class AvroSchemaConverterTest {
 				DataTypes.FIELD("row3", DataTypes.ROW(DataTypes.FIELD("c", DataTypes.STRING())))))
 			.build().toRowDataType().getLogicalType();
 		Schema schema = AvroSchemaConverter.convertToSchema(rowType);
-		assertEquals("[ {\n" +
-				"  \"type\" : \"record\",\n" +
-				"  \"name\" : \"record\",\n" +
-				"  \"fields\" : [ {\n" +
-				"    \"name\" : \"row1\",\n" +
-				"    \"type\" : [ {\n" +
-				"      \"type\" : \"record\",\n" +
-				"      \"name\" : \"record_row1\",\n" +
-				"      \"fields\" : [ {\n" +
-				"        \"name\" : \"a\",\n" +
-				"        \"type\" : [ \"string\", \"null\" ]\n" +
-				"      } ]\n" +
-				"    }, \"null\" ]\n" +
-				"  }, {\n" +
-				"    \"name\" : \"row2\",\n" +
-				"    \"type\" : [ {\n" +
-				"      \"type\" : \"record\",\n" +
-				"      \"name\" : \"record_row2\",\n" +
-				"      \"fields\" : [ {\n" +
-				"        \"name\" : \"b\",\n" +
-				"        \"type\" : [ \"string\", \"null\" ]\n" +
-				"      } ]\n" +
-				"    }, \"null\" ]\n" +
-				"  }, {\n" +
-				"    \"name\" : \"row3\",\n" +
-				"    \"type\" : [ {\n" +
-				"      \"type\" : \"record\",\n" +
-				"      \"name\" : \"record_row3\",\n" +
-				"      \"fields\" : [ {\n" +
-				"        \"name\" : \"row3\",\n" +
-				"        \"type\" : [ {\n" +
-				"          \"type\" : \"record\",\n" +
-				"          \"name\" : \"record_row3_row3\",\n" +
-				"          \"fields\" : [ {\n" +
-				"            \"name\" : \"c\",\n" +
-				"            \"type\" : [ \"string\", \"null\" ]\n" +
-				"          } ]\n" +
-				"        }, \"null\" ]\n" +
-				"      } ]\n" +
-				"    }, \"null\" ]\n" +
-				"  } ]\n" +
-				"}, \"null\" ]", schema.toString(true));
+		assertEquals("{\n" +
+			"  \"type\" : \"record\",\n" +
+			"  \"name\" : \"record\",\n" +
+			"  \"fields\" : [ {\n" +
+			"    \"name\" : \"row1\",\n" +
+			"    \"type\" : [ {\n" +
+			"      \"type\" : \"record\",\n" +
+			"      \"name\" : \"record_row1\",\n" +
+			"      \"fields\" : [ {\n" +
+			"        \"name\" : \"a\",\n" +
+			"        \"type\" : [ \"string\", \"null\" ]\n" +
+			"      } ]\n" +
+			"    }, \"null\" ]\n" +
+			"  }, {\n" +
+			"    \"name\" : \"row2\",\n" +
+			"    \"type\" : [ {\n" +
+			"      \"type\" : \"record\",\n" +
+			"      \"name\" : \"record_row2\",\n" +
+			"      \"fields\" : [ {\n" +
+			"        \"name\" : \"b\",\n" +
+			"        \"type\" : [ \"string\", \"null\" ]\n" +
+			"      } ]\n" +
+			"    }, \"null\" ]\n" +
+			"  }, {\n" +
+			"    \"name\" : \"row3\",\n" +
+			"    \"type\" : [ {\n" +
+			"      \"type\" : \"record\",\n" +
+			"      \"name\" : \"record_row3\",\n" +
+			"      \"fields\" : [ {\n" +
+			"        \"name\" : \"row3\",\n" +
+			"        \"type\" : [ {\n" +
+			"          \"type\" : \"record\",\n" +
+			"          \"name\" : \"record_row3_row3\",\n" +
+			"          \"fields\" : [ {\n" +
+			"            \"name\" : \"c\",\n" +
+			"            \"type\" : [ \"string\", \"null\" ]\n" +
+			"          } ]\n" +
+			"        }, \"null\" ]\n" +
+			"      } ]\n" +
+			"    }, \"null\" ]\n" +
+			"  } ]\n" +
+			"}", schema.toString(true));
 	}
 
 	/**
 	 * Test convert nullable data type to Avro schema then converts back.
 	 */
 	@Test
-	public void testConversionIntegralityNullable() {
+	public void testDataTypeToSchemaToDataTypeNullable() {
 		DataType dataType = DataTypes.ROW(
-				DataTypes.FIELD("f_null", DataTypes.NULL()),
-				DataTypes.FIELD("f_boolean", DataTypes.BOOLEAN()),
-				// tinyint and smallint all convert to int
-				DataTypes.FIELD("f_int", DataTypes.INT()),
-				DataTypes.FIELD("f_bigint", DataTypes.BIGINT()),
-				DataTypes.FIELD("f_float", DataTypes.FLOAT()),
-				DataTypes.FIELD("f_double", DataTypes.DOUBLE()),
-				// char converts to string
-				DataTypes.FIELD("f_string", DataTypes.STRING()),
-				// binary converts to bytes
-				DataTypes.FIELD("f_varbinary", DataTypes.BYTES()),
-				DataTypes.FIELD("f_timestamp", DataTypes.TIMESTAMP(3)),
-				DataTypes.FIELD("f_date", DataTypes.DATE()),
-				DataTypes.FIELD("f_time", DataTypes.TIME(3)),
-				DataTypes.FIELD("f_decimal", DataTypes.DECIMAL(10, 0)),
-				DataTypes.FIELD("f_row", DataTypes.ROW(
-					DataTypes.FIELD("f0", DataTypes.INT()),
-					DataTypes.FIELD("f1", DataTypes.TIMESTAMP(3)))),
-				// multiset converts to map
-				// map key is always not null
-				DataTypes.FIELD("f_map",
-						DataTypes.MAP(DataTypes.STRING().notNull(), DataTypes.INT())),
-				DataTypes.FIELD("f_array", DataTypes.ARRAY(DataTypes.INT())))
-				.notNull();
+			DataTypes.FIELD("f_null", DataTypes.NULL()),
+			DataTypes.FIELD("f_boolean", DataTypes.BOOLEAN()),
+			// tinyint and smallint all convert to int
+			DataTypes.FIELD("f_int", DataTypes.INT()),
+			DataTypes.FIELD("f_bigint", DataTypes.BIGINT()),
+			DataTypes.FIELD("f_float", DataTypes.FLOAT()),
+			DataTypes.FIELD("f_double", DataTypes.DOUBLE()),
+			// char converts to string
+			DataTypes.FIELD("f_string", DataTypes.STRING()),
+			// binary converts to bytes
+			DataTypes.FIELD("f_varbinary", DataTypes.BYTES()),
+			DataTypes.FIELD("f_timestamp", DataTypes.TIMESTAMP(3)),
+			DataTypes.FIELD("f_date", DataTypes.DATE()),
+			DataTypes.FIELD("f_time", DataTypes.TIME(3)),
+			DataTypes.FIELD("f_decimal", DataTypes.DECIMAL(10, 0)),
+			DataTypes.FIELD("f_row", DataTypes.ROW(
+				DataTypes.FIELD("f0", DataTypes.INT()),
+				DataTypes.FIELD("f1", DataTypes.TIMESTAMP(3)))),
+			// multiset converts to map
+			// map key is always not null
+			DataTypes.FIELD("f_map",
+				DataTypes.MAP(DataTypes.STRING().notNull(), DataTypes.INT())),
+			DataTypes.FIELD("f_array", DataTypes.ARRAY(DataTypes.INT())));
 		Schema schema = AvroSchemaConverter.convertToSchema(dataType.getLogicalType());
 		DataType converted = AvroSchemaConverter.convertToDataType(schema.toString());
 		assertEquals(dataType, converted);
@@ -187,41 +186,225 @@ public class AvroSchemaConverterTest {
 	 * Test convert non-nullable data type to Avro schema then converts back.
 	 */
 	@Test
-	public void testConversionIntegralityNonNullable() {
+	public void testDataTypeToSchemaToDataTypeNonNullable() {
 		DataType dataType = DataTypes.ROW(
-				DataTypes.FIELD("f_boolean", DataTypes.BOOLEAN().notNull()),
-				// tinyint and smallint all convert to int
-				DataTypes.FIELD("f_int", DataTypes.INT().notNull()),
-				DataTypes.FIELD("f_bigint", DataTypes.BIGINT().notNull()),
-				DataTypes.FIELD("f_float", DataTypes.FLOAT().notNull()),
-				DataTypes.FIELD("f_double", DataTypes.DOUBLE().notNull()),
-				// char converts to string
-				DataTypes.FIELD("f_string", DataTypes.STRING().notNull()),
-				// binary converts to bytes
-				DataTypes.FIELD("f_varbinary", DataTypes.BYTES().notNull()),
-				DataTypes.FIELD("f_timestamp", DataTypes.TIMESTAMP(3).notNull()),
-				DataTypes.FIELD("f_date", DataTypes.DATE().notNull()),
-				DataTypes.FIELD("f_time", DataTypes.TIME(3).notNull()),
-				DataTypes.FIELD("f_decimal",
-						DataTypes.DECIMAL(10, 0).notNull()),
-				DataTypes.FIELD("f_row", DataTypes.ROW(
-						DataTypes.FIELD("f0", DataTypes.INT().notNull()),
-						DataTypes.FIELD("f1", DataTypes.TIMESTAMP(3).notNull()))
-						.notNull()),
-				// multiset converts to map
-				// map key is always not null
-				DataTypes.FIELD("f_map",
-						DataTypes.MAP(
-								DataTypes.STRING().notNull(),
-								DataTypes.INT().notNull())
-								.notNull()),
-				DataTypes.FIELD("f_array",
-						DataTypes.ARRAY(DataTypes.INT().notNull()).notNull()));
+			DataTypes.FIELD("f_boolean", DataTypes.BOOLEAN().notNull()),
+			// tinyint and smallint all convert to int
+			DataTypes.FIELD("f_int", DataTypes.INT().notNull()),
+			DataTypes.FIELD("f_bigint", DataTypes.BIGINT().notNull()),
+			DataTypes.FIELD("f_float", DataTypes.FLOAT().notNull()),
+			DataTypes.FIELD("f_double", DataTypes.DOUBLE().notNull()),
+			// char converts to string
+			DataTypes.FIELD("f_string", DataTypes.STRING().notNull()),
+			// binary converts to bytes
+			DataTypes.FIELD("f_varbinary", DataTypes.BYTES().notNull()),
+			DataTypes.FIELD("f_timestamp", DataTypes.TIMESTAMP(3).notNull()),
+			DataTypes.FIELD("f_date", DataTypes.DATE().notNull()),
+			DataTypes.FIELD("f_time", DataTypes.TIME(3).notNull()),
+			DataTypes.FIELD("f_decimal",
+				DataTypes.DECIMAL(10, 0).notNull()),
+			DataTypes.FIELD("f_row", DataTypes.ROW(
+				DataTypes.FIELD("f0", DataTypes.INT().notNull()),
+				DataTypes.FIELD("f1", DataTypes.TIMESTAMP(3).notNull()))
+				.notNull()),
+			// multiset converts to map
+			// map key is always not null
+			DataTypes.FIELD("f_map",
+				DataTypes.MAP(
+					DataTypes.STRING().notNull(),
+					DataTypes.INT().notNull())
+					.notNull()),
+			DataTypes.FIELD("f_array",
+				DataTypes.ARRAY(DataTypes.INT().notNull()).notNull()))
+			.notNull();
 		Schema schema = AvroSchemaConverter.convertToSchema(dataType.getLogicalType());
 		DataType converted = AvroSchemaConverter.convertToDataType(schema.toString());
 		assertEquals(dataType, converted);
 	}
 
+	/**
+	 * Test convert nullable Avro schema to data type then converts back.
+	 */
+	@Test
+	public void testSchemaToDataTypeToSchemaNullable() {
+		String schemaStr = "{\n" +
+			"  \"type\" : \"record\",\n" +
+			"  \"name\" : \"record\",\n" +
+			"  \"fields\" : [ {\n" +
+			"    \"name\" : \"f_null\",\n" +
+			"    \"type\" : \"null\"\n" +
+			"  }, {\n" +
+			"    \"name\" : \"f_boolean\",\n" +
+			"    \"type\" : [ \"boolean\", \"null\" ]\n" +
+			"  }, {\n" +
+			"    \"name\" : \"f_int\",\n" +
+			"    \"type\" : [ \"int\", \"null\" ]\n" +
+			"  }, {\n" +
+			"    \"name\" : \"f_bigint\",\n" +
+			"    \"type\" : [ \"long\", \"null\" ]\n" +
+			"  }, {\n" +
+			"    \"name\" : \"f_float\",\n" +
+			"    \"type\" : [ \"float\", \"null\" ]\n" +
+			"  }, {\n" +
+			"    \"name\" : \"f_double\",\n" +
+			"    \"type\" : [ \"double\", \"null\" ]\n" +
+			"  }, {\n" +
+			"    \"name\" : \"f_string\",\n" +
+			"    \"type\" : [ \"string\", \"null\" ]\n" +
+			"  }, {\n" +
+			"    \"name\" : \"f_varbinary\",\n" +
+			"    \"type\" : [ \"bytes\", \"null\" ]\n" +
+			"  }, {\n" +
+			"    \"name\" : \"f_timestamp\",\n" +
+			"    \"type\" : [ {\n" +
+			"      \"type\" : \"long\",\n" +
+			"      \"logicalType\" : \"timestamp-millis\"\n" +
+			"    }, \"null\" ]\n" +
+			"  }, {\n" +
+			"    \"name\" : \"f_date\",\n" +
+			"    \"type\" : [ {\n" +
+			"      \"type\" : \"int\",\n" +
+			"      \"logicalType\" : \"date\"\n" +
+			"    }, \"null\" ]\n" +
+			"  }, {\n" +
+			"    \"name\" : \"f_time\",\n" +
+			"    \"type\" : [ {\n" +
+			"      \"type\" : \"int\",\n" +
+			"      \"logicalType\" : \"time-millis\"\n" +
+			"    }, \"null\" ]\n" +
+			"  }, {\n" +
+			"    \"name\" : \"f_decimal\",\n" +
+			"    \"type\" : [ {\n" +
+			"      \"type\" : \"bytes\",\n" +
+			"      \"logicalType\" : \"decimal\",\n" +
+			"      \"precision\" : 10,\n" +
+			"      \"scale\" : 0\n" +
+			"    }, \"null\" ]\n" +
+			"  }, {\n" +
+			"    \"name\" : \"f_row\",\n" +
+			"    \"type\" : [ {\n" +
+			"      \"type\" : \"record\",\n" +
+			"      \"name\" : \"record_f_row\",\n" +
+			"      \"fields\" : [ {\n" +
+			"        \"name\" : \"f0\",\n" +
+			"        \"type\" : [ \"int\", \"null\" ]\n" +
+			"      }, {\n" +
+			"        \"name\" : \"f1\",\n" +
+			"        \"type\" : [ {\n" +
+			"          \"type\" : \"long\",\n" +
+			"          \"logicalType\" : \"timestamp-millis\"\n" +
+			"        }, \"null\" ]\n" +
+			"      } ]\n" +
+			"    }, \"null\" ]\n" +
+			"  }, {\n" +
+			"    \"name\" : \"f_map\",\n" +
+			"    \"type\" : [ {\n" +
+			"      \"type\" : \"map\",\n" +
+			"      \"values\" : [ \"int\", \"null\" ]\n" +
+			"    }, \"null\" ]\n" +
+			"  }, {\n" +
+			"    \"name\" : \"f_array\",\n" +
+			"    \"type\" : [ {\n" +
+			"      \"type\" : \"array\",\n" +
+			"      \"items\" : [ \"int\", \"null\" ]\n" +
+			"    }, \"null\" ]\n" +
+			"  } ]\n" +
+			"}";
+		DataType dataType = AvroSchemaConverter.convertToDataType(schemaStr);
+		Schema schema = AvroSchemaConverter.convertToSchema(dataType.getLogicalType());
+		assertEquals(new Schema.Parser().parse(schemaStr), schema);
+	}
+
+	/**
+	 * Test convert non-nullable Avro schema to data type then converts back.
+	 */
+	@Test
+	public void testSchemaToDataTypeToSchemaNonNullable() {
+		String schemaStr = "{\n" +
+			"  \"type\" : \"record\",\n" +
+			"  \"name\" : \"record\",\n" +
+			"  \"fields\" : [ {\n" +
+			"    \"name\" : \"f_boolean\",\n" +
+			"    \"type\" : \"boolean\"\n" +
+			"  }, {\n" +
+			"    \"name\" : \"f_int\",\n" +
+			"    \"type\" : \"int\"\n" +
+			"  }, {\n" +
+			"    \"name\" : \"f_bigint\",\n" +
+			"    \"type\" : \"long\"\n" +
+			"  }, {\n" +
+			"    \"name\" : \"f_float\",\n" +
+			"    \"type\" : \"float\"\n" +
+			"  }, {\n" +
+			"    \"name\" : \"f_double\",\n" +
+			"    \"type\" : \"double\"\n" +
+			"  }, {\n" +
+			"    \"name\" : \"f_string\",\n" +
+			"    \"type\" : \"string\"\n" +
+			"  }, {\n" +
+			"    \"name\" : \"f_varbinary\",\n" +
+			"    \"type\" : \"bytes\"\n" +
+			"  }, {\n" +
+			"    \"name\" : \"f_timestamp\",\n" +
+			"    \"type\" : {\n" +
+			"      \"type\" : \"long\",\n" +
+			"      \"logicalType\" : \"timestamp-millis\"\n" +
+			"    }\n" +
+			"  }, {\n" +
+			"    \"name\" : \"f_date\",\n" +
+			"    \"type\" : {\n" +
+			"      \"type\" : \"int\",\n" +
+			"      \"logicalType\" : \"date\"\n" +
+			"    }\n" +
+			"  }, {\n" +
+			"    \"name\" : \"f_time\",\n" +
+			"    \"type\" : {\n" +
+			"      \"type\" : \"int\",\n" +
+			"      \"logicalType\" : \"time-millis\"\n" +
+			"    }\n" +
+			"  }, {\n" +
+			"    \"name\" : \"f_decimal\",\n" +
+			"    \"type\" : {\n" +
+			"      \"type\" : \"bytes\",\n" +
+			"      \"logicalType\" : \"decimal\",\n" +
+			"      \"precision\" : 10,\n" +
+			"      \"scale\" : 0\n" +
+			"    }\n" +
+			"  }, {\n" +
+			"    \"name\" : \"f_row\",\n" +
+			"    \"type\" : {\n" +
+			"      \"type\" : \"record\",\n" +
+			"      \"name\" : \"record_f_row\",\n" +
+			"      \"fields\" : [ {\n" +
+			"        \"name\" : \"f0\",\n" +
+			"        \"type\" : \"int\"\n" +
+			"      }, {\n" +
+			"        \"name\" : \"f1\",\n" +
+			"        \"type\" : {\n" +
+			"          \"type\" : \"long\",\n" +
+			"          \"logicalType\" : \"timestamp-millis\"\n" +
+			"        }\n" +
+			"      } ]\n" +
+			"    }\n" +
+			"  }, {\n" +
+			"    \"name\" : \"f_map\",\n" +
+			"    \"type\" : {\n" +
+			"      \"type\" : \"map\",\n" +
+			"      \"values\" : \"int\"\n" +
+			"    }\n" +
+			"  }, {\n" +
+			"    \"name\" : \"f_array\",\n" +
+			"    \"type\" : {\n" +
+			"      \"type\" : \"array\",\n" +
+			"      \"items\" : \"int\"\n" +
+			"    }\n" +
+			"  } ]\n" +
+			"}";
+		DataType dataType = AvroSchemaConverter.convertToDataType(schemaStr);
+		Schema schema = AvroSchemaConverter.convertToSchema(dataType.getLogicalType());
+		assertEquals(new Schema.Parser().parse(schemaStr), schema);
+	}
+
 	private void validateUserSchema(TypeInformation<?> actual) {
 		final TypeInformation<Row> address = Types.ROW_NAMED(
 			new String[]{
@@ -320,7 +503,7 @@ public class AvroSchemaConverterTest {
 				DataTypes.FIELD("type_nested", address),
 				DataTypes.FIELD("type_bytes", DataTypes.BYTES().notNull()),
 				DataTypes.FIELD("type_date", DataTypes.DATE().notNull()),
-				DataTypes.FIELD("type_time_millis", DataTypes.TIME().notNull()),
+				DataTypes.FIELD("type_time_millis", DataTypes.TIME(3).notNull()),
 				DataTypes.FIELD("type_time_micros", DataTypes.TIME(6).notNull()),
 				DataTypes.FIELD("type_timestamp_millis",
 						DataTypes.TIMESTAMP(3).notNull()),
diff --git a/flink-python/pyflink/table/table_environment.py b/flink-python/pyflink/table/table_environment.py
index 6f2cbebda27..4b760a585f1 100644
--- a/flink-python/pyflink/table/table_environment.py
+++ b/flink-python/pyflink/table/table_environment.py
@@ -1534,7 +1534,7 @@ class TableEnvironment(object):
             jvm = get_gateway().jvm
 
             data_type = jvm.org.apache.flink.table.types.utils.TypeConversions\
-                .fromLegacyInfoToDataType(_to_java_type(result_type))
+                .fromLegacyInfoToDataType(_to_java_type(result_type)).notNull()
             if self._is_blink_planner:
                 data_type = data_type.bridgedTo(
                     load_java_class('org.apache.flink.table.data.RowData'))
diff --git a/flink-python/pyflink/table/tests/test_pandas_conversion.py b/flink-python/pyflink/table/tests/test_pandas_conversion.py
index 1e3f44dd451..1429ac35254 100644
--- a/flink-python/pyflink/table/tests/test_pandas_conversion.py
+++ b/flink-python/pyflink/table/tests/test_pandas_conversion.py
@@ -63,7 +63,7 @@ class PandasConversionTestBase(object):
                  [DataTypes.FIELD("a", DataTypes.INT()),
                   DataTypes.FIELD("b", DataTypes.STRING()),
                   DataTypes.FIELD("c", DataTypes.TIMESTAMP(3)),
-                  DataTypes.FIELD("d", DataTypes.ARRAY(DataTypes.INT()))]))])
+                  DataTypes.FIELD("d", DataTypes.ARRAY(DataTypes.INT()))]))], False)
         cls.pdf = cls.create_pandas_data_frame()
 
     @classmethod
diff --git a/flink-python/pyflink/table/tests/test_table_schema.py b/flink-python/pyflink/table/tests/test_table_schema.py
index 8d70d882483..47122bbff41 100644
--- a/flink-python/pyflink/table/tests/test_table_schema.py
+++ b/flink-python/pyflink/table/tests/test_table_schema.py
@@ -100,7 +100,8 @@ class TableSchemaTests(PyFlinkTestCase):
 
         expected = DataTypes.ROW([DataTypes.FIELD("a", DataTypes.INT()),
                                   DataTypes.FIELD("b", DataTypes.BIGINT()),
-                                  DataTypes.FIELD("c", DataTypes.STRING())])
+                                  DataTypes.FIELD("c", DataTypes.STRING())],
+                                 nullable=False)
         self.assertEqual(expected, row_type)
 
     def test_hash(self):
diff --git a/flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/gateway/utils/SimpleCatalogFactory.java b/flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/gateway/utils/SimpleCatalogFactory.java
index 5f533fb88e4..bdf991ec61c 100644
--- a/flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/gateway/utils/SimpleCatalogFactory.java
+++ b/flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/gateway/utils/SimpleCatalogFactory.java
@@ -86,8 +86,8 @@ public class SimpleCatalogFactory implements CatalogFactory {
 			public DataType getProducedDataType() {
 				return DataTypes.ROW(
 					DataTypes.FIELD("id", DataTypes.INT()),
-					DataTypes.FIELD("string", DataTypes.STRING())
-				);
+					DataTypes.FIELD("string", DataTypes.STRING()))
+					.notNull();
 			}
 		};
 
diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/api/TableSchema.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/api/TableSchema.java
index 69fef514276..861ec35ac28 100644
--- a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/api/TableSchema.java
+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/api/TableSchema.java
@@ -245,7 +245,8 @@ public class TableSchema {
 		final Field[] fields = columns.stream()
 			.map(column -> FIELD(column.getName(), column.getType()))
 			.toArray(Field[]::new);
-		return ROW(fields);
+		// The row should be never null.
+		return ROW(fields).notNull();
 	}
 
 	/**
@@ -263,7 +264,8 @@ public class TableSchema {
 			.filter(TableColumn::isPhysical)
 			.map(column -> FIELD(column.getName(), column.getType()))
 			.toArray(Field[]::new);
-		return ROW(fields);
+		// The row should be never null.
+		return ROW(fields).notNull();
 	}
 
 	/**
@@ -283,7 +285,8 @@ public class TableSchema {
 			.filter(TableColumn::isPersisted)
 			.map(column -> FIELD(column.getName(), column.getType()))
 			.toArray(Field[]::new);
-		return ROW(fields);
+		// The row should be never null.
+		return ROW(fields).notNull();
 	}
 
 	/**
diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/FileSystemFormatFactory.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/FileSystemFormatFactory.java
index 95c51758cd8..6248bb00656 100644
--- a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/FileSystemFormatFactory.java
+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/FileSystemFormatFactory.java
@@ -134,6 +134,7 @@ public interface FileSystemFormatFactory extends Factory {
 		 */
 		default RowType getFormatRowType() {
 			return RowType.of(
+				false,
 				Arrays.stream(getFormatFieldTypes())
 					.map(DataType::getLogicalType)
 					.toArray(LogicalType[]::new),
@@ -198,6 +199,7 @@ public interface FileSystemFormatFactory extends Factory {
 		 */
 		default RowType getFormatRowType() {
 			return RowType.of(
+				false,
 				Arrays.stream(getFormatFieldTypes())
 					.map(DataType::getLogicalType)
 					.toArray(LogicalType[]::new),
diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/sources/TableSource.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/sources/TableSource.java
index a6fd0c5a3f3..10623e5ee18 100644
--- a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/sources/TableSource.java
+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/sources/TableSource.java
@@ -60,7 +60,7 @@ public interface TableSource<T> {
 		if (legacyType == null) {
 			throw new TableException("Table source does not implement a produced data type.");
 		}
-		return fromLegacyInfoToDataType(legacyType);
+		return fromLegacyInfoToDataType(legacyType).notNull();
 	}
 
 	/**
diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/logical/RowType.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/logical/RowType.java
index 1a740e8725f..1775100de69 100644
--- a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/logical/RowType.java
+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/logical/RowType.java
@@ -292,10 +292,14 @@ public final class RowType extends LogicalType {
 	}
 
 	public static RowType of(LogicalType[] types, String[] names) {
+		return of(true, types, names);
+	}
+
+	public static RowType of(boolean nullable, LogicalType[] types, String[] names) {
 		List<RowField> fields = new ArrayList<>();
 		for (int i = 0; i < types.length; i++) {
 			fields.add(new RowField(names[i], types[i]));
 		}
-		return new RowType(fields);
+		return new RowType(nullable, fields);
 	}
 }
diff --git a/flink-table/flink-table-common/src/test/java/org/apache/flink/table/api/TableSchemaTest.java b/flink-table/flink-table-common/src/test/java/org/apache/flink/table/api/TableSchemaTest.java
index 21081e7cfbb..baddd1fe5f8 100644
--- a/flink-table/flink-table-common/src/test/java/org/apache/flink/table/api/TableSchemaTest.java
+++ b/flink-table/flink-table-common/src/test/java/org/apache/flink/table/api/TableSchemaTest.java
@@ -109,12 +109,54 @@ public class TableSchemaTest {
 			DataTypes.FIELD("f0", DataTypes.BIGINT()),
 			DataTypes.FIELD("f2", DataTypes.BIGINT()),
 			DataTypes.FIELD("f3", DataTypes.STRING()),
-			DataTypes.FIELD("f5", DataTypes.BIGINT())
-		);
+			DataTypes.FIELD("f5", DataTypes.BIGINT()))
+			.notNull();
 
 		assertThat(schema.toPersistedRowDataType(), equalTo(expectedDataType));
 	}
 
+	@Test
+	public void testPhysicalRowDataType() {
+		final TableSchema schema = TableSchema.builder()
+			.add(TableColumn.physical("f0", DataTypes.BIGINT()))
+			.add(TableColumn.metadata("f1", DataTypes.BIGINT(), true))
+			.add(TableColumn.metadata("f2", DataTypes.BIGINT(), false))
+			.add(TableColumn.physical("f3", DataTypes.STRING()))
+			.add(TableColumn.computed("f4", DataTypes.BIGINT(), "f0 + 1"))
+			.add(TableColumn.metadata("f5", DataTypes.BIGINT(), false))
+			.build();
+
+		final DataType expectedDataType = DataTypes.ROW(
+			DataTypes.FIELD("f0", DataTypes.BIGINT()),
+			DataTypes.FIELD("f3", DataTypes.STRING()))
+			.notNull();
+
+		assertThat(schema.toPhysicalRowDataType(), equalTo(expectedDataType));
+	}
+
+	@Test
+	public void testRowDataType() {
+		final TableSchema schema = TableSchema.builder()
+			.add(TableColumn.physical("f0", DataTypes.BIGINT()))
+			.add(TableColumn.metadata("f1", DataTypes.BIGINT(), true))
+			.add(TableColumn.metadata("f2", DataTypes.BIGINT(), false))
+			.add(TableColumn.physical("f3", DataTypes.STRING()))
+			.add(TableColumn.computed("f4", DataTypes.BIGINT(), "f0 + 1"))
+			.add(TableColumn.metadata("f5", DataTypes.BIGINT(), false))
+			.build();
+
+		final DataType expectedDataType = DataTypes.ROW(
+			DataTypes.FIELD("f0", DataTypes.BIGINT()),
+			DataTypes.FIELD("f1", DataTypes.BIGINT()),
+			DataTypes.FIELD("f2", DataTypes.BIGINT()),
+			DataTypes.FIELD("f3", DataTypes.STRING()),
+			DataTypes.FIELD("f4", DataTypes.BIGINT()),
+			DataTypes.FIELD("f5", DataTypes.BIGINT()))
+			.notNull();
+
+		assertThat(schema.toRowDataType(), equalTo(expectedDataType));
+	}
+
 	@Test
 	public void testWatermarkOnDifferentFields() {
 		// column_name, column_type, exception_msg
diff --git a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/plan/nodes/dataset/BatchTableSourceScan.scala b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/plan/nodes/dataset/BatchTableSourceScan.scala
index fafb4dd25c8..9bffef2ce69 100644
--- a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/plan/nodes/dataset/BatchTableSourceScan.scala
+++ b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/plan/nodes/dataset/BatchTableSourceScan.scala
@@ -116,7 +116,8 @@ class BatchTableSourceScan(
       case _ => throw new TableException("Only BatchTableSource and InputFormatTableSource are " +
         "supported in BatchTableEnvironment.")
     }
-    val inputDataType = fromLegacyInfoToDataType(inputDataSet.getType)
+    // Fix the nullability of row type info.
+    val inputDataType = fromLegacyInfoToDataType(inputDataSet.getType).notNull()
     val producedDataType = tableSource.getProducedDataType
 
     // check that declared and actual type of table source DataSet are identical
diff --git a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/plan/nodes/datastream/StreamTableSourceScan.scala b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/plan/nodes/datastream/StreamTableSourceScan.scala
index 3080ca19337..0a3290acb04 100644
--- a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/plan/nodes/datastream/StreamTableSourceScan.scala
+++ b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/plan/nodes/datastream/StreamTableSourceScan.scala
@@ -106,7 +106,8 @@ class StreamTableSourceScan(
       .asInstanceOf[DataStream[Any]]
     val outputSchema = new RowSchema(this.getRowType)
 
-    val inputDataType = fromLegacyInfoToDataType(inputDataStream.getType)
+    // Fix the nullability of row type info.
+    val inputDataType = fromLegacyInfoToDataType(inputDataStream.getType).notNull()
     val producedDataType = tableSource.getProducedDataType
 
     // check that declared and actual type of table source DataStream are identical
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSource.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSource.java
index 7aa07cef356..f0bbe37e6a5 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSource.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSource.java
@@ -296,6 +296,7 @@ public class FileSystemTableSource extends AbstractFileSystemTable implements
 		return DataTypes.ROW(Arrays.stream(fields)
 				.mapToObj(i -> DataTypes.FIELD(schemaFieldNames[i], schemaTypes[i]))
 				.toArray(DataTypes.Field[]::new))
-				.bridgedTo(RowData.class);
+				.bridgedTo(RowData.class)
+			.notNull();
 	}
 }
