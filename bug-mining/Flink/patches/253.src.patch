diff --git a/flink-staging/flink-streaming/flink-streaming-connectors/pom.xml b/flink-staging/flink-streaming/flink-streaming-connectors/pom.xml
index df34b205f13..35b6567007a 100644
--- a/flink-staging/flink-streaming/flink-streaming-connectors/pom.xml
+++ b/flink-staging/flink-streaming/flink-streaming-connectors/pom.xml
@@ -52,7 +52,7 @@ under the License.
 		<dependency>
 			<groupId>org.apache.kafka</groupId>
 			<artifactId>kafka_2.10</artifactId>
-			<version>0.8.0</version>
+			<version>0.8.2.0</version>
 			<exclusions>
 				<exclusion>
 					<groupId>com.sun.jmx</groupId>
diff --git a/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/KafkaTopology.java b/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerExample.java
similarity index 55%
rename from flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/KafkaTopology.java
rename to flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerExample.java
index 7801d56d6ac..c06bf36147b 100644
--- a/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/KafkaTopology.java
+++ b/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerExample.java
@@ -19,39 +19,43 @@ package org.apache.flink.streaming.connectors.kafka;
 
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
-import org.apache.flink.streaming.api.function.source.SourceFunction;
-import org.apache.flink.streaming.connectors.util.SimpleStringSchema;
-import org.apache.flink.util.Collector;
+import org.apache.flink.streaming.connectors.kafka.api.KafkaSource;
+import org.apache.flink.streaming.connectors.util.JavaDefaultStringSchema;
 
-public class KafkaTopology {
+public class KafkaConsumerExample {
 
-	public static final class MySource implements SourceFunction<String> {
-		private static final long serialVersionUID = 1L;
+	private static String host;
+	private static int port;
+	private static String topic;
 
-		@Override
-		public void invoke(Collector<String> collector) throws Exception {
-			for (int i = 0; i < 10; i++) {
-				collector.collect(new String(Integer.toString(i)));
-			}
-			collector.collect(new String("q"));
+	public static void main(String[] args) throws Exception {
 
+		if (!parseParameters(args)) {
+			return;
 		}
-	}
 
-	public static void main(String[] args) throws Exception {
-
-		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
+		StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment().setDegreeOfParallelism(4);
 
 		@SuppressWarnings("unused")
 		DataStream<String> stream1 = env
 				.addSource(
-						new KafkaSource<String>("localhost:2181", "group", "test",
-								new SimpleStringSchema())).print();
-
-		@SuppressWarnings("unused")
-		DataStream<String> stream2 = env.addSource(new MySource()).addSink(
-				new KafkaSink<String, String>("test", "localhost:9092", new SimpleStringSchema()));
+						new KafkaSource<String>(host + ":" + port, topic, new JavaDefaultStringSchema()))
+				.setParallelism(3)
+				.print().setParallelism(3);
 
 		env.execute();
 	}
-}
+
+	private static boolean parseParameters(String[] args) {
+		if (args.length == 3) {
+			host = args[0];
+			port = Integer.parseInt(args[1]);
+			topic = args[2];
+			return true;
+		} else {
+			System.err.println("Usage: KafkaConsumerExample <host> <port> <topic>");
+			return false;
+		}
+	}
+
+}
\ No newline at end of file
diff --git a/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/KafkaProducerExample.java b/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/KafkaProducerExample.java
new file mode 100644
index 00000000000..c6fa39fc6bf
--- /dev/null
+++ b/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/KafkaProducerExample.java
@@ -0,0 +1,71 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.kafka;
+
+import org.apache.flink.streaming.api.datastream.DataStream;
+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
+import org.apache.flink.streaming.api.function.source.SourceFunction;
+import org.apache.flink.streaming.connectors.kafka.api.KafkaSink;
+import org.apache.flink.streaming.connectors.util.JavaDefaultStringSchema;
+import org.apache.flink.util.Collector;
+
+public class KafkaProducerExample {
+
+	private static String host;
+	private static int port;
+	private static String topic;
+
+	public static void main(String[] args) throws Exception {
+
+		if (!parseParameters(args)) {
+			return;
+		}
+
+		StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment().setDegreeOfParallelism(4);
+
+		@SuppressWarnings("unused")
+		DataStream<String> stream1 = env.addSource(new SourceFunction<String>() {
+			@Override
+			public void invoke(Collector<String> collector) throws Exception {
+				for (int i = 0; i < 100; i++) {
+					collector.collect("message #" + i);
+					Thread.sleep(100L);
+				}
+
+				collector.collect(new String("q"));
+			}
+		}).addSink(
+				new KafkaSink<String>(topic, host + ":" + port, new JavaDefaultStringSchema())
+		)
+		.setParallelism(3);
+
+		env.execute();
+	}
+
+	private static boolean parseParameters(String[] args) {
+		if (args.length == 3) {
+			host = args[0];
+			port = Integer.parseInt(args[1]);
+			topic = args[2];
+			return true;
+		} else {
+			System.err.println("Usage: KafkaProducerExample <host> <port> <topic>");
+			return false;
+		}
+	}
+}
diff --git a/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/KafkaSink.java b/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/KafkaSink.java
deleted file mode 100644
index 9bb87a06647..00000000000
--- a/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/KafkaSink.java
+++ /dev/null
@@ -1,83 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.streaming.connectors.kafka;
-
-import java.util.Properties;
-
-import kafka.javaapi.producer.Producer;
-import kafka.producer.KeyedMessage;
-import kafka.producer.ProducerConfig;
-
-import org.apache.flink.streaming.api.function.sink.RichSinkFunction;
-import org.apache.flink.streaming.connectors.util.SerializationSchema;
-
-public class KafkaSink<IN, OUT> extends RichSinkFunction<IN> {
-	private static final long serialVersionUID = 1L;
-
-	private kafka.javaapi.producer.Producer<Integer, OUT> producer;
-	private Properties props;
-	private String topicId;
-	private String brokerAddr;
-	private boolean initDone = false;
-	private SerializationSchema<IN, OUT> scheme;
-
-	public KafkaSink(String topicId, String brokerAddr,
-			SerializationSchema<IN, OUT> serializationSchema) {
-		this.topicId = topicId;
-		this.brokerAddr = brokerAddr;
-		this.scheme = serializationSchema;
-
-	}
-
-	/**
-	 * Initializes the connection to Kafka.
-	 */
-	public void initialize() {
-		props = new Properties();
-
-		props.put("metadata.broker.list", brokerAddr);
-		props.put("serializer.class", "kafka.serializer.StringEncoder");
-		props.put("request.required.acks", "1");
-
-		ProducerConfig config = new ProducerConfig(props);
-		producer = new Producer<Integer, OUT>(config);
-		initDone = true;
-	}
-
-	/**
-	 * Called when new data arrives to the sink, and forwards it to Kafka.
-	 * 
-	 * @param next
-	 *            The incoming data
-	 */
-	@Override
-	public void invoke(IN next) {
-		if (!initDone) {
-			initialize();
-		}
-
-		producer.send(new KeyedMessage<Integer, OUT>(topicId, scheme.serialize(next)));
-
-	}
-
-	@Override
-	public void close() {
-		producer.close();
-	}
-
-}
diff --git a/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/api/KafkaSink.java b/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/api/KafkaSink.java
new file mode 100644
index 00000000000..1aca03ee98f
--- /dev/null
+++ b/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/api/KafkaSink.java
@@ -0,0 +1,137 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.kafka.api;
+
+import java.util.Properties;
+
+import org.apache.flink.streaming.api.function.sink.RichSinkFunction;
+import org.apache.flink.streaming.connectors.kafka.config.EncoderWrapper;
+import org.apache.flink.streaming.connectors.kafka.config.PartitionerWrapper;
+import org.apache.flink.streaming.connectors.kafka.partitioner.KafkaDistributePartitioner;
+import org.apache.flink.streaming.connectors.kafka.partitioner.KafkaPartitioner;
+import org.apache.flink.streaming.connectors.util.SerializationSchema;
+
+import kafka.javaapi.producer.Producer;
+import kafka.producer.KeyedMessage;
+import kafka.producer.ProducerConfig;
+import kafka.serializer.DefaultEncoder;
+import kafka.utils.VerifiableProperties;
+
+/**
+ * Sink that emits its inputs to a Kafka topic.
+ *
+ * @param <IN>
+ * 		Type of the sink input
+ */
+public class KafkaSink<IN> extends RichSinkFunction<IN> {
+	private static final long serialVersionUID = 1L;
+
+	private Producer<IN, byte[]> producer;
+	private Properties props;
+	private String topicId;
+	private String brokerAddr;
+	private boolean initDone = false;
+	private SerializationSchema<IN, byte[]> scheme;
+	private KafkaPartitioner<IN> partitioner;
+
+	/**
+	 * Creates a KafkaSink for a given topic. The partitioner distributes the messages between the partitions of the topics.
+	 *
+	 * @param topicId
+	 * 		ID of the Kafka topic.
+	 * @param brokerAddr
+	 * 		Address of the Kafka broker (with port number).
+	 * @param serializationSchema
+	 * 		User defined serialization schema.
+	 */
+	public KafkaSink(String topicId, String brokerAddr,
+			SerializationSchema<IN, byte[]> serializationSchema) {
+		this(topicId, brokerAddr, serializationSchema, new KafkaDistributePartitioner<IN>());
+	}
+
+	/**
+	 * Creates a KafkaSink for a given topic. The sink produces its input into the topic.
+	 *
+	 * @param topicId
+	 * 		ID of the Kafka topic.
+	 * @param brokerAddr
+	 * 		Address of the Kafka broker (with port number).
+	 * @param serializationSchema
+	 * 		User defined serialization schema.
+	 * @param partitioner
+	 * 		User defined partitioner.
+	 */
+	public KafkaSink(String topicId, String brokerAddr,
+			SerializationSchema<IN, byte[]> serializationSchema, KafkaPartitioner<IN> partitioner) {
+		this.topicId = topicId;
+		this.brokerAddr = brokerAddr;
+		this.scheme = serializationSchema;
+		this.partitioner = partitioner;
+	}
+
+	/**
+	 * Initializes the connection to Kafka.
+	 */
+	public void initialize() {
+
+		props = new Properties();
+
+		props.put("metadata.broker.list", brokerAddr);
+		props.put("request.required.acks", "1");
+
+		props.put("serializer.class", DefaultEncoder.class.getCanonicalName());
+		props.put("key.serializer.class", EncoderWrapper.class.getCanonicalName());
+		props.put("partitioner.class", PartitionerWrapper.class.getCanonicalName());
+
+		EncoderWrapper<IN> encoderWrapper = new EncoderWrapper<IN>(scheme);
+		encoderWrapper.write(props);
+
+		PartitionerWrapper<IN> partitionerWrapper = new PartitionerWrapper<IN>(partitioner);
+		partitionerWrapper.write(props);
+
+		ProducerConfig config = new ProducerConfig(props);
+		VerifiableProperties props1 = config.props();
+
+		producer = new Producer<IN, byte[]>(config);
+		initDone = true;
+	}
+
+	/**
+	 * Called when new data arrives to the sink, and forwards it to Kafka.
+	 *
+	 * @param next
+	 * 		The incoming data
+	 */
+	@Override
+	public void invoke(IN next) {
+		if (!initDone) {
+			initialize();
+		}
+
+		byte[] serialized = scheme.serialize(next);
+		producer.send(new KeyedMessage<IN, byte[]>(topicId, next, serialized));
+	}
+
+	@Override
+	public void close() {
+		if (producer != null) {
+			producer.close();
+		}
+	}
+
+}
diff --git a/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/KafkaSource.java b/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/api/KafkaSource.java
similarity index 52%
rename from flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/KafkaSource.java
rename to flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/api/KafkaSource.java
index 7328500536d..38287356f6b 100644
--- a/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/KafkaSource.java
+++ b/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/api/KafkaSource.java
@@ -15,40 +15,73 @@
  * limitations under the License.
  */
 
-package org.apache.flink.streaming.connectors.kafka;
+package org.apache.flink.streaming.connectors.kafka.api;
 
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import java.util.Properties;
 
-import kafka.consumer.ConsumerConfig;
-import kafka.consumer.ConsumerIterator;
-import kafka.consumer.KafkaStream;
-import kafka.javaapi.consumer.ConsumerConnector;
-
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.connectors.ConnectorSource;
 import org.apache.flink.streaming.connectors.util.DeserializationSchema;
 import org.apache.flink.util.Collector;
 
+import kafka.consumer.Consumer;
+import kafka.consumer.ConsumerConfig;
+import kafka.consumer.ConsumerIterator;
+import kafka.consumer.KafkaStream;
+import kafka.javaapi.consumer.ConsumerConnector;
+
+/**
+ * Source that listens to a Kafka topic.
+ *
+ * @param <OUT>
+ * 		Type of the messages on the topic.
+ */
 public class KafkaSource<OUT> extends ConnectorSource<OUT> {
 	private static final long serialVersionUID = 1L;
 
-	private final String zkQuorum;
+	private final String zookeeperHost;
 	private final String groupId;
 	private final String topicId;
-	private ConsumerConnector consumer;
 
-	OUT outTuple;
+	private transient ConsumerConnector consumer;
+	private transient ConsumerIterator<byte[], byte[]> consumerIterator;
 
-	public KafkaSource(String zkQuorum, String groupId, String topicId,
-			DeserializationSchema<OUT> deserializationSchema) {
+	private int partitionIndex;
+	private int numberOfInstances;
+
+	private long zookeeperSyncTimeMillis;
+	private static final long ZOOKEEPER_DEFAULT_SYNC_TIME = 200;
+
+	private OUT outTuple;
+
+	/**
+	 * Creates a KafkaSource that consumes a topic.
+	 *
+	 * @param zookeeperHost
+	 * 		Address of the Zookeeper host (with port number).
+	 * @param topicId
+	 * 		ID of the Kafka topic.
+	 * @param deserializationSchema
+	 * 		User defined deserialization schema.
+	 * @param zookeeperSyncTimeMillis
+	 * 		Synchronization time with zookeeper.
+	 */
+	public KafkaSource(String zookeeperHost, String topicId,
+			DeserializationSchema<OUT> deserializationSchema, long zookeeperSyncTimeMillis) {
 		super(deserializationSchema);
-		this.zkQuorum = zkQuorum;
-		this.groupId = groupId;
+		this.zookeeperHost = zookeeperHost;
+		this.groupId = "flink-group";
 		this.topicId = topicId;
+		this.zookeeperSyncTimeMillis = zookeeperSyncTimeMillis;
+	}
+
+	public KafkaSource(String zookeeperHost, String topicId,
+			DeserializationSchema<OUT> deserializationSchema) {
+		this(zookeeperHost, topicId, deserializationSchema, ZOOKEEPER_DEFAULT_SYNC_TIME);
 	}
 
 	/**
@@ -56,37 +89,41 @@ public class KafkaSource<OUT> extends ConnectorSource<OUT> {
 	 */
 	private void initializeConnection() {
 		Properties props = new Properties();
-		props.put("zookeeper.connect", zkQuorum);
+		props.put("zookeeper.connect", zookeeperHost);
 		props.put("group.id", groupId);
-		props.put("zookeeper.session.timeout.ms", "2000");
-		props.put("zookeeper.sync.time.ms", "200");
+		props.put("zookeeper.session.timeout.ms", "10000");
+		props.put("zookeeper.sync.time.ms", Long.toString(zookeeperSyncTimeMillis));
 		props.put("auto.commit.interval.ms", "1000");
-		consumer = kafka.consumer.Consumer.createJavaConsumerConnector(new ConsumerConfig(props));
+
+		consumer = Consumer.createJavaConsumerConnector(new ConsumerConfig(props));
+		partitionIndex = getRuntimeContext().getIndexOfThisSubtask();
+		numberOfInstances = getRuntimeContext().getNumberOfParallelSubtasks();
+
+		Map<String, List<KafkaStream<byte[], byte[]>>> consumerMap = consumer.createMessageStreams(Collections.singletonMap(topicId, 1));
+		List<KafkaStream<byte[], byte[]>> streams = consumerMap.get(topicId);
+		KafkaStream<byte[], byte[]> stream = streams.get(0);
+
+		consumerIterator = stream.iterator();
 	}
 
 	/**
 	 * Called to forward the data from the source to the {@link DataStream}.
-	 * 
+	 *
 	 * @param collector
-	 *            The Collector for sending data to the dataStream
+	 * 		The Collector for sending data to the dataStream
 	 */
 	@Override
 	public void invoke(Collector<OUT> collector) throws Exception {
 
-		Map<String, List<KafkaStream<byte[], byte[]>>> consumerMap = consumer
-				.createMessageStreams(Collections.singletonMap(topicId, 1));
-
-		KafkaStream<byte[], byte[]> stream = consumerMap.get(topicId).get(0);
-		ConsumerIterator<byte[], byte[]> it = stream.iterator();
-
-		while (it.hasNext()) {
-			OUT out = schema.deserialize(it.next().message());
+		while (consumerIterator.hasNext()) {
+			OUT out = schema.deserialize(consumerIterator.next().message());
 			if (schema.isEndOfStream(out)) {
 				break;
 			}
 			collector.collect(out);
 		}
 		consumer.shutdown();
+
 	}
 
 	@Override
diff --git a/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/config/EncoderWrapper.java b/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/config/EncoderWrapper.java
new file mode 100644
index 00000000000..288c2c14683
--- /dev/null
+++ b/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/config/EncoderWrapper.java
@@ -0,0 +1,46 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.kafka.config;
+
+import org.apache.flink.streaming.connectors.util.SerializationSchema;
+
+import kafka.serializer.Encoder;
+import kafka.utils.VerifiableProperties;
+
+/**
+ * Wraps an arbitrary SerializationScheme to use as a Kafka Encoder.
+ *
+ * @param <T>
+ * 		Type to serialize
+ */
+public class EncoderWrapper<T> extends KafkaConfigWrapper<SerializationSchema<T, byte[]>> implements Encoder<T> {
+
+	public EncoderWrapper(SerializationSchema<T, byte[]> wrapped) {
+		super(wrapped);
+	}
+
+	public EncoderWrapper(VerifiableProperties properties) {
+		super(properties);
+	}
+
+	@Override
+	public byte[] toBytes(T element) {
+		return wrapped.serialize(element);
+	}
+
+}
\ No newline at end of file
diff --git a/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/config/KafkaConfigWrapper.java b/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/config/KafkaConfigWrapper.java
new file mode 100644
index 00000000000..3de97c07295
--- /dev/null
+++ b/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/config/KafkaConfigWrapper.java
@@ -0,0 +1,60 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.kafka.config;
+
+import java.io.Serializable;
+import java.util.Properties;
+
+import kafka.utils.VerifiableProperties;
+
+/**
+ * Wraps an arbitrary Serializable (e.g. a Partitioner) object to use in Kafka, to shade the properties.
+ *
+ * @param <T>
+ * 		Type of the object to wrap
+ */
+public abstract class KafkaConfigWrapper<T extends Serializable> {
+
+	private StringSerializer<T> stringSerializer;
+
+	protected T wrapped;
+
+	public KafkaConfigWrapper(T wrapped) {
+		this();
+		this.wrapped = wrapped;
+	}
+
+	private KafkaConfigWrapper() {
+		stringSerializer = new StringSerializer<T>();
+	}
+
+	public KafkaConfigWrapper(VerifiableProperties properties) {
+		this();
+		read(properties);
+	}
+
+	public void read(VerifiableProperties properties) {
+		String stringWrapped = properties.getString(getClass().getCanonicalName());
+		wrapped = stringSerializer.deserialize(stringWrapped);
+	}
+
+	public void write(Properties properties) {
+		properties.put(getClass().getCanonicalName(), stringSerializer.serialize(wrapped));
+	}
+
+}
\ No newline at end of file
diff --git a/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/config/PartitionerWrapper.java b/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/config/PartitionerWrapper.java
new file mode 100644
index 00000000000..4c84ce1c9b6
--- /dev/null
+++ b/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/config/PartitionerWrapper.java
@@ -0,0 +1,46 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.kafka.config;
+
+import org.apache.flink.streaming.connectors.kafka.partitioner.KafkaPartitioner;
+
+import kafka.producer.Partitioner;
+import kafka.utils.VerifiableProperties;
+
+/**
+ * Wraps an arbitrary partitioner to use as a Kafka partitioner.
+ *
+ * @param <T>
+ * 		Type to partition
+ */
+public class PartitionerWrapper<T> extends KafkaConfigWrapper<KafkaPartitioner<T>> implements Partitioner {
+
+	public PartitionerWrapper(KafkaPartitioner<T> wrapped) {
+		super(wrapped);
+	}
+
+	public PartitionerWrapper(VerifiableProperties properties) {
+		super(properties);
+	}
+
+	@Override
+	public int partition(Object key, int numPartitions) {
+		return wrapped.partition((T) key, numPartitions);
+	}
+
+}
diff --git a/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/config/StringSerializer.java b/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/config/StringSerializer.java
new file mode 100644
index 00000000000..d7ac1986e88
--- /dev/null
+++ b/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/config/StringSerializer.java
@@ -0,0 +1,44 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.kafka.config;
+
+import java.io.Serializable;
+
+import org.apache.commons.codec.binary.Base64;
+
+import org.apache.commons.lang3.SerializationUtils;
+
+/**
+ * Serializer to serializer an arbitrary object to String.
+ *
+ * @param <T>
+ * 		Type to serialize.
+ */
+public class StringSerializer<T extends Serializable> {
+
+	public String serialize(T element) {
+		byte[] serialized = SerializationUtils.serialize(element);
+		return Base64.encodeBase64String(serialized);
+	}
+
+	public T deserialize(String stringSerialized) {
+		byte[] bytes = Base64.decodeBase64(stringSerialized);
+		return SerializationUtils.deserialize(bytes);
+	}
+
+}
diff --git a/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/partitioner/KafkaConstantPartitioner.java b/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/partitioner/KafkaConstantPartitioner.java
new file mode 100644
index 00000000000..f7e9d6bd4c9
--- /dev/null
+++ b/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/partitioner/KafkaConstantPartitioner.java
@@ -0,0 +1,33 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.kafka.partitioner;
+
+public class KafkaConstantPartitioner<T> implements KafkaPartitioner<T> {
+
+	private int partition;
+
+	public KafkaConstantPartitioner(int partition) {
+		this.partition = partition;
+	}
+
+	@Override
+	public int partition(T value, int numberOfPartitions) {
+		return partition;
+	}
+
+}
diff --git a/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/partitioner/KafkaDistributePartitioner.java b/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/partitioner/KafkaDistributePartitioner.java
new file mode 100644
index 00000000000..6f92e4e9124
--- /dev/null
+++ b/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/partitioner/KafkaDistributePartitioner.java
@@ -0,0 +1,40 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.kafka.partitioner;
+
+/**
+ * Kafka partitioner that distributes the data equally by cycling through the output
+ * channels.
+ *
+ * @param <T>
+ *     Type to partition.
+ */
+public class KafkaDistributePartitioner<T> implements KafkaPartitioner<T> {
+
+	int currentPartition;
+
+	public KafkaDistributePartitioner() {
+		currentPartition = 0;
+	}
+
+	@Override
+	public int partition(T value, int numberOfPartitions) {
+		return currentPartition++ % numberOfPartitions;
+	}
+
+}
\ No newline at end of file
diff --git a/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/partitioner/KafkaPartitioner.java b/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/partitioner/KafkaPartitioner.java
new file mode 100644
index 00000000000..f82ad7c1698
--- /dev/null
+++ b/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/kafka/partitioner/KafkaPartitioner.java
@@ -0,0 +1,26 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.kafka.partitioner;
+
+import java.io.Serializable;
+
+public interface KafkaPartitioner<T> extends Serializable {
+
+	public int partition(T value, int numberOfPartitions);
+
+}
diff --git a/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/util/JavaDefaultStringSchema.java b/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/util/JavaDefaultStringSchema.java
new file mode 100644
index 00000000000..569d3e6a748
--- /dev/null
+++ b/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/util/JavaDefaultStringSchema.java
@@ -0,0 +1,41 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.util;
+
+import org.apache.commons.lang3.SerializationUtils;
+
+public class JavaDefaultStringSchema implements DeserializationSchema<String>, SerializationSchema<String, byte[]> {
+
+	private static final long serialVersionUID = 1L;
+
+	@Override
+	public boolean isEndOfStream(String nextElement) {
+		return nextElement.equals("q");
+	}
+
+	@Override
+	public byte[] serialize(String element) {
+		return SerializationUtils.serialize(element);
+	}
+
+	@Override
+	public String deserialize(byte[] message) {
+		return SerializationUtils.deserialize(message);
+	}
+
+}
\ No newline at end of file
diff --git a/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/util/SerializationSchema.java b/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/util/SerializationSchema.java
index 7c32312e612..f8d2b2f725b 100644
--- a/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/util/SerializationSchema.java
+++ b/flink-staging/flink-streaming/flink-streaming-connectors/src/main/java/org/apache/flink/streaming/connectors/util/SerializationSchema.java
@@ -19,7 +19,7 @@ package org.apache.flink.streaming.connectors.util;
 
 import java.io.Serializable;
 
-public interface SerializationSchema<T,R> extends Serializable {
+public interface SerializationSchema<T, R> extends Serializable {
 
 	/**
 	 * Serializes the incoming element to a specified type.
diff --git a/flink-staging/flink-streaming/flink-streaming-connectors/src/test/java/org/apache/flink/streaming/connectors/kafka/StringSerializerTest.java b/flink-staging/flink-streaming/flink-streaming-connectors/src/test/java/org/apache/flink/streaming/connectors/kafka/StringSerializerTest.java
new file mode 100644
index 00000000000..b0b31cb89c5
--- /dev/null
+++ b/flink-staging/flink-streaming/flink-streaming-connectors/src/test/java/org/apache/flink/streaming/connectors/kafka/StringSerializerTest.java
@@ -0,0 +1,63 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.kafka;
+
+import static org.junit.Assert.assertEquals;
+
+import java.io.Serializable;
+
+import org.apache.flink.streaming.connectors.kafka.config.StringSerializer;
+import org.junit.Test;
+
+public class StringSerializerTest {
+
+	private static class MyClass implements Serializable {
+		private int a;
+		private String b;
+
+		public MyClass(int a, String b) {
+			this.a = a;
+			this.b = b;
+		}
+
+		@Override
+		public boolean equals(Object o) {
+			try {
+				MyClass other = (MyClass) o;
+				return a == other.a && b.equals(other.b);
+			} catch (ClassCastException e) {
+				return false;
+			}
+		}
+	}
+
+	@Test
+	public void test() {
+
+		MyClass myObject = new MyClass(42, "test string");
+
+		StringSerializer<MyClass> stringSerializer = new StringSerializer<MyClass>();
+
+		String store = stringSerializer.serialize(myObject);
+
+		MyClass myObjectDeserialized = stringSerializer.deserialize(store);
+
+		assertEquals(myObject, myObjectDeserialized);
+	}
+
+}
