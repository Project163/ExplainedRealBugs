diff --git a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/KafkaSourceReader.java b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/KafkaSourceReader.java
index 9fd3a70ca67..8ca03370d25 100644
--- a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/KafkaSourceReader.java
+++ b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/KafkaSourceReader.java
@@ -75,9 +75,11 @@ public class KafkaSourceReader<T>
     protected void onSplitFinished(Map<String, KafkaPartitionSplitState> finishedSplitIds) {
         finishedSplitIds.forEach(
                 (ignored, splitState) -> {
-                    offsetsOfFinishedSplits.put(
-                            splitState.getTopicPartition(),
-                            new OffsetAndMetadata(splitState.getCurrentOffset()));
+                    if (splitState.getCurrentOffset() >= 0) {
+                        offsetsOfFinishedSplits.put(
+                                splitState.getTopicPartition(),
+                                new OffsetAndMetadata(splitState.getCurrentOffset()));
+                    }
                 });
     }
 
@@ -91,9 +93,13 @@ public class KafkaSourceReader<T>
                     offsetsToCommit.computeIfAbsent(checkpointId, id -> new HashMap<>());
             // Put the offsets of the active splits.
             for (KafkaPartitionSplit split : splits) {
-                offsetsMap.put(
-                        split.getTopicPartition(),
-                        new OffsetAndMetadata(split.getStartingOffset(), null));
+                // If the checkpoint is triggered before the partition starting offsets
+                // is retrieved, do not commit the offsets for those partitions.
+                if (split.getStartingOffset() >= 0) {
+                    offsetsMap.put(
+                            split.getTopicPartition(),
+                            new OffsetAndMetadata(split.getStartingOffset()));
+                }
             }
             // Put offsets of all the finished splits.
             offsetsMap.putAll(offsetsOfFinishedSplits);
diff --git a/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/source/reader/KafkaSourceReaderTest.java b/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/source/reader/KafkaSourceReaderTest.java
index 6366d40de76..579bab3fe4b 100644
--- a/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/source/reader/KafkaSourceReaderTest.java
+++ b/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/source/reader/KafkaSourceReaderTest.java
@@ -207,6 +207,20 @@ public class KafkaSourceReaderTest extends SourceReaderTestBase<KafkaPartitionSp
         }
     }
 
+    @Test
+    public void testNotCommitOffsetsForUninitializedSplits() throws Exception {
+        final long checkpointId = 1234L;
+        try (KafkaSourceReader<Integer> reader = (KafkaSourceReader<Integer>) createReader()) {
+            KafkaPartitionSplit split =
+                    new KafkaPartitionSplit(
+                            new TopicPartition(TOPIC, 0), KafkaPartitionSplit.EARLIEST_OFFSET);
+            reader.addSplits(Collections.singletonList(split));
+            reader.snapshotState(checkpointId);
+            assertEquals(1, reader.getOffsetsToCommit().size());
+            assertTrue(reader.getOffsetsToCommit().get(checkpointId).isEmpty());
+        }
+    }
+
     // ------------------------------------------
 
     @Override
