diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/sources/TableSourceUtil.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/sources/TableSourceUtil.scala
index 89ef9804b0c..245a4b3769d 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/sources/TableSourceUtil.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/sources/TableSourceUtil.scala
@@ -149,19 +149,30 @@ object TableSourceUtil {
 
     // get the corresponding logical type according to the layout of source data type
     val sourceLogicalType = fromDataTypeToLogicalType(tableSource.getProducedDataType)
-    def mapping(physicalName: String): String = tableSource match {
+    def mapping(physicalName: String): Option[String] = tableSource match {
       case ts: DefinedFieldMapping if ts.getFieldMapping != null =>
         // revert key and value, mapping from physical field to logical field
         val map = ts.getFieldMapping.toMap.map(_.swap)
-        map(physicalName)
+        map.get(physicalName)
       case _ =>
-        physicalName
+        Some(physicalName)
     }
+
     val correspondingLogicalType = sourceLogicalType match {
       case outType: RowType =>
-        val fieldsDataType = schemaWithoutProctime.getFields.map(f => (f.getName, f.getType)).toMap
-        val fields = outType.getFieldNames.map(n =>
-          new RowField(n, fieldsDataType(mapping(n)))).asJava
+        val logicalNamesToTypes = schemaWithoutProctime
+          .getFields
+          .map(f => (f.getName, f.getType))
+          .toMap
+        val fields = outType.getFields.map(f => {
+          val t = mapping(f.getName) match {
+            case Some(n) if logicalNamesToTypes.contains(n) =>
+              logicalNamesToTypes(n) // find corresponding logical type
+            case _ =>
+              f.getType // use physical type if logical type can't find
+          }
+          new RowField(f.getName, t)
+        }).asJava
         new RowType(schemaWithoutProctime.isNullable, fields)
 
       case _ =>
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/TableScanITCase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/TableScanITCase.scala
index 13959ff26a9..b1521019320 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/TableScanITCase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/TableScanITCase.scala
@@ -96,6 +96,48 @@ class TableScanITCase extends StreamingTestBase {
     assertEquals(expected.sorted, sink.getAppendResults.sorted)
   }
 
+  @Test
+  def testRowtimeTableSourceWithFieldReMapping(): Unit = {
+    val tableName = "MyTable"
+
+    val data: Seq[Row] = Seq(
+      Row.of(Int.box(1), Long.box(11), "Mary"),
+      Row.of(Int.box(2), Long.box(12), "Peter"),
+      Row.of(Int.box(3), Long.box(13), "Bob"),
+      Row.of(Int.box(4), Long.box(14), "Liz"))
+
+    val schema = new TableSchema(
+      Array("key", "rowtime", "payload"),
+      Array(Types.INT(), Types.SQL_TIMESTAMP(), Types.STRING()))
+    val returnType = Types.ROW(Types.INT(), Types.LONG(), Types.STRING())
+    val mapping = Map("key" -> "f0", "ts" -> "f1", "payload" -> "f2")
+    val tableSource = new TestTableSourceWithTime(
+      false,
+      schema,
+      returnType,
+      data,
+      rowtime = "rowtime",
+      mapping = mapping,
+      existingTs = "ts")
+    tEnv.registerTableSource(tableName, tableSource)
+
+    val sqlQuery =
+      s"""
+         |SELECT
+         |  CAST(TUMBLE_START(rowtime, INTERVAL '0.005' SECOND) AS VARCHAR),
+         |  COUNT(payload)
+         |FROM $tableName
+         |GROUP BY TUMBLE(rowtime, INTERVAL '0.005' SECOND)
+       """.stripMargin
+    val result = tEnv.sqlQuery(sqlQuery).toAppendStream[Row]
+    val sink = new TestingAppendSink
+    result.addSink(sink)
+    env.execute()
+
+    val expected = Seq("1970-01-01 00:00:00.01,4")
+    assertEquals(expected.sorted, sink.getAppendResults.sorted)
+  }
+
   @Test
   def testRowtimeTableSourcePreserveWatermarks(): Unit = {
     val tableName = "MyTable"
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/utils/testTableSources.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/utils/testTableSources.scala
index d57cfa5b5d8..19870a82414 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/utils/testTableSources.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/utils/testTableSources.scala
@@ -150,7 +150,8 @@ class TestTableSourceWithTime[T](
     values: Seq[T],
     rowtime: String = null,
     proctime: String = null,
-    mapping: Map[String, String] = null)
+    mapping: Map[String, String] = null,
+    existingTs: String = null)
   extends StreamTableSource[T]
   with DefinedRowtimeAttributes
   with DefinedProctimeAttribute
@@ -165,9 +166,14 @@ class TestTableSourceWithTime[T](
   override def getRowtimeAttributeDescriptors: JList[RowtimeAttributeDescriptor] = {
     // return a RowtimeAttributeDescriptor if rowtime attribute is defined
     if (rowtime != null) {
+      val existingField = if (existingTs != null) {
+        existingTs
+      } else {
+        rowtime
+      }
       Collections.singletonList(new RowtimeAttributeDescriptor(
         rowtime,
-        new ExistingField(rowtime),
+        new ExistingField(existingField),
         new AscendingTimestamps))
     } else {
       Collections.EMPTY_LIST.asInstanceOf[JList[RowtimeAttributeDescriptor]]
