diff --git a/flink-runtime-web/src/main/java/org/apache/flink/runtime/webmonitor/history/HistoryServer.java b/flink-runtime-web/src/main/java/org/apache/flink/runtime/webmonitor/history/HistoryServer.java
index 384dda02430..25f4125c7bd 100644
--- a/flink-runtime-web/src/main/java/org/apache/flink/runtime/webmonitor/history/HistoryServer.java
+++ b/flink-runtime-web/src/main/java/org/apache/flink/runtime/webmonitor/history/HistoryServer.java
@@ -35,12 +35,16 @@ import org.apache.flink.runtime.rest.messages.DashboardConfiguration;
 import org.apache.flink.runtime.security.SecurityConfiguration;
 import org.apache.flink.runtime.security.SecurityUtils;
 import org.apache.flink.runtime.util.EnvironmentInformation;
+import org.apache.flink.runtime.util.Runnables;
 import org.apache.flink.runtime.webmonitor.utils.WebFrontendBootstrap;
 import org.apache.flink.util.ExceptionUtils;
+import org.apache.flink.util.ExecutorUtils;
+import org.apache.flink.util.FatalExitExceptionHandler;
 import org.apache.flink.util.FileUtils;
 import org.apache.flink.util.FlinkException;
 import org.apache.flink.util.Preconditions;
 import org.apache.flink.util.ShutdownHookUtil;
+import org.apache.flink.util.concurrent.ExecutorThreadFactory;
 
 import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectMapper;
 
@@ -60,6 +64,9 @@ import java.util.List;
 import java.util.UUID;
 import java.util.concurrent.Callable;
 import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.Executors;
+import java.util.concurrent.ScheduledExecutorService;
+import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.function.Consumer;
 
@@ -102,6 +109,11 @@ public class HistoryServer {
     @Nullable private final SSLHandlerFactory serverSSLFactory;
     private WebFrontendBootstrap netty;
 
+    private final long refreshIntervalMillis;
+    private final ScheduledExecutorService executor =
+            Executors.newSingleThreadScheduledExecutor(
+                    new ExecutorThreadFactory("Flink-HistoryServer-ArchiveFetcher"));
+
     private final Object startupShutdownLock = new Object();
     private final AtomicBoolean shutdownRequested = new AtomicBoolean(false);
     private final Thread shutdownHook;
@@ -219,7 +231,7 @@ public class HistoryServer {
                     "Failed to validate any of the configured directories to monitor.");
         }
 
-        long refreshIntervalMillis =
+        refreshIntervalMillis =
                 config.getLong(HistoryServerOptions.HISTORY_SERVER_ARCHIVE_REFRESH_INTERVAL);
         int maxHistorySize = config.getInteger(HistoryServerOptions.HISTORY_SERVER_RETAINED_JOBS);
         if (maxHistorySize == 0 || maxHistorySize < -1) {
@@ -229,7 +241,6 @@ public class HistoryServer {
         }
         archiveFetcher =
                 new HistoryServerArchiveFetcher(
-                        refreshIntervalMillis,
                         refreshDirs,
                         webDir,
                         jobArchiveEventListener,
@@ -273,7 +284,8 @@ public class HistoryServer {
 
             createDashboardConfigFile();
 
-            archiveFetcher.start();
+            executor.scheduleWithFixedDelay(
+                    getArchiveFetchingRunnable(), 0, refreshIntervalMillis, TimeUnit.MILLISECONDS);
 
             netty =
                     new WebFrontendBootstrap(
@@ -281,6 +293,11 @@ public class HistoryServer {
         }
     }
 
+    private Runnable getArchiveFetchingRunnable() {
+        return Runnables.withUncaughtExceptionHandler(
+                () -> archiveFetcher.fetchArchives(), FatalExitExceptionHandler.INSTANCE);
+    }
+
     void stop() {
         if (shutdownRequested.compareAndSet(false, true)) {
             synchronized (startupShutdownLock) {
@@ -292,7 +309,7 @@ public class HistoryServer {
                     LOG.warn("Error while shutting down WebFrontendBootstrap.", t);
                 }
 
-                archiveFetcher.stop();
+                ExecutorUtils.gracefulShutdown(1, TimeUnit.SECONDS, executor);
 
                 try {
                     LOG.info("Removing web dashboard root cache directory {}", webDir);
diff --git a/flink-runtime-web/src/main/java/org/apache/flink/runtime/webmonitor/history/HistoryServerArchiveFetcher.java b/flink-runtime-web/src/main/java/org/apache/flink/runtime/webmonitor/history/HistoryServerArchiveFetcher.java
index a547affe7a6..eb5a34b2c2d 100644
--- a/flink-runtime-web/src/main/java/org/apache/flink/runtime/webmonitor/history/HistoryServerArchiveFetcher.java
+++ b/flink-runtime-web/src/main/java/org/apache/flink/runtime/webmonitor/history/HistoryServerArchiveFetcher.java
@@ -29,10 +29,7 @@ import org.apache.flink.runtime.history.FsJobArchivist;
 import org.apache.flink.runtime.messages.webmonitor.JobDetails;
 import org.apache.flink.runtime.messages.webmonitor.MultipleJobsDetails;
 import org.apache.flink.runtime.rest.messages.JobsOverviewHeaders;
-import org.apache.flink.runtime.util.Runnables;
-import org.apache.flink.util.FatalExitExceptionHandler;
 import org.apache.flink.util.FileUtils;
-import org.apache.flink.util.concurrent.ExecutorThreadFactory;
 
 import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.JsonFactory;
 import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.JsonGenerator;
@@ -58,10 +55,6 @@ import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
-import java.util.TimerTask;
-import java.util.concurrent.Executors;
-import java.util.concurrent.ScheduledExecutorService;
-import java.util.concurrent.TimeUnit;
 import java.util.function.Consumer;
 
 import static org.apache.flink.util.Preconditions.checkNotNull;
@@ -72,6 +65,10 @@ import static org.apache.flink.util.Preconditions.checkNotNull;
  * intervals, defined by {@link HistoryServerOptions#HISTORY_SERVER_ARCHIVE_REFRESH_INTERVAL}.
  *
  * <p>The archives are downloaded and expanded into a file structure analog to the REST API.
+ *
+ * <p>Removes existing archives from these directories and the cache if configured by {@link
+ * HistoryServerOptions#HISTORY_SERVER_CLEANUP_EXPIRED_JOBS} or {@link
+ * HistoryServerOptions#HISTORY_SERVER_RETAINED_JOBS}.
  */
 class HistoryServerArchiveFetcher {
 
@@ -109,28 +106,44 @@ class HistoryServerArchiveFetcher {
     private static final JsonFactory jacksonFactory = new JsonFactory();
     private static final ObjectMapper mapper = new ObjectMapper();
 
-    private final ScheduledExecutorService executor =
-            Executors.newSingleThreadScheduledExecutor(
-                    new ExecutorThreadFactory("Flink-HistoryServer-ArchiveFetcher"));
-    private final JobArchiveFetcherTask fetcherTask;
-    private final long refreshIntervalMillis;
+    private static final String JSON_FILE_ENDING = ".json";
+
+    private final List<HistoryServer.RefreshLocation> refreshDirs;
+    private final Consumer<ArchiveEvent> jobArchiveEventListener;
+    private final boolean processExpiredArchiveDeletion;
+    private final boolean processBeyondLimitArchiveDeletion;
+    private final int maxHistorySize;
+
+    /** Cache of all available jobs identified by their id. */
+    private final Map<Path, Set<String>> cachedArchivesPerRefreshDirectory;
+
+    private final File webDir;
+    private final File webJobDir;
+    private final File webOverviewDir;
 
     HistoryServerArchiveFetcher(
-            long refreshIntervalMillis,
             List<HistoryServer.RefreshLocation> refreshDirs,
             File webDir,
             Consumer<ArchiveEvent> jobArchiveEventListener,
             boolean cleanupExpiredArchives,
             int maxHistorySize)
             throws IOException {
-        this.refreshIntervalMillis = refreshIntervalMillis;
-        this.fetcherTask =
-                new JobArchiveFetcherTask(
-                        refreshDirs,
-                        webDir,
-                        jobArchiveEventListener,
-                        cleanupExpiredArchives,
-                        maxHistorySize);
+        this.refreshDirs = checkNotNull(refreshDirs);
+        this.jobArchiveEventListener = jobArchiveEventListener;
+        this.processExpiredArchiveDeletion = cleanupExpiredArchives;
+        this.maxHistorySize = maxHistorySize;
+        this.processBeyondLimitArchiveDeletion = this.maxHistorySize > 0;
+        this.cachedArchivesPerRefreshDirectory = new HashMap<>();
+        for (HistoryServer.RefreshLocation refreshDir : refreshDirs) {
+            cachedArchivesPerRefreshDirectory.put(refreshDir.getPath(), new HashSet<>());
+        }
+        this.webDir = checkNotNull(webDir);
+        this.webJobDir = new File(webDir, "jobs");
+        Files.createDirectories(webJobDir.toPath());
+        this.webOverviewDir = new File(webDir, "overviews");
+        Files.createDirectories(webOverviewDir.toPath());
+        updateJobOverview(webOverviewDir, webDir);
+
         if (LOG.isInfoEnabled()) {
             for (HistoryServer.RefreshLocation refreshDir : refreshDirs) {
                 LOG.info("Monitoring directory {} for archived jobs.", refreshDir.getPath());
@@ -138,296 +151,218 @@ class HistoryServerArchiveFetcher {
         }
     }
 
-    void start() {
-        final Runnable guardedTask =
-                Runnables.withUncaughtExceptionHandler(
-                        fetcherTask, FatalExitExceptionHandler.INSTANCE);
-        executor.scheduleWithFixedDelay(
-                guardedTask, 0, refreshIntervalMillis, TimeUnit.MILLISECONDS);
-    }
-
-    void stop() {
-        executor.shutdown();
-
+    void fetchArchives() {
         try {
-            if (!executor.awaitTermination(1, TimeUnit.SECONDS)) {
-                executor.shutdownNow();
-            }
-        } catch (InterruptedException ignored) {
-            executor.shutdownNow();
-        }
-    }
-
-    /**
-     * {@link TimerTask} that polls the directories configured as {@link
-     * HistoryServerOptions#HISTORY_SERVER_ARCHIVE_DIRS} for new job archives. Removes existing
-     * archives from these directories and the cache if configured by {@link
-     * HistoryServerOptions#HISTORY_SERVER_CLEANUP_EXPIRED_JOBS} or {@link
-     * HistoryServerOptions#HISTORY_SERVER_RETAINED_JOBS}
-     */
-    static class JobArchiveFetcherTask extends TimerTask {
-
-        private final List<HistoryServer.RefreshLocation> refreshDirs;
-        private final Consumer<ArchiveEvent> jobArchiveEventListener;
-        private final boolean processExpiredArchiveDeletion;
-        private final boolean processBeyondLimitArchiveDeletion;
-        private final int maxHistorySize;
-
-        /** Cache of all available jobs identified by their id. */
-        private final Map<Path, Set<String>> cachedArchivesPerRefreshDirectory;
-
-        private final File webDir;
-        private final File webJobDir;
-        private final File webOverviewDir;
-
-        private static final String JSON_FILE_ENDING = ".json";
-
-        JobArchiveFetcherTask(
-                List<HistoryServer.RefreshLocation> refreshDirs,
-                File webDir,
-                Consumer<ArchiveEvent> jobArchiveEventListener,
-                boolean processExpiredArchiveDeletion,
-                int maxHistorySize)
-                throws IOException {
-            this.refreshDirs = checkNotNull(refreshDirs);
-            this.jobArchiveEventListener = jobArchiveEventListener;
-            this.processExpiredArchiveDeletion = processExpiredArchiveDeletion;
-            this.maxHistorySize = maxHistorySize;
-            this.processBeyondLimitArchiveDeletion = this.maxHistorySize > 0;
-            this.cachedArchivesPerRefreshDirectory = new HashMap<>();
-            for (HistoryServer.RefreshLocation refreshDir : refreshDirs) {
-                cachedArchivesPerRefreshDirectory.put(refreshDir.getPath(), new HashSet<>());
-            }
-            this.webDir = checkNotNull(webDir);
-            this.webJobDir = new File(webDir, "jobs");
-            Files.createDirectories(webJobDir.toPath());
-            this.webOverviewDir = new File(webDir, "overviews");
-            Files.createDirectories(webOverviewDir.toPath());
-            updateJobOverview(webOverviewDir, webDir);
-        }
+            LOG.debug("Starting archive fetching.");
+            List<ArchiveEvent> events = new ArrayList<>();
+            Map<Path, Set<String>> jobsToRemove = new HashMap<>();
+            cachedArchivesPerRefreshDirectory.forEach(
+                    (path, archives) -> jobsToRemove.put(path, new HashSet<>(archives)));
+            Map<Path, Set<Path>> archivesBeyondSizeLimit = new HashMap<>();
+            for (HistoryServer.RefreshLocation refreshLocation : refreshDirs) {
+                Path refreshDir = refreshLocation.getPath();
+                LOG.debug("Checking archive directory {}.", refreshDir);
+
+                // contents of /:refreshDir
+                FileStatus[] jobArchives;
+                try {
+                    jobArchives = listArchives(refreshLocation.getFs(), refreshDir);
+                } catch (IOException e) {
+                    LOG.error("Failed to access job archive location for path {}.", refreshDir, e);
+                    // something went wrong, potentially due to a concurrent deletion
+                    // do not remove any jobs now; we will retry later
+                    jobsToRemove.remove(refreshDir);
+                    continue;
+                }
 
-        @Override
-        public void run() {
-            try {
-                LOG.debug("Starting archive fetching.");
-                List<ArchiveEvent> events = new ArrayList<>();
-                Map<Path, Set<String>> jobsToRemove = new HashMap<>();
-                cachedArchivesPerRefreshDirectory.forEach(
-                        (path, archives) -> jobsToRemove.put(path, new HashSet<>(archives)));
-                Map<Path, Set<Path>> archivesBeyondSizeLimit = new HashMap<>();
-                for (HistoryServer.RefreshLocation refreshLocation : refreshDirs) {
-                    Path refreshDir = refreshLocation.getPath();
-                    LOG.debug("Checking archive directory {}.", refreshDir);
-
-                    // contents of /:refreshDir
-                    FileStatus[] jobArchives;
-                    try {
-                        jobArchives = listArchives(refreshLocation.getFs(), refreshDir);
-                    } catch (IOException e) {
-                        LOG.error(
-                                "Failed to access job archive location for path {}.",
-                                refreshDir,
-                                e);
-                        // something went wrong, potentially due to a concurrent deletion
-                        // do not remove any jobs now; we will retry later
-                        jobsToRemove.remove(refreshDir);
+                int historySize = 0;
+                for (FileStatus jobArchive : jobArchives) {
+                    Path jobArchivePath = jobArchive.getPath();
+                    String jobID = jobArchivePath.getName();
+                    if (!isValidJobID(jobID, refreshDir)) {
                         continue;
                     }
 
-                    int historySize = 0;
-                    for (FileStatus jobArchive : jobArchives) {
-                        Path jobArchivePath = jobArchive.getPath();
-                        String jobID = jobArchivePath.getName();
-                        if (!isValidJobID(jobID, refreshDir)) {
-                            continue;
-                        }
+                    jobsToRemove.get(refreshDir).remove(jobID);
 
-                        jobsToRemove.get(refreshDir).remove(jobID);
-
-                        historySize++;
-                        if (historySize > maxHistorySize && processBeyondLimitArchiveDeletion) {
-                            archivesBeyondSizeLimit
-                                    .computeIfAbsent(refreshDir, ignored -> new HashSet<>())
-                                    .add(jobArchivePath);
-                            continue;
-                        }
+                    historySize++;
+                    if (historySize > maxHistorySize && processBeyondLimitArchiveDeletion) {
+                        archivesBeyondSizeLimit
+                                .computeIfAbsent(refreshDir, ignored -> new HashSet<>())
+                                .add(jobArchivePath);
+                        continue;
+                    }
 
-                        if (cachedArchivesPerRefreshDirectory.get(refreshDir).contains(jobID)) {
-                            LOG.trace(
-                                    "Ignoring archive {} because it was already fetched.",
-                                    jobArchivePath);
-                        } else {
-                            LOG.info("Processing archive {}.", jobArchivePath);
-                            try {
-                                processArchive(jobID, jobArchivePath);
-                                events.add(new ArchiveEvent(jobID, ArchiveEventType.CREATED));
-                                cachedArchivesPerRefreshDirectory.get(refreshDir).add(jobID);
-                                LOG.info("Processing archive {} finished.", jobArchivePath);
-                            } catch (IOException e) {
-                                LOG.error(
-                                        "Failure while fetching/processing job archive for job {}.",
-                                        jobID,
-                                        e);
-                                deleteJobFiles(jobID);
-                            }
+                    if (cachedArchivesPerRefreshDirectory.get(refreshDir).contains(jobID)) {
+                        LOG.trace(
+                                "Ignoring archive {} because it was already fetched.",
+                                jobArchivePath);
+                    } else {
+                        LOG.info("Processing archive {}.", jobArchivePath);
+                        try {
+                            processArchive(jobID, jobArchivePath);
+                            events.add(new ArchiveEvent(jobID, ArchiveEventType.CREATED));
+                            cachedArchivesPerRefreshDirectory.get(refreshDir).add(jobID);
+                            LOG.info("Processing archive {} finished.", jobArchivePath);
+                        } catch (IOException e) {
+                            LOG.error(
+                                    "Failure while fetching/processing job archive for job {}.",
+                                    jobID,
+                                    e);
+                            deleteJobFiles(jobID);
                         }
                     }
                 }
+            }
 
-                if (jobsToRemove.values().stream().flatMap(Set::stream).findAny().isPresent()
-                        && processExpiredArchiveDeletion) {
-                    events.addAll(cleanupExpiredJobs(jobsToRemove));
-                }
-                if (!archivesBeyondSizeLimit.isEmpty() && processBeyondLimitArchiveDeletion) {
-                    events.addAll(cleanupJobsBeyondSizeLimit(archivesBeyondSizeLimit));
-                }
-                if (!events.isEmpty()) {
-                    updateJobOverview(webOverviewDir, webDir);
-                }
-                events.forEach(jobArchiveEventListener::accept);
-                LOG.debug("Finished archive fetching.");
-            } catch (Exception e) {
-                LOG.error("Critical failure while fetching/processing job archives.", e);
+            if (jobsToRemove.values().stream().flatMap(Set::stream).findAny().isPresent()
+                    && processExpiredArchiveDeletion) {
+                events.addAll(cleanupExpiredJobs(jobsToRemove));
+            }
+            if (!archivesBeyondSizeLimit.isEmpty() && processBeyondLimitArchiveDeletion) {
+                events.addAll(cleanupJobsBeyondSizeLimit(archivesBeyondSizeLimit));
             }
+            if (!events.isEmpty()) {
+                updateJobOverview(webOverviewDir, webDir);
+            }
+            events.forEach(jobArchiveEventListener::accept);
+            LOG.debug("Finished archive fetching.");
+        } catch (Exception e) {
+            LOG.error("Critical failure while fetching/processing job archives.", e);
         }
+    }
 
-        private static FileStatus[] listArchives(FileSystem refreshFS, Path refreshDir)
-                throws IOException {
-            // contents of /:refreshDir
-            FileStatus[] jobArchives = refreshFS.listStatus(refreshDir);
-            if (jobArchives == null) {
-                // the entire refreshDirectory was removed
-                return new FileStatus[0];
-            }
+    private static FileStatus[] listArchives(FileSystem refreshFS, Path refreshDir)
+            throws IOException {
+        // contents of /:refreshDir
+        FileStatus[] jobArchives = refreshFS.listStatus(refreshDir);
+        if (jobArchives == null) {
+            // the entire refreshDirectory was removed
+            return new FileStatus[0];
+        }
 
-            Arrays.sort(
-                    jobArchives,
-                    Comparator.comparingLong(FileStatus::getModificationTime).reversed());
+        Arrays.sort(
+                jobArchives, Comparator.comparingLong(FileStatus::getModificationTime).reversed());
 
-            return jobArchives;
+        return jobArchives;
+    }
+
+    private static boolean isValidJobID(String jobId, Path refreshDir) {
+        try {
+            JobID.fromHexString(jobId);
+            return true;
+        } catch (IllegalArgumentException iae) {
+            LOG.debug(
+                    "Archive directory {} contained file with unexpected name {}. Ignoring file.",
+                    refreshDir,
+                    jobId,
+                    iae);
+            return false;
         }
+    }
+
+    private void processArchive(String jobID, Path jobArchive) throws IOException {
+        for (ArchivedJson archive : FsJobArchivist.getArchivedJsons(jobArchive)) {
+            String path = archive.getPath();
+            String json = archive.getJson();
+
+            File target;
+            if (path.equals(JobsOverviewHeaders.URL)) {
+                target = new File(webOverviewDir, jobID + JSON_FILE_ENDING);
+            } else if (path.equals("/joboverview")) { // legacy path
+                LOG.debug("Migrating legacy archive {}", jobArchive);
+                json = convertLegacyJobOverview(json);
+                target = new File(webOverviewDir, jobID + JSON_FILE_ENDING);
+            } else {
+                // this implicitly writes into webJobDir
+                target = new File(webDir, path + JSON_FILE_ENDING);
+            }
+
+            java.nio.file.Path parent = target.getParentFile().toPath();
 
-        private static boolean isValidJobID(String jobId, Path refreshDir) {
             try {
-                JobID.fromHexString(jobId);
-                return true;
-            } catch (IllegalArgumentException iae) {
-                LOG.debug(
-                        "Archive directory {} contained file with unexpected name {}. Ignoring file.",
-                        refreshDir,
-                        jobId,
-                        iae);
-                return false;
+                Files.createDirectories(parent);
+            } catch (FileAlreadyExistsException ignored) {
+                // there may be left-over directories from the previous
+                // attempt
             }
-        }
 
-        private void processArchive(String jobID, Path jobArchive) throws IOException {
-            for (ArchivedJson archive : FsJobArchivist.getArchivedJsons(jobArchive)) {
-                String path = archive.getPath();
-                String json = archive.getJson();
-
-                File target;
-                if (path.equals(JobsOverviewHeaders.URL)) {
-                    target = new File(webOverviewDir, jobID + JSON_FILE_ENDING);
-                } else if (path.equals("/joboverview")) { // legacy path
-                    LOG.debug("Migrating legacy archive {}", jobArchive);
-                    json = convertLegacyJobOverview(json);
-                    target = new File(webOverviewDir, jobID + JSON_FILE_ENDING);
-                } else {
-                    // this implicitly writes into webJobDir
-                    target = new File(webDir, path + JSON_FILE_ENDING);
-                }
+            java.nio.file.Path targetPath = target.toPath();
 
-                java.nio.file.Path parent = target.getParentFile().toPath();
+            // We overwrite existing files since this may be another attempt
+            // at fetching this archive.
+            // Existing files may be incomplete/corrupt.
+            Files.deleteIfExists(targetPath);
 
-                try {
-                    Files.createDirectories(parent);
-                } catch (FileAlreadyExistsException ignored) {
-                    // there may be left-over directories from the previous
-                    // attempt
-                }
+            Files.createFile(target.toPath());
+            try (FileWriter fw = new FileWriter(target)) {
+                fw.write(json);
+                fw.flush();
+            }
+        }
+    }
 
-                java.nio.file.Path targetPath = target.toPath();
+    private List<ArchiveEvent> cleanupJobsBeyondSizeLimit(
+            Map<Path, Set<Path>> jobArchivesToRemove) {
+        Map<Path, Set<String>> allJobIdsToRemoveFromOverview = new HashMap<>();
 
-                // We overwrite existing files since this may be another attempt
-                // at fetching this archive.
-                // Existing files may be incomplete/corrupt.
-                Files.deleteIfExists(targetPath);
+        for (Map.Entry<Path, Set<Path>> pathSetEntry : jobArchivesToRemove.entrySet()) {
+            HashSet<String> jobIdsToRemoveFromOverview = new HashSet<>();
 
-                Files.createFile(target.toPath());
-                try (FileWriter fw = new FileWriter(target)) {
-                    fw.write(json);
-                    fw.flush();
+            for (Path archive : pathSetEntry.getValue()) {
+                jobIdsToRemoveFromOverview.add(archive.getName());
+                try {
+                    archive.getFileSystem().delete(archive, false);
+                } catch (IOException ioe) {
+                    LOG.warn("Could not delete old archive " + archive, ioe);
                 }
             }
+            allJobIdsToRemoveFromOverview.put(pathSetEntry.getKey(), jobIdsToRemoveFromOverview);
         }
 
-        private List<ArchiveEvent> cleanupJobsBeyondSizeLimit(
-                Map<Path, Set<Path>> jobArchivesToRemove) {
-            Map<Path, Set<String>> allJobIdsToRemoveFromOverview = new HashMap<>();
+        return cleanupExpiredJobs(allJobIdsToRemoveFromOverview);
+    }
 
-            for (Map.Entry<Path, Set<Path>> pathSetEntry : jobArchivesToRemove.entrySet()) {
-                HashSet<String> jobIdsToRemoveFromOverview = new HashSet<>();
+    private List<ArchiveEvent> cleanupExpiredJobs(Map<Path, Set<String>> jobsToRemove) {
 
-                for (Path archive : pathSetEntry.getValue()) {
-                    jobIdsToRemoveFromOverview.add(archive.getName());
-                    try {
-                        archive.getFileSystem().delete(archive, false);
-                    } catch (IOException ioe) {
-                        LOG.warn("Could not delete old archive " + archive, ioe);
-                    }
-                }
-                allJobIdsToRemoveFromOverview.put(
-                        pathSetEntry.getKey(), jobIdsToRemoveFromOverview);
-            }
+        List<ArchiveEvent> deleteLog = new ArrayList<>();
+        LOG.info("Archive directories for jobs {} were deleted.", jobsToRemove);
 
-            return cleanupExpiredJobs(allJobIdsToRemoveFromOverview);
-        }
+        jobsToRemove.forEach(
+                (refreshDir, archivesToRemove) -> {
+                    cachedArchivesPerRefreshDirectory.get(refreshDir).removeAll(archivesToRemove);
+                });
+        jobsToRemove.values().stream()
+                .flatMap(Set::stream)
+                .forEach(
+                        removedJobID -> {
+                            deleteJobFiles(removedJobID);
+                            deleteLog.add(new ArchiveEvent(removedJobID, ArchiveEventType.DELETED));
+                        });
 
-        private List<ArchiveEvent> cleanupExpiredJobs(Map<Path, Set<String>> jobsToRemove) {
-
-            List<ArchiveEvent> deleteLog = new ArrayList<>();
-            LOG.info("Archive directories for jobs {} were deleted.", jobsToRemove);
-
-            jobsToRemove.forEach(
-                    (refreshDir, archivesToRemove) -> {
-                        cachedArchivesPerRefreshDirectory
-                                .get(refreshDir)
-                                .removeAll(archivesToRemove);
-                    });
-            jobsToRemove.values().stream()
-                    .flatMap(Set::stream)
-                    .forEach(
-                            removedJobID -> {
-                                deleteJobFiles(removedJobID);
-                                deleteLog.add(
-                                        new ArchiveEvent(removedJobID, ArchiveEventType.DELETED));
-                            });
-
-            return deleteLog;
-        }
+        return deleteLog;
+    }
 
-        private void deleteJobFiles(String jobID) {
-            // Make sure we do not include this job in the overview
-            try {
-                Files.deleteIfExists(new File(webOverviewDir, jobID + JSON_FILE_ENDING).toPath());
-            } catch (IOException ioe) {
-                LOG.warn("Could not delete file from overview directory.", ioe);
-            }
+    private void deleteJobFiles(String jobID) {
+        // Make sure we do not include this job in the overview
+        try {
+            Files.deleteIfExists(new File(webOverviewDir, jobID + JSON_FILE_ENDING).toPath());
+        } catch (IOException ioe) {
+            LOG.warn("Could not delete file from overview directory.", ioe);
+        }
 
-            // Clean up job files we may have created
-            File jobDirectory = new File(webJobDir, jobID);
-            try {
-                FileUtils.deleteDirectory(jobDirectory);
-            } catch (IOException ioe) {
-                LOG.warn("Could not clean up job directory.", ioe);
-            }
+        // Clean up job files we may have created
+        File jobDirectory = new File(webJobDir, jobID);
+        try {
+            FileUtils.deleteDirectory(jobDirectory);
+        } catch (IOException ioe) {
+            LOG.warn("Could not clean up job directory.", ioe);
+        }
 
-            try {
-                Files.deleteIfExists(new File(webJobDir, jobID + JSON_FILE_ENDING).toPath());
-            } catch (IOException ioe) {
-                LOG.warn("Could not delete file from job directory.", ioe);
-            }
+        try {
+            Files.deleteIfExists(new File(webJobDir, jobID + JSON_FILE_ENDING).toPath());
+        } catch (IOException ioe) {
+            LOG.warn("Could not delete file from job directory.", ioe);
         }
     }
 
