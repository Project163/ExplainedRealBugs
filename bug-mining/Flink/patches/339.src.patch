diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/JSONGenerator.java b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/JSONGenerator.java
new file mode 100644
index 00000000000..ad9326aa0db
--- /dev/null
+++ b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/JSONGenerator.java
@@ -0,0 +1,180 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.api;
+
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.flink.streaming.api.invokable.StreamInvokable;
+import org.apache.sling.commons.json.JSONArray;
+import org.apache.sling.commons.json.JSONException;
+import org.apache.sling.commons.json.JSONObject;
+
+public class JSONGenerator {
+
+	public static final String STEPS = "step_function";
+	public static final String ID = "id";
+	public static final String SIDE = "side";
+	public static final String SHIP_STRATEGY = "ship_strategy";
+	public static final String PREDECESSORS = "predecessors";
+	public static final String TYPE = "type";
+	public static final String PACT = "pact";
+	public static final String CONTENTS = "contents";
+	public static final String PARALLELISM = "parallelism";
+
+	private StreamGraph streamGraph;
+
+	public JSONGenerator(StreamGraph streamGraph) {
+		this.streamGraph = streamGraph;
+	}
+
+	public String getJSON() throws JSONException {
+		JSONObject json = new JSONObject();
+		JSONArray nodes = new JSONArray();
+		json.put("nodes", nodes);
+		List<Integer> operatorIDs = new ArrayList<Integer>(streamGraph.getVertexIDs());
+		Collections.sort(operatorIDs);
+		visit(nodes, operatorIDs, new HashMap<Integer, Integer>());
+		return json.toString();
+	}
+
+	private void visit(JSONArray jsonArray, List<Integer> toVisit,
+			Map<Integer, Integer> edgeRemapings) throws JSONException {
+
+		Integer vertexID = toVisit.get(0);
+		StreamNode vertex = streamGraph.getVertex(vertexID);
+
+		if (streamGraph.getSourceIDs().contains(vertexID)
+				|| Collections.disjoint(vertex.getInEdges(), toVisit)) {
+
+			JSONObject node = new JSONObject();
+			decorateNode(vertexID, node);
+
+			if (!streamGraph.getSourceIDs().contains(vertexID)) {
+				JSONArray inputs = new JSONArray();
+				node.put(PREDECESSORS, inputs);
+
+				for (StreamEdge inEdge : vertex.getInEdges()) {
+					int inputID = inEdge.getSourceID();
+
+					Integer mappedID = (edgeRemapings.keySet().contains(inputID)) ? edgeRemapings
+							.get(inputID) : inputID;
+					decorateEdge(inputs, vertexID, mappedID, inputID);
+				}
+			}
+			jsonArray.put(node);
+			toVisit.remove(vertexID);
+		} else {
+			Integer iterationHead = -1;
+			for (StreamEdge inEdge : vertex.getInEdges()) {
+				int operator = inEdge.getSourceID();
+
+				if (streamGraph.vertexIDtoLoop.containsKey(operator)) {
+					iterationHead = operator;
+				}
+			}
+
+			JSONObject obj = new JSONObject();
+			JSONArray iterationSteps = new JSONArray();
+			obj.put(STEPS, iterationSteps);
+			obj.put(ID, iterationHead);
+			obj.put(PACT, "IterativeDataStream");
+			obj.put(PARALLELISM, streamGraph.getVertex(iterationHead).getParallelism());
+			obj.put(CONTENTS, "Stream Iteration");
+			JSONArray iterationInputs = new JSONArray();
+			obj.put(PREDECESSORS, iterationInputs);
+			toVisit.remove(iterationHead);
+			visitIteration(iterationSteps, toVisit, iterationHead, edgeRemapings, iterationInputs);
+			jsonArray.put(obj);
+		}
+
+		if (!toVisit.isEmpty()) {
+			visit(jsonArray, toVisit, edgeRemapings);
+		}
+	}
+
+	private void visitIteration(JSONArray jsonArray, List<Integer> toVisit, int headId,
+			Map<Integer, Integer> edgeRemapings, JSONArray iterationInEdges) throws JSONException {
+
+		Integer vertexID = toVisit.get(0);
+		StreamNode vertex = streamGraph.getVertex(vertexID);
+		toVisit.remove(vertexID);
+
+		// Ignoring head and tail to avoid redundancy
+		if (!streamGraph.vertexIDtoLoop.containsKey(vertexID)) {
+			JSONObject obj = new JSONObject();
+			jsonArray.put(obj);
+			decorateNode(vertexID, obj);
+			JSONArray inEdges = new JSONArray();
+			obj.put(PREDECESSORS, inEdges);
+
+			for (StreamEdge inEdge : vertex.getInEdges()) {
+				int inputID = inEdge.getSourceID();
+
+				if (edgeRemapings.keySet().contains(inputID)) {
+					decorateEdge(inEdges, vertexID, inputID, inputID);
+				} else if (!streamGraph.vertexIDtoLoop.containsKey(inputID)) {
+					decorateEdge(iterationInEdges, vertexID, inputID, inputID);
+				}
+			}
+
+			edgeRemapings.put(vertexID, headId);
+			visitIteration(jsonArray, toVisit, headId, edgeRemapings, iterationInEdges);
+		}
+
+	}
+
+	private void decorateEdge(JSONArray inputArray, int vertexID, int mappedInputID, int inputID)
+			throws JSONException {
+		JSONObject input = new JSONObject();
+		inputArray.put(input);
+		input.put(ID, mappedInputID);
+		input.put(SHIP_STRATEGY, streamGraph.getEdge(inputID, vertexID).getPartitioner()
+				.getStrategy());
+		input.put(SIDE, (inputArray.length() == 0) ? "first" : "second");
+	}
+
+	private void decorateNode(Integer vertexID, JSONObject node) throws JSONException {
+
+		StreamNode vertex = streamGraph.getVertex(vertexID);
+
+		node.put(ID, vertexID);
+		node.put(TYPE, vertex.getOperatorName());
+
+		if (streamGraph.getSourceIDs().contains(vertexID)) {
+			node.put(PACT, "Data Source");
+		} else {
+			node.put(PACT, "Data Stream");
+		}
+
+		StreamInvokable<?, ?> invokable = streamGraph.getVertex(vertexID).getInvokable();
+
+		if (invokable != null && invokable.getUserFunction() != null) {
+			node.put(CONTENTS, vertex.getOperatorName() + " at "
+					+ invokable.getUserFunction().getClass().getSimpleName());
+		} else {
+			node.put(CONTENTS, vertex.getOperatorName());
+		}
+
+		node.put(PARALLELISM, streamGraph.getVertex(vertexID).getParallelism());
+	}
+
+}
\ No newline at end of file
diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/StreamConfig.java b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/StreamConfig.java
index 152d4896c92..b04e75640f6 100644
--- a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/StreamConfig.java
+++ b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/StreamConfig.java
@@ -369,7 +369,7 @@ public class StreamConfig implements Serializable {
 		builder.append("\nOutput names: " + getNonChainedOutputs(cl));
 		builder.append("\nPartitioning:");
 		for (StreamEdge output : getNonChainedOutputs(cl)) {
-			int outputname = output.getTargetVertex();
+			int outputname = output.getTargetID();
 			builder.append("\n\t" + outputname + ": " + output.getPartitioner());
 		}
 
diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/StreamEdge.java b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/StreamEdge.java
index 74edb008239..066f3200e7a 100644
--- a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/StreamEdge.java
+++ b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/StreamEdge.java
@@ -24,7 +24,8 @@ import org.apache.flink.streaming.partitioner.StreamPartitioner;
 
 /**
  * An edge in the streaming topology. One edge like this does not necessarily
- * gets converted to a connection between two job vertices (due to chaining/optimization).
+ * gets converted to a connection between two job vertices (due to
+ * chaining/optimization).
  */
 public class StreamEdge implements Serializable {
 
@@ -32,8 +33,8 @@ public class StreamEdge implements Serializable {
 
 	final private String edgeId;
 
-	final private int sourceVertex;
-	final private int targetVertex;
+	final private StreamNode sourceVertex;
+	final private StreamNode targetVertex;
 
 	/**
 	 * The type number of the input for co-tasks.
@@ -41,33 +42,40 @@ public class StreamEdge implements Serializable {
 	final private int typeNumber;
 
 	/**
-	 * A list of output names that the target vertex listens to (if there is output selection).
+	 * A list of output names that the target vertex listens to (if there is
+	 * output selection).
 	 */
 	final private List<String> selectedNames;
 	final private StreamPartitioner<?> outputPartitioner;
 
-	public StreamEdge(int sourceVertex, int targetVertex, int typeNumber, List<String> selectedNames, StreamPartitioner<?> outputPartitioner) {
+	public StreamEdge(StreamNode sourceVertex, StreamNode targetVertex, int typeNumber,
+			List<String> selectedNames, StreamPartitioner<?> outputPartitioner) {
 		this.sourceVertex = sourceVertex;
 		this.targetVertex = targetVertex;
 		this.typeNumber = typeNumber;
 		this.selectedNames = selectedNames;
 		this.outputPartitioner = outputPartitioner;
 
-		this.edgeId = sourceVertex + "_"
-				+ targetVertex + "_"
-				+ typeNumber + "_"
-				+ selectedNames + "_"
-				+ outputPartitioner;
+		this.edgeId = sourceVertex + "_" + targetVertex + "_" + typeNumber + "_" + selectedNames
+				+ "_" + outputPartitioner;
 	}
 
-	public int getSourceVertex() {
+	public StreamNode getSourceVertex() {
 		return sourceVertex;
 	}
 
-	public int getTargetVertex() {
+	public StreamNode getTargetVertex() {
 		return targetVertex;
 	}
 
+	public int getSourceID() {
+		return sourceVertex.getID();
+	}
+
+	public int getTargetID() {
+		return targetVertex.getID();
+	}
+
 	public int getTypeNumber() {
 		return typeNumber;
 	}
@@ -105,12 +113,8 @@ public class StreamEdge implements Serializable {
 
 	@Override
 	public String toString() {
-		return "(" +
-				sourceVertex +
-				" -> " + targetVertex +
-				", typeNumber=" + typeNumber +
-				", selectedNames=" + selectedNames +
-				", outputPartitioner=" + outputPartitioner +
-				')';
+		return "(" + sourceVertex + " -> " + targetVertex + ", typeNumber=" + typeNumber
+				+ ", selectedNames=" + selectedNames + ", outputPartitioner=" + outputPartitioner
+				+ ')';
 	}
 }
diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/StreamEdgeList.java b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/StreamEdgeList.java
deleted file mode 100644
index d4210790220..00000000000
--- a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/StreamEdgeList.java
+++ /dev/null
@@ -1,152 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.streaming.api;
-
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-
-/**
- * Stores the stream topology with adjacency list graph representation.
- * Edges are represented with {@link org.apache.flink.streaming.api.StreamEdge}s
- */
-public class StreamEdgeList {
-
-	private Map<Integer, List<StreamEdge>> outEdgeLists;
-	private Map<Integer, List<StreamEdge>> inEdgeLists;
-
-	public StreamEdgeList() {
-		outEdgeLists = new HashMap<Integer, List<StreamEdge>>();
-		inEdgeLists = new HashMap<Integer, List<StreamEdge>>();
-	}
-
-	public void addVertex(int vertexId) {
-		outEdgeLists.put(vertexId, new ArrayList<StreamEdge>());
-		inEdgeLists.put(vertexId, new ArrayList<StreamEdge>());
-	}
-
-	public void removeVertex(int vertexId) {
-		ArrayList<StreamEdge> toRemove = new ArrayList<StreamEdge>();
-
-		for (StreamEdge edge : outEdgeLists.get(vertexId)) {
-			toRemove.add(edge);
-		}
-
-		for (StreamEdge edge : inEdgeLists.get(vertexId)) {
-			toRemove.add(edge);
-		}
-
-		for (StreamEdge edge : toRemove) {
-			removeEdge(edge);
-		}
-
-		outEdgeLists.remove(vertexId);
-		inEdgeLists.remove(vertexId);
-	}
-
-	public void addEdge(StreamEdge edge) {
-		int sourceId = edge.getSourceVertex();
-		int targetId = edge.getTargetVertex();
-		outEdgeLists.get(sourceId).add(edge);
-		inEdgeLists.get(targetId).add(edge);
-	}
-
-	public void removeEdge(StreamEdge edge) {
-		int sourceId = edge.getSourceVertex();
-		int targetId = edge.getTargetVertex();
-		removeEdge(sourceId, targetId);
-	}
-
-	public void removeEdge(int sourceId, int targetId) {
-		Iterator<StreamEdge> outIterator = outEdgeLists.get(sourceId).iterator();
-		while (outIterator.hasNext()) {
-			StreamEdge edge = outIterator.next();
-
-			if (edge.getTargetVertex() == targetId) {
-				outIterator.remove();
-			}
-		}
-
-		Iterator<StreamEdge> inIterator = inEdgeLists.get(targetId).iterator();
-		while (inIterator.hasNext()) {
-			StreamEdge edge = inIterator.next();
-
-			if (edge.getSourceVertex() == sourceId) {
-				inIterator.remove();
-			}
-		}
-	}
-
-	public StreamEdge getEdge(int sourceId, int targetId) {
-		Iterator<StreamEdge> outIterator = outEdgeLists.get(sourceId).iterator();
-		while (outIterator.hasNext()) {
-			StreamEdge edge = outIterator.next();
-
-			if (edge.getTargetVertex() == targetId) {
-				return edge;
-			}
-		}
-
-		throw new RuntimeException("No such edge in stream graph: " + sourceId + " -> " + targetId);
-	}
-
-	public List<StreamEdge> getOutEdges(int vertexId) {
-		List<StreamEdge> outEdges = outEdgeLists.get(vertexId);
-
-		if (outEdges == null) {
-			throw new RuntimeException("No such vertex in stream graph: " + vertexId);
-		}
-
-		return outEdges;
-	}
-
-	public List<StreamEdge> getInEdges(int vertexId) {
-		List<StreamEdge> inEdges = inEdgeLists.get(vertexId);
-
-		if (inEdges == null) {
-			throw new RuntimeException("No such vertex in stream graph: " + vertexId);
-		}
-
-		return inEdges;
-	}
-
-	public List<Integer> getOutEdgeIndices(int vertexId) {
-		List<StreamEdge> outEdges = getOutEdges(vertexId);
-		List<Integer> outEdgeIndices = new ArrayList<Integer>();
-
-		for (StreamEdge edge : outEdges) {
-			outEdgeIndices.add(edge.getTargetVertex());
-		}
-
-		return outEdgeIndices;
-	}
-
-	public List<Integer> getInEdgeIndices(int vertexId) {
-		List<StreamEdge> inEdges = getInEdges(vertexId);
-
-		List<Integer> inEdgeIndices = new ArrayList<Integer>();
-
-		for (StreamEdge edge : inEdges) {
-			inEdgeIndices.add(edge.getSourceVertex());
-		}
-
-		return inEdgeIndices;
-	}
-}
\ No newline at end of file
diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/StreamGraph.java b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/StreamGraph.java
index aa71804bd76..9e4a7e616af 100644
--- a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/StreamGraph.java
+++ b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/StreamGraph.java
@@ -23,24 +23,23 @@ import java.io.IOException;
 import java.io.PrintWriter;
 import java.util.ArrayList;
 import java.util.Collection;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.HashSet;
+import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
-import java.util.Map.Entry;
 import java.util.Set;
 
 import org.apache.flink.api.common.ExecutionConfig;
 import org.apache.flink.api.common.io.InputFormat;
 import org.apache.flink.api.common.typeinfo.TypeInformation;
+import org.apache.flink.api.java.tuple.Tuple2;
 import org.apache.flink.api.java.typeutils.MissingTypeInfo;
 import org.apache.flink.optimizer.plan.StreamingPlan;
 import org.apache.flink.runtime.jobgraph.JobGraph;
 import org.apache.flink.runtime.jobgraph.tasks.AbstractInvokable;
 import org.apache.flink.streaming.api.collector.selector.OutputSelector;
-import org.apache.flink.streaming.api.collector.selector.OutputSelectorWrapper;
-import org.apache.flink.streaming.api.collector.selector.OutputSelectorWrapperFactory;
+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 import org.apache.flink.streaming.api.invokable.StreamInvokable;
 import org.apache.flink.streaming.api.invokable.operator.co.CoInvokable;
 import org.apache.flink.streaming.api.streamrecord.StreamRecordSerializer;
@@ -49,124 +48,91 @@ import org.apache.flink.streaming.api.streamvertex.StreamIterationHead;
 import org.apache.flink.streaming.api.streamvertex.StreamIterationTail;
 import org.apache.flink.streaming.api.streamvertex.StreamVertex;
 import org.apache.flink.streaming.partitioner.StreamPartitioner;
-import org.apache.sling.commons.json.JSONArray;
 import org.apache.sling.commons.json.JSONException;
-import org.apache.sling.commons.json.JSONObject;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 /**
- * Object for building Apache Flink stream processing graphs
+ * Class representing the streaming topology. It contains all the information
+ * necessary to build the jobgraph for the execution.
+ * 
  */
 public class StreamGraph extends StreamingPlan {
 
 	private static final Logger LOG = LoggerFactory.getLogger(StreamGraph.class);
 	private final static String DEAFULT_JOB_NAME = "Flink Streaming Job";
-
-	protected boolean chaining = true;
 	private String jobName = DEAFULT_JOB_NAME;
 
-	// Graph attributes
-	private Map<Integer, Integer> operatorParallelisms;
-	private Map<Integer, Long> bufferTimeouts;
-
-	private StreamEdgeList edges;
-
-	private Map<Integer, List<OutputSelector<?>>> outputSelectors;
-
-	private Map<Integer, String> operatorNames;
-	private Map<Integer, StreamInvokable<?, ?>> invokableObjects;
-	private Map<Integer, StreamRecordSerializer<?>> typeSerializersIn1;
-	private Map<Integer, StreamRecordSerializer<?>> typeSerializersIn2;
-	private Map<Integer, StreamRecordSerializer<?>> typeSerializersOut1;
-	private Map<Integer, StreamRecordSerializer<?>> typeSerializersOut2;
-	private Map<Integer, Class<? extends AbstractInvokable>> jobVertexClasses;
-	private Map<Integer, Integer> iterationIds;
-	private Map<Integer, Integer> iterationIDtoHeadID;
-	private Map<Integer, Integer> iterationIDtoTailID;
-	private Map<Integer, Integer> iterationTailCount;
-	private Map<Integer, Long> iterationTimeouts;
-	private Map<Integer, InputFormat<String, ?>> inputFormatLists;
-	private List<Map<Integer, ?>> containingMaps;
-
-	private Set<Integer> sources;
-
-	private ExecutionConfig executionConfig;
-	
-	private boolean checkpointingEnabled;
-	
+	private final StreamExecutionEnvironment environemnt;
+	private final ExecutionConfig executionConfig;
+
+	private boolean checkpointingEnabled = false;
 	private long checkpointingInterval = 5000;
+	private boolean chaining = true;
 
-	public StreamGraph(ExecutionConfig executionConfig) {
+	private final Map<Integer, StreamNode> streamNodes;
+	private final Set<Integer> sources;
 
-		this.executionConfig = executionConfig;
+	private final Map<Integer, StreamLoop> streamLoops;
+	protected final Map<Integer, StreamLoop> vertexIDtoLoop;
 
-		initGraph();
+	public StreamGraph(StreamExecutionEnvironment environment) {
 
-		if (LOG.isDebugEnabled()) {
-			LOG.debug("StreamGraph created");
-		}
-	}
+		this.environemnt = environment;
+		executionConfig = environment.getConfig();
 
-	public void initGraph() {
-		containingMaps = new ArrayList<Map<Integer, ?>>();
-
-		operatorParallelisms = new HashMap<Integer, Integer>();
-		containingMaps.add(operatorParallelisms);
-		bufferTimeouts = new HashMap<Integer, Long>();
-
-		edges = new StreamEdgeList();
-
-		operatorNames = new HashMap<Integer, String>();
-		containingMaps.add(operatorNames);
-		invokableObjects = new HashMap<Integer, StreamInvokable<?, ?>>();
-		containingMaps.add(invokableObjects);
-		typeSerializersIn1 = new HashMap<Integer, StreamRecordSerializer<?>>();
-		containingMaps.add(typeSerializersIn1);
-		typeSerializersIn2 = new HashMap<Integer, StreamRecordSerializer<?>>();
-		containingMaps.add(typeSerializersIn2);
-		typeSerializersOut1 = new HashMap<Integer, StreamRecordSerializer<?>>();
-		containingMaps.add(typeSerializersOut1);
-		typeSerializersOut2 = new HashMap<Integer, StreamRecordSerializer<?>>();
-		containingMaps.add(typeSerializersOut1);
-		outputSelectors = new HashMap<Integer, List<OutputSelector<?>>>();
-		containingMaps.add(outputSelectors);
-		jobVertexClasses = new HashMap<Integer, Class<? extends AbstractInvokable>>();
-		containingMaps.add(jobVertexClasses);
-		iterationIds = new HashMap<Integer, Integer>();
-		containingMaps.add(jobVertexClasses);
-		iterationIDtoHeadID = new HashMap<Integer, Integer>();
-		iterationIDtoTailID = new HashMap<Integer, Integer>();
-		iterationTailCount = new HashMap<Integer, Integer>();
-		containingMaps.add(iterationTailCount);
-		iterationTimeouts = new HashMap<Integer, Long>();
-		containingMaps.add(iterationTailCount);
-		inputFormatLists = new HashMap<Integer, InputFormat<String, ?>>();
-		containingMaps.add(inputFormatLists);
+		streamNodes = new HashMap<Integer, StreamNode>();
+		streamLoops = new HashMap<Integer, StreamLoop>();
+		vertexIDtoLoop = new HashMap<Integer, StreamGraph.StreamLoop>();
 		sources = new HashSet<Integer>();
 	}
 
-	/**
-	 * Adds a vertex to the streaming graph with the given parameters
-	 * 
-	 * @param vertexID
-	 *            ID of the vertex
-	 * @param invokableObject
-	 *            User defined operator
-	 * @param inTypeInfo
-	 *            Input type for serialization
-	 * @param outTypeInfo
-	 *            Output type for serialization
-	 * @param operatorName
-	 *            Operator type
-	 * @param parallelism
-	 *            Number of parallel instances created
-	 */
-	public <IN, OUT> void addStreamVertex(Integer vertexID,
-			StreamInvokable<IN, OUT> invokableObject, TypeInformation<IN> inTypeInfo,
-			TypeInformation<OUT> outTypeInfo, String operatorName, int parallelism) {
+	protected ExecutionConfig getExecutionConfig() {
+		return executionConfig;
+	}
 
-		addVertex(vertexID, StreamVertex.class, invokableObject, operatorName, parallelism);
+	public void setJobName(String jobName) {
+		this.jobName = jobName;
+	}
+
+	public void setChaining(boolean chaining) {
+		this.chaining = chaining;
+	}
+
+	public void setCheckpointingEnabled(boolean checkpointingEnabled) {
+		this.checkpointingEnabled = checkpointingEnabled;
+	}
+
+	public void setCheckpointingInterval(long checkpointingInterval) {
+		this.checkpointingInterval = checkpointingInterval;
+	}
+
+	public long getCheckpointingInterval() {
+		return checkpointingInterval;
+	}
+
+	public boolean isChainingEnabled() {
+		return chaining;
+	}
+
+	public boolean isCheckpointingEnabled() {
+		return checkpointingEnabled;
+	}
+
+	public boolean isIterative() {
+		return !streamLoops.isEmpty();
+	}
+
+	public <IN, OUT> void addSource(Integer vertexID, StreamInvokable<IN, OUT> invokableObject,
+			TypeInformation<IN> inTypeInfo, TypeInformation<OUT> outTypeInfo, String operatorName) {
+		addOperator(vertexID, invokableObject, inTypeInfo, outTypeInfo, operatorName);
+		sources.add(vertexID);
+	}
+
+	public <IN, OUT> void addOperator(Integer vertexID, StreamInvokable<IN, OUT> invokableObject,
+			TypeInformation<IN> inTypeInfo, TypeInformation<OUT> outTypeInfo, String operatorName) {
+
+		addNode(vertexID, StreamVertex.class, invokableObject, operatorName);
 
 		StreamRecordSerializer<IN> inSerializer = inTypeInfo != null ? new StreamRecordSerializer<IN>(
 				inTypeInfo, executionConfig) : null;
@@ -175,55 +141,50 @@ public class StreamGraph extends StreamingPlan {
 				&& !(outTypeInfo instanceof MissingTypeInfo) ? new StreamRecordSerializer<OUT>(
 				outTypeInfo, executionConfig) : null;
 
-		addTypeSerializers(vertexID, inSerializer, null, outSerializer, null);
+		setSerializers(vertexID, inSerializer, null, outSerializer);
 
 		if (LOG.isDebugEnabled()) {
 			LOG.debug("Vertex: {}", vertexID);
 		}
 	}
 
-	public <IN, OUT> void addSourceVertex(Integer vertexID,
-			StreamInvokable<IN, OUT> invokableObject, TypeInformation<IN> inTypeInfo,
-			TypeInformation<OUT> outTypeInfo, String operatorName, int parallelism) {
-		addStreamVertex(vertexID, invokableObject, inTypeInfo, outTypeInfo, operatorName,
-				parallelism);
-		sources.add(vertexID);
+	public <IN1, IN2, OUT> void addCoOperator(Integer vertexID,
+			CoInvokable<IN1, IN2, OUT> taskInvokableObject, TypeInformation<IN1> in1TypeInfo,
+			TypeInformation<IN2> in2TypeInfo, TypeInformation<OUT> outTypeInfo, String operatorName) {
+
+		addNode(vertexID, CoStreamVertex.class, taskInvokableObject, operatorName);
+
+		StreamRecordSerializer<OUT> outSerializer = (outTypeInfo != null)
+				&& !(outTypeInfo instanceof MissingTypeInfo) ? new StreamRecordSerializer<OUT>(
+				outTypeInfo, executionConfig) : null;
+
+		setSerializers(vertexID, new StreamRecordSerializer<IN1>(in1TypeInfo, executionConfig),
+				new StreamRecordSerializer<IN2>(in2TypeInfo, executionConfig), outSerializer);
+
+		if (LOG.isDebugEnabled()) {
+			LOG.debug("CO-TASK: {}", vertexID);
+		}
 	}
 
-	/**
-	 * Adds a vertex for the iteration head to the {@link JobGraph}. The
-	 * iterated values will be fed from this vertex back to the graph.
-	 * 
-	 * @param vertexID
-	 *            ID of the vertex
-	 * @param iterationHead
-	 *            Id of the iteration head
-	 * @param iterationID
-	 *            ID of iteration for multiple iterations
-	 * @param parallelism
-	 *            Number of parallel instances created
-	 * @param waitTime
-	 *            Max wait time for next record
-	 */
 	public void addIterationHead(Integer vertexID, Integer iterationHead, Integer iterationID,
-			int parallelism, long waitTime) {
+			long timeOut) {
 
-		addVertex(vertexID, StreamIterationHead.class, null, null, parallelism);
+		addNode(vertexID, StreamIterationHead.class, null, null);
 
 		chaining = false;
 
-		iterationIds.put(vertexID, iterationID);
-		iterationIDtoHeadID.put(iterationID, vertexID);
+		StreamLoop iteration = new StreamLoop(iterationID, getVertex(iterationHead), timeOut);
+		streamLoops.put(iterationID, iteration);
+		vertexIDtoLoop.put(vertexID, iteration);
 
 		setSerializersFrom(iterationHead, vertexID);
+		getVertex(vertexID).setOperatorName("IterationHead-" + iterationHead);
 
-		int outpartitionerIndexToCopy = edges.getInEdgeIndices(iterationHead).get(0);
-		StreamPartitioner<?> outputPartitioner = edges.getOutEdges(outpartitionerIndexToCopy)
+		int outpartitionerIndex = getVertex(iterationHead).getInEdgeIndices().get(0);
+		StreamPartitioner<?> outputPartitioner = getVertex(outpartitionerIndex).getOutEdges()
 				.get(0).getPartitioner();
 
-		setEdge(vertexID, iterationHead, outputPartitioner, 0, new ArrayList<String>());
-
-		iterationTimeouts.put(iterationIDtoHeadID.get(iterationID), waitTime);
+		addEdge(vertexID, iterationHead, outputPartitioner, 0, new ArrayList<String>());
 
 		if (LOG.isDebugEnabled()) {
 			LOG.debug("ITERATION SOURCE: {}", vertexID);
@@ -232,37 +193,25 @@ public class StreamGraph extends StreamingPlan {
 		sources.add(vertexID);
 	}
 
-	/**
-	 * Adds a vertex for the iteration tail to the {@link JobGraph}. The values
-	 * intended to be iterated will be sent to this sink from the iteration
-	 * head.
-	 * 
-	 * @param vertexID
-	 *            ID of the vertex
-	 * @param iterationTail
-	 *            Id of the iteration tail
-	 * @param iterationID
-	 *            ID of iteration for mulitple iterations
-	 * @param waitTime
-	 *            Max waiting time for next record
-	 */
 	public void addIterationTail(Integer vertexID, Integer iterationTail, Integer iterationID,
 			long waitTime) {
 
-		if (bufferTimeouts.get(iterationTail) == 0) {
+		if (getVertex(iterationTail).getBufferTimeout() == 0) {
 			throw new RuntimeException("Buffer timeout 0 at iteration tail is not supported.");
 		}
 
-		addVertex(vertexID, StreamIterationTail.class, null, null, getParallelism(iterationTail));
+		addNode(vertexID, StreamIterationTail.class, null, null).setParallelism(
+				getVertex(iterationTail).getParallelism());
 
-		iterationIds.put(vertexID, iterationID);
-		iterationIDtoTailID.put(iterationID, vertexID);
+		StreamLoop iteration = streamLoops.get(iterationID);
+		iteration.setTail(getVertex(iterationTail));
+		vertexIDtoLoop.put(vertexID, iteration);
 
 		setSerializersFrom(iterationTail, vertexID);
-		iterationTimeouts.put(iterationIDtoTailID.get(iterationID), waitTime);
+		getVertex(vertexID).setOperatorName("IterationTail-" + iterationTail);
 
-		setParallelism(iterationIDtoHeadID.get(iterationID), getParallelism(iterationTail));
-		setBufferTimeout(iterationIDtoHeadID.get(iterationID), bufferTimeouts.get(iterationTail));
+		setParallelism(iteration.getHead().getID(), getVertex(iterationTail).getParallelism());
+		setBufferTimeout(iteration.getHead().getID(), getVertex(iterationTail).getBufferTimeout());
 
 		if (LOG.isDebugEnabled()) {
 			LOG.debug("ITERATION SINK: {}", vertexID);
@@ -270,199 +219,137 @@ public class StreamGraph extends StreamingPlan {
 
 	}
 
-	public <IN1, IN2, OUT> void addCoTask(Integer vertexID,
-			CoInvokable<IN1, IN2, OUT> taskInvokableObject, TypeInformation<IN1> in1TypeInfo,
-			TypeInformation<IN2> in2TypeInfo, TypeInformation<OUT> outTypeInfo,
-			String operatorName, int parallelism) {
+	protected StreamNode addNode(Integer vertexID, Class<? extends AbstractInvokable> vertexClass,
+			StreamInvokable<?, ?> invokableObject, String operatorName) {
 
-		addVertex(vertexID, CoStreamVertex.class, taskInvokableObject, operatorName, parallelism);
+		StreamNode vertex = new StreamNode(environemnt, vertexID, invokableObject, operatorName,
+				new ArrayList<OutputSelector<?>>(), vertexClass);
 
-		StreamRecordSerializer<OUT> outSerializer = (outTypeInfo != null)
-				&& !(outTypeInfo instanceof MissingTypeInfo) ? new StreamRecordSerializer<OUT>(
-				outTypeInfo, executionConfig) : null;
+		streamNodes.put(vertexID, vertex);
 
-		addTypeSerializers(vertexID, new StreamRecordSerializer<IN1>(in1TypeInfo, executionConfig),
-				new StreamRecordSerializer<IN2>(in2TypeInfo, executionConfig), outSerializer, null);
-
-		if (LOG.isDebugEnabled()) {
-			LOG.debug("CO-TASK: {}", vertexID);
-		}
+		return vertex;
 	}
 
-	/**
-	 * Sets vertex parameters in the JobGraph
-	 *
-	 * @param vertexID
-	 *            Name of the vertex
-	 * @param vertexClass
-	 *            The class of the vertex
-	 * @param operatorName
-	 *            Type of the user defined operator
-	 * @param parallelism
-	 *            Number of parallel instances created
-	 */
-	private void addVertex(Integer vertexID, Class<? extends AbstractInvokable> vertexClass,
-			StreamInvokable<?, ?> invokableObject, String operatorName, int parallelism) {
-
-		jobVertexClasses.put(vertexID, vertexClass);
-		setParallelism(vertexID, parallelism);
-		invokableObjects.put(vertexID, invokableObject);
-		operatorNames.put(vertexID, operatorName);
-
-		edges.addVertex(vertexID);
-		outputSelectors.put(vertexID, new ArrayList<OutputSelector<?>>());
+	public void addEdge(Integer upStreamVertexID, Integer downStreamVertexID,
+			StreamPartitioner<?> partitionerObject, int typeNumber, List<String> outputNames) {
 
-		iterationTailCount.put(vertexID, 0);
+		StreamEdge edge = new StreamEdge(getVertex(upStreamVertexID),
+				getVertex(downStreamVertexID), typeNumber, outputNames, partitionerObject);
+		getVertex(edge.getSourceID()).addOutEdge(edge);
+		getVertex(edge.getTargetID()).addInEdge(edge);
 	}
 
-	/**
-	 * Connects two vertices in the JobGraph using the selected partitioner
-	 * settings
-	 * 
-	 * @param upStreamVertexID
-	 *            ID of the upstream(output) vertex
-	 * @param downStreamVertexID
-	 *            ID of the downstream(input) vertex
-	 * @param partitionerObject
-	 *            Partitioner object
-	 * @param typeNumber
-	 *            Number of the type (used at co-functions)
-	 * @param outputNames
-	 *            User defined names of the out edge
-	 */
-	public void setEdge(Integer upStreamVertexID, Integer downStreamVertexID,
-			StreamPartitioner<?> partitionerObject, int typeNumber, List<String> outputNames) {
+	public <T> void addOutputSelector(Integer vertexID, OutputSelector<T> outputSelector) {
+		getVertex(vertexID).addOutputSelector(outputSelector);
+
+		if (LOG.isDebugEnabled()) {
+			LOG.debug("Outputselector set for {}", vertexID);
+		}
 
-		StreamEdge edge = new StreamEdge(upStreamVertexID, downStreamVertexID, typeNumber, outputNames, partitionerObject);
-		edges.addEdge(edge);
 	}
 
-	public void removeEdge(Integer upStream, Integer downStream) {
-		edges.removeEdge(upStream, downStream);
+	public void setParallelism(Integer vertexID, int parallelism) {
+		getVertex(vertexID).setParallelism(parallelism);
 	}
 
-	public void removeVertex(Integer toRemove) {
-		edges.removeVertex(toRemove);
+	public void setBufferTimeout(Integer vertexID, long bufferTimeout) {
+		getVertex(vertexID).setBufferTimeout(bufferTimeout);
+	}
 
-		for (Map<Integer, ?> map : containingMaps) {
-			map.remove(toRemove);
-		}
+	private void setSerializers(Integer vertexID, StreamRecordSerializer<?> in1,
+			StreamRecordSerializer<?> in2, StreamRecordSerializer<?> out) {
+		StreamNode vertex = getVertex(vertexID);
+		vertex.setSerializerIn1(in1);
+		vertex.setSerializerIn2(in2);
+		vertex.setSerializerOut(out);
 	}
 
-	private void addTypeSerializers(Integer vertexID, StreamRecordSerializer<?> in1,
-			StreamRecordSerializer<?> in2, StreamRecordSerializer<?> out1,
-			StreamRecordSerializer<?> out2) {
-		typeSerializersIn1.put(vertexID, in1);
-		typeSerializersIn2.put(vertexID, in2);
-		typeSerializersOut1.put(vertexID, out1);
-		typeSerializersOut2.put(vertexID, out2);
+	private void setSerializersFrom(Integer from, Integer to) {
+		StreamNode fromVertex = getVertex(from);
+		StreamNode toVertex = getVertex(to);
+
+		toVertex.setSerializerIn1(fromVertex.getTypeSerializerOut());
+		toVertex.setSerializerOut(fromVertex.getTypeSerializerIn1());
 	}
 
-	/**
-	 * Sets the number of parallel instances created for the given vertex.
-	 * 
-	 * @param vertexID
-	 *            ID of the vertex
-	 * @param parallelism
-	 *            Number of parallel instances created
-	 */
-	public void setParallelism(Integer vertexID, int parallelism) {
-		operatorParallelisms.put(vertexID, parallelism);
+	public <OUT> void setOutType(Integer vertexID, TypeInformation<OUT> outType) {
+		StreamRecordSerializer<OUT> serializer = new StreamRecordSerializer<OUT>(outType,
+				executionConfig);
+		getVertex(vertexID).setSerializerOut(serializer);
 	}
 
-	public int getParallelism(Integer vertexID) {
-		return operatorParallelisms.get(vertexID);
+	public <IN, OUT> void setInvokable(Integer vertexID, StreamInvokable<IN, OUT> invokableObject) {
+		getVertex(vertexID).setInvokable(invokableObject);
 	}
 
-	/**
-	 * Sets the input format for the given vertex.
-	 * 
-	 * @param vertexID
-	 *            Name of the vertex
-	 * @param inputFormat
-	 *            input format of the file source associated with the given
-	 *            vertex
-	 */
 	public void setInputFormat(Integer vertexID, InputFormat<String, ?> inputFormat) {
-		inputFormatLists.put(vertexID, inputFormat);
+		getVertex(vertexID).setInputFormat(inputFormat);
 	}
 
-	public void setBufferTimeout(Integer vertexID, long bufferTimeout) {
-		this.bufferTimeouts.put(vertexID, bufferTimeout);
+	public StreamNode getVertex(Integer vertexID) {
+		return streamNodes.get(vertexID);
 	}
 
-	public long getBufferTimeout(Integer vertexID) {
-		return this.bufferTimeouts.get(vertexID);
+	protected Collection<? extends Integer> getVertexIDs() {
+		return streamNodes.keySet();
 	}
 
-	/**
-	 * Sets a user defined {@link OutputSelector} for the given operator. Used
-	 * for directed emits.
-	 *
-	 * @param vertexID
-	 * 		Name of the vertex for which the output selector will be set
-	 * @param outputSelector
-	 * 		The user defined output selector.
-	 */
-	public <T> void setOutputSelector(Integer vertexID, OutputSelector<T> outputSelector) {
-		outputSelectors.get(vertexID).add(outputSelector);
+	protected StreamEdge getEdge(int sourceId, int targetId) {
+		Iterator<StreamEdge> outIterator = getVertex(sourceId).getOutEdges().iterator();
+		while (outIterator.hasNext()) {
+			StreamEdge edge = outIterator.next();
 
-		if (LOG.isDebugEnabled()) {
-			LOG.debug("Outputselector set for {}", vertexID);
+			if (edge.getTargetID() == targetId) {
+				return edge;
+			}
 		}
 
+		throw new RuntimeException("No such edge in stream graph: " + sourceId + " -> " + targetId);
 	}
 
-	public <IN, OUT> void setInvokable(Integer vertexID, StreamInvokable<IN, OUT> invokableObject) {
-		invokableObjects.put(vertexID, invokableObject);
+	public Collection<Integer> getSourceIDs() {
+		return sources;
 	}
 
-	public <OUT> void setOutType(Integer id, TypeInformation<OUT> outType) {
-		StreamRecordSerializer<OUT> serializer = new StreamRecordSerializer<OUT>(outType,
-				executionConfig);
-		typeSerializersOut1.put(id, serializer);
+	public Set<Tuple2<Integer, StreamInvokable<?, ?>>> getInvokables() {
+		Set<Tuple2<Integer, StreamInvokable<?, ?>>> invokableSet = new HashSet<Tuple2<Integer, StreamInvokable<?, ?>>>();
+		for (StreamNode vertex : streamNodes.values()) {
+			invokableSet.add(new Tuple2<Integer, StreamInvokable<?, ?>>(vertex.getID(), vertex
+					.getInvokable()));
+		}
+		return invokableSet;
 	}
 
-	public StreamInvokable<?, ?> getInvokable(Integer vertexID) {
-		return invokableObjects.get(vertexID);
+	public Collection<StreamLoop> getStreamLoops() {
+		return streamLoops.values();
 	}
 
-	@SuppressWarnings("unchecked")
-	public <OUT> StreamRecordSerializer<OUT> getOutSerializer1(Integer vertexID) {
-		return (StreamRecordSerializer<OUT>) typeSerializersOut1.get(vertexID);
+	public Integer getLoopID(Integer vertexID) {
+		return vertexIDtoLoop.get(vertexID).getID();
 	}
 
-	@SuppressWarnings("unchecked")
-	public <OUT> StreamRecordSerializer<OUT> getOutSerializer2(Integer vertexID) {
-		return (StreamRecordSerializer<OUT>) typeSerializersOut2.get(vertexID);
+	public long getLoopTimeout(Integer vertexID) {
+		return vertexIDtoLoop.get(vertexID).getTimeout();
 	}
 
-	@SuppressWarnings("unchecked")
-	public <IN> StreamRecordSerializer<IN> getInSerializer1(Integer vertexID) {
-		return (StreamRecordSerializer<IN>) typeSerializersIn1.get(vertexID);
-	}
+	protected void removeEdge(StreamEdge edge) {
+
+		edge.getSourceVertex().getOutEdges().remove(edge);
+		edge.getTargetVertex().getInEdges().remove(edge);
 
-	@SuppressWarnings("unchecked")
-	public <IN> StreamRecordSerializer<IN> getInSerializer2(Integer vertexID) {
-		return (StreamRecordSerializer<IN>) typeSerializersIn2.get(vertexID);
 	}
 
-	/**
-	 * Sets TypeSerializerWrapper from one vertex to another, used with some
-	 * sinks.
-	 *
-	 * @param from
-	 * 		from
-	 * @param to
-	 * 		to
-	 */
-	public void setSerializersFrom(Integer from, Integer to) {
-		operatorNames.put(to, operatorNames.get(from));
+	protected void removeVertex(StreamNode toRemove) {
+
+		Set<StreamEdge> edgesToRemove = new HashSet<StreamEdge>();
+
+		edgesToRemove.addAll(toRemove.getInEdges());
+		edgesToRemove.addAll(toRemove.getOutEdges());
 
-		typeSerializersIn1.put(to, typeSerializersOut1.get(from));
-		typeSerializersIn2.put(to, typeSerializersOut2.get(from));
-		typeSerializersOut1.put(to, typeSerializersOut1.get(from));
-		typeSerializersOut2.put(to, typeSerializersOut2.get(from));
+		for (StreamEdge edge : edgesToRemove) {
+			removeEdge(edge);
+		}
+		streamNodes.remove(toRemove.getID());
 	}
 
 	/**
@@ -475,18 +362,19 @@ public class StreamGraph extends StreamingPlan {
 	/**
 	 * Gets the assembled {@link JobGraph} and adds a user specified name for
 	 * it.
-	 *
+	 * 
 	 * @param jobGraphName
-	 * 		name of the jobGraph
+	 *            name of the jobGraph
 	 */
 	public JobGraph getJobGraph(String jobGraphName) {
 
 		// temporarily forbid checkpointing for iterative jobs
-		if (isIterative() && isCheckpointingEnabled()){
-			throw new UnsupportedOperationException("Checkpointing is currently not supported for iterative jobs!");
+		if (isIterative() && isCheckpointingEnabled()) {
+			throw new UnsupportedOperationException(
+					"Checkpointing is currently not supported for iterative jobs!");
 		}
 
-		this.jobName = jobGraphName;
+		setJobName(jobGraphName);
 
 		WindowingOptimizer.optimizeGraph(this);
 
@@ -495,107 +383,13 @@ public class StreamGraph extends StreamingPlan {
 		return jobgraphGenerator.createJobGraph(jobGraphName);
 	}
 
-	public void setJobName(String jobName) {
-		this.jobName = jobName;
-	}
-
-	public void setChaining(boolean chaining) {
-		this.chaining = chaining;
-	}
-
-	public Set<Entry<Integer, StreamInvokable<?, ?>>> getInvokables() {
-		return invokableObjects.entrySet();
-	}
-
-	public Collection<Integer> getSources() {
-		return sources;
-	}
-
-	public StreamEdge getEdge(Integer sourceId, Integer targetId) {
-		return edges.getEdge(sourceId, targetId);
-	}
-
-	public List<StreamEdge> getOutEdges(Integer vertexID) {
-		return edges.getOutEdges(vertexID);
-	}
-
-	public List<StreamEdge> getInEdges(Integer vertexID) {
-		return edges.getInEdges(vertexID);
-	}
-
-	public List<Integer> getOutEdgeIndices(Integer vertexID) {
-		return edges.getOutEdgeIndices(vertexID);
-	}
-
-	public List<Integer> getInEdgeIndices(Integer vertexID) {
-		return edges.getInEdgeIndices(vertexID);
-	}
-
-	public Collection<Integer> getIterationIDs() {
-		return new HashSet<Integer>(iterationIds.values());
-	}
-
-	public Integer getIterationTail(int iterID) {
-		return iterationIDtoTailID.get(iterID);
-	}
-
-	public Integer getIterationHead(int iterID) {
-		return iterationIDtoHeadID.get(iterID);
-	}
-
-	public Class<? extends AbstractInvokable> getJobVertexClass(Integer vertexID) {
-		return jobVertexClasses.get(vertexID);
-	}
-
-	public InputFormat<String, ?> getInputFormat(Integer vertexID) {
-		return inputFormatLists.get(vertexID);
-	}
-
-	public OutputSelectorWrapper<?> getOutputSelectorWrapper(Integer vertexID) {
-		return OutputSelectorWrapperFactory.create(outputSelectors.get(vertexID));
-	}
-
-	public Integer getIterationID(Integer vertexID) {
-		return iterationIds.get(vertexID);
-	}
-
-	public long getIterationTimeout(Integer vertexID) {
-		return iterationTimeouts.get(vertexID);
-	}
-
-	public boolean isIterative() { return !iterationIds.isEmpty(); }
-
-	public String getOperatorName(Integer vertexID) {
-		return operatorNames.get(vertexID);
-	}
-
-	public ExecutionConfig getExecutionConfig() {
-		return executionConfig;
-	}
-
-	public void setCheckpointingEnabled(boolean checkpointingEnabled) {
-		this.checkpointingEnabled = checkpointingEnabled;
-	}
-
-	public boolean isCheckpointingEnabled() {
-		return checkpointingEnabled;
-	}
-
-	public void setCheckpointingInterval(long checkpointingInterval) {
-		this.checkpointingInterval = checkpointingInterval;
-	}
-
-	public long getCheckpointingInterval() {
-		return checkpointingInterval;
-	}
-
 	@Override
 	public String getStreamingPlanAsJSON() {
 
 		WindowingOptimizer.optimizeGraph(this);
 
 		try {
-			return new JSONGenerator().getJSON();
+			return new JSONGenerator(this).getJSON();
 		} catch (JSONException e) {
 			if (LOG.isDebugEnabled()) {
 				LOG.debug("JSON plan creation failed: {}", e);
@@ -620,140 +414,44 @@ public class StreamGraph extends StreamingPlan {
 		}
 	}
 
-	private class JSONGenerator {
-
-		public static final String STEPS = "step_function";
-		public static final String ID = "id";
-		public static final String SIDE = "side";
-		public static final String SHIP_STRATEGY = "ship_strategy";
-		public static final String PREDECESSORS = "predecessors";
-		public static final String TYPE = "type";
-		public static final String PACT = "pact";
-		public static final String CONTENTS = "contents";
-		public static final String PARALLELISM = "parallelism";
-
-		public String getJSON() throws JSONException {
-			JSONObject json = new JSONObject();
-			JSONArray nodes = new JSONArray();
-			json.put("nodes", nodes);
-			List<Integer> operatorIDs = new ArrayList<Integer>(operatorNames.keySet());
-			Collections.sort(operatorIDs);
-			visit(nodes, operatorIDs, new HashMap<Integer, Integer>());
-			return json.toString();
-		}
-
-		private void visit(JSONArray jsonArray, List<Integer> toVisit,
-			Map<Integer, Integer> edgeRemapings) throws JSONException {
-
-			Integer vertexID = toVisit.get(0);
-			if (getSources().contains(vertexID) || Collections.disjoint(getInEdges(vertexID), toVisit)) {
-
-				JSONObject node = new JSONObject();
-				decorateNode(vertexID, node);
-
-				if (!getSources().contains(vertexID)) {
-					JSONArray inputs = new JSONArray();
-					node.put(PREDECESSORS, inputs);
-
-					for (StreamEdge inEdge : getInEdges(vertexID)) {
-						int inputID = inEdge.getSourceVertex();
-
-						Integer mappedID = (edgeRemapings.keySet().contains(inputID)) ?
-								edgeRemapings.get(inputID) : inputID;
-						decorateEdge(inputs, vertexID, mappedID, inputID);
-					}
-				}
-				jsonArray.put(node);
-				toVisit.remove(vertexID);
-			} else {
-				Integer iterationHead = -1;
-				for (StreamEdge inEdge : getInEdges(vertexID)) {
-					int operator = inEdge.getSourceVertex();
-
-					if (iterationIds.keySet().contains(operator)) {
-						iterationHead = operator;
-					}
-				}
-
-				JSONObject obj = new JSONObject();
-				JSONArray iterationSteps = new JSONArray();
-				obj.put(STEPS, iterationSteps);
-				obj.put(ID, iterationHead);
-				obj.put(PACT, "IterativeDataStream");
-				obj.put(PARALLELISM, getParallelism(iterationHead));
-				obj.put(CONTENTS,"Stream Iteration");
-				JSONArray iterationInputs = new JSONArray();
-				obj.put(PREDECESSORS, iterationInputs);
-				toVisit.remove(iterationHead);
-				visitIteration(iterationSteps, toVisit, iterationHead, edgeRemapings, iterationInputs);
-				jsonArray.put(obj);
-			}
-
-			if (!toVisit.isEmpty())
-			{
-				visit(jsonArray, toVisit, edgeRemapings);
-			}
-		}
-
-		private void visitIteration(JSONArray jsonArray, List<Integer> toVisit, int headId,
-			Map<Integer, Integer> edgeRemapings, JSONArray iterationInEdges) throws JSONException {
-
-			Integer vertexID = toVisit.get(0);
-			toVisit.remove(vertexID);
-
-			//Ignoring head and tail to avoid redundancy
-			if (!iterationIds.containsKey(vertexID)) {
-				JSONObject obj = new JSONObject();
-				jsonArray.put(obj);
-				decorateNode(vertexID, obj);
-				JSONArray inEdges = new JSONArray();
-				obj.put(PREDECESSORS, inEdges);
+	/**
+	 * Object for representing loops in streaming programs.
+	 * 
+	 */
+	protected static class StreamLoop {
 
-				for (StreamEdge inEdge : getInEdges(vertexID)) {
-					int inputID = inEdge.getSourceVertex();
+		private Integer loopID;
 
-					if (edgeRemapings.keySet().contains(inputID)) {
-						decorateEdge(inEdges, vertexID, inputID, inputID);
-					} else if (!iterationIds.containsKey(inputID)) {
-						decorateEdge(iterationInEdges, vertexID, inputID, inputID);
-					}
-				}
+		private StreamNode head;
+		private StreamNode tail;
 
-				edgeRemapings.put(vertexID, headId);
-				visitIteration(jsonArray, toVisit, headId, edgeRemapings, iterationInEdges);
-			}
+		private Long timeout;
 
+		public StreamLoop(Integer loopID, StreamNode head, Long timeout) {
+			this.loopID = loopID;
+			this.head = head;
+			this.timeout = timeout;
 		}
 
-		private void decorateEdge(JSONArray inputArray, int vertexID, int mappedInputID, int inputID) 
-				throws JSONException {
-			JSONObject input = new JSONObject();
-			inputArray.put(input);
-			input.put(ID, mappedInputID);
-			input.put(SHIP_STRATEGY, edges.getEdge(inputID, vertexID).getPartitioner().getStrategy());
-			input.put(SIDE, (inputArray.length() == 0) ? "first" : "second");
+		public Integer getID() {
+			return loopID;
 		}
 
-		private void decorateNode(Integer vertexID, JSONObject node) throws JSONException {
-			node.put(ID, vertexID);
-			node.put(TYPE, getOperatorName(vertexID));
-
-			if (sources.contains(vertexID)) {
-				node.put(PACT, "Data Source");
-			} else {
-				node.put(PACT, "Data Stream");
-			}
+		public Long getTimeout() {
+			return timeout;
+		}
 
-			if (getInvokable(vertexID) != null && getInvokable(vertexID).getUserFunction() != null) {
-				node.put(CONTENTS, getOperatorName(vertexID) + " at "
-						+ getInvokable(vertexID).getUserFunction().getClass().getSimpleName());
-			} else {
-				node.put(CONTENTS, getOperatorName(vertexID));
-			}
+		public void setTail(StreamNode tail) {
+			this.tail = tail;
+		}
 
-			node.put(PARALLELISM, getParallelism(vertexID));
+		public StreamNode getHead() {
+			return head;
 		}
 
+		public StreamNode getTail() {
+			return tail;
+		}
 
 	}
 
diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/StreamNode.java b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/StreamNode.java
new file mode 100644
index 00000000000..86b478211ad
--- /dev/null
+++ b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/StreamNode.java
@@ -0,0 +1,201 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.api;
+
+import java.io.Serializable;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.flink.api.common.io.InputFormat;
+import org.apache.flink.runtime.jobgraph.tasks.AbstractInvokable;
+import org.apache.flink.streaming.api.collector.selector.OutputSelector;
+import org.apache.flink.streaming.api.collector.selector.OutputSelectorWrapper;
+import org.apache.flink.streaming.api.collector.selector.OutputSelectorWrapperFactory;
+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
+import org.apache.flink.streaming.api.invokable.StreamInvokable;
+import org.apache.flink.streaming.api.streamrecord.StreamRecordSerializer;
+
+/**
+ * Class representing the operators in the streaming programs, with all their
+ * properties.
+ * 
+ */
+public class StreamNode implements Serializable {
+
+	private static final long serialVersionUID = 1L;
+
+	transient private StreamExecutionEnvironment env;
+
+	private Integer ID;
+	private Integer parallelism = null;
+	private Long bufferTimeout = null;
+	private String operatorName;
+
+	private StreamInvokable<?, ?> invokable;
+	private List<OutputSelector<?>> outputSelectors;
+	private StreamRecordSerializer<?> typeSerializerIn1;
+	private StreamRecordSerializer<?> typeSerializerIn2;
+	private StreamRecordSerializer<?> typeSerializerOut;
+
+	private List<StreamEdge> inEdges = new ArrayList<StreamEdge>();
+	private List<StreamEdge> outEdges = new ArrayList<StreamEdge>();
+
+	private Class<? extends AbstractInvokable> jobVertexClass;
+
+	private InputFormat<String, ?> inputFormat;
+
+	public StreamNode(StreamExecutionEnvironment env, Integer ID, StreamInvokable<?, ?> invokable,
+			String operatorName, List<OutputSelector<?>> outputSelector,
+			Class<? extends AbstractInvokable> jobVertexClass) {
+		this.env = env;
+		this.ID = ID;
+		this.operatorName = operatorName;
+		this.invokable = invokable;
+		this.outputSelectors = outputSelector;
+		this.jobVertexClass = jobVertexClass;
+	}
+
+	public void addInEdge(StreamEdge inEdge) {
+		if (inEdge.getTargetID() != getID()) {
+			throw new IllegalArgumentException("Destination ID doesn't match the StreamNode ID");
+		} else {
+			inEdges.add(inEdge);
+		}
+	}
+
+	public void addOutEdge(StreamEdge outEdge) {
+		if (outEdge.getSourceID() != getID()) {
+			throw new IllegalArgumentException("Source ID doesn't match the StreamNode ID");
+		} else {
+			outEdges.add(outEdge);
+		}
+	}
+
+	public List<StreamEdge> getOutEdges() {
+		return outEdges;
+	}
+
+	public List<StreamEdge> getInEdges() {
+		return inEdges;
+	}
+
+	public List<Integer> getOutEdgeIndices() {
+		List<Integer> outEdgeIndices = new ArrayList<Integer>();
+
+		for (StreamEdge edge : outEdges) {
+			outEdgeIndices.add(edge.getTargetID());
+		}
+
+		return outEdgeIndices;
+	}
+
+	public List<Integer> getInEdgeIndices() {
+		List<Integer> inEdgeIndices = new ArrayList<Integer>();
+
+		for (StreamEdge edge : inEdges) {
+			inEdgeIndices.add(edge.getSourceID());
+		}
+
+		return inEdgeIndices;
+	}
+
+	public Integer getID() {
+		return ID;
+	}
+
+	public Integer getParallelism() {
+		return parallelism != null ? parallelism : env.getParallelism();
+	}
+
+	public void setParallelism(Integer parallelism) {
+		this.parallelism = parallelism;
+	}
+
+	public Long getBufferTimeout() {
+		return bufferTimeout != null ? bufferTimeout : env.getBufferTimeout();
+	}
+
+	public void setBufferTimeout(Long bufferTimeout) {
+		this.bufferTimeout = bufferTimeout;
+	}
+
+	public StreamInvokable<?, ?> getInvokable() {
+		return invokable;
+	}
+
+	public void setInvokable(StreamInvokable<?, ?> invokable) {
+		this.invokable = invokable;
+	}
+
+	public String getOperatorName() {
+		return operatorName;
+	}
+
+	public void setOperatorName(String operatorName) {
+		this.operatorName = operatorName;
+	}
+
+	public List<OutputSelector<?>> getOutputSelectors() {
+		return outputSelectors;
+	}
+
+	public OutputSelectorWrapper<?> getOutputSelectorWrapper() {
+		return OutputSelectorWrapperFactory.create(getOutputSelectors());
+	}
+
+	public void addOutputSelector(OutputSelector<?> outputSelector) {
+		this.outputSelectors.add(outputSelector);
+	}
+
+	public StreamRecordSerializer<?> getTypeSerializerIn1() {
+		return typeSerializerIn1;
+	}
+
+	public void setSerializerIn1(StreamRecordSerializer<?> typeSerializerIn1) {
+		this.typeSerializerIn1 = typeSerializerIn1;
+	}
+
+	public StreamRecordSerializer<?> getTypeSerializerIn2() {
+		return typeSerializerIn2;
+	}
+
+	public void setSerializerIn2(StreamRecordSerializer<?> typeSerializerIn2) {
+		this.typeSerializerIn2 = typeSerializerIn2;
+	}
+
+	public StreamRecordSerializer<?> getTypeSerializerOut() {
+		return typeSerializerOut;
+	}
+
+	public void setSerializerOut(StreamRecordSerializer<?> typeSerializerOut) {
+		this.typeSerializerOut = typeSerializerOut;
+	}
+
+	public Class<? extends AbstractInvokable> getJobVertexClass() {
+		return jobVertexClass;
+	}
+
+	public InputFormat<String, ?> getInputFormat() {
+		return inputFormat;
+	}
+
+	public void setInputFormat(InputFormat<String, ?> inputFormat) {
+		this.inputFormat = inputFormat;
+	}
+
+}
diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/StreamingJobGraphGenerator.java b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/StreamingJobGraphGenerator.java
index a9232c6c03e..1502ec734d5 100644
--- a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/StreamingJobGraphGenerator.java
+++ b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/StreamingJobGraphGenerator.java
@@ -33,6 +33,7 @@ import org.apache.flink.runtime.jobgraph.ScheduleMode;
 import org.apache.flink.runtime.jobgraph.tasks.AbstractInvokable;
 import org.apache.flink.runtime.jobmanager.scheduler.CoLocationGroup;
 import org.apache.flink.runtime.jobmanager.scheduler.SlotSharingGroup;
+import org.apache.flink.streaming.api.StreamGraph.StreamLoop;
 import org.apache.flink.streaming.api.invokable.StreamInvokable;
 import org.apache.flink.streaming.api.invokable.StreamInvokable.ChainingStrategy;
 import org.apache.flink.streaming.api.streamvertex.StreamIterationHead;
@@ -48,7 +49,7 @@ public class StreamingJobGraphGenerator {
 
 	private StreamGraph streamGraph;
 
-	private Map<Integer, AbstractJobVertex> streamVertices;
+	private Map<Integer, AbstractJobVertex> jobVertices;
 	private JobGraph jobGraph;
 	private Collection<Integer> builtVertices;
 
@@ -64,7 +65,7 @@ public class StreamingJobGraphGenerator {
 	}
 
 	private void init() {
-		this.streamVertices = new HashMap<Integer, AbstractJobVertex>();
+		this.jobVertices = new HashMap<Integer, AbstractJobVertex>();
 		this.builtVertices = new HashSet<Integer>();
 		this.chainedConfigs = new HashMap<Integer, Map<Integer, StreamConfig>>();
 		this.vertexConfigs = new HashMap<Integer, StreamConfig>();
@@ -81,13 +82,12 @@ public class StreamingJobGraphGenerator {
 		jobGraph.setCheckpointingEnabled(streamGraph.isCheckpointingEnabled());
 		jobGraph.setCheckpointingInterval(streamGraph.getCheckpointingInterval());
 
-		if(jobGraph.isCheckpointingEnabled()) {
+		if (jobGraph.isCheckpointingEnabled()) {
 			int executionRetries = streamGraph.getExecutionConfig().getNumberOfExecutionRetries();
-			if(executionRetries != -1) {
+			if (executionRetries != -1) {
 				jobGraph.setNumberOfExecutionRetries(executionRetries);
-			}
-			else {
-				jobGraph.setNumberOfExecutionRetries(Integer.MAX_VALUE);	
+			} else {
+				jobGraph.setNumberOfExecutionRetries(Integer.MAX_VALUE);
 			}
 		}
 		init();
@@ -105,7 +105,7 @@ public class StreamingJobGraphGenerator {
 		Map<Integer, List<StreamEdge>> physicalInEdgesInOrder = new HashMap<Integer, List<StreamEdge>>();
 
 		for (StreamEdge edge : physicalEdgesInOrder) {
-			int target = edge.getTargetVertex();
+			int target = edge.getTargetID();
 
 			List<StreamEdge> inEdges = physicalInEdgesInOrder.get(target);
 
@@ -127,7 +127,7 @@ public class StreamingJobGraphGenerator {
 	}
 
 	private void setChaining() {
-		for (Integer sourceName : streamGraph.getSources()) {
+		for (Integer sourceName : streamGraph.getSourceIDs()) {
 			createChain(sourceName, sourceName);
 		}
 	}
@@ -141,7 +141,7 @@ public class StreamingJobGraphGenerator {
 			List<StreamEdge> chainableOutputs = new ArrayList<StreamEdge>();
 			List<StreamEdge> nonChainableOutputs = new ArrayList<StreamEdge>();
 
-			for (StreamEdge outEdge : streamGraph.getOutEdges(current)) {
+			for (StreamEdge outEdge : streamGraph.getVertex(current).getOutEdges()) {
 				if (isChainable(outEdge)) {
 					chainableOutputs.add(outEdge);
 				} else {
@@ -150,12 +150,12 @@ public class StreamingJobGraphGenerator {
 			}
 
 			for (StreamEdge chainable : chainableOutputs) {
-				transitiveOutEdges.addAll(createChain(startNode, chainable.getTargetVertex()));
+				transitiveOutEdges.addAll(createChain(startNode, chainable.getTargetID()));
 			}
 
 			for (StreamEdge nonChainable : nonChainableOutputs) {
 				transitiveOutEdges.add(nonChainable);
-				createChain(nonChainable.getTargetVertex(), nonChainable.getTargetVertex());
+				createChain(nonChainable.getTargetID(), nonChainable.getTargetID());
 			}
 
 			chainedNames.put(current, createChainedName(current, chainableOutputs));
@@ -169,7 +169,7 @@ public class StreamingJobGraphGenerator {
 
 				config.setChainStart();
 				config.setOutEdgesInOrder(transitiveOutEdges);
-				config.setOutEdges(streamGraph.getOutEdges(current));
+				config.setOutEdges(streamGraph.getVertex(current).getOutEdges());
 
 				for (StreamEdge edge : transitiveOutEdges) {
 					connect(startNode, edge);
@@ -195,18 +195,18 @@ public class StreamingJobGraphGenerator {
 	}
 
 	private String createChainedName(Integer vertexID, List<StreamEdge> chainedOutputs) {
-		String operatorName = streamGraph.getOperatorName(vertexID);
+		String operatorName = streamGraph.getVertex(vertexID).getOperatorName();
 		if (chainedOutputs.size() > 1) {
 			List<String> outputChainedNames = new ArrayList<String>();
 			for (StreamEdge chainable : chainedOutputs) {
-				outputChainedNames.add(chainedNames.get(chainable.getTargetVertex()));
+				outputChainedNames.add(chainedNames.get(chainable.getTargetID()));
 			}
 			String returnOperatorName = operatorName + " -> ("
 					+ StringUtils.join(outputChainedNames, ", ") + ")";
 			return returnOperatorName;
 		} else if (chainedOutputs.size() == 1) {
 			String returnOperatorName = operatorName + " -> "
-					+ chainedNames.get(chainedOutputs.get(0).getTargetVertex());
+					+ chainedNames.get(chainedOutputs.get(0).getTargetID());
 			return returnOperatorName;
 		} else {
 			return operatorName;
@@ -216,26 +216,30 @@ public class StreamingJobGraphGenerator {
 
 	private StreamConfig createProcessingVertex(Integer vertexID) {
 
-		AbstractJobVertex vertex = new AbstractJobVertex(chainedNames.get(vertexID));
+		AbstractJobVertex jobVertex = new AbstractJobVertex(chainedNames.get(vertexID));
+		StreamNode vertex = streamGraph.getVertex(vertexID);
+
+		jobVertex.setInvokableClass(vertex.getJobVertexClass());
 
-		vertex.setInvokableClass(streamGraph.getJobVertexClass(vertexID));
-		if (streamGraph.getParallelism(vertexID) > 0) {
-			vertex.setParallelism(streamGraph.getParallelism(vertexID));
+		int parallelism = vertex.getParallelism();
+
+		if (parallelism > 0) {
+			jobVertex.setParallelism(parallelism);
 		}
 
 		if (LOG.isDebugEnabled()) {
-			LOG.debug("Parallelism set: {} for {}", streamGraph.getParallelism(vertexID), vertexID);
+			LOG.debug("Parallelism set: {} for {}", parallelism, vertexID);
 		}
 
-		if (streamGraph.getInputFormat(vertexID) != null) {
-			vertex.setInputSplitSource(streamGraph.getInputFormat(vertexID));
+		if (vertex.getInputFormat() != null) {
+			jobVertex.setInputSplitSource(vertex.getInputFormat());
 		}
 
-		streamVertices.put(vertexID, vertex);
+		jobVertices.put(vertexID, jobVertex);
 		builtVertices.add(vertexID);
-		jobGraph.addVertex(vertex);
+		jobGraph.addVertex(jobVertex);
 
-		StreamConfig retConfig = new StreamConfig(vertex.getConfiguration());
+		StreamConfig retConfig = new StreamConfig(jobVertex.getConfiguration());
 		retConfig.setOperatorName(chainedNames.get(vertexID));
 		return retConfig;
 	}
@@ -243,36 +247,37 @@ public class StreamingJobGraphGenerator {
 	private void setVertexConfig(Integer vertexID, StreamConfig config,
 			List<StreamEdge> chainableOutputs, List<StreamEdge> nonChainableOutputs) {
 
+		StreamNode vertex = streamGraph.getVertex(vertexID);
+
 		config.setVertexID(vertexID);
-		config.setBufferTimeout(streamGraph.getBufferTimeout(vertexID));
+		config.setBufferTimeout(vertex.getBufferTimeout());
 
-		config.setTypeSerializerIn1(streamGraph.getInSerializer1(vertexID));
-		config.setTypeSerializerIn2(streamGraph.getInSerializer2(vertexID));
-		config.setTypeSerializerOut1(streamGraph.getOutSerializer1(vertexID));
-		config.setTypeSerializerOut2(streamGraph.getOutSerializer2(vertexID));
+		config.setTypeSerializerIn1(vertex.getTypeSerializerIn1());
+		config.setTypeSerializerIn2(vertex.getTypeSerializerIn2());
+		config.setTypeSerializerOut1(vertex.getTypeSerializerOut());
 
-		config.setUserInvokable(streamGraph.getInvokable(vertexID));
-		config.setOutputSelectorWrapper(streamGraph.getOutputSelectorWrapper(vertexID));
+		config.setUserInvokable(vertex.getInvokable());
+		config.setOutputSelectorWrapper(vertex.getOutputSelectorWrapper());
 
 		config.setNumberOfOutputs(nonChainableOutputs.size());
 		config.setNonChainedOutputs(nonChainableOutputs);
 		config.setChainedOutputs(chainableOutputs);
 		config.setStateMonitoring(streamGraph.isCheckpointingEnabled());
 
-		Class<? extends AbstractInvokable> vertexClass = streamGraph.getJobVertexClass(vertexID);
+		Class<? extends AbstractInvokable> vertexClass = vertex.getJobVertexClass();
 
 		if (vertexClass.equals(StreamIterationHead.class)
 				|| vertexClass.equals(StreamIterationTail.class)) {
-			config.setIterationId(streamGraph.getIterationID(vertexID));
-			config.setIterationWaitTime(streamGraph.getIterationTimeout(vertexID));
+			config.setIterationId(streamGraph.getLoopID(vertexID));
+			config.setIterationWaitTime(streamGraph.getLoopTimeout(vertexID));
 		}
 
 		List<StreamEdge> allOutputs = new ArrayList<StreamEdge>(chainableOutputs);
 		allOutputs.addAll(nonChainableOutputs);
 
 		for (StreamEdge output : allOutputs) {
-			config.setSelectedNames(output.getTargetVertex(),
-					streamGraph.getEdge(vertexID, output.getTargetVertex()).getSelectedNames());
+			config.setSelectedNames(output.getTargetID(),
+					streamGraph.getEdge(vertexID, output.getTargetID()).getSelectedNames());
 		}
 
 		vertexConfigs.put(vertexID, config);
@@ -282,10 +287,10 @@ public class StreamingJobGraphGenerator {
 
 		physicalEdgesInOrder.add(edge);
 
-		Integer downStreamvertexID = edge.getTargetVertex();
+		Integer downStreamvertexID = edge.getTargetID();
 
-		AbstractJobVertex headVertex = streamVertices.get(headOfChain);
-		AbstractJobVertex downStreamVertex = streamVertices.get(downStreamvertexID);
+		AbstractJobVertex headVertex = jobVertices.get(headOfChain);
+		AbstractJobVertex downStreamVertex = jobVertices.get(downStreamvertexID);
 
 		StreamConfig downStreamConfig = new StreamConfig(downStreamVertex.getConfiguration());
 
@@ -305,34 +310,34 @@ public class StreamingJobGraphGenerator {
 	}
 
 	private boolean isChainable(StreamEdge edge) {
-		int vertexID = edge.getSourceVertex();
-		int outName = edge.getTargetVertex();
+		StreamNode upStreamVertex = edge.getSourceVertex();
+		StreamNode downStreamVertex = edge.getTargetVertex();
 
-		StreamInvokable<?, ?> headInvokable = streamGraph.getInvokable(vertexID);
-		StreamInvokable<?, ?> outInvokable = streamGraph.getInvokable(outName);
+		StreamInvokable<?, ?> headInvokable = upStreamVertex.getInvokable();
+		StreamInvokable<?, ?> outInvokable = downStreamVertex.getInvokable();
 
-		return streamGraph.getInEdges(outName).size() == 1
+		return downStreamVertex.getInEdges().size() == 1
 				&& outInvokable != null
 				&& outInvokable.getChainingStrategy() == ChainingStrategy.ALWAYS
 				&& (headInvokable.getChainingStrategy() == ChainingStrategy.HEAD || headInvokable
 						.getChainingStrategy() == ChainingStrategy.ALWAYS)
-				&& (edge.getPartitioner().getStrategy() == PartitioningStrategy.FORWARD || streamGraph
-						.getParallelism(outName) == 1)
-				&& streamGraph.getParallelism(vertexID) == streamGraph.getParallelism(outName)
-				&& streamGraph.chaining;
+				&& (edge.getPartitioner().getStrategy() == PartitioningStrategy.FORWARD || downStreamVertex
+						.getParallelism() == 1)
+				&& upStreamVertex.getParallelism() == downStreamVertex.getParallelism()
+				&& streamGraph.isChainingEnabled();
 	}
 
 	private void setSlotSharing() {
 		SlotSharingGroup shareGroup = new SlotSharingGroup();
 
-		for (AbstractJobVertex vertex : streamVertices.values()) {
+		for (AbstractJobVertex vertex : jobVertices.values()) {
 			vertex.setSlotSharingGroup(shareGroup);
 		}
 
-		for (Integer iterID : streamGraph.getIterationIDs()) {
+		for (StreamLoop loop : streamGraph.getStreamLoops()) {
 			CoLocationGroup ccg = new CoLocationGroup();
-			AbstractJobVertex tail = streamVertices.get(streamGraph.getIterationTail(iterID));
-			AbstractJobVertex head = streamVertices.get(streamGraph.getIterationHead(iterID));
+			AbstractJobVertex tail = jobVertices.get(loop.getTail().getID());
+			AbstractJobVertex head = jobVertices.get(loop.getHead().getID());
 
 			ccg.addVertex(head);
 			ccg.addVertex(tail);
diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/WindowingOptimizer.java b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/WindowingOptimizer.java
index 3e98bdaec7b..ca663bb0e34 100644
--- a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/WindowingOptimizer.java
+++ b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/WindowingOptimizer.java
@@ -20,7 +20,6 @@ package org.apache.flink.streaming.api;
 import java.util.ArrayList;
 import java.util.HashSet;
 import java.util.List;
-import java.util.Map.Entry;
 import java.util.Set;
 
 import org.apache.flink.api.java.tuple.Tuple2;
@@ -43,36 +42,39 @@ public class WindowingOptimizer {
 
 	@SuppressWarnings("rawtypes")
 	private static void removeMergeBeforeFlatten(StreamGraph streamGraph) {
-		Set<Entry<Integer, StreamInvokable<?, ?>>> invokables = streamGraph.getInvokables();
+		Set<Tuple2<Integer, StreamInvokable<?, ?>>> invokables = streamGraph.getInvokables();
 		List<Integer> flatteners = new ArrayList<Integer>();
 
-		for (Entry<Integer, StreamInvokable<?, ?>> entry : invokables) {
-			if (entry.getValue() instanceof WindowFlattener) {
-				flatteners.add(entry.getKey());
+		for (Tuple2<Integer, StreamInvokable<?, ?>> entry : invokables) {
+			if (entry.f1 instanceof WindowFlattener) {
+				flatteners.add(entry.f0);
 			}
 		}
 
-		for (Integer flattener : flatteners) {
+		for (Integer flattenerID : flatteners) {
 			// Flatteners should have exactly one input
-			Integer input = streamGraph.getInEdges(flattener).get(0).getSourceVertex();
+			StreamNode input = streamGraph.getVertex(flattenerID).getInEdges().get(0)
+					.getSourceVertex();
 
 			// Check whether the flatten is applied after a merge
-			if (streamGraph.getInvokable(input) instanceof WindowMerger) {
+			if (input.getInvokable() instanceof WindowMerger) {
 
 				// Mergers should have exactly one input
-				Integer mergeInput = streamGraph.getInEdges(input).get(0).getSourceVertex();
-				streamGraph.setEdge(mergeInput, flattener, new DistributePartitioner(true), 0,
-						new ArrayList<String>());
+				StreamNode mergeInput = input.getInEdges().get(0).getSourceVertex();
+
+				// We connect the merge input to the flattener directly
+				streamGraph.addEdge(mergeInput.getID(), flattenerID,
+						new DistributePartitioner(true), 0, new ArrayList<String>());
 
 				// If the merger is only connected to the flattener we delete it
 				// completely, otherwise we only remove the edge
-				if (streamGraph.getOutEdges(input).size() > 1) {
-					streamGraph.removeEdge(input, flattener);
+				if (input.getOutEdges().size() > 1) {
+					streamGraph.removeEdge(streamGraph.getEdge(input.getID(), flattenerID));
 				} else {
 					streamGraph.removeVertex(input);
 				}
 
-				streamGraph.setParallelism(flattener, streamGraph.getParallelism(mergeInput));
+				streamGraph.setParallelism(flattenerID, mergeInput.getParallelism());
 			}
 		}
 
@@ -80,14 +82,14 @@ public class WindowingOptimizer {
 
 	private static void setDiscretizerReuse(StreamGraph streamGraph) {
 
-		Set<Entry<Integer, StreamInvokable<?, ?>>> invokables = streamGraph.getInvokables();
+		Set<Tuple2<Integer, StreamInvokable<?, ?>>> invokables = streamGraph.getInvokables();
 		List<Tuple2<Integer, StreamDiscretizer<?>>> discretizers = new ArrayList<Tuple2<Integer, StreamDiscretizer<?>>>();
 
 		// Get the discretizers
-		for (Entry<Integer, StreamInvokable<?, ?>> entry : invokables) {
-			if (entry.getValue() instanceof StreamDiscretizer) {
-				discretizers.add(new Tuple2<Integer, StreamDiscretizer<?>>(entry.getKey(),
-						(StreamDiscretizer<?>) entry.getValue()));
+		for (Tuple2<Integer, StreamInvokable<?, ?>> entry : invokables) {
+			if (entry.f1 instanceof StreamDiscretizer) {
+				discretizers.add(new Tuple2<Integer, StreamDiscretizer<?>>(entry.f0,
+						(StreamDiscretizer<?>) entry.f1));
 			}
 		}
 
@@ -96,10 +98,10 @@ public class WindowingOptimizer {
 		for (Tuple2<Integer, StreamDiscretizer<?>> discretizer : discretizers) {
 			boolean inMatching = false;
 			for (Tuple2<StreamDiscretizer<?>, List<Integer>> matching : matchingDiscretizers) {
-				Set<Integer> discretizerInEdges = new HashSet<Integer>(
-						streamGraph.getInEdgeIndices(discretizer.f0));
-				Set<Integer> matchingInEdges = new HashSet<Integer>(
-						streamGraph.getInEdgeIndices(matching.f1.get(0)));
+				Set<Integer> discretizerInEdges = new HashSet<Integer>(streamGraph.getVertex(
+						discretizer.f0).getInEdgeIndices());
+				Set<Integer> matchingInEdges = new HashSet<Integer>(streamGraph.getVertex(
+						matching.f1.get(0)).getInEdgeIndices());
 
 				if (discretizer.f1.equals(matching.f0)
 						&& discretizerInEdges.equals(matchingInEdges)) {
@@ -127,28 +129,23 @@ public class WindowingOptimizer {
 		}
 	}
 
-	private static void replaceDiscretizer(StreamGraph streamGraph, Integer toReplace,
-			Integer replaceWith) {
+	private static void replaceDiscretizer(StreamGraph streamGraph, Integer toReplaceID,
+			Integer replaceWithID) {
 		// Convert to array to create a copy
-		List<Integer> outEdges = new ArrayList<Integer>(streamGraph.getOutEdgeIndices(toReplace));
+		List<StreamEdge> outEdges = new ArrayList<StreamEdge>(streamGraph.getVertex(toReplaceID)
+				.getOutEdges());
 
 		int numOutputs = outEdges.size();
 
 		// Reconnect outputs
 		for (int i = 0; i < numOutputs; i++) {
-			Integer output = outEdges.get(i);
-
-			streamGraph.setEdge(replaceWith, output,
-					streamGraph.getEdge(toReplace, output).getPartitioner(), 0, new ArrayList<String>());
-			streamGraph.removeEdge(toReplace, output);
-		}
+			StreamEdge outEdge = outEdges.get(i);
 
-		List<Integer> inEdges = new ArrayList<Integer>(streamGraph.getInEdgeIndices(toReplace));
-		// Remove inputs
-		for (Integer input : inEdges) {
-			streamGraph.removeEdge(input, toReplace);
+			streamGraph.addEdge(replaceWithID, outEdge.getTargetID(), outEdge.getPartitioner(), 0,
+					new ArrayList<String>());
 		}
 
-		streamGraph.removeVertex(toReplace);
+		// Remove the other discretizer
+		streamGraph.removeVertex(streamGraph.getVertex(toReplaceID));
 	}
 }
diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/ConnectedDataStream.java b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/ConnectedDataStream.java
index e8673598fbb..d34d9d24ebe 100644
--- a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/ConnectedDataStream.java
+++ b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/ConnectedDataStream.java
@@ -409,8 +409,8 @@ public class ConnectedDataStream<IN1, IN2> {
 		SingleOutputStreamOperator<OUT, ?> returnStream = new SingleOutputStreamOperator(
 				environment, functionName, outTypeInfo, functionInvokable);
 
-		dataStream1.streamGraph.addCoTask(returnStream.getId(), functionInvokable, getInputType1(),
-				getInputType2(), outTypeInfo, functionName, environment.getParallelism());
+		dataStream1.streamGraph.addCoOperator(returnStream.getId(), functionInvokable, getInputType1(),
+				getInputType2(), outTypeInfo, functionName);
 
 		dataStream1.connectGraph(dataStream1, returnStream.getId(), 1);
 		dataStream1.connectGraph(dataStream2, returnStream.getId(), 2);
diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java
index 27c831f7d12..0e2a0673860 100644
--- a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java
+++ b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java
@@ -282,7 +282,7 @@ public class DataStream<OUT> {
 	 */
 	public SplitDataStream<OUT> split(OutputSelector<OUT> outputSelector) {
 		for (DataStream<OUT> ds : this.mergedStreams) {
-			streamGraph.setOutputSelector(ds.getId(), clean(outputSelector));
+			streamGraph.addOutputSelector(ds.getId(), clean(outputSelector));
 		}
 
 		return new SplitDataStream<OUT>(this);
@@ -1204,8 +1204,8 @@ public class DataStream<OUT> {
 		SingleOutputStreamOperator<R, ?> returnStream = new SingleOutputStreamOperator(environment,
 				operatorName, outTypeInfo, invokable);
 
-		streamGraph.addStreamVertex(returnStream.getId(), invokable, getType(), outTypeInfo,
-				operatorName, returnStream.getParallelism());
+		streamGraph.addOperator(returnStream.getId(), invokable, getType(), outTypeInfo,
+				operatorName);
 
 		connectGraph(inputStream, returnStream.getId(), 0);
 
@@ -1244,7 +1244,7 @@ public class DataStream<OUT> {
 	 */
 	protected <X> void connectGraph(DataStream<X> inputStream, Integer outputID, int typeNumber) {
 		for (DataStream<X> stream : inputStream.mergedStreams) {
-			streamGraph.setEdge(stream.getId(), outputID, stream.partitioner, typeNumber,
+			streamGraph.addEdge(stream.getId(), outputID, stream.partitioner, typeNumber,
 					inputStream.userDefinedNames);
 		}
 
@@ -1266,8 +1266,8 @@ public class DataStream<OUT> {
 		DataStreamSink<OUT> returnStream = new DataStreamSink<OUT>(environment, "sink", getType(),
 				sinkInvokable);
 
-		streamGraph.addStreamVertex(returnStream.getId(), sinkInvokable, getType(), null,
-				"Stream Sink", returnStream.getParallelism());
+		streamGraph.addOperator(returnStream.getId(), sinkInvokable, getType(), null,
+				"Stream Sink");
 
 		this.connectGraph(this.copy(), returnStream.getId(), 0);
 
diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStreamSource.java b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStreamSource.java
index b8e0a7d3dc1..bc416ba38b7 100644
--- a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStreamSource.java
+++ b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStreamSource.java
@@ -32,8 +32,13 @@ public class DataStreamSource<OUT> extends SingleOutputStreamOperator<OUT, DataS
 	boolean isParallel;
 
 	public DataStreamSource(StreamExecutionEnvironment environment, String operatorType,
-			TypeInformation<OUT> outTypeInfo, StreamInvokable<?, ?> invokable, boolean isParallel) {
+			TypeInformation<OUT> outTypeInfo, StreamInvokable<?, OUT> invokable,
+			boolean isParallel, String sourceName) {
 		super(environment, operatorType, outTypeInfo, invokable);
+
+		environment.getStreamGraph().addSource(getId(), invokable, null, outTypeInfo,
+				sourceName);
+
 		this.isParallel = isParallel;
 		if (!isParallel) {
 			setParallelism(1);
diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/IterativeDataStream.java b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/IterativeDataStream.java
index 306bfe8a320..3d29569cd2b 100644
--- a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/IterativeDataStream.java
+++ b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/IterativeDataStream.java
@@ -92,12 +92,9 @@ public class IterativeDataStream<IN> extends
 	}
 
 	private <X> void addIterationSource(DataStream<X> dataStream) {
-
-		DataStream<X> iterationSource = new DataStreamSource<X>(environment, "Iteration Source",
-				null, null, true);
-
-		streamGraph.addIterationHead(iterationSource.getId(), dataStream.getId(), iterationID,
-				dataStream.getParallelism(), waitTime);
+		Integer id = ++counter;
+		streamGraph.addIterationHead(id, dataStream.getId(), iterationID, waitTime);
+		streamGraph.setParallelism(id, dataStream.getParallelism());
 	}
 
 	@Override
diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/SingleOutputStreamOperator.java b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/SingleOutputStreamOperator.java
index 7b6a36789d6..cebebe0dbf7 100644
--- a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/SingleOutputStreamOperator.java
+++ b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/SingleOutputStreamOperator.java
@@ -43,7 +43,6 @@ public class SingleOutputStreamOperator<OUT, O extends SingleOutputStreamOperato
 	protected SingleOutputStreamOperator(StreamExecutionEnvironment environment,
 			String operatorType, TypeInformation<OUT> outTypeInfo, StreamInvokable<?, ?> invokable) {
 		super(environment, operatorType, outTypeInfo);
-		setBufferTimeout(environment.getBufferTimeout());
 		this.isSplit = false;
 		this.invokable = invokable;
 	}
diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/temporaloperator/StreamCrossOperator.java b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/temporaloperator/StreamCrossOperator.java
index 20a069960b0..f10c1c96b79 100644
--- a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/temporaloperator/StreamCrossOperator.java
+++ b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/temporaloperator/StreamCrossOperator.java
@@ -76,7 +76,8 @@ public class StreamCrossOperator<I1, I2> extends
 
 		@SuppressWarnings("unchecked")
 		public CrossWindow<I1, I2> every(long length) {
-			((CoWindowInvokable<I1, I2, ?>) streamGraph.getInvokable(id)).setSlideSize(length);
+			((CoWindowInvokable<I1, I2, ?>) streamGraph.getVertex(id).getInvokable())
+					.setSlideSize(length);
 			return this;
 		}
 
diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.java b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.java
index 0949d70d9cd..6097a0544ea 100644
--- a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.java
+++ b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.java
@@ -83,7 +83,7 @@ public abstract class StreamExecutionEnvironment {
 	 * Constructor for creating StreamExecutionEnvironment
 	 */
 	protected StreamExecutionEnvironment() {
-		streamGraph = new StreamGraph(config);
+		streamGraph = new StreamGraph(this);
 	}
 
 	/**
@@ -94,12 +94,12 @@ public abstract class StreamExecutionEnvironment {
 	}
 
 	/**
-	 * Gets the parallelism with which operation are executed by
-	 * default. Operations can individually override this value to use a
-	 * specific parallelism.
+	 * Gets the parallelism with which operation are executed by default.
+	 * Operations can individually override this value to use a specific
+	 * parallelism.
 	 * 
-	 * @return The parallelism used by operations, unless they
-	 *         override that value.
+	 * @return The parallelism used by operations, unless they override that
+	 *         value.
 	 * @deprecated Please use {@link #getParallelism}
 	 */
 	@Deprecated
@@ -108,28 +108,29 @@ public abstract class StreamExecutionEnvironment {
 	}
 
 	/**
-	 * Gets the parallelism with which operation are executed by
-	 * default. Operations can individually override this value to use a
-	 * specific parallelism.
-	 *
-	 * @return The parallelism used by operations, unless they
-	 *         override that value.
+	 * Gets the parallelism with which operation are executed by default.
+	 * Operations can individually override this value to use a specific
+	 * parallelism.
+	 * 
+	 * @return The parallelism used by operations, unless they override that
+	 *         value.
 	 */
 	public int getParallelism() {
 		return config.getParallelism();
 	}
 
 	/**
-	 * Sets the parallelism for operations executed through this
-	 * environment. Setting a parallelism of x here will cause all operators (such as
-	 * map, batchReduce) to run with x parallel instances. This method overrides
-	 * the default parallelism for this environment. The
+	 * Sets the parallelism for operations executed through this environment.
+	 * Setting a parallelism of x here will cause all operators (such as map,
+	 * batchReduce) to run with x parallel instances. This method overrides the
+	 * default parallelism for this environment. The
 	 * {@link LocalStreamEnvironment} uses by default a value equal to the
 	 * number of hardware contexts (CPU cores / threads). When executing the
 	 * program via the command line client from a JAR file, the default degree
 	 * of parallelism is the one configured for that setup.
 	 * 
-	 * @param parallelism The parallelism
+	 * @param parallelism
+	 *            The parallelism
 	 * @deprecated Please use {@link #setParallelism}
 	 */
 	@Deprecated
@@ -138,15 +139,15 @@ public abstract class StreamExecutionEnvironment {
 	}
 
 	/**
-	 * Sets the parallelism for operations executed through this
-	 * environment. Setting a parallelism of x here will cause all operators (such as
-	 * map, batchReduce) to run with x parallel instances. This method overrides
-	 * the default parallelism for this environment. The
+	 * Sets the parallelism for operations executed through this environment.
+	 * Setting a parallelism of x here will cause all operators (such as map,
+	 * batchReduce) to run with x parallel instances. This method overrides the
+	 * default parallelism for this environment. The
 	 * {@link LocalStreamEnvironment} uses by default a value equal to the
 	 * number of hardware contexts (CPU cores / threads). When executing the
 	 * program via the command line client from a JAR file, the default degree
 	 * of parallelism is the one configured for that setup.
-	 *
+	 * 
 	 * @param parallelism
 	 *            The parallelism
 	 */
@@ -187,14 +188,18 @@ public abstract class StreamExecutionEnvironment {
 	}
 
 	/**
-	 * Method for enabling fault-tolerance. Activates monitoring and backup of streaming operator states.
-	 *
+	 * Method for enabling fault-tolerance. Activates monitoring and backup of
+	 * streaming operator states.
+	 * 
 	 * <p>
-	 * Setting this option assumes that the job is used in production and thus if not stated explicitly
-	 * otherwise with calling with the {@link #setNumberOfExecutionRetries(int numberOfExecutionRetries)}
-	 * method in case of failure the job will be resubmitted to the cluster indefinitely.
-	 *
-	 * @param interval Time interval between state checkpoints in millis
+	 * Setting this option assumes that the job is used in production and thus
+	 * if not stated explicitly otherwise with calling with the
+	 * {@link #setNumberOfExecutionRetries(int numberOfExecutionRetries)} method
+	 * in case of failure the job will be resubmitted to the cluster
+	 * indefinitely.
+	 * 
+	 * @param interval
+	 *            Time interval between state checkpoints in millis
 	 */
 	public StreamExecutionEnvironment enableCheckpointing(long interval) {
 		streamGraph.setCheckpointingEnabled(true);
@@ -202,14 +207,16 @@ public abstract class StreamExecutionEnvironment {
 		return this;
 	}
 
-
 	/**
-	 * Method for enabling fault-tolerance. Activates monitoring and backup of streaming operator states.
-	 *
+	 * Method for enabling fault-tolerance. Activates monitoring and backup of
+	 * streaming operator states.
+	 * 
 	 * <p>
-	 * Setting this option assumes that the job is used in production and thus if not stated explicitly
-	 * otherwise with calling with the {@link #setNumberOfExecutionRetries(int numberOfExecutionRetries)}
-	 * method in case of failure the job will be resubmitted to the cluster indefinitely.
+	 * Setting this option assumes that the job is used in production and thus
+	 * if not stated explicitly otherwise with calling with the
+	 * {@link #setNumberOfExecutionRetries(int numberOfExecutionRetries)} method
+	 * in case of failure the job will be resubmitted to the cluster
+	 * indefinitely.
 	 */
 	public StreamExecutionEnvironment enableCheckpointing() {
 		streamGraph.setCheckpointingEnabled(true);
@@ -217,22 +224,26 @@ public abstract class StreamExecutionEnvironment {
 	}
 
 	/**
-	 * Sets the number of times that failed tasks are re-executed. A value of zero
-	 * effectively disables fault tolerance. A value of {@code -1} indicates that the system
-	 * default value (as defined in the configuration) should be used.
-	 *
-	 * @param numberOfExecutionRetries The number of times the system will try to re-execute failed tasks.
+	 * Sets the number of times that failed tasks are re-executed. A value of
+	 * zero effectively disables fault tolerance. A value of {@code -1}
+	 * indicates that the system default value (as defined in the configuration)
+	 * should be used.
+	 * 
+	 * @param numberOfExecutionRetries
+	 *            The number of times the system will try to re-execute failed
+	 *            tasks.
 	 */
 	public void setNumberOfExecutionRetries(int numberOfExecutionRetries) {
 		config.setNumberOfExecutionRetries(numberOfExecutionRetries);
 	}
 
 	/**
-	 * Gets the number of times the system will try to re-execute failed tasks. A value
-	 * of {@code -1} indicates that the system default value (as defined in the configuration)
-	 * should be used.
-	 *
-	 * @return The number of times the system will try to re-execute failed tasks.
+	 * Gets the number of times the system will try to re-execute failed tasks.
+	 * A value of {@code -1} indicates that the system default value (as defined
+	 * in the configuration) should be used.
+	 * 
+	 * @return The number of times the system will try to re-execute failed
+	 *         tasks.
 	 */
 	public int getNumberOfExecutionRetries() {
 		return config.getNumberOfExecutionRetries();
@@ -254,26 +265,27 @@ public abstract class StreamExecutionEnvironment {
 	 * environment created by {@link #createLocalEnvironment()}.
 	 * 
 	 * @param parallelism
-	 *            The parallelism to use as the default local
-	 *            parallelism.
+	 *            The parallelism to use as the default local parallelism.
 	 */
 	public static void setDefaultLocalParallelism(int parallelism) {
 		defaultLocalParallelism = parallelism;
 	}
 
 	// --------------------------------------------------------------------------------------------
-	//  Registry for types and serializers
+	// Registry for types and serializers
 	// --------------------------------------------------------------------------------------------
 
-
 	/**
 	 * Adds a new Kryo default serializer to the Runtime.
-	 *
-	 * Note that the serializer instance must be serializable (as defined by java.io.Serializable),
-	 * because it may be distributed to the worker nodes by java serialization.
-	 *
-	 * @param type The class of the types serialized with the given serializer.
-	 * @param serializer The serializer to use.
+	 * 
+	 * Note that the serializer instance must be serializable (as defined by
+	 * java.io.Serializable), because it may be distributed to the worker nodes
+	 * by java serialization.
+	 * 
+	 * @param type
+	 *            The class of the types serialized with the given serializer.
+	 * @param serializer
+	 *            The serializer to use.
 	 */
 	public void addDefaultKryoSerializer(Class<?> type, Serializer<?> serializer) {
 		config.addDefaultKryoSerializer(type, serializer);
@@ -281,44 +293,55 @@ public abstract class StreamExecutionEnvironment {
 
 	/**
 	 * Adds a new Kryo default serializer to the Runtime.
-	 *
-	 * @param type The class of the types serialized with the given serializer.
-	 * @param serializerClass The class of the serializer to use.
+	 * 
+	 * @param type
+	 *            The class of the types serialized with the given serializer.
+	 * @param serializerClass
+	 *            The class of the serializer to use.
 	 */
-	public void addDefaultKryoSerializer(Class<?> type, Class<? extends Serializer<?>> serializerClass) {
+	public void addDefaultKryoSerializer(Class<?> type,
+			Class<? extends Serializer<?>> serializerClass) {
 		config.addDefaultKryoSerializer(type, serializerClass);
 	}
 
 	/**
 	 * Registers the given type with a Kryo Serializer.
-	 *
-	 * Note that the serializer instance must be serializable (as defined by java.io.Serializable),
-	 * because it may be distributed to the worker nodes by java serialization.
-	 *
-	 * @param type The class of the types serialized with the given serializer.
-	 * @param serializer The serializer to use.
+	 * 
+	 * Note that the serializer instance must be serializable (as defined by
+	 * java.io.Serializable), because it may be distributed to the worker nodes
+	 * by java serialization.
+	 * 
+	 * @param type
+	 *            The class of the types serialized with the given serializer.
+	 * @param serializer
+	 *            The serializer to use.
 	 */
 	public void registerTypeWithKryoSerializer(Class<?> type, Serializer<?> serializer) {
 		config.registerTypeWithKryoSerializer(type, serializer);
 	}
 
 	/**
-	 * Registers the given Serializer via its class as a serializer for the given type at the KryoSerializer
-	 *
-	 * @param type The class of the types serialized with the given serializer.
-	 * @param serializerClass The class of the serializer to use.
+	 * Registers the given Serializer via its class as a serializer for the
+	 * given type at the KryoSerializer
+	 * 
+	 * @param type
+	 *            The class of the types serialized with the given serializer.
+	 * @param serializerClass
+	 *            The class of the serializer to use.
 	 */
-	public void registerTypeWithKryoSerializer(Class<?> type, Class<? extends Serializer<?>> serializerClass) {
+	public void registerTypeWithKryoSerializer(Class<?> type,
+			Class<? extends Serializer<?>> serializerClass) {
 		config.registerTypeWithKryoSerializer(type, serializerClass);
 	}
 
 	/**
-	 * Registers the given type with the serialization stack. If the type is eventually
-	 * serialized as a POJO, then the type is registered with the POJO serializer. If the
-	 * type ends up being serialized with Kryo, then it will be registered at Kryo to make
-	 * sure that only tags are written.
-	 *
-	 * @param type The class of the type to register.
+	 * Registers the given type with the serialization stack. If the type is
+	 * eventually serialized as a POJO, then the type is registered with the
+	 * POJO serializer. If the type ends up being serialized with Kryo, then it
+	 * will be registered at Kryo to make sure that only tags are written.
+	 * 
+	 * @param type
+	 *            The class of the type to register.
 	 */
 	public void registerType(Class<?> type) {
 		if (type == null) {
@@ -460,11 +483,13 @@ public abstract class StreamExecutionEnvironment {
 	 * from socket. Received strings are decoded by the system's default
 	 * character set. On the termination of the socket server connection retries
 	 * can be initiated.
-	 *
-	 *  <p>Let us note that the socket itself does not report on abort and
-	 * as a consequence retries are only initiated when the socket was gracefully
-	 * terminated.</p>
-	 *
+	 * 
+	 * <p>
+	 * Let us note that the socket itself does not report on abort and as a
+	 * consequence retries are only initiated when the socket was gracefully
+	 * terminated.
+	 * </p>
+	 * 
 	 * @param hostname
 	 *            The host name which a server socket bind.
 	 * @param port
@@ -473,16 +498,18 @@ public abstract class StreamExecutionEnvironment {
 	 * @param delimiter
 	 *            A character which split received strings into records.
 	 * @param maxRetry
-	 *            The maximal retry interval in seconds while the program waits for
-	 *            a socket that is temporarily down. Reconnection is initiated every
-	 *            second. A number of 0 means that the reader is immediately
-	 *            terminated, while a negative value ensures retrying forever.
+	 *            The maximal retry interval in seconds while the program waits
+	 *            for a socket that is temporarily down. Reconnection is
+	 *            initiated every second. A number of 0 means that the reader is
+	 *            immediately terminated, while a negative value ensures
+	 *            retrying forever.
 	 * @return A DataStream, containing the strings received from socket.
-	 *
+	 * 
 	 */
-	public DataStreamSource<String> socketTextStream(String hostname, int port, char delimiter, long maxRetry) {
+	public DataStreamSource<String> socketTextStream(String hostname, int port, char delimiter,
+			long maxRetry) {
 		return addSource(new SocketTextStreamFunction(hostname, port, delimiter, maxRetry), null,
-			"Socket Stream");
+				"Socket Stream");
 	}
 
 	/**
@@ -506,8 +533,8 @@ public abstract class StreamExecutionEnvironment {
 	/**
 	 * Creates a new DataStream that contains the strings received infinitely
 	 * from socket. Received strings are decoded by the system's default
-	 * character set, uses '\n' as delimiter. The reader is terminated immediately
-	 * when socket is down.
+	 * character set, uses '\n' as delimiter. The reader is terminated
+	 * immediately when socket is down.
 	 * 
 	 * @param hostname
 	 *            The host name which a server socket bind.
@@ -618,18 +645,12 @@ public abstract class StreamExecutionEnvironment {
 		}
 
 		boolean isParallel = function instanceof ParallelSourceFunction;
-		int parallelism = isParallel ? getParallelism() : 1;
 
 		ClosureCleaner.clean(function, true);
 		StreamInvokable<OUT, OUT> sourceInvokable = new SourceInvokable<OUT>(function);
 
-		DataStreamSource<OUT> returnStream = new DataStreamSource<OUT>(this, sourceName,
-				outTypeInfo, sourceInvokable, isParallel);
-
-		streamGraph.addSourceVertex(returnStream.getId(), sourceInvokable, null, outTypeInfo,
-				sourceName, parallelism);
-
-		return returnStream;
+		return new DataStreamSource<OUT>(this, sourceName, outTypeInfo, sourceInvokable,
+				isParallel, sourceName);
 	}
 
 	// --------------------------------------------------------------------------------------------
@@ -670,10 +691,9 @@ public abstract class StreamExecutionEnvironment {
 	/**
 	 * Creates a {@link LocalStreamEnvironment}. The local execution environment
 	 * will run the program in a multi-threaded fashion in the same JVM as the
-	 * environment was created in. The default parallelism of the
-	 * local environment is the number of hardware contexts (CPU cores /
-	 * threads), unless it was specified differently by
-	 * {@link #setParallelism(int)}.
+	 * environment was created in. The default parallelism of the local
+	 * environment is the number of hardware contexts (CPU cores / threads),
+	 * unless it was specified differently by {@link #setParallelism(int)}.
 	 * 
 	 * @return A local execution environment.
 	 */
@@ -684,13 +704,12 @@ public abstract class StreamExecutionEnvironment {
 	/**
 	 * Creates a {@link LocalStreamEnvironment}. The local execution environment
 	 * will run the program in a multi-threaded fashion in the same JVM as the
-	 * environment was created in. It will use the parallelism
-	 * specified in the parameter.
+	 * environment was created in. It will use the parallelism specified in the
+	 * parameter.
 	 * 
 	 * @param parallelism
 	 *            The parallelism for the local environment.
-	 * @return A local execution environment with the specified
-	 *         parallelism.
+	 * @return A local execution environment with the specified parallelism.
 	 */
 	public static LocalStreamEnvironment createLocalEnvironment(int parallelism) {
 		currentEnvironment = new LocalStreamEnvironment();
@@ -760,8 +779,9 @@ public abstract class StreamExecutionEnvironment {
 	 * <p>
 	 * The program execution will be logged and displayed with a generated
 	 * default name.
-	 *
-	 * @return The result of the job execution, containing elapsed time and accumulators.
+	 * 
+	 * @return The result of the job execution, containing elapsed time and
+	 *         accumulators.
 	 * @throws Exception
 	 **/
 	public abstract JobExecutionResult execute() throws Exception;
@@ -775,7 +795,8 @@ public abstract class StreamExecutionEnvironment {
 	 * 
 	 * @param jobName
 	 *            Desired name of the job
-	 * @return The result of the job execution, containing elapsed time and accumulators.
+	 * @return The result of the job execution, containing elapsed time and
+	 *         accumulators.
 	 * @throws Exception
 	 **/
 	public abstract JobExecutionResult execute(String jobName) throws Exception;
@@ -801,7 +822,6 @@ public abstract class StreamExecutionEnvironment {
 		return getStreamGraph().getStreamingPlanAsJSON();
 	}
 
-
 	protected static void initializeFromFactory(StreamExecutionEnvironmentFactory eef) {
 		currentEnvironment = eef.createExecutionEnvironment();
 	}
diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/streamvertex/OutputHandler.java b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/streamvertex/OutputHandler.java
index 42afcaec235..40a83f3c426 100644
--- a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/streamvertex/OutputHandler.java
+++ b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/streamvertex/OutputHandler.java
@@ -77,8 +77,8 @@ public class OutputHandler<OUT> {
 		for (StreamEdge outEdge : outEdgesInOrder) {
 			StreamOutput<?> streamOutput = createStreamOutput(
 					outEdge,
-					outEdge.getTargetVertex(),
-					chainedConfigs.get(outEdge.getSourceVertex()),
+					outEdge.getTargetID(),
+					chainedConfigs.get(outEdge.getSourceID()),
 					outEdgesInOrder.indexOf(outEdge));
 			outputMap.put(outEdge, streamOutput);
 		}
@@ -129,7 +129,7 @@ public class OutputHandler<OUT> {
 
 		// Create collectors for the chained outputs
 		for (StreamEdge outputEdge : chainedTaskConfig.getChainedOutputs(cl)) {
-			Integer output = outputEdge.getTargetVertex();
+			Integer output = outputEdge.getTargetID();
 
 			Collector<?> outCollector = createChainedCollector(chainedConfigs.get(output));
 
diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/api/StreamEdgeListTest.java b/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/api/StreamEdgeListTest.java
deleted file mode 100644
index c4ba987f89a..00000000000
--- a/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/api/StreamEdgeListTest.java
+++ /dev/null
@@ -1,116 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.streaming.api;
-
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.fail;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashSet;
-import java.util.List;
-
-import org.junit.Before;
-import org.junit.Test;
-
-public class StreamEdgeListTest {
-
-	private StreamEdgeList edgeList;
-
-	@Before
-	public void init() {
-		edgeList = new StreamEdgeList();
-	}
-
-	@Test
-	public void test() {
-		edgeList.addVertex(1);
-		edgeList.addVertex(2);
-		edgeList.addVertex(3);
-
-
-		// add edges
-		StreamEdge edge1 = new StreamEdge(1, 2, -1, null, null);
-		StreamEdge edge2 = new StreamEdge(2, 3, -1, null, null);
-		StreamEdge edge3 = new StreamEdge(1, 3, -1, null, null);
-
-		edgeList.addEdge(edge1);
-		edgeList.addEdge(edge2);
-		edgeList.addEdge(edge3);
-
-		// check adding
-		checkIfSameElements(edgeList.getOutEdges(1), Arrays.asList(edge1, edge3));
-		checkIfSameElements(edgeList.getOutEdges(2), Arrays.asList(edge2));
-		checkIfSameElements(edgeList.getOutEdges(3), new ArrayList<StreamEdge>());
-
-		checkIfSameElements(edgeList.getInEdges(1), new ArrayList<StreamEdge>());
-		checkIfSameElements(edgeList.getInEdges(2), Arrays.asList(edge1));
-		checkIfSameElements(edgeList.getInEdges(3), Arrays.asList(edge2, edge3));
-
-		// add duplicate edges
-		StreamEdge edge1new = new StreamEdge(1, 2, -2, null, null);
-		StreamEdge edge2new = new StreamEdge(2, 3, -2, null, null);
-
-		edgeList.addEdge(edge1new);
-		edgeList.addEdge(edge2new);
-
-		// check adding
-		checkIfSameElements(edgeList.getOutEdges(1), Arrays.asList(edge1, edge1new, edge3));
-		checkIfSameElements(edgeList.getOutEdges(2), Arrays.asList(edge2, edge2new));
-		checkIfSameElements(edgeList.getOutEdges(3), new ArrayList<StreamEdge>());
-
-		checkIfSameElements(edgeList.getInEdges(1), new ArrayList<StreamEdge>());
-		checkIfSameElements(edgeList.getInEdges(2), Arrays.asList(edge1, edge1new));
-		checkIfSameElements(edgeList.getInEdges(3), Arrays.asList(edge2, edge2new, edge3));
-
-		// remove a duplicate edge
-		edgeList.removeEdge(1, 2);
-
-		// check removing
-		checkIfSameElements(edgeList.getOutEdges(1), Arrays.asList(edge3));
-		checkIfSameElements(edgeList.getOutEdges(2), Arrays.asList(edge2, edge2new));
-
-		checkIfSameElements(edgeList.getInEdges(1), new ArrayList<StreamEdge>());
-		checkIfSameElements(edgeList.getInEdges(2), new ArrayList<StreamEdge>());
-
-		// add back an edge and delete a vertex
-		edgeList.addEdge(edge1);
-		edgeList.removeVertex(2);
-
-		// check removing
-		checkIfSameElements(edgeList.getOutEdges(1), Arrays.asList(edge3));
-		try {
-			checkIfSameElements(edgeList.getOutEdges(2), null);
-			fail();
-		} catch (RuntimeException e) {
-		}
-		checkIfSameElements(edgeList.getOutEdges(3), new ArrayList<StreamEdge>());
-
-		checkIfSameElements(edgeList.getInEdges(1), new ArrayList<StreamEdge>());
-		try {
-			checkIfSameElements(edgeList.getInEdges(2), null);
-			fail();
-		} catch (RuntimeException e) {
-		}
-		checkIfSameElements(edgeList.getInEdges(3), Arrays.asList(edge3));
-	}
-
-	private <T> void checkIfSameElements(List<T> expected, List<T> result) {
-		assertEquals(new HashSet<T>(expected), new HashSet<T>(result));
-	}
-}
\ No newline at end of file
diff --git a/flink-staging/flink-streaming/flink-streaming-scala/src/main/scala/org/apache/flink/streaming/api/scala/StreamCrossOperator.scala b/flink-staging/flink-streaming/flink-streaming-scala/src/main/scala/org/apache/flink/streaming/api/scala/StreamCrossOperator.scala
index a6b87115465..a7b34809936 100644
--- a/flink-staging/flink-streaming/flink-streaming-scala/src/main/scala/org/apache/flink/streaming/api/scala/StreamCrossOperator.scala
+++ b/flink-staging/flink-streaming/flink-streaming-scala/src/main/scala/org/apache/flink/streaming/api/scala/StreamCrossOperator.scala
@@ -99,8 +99,8 @@ object StreamCrossOperator {
     }
 
     override def every(length: Long): CrossWindow[I1, I2] = {
-      val builder = javaStream.getExecutionEnvironment().getStreamGraph()
-      val invokable = builder.getInvokable(javaStream.getId())
+      val graph = javaStream.getExecutionEnvironment().getStreamGraph()
+      val invokable = graph.getVertex(javaStream.getId()).getInvokable()
       invokable.asInstanceOf[CoWindowInvokable[_,_,_]].setSlideSize(length)
       this
     }
