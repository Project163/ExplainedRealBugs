diff --git a/flink-connectors/flink-jdbc/pom.xml b/flink-connectors/flink-jdbc/pom.xml
index 5cbd8c3f19e..8549e6046e2 100644
--- a/flink-connectors/flink-jdbc/pom.xml
+++ b/flink-connectors/flink-jdbc/pom.xml
@@ -75,6 +75,14 @@ under the License.
 			<scope>test</scope>
 		</dependency>
 
+		<dependency>
+			<groupId>org.apache.flink</groupId>
+			<artifactId>flink-table-planner_${scala.binary.version}</artifactId>
+			<version>${project.version}</version>
+			<type>test-jar</type>
+			<scope>test</scope>
+		</dependency>
+
 		<dependency>
 			<groupId>org.apache.flink</groupId>
 			<artifactId>flink-table-planner-blink_${scala.binary.version}</artifactId>
@@ -84,7 +92,7 @@ under the License.
 
 		<dependency>
 			<groupId>org.apache.flink</groupId>
-			<artifactId>flink-table-planner_${scala.binary.version}</artifactId>
+			<artifactId>flink-table-planner-blink_${scala.binary.version}</artifactId>
 			<version>${project.version}</version>
 			<type>test-jar</type>
 			<scope>test</scope>
diff --git a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java
index e5c876fe2dd..bbd5561465b 100644
--- a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java
+++ b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java
@@ -18,7 +18,6 @@
 
 package org.apache.flink.api.java.io.jdbc;
 
-import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.api.java.io.jdbc.dialect.JDBCDialect;
 import org.apache.flink.api.java.io.jdbc.split.NumericBetweenParametersProvider;
 import org.apache.flink.api.java.typeutils.RowTypeInfo;
@@ -31,6 +30,7 @@ import org.apache.flink.table.sources.LookupableTableSource;
 import org.apache.flink.table.sources.ProjectableTableSource;
 import org.apache.flink.table.sources.StreamTableSource;
 import org.apache.flink.table.sources.TableSource;
+import org.apache.flink.table.types.DataType;
 import org.apache.flink.table.utils.TableConnectorUtils;
 import org.apache.flink.types.Row;
 
@@ -38,6 +38,7 @@ import java.util.Arrays;
 import java.util.Objects;
 
 import static org.apache.flink.api.java.io.jdbc.JDBCTypeUtil.normalizeTableSchema;
+import static org.apache.flink.table.types.utils.TypeConversions.fromDataTypeToLegacyInfo;
 import static org.apache.flink.util.Preconditions.checkNotNull;
 
 /**
@@ -55,7 +56,7 @@ public class JDBCTableSource implements
 
 	// index of fields selected, null means that all fields are selected
 	private final int[] selectFields;
-	private final RowTypeInfo returnType;
+	private final DataType producedDataType;
 
 	private JDBCTableSource(
 		JDBCOptions options, JDBCReadOptions readOptions, JDBCLookupOptions lookupOptions, TableSchema schema) {
@@ -72,18 +73,19 @@ public class JDBCTableSource implements
 
 		this.selectFields = selectFields;
 
-		final TypeInformation<?>[] schemaTypeInfos = schema.getFieldTypes();
+		final DataType[] schemaDataTypes = schema.getFieldDataTypes();
 		final String[] schemaFieldNames = schema.getFieldNames();
 		if (selectFields != null) {
-			TypeInformation<?>[] typeInfos = new TypeInformation[selectFields.length];
-			String[] typeNames = new String[selectFields.length];
+			DataType[] dataTypes = new DataType[selectFields.length];
+			String[] fieldNames = new String[selectFields.length];
 			for (int i = 0; i < selectFields.length; i++) {
-				typeInfos[i] = schemaTypeInfos[selectFields[i]];
-				typeNames[i] = schemaFieldNames[selectFields[i]];
+				dataTypes[i] = schemaDataTypes[selectFields[i]];
+				fieldNames[i] = schemaFieldNames[selectFields[i]];
 			}
-			this.returnType = new RowTypeInfo(typeInfos, typeNames);
+			this.producedDataType =
+					TableSchema.builder().fields(fieldNames, dataTypes).build().toRowDataType();
 		} else {
-			this.returnType = new RowTypeInfo(schemaTypeInfos, schemaFieldNames);
+			this.producedDataType = schema.toRowDataType();
 		}
 	}
 
@@ -94,23 +96,28 @@ public class JDBCTableSource implements
 
 	@Override
 	public DataStream<Row> getDataStream(StreamExecutionEnvironment execEnv) {
-		return execEnv.createInput(getInputFormat(), getReturnType()).name(explainSource());
+		return execEnv
+				.createInput(
+						getInputFormat(),
+						(RowTypeInfo) fromDataTypeToLegacyInfo(producedDataType))
+				.name(explainSource());
 	}
 
 	@Override
 	public TableFunction<Row> getLookupFunction(String[] lookupKeys) {
+		final RowTypeInfo rowTypeInfo = (RowTypeInfo) fromDataTypeToLegacyInfo(producedDataType);
 		return JDBCLookupFunction.builder()
 				.setOptions(options)
 				.setLookupOptions(lookupOptions)
-				.setFieldTypes(returnType.getFieldTypes())
-				.setFieldNames(returnType.getFieldNames())
+				.setFieldTypes(rowTypeInfo.getFieldTypes())
+				.setFieldNames(rowTypeInfo.getFieldNames())
 				.setKeyNames(lookupKeys)
 				.build();
 	}
 
 	@Override
-	public TypeInformation<Row> getReturnType() {
-		return returnType;
+	public DataType getProducedDataType() {
+		return producedDataType;
 	}
 
 	@Override
@@ -135,7 +142,8 @@ public class JDBCTableSource implements
 
 	@Override
 	public String explainSource() {
-		return TableConnectorUtils.generateRuntimeName(getClass(), returnType.getFieldNames());
+		final RowTypeInfo rowTypeInfo = (RowTypeInfo) fromDataTypeToLegacyInfo(producedDataType);
+		return TableConnectorUtils.generateRuntimeName(getClass(), rowTypeInfo.getFieldNames());
 	}
 
 	public static Builder builder() {
@@ -143,12 +151,13 @@ public class JDBCTableSource implements
 	}
 
 	private JDBCInputFormat getInputFormat() {
+		final RowTypeInfo rowTypeInfo = (RowTypeInfo) fromDataTypeToLegacyInfo(producedDataType);
 		JDBCInputFormat.JDBCInputFormatBuilder builder = JDBCInputFormat.buildJDBCInputFormat()
 				.setDrivername(options.getDriverName())
 				.setDBUrl(options.getDbURL())
 				.setUsername(options.getUsername())
 				.setPassword(options.getPassword())
-				.setRowTypeInfo(new RowTypeInfo(returnType.getFieldTypes(), returnType.getFieldNames()));
+				.setRowTypeInfo(new RowTypeInfo(rowTypeInfo.getFieldTypes(), rowTypeInfo.getFieldNames()));
 
 		if (readOptions.getFetchSize() != 0) {
 			builder.setFetchSize(readOptions.getFetchSize());
@@ -156,7 +165,7 @@ public class JDBCTableSource implements
 
 		final JDBCDialect dialect = options.getDialect();
 		String query = dialect.getSelectFromStatement(
-			options.getTableName(), returnType.getFieldNames(), new String[0]);
+			options.getTableName(), rowTypeInfo.getFieldNames(), new String[0]);
 		if (readOptions.getPartitionColumnName().isPresent()) {
 			long lowerBound = readOptions.getPartitionLowerBound().get();
 			long upperBound = readOptions.getPartitionUpperBound().get();
diff --git a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialect.java b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialect.java
index 8c53eaceec8..5dba839edd6 100644
--- a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialect.java
+++ b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialect.java
@@ -18,6 +18,9 @@
 
 package org.apache.flink.api.java.io.jdbc.dialect;
 
+import org.apache.flink.table.api.TableSchema;
+import org.apache.flink.table.api.ValidationException;
+
 import java.io.Serializable;
 import java.util.Arrays;
 import java.util.Optional;
@@ -35,6 +38,14 @@ public interface JDBCDialect extends Serializable {
 	 */
 	boolean canHandle(String url);
 
+	/**
+	 * Check if this dialect instance support a specific data type in table schema.
+	 * @param schema the table schema.
+	 * @exception ValidationException in case of the table schema contains unsupported type.
+	 */
+	default void validate(TableSchema schema) throws ValidationException {
+	}
+
 	/**
 	 * @return the default driver class name, if user not configure the driver class name,
 	 * then will use this one.
diff --git a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java
index 9334ec27c71..ec97416d181 100644
--- a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java
+++ b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java
@@ -18,6 +18,14 @@
 
 package org.apache.flink.api.java.io.jdbc.dialect;
 
+import org.apache.flink.table.api.TableSchema;
+import org.apache.flink.table.api.ValidationException;
+import org.apache.flink.table.types.DataType;
+import org.apache.flink.table.types.logical.DecimalType;
+import org.apache.flink.table.types.logical.LogicalTypeRoot;
+import org.apache.flink.table.types.logical.TimestampType;
+import org.apache.flink.table.types.logical.VarBinaryType;
+
 import java.util.Arrays;
 import java.util.List;
 import java.util.Optional;
@@ -46,10 +54,89 @@ public final class JDBCDialects {
 		return Optional.empty();
 	}
 
-	private static class DerbyDialect implements JDBCDialect {
+	private abstract static class AbstractDialect implements JDBCDialect {
+
+		@Override
+		public void validate(TableSchema schema) throws ValidationException {
+			for (int i = 0; i < schema.getFieldCount(); i++) {
+				DataType dt = schema.getFieldDataType(i).get();
+				String fieldName = schema.getFieldName(i).get();
+
+				// TODO: We can't convert VARBINARY(n) data type to
+				//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter
+				//  when n is smaller than Integer.MAX_VALUE
+				if (unsupportedTypes().contains(dt.getLogicalType().getTypeRoot()) ||
+						(dt.getLogicalType() instanceof VarBinaryType
+							&& Integer.MAX_VALUE != ((VarBinaryType) dt.getLogicalType()).getLength())) {
+					throw new ValidationException(
+							String.format("The %s dialect doesn't support type: %s.",
+									dialectName(),
+									dt.toString()));
+				}
+
+				// only validate precision of DECIMAL type for blink planner
+				if (dt.getLogicalType() instanceof DecimalType) {
+					int precision = ((DecimalType) dt.getLogicalType()).getPrecision();
+					if (precision > maxDecimalPrecision()
+							|| precision < minDecimalPrecision()) {
+						throw new ValidationException(
+								String.format("The precision of field '%s' is out of the DECIMAL " +
+												"precision range [%d, %d] supported by %s dialect.",
+										fieldName,
+										minDecimalPrecision(),
+										maxDecimalPrecision(),
+										dialectName()));
+					}
+				}
+
+				// only validate precision of DECIMAL type for blink planner
+				if (dt.getLogicalType() instanceof TimestampType) {
+					int precision = ((TimestampType) dt.getLogicalType()).getPrecision();
+					if (precision > maxTimestampPrecision()
+							|| precision < minTimestampPrecision()) {
+						throw new ValidationException(
+								String.format("The precision of field '%s' is out of the TIMESTAMP " +
+												"precision range [%d, %d] supported by %s dialect.",
+										fieldName,
+										minTimestampPrecision(),
+										maxTimestampPrecision(),
+										dialectName()));
+					}
+				}
+			}
+		}
+
+		public abstract String dialectName();
+
+		public abstract int maxDecimalPrecision();
+
+		public abstract int minDecimalPrecision();
+
+		public abstract int maxTimestampPrecision();
+
+		public abstract int minTimestampPrecision();
+
+		/**
+		 * Defines the unsupported types for the dialect.
+		 * @return a list of logical type roots.
+		 */
+		public abstract List<LogicalTypeRoot> unsupportedTypes();
+	}
+
+	private static class DerbyDialect extends AbstractDialect {
 
 		private static final long serialVersionUID = 1L;
 
+		// Define MAX/MIN precision of TIMESTAMP type according to derby docs:
+		// http://db.apache.org/derby/docs/10.14/ref/rrefsqlj27620.html
+		private static final int MAX_TIMESTAMP_PRECISION = 9;
+		private static final int MIN_TIMESTAMP_PRECISION = 1;
+
+		// Define MAX/MIN precision of DECIMAL type according to derby docs:
+		// http://db.apache.org/derby/docs/10.14/ref/rrefsqlj15260.html
+		private static final int MAX_DECIMAL_PRECISION = 31;
+		private static final int MIN_DECIMAL_PRECISION = 1;
+
 		@Override
 		public boolean canHandle(String url) {
 			return url.startsWith("jdbc:derby:");
@@ -64,12 +151,72 @@ public final class JDBCDialects {
 		public String quoteIdentifier(String identifier) {
 			return identifier;
 		}
+
+		@Override
+		public String dialectName() {
+			return "derby";
+		}
+
+		@Override
+		public int maxDecimalPrecision() {
+			return MAX_DECIMAL_PRECISION;
+		}
+
+		@Override
+		public int minDecimalPrecision() {
+			return MIN_DECIMAL_PRECISION;
+		}
+
+		@Override
+		public int maxTimestampPrecision() {
+			return MAX_TIMESTAMP_PRECISION;
+		}
+
+		@Override
+		public int minTimestampPrecision() {
+			return MIN_TIMESTAMP_PRECISION;
+		}
+
+		@Override
+		public List<LogicalTypeRoot> unsupportedTypes() {
+			// The data types used in Derby are list at
+			// http://db.apache.org/derby/docs/10.14/ref/crefsqlj31068.html
+
+			// TODO: We can't convert BINARY data type to
+			//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter.
+			return Arrays.asList(
+					LogicalTypeRoot.BINARY,
+					LogicalTypeRoot.TIMESTAMP_WITH_LOCAL_TIME_ZONE,
+					LogicalTypeRoot.TIMESTAMP_WITH_TIME_ZONE,
+					LogicalTypeRoot.INTERVAL_YEAR_MONTH,
+					LogicalTypeRoot.INTERVAL_DAY_TIME,
+					LogicalTypeRoot.ARRAY,
+					LogicalTypeRoot.MULTISET,
+					LogicalTypeRoot.MAP,
+					LogicalTypeRoot.ROW,
+					LogicalTypeRoot.DISTINCT_TYPE,
+					LogicalTypeRoot.STRUCTURED_TYPE,
+					LogicalTypeRoot.NULL,
+					LogicalTypeRoot.RAW,
+					LogicalTypeRoot.SYMBOL,
+					LogicalTypeRoot.UNRESOLVED);
+		}
 	}
 
-	private static class MySQLDialect implements JDBCDialect {
+	private static class MySQLDialect extends AbstractDialect {
 
 		private static final long serialVersionUID = 1L;
 
+		// Define MAX/MIN precision of TIMESTAMP type according to Mysql docs:
+		// https://dev.mysql.com/doc/refman/8.0/en/fractional-seconds.html
+		private static final int MAX_TIMESTAMP_PRECISION = 6;
+		private static final int MIN_TIMESTAMP_PRECISION = 1;
+
+		// Define MAX/MIN precision of DECIMAL type according to Mysql docs:
+		// https://dev.mysql.com/doc/refman/8.0/en/fixed-point-types.html
+		private static final int MAX_DECIMAL_PRECISION = 65;
+		private static final int MIN_DECIMAL_PRECISION = 1;
+
 		@Override
 		public boolean canHandle(String url) {
 			return url.startsWith("jdbc:mysql:");
@@ -101,12 +248,73 @@ public final class JDBCDialects {
 					" ON DUPLICATE KEY UPDATE " + updateClause
 			);
 		}
+
+		@Override
+		public String dialectName() {
+			return "mysql";
+		}
+
+		@Override
+		public int maxDecimalPrecision() {
+			return MAX_DECIMAL_PRECISION;
+		}
+
+		@Override
+		public int minDecimalPrecision() {
+			return MIN_DECIMAL_PRECISION;
+		}
+
+		@Override
+		public int maxTimestampPrecision() {
+			return MAX_TIMESTAMP_PRECISION;
+		}
+
+		@Override
+		public int minTimestampPrecision() {
+			return MIN_TIMESTAMP_PRECISION;
+		}
+
+		@Override
+		public List<LogicalTypeRoot> unsupportedTypes() {
+			// The data types used in Mysql are list at:
+			// https://dev.mysql.com/doc/refman/8.0/en/data-types.html
+
+			// TODO: We can't convert BINARY data type to
+			//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter.
+			return Arrays.asList(
+					LogicalTypeRoot.BINARY,
+					LogicalTypeRoot.TIMESTAMP_WITH_LOCAL_TIME_ZONE,
+					LogicalTypeRoot.TIMESTAMP_WITH_TIME_ZONE,
+					LogicalTypeRoot.INTERVAL_YEAR_MONTH,
+					LogicalTypeRoot.INTERVAL_DAY_TIME,
+					LogicalTypeRoot.ARRAY,
+					LogicalTypeRoot.MULTISET,
+					LogicalTypeRoot.MAP,
+					LogicalTypeRoot.ROW,
+					LogicalTypeRoot.DISTINCT_TYPE,
+					LogicalTypeRoot.STRUCTURED_TYPE,
+					LogicalTypeRoot.NULL,
+					LogicalTypeRoot.RAW,
+					LogicalTypeRoot.SYMBOL,
+					LogicalTypeRoot.UNRESOLVED
+			);
+		}
 	}
 
-	private static class PostgresDialect implements JDBCDialect {
+	private static class PostgresDialect extends AbstractDialect {
 
 		private static final long serialVersionUID = 1L;
 
+		// Define MAX/MIN precision of TIMESTAMP type according to PostgreSQL docs:
+		// https://www.postgresql.org/docs/12/datatype-datetime.html
+		private static final int MAX_TIMESTAMP_PRECISION = 6;
+		private static final int MIN_TIMESTAMP_PRECISION = 1;
+
+		// Define MAX/MIN precision of TIMESTAMP type according to PostgreSQL docs:
+		// https://www.postgresql.org/docs/12/datatype-numeric.html#DATATYPE-NUMERIC-DECIMAL
+		private static final int MAX_DECIMAL_PRECISION = 1000;
+		private static final int MIN_DECIMAL_PRECISION = 1;
+
 		@Override
 		public boolean canHandle(String url) {
 			return url.startsWith("jdbc:postgresql:");
@@ -133,5 +341,56 @@ public final class JDBCDialects {
 							" DO UPDATE SET " + updateClause
 			);
 		}
+
+		@Override
+		public String dialectName() {
+			return "postgresql";
+		}
+
+		@Override
+		public int maxDecimalPrecision() {
+			return MAX_DECIMAL_PRECISION;
+		}
+
+		@Override
+		public int minDecimalPrecision() {
+			return MIN_DECIMAL_PRECISION;
+		}
+
+		@Override
+		public int maxTimestampPrecision() {
+			return MAX_TIMESTAMP_PRECISION;
+		}
+
+		@Override
+		public int minTimestampPrecision() {
+			return MIN_TIMESTAMP_PRECISION;
+		}
+
+		@Override
+		public List<LogicalTypeRoot> unsupportedTypes() {
+			// The data types used in PostgreSQL are list at:
+			// https://www.postgresql.org/docs/12/datatype.html
+
+			// TODO: We can't convert BINARY data type to
+			//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter.
+			return Arrays.asList(
+					LogicalTypeRoot.BINARY,
+					LogicalTypeRoot.TIMESTAMP_WITH_LOCAL_TIME_ZONE,
+					LogicalTypeRoot.TIMESTAMP_WITH_TIME_ZONE,
+					LogicalTypeRoot.INTERVAL_YEAR_MONTH,
+					LogicalTypeRoot.INTERVAL_DAY_TIME,
+					LogicalTypeRoot.MULTISET,
+					LogicalTypeRoot.MAP,
+					LogicalTypeRoot.ROW,
+					LogicalTypeRoot.DISTINCT_TYPE,
+					LogicalTypeRoot.STRUCTURED_TYPE,
+					LogicalTypeRoot.NULL,
+					LogicalTypeRoot.RAW,
+					LogicalTypeRoot.SYMBOL,
+					LogicalTypeRoot.UNRESOLVED
+			);
+
+		}
 	}
 }
diff --git a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/table/descriptors/JDBCValidator.java b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/table/descriptors/JDBCValidator.java
index b0a7c181d8f..8485f83809c 100644
--- a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/table/descriptors/JDBCValidator.java
+++ b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/table/descriptors/JDBCValidator.java
@@ -21,10 +21,14 @@ package org.apache.flink.table.descriptors;
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.api.java.io.jdbc.dialect.JDBCDialect;
 import org.apache.flink.api.java.io.jdbc.dialect.JDBCDialects;
+import org.apache.flink.table.api.TableSchema;
+import org.apache.flink.table.utils.TableSchemaUtils;
 import org.apache.flink.util.Preconditions;
 
 import java.util.Optional;
 
+import static org.apache.flink.table.descriptors.Schema.SCHEMA;
+
 /**
  * The validator for JDBC.
  */
@@ -73,6 +77,9 @@ public class JDBCValidator extends ConnectorDescriptorValidator {
 		final Optional<JDBCDialect> dialect = JDBCDialects.get(url);
 		Preconditions.checkState(dialect.isPresent(), "Cannot handle such jdbc url: " + url);
 
+		TableSchema schema = TableSchemaUtils.getPhysicalSchema(properties.getTableSchema(SCHEMA));
+		dialect.get().validate(schema);
+
 		Optional<String> password = properties.getOptionalString(CONNECTOR_PASSWORD);
 		if (password.isPresent()) {
 			Preconditions.checkArgument(
diff --git a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCDataTypeTest.java b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCDataTypeTest.java
new file mode 100644
index 00000000000..9f3ff5339c6
--- /dev/null
+++ b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCDataTypeTest.java
@@ -0,0 +1,195 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.api.java.io.jdbc;
+
+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
+import org.apache.flink.table.api.EnvironmentSettings;
+import org.apache.flink.table.api.ValidationException;
+import org.apache.flink.table.api.java.StreamTableEnvironment;
+
+import org.junit.Assert;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
+
+import javax.annotation.Nullable;
+
+import java.util.Arrays;
+import java.util.List;
+
+/**
+ * Tests for all DataTypes and Dialects of JDBC connector.
+ */
+@RunWith(Parameterized.class)
+public class JDBCDataTypeTest {
+
+	private static final String DDL_FORMAT = "CREATE TABLE T(\n" +
+		"f0 %s\n" +
+		") WITH (\n" +
+		"  'connector.type'='jdbc',\n" +
+		"  'connector.url'='" + "jdbc:%s:memory:test" + "',\n" +
+		"  'connector.table'='myTable'\n" +
+		")";
+
+	@Parameterized.Parameters(name = "{index}: {0}")
+	public static List<TestItem> testData() {
+		return Arrays.asList(
+			createTestItem("derby", "CHAR"),
+			createTestItem("derby", "VARCHAR"),
+			createTestItem("derby", "BOOLEAN"),
+			createTestItem("derby", "TINYINT"),
+			createTestItem("derby", "SMALLINT"),
+			createTestItem("derby", "INTEGER"),
+			createTestItem("derby", "BIGINT"),
+			createTestItem("derby", "FLOAT"),
+			createTestItem("derby", "DOUBLE"),
+			createTestItem("derby", "DECIMAL(10, 4)"),
+			createTestItem("derby", "DATE"),
+			createTestItem("derby", "TIME"),
+			createTestItem("derby", "TIMESTAMP(3)"),
+			createTestItem("derby", "TIMESTAMP WITHOUT TIME ZONE"),
+			createTestItem("derby", "TIMESTAMP(9) WITHOUT TIME ZONE"),
+			createTestItem("derby", "VARBINARY"),
+
+			createTestItem("mysql", "CHAR"),
+			createTestItem("mysql", "VARCHAR"),
+			createTestItem("mysql", "BOOLEAN"),
+			createTestItem("mysql", "TINYINT"),
+			createTestItem("mysql", "SMALLINT"),
+			createTestItem("mysql", "INTEGER"),
+			createTestItem("mysql", "BIGINT"),
+			createTestItem("mysql", "FLOAT"),
+			createTestItem("mysql", "DOUBLE"),
+			createTestItem("mysql", "DECIMAL(10, 4)"),
+			createTestItem("mysql", "DECIMAL(38, 18)"),
+			createTestItem("mysql", "DATE"),
+			createTestItem("mysql", "TIME"),
+			createTestItem("mysql", "TIMESTAMP(3)"),
+			createTestItem("mysql", "TIMESTAMP WITHOUT TIME ZONE"),
+			createTestItem("mysql", "VARBINARY"),
+
+			createTestItem("postgresql", "CHAR"),
+			createTestItem("postgresql", "VARCHAR"),
+			createTestItem("postgresql", "BOOLEAN"),
+			createTestItem("postgresql", "TINYINT"),
+			createTestItem("postgresql", "SMALLINT"),
+			createTestItem("postgresql", "INTEGER"),
+			createTestItem("postgresql", "BIGINT"),
+			createTestItem("postgresql", "FLOAT"),
+			createTestItem("postgresql", "DOUBLE"),
+			createTestItem("postgresql", "DECIMAL(10, 4)"),
+			createTestItem("postgresql", "DECIMAL(38, 18)"),
+			createTestItem("postgresql", "DATE"),
+			createTestItem("postgresql", "TIME"),
+			createTestItem("postgresql", "TIMESTAMP(3)"),
+			createTestItem("postgresql", "TIMESTAMP WITHOUT TIME ZONE"),
+			createTestItem("postgresql", "VARBINARY"),
+			createTestItem("postgresql", "ARRAY<INTEGER>"),
+
+			// Unsupported types throws errors.
+			createTestItem("derby", "BINARY", "The derby dialect doesn't support type: BINARY(1)."),
+			createTestItem("derby", "VARBINARY(10)", "The derby dialect doesn't support type: VARBINARY(10)."),
+			createTestItem("derby", "TIMESTAMP(3) WITH LOCAL TIME ZONE",
+					"The derby dialect doesn't support type: TIMESTAMP(3) WITH LOCAL TIME ZONE."),
+			createTestItem("derby", "DECIMAL(38, 18)",
+					"The precision of field 'f0' is out of the DECIMAL precision range [1, 31] supported by derby dialect."),
+
+			createTestItem("mysql", "BINARY", "The mysql dialect doesn't support type: BINARY(1)."),
+			createTestItem("mysql", "VARBINARY(10)", "The mysql dialect doesn't support type: VARBINARY(10)."),
+			createTestItem("mysql", "TIMESTAMP(9) WITHOUT TIME ZONE",
+					"The precision of field 'f0' is out of the TIMESTAMP precision range [1, 6] supported by mysql dialect."),
+			createTestItem("mysql", "TIMESTAMP(3) WITH LOCAL TIME ZONE",
+					"The mysql dialect doesn't support type: TIMESTAMP(3) WITH LOCAL TIME ZONE."),
+
+			createTestItem("postgresql", "BINARY", "The postgresql dialect doesn't support type: BINARY(1)."),
+			createTestItem("postgresql", "VARBINARY(10)", "The postgresql dialect doesn't support type: VARBINARY(10)."),
+			createTestItem("postgresql", "TIMESTAMP(9) WITHOUT TIME ZONE",
+					"The precision of field 'f0' is out of the TIMESTAMP precision range [1, 6] supported by postgresql dialect."),
+			createTestItem("postgresql", "TIMESTAMP(3) WITH LOCAL TIME ZONE",
+					"The postgresql dialect doesn't support type: TIMESTAMP(3) WITH LOCAL TIME ZONE.")
+		);
+	}
+
+	private static TestItem createTestItem(Object... args) {
+		assert args.length >= 2;
+		TestItem item = TestItem.fromDialetAndType((String) args[0], (String) args[1]);
+		if (args.length == 3) {
+			item.withExpectError((String) args[2]);
+		}
+		return item;
+	}
+
+	@Parameterized.Parameter
+	public TestItem testItem;
+
+	@Test
+	public void testDataTypeValidate() {
+		String sqlDDL = String.format(DDL_FORMAT, testItem.dataTypeExpr, testItem.dialect);
+
+		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
+		EnvironmentSettings envSettings = EnvironmentSettings.newInstance()
+				.useBlinkPlanner()
+				.inStreamingMode()
+				.build();
+		StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);
+
+		tEnv.sqlUpdate(sqlDDL);
+
+		if (testItem.expectError != null) {
+			try {
+				tEnv.sqlQuery("SELECT * FROM T");
+			} catch (Exception ex) {
+				Assert.assertTrue(ex.getCause() instanceof ValidationException);
+				Assert.assertEquals(testItem.expectError, ex.getCause().getMessage());
+			}
+		} else {
+			tEnv.sqlQuery("SELECT * FROM T");
+		}
+	}
+
+	//~ Inner Class
+	private static class TestItem {
+
+		private final String dialect;
+
+		private final String dataTypeExpr;
+
+		@Nullable
+		private String expectError;
+
+		private TestItem(String dialect, String dataTypeExpr) {
+			this.dialect = dialect;
+			this.dataTypeExpr = dataTypeExpr;
+		}
+
+		static TestItem fromDialetAndType(String dialect, String dataTypeExpr) {
+			return new TestItem(dialect, dataTypeExpr);
+		}
+
+		TestItem withExpectError(String expectError) {
+			this.expectError = expectError;
+			return this;
+		}
+
+		@Override
+		public String toString() {
+			return String.format("Dialect: %s, DataType: %s", dialect, dataTypeExpr);
+		}
+	}
+}
diff --git a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java
index a605b12d02e..5c8c4a31bdb 100644
--- a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java
+++ b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java
@@ -18,93 +18,139 @@
 
 package org.apache.flink.api.java.io.jdbc;
 
-import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 import org.apache.flink.table.api.EnvironmentSettings;
-import org.apache.flink.table.api.Table;
 import org.apache.flink.table.api.java.StreamTableEnvironment;
 import org.apache.flink.table.runtime.utils.StreamITCase;
+import org.apache.flink.test.util.AbstractTestBase;
 import org.apache.flink.types.Row;
 
-import org.junit.BeforeClass;
+import org.junit.After;
+import org.junit.Before;
 import org.junit.Test;
 
-import java.util.ArrayList;
+import java.sql.Connection;
+import java.sql.DriverManager;
+import java.sql.SQLException;
+import java.sql.Statement;
+import java.util.Arrays;
 import java.util.List;
 
+
 /**
- * IT case for {@link JDBCTableSource}.
+ * ITCase for {@link JDBCTableSource}.
  */
-public class JDBCTableSourceITCase extends JDBCTestBase {
-
-	private static final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
-	private static final EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();
-	private static final StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, bsSettings);
-
-	static final String TABLE_SOURCE_SQL = "CREATE TABLE books (" +
-		" id int, " +
-		" title varchar, " +
-		" author varchar, " +
-		" price double, " +
-		" qty int " +
-		") with (" +
-		" 'connector.type' = 'jdbc', " +
-		" 'connector.url' = 'jdbc:derby:memory:ebookshop', " +
-		" 'connector.table' = 'books', " +
-		" 'connector.driver' = 'org.apache.derby.jdbc.EmbeddedDriver' " +
-		")";
-
-	@BeforeClass
-	public static void createTable() {
-		tEnv.sqlUpdate(TABLE_SOURCE_SQL);
+public class JDBCTableSourceITCase extends AbstractTestBase {
+
+	public static final String DRIVER_CLASS = "org.apache.derby.jdbc.EmbeddedDriver";
+	public static final String DB_URL = "jdbc:derby:memory:test";
+	public static final String INPUT_TABLE = "jdbcSource";
+
+	@Before
+	public void before() throws ClassNotFoundException, SQLException {
+		System.setProperty("derby.stream.error.field", JDBCTestBase.class.getCanonicalName() + ".DEV_NULL");
+		Class.forName(DRIVER_CLASS);
+
+		try (
+			Connection conn = DriverManager.getConnection(DB_URL + ";create=true");
+			Statement statement = conn.createStatement()) {
+			statement.executeUpdate("CREATE TABLE " + INPUT_TABLE + " (" +
+					"id BIGINT NOT NULL," +
+					"timestamp6_col TIMESTAMP, " +
+					"timestamp9_col TIMESTAMP, " +
+					"time_col TIME, " +
+					"real_col FLOAT(23), " +    // A precision of 23 or less makes FLOAT equivalent to REAL.
+					"double_col FLOAT(24)," +   // A precision of 24 or greater makes FLOAT equivalent to DOUBLE PRECISION.
+					"decimal_col DECIMAL(10, 4))");
+			statement.executeUpdate("INSERT INTO " + INPUT_TABLE + " VALUES (" +
+					"1, TIMESTAMP('2020-01-01 15:35:00.123456'), TIMESTAMP('2020-01-01 15:35:00.123456789'), " +
+					"TIME('15:35:00'), 1.175E-37, 1.79769E+308, 100.1234)");
+			statement.executeUpdate("INSERT INTO " + INPUT_TABLE + " VALUES (" +
+					"2, TIMESTAMP('2020-01-01 15:36:01.123456'), TIMESTAMP('2020-01-01 15:36:01.123456789'), " +
+					"TIME('15:36:01'), -1.175E-37, -1.79769E+308, 101.1234)");
+		}
+	}
+
+	@After
+	public void clearOutputTable() throws Exception {
+		Class.forName(DRIVER_CLASS);
+		try (
+			Connection conn = DriverManager.getConnection(DB_URL);
+			Statement stat = conn.createStatement()) {
+			stat.executeUpdate("DROP TABLE " + INPUT_TABLE);
+		}
 	}
 
 	@Test
-	public void testFieldsProjection() throws Exception {
-		StreamITCase.clear();
+	public void testJDBCSource() throws Exception {
+		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
+		EnvironmentSettings envSettings = EnvironmentSettings.newInstance()
+			.useBlinkPlanner()
+			.inStreamingMode()
+			.build();
+		StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);
+
+		tEnv.sqlUpdate(
+			"CREATE TABLE " + INPUT_TABLE + "(" +
+				"id BIGINT," +
+				"timestamp6_col TIMESTAMP(6)," +
+				"timestamp9_col TIMESTAMP(9)," +
+				"time_col TIME," +
+				"real_col FLOAT," +
+				"double_col DOUBLE," +
+				"decimal_col DECIMAL(10, 4)" +
+				") WITH (" +
+				"  'connector.type'='jdbc'," +
+				"  'connector.url'='" + DB_URL + "'," +
+				"  'connector.table'='" + INPUT_TABLE + "'" +
+				")"
+		);
 
-		Table result = tEnv.sqlQuery(SELECT_ID_BOOKS);
-		DataStream<Row> resultSet = tEnv.toAppendStream(result, Row.class);
-		resultSet.addSink(new StreamITCase.StringSink<>());
+		StreamITCase.clear();
+		tEnv.toAppendStream(tEnv.sqlQuery("SELECT * FROM " + INPUT_TABLE), Row.class)
+			.addSink(new StreamITCase.StringSink<>());
 		env.execute();
 
-		List<String> expected = new ArrayList<>();
-		expected.add("1001");
-		expected.add("1002");
-		expected.add("1003");
-		expected.add("1004");
-		expected.add("1005");
-		expected.add("1006");
-		expected.add("1007");
-		expected.add("1008");
-		expected.add("1009");
-		expected.add("1010");
-
+		List<String> expected =
+			Arrays.asList(
+				"1,2020-01-01T15:35:00.123456,2020-01-01T15:35:00.123456789,15:35,1.175E-37,1.79769E308,100.1234",
+				"2,2020-01-01T15:36:01.123456,2020-01-01T15:36:01.123456789,15:36:01,-1.175E-37,-1.79769E308,101.1234");
 		StreamITCase.compareWithList(expected);
 	}
 
 	@Test
-	public void testAllFieldsSelection() throws Exception {
-		StreamITCase.clear();
+	public void testProjectableJDBCSource() throws Exception {
+		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
+		EnvironmentSettings envSettings = EnvironmentSettings.newInstance()
+				.useBlinkPlanner()
+				.inStreamingMode()
+				.build();
+		StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);
+
+		tEnv.sqlUpdate(
+			"CREATE TABLE " + INPUT_TABLE + "(" +
+				"id BIGINT," +
+				"timestamp6_col TIMESTAMP(6)," +
+				"timestamp9_col TIMESTAMP(9)," +
+				"time_col TIME," +
+				"real_col FLOAT," +
+				"decimal_col DECIMAL(10, 4)" +
+				") WITH (" +
+				"  'connector.type'='jdbc'," +
+				"  'connector.url'='" + DB_URL + "'," +
+				"  'connector.table'='" + INPUT_TABLE + "'" +
+				")"
+		);
 
-		Table result = tEnv.sqlQuery(SELECT_ALL_BOOKS);
-		DataStream<Row> resultSet = tEnv.toAppendStream(result, Row.class);
-		resultSet.addSink(new StreamITCase.StringSink<>());
+		StreamITCase.clear();
+		tEnv.toAppendStream(tEnv.sqlQuery("SELECT timestamp6_col, decimal_col FROM " + INPUT_TABLE), Row.class)
+				.addSink(new StreamITCase.StringSink<>());
 		env.execute();
 
-		List<String> expected = new ArrayList<>();
-		expected.add("1001,Java public for dummies,Tan Ah Teck,11.11,11");
-		expected.add("1002,More Java for dummies,Tan Ah Teck,22.22,22");
-		expected.add("1003,More Java for more dummies,Mohammad Ali,33.33,33");
-		expected.add("1004,A Cup of Java,Kumar,44.44,44");
-		expected.add("1005,A Teaspoon of Java,Kevin Jones,55.55,55");
-		expected.add("1006,A Teaspoon of Java 1.4,Kevin Jones,66.66,66");
-		expected.add("1007,A Teaspoon of Java 1.5,Kevin Jones,77.77,77");
-		expected.add("1008,A Teaspoon of Java 1.6,Kevin Jones,88.88,88");
-		expected.add("1009,A Teaspoon of Java 1.7,Kevin Jones,99.99,99");
-		expected.add("1010,A Teaspoon of Java 1.8,Kevin Jones,null,1010");
-
+		List<String> expected =
+			Arrays.asList(
+				"2020-01-01T15:35:00.123456,100.1234",
+				"2020-01-01T15:36:01.123456,101.1234");
 		StreamITCase.compareWithList(expected);
 	}
-
 }
diff --git a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceSinkFactoryTest.java b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceSinkFactoryTest.java
index b10f05c30f4..c76485d2a8c 100644
--- a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceSinkFactoryTest.java
+++ b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceSinkFactoryTest.java
@@ -52,7 +52,7 @@ public class JDBCTableSourceSinkFactoryTest {
 		.field("aaa", DataTypes.INT())
 		.field("bbb", DataTypes.STRING())
 		.field("ccc", DataTypes.DOUBLE())
-		.field("ddd", DataTypes.DECIMAL(38, 18))
+		.field("ddd", DataTypes.DECIMAL(31, 18))
 		.field("eee", DataTypes.TIMESTAMP(3))
 		.build();
 
