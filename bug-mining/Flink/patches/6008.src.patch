diff --git a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/filesystem/LimitableBulkFormat.java b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/filesystem/LimitableBulkFormat.java
index 5780e6f9cb0..3a5f183bf48 100644
--- a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/filesystem/LimitableBulkFormat.java
+++ b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/filesystem/LimitableBulkFormat.java
@@ -18,6 +18,7 @@
 
 package org.apache.flink.table.filesystem;
 
+import org.apache.flink.annotation.VisibleForTesting;
 import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.connector.file.src.FileSourceSplit;
@@ -41,6 +42,10 @@ public class LimitableBulkFormat<T, SplitT extends FileSourceSplit>
     /**
      * Limit the total number of records read by this format. When the limit is reached, subsequent
      * readers will no longer read data.
+     *
+     * <p>LIMIT N would produce a list of N records totally. But the source can produce more. There
+     * are limiters in the downstream operator. The globalNumberRead is just for the total limit, as
+     * soon as someone has reached its limit, the source can be ended.
      */
     @Nullable private transient AtomicLong globalNumberRead;
 
@@ -61,6 +66,11 @@ public class LimitableBulkFormat<T, SplitT extends FileSourceSplit>
         return wrapReader(reader);
     }
 
+    @VisibleForTesting
+    AtomicLong globalNumberRead() {
+        return globalNumberRead;
+    }
+
     private synchronized Reader<T> wrapReader(Reader<T> reader) {
         if (globalNumberRead == null) {
             globalNumberRead = new AtomicLong(0);
@@ -105,8 +115,19 @@ public class LimitableBulkFormat<T, SplitT extends FileSourceSplit>
                 return null;
             }
 
-            RecordIterator<T> batch = reader.readBatch();
-            return batch == null ? null : new LimitableIterator(batch);
+            try {
+                RecordIterator<T> batch = reader.readBatch();
+                return batch == null ? null : new LimitableIterator(batch);
+            } catch (Exception e) {
+                // ignore any exception if someone reached limit
+                // Theoretically, once the limit bar is reached, whatever problems are encountered
+                // subsequently, the required task has been completed
+                if (reachLimit()) {
+                    return null;
+                } else {
+                    throw e;
+                }
+            }
         }
 
         @Override
diff --git a/flink-table/flink-table-runtime/src/test/java/org/apache/flink/table/filesystem/LimitableBulkFormatTest.java b/flink-table/flink-table-runtime/src/test/java/org/apache/flink/table/filesystem/LimitableBulkFormatTest.java
index 04cab3a9791..48945973120 100644
--- a/flink-table/flink-table-runtime/src/test/java/org/apache/flink/table/filesystem/LimitableBulkFormatTest.java
+++ b/flink-table/flink-table-runtime/src/test/java/org/apache/flink/table/filesystem/LimitableBulkFormatTest.java
@@ -18,22 +18,29 @@
 
 package org.apache.flink.table.filesystem;
 
+import org.apache.flink.api.common.typeinfo.TypeInformation;
+import org.apache.flink.api.common.typeinfo.Types;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.configuration.MemorySize;
 import org.apache.flink.connector.file.src.FileSourceSplit;
 import org.apache.flink.connector.file.src.impl.StreamFormatAdapter;
 import org.apache.flink.connector.file.src.reader.BulkFormat;
+import org.apache.flink.connector.file.src.reader.SimpleStreamFormat;
 import org.apache.flink.connector.file.src.reader.StreamFormat;
 import org.apache.flink.connector.file.src.reader.TextLineFormat;
 import org.apache.flink.connector.file.src.util.Utils;
+import org.apache.flink.core.fs.FSDataInputStream;
 import org.apache.flink.core.fs.Path;
 import org.apache.flink.util.FileUtils;
 
 import org.junit.Assert;
+import org.junit.Before;
 import org.junit.ClassRule;
 import org.junit.Test;
 import org.junit.rules.TemporaryFolder;
 
+import javax.annotation.Nullable;
+
 import java.io.File;
 import java.io.IOException;
 import java.util.concurrent.atomic.AtomicInteger;
@@ -43,17 +50,21 @@ public class LimitableBulkFormatTest {
 
     @ClassRule public static final TemporaryFolder TEMP_FOLDER = new TemporaryFolder();
 
-    @Test
-    public void test() throws IOException {
-        // prepare file
-        File file = TEMP_FOLDER.newFile();
+    private File file;
+
+    @Before
+    public void prepare() throws IOException {
+        file = TEMP_FOLDER.newFile();
         file.createNewFile();
         StringBuilder builder = new StringBuilder();
         for (int i = 0; i < 10000; i++) {
             builder.append(i).append("\n");
         }
         FileUtils.writeFileUtf8(file, builder.toString());
+    }
 
+    @Test
+    public void test() throws IOException {
         // read
         BulkFormat<String, FileSourceSplit> format =
                 LimitableBulkFormat.create(new StreamFormatAdapter<>(new TextLineFormat()), 22L);
@@ -76,15 +87,6 @@ public class LimitableBulkFormatTest {
 
     @Test
     public void testLimitOverBatches() throws IOException {
-        // prepare file
-        File file = TEMP_FOLDER.newFile();
-        file.createNewFile();
-        StringBuilder builder = new StringBuilder();
-        for (int i = 0; i < 10000; i++) {
-            builder.append(i).append("\n");
-        }
-        FileUtils.writeFileUtf8(file, builder.toString());
-
         // set limit
         Long limit = 2048L;
 
@@ -112,4 +114,49 @@ public class LimitableBulkFormatTest {
         Utils.forEachRemaining(reader, s -> i.incrementAndGet());
         Assert.assertEquals(limit.intValue(), i.get());
     }
+
+    @Test
+    public void testSwallowExceptionWhenLimited() throws IOException {
+        long limit = 1000L;
+        LimitableBulkFormat<String, FileSourceSplit> format =
+                (LimitableBulkFormat<String, FileSourceSplit>)
+                        LimitableBulkFormat.create(
+                                new StreamFormatAdapter<>(new FailedFormat()), limit);
+
+        BulkFormat.Reader<String> reader =
+                format.createReader(
+                        new Configuration(),
+                        new FileSourceSplit("id", new Path(file.toURI()), 0, file.length()));
+
+        format.globalNumberRead().set(limit + 1);
+
+        // should swallow exception
+        reader.readBatch();
+    }
+
+    private static class FailedFormat extends SimpleStreamFormat<String> {
+
+        @Override
+        public FailedReader createReader(Configuration config, FSDataInputStream stream)
+                throws IOException {
+            return new FailedReader();
+        }
+
+        @Override
+        public TypeInformation<String> getProducedType() {
+            return Types.STRING;
+        }
+    }
+
+    private static final class FailedReader implements StreamFormat.Reader<String> {
+
+        @Nullable
+        @Override
+        public String read() throws IOException {
+            throw new RuntimeException();
+        }
+
+        @Override
+        public void close() throws IOException {}
+    }
 }
