diff --git a/flink-connectors/flink-connector-files/src/main/java/org/apache/flink/connector/file/table/batch/BatchSink.java b/flink-connectors/flink-connector-files/src/main/java/org/apache/flink/connector/file/table/batch/BatchSink.java
index 839a6c466b5..43f49a7910b 100644
--- a/flink-connectors/flink-connector-files/src/main/java/org/apache/flink/connector/file/table/batch/BatchSink.java
+++ b/flink-connectors/flink-connector-files/src/main/java/org/apache/flink/connector/file/table/batch/BatchSink.java
@@ -52,6 +52,11 @@ import java.util.LinkedHashMap;
 /** Helper for creating batch file sink. */
 @Internal
 public class BatchSink {
+
+    public static final String COORDINATOR_OP_NAME = "compact-coordinator";
+
+    public static final String COMPACT_OP_NAME = "compact-operator";
+
     private BatchSink() {}
 
     public static DataStreamSink<Row> createBatchNoCompactSink(
@@ -100,14 +105,14 @@ public class BatchSink {
         SingleOutputStreamOperator<CompactMessages.CompactOutput> transform =
                 dataStream
                         .transform(
-                                "coordinator",
+                                COORDINATOR_OP_NAME,
                                 TypeInformation.of(CompactMessages.CoordinatorOutput.class),
                                 new BatchCompactCoordinator(
                                         fsSupplier, compactAverageSize, compactTargetSize))
                         .setParallelism(1)
                         .setMaxParallelism(1)
                         .transform(
-                                "compact",
+                                COMPACT_OP_NAME,
                                 TypeInformation.of(CompactMessages.CompactOutput.class),
                                 new BatchCompactOperator<>(fsSupplier, readFactory, writerFactory));
         transform
diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java
index d90de292ddc..4d936ac63de 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java
@@ -123,6 +123,8 @@ public class HiveTableSink implements DynamicTableSink, SupportsPartitioning, Su
 
     private static final Logger LOG = LoggerFactory.getLogger(HiveTableSink.class);
 
+    public static final String BATCH_COMPACT_WRITER_OP_NAME = "batch_writer";
+
     private final boolean fallbackMappedReader;
     private final boolean fallbackMappedWriter;
     private final JobConf jobConf;
@@ -384,6 +386,13 @@ public class HiveTableSink implements DynamicTableSink, SupportsPartitioning, Su
             Optional<Integer> compactParallelismOptional =
                     conf.getOptional(FileSystemConnectorOptions.COMPACTION_PARALLELISM);
             int compactParallelism = compactParallelismOptional.orElse(sinkParallelism);
+            boolean compactParallelismConfigured =
+                    compactParallelismOptional.isPresent()
+                            ||
+                            // if only sink parallelism is set, compact operator should follow this
+                            // setting. that means its parallelism equals to sink and marked as
+                            // configured to disable auto parallelism inference.
+                            sinkParallelismConfigured;
             return createBatchCompactSink(
                     dataStream,
                     converter,
@@ -403,7 +412,7 @@ public class HiveTableSink implements DynamicTableSink, SupportsPartitioning, Su
                     sinkParallelism,
                     compactParallelism,
                     sinkParallelismConfigured,
-                    compactParallelismOptional.isPresent());
+                    compactParallelismConfigured);
         } else {
             return createBatchNoCompactSink(
                     dataStream,
@@ -470,7 +479,7 @@ public class HiveTableSink implements DynamicTableSink, SupportsPartitioning, Su
 
         DataStream<CoordinatorInput> writerDataStream =
                 map.transform(
-                        "batch_compact_writer",
+                        BATCH_COMPACT_WRITER_OP_NAME,
                         TypeInformation.of(CoordinatorInput.class),
                         new BatchFileWriter<>(
                                 fsFactory,
diff --git a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableCompactSinkParallelismTest.java b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableCompactSinkParallelismTest.java
new file mode 100644
index 00000000000..5d3e01fccf3
--- /dev/null
+++ b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableCompactSinkParallelismTest.java
@@ -0,0 +1,191 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connectors.hive;
+
+import org.apache.flink.api.dag.Transformation;
+import org.apache.flink.connector.file.table.batch.BatchSink;
+import org.apache.flink.table.api.SqlDialect;
+import org.apache.flink.table.api.TableEnvironment;
+import org.apache.flink.table.api.internal.TableEnvironmentImpl;
+import org.apache.flink.table.catalog.hive.HiveCatalog;
+import org.apache.flink.table.catalog.hive.HiveTestUtils;
+import org.apache.flink.table.operations.ModifyOperation;
+import org.apache.flink.table.operations.Operation;
+import org.apache.flink.table.planner.delegation.PlannerBase;
+import org.apache.flink.util.TestLoggerExtension;
+
+import org.junit.jupiter.api.AfterEach;
+import org.junit.jupiter.api.BeforeEach;
+import org.junit.jupiter.api.Test;
+import org.junit.jupiter.api.extension.ExtendWith;
+
+import java.util.Collections;
+import java.util.List;
+
+import static org.apache.flink.connector.file.table.FileSystemConnectorOptions.COMPACTION_PARALLELISM;
+import static org.apache.flink.connector.file.table.FileSystemConnectorOptions.SINK_PARALLELISM;
+import static org.assertj.core.api.Assertions.assertThat;
+
+/** Tests to verify operator's parallelism of {@link HiveTableSink} enabled auto-compaction. */
+@ExtendWith(TestLoggerExtension.class)
+class HiveTableCompactSinkParallelismTest {
+    /**
+     * Represents the parallelism doesn't need to be checked, it should follow the setting of planer
+     * or auto inference.
+     */
+    public static final int NO_NEED_TO_CHECK_PARALLELISM = -1;
+
+    private HiveCatalog catalog;
+
+    private TableEnvironment tableEnv;
+
+    @BeforeEach
+    void before() {
+        catalog = HiveTestUtils.createHiveCatalog();
+        catalog.open();
+        tableEnv = HiveTestUtils.createTableEnvInBatchMode(SqlDialect.HIVE);
+        tableEnv.registerCatalog(catalog.getName(), catalog);
+        tableEnv.useCatalog(catalog.getName());
+    }
+
+    @AfterEach
+    void after() {
+        if (catalog != null) {
+            catalog.close();
+        }
+    }
+
+    /** If only sink parallelism is set, compact operator should follow this setting. */
+    @Test
+    void testOnlySetSinkParallelism() {
+        final int sinkParallelism = 4;
+
+        tableEnv.executeSql(
+                String.format(
+                        "CREATE TABLE src ("
+                                + " key string,"
+                                + " value string"
+                                + ") TBLPROPERTIES ("
+                                + " 'auto-compaction' = 'true', "
+                                + " '%s' = '%s' )",
+                        SINK_PARALLELISM.key(), sinkParallelism));
+
+        assertSinkAndCompactOperatorParallelism(true, true, sinkParallelism, sinkParallelism);
+    }
+
+    @Test
+    void testOnlySetCompactParallelism() {
+        final int compactParallelism = 4;
+
+        tableEnv.executeSql(
+                String.format(
+                        "CREATE TABLE src ("
+                                + " key string,"
+                                + " value string"
+                                + ") TBLPROPERTIES ("
+                                + " 'auto-compaction' = 'true', "
+                                + " '%s' = '%s' )",
+                        COMPACTION_PARALLELISM.key(), compactParallelism));
+
+        assertSinkAndCompactOperatorParallelism(
+                false, true, NO_NEED_TO_CHECK_PARALLELISM, compactParallelism);
+    }
+
+    @Test
+    void testSetBothSinkAndCompactParallelism() {
+        final int sinkParallelism = 8;
+        final int compactParallelism = 4;
+
+        tableEnv.executeSql(
+                String.format(
+                        "CREATE TABLE src ("
+                                + " key string,"
+                                + " value string"
+                                + ") TBLPROPERTIES ("
+                                + " 'auto-compaction' = 'true', "
+                                + " '%s' = '%s', "
+                                + " '%s' = '%s' )",
+                        SINK_PARALLELISM.key(),
+                        sinkParallelism,
+                        COMPACTION_PARALLELISM.key(),
+                        compactParallelism));
+
+        assertSinkAndCompactOperatorParallelism(true, true, sinkParallelism, compactParallelism);
+    }
+
+    @Test
+    void testSinkAndCompactAllNotSetParallelism() {
+        tableEnv.executeSql(
+                "CREATE TABLE src ("
+                        + " key string,"
+                        + " value string"
+                        + ") TBLPROPERTIES ("
+                        + " 'auto-compaction' = 'true' )");
+        assertSinkAndCompactOperatorParallelism(
+                false, false, NO_NEED_TO_CHECK_PARALLELISM, NO_NEED_TO_CHECK_PARALLELISM);
+    }
+
+    private void assertSinkAndCompactOperatorParallelism(
+            boolean isSinkParallelismConfigured,
+            boolean isCompactParallelismConfigured,
+            int expectedSinkParallelism,
+            int expectedCompactParallelism) {
+        String statement = "insert into src values ('k1', 'v1'), ('k2', 'v2');";
+        PlannerBase planner = (PlannerBase) ((TableEnvironmentImpl) tableEnv).getPlanner();
+        planner.getExecEnv().setParallelism(10);
+        List<Operation> operations = planner.getParser().parse(statement);
+        List<Transformation<?>> transformations =
+                planner.translate(Collections.singletonList((ModifyOperation) (operations.get(0))));
+        assertThat(transformations).hasSize(1);
+        Transformation<?> rootTransformation = transformations.get(0);
+        Transformation<?> compactTransformation =
+                findTransformationByName(rootTransformation, BatchSink.COMPACT_OP_NAME);
+        Transformation<?> hiveSinkTransformation =
+                findTransformationByName(
+                        rootTransformation, HiveTableSink.BATCH_COMPACT_WRITER_OP_NAME);
+        assertThat(hiveSinkTransformation.isParallelismConfigured())
+                .isEqualTo(isSinkParallelismConfigured);
+        assertThat(compactTransformation.isParallelismConfigured())
+                .isEqualTo(isCompactParallelismConfigured);
+        if (expectedSinkParallelism != NO_NEED_TO_CHECK_PARALLELISM) {
+            assertThat(hiveSinkTransformation.getParallelism()).isEqualTo(expectedSinkParallelism);
+        }
+        if (expectedCompactParallelism != NO_NEED_TO_CHECK_PARALLELISM) {
+            assertThat(compactTransformation.getParallelism())
+                    .isEqualTo(expectedCompactParallelism);
+        }
+    }
+
+    /**
+     * This method will recursively look forward to the transformation whose name meets the
+     * requirements. For simplicity, let's assume that each transformation has at most one input.
+     */
+    private static Transformation<?> findTransformationByName(
+            Transformation<?> transformation, String name) {
+        if (transformation == null) {
+            return null;
+        }
+
+        if (transformation.getName().equals(name)) {
+            return transformation;
+        }
+
+        return findTransformationByName(transformation.getInputs().get(0), name);
+    }
+}
