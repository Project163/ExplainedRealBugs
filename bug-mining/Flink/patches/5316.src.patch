diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/connectors/DynamicSinkUtils.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/connectors/DynamicSinkUtils.java
index 59703498cbd..6dbd42393bc 100644
--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/connectors/DynamicSinkUtils.java
+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/connectors/DynamicSinkUtils.java
@@ -54,6 +54,7 @@ import org.apache.flink.table.types.utils.TypeConversions;
 
 import org.apache.calcite.plan.RelOptUtil;
 import org.apache.calcite.rel.RelNode;
+import org.apache.calcite.rel.hint.RelHint;
 import org.apache.calcite.rel.type.RelDataType;
 import org.apache.calcite.rex.RexBuilder;
 import org.apache.calcite.rex.RexNode;
@@ -142,6 +143,7 @@ public final class DynamicSinkUtils {
         return convertSinkToRel(
                 relBuilder,
                 input,
+                Collections.emptyMap(),
                 externalModifyOperation.getTableIdentifier(),
                 Collections.emptyMap(),
                 false,
@@ -162,6 +164,7 @@ public final class DynamicSinkUtils {
         return convertSinkToRel(
                 relBuilder,
                 input,
+                sinkModifyOperation.getDynamicOptions(),
                 sinkModifyOperation.getTableIdentifier(),
                 sinkModifyOperation.getStaticPartitions(),
                 sinkModifyOperation.isOverwrite(),
@@ -172,6 +175,7 @@ public final class DynamicSinkUtils {
     private static RelNode convertSinkToRel(
             FlinkRelBuilder relBuilder,
             RelNode input,
+            Map<String, String> dynamicOptions,
             ObjectIdentifier sinkIdentifier,
             Map<String, String> staticPartitions,
             boolean isOverwrite,
@@ -201,10 +205,15 @@ public final class DynamicSinkUtils {
             pushMetadataProjection(relBuilder, typeFactory, schema, sink);
         }
 
+        List<RelHint> hints = new ArrayList<>();
+        if (!dynamicOptions.isEmpty()) {
+            hints.add(RelHint.builder("OPTIONS").hintOptions(dynamicOptions).build());
+        }
         final RelNode finalQuery = relBuilder.build();
 
         return LogicalSink.create(
                 finalQuery,
+                hints,
                 sinkIdentifier,
                 table,
                 sink,
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/calcite/RelTimeIndicatorConverter.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/calcite/RelTimeIndicatorConverter.scala
index e0b0648107a..848e071208c 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/calcite/RelTimeIndicatorConverter.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/calcite/RelTimeIndicatorConverter.scala
@@ -186,6 +186,7 @@ class RelTimeIndicatorConverter(rexBuilder: RexBuilder) extends RelShuttle {
         sink.getCluster,
         sink.getTraitSet,
         newInput,
+        sink.hints,
         sink.tableIdentifier,
         sink.catalogTable,
         sink.tableSink,
@@ -198,6 +199,7 @@ class RelTimeIndicatorConverter(rexBuilder: RexBuilder) extends RelShuttle {
         sink.getCluster,
         sink.getTraitSet,
         newInput,
+        sink.hints,
         sink.sink,
         sink.sinkName,
         sink.catalogTable,
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/delegation/PlannerBase.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/delegation/PlannerBase.scala
index 58122cce3f8..8a0902bd6d8 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/delegation/PlannerBase.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/delegation/PlannerBase.scala
@@ -63,6 +63,8 @@ import java.lang.{Long => JLong}
 import java.util
 import java.util.TimeZone
 
+import org.apache.calcite.rel.hint.RelHint
+
 import _root_.scala.collection.JavaConversions._
 
 /**
@@ -216,8 +218,13 @@ abstract class PlannerBase(
               catalogSink.getTableIdentifier,
               dataTypeFactory,
               getTypeFactory)
+            val hints = new util.ArrayList[RelHint]
+            if (!dynamicOptions.isEmpty) {
+              hints.add(RelHint.builder("OPTIONS").hintOptions(dynamicOptions).build)
+            }
             LogicalLegacySink.create(
               query,
+              hints,
               sink,
               identifier.toString,
               table,
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/calcite/LegacySink.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/calcite/LegacySink.scala
index 8326370fa82..a2f11a848b2 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/calcite/LegacySink.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/calcite/LegacySink.scala
@@ -19,13 +19,17 @@
 package org.apache.flink.table.planner.plan.nodes.calcite
 
 import org.apache.flink.table.planner.calcite.FlinkTypeFactory
+import org.apache.flink.table.planner.plan.utils.RelExplainUtil
 import org.apache.flink.table.runtime.types.LogicalTypeDataTypeConverter.fromDataTypeToLogicalType
 import org.apache.flink.table.sinks.TableSink
 
 import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}
 import org.apache.calcite.rel.`type`.RelDataType
+import org.apache.calcite.rel.hint.RelHint
 import org.apache.calcite.rel.{RelNode, RelWriter, SingleRel}
 
+import java.util
+
 /**
   * Relational expression that writes out data of input node into a [[TableSink]].
   *
@@ -39,6 +43,7 @@ abstract class LegacySink(
     cluster: RelOptCluster,
     traitSet: RelTraitSet,
     input: RelNode,
+    val hints: util.List[RelHint],
     val sink: TableSink[_],
     val sinkName: String)
   extends SingleRel(cluster, traitSet, input) {
@@ -53,6 +58,7 @@ abstract class LegacySink(
     super.explainTerms(pw)
       .itemIf("name", sinkName, sinkName != null)
       .item("fields", sink.getTableSchema.getFieldNames.mkString(", "))
+      .itemIf("hints", RelExplainUtil.hintsToString(hints), !hints.isEmpty)
   }
 
 }
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/calcite/LogicalLegacySink.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/calcite/LogicalLegacySink.scala
index c185eb683e7..668149e61fc 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/calcite/LogicalLegacySink.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/calcite/LogicalLegacySink.scala
@@ -23,6 +23,7 @@ import org.apache.flink.table.sinks.TableSink
 
 import org.apache.calcite.plan.{Convention, RelOptCluster, RelTraitSet}
 import org.apache.calcite.rel.RelNode
+import org.apache.calcite.rel.hint.RelHint
 
 import java.util
 
@@ -37,15 +38,16 @@ final class LogicalLegacySink(
     cluster: RelOptCluster,
     traitSet: RelTraitSet,
     input: RelNode,
+    hints: util.List[RelHint],
     sink: TableSink[_],
     sinkName: String,
     val catalogTable: CatalogTable,
     val staticPartitions: Map[String, String])
-  extends LegacySink(cluster, traitSet, input, sink, sinkName) {
+  extends LegacySink(cluster, traitSet, input, hints, sink, sinkName) {
 
   override def copy(traitSet: RelTraitSet, inputs: util.List[RelNode]): RelNode = {
     new LogicalLegacySink(
-      cluster, traitSet, inputs.head, sink, sinkName, catalogTable, staticPartitions)
+      cluster, traitSet, inputs.head, hints, sink, sinkName, catalogTable, staticPartitions)
   }
 
 }
@@ -53,12 +55,21 @@ final class LogicalLegacySink(
 object LogicalLegacySink {
 
   def create(input: RelNode,
+      hints: util.List[RelHint],
       sink: TableSink[_],
       sinkName: String,
-      catalogTable: CatalogTable = null,
-      staticPartitions: Map[String, String] = Map()): LogicalLegacySink = {
+      catalogTable: CatalogTable,
+      staticPartitions: Map[String, String]): LogicalLegacySink = {
     val traits = input.getCluster.traitSetOf(Convention.NONE)
     new LogicalLegacySink(
-      input.getCluster, traits, input, sink, sinkName, catalogTable, staticPartitions)
+      input.getCluster, traits, input, hints, sink, sinkName, catalogTable, staticPartitions)
+  }
+
+  def create(input: RelNode,
+      sink: TableSink[_],
+      sinkName: String,
+      catalogTable: CatalogTable = null,
+      staticPartitions: Map[String, String] = Map()): LogicalLegacySink = {
+    create(input, util.Collections.emptyList(), sink, sinkName, catalogTable, staticPartitions)
   }
 }
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/calcite/LogicalSink.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/calcite/LogicalSink.scala
index f039b65910b..cadb54b7407 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/calcite/LogicalSink.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/calcite/LogicalSink.scala
@@ -24,6 +24,7 @@ import org.apache.flink.table.planner.plan.abilities.sink.SinkAbilitySpec
 
 import org.apache.calcite.plan.{Convention, RelOptCluster, RelTraitSet}
 import org.apache.calcite.rel.RelNode
+import org.apache.calcite.rel.hint.RelHint
 
 import java.util
 
@@ -38,18 +39,20 @@ final class LogicalSink(
     cluster: RelOptCluster,
     traitSet: RelTraitSet,
     input: RelNode,
+    hints: util.List[RelHint],
     tableIdentifier: ObjectIdentifier,
     catalogTable: ResolvedCatalogTable,
     tableSink: DynamicTableSink,
     val staticPartitions: Map[String, String],
     val abilitySpecs: Array[SinkAbilitySpec])
-  extends Sink(cluster, traitSet, input, tableIdentifier, catalogTable, tableSink) {
+  extends Sink(cluster, traitSet, input, hints, tableIdentifier, catalogTable, tableSink) {
 
   override def copy(traitSet: RelTraitSet, inputs: util.List[RelNode]): RelNode = {
     new LogicalSink(
       cluster,
       traitSet,
       inputs.head,
+      hints,
       tableIdentifier,
       catalogTable,
       tableSink,
@@ -62,6 +65,7 @@ object LogicalSink {
 
   def create(
       input: RelNode,
+      hints: util.List[RelHint],
       tableIdentifier: ObjectIdentifier,
       catalogTable: ResolvedCatalogTable,
       tableSink: DynamicTableSink,
@@ -72,6 +76,7 @@ object LogicalSink {
       input.getCluster,
       traits,
       input,
+      hints,
       tableIdentifier,
       catalogTable,
       tableSink,
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/calcite/Sink.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/calcite/Sink.scala
index d8bea60dcd5..df33c0397a0 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/calcite/Sink.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/calcite/Sink.scala
@@ -18,31 +18,35 @@
 
 package org.apache.flink.table.planner.plan.nodes.calcite
 
-import org.apache.flink.table.catalog.{CatalogTable, ObjectIdentifier, ResolvedCatalogTable}
+import org.apache.flink.table.catalog.{ObjectIdentifier, ResolvedCatalogTable}
 import org.apache.flink.table.connector.sink.DynamicTableSink
-import org.apache.flink.table.planner.calcite.FlinkTypeFactory
-import org.apache.flink.table.runtime.types.LogicalTypeDataTypeConverter.fromDataTypeToLogicalType
+import org.apache.flink.table.planner.plan.utils.RelExplainUtil
 
 import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}
 import org.apache.calcite.rel.`type`.RelDataType
+import org.apache.calcite.rel.hint.RelHint
 import org.apache.calcite.rel.{RelNode, RelWriter, SingleRel}
 
+import java.util
+
 import scala.collection.JavaConversions._
 
 /**
-  * Relational expression that writes out data of input node into a [[DynamicTableSink]].
-  *
-  * @param cluster  cluster that this relational expression belongs to
-  * @param traitSet the traits of this rel
-  * @param input    input relational expression
- *  @param tableIdentifier the full path of the table to retrieve.
- *  @param catalogTable Resolved catalog table where this table source table comes from
- *  @param tableSink the [[DynamicTableSink]] for which to write into
-  */
+ * Relational expression that writes out data of input node into a [[DynamicTableSink]].
+ *
+ * @param cluster  cluster that this relational expression belongs to
+ * @param traitSet the traits of this rel
+ * @param hints    the hints
+ * @param input    input relational expression
+ * @param tableIdentifier the full path of the table to retrieve.
+ * @param catalogTable Resolved catalog table where this table source table comes from
+ * @param tableSink the [[DynamicTableSink]] for which to write into
+ */
 abstract class Sink(
     cluster: RelOptCluster,
     traitSet: RelTraitSet,
     input: RelNode,
+    val hints: util.List[RelHint],
     val tableIdentifier: ObjectIdentifier,
     val catalogTable: ResolvedCatalogTable,
     val tableSink: DynamicTableSink)
@@ -56,5 +60,7 @@ abstract class Sink(
     super.explainTerms(pw)
       .item("table", tableIdentifier.asSummaryString())
       .item("fields", getRowType.getFieldNames.mkString(", "))
+      .itemIf("hints", RelExplainUtil.hintsToString(hints), !hints.isEmpty)
   }
 }
+
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/logical/FlinkLogicalLegacySink.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/logical/FlinkLogicalLegacySink.scala
index e338c6cf61b..843c6a7318a 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/logical/FlinkLogicalLegacySink.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/logical/FlinkLogicalLegacySink.scala
@@ -20,12 +20,13 @@ package org.apache.flink.table.planner.plan.nodes.logical
 
 import org.apache.flink.table.catalog.CatalogTable
 import org.apache.flink.table.planner.plan.nodes.FlinkConventions
-import org.apache.flink.table.planner.plan.nodes.calcite.{LogicalLegacySink, LegacySink}
+import org.apache.flink.table.planner.plan.nodes.calcite.{LegacySink, LogicalLegacySink}
 import org.apache.flink.table.sinks.TableSink
 
 import org.apache.calcite.plan.{Convention, RelOptCluster, RelOptRule, RelTraitSet}
 import org.apache.calcite.rel.RelNode
 import org.apache.calcite.rel.convert.ConverterRule
+import org.apache.calcite.rel.hint.RelHint
 
 import java.util
 
@@ -39,16 +40,17 @@ class FlinkLogicalLegacySink(
     cluster: RelOptCluster,
     traitSet: RelTraitSet,
     input: RelNode,
+    hints: util.List[RelHint],
     sink: TableSink[_],
     sinkName: String,
     val catalogTable: CatalogTable,
     val staticPartitions: Map[String, String])
-  extends LegacySink(cluster, traitSet, input, sink, sinkName)
+  extends LegacySink(cluster, traitSet, input, hints, sink, sinkName)
   with FlinkLogicalRel {
 
   override def copy(traitSet: RelTraitSet, inputs: util.List[RelNode]): RelNode = {
     new FlinkLogicalLegacySink(
-      cluster, traitSet, inputs.head, sink, sinkName, catalogTable, staticPartitions)
+      cluster, traitSet, inputs.head, hints, sink, sinkName, catalogTable, staticPartitions)
   }
 
 }
@@ -65,6 +67,7 @@ private class FlinkLogicalLegacySinkConverter
     val newInput = RelOptRule.convert(sink.getInput, FlinkConventions.LOGICAL)
     FlinkLogicalLegacySink.create(
       newInput,
+      sink.hints,
       sink.sink,
       sink.sinkName,
       sink.catalogTable,
@@ -77,6 +80,7 @@ object FlinkLogicalLegacySink {
 
   def create(
       input: RelNode,
+      hints: util.List[RelHint],
       sink: TableSink[_],
       sinkName: String,
       catalogTable: CatalogTable = null,
@@ -84,6 +88,6 @@ object FlinkLogicalLegacySink {
     val cluster = input.getCluster
     val traitSet = cluster.traitSetOf(FlinkConventions.LOGICAL).simplify()
     new FlinkLogicalLegacySink(
-      cluster, traitSet, input, sink, sinkName, catalogTable, staticPartitions)
+      cluster, traitSet, input, hints, sink, sinkName, catalogTable, staticPartitions)
   }
 }
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/logical/FlinkLogicalSink.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/logical/FlinkLogicalSink.scala
index 6cf43eae007..6f614c68391 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/logical/FlinkLogicalSink.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/logical/FlinkLogicalSink.scala
@@ -27,6 +27,7 @@ import org.apache.flink.table.planner.plan.nodes.calcite.{LogicalSink, Sink}
 import org.apache.calcite.plan.{Convention, RelOptCluster, RelOptRule, RelTraitSet}
 import org.apache.calcite.rel.RelNode
 import org.apache.calcite.rel.convert.ConverterRule
+import org.apache.calcite.rel.hint.RelHint
 
 import java.util
 
@@ -40,12 +41,13 @@ class FlinkLogicalSink(
     cluster: RelOptCluster,
     traitSet: RelTraitSet,
     input: RelNode,
+    hints: util.List[RelHint],
     tableIdentifier: ObjectIdentifier,
     catalogTable: ResolvedCatalogTable,
     tableSink: DynamicTableSink,
     val staticPartitions: Map[String, String],
     val abilitySpecs: Array[SinkAbilitySpec])
-  extends Sink(cluster, traitSet, input, tableIdentifier, catalogTable, tableSink)
+  extends Sink(cluster, traitSet, input, hints, tableIdentifier, catalogTable, tableSink)
   with FlinkLogicalRel {
 
   override def copy(traitSet: RelTraitSet, inputs: util.List[RelNode]): RelNode = {
@@ -53,6 +55,7 @@ class FlinkLogicalSink(
       cluster,
       traitSet,
       inputs.head,
+      hints,
       tableIdentifier,
       catalogTable,
       tableSink,
@@ -74,6 +77,7 @@ private class FlinkLogicalSinkConverter
     val newInput = RelOptRule.convert(sink.getInput, FlinkConventions.LOGICAL)
     FlinkLogicalSink.create(
       newInput,
+      sink.hints,
       sink.tableIdentifier,
       sink.catalogTable,
       sink.tableSink,
@@ -87,6 +91,7 @@ object FlinkLogicalSink {
 
   def create(
       input: RelNode,
+      hints: util.List[RelHint],
       tableIdentifier: ObjectIdentifier,
       catalogTable: ResolvedCatalogTable,
       tableSink: DynamicTableSink,
@@ -98,6 +103,7 @@ object FlinkLogicalSink {
       cluster,
       traitSet,
       input,
+      hints,
       tableIdentifier,
       catalogTable,
       tableSink,
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchPhysicalLegacySink.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchPhysicalLegacySink.scala
index 73b07d54ded..fcdb53a9591 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchPhysicalLegacySink.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchPhysicalLegacySink.scala
@@ -20,13 +20,14 @@ package org.apache.flink.table.planner.plan.nodes.physical.batch
 
 import org.apache.flink.table.planner.plan.nodes.calcite.LegacySink
 import org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecLegacySink
-import org.apache.flink.table.planner.plan.nodes.exec.{InputProperty, ExecNode}
+import org.apache.flink.table.planner.plan.nodes.exec.{ExecNode, InputProperty}
 import org.apache.flink.table.planner.plan.utils.UpdatingPlanChecker
 import org.apache.flink.table.runtime.types.LogicalTypeDataTypeConverter.fromDataTypeToLogicalType
 import org.apache.flink.table.sinks.{TableSink, UpsertStreamTableSink}
 
 import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}
 import org.apache.calcite.rel.RelNode
+import org.apache.calcite.rel.hint.RelHint
 
 import java.util
 
@@ -39,13 +40,14 @@ class BatchPhysicalLegacySink[T](
     cluster: RelOptCluster,
     traitSet: RelTraitSet,
     inputRel: RelNode,
+    hints: util.List[RelHint],
     sink: TableSink[T],
     sinkName: String)
-  extends LegacySink(cluster, traitSet, inputRel, sink, sinkName)
+  extends LegacySink(cluster, traitSet, inputRel, hints, sink, sinkName)
   with BatchPhysicalRel {
 
   override def copy(traitSet: RelTraitSet, inputs: util.List[RelNode]): RelNode = {
-    new BatchPhysicalLegacySink(cluster, traitSet, inputs.get(0), sink, sinkName)
+    new BatchPhysicalLegacySink(cluster, traitSet, inputs.get(0), hints, sink, sinkName)
   }
 
   override def translateToExecNode(): ExecNode[_] = {
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchPhysicalSink.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchPhysicalSink.scala
index 1c98d5a1e9f..d0266fa5d39 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchPhysicalSink.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchPhysicalSink.scala
@@ -30,6 +30,7 @@ import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil
 
 import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}
 import org.apache.calcite.rel.RelNode
+import org.apache.calcite.rel.hint.RelHint
 
 import java.util
 
@@ -41,16 +42,24 @@ class BatchPhysicalSink(
     cluster: RelOptCluster,
     traitSet: RelTraitSet,
     inputRel: RelNode,
+    hints: util.List[RelHint],
     tableIdentifier: ObjectIdentifier,
     catalogTable: ResolvedCatalogTable,
     tableSink: DynamicTableSink,
     abilitySpecs: Array[SinkAbilitySpec])
-  extends Sink(cluster, traitSet, inputRel, tableIdentifier, catalogTable, tableSink)
+  extends Sink(cluster, traitSet, inputRel, hints, tableIdentifier, catalogTable, tableSink)
   with BatchPhysicalRel {
 
   override def copy(traitSet: RelTraitSet, inputs: util.List[RelNode]): RelNode = {
     new BatchPhysicalSink(
-      cluster, traitSet, inputs.get(0), tableIdentifier, catalogTable, tableSink, abilitySpecs)
+      cluster,
+      traitSet,
+      inputs.get(0),
+      hints,
+      tableIdentifier,
+      catalogTable,
+      tableSink,
+      abilitySpecs)
   }
 
   override def translateToExecNode(): ExecNode[_] = {
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamPhysicalLegacySink.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamPhysicalLegacySink.scala
index 11b316c5110..4cb7bb7935d 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamPhysicalLegacySink.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamPhysicalLegacySink.scala
@@ -20,13 +20,14 @@ package org.apache.flink.table.planner.plan.nodes.physical.stream
 
 import org.apache.flink.table.planner.plan.nodes.calcite.LegacySink
 import org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLegacySink
-import org.apache.flink.table.planner.plan.nodes.exec.{InputProperty, ExecNode}
+import org.apache.flink.table.planner.plan.nodes.exec.{ExecNode, InputProperty}
 import org.apache.flink.table.planner.plan.utils.{ChangelogPlanUtils, UpdatingPlanChecker}
 import org.apache.flink.table.runtime.types.LogicalTypeDataTypeConverter.fromDataTypeToLogicalType
 import org.apache.flink.table.sinks._
 
 import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}
 import org.apache.calcite.rel.RelNode
+import org.apache.calcite.rel.hint.RelHint
 
 import java.util
 
@@ -39,15 +40,16 @@ class StreamPhysicalLegacySink[T](
     cluster: RelOptCluster,
     traitSet: RelTraitSet,
     inputRel: RelNode,
+    hints: util.List[RelHint],
     sink: TableSink[T],
     sinkName: String)
-  extends LegacySink(cluster, traitSet, inputRel, sink, sinkName)
+  extends LegacySink(cluster, traitSet, inputRel, hints, sink, sinkName)
   with StreamPhysicalRel {
 
   override def requireWatermark: Boolean = false
 
   override def copy(traitSet: RelTraitSet, inputs: util.List[RelNode]): RelNode = {
-    new StreamPhysicalLegacySink(cluster, traitSet, inputs.get(0), sink, sinkName)
+    new StreamPhysicalLegacySink(cluster, traitSet, inputs.get(0), hints, sink, sinkName)
   }
 
   override def translateToExecNode(): ExecNode[_] = {
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamPhysicalSink.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamPhysicalSink.scala
index 75b26659f9a..fda14e95c16 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamPhysicalSink.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamPhysicalSink.scala
@@ -18,7 +18,7 @@
 
 package org.apache.flink.table.planner.plan.nodes.physical.stream
 
-import org.apache.flink.table.catalog.{CatalogTable, ObjectIdentifier, ResolvedCatalogTable}
+import org.apache.flink.table.catalog.{ObjectIdentifier, ResolvedCatalogTable}
 import org.apache.flink.table.connector.sink.DynamicTableSink
 import org.apache.flink.table.planner.calcite.FlinkTypeFactory
 import org.apache.flink.table.planner.plan.abilities.sink.SinkAbilitySpec
@@ -30,6 +30,7 @@ import org.apache.flink.table.planner.plan.utils.{ChangelogPlanUtils, FlinkRelOp
 
 import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}
 import org.apache.calcite.rel.RelNode
+import org.apache.calcite.rel.hint.RelHint
 
 import java.util
 
@@ -41,18 +42,26 @@ class StreamPhysicalSink(
     cluster: RelOptCluster,
     traitSet: RelTraitSet,
     inputRel: RelNode,
+    hints: util.List[RelHint],
     tableIdentifier: ObjectIdentifier,
     catalogTable: ResolvedCatalogTable,
     tableSink: DynamicTableSink,
     abilitySpecs: Array[SinkAbilitySpec])
-  extends Sink(cluster, traitSet, inputRel, tableIdentifier, catalogTable, tableSink)
+  extends Sink(cluster, traitSet, inputRel, hints, tableIdentifier, catalogTable, tableSink)
   with StreamPhysicalRel {
 
   override def requireWatermark: Boolean = false
 
   override def copy(traitSet: RelTraitSet, inputs: util.List[RelNode]): RelNode = {
     new StreamPhysicalSink(
-      cluster, traitSet, inputs.get(0), tableIdentifier, catalogTable, tableSink, abilitySpecs)
+      cluster,
+      traitSet,
+      inputs.get(0),
+      hints,
+      tableIdentifier,
+      catalogTable,
+      tableSink,
+      abilitySpecs)
   }
 
   override def translateToExecNode(): ExecNode[_] = {
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/batch/BatchPhysicalLegacySinkRule.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/batch/BatchPhysicalLegacySinkRule.scala
index 4b942611b8f..257821342bb 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/batch/BatchPhysicalLegacySinkRule.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/batch/BatchPhysicalLegacySinkRule.scala
@@ -40,21 +40,21 @@ class BatchPhysicalLegacySinkRule extends ConverterRule(
     "BatchPhysicalLegacySinkRule") {
 
   def convert(rel: RelNode): RelNode = {
-    val sinkNode = rel.asInstanceOf[FlinkLogicalLegacySink]
+    val sink = rel.asInstanceOf[FlinkLogicalLegacySink]
     val newTrait = rel.getTraitSet.replace(FlinkConventions.BATCH_PHYSICAL)
-    var requiredTraitSet = sinkNode.getInput.getTraitSet.replace(FlinkConventions.BATCH_PHYSICAL)
-    if (sinkNode.catalogTable != null && sinkNode.catalogTable.isPartitioned) {
-      sinkNode.sink match {
+    var requiredTraitSet = sink.getInput.getTraitSet.replace(FlinkConventions.BATCH_PHYSICAL)
+    if (sink.catalogTable != null && sink.catalogTable.isPartitioned) {
+      sink.sink match {
         case partitionSink: PartitionableTableSink =>
-          partitionSink.setStaticPartition(sinkNode.staticPartitions)
-          val dynamicPartFields = sinkNode.catalogTable.getPartitionKeys
-              .filter(!sinkNode.staticPartitions.contains(_))
+          partitionSink.setStaticPartition(sink.staticPartitions)
+          val dynamicPartFields = sink.catalogTable.getPartitionKeys
+              .filter(!sink.staticPartitions.contains(_))
 
           if (dynamicPartFields.nonEmpty) {
             val dynamicPartIndices =
               dynamicPartFields.map(partitionSink.getTableSchema.getFieldNames.indexOf(_))
 
-            val shuffleEnable = sinkNode
+            val shuffleEnable = sink
                 .catalogTable
                 .getOptions
                 .get(FileSystemOptions.SINK_SHUFFLE_BY_PARTITION.key())
@@ -72,18 +72,19 @@ class BatchPhysicalLegacySinkRule extends ConverterRule(
             }
           }
         case _ => throw new TableException("We need PartitionableTableSink to write data to" +
-            s" partitioned table: ${sinkNode.sinkName}")
+            s" partitioned table: ${sink.sinkName}")
       }
     }
 
-    val newInput = RelOptRule.convert(sinkNode.getInput, requiredTraitSet)
+    val newInput = RelOptRule.convert(sink.getInput, requiredTraitSet)
 
     new BatchPhysicalLegacySink(
       rel.getCluster,
       newTrait,
       newInput,
-      sinkNode.sink,
-      sinkNode.sinkName)
+      sink.hints,
+      sink.sink,
+      sink.sinkName)
   }
 }
 
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/batch/BatchPhysicalSinkRule.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/batch/BatchPhysicalSinkRule.scala
index c82868b062d..939de77c75c 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/batch/BatchPhysicalSinkRule.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/batch/BatchPhysicalSinkRule.scala
@@ -43,23 +43,23 @@ class BatchPhysicalSinkRule extends ConverterRule(
     "BatchPhysicalSinkRule") {
 
   def convert(rel: RelNode): RelNode = {
-    val sinkNode = rel.asInstanceOf[FlinkLogicalSink]
+    val sink = rel.asInstanceOf[FlinkLogicalSink]
     val newTrait = rel.getTraitSet.replace(FlinkConventions.BATCH_PHYSICAL)
-    var requiredTraitSet = sinkNode.getInput.getTraitSet.replace(FlinkConventions.BATCH_PHYSICAL)
+    var requiredTraitSet = sink.getInput.getTraitSet.replace(FlinkConventions.BATCH_PHYSICAL)
     val abilitySpecs: mutable.ArrayBuffer[SinkAbilitySpec] =
-      mutable.ArrayBuffer(sinkNode.abilitySpecs: _*)
-    if (sinkNode.catalogTable != null && sinkNode.catalogTable.isPartitioned) {
-      sinkNode.tableSink match {
+      mutable.ArrayBuffer(sink.abilitySpecs: _*)
+    if (sink.catalogTable != null && sink.catalogTable.isPartitioned) {
+      sink.tableSink match {
         case partitionSink: SupportsPartitioning =>
-          if (sinkNode.staticPartitions.nonEmpty) {
-            val partitioningSpec = new PartitioningSpec(sinkNode.staticPartitions)
+          if (sink.staticPartitions.nonEmpty) {
+            val partitioningSpec = new PartitioningSpec(sink.staticPartitions)
             partitioningSpec.apply(partitionSink)
             abilitySpecs += partitioningSpec
           }
 
-          val dynamicPartFields = sinkNode.catalogTable.getPartitionKeys
-              .filter(!sinkNode.staticPartitions.contains(_))
-          val fieldNames = sinkNode.catalogTable
+          val dynamicPartFields = sink.catalogTable.getPartitionKeys
+              .filter(!sink.staticPartitions.contains(_))
+          val fieldNames = sink.catalogTable
             .getResolvedSchema
             .toPhysicalRowDataType
             .getLogicalType.asInstanceOf[RowType]
@@ -69,7 +69,7 @@ class BatchPhysicalSinkRule extends ConverterRule(
             val dynamicPartIndices =
               dynamicPartFields.map(fieldNames.indexOf(_))
 
-            val shuffleEnable = sinkNode
+            val shuffleEnable = sink
                 .catalogTable
                 .getOptions
                 .get(FileSystemOptions.SINK_SHUFFLE_BY_PARTITION.key())
@@ -94,21 +94,22 @@ class BatchPhysicalSinkRule extends ConverterRule(
             }
           }
         case _ => throw new TableException(
-          s"'${sinkNode.tableIdentifier.asSummaryString()}' is a partitioned table, " +
-            s"but the underlying [${sinkNode.tableSink.asSummaryString()}] DynamicTableSink " +
+          s"'${sink.tableIdentifier.asSummaryString()}' is a partitioned table, " +
+            s"but the underlying [${sink.tableSink.asSummaryString()}] DynamicTableSink " +
             s"doesn't implement SupportsPartitioning interface.")
       }
     }
 
-    val newInput = RelOptRule.convert(sinkNode.getInput, requiredTraitSet)
+    val newInput = RelOptRule.convert(sink.getInput, requiredTraitSet)
 
     new BatchPhysicalSink(
       rel.getCluster,
       newTrait,
       newInput,
-      sinkNode.tableIdentifier,
-      sinkNode.catalogTable,
-      sinkNode.tableSink,
+      sink.hints,
+      sink.tableIdentifier,
+      sink.catalogTable,
+      sink.tableSink,
       abilitySpecs.toArray)
   }
 }
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/stream/StreamPhysicalLegacySinkRule.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/stream/StreamPhysicalLegacySinkRule.scala
index 9b66d8759f9..fb786d2e62c 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/stream/StreamPhysicalLegacySinkRule.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/stream/StreamPhysicalLegacySinkRule.scala
@@ -39,21 +39,21 @@ class StreamPhysicalLegacySinkRule extends ConverterRule(
     "StreamPhysicalLegacySinkRule") {
 
   def convert(rel: RelNode): RelNode = {
-    val sinkNode = rel.asInstanceOf[FlinkLogicalLegacySink]
+    val sink = rel.asInstanceOf[FlinkLogicalLegacySink]
     val newTrait = rel.getTraitSet.replace(FlinkConventions.STREAM_PHYSICAL)
-    var requiredTraitSet = sinkNode.getInput.getTraitSet.replace(FlinkConventions.STREAM_PHYSICAL)
-    if (sinkNode.catalogTable != null && sinkNode.catalogTable.isPartitioned) {
-      sinkNode.sink match {
+    var requiredTraitSet = sink.getInput.getTraitSet.replace(FlinkConventions.STREAM_PHYSICAL)
+    if (sink.catalogTable != null && sink.catalogTable.isPartitioned) {
+      sink.sink match {
         case partitionSink: PartitionableTableSink =>
-          partitionSink.setStaticPartition(sinkNode.staticPartitions)
-          val dynamicPartFields = sinkNode.catalogTable.getPartitionKeys
-              .filter(!sinkNode.staticPartitions.contains(_))
+          partitionSink.setStaticPartition(sink.staticPartitions)
+          val dynamicPartFields = sink.catalogTable.getPartitionKeys
+              .filter(!sink.staticPartitions.contains(_))
 
           if (dynamicPartFields.nonEmpty) {
             val dynamicPartIndices =
               dynamicPartFields.map(partitionSink.getTableSchema.getFieldNames.indexOf(_))
 
-            val shuffleEnable = sinkNode
+            val shuffleEnable = sink
                 .catalogTable
                 .getOptions
                 .get(FileSystemOptions.SINK_SHUFFLE_BY_PARTITION.key())
@@ -69,18 +69,19 @@ class StreamPhysicalLegacySinkRule extends ConverterRule(
             }
           }
         case _ => throw new TableException("We need PartitionableTableSink to write data to" +
-            s" partitioned table: ${sinkNode.sinkName}")
+            s" partitioned table: ${sink.sinkName}")
       }
     }
 
-    val newInput = RelOptRule.convert(sinkNode.getInput, requiredTraitSet)
+    val newInput = RelOptRule.convert(sink.getInput, requiredTraitSet)
 
     new StreamPhysicalLegacySink(
       rel.getCluster,
       newTrait,
       newInput,
-      sinkNode.sink,
-      sinkNode.sinkName)
+      sink.hints,
+      sink.sink,
+      sink.sinkName)
   }
 }
 
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/stream/StreamPhysicalSinkRule.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/stream/StreamPhysicalSinkRule.scala
index 0819a540061..b2b85e3e033 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/stream/StreamPhysicalSinkRule.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/stream/StreamPhysicalSinkRule.scala
@@ -42,23 +42,23 @@ class StreamPhysicalSinkRule extends ConverterRule(
     "StreamPhysicalSinkRule") {
 
   def convert(rel: RelNode): RelNode = {
-    val sinkNode = rel.asInstanceOf[FlinkLogicalSink]
+    val sink = rel.asInstanceOf[FlinkLogicalSink]
     val newTrait = rel.getTraitSet.replace(FlinkConventions.STREAM_PHYSICAL)
-    var requiredTraitSet = sinkNode.getInput.getTraitSet.replace(FlinkConventions.STREAM_PHYSICAL)
+    var requiredTraitSet = sink.getInput.getTraitSet.replace(FlinkConventions.STREAM_PHYSICAL)
     val abilitySpecs: mutable.ArrayBuffer[SinkAbilitySpec] =
-      mutable.ArrayBuffer(sinkNode.abilitySpecs: _*)
-    if (sinkNode.catalogTable != null && sinkNode.catalogTable.isPartitioned) {
-      sinkNode.tableSink match {
+      mutable.ArrayBuffer(sink.abilitySpecs: _*)
+    if (sink.catalogTable != null && sink.catalogTable.isPartitioned) {
+      sink.tableSink match {
         case partitionSink: SupportsPartitioning =>
-          if (sinkNode.staticPartitions.nonEmpty) {
-            val partitioningSpec = new PartitioningSpec(sinkNode.staticPartitions)
+          if (sink.staticPartitions.nonEmpty) {
+            val partitioningSpec = new PartitioningSpec(sink.staticPartitions)
             partitioningSpec.apply(partitionSink)
             abilitySpecs += partitioningSpec
           }
 
-          val dynamicPartFields = sinkNode.catalogTable.getPartitionKeys
-              .filter(!sinkNode.staticPartitions.contains(_))
-          val fieldNames = sinkNode.catalogTable
+          val dynamicPartFields = sink.catalogTable.getPartitionKeys
+              .filter(!sink.staticPartitions.contains(_))
+          val fieldNames = sink.catalogTable
             .getSchema
             .toPhysicalRowDataType
             .getLogicalType.asInstanceOf[RowType]
@@ -68,7 +68,7 @@ class StreamPhysicalSinkRule extends ConverterRule(
             val dynamicPartIndices =
               dynamicPartFields.map(fieldNames.indexOf(_))
 
-            val shuffleEnable = sinkNode
+            val shuffleEnable = sink
                 .catalogTable
                 .getOptions
                 .get(FileSystemOptions.SINK_SHUFFLE_BY_PARTITION.key())
@@ -84,21 +84,22 @@ class StreamPhysicalSinkRule extends ConverterRule(
             }
           }
         case _ => throw new TableException(
-          s"'${sinkNode.tableIdentifier.asSummaryString()}' is a partitioned table, " +
-            s"but the underlying [${sinkNode.tableSink.asSummaryString()}] DynamicTableSink " +
+          s"'${sink.tableIdentifier.asSummaryString()}' is a partitioned table, " +
+            s"but the underlying [${sink.tableSink.asSummaryString()}] DynamicTableSink " +
             s"doesn't implement SupportsPartitioning interface.")
       }
     }
 
-    val newInput = RelOptRule.convert(sinkNode.getInput, requiredTraitSet)
+    val newInput = RelOptRule.convert(sink.getInput, requiredTraitSet)
 
     new StreamPhysicalSink(
       rel.getCluster,
       newTrait,
       newInput,
-      sinkNode.tableIdentifier,
-      sinkNode.catalogTable,
-      sinkNode.tableSink,
+      sink.hints,
+      sink.tableIdentifier,
+      sink.catalogTable,
+      sink.tableSink,
       abilitySpecs.toArray
     )
   }
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RelExplainUtil.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RelExplainUtil.scala
index 4d65de02173..5eb9ab39055 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RelExplainUtil.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RelExplainUtil.scala
@@ -862,7 +862,7 @@ object RelExplainUtil {
   /**
    * Converts [[RelHint]]s to String.
    */
-  def hintsToString(hints: ImmutableList[RelHint]): String = {
+  def hintsToString(hints: util.List[RelHint]): String = {
     val sb = new StringBuilder
     sb.append("[")
     hints.foreach { hint =>
diff --git a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/LegacySinkTest.xml b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/LegacySinkTest.xml
index 3778e3e72f7..ed2f07c1e2e 100644
--- a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/LegacySinkTest.xml
+++ b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/LegacySinkTest.xml
@@ -76,6 +76,30 @@ LegacySink(name=[`default_catalog`.`default_database`.`sink2`], fields=[total_mi
    +- Exchange(distribution=[single])
       +- LocalHashAggregate(select=[Partial_MIN(sum_a) AS min$0])
          +- Reused(reference_id=[1])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testTableHints">
+    <Resource name="ast">
+      <![CDATA[
+LogicalLegacySink(name=[`default_catalog`.`default_database`.`MySink`], fields=[a, b, c], hints=[[[OPTIONS options:{path=/tmp1}]]])
++- LogicalProject(a=[$0], b=[$1], c=[$2])
+   +- LogicalTableScan(table=[[default_catalog, default_database, MyTable]])
+
+LogicalLegacySink(name=[`default_catalog`.`default_database`.`MySink`], fields=[a, b, c], hints=[[[OPTIONS options:{path=/tmp2}]]])
++- LogicalProject(a=[$0], b=[$1], c=[$2])
+   +- LogicalTableScan(table=[[default_catalog, default_database, MyTable]])
+]]>
+    </Resource>
+    <Resource name="optimized exec plan">
+      <![CDATA[
+BoundedStreamScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c])(reuse_id=[1])
+
+LegacySink(name=[`default_catalog`.`default_database`.`MySink`], fields=[a, b, c], hints=[[[OPTIONS options:{path=/tmp1}]]])
++- Reused(reference_id=[1])
+
+LegacySink(name=[`default_catalog`.`default_database`.`MySink`], fields=[a, b, c], hints=[[[OPTIONS options:{path=/tmp2}]]])
++- Reused(reference_id=[1])
 ]]>
     </Resource>
   </TestCase>
diff --git a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/TableSinkTest.xml b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/TableSinkTest.xml
index 8351bc0e3a6..18c0c328104 100644
--- a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/TableSinkTest.xml
+++ b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/TableSinkTest.xml
@@ -95,6 +95,30 @@ Sink(table=[default_catalog.default_database.sink2], fields=[total_min])
    +- Exchange(distribution=[single])
       +- LocalHashAggregate(select=[Partial_MIN(sum_a) AS min$0])
          +- Reused(reference_id=[1])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testTableHints">
+    <Resource name="ast">
+      <![CDATA[
+LogicalSink(table=[default_catalog.default_database.MySink], fields=[a, b, c], hints=[[[OPTIONS options:{path=/tmp1}]]])
++- LogicalProject(a=[$0], b=[$1], c=[$2])
+   +- LogicalTableScan(table=[[default_catalog, default_database, MyTable]])
+
+LogicalSink(table=[default_catalog.default_database.MySink], fields=[a, b, c], hints=[[[OPTIONS options:{path=/tmp2}]]])
++- LogicalProject(a=[$0], b=[$1], c=[$2])
+   +- LogicalTableScan(table=[[default_catalog, default_database, MyTable]])
+]]>
+    </Resource>
+    <Resource name="optimized exec plan">
+      <![CDATA[
+BoundedStreamScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c])(reuse_id=[1])
+
+Sink(table=[default_catalog.default_database.MySink], fields=[a, b, c], hints=[[[OPTIONS options:{path=/tmp1}]]])
++- Reused(reference_id=[1])
+
+Sink(table=[default_catalog.default_database.MySink], fields=[a, b, c], hints=[[[OPTIONS options:{path=/tmp2}]]])
++- Reused(reference_id=[1])
 ]]>
     </Resource>
   </TestCase>
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/batch/sql/LegacySinkTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/batch/sql/LegacySinkTest.scala
index cdee488e890..89f01b4644f 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/batch/sql/LegacySinkTest.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/batch/sql/LegacySinkTest.scala
@@ -20,6 +20,7 @@ package org.apache.flink.table.planner.plan.batch.sql
 
 import org.apache.flink.api.scala._
 import org.apache.flink.table.api._
+import org.apache.flink.table.api.config.TableConfigOptions
 import org.apache.flink.table.api.internal.TableEnvironmentInternal
 import org.apache.flink.table.planner.plan.optimize.RelNodeBlockPlanBuilder
 import org.apache.flink.table.planner.utils.TableTestBase
@@ -62,4 +63,40 @@ class LegacySinkTest extends TableTestBase {
 
     util.verifyExecPlan(stmtSet)
   }
+
+  @Test
+  def testTableHints(): Unit = {
+    util.tableEnv.executeSql(
+      s"""
+         |CREATE TABLE MyTable (
+         |  `a` INT,
+         |  `b` BIGINT,
+         |  `c` STRING
+         |) WITH (
+         |  'connector' = 'values',
+         |  'bounded' = 'true'
+         |)
+       """.stripMargin)
+
+    util.tableEnv.getConfig.getConfiguration.setBoolean(
+      TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true)
+    util.tableEnv.executeSql(
+      s"""
+         |CREATE TABLE MySink (
+         |  `a` INT,
+         |  `b` BIGINT,
+         |  `c` STRING
+         |) WITH (
+         |  'connector' = 'OPTIONS',
+         |  'path' = '/tmp/test'
+         |)
+       """.stripMargin)
+    val stmtSet= util.tableEnv.createStatementSet()
+    stmtSet.addInsertSql(
+      "insert into MySink /*+ OPTIONS('path' = '/tmp1') */ select * from MyTable")
+    stmtSet.addInsertSql(
+      "insert into MySink /*+ OPTIONS('path' = '/tmp2') */ select * from MyTable")
+
+    util.verifyExecPlan(stmtSet)
+  }
 }
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/batch/sql/TableSinkTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/batch/sql/TableSinkTest.scala
index 4ab2388d902..07edc91cea6 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/batch/sql/TableSinkTest.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/batch/sql/TableSinkTest.scala
@@ -20,11 +20,15 @@ package org.apache.flink.table.planner.plan.batch.sql
 
 import org.apache.flink.api.scala._
 import org.apache.flink.table.api._
+import org.apache.flink.table.api.config.TableConfigOptions
+import org.apache.flink.table.planner.factories.TestValuesTableFactory
 import org.apache.flink.table.planner.plan.optimize.RelNodeBlockPlanBuilder
-import org.apache.flink.table.planner.utils.TableTestBase
+import org.apache.flink.table.planner.runtime.utils.BatchAbstractTestBase
+import org.apache.flink.table.planner.runtime.utils.TestData.smallData3
+import org.apache.flink.table.planner.utils.{TableTestBase, TableTestUtil}
 import org.apache.flink.table.types.logical.{BigIntType, IntType}
 
-import org.junit.Test
+import org.junit.{Assert, Test}
 
 class TableSinkTest extends TableTestBase {
 
@@ -96,4 +100,41 @@ class TableSinkTest extends TableTestBase {
     stmtSet.addInsertSql("INSERT INTO sink SELECT a,b FROM MyTable ORDER BY a")
     util.verifyExecPlan(stmtSet)
   }
+
+  @Test
+  def testTableHints(): Unit = {
+    util.tableEnv.executeSql(
+      s"""
+         |CREATE TABLE MyTable (
+         |  `a` INT,
+         |  `b` BIGINT,
+         |  `c` STRING
+         |) WITH (
+         |  'connector' = 'values',
+         |  'bounded' = 'true'
+         |)
+       """.stripMargin)
+
+    util.tableEnv.getConfig.getConfiguration.setBoolean(
+      TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true)
+    util.tableEnv.executeSql(
+      s"""
+         |CREATE TABLE MySink (
+         |  `a` INT,
+         |  `b` BIGINT,
+         |  `c` STRING
+         |) WITH (
+         |  'connector' = 'filesystem',
+         |  'format' = 'testcsv',
+         |  'path' = '/tmp/test'
+         |)
+       """.stripMargin)
+    val stmtSet= util.tableEnv.createStatementSet()
+    stmtSet.addInsertSql(
+      "insert into MySink /*+ OPTIONS('path' = '/tmp1') */ select * from MyTable")
+    stmtSet.addInsertSql(
+      "insert into MySink /*+ OPTIONS('path' = '/tmp2') */ select * from MyTable")
+
+    util.verifyExecPlan(stmtSet)
+  }
 }
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/TableSinkITCase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/TableSinkITCase.scala
new file mode 100644
index 00000000000..40adc57e1ab
--- /dev/null
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/TableSinkITCase.scala
@@ -0,0 +1,79 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.planner.runtime.batch.sql
+
+import org.apache.flink.table.api.config.TableConfigOptions
+import org.apache.flink.table.planner.factories.TestValuesTableFactory
+import org.apache.flink.table.planner.runtime.utils.TestData.smallData3
+import org.apache.flink.table.planner.runtime.utils.{BatchAbstractTestBase, BatchTestBase}
+import org.apache.flink.table.planner.utils.TableTestUtil
+
+import org.junit.{Assert, Test}
+
+class TableSinkITCase extends BatchTestBase {
+
+  @Test
+  def testTableHints(): Unit = {
+    val dataId = TestValuesTableFactory.registerData(smallData3)
+    tEnv.executeSql(
+      s"""
+         |CREATE TABLE MyTable (
+         |  `a` INT,
+         |  `b` BIGINT,
+         |  `c` STRING
+         |) WITH (
+         |  'connector' = 'values',
+         |  'bounded' = 'true',
+         |  'data-id' = '$dataId'
+         |)
+       """.stripMargin)
+
+    tEnv.getConfig.getConfiguration.setBoolean(
+      TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true)
+    val resultPath = BatchAbstractTestBase.TEMPORARY_FOLDER.newFolder().getAbsolutePath
+    tEnv.executeSql(
+      s"""
+         |CREATE TABLE MySink (
+         |  `a` INT,
+         |  `b` BIGINT,
+         |  `c` STRING
+         |) WITH (
+         |  'connector' = 'filesystem',
+         |  'format' = 'testcsv',
+         |  'path' = '$resultPath'
+         |)
+       """.stripMargin)
+    val stmtSet= tEnv.createStatementSet()
+    val newPath1 =  BatchAbstractTestBase.TEMPORARY_FOLDER.newFolder().getAbsolutePath
+    stmtSet.addInsertSql(
+      s"insert into MySink /*+ OPTIONS('path' = '$newPath1') */ select * from MyTable")
+    val newPath2 =  BatchAbstractTestBase.TEMPORARY_FOLDER.newFolder().getAbsolutePath
+    stmtSet.addInsertSql(
+      s"insert into MySink /*+ OPTIONS('path' = '$newPath2') */ select * from MyTable")
+    stmtSet.execute().await()
+
+    Assert.assertTrue(TableTestUtil.readFromFile(resultPath).isEmpty)
+    val expected = Seq("1,1,Hi", "2,2,Hello", "3,2,Hello world")
+    val result1 = TableTestUtil.readFromFile(newPath1)
+    Assert.assertEquals(expected.sorted, result1.sorted)
+    val result2 = TableTestUtil.readFromFile(newPath2)
+    Assert.assertEquals(expected.sorted, result2.sorted)
+  }
+
+}
