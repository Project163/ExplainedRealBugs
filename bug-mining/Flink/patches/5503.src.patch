diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveDynamicTableFactory.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveDynamicTableFactory.java
index 553cdaf11b3..c7a6c319f0c 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveDynamicTableFactory.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveDynamicTableFactory.java
@@ -20,6 +20,7 @@ package org.apache.flink.connectors.hive;
 
 import org.apache.flink.configuration.ConfigOption;
 import org.apache.flink.configuration.Configuration;
+import org.apache.flink.connectors.hive.util.JobConfUtils;
 import org.apache.flink.table.catalog.CatalogTable;
 import org.apache.flink.table.catalog.hive.HiveCatalog;
 import org.apache.flink.table.connector.sink.DynamicTableSink;
@@ -71,9 +72,10 @@ public class HiveDynamicTableFactory implements DynamicTableSourceFactory, Dynam
             Integer configuredParallelism =
                     Configuration.fromMap(context.getCatalogTable().getOptions())
                             .get(FileSystemOptions.SINK_PARALLELISM);
+            JobConf jobConf = JobConfUtils.createJobConfWithCredentials(hiveConf);
             return new HiveTableSink(
                     context.getConfiguration(),
-                    new JobConf(hiveConf),
+                    jobConf,
                     context.getObjectIdentifier(),
                     context.getCatalogTable(),
                     configuredParallelism);
@@ -114,17 +116,18 @@ public class HiveDynamicTableFactory implements DynamicTableSourceFactory, Dynam
                                                     STREAMING_SOURCE_PARTITION_INCLUDE.key(),
                                                     STREAMING_SOURCE_PARTITION_INCLUDE
                                                             .defaultValue()));
+            JobConf jobConf = JobConfUtils.createJobConfWithCredentials(hiveConf);
             // hive table source that has not lookup ability
             if (isStreamingSource && includeAllPartition) {
                 return new HiveTableSource(
-                        new JobConf(hiveConf),
+                        jobConf,
                         context.getConfiguration(),
                         context.getObjectIdentifier().toObjectPath(),
                         catalogTable);
             } else {
                 // hive table source that has scan and lookup ability
                 return new HiveLookupTableSource(
-                        new JobConf(hiveConf),
+                        jobConf,
                         context.getConfiguration(),
                         context.getObjectIdentifier().toObjectPath(),
                         catalogTable);
diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java
index 2a47e5e6d0c..138befcbfcf 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java
@@ -18,6 +18,7 @@
 
 package org.apache.flink.connectors.hive;
 
+import org.apache.flink.annotation.VisibleForTesting;
 import org.apache.flink.api.common.functions.MapFunction;
 import org.apache.flink.api.common.serialization.BulkWriter;
 import org.apache.flink.configuration.ReadableConfig;
@@ -507,4 +508,9 @@ public class HiveTableSink implements DynamicTableSink, SupportsPartitioning, Su
             }
         }
     }
+
+    @VisibleForTesting
+    public JobConf getJobConf() {
+        return jobConf;
+    }
 }
diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java
index 7cbc5928ca0..acc06eaf99a 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java
@@ -439,4 +439,9 @@ public class HiveTableSource
             }
         }
     }
+
+    @VisibleForTesting
+    public JobConf getJobConf() {
+        return jobConf;
+    }
 }
diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/util/JobConfUtils.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/util/JobConfUtils.java
index cfdd3f3e0cb..ae4acb5eb1a 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/util/JobConfUtils.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/util/JobConfUtils.java
@@ -18,10 +18,16 @@
 
 package org.apache.flink.connectors.hive.util;
 
+import org.apache.flink.api.java.hadoop.common.HadoopInputFormatCommonBase;
 import org.apache.flink.connectors.hive.JobConfWrapper;
 
+import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.security.Credentials;
+import org.apache.hadoop.security.UserGroupInformation;
+
+import java.io.IOException;
 
 /** Utilities for {@link JobConf}. */
 public class JobConfUtils {
@@ -40,4 +46,24 @@ public class JobConfUtils {
                 HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,
                 HiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal);
     }
+
+    private static void addCredentialsIntoJobConf(JobConf jobConf) {
+        UserGroupInformation currentUser = null;
+        try {
+            currentUser = UserGroupInformation.getCurrentUser();
+        } catch (IOException e) {
+            throw new RuntimeException("Unable to determine current user", e);
+        }
+        Credentials currentUserCreds =
+                HadoopInputFormatCommonBase.getCredentialsFromUGI(currentUser);
+        if (currentUserCreds != null) {
+            jobConf.getCredentials().mergeAll(currentUserCreds);
+        }
+    }
+
+    public static JobConf createJobConfWithCredentials(Configuration configuration) {
+        JobConf jobConf = new JobConf(configuration);
+        addCredentialsIntoJobConf(jobConf);
+        return jobConf;
+    }
 }
diff --git a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveDynamicTableFactoryTest.java b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveDynamicTableFactoryTest.java
index f256ef73338..db579bdf751 100644
--- a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveDynamicTableFactoryTest.java
+++ b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveDynamicTableFactoryTest.java
@@ -27,11 +27,16 @@ import org.apache.flink.table.catalog.CatalogTable;
 import org.apache.flink.table.catalog.ObjectIdentifier;
 import org.apache.flink.table.catalog.hive.HiveCatalog;
 import org.apache.flink.table.catalog.hive.HiveTestUtils;
+import org.apache.flink.table.connector.sink.DynamicTableSink;
 import org.apache.flink.table.connector.source.DynamicTableSource;
 import org.apache.flink.table.factories.FactoryUtil;
 import org.apache.flink.table.filesystem.FileSystemLookupFunction;
 import org.apache.flink.util.ExceptionUtils;
 
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.security.Credentials;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.security.token.Token;
 import org.junit.BeforeClass;
 import org.junit.Test;
 
@@ -47,6 +52,7 @@ import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOUR
 import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_ORDER;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertNotNull;
 import static org.junit.Assert.assertTrue;
 
 /** Unit tests for {@link HiveDynamicTableFactory}. */
@@ -228,6 +234,43 @@ public class HiveDynamicTableFactoryTest {
         }
     }
 
+    @Test
+    public void testJobConfWithCredentials() throws Exception {
+        final Text hdfsDelegationTokenKind = new Text("HDFS_DELEGATION_TOKEN");
+        final Text hdfsDelegationTokenService = new Text("ha-hdfs:hadoop-namespace");
+        Credentials credentials = new Credentials();
+        credentials.addToken(
+                hdfsDelegationTokenService,
+                new Token<>(
+                        new byte[4],
+                        new byte[4],
+                        hdfsDelegationTokenKind,
+                        hdfsDelegationTokenService));
+        UserGroupInformation.getCurrentUser().addCredentials(credentials);
+
+        // test table source's jobConf with credentials
+        tableEnv.executeSql(
+                String.format(
+                        "create table table10 (x int, y string, z int) partitioned by ("
+                                + " pt_year int, pt_mon string, pt_day string)"));
+        DynamicTableSource tableSource1 = getTableSource("table10");
+
+        HiveTableSource tableSource = (HiveTableSource) tableSource1;
+        Token token =
+                tableSource.getJobConf().getCredentials().getToken(hdfsDelegationTokenService);
+        assertNotNull(token);
+        assertEquals(hdfsDelegationTokenKind, token.getKind());
+        assertEquals(hdfsDelegationTokenService, token.getService());
+
+        // test table sink's jobConf with credentials
+        DynamicTableSink tableSink1 = getTableSink("table10");
+        HiveTableSink tableSink = (HiveTableSink) tableSink1;
+        token = tableSink.getJobConf().getCredentials().getToken(hdfsDelegationTokenService);
+        assertNotNull(token);
+        assertEquals(hdfsDelegationTokenKind, token.getKind());
+        assertEquals(hdfsDelegationTokenService, token.getService());
+    }
+
     private DynamicTableSource getTableSource(String tableName) throws Exception {
         TableEnvironmentInternal tableEnvInternal = (TableEnvironmentInternal) tableEnv;
         ObjectIdentifier tableIdentifier =
@@ -242,4 +285,19 @@ public class HiveDynamicTableFactoryTest {
                 Thread.currentThread().getContextClassLoader(),
                 false);
     }
+
+    private DynamicTableSink getTableSink(String tableName) throws Exception {
+        TableEnvironmentInternal tableEnvInternal = (TableEnvironmentInternal) tableEnv;
+        ObjectIdentifier tableIdentifier =
+                ObjectIdentifier.of(hiveCatalog.getName(), "default", tableName);
+        CatalogTable catalogTable =
+                (CatalogTable) hiveCatalog.getTable(tableIdentifier.toObjectPath());
+        return FactoryUtil.createTableSink(
+                hiveCatalog,
+                tableIdentifier,
+                tableEnvInternal.getCatalogManager().resolveCatalogTable(catalogTable),
+                tableEnv.getConfig().getConfiguration(),
+                Thread.currentThread().getContextClassLoader(),
+                false);
+    }
 }
