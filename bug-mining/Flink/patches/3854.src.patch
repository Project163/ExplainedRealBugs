diff --git a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCUpsertTableSinkITCase.java b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCUpsertTableSinkITCase.java
index e8b3b79d630..ecba1977fcf 100644
--- a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCUpsertTableSinkITCase.java
+++ b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCUpsertTableSinkITCase.java
@@ -18,24 +18,15 @@
 
 package org.apache.flink.api.java.io.jdbc;
 
-import org.apache.flink.api.common.ExecutionConfig;
-import org.apache.flink.api.common.io.InputFormat;
-import org.apache.flink.api.common.typeinfo.TypeInformation;
-import org.apache.flink.api.common.typeinfo.Types;
-import org.apache.flink.api.java.io.CollectionInputFormat;
 import org.apache.flink.api.java.tuple.Tuple4;
-import org.apache.flink.api.java.typeutils.RowTypeInfo;
 import org.apache.flink.streaming.api.TimeCharacteristic;
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 import org.apache.flink.streaming.api.functions.timestamps.AscendingTimestampExtractor;
 import org.apache.flink.table.api.EnvironmentSettings;
 import org.apache.flink.table.api.Table;
-import org.apache.flink.table.api.TableSchema;
+import org.apache.flink.table.api.TableEnvironment;
 import org.apache.flink.table.api.java.StreamTableEnvironment;
-import org.apache.flink.table.sources.InputFormatTableSource;
-import org.apache.flink.table.types.DataType;
-import org.apache.flink.table.types.utils.TypeConversions;
 import org.apache.flink.test.util.AbstractTestBase;
 import org.apache.flink.types.Row;
 
@@ -49,7 +40,6 @@ import java.sql.SQLException;
 import java.sql.Statement;
 import java.sql.Timestamp;
 import java.util.ArrayList;
-import java.util.Collection;
 import java.util.Collections;
 import java.util.List;
 
@@ -230,14 +220,12 @@ public class JDBCUpsertTableSinkITCase extends AbstractTestBase {
 	}
 
 	@Test
-	public void testBatchUpsert() throws Exception {
-		StreamExecutionEnvironment bsEnv = StreamExecutionEnvironment.getExecutionEnvironment();
-		EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();
-		StreamTableEnvironment bsTableEnv = StreamTableEnvironment.create(bsEnv, bsSettings);
-		RowTypeInfo rt = (RowTypeInfo) Types.ROW_NAMED(new String[]{"NAME", "SCORE"}, Types.STRING, Types.LONG);
-		Table source = bsTableEnv.fromTableSource(new CollectionTableSource(generateRecords(2), rt));
-		bsTableEnv.registerTable("sourceTable", source);
-		bsTableEnv.sqlUpdate(
+	public void testBatchSink() throws Exception {
+		EnvironmentSettings bsSettings = EnvironmentSettings.newInstance()
+				.useBlinkPlanner().inBatchMode().build();
+		TableEnvironment tEnv = TableEnvironment.create(bsSettings);
+
+		tEnv.sqlUpdate(
 			"CREATE TABLE USER_RESULT(" +
 				"NAME VARCHAR," +
 				"SCORE BIGINT" +
@@ -247,57 +235,19 @@ public class JDBCUpsertTableSinkITCase extends AbstractTestBase {
 				"'connector.table' = '" + OUTPUT_TABLE3 + "'" +
 				")");
 
-		bsTableEnv.sqlUpdate("insert into USER_RESULT SELECT s.NAME, s.SCORE " +
-			"FROM sourceTable as s ");
-		bsTableEnv.execute("test");
+		tEnv.sqlUpdate("INSERT INTO USER_RESULT\n" +
+				"SELECT user_name, score " +
+				"FROM (VALUES (1, 'Bob'), (22, 'Tom'), (42, 'Kim'), " +
+				"(42, 'Kim'), (1, 'Bob')) " +
+				"AS UserCountTable(score, user_name)");
+		tEnv.execute("test");
 
 		check(new Row[] {
-			Row.of("a0", 0L),
-			Row.of("a1", 1L)
+				Row.of("Bob", 1),
+				Row.of("Tom", 22),
+				Row.of("Kim", 42),
+				Row.of("Kim", 42),
+				Row.of("Bob", 1)
 		}, DB_URL, OUTPUT_TABLE3, new String[]{"NAME", "SCORE"});
 	}
-
-	private List<Row> generateRecords(int numRecords) {
-		int arity = 2;
-		List<Row> res = new ArrayList<>(numRecords);
-		for (long i = 0; i < numRecords; i++) {
-			Row row = new Row(arity);
-			row.setField(0, "a" + i);
-			row.setField(1, i);
-			res.add(row);
-		}
-		return res;
-	}
-
-	private static class CollectionTableSource extends InputFormatTableSource<Row> {
-
-		private final Collection<Row> data;
-		private final RowTypeInfo rowTypeInfo;
-
-		CollectionTableSource(Collection<Row> data, RowTypeInfo rowTypeInfo) {
-			this.data = data;
-			this.rowTypeInfo = rowTypeInfo;
-		}
-
-		@Override
-		public DataType getProducedDataType() {
-			return TypeConversions.fromLegacyInfoToDataType(rowTypeInfo);
-		}
-
-		@Override
-		public TypeInformation<Row> getReturnType() {
-			return rowTypeInfo;
-		}
-
-		@Override
-		public InputFormat<Row, ?> getInputFormat() {
-			return new CollectionInputFormat<>(data, rowTypeInfo.createSerializer(new ExecutionConfig()));
-		}
-
-		@Override
-		public TableSchema getTableSchema() {
-			return new TableSchema.Builder().fields(rowTypeInfo.getFieldNames(),
-				TypeConversions.fromLegacyInfoToDataType(rowTypeInfo.getFieldTypes())).build();
-		}
-	}
 }
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSink.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSink.scala
index 1d6c46ea90b..6049882f781 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSink.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSink.scala
@@ -28,18 +28,19 @@ import org.apache.flink.table.planner.codegen.{CodeGenUtils, CodeGeneratorContex
 import org.apache.flink.table.planner.delegation.BatchPlanner
 import org.apache.flink.table.planner.plan.nodes.calcite.Sink
 import org.apache.flink.table.planner.plan.nodes.exec.{BatchExecNode, ExecNode}
+import org.apache.flink.table.planner.plan.utils.UpdatingPlanChecker
 import org.apache.flink.table.planner.sinks.DataStreamTableSink
 import org.apache.flink.table.runtime.types.ClassLogicalTypeConverter
 import org.apache.flink.table.runtime.typeutils.BaseRowTypeInfo
-import org.apache.flink.table.sinks.{AppendStreamTableSink, RetractStreamTableSink, StreamTableSink, TableSink, UpsertStreamTableSink}
+import org.apache.flink.table.sinks.{RetractStreamTableSink, StreamTableSink, TableSink, UpsertStreamTableSink}
 import org.apache.flink.table.types.DataType
+
 import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}
 import org.apache.calcite.rel.RelNode
+
 import java.lang.reflect.Modifier
 import java.util
 
-import org.apache.flink.table.planner.plan.utils.UpdatingPlanChecker
-
 import scala.collection.JavaConversions._
 
 /**
@@ -87,30 +88,11 @@ class BatchExecSink[T](
             translateToTransformation(withChangeFlag = true, planner)
 
           case upsertSink: UpsertStreamTableSink[T] =>
-            // check for append only table
-            val isAppendOnlyTable = UpdatingPlanChecker.isAppendOnly(this)
-            upsertSink.setIsAppendOnly(isAppendOnlyTable)
-            val tableKeys = {
-              val sinkFieldNames = upsertSink.getTableSchema.getFieldNames
-              UpdatingPlanChecker.getUniqueKeyFields(getInput, planner, sinkFieldNames) match {
-                case Some(keys) => keys.sortBy(_.length).headOption
-                case None => None
-              }
-            }
-
-            // check that we have keys if the table has changes (is not append-only)
-            tableKeys match {
-              case Some(keys) => upsertSink.setKeyFields(keys)
-              case None if isAppendOnlyTable => upsertSink.setKeyFields(null)
-              case None if !isAppendOnlyTable => throw new TableException(
-                "UpsertStreamTableSink requires that Table has" +
-                  " a full primary keys if it is updated.")
-            }
-
+            upsertSink.setIsAppendOnly(true)
+            upsertSink.setKeyFields(
+              UpdatingPlanChecker.getUniqueKeyForUpsertSink(this, planner, upsertSink).orNull)
             translateToTransformation(withChangeFlag = true, planner)
-          case _: AppendStreamTableSink[T] =>
-            // we can insert the bounded DataStream into a StreamTableSink
-            translateToTransformation(withChangeFlag = false, planner)
+
           case _ =>
             translateToTransformation(withChangeFlag = false, planner)
         }
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecSink.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecSink.scala
index 6ccebb60881..67397702ae5 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecSink.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecSink.scala
@@ -95,19 +95,8 @@ class StreamExecSink[T](
             val isAppendOnlyTable = UpdatingPlanChecker.isAppendOnly(this)
             upsertSink.setIsAppendOnly(isAppendOnlyTable)
 
-            // extract unique key fields
-            // Now we pick shortest one to sink
-            // TODO UpsertStreamTableSink setKeyFields interface should be Array[Array[String]]
-            val tableKeys = {
-              val sinkFieldNames = upsertSink.getTableSchema.getFieldNames
-              UpdatingPlanChecker.getUniqueKeyFields(getInput, planner, sinkFieldNames) match {
-                case Some(keys) => keys.sortBy(_.length).headOption
-                case None => None
-              }
-            }
-
             // check that we have keys if the table has changes (is not append-only)
-            tableKeys match {
+            UpdatingPlanChecker.getUniqueKeyForUpsertSink(this, planner, upsertSink) match {
               case Some(keys) => upsertSink.setKeyFields(keys)
               case None if isAppendOnlyTable => upsertSink.setKeyFields(null)
               case None if !isAppendOnlyTable => throw new TableException(
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/UpdatingPlanChecker.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/UpdatingPlanChecker.scala
index a2e78aba3ef..d0b0cc5fe62 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/UpdatingPlanChecker.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/UpdatingPlanChecker.scala
@@ -19,7 +19,10 @@ package org.apache.flink.table.planner.plan.utils
 
 import org.apache.flink.table.planner.delegation.PlannerBase
 import org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery
+import org.apache.flink.table.planner.plan.nodes.calcite.Sink
 import org.apache.flink.table.planner.plan.nodes.physical.stream._
+import org.apache.flink.table.sinks.UpsertStreamTableSink
+
 import org.apache.calcite.plan.hep.HepRelVertex
 import org.apache.calcite.plan.volcano.RelSubset
 import org.apache.calcite.rel.{RelNode, RelVisitor}
@@ -36,20 +39,6 @@ object UpdatingPlanChecker {
     appendOnlyValidator.isAppendOnly
   }
 
-  /** Extracts the unique keys of the table produced by the plan. */
-  def getUniqueKeyFields(
-      relNode: RelNode,
-      planner: PlannerBase,
-      sinkFieldNames: Array[String]): Option[Array[Array[String]]] = {
-    val fmq = FlinkRelMetadataQuery.reuseOrCreate(planner.getRelBuilder.getCluster.getMetadataQuery)
-    val uniqueKeys = fmq.getUniqueKeys(relNode)
-    if (uniqueKeys != null && uniqueKeys.size() > 0) {
-      Some(uniqueKeys.filter(_.nonEmpty).map(_.toArray.map(sinkFieldNames)).toArray)
-    } else {
-      None
-    }
-  }
-
   private class AppendOnlyValidator extends RelVisitor {
 
     var isAppendOnly = true
@@ -67,4 +56,28 @@ object UpdatingPlanChecker {
       }
     }
   }
+
+  def getUniqueKeyForUpsertSink(
+      sinkNode: Sink,
+      planner: PlannerBase,
+      sink: UpsertStreamTableSink[_]): Option[Array[String]] = {
+    // extract unique key fields
+    // Now we pick shortest one to sink
+    // TODO UpsertStreamTableSink setKeyFields interface should be Array[Array[String]]
+    val sinkFieldNames = sink.getTableSchema.getFieldNames
+    /** Extracts the unique keys of the table produced by the plan. */
+    val fmq = FlinkRelMetadataQuery.reuseOrCreate(
+      planner.getRelBuilder.getCluster.getMetadataQuery)
+    val uniqueKeys = fmq.getUniqueKeys(sinkNode.getInput)
+    if (uniqueKeys != null && uniqueKeys.size() > 0) {
+      uniqueKeys
+          .filter(_.nonEmpty)
+          .map(_.toArray.map(sinkFieldNames))
+          .toSeq
+          .sortBy(_.length)
+          .headOption
+    } else {
+      None
+    }
+  }
 }
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/table/TableSinkITCase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/table/TableSinkITCase.scala
index 3c9e6b4f368..8cd3051318f 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/table/TableSinkITCase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/table/TableSinkITCase.scala
@@ -20,14 +20,17 @@ package org.apache.flink.table.planner.runtime.batch.table
 
 import org.apache.flink.table.api.scala._
 import org.apache.flink.table.api.{DataTypes, TableSchema}
-import org.apache.flink.table.planner.runtime.utils.BatchTestBase
+import org.apache.flink.table.planner.runtime.utils.{BatchTestBase, TestingRetractTableSink, TestingUpsertTableSink}
 import org.apache.flink.table.planner.runtime.utils.TestData._
 import org.apache.flink.table.planner.utils.MemoryTableSourceSinkUtil
 import org.apache.flink.table.planner.utils.MemoryTableSourceSinkUtil.{DataTypeAppendStreamTableSink, DataTypeOutputFormatTableSink}
 import org.apache.flink.test.util.TestBaseUtils
 
+import org.junit.Assert._
 import org.junit._
 
+import java.util.TimeZone
+
 import scala.collection.JavaConverters._
 
 class TableSinkITCase extends BatchTestBase {
@@ -115,5 +118,87 @@ class TableSinkITCase extends BatchTestBase {
     TestBaseUtils.compareResultAsText(results, expected)
   }
 
+  private def prepareForUpsertSink(): TestingUpsertTableSink = {
+    val schema = TableSchema.builder()
+        .field("a", DataTypes.INT())
+        .field("b", DataTypes.DOUBLE())
+        .build()
+    val sink = new TestingUpsertTableSink(Array(0), TimeZone.getDefault)
+    tEnv.registerTableSink("testSink", sink.configure(schema.getFieldNames, schema.getFieldTypes))
+    registerCollection("MyTable", simpleData2, simpleType2, "a, b", nullableOfSimpleData2)
+    sink
+  }
+
+  @Test
+  def testUpsertSink(): Unit = {
+    val sink = prepareForUpsertSink()
+    sink.expectedKeys = Some(Array("a"))
+    sink.expectedIsAppendOnly = Some(false)
+
+    tEnv.from("MyTable")
+        .groupBy('a)
+        .select('a, 'b.sum())
+        .insertInto("testSink")
+    tEnv.execute("")
+
+    val result = sink.getUpsertResults.sorted
+    val expected = List(
+      "1,0.1",
+      "2,0.4",
+      "3,1.0",
+      "4,2.2",
+      "5,3.9").sorted
+    assertEquals(expected, result)
+  }
+
+  @Test
+  def testUpsertSinkWithAppend(): Unit = {
+    val sink = prepareForUpsertSink()
+    sink.expectedKeys = None
+    sink.expectedIsAppendOnly = Some(true)
+
+    tEnv.from("MyTable")
+        .select('a, 'b)
+        .where('a < 3)
+        .insertInto("testSink")
+    tEnv.execute("")
+
+    val result = sink.getRawResults.sorted
+    val expected = List(
+      "(true,1,0.1)",
+      "(true,2,0.2)",
+      "(true,2,0.2)").sorted
+    assertEquals(expected, result)
+  }
+
+  private def prepareForRetractSink(): TestingRetractTableSink = {
+    val schema = TableSchema.builder()
+        .field("a", DataTypes.INT())
+        .field("b", DataTypes.DOUBLE())
+        .build()
+    val sink = new TestingRetractTableSink(TimeZone.getDefault)
+    tEnv.registerTableSink("testSink", sink.configure(schema.getFieldNames, schema.getFieldTypes))
+    registerCollection("MyTable", simpleData2, simpleType2, "a, b", nullableOfSimpleData2)
+    sink
+  }
+
+  @Test
+  def testRetractSink(): Unit = {
+    val sink = prepareForRetractSink()
 
+    tEnv.from("MyTable")
+        .groupBy('a)
+        .select('a, 'b.sum())
+        .insertInto("testSink")
+    tEnv.execute("")
+
+    val result = sink.getRawResults.sorted
+    val expected = List(
+      "(true,1,0.1)",
+      "(true,2,0.4)",
+      "(true,3,1.0)",
+      "(true,4,2.2)",
+      "(true,5,3.9)").sorted
+    assertEquals(expected, result)
+  }
 }
