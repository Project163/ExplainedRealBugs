diff --git a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/stream/StreamPhysicalIntervalJoinRule.scala b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/stream/StreamPhysicalIntervalJoinRule.scala
index 7a9907f82aa..67c46791fc4 100644
--- a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/stream/StreamPhysicalIntervalJoinRule.scala
+++ b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/stream/StreamPhysicalIntervalJoinRule.scala
@@ -27,6 +27,7 @@ import org.apache.calcite.plan.{RelOptRule, RelOptRuleCall, RelTraitSet}
 import org.apache.calcite.rel.RelNode
 import org.apache.flink.table.planner.calcite.FlinkTypeFactory.isRowtimeIndicatorType
 
+import java.util
 import scala.collection.JavaConversions._
 
 /**
@@ -74,6 +75,20 @@ class StreamPhysicalIntervalJoinRule
     true
   }
 
+  override protected def computeJoinLeftKeys(join: FlinkLogicalJoin): util.Collection[Integer] = {
+    val (windowBounds, _) = extractWindowBounds(join)
+    join.analyzeCondition().leftKeys
+      .filter(k => windowBounds.get.getLeftTimeIdx != k)
+      .toList
+  }
+
+  override protected def computeJoinRightKeys(join: FlinkLogicalJoin): util.Collection[Integer] = {
+    val (windowBounds, _) = extractWindowBounds(join)
+    join.analyzeCondition().rightKeys
+      .filter(k => windowBounds.get.getRightTimeIdx != k)
+      .toList
+  }
+
   override protected def transform(
       join: FlinkLogicalJoin,
       leftInput: FlinkRelNode,
diff --git a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/stream/StreamPhysicalJoinRuleBase.scala b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/stream/StreamPhysicalJoinRuleBase.scala
index 5afaba08d0a..88284a301e4 100644
--- a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/stream/StreamPhysicalJoinRuleBase.scala
+++ b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/stream/StreamPhysicalJoinRuleBase.scala
@@ -23,7 +23,6 @@ import org.apache.flink.table.planner.plan.nodes.{FlinkConventions, FlinkRelNode
 import org.apache.flink.table.planner.plan.nodes.exec.spec.IntervalJoinSpec.WindowBounds
 import org.apache.flink.table.planner.plan.nodes.logical.{FlinkLogicalJoin, FlinkLogicalRel}
 import org.apache.flink.table.planner.plan.utils.{FlinkRelOptUtil, IntervalJoinUtil}
-
 import org.apache.calcite.plan.{RelOptRule, RelOptRuleCall, RelTraitSet}
 import org.apache.calcite.plan.RelOptRule.{any, operand}
 import org.apache.calcite.rel.RelNode
@@ -76,20 +75,27 @@ abstract class StreamPhysicalJoinRuleBase(description: String)
       RelOptRule.convert(input, requiredTraitSet)
     }
 
-    val joinInfo = join.analyzeCondition
-    val providedTraitSet: RelTraitSet = join.getTraitSet.replace(FlinkConventions.STREAM_PHYSICAL)
-
-    val leftConversion: RelNode => RelNode = leftInput => {
-      convertInput(leftInput, joinInfo.leftKeys)
-    }
-    val rightConversion: RelNode => RelNode = rightInput => {
-      convertInput(rightInput, joinInfo.rightKeys)
-    }
-
-    val newJoin = transform(join, left, leftConversion, right, rightConversion, providedTraitSet)
+    val newJoin = transform(
+      join,
+      left,
+      leftInput => {
+        convertInput(leftInput, computeJoinLeftKeys(join))
+      },
+      right,
+      rightInput => {
+        convertInput(rightInput, computeJoinRightKeys(join))
+      },
+      join.getTraitSet.replace(FlinkConventions.STREAM_PHYSICAL)
+    )
     call.transformTo(newJoin)
   }
 
+  protected def computeJoinLeftKeys(join: FlinkLogicalJoin): util.Collection[Integer] =
+    join.analyzeCondition().leftKeys
+
+  protected def computeJoinRightKeys(join: FlinkLogicalJoin): util.Collection[Integer] =
+    join.analyzeCondition().rightKeys
+
   protected def transform(
       join: FlinkLogicalJoin,
       leftInput: FlinkRelNode,
diff --git a/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/join/IntervalJoinTest.xml b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/join/IntervalJoinTest.xml
index 0571d735432..331c8bbabcc 100644
--- a/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/join/IntervalJoinTest.xml
+++ b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/join/IntervalJoinTest.xml
@@ -196,10 +196,10 @@ LogicalProject(a=[$0], b=[$6])
       <![CDATA[
 Calc(select=[a, b])
 +- IntervalJoin(joinType=[InnerJoin], windowBounds=[isRowTime=true, leftLowerBound=0, leftUpperBound=0, leftTimeIndex=1, rightTimeIndex=2], where=[((a = a0) AND (rowtime = rowtime0))], select=[a, rowtime, a0, b, rowtime0])
-   :- Exchange(distribution=[hash[a, rowtime]])
+   :- Exchange(distribution=[hash[a]])
    :  +- Calc(select=[a, rowtime])
    :     +- DataStreamScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c, proctime, rowtime])
-   +- Exchange(distribution=[hash[a, rowtime]])
+   +- Exchange(distribution=[hash[a]])
       +- Calc(select=[a, b, rowtime])
          +- DataStreamScan(table=[[default_catalog, default_database, MyTable2]], fields=[a, b, c, proctime, rowtime])
 ]]>
@@ -403,10 +403,10 @@ LogicalProject(a=[$0], b=[$6])
       <![CDATA[
 Calc(select=[a, b])
 +- IntervalJoin(joinType=[InnerJoin], windowBounds=[isRowTime=false, leftLowerBound=0, leftUpperBound=0, leftTimeIndex=1, rightTimeIndex=2], where=[((a = a0) AND (proctime = proctime0))], select=[a, proctime, a0, b, proctime0])
-   :- Exchange(distribution=[hash[a, proctime]])
+   :- Exchange(distribution=[hash[a]])
    :  +- Calc(select=[a, proctime])
    :     +- DataStreamScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c, proctime, rowtime])
-   +- Exchange(distribution=[hash[a, proctime]])
+   +- Exchange(distribution=[hash[a]])
       +- Calc(select=[a, b, proctime])
          +- DataStreamScan(table=[[default_catalog, default_database, MyTable2]], fields=[a, b, c, proctime, rowtime])
 ]]>
diff --git a/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/stream/table/JoinTest.xml b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/stream/table/JoinTest.xml
index 5e18cc02263..139464d4b97 100644
--- a/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/stream/table/JoinTest.xml
+++ b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/stream/table/JoinTest.xml
@@ -226,10 +226,10 @@ LogicalProject(a=[$0], e=[$5], lptime=[$3])
       <![CDATA[
 Calc(select=[a, e, PROCTIME_MATERIALIZE(lptime) AS lptime])
 +- IntervalJoin(joinType=[InnerJoin], windowBounds=[isRowTime=false, leftLowerBound=0, leftUpperBound=0, leftTimeIndex=1, rightTimeIndex=2], where=[((a = d) AND (lptime = rptime))], select=[a, lptime, d, e, rptime])
-   :- Exchange(distribution=[hash[a, lptime]])
+   :- Exchange(distribution=[hash[a]])
    :  +- Calc(select=[a, lptime])
    :     +- DataStreamScan(table=[[default_catalog, default_database, T1]], fields=[a, b, c, lptime])
-   +- Exchange(distribution=[hash[d, rptime]])
+   +- Exchange(distribution=[hash[d]])
       +- Calc(select=[d, e, rptime])
          +- DataStreamScan(table=[[default_catalog, default_database, T2]], fields=[d, e, f, rptime])
 ]]>
diff --git a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/IntervalJoinITCase.scala b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/IntervalJoinITCase.scala
index baffaadc703..4f637db511d 100644
--- a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/IntervalJoinITCase.scala
+++ b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/IntervalJoinITCase.scala
@@ -18,6 +18,7 @@
 
 package org.apache.flink.table.planner.runtime.stream.sql
 
+import org.apache.flink.api.common.eventtime.{SerializableTimestampAssigner, WatermarkStrategy}
 import org.apache.flink.api.scala._
 import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks
 import org.apache.flink.streaming.api.watermark.Watermark
@@ -26,7 +27,6 @@ import org.apache.flink.table.api.bridge.scala._
 import org.apache.flink.table.planner.runtime.utils.StreamingWithStateTestBase.StateBackendMode
 import org.apache.flink.table.planner.runtime.utils._
 import org.apache.flink.types.Row
-
 import org.junit.Assert._
 import org.junit.Test
 import org.junit.runner.RunWith
@@ -378,32 +378,45 @@ class IntervalJoinITCase(mode: StateBackendMode) extends StreamingWithStateTestB
 
     val sqlQuery =
       """
-        |SELECT t2.key, t2.id, t1.id
+        |SELECT t1.key, t1._2, t1.val, t2.val
         |FROM T1 AS t1 JOIN T2 AS t2 ON
         |t1.key = t2.key AND
         |t2.rowtime = t1.rowtime
       """.stripMargin
 
-    val data1 = new mutable.MutableList[(Int, Long, String, Long)]
+    val data1 = new mutable.MutableList[(String, Long, String)]
+    data1.+=(("K1", 1000L, "L1"))
+    data1.+=(("K1", 1000L, "L2"))
+    data1.+=(("K1", 1000L, "L3"))
+    data1.+=(("K2", 2000L, "L4"))
+    data1.+=(("K1", 4000L, "L5"))
+    data1.+=(("K1", 1000L, "should-be-discarded"))
+    data1.+=(("K1", 6000L, "L7"))
+    data1.+=(("K1", 5001L, "L8"))
+    data1.+=(("K2", 1000L, "should-be-discarded"))
 
-    data1.+=((4, 4000L, "A", 4000L))
-    data1.+=((5, 5000L, "A", 5000L))
-    data1.+=((6, 6000L, "A", 6000L))
-    data1.+=((6, 6000L, "B", 6000L))
+    val data2 = new mutable.MutableList[(String, Long, String)]
+    data2.+=(("K1", 1000L, "R1"))
+    data2.+=(("K1", 1000L, "R2"))
+    data2.+=(("K1", 1000L, "R3"))
+    data2.+=(("K2", 3000L, "R4"))
+    data2.+=(("K1", 4000L, "R5"))
+    data2.+=(("K1", 6000L, "R6"))
+    data2.+=(("K1", 5001L, "R7"))
 
-    val data2 = new mutable.MutableList[(String, String, Long)]
-    data2.+=(("A", "R-5", 5000L))
-    data2.+=(("B", "R-6", 6000L))
+    val schema = Schema.newBuilder()
+      .columnByExpression("key", "_1")
+      .columnByExpression("rowtime", "TO_TIMESTAMP_LTZ(_2, 3)")
+      .columnByExpression("val", "_3")
+      .watermark("rowtime", "rowtime - INTERVAL '1' SECOND")
+      .build()
 
-    val t1 = env.fromCollection(data1)
-      .assignTimestampsAndWatermarks(new Row4WatermarkExtractor)
-      .toTable(tEnv, 'id, 'tm, 'key, 'rowtime)
-    val t2 = env.fromCollection(data2)
-      .assignTimestampsAndWatermarks(new Row3WatermarkExtractor2)
-      .toTable(tEnv, 'key, 'id, 'rowtime)
+    val t1 = tEnv.fromDataStream(env.fromCollection(data1), schema)
 
-    tEnv.registerTable("T1", t1)
-    tEnv.registerTable("T2", t2)
+    val t2 = tEnv.fromDataStream(env.fromCollection(data2), schema)
+
+    tEnv.createTemporaryView("T1", t1)
+    tEnv.createTemporaryView("T2", t2)
 
     val sink = new TestingAppendSink
     val result = tEnv.sqlQuery(sqlQuery).toAppendStream[Row]
@@ -411,8 +424,18 @@ class IntervalJoinITCase(mode: StateBackendMode) extends StreamingWithStateTestB
     env.execute()
 
     val expected = mutable.MutableList[String](
-      "A,R-5,5",
-      "B,R-6,6"
+      "K1,1000,L1,R1",
+      "K1,1000,L1,R2",
+      "K1,1000,L1,R3",
+      "K1,1000,L2,R1",
+      "K1,1000,L2,R2",
+      "K1,1000,L2,R3",
+      "K1,1000,L3,R1",
+      "K1,1000,L3,R2",
+      "K1,1000,L3,R3",
+      "K1,4000,L5,R5",
+      "K1,6000,L7,R6",
+      "K1,5001,L8,R7"
     )
 
     assertEquals(expected.toList.sorted, sink.getAppendResults.sorted)
