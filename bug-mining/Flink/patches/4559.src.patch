diff --git a/flink-connectors/flink-connector-hbase/src/main/java/org/apache/flink/connector/hbase/source/AbstractTableInputFormat.java b/flink-connectors/flink-connector-hbase/src/main/java/org/apache/flink/connector/hbase/source/AbstractTableInputFormat.java
index feeff3db387..6cd7a4d05db 100644
--- a/flink-connectors/flink-connector-hbase/src/main/java/org/apache/flink/connector/hbase/source/AbstractTableInputFormat.java
+++ b/flink-connectors/flink-connector-hbase/src/main/java/org/apache/flink/connector/hbase/source/AbstractTableInputFormat.java
@@ -19,6 +19,7 @@
 package org.apache.flink.connector.hbase.source;
 
 import org.apache.flink.annotation.Internal;
+import org.apache.flink.annotation.VisibleForTesting;
 import org.apache.flink.api.common.io.InputFormat;
 import org.apache.flink.api.common.io.LocatableInputSplitAssigner;
 import org.apache.flink.api.common.io.RichInputFormat;
@@ -27,6 +28,7 @@ import org.apache.flink.configuration.Configuration;
 import org.apache.flink.connector.hbase.util.HBaseConfigurationUtil;
 import org.apache.flink.core.io.InputSplitAssigner;
 
+import org.apache.hadoop.hbase.client.Connection;
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.client.ResultScanner;
@@ -44,7 +46,7 @@ import java.util.List;
  * Abstract {@link InputFormat} to read data from HBase tables.
  */
 @Internal
-abstract class AbstractTableInputFormat<T> extends RichInputFormat<T, TableInputSplit> {
+public abstract class AbstractTableInputFormat<T> extends RichInputFormat<T, TableInputSplit> {
 
 	protected static final Logger LOG = LoggerFactory.getLogger(AbstractTableInputFormat.class);
 	private static final long serialVersionUID = 1L;
@@ -52,6 +54,7 @@ abstract class AbstractTableInputFormat<T> extends RichInputFormat<T, TableInput
 	// helper variable to decide whether the input is exhausted or not
 	protected boolean endReached = false;
 
+	protected transient Connection connection = null;
 	protected transient HTable table = null;
 	protected transient Scan scan = null;
 
@@ -68,6 +71,13 @@ abstract class AbstractTableInputFormat<T> extends RichInputFormat<T, TableInput
 		serializedConfig = HBaseConfigurationUtil.serializeConfiguration(hConf);
 	}
 
+	/**
+	 * Creates a {@link Scan} object and opens the {@link HTable} connection to initialize the HBase table.
+	 *
+	 * @throws IOException Thrown, if the connection could not be opened due to an I/O problem.
+	 */
+	protected abstract void initTable() throws IOException;
+
 	/**
 	 * Returns an instance of Scan that retrieves the required subset of records from the HBase table.
 	 *
@@ -94,33 +104,25 @@ abstract class AbstractTableInputFormat<T> extends RichInputFormat<T, TableInput
 	 */
 	protected abstract T mapResultToOutType(Result r);
 
-	/**
-	 * Creates a {@link Scan} object and opens the {@link HTable} connection.
-	 *
-	 * <p>These are opened here because they are needed in the createInputSplits
-	 * which is called before the openInputFormat method.
-	 *
-	 * <p>The connection is opened in this method and closed in {@link #closeInputFormat()}.
-	 *
-	 * @param parameters The configuration that is to be used
-	 * @see Configuration
-	 */
-	public abstract void configure(Configuration parameters);
+	@Override
+	public void configure(Configuration parameters) {
+	}
 
 	protected org.apache.hadoop.conf.Configuration getHadoopConfiguration() {
 		return HBaseConfigurationUtil.deserializeConfiguration(serializedConfig, HBaseConfigurationUtil.getHBaseConfiguration());
 	}
 
+	/**
+	 * Creates a {@link Scan} object and opens the {@link HTable} connection.
+	 * The connection is opened in this method and closed in {@link #close()}.
+	 *
+	 * @param split The split to be opened.
+	 * @throws IOException Thrown, if the spit could not be opened due to an I/O problem.
+	 */
 	@Override
 	public void open(TableInputSplit split) throws IOException {
-		if (table == null) {
-			throw new IOException("The HBase table has not been opened! " +
-				"This needs to be done in configure().");
-		}
-		if (scan == null) {
-			throw new IOException("Scan has not been initialized! " +
-				"This needs to be done in configure().");
-		}
+		initTable();
+
 		if (split == null) {
 			throw new IOException("Input split is null!");
 		}
@@ -186,73 +188,79 @@ abstract class AbstractTableInputFormat<T> extends RichInputFormat<T, TableInput
 			if (resultScanner != null) {
 				resultScanner.close();
 			}
+			closeTable();
 		} finally {
 			resultScanner = null;
 		}
 	}
 
-	@Override
-	public void closeInputFormat() throws IOException {
-		try {
-			if (table != null) {
+	public void closeTable() {
+		if (table != null) {
+			try {
 				table.close();
+			} catch (IOException e) {
+				LOG.warn("Exception occurs while closing HBase Table.", e);
 			}
-		} finally {
 			table = null;
 		}
+		if (connection != null) {
+			try {
+				connection.close();
+			} catch (IOException e) {
+				LOG.warn("Exception occurs while closing HBase Connection.", e);
+			}
+			connection = null;
+		}
 	}
 
 	@Override
 	public TableInputSplit[] createInputSplits(final int minNumSplits) throws IOException {
-		if (table == null) {
-			throw new IOException("The HBase table has not been opened! " +
-				"This needs to be done in configure().");
-		}
-		if (scan == null) {
-			throw new IOException("Scan has not been initialized! " +
-				"This needs to be done in configure().");
-		}
+		try {
+			initTable();
 
-		// Get the starting and ending row keys for every region in the currently open table
-		final Pair<byte[][], byte[][]> keys = table.getRegionLocator().getStartEndKeys();
-		if (keys == null || keys.getFirst() == null || keys.getFirst().length == 0) {
-			throw new IOException("Expecting at least one region.");
-		}
-		final byte[] startRow = scan.getStartRow();
-		final byte[] stopRow = scan.getStopRow();
-		final boolean scanWithNoLowerBound = startRow.length == 0;
-		final boolean scanWithNoUpperBound = stopRow.length == 0;
-
-		final List<TableInputSplit> splits = new ArrayList<TableInputSplit>(minNumSplits);
-		for (int i = 0; i < keys.getFirst().length; i++) {
-			final byte[] startKey = keys.getFirst()[i];
-			final byte[] endKey = keys.getSecond()[i];
-			final String regionLocation = table.getRegionLocator().getRegionLocation(startKey, false).getHostnamePort();
-			// Test if the given region is to be included in the InputSplit while splitting the regions of a table
-			if (!includeRegionInScan(startKey, endKey)) {
-				continue;
+			// Get the starting and ending row keys for every region in the currently open table
+			final Pair<byte[][], byte[][]> keys = table.getRegionLocator().getStartEndKeys();
+			if (keys == null || keys.getFirst() == null || keys.getFirst().length == 0) {
+				throw new IOException("Expecting at least one region.");
 			}
-			// Find the region on which the given row is being served
-			final String[] hosts = new String[]{regionLocation};
-
-			// Determine if regions contains keys used by the scan
-			boolean isLastRegion = endKey.length == 0;
-			if ((scanWithNoLowerBound || isLastRegion || Bytes.compareTo(startRow, endKey) < 0) &&
-				(scanWithNoUpperBound || Bytes.compareTo(stopRow, startKey) > 0)) {
-
-				final byte[] splitStart = scanWithNoLowerBound || Bytes.compareTo(startKey, startRow) >= 0 ? startKey : startRow;
-				final byte[] splitStop = (scanWithNoUpperBound || Bytes.compareTo(endKey, stopRow) <= 0)
-					&& !isLastRegion ? endKey : stopRow;
-				int id = splits.size();
-				final TableInputSplit split = new TableInputSplit(id, hosts, table.getTableName(), splitStart, splitStop);
-				splits.add(split);
+			final byte[] startRow = scan.getStartRow();
+			final byte[] stopRow = scan.getStopRow();
+			final boolean scanWithNoLowerBound = startRow.length == 0;
+			final boolean scanWithNoUpperBound = stopRow.length == 0;
+
+			final List<TableInputSplit> splits = new ArrayList<>(minNumSplits);
+			for (int i = 0; i < keys.getFirst().length; i++) {
+				final byte[] startKey = keys.getFirst()[i];
+				final byte[] endKey = keys.getSecond()[i];
+				final String regionLocation = table.getRegionLocator().getRegionLocation(startKey, false).getHostnamePort();
+				// Test if the given region is to be included in the InputSplit while splitting the regions of a table
+				if (!includeRegionInScan(startKey, endKey)) {
+					continue;
+				}
+				// Find the region on which the given row is being served
+				final String[] hosts = new String[]{regionLocation};
+
+				// Determine if regions contains keys used by the scan
+				boolean isLastRegion = endKey.length == 0;
+				if ((scanWithNoLowerBound || isLastRegion || Bytes.compareTo(startRow, endKey) < 0) &&
+					(scanWithNoUpperBound || Bytes.compareTo(stopRow, startKey) > 0)) {
+
+					final byte[] splitStart = scanWithNoLowerBound || Bytes.compareTo(startKey, startRow) >= 0 ? startKey : startRow;
+					final byte[] splitStop = (scanWithNoUpperBound || Bytes.compareTo(endKey, stopRow) <= 0)
+						&& !isLastRegion ? endKey : stopRow;
+					int id = splits.size();
+					final TableInputSplit split = new TableInputSplit(id, hosts, table.getTableName(), splitStart, splitStop);
+					splits.add(split);
+				}
 			}
+			LOG.info("Created " + splits.size() + " splits");
+			for (TableInputSplit split : splits) {
+				logSplitInfo("created", split);
+			}
+			return splits.toArray(new TableInputSplit[splits.size()]);
+		} finally {
+			closeTable();
 		}
-		LOG.info("Created " + splits.size() + " splits");
-		for (TableInputSplit split : splits) {
-			logSplitInfo("created", split);
-		}
-		return splits.toArray(new TableInputSplit[splits.size()]);
 	}
 
 	/**
@@ -276,4 +284,8 @@ abstract class AbstractTableInputFormat<T> extends RichInputFormat<T, TableInput
 		return null;
 	}
 
+	@VisibleForTesting
+	public Connection getConnection() {
+		return connection;
+	}
 }
diff --git a/flink-connectors/flink-connector-hbase/src/main/java/org/apache/flink/connector/hbase/source/HBaseInputFormat.java b/flink-connectors/flink-connector-hbase/src/main/java/org/apache/flink/connector/hbase/source/HBaseInputFormat.java
index 45d5b3fc935..8076e8fa496 100644
--- a/flink-connectors/flink-connector-hbase/src/main/java/org/apache/flink/connector/hbase/source/HBaseInputFormat.java
+++ b/flink-connectors/flink-connector-hbase/src/main/java/org/apache/flink/connector/hbase/source/HBaseInputFormat.java
@@ -21,12 +21,13 @@ package org.apache.flink.connector.hbase.source;
 import org.apache.flink.annotation.Experimental;
 import org.apache.flink.api.common.io.InputFormat;
 import org.apache.flink.api.java.tuple.Tuple;
-import org.apache.flink.configuration.Configuration;
 
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.client.Scan;
 
+import java.io.IOException;
+
 /**
  * {@link InputFormat} subclass that wraps the access for HTables.
  */
@@ -65,19 +66,12 @@ public abstract class HBaseInputFormat<T extends Tuple> extends AbstractTableInp
 	 */
 	protected abstract T mapResultToTuple(Result r);
 
-	/**
-	 * Creates a {@link Scan} object and opens the {@link HTable} connection.
-	 * These are opened here because they are needed in the createInputSplits
-	 * which is called before the openInputFormat method.
-	 * So the connection is opened in {@link #configure(Configuration)} and closed in {@link #closeInputFormat()}.
-	 *
-	 * @param parameters The configuration that is to be used
-	 * @see Configuration
-	 */
 	@Override
-	public void configure(Configuration parameters) {
-		table = createTable();
-		if (table != null) {
+	protected void initTable() throws IOException {
+		if (table == null) {
+			table = createTable();
+		}
+		if (table != null && scan == null) {
 			scan = getScanner();
 		}
 	}
diff --git a/flink-connectors/flink-connector-hbase/src/main/java/org/apache/flink/connector/hbase/source/HBaseRowDataInputFormat.java b/flink-connectors/flink-connector-hbase/src/main/java/org/apache/flink/connector/hbase/source/HBaseRowDataInputFormat.java
index 30be6d87a01..43a4fade1a6 100644
--- a/flink-connectors/flink-connector-hbase/src/main/java/org/apache/flink/connector/hbase/source/HBaseRowDataInputFormat.java
+++ b/flink-connectors/flink-connector-hbase/src/main/java/org/apache/flink/connector/hbase/source/HBaseRowDataInputFormat.java
@@ -19,14 +19,12 @@
 package org.apache.flink.connector.hbase.source;
 
 import org.apache.flink.api.common.io.InputFormat;
-import org.apache.flink.configuration.Configuration;
 import org.apache.flink.connector.hbase.util.HBaseSerde;
 import org.apache.flink.connector.hbase.util.HBaseTableSchema;
 import org.apache.flink.table.data.RowData;
 
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.TableNotFoundException;
-import org.apache.hadoop.hbase.client.Connection;
 import org.apache.hadoop.hbase.client.ConnectionFactory;
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Result;
@@ -61,11 +59,12 @@ public class HBaseRowDataInputFormat extends AbstractTableInputFormat<RowData> {
 	}
 
 	@Override
-	public void configure(Configuration parameters) {
-		LOG.info("Initializing HBase configuration.");
+	protected void initTable() throws IOException {
 		this.serde = new HBaseSerde(schema, nullStringLiteral);
-		connectToTable();
-		if (table != null) {
+		if (table == null) {
+			connectToTable();
+		}
+		if (table != null && scan == null) {
 			scan = getScanner();
 		}
 	}
@@ -85,16 +84,13 @@ public class HBaseRowDataInputFormat extends AbstractTableInputFormat<RowData> {
 		return serde.convertToRow(res);
 	}
 
-	private void connectToTable() {
+	private void connectToTable() throws IOException {
 		try {
-			Connection conn = ConnectionFactory.createConnection(getHadoopConfiguration());
-			super.table = (HTable) conn.getTable(TableName.valueOf(tableName));
+			connection = ConnectionFactory.createConnection(getHadoopConfiguration());
+			table = (HTable) connection.getTable(TableName.valueOf(tableName));
 		} catch (TableNotFoundException tnfe) {
 			LOG.error("The table " + tableName + " not found ", tnfe);
 			throw new RuntimeException("HBase table '" + tableName + "' not found.", tnfe);
-		} catch (IOException ioe) {
-			LOG.error("Exception while creating connection to HBase.", ioe);
-			throw new RuntimeException("Cannot create connection to HBase.", ioe);
 		}
 	}
 }
diff --git a/flink-connectors/flink-connector-hbase/src/main/java/org/apache/flink/connector/hbase/source/HBaseRowInputFormat.java b/flink-connectors/flink-connector-hbase/src/main/java/org/apache/flink/connector/hbase/source/HBaseRowInputFormat.java
index f7100ed7a8f..4d5f5b4c471 100644
--- a/flink-connectors/flink-connector-hbase/src/main/java/org/apache/flink/connector/hbase/source/HBaseRowInputFormat.java
+++ b/flink-connectors/flink-connector-hbase/src/main/java/org/apache/flink/connector/hbase/source/HBaseRowInputFormat.java
@@ -23,14 +23,12 @@ import org.apache.flink.api.common.io.InputFormat;
 import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.api.java.typeutils.ResultTypeQueryable;
 import org.apache.flink.api.java.typeutils.RowTypeInfo;
-import org.apache.flink.configuration.Configuration;
 import org.apache.flink.connector.hbase.util.HBaseReadWriteHelper;
 import org.apache.flink.connector.hbase.util.HBaseTableSchema;
 import org.apache.flink.types.Row;
 
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.TableNotFoundException;
-import org.apache.hadoop.hbase.client.Connection;
 import org.apache.hadoop.hbase.client.ConnectionFactory;
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Result;
@@ -62,12 +60,12 @@ public class HBaseRowInputFormat extends AbstractTableInputFormat<Row> implement
 	}
 
 	@Override
-	public void configure(Configuration parameters) {
-		LOG.info("Initializing HBase configuration.");
-		// prepare hbase read helper
+	public void initTable() throws IOException {
 		this.readHelper = new HBaseReadWriteHelper(schema);
-		connectToTable();
-		if (table != null) {
+		if (table == null) {
+			connectToTable();
+		}
+		if (table != null && scan == null) {
 			scan = getScanner();
 		}
 	}
@@ -87,16 +85,13 @@ public class HBaseRowInputFormat extends AbstractTableInputFormat<Row> implement
 		return readHelper.parseToRow(res);
 	}
 
-	private void connectToTable() {
+	private void connectToTable() throws IOException {
 		try {
-			Connection conn = ConnectionFactory.createConnection(getHadoopConfiguration());
-			super.table = (HTable) conn.getTable(TableName.valueOf(tableName));
+			connection = ConnectionFactory.createConnection(getHadoopConfiguration());
+			table = (HTable) connection.getTable(TableName.valueOf(tableName));
 		} catch (TableNotFoundException tnfe) {
 			LOG.error("The table " + tableName + " not found ", tnfe);
 			throw new RuntimeException("HBase table '" + tableName + "' not found.", tnfe);
-		} catch (IOException ioe) {
-			LOG.error("Exception while creating connection to HBase.", ioe);
-			throw new RuntimeException("Cannot create connection to HBase.", ioe);
 		}
 	}
 
diff --git a/flink-connectors/flink-connector-hbase/src/test/java/org/apache/flink/connector/hbase/HBaseConnectorITCase.java b/flink-connectors/flink-connector-hbase/src/test/java/org/apache/flink/connector/hbase/HBaseConnectorITCase.java
index 84a4c6df3db..a784805bc14 100644
--- a/flink-connectors/flink-connector-hbase/src/test/java/org/apache/flink/connector/hbase/HBaseConnectorITCase.java
+++ b/flink-connectors/flink-connector-hbase/src/test/java/org/apache/flink/connector/hbase/HBaseConnectorITCase.java
@@ -25,8 +25,12 @@ import org.apache.flink.api.java.DataSet;
 import org.apache.flink.api.java.ExecutionEnvironment;
 import org.apache.flink.api.java.tuple.Tuple1;
 import org.apache.flink.api.java.typeutils.RowTypeInfo;
+import org.apache.flink.connector.hbase.source.AbstractTableInputFormat;
 import org.apache.flink.connector.hbase.source.HBaseInputFormat;
+import org.apache.flink.connector.hbase.source.HBaseRowDataInputFormat;
+import org.apache.flink.connector.hbase.source.HBaseRowInputFormat;
 import org.apache.flink.connector.hbase.source.HBaseTableSource;
+import org.apache.flink.connector.hbase.util.HBaseTableSchema;
 import org.apache.flink.connector.hbase.util.HBaseTestBase;
 import org.apache.flink.connector.hbase.util.PlannerType;
 import org.apache.flink.streaming.api.datastream.DataStream;
@@ -45,6 +49,7 @@ import org.apache.flink.test.util.TestBaseUtils;
 import org.apache.flink.types.Row;
 import org.apache.flink.util.CollectionUtil;
 
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.client.Scan;
 import org.apache.hadoop.hbase.util.Bytes;
@@ -52,6 +57,7 @@ import org.junit.Test;
 import org.junit.runner.RunWith;
 import org.junit.runners.Parameterized;
 
+import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Iterator;
 import java.util.List;
@@ -60,6 +66,8 @@ import java.util.stream.Collectors;
 import static org.apache.flink.connector.hbase.util.PlannerType.OLD_PLANNER;
 import static org.apache.flink.table.api.Expressions.$;
 import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertNotNull;
+import static org.junit.Assert.assertNull;
 
 /**
  * IT cases for HBase connector (including HBaseTableSource and HBaseTableSink).
@@ -499,6 +507,24 @@ public class HBaseConnectorITCase extends HBaseTestBase {
 		assertEquals(expected, result);
 	}
 
+	@Test
+	public void testTableInputFormatOpenClose() throws IOException {
+		HBaseTableSchema tableSchema = new HBaseTableSchema();
+		tableSchema.addColumn(FAMILY1, F1COL1, byte[].class);
+		AbstractTableInputFormat<?> inputFormat;
+		if (isLegacyConnector) {
+			inputFormat = new HBaseRowInputFormat(getConf(), TEST_TABLE_1, tableSchema);
+		} else {
+			inputFormat = new HBaseRowDataInputFormat(getConf(), TEST_TABLE_1, tableSchema, "null");
+		}
+		inputFormat.open(inputFormat.createInputSplits(1)[0]);
+		assertNotNull(inputFormat.getConnection());
+		assertNotNull(inputFormat.getConnection().getTable(TableName.valueOf(TEST_TABLE_1)));
+
+		inputFormat.close();
+		assertNull(inputFormat.getConnection());
+	}
+
 	// -------------------------------------------------------------------------------------
 	// HBase lookup source tests
 	// -------------------------------------------------------------------------------------
