diff --git a/docs/dev/table/connectors/filesystem.md b/docs/dev/table/connectors/filesystem.md
index a39e76ea5e3..1300bfa510a 100644
--- a/docs/dev/table/connectors/filesystem.md
+++ b/docs/dev/table/connectors/filesystem.md
@@ -129,10 +129,17 @@ a timeout that specifies the maximum duration for which a file can be open.
         <td>The maximum part file size before rolling.</td>
     </tr>
     <tr>
-        <td><h5>sink.rolling-policy.time-interval</h5></td>
+        <td><h5>sink.rolling-policy.rollover-interval</h5></td>
         <td style="word-wrap: break-word;">30 m</td>
         <td>Duration</td>
-        <td>The maximum time duration a part file can stay open before rolling (by default 30 min to avoid to many small files).</td>
+        <td>The maximum time duration a part file can stay open before rolling (by default 30 min to avoid to many small files).
+        The frequency at which this is checked is controlled by the 'sink.rolling-policy.check-interval' option.</td>
+    </tr>
+    <tr>
+        <td><h5>sink.rolling-policy.check-interval</h5></td>
+        <td style="word-wrap: break-word;">1 m</td>
+        <td>Duration</td>
+        <td>The interval for checking time based rolling policies. This controls the frequency to check whether a part file should rollover based on 'sink.rolling-policy.rollover-interval'.</td>
     </tr>
   </tbody>
 </table>
@@ -140,7 +147,7 @@ a timeout that specifies the maximum duration for which a file can be open.
 **NOTE:** For bulk formats (parquet, orc, avro), the rolling policy in combination with the checkpoint interval(pending files
 become finished on the next checkpoint) control the size and number of these parts.
 
-**NOTE:** For row formats (csv, json), you can set the parameter `sink.rolling-policy.file-size` or `sink.rolling-policy.time-interval` in the connector properties and parameter `execution.checkpointing.interval` in flink-conf.yaml together
+**NOTE:** For row formats (csv, json), you can set the parameter `sink.rolling-policy.file-size` or `sink.rolling-policy.rollover-interval` in the connector properties and parameter `execution.checkpointing.interval` in flink-conf.yaml together
 if you don't want to wait a long period before observe the data exists in file system. For other formats (avro, orc), you can just set parameter `execution.checkpointing.interval` in flink-conf.yaml.
 
 ### Partition Commit
diff --git a/docs/dev/table/connectors/filesystem.zh.md b/docs/dev/table/connectors/filesystem.zh.md
index a39e76ea5e3..1300bfa510a 100644
--- a/docs/dev/table/connectors/filesystem.zh.md
+++ b/docs/dev/table/connectors/filesystem.zh.md
@@ -129,10 +129,17 @@ a timeout that specifies the maximum duration for which a file can be open.
         <td>The maximum part file size before rolling.</td>
     </tr>
     <tr>
-        <td><h5>sink.rolling-policy.time-interval</h5></td>
+        <td><h5>sink.rolling-policy.rollover-interval</h5></td>
         <td style="word-wrap: break-word;">30 m</td>
         <td>Duration</td>
-        <td>The maximum time duration a part file can stay open before rolling (by default 30 min to avoid to many small files).</td>
+        <td>The maximum time duration a part file can stay open before rolling (by default 30 min to avoid to many small files).
+        The frequency at which this is checked is controlled by the 'sink.rolling-policy.check-interval' option.</td>
+    </tr>
+    <tr>
+        <td><h5>sink.rolling-policy.check-interval</h5></td>
+        <td style="word-wrap: break-word;">1 m</td>
+        <td>Duration</td>
+        <td>The interval for checking time based rolling policies. This controls the frequency to check whether a part file should rollover based on 'sink.rolling-policy.rollover-interval'.</td>
     </tr>
   </tbody>
 </table>
@@ -140,7 +147,7 @@ a timeout that specifies the maximum duration for which a file can be open.
 **NOTE:** For bulk formats (parquet, orc, avro), the rolling policy in combination with the checkpoint interval(pending files
 become finished on the next checkpoint) control the size and number of these parts.
 
-**NOTE:** For row formats (csv, json), you can set the parameter `sink.rolling-policy.file-size` or `sink.rolling-policy.time-interval` in the connector properties and parameter `execution.checkpointing.interval` in flink-conf.yaml together
+**NOTE:** For row formats (csv, json), you can set the parameter `sink.rolling-policy.file-size` or `sink.rolling-policy.rollover-interval` in the connector properties and parameter `execution.checkpointing.interval` in flink-conf.yaml together
 if you don't want to wait a long period before observe the data exists in file system. For other formats (avro, orc), you can just set parameter `execution.checkpointing.interval` in flink-conf.yaml.
 
 ### Partition Commit
diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java
index c49905cb781..03f21df2515 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java
@@ -79,8 +79,9 @@ import java.util.Map;
 import java.util.Optional;
 import java.util.UUID;
 
+import static org.apache.flink.table.filesystem.FileSystemOptions.SINK_ROLLING_POLICY_CHECK_INTERVAL;
 import static org.apache.flink.table.filesystem.FileSystemOptions.SINK_ROLLING_POLICY_FILE_SIZE;
-import static org.apache.flink.table.filesystem.FileSystemOptions.SINK_ROLLING_POLICY_TIME_INTERVAL;
+import static org.apache.flink.table.filesystem.FileSystemOptions.SINK_ROLLING_POLICY_ROLLOVER_INTERVAL;
 
 /**
  * Table sink to write to Hive tables.
@@ -186,7 +187,7 @@ public class HiveTableSink implements AppendStreamTableSink, PartitionableTableS
 				TableRollingPolicy rollingPolicy = new TableRollingPolicy(
 						true,
 						conf.get(SINK_ROLLING_POLICY_FILE_SIZE).getBytes(),
-						conf.get(SINK_ROLLING_POLICY_TIME_INTERVAL).toMillis());
+						conf.get(SINK_ROLLING_POLICY_ROLLOVER_INTERVAL).toMillis());
 
 				Optional<BulkWriter.Factory<RowData>> bulkFactory = createBulkWriterFactory(partitionColumns, sd);
 				BucketsBuilder<RowData, String, ? extends BucketsBuilder<RowData, ?, ?>> builder;
@@ -215,7 +216,8 @@ public class HiveTableSink implements AppendStreamTableSink, PartitionableTableS
 						dataStream,
 						builder,
 						msFactory,
-						fsFactory);
+						fsFactory,
+						conf.get(SINK_ROLLING_POLICY_CHECK_INTERVAL).toMillis());
 			}
 		} catch (TException e) {
 			throw new CatalogException("Failed to query Hive metaStore", e);
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java
index 665081b5dd1..ab1687afb7e 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java
@@ -46,11 +46,18 @@ public class FileSystemOptions {
 			.defaultValue(MemorySize.ofMebiBytes(128))
 			.withDescription("The maximum part file size before rolling (by default 128MB).");
 
-	public static final ConfigOption<Duration> SINK_ROLLING_POLICY_TIME_INTERVAL = key("sink.rolling-policy.time-interval")
+	public static final ConfigOption<Duration> SINK_ROLLING_POLICY_ROLLOVER_INTERVAL = key("sink.rolling-policy.rollover-interval")
 			.durationType()
 			.defaultValue(Duration.ofMinutes(30))
 			.withDescription("The maximum time duration a part file can stay open before rolling" +
-					" (by default 30 min to avoid to many small files).");
+					" (by default 30 min to avoid to many small files). The frequency at which" +
+					" this is checked is controlled by the 'sink.rolling-policy.check-interval' option.");
+
+	public static final ConfigOption<Duration> SINK_ROLLING_POLICY_CHECK_INTERVAL = key("sink.rolling-policy.check-interval")
+			.durationType()
+			.defaultValue(Duration.ofMinutes(1))
+			.withDescription("The interval for checking time based rolling policies. " +
+					"This controls the frequency to check whether a part file should rollover based on 'sink.rolling-policy.rollover-interval'.");
 
 	public static final ConfigOption<Boolean> SINK_SHUFFLE_BY_PARTITION = key("sink.shuffle-by-partition.enable")
 			.booleanType()
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java
index d943ee941d6..abdab51d399 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java
@@ -67,8 +67,9 @@ import java.util.Optional;
 import java.util.UUID;
 
 import static org.apache.flink.table.filesystem.FileSystemOptions.SINK_PARTITION_COMMIT_POLICY_KIND;
+import static org.apache.flink.table.filesystem.FileSystemOptions.SINK_ROLLING_POLICY_CHECK_INTERVAL;
 import static org.apache.flink.table.filesystem.FileSystemOptions.SINK_ROLLING_POLICY_FILE_SIZE;
-import static org.apache.flink.table.filesystem.FileSystemOptions.SINK_ROLLING_POLICY_TIME_INTERVAL;
+import static org.apache.flink.table.filesystem.FileSystemOptions.SINK_ROLLING_POLICY_ROLLOVER_INTERVAL;
 import static org.apache.flink.table.filesystem.FileSystemTableFactory.createFormatFactory;
 
 /**
@@ -155,7 +156,7 @@ public class FileSystemTableSink implements
 			TableRollingPolicy rollingPolicy = new TableRollingPolicy(
 					!(writer instanceof Encoder),
 					conf.get(SINK_ROLLING_POLICY_FILE_SIZE).getBytes(),
-					conf.get(SINK_ROLLING_POLICY_TIME_INTERVAL).toMillis());
+					conf.get(SINK_ROLLING_POLICY_ROLLOVER_INTERVAL).toMillis());
 
 			BucketsBuilder<RowData, String, ? extends BucketsBuilder<RowData, ?, ?>> bucketsBuilder;
 			if (writer instanceof Encoder) {
@@ -182,7 +183,8 @@ public class FileSystemTableSink implements
 					dataStream,
 					bucketsBuilder,
 					metaStoreFactory,
-					fsFactory);
+					fsFactory,
+					conf.get(SINK_ROLLING_POLICY_CHECK_INTERVAL).toMillis());
 		}
 	}
 
@@ -195,13 +197,15 @@ public class FileSystemTableSink implements
 			DataStream<RowData> inputStream,
 			BucketsBuilder<RowData, String, ? extends BucketsBuilder<RowData, ?, ?>> bucketsBuilder,
 			TableMetaStoreFactory msFactory,
-			FileSystemFactory fsFactory) {
+			FileSystemFactory fsFactory,
+			long rollingCheckInterval) {
 		if (overwrite) {
 			throw new IllegalStateException("Streaming mode not support overwrite.");
 		}
 
 		StreamingFileWriter fileWriter = new StreamingFileWriter(
-				BucketsBuilder.DEFAULT_BUCKET_CHECK_INTERVAL, bucketsBuilder);
+				rollingCheckInterval,
+				bucketsBuilder);
 		DataStream<CommitMessage> writerStream = inputStream.transform(
 				StreamingFileWriter.class.getSimpleName(),
 				TypeExtractor.createTypeInfo(CommitMessage.class),
