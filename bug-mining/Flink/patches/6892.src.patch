diff --git a/flink-connectors/flink-connector-hbase-1.4/src/test/java/org/apache/flink/connector/hbase1/HBaseConnectorITCase.java b/flink-connectors/flink-connector-hbase-1.4/src/test/java/org/apache/flink/connector/hbase1/HBaseConnectorITCase.java
index 1ad2dd2d196..35be358ad7e 100644
--- a/flink-connectors/flink-connector-hbase-1.4/src/test/java/org/apache/flink/connector/hbase1/HBaseConnectorITCase.java
+++ b/flink-connectors/flink-connector-hbase-1.4/src/test/java/org/apache/flink/connector/hbase1/HBaseConnectorITCase.java
@@ -152,7 +152,7 @@ public class HBaseConnectorITCase extends HBaseTestBase {
 
         tEnv.executeSql(
                 "CREATE TABLE hTable ("
-                        + " rowkey INT PRIMARY KEY,"
+                        + " rowkey INT PRIMARY KEY NOT ENFORCED,"
                         + " family2 ROW<col1 STRING, col2 BIGINT>,"
                         + " family3 ROW<col1 DOUBLE, col2 BOOLEAN, col3 STRING>,"
                         + " family1 ROW<col1 INT>"
diff --git a/flink-connectors/flink-connector-hbase-2.2/src/test/java/org/apache/flink/connector/hbase2/HBaseConnectorITCase.java b/flink-connectors/flink-connector-hbase-2.2/src/test/java/org/apache/flink/connector/hbase2/HBaseConnectorITCase.java
index e75d0d07a27..fbd7b2cf134 100644
--- a/flink-connectors/flink-connector-hbase-2.2/src/test/java/org/apache/flink/connector/hbase2/HBaseConnectorITCase.java
+++ b/flink-connectors/flink-connector-hbase-2.2/src/test/java/org/apache/flink/connector/hbase2/HBaseConnectorITCase.java
@@ -168,7 +168,7 @@ public class HBaseConnectorITCase extends HBaseTestBase {
 
         tEnv.executeSql(
                 "CREATE TABLE hTable ("
-                        + " rowkey INT PRIMARY KEY,"
+                        + " rowkey INT PRIMARY KEY NOT ENFORCED,"
                         + " family2 ROW<col1 STRING, col2 BIGINT>,"
                         + " family3 ROW<col1 DOUBLE, col2 BOOLEAN, col3 STRING>,"
                         + " family1 ROW<col1 INT>"
diff --git a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/endpoint/hive/HiveServer2EndpointITCase.java b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/endpoint/hive/HiveServer2EndpointITCase.java
index 9723a715802..d54ed64732f 100644
--- a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/endpoint/hive/HiveServer2EndpointITCase.java
+++ b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/endpoint/hive/HiveServer2EndpointITCase.java
@@ -750,7 +750,7 @@ public class HiveServer2EndpointITCase extends TestLogger {
 
             statement.execute(
                     "CREATE TABLE db_test1.tbl_1(\n"
-                            + "`user` BIGINT CONSTRAINT `pk` PRIMARY KEY COMMENT 'user id.',\n"
+                            + "`user` BIGINT CONSTRAINT `pk` PRIMARY KEY NOT ENFORCED COMMENT 'user id.',\n"
                             + "`product` STRING NOT NULL,\n"
                             + "`amount`  INT) COMMENT 'temporary table tbl_1'");
             statement.execute(
diff --git a/flink-table/flink-sql-parser/src/main/java/org/apache/flink/sql/parser/SqlConstraintValidator.java b/flink-table/flink-sql-parser/src/main/java/org/apache/flink/sql/parser/SqlConstraintValidator.java
index d0fb49eabdc..8a9a7727b54 100644
--- a/flink-table/flink-sql-parser/src/main/java/org/apache/flink/sql/parser/SqlConstraintValidator.java
+++ b/flink-table/flink-sql-parser/src/main/java/org/apache/flink/sql/parser/SqlConstraintValidator.java
@@ -52,22 +52,29 @@ public class SqlConstraintValidator {
         return ret;
     }
 
-    /** Check duplicate constraints and change the nullability of primary key columns. */
+    /**
+     * Check constraints and change the nullability of primary key columns.
+     *
+     * @throws SqlValidateException if encountered duplicate primary key constraints, or the
+     *     constraint is enforced or unique.
+     */
     public static void validateAndChangeColumnNullability(
             List<SqlTableConstraint> tableConstraints, SqlNodeList columnList)
             throws SqlValidateException {
-        List<SqlTableConstraint> constraints =
-                getFullConstraints(tableConstraints, columnList).stream()
-                        .filter(SqlTableConstraint::isPrimaryKey)
-                        .collect(Collectors.toList());
-
-        if (constraints.size() > 1) {
+        List<SqlTableConstraint> fullConstraints = getFullConstraints(tableConstraints, columnList);
+        if (fullConstraints.stream().filter(SqlTableConstraint::isPrimaryKey).count() > 1) {
             throw new SqlValidateException(
-                    constraints.get(1).getParserPosition(), "Duplicate primary key definition");
-        } else if (constraints.size() == 1) {
+                    fullConstraints.get(1).getParserPosition(), "Duplicate primary key definition");
+        }
+        for (SqlTableConstraint constraint : fullConstraints) {
+            validate(constraint);
             Set<String> primaryKeyColumns =
-                    Arrays.stream(constraints.get(0).getColumnNames()).collect(Collectors.toSet());
+                    Arrays.stream(constraint.getColumnNames()).collect(Collectors.toSet());
 
+            // rewrite primary key's nullability to false
+            // e.g. CREATE TABLE tbl (`a` STRING PRIMARY KEY NOT ENFORCED, ...) or
+            // CREATE TABLE tbl (`a` STRING, PRIMARY KEY(`a`) NOT ENFORCED) will change `a`
+            // to STRING NOT NULL
             for (SqlNode column : columnList) {
                 SqlTableColumn tableColumn = (SqlTableColumn) column;
                 if (tableColumn instanceof SqlTableColumn.SqlRegularColumn
@@ -80,4 +87,19 @@ public class SqlConstraintValidator {
             }
         }
     }
+
+    /** Check table constraint. */
+    private static void validate(SqlTableConstraint constraint) throws SqlValidateException {
+        if (constraint.isUnique()) {
+            throw new SqlValidateException(
+                    constraint.getParserPosition(), "UNIQUE constraint is not supported yet");
+        }
+        if (constraint.isEnforced()) {
+            throw new SqlValidateException(
+                    constraint.getParserPosition(),
+                    "Flink doesn't support ENFORCED mode for PRIMARY KEY constraint. ENFORCED/NOT ENFORCED "
+                            + "controls if the constraint checks are performed on the incoming/outgoing data. "
+                            + "Flink does not own the data therefore the only supported mode is the NOT ENFORCED mode");
+        }
+    }
 }
diff --git a/flink-table/flink-sql-parser/src/test/java/org/apache/flink/sql/parser/FlinkSqlParserImplTest.java b/flink-table/flink-sql-parser/src/test/java/org/apache/flink/sql/parser/FlinkSqlParserImplTest.java
index 41ea0f2619e..b75f19a88ac 100644
--- a/flink-table/flink-sql-parser/src/test/java/org/apache/flink/sql/parser/FlinkSqlParserImplTest.java
+++ b/flink-table/flink-sql-parser/src/test/java/org/apache/flink/sql/parser/FlinkSqlParserImplTest.java
@@ -319,10 +319,23 @@ class FlinkSqlParserImplTest extends SqlParserTest {
         sql("alter table c1.d1.t1 rename to t2").ok("ALTER TABLE `C1`.`D1`.`T1` RENAME TO `T2`");
         sql("alter table if exists c1.d1.t1 rename to t2")
                 .ok("ALTER TABLE IF EXISTS `C1`.`D1`.`T1` RENAME TO `T2`");
+
         sql("alter table t1 set ('key1'='value1')")
                 .ok("ALTER TABLE `T1` SET (\n" + "  'key1' = 'value1'\n" + ")");
         sql("alter table if exists t1 set ('key1'='value1')")
                 .ok("ALTER TABLE IF EXISTS `T1` SET (\n" + "  'key1' = 'value1'\n" + ")");
+
+        sql("alter table t1 add constraint ct1 primary key(a, b)")
+                .ok(
+                        "ALTER TABLE `T1` ADD (\n"
+                                + "  CONSTRAINT `CT1` PRIMARY KEY (`A`, `B`)\n"
+                                + ")")
+                .node(
+                        new ValidationMatcher()
+                                .fails(
+                                        "Flink doesn't support ENFORCED mode for PRIMARY KEY constraint. "
+                                                + "ENFORCED/NOT ENFORCED controls if the constraint checks are performed on the incoming/outgoing data. "
+                                                + "Flink does not own the data therefore the only supported mode is the NOT ENFORCED mode"));
         sql("alter table t1 add constraint ct1 primary key(a, b) not enforced")
                 .ok(
                         "ALTER TABLE `T1` ADD (\n"
@@ -334,12 +347,15 @@ class FlinkSqlParserImplTest extends SqlParserTest {
                                 + "  CONSTRAINT `CT1` PRIMARY KEY (`A`, `B`) NOT ENFORCED\n"
                                 + ")");
         sql("alter table t1 " + "add unique(a, b)")
-                .ok("ALTER TABLE `T1` ADD (\n" + "  UNIQUE (`A`, `B`)\n" + ")");
+                .ok("ALTER TABLE `T1` ADD (\n" + "  UNIQUE (`A`, `B`)\n" + ")")
+                .node(new ValidationMatcher().fails("UNIQUE constraint is not supported yet"));
         sql("alter table if exists t1 " + "add unique(a, b)")
                 .ok("ALTER TABLE IF EXISTS `T1` ADD (\n" + "  UNIQUE (`A`, `B`)\n" + ")");
+
         sql("alter table t1 drop constraint ct1").ok("ALTER TABLE `T1` DROP CONSTRAINT `CT1`");
         sql("alter table if exists t1 drop constraint ct1")
                 .ok("ALTER TABLE IF EXISTS `T1` DROP CONSTRAINT `CT1`");
+
         sql("alter table t1 rename a to b").ok("ALTER TABLE `T1` RENAME `A` TO `B`");
         sql("alter table if exists t1 rename a to b")
                 .ok("ALTER TABLE IF EXISTS `T1` RENAME `A` TO `B`");
@@ -846,7 +862,7 @@ class FlinkSqlParserImplTest extends SqlParserTest {
 
     @Test
     void testTableConstraints() {
-        final String sql =
+        final String sql1 =
                 "CREATE TABLE tbl1 (\n"
                         + "  a bigint,\n"
                         + "  h varchar, \n"
@@ -860,7 +876,7 @@ class FlinkSqlParserImplTest extends SqlParserTest {
                         + "  'connector' = 'kafka',\n"
                         + "  'kafka.topic' = 'log.test'\n"
                         + ")\n";
-        final String expected =
+        final String expected1 =
                 "CREATE TABLE `TBL1` (\n"
                         + "  `A` BIGINT,\n"
                         + "  `H` VARCHAR,\n"
@@ -874,11 +890,193 @@ class FlinkSqlParserImplTest extends SqlParserTest {
                         + "  'connector' = 'kafka',\n"
                         + "  'kafka.topic' = 'log.test'\n"
                         + ")";
-        sql(sql).ok(expected);
+        sql(sql1)
+                .ok(expected1)
+                .node(
+                        new ValidationMatcher()
+                                .fails(
+                                        "Flink doesn't support ENFORCED mode for PRIMARY KEY constraint. "
+                                                + "ENFORCED/NOT ENFORCED controls if the constraint checks are performed on the incoming/outgoing data. "
+                                                + "Flink does not own the data therefore the only supported mode is the NOT ENFORCED mode"));
+
+        final String sql2 =
+                "CREATE TABLE tbl1 (\n"
+                        + "  a bigint,\n"
+                        + "  h varchar, \n"
+                        + "  g as 2 * (a + 1),\n"
+                        + "  ts as toTimestamp(b, 'yyyy-MM-dd HH:mm:ss'),\n"
+                        + "  b varchar,\n"
+                        + "  proc as PROCTIME(),\n"
+                        + "  PRIMARY KEY (a, b) NOT ENFORCED,\n"
+                        + "  UNIQUE (h, g)\n"
+                        + ") with (\n"
+                        + "  'connector' = 'kafka',\n"
+                        + "  'kafka.topic' = 'log.test'\n"
+                        + ")\n";
+        final String expected2 =
+                "CREATE TABLE `TBL1` (\n"
+                        + "  `A` BIGINT,\n"
+                        + "  `H` VARCHAR,\n"
+                        + "  `G` AS (2 * (`A` + 1)),\n"
+                        + "  `TS` AS `TOTIMESTAMP`(`B`, 'yyyy-MM-dd HH:mm:ss'),\n"
+                        + "  `B` VARCHAR,\n"
+                        + "  `PROC` AS `PROCTIME`(),\n"
+                        + "  PRIMARY KEY (`A`, `B`) NOT ENFORCED,\n"
+                        + "  UNIQUE (`H`, `G`)\n"
+                        + ") WITH (\n"
+                        + "  'connector' = 'kafka',\n"
+                        + "  'kafka.topic' = 'log.test'\n"
+                        + ")";
+        sql(sql2)
+                .ok(expected2)
+                .node(new ValidationMatcher().fails("UNIQUE constraint is not supported yet"));
+
+        final String sql3 =
+                "CREATE TABLE tbl1 (\n"
+                        + "  a bigint,\n"
+                        + "  h varchar, \n"
+                        + "  g as 2 * (a + 1),\n"
+                        + "  ts as toTimestamp(b, 'yyyy-MM-dd HH:mm:ss'),\n"
+                        + "  b varchar,\n"
+                        + "  proc as PROCTIME(),\n"
+                        + "  PRIMARY KEY (a, b) NOT ENFORCED\n"
+                        + ") with (\n"
+                        + "  'connector' = 'kafka',\n"
+                        + "  'kafka.topic' = 'log.test'\n"
+                        + ")\n";
+        final String expectParsed =
+                "CREATE TABLE `TBL1` (\n"
+                        + "  `A` BIGINT,\n"
+                        + "  `H` VARCHAR,\n"
+                        + "  `G` AS (2 * (`A` + 1)),\n"
+                        + "  `TS` AS `TOTIMESTAMP`(`B`, 'yyyy-MM-dd HH:mm:ss'),\n"
+                        + "  `B` VARCHAR,\n"
+                        + "  `PROC` AS `PROCTIME`(),\n"
+                        + "  PRIMARY KEY (`A`, `B`) NOT ENFORCED\n"
+                        + ") WITH (\n"
+                        + "  'connector' = 'kafka',\n"
+                        + "  'kafka.topic' = 'log.test'\n"
+                        + ")";
+        final String expectValidated =
+                "CREATE TABLE `TBL1` (\n"
+                        + "  `A` BIGINT NOT NULL,\n"
+                        + "  `H` VARCHAR,\n"
+                        + "  `G` AS (2 * (`A` + 1)),\n"
+                        + "  `TS` AS `TOTIMESTAMP`(`B`, 'yyyy-MM-dd HH:mm:ss'),\n"
+                        + "  `B` VARCHAR NOT NULL,\n"
+                        + "  `PROC` AS `PROCTIME`(),\n"
+                        + "  PRIMARY KEY (`A`, `B`) NOT ENFORCED\n"
+                        + ") WITH (\n"
+                        + "  'connector' = 'kafka',\n"
+                        + "  'kafka.topic' = 'log.test'\n"
+                        + ")";
+        sql(sql3).ok(expectParsed).node(validated(expectValidated));
     }
 
     @Test
-    void testTableConstraintsValidated() {
+    void testColumnConstraints() {
+        final String sql1 =
+                "CREATE TABLE tbl1 (\n"
+                        + "  a bigint primary key,\n"
+                        + "  h varchar unique,\n"
+                        + "  g as 2 * (a + 1),\n"
+                        + "  ts as toTimestamp(b, 'yyyy-MM-dd HH:mm:ss'),\n"
+                        + "  b varchar,\n"
+                        + "  proc as PROCTIME()\n"
+                        + ") with (\n"
+                        + "  'connector' = 'kafka',\n"
+                        + "  'kafka.topic' = 'log.test'\n"
+                        + ")\n";
+        final String expected1 =
+                "CREATE TABLE `TBL1` (\n"
+                        + "  `A` BIGINT PRIMARY KEY,\n"
+                        + "  `H` VARCHAR UNIQUE,\n"
+                        + "  `G` AS (2 * (`A` + 1)),\n"
+                        + "  `TS` AS `TOTIMESTAMP`(`B`, 'yyyy-MM-dd HH:mm:ss'),\n"
+                        + "  `B` VARCHAR,\n"
+                        + "  `PROC` AS `PROCTIME`()\n"
+                        + ") WITH (\n"
+                        + "  'connector' = 'kafka',\n"
+                        + "  'kafka.topic' = 'log.test'\n"
+                        + ")";
+        sql(sql1)
+                .ok(expected1)
+                .node(
+                        new ValidationMatcher()
+                                .fails(
+                                        "Flink doesn't support ENFORCED mode for PRIMARY KEY constraint. "
+                                                + "ENFORCED/NOT ENFORCED controls if the constraint checks are performed on the incoming/outgoing data. "
+                                                + "Flink does not own the data therefore the only supported mode is the NOT ENFORCED mode"));
+
+        final String sql2 =
+                "CREATE TABLE tbl1 (\n"
+                        + "  a bigint primary key not enforced,\n"
+                        + "  h varchar unique,\n"
+                        + "  g as 2 * (a + 1),\n"
+                        + "  ts as toTimestamp(b, 'yyyy-MM-dd HH:mm:ss'),\n"
+                        + "  b varchar,\n"
+                        + "  proc as PROCTIME()\n"
+                        + ") with (\n"
+                        + "  'connector' = 'kafka',\n"
+                        + "  'kafka.topic' = 'log.test'\n"
+                        + ")\n";
+        final String expected2 =
+                "CREATE TABLE `TBL1` (\n"
+                        + "  `A` BIGINT PRIMARY KEY NOT ENFORCED,\n"
+                        + "  `H` VARCHAR UNIQUE,\n"
+                        + "  `G` AS (2 * (`A` + 1)),\n"
+                        + "  `TS` AS `TOTIMESTAMP`(`B`, 'yyyy-MM-dd HH:mm:ss'),\n"
+                        + "  `B` VARCHAR,\n"
+                        + "  `PROC` AS `PROCTIME`()\n"
+                        + ") WITH (\n"
+                        + "  'connector' = 'kafka',\n"
+                        + "  'kafka.topic' = 'log.test'\n"
+                        + ")";
+        sql(sql2)
+                .ok(expected2)
+                .node(new ValidationMatcher().fails("UNIQUE constraint is not supported yet"));
+
+        final String sql3 =
+                "CREATE TABLE tbl1 (\n"
+                        + "  a bigint primary key not enforced,\n"
+                        + "  h varchar,\n"
+                        + "  g as 2 * (a + 1),\n"
+                        + "  ts as toTimestamp(b, 'yyyy-MM-dd HH:mm:ss'),\n"
+                        + "  b varchar,\n"
+                        + "  proc as PROCTIME()\n"
+                        + ") with (\n"
+                        + "  'connector' = 'kafka',\n"
+                        + "  'kafka.topic' = 'log.test'\n"
+                        + ")\n";
+        final String expectParsed =
+                "CREATE TABLE `TBL1` (\n"
+                        + "  `A` BIGINT PRIMARY KEY NOT ENFORCED,\n"
+                        + "  `H` VARCHAR,\n"
+                        + "  `G` AS (2 * (`A` + 1)),\n"
+                        + "  `TS` AS `TOTIMESTAMP`(`B`, 'yyyy-MM-dd HH:mm:ss'),\n"
+                        + "  `B` VARCHAR,\n"
+                        + "  `PROC` AS `PROCTIME`()\n"
+                        + ") WITH (\n"
+                        + "  'connector' = 'kafka',\n"
+                        + "  'kafka.topic' = 'log.test'\n"
+                        + ")";
+        final String expectValidated =
+                "CREATE TABLE `TBL1` (\n"
+                        + "  `A` BIGINT NOT NULL PRIMARY KEY NOT ENFORCED,\n"
+                        + "  `H` VARCHAR,\n"
+                        + "  `G` AS (2 * (`A` + 1)),\n"
+                        + "  `TS` AS `TOTIMESTAMP`(`B`, 'yyyy-MM-dd HH:mm:ss'),\n"
+                        + "  `B` VARCHAR,\n"
+                        + "  `PROC` AS `PROCTIME`()\n"
+                        + ") WITH (\n"
+                        + "  'connector' = 'kafka',\n"
+                        + "  'kafka.topic' = 'log.test'\n"
+                        + ")";
+        sql(sql3).ok(expectParsed).node(validated(expectValidated));
+    }
+
+    @Test
+    void testUniqueTableConstraint() {
         final String sql =
                 "CREATE TABLE tbl1 (\n"
                         + "  a bigint,\n"
@@ -887,7 +1085,7 @@ class FlinkSqlParserImplTest extends SqlParserTest {
                         + "  ts as toTimestamp(b, 'yyyy-MM-dd HH:mm:ss'),\n"
                         + "  b varchar,\n"
                         + "  proc as PROCTIME(),\n"
-                        + "  PRIMARY KEY (a, b),\n"
+                        + "  PRIMARY KEY (a, b) NOT ENFORCED,\n"
                         + "  UNIQUE (h, g)\n"
                         + ") with (\n"
                         + "  'connector' = 'kafka',\n"
@@ -895,19 +1093,20 @@ class FlinkSqlParserImplTest extends SqlParserTest {
                         + ")\n";
         final String expected =
                 "CREATE TABLE `TBL1` (\n"
-                        + "  `A` BIGINT NOT NULL,\n"
+                        + "  `A` BIGINT,\n"
                         + "  `H` VARCHAR,\n"
                         + "  `G` AS (2 * (`A` + 1)),\n"
                         + "  `TS` AS `TOTIMESTAMP`(`B`, 'yyyy-MM-dd HH:mm:ss'),\n"
-                        + "  `B` VARCHAR NOT NULL,\n"
+                        + "  `B` VARCHAR,\n"
                         + "  `PROC` AS `PROCTIME`(),\n"
-                        + "  PRIMARY KEY (`A`, `B`),\n"
+                        + "  PRIMARY KEY (`A`, `B`) NOT ENFORCED,\n"
                         + "  UNIQUE (`H`, `G`)\n"
                         + ") WITH (\n"
                         + "  'connector' = 'kafka',\n"
                         + "  'kafka.topic' = 'log.test'\n"
                         + ")";
-        sql(sql).node(validated(expected));
+        sql(sql).ok(expected);
+        sql(sql).node(new ValidationMatcher().fails("UNIQUE constraint is not supported yet"));
     }
 
     @Test
@@ -1254,6 +1453,46 @@ class FlinkSqlParserImplTest extends SqlParserTest {
         sql(sql).ok(expected);
     }
 
+    @Test
+    void testCreateTableLikeWithConstraints() {
+        final String sql1 =
+                "create table source_table(\n"
+                        + "  a int primary key,\n"
+                        + "  b bigint,\n"
+                        + "  c string\n"
+                        + ")\n"
+                        + "LIKE parent_table";
+        sql(sql1)
+                .node(
+                        new ValidationMatcher()
+                                .fails(
+                                        "Flink doesn't support ENFORCED mode for PRIMARY KEY constraint. ENFORCED/NOT ENFORCED "
+                                                + "controls if the constraint checks are performed on the incoming/outgoing data. "
+                                                + "Flink does not own the data therefore the only supported mode is the NOT ENFORCED mode"));
+
+        final String sql2 =
+                "create table source_table(\n"
+                        + "  a int primary key,\n"
+                        + "  b bigint,\n"
+                        + "  c string,\n"
+                        + "  primary key(a) not enforced\n"
+                        + ")\n"
+                        + "LIKE parent_table";
+
+        sql(sql2).node(new ValidationMatcher().fails("Duplicate primary key definition"));
+
+        final String sql3 =
+                "create table source_table(\n"
+                        + "  a int,\n"
+                        + "  b bigint,\n"
+                        + "  c string,\n"
+                        + "  unique (a)\n"
+                        + ")\n"
+                        + "LIKE parent_table";
+
+        sql(sql3).node(new ValidationMatcher().fails("UNIQUE constraint is not supported yet"));
+    }
+
     @Test
     void testCreateTableWithLikeClause() {
         final String sql =
@@ -2139,7 +2378,15 @@ class FlinkSqlParserImplTest extends SqlParserTest {
                 .node(
                         new ValidationMatcher()
                                 .fails(
-                                        "CREATE TABLE AS SELECT syntax does not support primary key constraints yet."));
+                                        "Flink doesn't support ENFORCED mode for PRIMARY KEY constraint. ENFORCED/NOT ENFORCED controls "
+                                                + "if the constraint checks are performed on the incoming/outgoing data. "
+                                                + "Flink does not own the data therefore the only supported mode is the NOT ENFORCED mode"));
+
+        sql("CREATE TABLE t (PRIMARY KEY (col1), PRIMARY KEY (col2) NOT ENFORCED) WITH ('test' = 'zm') AS SELECT col1 FROM b")
+                .node(new ValidationMatcher().fails("Duplicate primary key definition"));
+
+        sql("CREATE TABLE t (UNIQUE (col1)) WITH ('test' = 'zm') AS SELECT col1 FROM b")
+                .node(new ValidationMatcher().fails("UNIQUE constraint is not supported yet"));
     }
 
     @Test
diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/operations/AlterSchemaConverter.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/operations/AlterSchemaConverter.java
index 15c0718c2a8..8899ecb3243 100644
--- a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/operations/AlterSchemaConverter.java
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/operations/AlterSchemaConverter.java
@@ -73,7 +73,6 @@ import java.util.Map;
 import java.util.Optional;
 import java.util.Set;
 import java.util.function.BiConsumer;
-import java.util.function.Consumer;
 import java.util.function.Function;
 import java.util.stream.Collectors;
 
@@ -91,17 +90,14 @@ public class AlterSchemaConverter {
 
     private final SqlValidator sqlValidator;
     private final Function<SqlNode, String> escapeExpression;
-    private final Consumer<SqlTableConstraint> constraintValidator;
     private final CatalogManager catalogManager;
 
     AlterSchemaConverter(
             SqlValidator sqlValidator,
-            Consumer<SqlTableConstraint> constraintValidator,
             Function<SqlNode, String> escapeExpression,
             CatalogManager catalogManager) {
         this.sqlValidator = sqlValidator;
         this.escapeExpression = escapeExpression;
-        this.constraintValidator = constraintValidator;
         this.catalogManager = catalogManager;
     }
 
@@ -305,7 +301,6 @@ public class AlterSchemaConverter {
         Function<SqlNode, String> escapeExpressions;
         FlinkTypeFactory typeFactory;
         SqlValidator sqlValidator;
-        Consumer<SqlTableConstraint> constraintValidator;
         SchemaResolver schemaResolver;
 
         List<TableChange> changesCollector;
@@ -315,12 +310,10 @@ public class AlterSchemaConverter {
                 Schema oldSchema,
                 FlinkTypeFactory typeFactory,
                 SqlValidator sqlValidator,
-                Consumer<SqlTableConstraint> constraintValidator,
                 Function<SqlNode, String> escapeExpressions,
                 SchemaResolver schemaResolver) {
             this.typeFactory = typeFactory;
             this.sqlValidator = sqlValidator;
-            this.constraintValidator = constraintValidator;
             this.escapeExpressions = escapeExpressions;
             this.schemaResolver = schemaResolver;
             this.changesCollector = new ArrayList<>();
@@ -380,7 +373,6 @@ public class AlterSchemaConverter {
 
         private void updatePrimaryKey(SqlTableConstraint alterPrimaryKey) {
             checkAndCollectPrimaryKeyChange();
-            constraintValidator.accept(alterPrimaryKey);
             List<String> primaryKeyColumns = Arrays.asList(alterPrimaryKey.getColumnNames());
             primaryKey =
                     new Schema.UnresolvedPrimaryKey(
@@ -550,16 +542,9 @@ public class AlterSchemaConverter {
                 Schema oldSchema,
                 FlinkTypeFactory typeFactory,
                 SqlValidator sqlValidator,
-                Consumer<SqlTableConstraint> constraintValidator,
                 Function<SqlNode, String> escapeExpressions,
                 SchemaResolver schemaResolver) {
-            super(
-                    oldSchema,
-                    typeFactory,
-                    sqlValidator,
-                    constraintValidator,
-                    escapeExpressions,
-                    schemaResolver);
+            super(oldSchema, typeFactory, sqlValidator, escapeExpressions, schemaResolver);
         }
 
         @Override
@@ -642,14 +627,12 @@ public class AlterSchemaConverter {
                 ResolvedCatalogTable oldTable,
                 FlinkTypeFactory typeFactory,
                 SqlValidator sqlValidator,
-                Consumer<SqlTableConstraint> constraintValidator,
                 Function<SqlNode, String> escapeExpressions,
                 SchemaResolver schemaResolver) {
             super(
                     oldTable.getUnresolvedSchema(),
                     typeFactory,
                     sqlValidator,
-                    constraintValidator,
                     escapeExpressions,
                     schemaResolver);
             this.oldTable = oldTable;
@@ -962,7 +945,6 @@ public class AlterSchemaConverter {
                     oldTable.getUnresolvedSchema(),
                     (FlinkTypeFactory) sqlValidator.getTypeFactory(),
                     sqlValidator,
-                    constraintValidator,
                     escapeExpression,
                     catalogManager.getSchemaResolver());
         } else if (alterTableSchema instanceof SqlAlterTableModify) {
@@ -970,7 +952,6 @@ public class AlterSchemaConverter {
                     oldTable,
                     (FlinkTypeFactory) sqlValidator.getTypeFactory(),
                     sqlValidator,
-                    constraintValidator,
                     escapeExpression,
                     catalogManager.getSchemaResolver());
         }
diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/operations/SqlCreateTableConverter.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/operations/SqlCreateTableConverter.java
index 5b3fa879a37..5d878341577 100644
--- a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/operations/SqlCreateTableConverter.java
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/operations/SqlCreateTableConverter.java
@@ -52,7 +52,6 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.Optional;
-import java.util.function.Consumer;
 import java.util.function.Function;
 import java.util.stream.Collectors;
 
@@ -61,21 +60,17 @@ class SqlCreateTableConverter {
 
     private final MergeTableLikeUtil mergeTableLikeUtil;
     private final CatalogManager catalogManager;
-    private final Consumer<SqlTableConstraint> validateTableConstraint;
 
     SqlCreateTableConverter(
             FlinkCalciteSqlValidator sqlValidator,
             CatalogManager catalogManager,
-            Function<SqlNode, String> escapeExpression,
-            Consumer<SqlTableConstraint> validateTableConstraint) {
+            Function<SqlNode, String> escapeExpression) {
         this.mergeTableLikeUtil = new MergeTableLikeUtil(sqlValidator, escapeExpression);
         this.catalogManager = catalogManager;
-        this.validateTableConstraint = validateTableConstraint;
     }
 
     /** Convert the {@link SqlCreateTable} node. */
     Operation convertCreateTable(SqlCreateTable sqlCreateTable) {
-        sqlCreateTable.getTableConstraints().forEach(validateTableConstraint);
         CatalogTable catalogTable = createCatalogTable(sqlCreateTable);
 
         UnresolvedIdentifier unresolvedIdentifier =
diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/operations/SqlToOperationConverter.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/operations/SqlToOperationConverter.java
index be90e92557b..7cf4bc3199b 100644
--- a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/operations/SqlToOperationConverter.java
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/operations/SqlToOperationConverter.java
@@ -61,7 +61,6 @@ import org.apache.flink.sql.parser.ddl.SqlTableOption;
 import org.apache.flink.sql.parser.ddl.SqlUseCatalog;
 import org.apache.flink.sql.parser.ddl.SqlUseDatabase;
 import org.apache.flink.sql.parser.ddl.SqlUseModules;
-import org.apache.flink.sql.parser.ddl.constraint.SqlTableConstraint;
 import org.apache.flink.sql.parser.ddl.resource.SqlResource;
 import org.apache.flink.sql.parser.ddl.resource.SqlResourceType;
 import org.apache.flink.sql.parser.dml.RichSqlInsert;
@@ -259,12 +258,10 @@ public class SqlToOperationConverter {
                 new SqlCreateTableConverter(
                         flinkPlanner.getOrCreateSqlValidator(),
                         catalogManager,
-                        this::getQuotedSqlString,
-                        this::validateTableConstraint);
+                        this::getQuotedSqlString);
         this.alterSchemaConverter =
                 new AlterSchemaConverter(
                         flinkPlanner.getOrCreateSqlValidator(),
-                        this::validateTableConstraint,
                         this::getQuotedSqlString,
                         catalogManager);
     }
@@ -1566,20 +1563,6 @@ public class SqlToOperationConverter {
                 contextResolvedTable, queryOperation, SinkModifyOperation.ModifyType.UPDATE);
     }
 
-    private void validateTableConstraint(SqlTableConstraint constraint) {
-        if (constraint.isUnique()) {
-            throw new UnsupportedOperationException("UNIQUE constraint is not supported yet");
-        }
-        if (constraint.isEnforced()) {
-            throw new ValidationException(
-                    "Flink doesn't support ENFORCED mode for "
-                            + "PRIMARY KEY constraint. ENFORCED/NOT ENFORCED controls if the constraint "
-                            + "checks are performed on the incoming/outgoing data. "
-                            + "Flink does not own the data therefore the only supported mode "
-                            + "is the NOT ENFORCED mode");
-        }
-    }
-
     private String getQuotedSqlString(SqlNode sqlNode) {
         SqlParser.Config parserConfig = flinkPlanner.config().getParserConfig();
         SqlDialect dialect =
diff --git a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/operations/SqlDdlToOperationConverterTest.java b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/operations/SqlDdlToOperationConverterTest.java
index e0945d65267..d5d097f9f78 100644
--- a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/operations/SqlDdlToOperationConverterTest.java
+++ b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/operations/SqlDdlToOperationConverterTest.java
@@ -19,6 +19,7 @@
 package org.apache.flink.table.planner.operations;
 
 import org.apache.flink.sql.parser.ddl.SqlCreateTable;
+import org.apache.flink.sql.parser.error.SqlValidateException;
 import org.apache.flink.table.api.DataTypes;
 import org.apache.flink.table.api.Schema;
 import org.apache.flink.table.api.SqlDialect;
@@ -1477,12 +1478,12 @@ public class SqlDdlToOperationConverterTest extends SqlToOperationConverterTestB
 
         // add unique constraint
         assertThatThrownBy(() -> parse("alter table tb3 add unique(b)"))
-                .isInstanceOf(UnsupportedOperationException.class)
+                .isInstanceOf(SqlValidateException.class)
                 .hasMessageContaining("UNIQUE constraint is not supported yet");
 
         // lack NOT ENFORCED
         assertThatThrownBy(() -> parse("alter table tb3 add primary key(b)"))
-                .isInstanceOf(ValidationException.class)
+                .isInstanceOf(SqlValidateException.class)
                 .hasMessageContaining(
                         "Flink doesn't support ENFORCED mode for PRIMARY KEY constraint");
 
diff --git a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/api/TableEnvironmentTest.scala b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/api/TableEnvironmentTest.scala
index 0b2c47ce5ec..09732e37022 100644
--- a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/api/TableEnvironmentTest.scala
+++ b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/api/TableEnvironmentTest.scala
@@ -105,6 +105,38 @@ class TableEnvironmentTest {
     assertEquals(expected, actual)
   }
 
+  @Test
+  def testCreateTableWithEnforcedMode(): Unit = {
+    // check column constraint
+    assertThatThrownBy(() => tableEnv.executeSql("""
+                                                   |CREATE TABLE MyTable (
+                                                   |  a bigint primary key,
+                                                   |  b int,
+                                                   |  c varchar
+                                                   |) with (
+                                                   |  'connector' = 'COLLECTION',
+                                                   |  'is-bounded' = 'false'
+                                                   |)
+    """.stripMargin))
+      .hasMessageContaining("Flink doesn't support ENFORCED mode for PRIMARY KEY constraint.")
+      .isInstanceOf[ValidationException]
+
+    // check table constraint
+    assertThatThrownBy(() => tableEnv.executeSql("""
+                                                   |CREATE TABLE MyTable (
+                                                   |  a bigint,
+                                                   |  b int,
+                                                   |  c varchar,
+                                                   |  primary key(a)
+                                                   |) with (
+                                                   |  'connector' = 'COLLECTION',
+                                                   |  'is-bounded' = 'false'
+                                                   |)
+    """.stripMargin))
+      .hasMessageContaining("Flink doesn't support ENFORCED mode for PRIMARY KEY constraint.")
+      .isInstanceOf[ValidationException]
+  }
+
   @Test
   def testStreamTableEnvironmentExplain(): Unit = {
     val execEnv = StreamExecutionEnvironment.getExecutionEnvironment
