diff --git a/flink-libraries/flink-sql-client/pom.xml b/flink-libraries/flink-sql-client/pom.xml
index 03fca24c917..300f6ceadf2 100644
--- a/flink-libraries/flink-sql-client/pom.xml
+++ b/flink-libraries/flink-sql-client/pom.xml
@@ -133,6 +133,7 @@ under the License.
 
 	<build>
 		<plugins>
+			<!-- Build flink-sql-client jar -->
 			<plugin>
 				<groupId>org.apache.maven.plugins</groupId>
 				<artifactId>maven-shade-plugin</artifactId>
@@ -167,6 +168,28 @@ under the License.
 					</execution>
 				</executions>
 			</plugin>
+
+			<!-- Test dependency classloading -->
+			<plugin>
+				<artifactId>maven-assembly-plugin</artifactId>
+				<version>2.4</version>
+				<executions>
+					<execution>
+						<id>create-table-source-factory-jar</id>
+						<phase>process-test-classes</phase>
+						<goals>
+							<goal>single</goal>
+						</goals>
+						<configuration>
+							<finalName>table-source-factory</finalName>
+							<attach>false</attach>
+							<descriptors>
+								<descriptor>src/test/assembly/test-table-source-factory.xml</descriptor>
+							</descriptors>
+						</configuration>
+					</execution>
+				</executions>
+			</plugin>
 		</plugins>
 	</build>
 </project>
diff --git a/flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/Executor.java b/flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/Executor.java
index 512c1943937..4a41222700e 100644
--- a/flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/Executor.java
+++ b/flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/Executor.java
@@ -38,38 +38,38 @@ public interface Executor {
 	/**
 	 * Lists all session properties that are defined by the executor and the session.
 	 */
-	Map<String, String> getSessionProperties(SessionContext context) throws SqlExecutionException;
+	Map<String, String> getSessionProperties(SessionContext session) throws SqlExecutionException;
 
 	/**
 	 * Lists all tables known to the executor.
 	 */
-	List<String> listTables(SessionContext context) throws SqlExecutionException;
+	List<String> listTables(SessionContext session) throws SqlExecutionException;
 
 	/**
 	 * Returns the schema of a table. Throws an exception if the table could not be found.
 	 */
-	TableSchema getTableSchema(SessionContext context, String name) throws SqlExecutionException;
+	TableSchema getTableSchema(SessionContext session, String name) throws SqlExecutionException;
 
 	/**
 	 * Returns a string-based explanation about AST and execution plan of the given statement.
 	 */
-	String explainStatement(SessionContext context, String statement) throws SqlExecutionException;
+	String explainStatement(SessionContext session, String statement) throws SqlExecutionException;
 
 	/**
 	 * Submits a Flink job (detached) and returns the result descriptor.
 	 */
-	ResultDescriptor executeQuery(SessionContext context, String query) throws SqlExecutionException;
+	ResultDescriptor executeQuery(SessionContext session, String query) throws SqlExecutionException;
 
 	/**
 	 * Asks for the next changelog results (non-blocking).
 	 */
-	TypedResult<List<Tuple2<Boolean, Row>>> retrieveResultChanges(SessionContext context, String resultId) throws SqlExecutionException;
+	TypedResult<List<Tuple2<Boolean, Row>>> retrieveResultChanges(SessionContext session, String resultId) throws SqlExecutionException;
 
 	/**
 	 * Creates an immutable result snapshot of the running Flink job. Throws an exception if no Flink job can be found.
 	 * Returns the number of pages.
 	 */
-	TypedResult<Integer> snapshotResult(SessionContext context, String resultId, int pageSize) throws SqlExecutionException;
+	TypedResult<Integer> snapshotResult(SessionContext session, String resultId, int pageSize) throws SqlExecutionException;
 
 	/**
 	 * Returns the rows that are part of the current page or throws an exception if the snapshot has been expired.
@@ -79,10 +79,10 @@ public interface Executor {
 	/**
 	 * Cancels a table program and stops the result retrieval.
 	 */
-	void cancelQuery(SessionContext context, String resultId) throws SqlExecutionException;
+	void cancelQuery(SessionContext session, String resultId) throws SqlExecutionException;
 
 	/**
 	 * Stops the executor.
 	 */
-	void stop(SessionContext context);
+	void stop(SessionContext session);
 }
diff --git a/flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/SessionContext.java b/flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/SessionContext.java
index 1058eb6a3f6..0b6ee2c8ef5 100644
--- a/flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/SessionContext.java
+++ b/flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/SessionContext.java
@@ -22,6 +22,7 @@ import org.apache.flink.table.client.config.Environment;
 
 import java.util.HashMap;
 import java.util.Map;
+import java.util.Objects;
 
 /**
  * Context describing a session.
@@ -54,4 +55,23 @@ public class SessionContext {
 		// enrich with session properties
 		return Environment.enrich(defaultEnvironment, sessionProperties);
 	}
+
+	@Override
+	public boolean equals(Object o) {
+		if (this == o) {
+			return true;
+		}
+		if (!(o instanceof SessionContext)) {
+			return false;
+		}
+		SessionContext context = (SessionContext) o;
+		return Objects.equals(name, context.name) &&
+			Objects.equals(defaultEnvironment, context.defaultEnvironment) &&
+			Objects.equals(sessionProperties, context.sessionProperties);
+	}
+
+	@Override
+	public int hashCode() {
+		return Objects.hash(name, defaultEnvironment, sessionProperties);
+	}
 }
diff --git a/flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/local/ExecutionContext.java b/flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/local/ExecutionContext.java
new file mode 100644
index 00000000000..15a3c129bb8
--- /dev/null
+++ b/flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/local/ExecutionContext.java
@@ -0,0 +1,166 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.client.gateway.local;
+
+import org.apache.flink.api.common.ExecutionConfig;
+import org.apache.flink.api.common.Plan;
+import org.apache.flink.api.common.time.Time;
+import org.apache.flink.api.java.ExecutionEnvironment;
+import org.apache.flink.client.program.ClusterClient;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.optimizer.DataStatistics;
+import org.apache.flink.optimizer.Optimizer;
+import org.apache.flink.optimizer.costs.DefaultCostEstimator;
+import org.apache.flink.optimizer.plan.FlinkPlan;
+import org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders;
+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
+import org.apache.flink.streaming.api.graph.StreamGraph;
+import org.apache.flink.table.api.BatchQueryConfig;
+import org.apache.flink.table.api.QueryConfig;
+import org.apache.flink.table.api.StreamQueryConfig;
+import org.apache.flink.table.api.TableEnvironment;
+import org.apache.flink.table.client.config.Environment;
+import org.apache.flink.table.client.gateway.SessionContext;
+import org.apache.flink.table.sources.TableSource;
+import org.apache.flink.table.sources.TableSourceFactoryService;
+
+import java.net.URL;
+import java.util.List;
+
+/**
+ * Context for executing table programs. It contains configured environments and environment
+ * specific logic such as plan translation.
+ */
+public class ExecutionContext {
+
+	private final SessionContext sessionContext;
+	private final Environment mergedEnv;
+	private final ExecutionEnvironment execEnv;
+	private final StreamExecutionEnvironment streamExecEnv;
+	private final TableEnvironment tableEnv;
+	private final ClassLoader classLoader;
+	private final QueryConfig queryConfig;
+
+	public ExecutionContext(Environment defaultEnvironment, SessionContext sessionContext, List<URL> dependencies) {
+		this.sessionContext = sessionContext;
+		this.mergedEnv = Environment.merge(defaultEnvironment, sessionContext.getEnvironment());
+
+		// create environments
+		if (mergedEnv.getExecution().isStreamingExecution()) {
+			streamExecEnv = createStreamExecutionEnvironment();
+			execEnv = null;
+			tableEnv = TableEnvironment.getTableEnvironment(streamExecEnv);
+		} else {
+			streamExecEnv = null;
+			execEnv = createExecutionEnvironment();
+			tableEnv = TableEnvironment.getTableEnvironment(execEnv);
+		}
+
+		// create class loader
+		classLoader = FlinkUserCodeClassLoaders.parentFirst(
+			dependencies.toArray(new URL[dependencies.size()]),
+			this.getClass().getClassLoader());
+
+		// create table sources
+		mergedEnv.getSources().forEach((name, source) -> {
+			TableSource<?> tableSource = TableSourceFactoryService.findAndCreateTableSource(source, classLoader);
+			tableEnv.registerTableSource(name, tableSource);
+		});
+
+		// create query config
+		queryConfig = createQueryConfig();
+	}
+
+	public SessionContext getSessionContext() {
+		return sessionContext;
+	}
+
+	public ExecutionEnvironment getExecutionEnvironment() {
+		return execEnv;
+	}
+
+	public StreamExecutionEnvironment getStreamExecutionEnvironment() {
+		return streamExecEnv;
+	}
+
+	public TableEnvironment getTableEnvironment() {
+		return tableEnv;
+	}
+
+	public ClassLoader getClassLoader() {
+		return classLoader;
+	}
+
+	public Environment getMergedEnvironment() {
+		return mergedEnv;
+	}
+
+	public QueryConfig getQueryConfig() {
+		return queryConfig;
+	}
+
+	public ExecutionConfig getExecutionConfig() {
+		if (streamExecEnv != null) {
+			return streamExecEnv.getConfig();
+		} else {
+			return execEnv.getConfig();
+		}
+	}
+
+	public FlinkPlan createPlan(String name, Configuration flinkConfig) {
+		if (streamExecEnv != null) {
+			final StreamGraph graph = streamExecEnv.getStreamGraph();
+			graph.setJobName(name);
+			return graph;
+		} else {
+			final int parallelism = execEnv.getParallelism();
+			final Plan unoptimizedPlan = execEnv.createProgramPlan();
+			unoptimizedPlan.setJobName(name);
+			final Optimizer compiler = new Optimizer(new DataStatistics(), new DefaultCostEstimator(), flinkConfig);
+			return ClusterClient.getOptimizedPlan(compiler, unoptimizedPlan, parallelism);
+		}
+	}
+
+	// --------------------------------------------------------------------------------------------
+
+	private ExecutionEnvironment createExecutionEnvironment() {
+		final ExecutionEnvironment execEnv = ExecutionEnvironment.getExecutionEnvironment();
+		execEnv.setParallelism(mergedEnv.getExecution().getParallelism());
+		return execEnv;
+	}
+
+	private StreamExecutionEnvironment createStreamExecutionEnvironment() {
+		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
+		env.setParallelism(mergedEnv.getExecution().getParallelism());
+		env.setMaxParallelism(mergedEnv.getExecution().getMaxParallelism());
+		return env;
+	}
+
+	private QueryConfig createQueryConfig() {
+		if (streamExecEnv != null) {
+			final StreamQueryConfig config = new StreamQueryConfig();
+			final long minRetention = mergedEnv.getExecution().getMinStateRetention();
+			final long maxRetention = mergedEnv.getExecution().getMaxStateRetention();
+			config.withIdleStateRetentionTime(Time.milliseconds(minRetention), Time.milliseconds(maxRetention));
+			return config;
+		} else {
+			return new BatchQueryConfig();
+		}
+	}
+}
diff --git a/flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/local/LocalExecutor.java b/flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/local/LocalExecutor.java
index 8c40885d36a..35d7da9bbfd 100644
--- a/flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/local/LocalExecutor.java
+++ b/flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/local/LocalExecutor.java
@@ -18,11 +18,7 @@
 
 package org.apache.flink.table.client.gateway.local;
 
-import org.apache.flink.api.common.ExecutionConfig;
 import org.apache.flink.api.common.JobID;
-import org.apache.flink.api.common.Plan;
-import org.apache.flink.api.common.time.Time;
-import org.apache.flink.api.java.ExecutionEnvironment;
 import org.apache.flink.api.java.tuple.Tuple2;
 import org.apache.flink.client.cli.CliFrontend;
 import org.apache.flink.client.deployment.ClusterDescriptor;
@@ -36,34 +32,21 @@ import org.apache.flink.configuration.AkkaOptions;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.configuration.GlobalConfiguration;
 import org.apache.flink.core.fs.Path;
-import org.apache.flink.optimizer.DataStatistics;
-import org.apache.flink.optimizer.Optimizer;
-import org.apache.flink.optimizer.costs.DefaultCostEstimator;
 import org.apache.flink.optimizer.plan.FlinkPlan;
 import org.apache.flink.runtime.jobgraph.JobGraph;
 import org.apache.flink.runtime.jobgraph.SavepointRestoreSettings;
-import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
-import org.apache.flink.streaming.api.graph.StreamGraph;
-import org.apache.flink.table.api.BatchQueryConfig;
-import org.apache.flink.table.api.QueryConfig;
-import org.apache.flink.table.api.StreamQueryConfig;
 import org.apache.flink.table.api.Table;
 import org.apache.flink.table.api.TableEnvironment;
 import org.apache.flink.table.api.TableSchema;
-import org.apache.flink.table.api.java.BatchTableEnvironment;
-import org.apache.flink.table.api.java.StreamTableEnvironment;
 import org.apache.flink.table.client.SqlClientException;
 import org.apache.flink.table.client.config.Deployment;
 import org.apache.flink.table.client.config.Environment;
-import org.apache.flink.table.client.config.Execution;
 import org.apache.flink.table.client.gateway.Executor;
 import org.apache.flink.table.client.gateway.ResultDescriptor;
 import org.apache.flink.table.client.gateway.SessionContext;
 import org.apache.flink.table.client.gateway.SqlExecutionException;
 import org.apache.flink.table.client.gateway.TypedResult;
 import org.apache.flink.table.sinks.TableSink;
-import org.apache.flink.table.sources.TableSource;
-import org.apache.flink.table.sources.TableSourceFactoryService;
 import org.apache.flink.types.Row;
 import org.apache.flink.util.StringUtils;
 
@@ -86,11 +69,17 @@ public class LocalExecutor implements Executor {
 
 	private static final String DEFAULT_ENV_FILE = "sql-client-defaults.yaml";
 
-	private final Environment environment;
+	private final Environment defaultEnvironment;
 	private final List<URL> dependencies;
 	private final Configuration flinkConfig;
 	private final ResultStore resultStore;
 
+	/**
+	 * Cached execution context for unmodified sessions. Do not access this variable directly
+	 * but through {@link LocalExecutor#getOrCreateExecutionContext}.
+	 */
+	private ExecutionContext executionContext;
+
 	/**
 	 * Creates a local executor for submitting table programs and retrieving results.
 	 */
@@ -129,12 +118,12 @@ public class LocalExecutor implements Executor {
 		if (defaultEnv != null) {
 			System.out.println("Reading default environment from: " + defaultEnv);
 			try {
-				environment = Environment.parse(defaultEnv);
+				defaultEnvironment = Environment.parse(defaultEnv);
 			} catch (IOException e) {
 				throw new SqlClientException("Could not read default environment file at: " + defaultEnv, e);
 			}
 		} else {
-			environment = new Environment();
+			defaultEnvironment = new Environment();
 		}
 
 		// discover dependencies
@@ -175,8 +164,11 @@ public class LocalExecutor implements Executor {
 		resultStore = new ResultStore(flinkConfig);
 	}
 
-	public LocalExecutor(Environment environment, List<URL> dependencies, Configuration flinkConfig) {
-		this.environment = environment;
+	/**
+	 * Constructor for testing purposes.
+	 */
+	public LocalExecutor(Environment defaultEnvironment, List<URL> dependencies, Configuration flinkConfig) {
+		this.defaultEnvironment = defaultEnvironment;
 		this.dependencies = dependencies;
 		this.flinkConfig = flinkConfig;
 
@@ -190,8 +182,8 @@ public class LocalExecutor implements Executor {
 	}
 
 	@Override
-	public Map<String, String> getSessionProperties(SessionContext context) throws SqlExecutionException {
-		final Environment env = createEnvironment(context);
+	public Map<String, String> getSessionProperties(SessionContext session) throws SqlExecutionException {
+		final Environment env = getOrCreateExecutionContext(session).getMergedEnvironment();
 		final Map<String, String> properties = new HashMap<>();
 		properties.putAll(env.getExecution().toProperties());
 		properties.putAll(env.getDeployment().toProperties());
@@ -199,16 +191,14 @@ public class LocalExecutor implements Executor {
 	}
 
 	@Override
-	public List<String> listTables(SessionContext context) throws SqlExecutionException {
-		final Environment env = createEnvironment(context);
-		final TableEnvironment tableEnv = createTableEnvironment(env);
+	public List<String> listTables(SessionContext session) throws SqlExecutionException {
+		final TableEnvironment tableEnv = getOrCreateExecutionContext(session).getTableEnvironment();
 		return Arrays.asList(tableEnv.listTables());
 	}
 
 	@Override
-	public TableSchema getTableSchema(SessionContext context, String name) throws SqlExecutionException {
-		final Environment env = createEnvironment(context);
-		final TableEnvironment tableEnv = createTableEnvironment(env);
+	public TableSchema getTableSchema(SessionContext session, String name) throws SqlExecutionException {
+		final TableEnvironment tableEnv = getOrCreateExecutionContext(session).getTableEnvironment();
 		try {
 			return tableEnv.scan(name).getSchema();
 		} catch (Throwable t) {
@@ -218,13 +208,13 @@ public class LocalExecutor implements Executor {
 	}
 
 	@Override
-	public String explainStatement(SessionContext context, String statement) throws SqlExecutionException {
-		final Environment env = createEnvironment(context);
+	public String explainStatement(SessionContext session, String statement) throws SqlExecutionException {
+		final ExecutionContext context = getOrCreateExecutionContext(session);
 
 		// translate
 		try {
-			final Tuple2<TableEnvironment, Table> table = createTable(env, statement);
-			return table.f0.explain(table.f1);
+			final Table table = createTable(context, statement);
+			return context.getTableEnvironment().explain(table);
 		} catch (Throwable t) {
 			// catch everything such that the query does not crash the executor
 			throw new SqlExecutionException("Invalid SQL statement.", t);
@@ -232,29 +222,26 @@ public class LocalExecutor implements Executor {
 	}
 
 	@Override
-	public ResultDescriptor executeQuery(SessionContext context, String query) throws SqlExecutionException {
-		final Environment env = createEnvironment(context);
+	public ResultDescriptor executeQuery(SessionContext session, String query) throws SqlExecutionException {
+		final ExecutionContext context = getOrCreateExecutionContext(session);
+		final Environment mergedEnv = context.getMergedEnvironment();
 
 		// create table here to fail quickly for wrong queries
-		final Tuple2<TableEnvironment, Table> table = createTable(env, query);
+		final Table table = createTable(context, query);
 
 		// deployment
-		final ClusterClient<?> clusterClient = createDeployment(env.getDeployment());
+		final ClusterClient<?> clusterClient = createDeployment(mergedEnv.getDeployment());
 
 		// initialize result
 		final DynamicResult result = resultStore.createResult(
-			env,
-			table.f1.getSchema(),
-			getExecutionConfig(table.f0));
+			mergedEnv,
+			table.getSchema(),
+			context.getExecutionConfig());
 
 		// create job graph with jars
 		final JobGraph jobGraph;
 		try {
-			jobGraph = createJobGraph(
-				context.getName() + ": " + query,
-				env.getExecution(),
-				table.f0,
-				table.f1,
+			jobGraph = createJobGraph(context, context.getSessionContext().getName() + ": " + query, table,
 				result.getTableSink(),
 				clusterClient);
 		} catch (Throwable t) {
@@ -268,18 +255,12 @@ public class LocalExecutor implements Executor {
 		final String resultId = jobGraph.getJobID().toString();
 		resultStore.storeResult(resultId, result);
 
-		// create class loader
-		final ClassLoader classLoader = JobWithJars.buildUserCodeClassLoader(
-			dependencies,
-			Collections.emptyList(),
-			this.getClass().getClassLoader());
-
 		// create execution
 		final Runnable program = () -> {
 			// we need to submit the job attached for now
 			// otherwise it is not possible to retrieve the reason why an execution failed
 			try {
-				clusterClient.run(jobGraph, classLoader);
+				clusterClient.run(jobGraph, context.getClassLoader());
 			} catch (ProgramInvocationException e) {
 				throw new SqlExecutionException("Could not execute table program.", e);
 			} finally {
@@ -294,11 +275,12 @@ public class LocalExecutor implements Executor {
 		// start result retrieval
 		result.startRetrieval(program);
 
-		return new ResultDescriptor(resultId, table.f1.getSchema(), result.isMaterialized());
+		return new ResultDescriptor(resultId, table.getSchema(), result.isMaterialized());
 	}
 
 	@Override
-	public TypedResult<List<Tuple2<Boolean, Row>>> retrieveResultChanges(SessionContext context, String resultId) throws SqlExecutionException {
+	public TypedResult<List<Tuple2<Boolean, Row>>> retrieveResultChanges(SessionContext session,
+			String resultId) throws SqlExecutionException {
 		final DynamicResult result = resultStore.getResult(resultId);
 		if (result == null) {
 			throw new SqlExecutionException("Could not find a result with result identifier '" + resultId + "'.");
@@ -310,7 +292,7 @@ public class LocalExecutor implements Executor {
 	}
 
 	@Override
-	public TypedResult<Integer> snapshotResult(SessionContext context, String resultId, int pageSize) throws SqlExecutionException {
+	public TypedResult<Integer> snapshotResult(SessionContext session, String resultId, int pageSize) throws SqlExecutionException {
 		final DynamicResult result = resultStore.getResult(resultId);
 		if (result == null) {
 			throw new SqlExecutionException("Could not find a result with result identifier '" + resultId + "'.");
@@ -334,7 +316,7 @@ public class LocalExecutor implements Executor {
 	}
 
 	@Override
-	public void cancelQuery(SessionContext context, String resultId) throws SqlExecutionException {
+	public void cancelQuery(SessionContext session, String resultId) throws SqlExecutionException {
 		final DynamicResult result = resultStore.getResult(resultId);
 		if (result == null) {
 			throw new SqlExecutionException("Could not find a result with result identifier '" + resultId + "'.");
@@ -345,8 +327,8 @@ public class LocalExecutor implements Executor {
 		resultStore.removeResult(resultId);
 
 		// stop Flink job
-		final Environment env = createEnvironment(context);
-		final ClusterClient<?> clusterClient = createDeployment(env.getDeployment());
+		final Environment mergedEnv = getOrCreateExecutionContext(session).getMergedEnvironment();
+		final ClusterClient<?> clusterClient = createDeployment(mergedEnv.getDeployment());
 		try {
 			clusterClient.cancel(new JobID(StringUtils.hexStringToByte(resultId)));
 		} catch (Throwable t) {
@@ -361,10 +343,10 @@ public class LocalExecutor implements Executor {
 	}
 
 	@Override
-	public void stop(SessionContext context) {
+	public void stop(SessionContext session) {
 		resultStore.getResults().forEach((resultId) -> {
 			try {
-				cancelQuery(context, resultId);
+				cancelQuery(session, resultId);
 			} catch (Throwable t) {
 				// ignore any throwable to keep the clean up running
 			}
@@ -373,45 +355,29 @@ public class LocalExecutor implements Executor {
 
 	// --------------------------------------------------------------------------------------------
 
-	private Tuple2<TableEnvironment, Table> createTable(Environment env, String query) {
-		final TableEnvironment tableEnv = createTableEnvironment(env);
-
+	private Table createTable(ExecutionContext context, String query) {
 		// parse and validate query
 		try {
-			return Tuple2.of(tableEnv, tableEnv.sqlQuery(query));
+			return context.getTableEnvironment().sqlQuery(query);
 		} catch (Throwable t) {
 			// catch everything such that the query does not crash the executor
 			throw new SqlExecutionException("Invalid SQL statement.", t);
 		}
 	}
 
-	private JobGraph createJobGraph(String name, Execution exec, TableEnvironment tableEnv,
-		Table table, TableSink<?> sink, ClusterClient<?> clusterClient) {
-
-		final QueryConfig queryConfig = createQueryConfig(exec);
+	private JobGraph createJobGraph(ExecutionContext context, String name, Table table,
+			TableSink<?> sink, ClusterClient<?> clusterClient) {
 
 		// translate
 		try {
-			table.writeToSink(sink, queryConfig);
+			table.writeToSink(sink, context.getQueryConfig());
 		} catch (Throwable t) {
 			// catch everything such that the query does not crash the executor
 			throw new SqlExecutionException("Invalid SQL statement.", t);
 		}
 
 		// extract plan
-		final FlinkPlan plan;
-		if (exec.isStreamingExecution()) {
-			final StreamGraph graph = ((StreamTableEnvironment) tableEnv).execEnv().getStreamGraph();
-			graph.setJobName(name);
-			plan = graph;
-		} else {
-			final int parallelism = exec.getParallelism();
-			final Plan unoptimizedPlan = ((BatchTableEnvironment) tableEnv).execEnv().createProgramPlan();
-			unoptimizedPlan.setJobName(name);
-			final Optimizer compiler = new Optimizer(new DataStatistics(), new DefaultCostEstimator(),
-				clusterClient.getFlinkConfiguration());
-			plan = ClusterClient.getOptimizedPlan(compiler, unoptimizedPlan, parallelism);
-		}
+		final FlinkPlan plan = context.createPlan(name, clusterClient.getFlinkConfiguration());
 
 		// create job graph
 		return clusterClient.getJobGraph(
@@ -421,15 +387,6 @@ public class LocalExecutor implements Executor {
 			SavepointRestoreSettings.none());
 	}
 
-	@SuppressWarnings("unchecked")
-	private ExecutionConfig getExecutionConfig(TableEnvironment tableEnv) {
-		if (tableEnv instanceof StreamTableEnvironment) {
-			return ((StreamTableEnvironment) tableEnv).execEnv().getConfig();
-		} else {
-			return ((BatchTableEnvironment) tableEnv).execEnv().getConfig();
-		}
-	}
-
 	private ClusterClient<?> createDeployment(Deployment deploy) {
 
 		// change some configuration options for being more responsive
@@ -457,44 +414,18 @@ public class LocalExecutor implements Executor {
 		}
 	}
 
-	private Environment createEnvironment(SessionContext context) {
-		return Environment.merge(environment, context.getEnvironment());
-	}
-
-	private TableEnvironment createTableEnvironment(Environment env) {
-		try {
-			final TableEnvironment tableEnv;
-			if (env.getExecution().isStreamingExecution()) {
-				final StreamExecutionEnvironment execEnv = StreamExecutionEnvironment.getExecutionEnvironment();
-				execEnv.setParallelism(env.getExecution().getParallelism());
-				execEnv.setMaxParallelism(env.getExecution().getMaxParallelism());
-				tableEnv = StreamTableEnvironment.getTableEnvironment(execEnv);
-			} else {
-				final ExecutionEnvironment execEnv = ExecutionEnvironment.getExecutionEnvironment();
-				execEnv.setParallelism(env.getExecution().getParallelism());
-				tableEnv = BatchTableEnvironment.getTableEnvironment(execEnv);
+	/**
+	 * Creates or reuses the execution context.
+	 */
+	private synchronized ExecutionContext getOrCreateExecutionContext(SessionContext session) throws SqlExecutionException {
+		if (executionContext == null || !executionContext.getSessionContext().equals(session)) {
+			try {
+				executionContext = new ExecutionContext(defaultEnvironment, session, dependencies);
+			} catch (Throwable t) {
+				// catch everything such that a configuration does not crash the executor
+				throw new SqlExecutionException("Could not create execution context.", t);
 			}
-
-			env.getSources().forEach((name, source) -> {
-				TableSource<?> tableSource = TableSourceFactoryService.findAndCreateTableSource(source);
-				tableEnv.registerTableSource(name, tableSource);
-			});
-
-			return tableEnv;
-		} catch (Exception e) {
-			throw new SqlExecutionException("Could not create table environment.", e);
-		}
-	}
-
-	private QueryConfig createQueryConfig(Execution exec) {
-		if (exec.isStreamingExecution()) {
-			final StreamQueryConfig config = new StreamQueryConfig();
-			final long minRetention = exec.getMinStateRetention();
-			final long maxRetention = exec.getMaxStateRetention();
-			config.withIdleStateRetentionTime(Time.milliseconds(minRetention), Time.milliseconds(maxRetention));
-			return config;
-		} else {
-			return new BatchQueryConfig();
 		}
+		return executionContext;
 	}
 }
diff --git a/flink-libraries/flink-sql-client/src/test/assembly/test-table-source-factory.xml b/flink-libraries/flink-sql-client/src/test/assembly/test-table-source-factory.xml
new file mode 100644
index 00000000000..fb9673c593e
--- /dev/null
+++ b/flink-libraries/flink-sql-client/src/test/assembly/test-table-source-factory.xml
@@ -0,0 +1,47 @@
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+  http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+
+-->
+
+<assembly>
+	<id>test-jar</id>
+	<formats>
+		<format>jar</format>
+	</formats>
+	<includeBaseDirectory>false</includeBaseDirectory>
+	<fileSets>
+		<fileSet>
+			<directory>${project.build.testOutputDirectory}</directory>
+			<outputDirectory>/</outputDirectory>
+			<!-- modify/add include to match your package(s) -->
+			<includes>
+				<include>org/apache/flink/table/client/gateway/utils/TestTableSourceFactory.class</include>
+				<include>org/apache/flink/table/client/gateway/utils/TestTableSourceFactory$*.class</include>
+			</includes>
+		</fileSet>
+	</fileSets>
+	<files>
+		<!-- Create META-INF/services -->
+		<file>
+			<source>src/test/resources/test-factory-services-file</source>
+			<outputDirectory>META-INF/services</outputDirectory>
+			<destName>org.apache.flink.table.sources.TableSourceFactory</destName>
+			<fileMode>0755</fileMode>
+		</file>
+	</files>
+</assembly>
diff --git a/flink-libraries/flink-sql-client/src/test/java/org/apache/flink/table/client/gateway/local/DependencyTest.java b/flink-libraries/flink-sql-client/src/test/java/org/apache/flink/table/client/gateway/local/DependencyTest.java
new file mode 100644
index 00000000000..715d2db5c39
--- /dev/null
+++ b/flink-libraries/flink-sql-client/src/test/java/org/apache/flink/table/client/gateway/local/DependencyTest.java
@@ -0,0 +1,72 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.client.gateway.local;
+
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.table.api.TableSchema;
+import org.apache.flink.table.api.Types;
+import org.apache.flink.table.client.config.Environment;
+import org.apache.flink.table.client.gateway.SessionContext;
+import org.apache.flink.table.client.gateway.utils.EnvironmentFileUtil;
+
+import org.junit.Test;
+
+import java.net.URL;
+import java.nio.file.Paths;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.junit.Assert.assertEquals;
+
+/**
+ * Dependency tests for {@link LocalExecutor}. Mainly for testing classloading of dependencies.
+ */
+public class DependencyTest {
+
+	private static final String FACTORY_ENVIRONMENT_FILE = "test-sql-client-factory.yaml";
+	private static final String TABLE_SOURCE_FACTORY_JAR_FILE = "table-source-factory-test-jar.jar";
+
+	@Test
+	public void testTableSourceFactoryDiscovery() throws Exception {
+		// create environment
+		final Map<String, String> replaceVars = new HashMap<>();
+		replaceVars.put("$VAR_0", "test-table-source-factory");
+		replaceVars.put("$VAR_1", "test-property");
+		replaceVars.put("$VAR_2", "test-value");
+		final Environment env = EnvironmentFileUtil.parseModified(FACTORY_ENVIRONMENT_FILE, replaceVars);
+
+		// create executor with dependencies
+		final URL dependency = Paths.get("target", TABLE_SOURCE_FACTORY_JAR_FILE).toUri().toURL();
+		final LocalExecutor executor = new LocalExecutor(
+			env,
+			Collections.singletonList(dependency),
+			new Configuration());
+
+		final SessionContext session = new SessionContext("test-session", new Environment());
+
+		final TableSchema result = executor.getTableSchema(session, "TableNumber1");
+		final TableSchema expected = TableSchema.builder()
+			.field("IntegerField1", Types.INT())
+			.field("StringField1", Types.STRING())
+			.build();
+
+		assertEquals(expected, result);
+	}
+}
diff --git a/flink-libraries/flink-sql-client/src/test/java/org/apache/flink/table/client/gateway/local/LocalExecutorITCase.java b/flink-libraries/flink-sql-client/src/test/java/org/apache/flink/table/client/gateway/local/LocalExecutorITCase.java
index f30cafe890e..a2ae28108bf 100644
--- a/flink-libraries/flink-sql-client/src/test/java/org/apache/flink/table/client/gateway/local/LocalExecutorITCase.java
+++ b/flink-libraries/flink-sql-client/src/test/java/org/apache/flink/table/client/gateway/local/LocalExecutorITCase.java
@@ -31,16 +31,15 @@ import org.apache.flink.table.client.gateway.Executor;
 import org.apache.flink.table.client.gateway.ResultDescriptor;
 import org.apache.flink.table.client.gateway.SessionContext;
 import org.apache.flink.table.client.gateway.TypedResult;
+import org.apache.flink.table.client.gateway.utils.EnvironmentFileUtil;
 import org.apache.flink.test.util.TestBaseUtils;
 import org.apache.flink.types.Row;
-import org.apache.flink.util.FileUtils;
 import org.apache.flink.util.TestLogger;
 
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 import org.junit.Test;
 
-import java.io.File;
 import java.net.URL;
 import java.util.ArrayList;
 import java.util.Arrays;
@@ -61,6 +60,8 @@ import static org.junit.Assert.assertTrue;
  */
 public class LocalExecutorITCase extends TestLogger {
 
+	private static final String DEFAULTS_ENVIRONMENT_FILE = "test-sql-client-defaults.yaml";
+
 	private static StandaloneMiniCluster cluster;
 
 	@BeforeClass
@@ -80,9 +81,9 @@ public class LocalExecutorITCase extends TestLogger {
 	@Test
 	public void testListTables() throws Exception {
 		final Executor executor = createDefaultExecutor();
-		final SessionContext context = new SessionContext("test-session", new Environment());
+		final SessionContext session = new SessionContext("test-session", new Environment());
 
-		final List<String> actualTables = executor.listTables(context);
+		final List<String> actualTables = executor.listTables(session);
 
 		final List<String> expectedTables = Arrays.asList("TableNumber1", "TableNumber2");
 		assertEquals(expectedTables, actualTables);
@@ -91,12 +92,12 @@ public class LocalExecutorITCase extends TestLogger {
 	@Test
 	public void testGetSessionProperties() throws Exception {
 		final Executor executor = createDefaultExecutor();
-		final SessionContext context = new SessionContext("test-session", new Environment());
+		final SessionContext session = new SessionContext("test-session", new Environment());
 
 		// modify defaults
-		context.setSessionProperty("execution.result-mode", "table");
+		session.setSessionProperty("execution.result-mode", "table");
 
-		final Map<String, String> actualProperties = executor.getSessionProperties(context);
+		final Map<String, String> actualProperties = executor.getSessionProperties(session);
 
 		final Map<String, String> expectedProperties = new HashMap<>();
 		expectedProperties.put("execution.type", "streaming");
@@ -114,9 +115,9 @@ public class LocalExecutorITCase extends TestLogger {
 	@Test
 	public void testTableSchema() throws Exception {
 		final Executor executor = createDefaultExecutor();
-		final SessionContext context = new SessionContext("test-session", new Environment());
+		final SessionContext session = new SessionContext("test-session", new Environment());
 
-		final TableSchema actualTableSchema = executor.getTableSchema(context, "TableNumber2");
+		final TableSchema actualTableSchema = executor.getTableSchema(session, "TableNumber2");
 
 		final TableSchema expectedTableSchema = new TableSchema(
 			new String[] {"IntegerField2", "StringField2"},
@@ -135,11 +136,11 @@ public class LocalExecutorITCase extends TestLogger {
 		replaceVars.put("$VAR_2", "changelog");
 
 		final Executor executor = createModifiedExecutor(replaceVars);
-		final SessionContext context = new SessionContext("test-session", new Environment());
+		final SessionContext session = new SessionContext("test-session", new Environment());
 
 		try {
 			// start job and retrieval
-			final ResultDescriptor desc = executor.executeQuery(context, "SELECT * FROM TableNumber1");
+			final ResultDescriptor desc = executor.executeQuery(session, "SELECT * FROM TableNumber1");
 
 			assertFalse(desc.isMaterialized());
 
@@ -148,7 +149,7 @@ public class LocalExecutorITCase extends TestLogger {
 			while (true) {
 				Thread.sleep(50); // slow the processing down
 				final TypedResult<List<Tuple2<Boolean, Row>>> result =
-					executor.retrieveResultChanges(context, desc.getResultId());
+					executor.retrieveResultChanges(session, desc.getResultId());
 				if (result.getType() == TypedResult.ResultType.PAYLOAD) {
 					for (Tuple2<Boolean, Row> change : result.getPayload()) {
 						actualResults.add(change.toString());
@@ -168,7 +169,7 @@ public class LocalExecutorITCase extends TestLogger {
 
 			TestBaseUtils.compareResultCollections(expectedResults, actualResults, Comparator.naturalOrder());
 		} finally {
-			executor.stop(context);
+			executor.stop(session);
 		}
 	}
 
@@ -182,11 +183,11 @@ public class LocalExecutorITCase extends TestLogger {
 		replaceVars.put("$VAR_2", "table");
 
 		final Executor executor = createModifiedExecutor(replaceVars);
-		final SessionContext context = new SessionContext("test-session", new Environment());
+		final SessionContext session = new SessionContext("test-session", new Environment());
 
 		try {
 			// start job and retrieval
-			final ResultDescriptor desc = executor.executeQuery(context, "SELECT IntegerField1 FROM TableNumber1");
+			final ResultDescriptor desc = executor.executeQuery(session, "SELECT IntegerField1 FROM TableNumber1");
 
 			assertTrue(desc.isMaterialized());
 
@@ -194,7 +195,7 @@ public class LocalExecutorITCase extends TestLogger {
 
 			while (true) {
 				Thread.sleep(50); // slow the processing down
-				final TypedResult<Integer> result = executor.snapshotResult(context, desc.getResultId(), 2);
+				final TypedResult<Integer> result = executor.snapshotResult(session, desc.getResultId(), 2);
 				if (result.getType() == TypedResult.ResultType.PAYLOAD) {
 					actualResults.clear();
 					IntStream.rangeClosed(1, result.getPayload()).forEach((page) -> {
@@ -217,29 +218,21 @@ public class LocalExecutorITCase extends TestLogger {
 
 			TestBaseUtils.compareResultCollections(expectedResults, actualResults, Comparator.naturalOrder());
 		} finally {
-			executor.stop(context);
+			executor.stop(session);
 		}
 	}
 
 	private LocalExecutor createDefaultExecutor() throws Exception {
-		final URL url = getClass().getClassLoader().getResource("test-sql-client-defaults.yaml");
-		Objects.requireNonNull(url);
-		final Environment env = Environment.parse(url);
-
-		return new LocalExecutor(env, Collections.emptyList(), cluster.getConfiguration());
+		return new LocalExecutor(
+			EnvironmentFileUtil.parseUnmodified(DEFAULTS_ENVIRONMENT_FILE),
+			Collections.emptyList(),
+			cluster.getConfiguration());
 	}
 
 	private LocalExecutor createModifiedExecutor(Map<String, String> replaceVars) throws Exception {
-		final URL url = getClass().getClassLoader().getResource("test-sql-client-defaults.yaml");
-		Objects.requireNonNull(url);
-		String schema = FileUtils.readFileUtf8(new File(url.getFile()));
-
-		for (Map.Entry<String, String> replaceVar : replaceVars.entrySet()) {
-			schema = schema.replace(replaceVar.getKey(), replaceVar.getValue());
-		}
-
-		final Environment env = Environment.parse(schema);
-
-		return new LocalExecutor(env, Collections.emptyList(), cluster.getConfiguration());
+		return new LocalExecutor(
+			EnvironmentFileUtil.parseModified(DEFAULTS_ENVIRONMENT_FILE, replaceVars),
+			Collections.emptyList(),
+			cluster.getConfiguration());
 	}
 }
diff --git a/flink-libraries/flink-sql-client/src/test/java/org/apache/flink/table/client/gateway/utils/EnvironmentFileUtil.java b/flink-libraries/flink-sql-client/src/test/java/org/apache/flink/table/client/gateway/utils/EnvironmentFileUtil.java
new file mode 100644
index 00000000000..4645b424e2d
--- /dev/null
+++ b/flink-libraries/flink-sql-client/src/test/java/org/apache/flink/table/client/gateway/utils/EnvironmentFileUtil.java
@@ -0,0 +1,56 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.client.gateway.utils;
+
+import org.apache.flink.table.client.config.Environment;
+import org.apache.flink.util.FileUtils;
+
+import java.io.File;
+import java.io.IOException;
+import java.net.URL;
+import java.util.Map;
+import java.util.Objects;
+
+/**
+ * Utilities for reading an environment file.
+ */
+public final class EnvironmentFileUtil {
+
+	private EnvironmentFileUtil() {
+		// private
+	}
+
+	public static Environment parseUnmodified(String fileName) throws IOException {
+		final URL url = EnvironmentFileUtil.class.getClassLoader().getResource(fileName);
+		Objects.requireNonNull(url);
+		return Environment.parse(url);
+	}
+
+	public static Environment parseModified(String fileName, Map<String, String> replaceVars) throws IOException {
+		final URL url = EnvironmentFileUtil.class.getClassLoader().getResource(fileName);
+		Objects.requireNonNull(url);
+		String schema = FileUtils.readFileUtf8(new File(url.getFile()));
+
+		for (Map.Entry<String, String> replaceVar : replaceVars.entrySet()) {
+			schema = schema.replace(replaceVar.getKey(), replaceVar.getValue());
+		}
+
+		return Environment.parse(schema);
+	}
+}
diff --git a/flink-libraries/flink-sql-client/src/test/java/org/apache/flink/table/client/gateway/utils/TestTableSourceFactory.java b/flink-libraries/flink-sql-client/src/test/java/org/apache/flink/table/client/gateway/utils/TestTableSourceFactory.java
new file mode 100644
index 00000000000..40a7e7bac67
--- /dev/null
+++ b/flink-libraries/flink-sql-client/src/test/java/org/apache/flink/table/client/gateway/utils/TestTableSourceFactory.java
@@ -0,0 +1,112 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.client.gateway.utils;
+
+import org.apache.flink.api.common.typeinfo.TypeInformation;
+import org.apache.flink.streaming.api.datastream.DataStream;
+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
+import org.apache.flink.table.api.TableSchema;
+import org.apache.flink.table.api.Types;
+import org.apache.flink.table.client.gateway.local.DependencyTest;
+import org.apache.flink.table.descriptors.DescriptorProperties;
+import org.apache.flink.table.sources.StreamTableSource;
+import org.apache.flink.table.sources.TableSource;
+import org.apache.flink.table.sources.TableSourceFactory;
+import org.apache.flink.types.Row;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import static org.apache.flink.table.descriptors.ConnectorDescriptorValidator.CONNECTOR_TYPE;
+import static org.apache.flink.table.descriptors.SchemaValidator.SCHEMA;
+import static org.apache.flink.table.descriptors.SchemaValidator.SCHEMA_NAME;
+import static org.apache.flink.table.descriptors.SchemaValidator.SCHEMA_TYPE;
+
+/**
+ * Table source factory for testing the classloading in {@link DependencyTest}.
+ */
+public class TestTableSourceFactory implements TableSourceFactory<Row> {
+
+	@Override
+	public Map<String, String> requiredContext() {
+		final Map<String, String> context = new HashMap<>();
+		context.put(CONNECTOR_TYPE(), "test-table-source-factory");
+		return context;
+	}
+
+	@Override
+	public List<String> supportedProperties() {
+		final List<String> properties = new ArrayList<>();
+		properties.add("connector.test-property");
+		properties.add(SCHEMA() + ".#." + SCHEMA_TYPE());
+		properties.add(SCHEMA() + ".#." + SCHEMA_NAME());
+		return properties;
+	}
+
+	@Override
+	public TableSource<Row> create(Map<String, String> properties) {
+		final DescriptorProperties params = new DescriptorProperties(true);
+		params.putProperties(properties);
+		return new TestTableSource(
+			params.getTableSchema(SCHEMA()),
+			properties.get("connector.test-property"));
+	}
+
+	// --------------------------------------------------------------------------------------------
+
+	/**
+	 * Test table source.
+	 */
+	public static class TestTableSource implements StreamTableSource<Row> {
+
+		private final TableSchema schema;
+		private final String property;
+
+		public TestTableSource(TableSchema schema, String property) {
+			this.schema = schema;
+			this.property = property;
+		}
+
+		public String getProperty() {
+			return property;
+		}
+
+		@Override
+		public DataStream<Row> getDataStream(StreamExecutionEnvironment execEnv) {
+			return null;
+		}
+
+		@Override
+		public TypeInformation<Row> getReturnType() {
+			return Types.ROW(schema.getColumnNames(), schema.getTypes());
+		}
+
+		@Override
+		public TableSchema getTableSchema() {
+			return schema;
+		}
+
+		@Override
+		public String explainSource() {
+			return "TestTableSource";
+		}
+	}
+}
diff --git a/flink-libraries/flink-sql-client/src/test/resources/test-factory-services-file b/flink-libraries/flink-sql-client/src/test/resources/test-factory-services-file
new file mode 100644
index 00000000000..41e7fb2cbd5
--- /dev/null
+++ b/flink-libraries/flink-sql-client/src/test/resources/test-factory-services-file
@@ -0,0 +1,20 @@
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+#==============================================================================
+# Test file for org.apache.flink.table.client.gateway.local.DependencyTest.
+#==============================================================================
+
+org.apache.flink.table.client.gateway.utils.TestTableSourceFactory
diff --git a/flink-libraries/flink-sql-client/src/test/resources/test-sql-client-defaults.yaml b/flink-libraries/flink-sql-client/src/test/resources/test-sql-client-defaults.yaml
index 1c2a705ae57..9cbecb07903 100644
--- a/flink-libraries/flink-sql-client/src/test/resources/test-sql-client-defaults.yaml
+++ b/flink-libraries/flink-sql-client/src/test/resources/test-sql-client-defaults.yaml
@@ -18,6 +18,7 @@
 
 #==============================================================================
 # TEST ENVIRONMENT FILE
+# Intended for org.apache.flink.table.client.gateway.local.LocalExecutorITCase.
 #==============================================================================
 
 # this file has variables that can be filled with content by replacing $VAR_XXX
diff --git a/flink-libraries/flink-sql-client/src/test/resources/test-sql-client-factory.yaml b/flink-libraries/flink-sql-client/src/test/resources/test-sql-client-factory.yaml
new file mode 100644
index 00000000000..1bb69e537f4
--- /dev/null
+++ b/flink-libraries/flink-sql-client/src/test/resources/test-sql-client-factory.yaml
@@ -0,0 +1,45 @@
+################################################################################
+#  Licensed to the Apache Software Foundation (ASF) under one
+#  or more contributor license agreements.  See the NOTICE file
+#  distributed with this work for additional information
+#  regarding copyright ownership.  The ASF licenses this file
+#  to you under the Apache License, Version 2.0 (the
+#  "License"); you may not use this file except in compliance
+#  with the License.  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+# limitations under the License.
+################################################################################
+
+#==============================================================================
+# TEST ENVIRONMENT FILE
+# Intended for org.apache.flink.table.client.gateway.local.DependencyTest.
+#==============================================================================
+
+# this file has variables that can be filled with content by replacing $VAR_XXX
+
+sources:
+  - name: TableNumber1
+    schema:
+      - name: IntegerField1
+        type: INT
+      - name: StringField1
+        type: VARCHAR
+    connector:
+      type: "$VAR_0"
+      $VAR_1: "$VAR_2"
+
+execution:
+  type: streaming
+  parallelism: 1
+
+deployment:
+  type: standalone
+  response-timeout: 5000
+
+
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/descriptors/Rowtime.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/descriptors/Rowtime.scala
index ed3854df36b..1e28303d0e4 100644
--- a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/descriptors/Rowtime.scala
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/descriptors/Rowtime.scala
@@ -85,7 +85,7 @@ class Rowtime extends Descriptor {
     *
     * Emits watermarks which are the maximum observed timestamp minus the specified delay.
     */
-  def watermarksPeriodicBounding(delay: Long): Rowtime = {
+  def watermarksPeriodicBounded(delay: Long): Rowtime = {
     watermarkStrategy = Some(new BoundedOutOfOrderTimestamps(delay))
     this
   }
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/sources/TableSourceFactoryService.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/sources/TableSourceFactoryService.scala
index 877cb7b5f39..0c81335f6b6 100644
--- a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/sources/TableSourceFactoryService.scala
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/sources/TableSourceFactoryService.scala
@@ -36,18 +36,39 @@ import scala.collection.mutable
   */
 object TableSourceFactoryService extends Logging {
 
-  private lazy val loader = ServiceLoader.load(classOf[TableSourceFactory[_]])
+  private lazy val defaultLoader = ServiceLoader.load(classOf[TableSourceFactory[_]])
 
   def findAndCreateTableSource(descriptor: TableSourceDescriptor): TableSource[_] = {
+    findAndCreateTableSource(descriptor, null)
+  }
+
+  def findAndCreateTableSource(
+      descriptor: TableSourceDescriptor,
+      classLoader: ClassLoader)
+    : TableSource[_] = {
+
     val properties = new DescriptorProperties()
     descriptor.addProperties(properties)
-    findAndCreateTableSource(properties.asMap.asScala.toMap)
+    findAndCreateTableSource(properties.asMap.asScala.toMap, classLoader)
   }
 
   def findAndCreateTableSource(properties: Map[String, String]): TableSource[_] = {
+    findAndCreateTableSource(properties, null)
+  }
+
+  def findAndCreateTableSource(
+      properties: Map[String, String],
+      classLoader: ClassLoader)
+    : TableSource[_] = {
+
     var matchingFactory: Option[(TableSourceFactory[_], Seq[String])] = None
     try {
-      val iter = loader.iterator()
+      val iter = if (classLoader == null) {
+        defaultLoader.iterator()
+      } else {
+        val customLoader = ServiceLoader.load(classOf[TableSourceFactory[_]], classLoader)
+        customLoader.iterator()
+      }
       while (iter.hasNext) {
         val factory = iter.next()
 
diff --git a/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/descriptors/RowtimeTest.scala b/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/descriptors/RowtimeTest.scala
index 7968b481db3..9e339d02adc 100644
--- a/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/descriptors/RowtimeTest.scala
+++ b/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/descriptors/RowtimeTest.scala
@@ -46,7 +46,7 @@ class RowtimeTest extends DescriptorTestBase {
   override def descriptors(): util.List[Descriptor] = {
     val desc1 = Rowtime()
       .timestampsFromField("otherField")
-      .watermarksPeriodicBounding(1000L)
+      .watermarksPeriodicBounded(1000L)
 
     val desc2 = Rowtime()
       .timestampsFromSource()
