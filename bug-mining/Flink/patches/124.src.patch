diff --git a/docs/hadoop_compatibility.md b/docs/hadoop_compatibility.md
index 73b7f5e4503..59e8c514f4b 100644
--- a/docs/hadoop_compatibility.md
+++ b/docs/hadoop_compatibility.md
@@ -114,12 +114,9 @@ hadoopOF.getConfiguration().set("mapreduce.output.textoutputformat.separator", "
 TextOutputFormat.setOutputPath(job, new Path(outputPath));
 		
 // Emit data using the Hadoop TextOutputFormat.
-result.output(hadoopOF)
-      .setParallelism(1);
+result.output(hadoopOF);
 ~~~
 
-**Please note:** At the moment, Hadoop OutputFormats must be executed with a parallelism of 1 (DOP = 1). This limitation will be resolved soon.
-
 ### Using Hadoop Mappers and Reducers
 
 Hadoop Mappers are semantically equivalent to Flink's [FlatMapFunctions](dataset_transformations.html#flatmap) and Hadoop Reducers are equivalent to Flink's [GroupReduceFunctions](dataset_transformations.html#groupreduce-on-grouped-dataset). Flink provides wrappers for implementations of Hadoop MapReduce's `Mapper` and `Reducer` interfaces, i.e., you can reuse your Hadoop Mappers and Reducers in regular Flink programs. At the moment, only the Mapper and Reduce interfaces of Hadoop's mapred API (`org.apache.hadoop.mapred`) are supported.
@@ -192,8 +189,7 @@ hadoopOF.getConfiguration().set("mapreduce.output.textoutputformat.separator", "
 TextOutputFormat.setOutputPath(job, new Path(outputPath));
 		
 // Emit data using the Hadoop TextOutputFormat.
-result.output(hadoopOF)
-      .setParallelism(1);
+result.output(hadoopOF);
 
 // Execute Program
 env.execute("Hadoop WordCount");
