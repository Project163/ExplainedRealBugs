diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/planner/delegation/hive/SqlFunctionConverter.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/planner/delegation/hive/SqlFunctionConverter.java
index a154cdc6afa..b5d0d5963f1 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/planner/delegation/hive/SqlFunctionConverter.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/planner/delegation/hive/SqlFunctionConverter.java
@@ -22,7 +22,7 @@ import org.apache.flink.connectors.hive.FlinkHiveException;
 import org.apache.flink.table.catalog.hive.client.HiveShim;
 import org.apache.flink.table.planner.delegation.hive.copy.HiveParserBetween;
 import org.apache.flink.table.planner.delegation.hive.copy.HiveParserSqlFunctionConverter;
-import org.apache.flink.table.planner.functions.sql.FlinkSqlTimestampFunction;
+import org.apache.flink.table.planner.functions.sql.FlinkTimestampWithPrecisionDynamicFunction;
 import org.apache.flink.util.Preconditions;
 
 import org.apache.calcite.plan.RelOptCluster;
@@ -43,6 +43,7 @@ import org.apache.calcite.sql.SqlOperatorTable;
 import org.apache.calcite.sql.SqlSyntax;
 import org.apache.calcite.sql.fun.SqlCastFunction;
 import org.apache.calcite.sql.fun.SqlMonotonicBinaryOperator;
+import org.apache.calcite.sql.fun.SqlStdOperatorTable;
 import org.apache.calcite.sql.type.SqlTypeName;
 import org.apache.calcite.sql.validate.SqlNameMatcher;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
@@ -94,8 +95,12 @@ public class SqlFunctionConverter extends RexShuttle {
             RelDataType type = call.getType();
             return builder.makeCall(type, convertedOp, visitList(operands, update));
         } else {
-            if (convertedOp instanceof FlinkSqlTimestampFunction) {
-                // flink's current_timestamp has different type from hive's, convert it to a literal
+            if (convertedOp instanceof FlinkTimestampWithPrecisionDynamicFunction
+                    && convertedOp
+                            .getName()
+                            .equalsIgnoreCase(SqlStdOperatorTable.CURRENT_TIMESTAMP.getName())) {
+                // flink's current_timestamp(no localtimestamp or now or current_row_timestamp in
+                // hive built-in functions) has different type from hive's, convert it to a literal
                 Timestamp currentTS =
                         ((HiveSessionState) SessionState.get()).getHiveParserCurrentTS();
                 HiveShim hiveShim = HiveParserUtils.getSessionHiveShim();
diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/functions/BuiltInFunctionDefinitions.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/functions/BuiltInFunctionDefinitions.java
index d7daa450051..5e2e7581d04 100644
--- a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/functions/BuiltInFunctionDefinitions.java
+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/functions/BuiltInFunctionDefinitions.java
@@ -1455,6 +1455,7 @@ public final class BuiltInFunctionDefinitions {
                     .name("currentDatabase")
                     .kind(SCALAR)
                     .outputTypeStrategy(explicit(STRING().notNull()))
+                    .notDeterministic()
                     .build();
 
     // --------------------------------------------------------------------------------------------
@@ -1483,6 +1484,13 @@ public final class BuiltInFunctionDefinitions {
                     .outputTypeStrategy(explicit(TIME().notNull()))
                     .build();
 
+    public static final BuiltInFunctionDefinition LOCAL_TIME =
+            BuiltInFunctionDefinition.newBuilder()
+                    .name("localTime")
+                    .kind(SCALAR)
+                    .outputTypeStrategy(explicit(TIME().notNull()))
+                    .build();
+
     public static final BuiltInFunctionDefinition CURRENT_TIMESTAMP =
             BuiltInFunctionDefinition.newBuilder()
                     .name("currentTimestamp")
@@ -1490,18 +1498,19 @@ public final class BuiltInFunctionDefinitions {
                     .outputTypeStrategy(explicit(TIMESTAMP_LTZ(3).notNull()))
                     .build();
 
-    public static final BuiltInFunctionDefinition CURRENT_ROW_TIMESTAMP =
+    public static final BuiltInFunctionDefinition NOW =
             BuiltInFunctionDefinition.newBuilder()
-                    .name("currentRowTimestamp")
+                    .name("now")
                     .kind(SCALAR)
                     .outputTypeStrategy(explicit(TIMESTAMP_LTZ(3).notNull()))
                     .build();
 
-    public static final BuiltInFunctionDefinition LOCAL_TIME =
+    public static final BuiltInFunctionDefinition CURRENT_ROW_TIMESTAMP =
             BuiltInFunctionDefinition.newBuilder()
-                    .name("localTime")
+                    .name("currentRowTimestamp")
                     .kind(SCALAR)
-                    .outputTypeStrategy(explicit(TIME().notNull()))
+                    .outputTypeStrategy(explicit(TIMESTAMP_LTZ(3).notNull()))
+                    .notDeterministic()
                     .build();
 
     public static final BuiltInFunctionDefinition LOCAL_TIMESTAMP =
diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/delegation/PlannerContext.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/delegation/PlannerContext.java
index fb9940c9e5b..f6f9c7fb866 100644
--- a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/delegation/PlannerContext.java
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/delegation/PlannerContext.java
@@ -319,6 +319,6 @@ public class PlannerContext {
                         context.getCatalogManager().getDataTypeFactory(),
                         typeFactory,
                         context.getRexFactory()),
-                FlinkSqlOperatorTable.instance());
+                FlinkSqlOperatorTable.instance(context.isBatchMode()));
     }
 }
diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/expressions/converter/DirectConvertRule.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/expressions/converter/DirectConvertRule.java
index 80906830b4e..2982fb03a6a 100644
--- a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/expressions/converter/DirectConvertRule.java
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/expressions/converter/DirectConvertRule.java
@@ -19,13 +19,16 @@
 package org.apache.flink.table.planner.expressions.converter;
 
 import org.apache.flink.annotation.Internal;
+import org.apache.flink.table.api.TableException;
 import org.apache.flink.table.expressions.CallExpression;
 import org.apache.flink.table.functions.BuiltInFunctionDefinitions;
 import org.apache.flink.table.functions.FunctionDefinition;
 import org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable;
+import org.apache.flink.table.planner.functions.sql.FlinkTimestampWithPrecisionDynamicFunction;
 
 import org.apache.calcite.rex.RexNode;
 import org.apache.calcite.sql.SqlOperator;
+import org.apache.calcite.sql.fun.SqlStdOperatorTable;
 
 import java.util.HashMap;
 import java.util.Map;
@@ -39,220 +42,291 @@ import static org.apache.flink.table.planner.expressions.converter.ExpressionCon
  */
 @Internal
 public class DirectConvertRule implements CallExpressionConvertRule {
+    private static Map<Boolean, DirectConvertRule> cachedInstances = new HashMap<>();
 
-    private static final Map<FunctionDefinition, SqlOperator> DEFINITION_OPERATOR_MAP =
+    public static synchronized DirectConvertRule instance(boolean isBatchMode) {
+        DirectConvertRule instance = cachedInstances.get(isBatchMode);
+        if (instance == null) {
+            instance = new DirectConvertRule();
+            instance.initNonDynamicFunctions();
+            instance.initDynamicFunctions(isBatchMode);
+            cachedInstances.put(isBatchMode, instance);
+        }
+        return instance;
+    }
+
+    private final Map<FunctionDefinition, SqlOperator> definitionSqlOperatorHashMap =
             new HashMap<>();
 
-    static {
+    void initDynamicFunctions(boolean isBatchMode) {
+        FlinkSqlOperatorTable.dynamicFunctions(isBatchMode)
+                .forEach(
+                        func -> {
+                            if (func.getName()
+                                    .equalsIgnoreCase(SqlStdOperatorTable.CURRENT_DATE.getName())) {
+                                definitionSqlOperatorHashMap.put(
+                                        BuiltInFunctionDefinitions.CURRENT_DATE, func);
+                            } else if (func.getName()
+                                    .equalsIgnoreCase(SqlStdOperatorTable.CURRENT_TIME.getName())) {
+                                definitionSqlOperatorHashMap.put(
+                                        BuiltInFunctionDefinitions.CURRENT_TIME, func);
+                            } else if (func.getName()
+                                    .equalsIgnoreCase(SqlStdOperatorTable.LOCALTIME.getName())) {
+                                definitionSqlOperatorHashMap.put(
+                                        BuiltInFunctionDefinitions.LOCAL_TIME, func);
+                            } else if (func.getName()
+                                    .equalsIgnoreCase(
+                                            SqlStdOperatorTable.CURRENT_TIMESTAMP.getName())) {
+                                definitionSqlOperatorHashMap.put(
+                                        BuiltInFunctionDefinitions.CURRENT_TIMESTAMP, func);
+                            } else if (func.getName()
+                                    .equalsIgnoreCase(
+                                            SqlStdOperatorTable.LOCALTIMESTAMP.getName())) {
+                                definitionSqlOperatorHashMap.put(
+                                        BuiltInFunctionDefinitions.LOCAL_TIMESTAMP, func);
+                            } else if (func.getName()
+                                    .equalsIgnoreCase(
+                                            FlinkTimestampWithPrecisionDynamicFunction.NOW)) {
+                                definitionSqlOperatorHashMap.put(
+                                        BuiltInFunctionDefinitions.NOW, func);
+                            } else {
+                                throw new TableException(
+                                        String.format(
+                                                "Unsupported mapping for dynamic function: %s",
+                                                func.getName()));
+                            }
+                        });
+    }
+
+    void initNonDynamicFunctions() {
         // logic functions
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.AND, FlinkSqlOperatorTable.AND);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.OR, FlinkSqlOperatorTable.OR);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.NOT, FlinkSqlOperatorTable.NOT);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.IF, FlinkSqlOperatorTable.CASE);
+        definitionSqlOperatorHashMap.put(BuiltInFunctionDefinitions.AND, FlinkSqlOperatorTable.AND);
+        definitionSqlOperatorHashMap.put(BuiltInFunctionDefinitions.OR, FlinkSqlOperatorTable.OR);
+        definitionSqlOperatorHashMap.put(BuiltInFunctionDefinitions.NOT, FlinkSqlOperatorTable.NOT);
+        definitionSqlOperatorHashMap.put(BuiltInFunctionDefinitions.IF, FlinkSqlOperatorTable.CASE);
 
         // comparison functions
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.EQUALS, FlinkSqlOperatorTable.EQUALS);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.GREATER_THAN, FlinkSqlOperatorTable.GREATER_THAN);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.GREATER_THAN_OR_EQUAL,
                 FlinkSqlOperatorTable.GREATER_THAN_OR_EQUAL);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.LESS_THAN, FlinkSqlOperatorTable.LESS_THAN);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.LESS_THAN_OR_EQUAL,
                 FlinkSqlOperatorTable.LESS_THAN_OR_EQUAL);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.NOT_EQUALS, FlinkSqlOperatorTable.NOT_EQUALS);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.IS_NULL, FlinkSqlOperatorTable.IS_NULL);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.IS_NOT_NULL, FlinkSqlOperatorTable.IS_NOT_NULL);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.IS_TRUE, FlinkSqlOperatorTable.IS_TRUE);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.IS_FALSE, FlinkSqlOperatorTable.IS_FALSE);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.IS_NOT_TRUE, FlinkSqlOperatorTable.IS_NOT_TRUE);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.IS_NOT_FALSE, FlinkSqlOperatorTable.IS_NOT_FALSE);
 
         // string functions
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.CHAR_LENGTH, FlinkSqlOperatorTable.CHAR_LENGTH);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.INIT_CAP, FlinkSqlOperatorTable.INITCAP);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.LIKE, FlinkSqlOperatorTable.LIKE);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.LOWER, FlinkSqlOperatorTable.LOWER);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
+                BuiltInFunctionDefinitions.LIKE, FlinkSqlOperatorTable.LIKE);
+        definitionSqlOperatorHashMap.put(
+                BuiltInFunctionDefinitions.LOWER, FlinkSqlOperatorTable.LOWER);
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.LOWERCASE, FlinkSqlOperatorTable.LOWER);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.SIMILAR, FlinkSqlOperatorTable.SIMILAR_TO);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.SUBSTRING, FlinkSqlOperatorTable.SUBSTRING);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.SUBSTR, FlinkSqlOperatorTable.SUBSTR);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.UPPER, FlinkSqlOperatorTable.UPPER);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
+                BuiltInFunctionDefinitions.UPPER, FlinkSqlOperatorTable.UPPER);
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.UPPERCASE, FlinkSqlOperatorTable.UPPER);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.POSITION, FlinkSqlOperatorTable.POSITION);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.OVERLAY, FlinkSqlOperatorTable.OVERLAY);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.CONCAT, FlinkSqlOperatorTable.CONCAT_FUNCTION);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.CONCAT_WS, FlinkSqlOperatorTable.CONCAT_WS);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.LPAD, FlinkSqlOperatorTable.LPAD);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.RPAD, FlinkSqlOperatorTable.RPAD);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
+                BuiltInFunctionDefinitions.LPAD, FlinkSqlOperatorTable.LPAD);
+        definitionSqlOperatorHashMap.put(
+                BuiltInFunctionDefinitions.RPAD, FlinkSqlOperatorTable.RPAD);
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.REGEXP_EXTRACT, FlinkSqlOperatorTable.REGEXP_EXTRACT);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.FROM_BASE64, FlinkSqlOperatorTable.FROM_BASE64);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.TO_BASE64, FlinkSqlOperatorTable.TO_BASE64);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.ASCII, FlinkSqlOperatorTable.ASCII);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.CHR, FlinkSqlOperatorTable.CHR);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
+                BuiltInFunctionDefinitions.ASCII, FlinkSqlOperatorTable.ASCII);
+        definitionSqlOperatorHashMap.put(BuiltInFunctionDefinitions.CHR, FlinkSqlOperatorTable.CHR);
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.DECODE, FlinkSqlOperatorTable.DECODE);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.ENCODE, FlinkSqlOperatorTable.ENCODE);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.LEFT, FlinkSqlOperatorTable.LEFT);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.RIGHT, FlinkSqlOperatorTable.RIGHT);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.INSTR, FlinkSqlOperatorTable.INSTR);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
+                BuiltInFunctionDefinitions.LEFT, FlinkSqlOperatorTable.LEFT);
+        definitionSqlOperatorHashMap.put(
+                BuiltInFunctionDefinitions.RIGHT, FlinkSqlOperatorTable.RIGHT);
+        definitionSqlOperatorHashMap.put(
+                BuiltInFunctionDefinitions.INSTR, FlinkSqlOperatorTable.INSTR);
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.LOCATE, FlinkSqlOperatorTable.LOCATE);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.PARSE_URL, FlinkSqlOperatorTable.PARSE_URL);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.UUID, FlinkSqlOperatorTable.UUID);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.LTRIM, FlinkSqlOperatorTable.LTRIM);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.RTRIM, FlinkSqlOperatorTable.RTRIM);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
+                BuiltInFunctionDefinitions.UUID, FlinkSqlOperatorTable.UUID);
+        definitionSqlOperatorHashMap.put(
+                BuiltInFunctionDefinitions.LTRIM, FlinkSqlOperatorTable.LTRIM);
+        definitionSqlOperatorHashMap.put(
+                BuiltInFunctionDefinitions.RTRIM, FlinkSqlOperatorTable.RTRIM);
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.REPEAT, FlinkSqlOperatorTable.REPEAT);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.REGEXP, FlinkSqlOperatorTable.REGEXP);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.REGEXP_REPLACE, FlinkSqlOperatorTable.REGEXP_REPLACE);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.REVERSE, FlinkSqlOperatorTable.REVERSE);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.SPLIT_INDEX, FlinkSqlOperatorTable.SPLIT_INDEX);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.STR_TO_MAP, FlinkSqlOperatorTable.STR_TO_MAP);
 
         // math functions
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.MINUS, FlinkSqlOperatorTable.MINUS);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
+                BuiltInFunctionDefinitions.MINUS, FlinkSqlOperatorTable.MINUS);
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.DIVIDE, FlinkSqlOperatorTable.DIVIDE);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.TIMES, FlinkSqlOperatorTable.MULTIPLY);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.FLOOR, FlinkSqlOperatorTable.FLOOR);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.CEIL, FlinkSqlOperatorTable.CEIL);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.ABS, FlinkSqlOperatorTable.ABS);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.EXP, FlinkSqlOperatorTable.EXP);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.LOG10, FlinkSqlOperatorTable.LOG10);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.LOG2, FlinkSqlOperatorTable.LOG2);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.LN, FlinkSqlOperatorTable.LN);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.LOG, FlinkSqlOperatorTable.LOG);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.POWER, FlinkSqlOperatorTable.POWER);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.MOD, FlinkSqlOperatorTable.MOD);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
+                BuiltInFunctionDefinitions.FLOOR, FlinkSqlOperatorTable.FLOOR);
+        definitionSqlOperatorHashMap.put(
+                BuiltInFunctionDefinitions.CEIL, FlinkSqlOperatorTable.CEIL);
+        definitionSqlOperatorHashMap.put(BuiltInFunctionDefinitions.ABS, FlinkSqlOperatorTable.ABS);
+        definitionSqlOperatorHashMap.put(BuiltInFunctionDefinitions.EXP, FlinkSqlOperatorTable.EXP);
+        definitionSqlOperatorHashMap.put(
+                BuiltInFunctionDefinitions.LOG10, FlinkSqlOperatorTable.LOG10);
+        definitionSqlOperatorHashMap.put(
+                BuiltInFunctionDefinitions.LOG2, FlinkSqlOperatorTable.LOG2);
+        definitionSqlOperatorHashMap.put(BuiltInFunctionDefinitions.LN, FlinkSqlOperatorTable.LN);
+        definitionSqlOperatorHashMap.put(BuiltInFunctionDefinitions.LOG, FlinkSqlOperatorTable.LOG);
+        definitionSqlOperatorHashMap.put(
+                BuiltInFunctionDefinitions.POWER, FlinkSqlOperatorTable.POWER);
+        definitionSqlOperatorHashMap.put(BuiltInFunctionDefinitions.MOD, FlinkSqlOperatorTable.MOD);
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.MINUS_PREFIX, FlinkSqlOperatorTable.UNARY_MINUS);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.SIN, FlinkSqlOperatorTable.SIN);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.COS, FlinkSqlOperatorTable.COS);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.SINH, FlinkSqlOperatorTable.SINH);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.TAN, FlinkSqlOperatorTable.TAN);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.TANH, FlinkSqlOperatorTable.TANH);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.COT, FlinkSqlOperatorTable.COT);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.ASIN, FlinkSqlOperatorTable.ASIN);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.ACOS, FlinkSqlOperatorTable.ACOS);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.ATAN, FlinkSqlOperatorTable.ATAN);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.ATAN2, FlinkSqlOperatorTable.ATAN2);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.COSH, FlinkSqlOperatorTable.COSH);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(BuiltInFunctionDefinitions.SIN, FlinkSqlOperatorTable.SIN);
+        definitionSqlOperatorHashMap.put(BuiltInFunctionDefinitions.COS, FlinkSqlOperatorTable.COS);
+        definitionSqlOperatorHashMap.put(
+                BuiltInFunctionDefinitions.SINH, FlinkSqlOperatorTable.SINH);
+        definitionSqlOperatorHashMap.put(BuiltInFunctionDefinitions.TAN, FlinkSqlOperatorTable.TAN);
+        definitionSqlOperatorHashMap.put(
+                BuiltInFunctionDefinitions.TANH, FlinkSqlOperatorTable.TANH);
+        definitionSqlOperatorHashMap.put(BuiltInFunctionDefinitions.COT, FlinkSqlOperatorTable.COT);
+        definitionSqlOperatorHashMap.put(
+                BuiltInFunctionDefinitions.ASIN, FlinkSqlOperatorTable.ASIN);
+        definitionSqlOperatorHashMap.put(
+                BuiltInFunctionDefinitions.ACOS, FlinkSqlOperatorTable.ACOS);
+        definitionSqlOperatorHashMap.put(
+                BuiltInFunctionDefinitions.ATAN, FlinkSqlOperatorTable.ATAN);
+        definitionSqlOperatorHashMap.put(
+                BuiltInFunctionDefinitions.ATAN2, FlinkSqlOperatorTable.ATAN2);
+        definitionSqlOperatorHashMap.put(
+                BuiltInFunctionDefinitions.COSH, FlinkSqlOperatorTable.COSH);
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.DEGREES, FlinkSqlOperatorTable.DEGREES);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.RADIANS, FlinkSqlOperatorTable.RADIANS);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.SIGN, FlinkSqlOperatorTable.SIGN);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.ROUND, FlinkSqlOperatorTable.ROUND);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.PI, FlinkSqlOperatorTable.PI);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.E, FlinkSqlOperatorTable.E);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.RAND, FlinkSqlOperatorTable.RAND);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
+                BuiltInFunctionDefinitions.SIGN, FlinkSqlOperatorTable.SIGN);
+        definitionSqlOperatorHashMap.put(
+                BuiltInFunctionDefinitions.ROUND, FlinkSqlOperatorTable.ROUND);
+        definitionSqlOperatorHashMap.put(BuiltInFunctionDefinitions.PI, FlinkSqlOperatorTable.PI);
+        definitionSqlOperatorHashMap.put(BuiltInFunctionDefinitions.E, FlinkSqlOperatorTable.E);
+        definitionSqlOperatorHashMap.put(
+                BuiltInFunctionDefinitions.RAND, FlinkSqlOperatorTable.RAND);
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.RAND_INTEGER, FlinkSqlOperatorTable.RAND_INTEGER);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.BIN, FlinkSqlOperatorTable.BIN);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.HEX, FlinkSqlOperatorTable.HEX);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(BuiltInFunctionDefinitions.BIN, FlinkSqlOperatorTable.BIN);
+        definitionSqlOperatorHashMap.put(BuiltInFunctionDefinitions.HEX, FlinkSqlOperatorTable.HEX);
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.TRUNCATE, FlinkSqlOperatorTable.TRUNCATE);
 
         // time functions
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.EXTRACT, FlinkSqlOperatorTable.EXTRACT);
-        DEFINITION_OPERATOR_MAP.put(
-                BuiltInFunctionDefinitions.CURRENT_DATE, FlinkSqlOperatorTable.CURRENT_DATE);
-        DEFINITION_OPERATOR_MAP.put(
-                BuiltInFunctionDefinitions.CURRENT_TIME, FlinkSqlOperatorTable.CURRENT_TIME);
-        DEFINITION_OPERATOR_MAP.put(
-                BuiltInFunctionDefinitions.CURRENT_TIMESTAMP,
-                FlinkSqlOperatorTable.CURRENT_TIMESTAMP);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.CURRENT_ROW_TIMESTAMP,
                 FlinkSqlOperatorTable.CURRENT_ROW_TIMESTAMP);
-        DEFINITION_OPERATOR_MAP.put(
-                BuiltInFunctionDefinitions.LOCAL_TIME, FlinkSqlOperatorTable.LOCALTIME);
-        DEFINITION_OPERATOR_MAP.put(
-                BuiltInFunctionDefinitions.LOCAL_TIMESTAMP, FlinkSqlOperatorTable.LOCALTIMESTAMP);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.DATE_FORMAT, FlinkSqlOperatorTable.DATE_FORMAT);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.CONVERT_TZ, FlinkSqlOperatorTable.CONVERT_TZ);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.FROM_UNIXTIME, FlinkSqlOperatorTable.FROM_UNIXTIME);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.UNIX_TIMESTAMP, FlinkSqlOperatorTable.UNIX_TIMESTAMP);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.TO_DATE, FlinkSqlOperatorTable.TO_DATE);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.TO_TIMESTAMP_LTZ,
                 FlinkSqlOperatorTable.TO_TIMESTAMP_LTZ);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.TO_TIMESTAMP, FlinkSqlOperatorTable.TO_TIMESTAMP);
 
         // catalog functions
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.CURRENT_DATABASE,
                 FlinkSqlOperatorTable.CURRENT_DATABASE);
 
         // collection
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.AT, FlinkSqlOperatorTable.ITEM);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(BuiltInFunctionDefinitions.AT, FlinkSqlOperatorTable.ITEM);
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.CARDINALITY, FlinkSqlOperatorTable.CARDINALITY);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.ORDER_DESC, FlinkSqlOperatorTable.DESC);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.ARRAY_ELEMENT, FlinkSqlOperatorTable.ELEMENT);
 
         // crypto hash
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.MD5, FlinkSqlOperatorTable.MD5);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.SHA2, FlinkSqlOperatorTable.SHA2);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(BuiltInFunctionDefinitions.MD5, FlinkSqlOperatorTable.MD5);
+        definitionSqlOperatorHashMap.put(
+                BuiltInFunctionDefinitions.SHA2, FlinkSqlOperatorTable.SHA2);
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.SHA224, FlinkSqlOperatorTable.SHA224);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.SHA256, FlinkSqlOperatorTable.SHA256);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.SHA384, FlinkSqlOperatorTable.SHA384);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.SHA512, FlinkSqlOperatorTable.SHA512);
-        DEFINITION_OPERATOR_MAP.put(BuiltInFunctionDefinitions.SHA1, FlinkSqlOperatorTable.SHA1);
-        DEFINITION_OPERATOR_MAP.put(
+        definitionSqlOperatorHashMap.put(
+                BuiltInFunctionDefinitions.SHA1, FlinkSqlOperatorTable.SHA1);
+        definitionSqlOperatorHashMap.put(
                 BuiltInFunctionDefinitions.STREAM_RECORD_TIMESTAMP,
                 FlinkSqlOperatorTable.STREAMRECORD_TIMESTAMP);
     }
 
     @Override
     public Optional<RexNode> convert(CallExpression call, ConvertContext context) {
-        SqlOperator operator = DEFINITION_OPERATOR_MAP.get(call.getFunctionDefinition());
+        SqlOperator operator = definitionSqlOperatorHashMap.get(call.getFunctionDefinition());
         return Optional.ofNullable(operator)
                 .map(
                         op ->
diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/expressions/converter/ExpressionConverter.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/expressions/converter/ExpressionConverter.java
index aa56faf45bb..ab5e0cf09c3 100644
--- a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/expressions/converter/ExpressionConverter.java
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/expressions/converter/ExpressionConverter.java
@@ -34,7 +34,6 @@ import org.apache.flink.table.planner.calcite.FlinkTypeFactory;
 import org.apache.flink.table.planner.calcite.RexFieldVariable;
 import org.apache.flink.table.planner.expressions.RexNodeExpression;
 import org.apache.flink.table.planner.expressions.converter.CallExpressionConvertRule.ConvertContext;
-import org.apache.flink.table.planner.utils.ShortcutUtils;
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.table.types.logical.TimeType;
 
@@ -63,20 +62,13 @@ import java.util.Optional;
 import java.util.stream.Collectors;
 
 import static org.apache.flink.table.planner.typeutils.SymbolUtil.commonToCalcite;
+import static org.apache.flink.table.planner.utils.ShortcutUtils.unwrapContext;
 import static org.apache.flink.table.planner.utils.TimestampStringUtils.fromLocalDateTime;
 import static org.apache.flink.table.runtime.types.LogicalTypeDataTypeConverter.fromDataTypeToLogicalType;
 
 /** Visit expression to generator {@link RexNode}. */
 public class ExpressionConverter implements ExpressionVisitor<RexNode> {
 
-    private static final List<CallExpressionConvertRule> FUNCTION_CONVERT_CHAIN =
-            Arrays.asList(
-                    new LegacyScalarFunctionConvertRule(),
-                    new FunctionDefinitionConvertRule(),
-                    new OverConvertRule(),
-                    new DirectConvertRule(),
-                    new CustomizedConvertRule());
-
     private final RelBuilder relBuilder;
     private final FlinkTypeFactory typeFactory;
     private final DataTypeFactory dataTypeFactory;
@@ -85,14 +77,22 @@ public class ExpressionConverter implements ExpressionVisitor<RexNode> {
         this.relBuilder = relBuilder;
         this.typeFactory = (FlinkTypeFactory) relBuilder.getRexBuilder().getTypeFactory();
         this.dataTypeFactory =
-                ShortcutUtils.unwrapContext(relBuilder.getCluster())
-                        .getCatalogManager()
-                        .getDataTypeFactory();
+                unwrapContext(relBuilder.getCluster()).getCatalogManager().getDataTypeFactory();
+    }
+
+    private List<CallExpressionConvertRule> getFunctionConvertChain(boolean isBatchMode) {
+        return Arrays.asList(
+                new LegacyScalarFunctionConvertRule(),
+                new FunctionDefinitionConvertRule(),
+                new OverConvertRule(),
+                DirectConvertRule.instance(isBatchMode),
+                new CustomizedConvertRule());
     }
 
     @Override
     public RexNode visit(CallExpression call) {
-        for (CallExpressionConvertRule rule : FUNCTION_CONVERT_CHAIN) {
+        boolean isBatchMode = unwrapContext(relBuilder).isBatchMode();
+        for (CallExpressionConvertRule rule : getFunctionConvertChain(isBatchMode)) {
             Optional<RexNode> converted = rule.convert(call, newFunctionContext());
             if (converted.isPresent()) {
                 return converted.get();
diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/functions/sql/FlinkCurrentDateDynamicFunction.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/functions/sql/FlinkCurrentDateDynamicFunction.java
new file mode 100644
index 00000000000..104111c1210
--- /dev/null
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/functions/sql/FlinkCurrentDateDynamicFunction.java
@@ -0,0 +1,71 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.planner.functions.sql;
+
+import org.apache.flink.annotation.Internal;
+
+import org.apache.calcite.sql.fun.SqlCurrentDateFunction;
+import org.checkerframework.checker.nullness.qual.Nullable;
+
+import java.util.Objects;
+
+/**
+ * The Flink CURRENT_DATE function differs from the parent {@link SqlCurrentDateFunction} which is
+ * aware of whether it is used in batch mode, if true it will act totally same as the parent {@link
+ * SqlCurrentDateFunction}, but will be a non-deterministic function if not in batch mode.
+ */
+@Internal
+public class FlinkCurrentDateDynamicFunction extends SqlCurrentDateFunction {
+
+    private final boolean isBatchMode;
+
+    public FlinkCurrentDateDynamicFunction(boolean isBatchMode) {
+        this.isBatchMode = isBatchMode;
+    }
+
+    @Override
+    public boolean isDynamicFunction() {
+        return isBatchMode && super.isDynamicFunction();
+    }
+
+    @Override
+    public boolean isDeterministic() {
+        // be a non-deterministic function in streaming mode
+        return isBatchMode;
+    }
+
+    @Override
+    public boolean equals(@Nullable Object obj) {
+        if (!(obj instanceof FlinkCurrentDateDynamicFunction)) {
+            return false;
+        }
+        if (!obj.getClass().equals(this.getClass())) {
+            return false;
+        }
+        FlinkCurrentDateDynamicFunction other = (FlinkCurrentDateDynamicFunction) obj;
+        return this.getName().equals(other.getName())
+                && kind == other.kind
+                && this.isBatchMode == other.isBatchMode;
+    }
+
+    @Override
+    public int hashCode() {
+        return Objects.hash(kind, this.getName(), isBatchMode);
+    }
+}
diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/functions/sql/FlinkSqlTimestampFunction.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/functions/sql/FlinkCurrentRowTimestampFunction.java
similarity index 61%
rename from flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/functions/sql/FlinkSqlTimestampFunction.java
rename to flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/functions/sql/FlinkCurrentRowTimestampFunction.java
index f938ee23e5a..fd39476e449 100644
--- a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/functions/sql/FlinkSqlTimestampFunction.java
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/functions/sql/FlinkCurrentRowTimestampFunction.java
@@ -24,19 +24,21 @@ import org.apache.calcite.rel.type.RelDataType;
 import org.apache.calcite.sql.SqlOperatorBinding;
 import org.apache.calcite.sql.fun.SqlAbstractTimeFunction;
 import org.apache.calcite.sql.type.SqlTypeName;
+import org.checkerframework.checker.nullness.qual.Nullable;
+
+import java.util.Objects;
 
 /**
- * Function that used to define SQL time function like LOCALTIMESTAMP, CURRENT_TIMESTAMP,
- * CURRENT_ROW_TIMESTAMP(), NOW() in Flink, the function support configuring the return type and the
+ * The function CURRENT_ROW_TIMESTAMP() in Flink which supports configuring the return type and the
  * precision of return type.
  */
 @Internal
-public class FlinkSqlTimestampFunction extends SqlAbstractTimeFunction {
+public class FlinkCurrentRowTimestampFunction extends SqlAbstractTimeFunction {
 
     private final SqlTypeName returnTypeName;
     private final int precision;
 
-    public FlinkSqlTimestampFunction(
+    public FlinkCurrentRowTimestampFunction(
             String functionName, SqlTypeName returnTypeName, int precision) {
         // access protected constructor
         super(functionName, returnTypeName);
@@ -49,8 +51,33 @@ public class FlinkSqlTimestampFunction extends SqlAbstractTimeFunction {
         return opBinding.getTypeFactory().createSqlType(returnTypeName, precision);
     }
 
+    @Override
+    public boolean isDynamicFunction() {
+        return false;
+    }
+
     @Override
     public boolean isDeterministic() {
         return false;
     }
+
+    @Override
+    public boolean equals(@Nullable Object obj) {
+        if (!(obj instanceof FlinkCurrentRowTimestampFunction)) {
+            return false;
+        }
+        if (!obj.getClass().equals(this.getClass())) {
+            return false;
+        }
+        FlinkCurrentRowTimestampFunction other = (FlinkCurrentRowTimestampFunction) obj;
+        return this.getName().equals(other.getName())
+                && kind == other.kind
+                && this.precision == other.precision
+                && this.returnTypeName.equals(other.returnTypeName);
+    }
+
+    @Override
+    public int hashCode() {
+        return Objects.hash(kind, this.getName(), precision, returnTypeName);
+    }
 }
diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/functions/sql/FlinkSqlOperatorTable.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/functions/sql/FlinkSqlOperatorTable.java
index 223783ed7fa..a09e809cd9c 100644
--- a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/functions/sql/FlinkSqlOperatorTable.java
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/functions/sql/FlinkSqlOperatorTable.java
@@ -18,6 +18,7 @@
 
 package org.apache.flink.table.planner.functions.sql;
 
+import org.apache.flink.table.api.TableException;
 import org.apache.flink.table.planner.calcite.FlinkTypeFactory;
 import org.apache.flink.table.planner.functions.sql.internal.SqlAuxiliaryGroupAggFunction;
 import org.apache.flink.table.planner.plan.type.FlinkReturnTypes;
@@ -49,7 +50,9 @@ import org.apache.calcite.sql.validate.SqlNameMatcher;
 import org.apache.calcite.sql.validate.SqlNameMatchers;
 
 import java.util.Arrays;
+import java.util.HashMap;
 import java.util.List;
+import java.util.Map;
 
 import static org.apache.flink.table.planner.plan.type.FlinkReturnTypes.ARG0_VARCHAR_FORCE_NULLABLE;
 import static org.apache.flink.table.planner.plan.type.FlinkReturnTypes.STR_MAP_NULLABLE;
@@ -60,20 +63,72 @@ import static org.apache.flink.table.planner.plan.type.FlinkReturnTypes.VARCHAR_
 public class FlinkSqlOperatorTable extends ReflectiveSqlOperatorTable {
 
     /** The table of contains Flink-specific operators. */
-    private static FlinkSqlOperatorTable instance;
+    private static final Map<Boolean, FlinkSqlOperatorTable> cachedInstances = new HashMap<>();
 
     /** Returns the Flink operator table, creating it if necessary. */
-    public static synchronized FlinkSqlOperatorTable instance() {
+    public static synchronized FlinkSqlOperatorTable instance(boolean isBatchMode) {
+        FlinkSqlOperatorTable instance = cachedInstances.get(isBatchMode);
         if (instance == null) {
             // Creates and initializes the standard operator table.
             // Uses two-phase construction, because we can't initialize the
             // table until the constructor of the sub-class has completed.
             instance = new FlinkSqlOperatorTable();
             instance.init();
+
+            // ensure no dynamic function declares directly
+            validateNoDynamicFunction(instance);
+
+            // register functions based on batch or streaming mode
+            final FlinkSqlOperatorTable finalInstance = instance;
+            dynamicFunctions(isBatchMode).forEach(f -> finalInstance.register(f));
+            cachedInstances.put(isBatchMode, finalInstance);
         }
         return instance;
     }
 
+    public static List<SqlFunction> dynamicFunctions(boolean isBatchMode) {
+        return Arrays.asList(
+                new FlinkTimestampDynamicFunction(
+                        SqlStdOperatorTable.LOCALTIME.getName(), SqlTypeName.TIME, isBatchMode),
+                new FlinkTimestampDynamicFunction(
+                        SqlStdOperatorTable.CURRENT_TIME.getName(), SqlTypeName.TIME, isBatchMode),
+                new FlinkCurrentDateDynamicFunction(isBatchMode),
+                new FlinkTimestampWithPrecisionDynamicFunction(
+                        SqlStdOperatorTable.LOCALTIMESTAMP.getName(),
+                        SqlTypeName.TIMESTAMP,
+                        isBatchMode,
+                        3),
+                new FlinkTimestampWithPrecisionDynamicFunction(
+                        SqlStdOperatorTable.CURRENT_TIMESTAMP.getName(),
+                        SqlTypeName.TIMESTAMP_WITH_LOCAL_TIME_ZONE,
+                        isBatchMode,
+                        3),
+                new FlinkTimestampWithPrecisionDynamicFunction(
+                        FlinkTimestampWithPrecisionDynamicFunction.NOW,
+                        SqlTypeName.TIMESTAMP_WITH_LOCAL_TIME_ZONE,
+                        isBatchMode,
+                        3) {
+                    @Override
+                    public SqlSyntax getSyntax() {
+                        return SqlSyntax.FUNCTION;
+                    }
+                });
+    }
+
+    private static void validateNoDynamicFunction(FlinkSqlOperatorTable instance)
+            throws TableException {
+        instance.getOperatorList()
+                .forEach(
+                        op -> {
+                            if (op.isDynamicFunction() && op.isDeterministic()) {
+                                throw new TableException(
+                                        String.format(
+                                                "Dynamic function: %s is not allowed declaring directly, please add it to initializing.",
+                                                op.getName()));
+                            }
+                        });
+    }
+
     private static final SqlReturnTypeInference PROCTIME_TYPE_INFERENCE =
             ReturnTypes.explicit(
                     factory -> ((FlinkTypeFactory) factory).createProctimeIndicatorType(false));
@@ -560,29 +615,10 @@ public class FlinkSqlOperatorTable extends ReflectiveSqlOperatorTable {
                     SqlFunctionCategory.STRING);
 
     // Flink timestamp functions
-    public static final SqlFunction LOCALTIMESTAMP =
-            new FlinkSqlTimestampFunction("LOCALTIMESTAMP", SqlTypeName.TIMESTAMP, 3);
-
-    public static final SqlFunction CURRENT_TIMESTAMP =
-            new FlinkSqlTimestampFunction(
-                    "CURRENT_TIMESTAMP", SqlTypeName.TIMESTAMP_WITH_LOCAL_TIME_ZONE, 3);
-
-    public static final SqlFunction NOW =
-            new FlinkSqlTimestampFunction("NOW", SqlTypeName.TIMESTAMP_WITH_LOCAL_TIME_ZONE, 3) {
-                @Override
-                public SqlSyntax getSyntax() {
-                    return SqlSyntax.FUNCTION;
-                }
-            };
     public static final SqlFunction CURRENT_ROW_TIMESTAMP =
-            new FlinkSqlTimestampFunction(
+            new FlinkCurrentRowTimestampFunction(
                     "CURRENT_ROW_TIMESTAMP", SqlTypeName.TIMESTAMP_WITH_LOCAL_TIME_ZONE, 3) {
 
-                @Override
-                public boolean isDeterministic() {
-                    return false;
-                }
-
                 @Override
                 public SqlSyntax getSyntax() {
                     return SqlSyntax.FUNCTION;
@@ -1132,9 +1168,6 @@ public class FlinkSqlOperatorTable extends ReflectiveSqlOperatorTable {
     public static final SqlFunction NULLIF = SqlStdOperatorTable.NULLIF;
     public static final SqlFunction FLOOR = SqlStdOperatorTable.FLOOR;
     public static final SqlFunction CEIL = SqlStdOperatorTable.CEIL;
-    public static final SqlFunction LOCALTIME = SqlStdOperatorTable.LOCALTIME;
-    public static final SqlFunction CURRENT_TIME = SqlStdOperatorTable.CURRENT_TIME;
-    public static final SqlFunction CURRENT_DATE = SqlStdOperatorTable.CURRENT_DATE;
     public static final SqlFunction CAST = SqlStdOperatorTable.CAST;
     public static final SqlOperator SCALAR_QUERY = SqlStdOperatorTable.SCALAR_QUERY;
     public static final SqlOperator EXISTS = SqlStdOperatorTable.EXISTS;
diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/functions/sql/FlinkTimestampDynamicFunction.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/functions/sql/FlinkTimestampDynamicFunction.java
new file mode 100644
index 00000000000..2db95043755
--- /dev/null
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/functions/sql/FlinkTimestampDynamicFunction.java
@@ -0,0 +1,77 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.planner.functions.sql;
+
+import org.apache.flink.annotation.Internal;
+
+import org.apache.calcite.sql.fun.SqlAbstractTimeFunction;
+import org.apache.calcite.sql.fun.SqlStdOperatorTable;
+import org.apache.calcite.sql.type.SqlTypeName;
+import org.checkerframework.checker.nullness.qual.Nullable;
+
+import java.util.Objects;
+
+/**
+ * Function that used to define SQL time functions like LOCALTIME, CURRENT_TIME(these are all
+ * dynamic functions in Calcite's {@link SqlStdOperatorTable}) in Flink, the difference from the
+ * parent {@link SqlAbstractTimeFunction} is this function class be aware of whether it is used in
+ * batch mode, if true it will act totally same as the parent {@link SqlAbstractTimeFunction}, but
+ * will be a non-deterministic function if not in batch mode.
+ */
+@Internal
+public class FlinkTimestampDynamicFunction extends SqlAbstractTimeFunction {
+
+    protected final boolean isBatchMode;
+
+    public FlinkTimestampDynamicFunction(
+            String functionName, SqlTypeName returnTypeName, boolean isBatchMode) {
+        super(functionName, returnTypeName);
+        this.isBatchMode = isBatchMode;
+    }
+
+    @Override
+    public boolean isDynamicFunction() {
+        return isBatchMode && super.isDynamicFunction();
+    }
+
+    @Override
+    public boolean isDeterministic() {
+        // be a non-deterministic function in streaming mode
+        return isBatchMode;
+    }
+
+    @Override
+    public boolean equals(@Nullable Object obj) {
+        if (!(obj instanceof FlinkTimestampDynamicFunction)) {
+            return false;
+        }
+        if (!obj.getClass().equals(this.getClass())) {
+            return false;
+        }
+        FlinkTimestampDynamicFunction other = (FlinkTimestampDynamicFunction) obj;
+        return this.getName().equals(other.getName())
+                && kind == other.kind
+                && this.isBatchMode == other.isBatchMode;
+    }
+
+    @Override
+    public int hashCode() {
+        return Objects.hash(kind, this.getName(), isBatchMode);
+    }
+}
diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/functions/sql/FlinkTimestampWithPrecisionDynamicFunction.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/functions/sql/FlinkTimestampWithPrecisionDynamicFunction.java
new file mode 100644
index 00000000000..2991e802149
--- /dev/null
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/functions/sql/FlinkTimestampWithPrecisionDynamicFunction.java
@@ -0,0 +1,76 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.planner.functions.sql;
+
+import org.apache.flink.annotation.Internal;
+
+import org.apache.calcite.rel.type.RelDataType;
+import org.apache.calcite.sql.SqlOperatorBinding;
+import org.apache.calcite.sql.type.SqlTypeName;
+import org.checkerframework.checker.nullness.qual.Nullable;
+
+import java.util.Objects;
+
+/**
+ * Function that used to define SQL time function like LOCALTIMESTAMP, CURRENT_TIMESTAMP, NOW() in
+ * Flink, the function support configuring the return type and the * precision of return type.
+ */
+@Internal
+public class FlinkTimestampWithPrecisionDynamicFunction extends FlinkTimestampDynamicFunction {
+    /** function name for 'NOW()'. */
+    public static final String NOW = "NOW";
+
+    private final SqlTypeName returnTypeName;
+
+    private final int precision;
+
+    public FlinkTimestampWithPrecisionDynamicFunction(
+            String name, SqlTypeName typeName, boolean isBatchMode, int precision) {
+        super(name, typeName, isBatchMode);
+        this.returnTypeName = typeName;
+        this.precision = precision;
+    }
+
+    @Override
+    public RelDataType inferReturnType(SqlOperatorBinding opBinding) {
+        return opBinding.getTypeFactory().createSqlType(returnTypeName, precision);
+    }
+
+    @Override
+    public boolean equals(@Nullable Object obj) {
+        if (!(obj instanceof FlinkTimestampWithPrecisionDynamicFunction)) {
+            return false;
+        }
+        if (!obj.getClass().equals(this.getClass())) {
+            return false;
+        }
+        FlinkTimestampWithPrecisionDynamicFunction other =
+                (FlinkTimestampWithPrecisionDynamicFunction) obj;
+        return this.getName().equals(other.getName())
+                && kind == other.kind
+                && this.isBatchMode == other.isBatchMode
+                && this.precision == other.precision
+                && this.returnTypeName.equals(other.returnTypeName);
+    }
+
+    @Override
+    public int hashCode() {
+        return Objects.hash(kind, this.getName(), isBatchMode, precision, returnTypeName);
+    }
+}
diff --git a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/codegen/calls/FunctionGenerator.scala b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/codegen/calls/FunctionGenerator.scala
index 7278b2a0eae..53db45fe7c2 100644
--- a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/codegen/calls/FunctionGenerator.scala
+++ b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/codegen/calls/FunctionGenerator.scala
@@ -19,12 +19,15 @@ package org.apache.flink.table.planner.codegen.calls
 
 import org.apache.flink.api.common.RuntimeExecutionMode
 import org.apache.flink.configuration.{ExecutionOptions, ReadableConfig}
+import org.apache.flink.table.api.TableException
+import org.apache.flink.table.planner.functions.sql.{FlinkSqlOperatorTable, FlinkTimestampWithPrecisionDynamicFunction}
 import org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable._
 import org.apache.flink.table.runtime.types.PlannerTypeUtils.isPrimitive
 import org.apache.flink.table.types.logical.{LogicalType, LogicalTypeRoot}
 import org.apache.flink.table.types.logical.LogicalTypeRoot._
 
 import org.apache.calcite.sql.SqlOperator
+import org.apache.calcite.sql.fun.SqlStdOperatorTable
 
 import java.lang.reflect.Method
 
@@ -320,21 +323,9 @@ class FunctionGenerator private (tableConfig: ReadableConfig) {
       Some(BuiltInMethods.TIMESTAMP_CEIL_TIME_ZONE))
   )
 
-  addSqlFunction(CURRENT_DATE, Seq(), new CurrentTimePointCallGen(false, isStreamingMode))
-
-  addSqlFunction(CURRENT_TIME, Seq(), new CurrentTimePointCallGen(false, isStreamingMode))
-
-  addSqlFunction(NOW, Seq(), new CurrentTimePointCallGen(false, isStreamingMode))
-
-  addSqlFunction(CURRENT_TIMESTAMP, Seq(), new CurrentTimePointCallGen(false, isStreamingMode))
-
   // CURRENT_ROW_TIMESTAMP evaluates in row-level
   addSqlFunction(CURRENT_ROW_TIMESTAMP, Seq(), new CurrentTimePointCallGen(false, true))
 
-  addSqlFunction(LOCALTIME, Seq(), new CurrentTimePointCallGen(true, isStreamingMode))
-
-  addSqlFunction(LOCALTIMESTAMP, Seq(), new CurrentTimePointCallGen(true, isStreamingMode))
-
   addSqlFunctionMethod(LOG2, Seq(DOUBLE), BuiltInMethods.LOG2)
 
   addSqlFunctionMethod(LOG2, Seq(DECIMAL), BuiltInMethods.LOG2_DEC)
@@ -544,6 +535,27 @@ class FunctionGenerator private (tableConfig: ReadableConfig) {
     Seq(VARCHAR),
     new NotCallGen(new MethodCallGen(BuiltInMethods.IS_JSON_SCALAR, argsNullable = true)))
 
+  FlinkSqlOperatorTable
+    .dynamicFunctions(!isStreamingMode)
+    .forEach(
+      func => {
+        if (
+          func.getName == SqlStdOperatorTable.LOCALTIME.getName || func.getName == SqlStdOperatorTable.LOCALTIMESTAMP.getName
+        ) {
+          addSqlFunction(func, Seq(), new CurrentTimePointCallGen(true, isStreamingMode))
+        } else if (
+          func.getName == SqlStdOperatorTable.CURRENT_DATE.getName
+          || func.getName == SqlStdOperatorTable.CURRENT_TIME.getName
+          || func.getName == SqlStdOperatorTable.CURRENT_TIMESTAMP.getName
+          || func.getName == FlinkTimestampWithPrecisionDynamicFunction.NOW
+        ) {
+          addSqlFunction(func, Seq(), new CurrentTimePointCallGen(false, isStreamingMode))
+        } else {
+          throw new TableException(
+            s"Unsupported dynamic function ${func.getName} for FunctionGenerator")
+        }
+      })
+
   // ----------------------------------------------------------------------------------------------
 
   /**
diff --git a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/batch/sql/DynamicFunctionPlanTest.java b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/batch/sql/DynamicFunctionPlanTest.java
new file mode 100644
index 00000000000..a765f94c881
--- /dev/null
+++ b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/batch/sql/DynamicFunctionPlanTest.java
@@ -0,0 +1,36 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.planner.plan.batch.sql;
+
+import org.apache.flink.table.api.TableConfig;
+import org.apache.flink.table.planner.plan.common.DynamicFunctionPlanTestBase;
+import org.apache.flink.table.planner.utils.TableTestUtil;
+
+/** Plan test for queries contain dynamic functions in batch. */
+public class DynamicFunctionPlanTest extends DynamicFunctionPlanTestBase {
+    @Override
+    protected TableTestUtil getTableTestUtil() {
+        return batchTestUtil(TableConfig.getDefault());
+    }
+
+    @Override
+    protected boolean isBatchMode() {
+        return true;
+    }
+}
diff --git a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/common/DynamicFunctionPlanTestBase.java b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/common/DynamicFunctionPlanTestBase.java
new file mode 100644
index 00000000000..22140d384a6
--- /dev/null
+++ b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/common/DynamicFunctionPlanTestBase.java
@@ -0,0 +1,99 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.planner.plan.common;
+
+import org.apache.flink.table.planner.utils.TableTestBase;
+import org.apache.flink.table.planner.utils.TableTestUtil;
+
+import org.junit.Before;
+import org.junit.Test;
+
+/** Plan test for queries contain dynamic functions. */
+public abstract class DynamicFunctionPlanTestBase extends TableTestBase {
+
+    private TableTestUtil util;
+
+    protected abstract boolean isBatchMode();
+
+    protected abstract TableTestUtil getTableTestUtil();
+
+    @Before
+    public void setup() {
+        util = getTableTestUtil();
+
+        util.tableEnv()
+                .executeSql(
+                        "CREATE TABLE src (\n"
+                                + " a INTEGER,\n"
+                                + " b VARCHAR,\n"
+                                + " cat VARCHAR,\n"
+                                + " gmt_date DATE,\n"
+                                + " cnt BIGINT,\n"
+                                + " ts TIME,\n"
+                                + " PRIMARY KEY (cat) NOT ENFORCED\n"
+                                + ") WITH (\n"
+                                + " 'connector' = 'values'\n"
+                                + " ,'bounded' = '"
+                                + isBatchMode()
+                                + "'\n"
+                                + ")");
+    }
+
+    @Test
+    public void testAggregateReduceConstants() {
+        util.verifyExecPlan(
+                "SELECT\n"
+                        + "     cat, gmt_date, SUM(cnt), count(*)\n"
+                        + "FROM src\n"
+                        + "WHERE gmt_date = current_date\n"
+                        + "GROUP BY cat, gmt_date");
+    }
+
+    @Test
+    public void testAggregateReduceConstants2() {
+        // current RelMdPredicates only look at columns that are projected without any function
+        // applied, so 'SUBSTR(CAST(LOCALTIME AS VARCHAR), 1, 2)' will never be inferred as constant
+        util.verifyExecPlan(
+                "SELECT\n"
+                        + "cat, hh, SUM(cnt), COUNT(*)\n"
+                        + "FROM (SELECT *, SUBSTR(CAST(LOCALTIME AS VARCHAR), 1, 2) hh FROM src)\n"
+                        + "WHERE SUBSTR(CAST(ts AS VARCHAR), 1, 2) = hh\n"
+                        + "GROUP BY cat, hh");
+    }
+
+    @Test
+    public void testAggregateReduceConstants3() {
+        util.verifyExecPlan(
+                "SELECT\n"
+                        + "     gmt_date, ts, cat, SUBSTR(CAST(ts AS VARCHAR), 1, 2), SUM(cnt)\n"
+                        + "FROM src\n"
+                        + "WHERE gmt_date = CURRENT_DATE\n"
+                        + "  AND cat = 'fruit' AND ts = CURRENT_TIME\n"
+                        + "GROUP BY gmt_date, ts, cat");
+    }
+
+    @Test
+    public void testCalcMerge() {
+        util.verifyExecPlan(
+                "SELECT * FROM ( \n"
+                        + "   SELECT *, SUBSTR(CAST(LOCALTIME AS VARCHAR), 1, 2) hh\n"
+                        + "   FROM src\n"
+                        + " ) t1 WHERE hh > 12 AND cat LIKE 'fruit%'\n");
+    }
+}
diff --git a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/stream/sql/DynamicFunctionPlanTest.java b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/stream/sql/DynamicFunctionPlanTest.java
new file mode 100644
index 00000000000..adb4523e78d
--- /dev/null
+++ b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/stream/sql/DynamicFunctionPlanTest.java
@@ -0,0 +1,36 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.planner.plan.stream.sql;
+
+import org.apache.flink.table.api.TableConfig;
+import org.apache.flink.table.planner.plan.common.DynamicFunctionPlanTestBase;
+import org.apache.flink.table.planner.utils.TableTestUtil;
+
+/** Plan test for queries contain dynamic functions in streaming. */
+public class DynamicFunctionPlanTest extends DynamicFunctionPlanTestBase {
+    @Override
+    protected TableTestUtil getTableTestUtil() {
+        return streamTestUtil(TableConfig.getDefault());
+    }
+
+    @Override
+    protected boolean isBatchMode() {
+        return false;
+    }
+}
diff --git a/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/DynamicFunctionPlanTest.xml b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/DynamicFunctionPlanTest.xml
new file mode 100644
index 00000000000..1405e31e772
--- /dev/null
+++ b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/DynamicFunctionPlanTest.xml
@@ -0,0 +1,122 @@
+<?xml version="1.0" ?>
+<!--
+Licensed to the Apache Software Foundation (ASF) under one or more
+contributor license agreements.  See the NOTICE file distributed with
+this work for additional information regarding copyright ownership.
+The ASF licenses this file to you under the Apache License, Version 2.0
+(the "License"); you may not use this file except in compliance with
+the License.  You may obtain a copy of the License at
+
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+-->
+<Root>
+  <TestCase name="testAggregateReduceConstants">
+    <Resource name="sql">
+      <![CDATA[SELECT
+     cat, gmt_date, SUM(cnt), count(*)
+FROM src
+WHERE gmt_date = current_date
+GROUP BY cat, gmt_date]]>
+    </Resource>
+    <Resource name="ast">
+      <![CDATA[
+LogicalAggregate(group=[{0, 1}], EXPR$2=[SUM($2)], EXPR$3=[COUNT()])
++- LogicalProject(cat=[$2], gmt_date=[$3], cnt=[$4])
+   +- LogicalFilter(condition=[=($3, CURRENT_DATE)])
+      +- LogicalTableScan(table=[[default_catalog, default_database, src]])
+]]>
+    </Resource>
+    <Resource name="optimized exec plan">
+      <![CDATA[
+HashAggregate(isMerge=[false], groupBy=[cat], auxGrouping=[gmt_date], select=[cat, gmt_date, SUM(cnt) AS EXPR$2, COUNT(*) AS EXPR$3])
++- Exchange(distribution=[hash[cat]])
+   +- Calc(select=[cat, gmt_date, cnt], where=[(gmt_date = CURRENT_DATE())])
+      +- TableSourceScan(table=[[default_catalog, default_database, src, filter=[], project=[cat, gmt_date, cnt], metadata=[]]], fields=[cat, gmt_date, cnt])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testCalcMerge">
+    <Resource name="sql">
+      <![CDATA[SELECT * FROM ( 
+   SELECT *, SUBSTR(CAST(LOCALTIME AS VARCHAR), 1, 2) hh
+   FROM src
+ ) t1 WHERE hh > 12 AND cat LIKE 'fruit%'
+]]>
+    </Resource>
+    <Resource name="ast">
+      <![CDATA[
+LogicalProject(a=[$0], b=[$1], cat=[$2], gmt_date=[$3], cnt=[$4], ts=[$5], hh=[$6])
++- LogicalFilter(condition=[AND(>(CAST($6):BIGINT, 12), LIKE($2, _UTF-16LE'fruit%'))])
+   +- LogicalProject(a=[$0], b=[$1], cat=[$2], gmt_date=[$3], cnt=[$4], ts=[$5], hh=[SUBSTR(CAST(LOCALTIME):VARCHAR(2147483647) CHARACTER SET "UTF-16LE" NOT NULL, 1, 2)])
+      +- LogicalTableScan(table=[[default_catalog, default_database, src]])
+]]>
+    </Resource>
+    <Resource name="optimized exec plan">
+      <![CDATA[
+Calc(select=[a, b, cat, gmt_date, cnt, ts, SUBSTR(CAST(LOCALTIME() AS VARCHAR(2147483647)), 1, 2) AS hh], where=[((CAST(SUBSTR(CAST(LOCALTIME() AS VARCHAR(2147483647)), 1, 2) AS BIGINT) > 12) AND LIKE(cat, 'fruit%'))])
++- TableSourceScan(table=[[default_catalog, default_database, src, filter=[]]], fields=[a, b, cat, gmt_date, cnt, ts])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testAggregateReduceConstants2">
+    <Resource name="sql">
+      <![CDATA[SELECT
+cat, hh, SUM(cnt), COUNT(*)
+FROM (SELECT *, SUBSTR(CAST(LOCALTIME AS VARCHAR), 1, 2) hh FROM src)
+WHERE SUBSTR(CAST(ts AS VARCHAR), 1, 2) = hh
+GROUP BY cat, hh]]>
+    </Resource>
+    <Resource name="ast">
+      <![CDATA[
+LogicalAggregate(group=[{0, 1}], EXPR$2=[SUM($2)], EXPR$3=[COUNT()])
++- LogicalProject(cat=[$2], hh=[$6], cnt=[$4])
+   +- LogicalFilter(condition=[=(SUBSTR(CAST($5):VARCHAR(2147483647) CHARACTER SET "UTF-16LE", 1, 2), $6)])
+      +- LogicalProject(a=[$0], b=[$1], cat=[$2], gmt_date=[$3], cnt=[$4], ts=[$5], hh=[SUBSTR(CAST(LOCALTIME):VARCHAR(2147483647) CHARACTER SET "UTF-16LE" NOT NULL, 1, 2)])
+         +- LogicalTableScan(table=[[default_catalog, default_database, src]])
+]]>
+    </Resource>
+    <Resource name="optimized exec plan">
+      <![CDATA[
+HashAggregate(isMerge=[false], groupBy=[cat, hh], select=[cat, hh, SUM(cnt) AS EXPR$2, COUNT(*) AS EXPR$3])
++- Exchange(distribution=[hash[cat, hh]])
+   +- Calc(select=[cat, SUBSTR(CAST(LOCALTIME() AS VARCHAR(2147483647)), 1, 2) AS hh, cnt], where=[(SUBSTR(CAST(ts AS VARCHAR(2147483647)), 1, 2) = SUBSTR(CAST(LOCALTIME() AS VARCHAR(2147483647)), 1, 2))])
+      +- TableSourceScan(table=[[default_catalog, default_database, src, filter=[], project=[cat, cnt, ts], metadata=[]]], fields=[cat, cnt, ts])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testAggregateReduceConstants3">
+    <Resource name="sql">
+      <![CDATA[SELECT
+     gmt_date, ts, cat, SUBSTR(CAST(ts AS VARCHAR), 1, 2), SUM(cnt)
+FROM src
+WHERE gmt_date = CURRENT_DATE
+  AND cat = 'fruit' AND ts = CURRENT_TIME
+GROUP BY gmt_date, ts, cat]]>
+    </Resource>
+    <Resource name="ast">
+      <![CDATA[
+LogicalProject(gmt_date=[$0], ts=[$1], cat=[$2], EXPR$3=[SUBSTR(CAST($1):VARCHAR(2147483647) CHARACTER SET "UTF-16LE", 1, 2)], EXPR$4=[$3])
++- LogicalAggregate(group=[{0, 1, 2}], EXPR$4=[SUM($3)])
+   +- LogicalProject(gmt_date=[$3], ts=[$5], cat=[$2], cnt=[$4])
+      +- LogicalFilter(condition=[AND(=($3, CURRENT_DATE), =($2, _UTF-16LE'fruit'), =($5, CURRENT_TIME))])
+         +- LogicalTableScan(table=[[default_catalog, default_database, src]])
+]]>
+    </Resource>
+    <Resource name="optimized exec plan">
+      <![CDATA[
+Calc(select=[gmt_date, CAST(CURRENT_TIME() AS TIME(0)) AS ts, 'fruit' AS cat, SUBSTR(CAST(CURRENT_TIME() AS VARCHAR(2147483647)), 1, 2) AS EXPR$3, EXPR$4])
++- HashAggregate(isMerge=[true], groupBy=[gmt_date], select=[gmt_date, Final_SUM(sum$0) AS EXPR$4])
+   +- Exchange(distribution=[hash[gmt_date]])
+      +- LocalHashAggregate(groupBy=[gmt_date], select=[gmt_date, Partial_SUM(cnt) AS sum$0])
+         +- Calc(select=[gmt_date, cnt], where=[((gmt_date = CURRENT_DATE()) AND (cat = 'fruit') AND (ts = CURRENT_TIME()))])
+            +- TableSourceScan(table=[[default_catalog, default_database, src, filter=[], project=[cat, gmt_date, cnt, ts], metadata=[]]], fields=[cat, gmt_date, cnt, ts])
+]]>
+    </Resource>
+  </TestCase>
+</Root>
diff --git a/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/DynamicFunctionPlanTest.xml b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/DynamicFunctionPlanTest.xml
new file mode 100644
index 00000000000..15d493d32f8
--- /dev/null
+++ b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/DynamicFunctionPlanTest.xml
@@ -0,0 +1,123 @@
+<?xml version="1.0" ?>
+<!--
+Licensed to the Apache Software Foundation (ASF) under one or more
+contributor license agreements.  See the NOTICE file distributed with
+this work for additional information regarding copyright ownership.
+The ASF licenses this file to you under the Apache License, Version 2.0
+(the "License"); you may not use this file except in compliance with
+the License.  You may obtain a copy of the License at
+
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+-->
+<Root>
+  <TestCase name="testAggregateReduceConstants">
+    <Resource name="sql">
+      <![CDATA[SELECT
+     cat, gmt_date, SUM(cnt), count(*)
+FROM src
+WHERE gmt_date = current_date
+GROUP BY cat, gmt_date]]>
+    </Resource>
+    <Resource name="ast">
+      <![CDATA[
+LogicalAggregate(group=[{0, 1}], EXPR$2=[SUM($2)], EXPR$3=[COUNT()])
++- LogicalProject(cat=[$2], gmt_date=[$3], cnt=[$4])
+   +- LogicalFilter(condition=[=($3, CURRENT_DATE)])
+      +- LogicalTableScan(table=[[default_catalog, default_database, src]])
+]]>
+    </Resource>
+    <Resource name="optimized exec plan">
+      <![CDATA[
+GroupAggregate(groupBy=[cat, gmt_date], select=[cat, gmt_date, SUM(cnt) AS EXPR$2, COUNT(*) AS EXPR$3])
++- Exchange(distribution=[hash[cat, gmt_date]])
+   +- Calc(select=[cat, gmt_date, cnt], where=[(gmt_date = CURRENT_DATE())])
+      +- TableSourceScan(table=[[default_catalog, default_database, src, filter=[], project=[cat, gmt_date, cnt], metadata=[]]], fields=[cat, gmt_date, cnt])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testCalcMerge">
+    <Resource name="sql">
+      <![CDATA[SELECT * FROM ( 
+   SELECT *, SUBSTR(CAST(LOCALTIME AS VARCHAR), 1, 2) hh
+   FROM src
+ ) t1 WHERE hh > 12 AND cat LIKE 'fruit%'
+]]>
+    </Resource>
+    <Resource name="ast">
+      <![CDATA[
+LogicalProject(a=[$0], b=[$1], cat=[$2], gmt_date=[$3], cnt=[$4], ts=[$5], hh=[$6])
++- LogicalFilter(condition=[AND(>(CAST($6):BIGINT, 12), LIKE($2, _UTF-16LE'fruit%'))])
+   +- LogicalProject(a=[$0], b=[$1], cat=[$2], gmt_date=[$3], cnt=[$4], ts=[$5], hh=[SUBSTR(CAST(LOCALTIME):VARCHAR(2147483647) CHARACTER SET "UTF-16LE" NOT NULL, 1, 2)])
+      +- LogicalTableScan(table=[[default_catalog, default_database, src]])
+]]>
+    </Resource>
+    <Resource name="optimized exec plan">
+      <![CDATA[
+Calc(select=[a, b, cat, gmt_date, cnt, ts, hh], where=[((CAST(hh AS BIGINT) > 12) AND LIKE(cat, 'fruit%'))])
++- Calc(select=[a, b, cat, gmt_date, cnt, ts, SUBSTR(CAST(LOCALTIME() AS VARCHAR(2147483647)), 1, 2) AS hh])
+   +- TableSourceScan(table=[[default_catalog, default_database, src]], fields=[a, b, cat, gmt_date, cnt, ts])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testAggregateReduceConstants2">
+    <Resource name="sql">
+      <![CDATA[SELECT
+cat, hh, SUM(cnt), COUNT(*)
+FROM (SELECT *, SUBSTR(CAST(LOCALTIME AS VARCHAR), 1, 2) hh FROM src)
+WHERE SUBSTR(CAST(ts AS VARCHAR), 1, 2) = hh
+GROUP BY cat, hh]]>
+    </Resource>
+    <Resource name="ast">
+      <![CDATA[
+LogicalAggregate(group=[{0, 1}], EXPR$2=[SUM($2)], EXPR$3=[COUNT()])
++- LogicalProject(cat=[$2], hh=[$6], cnt=[$4])
+   +- LogicalFilter(condition=[=(SUBSTR(CAST($5):VARCHAR(2147483647) CHARACTER SET "UTF-16LE", 1, 2), $6)])
+      +- LogicalProject(a=[$0], b=[$1], cat=[$2], gmt_date=[$3], cnt=[$4], ts=[$5], hh=[SUBSTR(CAST(LOCALTIME):VARCHAR(2147483647) CHARACTER SET "UTF-16LE" NOT NULL, 1, 2)])
+         +- LogicalTableScan(table=[[default_catalog, default_database, src]])
+]]>
+    </Resource>
+    <Resource name="optimized exec plan">
+      <![CDATA[
+GroupAggregate(groupBy=[cat, hh], select=[cat, hh, SUM(cnt) AS EXPR$2, COUNT(*) AS EXPR$3])
++- Exchange(distribution=[hash[cat, hh]])
+   +- Calc(select=[cat, hh, cnt], where=[(SUBSTR(CAST(ts AS VARCHAR(2147483647)), 1, 2) = hh)])
+      +- Calc(select=[cat, cnt, ts, SUBSTR(CAST(LOCALTIME() AS VARCHAR(2147483647)), 1, 2) AS hh])
+         +- TableSourceScan(table=[[default_catalog, default_database, src, project=[cat, cnt, ts], metadata=[]]], fields=[cat, cnt, ts])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testAggregateReduceConstants3">
+    <Resource name="sql">
+      <![CDATA[SELECT
+     gmt_date, ts, cat, SUBSTR(CAST(ts AS VARCHAR), 1, 2), SUM(cnt)
+FROM src
+WHERE gmt_date = CURRENT_DATE
+  AND cat = 'fruit' AND ts = CURRENT_TIME
+GROUP BY gmt_date, ts, cat]]>
+    </Resource>
+    <Resource name="ast">
+      <![CDATA[
+LogicalProject(gmt_date=[$0], ts=[$1], cat=[$2], EXPR$3=[SUBSTR(CAST($1):VARCHAR(2147483647) CHARACTER SET "UTF-16LE", 1, 2)], EXPR$4=[$3])
++- LogicalAggregate(group=[{0, 1, 2}], EXPR$4=[SUM($3)])
+   +- LogicalProject(gmt_date=[$3], ts=[$5], cat=[$2], cnt=[$4])
+      +- LogicalFilter(condition=[AND(=($3, CURRENT_DATE), =($2, _UTF-16LE'fruit'), =($5, CURRENT_TIME))])
+         +- LogicalTableScan(table=[[default_catalog, default_database, src]])
+]]>
+    </Resource>
+    <Resource name="optimized exec plan">
+      <![CDATA[
+Calc(select=[gmt_date, ts, 'fruit' AS cat, SUBSTR(CAST(ts AS VARCHAR(2147483647)), 1, 2) AS EXPR$3, EXPR$4])
++- GroupAggregate(groupBy=[gmt_date, ts], select=[gmt_date, ts, SUM(cnt) AS EXPR$4])
+   +- Exchange(distribution=[hash[gmt_date, ts]])
+      +- Calc(select=[gmt_date, ts, cnt], where=[((gmt_date = CURRENT_DATE()) AND (cat = 'fruit') AND (ts = CURRENT_TIME()))])
+         +- TableSourceScan(table=[[default_catalog, default_database, src, filter=[], project=[cat, gmt_date, cnt, ts], metadata=[]]], fields=[cat, gmt_date, cnt, ts])
+]]>
+    </Resource>
+  </TestCase>
+</Root>
