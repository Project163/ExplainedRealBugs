diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/InputProperty.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/InputProperty.java
index ea764cf3392..d4ecbcf2df1 100644
--- a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/InputProperty.java
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/InputProperty.java
@@ -269,19 +269,30 @@ public class InputProperty {
         }
     }
 
-    /** A special distribution which indicators the data distribution is the same as its input. */
+    /**
+     * A special distribution which indicators the data distribution is the same as its input. '
+     *
+     * <p>TODO This class can be removed once FLINK-21224 is finished.
+     */
     public static class KeepInputAsIsDistribution extends RequiredDistribution {
         private final RequiredDistribution inputDistribution;
+        /** whether the input distribution is strictly guaranteed. */
+        private final boolean strict;
 
-        private KeepInputAsIsDistribution(RequiredDistribution inputDistribution) {
+        private KeepInputAsIsDistribution(RequiredDistribution inputDistribution, boolean strict) {
             super(DistributionType.KEEP_INPUT_AS_IS);
             this.inputDistribution = checkNotNull(inputDistribution);
+            this.strict = strict;
         }
 
         public RequiredDistribution getInputDistribution() {
             return inputDistribution;
         }
 
+        public boolean isStrict() {
+            return strict;
+        }
+
         @Override
         public boolean equals(Object o) {
             if (this == o) {
@@ -294,17 +305,21 @@ public class InputProperty {
                 return false;
             }
             KeepInputAsIsDistribution that = (KeepInputAsIsDistribution) o;
-            return inputDistribution.equals(that.inputDistribution);
+            return strict == that.strict && inputDistribution.equals(that.inputDistribution);
         }
 
         @Override
         public int hashCode() {
-            return Objects.hash(super.hashCode(), inputDistribution);
+            return Objects.hash(super.hashCode(), strict, inputDistribution);
         }
 
         @Override
         public String toString() {
-            return "KEEP_INPUT_AS_IS(" + inputDistribution + ")";
+            if (strict) {
+                return "KEEP_INPUT_AS_IS(strict, " + inputDistribution + ")";
+            } else {
+                return "KEEP_INPUT_AS_IS(" + inputDistribution + ")";
+            }
         }
     }
 
@@ -321,10 +336,11 @@ public class InputProperty {
      * A special distribution which indicators the data distribution is the same as its input.
      *
      * @param inputDistribution the input distribution
+     * @param strict whether the input distribution is strictly guaranteed
      */
     public static KeepInputAsIsDistribution keepInputAsIsDistribution(
-            RequiredDistribution inputDistribution) {
-        return new KeepInputAsIsDistribution(inputDistribution);
+            RequiredDistribution inputDistribution, boolean strict) {
+        return new KeepInputAsIsDistribution(inputDistribution, strict);
     }
 
     /** Enumeration which describes the type of the input data distribution. */
diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/BatchExecExchange.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/BatchExecExchange.java
index f2cd218059b..69df8c462b6 100644
--- a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/BatchExecExchange.java
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/BatchExecExchange.java
@@ -25,6 +25,7 @@ import org.apache.flink.streaming.api.transformations.PartitionTransformation;
 import org.apache.flink.streaming.api.transformations.StreamExchangeMode;
 import org.apache.flink.streaming.runtime.partitioner.BroadcastPartitioner;
 import org.apache.flink.streaming.runtime.partitioner.ForwardForConsecutiveHashPartitioner;
+import org.apache.flink.streaming.runtime.partitioner.ForwardPartitioner;
 import org.apache.flink.streaming.runtime.partitioner.GlobalPartitioner;
 import org.apache.flink.streaming.runtime.partitioner.StreamPartitioner;
 import org.apache.flink.table.api.TableException;
@@ -49,6 +50,7 @@ import javax.annotation.Nullable;
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.Optional;
+import java.util.stream.Collectors;
 
 import static org.apache.flink.table.planner.utils.StreamExchangeModeUtils.getBatchStreamExchangeMode;
 import static org.apache.flink.util.Preconditions.checkArgument;
@@ -85,22 +87,37 @@ public class BatchExecExchange extends CommonExecExchange implements BatchExecNo
         String type = requiredDistribution.getType().name().toLowerCase();
         if (type.equals("singleton")) {
             type = "single";
+        } else if (requiredDistribution instanceof KeepInputAsIsDistribution
+                && ((KeepInputAsIsDistribution) requiredDistribution).isStrict()) {
+            type = "forward";
         }
         sb.append("distribution=[").append(type);
-        if (requiredDistribution.getType() == InputProperty.DistributionType.HASH) {
-            RowType inputRowType = (RowType) getInputEdges().get(0).getOutputType();
-            HashDistribution hashDistribution = (HashDistribution) requiredDistribution;
-            String[] fieldNames =
-                    Arrays.stream(hashDistribution.getKeys())
-                            .mapToObj(i -> inputRowType.getFieldNames().get(i))
-                            .toArray(String[]::new);
-            sb.append("[").append(String.join(", ", fieldNames)).append("]");
+        if (requiredDistribution instanceof HashDistribution) {
+            sb.append(getHashDistributionDescription((HashDistribution) requiredDistribution));
+        } else if (requiredDistribution instanceof KeepInputAsIsDistribution
+                && !((KeepInputAsIsDistribution) requiredDistribution).isStrict()) {
+            KeepInputAsIsDistribution distribution =
+                    (KeepInputAsIsDistribution) requiredDistribution;
+            sb.append("[hash")
+                    .append(
+                            getHashDistributionDescription(
+                                    (HashDistribution) distribution.getInputDistribution()))
+                    .append("]");
         }
         sb.append("]");
         if (requiredExchangeMode == StreamExchangeMode.BATCH) {
             sb.append(", shuffle_mode=[BATCH]");
         }
-        return String.format("Exchange(%s)", sb.toString());
+        return String.format("Exchange(%s)", sb);
+    }
+
+    private String getHashDistributionDescription(HashDistribution hashDistribution) {
+        RowType inputRowType = (RowType) getInputEdges().get(0).getOutputType();
+        String[] fieldNames =
+                Arrays.stream(hashDistribution.getKeys())
+                        .mapToObj(i -> inputRowType.getFieldNames().get(i))
+                        .toArray(String[]::new);
+        return Arrays.stream(fieldNames).collect(Collectors.joining(", ", "[", "]"));
     }
 
     @SuppressWarnings("unchecked")
@@ -111,6 +128,7 @@ public class BatchExecExchange extends CommonExecExchange implements BatchExecNo
                 (Transformation<RowData>) inputEdge.translateToPlan(planner);
         final RowType inputType = (RowType) inputEdge.getOutputType();
 
+        boolean requireUndefinedExchangeMode = false;
         final StreamPartitioner<RowData> partitioner;
         final int parallelism;
         final InputProperty inputProperty = getInputProperties().get(0);
@@ -136,17 +154,27 @@ public class BatchExecExchange extends CommonExecExchange implements BatchExecNo
                 parallelism = ExecutionConfig.PARALLELISM_DEFAULT;
                 break;
             case KEEP_INPUT_AS_IS:
-                RequiredDistribution inputDistribution =
-                        ((KeepInputAsIsDistribution) requiredDistribution).getInputDistribution();
-                checkArgument(
-                        inputDistribution instanceof HashDistribution,
-                        "Only HashDistribution is supported now");
-                partitioner =
-                        new ForwardForConsecutiveHashPartitioner<>(
-                                createHashPartitioner(
-                                        ((HashDistribution) inputDistribution),
-                                        inputType,
-                                        planner));
+                KeepInputAsIsDistribution keepInputAsIsDistribution =
+                        (KeepInputAsIsDistribution) requiredDistribution;
+                if (keepInputAsIsDistribution.isStrict()) {
+                    // explicitly use ForwardPartitioner to guarantee the data distribution is
+                    // exactly the same as input
+                    partitioner = new ForwardPartitioner<>();
+                    requireUndefinedExchangeMode = true;
+                } else {
+                    RequiredDistribution inputDistribution =
+                            ((KeepInputAsIsDistribution) requiredDistribution)
+                                    .getInputDistribution();
+                    checkArgument(
+                            inputDistribution instanceof HashDistribution,
+                            "Only HashDistribution is supported now");
+                    partitioner =
+                            new ForwardForConsecutiveHashPartitioner<>(
+                                    createHashPartitioner(
+                                            ((HashDistribution) inputDistribution),
+                                            inputType,
+                                            planner));
+                }
                 parallelism = inputTransform.getParallelism();
                 break;
             default:
@@ -154,7 +182,10 @@ public class BatchExecExchange extends CommonExecExchange implements BatchExecNo
         }
 
         final StreamExchangeMode exchangeMode =
-                getBatchStreamExchangeMode(planner.getConfiguration(), requiredExchangeMode);
+                requireUndefinedExchangeMode
+                        ? StreamExchangeMode.UNDEFINED
+                        : getBatchStreamExchangeMode(
+                                planner.getConfiguration(), requiredExchangeMode);
         final Transformation<RowData> transformation =
                 new PartitionTransformation<>(inputTransform, partitioner, exchangeMode);
         transformation.setParallelism(parallelism);
diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/BatchExecOverAggregateBase.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/BatchExecOverAggregateBase.java
index a6919d40a18..3121d9d2c21 100644
--- a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/BatchExecOverAggregateBase.java
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/BatchExecOverAggregateBase.java
@@ -40,7 +40,7 @@ import java.util.List;
 
 /** Batch {@link ExecNode} base class for sort-based over window aggregate. */
 public abstract class BatchExecOverAggregateBase extends ExecNodeBase<RowData>
-        implements BatchExecNode<RowData>, SingleTransformationTranslator<RowData> {
+        implements InputSortedExecNode<RowData>, SingleTransformationTranslator<RowData> {
 
     protected final OverSpec overSpec;
 
diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/BatchExecRank.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/BatchExecRank.java
index 23100d45201..dc1012c3d79 100644
--- a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/BatchExecRank.java
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/BatchExecRank.java
@@ -41,7 +41,7 @@ import java.util.Collections;
  *
  * <p>This node supports two-stage(local and global) rank to reduce data-shuffling.
  */
-public class BatchExecRank extends ExecNodeBase<RowData> implements BatchExecNode<RowData> {
+public class BatchExecRank extends ExecNodeBase<RowData> implements InputSortedExecNode<RowData> {
 
     private final int[] partitionFields;
     private final int[] sortFields;
diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/BatchExecSortAggregate.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/BatchExecSortAggregate.java
index 9ba36fb12a5..3e077ac9cab 100644
--- a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/BatchExecSortAggregate.java
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/BatchExecSortAggregate.java
@@ -47,7 +47,7 @@ import java.util.Collections;
 
 /** Batch {@link ExecNode} for (global) sort-based aggregate operator. */
 public class BatchExecSortAggregate extends ExecNodeBase<RowData>
-        implements BatchExecNode<RowData>, SingleTransformationTranslator<RowData> {
+        implements InputSortedExecNode<RowData>, SingleTransformationTranslator<RowData> {
 
     private final int[] grouping;
     private final int[] auxGrouping;
diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/BatchExecSortWindowAggregate.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/BatchExecSortWindowAggregate.java
index 1e8d29c0fda..34de3ce90db 100644
--- a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/BatchExecSortWindowAggregate.java
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/BatchExecSortWindowAggregate.java
@@ -52,7 +52,7 @@ import java.util.Collections;
 
 /** Batch {@link ExecNode} for sort-based window aggregate operator. */
 public class BatchExecSortWindowAggregate extends ExecNodeBase<RowData>
-        implements BatchExecNode<RowData>, SingleTransformationTranslator<RowData> {
+        implements InputSortedExecNode<RowData>, SingleTransformationTranslator<RowData> {
 
     private final int[] grouping;
     private final int[] auxGrouping;
diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/InputSortedExecNode.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/InputSortedExecNode.java
new file mode 100644
index 00000000000..897c0ae0085
--- /dev/null
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/InputSortedExecNode.java
@@ -0,0 +1,27 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * imitations under the License.
+ */
+
+package org.apache.flink.table.planner.plan.nodes.exec.batch;
+
+import org.apache.flink.annotation.Internal;
+
+/**
+ * A {@link BatchExecNode} which does not sort the input data within the operator, but requires the
+ * input data is already sorted.
+ */
+@Internal
+public interface InputSortedExecNode<T> extends BatchExecNode<T> {}
diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/processor/ForwardHashExchangeProcessor.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/processor/ForwardHashExchangeProcessor.java
index cef2a03748a..eae6279b901 100644
--- a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/processor/ForwardHashExchangeProcessor.java
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/processor/ForwardHashExchangeProcessor.java
@@ -22,8 +22,16 @@ import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;
 import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;
 import org.apache.flink.table.planner.plan.nodes.exec.ExecNodeGraph;
 import org.apache.flink.table.planner.plan.nodes.exec.InputProperty;
+import org.apache.flink.table.planner.plan.nodes.exec.InputProperty.DistributionType;
+import org.apache.flink.table.planner.plan.nodes.exec.InputProperty.RequiredDistribution;
+import org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecCalc;
+import org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecCorrelate;
 import org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecExchange;
 import org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecMultipleInput;
+import org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecPythonCalc;
+import org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecPythonCorrelate;
+import org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSort;
+import org.apache.flink.table.planner.plan.nodes.exec.batch.InputSortedExecNode;
 import org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecExchange;
 import org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecNode;
 import org.apache.flink.table.planner.plan.nodes.exec.visitor.AbstractExecNodeExactlyOnceVisitor;
@@ -73,46 +81,65 @@ public class ForwardHashExchangeProcessor implements ExecNodeGraphProcessor {
                         List<ExecEdge> newEdges = new ArrayList<>(node.getInputEdges());
                         for (int i = 0; i < node.getInputProperties().size(); ++i) {
                             InputProperty inputProperty = node.getInputProperties().get(i);
-                            InputProperty.RequiredDistribution requiredDistribution =
+                            RequiredDistribution requiredDistribution =
                                     inputProperty.getRequiredDistribution();
-                            if (requiredDistribution.getType()
-                                    != InputProperty.DistributionType.HASH) {
+                            ExecEdge edge = node.getInputEdges().get(i);
+
+                            if (requiredDistribution.getType() == DistributionType.SINGLETON) {
+                                if (!hasExchangeInput(edge) && isInputSortedNode(node)) {
+                                    // if operation chaining is disabled, this could mark sure the
+                                    // sort node and its output can also be connected by
+                                    // ForwardPartitioner
+                                    ExecEdge newEdge =
+                                            addExchangeAndReconnectEdge(edge, inputProperty, true);
+                                    newEdges.set(i, newEdge);
+                                    changed = true;
+                                }
                                 continue;
                             }
-                            ExecEdge edge = node.getInputEdges().get(i);
-                            if (!isExchangeInput(edge)) {
-                                InputProperty newInputProperty =
-                                        InputProperty.builder()
-                                                .requiredDistribution(
-                                                        InputProperty.keepInputAsIsDistribution(
-                                                                requiredDistribution))
-                                                .damBehavior(inputProperty.getDamBehavior())
-                                                .priority(inputProperty.getPriority())
-                                                .build();
-                                BatchExecExchange newExchange =
-                                        new BatchExecExchange(
-                                                newInputProperty,
-                                                (RowType) edge.getOutputType(),
-                                                newInputProperty.toString());
-
-                                ExecEdge newEdge1 =
-                                        new ExecEdge(
-                                                edge.getSource(),
-                                                newExchange,
-                                                edge.getShuffle(),
-                                                edge.getExchangeMode());
-                                newExchange.setInputEdges(Collections.singletonList(newEdge1));
-
-                                ExecEdge newEdge2 =
-                                        new ExecEdge(
-                                                newExchange,
-                                                edge.getTarget(),
-                                                edge.getShuffle(),
-                                                edge.getExchangeMode());
 
+                            if (requiredDistribution.getType() != DistributionType.HASH) {
+                                continue;
+                            }
+
+                            if (!hasExchangeInput(edge)) {
+                                ExecEdge newEdge;
+                                if (isInputSortedNode(node)) {
+                                    if (hasSortInputForInputSortedNode(node)) {
+                                        // add Exchange with keep_input_as_is distribution as the
+                                        // input of Sort
+                                        ExecNode<?> sort = edge.getSource();
+                                        ExecEdge newEdgeOfSort =
+                                                addExchangeAndReconnectEdge(
+                                                        sort.getInputEdges().get(0),
+                                                        inputProperty,
+                                                        false);
+                                        sort.setInputEdges(
+                                                Collections.singletonList(newEdgeOfSort));
+                                    }
+
+                                    // if operation chaining is disabled, this could mark sure the
+                                    // sort node and its output can also be connected by
+                                    // ForwardPartitioner
+                                    newEdge =
+                                            addExchangeAndReconnectEdge(edge, inputProperty, true);
+                                } else {
+                                    // add Exchange with keep_input_as_is distribution as the input
+                                    // of the node
+                                    newEdge =
+                                            addExchangeAndReconnectEdge(edge, inputProperty, false);
+                                    updateOriginalEdgeInMultipleInput(
+                                            node, i, (BatchExecExchange) newEdge.getSource());
+                                }
                                 // update the edge
-                                newEdges.set(i, newEdge2);
-                                updateOriginalEdgeInMultipleInput(node, i, newExchange);
+                                newEdges.set(i, newEdge);
+                                changed = true;
+                            } else if (hasSortInputForInputSortedNode(node)) {
+                                // if operation chaining is disabled, this could mark sure the sort
+                                // node and its output can also be connected by ForwardPartitioner
+                                ExecEdge newEdge =
+                                        addExchangeAndReconnectEdge(edge, inputProperty, true);
+                                newEdges.set(i, newEdge);
                                 changed = true;
                             }
                         }
@@ -125,8 +152,64 @@ public class ForwardHashExchangeProcessor implements ExecNodeGraphProcessor {
         return execGraph;
     }
 
-    private boolean isExchangeInput(ExecEdge edge) {
-        return edge.getSource() instanceof CommonExecExchange;
+    // TODO This implementation should be updated once FLINK-21224 is finished.
+    private ExecEdge addExchangeAndReconnectEdge(
+            ExecEdge edge, InputProperty inputProperty, boolean strict) {
+        ExecNode<?> target = edge.getTarget();
+        ExecNode<?> source = edge.getSource();
+        if (source instanceof CommonExecExchange) {
+            return edge;
+        }
+        // only Calc, Correlate and Sort can propagate sort property and distribution property
+        if (source instanceof BatchExecCalc
+                || source instanceof BatchExecPythonCalc
+                || source instanceof BatchExecSort
+                || source instanceof BatchExecCorrelate
+                || source instanceof BatchExecPythonCorrelate) {
+            ExecEdge newEdge =
+                    addExchangeAndReconnectEdge(
+                            source.getInputEdges().get(0), inputProperty, strict);
+            source.setInputEdges(Collections.singletonList(newEdge));
+        }
+
+        BatchExecExchange exchange =
+                createExchangeWithKeepInputAsIsDistribution(
+                        inputProperty, strict, (RowType) edge.getOutputType());
+        ExecEdge newEdge =
+                new ExecEdge(source, exchange, edge.getShuffle(), edge.getExchangeMode());
+        exchange.setInputEdges(Collections.singletonList(newEdge));
+        return new ExecEdge(exchange, target, edge.getShuffle(), edge.getExchangeMode());
+    }
+
+    private BatchExecExchange createExchangeWithKeepInputAsIsDistribution(
+            InputProperty inputProperty, boolean strict, RowType outputRowType) {
+        InputProperty newInputProperty =
+                InputProperty.builder()
+                        .requiredDistribution(
+                                InputProperty.keepInputAsIsDistribution(
+                                        inputProperty.getRequiredDistribution(), strict))
+                        .damBehavior(inputProperty.getDamBehavior())
+                        .priority(inputProperty.getPriority())
+                        .build();
+        return new BatchExecExchange(newInputProperty, outputRowType, newInputProperty.toString());
+    }
+
+    private boolean hasExchangeInput(ExecEdge edge) {
+        ExecNode<?> input = edge.getSource();
+        if (hasSortInputForInputSortedNode(edge.getTarget())) {
+            // skip Sort node
+            input = input.getInputEdges().get(0).getSource();
+        }
+        return input instanceof CommonExecExchange;
+    }
+
+    private boolean hasSortInputForInputSortedNode(ExecNode<?> node) {
+        return isInputSortedNode(node)
+                && node.getInputEdges().get(0).getSource() instanceof BatchExecSort;
+    }
+
+    private boolean isInputSortedNode(ExecNode<?> node) {
+        return node instanceof InputSortedExecNode;
     }
 
     private void updateOriginalEdgeInMultipleInput(
diff --git a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/batch/sql/ForwardHashExchangeTest.java b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/batch/sql/ForwardHashExchangeTest.java
index 4cc22bf07eb..3297879b04e 100644
--- a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/batch/sql/ForwardHashExchangeTest.java
+++ b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/batch/sql/ForwardHashExchangeTest.java
@@ -72,7 +72,36 @@ public class ForwardHashExchangeTest extends TableTestBase {
     }
 
     @Test
-    public void testOverAgg() {
+    public void testOverAggOnHashAggWithHashShuffle() {
+        util.tableEnv()
+                .getConfig()
+                .getConfiguration()
+                .setString(ExecutionConfigOptions.TABLE_EXEC_DISABLED_OPERATORS, "SortAgg");
+        util.verifyExecPlan(
+                " SELECT\n"
+                        + "   SUM(b) sum_b,\n"
+                        + "   AVG(SUM(b)) OVER (PARTITION BY c) avg_b,\n"
+                        + "   RANK() OVER (PARTITION BY c ORDER BY c) rn,\n"
+                        + "   c\n"
+                        + " FROM T\n"
+                        + " GROUP BY c");
+    }
+
+    @Test
+    public void testOverAggOnHashAggWithGlobalShuffle() {
+        util.tableEnv()
+                .getConfig()
+                .getConfiguration()
+                .setString(ExecutionConfigOptions.TABLE_EXEC_DISABLED_OPERATORS, "SortAgg");
+        util.verifyExecPlan("SELECT b, RANK() OVER (ORDER BY b) FROM (SELECT SUM(b) AS b FROM T)");
+    }
+
+    @Test
+    public void testOverAggOnSortAggWithHashShuffle() {
+        util.tableEnv()
+                .getConfig()
+                .getConfiguration()
+                .setString(ExecutionConfigOptions.TABLE_EXEC_DISABLED_OPERATORS, "HashAgg");
         util.verifyExecPlan(
                 " SELECT\n"
                         + "   SUM(b) sum_b,\n"
@@ -84,7 +113,16 @@ public class ForwardHashExchangeTest extends TableTestBase {
     }
 
     @Test
-    public void testHashAgg() {
+    public void testOverAggOnSortAggWithGlobalShuffle() {
+        util.tableEnv()
+                .getConfig()
+                .getConfiguration()
+                .setString(ExecutionConfigOptions.TABLE_EXEC_DISABLED_OPERATORS, "HashAgg");
+        util.verifyExecPlan("SELECT b, RANK() OVER (ORDER BY b) FROM (SELECT SUM(b) AS b FROM T)");
+    }
+
+    @Test
+    public void testHashAggOnHashJoinWithHashShuffle() {
         util.tableEnv()
                 .getConfig()
                 .getConfiguration()
@@ -97,7 +135,7 @@ public class ForwardHashExchangeTest extends TableTestBase {
     }
 
     @Test
-    public void testSortAgg() {
+    public void testSortAggOnSortMergeJoinWithHashShuffle() {
         util.tableEnv()
                 .getConfig()
                 .getConfiguration()
@@ -110,7 +148,39 @@ public class ForwardHashExchangeTest extends TableTestBase {
     }
 
     @Test
-    public void testRank() {
+    public void testHashAggOnNestedLoopJoinWithGlobalShuffle() {
+        util.tableEnv()
+                .getConfig()
+                .getConfiguration()
+                .setString(ExecutionConfigOptions.TABLE_EXEC_DISABLED_OPERATORS, "SortAgg");
+        util.tableEnv()
+                .getConfig()
+                .getConfiguration()
+                .setString(OptimizerConfigOptions.TABLE_OPTIMIZER_AGG_PHASE_STRATEGY, "ONE_PHASE");
+        // TODO the shuffle between join and agg can be removed
+        util.verifyExecPlan(
+                "WITH r AS (SELECT * FROM T1 FULL OUTER JOIN T2 ON true)\n"
+                        + "SELECT sum(b1) FROM r");
+    }
+
+    @Test
+    public void testSortAggOnNestedLoopJoinWithGlobalShuffle() {
+        util.tableEnv()
+                .getConfig()
+                .getConfiguration()
+                .setString(ExecutionConfigOptions.TABLE_EXEC_DISABLED_OPERATORS, "HashAgg");
+        util.tableEnv()
+                .getConfig()
+                .getConfiguration()
+                .setString(OptimizerConfigOptions.TABLE_OPTIMIZER_AGG_PHASE_STRATEGY, "ONE_PHASE");
+        // TODO the shuffle between join and agg can be removed
+        util.verifyExecPlan(
+                "WITH r AS (SELECT * FROM T1 FULL OUTER JOIN T2 ON true)\n"
+                        + "SELECT sum(b1) FROM r");
+    }
+
+    @Test
+    public void testRankOnHashAggWithHashShuffle() {
         util.tableEnv()
                 .getConfig()
                 .getConfiguration()
@@ -123,6 +193,48 @@ public class ForwardHashExchangeTest extends TableTestBase {
                         + "        ) WHERE rk <= 10");
     }
 
+    @Test
+    public void testRankOnHashAggWithGlobalShuffle() {
+        util.tableEnv()
+                .getConfig()
+                .getConfiguration()
+                .setString(ExecutionConfigOptions.TABLE_EXEC_DISABLED_OPERATORS, "SortAgg");
+        util.verifyExecPlan(
+                "SELECT * FROM (\n"
+                        + "                SELECT b, RANK() OVER(ORDER BY b) rk FROM (\n"
+                        + "                        SELECT SUM(b) AS b FROM T\n"
+                        + "                )\n"
+                        + "        ) WHERE rk <= 10");
+    }
+
+    @Test
+    public void testRankOnSortAggWithHashShuffle() {
+        util.tableEnv()
+                .getConfig()
+                .getConfiguration()
+                .setString(ExecutionConfigOptions.TABLE_EXEC_DISABLED_OPERATORS, "SortAgg");
+        util.verifyExecPlan(
+                "SELECT * FROM (\n"
+                        + "                SELECT a, b, RANK() OVER(PARTITION BY a ORDER BY b) rk FROM (\n"
+                        + "                        SELECT a, SUM(b) AS b FROM T GROUP BY a\n"
+                        + "                )\n"
+                        + "        ) WHERE rk <= 10");
+    }
+
+    @Test
+    public void testRankOnSortAggWithGlobalShuffle() {
+        util.tableEnv()
+                .getConfig()
+                .getConfiguration()
+                .setString(ExecutionConfigOptions.TABLE_EXEC_DISABLED_OPERATORS, "SortAgg");
+        util.verifyExecPlan(
+                "SELECT * FROM (\n"
+                        + "                SELECT b, RANK() OVER(ORDER BY b) rk FROM (\n"
+                        + "                        SELECT SUM(b) AS b FROM T\n"
+                        + "                )\n"
+                        + "        ) WHERE rk <= 10");
+    }
+
     @Test
     public void testHashJoinWithMultipleInputDisabled() {
         util.tableEnv()
diff --git a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/runtime/batch/sql/ForwardHashExchangeITCase.java b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/runtime/batch/sql/ForwardHashExchangeITCase.java
index 0fdba4c7d42..cda6090f054 100644
--- a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/runtime/batch/sql/ForwardHashExchangeITCase.java
+++ b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/runtime/batch/sql/ForwardHashExchangeITCase.java
@@ -19,6 +19,7 @@ package org.apache.flink.table.planner.runtime.batch.sql;
 
 import org.apache.flink.api.common.BatchShuffleMode;
 import org.apache.flink.configuration.ExecutionOptions;
+import org.apache.flink.table.api.config.ExecutionConfigOptions;
 import org.apache.flink.table.planner.factories.TestValuesTableFactory;
 import org.apache.flink.table.planner.runtime.utils.BatchTestBase;
 import org.apache.flink.table.planner.runtime.utils.TestData;
@@ -59,7 +60,34 @@ public class ForwardHashExchangeITCase extends BatchTestBase {
     }
 
     @Test
-    public void testAgg() {
+    public void testOverAggWithHashAgg() {
+        tEnv().getConfig()
+                .getConfiguration()
+                .setString(ExecutionConfigOptions.TABLE_EXEC_DISABLED_OPERATORS, "SortAgg");
+        checkResult(
+                "SELECT\n"
+                        + "   b,\n"
+                        + "   SUM(a) sum_a,\n"
+                        + "   AVG(SUM(a)) OVER (PARTITION BY b) avg_a,\n"
+                        + "   RANK() OVER (PARTITION BY b ORDER BY b) rn\n"
+                        + " FROM MyTable\n"
+                        + " GROUP BY b",
+                JavaScalaConversionUtil.toScala(
+                        Arrays.asList(
+                                Row.of(1, 1, 1, 1),
+                                Row.of(2, 5, 5, 1),
+                                Row.of(3, 15, 15, 1),
+                                Row.of(4, 34, 34, 1),
+                                Row.of(5, 65, 65, 1),
+                                Row.of(6, 111, 111, 1))),
+                false);
+    }
+
+    @Test
+    public void testOverAggWithSortAgg() {
+        tEnv().getConfig()
+                .getConfiguration()
+                .setString(ExecutionConfigOptions.TABLE_EXEC_DISABLED_OPERATORS, "HashAgg");
         checkResult(
                 "SELECT\n"
                         + "   b,\n"
diff --git a/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/ForwardHashExchangeTest.xml b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/ForwardHashExchangeTest.xml
index 7a1aa11f9cd..960e1165e48 100644
--- a/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/ForwardHashExchangeTest.xml
+++ b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/ForwardHashExchangeTest.xml
@@ -16,7 +16,7 @@ See the License for the specific language governing permissions and
 limitations under the License.
 -->
 <Root>
-  <TestCase name="testHashAgg">
+  <TestCase name="testHashAggOnHashJoinWithHashShuffle">
     <Resource name="sql">
       <![CDATA[WITH r AS (SELECT * FROM T1, T2 WHERE a1 = a2 AND c1 LIKE 'He%')
 SELECT sum(b1) FROM r group by a1]]>
@@ -36,14 +36,42 @@ LogicalProject(EXPR$0=[$1])
       <![CDATA[
 Calc(select=[EXPR$0])
 +- HashAggregate(isMerge=[false], groupBy=[a1], select=[a1, SUM(b1) AS EXPR$0])
-   +- Exchange(distribution=[keep_input_as_is])
+   +- Exchange(distribution=[keep_input_as_is[hash[a1]]])
       +- Calc(select=[a1, b1])
-         +- HashJoin(joinType=[InnerJoin], where=[(a1 = a2)], select=[a1, b1, a2], build=[left])
-            :- Exchange(distribution=[hash[a1]])
-            :  +- Calc(select=[a1, b1], where=[LIKE(c1, 'He%')])
-            :     +- TableSourceScan(table=[[default_catalog, default_database, T1, filter=[], project=[a1, b1, c1], metadata=[]]], fields=[a1, b1, c1])
-            +- Exchange(distribution=[hash[a2]])
-               +- TableSourceScan(table=[[default_catalog, default_database, T2, project=[a2], metadata=[]]], fields=[a2])
+         +- Exchange(distribution=[keep_input_as_is[hash[a1]]])
+            +- HashJoin(joinType=[InnerJoin], where=[(a1 = a2)], select=[a1, b1, a2], build=[left])
+               :- Exchange(distribution=[hash[a1]])
+               :  +- Calc(select=[a1, b1], where=[LIKE(c1, 'He%')])
+               :     +- TableSourceScan(table=[[default_catalog, default_database, T1, filter=[], project=[a1, b1, c1], metadata=[]]], fields=[a1, b1, c1])
+               +- Exchange(distribution=[hash[a2]])
+                  +- TableSourceScan(table=[[default_catalog, default_database, T2, project=[a2], metadata=[]]], fields=[a2])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testHashAggOnNestedLoopJoinWithGlobalShuffle">
+    <Resource name="sql">
+      <![CDATA[WITH r AS (SELECT * FROM T1 FULL OUTER JOIN T2 ON true)
+SELECT sum(b1) FROM r]]>
+    </Resource>
+    <Resource name="ast">
+      <![CDATA[
+LogicalAggregate(group=[{}], EXPR$0=[SUM($0)])
++- LogicalProject(b1=[$1])
+   +- LogicalJoin(condition=[true], joinType=[full])
+      :- LogicalTableScan(table=[[default_catalog, default_database, T1]])
+      +- LogicalTableScan(table=[[default_catalog, default_database, T2]])
+]]>
+    </Resource>
+    <Resource name="optimized exec plan">
+      <![CDATA[
+HashAggregate(isMerge=[false], select=[SUM(b1) AS EXPR$0])
++- Exchange(distribution=[single])
+   +- Calc(select=[b1])
+      +- NestedLoopJoin(joinType=[FullOuterJoin], where=[true], select=[b1, a2], build=[left])
+         :- Exchange(distribution=[single])
+         :  +- TableSourceScan(table=[[default_catalog, default_database, T1, project=[b1], metadata=[]]], fields=[b1])
+         +- Exchange(distribution=[single])
+            +- TableSourceScan(table=[[default_catalog, default_database, T2, project=[a2], metadata=[]]], fields=[a2])
 ]]>
     </Resource>
   </TestCase>
@@ -72,19 +100,21 @@ LogicalProject(a=[$0], d2=[$1])
     <Resource name="optimized exec plan">
       <![CDATA[
 HashJoin(joinType=[InnerJoin], where=[(a = d2)], select=[a, d2], build=[right])
-:- Exchange(distribution=[keep_input_as_is])
+:- Exchange(distribution=[keep_input_as_is[hash[a]]])
 :  +- Calc(select=[a])
-:     +- HashJoin(joinType=[InnerJoin], where=[(a = a1)], select=[a1, a], build=[right])
-:        :- Exchange(distribution=[hash[a1]])
-:        :  +- TableSourceScan(table=[[default_catalog, default_database, T1, project=[a1], metadata=[]]], fields=[a1])
-:        +- Exchange(distribution=[hash[a]])(reuse_id=[1])
-:           +- TableSourceScan(table=[[default_catalog, default_database, T, project=[a], metadata=[]]], fields=[a])
-+- Exchange(distribution=[keep_input_as_is])
+:     +- Exchange(distribution=[keep_input_as_is[hash[a1]]])
+:        +- HashJoin(joinType=[InnerJoin], where=[(a = a1)], select=[a1, a], build=[right])
+:           :- Exchange(distribution=[hash[a1]])
+:           :  +- TableSourceScan(table=[[default_catalog, default_database, T1, project=[a1], metadata=[]]], fields=[a1])
+:           +- Exchange(distribution=[hash[a]])(reuse_id=[1])
+:              +- TableSourceScan(table=[[default_catalog, default_database, T, project=[a], metadata=[]]], fields=[a])
++- Exchange(distribution=[keep_input_as_is[hash[d2]]])
    +- Calc(select=[d2])
-      +- HashJoin(joinType=[InnerJoin], where=[(d2 = a)], select=[a, d2], build=[right])
-         :- Reused(reference_id=[1])
-         +- Exchange(distribution=[hash[d2]])
-            +- TableSourceScan(table=[[default_catalog, default_database, T2, project=[d2], metadata=[]]], fields=[d2])
+      +- Exchange(distribution=[keep_input_as_is[hash[a]]])
+         +- HashJoin(joinType=[InnerJoin], where=[(d2 = a)], select=[a, d2], build=[right])
+            :- Reused(reference_id=[1])
+            +- Exchange(distribution=[hash[d2]])
+               +- TableSourceScan(table=[[default_catalog, default_database, T2, project=[d2], metadata=[]]], fields=[d2])
 ]]>
     </Resource>
   </TestCase>
@@ -158,7 +188,7 @@ MultipleInput(readOrder=[1,1,0,0], members=[\nUnion(all=[true], union=[b, sd, sy
 :        +- MultipleInput(readOrder=[0,1,0], members=[\nHashJoin(joinType=[InnerJoin], where=[(a = d)], select=[a, ny, nz, b, d], build=[right])\n:- Calc(select=[a, ny, nz, b])\n:  +- HashJoin(joinType=[LeftOuterJoin], where=[(a = a0)], select=[a, ny, nz, a0, b], build=[right])\n:     :- [#2] MultipleInput(readOrder=[0,1,0], members=[\nHashJoin(joinType=[LeftOuterJoin], where=[(a = nz)], select=[a, ny, nz], build=[right])\n:- HashJoin(joinType=[LeftOuterJoin], where=[(a = ny)], select=[a, ny], build=[right])\n:  :- [#2] Exchange(distribution=[hash[a]])\n:  +- [#3] Exchange(distribution=[hash[ny]])\n+- [#1] Exchange(distribution=[hash[nz]])\n])\n:     +- [#3] Exchange(distribution=[hash[a]])\n+- [#1] Exchange(distribution=[hash[d]])\n])
 :           :- Exchange(distribution=[hash[d]])(reuse_id=[3])
 :           :  +- TableSourceScan(table=[[default_catalog, default_database, y, project=[d], metadata=[]]], fields=[d])
-:           :- Exchange(distribution=[keep_input_as_is])
+:           :- Exchange(distribution=[keep_input_as_is[hash[a]]])
 :           :  +- MultipleInput(readOrder=[0,1,0], members=[\nHashJoin(joinType=[LeftOuterJoin], where=[(a = nz)], select=[a, ny, nz], build=[right])\n:- HashJoin(joinType=[LeftOuterJoin], where=[(a = ny)], select=[a, ny], build=[right])\n:  :- [#2] Exchange(distribution=[hash[a]])\n:  +- [#3] Exchange(distribution=[hash[ny]])\n+- [#1] Exchange(distribution=[hash[nz]])\n])(reuse_id=[2])
 :           :     :- Exchange(distribution=[hash[nz]])
 :           :     :  +- TableSourceScan(table=[[default_catalog, default_database, z, project=[nz], metadata=[]]], fields=[nz])
@@ -173,13 +203,37 @@ MultipleInput(readOrder=[1,1,0,0], members=[\nUnion(all=[true], union=[b, sd, sy
       +- Calc(select=[b, d, ny, nz])
          +- MultipleInput(readOrder=[0,1,0], members=[\nHashJoin(joinType=[InnerJoin], where=[(a = a0)], select=[a, ny, nz, d, a0, b], build=[right])\n:- HashJoin(joinType=[LeftOuterJoin], where=[(a = d)], select=[a, ny, nz, d], build=[right])\n:  :- [#2] MultipleInput(readOrder=[0,1,0], members=[\nHashJoin(joinType=[LeftOuterJoin], where=[(a = nz)], select=[a, ny, nz], build=[right])\n:- HashJoin(joinType=[LeftOuterJoin], where=[(a = ny)], select=[a, ny], build=[right])\n:  :- [#2] Exchange(distribution=[hash[a]])\n:  +- [#3] Exchange(distribution=[hash[ny]])\n+- [#1] Exchange(distribution=[hash[nz]])\n])\n:  +- [#3] Exchange(distribution=[hash[d]])\n+- [#1] Exchange(distribution=[hash[a]])\n])
             :- Reused(reference_id=[1])
-            :- Exchange(distribution=[keep_input_as_is])
+            :- Exchange(distribution=[keep_input_as_is[hash[a]]])
             :  +- Reused(reference_id=[2])
             +- Reused(reference_id=[3])
 ]]>
     </Resource>
   </TestCase>
-  <TestCase name="testOverAgg">
+  <TestCase name="testOverAggOnHashAggWithGlobalShuffle">
+    <Resource name="sql">
+      <![CDATA[SELECT b, RANK() OVER (ORDER BY b) FROM (SELECT SUM(b) AS b FROM T)]]>
+    </Resource>
+    <Resource name="ast">
+      <![CDATA[
+LogicalProject(b=[$0], EXPR$1=[RANK() OVER (ORDER BY $0 NULLS FIRST)])
++- LogicalAggregate(group=[{}], b=[SUM($0)])
+   +- LogicalProject(b=[$1])
+      +- LogicalTableScan(table=[[default_catalog, default_database, T]])
+]]>
+    </Resource>
+    <Resource name="optimized exec plan">
+      <![CDATA[
+OverAggregate(orderBy=[b ASC], window#0=[RANK(*) AS w0$o0 RANG BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW], select=[b, w0$o0])
++- Exchange(distribution=[forward])
+   +- Sort(orderBy=[b ASC])
+      +- Exchange(distribution=[forward])
+         +- HashAggregate(isMerge=[true], select=[Final_SUM(sum$0) AS b])
+            +- Exchange(distribution=[single])
+               +- TableSourceScan(table=[[default_catalog, default_database, T, project=[b], metadata=[], aggregates=[grouping=[], aggFunctions=[LongSumAggFunction(b)]]]], fields=[sum$0])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testOverAggOnHashAggWithHashShuffle">
     <Resource name="sql">
       <![CDATA[ SELECT
    SUM(b) sum_b,
@@ -201,15 +255,161 @@ LogicalProject(sum_b=[$1], avg_b=[/(CASE(>(COUNT($1) OVER (PARTITION BY $0), 0),
       <![CDATA[
 Calc(select=[sum_b, (CASE((w0$o0 > 0), w0$o1, null:BIGINT) / w0$o0) AS avg_b, w1$o0 AS rn, c])
 +- OverAggregate(partitionBy=[c], window#0=[COUNT(sum_b) AS w0$o0, $SUM0(sum_b) AS w0$o1 RANG BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING], window#1=[RANK(*) AS w1$o0 RANG BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW], select=[c, sum_b, w0$o0, w0$o1, w1$o0])
-   +- Exchange(distribution=[keep_input_as_is])
+   +- Exchange(distribution=[forward])
       +- Sort(orderBy=[c ASC])
-         +- HashAggregate(isMerge=[true], groupBy=[c], select=[c, Final_SUM(sum$0) AS sum_b])
-            +- Exchange(distribution=[hash[c]])
-               +- TableSourceScan(table=[[default_catalog, default_database, T, project=[c, b], metadata=[], aggregates=[grouping=[c], aggFunctions=[LongSumAggFunction(b)]]]], fields=[c, sum$0])
+         +- Exchange(distribution=[keep_input_as_is[hash[c]]])
+            +- HashAggregate(isMerge=[true], groupBy=[c], select=[c, Final_SUM(sum$0) AS sum_b])
+               +- Exchange(distribution=[hash[c]])
+                  +- TableSourceScan(table=[[default_catalog, default_database, T, project=[c, b], metadata=[], aggregates=[grouping=[c], aggFunctions=[LongSumAggFunction(b)]]]], fields=[c, sum$0])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testOverAggOnSortAggWithGlobalShuffle">
+    <Resource name="sql">
+      <![CDATA[SELECT b, RANK() OVER (ORDER BY b) FROM (SELECT SUM(b) AS b FROM T)]]>
+    </Resource>
+    <Resource name="ast">
+      <![CDATA[
+LogicalProject(b=[$0], EXPR$1=[RANK() OVER (ORDER BY $0 NULLS FIRST)])
++- LogicalAggregate(group=[{}], b=[SUM($0)])
+   +- LogicalProject(b=[$1])
+      +- LogicalTableScan(table=[[default_catalog, default_database, T]])
+]]>
+    </Resource>
+    <Resource name="optimized exec plan">
+      <![CDATA[
+OverAggregate(orderBy=[b ASC], window#0=[RANK(*) AS w0$o0 RANG BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW], select=[b, w0$o0])
++- Exchange(distribution=[forward])
+   +- Sort(orderBy=[b ASC])
+      +- Exchange(distribution=[forward])
+         +- SortAggregate(isMerge=[true], select=[Final_SUM(sum$0) AS b])
+            +- Exchange(distribution=[single])
+               +- TableSourceScan(table=[[default_catalog, default_database, T, project=[b], metadata=[], aggregates=[grouping=[], aggFunctions=[LongSumAggFunction(b)]]]], fields=[sum$0])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testOverAggOnSortAggWithHashShuffle">
+    <Resource name="sql">
+      <![CDATA[ SELECT
+   SUM(b) sum_b,
+   AVG(SUM(b)) OVER (PARTITION BY c) avg_b,
+   RANK() OVER (PARTITION BY c ORDER BY c) rn,
+   c
+ FROM T
+ GROUP BY c]]>
+    </Resource>
+    <Resource name="ast">
+      <![CDATA[
+LogicalProject(sum_b=[$1], avg_b=[/(CASE(>(COUNT($1) OVER (PARTITION BY $0), 0), $SUM0($1) OVER (PARTITION BY $0), null:BIGINT), COUNT($1) OVER (PARTITION BY $0))], rn=[RANK() OVER (PARTITION BY $0 ORDER BY $0 NULLS FIRST)], c=[$0])
++- LogicalAggregate(group=[{0}], sum_b=[SUM($1)])
+   +- LogicalProject(c=[$2], b=[$1])
+      +- LogicalTableScan(table=[[default_catalog, default_database, T]])
+]]>
+    </Resource>
+    <Resource name="optimized exec plan">
+      <![CDATA[
+Calc(select=[sum_b, (CASE((w0$o0 > 0), w0$o1, null:BIGINT) / w0$o0) AS avg_b, w1$o0 AS rn, c])
++- OverAggregate(partitionBy=[c], window#0=[COUNT(sum_b) AS w0$o0, $SUM0(sum_b) AS w0$o1 RANG BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING], window#1=[RANK(*) AS w1$o0 RANG BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW], select=[c, sum_b, w0$o0, w0$o1, w1$o0])
+   +- Exchange(distribution=[forward])
+      +- SortAggregate(isMerge=[true], groupBy=[c], select=[c, Final_SUM(sum$0) AS sum_b])
+         +- Exchange(distribution=[forward])
+            +- Sort(orderBy=[c ASC])
+               +- Exchange(distribution=[hash[c]])
+                  +- TableSourceScan(table=[[default_catalog, default_database, T, project=[c, b], metadata=[], aggregates=[grouping=[c], aggFunctions=[LongSumAggFunction(b)]]]], fields=[c, sum$0])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testRankOnHashAggWithGlobalShuffle">
+    <Resource name="sql">
+      <![CDATA[SELECT * FROM (
+                SELECT b, RANK() OVER(ORDER BY b) rk FROM (
+                        SELECT SUM(b) AS b FROM T
+                )
+        ) WHERE rk <= 10]]>
+    </Resource>
+    <Resource name="ast">
+      <![CDATA[
+LogicalProject(b=[$0], rk=[$1])
++- LogicalFilter(condition=[<=($1, 10)])
+   +- LogicalProject(b=[$0], rk=[RANK() OVER (ORDER BY $0 NULLS FIRST)])
+      +- LogicalAggregate(group=[{}], b=[SUM($0)])
+         +- LogicalProject(b=[$1])
+            +- LogicalTableScan(table=[[default_catalog, default_database, T]])
+]]>
+    </Resource>
+    <Resource name="optimized exec plan">
+      <![CDATA[
+Rank(rankType=[RANK], rankRange=[rankStart=1, rankEnd=10], partitionBy=[], orderBy=[b ASC], global=[true], select=[b, w0$o0])
++- Exchange(distribution=[forward])
+   +- Sort(orderBy=[b ASC])
+      +- Exchange(distribution=[forward])
+         +- HashAggregate(isMerge=[true], select=[Final_SUM(sum$0) AS b])
+            +- Exchange(distribution=[single])
+               +- TableSourceScan(table=[[default_catalog, default_database, T, project=[b], metadata=[], aggregates=[grouping=[], aggFunctions=[LongSumAggFunction(b)]]]], fields=[sum$0])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testRankOnHashAggWithHashShuffle">
+    <Resource name="sql">
+      <![CDATA[SELECT * FROM (
+                SELECT a, b, RANK() OVER(PARTITION BY a ORDER BY b) rk FROM (
+                        SELECT a, SUM(b) AS b FROM T GROUP BY a
+                )
+        ) WHERE rk <= 10]]>
+    </Resource>
+    <Resource name="ast">
+      <![CDATA[
+LogicalProject(a=[$0], b=[$1], rk=[$2])
++- LogicalFilter(condition=[<=($2, 10)])
+   +- LogicalProject(a=[$0], b=[$1], rk=[RANK() OVER (PARTITION BY $0 ORDER BY $1 NULLS FIRST)])
+      +- LogicalAggregate(group=[{0}], b=[SUM($1)])
+         +- LogicalProject(a=[$0], b=[$1])
+            +- LogicalTableScan(table=[[default_catalog, default_database, T]])
+]]>
+    </Resource>
+    <Resource name="optimized exec plan">
+      <![CDATA[
+Rank(rankType=[RANK], rankRange=[rankStart=1, rankEnd=10], partitionBy=[a], orderBy=[b ASC], global=[true], select=[a, b, w0$o0])
++- Exchange(distribution=[forward])
+   +- Sort(orderBy=[a ASC, b ASC])
+      +- Exchange(distribution=[keep_input_as_is[hash[a]]])
+         +- HashAggregate(isMerge=[true], groupBy=[a], select=[a, Final_SUM(sum$0) AS b])
+            +- Exchange(distribution=[hash[a]])
+               +- TableSourceScan(table=[[default_catalog, default_database, T, project=[a, b], metadata=[], aggregates=[grouping=[a], aggFunctions=[LongSumAggFunction(b)]]]], fields=[a, sum$0])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testRankOnSortAggWithGlobalShuffle">
+    <Resource name="sql">
+      <![CDATA[SELECT * FROM (
+                SELECT b, RANK() OVER(ORDER BY b) rk FROM (
+                        SELECT SUM(b) AS b FROM T
+                )
+        ) WHERE rk <= 10]]>
+    </Resource>
+    <Resource name="ast">
+      <![CDATA[
+LogicalProject(b=[$0], rk=[$1])
++- LogicalFilter(condition=[<=($1, 10)])
+   +- LogicalProject(b=[$0], rk=[RANK() OVER (ORDER BY $0 NULLS FIRST)])
+      +- LogicalAggregate(group=[{}], b=[SUM($0)])
+         +- LogicalProject(b=[$1])
+            +- LogicalTableScan(table=[[default_catalog, default_database, T]])
+]]>
+    </Resource>
+    <Resource name="optimized exec plan">
+      <![CDATA[
+Rank(rankType=[RANK], rankRange=[rankStart=1, rankEnd=10], partitionBy=[], orderBy=[b ASC], global=[true], select=[b, w0$o0])
++- Exchange(distribution=[forward])
+   +- Sort(orderBy=[b ASC])
+      +- Exchange(distribution=[forward])
+         +- HashAggregate(isMerge=[true], select=[Final_SUM(sum$0) AS b])
+            +- Exchange(distribution=[single])
+               +- TableSourceScan(table=[[default_catalog, default_database, T, project=[b], metadata=[], aggregates=[grouping=[], aggFunctions=[LongSumAggFunction(b)]]]], fields=[sum$0])
 ]]>
     </Resource>
   </TestCase>
-  <TestCase name="testRank">
+  <TestCase name="testRankOnSortAggWithHashShuffle">
     <Resource name="sql">
       <![CDATA[SELECT * FROM (
                 SELECT a, b, RANK() OVER(PARTITION BY a ORDER BY b) rk FROM (
@@ -230,11 +430,12 @@ LogicalProject(a=[$0], b=[$1], rk=[$2])
     <Resource name="optimized exec plan">
       <![CDATA[
 Rank(rankType=[RANK], rankRange=[rankStart=1, rankEnd=10], partitionBy=[a], orderBy=[b ASC], global=[true], select=[a, b, w0$o0])
-+- Exchange(distribution=[keep_input_as_is])
++- Exchange(distribution=[forward])
    +- Sort(orderBy=[a ASC, b ASC])
-      +- HashAggregate(isMerge=[true], groupBy=[a], select=[a, Final_SUM(sum$0) AS b])
-         +- Exchange(distribution=[hash[a]])
-            +- TableSourceScan(table=[[default_catalog, default_database, T, project=[a, b], metadata=[], aggregates=[grouping=[a], aggFunctions=[LongSumAggFunction(b)]]]], fields=[a, sum$0])
+      +- Exchange(distribution=[keep_input_as_is[hash[a]]])
+         +- HashAggregate(isMerge=[true], groupBy=[a], select=[a, Final_SUM(sum$0) AS b])
+            +- Exchange(distribution=[hash[a]])
+               +- TableSourceScan(table=[[default_catalog, default_database, T, project=[a, b], metadata=[], aggregates=[grouping=[a], aggFunctions=[LongSumAggFunction(b)]]]], fields=[a, sum$0])
 ]]>
     </Resource>
   </TestCase>
@@ -263,23 +464,52 @@ LogicalProject(a=[$0], d2=[$1])
     <Resource name="optimized exec plan">
       <![CDATA[
 SortMergeJoin(joinType=[InnerJoin], where=[(a = d2)], select=[a, d2])
-:- Exchange(distribution=[keep_input_as_is])
+:- Exchange(distribution=[keep_input_as_is[hash[a]]])
 :  +- Calc(select=[a])
-:     +- SortMergeJoin(joinType=[InnerJoin], where=[(a = a1)], select=[a1, a])
-:        :- Exchange(distribution=[hash[a1]])
-:        :  +- TableSourceScan(table=[[default_catalog, default_database, T1, project=[a1], metadata=[]]], fields=[a1])
-:        +- Exchange(distribution=[hash[a]])(reuse_id=[1])
-:           +- TableSourceScan(table=[[default_catalog, default_database, T, project=[a], metadata=[]]], fields=[a])
-+- Exchange(distribution=[keep_input_as_is])
+:     +- Exchange(distribution=[keep_input_as_is[hash[a1]]])
+:        +- SortMergeJoin(joinType=[InnerJoin], where=[(a = a1)], select=[a1, a])
+:           :- Exchange(distribution=[hash[a1]])
+:           :  +- TableSourceScan(table=[[default_catalog, default_database, T1, project=[a1], metadata=[]]], fields=[a1])
+:           +- Exchange(distribution=[hash[a]])(reuse_id=[1])
+:              +- TableSourceScan(table=[[default_catalog, default_database, T, project=[a], metadata=[]]], fields=[a])
++- Exchange(distribution=[keep_input_as_is[hash[d2]]])
    +- Calc(select=[d2])
-      +- SortMergeJoin(joinType=[InnerJoin], where=[(d2 = a)], select=[a, d2])
-         :- Reused(reference_id=[1])
-         +- Exchange(distribution=[hash[d2]])
-            +- TableSourceScan(table=[[default_catalog, default_database, T2, project=[d2], metadata=[]]], fields=[d2])
+      +- Exchange(distribution=[keep_input_as_is[hash[a]]])
+         +- SortMergeJoin(joinType=[InnerJoin], where=[(d2 = a)], select=[a, d2])
+            :- Reused(reference_id=[1])
+            +- Exchange(distribution=[hash[d2]])
+               +- TableSourceScan(table=[[default_catalog, default_database, T2, project=[d2], metadata=[]]], fields=[d2])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testSortAggOnNestedLoopJoinWithGlobalShuffle">
+    <Resource name="sql">
+      <![CDATA[WITH r AS (SELECT * FROM T1 FULL OUTER JOIN T2 ON true)
+SELECT sum(b1) FROM r]]>
+    </Resource>
+    <Resource name="ast">
+      <![CDATA[
+LogicalAggregate(group=[{}], EXPR$0=[SUM($0)])
++- LogicalProject(b1=[$1])
+   +- LogicalJoin(condition=[true], joinType=[full])
+      :- LogicalTableScan(table=[[default_catalog, default_database, T1]])
+      +- LogicalTableScan(table=[[default_catalog, default_database, T2]])
+]]>
+    </Resource>
+    <Resource name="optimized exec plan">
+      <![CDATA[
+SortAggregate(isMerge=[false], select=[SUM(b1) AS EXPR$0])
++- Exchange(distribution=[single])
+   +- Calc(select=[b1])
+      +- NestedLoopJoin(joinType=[FullOuterJoin], where=[true], select=[b1, a2], build=[left])
+         :- Exchange(distribution=[single])
+         :  +- TableSourceScan(table=[[default_catalog, default_database, T1, project=[b1], metadata=[]]], fields=[b1])
+         +- Exchange(distribution=[single])
+            +- TableSourceScan(table=[[default_catalog, default_database, T2, project=[a2], metadata=[]]], fields=[a2])
 ]]>
     </Resource>
   </TestCase>
-  <TestCase name="testSortAgg">
+  <TestCase name="testSortAggOnSortMergeJoinWithHashShuffle">
     <Resource name="sql">
       <![CDATA[WITH r AS (SELECT * FROM T1, T2 WHERE a1 = a2 AND c1 LIKE 'He%')
 SELECT sum(b1) FROM r group by a1]]>
@@ -299,14 +529,15 @@ LogicalProject(EXPR$0=[$1])
       <![CDATA[
 Calc(select=[EXPR$0])
 +- SortAggregate(isMerge=[false], groupBy=[a1], select=[a1, SUM(b1) AS EXPR$0])
-   +- Exchange(distribution=[keep_input_as_is])
+   +- Exchange(distribution=[forward])
       +- Calc(select=[a1, b1])
-         +- SortMergeJoin(joinType=[InnerJoin], where=[(a1 = a2)], select=[a1, b1, a2])
-            :- Exchange(distribution=[hash[a1]])
-            :  +- Calc(select=[a1, b1], where=[LIKE(c1, 'He%')])
-            :     +- TableSourceScan(table=[[default_catalog, default_database, T1, filter=[], project=[a1, b1, c1], metadata=[]]], fields=[a1, b1, c1])
-            +- Exchange(distribution=[hash[a2]])
-               +- TableSourceScan(table=[[default_catalog, default_database, T2, project=[a2], metadata=[]]], fields=[a2])
+         +- Exchange(distribution=[forward])
+            +- SortMergeJoin(joinType=[InnerJoin], where=[(a1 = a2)], select=[a1, b1, a2])
+               :- Exchange(distribution=[hash[a1]])
+               :  +- Calc(select=[a1, b1], where=[LIKE(c1, 'He%')])
+               :     +- TableSourceScan(table=[[default_catalog, default_database, T1, filter=[], project=[a1, b1, c1], metadata=[]]], fields=[a1, b1, c1])
+               +- Exchange(distribution=[hash[a2]])
+                  +- TableSourceScan(table=[[default_catalog, default_database, T2, project=[a2], metadata=[]]], fields=[a2])
 ]]>
     </Resource>
   </TestCase>
