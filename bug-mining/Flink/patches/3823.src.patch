diff --git a/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/TableEnvironment.java b/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/TableEnvironment.java
index 37a0dd1990f..f086dd8f88c 100644
--- a/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/TableEnvironment.java
+++ b/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/TableEnvironment.java
@@ -858,6 +858,11 @@ public interface TableEnvironment {
 	 * other hand some values might be evaluated according to the state from the time when
 	 * this method is called (e.g. timeZone).
 	 *
+	 * <p>Once the execution finishes, any previously defined DMLs will be cleared, no matter
+	 * whether the execution succeeds or not. Therefore, if you want to retry in case of
+	 * failures, you have to re-define the DMLs, i.e. by calling {@link #sqlUpdate(String)},
+	 * before you call this method again.
+	 *
 	 * @param jobName Desired name of the job
 	 * @return The result of the job execution, containing elapsed time and accumulators.
 	 * @throws Exception which occurs during job execution.
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/api/TableEnvironmentITCase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/api/TableEnvironmentITCase.scala
index 24ee0c203ed..5501e1a468d 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/api/TableEnvironmentITCase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/api/TableEnvironmentITCase.scala
@@ -38,7 +38,7 @@ import org.junit.Assert.{assertEquals, assertFalse, assertTrue}
 import org.junit.rules.TemporaryFolder
 import org.junit.runner.RunWith
 import org.junit.runners.Parameterized
-import org.junit.{Before, Rule, Test}
+import org.junit.{Assert, Before, Rule, Test}
 
 import _root_.java.io.File
 import _root_.java.util
@@ -249,6 +249,27 @@ class TableEnvironmentITCase(tableEnvName: String, isStreaming: Boolean) {
     assertEquals(getExpectedLastValues.sorted, sink.getAppendResults.sorted)
   }
 
+  @Test
+  def testClearOperation(): Unit = {
+    val tableEnv = TableEnvironmentImpl.create(settings)
+    tableEnv.sqlUpdate("create table dest1(x map<int,bigint>) with('connector' = 'COLLECTION')")
+    tableEnv.sqlUpdate("create table dest2(x int) with('connector' = 'COLLECTION')")
+    tableEnv.sqlUpdate("create table src(x int) with('connector' = 'COLLECTION')")
+
+    try {
+      // it would fail due to query and sink type mismatch
+      tableEnv.sqlUpdate("insert into dest1 select count(*) from src")
+      tableEnv.execute("insert dest1")
+      Assert.fail("insert is expected to fail due to type mismatch")
+    } catch {
+      case _: Exception => //expected
+    }
+
+    tableEnv.sqlUpdate("drop table dest1")
+    tableEnv.sqlUpdate("insert into dest2 select x from src")
+    tableEnv.execute("insert dest2")
+  }
+
   private def registerCsvTableSink(
       tEnv: TableEnvironment,
       fieldNames: Array[String],
