diff --git a/docs/layouts/shortcodes/generated/execution_config_configuration.html b/docs/layouts/shortcodes/generated/execution_config_configuration.html
index e809eb06c99..2a35fc8d6b7 100644
--- a/docs/layouts/shortcodes/generated/execution_config_configuration.html
+++ b/docs/layouts/shortcodes/generated/execution_config_configuration.html
@@ -58,6 +58,13 @@ By default no operator is disabled.</td>
             <td><p>Enum</p></td>
             <td>Determines whether string values for columns with CHAR(&lt;precision&gt;)/VARCHAR(&lt;precision&gt;) types will be trimmed or padded (only for CHAR(&lt;precision&gt;)), so that their length will match the one defined by the precision of their respective CHAR/VARCHAR column type.<br /><br />Possible values:<ul><li>"IGNORE": Don't apply any trimming and padding, and instead ignore the CHAR/VARCHAR precision directive.</li><li>"TRIM_PAD": Trim and pad string values to match the length defined by the CHAR/VARCHAR precision.</li></ul></td>
         </tr>
+        <tr>
+            <td><h5>table.exec.sink.keyed-shuffle</h5><br> <span class="label label-primary">Streaming</span></td>
+            <td style="word-wrap: break-word;">AUTO</td>
+            <td><p>Enum</p></td>
+            <td>In order to minimize the distributed disorder problem when writing data into table with primary keys that many users suffers. FLINK will auto add a keyed shuffle by default when the sink's parallelism differs from upstream operator and upstream is append only. This works only when the upstream ensures the multi-records' order on the primary key, if not, the added shuffle can not solve the problem (In this situation, a more proper way is to consider the deduplicate operation for the source firstly or use an upsert source with primary key definition which truly reflect the records evolution).<br />By default, the keyed shuffle will be added when the sink's parallelism differs from upstream operator. You can set to no shuffle(NONE) or force shuffle(FORCE).<br /><br />Possible values:<ul><li>"NONE"</li><li>"AUTO"</li><li>"FORCE"</li></ul></td>
+        </tr>
+        <tr>
             <td><h5>table.exec.sink.legacy-cast-behaviour</h5><br> <span class="label label-primary">Batch</span> <span class="label label-primary">Streaming</span></td>
             <td style="word-wrap: break-word;">ENABLED</td>
             <td><p>Enum</p></td>
diff --git a/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/config/ExecutionConfigOptions.java b/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/config/ExecutionConfigOptions.java
index 0a8d417d9d9..6f655b22135 100644
--- a/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/config/ExecutionConfigOptions.java
+++ b/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/config/ExecutionConfigOptions.java
@@ -150,6 +150,25 @@ public class ExecutionConfigOptions {
                                                     + "or force materialization(FORCE).")
                                     .build());
 
+    @Documentation.TableOption(execMode = Documentation.ExecMode.STREAMING)
+    public static final ConfigOption<SinkKeyedShuffle> TABLE_EXEC_SINK_KEYED_SHUFFLE =
+            key("table.exec.sink.keyed-shuffle")
+                    .enumType(SinkKeyedShuffle.class)
+                    .defaultValue(SinkKeyedShuffle.AUTO)
+                    .withDescription(
+                            Description.builder()
+                                    .text(
+                                            "In order to minimize the distributed disorder problem when writing data into table with primary keys that many users suffers. "
+                                                    + "FLINK will auto add a keyed shuffle by default when the sink's parallelism differs from upstream operator and upstream is append only. "
+                                                    + "This works only when the upstream ensures the multi-records' order on the primary key, if not, the added shuffle can not solve "
+                                                    + "the problem (In this situation, a more proper way is to consider the deduplicate operation for the source firstly or use an "
+                                                    + "upsert source with primary key definition which truly reflect the records evolution).")
+                                    .linebreak()
+                                    .text(
+                                            "By default, the keyed shuffle will be added when the sink's parallelism differs from upstream operator. "
+                                                    + "You can set to no shuffle(NONE) or force shuffle(FORCE).")
+                                    .build());
+
     // ------------------------------------------------------------------------
     //  Sort Options
     // ------------------------------------------------------------------------
@@ -463,6 +482,20 @@ public class ExecutionConfigOptions {
         FORCE
     }
 
+    /** Shuffle by primary key before sink. */
+    @PublicEvolving
+    public enum SinkKeyedShuffle {
+
+        /** No keyed shuffle will be added for sink. */
+        NONE,
+
+        /** Auto add keyed shuffle when the sink's parallelism differs from upstream operator. */
+        AUTO,
+
+        /** Add keyed shuffle in any case except single parallelism. */
+        FORCE
+    }
+
     /** Determine if CAST operates using the legacy behaviour or the new one. */
     @Deprecated
     public enum LegacyCastBehaviour implements DescribedEnum {
diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/common/CommonExecSink.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/common/CommonExecSink.java
index 1967ce27d7b..9c1870f4682 100644
--- a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/common/CommonExecSink.java
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/common/CommonExecSink.java
@@ -120,31 +120,48 @@ public abstract class CommonExecSink extends ExecNodeBase<Object>
             int rowtimeFieldIndex,
             boolean upsertMaterialize) {
         final DynamicTableSink tableSink = tableSinkSpec.getTableSink(planner.getFlinkContext());
-        final ChangelogMode changelogMode = tableSink.getChangelogMode(inputChangelogMode);
         final ResolvedSchema schema = tableSinkSpec.getCatalogTable().getResolvedSchema();
-
         final SinkRuntimeProvider runtimeProvider =
                 tableSink.getSinkRuntimeProvider(new SinkRuntimeProviderContext(isBounded));
-
         final RowType physicalRowType = getPhysicalRowType(schema);
-
         final int[] primaryKeys = getPrimaryKeyIndices(physicalRowType, schema);
-
         final int sinkParallelism = deriveSinkParallelism(inputTransform, runtimeProvider);
+        final int inputParallelism = inputTransform.getParallelism();
+        final boolean inputInsertOnly = inputChangelogMode.containsOnly(RowKind.INSERT);
+        final boolean hasPk = primaryKeys.length > 0;
+
+        if (!inputInsertOnly && sinkParallelism != inputParallelism && !hasPk) {
+            throw new TableException(
+                    String.format(
+                            "The sink for table '%s' has a configured parallelism of %s, while the input parallelism is %s. "
+                                    + "Since the configured parallelism is different from the input's parallelism and "
+                                    + "the changelog mode is not insert-only, a primary key is required but could not "
+                                    + "be found.",
+                            tableSinkSpec.getObjectIdentifier().asSummaryString(),
+                            sinkParallelism,
+                            inputParallelism));
+        }
+
+        // only add materialization if input has change
+        final boolean needMaterialization = !inputInsertOnly && upsertMaterialize;
 
         Transformation<RowData> sinkTransform =
                 applyConstraintValidations(
                         inputTransform, planner.getTableConfig(), physicalRowType);
 
-        sinkTransform =
-                applyKeyBy(
-                        changelogMode,
-                        sinkTransform,
-                        primaryKeys,
-                        sinkParallelism,
-                        upsertMaterialize);
+        if (hasPk) {
+            sinkTransform =
+                    applyKeyBy(
+                            planner.getTableConfig(),
+                            sinkTransform,
+                            primaryKeys,
+                            sinkParallelism,
+                            inputParallelism,
+                            inputInsertOnly,
+                            needMaterialization);
+        }
 
-        if (upsertMaterialize) {
+        if (needMaterialization) {
             sinkTransform =
                     applyUpsertMaterialize(
                             sinkTransform,
@@ -280,26 +297,29 @@ public abstract class CommonExecSink extends ExecNodeBase<Object>
      * messages.
      */
     private Transformation<RowData> applyKeyBy(
-            ChangelogMode changelogMode,
+            TableConfig config,
             Transformation<RowData> inputTransform,
             int[] primaryKeys,
             int sinkParallelism,
-            boolean upsertMaterialize) {
-        final int inputParallelism = inputTransform.getParallelism();
-        if ((inputParallelism == sinkParallelism || changelogMode.containsOnly(RowKind.INSERT))
-                && !upsertMaterialize) {
-            return inputTransform;
+            int inputParallelism,
+            boolean inputInsertOnly,
+            boolean needMaterialize) {
+        final ExecutionConfigOptions.SinkKeyedShuffle sinkShuffleByPk =
+                config.getConfiguration().get(ExecutionConfigOptions.TABLE_EXEC_SINK_KEYED_SHUFFLE);
+        boolean sinkKeyBy = false;
+        switch (sinkShuffleByPk) {
+            case NONE:
+                break;
+            case AUTO:
+                sinkKeyBy = inputInsertOnly && sinkParallelism != inputParallelism;
+                break;
+            case FORCE:
+                // single parallelism has no problem
+                sinkKeyBy = sinkParallelism != 1 || inputParallelism != 1;
+                break;
         }
-        if (primaryKeys.length == 0) {
-            throw new TableException(
-                    String.format(
-                            "The sink for table '%s' has a configured parallelism of %s, while the input parallelism is %s. "
-                                    + "Since the configured parallelism is different from the input's parallelism and "
-                                    + "the changelog mode is not insert-only, a primary key is required but could not "
-                                    + "be found.",
-                            tableSinkSpec.getObjectIdentifier().asSummaryString(),
-                            sinkParallelism,
-                            inputParallelism));
+        if (!sinkKeyBy && !needMaterialize) {
+            return inputTransform;
         }
 
         final RowDataKeySelector selector =
diff --git a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java
index 02e71de1309..431251c8018 100644
--- a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java
+++ b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java
@@ -1676,7 +1676,7 @@ public final class TestValuesTableFactory
                                     + "' yet.");
                     sinkFunction = new RetractingSinkFunction(tableName, converter);
                 }
-                return SinkFunctionProvider.of(sinkFunction);
+                return SinkFunctionProvider.of(sinkFunction, this.parallelism);
             }
         }
 
diff --git a/flink-table/flink-table-planner/src/test/resources/META-INF/services/org.apache.flink.table.factories.Factory b/flink-table/flink-table-planner/src/test/resources/META-INF/services/org.apache.flink.table.factories.Factory
index 12deb1d2e2c..7386413a5d2 100644
--- a/flink-table/flink-table-planner/src/test/resources/META-INF/services/org.apache.flink.table.factories.Factory
+++ b/flink-table/flink-table-planner/src/test/resources/META-INF/services/org.apache.flink.table.factories.Factory
@@ -17,3 +17,4 @@ org.apache.flink.formats.testcsv.TestCsvFormatFactory
 org.apache.flink.table.planner.factories.TestValuesTableFactory
 org.apache.flink.table.planner.factories.TestFileFactory
 org.apache.flink.table.planner.factories.TableFactoryHarness$Factory
+org.apache.flink.table.planner.plan.stream.sql.TestTableFactory
diff --git a/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/TableSinkTest.xml b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/TableSinkTest.xml
index 5feee2e5c47..0d63f48536e 100644
--- a/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/TableSinkTest.xml
+++ b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/TableSinkTest.xml
@@ -32,6 +32,191 @@ Sink(table=[default_catalog.default_database.appendSink], fields=[EXPR$0, c], ch
 ]]>
     </Resource>
   </TestCase>
+  <TestCase name="testAppendStreamToSinkWithoutPkForceKeyBy">
+    <Resource name="explain">
+      <![CDATA[== Abstract Syntax Tree ==
+LogicalSink(table=[default_catalog.default_database.sink], fields=[id, city_name])
++- LogicalProject(id=[$0], city_name=[$1])
+   +- LogicalTableScan(table=[[default_catalog, default_database, source]])
+
+== Optimized Physical Plan ==
+Sink(table=[default_catalog.default_database.sink], fields=[id, city_name])
++- TableSourceScan(table=[[default_catalog, default_database, source]], fields=[id, city_name])
+
+== Optimized Execution Plan ==
+Sink(table=[default_catalog.default_database.sink], fields=[id, city_name])
++- TableSourceScan(table=[[default_catalog, default_database, source]], fields=[id, city_name])
+
+== Physical Execution Plan ==
+{
+  "nodes" : [ {
+    "id" : ,
+    "type" : "Source: TableSourceScan(table=[[default_catalog, default_database, source]], fields=[id, city_name])",
+    "pact" : "Data Source",
+    "contents" : "Source: TableSourceScan(table=[[default_catalog, default_database, source]], fields=[id, city_name])",
+    "parallelism" : 4
+  }, {
+    "id" : ,
+    "type" : "Sink: Sink(table=[default_catalog.default_database.sink], fields=[id, city_name])",
+    "pact" : "Data Sink",
+    "contents" : "Sink: Sink(table=[default_catalog.default_database.sink], fields=[id, city_name])",
+    "parallelism" : 4,
+    "predecessors" : [ {
+      "id" : ,
+      "ship_strategy" : "FORWARD",
+      "side" : "second"
+    } ]
+  } ]
+}]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testAppendStreamToSinkWithPkAutoKeyBy">
+    <Resource name="explain">
+      <![CDATA[== Abstract Syntax Tree ==
+LogicalSink(table=[default_catalog.default_database.sink], fields=[id, city_name])
++- LogicalProject(id=[$0], city_name=[$1])
+   +- LogicalTableScan(table=[[default_catalog, default_database, source]])
+
+== Optimized Physical Plan ==
+Sink(table=[default_catalog.default_database.sink], fields=[id, city_name])
++- TableSourceScan(table=[[default_catalog, default_database, source]], fields=[id, city_name])
+
+== Optimized Execution Plan ==
+Sink(table=[default_catalog.default_database.sink], fields=[id, city_name])
++- TableSourceScan(table=[[default_catalog, default_database, source]], fields=[id, city_name])
+
+== Physical Execution Plan ==
+{
+  "nodes" : [ {
+    "id" : ,
+    "type" : "Source: TableSourceScan(table=[[default_catalog, default_database, source]], fields=[id, city_name])",
+    "pact" : "Data Source",
+    "contents" : "Source: TableSourceScan(table=[[default_catalog, default_database, source]], fields=[id, city_name])",
+    "parallelism" : 1
+  }, {
+    "id" : ,
+    "type" : "ConstraintEnforcer[NotNullEnforcer(fields=[id])]",
+    "pact" : "Operator",
+    "contents" : "ConstraintEnforcer[NotNullEnforcer(fields=[id])]",
+    "parallelism" : 1,
+    "predecessors" : [ {
+      "id" : ,
+      "ship_strategy" : "FORWARD",
+      "side" : "second"
+    } ]
+  }, {
+    "id" : ,
+    "type" : "Sink: Sink(table=[default_catalog.default_database.sink], fields=[id, city_name])",
+    "pact" : "Data Sink",
+    "contents" : "Sink: Sink(table=[default_catalog.default_database.sink], fields=[id, city_name])",
+    "parallelism" : 9,
+    "predecessors" : [ {
+      "id" : ,
+      "ship_strategy" : "HASH",
+      "side" : "second"
+    } ]
+  } ]
+}]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testAppendStreamToSinkWithPkForceKeyBy">
+    <Resource name="explain">
+      <![CDATA[== Abstract Syntax Tree ==
+LogicalSink(table=[default_catalog.default_database.sink], fields=[id, city_name])
++- LogicalProject(id=[$0], city_name=[$1])
+   +- LogicalTableScan(table=[[default_catalog, default_database, source]])
+
+== Optimized Physical Plan ==
+Sink(table=[default_catalog.default_database.sink], fields=[id, city_name])
++- TableSourceScan(table=[[default_catalog, default_database, source]], fields=[id, city_name])
+
+== Optimized Execution Plan ==
+Sink(table=[default_catalog.default_database.sink], fields=[id, city_name])
++- TableSourceScan(table=[[default_catalog, default_database, source]], fields=[id, city_name])
+
+== Physical Execution Plan ==
+{
+  "nodes" : [ {
+    "id" : ,
+    "type" : "Source: TableSourceScan(table=[[default_catalog, default_database, source]], fields=[id, city_name])",
+    "pact" : "Data Source",
+    "contents" : "Source: TableSourceScan(table=[[default_catalog, default_database, source]], fields=[id, city_name])",
+    "parallelism" : 4
+  }, {
+    "id" : ,
+    "type" : "ConstraintEnforcer[NotNullEnforcer(fields=[id])]",
+    "pact" : "Operator",
+    "contents" : "ConstraintEnforcer[NotNullEnforcer(fields=[id])]",
+    "parallelism" : 4,
+    "predecessors" : [ {
+      "id" : ,
+      "ship_strategy" : "FORWARD",
+      "side" : "second"
+    } ]
+  }, {
+    "id" : ,
+    "type" : "Sink: Sink(table=[default_catalog.default_database.sink], fields=[id, city_name])",
+    "pact" : "Data Sink",
+    "contents" : "Sink: Sink(table=[default_catalog.default_database.sink], fields=[id, city_name])",
+    "parallelism" : 4,
+    "predecessors" : [ {
+      "id" : ,
+      "ship_strategy" : "HASH",
+      "side" : "second"
+    } ]
+  } ]
+}]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testAppendStreamToSinkWithPkNoKeyBy">
+    <Resource name="explain">
+      <![CDATA[== Abstract Syntax Tree ==
+LogicalSink(table=[default_catalog.default_database.sink], fields=[id, city_name])
++- LogicalProject(id=[$0], city_name=[$1])
+   +- LogicalTableScan(table=[[default_catalog, default_database, source]])
+
+== Optimized Physical Plan ==
+Sink(table=[default_catalog.default_database.sink], fields=[id, city_name])
++- TableSourceScan(table=[[default_catalog, default_database, source]], fields=[id, city_name])
+
+== Optimized Execution Plan ==
+Sink(table=[default_catalog.default_database.sink], fields=[id, city_name])
++- TableSourceScan(table=[[default_catalog, default_database, source]], fields=[id, city_name])
+
+== Physical Execution Plan ==
+{
+  "nodes" : [ {
+    "id" : ,
+    "type" : "Source: TableSourceScan(table=[[default_catalog, default_database, source]], fields=[id, city_name])",
+    "pact" : "Data Source",
+    "contents" : "Source: TableSourceScan(table=[[default_catalog, default_database, source]], fields=[id, city_name])",
+    "parallelism" : 1
+  }, {
+    "id" : ,
+    "type" : "ConstraintEnforcer[NotNullEnforcer(fields=[id])]",
+    "pact" : "Operator",
+    "contents" : "ConstraintEnforcer[NotNullEnforcer(fields=[id])]",
+    "parallelism" : 1,
+    "predecessors" : [ {
+      "id" : ,
+      "ship_strategy" : "FORWARD",
+      "side" : "second"
+    } ]
+  }, {
+    "id" : ,
+    "type" : "Sink: Sink(table=[default_catalog.default_database.sink], fields=[id, city_name])",
+    "pact" : "Data Sink",
+    "contents" : "Sink: Sink(table=[default_catalog.default_database.sink], fields=[id, city_name])",
+    "parallelism" : 9,
+    "predecessors" : [ {
+      "id" : ,
+      "ship_strategy" : "REBALANCE",
+      "side" : "second"
+    } ]
+  } ]
+}]]>
+    </Resource>
+  </TestCase>
   <TestCase name="testAppendUpsertAndRetractSink">
     <Resource name="ast">
       <![CDATA[
@@ -171,25 +356,6 @@ Sink(table=[default_catalog.default_database.upsertSink], fields=[cnt, frequency
             +- Exchange(distribution=[hash[b]], changelogMode=[I])
                +- Calc(select=[b, a], changelogMode=[I])
                   +- DataStreamScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c], changelogMode=[I])
-]]>
-    </Resource>
-  </TestCase>
-  <TestCase name="testUpsertSink">
-    <Resource name="ast">
-      <![CDATA[
-LogicalSink(table=[default_catalog.default_database.upsertSink], fields=[a, cnt])
-+- LogicalAggregate(group=[{0}], cnt=[COUNT()])
-   +- LogicalProject(a=[$0])
-      +- LogicalTableScan(table=[[default_catalog, default_database, MyTable]])
-]]>
-    </Resource>
-    <Resource name="optimized rel plan">
-      <![CDATA[
-Sink(table=[default_catalog.default_database.upsertSink], fields=[a, cnt], changelogMode=[NONE])
-+- GroupAggregate(groupBy=[a], select=[a, COUNT(*) AS cnt], changelogMode=[I,UA])
-   +- Exchange(distribution=[hash[a]], changelogMode=[I])
-      +- Calc(select=[a], changelogMode=[I])
-         +- DataStreamScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c], changelogMode=[I])
 ]]>
     </Resource>
   </TestCase>
@@ -235,6 +401,74 @@ Sink(table=[default_catalog.default_database.retractSink], fields=[cnt, a], chan
 ]]>
     </Resource>
   </TestCase>
+  <TestCase name="testUpsertSink">
+    <Resource name="ast">
+      <![CDATA[
+LogicalSink(table=[default_catalog.default_database.upsertSink], fields=[a, cnt])
++- LogicalAggregate(group=[{0}], cnt=[COUNT()])
+   +- LogicalProject(a=[$0])
+      +- LogicalTableScan(table=[[default_catalog, default_database, MyTable]])
+]]>
+    </Resource>
+    <Resource name="optimized rel plan">
+      <![CDATA[
+Sink(table=[default_catalog.default_database.upsertSink], fields=[a, cnt], changelogMode=[NONE])
++- GroupAggregate(groupBy=[a], select=[a, COUNT(*) AS cnt], changelogMode=[I,UA])
+   +- Exchange(distribution=[hash[a]], changelogMode=[I])
+      +- Calc(select=[a], changelogMode=[I])
+         +- DataStreamScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c], changelogMode=[I])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testSingleParallelismAppendStreamToSinkWithPkForceKeyBy">
+    <Resource name="explain">
+      <![CDATA[== Abstract Syntax Tree ==
+LogicalSink(table=[default_catalog.default_database.sink], fields=[id, city_name])
++- LogicalProject(id=[$0], city_name=[$1])
+   +- LogicalTableScan(table=[[default_catalog, default_database, source]])
+
+== Optimized Physical Plan ==
+Sink(table=[default_catalog.default_database.sink], fields=[id, city_name])
++- TableSourceScan(table=[[default_catalog, default_database, source]], fields=[id, city_name])
+
+== Optimized Execution Plan ==
+Sink(table=[default_catalog.default_database.sink], fields=[id, city_name])
++- TableSourceScan(table=[[default_catalog, default_database, source]], fields=[id, city_name])
+
+== Physical Execution Plan ==
+{
+  "nodes" : [ {
+    "id" : ,
+    "type" : "Source: TableSourceScan(table=[[default_catalog, default_database, source]], fields=[id, city_name])",
+    "pact" : "Data Source",
+    "contents" : "Source: TableSourceScan(table=[[default_catalog, default_database, source]], fields=[id, city_name])",
+    "parallelism" : 1
+  }, {
+    "id" : ,
+    "type" : "ConstraintEnforcer[NotNullEnforcer(fields=[id])]",
+    "pact" : "Operator",
+    "contents" : "ConstraintEnforcer[NotNullEnforcer(fields=[id])]",
+    "parallelism" : 1,
+    "predecessors" : [ {
+      "id" : ,
+      "ship_strategy" : "FORWARD",
+      "side" : "second"
+    } ]
+  }, {
+    "id" : ,
+    "type" : "Sink: Sink(table=[default_catalog.default_database.sink], fields=[id, city_name])",
+    "pact" : "Data Sink",
+    "contents" : "Sink: Sink(table=[default_catalog.default_database.sink], fields=[id, city_name])",
+    "parallelism" : 1,
+    "predecessors" : [ {
+      "id" : ,
+      "ship_strategy" : "FORWARD",
+      "side" : "second"
+    } ]
+  } ]
+}]]>
+    </Resource>
+  </TestCase>
   <TestCase name="testSinkDisorderChangeLogWithRank">
     <Resource name="sql">
       <![CDATA[
diff --git a/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/join/TemporalJoinTest.xml b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/join/TemporalJoinTest.xml
index a469f19cbc8..4337af0a8b2 100644
--- a/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/join/TemporalJoinTest.xml
+++ b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/join/TemporalJoinTest.xml
@@ -145,6 +145,45 @@ Calc(select=[amount, currency, ts, rowtime, currency0, rate, ts0, CAST(rowtime0)
       +- WatermarkAssigner(rowtime=[rowtime], watermark=[rowtime])
          +- Calc(select=[currency, rate, ts, TO_TIMESTAMP_LTZ(ts, 3) AS rowtime])
             +- TableSourceScan(table=[[default_catalog, default_database, RatesLtz]], fields=[currency, rate, ts])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testEventTimeTemporalJoinToSinkWithPk">
+    <Resource name="explain">
+      <![CDATA[== Abstract Syntax Tree ==
+LogicalSink(table=[default_catalog.default_database.rowtime_default_sink], fields=[order_id, currency, amount, order_time, rate, currency_time])
++- LogicalProject(order_id=[$0], currency=[$1], amount=[$3], order_time=[$4], rate=[$7], currency_time=[$8])
+   +- LogicalCorrelate(correlation=[$cor0], joinType=[inner], requiredColumns=[{1, 2, 4}])
+      :- LogicalWatermarkAssigner(rowtime=[order_time], watermark=[$4])
+      :  +- LogicalTableScan(table=[[default_catalog, default_database, orders_rowtime]])
+      +- LogicalFilter(condition=[AND(=($cor0.currency_no, $1), =($cor0.currency, $0))])
+         +- LogicalSnapshot(period=[$cor0.order_time])
+            +- LogicalWatermarkAssigner(rowtime=[currency_time], watermark=[-($3, 10000:INTERVAL SECOND)])
+               +- LogicalTableScan(table=[[default_catalog, default_database, versioned_currency_with_multi_key]])
+
+== Optimized Physical Plan ==
+Sink(table=[default_catalog.default_database.rowtime_default_sink], fields=[order_id, currency, amount, order_time, rate, currency_time], changelogMode=[NONE])
++- Calc(select=[order_id, currency, amount, order_time, rate, CAST(currency_time) AS currency_time], changelogMode=[I,UB,UA,D])
+   +- TemporalJoin(joinType=[InnerJoin], where=[AND(=(currency_no, currency_no0), =(currency, currency0), __TEMPORAL_JOIN_CONDITION(order_time, currency_time, __TEMPORAL_JOIN_CONDITION_PRIMARY_KEY(currency0, currency_no0), __TEMPORAL_JOIN_LEFT_KEY(currency_no, currency), __TEMPORAL_JOIN_RIGHT_KEY(currency_no0, currency0)))], select=[order_id, currency, currency_no, amount, order_time, currency0, currency_no0, rate, currency_time], changelogMode=[I,UB,UA,D])
+      :- Exchange(distribution=[hash[currency_no, currency]], changelogMode=[I,UB,UA,D])
+      :  +- WatermarkAssigner(rowtime=[order_time], watermark=[order_time], changelogMode=[I,UB,UA,D])
+      :     +- TableSourceScan(table=[[default_catalog, default_database, orders_rowtime]], fields=[order_id, currency, currency_no, amount, order_time], changelogMode=[I,UB,UA,D])
+      +- Exchange(distribution=[hash[currency_no, currency]], changelogMode=[I,UA,D])
+         +- WatermarkAssigner(rowtime=[currency_time], watermark=[-(currency_time, 10000:INTERVAL SECOND)], changelogMode=[I,UA,D])
+            +- DropUpdateBefore(changelogMode=[I,UA,D])
+               +- TableSourceScan(table=[[default_catalog, default_database, versioned_currency_with_multi_key]], fields=[currency, currency_no, rate, currency_time], changelogMode=[I,UB,UA,D])
+
+== Optimized Execution Plan ==
+Sink(table=[default_catalog.default_database.rowtime_default_sink], fields=[order_id, currency, amount, order_time, rate, currency_time], upsertMaterialize=[true])
++- Calc(select=[order_id, currency, amount, order_time, rate, CAST(currency_time) AS currency_time])
+   +- TemporalJoin(joinType=[InnerJoin], where=[((currency_no = currency_no0) AND (currency = currency0) AND __TEMPORAL_JOIN_CONDITION(order_time, currency_time, __TEMPORAL_JOIN_CONDITION_PRIMARY_KEY(currency0, currency_no0), __TEMPORAL_JOIN_LEFT_KEY(currency_no, currency), __TEMPORAL_JOIN_RIGHT_KEY(currency_no0, currency0)))], select=[order_id, currency, currency_no, amount, order_time, currency0, currency_no0, rate, currency_time])
+      :- Exchange(distribution=[hash[currency_no, currency]])
+      :  +- WatermarkAssigner(rowtime=[order_time], watermark=[order_time])
+      :     +- TableSourceScan(table=[[default_catalog, default_database, orders_rowtime]], fields=[order_id, currency, currency_no, amount, order_time])
+      +- Exchange(distribution=[hash[currency_no, currency]])
+         +- WatermarkAssigner(rowtime=[currency_time], watermark=[(currency_time - 10000:INTERVAL SECOND)])
+            +- DropUpdateBefore
+               +- TableSourceScan(table=[[default_catalog, default_database, versioned_currency_with_multi_key]], fields=[currency, currency_no, rate, currency_time])
 ]]>
     </Resource>
   </TestCase>
diff --git a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/plan/stream/sql/TableSinkTest.scala b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/plan/stream/sql/TableSinkTest.scala
index 461c4fcf0fd..4dfae8e2779 100644
--- a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/plan/stream/sql/TableSinkTest.scala
+++ b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/plan/stream/sql/TableSinkTest.scala
@@ -19,11 +19,20 @@
 package org.apache.flink.table.planner.plan.stream.sql
 
 import org.apache.flink.api.scala._
+import org.apache.flink.configuration.ConfigOption
+import org.apache.flink.streaming.api.functions.source.{ParallelSourceFunction, SourceFunction}
 import org.apache.flink.table.api._
-import org.apache.flink.table.planner.utils.TableTestBase
+import org.apache.flink.table.api.config.ExecutionConfigOptions
+import org.apache.flink.table.connector.ChangelogMode
+import org.apache.flink.table.connector.source.{DynamicTableSource, ScanTableSource, SourceFunctionProvider}
+import org.apache.flink.table.data.RowData
+import org.apache.flink.table.factories.{DynamicTableFactory, DynamicTableSourceFactory}
+import org.apache.flink.table.planner.utils.{TableTestBase, TestingTableEnvironment}
 
 import org.junit.Test
 
+import java.util
+
 class TableSinkTest extends TableTestBase {
 
   private val util = streamTestUtil()
@@ -464,4 +473,188 @@ class TableSinkTest extends TableTestBase {
         |   WHERE rank_number < 10
         |""".stripMargin)
   }
+
+  @Test def testAppendStreamToSinkWithPkAutoKeyBy(): Unit = {
+    val tEnv = util.tableEnv
+    tEnv.executeSql(
+      """
+        |create table source (
+        | id varchar,
+        | city_name varchar
+        |) with (
+        | 'connector' = 'values',
+        | 'changelog-mode' = 'I'
+        |)""".stripMargin)
+    tEnv.executeSql(
+      """
+        |create table sink (
+        | id varchar,
+        | city_name varchar,
+        | primary key (id) not enforced
+        |) with (
+        | 'connector' = 'values',
+        | 'sink-insert-only' = 'false',
+        | 'sink.parallelism' = '9'
+        |)""".stripMargin)
+    val stmtSet = tEnv.asInstanceOf[TestingTableEnvironment].createStatementSet
+    stmtSet.addInsertSql("insert into sink select * from source")
+    // we set the sink parallelism to 9 which differs from the source, expect 'keyby' was added.
+    util.verifyExplain(stmtSet, ExplainDetail.JSON_EXECUTION_PLAN)
+  }
+
+  @Test def testAppendStreamToSinkWithPkNoKeyBy(): Unit = {
+    val tEnv = util.tableEnv
+    tEnv.getConfig.getConfiguration.set(ExecutionConfigOptions.TABLE_EXEC_SINK_KEYED_SHUFFLE,
+      ExecutionConfigOptions.SinkKeyedShuffle.NONE)
+    tEnv.executeSql(
+      """
+        |create table source (
+        | id varchar,
+        | city_name varchar
+        |) with (
+        | 'connector' = 'values',
+        | 'changelog-mode' = 'I'
+        |)""".stripMargin)
+    tEnv.executeSql(
+      """
+        |create table sink (
+        | id varchar,
+        | city_name varchar,
+        | primary key (id) not enforced
+        |) with (
+        | 'connector' = 'values',
+        | 'sink-insert-only' = 'false',
+        | 'sink.parallelism' = '9'
+        |)""".stripMargin)
+    val stmtSet = tEnv.asInstanceOf[TestingTableEnvironment].createStatementSet
+    stmtSet.addInsertSql("insert into sink select * from source")
+    // we set the sink parallelism to 9 which differs from the source, but disable auto keyby
+    util.verifyExplain(stmtSet, ExplainDetail.JSON_EXECUTION_PLAN)
+  }
+
+  @Test def testAppendStreamToSinkWithPkForceKeyBy(): Unit = {
+    util.getStreamEnv.setParallelism(4)
+    val tEnv = util.tableEnv
+    tEnv.getConfig.getConfiguration.set(ExecutionConfigOptions.TABLE_EXEC_SINK_KEYED_SHUFFLE,
+      ExecutionConfigOptions.SinkKeyedShuffle.FORCE)
+    tEnv.executeSql(
+      """
+        |create table source (
+        | id varchar,
+        | city_name varchar
+        |) with (
+        | 'connector' = 'test_source'
+        |)""".stripMargin)
+
+    tEnv.executeSql(
+      """
+        |create table sink (
+        | id varchar,
+        | city_name varchar,
+        | primary key (id) not enforced
+        |) with (
+        | 'connector' = 'values',
+        | 'sink-insert-only' = 'false',
+        | 'sink.parallelism' = '4'
+        |)""".stripMargin)
+    val stmtSet = tEnv.asInstanceOf[TestingTableEnvironment].createStatementSet
+    stmtSet.addInsertSql("insert into sink select * from source")
+    // source and sink has same parallelism, but sink shuffle by pk is enforced
+    util.verifyExplain(stmtSet, ExplainDetail.JSON_EXECUTION_PLAN)
+  }
+
+  @Test def testSingleParallelismAppendStreamToSinkWithPkForceKeyBy(): Unit = {
+    util.getStreamEnv.setParallelism(1)
+    val tEnv = util.tableEnv
+    tEnv.getConfig.getConfiguration.set(ExecutionConfigOptions.TABLE_EXEC_SINK_KEYED_SHUFFLE,
+      ExecutionConfigOptions.SinkKeyedShuffle.FORCE)
+    tEnv.executeSql(
+      """
+        |create table source (
+        | id varchar,
+        | city_name varchar
+        |) with (
+        | 'connector' = 'test_source'
+        |)""".stripMargin)
+
+    tEnv.executeSql(
+      """
+        |create table sink (
+        | id varchar,
+        | city_name varchar,
+        | primary key (id) not enforced
+        |) with (
+        | 'connector' = 'values',
+        | 'sink-insert-only' = 'false',
+        | 'sink.parallelism' = '1'
+        |)""".stripMargin)
+    val stmtSet = tEnv.asInstanceOf[TestingTableEnvironment].createStatementSet
+    stmtSet.addInsertSql("insert into sink select * from source")
+    // source and sink has same parallelism, but sink shuffle by pk is enforced
+    util.verifyExplain(stmtSet, ExplainDetail.JSON_EXECUTION_PLAN)
+  }
+
+  @Test def testAppendStreamToSinkWithoutPkForceKeyBy(): Unit = {
+    util.getStreamEnv.setParallelism(4)
+    val tEnv = util.tableEnv
+    tEnv.getConfig.getConfiguration.set(ExecutionConfigOptions.TABLE_EXEC_SINK_KEYED_SHUFFLE,
+      ExecutionConfigOptions.SinkKeyedShuffle.FORCE)
+    tEnv.executeSql(
+      """
+        |create table source (
+        | id varchar,
+        | city_name varchar
+        |) with (
+        | 'connector' = 'test_source'
+        |)""".stripMargin)
+
+    tEnv.executeSql(
+      """
+        |create table sink (
+        | id varchar,
+        | city_name varchar
+        |) with (
+        | 'connector' = 'values',
+        | 'sink-insert-only' = 'false',
+        | 'sink.parallelism' = '4'
+        |)""".stripMargin)
+    val stmtSet = tEnv.asInstanceOf[TestingTableEnvironment].createStatementSet
+    stmtSet.addInsertSql("insert into sink select * from source")
+    // source and sink has same parallelism, but sink shuffle by pk is enforced
+    util.verifyExplain(stmtSet, ExplainDetail.JSON_EXECUTION_PLAN)
+  }
+}
+
+/** tests table factory use ParallelSourceFunction which support parallelism by env*/
+class TestTableFactory extends DynamicTableSourceFactory {
+  override def createDynamicTableSource(context: DynamicTableFactory.Context):
+    DynamicTableSource = {
+    new TestParallelSource()
+  }
+
+  override def factoryIdentifier = "test_source"
+
+  override def requiredOptions = new util.HashSet[ConfigOption[_]]
+
+  override def optionalOptions = {
+    new util.HashSet[ConfigOption[_]]()
+  }
+
+}
+
+/** tests table source provide a {@link ParallelSourceFunction}. */
+class TestParallelSource() extends ScanTableSource {
+  override def copy = throw new TableException("Not supported")
+
+  override def asSummaryString = "test source"
+
+  override def getChangelogMode: ChangelogMode = ChangelogMode.insertOnly()
+
+  override def getScanRuntimeProvider(runtimeProviderContext: ScanTableSource.ScanContext):
+    ScanTableSource.ScanRuntimeProvider = {
+    SourceFunctionProvider.of(new ParallelSourceFunction[RowData] {
+      override def run(ctx: SourceFunction.SourceContext[RowData]): Unit = ???
+      override def cancel(): Unit = ???
+    }, false)
+  }
 }
diff --git a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/plan/stream/sql/join/TemporalJoinTest.scala b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/plan/stream/sql/join/TemporalJoinTest.scala
index 2750ac1a3c2..52ab763a9de 100644
--- a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/plan/stream/sql/join/TemporalJoinTest.scala
+++ b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/plan/stream/sql/join/TemporalJoinTest.scala
@@ -17,8 +17,9 @@
  */
 package org.apache.flink.table.planner.plan.stream.sql.join
 
-import org.apache.flink.table.api.ValidationException
+import org.apache.flink.table.api.{ExplainDetail, ValidationException}
 import org.apache.flink.table.planner.utils.{StreamTableTestUtil, TableTestBase}
+
 import org.junit.Assert.{assertTrue, fail}
 import org.junit.{Before, Test}
 
@@ -519,6 +520,67 @@ class TemporalJoinTest extends TableTestBase {
       classOf[ValidationException])
   }
 
+  @Test
+  def testEventTimeTemporalJoinToSinkWithPk(): Unit = {
+    val tEnv = util.tableEnv
+    tEnv.executeSql(
+      s"""
+         |CREATE TABLE orders_rowtime (
+         |  order_id BIGINT,
+         |  currency STRING,
+         |  currency_no STRING,
+         |  amount BIGINT,
+         |  order_time TIMESTAMP(3),
+         |  WATERMARK FOR order_time AS order_time,
+         |  PRIMARY KEY (order_id) NOT ENFORCED
+         |) WITH (
+         |  'connector' = 'values',
+         |  'changelog-mode' = 'I,UA,UB,D',
+         |  'data-id' = 'rowTimeOrderDataId'
+         |)
+         |""".stripMargin)
+    tEnv.executeSql(
+      s"""
+         |CREATE TABLE versioned_currency_with_multi_key (
+         |  currency STRING,
+         |  currency_no STRING,
+         |  rate  BIGINT,
+         |  currency_time TIMESTAMP(3),
+         |  WATERMARK FOR currency_time AS currency_time - interval '10' SECOND,
+         |  PRIMARY KEY(currency, currency_no) NOT ENFORCED
+         |) WITH (
+         |  'connector' = 'values',
+         |  'changelog-mode' = 'I,UA,UB,D',
+         |  'data-id' = 'rowTimeCurrencyDataId'
+         |)
+         |""".stripMargin)
+    tEnv.executeSql(
+      s"""
+         |CREATE TABLE rowtime_default_sink (
+         |  order_id BIGINT,
+         |  currency STRING,
+         |  amount BIGINT,
+         |  l_time TIMESTAMP(3),
+         |  rate BIGINT,
+         |  r_time TIMESTAMP(3),
+         |  PRIMARY KEY(order_id) NOT ENFORCED
+         |) WITH (
+         |  'connector' = 'values',
+         |  'sink-insert-only' = 'false',
+         |  'changelog-mode' = 'I,UA,UB,D'
+         |)
+         |""".stripMargin)
+    val sql =
+      """
+        |INSERT INTO rowtime_default_sink
+        |  SELECT o.order_id, o.currency, o.amount, o.order_time, r.rate, r.currency_time
+        |  FROM orders_rowtime AS o JOIN versioned_currency_with_multi_key
+        |    FOR SYSTEM_TIME AS OF o.order_time as r
+        |      ON o.currency_no = r.currency_no AND o.currency = r.currency
+        |""".stripMargin
+    util.verifyExplainInsert(sql, ExplainDetail.CHANGELOG_MODE)
+  }
+
   private def expectExceptionThrown(
     sql: String,
     keywords: String,
diff --git a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/ChangelogSourceITCase.scala b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/ChangelogSourceITCase.scala
index 8ca8a428a73..a5b3e88a42c 100644
--- a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/ChangelogSourceITCase.scala
+++ b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/ChangelogSourceITCase.scala
@@ -121,9 +121,10 @@ class ChangelogSourceITCase(
       "user4,Tina,tina@gmail.com,11.30,22.60")
     assertEquals(expected.sorted, TestValuesTableFactory.getResults("user_sink").sorted)
 
-    // verify the update_before messages haven been filtered when scanning changelog source
     sourceMode match {
-      case CHANGELOG_SOURCE | CHANGELOG_SOURCE_WITH_EVENTS_DUPLICATE =>
+      // verify the update_before messages haven been filtered when scanning changelog source
+      // the CHANGELOG_SOURCE has I,UA,UB,D but no primary key, so we can not omit UB
+      case CHANGELOG_SOURCE_WITH_EVENTS_DUPLICATE =>
         val rawResult = TestValuesTableFactory.getRawResults("user_sink")
         val hasUB = rawResult.exists(r => r.startsWith("-U"))
         assertFalse(
diff --git a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/runtime/stream/table/TableSinkITCase.scala b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/runtime/stream/table/TableSinkITCase.scala
index fd207f87cec..da20323a1b5 100644
--- a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/runtime/stream/table/TableSinkITCase.scala
+++ b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/runtime/stream/table/TableSinkITCase.scala
@@ -21,6 +21,7 @@ package org.apache.flink.table.planner.runtime.stream.table
 import org.apache.flink.api.scala._
 import org.apache.flink.table.api._
 import org.apache.flink.table.api.bridge.scala._
+import org.apache.flink.table.api.config.ExecutionConfigOptions
 import org.apache.flink.table.planner.factories.TestValuesTableFactory
 import org.apache.flink.table.planner.factories.TestValuesTableFactory.changelogRow
 import org.apache.flink.table.planner.runtime.utils.BatchTestBase.row
@@ -28,6 +29,7 @@ import org.apache.flink.table.planner.runtime.utils.StreamingTestBase
 import org.apache.flink.table.planner.runtime.utils.TestData.{data1, nullData4, smallTupleData3, tupleData2, tupleData3, tupleData5}
 import org.apache.flink.table.utils.LegacyRowResource
 import org.apache.flink.util.ExceptionUtils
+
 import org.junit.Assert.{assertEquals, assertFalse, assertTrue, fail}
 import org.junit.{Rule, Test}
 import org.junit.rules.ExpectedException
@@ -35,6 +37,7 @@ import org.junit.rules.ExpectedException
 import java.lang.{Long => JLong}
 import java.math.{BigDecimal => JBigDecimal}
 import java.util.concurrent.atomic.AtomicInteger
+
 import scala.collection.JavaConversions._
 import scala.collection.Seq
 import scala.util.{Failure, Success, Try}
@@ -687,16 +690,10 @@ class TableSinkITCase extends StreamingTestBase {
          |  'sink-changelog-mode-enforced' = 'I,D'
          |)
          |""".stripMargin)
+    // source is insert only, it never produce a delete, so we do not require a pk for the sink
     Try(tEnv
       .executeSql(s"INSERT INTO $sinkTableWithoutPkName SELECT * FROM $sourceTableName")
-      .await()) match {
-      case Failure(e) =>
-        val exception = ExceptionUtils
-          .findThrowableWithMessage(
-            e,
-            "a primary key is required")
-        assertTrue(exception.isPresent)
-    }
+      .await()).isSuccess
 
     tEnv.executeSql(
       s"""
@@ -1249,4 +1246,40 @@ class TableSinkITCase extends StreamingTestBase {
     ).execute().await()
     assertEquals(Seq("+I(42)"), TestValuesTableFactory.getOnlyRawResults.toList)
   }
+
+  @Test
+  def testAppendStreamToSinkWithoutPkForceKeyBy(): Unit = {
+    val t = env.fromCollection(tupleData3)
+        .assignAscendingTimestamps(_._1.toLong)
+        .toTable(tEnv, 'id, 'num, 'text, 'rowtime.rowtime)
+    tEnv.getConfig.getConfiguration.set(ExecutionConfigOptions.TABLE_EXEC_SINK_KEYED_SHUFFLE,
+      ExecutionConfigOptions.SinkKeyedShuffle.FORCE)
+
+    tEnv.executeSql(
+      s"""
+         |CREATE TABLE sink (
+         |  `t` TIMESTAMP(3),
+         |  `icnt` BIGINT,
+         |  `nsum` BIGINT
+         |) WITH (
+         |  'connector' = 'values',
+         |  'sink-insert-only' = 'false',
+         |  'sink.parallelism' = '4'
+         |)
+         |""".stripMargin)
+
+    val table = t.window(Tumble over 5.millis on 'rowtime as 'w)
+        .groupBy('w)
+        .select('w.end as 't, 'id.count as 'icnt, 'num.sum as 'nsum)
+    table.executeInsert("sink").await()
+
+    val result = TestValuesTableFactory.getResults("sink")
+    val expected = List(
+      "1970-01-01T00:00:00.005,4,8",
+      "1970-01-01T00:00:00.010,5,18",
+      "1970-01-01T00:00:00.015,5,24",
+      "1970-01-01T00:00:00.020,5,29",
+      "1970-01-01T00:00:00.025,2,12")
+    assertEquals(expected.sorted, result.sorted)
+  }
 }
diff --git a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/utils/TableTestBase.scala b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/utils/TableTestBase.scala
index d27af9c1c66..ef39ea700b7 100644
--- a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/utils/TableTestBase.scala
+++ b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/utils/TableTestBase.scala
@@ -17,22 +17,11 @@
  */
 package org.apache.flink.table.planner.utils
 
-import _root_.java.math.{BigDecimal => JBigDecimal}
-import _root_.java.util
-import java.io.{File, IOException}
-import java.nio.file.{Files, Paths}
-import java.time.Duration
-
-import org.apache.calcite.avatica.util.TimeUnit
-import org.apache.calcite.rel.RelNode
-import org.apache.calcite.sql.parser.SqlParserPos
-import org.apache.calcite.sql.{SqlExplainLevel, SqlIntervalQualifier}
 import org.apache.flink.api.common.BatchShuffleMode
 import org.apache.flink.api.common.typeinfo.{AtomicType, TypeInformation}
 import org.apache.flink.api.java.typeutils.{PojoTypeInfo, RowTypeInfo, TupleTypeInfo}
 import org.apache.flink.api.scala.typeutils.CaseClassTypeInfo
 import org.apache.flink.configuration.ExecutionOptions
-import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.{JsonNode, ObjectMapper}
 import org.apache.flink.streaming.api.datastream.DataStream
 import org.apache.flink.streaming.api.environment.{LocalStreamEnvironment, StreamExecutionEnvironment}
 import org.apache.flink.streaming.api.scala.{StreamExecutionEnvironment => ScalaStreamExecEnv}
@@ -75,10 +64,23 @@ import org.apache.flink.table.types.logical.LogicalType
 import org.apache.flink.table.types.utils.TypeConversions
 import org.apache.flink.table.typeutils.FieldInfoUtils
 import org.apache.flink.types.Row
+
+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.{JsonNode, ObjectMapper}
+
+import org.apache.calcite.avatica.util.TimeUnit
+import org.apache.calcite.rel.RelNode
+import org.apache.calcite.sql.parser.SqlParserPos
+import org.apache.calcite.sql.{SqlExplainLevel, SqlIntervalQualifier}
 import org.junit.Assert.{assertEquals, assertTrue, fail}
 import org.junit.Rule
 import org.junit.rules.{ExpectedException, TemporaryFolder, TestName}
 
+import _root_.java.math.{BigDecimal => JBigDecimal}
+import _root_.java.util
+import java.io.{File, IOException}
+import java.nio.file.{Files, Paths}
+import java.time.Duration
+
 import _root_.scala.collection.JavaConversions._
 import _root_.scala.io.Source
 
@@ -680,7 +682,7 @@ abstract class TableTestUtilBase(test: TableTestBase, isStreamingMode: Boolean)
    * Verify the explain result for the given [[Table]]. See more about [[Table#explain()]].
    */
   def verifyExplain(table: Table): Unit = {
-    doVerifyExplain(table.explain(), needReplaceEstimatedCost = false)
+    doVerifyExplain(table.explain())
   }
 
   /**
@@ -688,9 +690,7 @@ abstract class TableTestUtilBase(test: TableTestBase, isStreamingMode: Boolean)
    * the extra [[ExplainDetail]]s. See more about [[Table#explain()]].
    */
   def verifyExplain(table: Table, extraDetails: ExplainDetail*): Unit = {
-    doVerifyExplain(
-      table.explain(extraDetails: _*),
-      extraDetails.contains(ExplainDetail.ESTIMATED_COST))
+    doVerifyExplain(table.explain(extraDetails: _*), extraDetails: _*)
   }
 
   /**
@@ -725,7 +725,7 @@ abstract class TableTestUtilBase(test: TableTestBase, isStreamingMode: Boolean)
    * See more about [[StatementSet#explain()]].
    */
   def verifyExplain(stmtSet: StatementSet): Unit = {
-    doVerifyExplain(stmtSet.explain(), needReplaceEstimatedCost = false)
+    doVerifyExplain(stmtSet.explain())
   }
 
   /**
@@ -733,9 +733,7 @@ abstract class TableTestUtilBase(test: TableTestBase, isStreamingMode: Boolean)
    * the extra [[ExplainDetail]]s. See more about [[StatementSet#explain()]].
    */
   def verifyExplain(stmtSet: StatementSet, extraDetails: ExplainDetail*): Unit = {
-    doVerifyExplain(
-      stmtSet.explain(extraDetails: _*),
-      extraDetails.contains(ExplainDetail.ESTIMATED_COST))
+    doVerifyExplain(stmtSet.explain(extraDetails: _*), extraDetails: _*)
   }
 
   /**
@@ -919,13 +917,21 @@ abstract class TableTestUtilBase(test: TableTestBase, isStreamingMode: Boolean)
     }
   }
 
-  private def doVerifyExplain(explainResult: String, needReplaceEstimatedCost: Boolean): Unit = {
-    val actual = if (needReplaceEstimatedCost) {
-      replaceEstimatedCost(explainResult)
-    } else {
-      explainResult
+  private def doVerifyExplain(explainResult: String, extraDetails: ExplainDetail*): Unit = {
+    def replace(result: String, explainDetail: ExplainDetail): String = {
+      val replaced = explainDetail match {
+        case ExplainDetail.ESTIMATED_COST => replaceEstimatedCost(result)
+        case ExplainDetail.JSON_EXECUTION_PLAN => TableTestUtil.replaceStreamNodeId(result)
+        case _ => result
+      }
+      replaced
+    }
+    var replacedResult = explainResult
+    extraDetails.foreach {
+      detail =>
+        replacedResult = replace(replacedResult, detail)
     }
-    assertEqualsOrExpand("explain", TableTestUtil.replaceStageId(actual), expand = false)
+    assertEqualsOrExpand("explain", TableTestUtil.replaceStageId(replacedResult), expand = false)
   }
 
   protected def getOptimizedRelPlan(
