diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/calcite/FlinkToRelContext.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/calcite/SqlExprToRexConverterFactory.java
similarity index 58%
rename from flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/calcite/FlinkToRelContext.java
rename to flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/calcite/SqlExprToRexConverterFactory.java
index e64fd202f09..2e3864df32c 100644
--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/calcite/FlinkToRelContext.java
+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/calcite/SqlExprToRexConverterFactory.java
@@ -7,7 +7,7 @@
  * "License"); you may not use this file except in compliance
  * with the License.  You may obtain a copy of the License at
  *
- * http://www.apache.org/licenses/LICENSE-2.0
+ *     http://www.apache.org/licenses/LICENSE-2.0
  *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
@@ -18,26 +18,16 @@
 
 package org.apache.flink.table.planner.calcite;
 
-import org.apache.calcite.plan.RelOptTable;
 import org.apache.calcite.rel.type.RelDataType;
 
 /**
- * A ToRelContext impl that takes the context variables
- * used for sql expression transformation.
+ * Factory to create {@link SqlExprToRexConverter}.
  */
-public interface FlinkToRelContext extends RelOptTable.ToRelContext {
+public interface SqlExprToRexConverterFactory {
 
 	/**
-	 * Creates a new instance of {@link SqlExprToRexConverter} to convert sql statements
-	 * to {@link org.apache.calcite.rex.RexNode}.
-	 *
-	 * <p>See {@link org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase#toRel}
-	 * for details.
+	 * Creates a new instance of {@link SqlExprToRexConverter} to convert SQL expression
+	 * to RexNode.
 	 */
-	SqlExprToRexConverter createSqlExprToRexConverter(RelDataType tableRowType);
-
-	/**
-	 * Creates a new instance of {@link FlinkRelBuilder} to build relational expressions.
-	 */
-	FlinkRelBuilder createRelBuilder();
+	SqlExprToRexConverter create(RelDataType tableRowType);
 }
diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/delegation/PlannerContext.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/delegation/PlannerContext.java
index c50d2d3caac..20823ea8f15 100644
--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/delegation/PlannerContext.java
+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/delegation/PlannerContext.java
@@ -37,6 +37,8 @@ import org.apache.flink.table.planner.calcite.FlinkRelFactories;
 import org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory;
 import org.apache.flink.table.planner.calcite.FlinkTypeFactory;
 import org.apache.flink.table.planner.calcite.FlinkTypeSystem;
+import org.apache.flink.table.planner.calcite.SqlExprToRexConverter;
+import org.apache.flink.table.planner.calcite.SqlExprToRexConverterImpl;
 import org.apache.flink.table.planner.catalog.FunctionCatalogOperatorTable;
 import org.apache.flink.table.planner.codegen.ExpressionReducer;
 import org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable;
@@ -57,6 +59,7 @@ import org.apache.calcite.plan.RelTraitDef;
 import org.apache.calcite.plan.ViewExpanders;
 import org.apache.calcite.plan.volcano.VolcanoPlanner;
 import org.apache.calcite.rel.core.RelFactories;
+import org.apache.calcite.rel.type.RelDataType;
 import org.apache.calcite.rel.type.RelDataTypeSystem;
 import org.apache.calcite.rex.RexBuilder;
 import org.apache.calcite.schema.SchemaPlus;
@@ -71,6 +74,7 @@ import java.util.List;
 
 import static java.util.Arrays.asList;
 import static java.util.Collections.singletonList;
+import static org.apache.flink.util.Preconditions.checkNotNull;
 
 /**
  * Utility class to create {@link org.apache.calcite.tools.RelBuilder} or {@link FrameworkConfig} used to create
@@ -80,6 +84,7 @@ import static java.util.Collections.singletonList;
  */
 @Internal
 public class PlannerContext {
+
 	private final RelDataTypeSystem typeSystem = new FlinkTypeSystem();
 	private final FlinkTypeFactory typeFactory = new FlinkTypeFactory(typeSystem);
 	private final TableConfig tableConfig;
@@ -87,6 +92,7 @@ public class PlannerContext {
 	private final FlinkContext context;
 	private final CalciteSchema rootSchema;
 	private final List<RelTraitDef> traitDefs;
+	private final FrameworkConfig frameworkConfig;
 
 	public PlannerContext(
 			TableConfig tableConfig,
@@ -95,13 +101,19 @@ public class PlannerContext {
 			CalciteSchema rootSchema,
 			List<RelTraitDef> traitDefs) {
 		this.tableConfig = tableConfig;
-		this.context = new FlinkContextImpl(tableConfig, functionCatalog, catalogManager);
+
+		this.context = new FlinkContextImpl(
+				tableConfig,
+				functionCatalog,
+				catalogManager,
+				this::createSqlExprToRexConverter);
+
 		this.rootSchema = rootSchema;
 		this.traitDefs = traitDefs;
 		// Make a framework config to initialize the RelOptCluster instance,
 		// caution that we can only use the attributes that can not be overwrite/configured
 		// by user.
-		final FrameworkConfig frameworkConfig = createFrameworkConfig();
+		this.frameworkConfig = createFrameworkConfig();
 
 		RelOptPlanner planner = new VolcanoPlanner(frameworkConfig.getCostFactory(), frameworkConfig.getContext());
 		planner.setExecutor(frameworkConfig.getExecutor());
@@ -111,6 +123,14 @@ public class PlannerContext {
 		this.cluster = FlinkRelOptClusterFactory.create(planner, new RexBuilder(typeFactory));
 	}
 
+	private SqlExprToRexConverter createSqlExprToRexConverter(RelDataType rowType) {
+		return new SqlExprToRexConverterImpl(
+				checkNotNull(frameworkConfig),
+				checkNotNull(typeFactory),
+				checkNotNull(cluster),
+				rowType);
+	}
+
 	private FrameworkConfig createFrameworkConfig() {
 		return Frameworks.newConfigBuilder()
 			.defaultSchema(rootSchema.plus())
@@ -146,7 +166,8 @@ public class PlannerContext {
 			// We need to overwrite the default scan factory, which does not
 			// expand views. The expandingScanFactory uses the FlinkPlanner to translate a view
 			// into a rel tree, before applying any subsequent rules.
-			Contexts.of(expandingScanFactory(createFlinkPlanner(currentCatalog, currentDatabase)))
+			Contexts.of(expandingScanFactory(
+					createFlinkPlanner(currentCatalog, currentDatabase).createToRelContext()))
 		);
 		return new FlinkRelBuilder(chain, cluster, relOptSchema);
 	}
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/calcite/FlinkContext.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/calcite/FlinkContext.scala
index 06f6998ab6c..f6b19d409ce 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/calcite/FlinkContext.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/calcite/FlinkContext.scala
@@ -43,6 +43,11 @@ trait FlinkContext extends Context {
     */
   def getCatalogManager: CatalogManager
 
+  /**
+    * Gets [[SqlExprToRexConverterFactory]] instance to convert sql expression to rex node.
+    */
+  def getSqlExprToRexConverterFactory: SqlExprToRexConverterFactory
+
   override def unwrap[C](clazz: Class[C]): C = {
     if (clazz.isInstance(this)) clazz.cast(this) else null.asInstanceOf[C]
   }
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/calcite/FlinkContextImpl.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/calcite/FlinkContextImpl.scala
index 99db1eb34e8..6c669e05ae0 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/calcite/FlinkContextImpl.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/calcite/FlinkContextImpl.scala
@@ -24,7 +24,8 @@ import org.apache.flink.table.catalog.{CatalogManager, FunctionCatalog}
 class FlinkContextImpl(
     tableConfig: TableConfig,
     functionCatalog: FunctionCatalog,
-    catalogManager: CatalogManager)
+    catalogManager: CatalogManager,
+    toRexFactory: SqlExprToRexConverterFactory)
   extends FlinkContext {
 
   override def getTableConfig: TableConfig = tableConfig
@@ -32,4 +33,6 @@ class FlinkContextImpl(
   override def getFunctionCatalog: FunctionCatalog = functionCatalog
 
   override def getCatalogManager: CatalogManager = catalogManager
+
+  override def getSqlExprToRexConverterFactory: SqlExprToRexConverterFactory = toRexFactory
 }
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/calcite/FlinkPlannerImpl.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/calcite/FlinkPlannerImpl.scala
index cb2e390524f..0ad903be219 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/calcite/FlinkPlannerImpl.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/calcite/FlinkPlannerImpl.scala
@@ -49,7 +49,7 @@ class FlinkPlannerImpl(
     config: FrameworkConfig,
     catalogReaderSupplier: JFunction[JBoolean, CalciteCatalogReader],
     typeFactory: FlinkTypeFactory,
-    cluster: RelOptCluster) extends FlinkToRelContext {
+    cluster: RelOptCluster) {
 
   val operatorTable: SqlOperatorTable = config.getOperatorTable
   val parser: CalciteParser = new CalciteParser(config.getParserConfig)
@@ -139,7 +139,7 @@ class FlinkPlannerImpl(
     try {
       assert(validatedSqlNode != null)
       val sqlToRelConverter: SqlToRelConverter = new SqlToRelConverter(
-        this,
+        createToRelContext(),
         sqlValidator,
         sqlValidator.getCatalogReader.unwrap(classOf[CalciteCatalogReader]),
         cluster,
@@ -160,38 +160,36 @@ class FlinkPlannerImpl(
   }
 
   /**
-    * Creates a new instance of [[SqlExprToRexConverter]] to convert SQL expression
-    * to RexNode.
+    * Creates a new instance of [[RelOptTable.ToRelContext]] for [[RelOptTable]].
     */
-  def createSqlExprToRexConverter(tableRowType: RelDataType): SqlExprToRexConverter = {
-    new SqlExprToRexConverterImpl(config, typeFactory, cluster, tableRowType)
-  }
+  def createToRelContext(): RelOptTable.ToRelContext = new ToRelContextImpl
 
-  override def getCluster: RelOptCluster = cluster
+  /**
+    * Implements [[RelOptTable.ToRelContext]] interface for [[RelOptTable]] and
+    * [[org.apache.calcite.tools.Planner]].
+    */
+  class ToRelContextImpl extends RelOptTable.ToRelContext {
 
-  override def expandView(
-      rowType: RelDataType,
-      queryString: String,
-      schemaPath: util.List[String],
-      viewPath: util.List[String])
+    override def expandView(
+        rowType: RelDataType,
+        queryString: String,
+        schemaPath: util.List[String],
+        viewPath: util.List[String])
     : RelRoot = {
-    val parsed = parser.parse(queryString)
-    val originalReader = catalogReaderSupplier.apply(false)
-    val readerWithPathAdjusted = new FlinkCalciteCatalogReader(
-      originalReader.getRootSchema,
-      List(schemaPath, schemaPath.subList(0, 1)).asJava,
-      originalReader.getTypeFactory,
-      originalReader.getConfig
-    )
-    val validator = createSqlValidator(readerWithPathAdjusted)
-    val validated = validate(parsed, validator)
-    rel(validated, validator)
-  }
+      val parsed = parser.parse(queryString)
+      val originalReader = catalogReaderSupplier.apply(false)
+      val readerWithPathAdjusted = new FlinkCalciteCatalogReader(
+        originalReader.getRootSchema,
+        List(schemaPath, schemaPath.subList(0, 1)).asJava,
+        originalReader.getTypeFactory,
+        originalReader.getConfig
+      )
+      val validator = createSqlValidator(readerWithPathAdjusted)
+      val validated = validate(parsed, validator)
+      rel(validated, validator)
+    }
 
-  override def createRelBuilder(): FlinkRelBuilder = {
-    sqlToRelConverterConfig.getRelBuilderFactory
-      .create(cluster, null)
-      .asInstanceOf[FlinkRelBuilder]
+    override def getCluster: RelOptCluster = cluster
   }
 }
 
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/optimize/BatchCommonSubGraphBasedOptimizer.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/optimize/BatchCommonSubGraphBasedOptimizer.scala
index 9ac5a5e6538..0b80bdd8dd1 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/optimize/BatchCommonSubGraphBasedOptimizer.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/optimize/BatchCommonSubGraphBasedOptimizer.scala
@@ -20,6 +20,7 @@ package org.apache.flink.table.planner.plan.optimize
 
 import org.apache.flink.table.api.TableConfig
 import org.apache.flink.table.catalog.{CatalogManager, FunctionCatalog}
+import org.apache.flink.table.planner.calcite.{FlinkContext, SqlExprToRexConverterFactory}
 import org.apache.flink.table.planner.delegation.BatchPlanner
 import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSink
 import org.apache.flink.table.planner.plan.optimize.program.{BatchOptimizeContext, FlinkBatchProgram}
@@ -80,12 +81,17 @@ class BatchCommonSubGraphBasedOptimizer(planner: BatchPlanner)
       .getOrElse(FlinkBatchProgram.buildProgram(config.getConfiguration))
     Preconditions.checkNotNull(programs)
 
+    val context = relNode.getCluster.getPlanner.getContext.unwrap(classOf[FlinkContext])
+
     programs.optimize(relNode, new BatchOptimizeContext {
       override def getTableConfig: TableConfig = config
 
       override def getFunctionCatalog: FunctionCatalog = planner.functionCatalog
 
       override def getCatalogManager: CatalogManager = planner.catalogManager
+
+      override def getSqlExprToRexConverterFactory: SqlExprToRexConverterFactory =
+        context.getSqlExprToRexConverterFactory
     })
   }
 
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/optimize/StreamCommonSubGraphBasedOptimizer.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/optimize/StreamCommonSubGraphBasedOptimizer.scala
index a946227cefa..de3b570b1a1 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/optimize/StreamCommonSubGraphBasedOptimizer.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/optimize/StreamCommonSubGraphBasedOptimizer.scala
@@ -21,6 +21,7 @@ package org.apache.flink.table.planner.plan.optimize
 import org.apache.flink.table.api.TableConfig
 import org.apache.flink.table.api.config.ExecutionConfigOptions
 import org.apache.flink.table.catalog.{CatalogManager, FunctionCatalog}
+import org.apache.flink.table.planner.calcite.{FlinkContext, SqlExprToRexConverterFactory}
 import org.apache.flink.table.planner.delegation.StreamPlanner
 import org.apache.flink.table.planner.plan.`trait`.{AccMode, AccModeTraitDef, MiniBatchInterval, MiniBatchIntervalTrait, MiniBatchIntervalTraitDef, MiniBatchMode, UpdateAsRetractionTraitDef}
 import org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery
@@ -164,6 +165,8 @@ class StreamCommonSubGraphBasedOptimizer(planner: StreamPlanner)
       .getOrElse(FlinkStreamProgram.buildProgram(config.getConfiguration))
     Preconditions.checkNotNull(programs)
 
+    val context = relNode.getCluster.getPlanner.getContext.unwrap(classOf[FlinkContext])
+
     programs.optimize(relNode, new StreamOptimizeContext() {
 
       override def getTableConfig: TableConfig = config
@@ -172,6 +175,9 @@ class StreamCommonSubGraphBasedOptimizer(planner: StreamPlanner)
 
       override def getCatalogManager: CatalogManager = planner.catalogManager
 
+      override def getSqlExprToRexConverterFactory: SqlExprToRexConverterFactory =
+        context.getSqlExprToRexConverterFactory
+
       override def getRexBuilder: RexBuilder = planner.getRelBuilder.getRexBuilder
 
       override def updateAsRetraction: Boolean = updatesAsRetraction
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/schema/CatalogSourceTable.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/schema/CatalogSourceTable.scala
index 86af05b7d1d..90ace18ba36 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/schema/CatalogSourceTable.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/schema/CatalogSourceTable.scala
@@ -19,20 +19,22 @@
 package org.apache.flink.table.planner.plan.schema
 
 import org.apache.flink.table.api.TableException
-import org.apache.flink.table.catalog.{CatalogTable, ObjectIdentifier}
+import org.apache.flink.table.catalog.CatalogTable
 import org.apache.flink.table.factories.{TableFactoryUtil, TableSourceFactory}
-import org.apache.flink.table.sources.{StreamTableSource, TableSource, TableSourceValidation}
-import org.apache.calcite.rel.`type`.RelDataType
-import org.apache.flink.table.planner.calcite.FlinkToRelContext
+import org.apache.flink.table.planner.calcite.{FlinkContext, FlinkRelBuilder}
 import org.apache.flink.table.planner.catalog.CatalogSchemaTable
+import org.apache.flink.table.sources.{StreamTableSource, TableSource, TableSourceValidation}
+
 import org.apache.calcite.plan.{RelOptSchema, RelOptTable}
 import org.apache.calcite.rel.RelNode
+import org.apache.calcite.rel.`type`.RelDataType
 import org.apache.calcite.rel.logical.LogicalTableScan
 
+import java.util
 import java.util.{List => JList}
 
-import scala.collection.JavaConverters._
 import scala.collection.JavaConversions._
+import scala.collection.JavaConverters._
 
 /**
   * A [[FlinkPreparingTableBase]] implementation which defines the interfaces required to translate
@@ -86,24 +88,27 @@ class CatalogSourceTable[T](
     // Copy this table with physical scan row type.
     val newRelTable = tableSourceTable.copy(tableSource, physicalFields)
     val scan = LogicalTableScan.create(cluster, newRelTable)
-    val toRelContext = context.asInstanceOf[FlinkToRelContext]
-    val relBuilder = toRelContext.createRelBuilder()
+    val relBuilder = FlinkRelBuilder.of(cluster, getRelOptSchema)
     relBuilder.push(scan)
 
+    val toRexFactory = cluster
+        .getPlanner
+        .getContext
+        .unwrap(classOf[FlinkContext])
+        .getSqlExprToRexConverterFactory
+
     // 2. push computed column project
     val fieldNames = rowType.getFieldNames.asScala
     if (columnExprs.nonEmpty) {
       val fieldExprs = fieldNames
-        .map { name =>
-          if (columnExprs.contains(name)) {
-            columnExprs(name)
-          } else {
-            name
-          }
-        }.toArray
-      val rexNodes = toRelContext
-        .createSqlExprToRexConverter(newRelTable.getRowType)
-        .convertToRexNodes(fieldExprs)
+          .map { name =>
+            if (columnExprs.contains(name)) {
+              columnExprs(name)
+            } else {
+              name
+            }
+          }.toArray
+      val rexNodes = toRexFactory.create(newRelTable.getRowType).convertToRexNodes(fieldExprs)
       relBuilder.projectNamed(rexNodes.toList, fieldNames, true)
     }
 
@@ -115,9 +120,9 @@ class CatalogSourceTable[T](
     if (schemaTable.isStreamingMode && watermarkSpec.nonEmpty) {
       if (TableSourceValidation.hasRowtimeAttribute(tableSource)) {
         throw new TableException(
-          "If watermark is specified in DDL, the underlying TableSource of connector shouldn't" +
-            " return an non-empty list of RowtimeAttributeDescriptor" +
-            " via DefinedRowtimeAttributes interface.")
+          "If watermark is specified in DDL, the underlying TableSource of connector" +
+              " shouldn't return an non-empty list of RowtimeAttributeDescriptor" +
+              " via DefinedRowtimeAttributes interface.")
       }
       val rowtime = watermarkSpec.get.getRowtimeAttribute
       if (rowtime.contains(".")) {
@@ -125,9 +130,9 @@ class CatalogSourceTable[T](
           s"Nested field '$rowtime' as rowtime attribute is not supported right now.")
       }
       val rowtimeIndex = fieldNames.indexOf(rowtime)
-      val watermarkRexNode = toRelContext
-        .createSqlExprToRexConverter(rowType)
-        .convertToRexNode(watermarkSpec.get.getWatermarkExpr)
+      val watermarkRexNode = toRexFactory
+          .create(rowType)
+          .convertToRexNode(watermarkSpec.get.getWatermarkExpr)
       relBuilder.watermark(rowtimeIndex, watermarkRexNode)
     }
 
@@ -156,4 +161,11 @@ class CatalogSourceTable[T](
     }
     tableSource
   }
+
+  override protected def explainSourceAsString(ts: TableSource[_]): JList[String] = {
+    val ret = new util.ArrayList[String](super.explainSourceAsString(ts))
+    // Add class name to distinguish TableSourceTable.
+    ret.add("class: " + classOf[CatalogSourceTable[_]].getSimpleName)
+    ret
+  }
 }
diff --git a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/TableScanTest.xml b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/TableScanTest.xml
index 5ec81662778..39a67ca51e2 100644
--- a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/TableScanTest.xml
+++ b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/TableScanTest.xml
@@ -56,64 +56,98 @@ TableSourceScan(table=[[default_catalog, default_database, MyTable, source: [Tes
 
     </Resource>
   </TestCase>
-  <TestCase name="testDDLWithComputedColumn">
+
+  <TestCase name="testDDLWithWatermarkComputedColumn">
     <Resource name="sql">
-      <![CDATA[SELECT * FROM t1]]>
+      <![CDATA[SELECT * FROM c_watermark_t]]>
 
     </Resource>
     <Resource name="planBefore">
       <![CDATA[
 LogicalProject(a=[$0], b=[$1], c=[+($0, 1)], d=[TO_TIMESTAMP($1)], e=[my_udf($0)])
-+- LogicalTableScan(table=[[default_catalog, default_database, t1, source: [CollectionTableSource(a, b)]]])
++- LogicalTableScan(table=[[default_catalog, default_database, c_watermark_t, source: [CollectionTableSource(a, b)]]])
 ]]>
 
     </Resource>
     <Resource name="planAfter">
       <![CDATA[
 Calc(select=[a, b, +(a, 1) AS c, TO_TIMESTAMP(b) AS d, my_udf(a) AS e])
-+- TableSourceScan(table=[[default_catalog, default_database, t1, source: [CollectionTableSource(a, b)]]], fields=[a, b])
++- TableSourceScan(table=[[default_catalog, default_database, c_watermark_t, source: [CollectionTableSource(a, b)]]], fields=[a, b])
 ]]>
 
     </Resource>
   </TestCase>
-  <TestCase name="testDDLWithWatermarkComputedColumn">
+  <TestCase name="testDDLWithComputedColumn">
     <Resource name="sql">
-      <![CDATA[SELECT * FROM t1]]>
+      <![CDATA[SELECT * FROM computed_column_t]]>
 
     </Resource>
     <Resource name="planBefore">
       <![CDATA[
 LogicalProject(a=[$0], b=[$1], c=[+($0, 1)], d=[TO_TIMESTAMP($1)], e=[my_udf($0)])
-+- LogicalTableScan(table=[[default_catalog, default_database, t1, source: [CollectionTableSource(a, b)]]])
++- LogicalTableScan(table=[[default_catalog, default_database, computed_column_t, source: [CollectionTableSource(a, b)]]])
 ]]>
 
     </Resource>
     <Resource name="planAfter">
       <![CDATA[
 Calc(select=[a, b, +(a, 1) AS c, TO_TIMESTAMP(b) AS d, my_udf(a) AS e])
-+- TableSourceScan(table=[[default_catalog, default_database, t1, source: [CollectionTableSource(a, b)]]], fields=[a, b])
++- TableSourceScan(table=[[default_catalog, default_database, computed_column_t, source: [CollectionTableSource(a, b)]]], fields=[a, b])
 ]]>
 
     </Resource>
   </TestCase>
-  <TestCase name="testDDLWithComputedColumn">
-    <Resource name="sql">
-      <![CDATA[SELECT * FROM t1]]>
-
+  <TestCase name="testTableApiScanWithDDL">
+    <Resource name="planBefore">
+      <![CDATA[
+LogicalTableScan(table=[[default_catalog, default_database, t1, source: [CollectionTableSource(a, b)], class: CatalogSourceTable]])
+]]>
+    </Resource>
+    <Resource name="planAfter">
+      <![CDATA[
+TableSourceScan(table=[[default_catalog, default_database, t1, source: [CollectionTableSource(a, b)]]], fields=[a, b])
+]]>
     </Resource>
+  </TestCase>
+
+  <TestCase name="testTableApiScanWithTemporaryTable">
     <Resource name="planBefore">
       <![CDATA[
-LogicalProject(a=[$0], b=[$1], c=[+($0, 1)], d=[TO_TIMESTAMP($1)], e=[my_udf($0)])
-+- LogicalTableScan(table=[[default_catalog, default_database, t1, source: [CollectionTableSource(a, b)]]])
+LogicalTableScan(table=[[default_catalog, default_database, t1, source: [CsvTableSource(read fields: word)], class: CatalogSourceTable]])
 ]]>
+    </Resource>
+    <Resource name="planAfter">
+      <![CDATA[
+TableSourceScan(table=[[default_catalog, default_database, t1, source: [CsvTableSource(read fields: word)]]], fields=[word])
+]]>
+    </Resource>
+  </TestCase>
 
+  <TestCase name="testTableApiScanWithWatermark">
+    <Resource name="planBefore">
+      <![CDATA[
+LogicalTableScan(table=[[default_catalog, default_database, c_watermark_t, source: [CollectionTableSource(a, b)], class: CatalogSourceTable]])
+]]>
     </Resource>
     <Resource name="planAfter">
       <![CDATA[
 Calc(select=[a, b, +(a, 1) AS c, TO_TIMESTAMP(b) AS d, my_udf(a) AS e])
-+- TableSourceScan(table=[[default_catalog, default_database, t1, source: [CollectionTableSource(a, b)]]], fields=[a, b])
++- TableSourceScan(table=[[default_catalog, default_database, c_watermark_t, source: [CollectionTableSource(a, b)]]], fields=[a, b])
 ]]>
+    </Resource>
+  </TestCase>
 
+  <TestCase name="testTableApiScanWithComputedColumn">
+    <Resource name="planBefore">
+      <![CDATA[
+LogicalTableScan(table=[[default_catalog, default_database, computed_column_t, source: [CollectionTableSource(a, b)], class: CatalogSourceTable]])
+]]>
+    </Resource>
+    <Resource name="planAfter">
+      <![CDATA[
+Calc(select=[a, b, +(a, 1) AS c, TO_TIMESTAMP(b) AS d, my_udf(a) AS e])
++- TableSourceScan(table=[[default_catalog, default_database, computed_column_t, source: [CollectionTableSource(a, b)]]], fields=[a, b])
+]]>
     </Resource>
   </TestCase>
 </Root>
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/codegen/WatermarkGeneratorCodeGenTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/codegen/WatermarkGeneratorCodeGenTest.scala
index 0cf06b5b026..308406e9274 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/codegen/WatermarkGeneratorCodeGenTest.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/codegen/WatermarkGeneratorCodeGenTest.scala
@@ -20,7 +20,6 @@ package org.apache.flink.table.planner.codegen
 
 import java.lang.{Integer => JInt, Long => JLong}
 import java.util.Collections
-
 import org.apache.calcite.jdbc.CalciteSchemaBuilder.asRootSchema
 import org.apache.calcite.plan.ConventionTraitDef
 import org.apache.flink.configuration.Configuration
@@ -29,13 +28,14 @@ import org.apache.flink.table.api.TableConfig
 import org.apache.flink.table.catalog.{CatalogManager, FunctionCatalog, ObjectIdentifier}
 import org.apache.flink.table.dataformat.{GenericRow, SqlTimestamp}
 import org.apache.flink.table.module.ModuleManager
-import org.apache.flink.table.planner.calcite.{FlinkPlannerImpl, FlinkTypeFactory}
+import org.apache.flink.table.planner.calcite.{FlinkContext, FlinkPlannerImpl, FlinkTypeFactory}
 import org.apache.flink.table.planner.catalog.CatalogManagerCalciteSchema
 import org.apache.flink.table.planner.delegation.PlannerContext
 import org.apache.flink.table.planner.runtime.utils.JavaUserDefinedScalarFunctions.JavaFunc5
 import org.apache.flink.table.runtime.generated.WatermarkGenerator
 import org.apache.flink.table.types.logical.{IntType, TimestampType}
 import org.apache.flink.table.utils.CatalogManagerMocks
+
 import org.junit.Assert.{assertEquals, assertTrue}
 import org.junit.Test
 
@@ -132,7 +132,13 @@ class WatermarkGeneratorCodeGenTest {
         new IntType()
       ))
     val rowType = FlinkTypeFactory.toLogicalRowType(tableRowType)
-    val converter = planner.createSqlExprToRexConverter(tableRowType)
+    val converter = planner.createToRelContext()
+        .getCluster
+        .getPlanner
+        .getContext
+        .unwrap(classOf[FlinkContext])
+        .getSqlExprToRexConverterFactory
+        .create(tableRowType)
     val rexNode = converter.convertToRexNode(expr)
     val generated = WatermarkGeneratorCodeGenerator
       .generateWatermarkGenerator(new TableConfig(), rowType, rexNode)
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/batch/sql/TableScanTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/batch/sql/TableScanTest.scala
index 715c03de960..6a8039cc356 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/batch/sql/TableScanTest.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/batch/sql/TableScanTest.scala
@@ -19,15 +19,52 @@
 package org.apache.flink.table.planner.plan.batch.sql
 
 import org.apache.flink.api.scala._
+import org.apache.flink.table.api.DataTypes
 import org.apache.flink.table.api.scala._
+import org.apache.flink.table.descriptors.{FileSystem, OldCsv, Schema}
 import org.apache.flink.table.planner.expressions.utils.Func0
 import org.apache.flink.table.planner.utils.TableTestBase
-import org.junit.Test
+
+import org.junit.{Before, Test}
 
 class TableScanTest extends TableTestBase {
 
   private val util = batchTestUtil()
 
+  @Before
+  def before(): Unit = {
+    util.tableEnv.registerFunction("my_udf", Func0)
+
+    util.addTable(
+      s"""
+         |create table computed_column_t(
+         |  a int,
+         |  b varchar,
+         |  c as a + 1,
+         |  d as to_timestamp(b),
+         |  e as my_udf(a)
+         |) with (
+         |  'connector' = 'COLLECTION',
+         |  'is-bounded' = 'true'
+         |)
+       """.stripMargin)
+
+    util.addTable(
+      s"""
+         |create table c_watermark_t(
+         |  a int,
+         |  b varchar,
+         |  c as a + 1,
+         |  d as to_timestamp(b),
+         |  e as my_udf(a),
+         |  WATERMARK FOR d AS d - INTERVAL '0.001' SECOND
+         |) with (
+         |  'connector' = 'COLLECTION',
+         |  'is-bounded' = 'true'
+         |)
+       """.stripMargin)
+  }
+
   @Test
   def testTableSourceScan(): Unit = {
     util.addTableSource[(Int, Long, String)]("MyTable", 'a, 'b, 'c)
@@ -53,43 +90,45 @@ class TableScanTest extends TableTestBase {
 
   @Test
   def testDDLWithComputedColumn(): Unit = {
-    // Create table with field as atom expression.
-    util.tableEnv.registerFunction("my_udf", Func0)
-    util.addTable(
-      s"""
-         |create table t1(
-         |  a int,
-         |  b varchar,
-         |  c as a + 1,
-         |  d as to_timestamp(b),
-         |  e as my_udf(a)
-         |) with (
-         |  'connector' = 'COLLECTION',
-         |  'is-bounded' = 'true'
-         |)
-       """.stripMargin)
-    util.verifyPlan("SELECT * FROM t1")
+    util.verifyPlan("SELECT * FROM computed_column_t")
   }
 
-
   @Test
   def testDDLWithWatermarkComputedColumn(): Unit = {
-    // Create table with field as atom expression.
-    util.tableEnv.registerFunction("my_udf", Func0)
+    util.verifyPlan("SELECT * FROM c_watermark_t")
+  }
+
+  @Test
+  def testTableApiScanWithComputedColumn(): Unit = {
+    util.verifyPlan(util.tableEnv.from("computed_column_t"))
+  }
+
+  @Test
+  def testTableApiScanWithWatermark(): Unit = {
+    util.verifyPlan(util.tableEnv.from("c_watermark_t"))
+  }
+
+  @Test
+  def testTableApiScanWithDDL(): Unit = {
     util.addTable(
       s"""
          |create table t1(
          |  a int,
-         |  b varchar,
-         |  c as a + 1,
-         |  d as to_timestamp(b),
-         |  e as my_udf(a),
-         |  WATERMARK FOR d AS d - INTERVAL '0.001' SECOND
+         |  b varchar
          |) with (
          |  'connector' = 'COLLECTION',
          |  'is-bounded' = 'true'
          |)
        """.stripMargin)
-    util.verifyPlan("SELECT * FROM t1")
+    util.verifyPlan(util.tableEnv.from("t1"))
+  }
+
+  @Test
+  def testTableApiScanWithTemporaryTable(): Unit = {
+    util.tableEnv.connect(new FileSystem().path(tempFolder.newFile.getPath))
+        .withFormat(new OldCsv())
+        .withSchema(new Schema().field("word", DataTypes.STRING()))
+        .createTemporaryTable("t1")
+    util.verifyPlan(util.tableEnv.from("t1"))
   }
 }
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/AggCallSelectivityEstimatorTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/AggCallSelectivityEstimatorTest.scala
index 2316bd8d5d5..da81b360db6 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/AggCallSelectivityEstimatorTest.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/AggCallSelectivityEstimatorTest.scala
@@ -87,7 +87,7 @@ class AggCallSelectivityEstimatorTest {
     val moduleManager = mock(classOf[ModuleManager])
     val config = new TableConfig
     val functionCatalog = new FunctionCatalog(config, catalogManager, moduleManager)
-    val context = new FlinkContextImpl(new TableConfig, functionCatalog, catalogManager)
+    val context = new FlinkContextImpl(new TableConfig, functionCatalog, catalogManager, null)
     when(tableScan, "getCluster").thenReturn(cluster)
     when(cluster, "getRexBuilder").thenReturn(rexBuilder)
     when(cluster, "getTypeFactory").thenReturn(typeFactory)
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/SelectivityEstimatorTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/SelectivityEstimatorTest.scala
index d8fab41b8af..7a7e034c40b 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/SelectivityEstimatorTest.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/SelectivityEstimatorTest.scala
@@ -89,7 +89,8 @@ class SelectivityEstimatorTest {
     val catalogManager = CatalogManagerMocks.createEmptyCatalogManager()
     val moduleManager = mock(classOf[ModuleManager])
     val functionCatalog = new FunctionCatalog(tableConfig, catalogManager, moduleManager)
-    val context: FlinkContext = new FlinkContextImpl(tableConfig, functionCatalog, catalogManager)
+    val context: FlinkContext = new FlinkContextImpl(
+      tableConfig, functionCatalog, catalogManager, null)
     when(tableScan, "getCluster").thenReturn(cluster)
     when(cluster, "getRexBuilder").thenReturn(rexBuilder)
     when(cluster, "getPlanner").thenReturn(planner)
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/utils/TableTestBase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/utils/TableTestBase.scala
index 94cc114075e..b89088c7303 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/utils/TableTestBase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/utils/TableTestBase.scala
@@ -66,10 +66,9 @@ import org.apache.calcite.rel.RelNode
 import org.apache.calcite.sql.parser.SqlParserPos
 import org.apache.calcite.sql.{SqlExplainLevel, SqlIntervalQualifier}
 import org.apache.commons.lang3.SystemUtils
-
 import org.junit.Assert.{assertEquals, assertTrue}
 import org.junit.Rule
-import org.junit.rules.{ExpectedException, TestName}
+import org.junit.rules.{ExpectedException, TemporaryFolder, TestName}
 
 import _root_.java.math.{BigDecimal => JBigDecimal}
 import _root_.java.util
@@ -88,6 +87,11 @@ abstract class TableTestBase {
   // used for get test case method name
   val testName: TestName = new TestName
 
+  val _tempFolder = new TemporaryFolder
+
+  @Rule
+  def tempFolder: TemporaryFolder = _tempFolder
+
   @Rule
   def thrown: ExpectedException = expectedException
 
