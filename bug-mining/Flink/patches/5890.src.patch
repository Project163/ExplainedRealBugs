diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java b/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java
index 7ccfce7b596..d4648acb6aa 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java
@@ -869,7 +869,13 @@ public class CheckpointCoordinator {
                 getCheckpointException(
                         CheckpointFailureReason.TRIGGER_CHECKPOINT_FAILURE, throwable);
         onCompletionPromise.completeExceptionally(checkpointException);
-        onTriggerFailure((PendingCheckpoint) null, checkpointException);
+        onTriggerFailure((PendingCheckpoint) null, onCompletionPromise.props, checkpointException);
+    }
+
+    private void onTriggerFailure(PendingCheckpoint checkpoint, Throwable throwable) {
+        checkArgument(checkpoint != null, "Pending checkpoint can not be null.");
+
+        onTriggerFailure(checkpoint, checkpoint.getProps(), throwable);
     }
 
     /**
@@ -879,7 +885,10 @@ public class CheckpointCoordinator {
      *     prematurely without a proper initialization.
      * @param throwable the reason of trigger failure
      */
-    private void onTriggerFailure(@Nullable PendingCheckpoint checkpoint, Throwable throwable) {
+    private void onTriggerFailure(
+            @Nullable PendingCheckpoint checkpoint,
+            CheckpointProperties checkpointProperties,
+            Throwable throwable) {
         // beautify the stack trace a bit
         throwable = ExceptionUtils.stripCompletionException(throwable);
 
@@ -887,6 +896,10 @@ public class CheckpointCoordinator {
             coordinatorsToCheckpoint.forEach(
                     OperatorCoordinatorCheckpointContext::abortCurrentTriggering);
 
+            final CheckpointException cause =
+                    getCheckpointException(
+                            CheckpointFailureReason.TRIGGER_CHECKPOINT_FAILURE, throwable);
+
             if (checkpoint != null && !checkpoint.isDisposed()) {
                 int numUnsuccessful = numUnsuccessfulCheckpointsTriggers.incrementAndGet();
                 LOG.warn(
@@ -895,9 +908,7 @@ public class CheckpointCoordinator {
                         job,
                         numUnsuccessful,
                         throwable);
-                final CheckpointException cause =
-                        getCheckpointException(
-                                CheckpointFailureReason.TRIGGER_CHECKPOINT_FAILURE, throwable);
+
                 synchronized (lock) {
                     abortPendingCheckpoint(checkpoint, cause);
                 }
@@ -906,6 +917,9 @@ public class CheckpointCoordinator {
                         "Failed to trigger checkpoint for job {} because {}",
                         job,
                         throwable.getMessage());
+
+                failureManager.handleCheckpointException(
+                        checkpoint, checkpointProperties, cause, null);
             }
         } finally {
             isTriggering = false;
@@ -1933,16 +1947,11 @@ public class CheckpointCoordinator {
                         executor,
                         getStatsCallback(pendingCheckpoint));
 
-                if (pendingCheckpoint.getProps().isSavepoint()
-                        && pendingCheckpoint.getProps().isSynchronous()) {
-                    failureManager.handleSynchronousSavepointFailure(exception);
-                } else if (executionAttemptID != null) {
-                    failureManager.handleTaskLevelCheckpointException(
-                            exception, pendingCheckpoint.getCheckpointId(), executionAttemptID);
-                } else {
-                    failureManager.handleJobLevelCheckpointException(
-                            exception, pendingCheckpoint.getCheckpointId());
-                }
+                failureManager.handleCheckpointException(
+                        pendingCheckpoint,
+                        pendingCheckpoint.getProps(),
+                        exception,
+                        executionAttemptID);
             } finally {
                 sendAbortedMessages(
                         pendingCheckpoint.getCheckpointPlan().getTasksToCommitTo(),
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointFailureManager.java b/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointFailureManager.java
index ef0c99d7193..8ccccc676fd 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointFailureManager.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointFailureManager.java
@@ -21,6 +21,8 @@ import org.apache.flink.runtime.executiongraph.ExecutionAttemptID;
 import org.apache.flink.util.ExceptionUtils;
 import org.apache.flink.util.FlinkRuntimeException;
 
+import javax.annotation.Nullable;
+
 import java.util.Set;
 import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.atomic.AtomicInteger;
@@ -35,6 +37,7 @@ public class CheckpointFailureManager {
     public static final int UNLIMITED_TOLERABLE_FAILURE_NUMBER = Integer.MAX_VALUE;
     public static final String EXCEEDED_CHECKPOINT_TOLERABLE_FAILURE_MESSAGE =
             "Exceeded checkpoint tolerable failure threshold.";
+    private static final int UNKNOWN_CHECKPOINT_ID = -1;
 
     private final int tolerableCpFailureNumber;
     private final FailJobCallback failureCallback;
@@ -53,6 +56,56 @@ public class CheckpointFailureManager {
         this.countedCheckpointIds = ConcurrentHashMap.newKeySet();
     }
 
+    /**
+     * Failures on JM:
+     *
+     * <ul>
+     *   <li>all checkpoints - go against failure counter.
+     *   <li>any savepoints - donâ€™t do anything, manual action, the failover will not help anyway.
+     * </ul>
+     *
+     * <p>Failures on TM:
+     *
+     * <ul>
+     *   <li>all checkpoints - go against failure counter (failover might help and we want to notify
+     *       users).
+     *   <li>sync savepoints - we must always fail, otherwise we risk deadlock when the job
+     *       cancelation waiting for finishing savepoint which never happens.
+     *   <li>non sync savepoints - go against failure counter (failover might help solve the
+     *       problem).
+     * </ul>
+     *
+     * @param pendingCheckpoint the failed checkpoint if it was initialized already.
+     * @param checkpointProperties the checkpoint properties in order to determinate which handle
+     *     strategy can be used.
+     * @param exception the checkpoint exception.
+     * @param executionAttemptID the execution attempt id, as a safe guard.
+     */
+    public void handleCheckpointException(
+            @Nullable PendingCheckpoint pendingCheckpoint,
+            CheckpointProperties checkpointProperties,
+            CheckpointException exception,
+            @Nullable ExecutionAttemptID executionAttemptID) {
+        if (isJobManagerFailure(exception, executionAttemptID)) {
+            handleJobLevelCheckpointException(
+                    checkpointProperties,
+                    exception,
+                    pendingCheckpoint == null
+                            ? UNKNOWN_CHECKPOINT_ID
+                            : pendingCheckpoint.getCheckpointID());
+        } else {
+            handleTaskLevelCheckpointException(
+                    checkNotNull(pendingCheckpoint), exception, checkNotNull(executionAttemptID));
+        }
+    }
+
+    private boolean isJobManagerFailure(
+            CheckpointException exception, @Nullable ExecutionAttemptID executionAttemptID) {
+        // TODO: Try to get rid of checking nullability of executionAttemptID because false value of
+        // isPreFlightFailure should guarantee that executionAttemptID is always not null.
+        return isPreFlightFailure(exception) || executionAttemptID == null;
+    }
+
     /**
      * Handle job level checkpoint exception with a handler callback.
      *
@@ -62,36 +115,45 @@ public class CheckpointFailureManager {
      *     the failure happens before the checkpoint id generation. In this case, it will be
      *     specified a negative latest generated checkpoint id as a special flag.
      */
-    public void handleJobLevelCheckpointException(
-            CheckpointException exception, long checkpointId) {
-        handleCheckpointException(exception, checkpointId, failureCallback::failJob);
+    void handleJobLevelCheckpointException(
+            CheckpointProperties checkpointProperties,
+            CheckpointException exception,
+            long checkpointId) {
+        if (!checkpointProperties.isSavepoint()) {
+            checkFailureAgainstCounter(exception, checkpointId, failureCallback::failJob);
+        }
     }
 
     /**
      * Handle task level checkpoint exception with a handler callback.
      *
-     * @param exception the checkpoint exception.
-     * @param checkpointId the failed checkpoint id used to count the continuous failure number
+     * @param pendingCheckpoint the failed checkpoint used to count the continuous failure number
      *     based on checkpoint id sequence. In trigger phase, we may not get the checkpoint id when
      *     the failure happens before the checkpoint id generation. In this case, it will be
      *     specified a negative latest generated checkpoint id as a special flag.
+     * @param exception the checkpoint exception.
      * @param executionAttemptID the execution attempt id, as a safe guard.
      */
-    public void handleTaskLevelCheckpointException(
+    void handleTaskLevelCheckpointException(
+            PendingCheckpoint pendingCheckpoint,
             CheckpointException exception,
-            long checkpointId,
             ExecutionAttemptID executionAttemptID) {
-        handleCheckpointException(
-                exception,
-                checkpointId,
-                e -> failureCallback.failJobDueToTaskFailure(e, executionAttemptID));
+        CheckpointProperties checkpointProps = pendingCheckpoint.getProps();
+        if (checkpointProps.isSavepoint() && checkpointProps.isSynchronous()) {
+            failureCallback.failJob(exception);
+        } else {
+            checkFailureAgainstCounter(
+                    exception,
+                    pendingCheckpoint.getCheckpointID(),
+                    e -> failureCallback.failJobDueToTaskFailure(e, executionAttemptID));
+        }
     }
 
-    private void handleCheckpointException(
+    private void checkFailureAgainstCounter(
             CheckpointException exception,
             long checkpointId,
             Consumer<FlinkRuntimeException> errorHandler) {
-        if (checkpointId > lastSucceededCheckpointId) {
+        if (checkpointId == UNKNOWN_CHECKPOINT_ID || checkpointId > lastSucceededCheckpointId) {
             checkFailureCounter(exception, checkpointId);
             if (continuousFailureCounter.get() > tolerableCpFailureNumber) {
                 clearCount();
@@ -140,7 +202,8 @@ public class CheckpointFailureManager {
             case CHECKPOINT_DECLINED:
             case CHECKPOINT_EXPIRED:
                 // we should make sure one checkpoint only be counted once
-                if (countedCheckpointIds.add(checkpointId)) {
+                if (checkpointId == UNKNOWN_CHECKPOINT_ID
+                        || countedCheckpointIds.add(checkpointId)) {
                     continuousFailureCounter.incrementAndGet();
                 }
 
@@ -170,21 +233,6 @@ public class CheckpointFailureManager {
         countedCheckpointIds.clear();
     }
 
-    /**
-     * Fails the whole job graph in case an in-progress synchronous savepoint is discarded.
-     *
-     * <p>If the checkpoint was cancelled at the checkpoint coordinator, i.e. before the synchronous
-     * savepoint barrier was sent to the tasks, then we do not cancel the job as we do not risk
-     * having a deadlock.
-     *
-     * @param cause The reason why the job is cancelled.
-     */
-    void handleSynchronousSavepointFailure(final Throwable cause) {
-        if (!isPreFlightFailure(cause)) {
-            failureCallback.failJob(cause);
-        }
-    }
-
     private static boolean isPreFlightFailure(final Throwable cause) {
         return ExceptionUtils.findThrowable(cause, CheckpointException.class)
                 .map(CheckpointException::getCheckpointFailureReason)
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinatorTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinatorTest.java
index c6b99c35c85..e1a87e3b803 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinatorTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinatorTest.java
@@ -676,13 +676,18 @@ public class CheckpointCoordinatorTest extends TestLogger {
     @Test
     public void testTriggerCheckpointAfterIOException() throws Exception {
         // given: Checkpoint coordinator which fails on initializeLocationForCheckpoint.
+        TestFailJobCallback failureCallback = new TestFailJobCallback();
         CheckpointCoordinator checkpointCoordinator =
                 new CheckpointCoordinatorBuilder()
+                        .setFailureManager(new CheckpointFailureManager(0, failureCallback))
                         .setCheckpointStorage(new IOExceptionCheckpointStorage())
                         .setTimer(manuallyTriggeredScheduledExecutor)
                         .build();
         // when: The checkpoint is triggered.
         testTriggerCheckpoint(checkpointCoordinator, IO_EXCEPTION);
+
+        // then: Failure manager should fail the job.
+        assertEquals(1, failureCallback.getInvokeCounter());
     }
 
     @Test
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/CheckpointFailureManagerTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/CheckpointFailureManagerTest.java
index b29b1cc5968..8ed9b037ade 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/CheckpointFailureManagerTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/CheckpointFailureManagerTest.java
@@ -17,82 +17,106 @@
 
 package org.apache.flink.runtime.checkpoint;
 
+import org.apache.flink.runtime.JobException;
 import org.apache.flink.runtime.executiongraph.ExecutionAttemptID;
 import org.apache.flink.util.TestLogger;
 
 import org.junit.Test;
 
+import java.io.IOException;
+
 import static org.apache.flink.runtime.checkpoint.CheckpointFailureReason.CHECKPOINT_EXPIRED;
+import static org.apache.flink.runtime.checkpoint.CheckpointProperties.forCheckpoint;
+import static org.apache.flink.runtime.checkpoint.CheckpointRetentionPolicy.NEVER_RETAIN_AFTER_TERMINATION;
 import static org.junit.Assert.assertEquals;
 
 /** Tests for the checkpoint failure manager. */
 public class CheckpointFailureManagerTest extends TestLogger {
 
     @Test
-    public void testIgnoresPastCheckpoints() {
+    public void testIgnoresPastCheckpoints() throws IOException, JobException {
         TestFailJobCallback callback = new TestFailJobCallback();
         CheckpointFailureManager failureManager = new CheckpointFailureManager(2, callback);
+        CheckpointProperties checkpointProperties = forCheckpoint(NEVER_RETAIN_AFTER_TERMINATION);
+
         failureManager.handleJobLevelCheckpointException(
-                new CheckpointException(CHECKPOINT_EXPIRED), 1L);
+                checkpointProperties, new CheckpointException(CHECKPOINT_EXPIRED), 1L);
         failureManager.handleJobLevelCheckpointException(
-                new CheckpointException(CHECKPOINT_EXPIRED), 2L);
+                checkpointProperties, new CheckpointException(CHECKPOINT_EXPIRED), 2L);
         failureManager.handleCheckpointSuccess(2L);
         failureManager.handleJobLevelCheckpointException(
-                new CheckpointException(CHECKPOINT_EXPIRED), 1L);
+                checkpointProperties, new CheckpointException(CHECKPOINT_EXPIRED), 1L);
         failureManager.handleJobLevelCheckpointException(
-                new CheckpointException(CHECKPOINT_EXPIRED), 3L);
+                checkpointProperties, new CheckpointException(CHECKPOINT_EXPIRED), 3L);
         failureManager.handleJobLevelCheckpointException(
-                new CheckpointException(CHECKPOINT_EXPIRED), 4L);
+                checkpointProperties, new CheckpointException(CHECKPOINT_EXPIRED), 4L);
         assertEquals(0, callback.getInvokeCounter());
     }
 
     @Test
-    public void testContinuousFailure() {
+    public void testContinuousFailure() throws IOException, JobException {
         TestFailJobCallback callback = new TestFailJobCallback();
         CheckpointFailureManager failureManager = new CheckpointFailureManager(2, callback);
-
+        CheckpointProperties checkpointProperties = forCheckpoint(NEVER_RETAIN_AFTER_TERMINATION);
         failureManager.handleJobLevelCheckpointException(
-                new CheckpointException(CheckpointFailureReason.CHECKPOINT_DECLINED), 1);
+                checkpointProperties,
+                new CheckpointException(CheckpointFailureReason.CHECKPOINT_DECLINED),
+                1);
         failureManager.handleJobLevelCheckpointException(
-                new CheckpointException(CheckpointFailureReason.CHECKPOINT_DECLINED), 2);
+                checkpointProperties,
+                new CheckpointException(CheckpointFailureReason.CHECKPOINT_DECLINED),
+                2);
 
         // ignore this
         failureManager.handleJobLevelCheckpointException(
-                new CheckpointException(CheckpointFailureReason.JOB_FAILOVER_REGION), 3);
+                checkpointProperties,
+                new CheckpointException(CheckpointFailureReason.JOB_FAILOVER_REGION),
+                3);
 
         failureManager.handleJobLevelCheckpointException(
-                new CheckpointException(CheckpointFailureReason.CHECKPOINT_DECLINED), 4);
+                checkpointProperties,
+                new CheckpointException(CheckpointFailureReason.CHECKPOINT_DECLINED),
+                4);
         assertEquals(1, callback.getInvokeCounter());
     }
 
     @Test
-    public void testBreakContinuousFailure() {
+    public void testBreakContinuousFailure() throws IOException, JobException {
         TestFailJobCallback callback = new TestFailJobCallback();
         CheckpointFailureManager failureManager = new CheckpointFailureManager(2, callback);
+        CheckpointProperties checkpointProperties = forCheckpoint(NEVER_RETAIN_AFTER_TERMINATION);
 
         failureManager.handleJobLevelCheckpointException(
-                new CheckpointException(CheckpointFailureReason.IO_EXCEPTION), 1);
+                checkpointProperties,
+                new CheckpointException(CheckpointFailureReason.IO_EXCEPTION),
+                1);
         failureManager.handleJobLevelCheckpointException(
-                new CheckpointException(CheckpointFailureReason.CHECKPOINT_DECLINED), 2);
+                checkpointProperties,
+                new CheckpointException(CheckpointFailureReason.CHECKPOINT_DECLINED),
+                2);
 
         // ignore this
         failureManager.handleJobLevelCheckpointException(
-                new CheckpointException(CheckpointFailureReason.JOB_FAILOVER_REGION), 3);
+                checkpointProperties,
+                new CheckpointException(CheckpointFailureReason.JOB_FAILOVER_REGION),
+                3);
 
         // reset
         failureManager.handleCheckpointSuccess(4);
 
         failureManager.handleJobLevelCheckpointException(
-                new CheckpointException(CHECKPOINT_EXPIRED), 5);
+                checkpointProperties, new CheckpointException(CHECKPOINT_EXPIRED), 5);
         assertEquals(0, callback.getInvokeCounter());
     }
 
     @Test
-    public void testTotalCountValue() {
+    public void testTotalCountValue() throws IOException, JobException {
         TestFailJobCallback callback = new TestFailJobCallback();
+        CheckpointProperties checkpointProperties = forCheckpoint(NEVER_RETAIN_AFTER_TERMINATION);
         CheckpointFailureManager failureManager = new CheckpointFailureManager(0, callback);
         for (CheckpointFailureReason reason : CheckpointFailureReason.values()) {
-            failureManager.handleJobLevelCheckpointException(new CheckpointException(reason), -1);
+            failureManager.handleJobLevelCheckpointException(
+                    checkpointProperties, new CheckpointException(reason), -2);
         }
 
         // IO_EXCEPTION, CHECKPOINT_DECLINED, CHECKPOINT_EXPIRED and CHECKPOINT_ASYNC_EXCEPTION
@@ -100,22 +124,32 @@ public class CheckpointFailureManagerTest extends TestLogger {
     }
 
     @Test
-    public void testIgnoreOneCheckpointRepeatedlyCountMultiTimes() {
+    public void testIgnoreOneCheckpointRepeatedlyCountMultiTimes()
+            throws IOException, JobException {
         TestFailJobCallback callback = new TestFailJobCallback();
         CheckpointFailureManager failureManager = new CheckpointFailureManager(2, callback);
+        CheckpointProperties checkpointProperties = forCheckpoint(NEVER_RETAIN_AFTER_TERMINATION);
 
         failureManager.handleJobLevelCheckpointException(
-                new CheckpointException(CheckpointFailureReason.CHECKPOINT_DECLINED), 1);
+                checkpointProperties,
+                new CheckpointException(CheckpointFailureReason.CHECKPOINT_DECLINED),
+                1);
         failureManager.handleJobLevelCheckpointException(
-                new CheckpointException(CheckpointFailureReason.CHECKPOINT_DECLINED), 2);
+                checkpointProperties,
+                new CheckpointException(CheckpointFailureReason.CHECKPOINT_DECLINED),
+                2);
 
         // ignore this
         failureManager.handleJobLevelCheckpointException(
-                new CheckpointException(CheckpointFailureReason.JOB_FAILOVER_REGION), 3);
+                checkpointProperties,
+                new CheckpointException(CheckpointFailureReason.JOB_FAILOVER_REGION),
+                3);
 
         // ignore repeatedly report from one checkpoint
         failureManager.handleJobLevelCheckpointException(
-                new CheckpointException(CheckpointFailureReason.CHECKPOINT_DECLINED), 2);
+                checkpointProperties,
+                new CheckpointException(CheckpointFailureReason.CHECKPOINT_DECLINED),
+                2);
         assertEquals(0, callback.getInvokeCounter());
     }
 
