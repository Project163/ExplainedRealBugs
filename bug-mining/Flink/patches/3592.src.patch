diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecSink.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecSink.scala
index 96883a85760..6ccebb60881 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecSink.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecSink.scala
@@ -99,7 +99,8 @@ class StreamExecSink[T](
             // Now we pick shortest one to sink
             // TODO UpsertStreamTableSink setKeyFields interface should be Array[Array[String]]
             val tableKeys = {
-              UpdatingPlanChecker.getUniqueKeyFields(getInput, planner) match {
+              val sinkFieldNames = upsertSink.getTableSchema.getFieldNames
+              UpdatingPlanChecker.getUniqueKeyFields(getInput, planner, sinkFieldNames) match {
                 case Some(keys) => keys.sortBy(_.length).headOption
                 case None => None
               }
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/UpdatingPlanChecker.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/UpdatingPlanChecker.scala
index b0aaf9ee169..9fc9e44ba90 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/UpdatingPlanChecker.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/UpdatingPlanChecker.scala
@@ -38,12 +38,14 @@ object UpdatingPlanChecker {
   }
 
   /** Extracts the unique keys of the table produced by the plan. */
-  def getUniqueKeyFields(relNode: RelNode, planner: StreamPlanner): Option[Array[Array[String]]] = {
-    val rowType = relNode.getRowType
+  def getUniqueKeyFields(
+      relNode: RelNode,
+      planner: StreamPlanner,
+      sinkFieldNames: Array[String]): Option[Array[Array[String]]] = {
     val fmq = FlinkRelMetadataQuery.reuseOrCreate(planner.getRelBuilder.getCluster.getMetadataQuery)
     val uniqueKeys = fmq.getUniqueKeys(relNode)
     if (uniqueKeys != null && uniqueKeys.size() > 0) {
-      Some(uniqueKeys.filter(_.nonEmpty).map(_.toArray.map(rowType.getFieldNames.get)).toArray)
+      Some(uniqueKeys.filter(_.nonEmpty).map(_.toArray.map(sinkFieldNames)).toArray)
     } else {
       None
     }
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/table/TableSinkITCase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/table/TableSinkITCase.scala
index f092d513958..23f69f0bd89 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/table/TableSinkITCase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/table/TableSinkITCase.scala
@@ -302,20 +302,19 @@ class TableSinkITCase extends AbstractTestBase {
       .assignAscendingTimestamps(_._1.toLong)
       .toTable(tEnv, 'id, 'num, 'text)
 
-    val sink = new TestingUpsertTableSink(Array(0, 2), TimeZone.getDefault)
+    val sink = new TestingUpsertTableSink(Array(0, 2), TimeZone.getDefault).configure(
+      Array[String]("cnt", "lencnt", "cTrue"),
+      Array[TypeInformation[_]](Types.LONG, Types.DECIMAL(), Types.BOOLEAN))
     sink.expectedKeys = Some(Array("cnt", "cTrue"))
     sink.expectedIsAppendOnly = Some(false)
-    tEnv.registerTableSink(
-      "upsertSink",
-      sink.configure(
-        Array[String]("cnt", "lencnt", "cTrue"),
-        Array[TypeInformation[_]](Types.LONG, Types.DECIMAL(), Types.BOOLEAN)))
+    tEnv.registerTableSink("upsertSink", sink)
 
     t.select('id, 'num, 'text.charLength() as 'len, ('id > 0) as 'cTrue)
       .groupBy('len, 'cTrue)
-      .select('len, 'id.count as 'cnt, 'cTrue)
-      .groupBy('cnt, 'cTrue)
-      .select('cnt, 'len.count as 'lencnt, 'cTrue)
+      // test query field name is different with registered sink field name
+      .select('len, 'id.count as 'count, 'cTrue)
+      .groupBy('count, 'cTrue)
+      .select('count, 'len.count as 'lencnt, 'cTrue)
       .insertInto("upsertSink")
 
     env.execute()
@@ -344,18 +343,17 @@ class TableSinkITCase extends AbstractTestBase {
       .assignAscendingTimestamps(_._1.toLong)
       .toTable(tEnv, 'id, 'num, 'text, 'rowtime.rowtime)
 
-    val sink = new TestingUpsertTableSink(Array(0, 1, 2), TimeZone.getDefault)
+    val sink = new TestingUpsertTableSink(Array(0, 1, 2), TimeZone.getDefault).configure(
+      Array[String]("num", "wend", "icnt"),
+      Array[TypeInformation[_]](Types.LONG, Types.SQL_TIMESTAMP, Types.LONG))
     sink.expectedKeys = Some(Array("wend", "num"))
     sink.expectedIsAppendOnly = Some(true)
-    tEnv.registerTableSink(
-      "upsertSink",
-      sink.configure(
-        Array[String]("num", "wend", "icnt"),
-        Array[TypeInformation[_]](Types.LONG, Types.SQL_TIMESTAMP, Types.LONG)))
+    tEnv.registerTableSink("upsertSink", sink)
 
     t.window(Tumble over 5.millis on 'rowtime as 'w)
       .groupBy('w, 'num)
-      .select('num, 'w.end as 'wend, 'id.count as 'icnt)
+      // test query field name is different with registered sink field name
+      .select('num, 'w.end as 'window_end, 'id.count as 'icnt)
       .insertInto("upsertSink")
 
     env.execute()
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/utils/StreamTestSink.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/utils/StreamTestSink.scala
index b3ad30d26be..ac54ecab71b 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/utils/StreamTestSink.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/utils/StreamTestSink.scala
@@ -32,7 +32,6 @@ import org.apache.flink.streaming.api.functions.sink.RichSinkFunction
 import org.apache.flink.table.api.Types
 import org.apache.flink.table.dataformat.{BaseRow, DataFormatConverters, GenericRow}
 import org.apache.flink.table.planner.utils.BaseRowTestUtil
-import org.apache.flink.table.runtime.types.TypeInfoLogicalTypeConverter
 import org.apache.flink.table.runtime.typeutils.BaseRowTypeInfo
 import org.apache.flink.table.sinks._
 import org.apache.flink.table.types.utils.TypeConversions
@@ -250,9 +249,9 @@ final class TestingUpsertSink(keys: Array[Int], tz: TimeZone)
 
 final class TestingUpsertTableSink(val keys: Array[Int], val tz: TimeZone)
   extends UpsertStreamTableSink[BaseRow] {
-  var fNames: Array[String] = _
-  var fTypes: Array[TypeInformation[_]] = _
-  var sink = new TestingUpsertSink(keys, tz)
+  private var fNames: Array[String] = _
+  private var fTypes: Array[TypeInformation[_]] = _
+  private var sink = new TestingUpsertSink(keys, tz)
   var expectedKeys: Option[Array[String]] = None
   var expectedIsAppendOnly: Option[Boolean] = None
 
diff --git a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/plan/util/UpdatingPlanChecker.scala b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/plan/util/UpdatingPlanChecker.scala
index c47898730b2..158fdb3230c 100644
--- a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/plan/util/UpdatingPlanChecker.scala
+++ b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/plan/util/UpdatingPlanChecker.scala
@@ -37,8 +37,9 @@ object UpdatingPlanChecker {
   }
 
   /** Extracts the unique keys of the table produced by the plan. */
-  def getUniqueKeyFields(plan: RelNode): Option[Array[String]] = {
-    getUniqueKeyGroups(plan).map(_.map(_._1).toArray)
+  def getUniqueKeyFields(plan: RelNode, sinkFieldNames: Array[String]): Option[Array[String]] = {
+    val relFieldNames = plan.getRowType.getFieldNames
+    getUniqueKeyGroups(plan).map(_.map(r => sinkFieldNames(relFieldNames.indexOf(r._1))).toArray)
   }
 
   /** Extracts the unique keys and groups of the table produced by the plan. */
diff --git a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/StreamPlanner.scala b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/StreamPlanner.scala
index 45f7067cb8b..6592f1b1711 100644
--- a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/StreamPlanner.scala
+++ b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/StreamPlanner.scala
@@ -355,7 +355,9 @@ class StreamPlanner(
     val isAppendOnlyTable = UpdatingPlanChecker.isAppendOnly(optimizedPlan)
     sink.setIsAppendOnly(isAppendOnlyTable)
     // extract unique key fields
-    val tableKeys: Option[Array[String]] = UpdatingPlanChecker.getUniqueKeyFields(optimizedPlan)
+    val sinkFieldNames = sink.getTableSchema.getFieldNames
+    val tableKeys: Option[Array[String]] = UpdatingPlanChecker
+      .getUniqueKeyFields(optimizedPlan, sinkFieldNames)
     // check that we have keys if the table has changes (is not append-only)
     tableKeys match {
       case Some(keys) => sink.setKeyFields(keys)
diff --git a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/runtime/stream/table/TableSinkITCase.scala b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/runtime/stream/table/TableSinkITCase.scala
index e6ff9818dbb..e6f671d9f8c 100644
--- a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/runtime/stream/table/TableSinkITCase.scala
+++ b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/runtime/stream/table/TableSinkITCase.scala
@@ -283,9 +283,10 @@ class TableSinkITCase extends AbstractTestBase {
 
     t.select('id, 'num, 'text.charLength() as 'len, ('id > 0) as 'cTrue)
       .groupBy('len, 'cTrue)
-      .select('len, 'id.count as 'cnt, 'cTrue)
-      .groupBy('cnt, 'cTrue)
-      .select('cnt, 'len.count as 'lencnt, 'cTrue)
+      // test query field name is different with registered sink field name
+      .select('len, 'id.count as 'count, 'cTrue)
+      .groupBy('count, 'cTrue)
+      .select('count, 'len.count as 'lencnt, 'cTrue)
       .insertInto("upsertSink")
 
     env.execute()
@@ -324,7 +325,8 @@ class TableSinkITCase extends AbstractTestBase {
 
     t.window(Tumble over 5.millis on 'rowtime as 'w)
       .groupBy('w, 'num)
-      .select('num, 'w.end as 'wend, 'id.count as 'icnt)
+      // test query field name is different with registered sink field name
+      .select('num, 'w.end as 'window_end, 'id.count as 'icnt)
       .insertInto("upsertSink")
 
     env.execute()
