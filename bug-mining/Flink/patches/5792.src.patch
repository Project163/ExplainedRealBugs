diff --git a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaCommittable.java b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaCommittable.java
index 38579a35760..72a3281e683 100644
--- a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaCommittable.java
+++ b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaCommittable.java
@@ -21,6 +21,7 @@ import javax.annotation.Nullable;
 
 import java.util.Objects;
 import java.util.Optional;
+import java.util.function.Consumer;
 
 /**
  * This class holds the necessary information to construct a new {@link FlinkKafkaInternalProducer}
@@ -31,25 +32,27 @@ class KafkaCommittable {
     private final long producerId;
     private final short epoch;
     private final String transactionalId;
-    @Nullable private FlinkKafkaInternalProducer<?, ?> producer;
+    @Nullable private Recyclable<? extends FlinkKafkaInternalProducer<?, ?>> producer;
 
     public KafkaCommittable(
             long producerId,
             short epoch,
             String transactionalId,
-            @Nullable FlinkKafkaInternalProducer<?, ?> producer) {
+            @Nullable Recyclable<? extends FlinkKafkaInternalProducer<?, ?>> producer) {
         this.producerId = producerId;
         this.epoch = epoch;
         this.transactionalId = transactionalId;
         this.producer = producer;
     }
 
-    public static KafkaCommittable of(FlinkKafkaInternalProducer<byte[], byte[]> producer) {
+    public static <K, V> KafkaCommittable of(
+            FlinkKafkaInternalProducer<K, V> producer,
+            Consumer<FlinkKafkaInternalProducer<K, V>> recycler) {
         return new KafkaCommittable(
                 producer.getProducerId(),
                 producer.getEpoch(),
                 producer.getTransactionalId(),
-                producer);
+                new Recyclable<>(producer, recycler));
     }
 
     public long getProducerId() {
@@ -64,7 +67,7 @@ class KafkaCommittable {
         return transactionalId;
     }
 
-    public Optional<FlinkKafkaInternalProducer<?, ?>> getProducer() {
+    public Optional<Recyclable<? extends FlinkKafkaInternalProducer<?, ?>>> getProducer() {
         return Optional.ofNullable(producer);
     }
 
diff --git a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaCommitter.java b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaCommitter.java
index d9c0d29b25c..24d077dc349 100644
--- a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaCommitter.java
+++ b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaCommitter.java
@@ -24,9 +24,12 @@ import org.apache.kafka.common.errors.ProducerFencedException;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import javax.annotation.Nullable;
+
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
+import java.util.Optional;
 import java.util.Properties;
 
 /**
@@ -40,6 +43,8 @@ class KafkaCommitter implements Committer<KafkaCommittable> {
 
     private final Properties kafkaProducerConfig;
 
+    @Nullable private FlinkKafkaInternalProducer<?, ?> recoveryProducer;
+
     KafkaCommitter(Properties kafkaProducerConfig) {
         this.kafkaProducerConfig = kafkaProducerConfig;
     }
@@ -50,9 +55,16 @@ class KafkaCommitter implements Committer<KafkaCommittable> {
         for (KafkaCommittable committable : committables) {
             final String transactionalId = committable.getTransactionalId();
             LOG.debug("Committing Kafka transaction {}", transactionalId);
-            try (FlinkKafkaInternalProducer<?, ?> producer =
-                    committable.getProducer().orElseGet(() -> createProducer(committable))) {
+            Optional<Recyclable<? extends FlinkKafkaInternalProducer<?, ?>>> recyclable =
+                    committable.getProducer();
+            FlinkKafkaInternalProducer<?, ?> producer;
+            try {
+                producer =
+                        recyclable
+                                .<FlinkKafkaInternalProducer<?, ?>>map(Recyclable::getObject)
+                                .orElseGet(() -> getRecoveryProducer(committable));
                 producer.commitTransaction();
+                recyclable.ifPresent(Recyclable::close);
             } catch (ProducerFencedException | InvalidTxnStateException e) {
                 // That means we have committed this transaction before.
                 LOG.warn(
@@ -60,6 +72,7 @@ class KafkaCommitter implements Committer<KafkaCommittable> {
                                 + "Presumably this transaction has been already committed before",
                         e,
                         committable);
+                recyclable.ifPresent(Recyclable::close);
             } catch (Throwable e) {
                 LOG.warn("Cannot commit Kafka transaction, retrying.", e);
                 retryableCommittables.add(committable);
@@ -69,17 +82,25 @@ class KafkaCommitter implements Committer<KafkaCommittable> {
     }
 
     @Override
-    public void close() throws Exception {}
+    public void close() throws Exception {
+        if (recoveryProducer != null) {
+            recoveryProducer.close();
+        }
+    }
 
     /**
      * Creates a producer that can commit into the same transaction as the upstream producer that
      * was serialized into {@link KafkaCommittable}.
      */
-    private FlinkKafkaInternalProducer<?, ?> createProducer(KafkaCommittable committable) {
-        FlinkKafkaInternalProducer<?, ?> producer =
-                new FlinkKafkaInternalProducer<>(
-                        kafkaProducerConfig, committable.getTransactionalId());
-        producer.resumeTransaction(committable.getProducerId(), committable.getEpoch());
-        return producer;
+    private FlinkKafkaInternalProducer<?, ?> getRecoveryProducer(KafkaCommittable committable) {
+        if (recoveryProducer == null) {
+            recoveryProducer =
+                    new FlinkKafkaInternalProducer<>(
+                            kafkaProducerConfig, committable.getTransactionalId());
+        } else {
+            recoveryProducer.setTransactionId(committable.getTransactionalId());
+        }
+        recoveryProducer.resumeTransaction(committable.getProducerId(), committable.getEpoch());
+        return recoveryProducer;
     }
 }
diff --git a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaWriter.java b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaWriter.java
index 02af7ff2190..8be56f88d19 100644
--- a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaWriter.java
+++ b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaWriter.java
@@ -17,6 +17,7 @@
 
 package org.apache.flink.connector.kafka.sink;
 
+import org.apache.flink.annotation.VisibleForTesting;
 import org.apache.flink.api.common.serialization.SerializationSchema;
 import org.apache.flink.api.connector.sink.Sink;
 import org.apache.flink.api.connector.sink.SinkWriter;
@@ -43,7 +44,9 @@ import org.slf4j.LoggerFactory;
 import javax.annotation.Nullable;
 
 import java.io.IOException;
+import java.util.ArrayDeque;
 import java.util.Collections;
+import java.util.Deque;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -84,7 +87,10 @@ class KafkaWriter<IN> implements SinkWriter<IN, KafkaCommittable, KafkaWriterSta
 
     private Metric byteOutMetric;
     private FlinkKafkaInternalProducer<byte[], byte[]> currentProducer;
-    private KafkaWriterState kafkaWriterState;
+    private final KafkaWriterState kafkaWriterState;
+    // producer pool only used for exactly once
+    private final Deque<FlinkKafkaInternalProducer<byte[], byte[]>> producerPool =
+            new ArrayDeque<>();
     private final Closer closer = Closer.create();
     @Nullable private volatile Exception producerAsyncException;
     private long lastCheckpointId;
@@ -147,11 +153,23 @@ class KafkaWriter<IN> implements SinkWriter<IN, KafkaCommittable, KafkaWriterSta
         } catch (Exception e) {
             throw new FlinkRuntimeException("Cannot initialize schema.", e);
         }
-        lastCheckpointId = sinkInitContext.getRestoredCheckpointId().orElse(0);
-        abortLingeringTransactions(
-                checkNotNull(recoveredStates, "recoveredStates"), lastCheckpointId + 1);
+
         this.kafkaWriterState = new KafkaWriterState(transactionalIdPrefix);
-        this.currentProducer = createProducer(lastCheckpointId + 1);
+        this.lastCheckpointId = sinkInitContext.getRestoredCheckpointId().orElse(-1);
+        if (deliveryGuarantee == DeliveryGuarantee.EXACTLY_ONCE) {
+            abortLingeringTransactions(
+                    checkNotNull(recoveredStates, "recoveredStates"), lastCheckpointId + 1);
+            this.currentProducer = getTransactionalProducer(lastCheckpointId + 1);
+            this.currentProducer.beginTransaction();
+        } else if (deliveryGuarantee == DeliveryGuarantee.AT_LEAST_ONCE
+                || deliveryGuarantee == DeliveryGuarantee.NONE) {
+            this.currentProducer = new FlinkKafkaInternalProducer<>(this.kafkaProducerConfig, null);
+            closer.register(this.currentProducer);
+            initMetrics(this.currentProducer);
+        } else {
+            throw new UnsupportedOperationException(
+                    "Unsupported Kafka writer semantic " + this.deliveryGuarantee);
+        }
         registerMetricSync();
     }
 
@@ -167,27 +185,48 @@ class KafkaWriter<IN> implements SinkWriter<IN, KafkaCommittable, KafkaWriterSta
     @Override
     public List<KafkaCommittable> prepareCommit(boolean flush) {
         flushRecords(flush);
-        return precommit();
+        if (deliveryGuarantee == DeliveryGuarantee.EXACTLY_ONCE) {
+            final List<KafkaCommittable> committables =
+                    Collections.singletonList(
+                            KafkaCommittable.of(currentProducer, producerPool::add));
+            LOG.info("Committing {} committables.", committables);
+            return committables;
+        }
+        return Collections.emptyList();
     }
 
     @Override
     public List<KafkaWriterState> snapshotState(long checkpointId) throws IOException {
-        currentProducer = createProducer(checkpointId + 1);
+        if (deliveryGuarantee == DeliveryGuarantee.EXACTLY_ONCE) {
+            currentProducer = getTransactionalProducer(checkpointId + 1);
+            currentProducer.beginTransaction();
+        }
         return ImmutableList.of(kafkaWriterState);
     }
 
     @Override
     public void close() throws Exception {
-
         if (currentProducer.isInTransaction()) {
             currentProducer.abortTransaction();
         }
         closed = true;
         closer.close();
+        producerPool.clear();
         checkState(currentProducer.isClosed());
+        currentProducer = null;
+    }
+
+    @VisibleForTesting
+    Deque<FlinkKafkaInternalProducer<byte[], byte[]>> getProducerPool() {
+        return producerPool;
+    }
+
+    @VisibleForTesting
+    FlinkKafkaInternalProducer<byte[], byte[]> getCurrentProducer() {
+        return currentProducer;
     }
 
-    private void abortLingeringTransactions(
+    void abortLingeringTransactions(
             List<KafkaWriterState> recoveredStates, long startCheckpointId) {
         List<String> prefixesToAbort = Lists.newArrayList(transactionalIdPrefix);
 
@@ -207,7 +246,7 @@ class KafkaWriter<IN> implements SinkWriter<IN, KafkaCommittable, KafkaWriterSta
                         kafkaSinkContext.getParallelInstanceId(),
                         kafkaSinkContext.getNumberOfParallelInstances(),
                         this::getOrCreateTransactionalProducer,
-                        FlinkKafkaInternalProducer::close)) {
+                        producerPool::add)) {
             transactionAborter.abortLingeringTransactions(prefixesToAbort, startCheckpointId);
         }
     }
@@ -225,31 +264,6 @@ class KafkaWriter<IN> implements SinkWriter<IN, KafkaCommittable, KafkaWriterSta
         }
     }
 
-    private FlinkKafkaInternalProducer<byte[], byte[]> createProducer(long checkpointId) {
-        switch (deliveryGuarantee) {
-            case EXACTLY_ONCE:
-                final FlinkKafkaInternalProducer<byte[], byte[]> transactionalProducer =
-                        getTransactionalProducer(checkpointId);
-                initMetrics(transactionalProducer);
-                transactionalProducer.beginTransaction();
-                return transactionalProducer;
-            case AT_LEAST_ONCE:
-            case NONE:
-                if (currentProducer != null) {
-                    LOG.debug("Reusing existing KafkaProducer");
-                    return currentProducer;
-                }
-                final FlinkKafkaInternalProducer<byte[], byte[]> producer =
-                        new FlinkKafkaInternalProducer<>(kafkaProducerConfig, null);
-                closer.register(producer);
-                initMetrics(producer);
-                return producer;
-            default:
-                throw new UnsupportedOperationException(
-                        "Unsupported Kafka writer semantic " + deliveryGuarantee);
-        }
-    }
-
     private void flushRecords(boolean finalFlush) {
         switch (deliveryGuarantee) {
             case EXACTLY_ONCE:
@@ -275,24 +289,6 @@ class KafkaWriter<IN> implements SinkWriter<IN, KafkaCommittable, KafkaWriterSta
         checkErroneous();
     }
 
-    private List<KafkaCommittable> precommit() {
-        final List<KafkaCommittable> committables;
-        switch (deliveryGuarantee) {
-            case EXACTLY_ONCE:
-                committables = Collections.singletonList(KafkaCommittable.of(currentProducer));
-                break;
-            case AT_LEAST_ONCE:
-            case NONE:
-                committables = Collections.emptyList();
-                break;
-            default:
-                throw new UnsupportedOperationException(
-                        "Unsupported Kafka writer semantic " + deliveryGuarantee);
-        }
-        LOG.info("Committing {} committables.", committables);
-        return committables;
-    }
-
     /**
      * For each checkpoint we create new {@link FlinkKafkaInternalProducer} so that new transactions
      * will not clash with transactions created during previous checkpoints ({@code
@@ -324,11 +320,15 @@ class KafkaWriter<IN> implements SinkWriter<IN, KafkaCommittable, KafkaWriterSta
 
     private FlinkKafkaInternalProducer<byte[], byte[]> getOrCreateTransactionalProducer(
             String transactionalId) {
-        FlinkKafkaInternalProducer<byte[], byte[]> producer =
-                new FlinkKafkaInternalProducer<>(kafkaProducerConfig, transactionalId);
-        closer.register(producer);
-        producer.initTransactions();
-        initMetrics(producer);
+        FlinkKafkaInternalProducer<byte[], byte[]> producer = producerPool.poll();
+        if (producer == null) {
+            producer = new FlinkKafkaInternalProducer<>(kafkaProducerConfig, transactionalId);
+            closer.register(producer);
+            producer.initTransactions();
+            initMetrics(producer);
+        } else {
+            producer.initTransactionId(transactionalId);
+        }
         return producer;
     }
 
diff --git a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/Recyclable.java b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/Recyclable.java
new file mode 100644
index 00000000000..163592a7b89
--- /dev/null
+++ b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/Recyclable.java
@@ -0,0 +1,45 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connector.kafka.sink;
+
+import java.io.Closeable;
+import java.util.function.Consumer;
+
+import static org.apache.flink.util.Preconditions.checkNotNull;
+import static org.apache.flink.util.Preconditions.checkState;
+
+class Recyclable<T> implements Closeable {
+    private T object;
+    private final Consumer<T> recycler;
+
+    public Recyclable(T object, Consumer<T> recycler) {
+        this.object = checkNotNull(object);
+        this.recycler = checkNotNull(recycler);
+    }
+
+    public T getObject() {
+        checkState(object != null, "Already recycled");
+        return object;
+    }
+
+    @Override
+    public void close() {
+        recycler.accept(object);
+        object = null;
+    }
+}
diff --git a/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/sink/FlinkKafkaInternalProducerITCase.java b/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/sink/FlinkKafkaInternalProducerITCase.java
index 585303c31c3..a580db95d94 100644
--- a/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/sink/FlinkKafkaInternalProducerITCase.java
+++ b/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/sink/FlinkKafkaInternalProducerITCase.java
@@ -58,6 +58,7 @@ class FlinkKafkaInternalProducerITCase extends TestLogger {
             new KafkaContainer(DockerImageName.parse("confluentinc/cp-kafka:5.5.2"))
                     .withEnv("KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR", "1")
                     .withEnv("KAFKA_TRANSACTION_STATE_LOG_MIN_ISR", "1")
+                    .withEnv("KAFKA_CONFLUENT_SUPPORT_METRICS_ENABLE", "false")
                     .withLogConsumer(new Slf4jLogConsumer(LOG))
                     .withEmbeddedZookeeper();
 
diff --git a/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/sink/KafkaWriterITCase.java b/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/sink/KafkaWriterITCase.java
index 1a75ccac207..62b382a5263 100644
--- a/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/sink/KafkaWriterITCase.java
+++ b/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/sink/KafkaWriterITCase.java
@@ -36,6 +36,7 @@ import org.apache.flink.util.UserCodeClassLoader;
 
 import org.apache.flink.shaded.guava30.com.google.common.collect.ImmutableList;
 
+import org.apache.kafka.clients.consumer.ConsumerRecord;
 import org.apache.kafka.clients.producer.ProducerRecord;
 import org.apache.kafka.common.serialization.ByteArraySerializer;
 import org.junit.jupiter.api.AfterAll;
@@ -66,8 +67,11 @@ import java.util.stream.IntStream;
 
 import static org.apache.flink.connector.kafka.sink.KafkaSinkITCase.drainAllRecordsFromTopic;
 import static org.hamcrest.MatcherAssert.assertThat;
+import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.greaterThan;
 import static org.hamcrest.Matchers.hasSize;
+import static org.hamcrest.Matchers.not;
+import static org.hamcrest.Matchers.sameInstance;
 import static org.junit.jupiter.api.Assertions.assertTrue;
 
 /** Tests for the standalone KafkaWriter. */
@@ -178,6 +182,101 @@ public class KafkaWriterITCase extends TestLogger {
         }
     }
 
+    /** Test that producer is not accidentally recreated or pool is used. */
+    @Test
+    void testLingeringTransaction() throws Exception {
+        final KafkaWriter<Integer> failedWriter =
+                createWriterWithConfiguration(
+                        getKafkaClientConfiguration(), DeliveryGuarantee.EXACTLY_ONCE);
+
+        // create two lingering transactions
+        failedWriter.prepareCommit(false);
+        failedWriter.snapshotState(0);
+        failedWriter.prepareCommit(false);
+        failedWriter.snapshotState(1);
+
+        try (final KafkaWriter<Integer> recoveredWriter =
+                createWriterWithConfiguration(
+                        getKafkaClientConfiguration(), DeliveryGuarantee.EXACTLY_ONCE)) {
+            recoveredWriter.write(1, SINK_WRITER_CONTEXT);
+
+            List<KafkaCommittable> committables = recoveredWriter.prepareCommit(false);
+            recoveredWriter.snapshotState(0);
+            assertThat(committables, hasSize(1));
+            assertThat(committables.get(0).getProducer().isPresent(), equalTo(true));
+
+            committables.get(0).getProducer().get().getObject().commitTransaction();
+
+            List<ConsumerRecord<byte[], byte[]>> records =
+                    KafkaSinkITCase.drainAllRecordsFromTopic(topic, getKafkaClientConfiguration());
+            assertThat(records, hasSize(1));
+        }
+
+        failedWriter.close();
+    }
+
+    /** Test that producer is not accidentally recreated or pool is used. */
+    @ParameterizedTest
+    @EnumSource(
+            value = DeliveryGuarantee.class,
+            names = "EXACTLY_ONCE",
+            mode = EnumSource.Mode.EXCLUDE)
+    void useSameProducerForNonTransactional(DeliveryGuarantee guarantee) throws Exception {
+        try (final KafkaWriter<Integer> writer =
+                createWriterWithConfiguration(getKafkaClientConfiguration(), guarantee)) {
+            assertThat(writer.getProducerPool(), hasSize(0));
+
+            FlinkKafkaInternalProducer<byte[], byte[]> firstProducer = writer.getCurrentProducer();
+            List<KafkaCommittable> committables = writer.prepareCommit(false);
+            writer.snapshotState(0);
+            assertThat(committables, hasSize(0));
+
+            assertThat(
+                    "Expected same producer",
+                    writer.getCurrentProducer(),
+                    sameInstance(firstProducer));
+            assertThat(writer.getProducerPool(), hasSize(0));
+        }
+    }
+
+    /** Test that producers are reused when committed. */
+    @Test
+    void usePoolForTransactional() throws Exception {
+        try (final KafkaWriter<Integer> writer =
+                createWriterWithConfiguration(
+                        getKafkaClientConfiguration(), DeliveryGuarantee.EXACTLY_ONCE)) {
+            assertThat(writer.getProducerPool(), hasSize(0));
+
+            List<KafkaCommittable> committables0 = writer.prepareCommit(false);
+            writer.snapshotState(0);
+            assertThat(committables0, hasSize(1));
+            assertThat(committables0.get(0).getProducer().isPresent(), equalTo(true));
+
+            FlinkKafkaInternalProducer<?, ?> firstProducer =
+                    committables0.get(0).getProducer().get().getObject();
+            assertThat(
+                    "Expected different producer",
+                    firstProducer,
+                    not(sameInstance(writer.getCurrentProducer())));
+
+            // recycle first producer, KafkaCommitter would commit it and then return it
+            assertThat(writer.getProducerPool(), hasSize(0));
+            firstProducer.commitTransaction();
+            committables0.get(0).getProducer().get().close();
+            assertThat(writer.getProducerPool(), hasSize(1));
+
+            List<KafkaCommittable> committables1 = writer.prepareCommit(false);
+            writer.snapshotState(1);
+            assertThat(committables1, hasSize(1));
+            assertThat(committables1.get(0).getProducer().isPresent(), equalTo(true));
+
+            assertThat(
+                    "Expected recycled producer",
+                    firstProducer,
+                    sameInstance(writer.getCurrentProducer()));
+        }
+    }
+
     /**
      * Tests that open transactions are automatically aborted on close such that successive writes
      * succeed.
