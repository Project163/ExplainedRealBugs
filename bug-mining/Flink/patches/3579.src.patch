diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/stream/StreamExecMatchRule.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/stream/StreamExecMatchRule.scala
index f3bf625b823..54f36c8358c 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/stream/StreamExecMatchRule.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/stream/StreamExecMatchRule.scala
@@ -15,10 +15,10 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-
 package org.apache.flink.table.planner.plan.rules.physical.stream
 
 import org.apache.flink.table.api.{TableException, ValidationException}
+import org.apache.flink.table.planner.plan.`trait`.FlinkRelDistribution
 import org.apache.flink.table.planner.plan.logical.MatchRecognize
 import org.apache.flink.table.planner.plan.nodes.FlinkConventions
 import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalMatch
@@ -56,8 +56,19 @@ class StreamExecMatchRule
   override def convert(rel: RelNode): RelNode = {
     val logicalMatch: FlinkLogicalMatch = rel.asInstanceOf[FlinkLogicalMatch]
     val traitSet: RelTraitSet = rel.getTraitSet.replace(FlinkConventions.STREAM_PHYSICAL)
+    val partitionKeys = logicalMatch.getPartitionKeys
+
+    val requiredDistribution = if (!partitionKeys.isEmpty) {
+      FlinkRelDistribution.hash(logicalMatch.getPartitionKeys.asList())
+    } else {
+      FlinkRelDistribution.SINGLETON
+    }
+    val requiredTraitSet = rel.getCluster.getPlanner.emptyTraitSet()
+      .replace(requiredDistribution)
+      .replace(FlinkConventions.STREAM_PHYSICAL)
+
     val convertInput: RelNode =
-      RelOptRule.convert(logicalMatch.getInput, FlinkConventions.STREAM_PHYSICAL)
+      RelOptRule.convert(logicalMatch.getInput, requiredTraitSet)
 
     try {
       Class
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/match/PatternTranslatorTestBase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/match/PatternTranslatorTestBase.scala
index 211ce893004..2f56132b014 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/match/PatternTranslatorTestBase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/match/PatternTranslatorTestBase.scala
@@ -94,7 +94,8 @@ abstract class PatternTranslatorTestBase extends TestLogger {
     val optimized: RelNode = plannerBase.optimize(Seq(converted)).head
 
     // throw exception if plan contains more than a match
-    if (!optimized.getInput(0).isInstanceOf[StreamExecDataStreamScan]) {
+    // the plan should be: StreamExecMatch -> StreamExecExchange -> StreamExecDataStreamScan
+    if (!optimized.getInput(0).getInput(0).isInstanceOf[StreamExecDataStreamScan]) {
       fail("Expression is converted into more than a Match operation. Use a different test method.")
     }
 
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/MatchRecognizeITCase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/MatchRecognizeITCase.scala
index f563de5633c..b9b91987b14 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/MatchRecognizeITCase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/MatchRecognizeITCase.scala
@@ -416,6 +416,52 @@ class MatchRecognizeITCase(backend: StateBackendMode) extends StreamingWithState
     assertEquals(expected.sorted, sink.getAppendResults.sorted)
   }
 
+  @Test
+  def testPartitionByWithParallelSource(): Unit = {
+    val env = StreamExecutionEnvironment.getExecutionEnvironment
+    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
+    val tEnv = StreamTableEnvironment.create(env, TableTestUtil.STREAM_SETTING)
+
+    val data = new mutable.MutableList[(String, Long, Int, Int)]
+    data.+=(("ACME", 1L, 19, 1))
+    data.+=(("ACME", 2L, 17, 2))
+    data.+=(("ACME", 3L, 13, 3))
+    data.+=(("ACME", 4L, 20, 4))
+
+    val t = env.fromCollection(data)
+      .assignAscendingTimestamps(tickerEvent => tickerEvent._2)
+      .setParallelism(env.getParallelism)
+      .toTable(tEnv, 'symbol, 'rowtime.rowtime, 'price, 'tax)
+    tEnv.registerTable("Ticker", t)
+
+    val sqlQuery =
+      s"""
+         |SELECT *
+         |FROM Ticker
+         |MATCH_RECOGNIZE (
+         |  PARTITION BY symbol
+         |  ORDER BY rowtime
+         |  MEASURES
+         |    DOWN.tax AS bottom_tax,
+         |    UP.tax AS end_tax
+         |  ONE ROW PER MATCH
+         |  AFTER MATCH SKIP PAST LAST ROW
+         |  PATTERN (DOWN UP)
+         |  DEFINE
+         |    DOWN AS DOWN.price = 13,
+         |    UP AS UP.price = 20
+         |) AS T
+         |""".stripMargin
+
+    val sink = new TestingAppendSink()
+    val result = tEnv.sqlQuery(sqlQuery).toAppendStream[Row]
+    result.addSink(sink)
+    env.execute()
+
+    val expected = List("ACME,3,4")
+    assertEquals(expected.sorted, sink.getAppendResults.sorted)
+  }
+
   @Test
   def testLogicalOffsetsWithStarVariable(): Unit = {
     val env = StreamExecutionEnvironment.getExecutionEnvironment
