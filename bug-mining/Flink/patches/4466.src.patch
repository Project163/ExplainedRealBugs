diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/JdbcDialect.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/JdbcDialect.java
index 3d311be00bd..b3635d188ae 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/JdbcDialect.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/JdbcDialect.java
@@ -29,6 +29,8 @@ import java.util.Arrays;
 import java.util.Optional;
 import java.util.stream.Collectors;
 
+import static java.lang.String.format;
+
 /**
  * Handle the SQL dialect of jdbc driver.
  */
@@ -97,7 +99,7 @@ public interface JdbcDialect extends Serializable {
 	 */
 	default String getRowExistsStatement(String tableName, String[] conditionFields) {
 		String fieldExpressions = Arrays.stream(conditionFields)
-			.map(f -> quoteIdentifier(f) + "=?")
+			.map(f -> format("%s = :%s", quoteIdentifier(f), f))
 			.collect(Collectors.joining(" AND "));
 		return "SELECT 1 FROM " + quoteIdentifier(tableName) + " WHERE " + fieldExpressions;
 	}
@@ -110,7 +112,7 @@ public interface JdbcDialect extends Serializable {
 			.map(this::quoteIdentifier)
 			.collect(Collectors.joining(", "));
 		String placeholders = Arrays.stream(fieldNames)
-			.map(f -> "?")
+			.map(f -> ":" + f)
 			.collect(Collectors.joining(", "));
 		return "INSERT INTO " + quoteIdentifier(tableName) +
 			"(" + columns + ")" + " VALUES (" + placeholders + ")";
@@ -122,10 +124,10 @@ public interface JdbcDialect extends Serializable {
 	 */
 	default String getUpdateStatement(String tableName, String[] fieldNames, String[] conditionFields) {
 		String setClause = Arrays.stream(fieldNames)
-			.map(f -> quoteIdentifier(f) + "=?")
+			.map(f -> format("%s = :%s", quoteIdentifier(f), f))
 			.collect(Collectors.joining(", "));
 		String conditionClause = Arrays.stream(conditionFields)
-			.map(f -> quoteIdentifier(f) + "=?")
+			.map(f -> format("%s = :%s", quoteIdentifier(f), f))
 			.collect(Collectors.joining(" AND "));
 		return "UPDATE " + quoteIdentifier(tableName) +
 			" SET " + setClause +
@@ -138,7 +140,7 @@ public interface JdbcDialect extends Serializable {
 	 */
 	default String getDeleteStatement(String tableName, String[] conditionFields) {
 		String conditionClause = Arrays.stream(conditionFields)
-			.map(f -> quoteIdentifier(f) + "=?")
+			.map(f -> format("%s = :%s", quoteIdentifier(f), f))
 			.collect(Collectors.joining(" AND "));
 		return "DELETE FROM " + quoteIdentifier(tableName) + " WHERE " + conditionClause;
 	}
@@ -151,7 +153,7 @@ public interface JdbcDialect extends Serializable {
 				.map(this::quoteIdentifier)
 				.collect(Collectors.joining(", "));
 		String fieldExpressions = Arrays.stream(conditionFields)
-				.map(f -> quoteIdentifier(f) + "=?")
+				.map(f -> format("%s = :%s", quoteIdentifier(f), f))
 				.collect(Collectors.joining(" AND "));
 		return "SELECT " + selectExpressions + " FROM " +
 				quoteIdentifier(tableName) + (conditionFields.length > 0 ? " WHERE " + fieldExpressions : "");
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/JdbcBatchingOutputFormat.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/JdbcBatchingOutputFormat.java
index c7afc31cde7..1a5718c3d93 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/JdbcBatchingOutputFormat.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/JdbcBatchingOutputFormat.java
@@ -28,6 +28,7 @@ import org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionP
 import org.apache.flink.connector.jdbc.internal.executor.JdbcBatchStatementExecutor;
 import org.apache.flink.connector.jdbc.internal.options.JdbcDmlOptions;
 import org.apache.flink.connector.jdbc.internal.options.JdbcOptions;
+import org.apache.flink.connector.jdbc.statement.FieldNamedPreparedStatementImpl;
 import org.apache.flink.connector.jdbc.utils.JdbcUtils;
 import org.apache.flink.runtime.util.ExecutorThreadFactory;
 import org.apache.flink.types.Row;
@@ -41,6 +42,7 @@ import javax.annotation.Nonnull;
 import java.io.IOException;
 import java.io.Serializable;
 import java.sql.SQLException;
+import java.util.HashMap;
 import java.util.concurrent.Executors;
 import java.util.concurrent.ScheduledExecutorService;
 import java.util.concurrent.ScheduledFuture;
@@ -219,6 +221,7 @@ public class JdbcBatchingOutputFormat<In, JdbcIn, JdbcExec extends JdbcBatchStat
 					flush();
 				} catch (Exception e) {
 					LOG.warn("Writing records to JDBC failed.", e);
+					throw new RuntimeException("Writing records to JDBC failed.", e);
 				}
 			}
 
@@ -323,7 +326,9 @@ public class JdbcBatchingOutputFormat<In, JdbcIn, JdbcExec extends JdbcBatchStat
 					executionOptionsBuilder.build());
 			} else {
 				// warn: don't close over builder fields
-				String sql = options.getDialect().getInsertIntoStatement(dml.getTableName(), dml.getFieldNames());
+				String sql = FieldNamedPreparedStatementImpl.parseNamedStatement(
+					options.getDialect().getInsertIntoStatement(dml.getTableName(), dml.getFieldNames()),
+					new HashMap<>());
 				return new JdbcBatchingOutputFormat<>(
 					new SimpleJdbcConnectionProvider(options),
 					executionOptionsBuilder.build(),
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/TableJdbcUpsertOutputFormat.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/TableJdbcUpsertOutputFormat.java
index 4afea82ce32..a62d2c34ba5 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/TableJdbcUpsertOutputFormat.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/TableJdbcUpsertOutputFormat.java
@@ -24,6 +24,7 @@ import org.apache.flink.connector.jdbc.internal.connection.JdbcConnectionProvide
 import org.apache.flink.connector.jdbc.internal.executor.InsertOrUpdateJdbcExecutor;
 import org.apache.flink.connector.jdbc.internal.executor.JdbcBatchStatementExecutor;
 import org.apache.flink.connector.jdbc.internal.options.JdbcDmlOptions;
+import org.apache.flink.connector.jdbc.statement.FieldNamedPreparedStatementImpl;
 import org.apache.flink.types.Row;
 
 import org.slf4j.Logger;
@@ -32,6 +33,7 @@ import org.slf4j.LoggerFactory;
 import java.io.IOException;
 import java.sql.SQLException;
 import java.util.Arrays;
+import java.util.HashMap;
 import java.util.function.Function;
 
 import static org.apache.flink.connector.jdbc.utils.JdbcUtils.getPrimaryKey;
@@ -64,7 +66,9 @@ class TableJdbcUpsertOutputFormat extends JdbcBatchingOutputFormat<Tuple2<Boolea
 		int[] pkFields = Arrays.stream(dmlOptions.getFieldNames()).mapToInt(Arrays.asList(dmlOptions.getFieldNames())::indexOf).toArray();
 		int[] pkTypes = dmlOptions.getFieldTypes() == null ? null :
 				Arrays.stream(pkFields).map(f -> dmlOptions.getFieldTypes()[f]).toArray();
-		String deleteSql = dmlOptions.getDialect().getDeleteStatement(dmlOptions.getTableName(), dmlOptions.getFieldNames());
+		String deleteSql = FieldNamedPreparedStatementImpl.parseNamedStatement(
+			dmlOptions.getDialect().getDeleteStatement(dmlOptions.getTableName(), dmlOptions.getFieldNames()),
+			new HashMap<>());
 		return createKeyedRowExecutor(pkFields, pkTypes, deleteSql);
 	}
 
@@ -113,12 +117,12 @@ class TableJdbcUpsertOutputFormat extends JdbcBatchingOutputFormat<Tuple2<Boolea
 
 		return opt.getDialect()
 			.getUpsertStatement(opt.getTableName(), opt.getFieldNames(), opt.getKeyFields().get())
-			.map(sql -> createSimpleRowExecutor(sql, opt.getFieldTypes(), ctx.getExecutionConfig().isObjectReuseEnabled()))
+			.map(sql -> createSimpleRowExecutor(parseNamedStatement(sql), opt.getFieldTypes(), ctx.getExecutionConfig().isObjectReuseEnabled()))
 			.orElseGet(() ->
 				new InsertOrUpdateJdbcExecutor<>(
-					opt.getDialect().getRowExistsStatement(opt.getTableName(), opt.getKeyFields().get()),
-					opt.getDialect().getInsertIntoStatement(opt.getTableName(), opt.getFieldNames()),
-					opt.getDialect().getUpdateStatement(opt.getTableName(), opt.getFieldNames(), opt.getKeyFields().get()),
+					parseNamedStatement(opt.getDialect().getRowExistsStatement(opt.getTableName(), opt.getKeyFields().get())),
+					parseNamedStatement(opt.getDialect().getInsertIntoStatement(opt.getTableName(), opt.getFieldNames())),
+					parseNamedStatement(opt.getDialect().getUpdateStatement(opt.getTableName(), opt.getFieldNames(), opt.getKeyFields().get())),
 					createRowJdbcStatementBuilder(pkTypes),
 					createRowJdbcStatementBuilder(opt.getFieldTypes()),
 					createRowJdbcStatementBuilder(opt.getFieldTypes()),
@@ -126,6 +130,10 @@ class TableJdbcUpsertOutputFormat extends JdbcBatchingOutputFormat<Tuple2<Boolea
 					ctx.getExecutionConfig().isObjectReuseEnabled() ? Row::copy : Function.identity()));
 	}
 
+	private static String parseNamedStatement(String statement) {
+		return FieldNamedPreparedStatementImpl.parseNamedStatement(statement, new HashMap<>());
+	}
+
 	private static Function<Row, Row> createRowKeyExtractor(int[] pkFields) {
 		return row -> getPrimaryKey(row, pkFields);
 	}
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/AbstractJdbcRowConverter.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/AbstractJdbcRowConverter.java
index a622e6c87fd..f0f06ebaa84 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/AbstractJdbcRowConverter.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/AbstractJdbcRowConverter.java
@@ -18,6 +18,7 @@
 
 package org.apache.flink.connector.jdbc.internal.converter;
 
+import org.apache.flink.connector.jdbc.statement.FieldNamedPreparedStatement;
 import org.apache.flink.connector.jdbc.utils.JdbcTypeUtil;
 import org.apache.flink.table.data.DecimalData;
 import org.apache.flink.table.data.GenericRowData;
@@ -81,7 +82,7 @@ public abstract class AbstractJdbcRowConverter implements JdbcRowConverter {
 	}
 
 	@Override
-	public PreparedStatement toExternal(RowData rowData, PreparedStatement statement) throws SQLException {
+	public FieldNamedPreparedStatement toExternal(RowData rowData, FieldNamedPreparedStatement statement) throws SQLException {
 		for (int index = 0; index < rowData.getArity(); index++) {
 			toExternalConverters[index].serialize(rowData, index, statement);
 		}
@@ -105,7 +106,7 @@ public abstract class AbstractJdbcRowConverter implements JdbcRowConverter {
 	 */
 	@FunctionalInterface
 	interface JdbcSerializationConverter extends Serializable {
-		void serialize(RowData rowData, int index, PreparedStatement statement) throws SQLException;
+		void serialize(RowData rowData, int index, FieldNamedPreparedStatement statement) throws SQLException;
 	}
 
 	/**
@@ -188,7 +189,7 @@ public abstract class AbstractJdbcRowConverter implements JdbcRowConverter {
 			TypeConversions.fromLogicalToDataType(type)));
 		return (val, index, statement)  -> {
 			if (val == null || val.isNullAt(index) || LogicalTypeRoot.NULL.equals(type.getTypeRoot())) {
-				statement.setNull(index + 1, sqlType);
+				statement.setNull(index, sqlType);
 			} else {
 				jdbcSerializationConverter.serialize(val, index, statement);
 			}
@@ -198,47 +199,47 @@ public abstract class AbstractJdbcRowConverter implements JdbcRowConverter {
 	protected JdbcSerializationConverter createExternalConverter(LogicalType type) {
 		switch (type.getTypeRoot()) {
 			case BOOLEAN:
-				return (val, index, statement) -> statement.setBoolean(index + 1, val.getBoolean(index));
+				return (val, index, statement) -> statement.setBoolean(index, val.getBoolean(index));
 			case TINYINT:
-				return (val, index, statement) -> statement.setByte(index + 1, val.getByte(index));
+				return (val, index, statement) -> statement.setByte(index, val.getByte(index));
 			case SMALLINT:
-				return (val, index, statement) -> statement.setShort(index + 1, val.getShort(index));
+				return (val, index, statement) -> statement.setShort(index, val.getShort(index));
 			case INTEGER:
 			case INTERVAL_YEAR_MONTH:
-				return (val, index, statement) -> statement.setInt(index + 1, val.getInt(index));
+				return (val, index, statement) -> statement.setInt(index, val.getInt(index));
 			case BIGINT:
 			case INTERVAL_DAY_TIME:
-				return (val, index, statement) -> statement.setLong(index + 1, val.getLong(index));
+				return (val, index, statement) -> statement.setLong(index, val.getLong(index));
 			case FLOAT:
-				return (val, index, statement) -> statement.setFloat(index + 1, val.getFloat(index));
+				return (val, index, statement) -> statement.setFloat(index, val.getFloat(index));
 			case DOUBLE:
-				return (val, index, statement) -> statement.setDouble(index + 1, val.getDouble(index));
+				return (val, index, statement) -> statement.setDouble(index, val.getDouble(index));
 			case CHAR:
 			case VARCHAR:
 				// value is BinaryString
-				return (val, index, statement) -> statement.setString(index + 1, val.getString(index).toString());
+				return (val, index, statement) -> statement.setString(index, val.getString(index).toString());
 			case BINARY:
 			case VARBINARY:
-				return (val, index, statement) -> statement.setBytes(index + 1, val.getBinary(index));
+				return (val, index, statement) -> statement.setBytes(index, val.getBinary(index));
 			case DATE:
 				return (val, index, statement) ->
-					statement.setDate(index + 1, Date.valueOf(LocalDate.ofEpochDay(val.getInt(index))));
+					statement.setDate(index, Date.valueOf(LocalDate.ofEpochDay(val.getInt(index))));
 			case TIME_WITHOUT_TIME_ZONE:
 				return (val, index, statement) ->
-					statement.setTime(index + 1, Time.valueOf(LocalTime.ofNanoOfDay(val.getInt(index) * 1_000_000L)));
+					statement.setTime(index, Time.valueOf(LocalTime.ofNanoOfDay(val.getInt(index) * 1_000_000L)));
 			case TIMESTAMP_WITH_TIME_ZONE:
 			case TIMESTAMP_WITHOUT_TIME_ZONE:
 				final int timestampPrecision = ((TimestampType) type).getPrecision();
 				return (val, index, statement) ->
 					statement.setTimestamp(
-						index + 1,
+						index,
 						val.getTimestamp(index, timestampPrecision).toTimestamp());
 			case DECIMAL:
 				final int decimalPrecision = ((DecimalType) type).getPrecision();
 				final int decimalScale = ((DecimalType) type).getScale();
 				return (val, index, statement) ->
 					statement.setBigDecimal(
-						index + 1,
+						index,
 						val.getDecimal(index, decimalPrecision, decimalScale).toBigDecimal());
 			case ARRAY:
 			case MAP:
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/JdbcRowConverter.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/JdbcRowConverter.java
index 18e6f613a50..eca2bbf0550 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/JdbcRowConverter.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/JdbcRowConverter.java
@@ -18,10 +18,10 @@
 
 package org.apache.flink.connector.jdbc.internal.converter;
 
+import org.apache.flink.connector.jdbc.statement.FieldNamedPreparedStatement;
 import org.apache.flink.table.data.RowData;
 
 import java.io.Serializable;
-import java.sql.PreparedStatement;
 import java.sql.ResultSet;
 import java.sql.SQLException;
 
@@ -44,5 +44,5 @@ public interface JdbcRowConverter extends Serializable {
 	 * @param statement The statement to be filled.
 	 * @return The filled statement.
 	 */
-	PreparedStatement toExternal(RowData rowData, PreparedStatement statement) throws SQLException;
+	FieldNamedPreparedStatement toExternal(RowData rowData, FieldNamedPreparedStatement statement) throws SQLException;
 }
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/executor/InsertOrUpdateJdbcExecutor.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/executor/InsertOrUpdateJdbcExecutor.java
index 14b758ecaaa..d931773f6bc 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/executor/InsertOrUpdateJdbcExecutor.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/executor/InsertOrUpdateJdbcExecutor.java
@@ -39,6 +39,9 @@ import static org.apache.flink.util.Preconditions.checkNotNull;
 /**
  * {@link JdbcBatchStatementExecutor} that provides upsert semantics by updating row if it exists and inserting otherwise.
  * Used in Table API.
+ *
+ * @deprecated This has been replaced with {@link TableInsertOrUpdateStatementExecutor}, will remove
+ * this once {@link org.apache.flink.connector.jdbc.table.JdbcUpsertTableSink} is removed.
  */
 @Internal
 public final class InsertOrUpdateJdbcExecutor<R, K, V> implements JdbcBatchStatementExecutor<R> {
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/executor/BufferReduceStatementExecutor.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/executor/TableBufferReducedStatementExecutor.java
similarity index 89%
rename from flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/executor/BufferReduceStatementExecutor.java
rename to flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/executor/TableBufferReducedStatementExecutor.java
index 39d0959f4ce..cfe12d81e78 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/executor/BufferReduceStatementExecutor.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/executor/TableBufferReducedStatementExecutor.java
@@ -32,7 +32,7 @@ import java.util.function.Function;
  * Currently, this statement executor is only used for table/sql to buffer insert/update/delete
  * events, and reduce them in buffer before submit to external database.
  */
-public class BufferReduceStatementExecutor implements JdbcBatchStatementExecutor<RowData> {
+public final class TableBufferReducedStatementExecutor implements JdbcBatchStatementExecutor<RowData> {
 
 	private final JdbcBatchStatementExecutor<RowData> upsertExecutor;
 	private final JdbcBatchStatementExecutor<RowData> deleteExecutor;
@@ -41,7 +41,7 @@ public class BufferReduceStatementExecutor implements JdbcBatchStatementExecutor
 	// the mapping is [KEY, <+/-, VALUE>]
 	private final Map<RowData, Tuple2<Boolean, RowData>> reduceBuffer = new HashMap<>();
 
-	public BufferReduceStatementExecutor(
+	public TableBufferReducedStatementExecutor(
 			JdbcBatchStatementExecutor<RowData> upsertExecutor,
 			JdbcBatchStatementExecutor<RowData> deleteExecutor,
 			Function<RowData, RowData> keyExtractor,
@@ -87,12 +87,12 @@ public class BufferReduceStatementExecutor implements JdbcBatchStatementExecutor
 
 	@Override
 	public void executeBatch() throws SQLException {
-		// TODO: reuse the extracted key and avoid value copy in the future
-		for (Tuple2<Boolean, RowData> tuple : reduceBuffer.values()) {
-			if (tuple.f0) {
-				upsertExecutor.addToBatch(tuple.f1);
+		for (Map.Entry<RowData, Tuple2<Boolean, RowData>> entry : reduceBuffer.entrySet()) {
+			if (entry.getValue().f0) {
+				upsertExecutor.addToBatch(entry.getValue().f1);
 			} else {
-				deleteExecutor.addToBatch(tuple.f1);
+				// delete by key
+				deleteExecutor.addToBatch(entry.getKey());
 			}
 		}
 		upsertExecutor.executeBatch();
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/executor/TableBufferedStatementExecutor.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/executor/TableBufferedStatementExecutor.java
new file mode 100644
index 00000000000..7f099b651ea
--- /dev/null
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/executor/TableBufferedStatementExecutor.java
@@ -0,0 +1,72 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connector.jdbc.internal.executor;
+
+import org.apache.flink.table.data.RowData;
+
+import java.sql.Connection;
+import java.sql.PreparedStatement;
+import java.sql.SQLException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.function.Function;
+
+/**
+ * Currently, this statement executor is only used for table/sql to buffer records,
+ * because the {@link PreparedStatement#executeBatch()} may fail and clear buffered records,
+ * so we have to buffer the records and replay the records when retrying {@link #executeBatch()}.
+ */
+public final class TableBufferedStatementExecutor implements JdbcBatchStatementExecutor<RowData> {
+
+	private final JdbcBatchStatementExecutor<RowData> statementExecutor;
+	private final Function<RowData, RowData> valueTransform;
+	private final List<RowData> buffer = new ArrayList<>();
+
+	public TableBufferedStatementExecutor(
+			JdbcBatchStatementExecutor<RowData> statementExecutor,
+			Function<RowData, RowData> valueTransform) {
+		this.statementExecutor = statementExecutor;
+		this.valueTransform = valueTransform;
+	}
+
+	@Override
+	public void prepareStatements(Connection connection) throws SQLException {
+		statementExecutor.prepareStatements(connection);
+	}
+
+	@Override
+	public void addToBatch(RowData record) throws SQLException {
+		RowData value = valueTransform.apply(record); // copy or not
+		buffer.add(value);
+	}
+
+	@Override
+	public void executeBatch() throws SQLException {
+		for (RowData value : buffer) {
+			statementExecutor.addToBatch(value);
+		}
+		statementExecutor.executeBatch();
+		buffer.clear();
+	}
+
+	@Override
+	public void closeStatements() throws SQLException {
+		statementExecutor.closeStatements();
+	}
+}
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/executor/TableInsertOrUpdateStatementExecutor.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/executor/TableInsertOrUpdateStatementExecutor.java
new file mode 100644
index 00000000000..e2fa8500121
--- /dev/null
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/executor/TableInsertOrUpdateStatementExecutor.java
@@ -0,0 +1,115 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connector.jdbc.internal.executor;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.connector.jdbc.internal.converter.JdbcRowConverter;
+import org.apache.flink.connector.jdbc.statement.FieldNamedPreparedStatement;
+import org.apache.flink.connector.jdbc.statement.StatementFactory;
+import org.apache.flink.table.data.RowData;
+
+import java.sql.Connection;
+import java.sql.ResultSet;
+import java.sql.SQLException;
+import java.util.Arrays;
+import java.util.function.Function;
+
+import static org.apache.flink.util.Preconditions.checkNotNull;
+
+/**
+ * {@link JdbcBatchStatementExecutor} that provides upsert semantics by updating row
+ * if it exists and inserting otherwise. Only used in Table/SQL API.
+ */
+@Internal
+public final class TableInsertOrUpdateStatementExecutor implements JdbcBatchStatementExecutor<RowData> {
+
+	private final StatementFactory existStmtFactory;
+	private final StatementFactory insertStmtFactory;
+	private final StatementFactory updateStmtFactory;
+
+	private final JdbcRowConverter existSetter;
+	private final JdbcRowConverter insertSetter;
+	private final JdbcRowConverter updateSetter;
+
+	private final Function<RowData, RowData> keyExtractor;
+
+	private transient FieldNamedPreparedStatement existStatement;
+	private transient FieldNamedPreparedStatement insertStatement;
+	private transient FieldNamedPreparedStatement updateStatement;
+
+	public TableInsertOrUpdateStatementExecutor(
+			StatementFactory existStmtFactory,
+			StatementFactory insertStmtFactory,
+			StatementFactory updateStmtFactory,
+			JdbcRowConverter existSetter,
+			JdbcRowConverter insertSetter,
+			JdbcRowConverter updateSetter,
+			Function<RowData, RowData> keyExtractor) {
+		this.existStmtFactory = checkNotNull(existStmtFactory);
+		this.insertStmtFactory = checkNotNull(insertStmtFactory);
+		this.updateStmtFactory = checkNotNull(updateStmtFactory);
+		this.existSetter = checkNotNull(existSetter);
+		this.insertSetter = checkNotNull(insertSetter);
+		this.updateSetter = checkNotNull(updateSetter);
+		this.keyExtractor = keyExtractor;
+	}
+
+	@Override
+	public void prepareStatements(Connection connection) throws SQLException {
+		existStatement = existStmtFactory.createStatement(connection);
+		insertStatement = insertStmtFactory.createStatement(connection);
+		updateStatement = updateStmtFactory.createStatement(connection);
+	}
+
+	@Override
+	public void addToBatch(RowData record) throws SQLException {
+		processOneRowInBatch(keyExtractor.apply(record), record);
+	}
+
+	private void processOneRowInBatch(RowData pk, RowData row) throws SQLException {
+		if (exist(pk)) {
+			updateSetter.toExternal(row, updateStatement);
+			updateStatement.addBatch();
+		} else {
+			insertSetter.toExternal(row, insertStatement);
+			insertStatement.addBatch();
+		}
+	}
+
+	private boolean exist(RowData pk) throws SQLException {
+		existSetter.toExternal(pk, existStatement);
+		try (ResultSet resultSet = existStatement.executeQuery()) {
+			return resultSet.next();
+		}
+	}
+
+	@Override
+	public void executeBatch() throws SQLException {
+		updateStatement.executeBatch();
+		insertStatement.executeBatch();
+	}
+
+	@Override
+	public void closeStatements() throws SQLException {
+		for (FieldNamedPreparedStatement s : Arrays.asList(existStatement, insertStatement, updateStatement)) {
+			if (s != null) {
+				s.close();
+			}
+		}
+	}
+}
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/executor/TableSimpleStatementExecutor.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/executor/TableSimpleStatementExecutor.java
new file mode 100644
index 00000000000..6a85a28b68c
--- /dev/null
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/executor/TableSimpleStatementExecutor.java
@@ -0,0 +1,73 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connector.jdbc.internal.executor;
+
+import org.apache.flink.connector.jdbc.internal.converter.JdbcRowConverter;
+import org.apache.flink.connector.jdbc.statement.FieldNamedPreparedStatement;
+import org.apache.flink.connector.jdbc.statement.StatementFactory;
+import org.apache.flink.table.data.RowData;
+
+import java.sql.Connection;
+import java.sql.SQLException;
+
+import static org.apache.flink.util.Preconditions.checkNotNull;
+
+/**
+ * A {@link JdbcBatchStatementExecutor} that simply adds the records into batches of
+ * {@link java.sql.PreparedStatement} and doesn't buffer records in memory. Only used in Table/SQL API.
+ */
+public final class TableSimpleStatementExecutor implements JdbcBatchStatementExecutor<RowData> {
+
+	private final StatementFactory stmtFactory;
+	private final JdbcRowConverter converter;
+
+	private transient FieldNamedPreparedStatement st;
+
+	/**
+	 * Keep in mind object reuse: if it's on then key extractor may be required to return new object.
+	 */
+	public TableSimpleStatementExecutor(StatementFactory stmtFactory, JdbcRowConverter converter) {
+		this.stmtFactory = checkNotNull(stmtFactory);
+		this.converter = checkNotNull(converter);
+	}
+
+	@Override
+	public void prepareStatements(Connection connection) throws SQLException {
+		st = stmtFactory.createStatement(connection);
+	}
+
+	@Override
+	public void addToBatch(RowData record) throws SQLException {
+		converter.toExternal(record, st);
+		st.addBatch();
+	}
+
+	@Override
+	public void executeBatch() throws SQLException {
+		st.executeBatch();
+	}
+
+	@Override
+	public void closeStatements() throws SQLException {
+		if (st != null) {
+			st.close();
+			st = null;
+		}
+	}
+}
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/statement/FieldNamedPreparedStatement.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/statement/FieldNamedPreparedStatement.java
new file mode 100644
index 00000000000..ab4f506fcca
--- /dev/null
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/statement/FieldNamedPreparedStatement.java
@@ -0,0 +1,264 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connector.jdbc.statement;
+
+import java.math.BigDecimal;
+import java.sql.Connection;
+import java.sql.Date;
+import java.sql.PreparedStatement;
+import java.sql.ResultSet;
+import java.sql.SQLException;
+import java.sql.Time;
+import java.sql.Timestamp;
+
+/**
+ * This is a wrapper around {@link PreparedStatement} and allows the users to set parameters
+ * by name instead of by index. This allows users to use the same variable parameter multiple
+ * times in a statement.
+ *
+ * <p>Code such as this:
+ *
+ * <pre>
+ *   Connection con = getConnection();
+ *   String query = "select * from my_table where first_name=? or last_name=?";
+ *   PreparedStatement st = con.prepareStatement(query);
+ *   st.setString(1, "bob");
+ *   st.setString(2, "bob");
+ *   ResultSet rs = st.executeQuery();
+ * </pre>
+ *
+ * <p>Can be replaced with:
+ *
+ * <pre>
+ *   Connection con = getConnection();
+ *   String query = "select * from my_table where first_name=:name or last_name=:name";
+ *   FieldNamedPreparedStatement st = FieldNamedPreparedStatement.prepareStatement(con, query, new String[]{"name"});
+ *   st.setString(0, "bob");
+ *   ResultSet rs = st.executeQuery();
+ * </pre>
+ */
+public interface FieldNamedPreparedStatement extends AutoCloseable {
+
+	/**
+	 * Creates a <code>NamedPreparedStatement</code> object for sending
+	 * parameterized SQL statements to the database.
+	 * @param connection the connection used to connect to database.
+	 * @param sql an SQL statement that may contain one or more ':fieldName' as parameter placeholders
+	 * @param fieldNames the field names in schema order used as the parameter names
+	 */
+	static FieldNamedPreparedStatement prepareStatement(
+			Connection connection, String sql, String[] fieldNames) throws SQLException {
+		return FieldNamedPreparedStatementImpl.prepareStatement(connection, sql, fieldNames);
+	}
+
+	/**
+	 * Clears the current parameter values immediately.
+	 *
+	 * <P>In general, parameter values remain in force for repeated use of a
+	 * statement. Setting a parameter value automatically clears its
+	 * previous value.  However, in some cases it is useful to immediately
+	 * release the resources used by the current parameter values; this can
+	 * be done by calling the method <code>clearParameters</code>.
+	 *
+	 * @see PreparedStatement#clearParameters()
+	 */
+	void clearParameters() throws SQLException;
+
+	/**
+	 * Executes the SQL query in this <code>NamedPreparedStatement</code> object
+	 * and returns the <code>ResultSet</code> object generated by the query.
+	 *
+	 * @see PreparedStatement#executeQuery()
+	 */
+	ResultSet executeQuery() throws SQLException;
+
+	/**
+	 * Adds a set of parameters to this <code>NamedPreparedStatement</code>
+	 * object's batch of commands.
+	 *
+	 * @see PreparedStatement#addBatch()
+	 */
+	void addBatch() throws SQLException;
+
+	/**
+	 * Submits a batch of commands to the database for execution and
+	 * if all commands execute successfully, returns an array of update counts.
+	 * The <code>int</code> elements of the array that is returned are ordered
+	 * to correspond to the commands in the batch, which are ordered
+	 * according to the order in which they were added to the batch.
+	 *
+	 * @see PreparedStatement#executeBatch()
+	 */
+	int[] executeBatch() throws SQLException;
+
+	/**
+	 * Sets the designated parameter to SQL <code>NULL</code>.
+	 *
+	 * <P><B>Note:</B> You must specify the parameter's SQL type.
+	 *
+	 * @see PreparedStatement#setNull(int, int)
+	 */
+	void setNull(int fieldIndex, int sqlType) throws SQLException;
+
+	/**
+	 * Sets the designated parameter to the given Java <code>boolean</code> value.
+	 * The driver converts this
+	 * to an SQL <code>BIT</code> or <code>BOOLEAN</code> value when it sends it to the database.
+	 *
+	 * @see PreparedStatement#setBoolean(int, boolean)
+	 */
+	void setBoolean(int fieldIndex, boolean x) throws SQLException;
+
+	/**
+	 * Sets the designated parameter to the given Java <code>byte</code> value.
+	 * The driver converts this
+	 * to an SQL <code>TINYINT</code> value when it sends it to the database.
+	 *
+	 * @see PreparedStatement#setByte(int, byte)
+	 */
+	void setByte(int fieldIndex, byte x) throws SQLException;
+
+	/**
+	 * Sets the designated parameter to the given Java <code>short</code> value.
+	 * The driver converts this
+	 * to an SQL <code>SMALLINT</code> value when it sends it to the database.
+	 *
+	 * @see PreparedStatement#setShort(int, short)
+	 */
+	void setShort(int fieldIndex, short x) throws SQLException;
+
+	/**
+	 * Sets the designated parameter to the given Java <code>int</code> value.
+	 * The driver converts this
+	 * to an SQL <code>INTEGER</code> value when it sends it to the database.
+	 *
+	 * @see PreparedStatement#setInt(int, int)
+	 */
+	void setInt(int fieldIndex, int x) throws SQLException;
+
+	/**
+	 * Sets the designated parameter to the given Java <code>long</code> value.
+	 * The driver converts this
+	 * to an SQL <code>BIGINT</code> value when it sends it to the database.
+	 *
+	 * @see PreparedStatement#setLong(int, long)
+	 */
+	void setLong(int fieldIndex, long x) throws SQLException;
+
+	/**
+	 * Sets the designated parameter to the given Java <code>float</code> value.
+	 * The driver converts this
+	 * to an SQL <code>REAL</code> value when it sends it to the database.
+	 *
+	 * @see PreparedStatement#setFloat(int, float)
+	 */
+	void setFloat(int fieldIndex, float x) throws SQLException;
+
+	/**
+	 * Sets the designated parameter to the given Java <code>double</code> value.
+	 * The driver converts this
+	 * to an SQL <code>DOUBLE</code> value when it sends it to the database.
+	 *
+	 * @see PreparedStatement#setDouble(int, double)
+	 */
+	void setDouble(int fieldIndex, double x) throws SQLException;
+
+	/**
+	 * Sets the designated parameter to the given <code>java.math.BigDecimal</code> value.
+	 * The driver converts this to an SQL <code>NUMERIC</code> value when
+	 * it sends it to the database.
+	 *
+	 * @see PreparedStatement#setBigDecimal(int, BigDecimal)
+	 */
+	void setBigDecimal(int fieldIndex, BigDecimal x) throws SQLException;
+
+	/**
+	 * Sets the designated parameter to the given Java <code>String</code> value.
+	 * The driver converts this
+	 * to an SQL <code>VARCHAR</code> or <code>LONGVARCHAR</code> value
+	 * (depending on the argument's
+	 * size relative to the driver's limits on <code>VARCHAR</code> values)
+	 * when it sends it to the database.
+	 *
+	 * @see PreparedStatement#setString(int, String)
+	 */
+	void setString(int fieldIndex, String x) throws SQLException;
+
+	/**
+	 * Sets the designated parameter to the given Java array of bytes. The driver converts
+	 * this to an SQL <code>VARBINARY</code> or <code>LONGVARBINARY</code>
+	 * (depending on the argument's size relative to the driver's limits on
+	 * <code>VARBINARY</code> values) when it sends it to the database.
+	 *
+	 * @see PreparedStatement#setBytes(int, byte[])
+	 */
+	void setBytes(int fieldIndex, byte[] x) throws SQLException;
+
+	/**
+	 * Sets the designated parameter to the given <code>java.sql.Date</code> value
+	 * using the default time zone of the virtual machine that is running
+	 * the application.
+	 * The driver converts this
+	 * to an SQL <code>DATE</code> value when it sends it to the database.
+	 *
+	 * @see PreparedStatement#setDate(int, Date)
+	 */
+	void setDate(int fieldIndex, java.sql.Date x)
+		throws SQLException;
+
+	/**
+	 * Sets the designated parameter to the given <code>java.sql.Time</code> value.
+	 * The driver converts this
+	 * to an SQL <code>TIME</code> value when it sends it to the database.
+	 *
+	 * @see PreparedStatement#setTime(int, Time)
+	 */
+	void setTime(int fieldIndex, java.sql.Time x)
+		throws SQLException;
+
+	/**
+	 * Sets the designated parameter to the given <code>java.sql.Timestamp</code> value.
+	 * The driver
+	 * converts this to an SQL <code>TIMESTAMP</code> value when it sends it to the
+	 * database.
+	 *
+	 * @see PreparedStatement#setTimestamp(int, Timestamp)
+	 * */
+	void setTimestamp(int fieldIndex, java.sql.Timestamp x)
+		throws SQLException;
+
+	/**
+	 * Sets the value of the designated parameter using the given object.
+	 *
+	 * @see PreparedStatement#setObject(int, Object)
+	 */
+	void setObject(int fieldIndex, Object x) throws SQLException;
+
+	/**
+	 * Releases this <code>Statement</code> object's database
+	 * and JDBC resources immediately instead of waiting for
+	 * this to happen when it is automatically closed.
+	 * It is generally good practice to release resources as soon as
+	 * you are finished with them to avoid tying up database
+	 * resources.
+	 *
+	 * @see PreparedStatement#close()
+	 */
+	void close() throws SQLException;
+}
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/statement/FieldNamedPreparedStatementImpl.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/statement/FieldNamedPreparedStatementImpl.java
new file mode 100644
index 00000000000..89c4f0a50bd
--- /dev/null
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/statement/FieldNamedPreparedStatementImpl.java
@@ -0,0 +1,240 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connector.jdbc.statement;
+
+import java.math.BigDecimal;
+import java.sql.Connection;
+import java.sql.Date;
+import java.sql.PreparedStatement;
+import java.sql.ResultSet;
+import java.sql.SQLException;
+import java.sql.Time;
+import java.sql.Timestamp;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import static org.apache.flink.util.Preconditions.checkArgument;
+import static org.apache.flink.util.Preconditions.checkNotNull;
+
+/**
+ * Simple implementation of {@link FieldNamedPreparedStatement}.
+ */
+public class FieldNamedPreparedStatementImpl implements FieldNamedPreparedStatement {
+
+	private final PreparedStatement statement;
+	private final int[][] indexMapping;
+
+	private FieldNamedPreparedStatementImpl(PreparedStatement statement, int[][] indexMapping) {
+		this.statement = statement;
+		this.indexMapping = indexMapping;
+	}
+
+	@Override
+	public void clearParameters() throws SQLException {
+		statement.clearParameters();
+	}
+
+	@Override
+	public ResultSet executeQuery() throws SQLException {
+		return statement.executeQuery();
+	}
+
+	@Override
+	public void addBatch() throws SQLException {
+		statement.addBatch();
+	}
+
+	@Override
+	public int[] executeBatch() throws SQLException {
+		return statement.executeBatch();
+	}
+
+	@Override
+	public void setNull(int fieldIndex, int sqlType) throws SQLException {
+		for (int index : indexMapping[fieldIndex]) {
+			statement.setNull(index, sqlType);
+		}
+	}
+
+	@Override
+	public void setBoolean(int fieldIndex, boolean x) throws SQLException {
+		for (int index : indexMapping[fieldIndex]) {
+			statement.setBoolean(index, x);
+		}
+	}
+
+	@Override
+	public void setByte(int fieldIndex, byte x) throws SQLException {
+		for (int index : indexMapping[fieldIndex]) {
+			statement.setByte(index, x);
+		}
+	}
+
+	@Override
+	public void setShort(int fieldIndex, short x) throws SQLException {
+		for (int index : indexMapping[fieldIndex]) {
+			statement.setShort(index, x);
+		}
+	}
+
+	@Override
+	public void setInt(int fieldIndex, int x) throws SQLException {
+		for (int index : indexMapping[fieldIndex]) {
+			statement.setInt(index, x);
+		}
+	}
+
+	@Override
+	public void setLong(int fieldIndex, long x) throws SQLException {
+		for (int index : indexMapping[fieldIndex]) {
+			statement.setLong(index, x);
+		}
+	}
+
+	@Override
+	public void setFloat(int fieldIndex, float x) throws SQLException {
+		for (int index : indexMapping[fieldIndex]) {
+			statement.setFloat(index, x);
+		}
+	}
+
+	@Override
+	public void setDouble(int fieldIndex, double x) throws SQLException {
+		for (int index : indexMapping[fieldIndex]) {
+			statement.setDouble(index, x);
+		}
+	}
+
+	@Override
+	public void setBigDecimal(int fieldIndex, BigDecimal x) throws SQLException {
+		for (int index : indexMapping[fieldIndex]) {
+			statement.setBigDecimal(index, x);
+		}
+	}
+
+	@Override
+	public void setString(int fieldIndex, String x) throws SQLException {
+		for (int index : indexMapping[fieldIndex]) {
+			statement.setString(index, x);
+		}
+	}
+
+	@Override
+	public void setBytes(int fieldIndex, byte[] x) throws SQLException {
+		for (int index : indexMapping[fieldIndex]) {
+			statement.setBytes(index, x);
+		}
+	}
+
+	@Override
+	public void setDate(int fieldIndex, Date x) throws SQLException {
+		for (int index : indexMapping[fieldIndex]) {
+			statement.setDate(index, x);
+		}
+	}
+
+	@Override
+	public void setTime(int fieldIndex, Time x) throws SQLException {
+		for (int index : indexMapping[fieldIndex]) {
+			statement.setTime(index, x);
+		}
+	}
+
+	@Override
+	public void setTimestamp(int fieldIndex, Timestamp x) throws SQLException {
+		for (int index : indexMapping[fieldIndex]) {
+			statement.setTimestamp(index, x);
+		}
+	}
+
+	@Override
+	public void setObject(int fieldIndex, Object x) throws SQLException {
+		for (int index : indexMapping[fieldIndex]) {
+			statement.setObject(index, x);
+		}
+	}
+
+	@Override
+	public void close() throws SQLException {
+		statement.close();
+	}
+
+	// ----------------------------------------------------------------------------------------
+
+	public static FieldNamedPreparedStatement prepareStatement(Connection connection, String sql, String[] fieldNames) throws SQLException {
+		checkNotNull(connection, "connection must not be null.");
+		checkNotNull(sql, "sql must not be null.");
+		checkNotNull(fieldNames, "fieldNames must not be null.");
+
+		if (sql.contains("?")) {
+			throw new IllegalArgumentException("SQL statement must not contain ? character.");
+		}
+
+		HashMap<String, List<Integer>> parameterMap = new HashMap<>();
+		String parsedSQL = parseNamedStatement(sql, parameterMap);
+		// currently, the statements must contain all the field parameters
+		checkArgument(parameterMap.size() == fieldNames.length);
+		int[][] indexMapping = new int[fieldNames.length][];
+		for (int i = 0; i < fieldNames.length; i++) {
+			String fieldName = fieldNames[i];
+			checkArgument(
+				parameterMap.containsKey(fieldName),
+				fieldName + " doesn't exist in the parameters of SQL statement: " + sql);
+			indexMapping[i] = parameterMap
+				.get(fieldName)
+				.stream()
+				.mapToInt(v -> v).toArray();
+		}
+
+		return new FieldNamedPreparedStatementImpl(connection.prepareStatement(parsedSQL), indexMapping);
+	}
+
+	/**
+	 * Parses a sql with named parameters. The parameter-index mappings are put into the map,
+	 * and the parsed sql is returned.
+	 * @param sql    sql to parse
+	 * @param paramMap map to hold parameter-index mappings
+	 * @return the parsed sql
+	 */
+	public static String parseNamedStatement(String sql, Map<String, List<Integer>> paramMap) {
+		StringBuilder parsedSql = new StringBuilder();
+		int fieldIndex = 1; // SQL statement parameter index starts from 1
+		int length = sql.length();
+		for (int i = 0; i < length; i++) {
+			char c = sql.charAt(i);
+			if (':' == c) {
+				int j = i + 1;
+				while (j < length && Character.isJavaIdentifierPart(sql.charAt(j))) {
+					j++;
+				}
+				String parameterName = sql.substring(i + 1, j);
+				checkArgument(!parameterName.isEmpty(), "Named parameters in SQL statement must not be empty.");
+				paramMap.computeIfAbsent(parameterName, n -> new ArrayList<>()).add(fieldIndex);
+				fieldIndex++;
+				i = j - 1;
+				parsedSql.append('?');
+			} else {
+				parsedSql.append(c);
+			}
+		}
+		return parsedSql.toString();
+	}
+}
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/statement/StatementFactory.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/statement/StatementFactory.java
new file mode 100644
index 00000000000..ac58c866ce6
--- /dev/null
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/statement/StatementFactory.java
@@ -0,0 +1,33 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connector.jdbc.statement;
+
+import java.sql.Connection;
+import java.sql.SQLException;
+
+/**
+ * A factory to create {@link FieldNamedPreparedStatement} with the given {@link Connection}.
+ */
+public interface StatementFactory {
+
+	/**
+	 * Creates {@link FieldNamedPreparedStatement} with the given {@link Connection}.
+	 */
+	FieldNamedPreparedStatement createStatement(Connection connection) throws SQLException;
+}
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicOutputFormatBuilder.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicOutputFormatBuilder.java
index 3b31d3941a9..1178987e83b 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicOutputFormatBuilder.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicOutputFormatBuilder.java
@@ -22,17 +22,18 @@ import org.apache.flink.api.common.functions.RuntimeContext;
 import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.connector.jdbc.JdbcExecutionOptions;
-import org.apache.flink.connector.jdbc.JdbcStatementBuilder;
 import org.apache.flink.connector.jdbc.dialect.JdbcDialect;
 import org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat;
 import org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider;
 import org.apache.flink.connector.jdbc.internal.converter.JdbcRowConverter;
-import org.apache.flink.connector.jdbc.internal.executor.BufferReduceStatementExecutor;
-import org.apache.flink.connector.jdbc.internal.executor.InsertOrUpdateJdbcExecutor;
 import org.apache.flink.connector.jdbc.internal.executor.JdbcBatchStatementExecutor;
+import org.apache.flink.connector.jdbc.internal.executor.TableBufferReducedStatementExecutor;
+import org.apache.flink.connector.jdbc.internal.executor.TableBufferedStatementExecutor;
+import org.apache.flink.connector.jdbc.internal.executor.TableInsertOrUpdateStatementExecutor;
+import org.apache.flink.connector.jdbc.internal.executor.TableSimpleStatementExecutor;
 import org.apache.flink.connector.jdbc.internal.options.JdbcDmlOptions;
 import org.apache.flink.connector.jdbc.internal.options.JdbcOptions;
-import org.apache.flink.connector.jdbc.utils.JdbcUtils;
+import org.apache.flink.connector.jdbc.statement.FieldNamedPreparedStatement;
 import org.apache.flink.table.data.GenericRowData;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.types.DataType;
@@ -52,6 +53,8 @@ import static org.apache.flink.util.Preconditions.checkNotNull;
  */
 public class JdbcDynamicOutputFormatBuilder implements Serializable {
 
+	private static final long serialVersionUID = 1L;
+
 	private JdbcOptions jdbcOptions;
 	private JdbcExecutionOptions executionOptions;
 	private JdbcDmlOptions dmlOptions;
@@ -109,80 +112,107 @@ public class JdbcDynamicOutputFormatBuilder implements Serializable {
 			return new JdbcBatchingOutputFormat<>(
 				new SimpleJdbcConnectionProvider(jdbcOptions),
 				executionOptions,
-				ctx -> createSimpleRowDataExecutor(dmlOptions.getDialect(), sql, logicalTypes, ctx, rowDataTypeInformation),
+				ctx -> createSimpleBufferedExecutor(
+					ctx,
+					dmlOptions.getDialect(),
+					dmlOptions.getFieldNames(),
+					logicalTypes,
+					sql,
+					rowDataTypeInformation),
 				JdbcBatchingOutputFormat.RecordExtractor.identity());
 		}
 	}
 
-	private static JdbcBatchStatementExecutor<RowData> createKeyedRowExecutor(
-			JdbcDialect dialect,
-			int[] pkFields,
-			LogicalType[] pkTypes,
-			String sql,
-			LogicalType[] logicalTypes) {
-		final JdbcRowConverter rowConverter = dialect.getRowConverter(RowType.of(pkTypes));
-		final Function<RowData, RowData> keyExtractor = createRowKeyExtractor(logicalTypes, pkFields);
-		return JdbcBatchStatementExecutor.keyed(
-			sql,
-			keyExtractor,
-			(st, record) -> rowConverter.toExternal(keyExtractor.apply(record), st));
-	}
-
 	private static JdbcBatchStatementExecutor<RowData> createBufferReduceExecutor(
 			JdbcDmlOptions opt,
 			RuntimeContext ctx,
 			TypeInformation<RowData> rowDataTypeInfo,
 			LogicalType[] fieldTypes) {
 		checkArgument(opt.getKeyFields().isPresent());
-		int[] pkFields = Arrays.stream(opt.getKeyFields().get()).mapToInt(Arrays.asList(opt.getFieldNames())::indexOf).toArray();
+		JdbcDialect dialect = opt.getDialect();
+		String tableName = opt.getTableName();
+		String[] pkNames = opt.getKeyFields().get();
+		int[] pkFields = Arrays.stream(pkNames).mapToInt(Arrays.asList(opt.getFieldNames())::indexOf).toArray();
 		LogicalType[] pkTypes = Arrays.stream(pkFields).mapToObj(f -> fieldTypes[f]).toArray(LogicalType[]::new);
 		final TypeSerializer<RowData> typeSerializer = rowDataTypeInfo.createSerializer(ctx.getExecutionConfig());
 		final Function<RowData, RowData> valueTransform = ctx.getExecutionConfig().isObjectReuseEnabled() ? typeSerializer::copy : Function.identity();
 
-		JdbcBatchStatementExecutor<RowData> upsertExecutor = createUpsertRowExecutor(opt, ctx, rowDataTypeInfo, pkFields, pkTypes, fieldTypes, valueTransform);
-		JdbcBatchStatementExecutor<RowData> deleteExecutor = createDeleteExecutor(opt, pkFields, pkTypes, fieldTypes);
-
-		return new BufferReduceStatementExecutor(
-			upsertExecutor,
-			deleteExecutor,
+		return new TableBufferReducedStatementExecutor(
+			createUpsertRowExecutor(dialect, tableName, opt.getFieldNames(), fieldTypes, pkFields, pkNames, pkTypes),
+			createDeleteExecutor(dialect, tableName, pkNames, pkTypes),
 			createRowKeyExtractor(fieldTypes, pkFields),
 			valueTransform);
 	}
 
-	private static JdbcBatchStatementExecutor<RowData> createUpsertRowExecutor(
-			JdbcDmlOptions opt,
+	private static JdbcBatchStatementExecutor<RowData> createSimpleBufferedExecutor(
 			RuntimeContext ctx,
-			TypeInformation<RowData> rowDataTypeInfo,
-			int[] pkFields,
-			LogicalType[] pkTypes,
+			JdbcDialect dialect,
+			String[] fieldNames,
 			LogicalType[] fieldTypes,
-			Function<RowData, RowData> valueTransform) {
-		checkArgument(opt.getKeyFields().isPresent());
-		JdbcDialect dialect = opt.getDialect();
-		return opt.getDialect()
-			.getUpsertStatement(opt.getTableName(), opt.getFieldNames(), opt.getKeyFields().get())
-			.map(sql -> createSimpleRowDataExecutor(dialect, sql, fieldTypes, ctx, rowDataTypeInfo))
-			.orElseGet(() ->
-				new InsertOrUpdateJdbcExecutor<>(
-					opt.getDialect().getRowExistsStatement(opt.getTableName(), opt.getKeyFields().get()),
-					opt.getDialect().getInsertIntoStatement(opt.getTableName(), opt.getFieldNames()),
-					opt.getDialect().getUpdateStatement(opt.getTableName(), opt.getFieldNames(), opt.getKeyFields().get()),
-					createRowDataJdbcStatementBuilder(dialect, pkTypes),
-					createRowDataJdbcStatementBuilder(dialect, fieldTypes),
-					createRowDataJdbcStatementBuilder(dialect, fieldTypes),
-					createRowKeyExtractor(fieldTypes, pkFields),
-					valueTransform));
+			String sql,
+			TypeInformation<RowData> rowDataTypeInfo) {
+		final TypeSerializer<RowData> typeSerializer = rowDataTypeInfo.createSerializer(ctx.getExecutionConfig());
+		return new TableBufferedStatementExecutor(
+			createSimpleRowExecutor(dialect, fieldNames, fieldTypes, sql),
+			ctx.getExecutionConfig().isObjectReuseEnabled() ? typeSerializer::copy : Function.identity()
+		);
+	}
+
+	private static JdbcBatchStatementExecutor<RowData> createUpsertRowExecutor(
+			JdbcDialect dialect,
+			String tableName,
+			String[] fieldNames,
+			LogicalType[] fieldTypes,
+			int[] pkFields,
+			String[] pkNames,
+			LogicalType[] pkTypes) {
+		return dialect
+			.getUpsertStatement(tableName, fieldNames, pkNames)
+			.map(sql -> createSimpleRowExecutor(dialect, fieldNames, fieldTypes, sql))
+			.orElseGet(() -> createInsertOrUpdateExecutor(dialect, tableName, fieldNames, fieldTypes, pkFields, pkNames, pkTypes));
 	}
 
 	private static JdbcBatchStatementExecutor<RowData> createDeleteExecutor(
-			JdbcDmlOptions dmlOptions,
+			JdbcDialect dialect,
+			String tableName,
+			String[] pkNames,
+			LogicalType[] pkTypes) {
+		String deleteSql = dialect.getDeleteStatement(tableName, pkNames);
+		return createSimpleRowExecutor(dialect, pkNames, pkTypes, deleteSql);
+	}
+
+	private static JdbcBatchStatementExecutor<RowData> createSimpleRowExecutor(
+			JdbcDialect dialect,
+			String[] fieldNames,
+			LogicalType[] fieldTypes,
+			final String sql) {
+		final JdbcRowConverter rowConverter = dialect.getRowConverter(RowType.of(fieldTypes));
+		return new TableSimpleStatementExecutor(
+			connection -> FieldNamedPreparedStatement.prepareStatement(connection, sql, fieldNames),
+			rowConverter
+		);
+	}
+
+	private static JdbcBatchStatementExecutor<RowData> createInsertOrUpdateExecutor(
+			JdbcDialect dialect,
+			String tableName,
+			String[] fieldNames,
+			LogicalType[] fieldTypes,
 			int[] pkFields,
-			LogicalType[] pkTypes,
-			LogicalType[] fieldTypes) {
-		checkArgument(dmlOptions.getKeyFields().isPresent());
-		String[] pkNames = Arrays.stream(pkFields).mapToObj(k -> dmlOptions.getFieldNames()[k]).toArray(String[]::new);
-		String deleteSql = dmlOptions.getDialect().getDeleteStatement(dmlOptions.getTableName(), pkNames);
-		return createKeyedRowExecutor(dmlOptions.getDialect(), pkFields, pkTypes, deleteSql, fieldTypes);
+			String[] pkNames,
+			LogicalType[] pkTypes) {
+		final String existStmt = dialect.getRowExistsStatement(tableName, pkNames);
+		final String insertStmt = dialect.getInsertIntoStatement(tableName, fieldNames);
+		final String updateStmt = dialect.getUpdateStatement(tableName, fieldNames, pkNames);
+		return new TableInsertOrUpdateStatementExecutor(
+			connection -> FieldNamedPreparedStatement.prepareStatement(connection, existStmt, pkNames),
+			connection -> FieldNamedPreparedStatement.prepareStatement(connection, insertStmt, fieldNames),
+			connection -> FieldNamedPreparedStatement.prepareStatement(connection, updateStmt, fieldNames),
+			dialect.getRowConverter(RowType.of(pkTypes)),
+			dialect.getRowConverter(RowType.of(fieldTypes)),
+			dialect.getRowConverter(RowType.of(fieldTypes)),
+			createRowKeyExtractor(fieldTypes, pkFields)
+		);
 	}
 
 	private static Function<RowData, RowData> createRowKeyExtractor(LogicalType[] logicalTypes, int[] pkFields) {
@@ -193,23 +223,6 @@ public class JdbcDynamicOutputFormatBuilder implements Serializable {
 		return row -> getPrimaryKey(row, fieldGetters);
 	}
 
-	private static JdbcBatchStatementExecutor<RowData> createSimpleRowDataExecutor(JdbcDialect dialect, String sql, LogicalType[] fieldTypes, RuntimeContext ctx, TypeInformation<RowData> rowDataTypeInfo) {
-		final TypeSerializer<RowData> typeSerializer = rowDataTypeInfo.createSerializer(ctx.getExecutionConfig());
-		return JdbcBatchStatementExecutor.simple(
-			sql,
-			createRowDataJdbcStatementBuilder(dialect, fieldTypes),
-			ctx.getExecutionConfig().isObjectReuseEnabled() ? typeSerializer::copy : Function.identity());
-	}
-
-	/**
-	 * Creates a {@link JdbcStatementBuilder} for {@link RowData} using the provided SQL types array.
-	 * Uses {@link JdbcUtils#setRecordToStatement}
-	 */
-	private static JdbcStatementBuilder<RowData> createRowDataJdbcStatementBuilder(JdbcDialect dialect, LogicalType[] types) {
-		final JdbcRowConverter converter = dialect.getRowConverter(RowType.of(types));
-		return (st, record) -> converter.toExternal(record, st);
-	}
-
 	private static RowData getPrimaryKey(RowData row, RowData.FieldGetter[] fieldGetters) {
 		GenericRowData pkRow = new GenericRowData(fieldGetters.length);
 		for (int i = 0; i < fieldGetters.length; i++) {
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcLookupFunction.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcLookupFunction.java
index 9a4598e3530..422ca2172a1 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcLookupFunction.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcLookupFunction.java
@@ -23,6 +23,7 @@ import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.api.java.typeutils.RowTypeInfo;
 import org.apache.flink.connector.jdbc.internal.options.JdbcLookupOptions;
 import org.apache.flink.connector.jdbc.internal.options.JdbcOptions;
+import org.apache.flink.connector.jdbc.statement.FieldNamedPreparedStatementImpl;
 import org.apache.flink.connector.jdbc.utils.JdbcTypeUtil;
 import org.apache.flink.connector.jdbc.utils.JdbcUtils;
 import org.apache.flink.table.functions.FunctionContext;
@@ -43,6 +44,7 @@ import java.sql.ResultSet;
 import java.sql.SQLException;
 import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.HashMap;
 import java.util.List;
 import java.util.concurrent.TimeUnit;
 
@@ -75,6 +77,7 @@ public class JdbcLookupFunction extends TableFunction<Row> {
 	private final TypeInformation[] keyTypes;
 	private final int[] keySqlTypes;
 	private final String[] fieldNames;
+	private final String[] keyNames;
 	private final TypeInformation[] fieldTypes;
 	private final int[] outputSqlTypes;
 	private final long cacheMaxSize;
@@ -94,6 +97,7 @@ public class JdbcLookupFunction extends TableFunction<Row> {
 		this.password = options.getPassword().orElse(null);
 		this.fieldNames = fieldNames;
 		this.fieldTypes = fieldTypes;
+		this.keyNames = keyNames;
 		List<String> nameList = Arrays.asList(fieldNames);
 		this.keyTypes = Arrays.stream(keyNames)
 				.map(s -> {
@@ -107,8 +111,9 @@ public class JdbcLookupFunction extends TableFunction<Row> {
 		this.maxRetryTimes = lookupOptions.getMaxRetryTimes();
 		this.keySqlTypes = Arrays.stream(keyTypes).mapToInt(JdbcTypeUtil::typeInformationToSqlType).toArray();
 		this.outputSqlTypes = Arrays.stream(fieldTypes).mapToInt(JdbcTypeUtil::typeInformationToSqlType).toArray();
-		this.query = options.getDialect().getSelectFromStatement(
-				options.getTableName(), fieldNames, keyNames);
+		this.query = FieldNamedPreparedStatementImpl.parseNamedStatement(
+			options.getDialect().getSelectFromStatement(options.getTableName(), fieldNames, keyNames),
+			new HashMap<>());
 	}
 
 	public static Builder builder() {
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcRowDataLookupFunction.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcRowDataLookupFunction.java
index 3c3ecf28836..d1526c3bcc8 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcRowDataLookupFunction.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcRowDataLookupFunction.java
@@ -25,6 +25,7 @@ import org.apache.flink.connector.jdbc.dialect.JdbcDialects;
 import org.apache.flink.connector.jdbc.internal.converter.JdbcRowConverter;
 import org.apache.flink.connector.jdbc.internal.options.JdbcLookupOptions;
 import org.apache.flink.connector.jdbc.internal.options.JdbcOptions;
+import org.apache.flink.connector.jdbc.statement.FieldNamedPreparedStatement;
 import org.apache.flink.table.data.GenericRowData;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.functions.FunctionContext;
@@ -42,7 +43,6 @@ import org.slf4j.LoggerFactory;
 import java.io.IOException;
 import java.sql.Connection;
 import java.sql.DriverManager;
-import java.sql.PreparedStatement;
 import java.sql.ResultSet;
 import java.sql.SQLException;
 import java.util.ArrayList;
@@ -69,6 +69,7 @@ public class JdbcRowDataLookupFunction extends TableFunction<RowData> {
 	private final String username;
 	private final String password;
 	private final DataType[] keyTypes;
+	private final String[] keyNames;
 	private final long cacheMaxSize;
 	private final long cacheExpireMs;
 	private final int maxRetryTimes;
@@ -77,7 +78,7 @@ public class JdbcRowDataLookupFunction extends TableFunction<RowData> {
 	private final JdbcRowConverter lookupKeyRowConverter;
 
 	private transient Connection dbConn;
-	private transient PreparedStatement statement;
+	private transient FieldNamedPreparedStatement statement;
 	private transient Cache<RowData, List<RowData>> cache;
 
 	public JdbcRowDataLookupFunction(
@@ -95,6 +96,7 @@ public class JdbcRowDataLookupFunction extends TableFunction<RowData> {
 		this.dbURL = options.getDbURL();
 		this.username = options.getUsername().orElse(null);
 		this.password = options.getPassword().orElse(null);
+		this.keyNames = keyNames;
 		List<String> nameList = Arrays.asList(fieldNames);
 		this.keyTypes = Arrays.stream(keyNames)
 			.map(s -> {
@@ -199,7 +201,7 @@ public class JdbcRowDataLookupFunction extends TableFunction<RowData> {
 		} else {
 			dbConn = DriverManager.getConnection(dbURL, username, password);
 		}
-		statement = dbConn.prepareStatement(query);
+		statement = FieldNamedPreparedStatement.prepareStatement(dbConn, query, keyNames);
 	}
 
 	@Override
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcTableSource.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcTableSource.java
index ff21aae7223..a8462efe2d2 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcTableSource.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcTableSource.java
@@ -25,6 +25,7 @@ import org.apache.flink.connector.jdbc.internal.options.JdbcLookupOptions;
 import org.apache.flink.connector.jdbc.internal.options.JdbcOptions;
 import org.apache.flink.connector.jdbc.internal.options.JdbcReadOptions;
 import org.apache.flink.connector.jdbc.split.JdbcNumericBetweenParametersProvider;
+import org.apache.flink.connector.jdbc.statement.FieldNamedPreparedStatementImpl;
 import org.apache.flink.connector.jdbc.utils.JdbcTypeUtil;
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
@@ -40,6 +41,7 @@ import org.apache.flink.table.utils.TableConnectorUtils;
 import org.apache.flink.types.Row;
 
 import java.util.Arrays;
+import java.util.HashMap;
 import java.util.Objects;
 
 import static org.apache.flink.table.types.utils.TypeConversions.fromDataTypeToLegacyInfo;
@@ -186,8 +188,11 @@ public class JdbcTableSource implements
 
 	private String getBaseQueryStatement(RowTypeInfo rowTypeInfo) {
 		return readOptions.getQuery().orElseGet(() ->
-			options.getDialect().getSelectFromStatement(
-				options.getTableName(), rowTypeInfo.getFieldNames(), new String[0]));
+			FieldNamedPreparedStatementImpl.parseNamedStatement(
+				options.getDialect().getSelectFromStatement(options.getTableName(), rowTypeInfo.getFieldNames(), new String[0]),
+				new HashMap<>()
+			)
+		);
 	}
 
 	@Override
diff --git a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/statement/FieldNamedPreparedStatementImplTest.java b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/statement/FieldNamedPreparedStatementImplTest.java
new file mode 100644
index 00000000000..a2ed14cd06a
--- /dev/null
+++ b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/statement/FieldNamedPreparedStatementImplTest.java
@@ -0,0 +1,174 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connector.jdbc.statement;
+
+import org.apache.flink.connector.jdbc.dialect.JdbcDialect;
+import org.apache.flink.connector.jdbc.dialect.JdbcDialects;
+
+import org.junit.Test;
+
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import static java.util.Arrays.asList;
+import static java.util.Collections.singletonList;
+import static org.junit.Assert.assertEquals;
+
+/**
+ * Tests for {@link FieldNamedPreparedStatementImpl}.
+ */
+public class FieldNamedPreparedStatementImplTest {
+
+	private final JdbcDialect dialect = JdbcDialects.get("jdbc:mysql://localhost:3306/test")
+		.orElseThrow(() -> new RuntimeException("Unsupported dialect."));
+	private final String[] fieldNames = new String[]{"id", "name", "email", "ts", "field1", "field_2", "__field_3__"};
+	private final String[] keyFields = new String[]{"id", "__field_3__"};
+	private final String tableName = "tbl";
+
+	@Test
+	public void testInsertStatement() {
+		String insertStmt = dialect.getInsertIntoStatement(tableName, fieldNames);
+		assertEquals(
+			"INSERT INTO `tbl`(`id`, `name`, `email`, `ts`, `field1`, `field_2`, `__field_3__`) " +
+				"VALUES (:id, :name, :email, :ts, :field1, :field_2, :__field_3__)",
+			insertStmt);
+		NamedStatementMatcher
+			.parsedSql("INSERT INTO `tbl`(`id`, `name`, `email`, `ts`, `field1`, `field_2`, `__field_3__`) " +
+				"VALUES (?, ?, ?, ?, ?, ?, ?)")
+			.parameter("id", singletonList(1))
+			.parameter("name", singletonList(2))
+			.parameter("email", singletonList(3))
+			.parameter("ts", singletonList(4))
+			.parameter("field1", singletonList(5))
+			.parameter("field_2", singletonList(6))
+			.parameter("__field_3__", singletonList(7))
+			.matches(insertStmt);
+	}
+
+	@Test
+	public void testDeleteStatement() {
+		String deleteStmt = dialect.getDeleteStatement(tableName, keyFields);
+		assertEquals(
+			"DELETE FROM `tbl` WHERE `id` = :id AND `__field_3__` = :__field_3__",
+			deleteStmt);
+		NamedStatementMatcher
+			.parsedSql("DELETE FROM `tbl` WHERE `id` = ? AND `__field_3__` = ?")
+			.parameter("id", singletonList(1))
+			.parameter("__field_3__", singletonList(2))
+			.matches(deleteStmt);
+	}
+
+	@Test
+	public void testRowExistsStatement() {
+		String rowExistStmt = dialect.getRowExistsStatement(tableName, keyFields);
+		assertEquals(
+			"SELECT 1 FROM `tbl` WHERE `id` = :id AND `__field_3__` = :__field_3__",
+			rowExistStmt);
+		NamedStatementMatcher
+			.parsedSql("SELECT 1 FROM `tbl` WHERE `id` = ? AND `__field_3__` = ?")
+			.parameter("id", singletonList(1))
+			.parameter("__field_3__", singletonList(2))
+			.matches(rowExistStmt);
+	}
+
+	@Test
+	public void testUpdateStatement() {
+		String updateStmt = dialect.getUpdateStatement(tableName, fieldNames, keyFields);
+		assertEquals(
+			"UPDATE `tbl` SET `id` = :id, `name` = :name, `email` = :email, `ts` = :ts, " +
+				"`field1` = :field1, `field_2` = :field_2, `__field_3__` = :__field_3__ " +
+				"WHERE `id` = :id AND `__field_3__` = :__field_3__",
+			updateStmt);
+		NamedStatementMatcher
+			.parsedSql("UPDATE `tbl` SET `id` = ?, `name` = ?, `email` = ?, `ts` = ?, `field1` = ?, " +
+				"`field_2` = ?, `__field_3__` = ? WHERE `id` = ? AND `__field_3__` = ?")
+			.parameter("id", asList(1, 8))
+			.parameter("name", singletonList(2))
+			.parameter("email", singletonList(3))
+			.parameter("ts", singletonList(4))
+			.parameter("field1", singletonList(5))
+			.parameter("field_2", singletonList(6))
+			.parameter("__field_3__", asList(7, 9))
+			.matches(updateStmt);
+	}
+
+	@Test
+	public void testUpsertStatement() {
+		String upsertStmt = dialect.getUpsertStatement(tableName, fieldNames, keyFields).get();
+		assertEquals(
+			"INSERT INTO `tbl`(`id`, `name`, `email`, `ts`, `field1`, `field_2`, `__field_3__`) " +
+				"VALUES (:id, :name, :email, :ts, :field1, :field_2, :__field_3__) " +
+				"ON DUPLICATE KEY UPDATE `id`=VALUES(`id`), `name`=VALUES(`name`), " +
+				"`email`=VALUES(`email`), `ts`=VALUES(`ts`), `field1`=VALUES(`field1`)," +
+				" `field_2`=VALUES(`field_2`), `__field_3__`=VALUES(`__field_3__`)",
+			upsertStmt);
+		NamedStatementMatcher
+			.parsedSql("INSERT INTO `tbl`(`id`, `name`, `email`, `ts`, `field1`, `field_2`, `__field_3__`) " +
+				"VALUES (?, ?, ?, ?, ?, ?, ?) ON DUPLICATE KEY UPDATE " +
+				"`id`=VALUES(`id`), `name`=VALUES(`name`), `email`=VALUES(`email`), `ts`=VALUES(`ts`)," +
+				" `field1`=VALUES(`field1`), `field_2`=VALUES(`field_2`), `__field_3__`=VALUES(`__field_3__`)")
+			.parameter("id", singletonList(1))
+			.parameter("name", singletonList(2))
+			.parameter("email", singletonList(3))
+			.parameter("ts", singletonList(4))
+			.parameter("field1", singletonList(5))
+			.parameter("field_2", singletonList(6))
+			.parameter("__field_3__", singletonList(7))
+			.matches(upsertStmt);
+	}
+
+	@Test
+	public void testSelectStatement() {
+		String selectStmt = dialect.getSelectFromStatement(tableName, fieldNames, keyFields);
+		assertEquals(
+			"SELECT `id`, `name`, `email`, `ts`, `field1`, `field_2`, `__field_3__` FROM `tbl` " +
+				"WHERE `id` = :id AND `__field_3__` = :__field_3__",
+			selectStmt);
+		NamedStatementMatcher
+			.parsedSql("SELECT `id`, `name`, `email`, `ts`, `field1`, `field_2`, `__field_3__` FROM `tbl` " +
+				"WHERE `id` = ? AND `__field_3__` = ?")
+			.parameter("id", singletonList(1))
+			.parameter("__field_3__", singletonList(2))
+			.matches(selectStmt);
+	}
+
+	private static class NamedStatementMatcher {
+		private String parsedSql;
+		private Map<String, List<Integer>> parameterMap = new HashMap<>();
+
+		public static NamedStatementMatcher parsedSql(String parsedSql) {
+			NamedStatementMatcher spec = new NamedStatementMatcher();
+			spec.parsedSql = parsedSql;
+			return spec;
+		}
+
+		public NamedStatementMatcher parameter(String name, List<Integer> index) {
+			this.parameterMap.put(name, index);
+			return this;
+		}
+
+		public void matches(String statement) {
+			Map<String, List<Integer>> actualParams = new HashMap<>();
+			String actualParsedStmt = FieldNamedPreparedStatementImpl.parseNamedStatement(statement, actualParams);
+			assertEquals(parsedSql, actualParsedStmt);
+			assertEquals(parameterMap, actualParams);
+		}
+	}
+}
diff --git a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicOutputFormatTest.java b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicOutputFormatTest.java
index 3f93b233bfc..8a9471697aa 100644
--- a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicOutputFormatTest.java
+++ b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicOutputFormatTest.java
@@ -57,6 +57,7 @@ import static org.apache.flink.util.ExceptionUtils.findThrowableWithMessage;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
 
 /**
  * Test suite for {@link JdbcDynamicOutputFormatBuilder}.
@@ -108,6 +109,7 @@ public class JdbcDynamicOutputFormatTest extends JdbcDataTestBase {
 				.setJdbcExecutionOptions(JdbcExecutionOptions.builder().build())
 				.build();
 			outputFormat.open(0, 1);
+			fail("Expected exception is not thrown.");
 		} catch (Exception e) {
 			assertTrue(findThrowable(e, IOException.class).isPresent());
 			assertTrue(findThrowableWithMessage(e, expectedMsg).isPresent());
@@ -136,6 +138,7 @@ public class JdbcDynamicOutputFormatTest extends JdbcDataTestBase {
 				.setJdbcExecutionOptions(JdbcExecutionOptions.builder().build())
 				.build();
 			outputFormat.open(0, 1);
+			fail("Expected exception is not thrown.");
 		} catch (Exception e) {
 			assertTrue(findThrowable(e, NullPointerException.class).isPresent());
 			assertTrue(findThrowableWithMessage(e, expectedMsg).isPresent());
@@ -170,6 +173,7 @@ public class JdbcDynamicOutputFormatTest extends JdbcDataTestBase {
 			RowData row = buildGenericData(4, "hello", "world", 0.99, "imthewrongtype");
 			outputFormat.writeRecord(row);
 			outputFormat.close();
+			fail("Expected exception is not thrown.");
 		} catch (Exception e) {
 			assertTrue(findThrowable(e, ClassCastException.class).isPresent());
 		}
@@ -203,6 +207,7 @@ public class JdbcDynamicOutputFormatTest extends JdbcDataTestBase {
 			RowData row = buildGenericData(entry.id, entry.title, entry.author, 0L, entry.qty);
 			outputFormat.writeRecord(row);
 			outputFormat.close();
+			fail("Expected exception is not thrown.");
 		} catch (Exception e) {
 			assertTrue(findThrowable(e, ClassCastException.class).isPresent());
 		}
@@ -238,8 +243,9 @@ public class JdbcDynamicOutputFormatTest extends JdbcDataTestBase {
 
 			outputFormat.writeRecord(row);
 			outputFormat.writeRecord(row); // writing the same record twice must yield a unique key violation.
-
 			outputFormat.close();
+
+			fail("Expected exception is not thrown.");
 		} catch (Exception e) {
 			assertTrue(findThrowable(e, RuntimeException.class).isPresent());
 			assertTrue(findThrowableWithMessage(e, expectedMsg).isPresent());
diff --git a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSinkITCase.java b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSinkITCase.java
index 2d988b080cd..ec552de4faa 100644
--- a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSinkITCase.java
+++ b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSinkITCase.java
@@ -200,7 +200,9 @@ public class JdbcDynamicTableSinkITCase extends AbstractTestBase {
 				") WITH (" +
 				"  'connector'='jdbc'," +
 				"  'url'='" + DB_URL + "'," +
-				"  'table-name'='" + OUTPUT_TABLE1 + "'" +
+				"  'table-name'='" + OUTPUT_TABLE1 + "'," +
+				"  'sink.buffer-flush.max-rows' = '2'," +
+				"  'sink.buffer-flush.interval' = '0'" +
 				")");
 
 		TableResult tableResult = tEnv.executeSql("INSERT INTO upsertSink \n" +
@@ -314,7 +316,7 @@ public class JdbcDynamicTableSinkITCase extends AbstractTestBase {
 			"  'connector' = 'jdbc'," +
 			"  'url'='" + DB_URL + "'," +
 			"  'table-name' = '" + USER_TABLE + "'," +
-			"  'sink.buffer-flush.max-rows' = '100'," +
+			"  'sink.buffer-flush.max-rows' = '2'," +
 			"  'sink.buffer-flush.interval' = '0'" + // disable async flush
 			")");
 		TableEnvUtil.execInsertSqlAndWaitResult(
diff --git a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcLookupTableITCase.java b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcLookupTableITCase.java
index 3e3a4498bd0..1ee6facce53 100644
--- a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcLookupTableITCase.java
+++ b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcLookupTableITCase.java
@@ -116,7 +116,7 @@ public class JdbcLookupTableITCase extends JdbcLookupTestBase {
 				.setTableName(LOOKUP_TABLE)
 				.build())
 			.setSchema(TableSchema.builder().fields(
-				new String[]{"id1", "id2", "comment1", "comment2"},
+				new String[]{"id1", "comment1", "comment2", "id2"},
 				new DataType[]{DataTypes.INT(), DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING()})
 				.build());
 		if (useCache) {
@@ -126,8 +126,9 @@ public class JdbcLookupTableITCase extends JdbcLookupTestBase {
 		tEnv.registerFunction("jdbcLookup",
 			builder.build().getLookupFunction(t.getSchema().getFieldNames()));
 
+		// do not use the first N fields as lookup keys for better coverage
 		String sqlQuery = "SELECT id1, id2, comment1, comment2 FROM T, " +
-			"LATERAL TABLE(jdbcLookup(id1, id2)) AS S(l_id1, l_id2, comment1, comment2)";
+			"LATERAL TABLE(jdbcLookup(id1, id2)) AS S(l_id1, comment1, comment2, l_id2)";
 		return tEnv.executeSql(sqlQuery).collect();
 	}
 
@@ -147,15 +148,16 @@ public class JdbcLookupTableITCase extends JdbcLookupTestBase {
 		tEnv.executeSql(
 			String.format("create table lookup (" +
 				"  id1 INT," +
-				"  id2 VARCHAR," +
 				"  comment1 VARCHAR," +
-				"  comment2 VARCHAR" +
+				"  comment2 VARCHAR," +
+				"  id2 VARCHAR" +
 				") with(" +
 				"  'connector'='jdbc'," +
 				"  'url'='" + DB_URL + "'," +
 				"  'table-name'='" + LOOKUP_TABLE + "'" +
 				"  %s)", useCache ? cacheConfig : ""));
 
+		// do not use the first N fields as lookup keys for better coverage
 		String sqlQuery = "SELECT source.id1, source.id2, L.comment1, L.comment2 FROM T AS source " +
 			"JOIN lookup for system_time as of source.proctime AS L " +
 			"ON source.id1 = L.id1 and source.id2 = L.id2";
