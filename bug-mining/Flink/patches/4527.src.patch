diff --git a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/plan/nodes/datastream/StreamScan.scala b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/plan/nodes/datastream/StreamScan.scala
index dd779deefd2..015622238c9 100644
--- a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/plan/nodes/datastream/StreamScan.scala
+++ b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/plan/nodes/datastream/StreamScan.scala
@@ -66,7 +66,7 @@ trait StreamScan extends CommonScan[CRow] with DataStreamRel {
           outCRow.row = v
           outCRow
         }
-      }).returns(cRowType)
+      }).returns(cRowType).setParallelism(input.getParallelism)
 
     } else {
       // input needs to be converted and wrapped as CRow or time indicators need to be generated
@@ -88,7 +88,11 @@ trait StreamScan extends CommonScan[CRow] with DataStreamRel {
 
       val opName = s"from: (${schema.fieldNames.mkString(", ")})"
 
-      input.process(processFunc).name(opName).returns(cRowType)
+      input
+        .process(processFunc)
+        .name(opName)
+        .returns(cRowType)
+        .setParallelism(input.getParallelism)
     }
   }
 
diff --git a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/factories/utils/TestCollectionTableFactory.scala b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/factories/utils/TestCollectionTableFactory.scala
index ded1cbdbcfa..3a4a45a401c 100644
--- a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/factories/utils/TestCollectionTableFactory.scala
+++ b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/factories/utils/TestCollectionTableFactory.scala
@@ -41,7 +41,7 @@ import org.apache.flink.types.Row
 
 import java.io.IOException
 import java.util
-import java.util.{ArrayList => JArrayList, LinkedList => JLinkedList, List => JList, Map => JMap}
+import java.util.{Optional, ArrayList => JArrayList, LinkedList => JLinkedList, List => JList, Map => JMap}
 
 import scala.collection.JavaConversions._
 
@@ -53,7 +53,7 @@ class TestCollectionTableFactory
 {
 
   override def createTableSource(properties: JMap[String, String]): TableSource[Row] = {
-    getCollectionSource(properties, isStreaming = TestCollectionTableFactory.isStreaming)
+    getCollectionSource(properties, TestCollectionTableFactory.isStreaming)
   }
 
   override def createTableSink(properties: JMap[String, String]): TableSink[Row] = {
@@ -117,7 +117,8 @@ object TestCollectionTableFactory {
     val properties = new DescriptorProperties()
     properties.putProperties(props)
     val schema = properties.getTableSchema(Schema.SCHEMA)
-    new CollectionTableSource(emitIntervalMS, schema, isStreaming)
+    val parallelism = properties.getOptionalInt("parallelism")
+    new CollectionTableSource(emitIntervalMS, schema, isStreaming, parallelism)
   }
 
   def getCollectionSink(props: JMap[String, String]): CollectionTableSink = {
@@ -133,7 +134,8 @@ object TestCollectionTableFactory {
   class CollectionTableSource(
       val emitIntervalMs: Long,
       val schema: TableSchema,
-      val isStreaming: Boolean)
+      val isStreaming: Boolean,
+      val parallelism: Optional[Integer])
     extends BatchTableSource[Row]
     with StreamTableSource[Row]
     with LookupableTableSource[Row] {
@@ -143,17 +145,25 @@ object TestCollectionTableFactory {
     override def isBounded: Boolean = !isStreaming
 
     def getDataSet(execEnv: ExecutionEnvironment): DataSet[Row] = {
-      execEnv.createInput(new TestCollectionInputFormat[Row](emitIntervalMs,
+      val dataSet = execEnv.createInput(new TestCollectionInputFormat[Row](emitIntervalMs,
         SOURCE_DATA,
         rowType.createSerializer(new ExecutionConfig)),
         rowType)
+      if (parallelism.isPresent) {
+        dataSet.setParallelism(parallelism.get())
+      }
+      dataSet
     }
 
     override def getDataStream(streamEnv: StreamExecutionEnvironment): DataStreamSource[Row] = {
-      streamEnv.createInput(new TestCollectionInputFormat[Row](emitIntervalMs,
+      val dataStream = streamEnv.createInput(new TestCollectionInputFormat[Row](emitIntervalMs,
         SOURCE_DATA,
         rowType.createSerializer(new ExecutionConfig)),
         rowType)
+      if (parallelism.isPresent) {
+        dataStream.setParallelism(parallelism.get())
+      }
+      dataStream
     }
 
     override def getReturnType: TypeInformation[Row] = rowType
diff --git a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/runtime/stream/table/TableSourceITCase.scala b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/runtime/stream/table/TableSourceITCase.scala
index 421b11f5913..7d0cd88e350 100644
--- a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/runtime/stream/table/TableSourceITCase.scala
+++ b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/runtime/stream/table/TableSourceITCase.scala
@@ -81,6 +81,46 @@ class TableSourceITCase extends AbstractTestBase {
     // info.
   }
 
+  @Test
+  def testStreamScanParallelism(): Unit = {
+    val env = StreamExecutionEnvironment.getExecutionEnvironment
+    val settings = EnvironmentSettings.newInstance().useOldPlanner().build()
+    val tEnv = StreamTableEnvironment.create(env, settings)
+
+    // set environment parallelism to 4
+    env.setParallelism(4)
+
+    // test DataStreamScan
+    val table = env.fromElements[String]()
+      .setParallelism(1)
+      .toTable(tEnv, 'a)
+
+    tEnv.createTemporaryView("MyTable1", table)
+    val parallelism = tEnv.from("MyTable1")
+      .toAppendStream[String]
+      .parallelism
+
+    assertEquals(1, parallelism)
+
+    // test StreamTableSourceScan
+    val createTableStmt =
+      """
+        |CREATE TEMPORARY TABLE MyTable2 (
+        | str varchar
+        |) with (
+        | 'connector' = 'COLLECTION',
+        | 'parallelism' = '1'
+        |)
+        |""".stripMargin
+    tEnv.executeSql(createTableStmt)
+
+    val parallelism2 = tEnv.from("MyTable2")
+      .toAppendStream[String]
+      .parallelism
+
+    assertEquals(1, parallelism2)
+  }
+
   @Test
   def testUnregisteredCsvTableSource(): Unit = {
     val csvTable = CommonTestData.getCsvTableSource
diff --git a/flink-table/flink-table-planner/src/test/scala/resources/testFilterStream0.out b/flink-table/flink-table-planner/src/test/scala/resources/testFilterStream0.out
index cfedce0ae7f..774a1657396 100644
--- a/flink-table/flink-table-planner/src/test/scala/resources/testFilterStream0.out
+++ b/flink-table/flink-table-planner/src/test/scala/resources/testFilterStream0.out
@@ -12,7 +12,7 @@ Stage 1 : Data Source
 
 	Stage 2 : Operator
 		content : from: (a, b)
-		ship_strategy : REBALANCE
+		ship_strategy : FORWARD
 
 		Stage 3 : Operator
 			content : where: (=(MOD(a, 2), 0)), select: (a, b)
diff --git a/flink-table/flink-table-planner/src/test/scala/resources/testUnionStream0.out b/flink-table/flink-table-planner/src/test/scala/resources/testUnionStream0.out
index e8fb70b23fe..3813d498eb5 100644
--- a/flink-table/flink-table-planner/src/test/scala/resources/testUnionStream0.out
+++ b/flink-table/flink-table-planner/src/test/scala/resources/testUnionStream0.out
@@ -17,9 +17,9 @@ Stage 2 : Data Source
 
 	Stage 3 : Operator
 		content : from: (count, word)
-		ship_strategy : REBALANCE
+		ship_strategy : FORWARD
 
 		Stage 4 : Operator
 			content : from: (count, word)
-			ship_strategy : REBALANCE
+			ship_strategy : FORWARD
 
