diff --git a/flink-table/flink-sql-client/src/test/resources/sql/table.q b/flink-table/flink-sql-client/src/test/resources/sql/table.q
index 25aba02ed7e..8e8c432ca13 100644
--- a/flink-table/flink-sql-client/src/test/resources/sql/table.q
+++ b/flink-table/flink-sql-client/src/test/resources/sql/table.q
@@ -556,6 +556,20 @@ CREATE TABLE IF NOT EXISTS orders2 (
 [INFO] Execute statement succeed.
 !info
 
+CREATE TABLE IF NOT EXISTS daily_orders (
+ `user` BIGINT NOT NULl,
+ product STRING,
+ amount INT,
+ dt STRING NOT NULL,
+ PRIMARY KEY(dt, `user`) NOT ENFORCED
+) PARTITIONED BY (dt) WITH (
+ 'connector' = 'filesystem',
+ 'path' = '$VAR_BATCH_PATH',
+ 'format' = 'csv'
+);
+[INFO] Execute statement succeed.
+!info
+
 # test explain plan for select
 explain plan for select `user`, product from orders;
 == Abstract Syntax Tree ==
@@ -599,6 +613,45 @@ Sink(table=[default_catalog.default_database.orders2], fields=[user, product, am
 
 !ok
 
+# test explain plan for insert into with static partition
+explain plan for insert into daily_orders partition (dt = '2022-06-12') values (123, 'toothpick', 1);
+== Abstract Syntax Tree ==
+LogicalSink(table=[default_catalog.default_database.daily_orders], fields=[user, product, amount, dt])
++- LogicalProject(user=[123:BIGINT], product=[_UTF-16LE'toothpick':VARCHAR(2147483647) CHARACTER SET "UTF-16LE"], amount=[1], dt=[_UTF-16LE'2022-06-12':VARCHAR(2147483647) CHARACTER SET "UTF-16LE"])
+   +- LogicalValues(tuples=[[{ 0 }]])
+
+== Optimized Physical Plan ==
+Sink(table=[default_catalog.default_database.daily_orders], fields=[user, product, amount, dt])
++- Calc(select=[123:BIGINT AS user, _UTF-16LE'toothpick':VARCHAR(2147483647) CHARACTER SET "UTF-16LE" AS product, 1 AS amount, _UTF-16LE'2022-06-12':VARCHAR(2147483647) CHARACTER SET "UTF-16LE" AS dt])
+   +- Values(type=[RecordType(INTEGER ZERO)], tuples=[[{ 0 }]])
+
+== Optimized Execution Plan ==
+Sink(table=[default_catalog.default_database.daily_orders], fields=[user, product, amount, dt])
++- Calc(select=[123 AS user, 'toothpick' AS product, 1 AS amount, '2022-06-12' AS dt])
+   +- Values(tuples=[[{ 0 }]])
+
+!ok
+
+# test explain plan for insert overwrite with static partition
+explain plan for insert into daily_orders partition (dt = '2022-06-12') select `user`, product, amount from daily_orders where dt = '2022-06-12';
+== Abstract Syntax Tree ==
+LogicalSink(table=[default_catalog.default_database.daily_orders], fields=[user, product, amount, EXPR$3])
++- LogicalProject(user=[$0], product=[$1], amount=[$2], EXPR$3=[_UTF-16LE'2022-06-12':VARCHAR(2147483647) CHARACTER SET "UTF-16LE"])
+   +- LogicalFilter(condition=[=($3, _UTF-16LE'2022-06-12')])
+      +- LogicalTableScan(table=[[default_catalog, default_database, daily_orders]])
+
+== Optimized Physical Plan ==
+Sink(table=[default_catalog.default_database.daily_orders], fields=[user, product, amount, EXPR$3])
++- Calc(select=[user, product, amount, _UTF-16LE'2022-06-12':VARCHAR(2147483647) CHARACTER SET "UTF-16LE" AS EXPR$3])
+   +- TableSourceScan(table=[[default_catalog, default_database, daily_orders, partitions=[], project=[user, product, amount], metadata=[]]], fields=[user, product, amount])
+
+== Optimized Execution Plan ==
+Sink(table=[default_catalog.default_database.daily_orders], fields=[user, product, amount, EXPR$3])
++- Calc(select=[user, product, amount, '2022-06-12' AS EXPR$3])
+   +- TableSourceScan(table=[[default_catalog, default_database, daily_orders, partitions=[], project=[user, product, amount], metadata=[]]], fields=[user, product, amount])
+
+!ok
+
 # test explain select
 explain select `user`, product from orders;
 == Abstract Syntax Tree ==
diff --git a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/calcite/FlinkPlannerImpl.scala b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/calcite/FlinkPlannerImpl.scala
index ef4491eeef5..7f8292284a7 100644
--- a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/calcite/FlinkPlannerImpl.scala
+++ b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/calcite/FlinkPlannerImpl.scala
@@ -149,7 +149,14 @@ class FlinkPlannerImpl(
       }
       sqlNode match {
         case richExplain: SqlRichExplain =>
-          richExplain.setOperand(0, validate(richExplain.getStatement))
+          val validatedStatement = richExplain.getStatement match {
+            // only validate source here
+            case insert: RichSqlInsert =>
+              validateRichSqlInsert(insert)
+            case others =>
+              validate(others)
+          }
+          richExplain.setOperand(0, validatedStatement)
           richExplain
         case statementSet: SqlStatementSet =>
           statementSet.getInserts.asScala.zipWithIndex.foreach {
@@ -160,16 +167,7 @@ class FlinkPlannerImpl(
           execute.setOperand(0, validate(execute.getStatement))
           execute
         case insert: RichSqlInsert =>
-          // We don't support UPSERT INTO semantics (see FLINK-24225).
-          if (insert.isUpsert) {
-            throw new ValidationException(
-              "UPSERT INTO statement is not supported. Please use INSERT INTO instead.")
-          }
-          // only validate source here.
-          // ignore row type which will be verified in table environment.
-          val validatedSource = validator.validate(insert.getSource)
-          insert.setOperand(2, validatedSource)
-          insert
+          validateRichSqlInsert(insert)
         case compile: SqlCompilePlan =>
           compile.setOperand(0, validate(compile.getOperandList.get(0)))
           compile
@@ -229,6 +227,19 @@ class FlinkPlannerImpl(
     sqlValidator.validateParameterizedExpression(sqlNode, nameToTypeMap)
   }
 
+  private def validateRichSqlInsert(insert: RichSqlInsert): SqlNode = {
+    // We don't support UPSERT INTO semantics (see FLINK-24225).
+    if (insert.isUpsert) {
+      throw new ValidationException(
+        "UPSERT INTO statement is not supported. Please use INSERT INTO instead.")
+    }
+    // only validate source here.
+    // ignore row type which will be verified in table environment.
+    val validatedSource = validate(insert.getSource)
+    insert.setOperand(2, validatedSource)
+    insert
+  }
+
   def rex(
       sqlNode: SqlNode,
       inputRowType: RelDataType,
diff --git a/flink-table/flink-table-planner/src/test/resources/explain/testExecuteSqlWithExplainInsertIntoStaticPartition.out b/flink-table/flink-table-planner/src/test/resources/explain/testExecuteSqlWithExplainInsertIntoStaticPartition.out
new file mode 100644
index 00000000000..f2c1e8dfe35
--- /dev/null
+++ b/flink-table/flink-table-planner/src/test/resources/explain/testExecuteSqlWithExplainInsertIntoStaticPartition.out
@@ -0,0 +1,14 @@
+== Abstract Syntax Tree ==
+LogicalSink(table=[default_catalog.default_database.MySink], fields=[f0, f1, EXPR$2])
++- LogicalProject(f0=[$0], f1=[$1], EXPR$2=[_UTF-16LE'123':VARCHAR(2147483647) CHARACTER SET "UTF-16LE"])
+   +- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [CollectionTableSource(f0, f1, f2)]]])
+
+== Optimized Physical Plan ==
+Sink(table=[default_catalog.default_database.MySink], fields=[f0, f1, EXPR$2])
++- Calc(select=[f0, f1, '123' AS EXPR$2])
+   +- LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [CollectionTableSource(f0, f1, f2)]]], fields=[f0, f1, f2])
+
+== Optimized Execution Plan ==
+Sink(table=[default_catalog.default_database.MySink], fields=[f0, f1, EXPR$2])
++- Calc(select=[f0, f1, '123' AS EXPR$2])
+   +- LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [CollectionTableSource(f0, f1, f2)]]], fields=[f0, f1, f2])
diff --git a/flink-table/flink-table-planner/src/test/resources/explain/testExecuteSqlWithExplainInsertOverwriteStaticPartition.out b/flink-table/flink-table-planner/src/test/resources/explain/testExecuteSqlWithExplainInsertOverwriteStaticPartition.out
new file mode 100644
index 00000000000..f2c1e8dfe35
--- /dev/null
+++ b/flink-table/flink-table-planner/src/test/resources/explain/testExecuteSqlWithExplainInsertOverwriteStaticPartition.out
@@ -0,0 +1,14 @@
+== Abstract Syntax Tree ==
+LogicalSink(table=[default_catalog.default_database.MySink], fields=[f0, f1, EXPR$2])
++- LogicalProject(f0=[$0], f1=[$1], EXPR$2=[_UTF-16LE'123':VARCHAR(2147483647) CHARACTER SET "UTF-16LE"])
+   +- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [CollectionTableSource(f0, f1, f2)]]])
+
+== Optimized Physical Plan ==
+Sink(table=[default_catalog.default_database.MySink], fields=[f0, f1, EXPR$2])
++- Calc(select=[f0, f1, '123' AS EXPR$2])
+   +- LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [CollectionTableSource(f0, f1, f2)]]], fields=[f0, f1, f2])
+
+== Optimized Execution Plan ==
+Sink(table=[default_catalog.default_database.MySink], fields=[f0, f1, EXPR$2])
++- Calc(select=[f0, f1, '123' AS EXPR$2])
+   +- LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [CollectionTableSource(f0, f1, f2)]]], fields=[f0, f1, f2])
diff --git a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/api/TableEnvironmentTest.scala b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/api/TableEnvironmentTest.scala
index 5b472b572da..a184bc610b1 100644
--- a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/api/TableEnvironmentTest.scala
+++ b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/api/TableEnvironmentTest.scala
@@ -57,6 +57,7 @@ class TableEnvironmentTest {
 
   val env = new StreamExecutionEnvironment(new LocalStreamEnvironment())
   val tableEnv = StreamTableEnvironment.create(env, TableTestUtil.STREAM_SETTING)
+  val batchTableEnv = StreamTableEnvironment.create(env, TableTestUtil.BATCH_SETTING)
 
   @Test
   def testScanNonExistTable(): Unit = {
@@ -1484,6 +1485,51 @@ class TableEnvironmentTest {
       "/explain/testExecuteSqlWithExplainInsertPartialColumn.out")
   }
 
+  @Test
+  def testExecuteSqlWithExplainInsertStaticPartition(): Unit = {
+    val createTableStmt1 =
+      """
+        |CREATE TABLE MyTable (
+        |  f0 BIGINT,
+        |  f1 INT,
+        |  f2 STRING
+        |) WITH (
+        |  'connector' = 'COLLECTION',
+        |  'is-bounded' = 'true'
+        |)
+      """.stripMargin
+    val tableResult1 = batchTableEnv.executeSql(createTableStmt1)
+    assertEquals(ResultKind.SUCCESS, tableResult1.getResultKind)
+
+    val createTableStmt2 =
+      """
+        |CREATE TABLE MySink (
+        |  f0 BIGINT,
+        |  f1 INT,
+        |  f2 STRING
+        |) PARTITIONED BY (f2)
+        |WITH (
+        |  'connector' = 'filesystem',
+        |  'path' = '/tmp',
+        |  'format' = 'testcsv'
+        |)
+      """.stripMargin
+    val tableResult2 = batchTableEnv.executeSql(createTableStmt2)
+    assertEquals(ResultKind.SUCCESS, tableResult2.getResultKind)
+
+    checkExplain(
+      "EXPLAIN PLAN FOR INSERT INTO MySink PARTITION (f2 = '123') SELECT f0, f1 FROM MyTable",
+      "/explain/testExecuteSqlWithExplainInsertIntoStaticPartition.out",
+      streaming = false
+    )
+
+    checkExplain(
+      "EXPLAIN PLAN FOR INSERT OVERWRITE MySink PARTITION (f2 = '123') SELECT f0, f1 FROM MyTable",
+      "/explain/testExecuteSqlWithExplainInsertOverwriteStaticPartition.out",
+      streaming = false
+    )
+  }
+
   @Test
   def testExecuteSqlWithUnsupportedExplain(): Unit = {
     val createTableStmt =
@@ -1998,8 +2044,12 @@ class TableEnvironmentTest {
     assertEquals(expectToBeBounded, source.asInstanceOf[CollectionTableSource].isBounded)
   }
 
-  private def checkExplain(sql: String, resultPath: String): Unit = {
-    val tableResult2 = tableEnv.executeSql(sql)
+  private def checkExplain(sql: String, resultPath: String, streaming: Boolean = true): Unit = {
+    val tableResult2 = if (streaming) {
+      tableEnv.executeSql(sql)
+    } else {
+      batchTableEnv.executeSql(sql)
+    }
     assertEquals(ResultKind.SUCCESS_WITH_CONTENT, tableResult2.getResultKind)
     val it = tableResult2.collect()
     assertTrue(it.hasNext)
