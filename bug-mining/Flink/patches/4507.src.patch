diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/functions/hive/HiveGenericUDF.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/functions/hive/HiveGenericUDF.java
index 1c3649aa352..31272219b64 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/functions/hive/HiveGenericUDF.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/functions/hive/HiveGenericUDF.java
@@ -27,6 +27,7 @@ import org.apache.flink.table.types.DataType;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseNumeric;
 import org.apache.hadoop.hive.serde2.objectinspector.ConstantObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
@@ -55,7 +56,7 @@ public class HiveGenericUDF extends HiveScalarFunction<GenericUDF> {
 
 		LOG.info("Open HiveGenericUDF as {}", hiveFunctionWrapper.getClassName());
 
-		function = hiveFunctionWrapper.createFunction();
+		function = createFunction();
 
 		ObjectInspector[] argInspectors = HiveInspectors.toInspectors(hiveShim, constantArguments, argTypes);
 
@@ -101,7 +102,7 @@ public class HiveGenericUDF extends HiveScalarFunction<GenericUDF> {
 			ObjectInspector[] argumentInspectors = HiveInspectors.toInspectors(hiveShim, constantArguments, argTypes);
 
 			ObjectInspector resultObjectInspector =
-				hiveFunctionWrapper.createFunction().initializeAndFoldConstants(argumentInspectors);
+					createFunction().initializeAndFoldConstants(argumentInspectors);
 
 			return HiveTypeUtil.toFlinkType(
 				TypeInfoUtils.getTypeInfoFromObjectInspector(resultObjectInspector));
@@ -109,4 +110,14 @@ public class HiveGenericUDF extends HiveScalarFunction<GenericUDF> {
 			throw new FlinkHiveUDFException(e);
 		}
 	}
+
+	private GenericUDF createFunction() {
+		function = hiveFunctionWrapper.createFunction();
+		// some UDFs may need to access SessionState HiveConf, tell them not to
+		if (function instanceof GenericUDFBaseNumeric) {
+			GenericUDFBaseNumeric baseNumeric = (GenericUDFBaseNumeric) function;
+			baseNumeric.setConfLookupNeeded(false);
+		}
+		return function;
+	}
 }
diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/module/hive/HiveModule.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/module/hive/HiveModule.java
index 12fc5f16466..c67a9cd5842 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/module/hive/HiveModule.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/module/hive/HiveModule.java
@@ -44,8 +44,8 @@ public class HiveModule implements Module {
 	// a set of functions that shouldn't be overridden by HiveModule
 	@VisibleForTesting
 	static final Set<String> BUILT_IN_FUNC_BLACKLIST = Collections.unmodifiableSet(new HashSet<>(
-			Arrays.asList("count", "dense_rank", "first_value", "lag", "last_value", "lead", "rank", "row_number",
-					"hop", "hop_end", "hop_proctime", "hop_rowtime", "hop_start",
+			Arrays.asList("count", "current_date", "current_timestamp", "dense_rank", "first_value", "lag", "last_value",
+					"lead", "rank", "row_number", "hop", "hop_end", "hop_proctime", "hop_rowtime", "hop_start",
 					"session", "session_end", "session_proctime", "session_rowtime", "session_start",
 					"tumble", "tumble_end", "tumble_proctime", "tumble_rowtime", "tumble_start")));
 
diff --git a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/module/hive/HiveModuleTest.java b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/module/hive/HiveModuleTest.java
index 343d8186f36..f64066978d5 100644
--- a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/module/hive/HiveModuleTest.java
+++ b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/module/hive/HiveModuleTest.java
@@ -25,6 +25,7 @@ import org.apache.flink.table.functions.FunctionDefinition;
 import org.apache.flink.table.functions.ScalarFunction;
 import org.apache.flink.table.functions.ScalarFunctionDefinition;
 import org.apache.flink.table.functions.hive.HiveSimpleUDF;
+import org.apache.flink.table.module.CoreModule;
 import org.apache.flink.table.types.DataType;
 import org.apache.flink.types.Row;
 
@@ -62,22 +63,22 @@ public class HiveModuleTest {
 
 		switch (hiveVersion) {
 			case HIVE_VERSION_V1_2_0:
-				assertEquals(231, hiveModule.listFunctions().size());
+				assertEquals(229, hiveModule.listFunctions().size());
 				break;
 			case HIVE_VERSION_V2_0_0:
-				assertEquals(235, hiveModule.listFunctions().size());
+				assertEquals(233, hiveModule.listFunctions().size());
 				break;
 			case HIVE_VERSION_V2_1_1:
-				assertEquals(245, hiveModule.listFunctions().size());
+				assertEquals(243, hiveModule.listFunctions().size());
 				break;
 			case HIVE_VERSION_V2_2_0:
-				assertEquals(261, hiveModule.listFunctions().size());
+				assertEquals(259, hiveModule.listFunctions().size());
 				break;
 			case HIVE_VERSION_V2_3_4:
-				assertEquals(279, hiveModule.listFunctions().size());
+				assertEquals(277, hiveModule.listFunctions().size());
 				break;
 			case HIVE_VERSION_V3_1_1:
-				assertEquals(298, hiveModule.listFunctions().size());
+				assertEquals(296, hiveModule.listFunctions().size());
 				break;
 		}
 	}
@@ -185,4 +186,19 @@ public class HiveModuleTest {
 		results = Lists.newArrayList(tableEnv.sqlQuery("select length('')").execute().collect());
 		assertEquals("[0]", results.toString());
 	}
+
+	@Test
+	public void testFunctionsNeedSessionState() {
+		TableEnvironment tableEnv = HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode();
+
+		tableEnv.unloadModule("core");
+		tableEnv.loadModule("hive", new HiveModule());
+		tableEnv.loadModule("core", CoreModule.INSTANCE);
+
+		tableEnv.sqlQuery("select current_timestamp,current_date").execute().collect();
+
+		List<Row> results = Lists.newArrayList(
+				tableEnv.sqlQuery("select mod(-1,2),pmod(-1,2)").execute().collect());
+		assertEquals("[-1,1]", results.toString());
+	}
 }
