diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/StreamConfig.java b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/StreamConfig.java
index fc1f43584ab..c1e9606dea7 100644
--- a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/StreamConfig.java
+++ b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/StreamConfig.java
@@ -25,27 +25,23 @@ import java.util.Map;
 
 import org.apache.commons.lang3.SerializationException;
 import org.apache.commons.lang3.SerializationUtils;
-import org.apache.flink.api.java.tuple.Tuple2;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.streaming.api.collector.selector.OutputSelectorWrapper;
 import org.apache.flink.streaming.api.invokable.StreamInvokable;
 import org.apache.flink.streaming.api.streamrecord.StreamRecordSerializer;
 import org.apache.flink.streaming.api.streamvertex.StreamVertexException;
-import org.apache.flink.streaming.partitioner.StreamPartitioner;
 import org.apache.flink.util.InstantiationUtil;
 
 public class StreamConfig implements Serializable {
 
 	private static final long serialVersionUID = 1L;
 
-	private static final String INPUT_TYPE = "inputType_";
 	private static final String NUMBER_OF_OUTPUTS = "numberOfOutputs";
 	private static final String NUMBER_OF_INPUTS = "numberOfInputs";
 	private static final String CHAINED_OUTPUTS = "chainedOutputs";
 	private static final String CHAINED_TASK_CONFIG = "chainedTaskConfig_";
 	private static final String IS_CHAINED_VERTEX = "isChainedSubtask";
 	private static final String OUTPUT_NAME = "outputName_";
-	private static final String PARTITIONER_OBJECT = "partitionerObject_";
 	private static final String VERTEX_NAME = "vertexID";
 	private static final String OPERATOR_NAME = "operatorName";
 	private static final String ITERATION_ID = "iteration-id";
@@ -219,24 +215,6 @@ public class StreamConfig implements Serializable {
 		return config.getLong(ITERATON_WAIT, 0);
 	}
 
-	public <T> void setPartitioner(Integer output, StreamPartitioner<T> partitionerObject) {
-
-		config.setBytes(PARTITIONER_OBJECT + output,
-				SerializationUtils.serialize(partitionerObject));
-	}
-
-	@SuppressWarnings("unchecked")
-	public <T> StreamPartitioner<T> getPartitioner(ClassLoader cl, Integer output) {
-		StreamPartitioner<T> partitioner = null;
-		try {
-			partitioner = (StreamPartitioner<T>) InstantiationUtil.readObjectFromConfig(
-					this.config, PARTITIONER_OBJECT + output, cl);
-		} catch (Exception e) {
-			throw new RuntimeException("Partitioner could not be instantiated.");
-		}
-		return partitioner;
-	}
-
 	public void setSelectedNames(Integer output, List<String> selected) {
 		if (selected != null) {
 			config.setBytes(OUTPUT_NAME + output,
@@ -311,12 +289,12 @@ public class StreamConfig implements Serializable {
 		}
 	}
 
-	public void setInEdges(List<StreamEdge> inEdges) {
+	public void setInPhysicalEdges(List<StreamEdge> inEdges) {
 		config.setBytes(IN_STREAM_EDGES, SerializationUtils.serialize((Serializable) inEdges));
 	}
 
 	@SuppressWarnings("unchecked")
-	public List<StreamEdge> getInEdges(ClassLoader cl) {
+	public List<StreamEdge> getInPhysicalEdges(ClassLoader cl) {
 		try {
 			return (List<StreamEdge>) InstantiationUtil.readObjectFromConfig(
 					this.config, IN_STREAM_EDGES, cl);
@@ -325,41 +303,31 @@ public class StreamConfig implements Serializable {
 		}
 	}
 
-	public void setOutEdgesInOrder(List<Tuple2<Integer, Integer>> outEdgeList) {
-
-		config.setBytes(EDGES_IN_ORDER, SerializationUtils.serialize((Serializable) outEdgeList));
-	}
-
-
 	public void setStateMonitoring(boolean stateMonitoring) {
-		
+
 		config.setBoolean(STATE_MONITORING, stateMonitoring);
-		
+
 	}
-	
+
 	public boolean getStateMonitoring()
 	{
 		return config.getBoolean(STATE_MONITORING, false);
 	}
 
+	public void setOutEdgesInOrder(List<StreamEdge> outEdgeList) {
+		config.setBytes(EDGES_IN_ORDER, SerializationUtils.serialize((Serializable) outEdgeList));
+	}
+
 	@SuppressWarnings("unchecked")
-	public List<Tuple2<Integer, Integer>> getOutEdgesInOrder(ClassLoader cl) {
+	public List<StreamEdge> getOutEdgesInOrder(ClassLoader cl) {
 		try {
-			return (List<Tuple2<Integer, Integer>>) InstantiationUtil.readObjectFromConfig(
+			return (List<StreamEdge>) InstantiationUtil.readObjectFromConfig(
 					this.config, EDGES_IN_ORDER, cl);
 		} catch (Exception e) {
 			throw new RuntimeException("Could not instantiate outputs.");
 		}
 	}
 
-	public void setInputIndex(int inputNumber, Integer inputTypeNumber) {
-		config.setInteger(INPUT_TYPE + inputNumber++, inputTypeNumber);
-	}
-
-	public int getInputIndex(int inputNumber) {
-		return config.getInteger(INPUT_TYPE + inputNumber, 0);
-	}
-
 	public void setTransitiveChainedTaskConfigs(Map<Integer, StreamConfig> chainedTaskConfigs) {
 		config.setBytes(CHAINED_TASK_CONFIG,
 				SerializationUtils.serialize((Serializable) chainedTaskConfigs));
@@ -402,7 +370,7 @@ public class StreamConfig implements Serializable {
 		builder.append("\nPartitioning:");
 		for (StreamEdge output : getNonChainedOutputs(cl)) {
 			int outputname = output.getTargetVertex();
-			builder.append("\n\t" + outputname + ": " + getPartitioner(cl, outputname));
+			builder.append("\n\t" + outputname + ": " + output.getPartitioner());
 		}
 
 		builder.append("\nChained subtasks: " + getChainedOutputs(cl));
diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/StreamEdge.java b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/StreamEdge.java
index a300fc2585f..74edb008239 100644
--- a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/StreamEdge.java
+++ b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/StreamEdge.java
@@ -28,6 +28,10 @@ import org.apache.flink.streaming.partitioner.StreamPartitioner;
  */
 public class StreamEdge implements Serializable {
 
+	private static final long serialVersionUID = 1L;
+
+	final private String edgeId;
+
 	final private int sourceVertex;
 	final private int targetVertex;
 
@@ -48,6 +52,12 @@ public class StreamEdge implements Serializable {
 		this.typeNumber = typeNumber;
 		this.selectedNames = selectedNames;
 		this.outputPartitioner = outputPartitioner;
+
+		this.edgeId = sourceVertex + "_"
+				+ targetVertex + "_"
+				+ typeNumber + "_"
+				+ selectedNames + "_"
+				+ outputPartitioner;
 	}
 
 	public int getSourceVertex() {
@@ -70,14 +80,37 @@ public class StreamEdge implements Serializable {
 		return outputPartitioner;
 	}
 
+	@Override
+	public int hashCode() {
+		return edgeId.hashCode();
+	}
+
+	@Override
+	public boolean equals(Object o) {
+		if (this == o) {
+			return true;
+		}
+		if (o == null || getClass() != o.getClass()) {
+			return false;
+		}
+
+		StreamEdge that = (StreamEdge) o;
+
+		if (!edgeId.equals(that.edgeId)) {
+			return false;
+		}
+
+		return true;
+	}
+
 	@Override
 	public String toString() {
-		return "StreamGraphEdge{" +
-				"sourceVertex=" + sourceVertex +
-				", targetVertex=" + targetVertex +
+		return "(" +
+				sourceVertex +
+				" -> " + targetVertex +
 				", typeNumber=" + typeNumber +
 				", selectedNames=" + selectedNames +
 				", outputPartitioner=" + outputPartitioner +
-				'}';
+				')';
 	}
 }
diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/StreamingJobGraphGenerator.java b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/StreamingJobGraphGenerator.java
index d42003049bb..8a110bfedf2 100644
--- a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/StreamingJobGraphGenerator.java
+++ b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/StreamingJobGraphGenerator.java
@@ -25,7 +25,6 @@ import java.util.List;
 import java.util.Map;
 
 import org.apache.commons.lang.StringUtils;
-import org.apache.flink.api.java.tuple.Tuple2;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.runtime.jobgraph.AbstractJobVertex;
 import org.apache.flink.runtime.jobgraph.DistributionPattern;
@@ -53,7 +52,10 @@ public class StreamingJobGraphGenerator {
 	private JobGraph jobGraph;
 	private Collection<Integer> builtVertices;
 
+	private List<StreamEdge> physicalEdgesInOrder;
+
 	private Map<Integer, Map<Integer, StreamConfig>> chainedConfigs;
+
 	private Map<Integer, StreamConfig> vertexConfigs;
 	private Map<Integer, String> chainedNames;
 
@@ -67,6 +69,7 @@ public class StreamingJobGraphGenerator {
 		this.chainedConfigs = new HashMap<Integer, Map<Integer, StreamConfig>>();
 		this.vertexConfigs = new HashMap<Integer, StreamConfig>();
 		this.chainedNames = new HashMap<Integer, String>();
+		this.physicalEdgesInOrder = new ArrayList<StreamEdge>();
 	}
 
 	public JobGraph createJobGraph(String jobName) {
@@ -77,30 +80,57 @@ public class StreamingJobGraphGenerator {
 		jobGraph.setJobType(JobGraph.JobType.STREAMING);
 		jobGraph.setMonitoringEnabled(streamGraph.isMonitoringEnabled());
 		jobGraph.setMonitorInterval(streamGraph.getMonitoringInterval());
-		if(jobGraph.isMonitoringEnabled())
-		{
+		if (jobGraph.isMonitoringEnabled()) {
 			jobGraph.setNumberOfExecutionRetries(Integer.MAX_VALUE);
 		}
 		init();
 
 		setChaining();
 
+		setPhysicalEdges();
+
 		setSlotSharing();
 
 		return jobGraph;
 	}
 
+	private void setPhysicalEdges() {
+		Map<Integer, List<StreamEdge>> physicalInEdgesInOrder = new HashMap<Integer, List<StreamEdge>>();
+
+		for (StreamEdge edge : physicalEdgesInOrder) {
+			int target = edge.getTargetVertex();
+
+			List<StreamEdge> inEdges = physicalInEdgesInOrder.get(target);
+
+			// create if not set
+			if (inEdges == null) {
+				inEdges = new ArrayList<StreamEdge>();
+				physicalInEdgesInOrder.put(target, inEdges);
+			}
+
+			inEdges.add(edge);
+		}
+
+		for (Map.Entry<Integer, List<StreamEdge>> inEdges : physicalInEdgesInOrder.entrySet()) {
+			int vertex = inEdges.getKey();
+			List<StreamEdge> edgeList = inEdges.getValue();
+
+			vertexConfigs.get(vertex).setInPhysicalEdges(edgeList);
+		}
+	}
+
 	private void setChaining() {
 		for (Integer sourceName : streamGraph.getSources()) {
 			createChain(sourceName, sourceName);
 		}
 	}
 
-	private List<Tuple2<Integer, Integer>> createChain(Integer startNode, Integer current) {
+	private List<StreamEdge> createChain(Integer startNode, Integer current) {
 
 		if (!builtVertices.contains(startNode)) {
 
-			List<Tuple2<Integer, Integer>> transitiveOutEdges = new ArrayList<Tuple2<Integer, Integer>>();
+			List<StreamEdge> transitiveOutEdges = new ArrayList<StreamEdge>();
+
 			List<StreamEdge> chainableOutputs = new ArrayList<StreamEdge>();
 			List<StreamEdge> nonChainableOutputs = new ArrayList<StreamEdge>();
 
@@ -117,7 +147,7 @@ public class StreamingJobGraphGenerator {
 			}
 
 			for (StreamEdge nonChainable : nonChainableOutputs) {
-				transitiveOutEdges.add(new Tuple2<Integer, Integer>(current, nonChainable.getTargetVertex()));
+				transitiveOutEdges.add(nonChainable);
 				createChain(nonChainable.getTargetVertex(), nonChainable.getTargetVertex());
 			}
 
@@ -133,9 +163,8 @@ public class StreamingJobGraphGenerator {
 				config.setChainStart();
 				config.setOutEdgesInOrder(transitiveOutEdges);
 				config.setOutEdges(streamGraph.getOutEdges(current));
-				config.setInEdges(streamGraph.getInEdges(current));
 
-				for (Tuple2<Integer, Integer> edge : transitiveOutEdges) {
+				for (StreamEdge edge : transitiveOutEdges) {
 					connect(startNode, edge);
 				}
 
@@ -154,7 +183,7 @@ public class StreamingJobGraphGenerator {
 			return transitiveOutEdges;
 
 		} else {
-			return new ArrayList<Tuple2<Integer, Integer>>();
+			return new ArrayList<StreamEdge>();
 		}
 	}
 
@@ -165,9 +194,11 @@ public class StreamingJobGraphGenerator {
 			for (StreamEdge chainable : chainedOutputs) {
 				outputChainedNames.add(chainedNames.get(chainable.getTargetVertex()));
 			}
-			return operatorName + " -> (" + StringUtils.join(outputChainedNames, ", ") + ")";
+			String returnOperatorName = operatorName + " -> (" + StringUtils.join(outputChainedNames, ", ") + ")";
+			return returnOperatorName;
 		} else if (chainedOutputs.size() == 1) {
-			return operatorName + " -> " + chainedNames.get(chainedOutputs.get(0));
+			String returnOperatorName = operatorName + " -> " + chainedNames.get(chainedOutputs.get(0).getTargetVertex());
+			return returnOperatorName;
 		} else {
 			return operatorName;
 		}
@@ -238,30 +269,20 @@ public class StreamingJobGraphGenerator {
 		vertexConfigs.put(vertexID, config);
 	}
 
-	private <T> void connect(Integer headOfChain, Tuple2<Integer, Integer> edge) {
+	private void connect(Integer headOfChain, StreamEdge edge) {
 
-		Integer upStreamvertexID = edge.f0;
-		Integer downStreamvertexID = edge.f1;
+		physicalEdgesInOrder.add(edge);
 
-		int outputIndex = streamGraph.getOutEdges(upStreamvertexID).indexOf(downStreamvertexID);
+		Integer downStreamvertexID = edge.getTargetVertex();
 
 		AbstractJobVertex headVertex = streamVertices.get(headOfChain);
 		AbstractJobVertex downStreamVertex = streamVertices.get(downStreamvertexID);
 
 		StreamConfig downStreamConfig = new StreamConfig(downStreamVertex.getConfiguration());
-		StreamConfig upStreamConfig = headOfChain.equals(upStreamvertexID) ? new StreamConfig(
-				headVertex.getConfiguration()) : chainedConfigs.get(headOfChain).get(
-				upStreamvertexID);
-
-		int numOfInputs = downStreamConfig.getNumberOfInputs();
-
-		downStreamConfig.setInputIndex(numOfInputs++, streamGraph.getEdge(upStreamvertexID, downStreamvertexID).getTypeNumber());
-		downStreamConfig.setNumberOfInputs(numOfInputs);
-
-		StreamPartitioner<?> partitioner = streamGraph.getEdge(upStreamvertexID, downStreamvertexID).getPartitioner();
 
-		upStreamConfig.setPartitioner(downStreamvertexID, partitioner);
+		downStreamConfig.setNumberOfInputs(downStreamConfig.getNumberOfInputs() + 1);
 
+		StreamPartitioner<?> partitioner = edge.getPartitioner();
 		if (partitioner.getStrategy() == PartitioningStrategy.FORWARD) {
 			downStreamVertex.connectNewDataSetAsInput(headVertex, DistributionPattern.POINTWISE);
 		} else {
@@ -281,14 +302,15 @@ public class StreamingJobGraphGenerator {
 		StreamInvokable<?, ?> headInvokable = streamGraph.getInvokable(vertexID);
 		StreamInvokable<?, ?> outInvokable = streamGraph.getInvokable(outName);
 
-		return streamGraph.getInEdges(outName).size() == 1
-				&& outInvokable != null
-				&& outInvokable.getChainingStrategy() == ChainingStrategy.ALWAYS
-				&& (headInvokable.getChainingStrategy() == ChainingStrategy.HEAD || headInvokable
+		return
+				streamGraph.getInEdges(outName).size() == 1
+						&& outInvokable != null
+						&& outInvokable.getChainingStrategy() == ChainingStrategy.ALWAYS
+						&& (headInvokable.getChainingStrategy() == ChainingStrategy.HEAD || headInvokable
 						.getChainingStrategy() == ChainingStrategy.ALWAYS)
-				&& streamGraph.getEdge(vertexID, outName).getPartitioner().getStrategy() == PartitioningStrategy.FORWARD
-				&& streamGraph.getParallelism(vertexID) == streamGraph.getParallelism(outName)
-				&& streamGraph.chaining;
+						&& edge.getPartitioner().getStrategy() == PartitioningStrategy.FORWARD
+						&& streamGraph.getParallelism(vertexID) == streamGraph.getParallelism(outName)
+						&& streamGraph.chaining;
 	}
 
 	private void setSlotSharing() {
diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/collector/CollectorWrapper.java b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/collector/CollectorWrapper.java
index bb0268a49c3..4a0369c8fd3 100644
--- a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/collector/CollectorWrapper.java
+++ b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/collector/CollectorWrapper.java
@@ -29,7 +29,6 @@ public class CollectorWrapper<OUT> implements Collector<OUT> {
 		this.outputSelectorWrapper = outputSelectorWrapper;
 	}
 
-	@SuppressWarnings("unchecked")
 	public void addCollector(Collector<?> output, StreamEdge edge) {
 		outputSelectorWrapper.addCollector(output, edge);
 	}
diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/collector/selector/BroadcastOutputSelectorWrapper.java b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/collector/selector/BroadcastOutputSelectorWrapper.java
index 44371f09649..78ef9141c76 100644
--- a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/collector/selector/BroadcastOutputSelectorWrapper.java
+++ b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/collector/selector/BroadcastOutputSelectorWrapper.java
@@ -25,12 +25,14 @@ import org.apache.flink.util.Collector;
 
 public class BroadcastOutputSelectorWrapper<OUT> implements OutputSelectorWrapper<OUT> {
 
+	private static final long serialVersionUID = 1L;
 	private List<Collector<OUT>> outputs;
 
 	public BroadcastOutputSelectorWrapper() {
 		outputs = new ArrayList<Collector<OUT>>();
 	}
 
+	@SuppressWarnings("unchecked")
 	@Override
 	public void addCollector(Collector<?> output, StreamEdge edge) {
 		outputs.add((Collector<OUT>) output);
diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/collector/selector/DirectedOutputSelectorWrapper.java b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/collector/selector/DirectedOutputSelectorWrapper.java
index a04900c2eab..1cb20d908ca 100644
--- a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/collector/selector/DirectedOutputSelectorWrapper.java
+++ b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/collector/selector/DirectedOutputSelectorWrapper.java
@@ -31,6 +31,8 @@ import org.slf4j.LoggerFactory;
 
 public class DirectedOutputSelectorWrapper<OUT> implements OutputSelectorWrapper<OUT> {
 
+	private static final long serialVersionUID = 1L;
+
 	private static final Logger LOG = LoggerFactory.getLogger(DirectedOutputSelectorWrapper.class);
 
 	private List<OutputSelector<OUT>> outputSelectors;
@@ -44,6 +46,7 @@ public class DirectedOutputSelectorWrapper<OUT> implements OutputSelectorWrapper
 		this.outputMap = new HashMap<String, List<Collector<OUT>>>();
 	}
 
+	@SuppressWarnings("unchecked")
 	@Override
 	public void addCollector(Collector<?> output, StreamEdge edge) {
 		List<String> selectedNames = edge.getSelectedNames();
diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/collector/selector/OutputSelectorWrapperFactory.java b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/collector/selector/OutputSelectorWrapperFactory.java
index c0f22c7a0cf..dca2ede1853 100644
--- a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/collector/selector/OutputSelectorWrapperFactory.java
+++ b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/collector/selector/OutputSelectorWrapperFactory.java
@@ -21,6 +21,7 @@ import java.util.List;
 
 public class OutputSelectorWrapperFactory {
 
+	@SuppressWarnings({ "rawtypes", "unchecked" })
 	public static OutputSelectorWrapper<?> create(List<OutputSelector<?>> outputSelectors) {
 		if (outputSelectors.size() == 0) {
 			return new BroadcastOutputSelectorWrapper();
diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/streamvertex/CoStreamVertex.java b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/streamvertex/CoStreamVertex.java
index b45fc4436ab..f277be05253 100644
--- a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/streamvertex/CoStreamVertex.java
+++ b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/streamvertex/CoStreamVertex.java
@@ -80,7 +80,7 @@ public class CoStreamVertex<IN1, IN2, OUT> extends StreamVertex<IN1, OUT> {
 		ArrayList<InputGate> inputList1 = new ArrayList<InputGate>();
 		ArrayList<InputGate> inputList2 = new ArrayList<InputGate>();
 
-		List<StreamEdge> inEdges = configuration.getInEdges(userClassLoader);
+		List<StreamEdge> inEdges = configuration.getInPhysicalEdges(userClassLoader);
 
 		for (int i = 0; i < numberOfInputs; i++) {
 			int inputType = inEdges.get(i).getTypeNumber();
diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/streamvertex/InputHandler.java b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/streamvertex/InputHandler.java
index 3988ec18f5b..c6a43773c44 100644
--- a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/streamvertex/InputHandler.java
+++ b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/streamvertex/InputHandler.java
@@ -66,12 +66,6 @@ public class InputHandler<IN> {
 		}
 	}
 
-	private IndexedReaderIterator<StreamRecord<IN>> createInputIterator() {
-		final IndexedReaderIterator<StreamRecord<IN>> iter = new IndexedReaderIterator<StreamRecord<IN>>(
-				inputs, inputSerializer);
-		return iter;
-	}
-
 	protected static <T> IndexedReaderIterator<StreamRecord<T>> staticCreateInputIterator(
 			MutableReader<?> inputReader, TypeSerializer<StreamRecord<T>> serializer) {
 
diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/streamvertex/OutputHandler.java b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/streamvertex/OutputHandler.java
index bb66c63ffa8..42afcaec235 100644
--- a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/streamvertex/OutputHandler.java
+++ b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/streamvertex/OutputHandler.java
@@ -24,7 +24,6 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
-import org.apache.flink.api.java.tuple.Tuple2;
 import org.apache.flink.runtime.io.network.api.writer.RecordWriter;
 import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;
 import org.apache.flink.runtime.plugable.SerializationDelegate;
@@ -52,9 +51,10 @@ public class OutputHandler<OUT> {
 
 	public List<ChainableInvokable<?, ?>> chainedInvokables;
 
-	private Map<Integer, StreamOutput<?>> outputMap;
+	private Map<StreamEdge, StreamOutput<?>> outputMap;
+
 	private Map<Integer, StreamConfig> chainedConfigs;
-	private List<Tuple2<Integer, Integer>> outEdgesInOrder;
+	private List<StreamEdge> outEdgesInOrder;
 
 	public OutputHandler(StreamVertex<?, OUT> vertex) {
 
@@ -62,7 +62,7 @@ public class OutputHandler<OUT> {
 		this.vertex = vertex;
 		this.configuration = new StreamConfig(vertex.getTaskConfiguration());
 		this.chainedInvokables = new ArrayList<ChainableInvokable<?, ?>>();
-		this.outputMap = new HashMap<Integer, StreamOutput<?>>();
+		this.outputMap = new HashMap<StreamEdge, StreamOutput<?>>();
 		this.cl = vertex.getUserCodeClassLoader();
 
 		// We read the chained configs, and the order of record writer
@@ -74,16 +74,18 @@ public class OutputHandler<OUT> {
 
 		// We iterate through all the out edges from this job vertex and create
 		// a stream output
-		for (Tuple2<Integer, Integer> outEdge : outEdgesInOrder) {
-			StreamOutput<?> streamOutput = createStreamOutput(outEdge.f1,
-					chainedConfigs.get(outEdge.f0), outEdgesInOrder.indexOf(outEdge));
-			outputMap.put(outEdge.f1, streamOutput);
+		for (StreamEdge outEdge : outEdgesInOrder) {
+			StreamOutput<?> streamOutput = createStreamOutput(
+					outEdge,
+					outEdge.getTargetVertex(),
+					chainedConfigs.get(outEdge.getSourceVertex()),
+					outEdgesInOrder.indexOf(outEdge));
+			outputMap.put(outEdge, streamOutput);
 		}
 
 		// We create the outer collector that will be passed to the first task
 		// in the chain
 		this.outerCollector = createChainedCollector(configuration);
-
 	}
 
 	public void broadcastBarrier(long id) throws IOException, InterruptedException {
@@ -120,9 +122,7 @@ public class OutputHandler<OUT> {
 
 		// Create collectors for the network outputs
 		for (StreamEdge outputEdge : chainedTaskConfig.getNonChainedOutputs(cl)) {
-			Integer output = outputEdge.getTargetVertex();
-
-			Collector<?> outCollector = outputMap.get(output);
+			Collector<?> outCollector = outputMap.get(outputEdge);
 
 			wrapper.addCollector(outCollector, outputEdge);
 		}
@@ -164,14 +164,14 @@ public class OutputHandler<OUT> {
 	 *
 	 * @param outputVertex
 	 * 		Name of the output to which the streamoutput will be set up
-	 * @param configuration
+	 * @param upStreamConfig
 	 * 		The config of upStream task
-	 * @return
+	 * @return The created StreamOutput
 	 */
-	private <T> StreamOutput<T> createStreamOutput(Integer outputVertex,
-			StreamConfig configuration, int outputIndex) {
+	private <T> StreamOutput<T> createStreamOutput(StreamEdge edge, Integer outputVertex,
+			StreamConfig upStreamConfig, int outputIndex) {
 
-		StreamRecordSerializer<T> outSerializer = configuration
+		StreamRecordSerializer<T> outSerializer = upStreamConfig
 				.getTypeSerializerOut1(vertex.userClassLoader);
 		SerializationDelegate<StreamRecord<T>> outSerializationDelegate = null;
 
@@ -180,12 +180,13 @@ public class OutputHandler<OUT> {
 			outSerializationDelegate.setInstance(outSerializer.createInstance());
 		}
 
-		StreamPartitioner<T> outputPartitioner = configuration.getPartitioner(cl, outputVertex);
+		@SuppressWarnings("unchecked")
+		StreamPartitioner<T> outputPartitioner = (StreamPartitioner<T>) edge.getPartitioner();
 
 		ResultPartitionWriter bufferWriter = vertex.getEnvironment().getWriter(outputIndex);
 
 		RecordWriter<SerializationDelegate<StreamRecord<T>>> output =
-				RecordWriterFactory.createRecordWriter(bufferWriter, outputPartitioner, configuration.getBufferTimeout());
+				RecordWriterFactory.createRecordWriter(bufferWriter, outputPartitioner, upStreamConfig.getBufferTimeout());
 
 		StreamOutput<T> streamOutput = new StreamOutput<T>(output, vertex.instanceID,
 				outSerializationDelegate);
diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/api/CoStreamTest.java b/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/api/CoStreamTest.java
index 20cd1891414..f527de4f720 100644
--- a/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/api/CoStreamTest.java
+++ b/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/api/CoStreamTest.java
@@ -21,67 +21,101 @@ import static org.junit.Assert.assertEquals;
 
 import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
 
+import org.apache.flink.api.common.functions.FilterFunction;
+import org.apache.flink.api.common.functions.MapFunction;
 import org.apache.flink.api.java.functions.KeySelector;
+import org.apache.flink.api.java.tuple.Tuple2;
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
-import org.apache.flink.streaming.api.function.co.CoMapFunction;
+import org.apache.flink.streaming.api.function.co.CoFlatMapFunction;
+import org.apache.flink.streaming.api.invokable.StreamInvokable;
 import org.apache.flink.streaming.util.TestListResultSink;
 import org.apache.flink.streaming.util.TestStreamEnvironment;
+import org.apache.flink.util.Collector;
+import org.junit.Test;
 
 public class CoStreamTest {
 
 	private static final long MEMORY_SIZE = 32;
 
-	private static ArrayList<String> expected;
+	private static ArrayList<String> expected = new ArrayList<String>();
 
-	public static void main(String[] args) throws InterruptedException {
-		for (int i = 0; i < 200; i++) {
-			test();
-		}
-	}
+	@Test
+	public void test() {
 
-	//	@Test
-	public static void test() {
-		expected = new ArrayList<String>();
-
-		StreamExecutionEnvironment env = new TestStreamEnvironment(3, MEMORY_SIZE);
+		StreamExecutionEnvironment env = new TestStreamEnvironment(1, MEMORY_SIZE);
 
 		TestListResultSink<String> resultSink = new TestListResultSink<String>();
 
 		DataStream<Integer> src = env.fromElements(1, 3, 5);
-		DataStream<Integer> src2 = env.fromElements(1, 3, 5);
 
-		DataStream<Integer> grouped = src.groupBy(new KeySelector<Integer, Integer>() {
+		DataStream<Integer> filter1 = src.filter(new FilterFunction<Integer>() {
+	
+			private static final long serialVersionUID = 1L;
+
 			@Override
-			public Integer getKey(Integer value) throws Exception {
-				return value;
+			public boolean filter(Integer value) throws Exception {
+				return true;
 			}
-		});
+		}).groupBy(new KeySelector<Integer, Integer>() {
+
+			private static final long serialVersionUID = 1L;
 
-		DataStream<Integer> grouped2 = src2.groupBy(new KeySelector<Integer, Integer>() {
 			@Override
 			public Integer getKey(Integer value) throws Exception {
 				return value;
 			}
 		});
 
-		DataStream<String> connected = grouped.connect(grouped2).map(new CoMapFunction<Integer, Integer, String>() {
+		DataStream<Tuple2<Integer, Integer>> filter2 = src
+				.map(new MapFunction<Integer, Tuple2<Integer, Integer>>() {
+
+					private static final long serialVersionUID = 1L;
+
+					@Override
+					public Tuple2<Integer, Integer> map(Integer value) throws Exception {
+						return new Tuple2<Integer, Integer>(value, value + 1);
+					}
+				})
+				.distribute()
+				.filter(new FilterFunction<Tuple2<Integer, Integer>>() {
+
+					private static final long serialVersionUID = 1L;
+
+					@Override
+					public boolean filter(Tuple2<Integer, Integer> value) throws Exception {
+						return true;
+					}
+				}).setChainingStrategy(StreamInvokable.ChainingStrategy.NEVER).groupBy(new KeySelector<Tuple2<Integer, Integer>, Integer>() {
+
+					private static final long serialVersionUID = 1L;
+
+					@Override
+					public Integer getKey(Tuple2<Integer, Integer> value) throws Exception {
+						return value.f0;
+					}
+				});
+
+		DataStream<String> connected = filter1.connect(filter2).flatMap(new CoFlatMapFunction<Integer, Tuple2<Integer, Integer>, String>() {
+
+			private static final long serialVersionUID = 1L;
+
 			@Override
-			public String map1(Integer value) {
-				return value.toString();
+			public void flatMap1(Integer value, Collector<String> out) throws Exception {
+				out.collect(value.toString());
 			}
 
 			@Override
-			public String map2(Integer value) {
-				return value.toString();
+			public void flatMap2(Tuple2<Integer, Integer> value, Collector<String> out) throws Exception {
+				out.collect(value.toString());
 			}
 		});
 
 		connected.addSink(resultSink);
 
-		connected.print();
-
 		try {
 			env.execute();
 		} catch (Exception e) {
@@ -89,9 +123,11 @@ public class CoStreamTest {
 		}
 
 		expected = new ArrayList<String>();
-		expected.addAll(Arrays.asList("1", "1", "3", "3", "5", "5"));
+		expected.addAll(Arrays.asList("(1,2)", "(3,4)", "(5,6)", "1", "3", "5"));
+
+		List<String> result = resultSink.getResult();
+		Collections.sort(result);
 
-		System.out.println(resultSink.getResult());
-		assertEquals(expected, expected);
+		assertEquals(expected, result);
 	}
 }
\ No newline at end of file
diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/api/invokable/operator/co/SelfConnectionTest.java b/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/api/invokable/operator/co/SelfConnectionTest.java
new file mode 100644
index 00000000000..b58245a9724
--- /dev/null
+++ b/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/api/invokable/operator/co/SelfConnectionTest.java
@@ -0,0 +1,252 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.api.invokable.operator.co;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.fail;
+
+import java.io.Serializable;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+
+import org.apache.flink.api.common.functions.FlatMapFunction;
+import org.apache.flink.api.common.functions.MapFunction;
+import org.apache.flink.api.java.functions.KeySelector;
+import org.apache.flink.api.java.tuple.Tuple2;
+import org.apache.flink.streaming.api.datastream.DataStream;
+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
+import org.apache.flink.streaming.api.function.co.CoMapFunction;
+import org.apache.flink.streaming.api.invokable.StreamInvokable;
+import org.apache.flink.streaming.api.windowing.helper.Timestamp;
+import org.apache.flink.streaming.util.TestListResultSink;
+import org.apache.flink.streaming.util.TestStreamEnvironment;
+import org.apache.flink.util.Collector;
+import org.junit.Test;
+
+public class SelfConnectionTest implements Serializable {
+
+	private static final long serialVersionUID = 1L;
+
+	private final int MEMORY_SIZE = 32;
+
+	private static List<String> expected;
+
+	@SuppressWarnings({ "unchecked", "rawtypes" })
+	@Test
+	public void sameDataStreamTest() {
+
+		StreamExecutionEnvironment env = new TestStreamEnvironment(3, MEMORY_SIZE);
+
+		TestListResultSink<String> resultSink = new TestListResultSink<String>();
+
+		Timestamp<Integer> timeStamp = new Timestamp<Integer>() {
+
+			private static final long serialVersionUID = 1L;
+
+			@Override
+			public long getTimestamp(Integer value) {
+				return value;
+			}
+
+		};
+
+		KeySelector keySelector = new KeySelector<Integer, Integer>() {
+
+			private static final long serialVersionUID = 1L;
+
+			@Override
+			public Integer getKey(Integer value) throws Exception {
+				return value;
+			}
+		};
+
+		DataStream<Integer> src = env.fromElements(1, 3, 5);
+
+		@SuppressWarnings("unused")
+		DataStream<Tuple2<Integer, Integer>> dataStream =
+				src.join(src).onWindow(50L, timeStamp, timeStamp).where(keySelector).equalTo(keySelector)
+						.map(new MapFunction<Tuple2<Integer, Integer>, String>() {
+
+							private static final long serialVersionUID = 1L;
+
+							@Override
+							public String map(Tuple2<Integer, Integer> value) throws Exception {
+								return value.toString();
+							}
+						})
+						.addSink(resultSink);
+
+
+		try {
+			env.execute();
+
+			expected = new ArrayList<String>();
+
+			expected.addAll(Arrays.asList("(1,1)", "(3,3)", "(5,5)"));
+
+			List<String> result = resultSink.getResult();
+
+			Collections.sort(expected);
+			Collections.sort(result);
+
+			assertEquals(expected, result);
+		} catch (Exception e) {
+			fail();
+			e.printStackTrace();
+		}
+	}
+
+	/**
+	 * We connect two different data streams in a chain to a CoMap.
+	 */
+	@Test
+	public void differentDataStreamSameChain() {
+
+		TestListResultSink<String> resultSink = new TestListResultSink<String>();
+
+		StreamExecutionEnvironment env = new TestStreamEnvironment(1, MEMORY_SIZE);
+
+		DataStream<Integer> src = env.fromElements(1, 3, 5);
+
+		DataStream<String> stringMap = src.map(new MapFunction<Integer, String>() {
+			private static final long serialVersionUID = 1L;
+
+			@Override
+			public String map(Integer value) throws Exception {
+				return "x " + value;
+			}
+		}).setChainingStrategy(StreamInvokable.ChainingStrategy.ALWAYS);
+
+		stringMap.connect(src).map(new CoMapFunction<String, Integer, String>() {
+
+			private static final long serialVersionUID = 1L;
+
+			@Override
+			public String map1(String value) {
+				return value;
+			}
+
+			@Override
+			public String map2(Integer value) {
+				return String.valueOf(value + 1);
+			}
+		}).addSink(resultSink);
+
+		try {
+			env.execute();
+		} catch (Exception e) {
+			e.printStackTrace();
+		}
+
+		expected = new ArrayList<String>();
+
+		expected.addAll(Arrays.asList("x 1", "x 3", "x 5", "2", "4", "6"));
+
+		List<String> result = resultSink.getResult();
+
+		Collections.sort(expected);
+		Collections.sort(result);
+
+		assertEquals(expected, result);
+	}
+
+	/**
+	 * We connect two different data streams in different chains to a CoMap.
+	 * (This is not actually self-connect.)
+	 */
+	@Test
+	public void differentDataStreamDifferentChain() {
+
+		TestListResultSink<String> resultSink = new TestListResultSink<String>();
+
+		StreamExecutionEnvironment env = new TestStreamEnvironment(3, MEMORY_SIZE);
+
+		DataStream<Integer> src = env.fromElements(1, 3, 5).setChainingStrategy(StreamInvokable.ChainingStrategy.NEVER);
+
+		DataStream<String> stringMap = src.flatMap(new FlatMapFunction<Integer, String>() {
+
+			private static final long serialVersionUID = 1L;
+
+			@Override
+			public void flatMap(Integer value, Collector<String> out) throws Exception {
+				out.collect("x " + value);
+			}
+		}).groupBy(new KeySelector<String, Integer>() {
+
+			private static final long serialVersionUID = 1L;
+
+			@Override
+			public Integer getKey(String value) throws Exception {
+				return value.length();
+			}
+		});
+
+		DataStream<Long> longMap = src.map(new MapFunction<Integer, Long>() {
+
+			private static final long serialVersionUID = 1L;
+
+			@Override
+			public Long map(Integer value) throws Exception {
+				return Long.valueOf(value + 1);
+			}
+		}).groupBy(new KeySelector<Long, Long>() {
+
+			private static final long serialVersionUID = 1L;
+
+			@Override
+			public Long getKey(Long value) throws Exception {
+				return value;
+			}
+		});
+
+
+		stringMap.connect(longMap).map(new CoMapFunction<String, Long, String>() {
+
+			private static final long serialVersionUID = 1L;
+
+			@Override
+			public String map1(String value) {
+				return value;
+			}
+
+			@Override
+			public String map2(Long value) {
+				return value.toString();
+			}
+		}).addSink(resultSink);
+
+		try {
+			env.execute();
+		} catch (Exception e) {
+			e.printStackTrace();
+		}
+
+		expected = new ArrayList<String>();
+
+		expected.addAll(Arrays.asList("x 1", "x 3", "x 5", "2", "4", "6"));
+
+		List<String> result = resultSink.getResult();
+
+		Collections.sort(expected);
+		Collections.sort(result);
+
+		assertEquals(expected, result);
+	}
+}
diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/api/streamvertex/StreamVertexTest.java b/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/api/streamvertex/StreamVertexTest.java
index 18a36ac5253..a88a60d5193 100644
--- a/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/api/streamvertex/StreamVertexTest.java
+++ b/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/api/streamvertex/StreamVertexTest.java
@@ -134,11 +134,13 @@ public class StreamVertexTest {
 
 		@Override
 		public String map1(String value) {
+//			System.out.println(value);
 			return value;
 		}
 
 		@Override
 		public String map2(Long value) {
+//			System.out.println(value);
 			return value.toString();
 		}
 	}
diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/util/TestListResultSink.java b/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/util/TestListResultSink.java
index 7f72173cdf1..8b78a42d7ab 100644
--- a/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/util/TestListResultSink.java
+++ b/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/util/TestListResultSink.java
@@ -27,6 +27,7 @@ import org.apache.flink.streaming.api.function.sink.RichSinkFunction;
 
 public class TestListResultSink<T> extends RichSinkFunction<T> {
 
+	private static final long serialVersionUID = 1L;
 	private int resultListId;
 
 	public TestListResultSink() {
@@ -50,6 +51,7 @@ public class TestListResultSink<T> extends RichSinkFunction<T> {
 		super.close();
 	}
 
+	@SuppressWarnings("unchecked")
 	private List<T> resultList() {
 		synchronized (TestListWrapper.getInstance()) {
 			return (List<T>) TestListWrapper.getInstance().getList(resultListId);
diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/util/TestListWrapper.java b/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/util/TestListWrapper.java
index 3c50f63009f..751f8360f13 100644
--- a/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/util/TestListWrapper.java
+++ b/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/util/TestListWrapper.java
@@ -25,8 +25,10 @@ public class TestListWrapper {
 
 	private static TestListWrapper instance;
 
+	@SuppressWarnings("rawtypes")
 	private List<List<? extends Comparable>> lists;
 
+	@SuppressWarnings("rawtypes")
 	private TestListWrapper() {
 		lists = Collections.synchronizedList(new ArrayList<List<? extends Comparable>>());
 	}
@@ -43,12 +45,14 @@ public class TestListWrapper {
 	 *
 	 * @return The ID of the list.
 	 */
+	@SuppressWarnings("rawtypes")
 	public int createList() {
 		lists.add(new ArrayList<Comparable>());
 		return lists.size() - 1;
 	}
 
 	public List<?> getList(int listId) {
+		@SuppressWarnings("rawtypes")
 		List<? extends Comparable> list = lists.get(listId);
 		if (list == null) {
 			throw new RuntimeException("No such list.");
