diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/DefaultVertexParallelismAndInputInfosDecider.java b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/DefaultVertexParallelismAndInputInfosDecider.java
index 381cedef4bc..540a56ef9fb 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/DefaultVertexParallelismAndInputInfosDecider.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/DefaultVertexParallelismAndInputInfosDecider.java
@@ -199,41 +199,49 @@ public class DefaultVertexParallelismAndInputInfosDecider
                     }
                 });
 
-        // For AllToAll like inputs, we derive parallelism as a whole, while for Pointwise inputs,
-        // we derive parallelism separately for each input, and our goal is ensured that the final
-        // parallelisms of those inputs are consistent and meet expectations.
-        // Since AllToAll supports deriving parallelism within a flexible range, this might
-        // interfere with the target parallelism. Therefore, in the following cases, we need to
-        // reset the minimum and maximum parallelism to limit the flexibility of parallelism
-        // derivation to achieve the goal:
-        // 1. There are pointwise inputs, which means that there may be inputs whose parallelism is
-        // derived one-by-one, we need to reset the min and max parallelism.
-        if (!pointwiseInputs.isEmpty()) {
+        // Currently, we decide parallelism separately for AllToAll and Pointwise. In order to make
+        // their data distribution as balanced as possible, we need to reset max and min parallelism
+        // to a pre-computed parallelism (which uses all inputs statistics) to limit their
+        // parallelism deciding flexibility to avoid the parallelism being decided too small.
+        // Specifically, if either of them is empty or all AllToAll inputs are the Broadcast type,
+        // this section will be skipped to enable more flexible parallelism deciding.
+        if (!pointwiseInputs.isEmpty()
+                && !allToAllInputs.isEmpty()
+                && !getNonBroadcastInputInfos(allToAllInputs).isEmpty()) {
             minParallelism = parallelism;
             maxParallelism = parallelism;
         }
 
         Map<IntermediateDataSetID, JobVertexInputInfo> vertexInputInfos = new HashMap<>();
 
-        if (!allToAllInputs.isEmpty()) {
+        if (!pointwiseInputs.isEmpty()) {
             vertexInputInfos.putAll(
-                    allToAllVertexInputInfoComputer.compute(
-                            jobVertexId,
-                            allToAllInputs,
+                    pointwiseVertexInputInfoComputer.compute(
+                            pointwiseInputs,
                             parallelism,
                             minParallelism,
                             maxParallelism,
                             calculateDataVolumePerTaskForInputsGroup(
-                                    dataVolumePerTask, allToAllInputs, consumedResults)));
+                                    dataVolumePerTask, pointwiseInputs, consumedResults)));
+            // We need to reset the minimum and maximum parallelism to limit the flexibility of
+            // parallelism derivation to make final parallelisms of all inputs are consistent
+            if (!allToAllInputs.isEmpty()) {
+                parallelism = checkAndGetParallelism(vertexInputInfos.values());
+                minParallelism = parallelism;
+                maxParallelism = parallelism;
+            }
         }
 
-        if (!pointwiseInputs.isEmpty()) {
+        if (!allToAllInputs.isEmpty()) {
             vertexInputInfos.putAll(
-                    pointwiseVertexInputInfoComputer.compute(
-                            pointwiseInputs,
+                    allToAllVertexInputInfoComputer.compute(
+                            jobVertexId,
+                            allToAllInputs,
                             parallelism,
+                            minParallelism,
+                            maxParallelism,
                             calculateDataVolumePerTaskForInputsGroup(
-                                    dataVolumePerTask, pointwiseInputs, consumedResults)));
+                                    dataVolumePerTask, allToAllInputs, consumedResults)));
         }
 
         for (BlockingInputInfo inputInfo : consumedResults) {
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/util/AllToAllVertexInputInfoComputer.java b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/util/AllToAllVertexInputInfoComputer.java
index ffae30b677c..c629ebab353 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/util/AllToAllVertexInputInfoComputer.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/util/AllToAllVertexInputInfoComputer.java
@@ -44,6 +44,7 @@ import static org.apache.flink.runtime.scheduler.adaptivebatch.util.VertexParall
 import static org.apache.flink.runtime.scheduler.adaptivebatch.util.VertexParallelismAndInputInfosDeciderUtils.checkAndGetParallelism;
 import static org.apache.flink.runtime.scheduler.adaptivebatch.util.VertexParallelismAndInputInfosDeciderUtils.checkAndGetSubpartitionNum;
 import static org.apache.flink.runtime.scheduler.adaptivebatch.util.VertexParallelismAndInputInfosDeciderUtils.checkAndGetSubpartitionNumForAggregatedInputs;
+import static org.apache.flink.runtime.scheduler.adaptivebatch.util.VertexParallelismAndInputInfosDeciderUtils.createJobVertexInputInfos;
 import static org.apache.flink.runtime.scheduler.adaptivebatch.util.VertexParallelismAndInputInfosDeciderUtils.createdJobVertexInputInfoForBroadcast;
 import static org.apache.flink.runtime.scheduler.adaptivebatch.util.VertexParallelismAndInputInfosDeciderUtils.createdJobVertexInputInfoForNonBroadcast;
 import static org.apache.flink.runtime.scheduler.adaptivebatch.util.VertexParallelismAndInputInfosDeciderUtils.getNonBroadcastInputInfos;
@@ -202,7 +203,10 @@ public class AllToAllVertexInputInfoComputer {
 
         // Create vertex input info based on the subpartition slice and its range.
         return createJobVertexInputInfos(
-                inputInfos, subpartitionSlicesByTypeNumber, subpartitionSliceRanges);
+                inputInfos,
+                subpartitionSlicesByTypeNumber,
+                subpartitionSliceRanges,
+                index -> inputInfos.get(index).getInputTypeNumber());
     }
 
     private Map<Integer, List<SubpartitionSlice>>
@@ -353,29 +357,6 @@ public class AllToAllVertexInputInfoComputer {
         return splitPartitionRange;
     }
 
-    private static Map<IntermediateDataSetID, JobVertexInputInfo> createJobVertexInputInfos(
-            List<BlockingInputInfo> inputInfos,
-            Map<Integer, List<SubpartitionSlice>> subpartitionSlices,
-            List<IndexRange> subpartitionSliceRanges) {
-        final Map<IntermediateDataSetID, JobVertexInputInfo> vertexInputInfos = new HashMap<>();
-        for (BlockingInputInfo inputInfo : inputInfos) {
-            if (inputInfo.isBroadcast()) {
-                vertexInputInfos.put(
-                        inputInfo.getResultId(),
-                        createdJobVertexInputInfoForBroadcast(
-                                inputInfo, subpartitionSliceRanges.size()));
-            } else {
-                vertexInputInfos.put(
-                        inputInfo.getResultId(),
-                        createdJobVertexInputInfoForNonBroadcast(
-                                inputInfo,
-                                subpartitionSliceRanges,
-                                subpartitionSlices.get(inputInfo.getInputTypeNumber())));
-            }
-        }
-        return vertexInputInfos;
-    }
-
     private Map<IntermediateDataSetID, JobVertexInputInfo>
             computeJobVertexInputInfosForInputsWithoutInterKeysCorrelation(
                     List<BlockingInputInfo> inputInfos, int parallelism, long dataVolumePerTask) {
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/util/PointwiseVertexInputInfoComputer.java b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/util/PointwiseVertexInputInfoComputer.java
index 4397bea6a26..498922f5e67 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/util/PointwiseVertexInputInfoComputer.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/util/PointwiseVertexInputInfoComputer.java
@@ -32,10 +32,14 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.Optional;
+import java.util.stream.Collectors;
 
 import static org.apache.flink.runtime.scheduler.adaptivebatch.util.SubpartitionSlice.createSubpartitionSlice;
-import static org.apache.flink.runtime.scheduler.adaptivebatch.util.VertexParallelismAndInputInfosDeciderUtils.calculateDataVolumePerTaskForInput;
-import static org.apache.flink.runtime.scheduler.adaptivebatch.util.VertexParallelismAndInputInfosDeciderUtils.createdJobVertexInputInfoForNonBroadcast;
+import static org.apache.flink.runtime.scheduler.adaptivebatch.util.VertexParallelismAndInputInfosDeciderUtils.checkAndGetPartitionNum;
+import static org.apache.flink.runtime.scheduler.adaptivebatch.util.VertexParallelismAndInputInfosDeciderUtils.checkAndGetSubpartitionNum;
+import static org.apache.flink.runtime.scheduler.adaptivebatch.util.VertexParallelismAndInputInfosDeciderUtils.createJobVertexInputInfos;
+import static org.apache.flink.runtime.scheduler.adaptivebatch.util.VertexParallelismAndInputInfosDeciderUtils.getInputsWithIntraCorrelation;
+import static org.apache.flink.runtime.scheduler.adaptivebatch.util.VertexParallelismAndInputInfosDeciderUtils.getMinSubpartitionCount;
 import static org.apache.flink.runtime.scheduler.adaptivebatch.util.VertexParallelismAndInputInfosDeciderUtils.isLegalParallelism;
 import static org.apache.flink.runtime.scheduler.adaptivebatch.util.VertexParallelismAndInputInfosDeciderUtils.tryComputeSubpartitionSliceRange;
 import static org.apache.flink.util.Preconditions.checkState;
@@ -45,43 +49,9 @@ public class PointwiseVertexInputInfoComputer {
     private static final Logger LOG =
             LoggerFactory.getLogger(PointwiseVertexInputInfoComputer.class);
 
-    /**
-     * Computes the input information for a job vertex based on the provided blocking input
-     * information and parallelism.
-     *
-     * @param inputInfos List of blocking input information for the job vertex.
-     * @param parallelism Parallelism of the job vertex.
-     * @param dataVolumePerTask Proposed data volume per task for this set of inputInfo.
-     * @return A map of intermediate data set IDs to their corresponding job vertex input
-     *     information.
-     */
-    public Map<IntermediateDataSetID, JobVertexInputInfo> compute(
-            List<BlockingInputInfo> inputInfos, int parallelism, long dataVolumePerTask) {
-        long totalDataBytes =
-                inputInfos.stream().mapToLong(BlockingInputInfo::getNumBytesProduced).sum();
-        Map<IntermediateDataSetID, JobVertexInputInfo> vertexInputInfos = new HashMap<>();
-        for (BlockingInputInfo inputInfo : inputInfos) {
-            // Currently, we consider all inputs in this method must don't have inter-inputs key
-            // correlation. If other possibilities are introduced in the future, please add new
-            // branches to this method.
-            checkState(!inputInfo.areInterInputsKeysCorrelated());
-            if (inputInfo.isIntraInputKeyCorrelated()) {
-                // In this case, we won't split subpartitions within the same partition, so need
-                // to ensure NumPartitions >= parallelism.
-                checkState(parallelism <= inputInfo.getNumPartitions());
-            }
-            vertexInputInfos.put(
-                    inputInfo.getResultId(),
-                    computeVertexInputInfo(
-                            inputInfo,
-                            parallelism,
-                            calculateDataVolumePerTaskForInput(
-                                    dataVolumePerTask,
-                                    inputInfo.getNumBytesProduced(),
-                                    totalDataBytes)));
-        }
-        return vertexInputInfos;
-    }
+    // Used to limit the maximum number of subpartition slices to prevent increasing the
+    // time complexity of the parallelism deciding.
+    private static final int MAX_NUM_SUBPARTITION_SLICES_FACTOR = 32;
 
     /**
      * Decide parallelism and input infos, which will make the data be evenly distributed to
@@ -100,57 +70,96 @@ public class PointwiseVertexInputInfoComputer {
      * The final result is the `SubpartitionGroup` that each of the three parallel tasks need to
      * subscribe.
      *
-     * @param inputInfo The information of consumed blocking results
-     * @param parallelism The parallelism of the job vertex. Since pointwise inputs always compute
-     *     vertex input info one-by-one, we need a determined parallelism to ensure the final
-     *     decided parallelism for all inputs is consistent.
-     * @return the vertex input info
+     * @param inputInfos The information of consumed blocking results
+     * @param parallelism The parallelism of the job vertex
+     * @param minParallelism the min parallelism
+     * @param maxParallelism the max parallelism
+     * @param dataVolumePerTask proposed data volume per task for this set of inputInfo
+     * @return the parallelism and vertex input infos
      */
-    private static JobVertexInputInfo computeVertexInputInfo(
-            BlockingInputInfo inputInfo, int parallelism, long dataVolumePerTask) {
-        List<SubpartitionSlice> subpartitionSlices = createSubpartitionSlices(inputInfo);
+    public Map<IntermediateDataSetID, JobVertexInputInfo> compute(
+            List<BlockingInputInfo> inputInfos,
+            int parallelism,
+            int minParallelism,
+            int maxParallelism,
+            long dataVolumePerTask) {
+        Map<Integer, List<SubpartitionSlice>> subpartitionSlicesByInputIndex =
+                createSubpartitionSlicesByInputIndex(inputInfos, maxParallelism);
 
-        // Node: SubpartitionSliceRanges does not represent the real index of the subpartitions, but
+        // Note: SubpartitionSliceRanges does not represent the real index of the subpartitions, but
         // the location of that subpartition in all subpartitions, as we aggregate all subpartitions
         // into a one-digit array to calculate.
         Optional<List<IndexRange>> optionalSubpartitionSliceRanges =
                 tryComputeSubpartitionSliceRange(
-                        parallelism,
-                        parallelism,
+                        minParallelism,
+                        maxParallelism,
                         dataVolumePerTask,
-                        Map.of(inputInfo.getInputTypeNumber(), subpartitionSlices));
+                        subpartitionSlicesByInputIndex);
 
         if (optionalSubpartitionSliceRanges.isEmpty()) {
             LOG.info(
-                    "Cannot find a legal parallelism to evenly distribute data amount for input {}, "
+                    "Cannot find a legal parallelism to evenly distribute data amount for inputs {}, "
                             + "fallback to compute a parallelism that can evenly distribute num subpartitions.",
-                    inputInfo.getResultId());
+                    inputInfos.stream()
+                            .map(BlockingInputInfo::getResultId)
+                            .collect(Collectors.toList()));
             // This computer is only used in the adaptive batch scenario, where isDynamicGraph
             // should always be true.
-            return VertexInputInfoComputationUtils.computeVertexInputInfoForPointwise(
-                    inputInfo.getNumPartitions(),
-                    parallelism,
-                    inputInfo::getNumSubpartitions,
-                    true);
+            return VertexInputInfoComputationUtils.computeVertexInputInfos(
+                    parallelism, inputInfos, true);
         }
 
         List<IndexRange> subpartitionSliceRanges = optionalSubpartitionSliceRanges.get();
 
-        checkState(isLegalParallelism(subpartitionSliceRanges.size(), parallelism, parallelism));
+        checkState(
+                isLegalParallelism(subpartitionSliceRanges.size(), minParallelism, maxParallelism));
 
-        // Create vertex input info based on the subpartition slice and ranges.
-        return createJobVertexInputInfo(inputInfo, subpartitionSliceRanges, subpartitionSlices);
+        // Create vertex input infos based on the subpartition slice and ranges.
+        return createJobVertexInputInfos(
+                inputInfos,
+                subpartitionSlicesByInputIndex,
+                subpartitionSliceRanges,
+                index -> index);
     }
 
-    private static List<SubpartitionSlice> createSubpartitionSlices(BlockingInputInfo inputInfo) {
+    private static Map<Integer, List<SubpartitionSlice>> createSubpartitionSlicesByInputIndex(
+            List<BlockingInputInfo> inputInfos, int maxParallelism) {
+        int numSubpartitionSlices;
+        List<BlockingInputInfo> inputsWithIntraCorrelation =
+                getInputsWithIntraCorrelation(inputInfos);
+        if (!inputsWithIntraCorrelation.isEmpty()) {
+            // Ensure that when creating subpartition slices, data with intra-correlation will
+            // not be split.
+            numSubpartitionSlices = checkAndGetPartitionNum(inputsWithIntraCorrelation);
+        } else {
+            // Use the minimum of the two to avoid creating too many subpartition slices, which will
+            // lead to too high the time complexity of the parallelism deciding.
+            numSubpartitionSlices =
+                    Math.min(
+                            getMinSubpartitionCount(inputInfos),
+                            MAX_NUM_SUBPARTITION_SLICES_FACTOR * maxParallelism);
+        }
+
+        Map<Integer, List<SubpartitionSlice>> subpartitionSlices = new HashMap<>();
+        for (int i = 0; i < inputInfos.size(); ++i) {
+            BlockingInputInfo inputInfo = inputInfos.get(i);
+            subpartitionSlices.put(i, createSubpartitionSlices(inputInfo, numSubpartitionSlices));
+        }
+
+        return subpartitionSlices;
+    }
+
+    private static List<SubpartitionSlice> createSubpartitionSlices(
+            BlockingInputInfo inputInfo, int total) {
         List<SubpartitionSlice> subpartitionSlices = new ArrayList<>();
-        if (inputInfo.isIntraInputKeyCorrelated()) {
-            // If the input has intra-input correlation, we need to ensure all subpartitions
-            // in the same partition index are assigned to the same downstream concurrent task.
-            for (int i = 0; i < inputInfo.getNumPartitions(); ++i) {
-                IndexRange partitionRange = new IndexRange(i, i);
-                IndexRange subpartitionRange =
-                        new IndexRange(0, inputInfo.getNumSubpartitions(i) - 1);
+        int numPartitions = inputInfo.getNumPartitions();
+        int numSubpartitions = checkAndGetSubpartitionNum(List.of(inputInfo));
+        if (numPartitions >= total) {
+            for (int i = 0; i < total; ++i) {
+                int start = i * numPartitions / total;
+                int nextStart = (i + 1) * numPartitions / total;
+                IndexRange partitionRange = new IndexRange(start, nextStart - 1);
+                IndexRange subpartitionRange = new IndexRange(0, numSubpartitions - 1);
                 subpartitionSlices.add(
                         createSubpartitionSlice(
                                 partitionRange,
@@ -158,10 +167,14 @@ public class PointwiseVertexInputInfoComputer {
                                 inputInfo.getNumBytesProduced(partitionRange, subpartitionRange)));
             }
         } else {
-            for (int i = 0; i < inputInfo.getNumPartitions(); ++i) {
+            for (int i = 0; i < numPartitions; i++) {
+                int count = (i + 1) * total / numPartitions - i * total / numPartitions;
+                checkState(count > 0 && count <= numSubpartitions);
                 IndexRange partitionRange = new IndexRange(i, i);
-                for (int j = 0; j < inputInfo.getNumSubpartitions(i); ++j) {
-                    IndexRange subpartitionRange = new IndexRange(j, j);
+                for (int j = 0; j < count; ++j) {
+                    int start = j * numSubpartitions / count;
+                    int nextStart = (j + 1) * numSubpartitions / count;
+                    IndexRange subpartitionRange = new IndexRange(start, nextStart - 1);
                     subpartitionSlices.add(
                             createSubpartitionSlice(
                                     partitionRange,
@@ -173,13 +186,4 @@ public class PointwiseVertexInputInfoComputer {
         }
         return subpartitionSlices;
     }
-
-    private static JobVertexInputInfo createJobVertexInputInfo(
-            BlockingInputInfo inputInfo,
-            List<IndexRange> subpartitionSliceRanges,
-            List<SubpartitionSlice> subpartitionSlices) {
-        checkState(!inputInfo.isBroadcast());
-        return createdJobVertexInputInfoForNonBroadcast(
-                inputInfo, subpartitionSliceRanges, subpartitionSlices);
-    }
 }
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/util/VertexParallelismAndInputInfosDeciderUtils.java b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/util/VertexParallelismAndInputInfosDeciderUtils.java
index 1b48a9f2aed..80a53cd6072 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/util/VertexParallelismAndInputInfosDeciderUtils.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/util/VertexParallelismAndInputInfosDeciderUtils.java
@@ -21,6 +21,7 @@ package org.apache.flink.runtime.scheduler.adaptivebatch.util;
 import org.apache.flink.runtime.executiongraph.ExecutionVertexInputInfo;
 import org.apache.flink.runtime.executiongraph.IndexRange;
 import org.apache.flink.runtime.executiongraph.JobVertexInputInfo;
+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;
 import org.apache.flink.runtime.jobgraph.JobVertexID;
 import org.apache.flink.runtime.scheduler.adaptivebatch.BisectionSearchUtils;
 import org.apache.flink.runtime.scheduler.adaptivebatch.BlockingInputInfo;
@@ -304,7 +305,7 @@ public class VertexParallelismAndInputInfosDeciderUtils {
      * @param minParallelism The minimum parallelism.
      * @param maxParallelism The maximum parallelism.
      * @param maxDataVolumePerTask The maximum data volume per task.
-     * @param subpartitionSlicesByTypeNumber A map of lists of subpartition slices grouped by type
+     * @param subpartitionSlices A map of lists of subpartition slices grouped by type or index
      *     number.
      * @return An {@code Optional} containing a list of index ranges representing the subpartition
      *     slice ranges. Returns an empty {@code Optional} if no suitable ranges can be computed.
@@ -313,24 +314,46 @@ public class VertexParallelismAndInputInfosDeciderUtils {
             int minParallelism,
             int maxParallelism,
             long maxDataVolumePerTask,
-            Map<Integer, List<SubpartitionSlice>> subpartitionSlicesByTypeNumber) {
+            Map<Integer, List<SubpartitionSlice>> subpartitionSlices) {
         Optional<List<IndexRange>> subpartitionSliceRanges =
                 tryComputeSubpartitionSliceRangeEvenlyDistributedData(
-                        minParallelism,
-                        maxParallelism,
-                        maxDataVolumePerTask,
-                        subpartitionSlicesByTypeNumber);
+                        minParallelism, maxParallelism, maxDataVolumePerTask, subpartitionSlices);
         if (subpartitionSliceRanges.isEmpty()) {
             LOG.info(
                     "Failed to compute a legal subpartition slice range that can evenly distribute data amount, "
                             + "fallback to compute it that can evenly distribute the number of subpartition slices.");
             subpartitionSliceRanges =
                     tryComputeSubpartitionSliceRangeEvenlyDistributedSubpartitionSlices(
-                            minParallelism, maxParallelism, subpartitionSlicesByTypeNumber);
+                            minParallelism, maxParallelism, subpartitionSlices);
         }
         return subpartitionSliceRanges;
     }
 
+    public static Map<IntermediateDataSetID, JobVertexInputInfo> createJobVertexInputInfos(
+            List<BlockingInputInfo> inputInfos,
+            Map<Integer, List<SubpartitionSlice>> subpartitionSlices,
+            List<IndexRange> subpartitionSliceRanges,
+            Function<Integer, Integer> subpartitionSliceKeyResolver) {
+        final Map<IntermediateDataSetID, JobVertexInputInfo> vertexInputInfos = new HashMap<>();
+        for (int i = 0; i < inputInfos.size(); ++i) {
+            BlockingInputInfo inputInfo = inputInfos.get(i);
+            if (inputInfo.isBroadcast()) {
+                vertexInputInfos.put(
+                        inputInfo.getResultId(),
+                        createdJobVertexInputInfoForBroadcast(
+                                inputInfo, subpartitionSliceRanges.size()));
+            } else {
+                vertexInputInfos.put(
+                        inputInfo.getResultId(),
+                        createdJobVertexInputInfoForNonBroadcast(
+                                inputInfo,
+                                subpartitionSliceRanges,
+                                subpartitionSlices.get(subpartitionSliceKeyResolver.apply(i))));
+            }
+        }
+        return vertexInputInfos;
+    }
+
     public static JobVertexInputInfo createdJobVertexInputInfoForBroadcast(
             BlockingInputInfo inputInfo, int parallelism) {
         checkArgument(inputInfo.isBroadcast());
@@ -383,24 +406,23 @@ public class VertexParallelismAndInputInfosDeciderUtils {
             int minParallelism,
             int maxParallelism,
             long maxDataVolumePerTask,
-            Map<Integer, List<SubpartitionSlice>> subpartitionSlicesByTypeNumber) {
-        int subpartitionSlicesSize =
-                checkAndGetSubpartitionSlicesSize(subpartitionSlicesByTypeNumber);
+            Map<Integer, List<SubpartitionSlice>> subpartitionSlices) {
+        int subpartitionSlicesSize = checkAndGetSubpartitionSlicesSize(subpartitionSlices);
         // Distribute the input data evenly among the downstream tasks and record the
         // subpartition slice range for each task.
         List<IndexRange> subpartitionSliceRanges =
                 computeSubpartitionSliceRanges(
-                        maxDataVolumePerTask,
-                        subpartitionSlicesSize,
-                        subpartitionSlicesByTypeNumber);
+                        maxDataVolumePerTask, subpartitionSlicesSize, subpartitionSlices);
         // if the parallelism is not legal, try to adjust to a legal parallelism
         if (!isLegalParallelism(subpartitionSliceRanges.size(), minParallelism, maxParallelism)) {
+            LOG.info(
+                    "Failed to compute a legal subpartition slice range that can evenly distribute data amount, "
+                            + "try to adjust to a legal parallelism.");
             long minBytesSize = maxDataVolumePerTask;
             long sumBytesSize = 0;
             for (int i = 0; i < subpartitionSlicesSize; ++i) {
                 long currentBytesSize = 0;
-                for (List<SubpartitionSlice> subpartitionSlice :
-                        subpartitionSlicesByTypeNumber.values()) {
+                for (List<SubpartitionSlice> subpartitionSlice : subpartitionSlices.values()) {
                     currentBytesSize += subpartitionSlice.get(i).getDataBytes();
                 }
                 minBytesSize = Math.min(minBytesSize, currentBytesSize);
@@ -413,12 +435,10 @@ public class VertexParallelismAndInputInfosDeciderUtils {
                     maxParallelism,
                     minBytesSize,
                     sumBytesSize,
-                    limit ->
-                            computeParallelism(
-                                    limit, subpartitionSlicesSize, subpartitionSlicesByTypeNumber),
+                    limit -> computeParallelism(limit, subpartitionSlicesSize, subpartitionSlices),
                     limit ->
                             computeSubpartitionSliceRanges(
-                                    limit, subpartitionSlicesSize, subpartitionSlicesByTypeNumber));
+                                    limit, subpartitionSlicesSize, subpartitionSlices));
         }
         return Optional.of(subpartitionSliceRanges);
     }
@@ -427,9 +447,8 @@ public class VertexParallelismAndInputInfosDeciderUtils {
             tryComputeSubpartitionSliceRangeEvenlyDistributedSubpartitionSlices(
                     int minParallelism,
                     int maxParallelism,
-                    Map<Integer, List<SubpartitionSlice>> subpartitionSlicesByTypeNumber) {
-        int subpartitionSlicesSize =
-                checkAndGetSubpartitionSlicesSize(subpartitionSlicesByTypeNumber);
+                    Map<Integer, List<SubpartitionSlice>> subpartitionSlices) {
+        int subpartitionSlicesSize = checkAndGetSubpartitionSlicesSize(subpartitionSlices);
         if (subpartitionSlicesSize < minParallelism) {
             return Optional.empty();
         }
@@ -710,4 +729,32 @@ public class VertexParallelismAndInputInfosDeciderUtils {
                     inputInfo.isSingleSubpartitionContainsAllData());
         }
     }
+
+    static int checkAndGetPartitionNum(List<BlockingInputInfo> consumedResults) {
+        final Set<Integer> subpartitionNumSet =
+                consumedResults.stream()
+                        .map(BlockingInputInfo::getNumPartitions)
+                        .collect(Collectors.toSet());
+        // all partitions have the same subpartition num
+        checkState(subpartitionNumSet.size() == 1);
+        return subpartitionNumSet.iterator().next();
+    }
+
+    static int getMinSubpartitionCount(List<BlockingInputInfo> consumedResults) {
+        checkState(!consumedResults.isEmpty());
+        int minSubpartitionCount = Integer.MAX_VALUE;
+        for (BlockingInputInfo inputInfo : consumedResults) {
+            int numPartitions = inputInfo.getNumPartitions();
+            int numSubpartitions = checkAndGetSubpartitionNum(List.of(inputInfo));
+            minSubpartitionCount = Math.min(minSubpartitionCount, numPartitions * numSubpartitions);
+        }
+        return minSubpartitionCount;
+    }
+
+    static List<BlockingInputInfo> getInputsWithIntraCorrelation(
+            List<BlockingInputInfo> inputInfos) {
+        return inputInfos.stream()
+                .filter(BlockingInputInfo::isIntraInputKeyCorrelated)
+                .collect(Collectors.toList());
+    }
 }
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adaptivebatch/DefaultVertexParallelismAndInputInfosDeciderTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adaptivebatch/DefaultVertexParallelismAndInputInfosDeciderTest.java
index a985e80e424..574f22248c3 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adaptivebatch/DefaultVertexParallelismAndInputInfosDeciderTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adaptivebatch/DefaultVertexParallelismAndInputInfosDeciderTest.java
@@ -299,6 +299,45 @@ class DefaultVertexParallelismAndInputInfosDeciderTest {
                         Map.of(new IndexRange(1, 1), new IndexRange(2, 4))));
     }
 
+    @Test
+    void testHavePointwiseAndBroadcastEdge() {
+        AllToAllBlockingResultInfo resultInfo1 =
+                createAllToAllBlockingResultInfo(
+                        new long[] {10L, 15L, 13L, 12L, 1L, 10L, 8L, 20L, 12L, 17L}, true, false);
+        PointwiseBlockingResultInfo resultInfo2 =
+                createPointwiseBlockingResultInfo(
+                        new long[] {8L, 12L, 21L, 9L, 13L}, new long[] {7L, 19L, 13L, 14L, 5L});
+        ParallelismAndInputInfos parallelismAndInputInfos =
+                createDeciderAndDecideParallelismAndInputInfos(
+                        1, 10, 60L, Arrays.asList(resultInfo1, resultInfo2));
+
+        assertThat(parallelismAndInputInfos.getParallelism()).isEqualTo(6);
+        assertThat(parallelismAndInputInfos.getJobVertexInputInfos()).hasSize(2);
+
+        checkAllToAllJobVertexInputInfo(
+                parallelismAndInputInfos.getJobVertexInputInfos().get(resultInfo1.getResultId()),
+                Arrays.asList(
+                        new IndexRange(0, 9),
+                        new IndexRange(0, 9),
+                        new IndexRange(0, 9),
+                        new IndexRange(0, 9),
+                        new IndexRange(0, 9),
+                        new IndexRange(0, 9)));
+        checkJobVertexInputInfo(
+                parallelismAndInputInfos.getJobVertexInputInfos().get(resultInfo2.getResultId()),
+                Arrays.asList(
+                        Map.of(new IndexRange(0, 0), new IndexRange(0, 1)),
+                        Map.of(new IndexRange(0, 0), new IndexRange(2, 3)),
+                        Map.of(
+                                new IndexRange(0, 0),
+                                new IndexRange(4, 4),
+                                new IndexRange(1, 1),
+                                new IndexRange(0, 0)),
+                        Map.of(new IndexRange(1, 1), new IndexRange(1, 1)),
+                        Map.of(new IndexRange(1, 1), new IndexRange(2, 3)),
+                        Map.of(new IndexRange(1, 1), new IndexRange(4, 4))));
+    }
+
     @Test
     void testSourceJobVertex() {
         ParallelismAndInputInfos parallelismAndInputInfos =
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adaptivebatch/util/PointwiseVertexInputInfoComputerTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adaptivebatch/util/PointwiseVertexInputInfoComputerTest.java
index c70225b31ed..8659ae138d6 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adaptivebatch/util/PointwiseVertexInputInfoComputerTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adaptivebatch/util/PointwiseVertexInputInfoComputerTest.java
@@ -42,13 +42,13 @@ class PointwiseVertexInputInfoComputerTest {
         PointwiseVertexInputInfoComputer computer = createPointwiseVertexInputInfoComputer();
         List<BlockingInputInfo> inputInfos = createBlockingInputInfos(2, List.of(), false);
         Map<IntermediateDataSetID, JobVertexInputInfo> vertexInputs =
-                computer.compute(inputInfos, 2, 10);
+                computer.compute(inputInfos, 2, 2, 2, 10);
         checkCorrectnessForNonCorrelatedInput(vertexInputs, inputInfos.get(0), 2);
         checkConsumedDataVolumePerSubtask(new long[] {3L, 3L}, inputInfos, vertexInputs);
 
         // with different parallelism
         Map<IntermediateDataSetID, JobVertexInputInfo> vertexInputs2 =
-                computer.compute(inputInfos, 3, 10);
+                computer.compute(inputInfos, 3, 3, 3, 10);
         checkCorrectnessForNonCorrelatedInput(vertexInputs2, inputInfos.get(0), 3);
         checkConsumedDataVolumePerSubtask(new long[] {2L, 2L, 2L}, inputInfos, vertexInputs2);
     }
@@ -62,14 +62,17 @@ class PointwiseVertexInputInfoComputerTest {
         inputInfosWithDifferentSkewedPartitions.add(inputInfo1);
         inputInfosWithDifferentSkewedPartitions.add(inputInfo2);
         Map<IntermediateDataSetID, JobVertexInputInfo> vertexInputs =
-                computer.compute(inputInfosWithDifferentSkewedPartitions, 3, 10);
+                computer.compute(inputInfosWithDifferentSkewedPartitions, 3, 3, 3, 10);
         checkCorrectnessForNonCorrelatedInput(vertexInputs, inputInfo1, 3);
         checkConsumedDataVolumePerSubtask(
-                new long[] {10L, 10L, 16L}, List.of(inputInfo1), vertexInputs);
+                new long[] {20L, 11L, 5L}, List.of(inputInfo1), vertexInputs);
 
         checkCorrectnessForNonCorrelatedInput(vertexInputs, inputInfo2, 3);
         checkConsumedDataVolumePerSubtask(
-                new long[] {13L, 10L, 13L}, List.of(inputInfo2), vertexInputs);
+                new long[] {2L, 11L, 23L}, List.of(inputInfo2), vertexInputs);
+
+        checkConsumedDataVolumePerSubtask(
+                new long[] {22L, 22L, 28L}, List.of(inputInfo1, inputInfo2), vertexInputs);
     }
 
     @Test
@@ -81,14 +84,17 @@ class PointwiseVertexInputInfoComputerTest {
         inputInfosWithDifferentNumPartitions.add(inputInfo1);
         inputInfosWithDifferentNumPartitions.add(inputInfo2);
         Map<IntermediateDataSetID, JobVertexInputInfo> vertexInputs =
-                computer.compute(inputInfosWithDifferentNumPartitions, 3, 10);
+                computer.compute(inputInfosWithDifferentNumPartitions, 3, 3, 3, 10);
         checkCorrectnessForNonCorrelatedInput(vertexInputs, inputInfo1, 3);
         checkConsumedDataVolumePerSubtask(
-                new long[] {13L, 10L, 13L}, List.of(inputInfo1), vertexInputs);
+                new long[] {13L, 20L, 3L}, List.of(inputInfo1), vertexInputs);
 
         checkCorrectnessForNonCorrelatedInput(vertexInputs, inputInfo2, 3);
         checkConsumedDataVolumePerSubtask(
-                new long[] {13L, 10L, 10L}, List.of(inputInfo2), vertexInputs);
+                new long[] {3L, 10L, 20L}, List.of(inputInfo2), vertexInputs);
+
+        checkConsumedDataVolumePerSubtask(
+                new long[] {16L, 30L, 23L}, List.of(inputInfo1, inputInfo2), vertexInputs);
     }
 
     @Test
@@ -100,14 +106,17 @@ class PointwiseVertexInputInfoComputerTest {
         inputInfosWithDifferentNumSubpartitions.add(inputInfo1);
         inputInfosWithDifferentNumSubpartitions.add(inputInfo2);
         Map<IntermediateDataSetID, JobVertexInputInfo> vertexInputs =
-                computer.compute(inputInfosWithDifferentNumSubpartitions, 3, 10);
+                computer.compute(inputInfosWithDifferentNumSubpartitions, 3, 3, 3, 10);
         checkCorrectnessForNonCorrelatedInput(vertexInputs, inputInfo1, 3);
         checkConsumedDataVolumePerSubtask(
                 new long[] {13L, 10L, 13L}, List.of(inputInfo1), vertexInputs);
 
         checkCorrectnessForNonCorrelatedInput(vertexInputs, inputInfo2, 3);
         checkConsumedDataVolumePerSubtask(
-                new long[] {25L, 20L, 15L}, List.of(inputInfo2), vertexInputs);
+                new long[] {15L, 20L, 25L}, List.of(inputInfo2), vertexInputs);
+
+        checkConsumedDataVolumePerSubtask(
+                new long[] {28L, 30L, 38L}, List.of(inputInfo1, inputInfo2), vertexInputs);
     }
 
     @Test
@@ -115,7 +124,7 @@ class PointwiseVertexInputInfoComputerTest {
         PointwiseVertexInputInfoComputer computer = createPointwiseVertexInputInfoComputer();
         List<BlockingInputInfo> inputInfos = createBlockingInputInfos(3, List.of(), true);
         Map<IntermediateDataSetID, JobVertexInputInfo> vertexInputs =
-                computer.compute(inputInfos, 3, 10);
+                computer.compute(inputInfos, 3, 3, 3, 10);
         checkCorrectnessForNonCorrelatedInput(vertexInputs, inputInfos.get(0), 3);
         checkConsumedSubpartitionGroups(
                 List.of(
@@ -127,7 +136,7 @@ class PointwiseVertexInputInfoComputerTest {
 
         // with different parallelism
         Map<IntermediateDataSetID, JobVertexInputInfo> vertexInputs2 =
-                computer.compute(inputInfos, 2, 10);
+                computer.compute(inputInfos, 2, 2, 2, 10);
         checkCorrectnessForNonCorrelatedInput(vertexInputs2, inputInfos.get(0), 2);
         checkConsumedSubpartitionGroups(
                 List.of(
