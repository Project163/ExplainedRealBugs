diff --git a/flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java b/flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java
index c29dd27a08a..79331df8ffa 100644
--- a/flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java
+++ b/flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java
@@ -43,6 +43,7 @@ import org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness;
 import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;
 import org.apache.flink.util.Preconditions;
 import org.apache.hadoop.fs.FSDataOutputStream;
+import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.FileUtil;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 import org.junit.AfterClass;
@@ -62,6 +63,7 @@ import java.util.List;
 import java.util.Map;
 import java.util.Set;
 import java.util.TreeSet;
+import java.util.UUID;
 import java.util.concurrent.ConcurrentLinkedQueue;
 
 public class ContinuousFileProcessingTest {
@@ -73,7 +75,7 @@ public class ContinuousFileProcessingTest {
 
 	private static File baseDir;
 
-	private static org.apache.hadoop.fs.FileSystem hdfs;
+	private static FileSystem hdfs;
 	private static String hdfsURI;
 	private static MiniDFSCluster hdfsCluster;
 
@@ -137,7 +139,7 @@ public class ContinuousFileProcessingTest {
 			Assert.fail("Test passed with an invalid path.");
 
 		} catch (FileNotFoundException e) {
-			Assert.assertEquals("The provided file path " + format.getFilePath().toString() + " does not exist.", e.getMessage());
+			Assert.assertEquals("The provided file path " + format.getFilePath() + " does not exist.", e.getMessage());
 		}
 	}
 
@@ -967,7 +969,7 @@ public class ContinuousFileProcessingTest {
 		}
 	}
 
-	private static abstract class DummySourceContext
+	private abstract static class DummySourceContext
 			implements SourceFunction.SourceContext<TimestampedFileInputSplit> {
 
 		private final Object lock = new Object();
@@ -992,7 +994,7 @@ public class ContinuousFileProcessingTest {
 
 	/////////				Auxiliary Methods				/////////////
 
-	private int getLineNo(String line) {
+	private static int getLineNo(String line) {
 		String[] tkns = line.split("\\s");
 		Assert.assertEquals(6, tkns.length);
 		return Integer.parseInt(tkns[tkns.length - 1]);
@@ -1002,15 +1004,17 @@ public class ContinuousFileProcessingTest {
 	 * Create a file with pre-determined String format of the form:
 	 * {@code fileIdx +": "+ sampleLine +" "+ lineNo}.
 	 * */
-	private Tuple2<org.apache.hadoop.fs.Path, String> createFileAndFillWithData(
+	private static Tuple2<org.apache.hadoop.fs.Path, String> createFileAndFillWithData(
 				String base, String fileName, int fileIdx, String sampleLine) throws IOException {
 
 		assert (hdfs != null);
 
-		org.apache.hadoop.fs.Path file = new org.apache.hadoop.fs.Path(base + "/" + fileName + fileIdx);
+		final String fileRandSuffix = UUID.randomUUID().toString();
+
+		org.apache.hadoop.fs.Path file = new org.apache.hadoop.fs.Path(base + "/" + fileName + fileRandSuffix);
 		Assert.assertFalse(hdfs.exists(file));
 
-		org.apache.hadoop.fs.Path tmp = new org.apache.hadoop.fs.Path(base + "/." + fileName + fileIdx);
+		org.apache.hadoop.fs.Path tmp = new org.apache.hadoop.fs.Path(base + "/." + fileName + fileRandSuffix);
 		FSDataOutputStream stream = hdfs.create(tmp);
 		StringBuilder str = new StringBuilder();
 		for (int i = 0; i < LINES_PER_FILE; i++) {
