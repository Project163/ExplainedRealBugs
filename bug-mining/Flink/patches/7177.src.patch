diff --git a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdDistribution.scala b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdDistribution.scala
index c70c785b66d..696de982bc4 100644
--- a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdDistribution.scala
+++ b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdDistribution.scala
@@ -24,7 +24,7 @@ import org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalSor
 import org.apache.flink.table.planner.utils.ShortcutUtils.unwrapTableConfig
 
 import org.apache.calcite.rel._
-import org.apache.calcite.rel.core.{Calc, Sort, TableScan}
+import org.apache.calcite.rel.core.{Calc, Exchange, Sort, TableScan}
 import org.apache.calcite.rel.metadata._
 import org.apache.calcite.rex.RexInputRef
 import org.apache.calcite.util.mapping.Mappings
@@ -87,6 +87,18 @@ class FlinkRelMdDistribution private extends MetadataHandler[FlinkDistribution]
     }
   }
 
+  def flinkDistribution(exchange: Exchange, mq: RelMetadataQuery): FlinkRelDistribution = {
+    val distribution = exchange.getDistribution
+    if (
+      distribution != null &&
+      distribution.getTraitDef.equals(FlinkRelDistributionTraitDef.INSTANCE)
+    ) {
+      distribution.asInstanceOf[FlinkRelDistribution]
+    } else {
+      getFlinkDistribution(exchange)
+    }
+  }
+
   private def getFlinkDistribution(relNode: RelNode): FlinkRelDistribution = {
     relNode.getTraitSet.getTrait(FlinkRelDistributionTraitDef.INSTANCE)
   }
diff --git a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/schema/IntermediateRelTable.scala b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/schema/IntermediateRelTable.scala
index e586130687d..61e66a4f691 100644
--- a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/schema/IntermediateRelTable.scala
+++ b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/schema/IntermediateRelTable.scala
@@ -18,14 +18,15 @@
 package org.apache.flink.table.planner.plan.schema
 
 import org.apache.flink.annotation.Internal
-import org.apache.flink.table.planner.plan.`trait`.{ModifyKindSet, UpdateKind}
+import org.apache.flink.table.planner.plan.`trait`.ModifyKindSet
+import org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery
 import org.apache.flink.table.planner.plan.stats.FlinkStatistic
 
-import org.apache.calcite.rel.RelNode
+import org.apache.calcite.rel.{RelCollation, RelDistribution, RelNode}
 import org.apache.calcite.util.ImmutableBitSet
 
 import java.util
-import java.util.{List => JList, Set}
+import java.util.{List => JList}
 
 /**
  * An intermediate Table to wrap a optimized RelNode inside. The input data of this Table is
@@ -54,4 +55,14 @@ class IntermediateRelTable(
   def this(names: JList[String], relNode: RelNode) {
     this(names, relNode, ModifyKindSet.INSERT_ONLY, false, new util.HashSet[ImmutableBitSet]())
   }
+
+  override def getCollationList: util.List[RelCollation] = {
+    val mq = relNode.getCluster.getMetadataQuery
+    mq.collations(relNode)
+  }
+
+  override def getDistribution: RelDistribution = {
+    val fmq = FlinkRelMetadataQuery.reuseOrCreate(relNode.getCluster.getMetadataQuery)
+    fmq.flinkDistribution(relNode)
+  }
 }
diff --git a/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/SubplanReuseTest.xml b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/SubplanReuseTest.xml
index ae8d85977e5..a4599e32ccb 100644
--- a/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/SubplanReuseTest.xml
+++ b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/SubplanReuseTest.xml
@@ -1183,6 +1183,43 @@ HashJoin(joinType=[InnerJoin], where=[(a = a0)], select=[a, b, w0$o0, a0, b0, w0
 +- Exchange(distribution=[hash[a]])
    +- Calc(select=[a, b, w0$o0], where=[(b > 10)])
       +- Reused(reference_id=[1])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testSubplanReuseOnSortedView">
+    <Resource name="ast">
+      <![CDATA[
+LogicalSink(table=[default_catalog.default_database.Sink1], fields=[a, b, EXPR$2])
++- LogicalAggregate(group=[{0, 1}], EXPR$2=[LISTAGG($2)])
+   +- LogicalProject(a=[$0], b=[$1], d=[$3])
+      +- LogicalProject(a=[$0], b=[$1], c=[$2], d=[$3], e=[$4])
+         +- LogicalTableScan(table=[[default_catalog, default_database, Source]])
+
+LogicalSink(table=[default_catalog.default_database.Sink2], fields=[a, b, c, d])
++- LogicalProject(a=[$0], b=[$1], c=[$2], d=[$3])
+   +- LogicalSort(sort0=[$2], dir0=[ASC-nulls-first])
+      +- LogicalProject(a=[$0], b=[$1], c=[$2], d=[$3], e=[$4])
+         +- LogicalTableScan(table=[[default_catalog, default_database, Source]])
+]]>
+    </Resource>
+    <Resource name="optimized exec plan">
+      <![CDATA[
+Sink(table=[default_catalog.default_database.Sink1], fields=[a, b, EXPR$2])
++- SortAggregate(isMerge=[true], groupBy=[a, b], select=[a, b, Final_LISTAGG(accDelimiter$0, concatAcc$1) AS EXPR$2])
+   +- Exchange(distribution=[forward])
+      +- Sort(orderBy=[a ASC, b ASC])
+         +- Exchange(distribution=[hash[a, b]])
+            +- LocalSortAggregate(groupBy=[a, b], select=[a, b, Partial_LISTAGG(d) AS (accDelimiter$0, concatAcc$1)])
+               +- Exchange(distribution=[forward])
+                  +- Sort(orderBy=[a ASC, b ASC])
+                     +- Calc(select=[a, b, d])
+                        +- TableSourceScan(table=[[default_catalog, default_database, Source]], fields=[a, b, c, d, e])
+
+Sink(table=[default_catalog.default_database.Sink2], fields=[a, b, c, d])
++- Calc(select=[a, b, c, d])
+   +- Sort(orderBy=[c ASC])
+      +- Exchange(distribution=[single])
+         +- TableSourceScan(table=[[default_catalog, default_database, Source]], fields=[a, b, c, d, e])
 ]]>
     </Resource>
   </TestCase>
diff --git a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/plan/batch/sql/SubplanReuseTest.scala b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/plan/batch/sql/SubplanReuseTest.scala
index de4944c9cd9..4bda75a5b3f 100644
--- a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/plan/batch/sql/SubplanReuseTest.scala
+++ b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/plan/batch/sql/SubplanReuseTest.scala
@@ -334,6 +334,52 @@ class SubplanReuseTest extends TableTestBase {
     util.verifyExecPlan(sqlQuery)
   }
 
+  @Test
+  def testSubplanReuseOnSortedView(): Unit = {
+    util.tableEnv.executeSql("""
+                               |CREATE TABLE Source (
+                               |   a int,
+                               |   b bigint,
+                               |   c string,
+                               |   d string,
+                               |   e string
+                               |) WITH (
+                               |   'connector' = 'values',
+                               |   'bounded' = 'true'
+                               |)
+                               |""".stripMargin)
+    val query = "SELECT * FROM Source order by c"
+    val table = util.tableEnv.sqlQuery(query)
+    // Define a sorted view.
+    util.tableEnv.createTemporaryView("SortedView", table)
+    util.tableEnv.executeSql("""
+                               |CREATE TABLE Sink1 (
+                               |   a int,
+                               |   b bigint,
+                               |   c string
+                               |) WITH (
+                               |   'connector' = 'values',
+                               |   'bounded' = 'true'
+                               |)
+                               |""".stripMargin)
+    util.tableEnv.executeSql("""
+                               |CREATE TABLE Sink2 (
+                               |   a int,
+                               |   b bigint,
+                               |   c string,
+                               |   d string
+                               |) WITH (
+                               |   'connector' = 'values',
+                               |   'bounded' = 'true'
+                               |)
+                               |""".stripMargin)
+    val stmtSet = util.tableEnv.createStatementSet()
+    stmtSet.addInsertSql("INSERT INTO Sink1 select a, b, listagg(d) from SortedView group by a, b")
+    stmtSet.addInsertSql("INSERT INTO Sink2 select a, b, c, d from SortedView")
+
+    util.verifyExecPlan(stmtSet)
+  }
+
   @Test
   def testSubplanReuseOnOverWindowWithNonDeterministicAggCall(): Unit = {
     // FirstValueAggFunction is not deterministic
diff --git a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdDistributionTest.scala b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdDistributionTest.scala
index 44307e30d4f..e7ac7c8ac2d 100644
--- a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdDistributionTest.scala
+++ b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdDistributionTest.scala
@@ -50,6 +50,14 @@ class FlinkRelMdDistributionTest extends FlinkRelMdHandlerTestBase {
       ImmutableList.of("student"),
       streamPhysicalTraits.replace(distribution01))
     assertEquals(distribution01, mq.flinkDistribution(streamScan))
+
+    // Test intermediate table scan.
+    Array(
+      flinkLogicalIntermediateTableScan,
+      batchPhysicalIntermediateTableScan,
+      streamPhysicalIntermediateTableScan).foreach {
+      scan => assertEquals(scan.getTable.getDistribution, mq.flinkDistribution(scan))
+    }
   }
 
   @Test
diff --git a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdHandlerTestBase.scala b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdHandlerTestBase.scala
index 48d2dc204e0..2853be2f324 100644
--- a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdHandlerTestBase.scala
+++ b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdHandlerTestBase.scala
@@ -39,6 +39,7 @@ import org.apache.flink.table.planner.plan.nodes.logical._
 import org.apache.flink.table.planner.plan.nodes.physical.batch._
 import org.apache.flink.table.planner.plan.nodes.physical.stream._
 import org.apache.flink.table.planner.plan.schema.{FlinkPreparingTableBase, IntermediateRelTable, TableSourceTable}
+import org.apache.flink.table.planner.plan.stats.FlinkStatistic
 import org.apache.flink.table.planner.plan.stream.sql.join.TestTemporalTable
 import org.apache.flink.table.planner.plan.utils._
 import org.apache.flink.table.planner.runtime.utils.JavaUserDefinedTableFunctions
@@ -175,6 +176,13 @@ class FlinkRelMdHandlerTestBase {
   protected lazy val tableSourceTableStreamScan: StreamPhysicalDataStreamScan =
     createTableSourceTable(ImmutableList.of("TableSourceTable1"), streamPhysicalTraits)
 
+  protected lazy val flinkLogicalIntermediateTableScan: FlinkLogicalIntermediateTableScan =
+    createIntermediateScan(streamExchangeById, flinkLogicalTraits, Set(ImmutableBitSet.of(0)))
+  protected lazy val batchPhysicalIntermediateTableScan: BatchPhysicalIntermediateTableScan =
+    createIntermediateScan(batchExchangeById, batchPhysicalTraits, Set(ImmutableBitSet.of(0)))
+  protected lazy val streamPhysicalIntermediateTableScan: StreamPhysicalIntermediateTableScan =
+    createIntermediateScan(streamExchangeById, streamPhysicalTraits, Set(ImmutableBitSet.of(0)))
+
   protected lazy val tablePartiallyProjectedKeyLogicalScan: LogicalTableScan =
     createTableSourceTable(
       ImmutableList.of("projected_table_source_table_with_partial_pk"),
@@ -3592,6 +3600,35 @@ class FlinkRelMdHandlerTestBase {
     scan.asInstanceOf[T]
   }
 
+  protected def createIntermediateScan[T](
+      relNode: RelNode,
+      traitSet: RelTraitSet,
+      upsertKeys: util.Set[ImmutableBitSet],
+      statistic: FlinkStatistic = FlinkStatistic.UNKNOWN): T = {
+    val intermediateTable =
+      new IntermediateRelTable(Seq(""), relNode, null, false, upsertKeys, statistic)
+
+    val conventionTrait = traitSet.getTrait(ConventionTraitDef.INSTANCE)
+    val scan = conventionTrait match {
+      case FlinkConventions.LOGICAL =>
+        new FlinkLogicalIntermediateTableScan(cluster, traitSet, intermediateTable)
+      case FlinkConventions.BATCH_PHYSICAL =>
+        new BatchPhysicalIntermediateTableScan(
+          cluster,
+          traitSet,
+          intermediateTable,
+          intermediateTable.getRowType)
+      case FlinkConventions.STREAM_PHYSICAL =>
+        new StreamPhysicalIntermediateTableScan(
+          cluster,
+          traitSet,
+          intermediateTable,
+          intermediateTable.getRowType)
+      case _ => throw new TableException(s"Unsupported convention trait: $conventionTrait")
+    }
+    scan.asInstanceOf[T]
+  }
+
   protected def createTableSourceTable[T](
       tableNames: util.List[String],
       traitSet: RelTraitSet): T = {
diff --git a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdRowCollationTest.scala b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdRowCollationTest.scala
index 68ef78d85aa..da98bbc0810 100644
--- a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdRowCollationTest.scala
+++ b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdRowCollationTest.scala
@@ -18,12 +18,16 @@
 package org.apache.flink.table.planner.plan.metadata
 
 import org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable
+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalIntermediateTableScan
+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalIntermediateTableScan
+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalIntermediateTableScan
 
 import com.google.common.collect.ImmutableList
 import org.apache.calcite.rel.{RelCollation, RelCollations, RelFieldCollation}
 import org.apache.calcite.rel.logical.{LogicalFilter, LogicalProject, LogicalValues}
 import org.apache.calcite.sql.`type`.SqlTypeName
 import org.apache.calcite.sql.fun.SqlStdOperatorTable.{LESS_THAN, PLUS}
+import org.apache.calcite.util.ImmutableBitSet
 import org.junit.Assert.{assertEquals, assertTrue}
 import org.junit.Test
 
@@ -54,6 +58,23 @@ class FlinkRelMdRowCollationTest extends FlinkRelMdHandlerTestBase {
     Array(studentLogicalScan, studentBatchScan, studentStreamScan).foreach {
       scan => assertEquals(ImmutableList.of(), mq.collations(scan))
     }
+
+    // Test intermediate table scan.
+    val flinkLogicalIntermediateTableScan: FlinkLogicalIntermediateTableScan =
+      createIntermediateScan(flinkLogicalSort, flinkLogicalTraits, Set(ImmutableBitSet.of(0)))
+    val batchPhysicalIntermediateTableScan: BatchPhysicalIntermediateTableScan =
+      createIntermediateScan(batchSort, batchPhysicalTraits, Set(ImmutableBitSet.of(0)))
+    val streamPhysicalIntermediateTableScan: StreamPhysicalIntermediateTableScan =
+      createIntermediateScan(streamSort, streamPhysicalTraits, Set(ImmutableBitSet.of(0)))
+    Array(
+      flinkLogicalIntermediateTableScan,
+      batchPhysicalIntermediateTableScan,
+      streamPhysicalIntermediateTableScan).foreach {
+      scan =>
+        assertEquals(
+          ImmutableList.of(scan.intermediateTable.relNode.getTraitSet.getCollation),
+          mq.collations(scan))
+    }
   }
 
   @Test
