diff --git a/flink-core/src/main/java/org/apache/flink/configuration/IllegalConfigurationException.java b/flink-core/src/main/java/org/apache/flink/configuration/IllegalConfigurationException.java
index 1e71fc700e9..e6a20221ea9 100644
--- a/flink-core/src/main/java/org/apache/flink/configuration/IllegalConfigurationException.java
+++ b/flink-core/src/main/java/org/apache/flink/configuration/IllegalConfigurationException.java
@@ -16,42 +16,35 @@
  * limitations under the License.
  */
 
-
 package org.apache.flink.configuration;
 
 /**
- * An <code>IllegalConfigurationException</code> is thrown when the user
- * has configured job vertices in a way that either conflicts
- * with the expected usage of the respective task of the configuration
- * of the Nephele framework.
+ * An {@code IllegalConfigurationException} is thrown when
+ * the values in a given {@link Configuration} are not valid. This may refer
+ * to the Flink configuration with which the framework is started,
+ * or a Configuration passed internally between components.
  */
 public class IllegalConfigurationException extends RuntimeException {
 
-	/**
-	 * Generated serial UID.
-	 */
 	private static final long serialVersionUID = 695506964810499989L;
 
 	/**
-	 * Constructs an new illegal configuration exception with the given error message.
+	 * Constructs an new IllegalConfigurationException with the given error message.
 	 * 
-	 * @param errorMsg
-	 *        the error message to be included in the exception
+	 * @param message The error message for the exception.
 	 */
-	public IllegalConfigurationException(final String errorMsg) {
-		super(errorMsg);
+	public IllegalConfigurationException(String message) {
+		super(message);
 	}
 
 	/**
-	 * Constructs an new illegal configuration exception with the given error message
+	 * Constructs an new IllegalConfigurationException with the given error message
 	 * and a given cause.
-	 * 
-	 * @param errorMsg
-	 *        The error message to be included in the exception.
-	 * @param cause
-	 *        The exception that caused this exception.
+	 *
+	 * @param message The error message for the exception.
+	 * @param cause The exception that caused this exception.
 	 */
-	public IllegalConfigurationException(final String errorMsg, final Throwable cause) {
-		super(errorMsg, cause);
+	public IllegalConfigurationException(String message, Throwable cause) {
+		super(message, cause);
 	}
 }
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/execution/ExecutionState.java b/flink-runtime/src/main/java/org/apache/flink/runtime/execution/ExecutionState.java
index e5235e3c830..2fcaea16cb3 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/execution/ExecutionState.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/execution/ExecutionState.java
@@ -18,6 +18,28 @@
 
 package org.apache.flink.runtime.execution;
 
+/**
+ * An enumeration of all states that a task can be in during its execution.
+ * Tasks usually start in the state {@code CREATED} and switch states according to
+ * this diagram:
+ * <pre>
+ *
+ *     CREATED  -> SCHEDULED -> DEPLOYING -> RUNNING -> FINISHED
+ *                     |            |          |
+ *                     |            |   +------+
+ *                     |            V   V
+ *                     |         CANCELLING -----+----> CANCELED
+ *                     |                         |
+ *                     +-------------------------+
+ *
+ *                                               ... -> FAILED
+ * </pre>
+ *
+ * It is possible to enter the {@code FAILED} state from any other state.
+ *
+ * The states {@code FINISHED}, {@code CANCELED}, and {@code FAILED} are
+ * considered terminal states.
+ */
 public enum ExecutionState {
 
 	CREATED,
@@ -34,5 +56,10 @@ public enum ExecutionState {
 	
 	CANCELED,
 	
-	FAILED
+	FAILED;
+
+
+	public boolean isTerminal() {
+		return this == FINISHED || this == CANCELED || this == FAILED;
+	}
 }
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java b/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java
index a9ee10565d8..3ba378c5f4d 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java
@@ -43,7 +43,7 @@ import org.apache.flink.runtime.jobmanager.scheduler.Scheduler;
 import org.apache.flink.runtime.jobmanager.scheduler.SlotAllocationFuture;
 import org.apache.flink.runtime.jobmanager.scheduler.SlotAllocationFutureAction;
 import org.apache.flink.runtime.jobmanager.scheduler.SlotSharingGroup;
-import org.apache.flink.runtime.messages.TaskManagerMessages.TaskOperationResult;
+import org.apache.flink.runtime.messages.TaskMessages.TaskOperationResult;
 import org.apache.flink.runtime.state.StateHandle;
 import org.apache.flink.util.ExceptionUtils;
 import org.slf4j.Logger;
@@ -68,12 +68,13 @@ import static org.apache.flink.runtime.execution.ExecutionState.FAILED;
 import static org.apache.flink.runtime.execution.ExecutionState.FINISHED;
 import static org.apache.flink.runtime.execution.ExecutionState.RUNNING;
 import static org.apache.flink.runtime.execution.ExecutionState.SCHEDULED;
-import static org.apache.flink.runtime.messages.TaskManagerMessages.CancelTask;
-import static org.apache.flink.runtime.messages.TaskManagerMessages.FailIntermediateResultPartitions;
-import static org.apache.flink.runtime.messages.TaskManagerMessages.SubmitTask;
-import static org.apache.flink.runtime.messages.TaskManagerMessages.UpdateTask;
-import static org.apache.flink.runtime.messages.TaskManagerMessages.UpdateTaskSinglePartitionInfo;
-import static org.apache.flink.runtime.messages.TaskManagerMessages.createUpdateTaskMultiplePartitionInfos;
+
+import static org.apache.flink.runtime.messages.TaskMessages.CancelTask;
+import static org.apache.flink.runtime.messages.TaskMessages.FailIntermediateResultPartitions;
+import static org.apache.flink.runtime.messages.TaskMessages.SubmitTask;
+import static org.apache.flink.runtime.messages.TaskMessages.UpdatePartitionInfo;
+import static org.apache.flink.runtime.messages.TaskMessages.UpdateTaskSinglePartitionInfo;
+import static org.apache.flink.runtime.messages.TaskMessages.createUpdateTaskMultiplePartitionInfos;
 
 /**
  * A single execution of a vertex. While an {@link ExecutionVertex} can be executed multiple times (for recovery,
@@ -526,7 +527,7 @@ public class Execution implements Serializable {
 					final InputChannelDeploymentDescriptor descriptor = new InputChannelDeploymentDescriptor(
 							partitionId, partitionLocation);
 
-					final UpdateTask updateTaskMessage = new UpdateTaskSinglePartitionInfo(
+					final UpdatePartitionInfo updateTaskMessage = new UpdateTaskSinglePartitionInfo(
 							consumer.getAttemptId(), partition.getIntermediateResult().getId(), descriptor);
 
 					sendUpdateTaskRpcCall(consumerSlot, updateTaskMessage);
@@ -685,7 +686,7 @@ public class Execution implements Serializable {
 				inputChannelDeploymentDescriptors.add(partialInputChannelDeploymentDescriptor.createInputChannelDeploymentDescriptor(this));
 			}
 
-			UpdateTask updateTaskMessage =
+			UpdatePartitionInfo updateTaskMessage =
 					createUpdateTaskMultiplePartitionInfos(attemptId, resultIDs,
 							inputChannelDeploymentDescriptors);
 
@@ -845,7 +846,7 @@ public class Execution implements Serializable {
 	}
 
 	private void sendUpdateTaskRpcCall(final SimpleSlot consumerSlot,
-									final UpdateTask updateTaskMsg) {
+										final UpdatePartitionInfo updateTaskMsg) {
 
 		if (consumerSlot != null) {
 			final Instance instance = consumerSlot.getInstance();
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/instance/InstanceConnectionInfo.java b/flink-runtime/src/main/java/org/apache/flink/runtime/instance/InstanceConnectionInfo.java
index ee79c238c09..eb87292afa8 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/instance/InstanceConnectionInfo.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/instance/InstanceConnectionInfo.java
@@ -101,7 +101,8 @@ public class InstanceConnectionInfo implements IOReadableWritable, Comparable<In
 			// take IP textual representation
 			this.hostName = this.fqdnHostName;
 			LOG.warn("No hostname could be resolved for the IP address {}, using IP address as host name. "
-					+ "Local input split assignment (such as for HDFS files) may be impacted.");
+					+ "Local input split assignment (such as for HDFS files) may be impacted.",
+					this.inetAddress.getHostAddress());
 		}
 		else {
 			this.hostName = NetUtils.getHostnameFromFQDN(this.fqdnHostName);
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/instance/Slot.java b/flink-runtime/src/main/java/org/apache/flink/runtime/instance/Slot.java
index bf8464cb8ff..082fbf2acb8 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/instance/Slot.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/instance/Slot.java
@@ -184,7 +184,7 @@ public abstract class Slot {
 		return "(" + slotNumber + ")" + (getParent() != null ? getParent().hierarchy() : "");
 	}
 
-	private static final String getStateName(int state) {
+	private static String getStateName(int state) {
 		switch (state) {
 			case ALLOCATED_AND_ALIVE:
 				return "ALLOCATED/ALIVE";
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/NetworkEnvironment.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/NetworkEnvironment.java
index 55b89b4176a..dbf15861247 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/NetworkEnvironment.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/NetworkEnvironment.java
@@ -49,7 +49,7 @@ import java.io.IOException;
 
 import static com.google.common.base.Preconditions.checkNotNull;
 import static org.apache.flink.runtime.messages.JobManagerMessages.ScheduleOrUpdateConsumers;
-import static org.apache.flink.runtime.messages.TaskManagerMessages.FailTask;
+import static org.apache.flink.runtime.messages.TaskMessages.FailTask;
 
 /**
  * Network I/O components of each {@link TaskManager} instance. The network environment contains
@@ -165,6 +165,7 @@ public class NetworkEnvironment {
 			{
 				// good, not currently associated. start the individual components
 
+				LOG.debug("Starting result partition manager and network connection manager");
 				this.partitionManager = new ResultPartitionManager();
 				this.taskEventDispatcher = new TaskEventDispatcher();
 				this.partitionConsumableNotifier = new JobManagerResultPartitionConsumableNotifier(
@@ -176,6 +177,7 @@ public class NetworkEnvironment {
 															: new LocalConnectionManager();
 
 				try {
+					LOG.debug("Starting network connection manager");
 					connectionManager.start(partitionManager, taskEventDispatcher, networkBufferPool);
 				}
 				catch (Throwable t) {
@@ -312,7 +314,7 @@ public class NetworkEnvironment {
 	}
 
 	public void unregisterTask(Task task) {
-		LOG.debug("Unregistering task {} ({}) from network environment (state: {}).",
+		LOG.debug("Unregister task {} from network environment (state: {}).",
 				task.getTaskNameWithSubtasks(), task.getExecutionState());
 
 		final ExecutionAttemptID executionId = task.getExecutionId();
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/memorymanager/DefaultMemoryManager.java b/flink-runtime/src/main/java/org/apache/flink/runtime/memorymanager/DefaultMemoryManager.java
index cd677acb247..28ebe13d161 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/memorymanager/DefaultMemoryManager.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/memorymanager/DefaultMemoryManager.java
@@ -165,6 +165,12 @@ public class DefaultMemoryManager implements MemoryManager {
 		// -------------------- END CRITICAL SECTION -------------------
 	}
 
+	@Override
+	public boolean isShutdown() {
+		return this.isShutDown;
+	}
+
+	@Override
 	public boolean verifyEmpty() {
 		synchronized (this.lock) {
 			return this.freeSegments.size() == this.totalNumPages;
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/memorymanager/MemoryManager.java b/flink-runtime/src/main/java/org/apache/flink/runtime/memorymanager/MemoryManager.java
index 1ab6931f069..631f0b8b08a 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/memorymanager/MemoryManager.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/memorymanager/MemoryManager.java
@@ -117,7 +117,14 @@ public interface MemoryManager {
 	 * code that allocated them from the memory manager.
 	 */
 	void shutdown();
-	
+
+	/**
+	 * Checks whether the MemoryManager has been shut down.
+	 *
+	 * @return True, if the memory manager is shut down, false otherwise.
+	 */
+	boolean isShutdown();
+
 	/**
 	 * Checks if the memory manager all memory available.
 	 * 
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/Task.java b/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/Task.java
index 3d1419adebf..f6eb9075ba9 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/Task.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/Task.java
@@ -30,8 +30,8 @@ import org.apache.flink.api.common.JobID;
 import org.apache.flink.runtime.jobgraph.JobVertexID;
 import org.apache.flink.runtime.memorymanager.MemoryManager;
 import org.apache.flink.runtime.messages.ExecutionGraphMessages;
-import org.apache.flink.runtime.messages.JobManagerMessages;
-import org.apache.flink.runtime.messages.TaskManagerMessages.UnregisterTask;
+import org.apache.flink.runtime.messages.TaskMessages;
+import org.apache.flink.runtime.messages.TaskMessages.UnregisterTask;
 import org.apache.flink.runtime.profiling.TaskManagerProfiler;
 import org.apache.flink.util.ExceptionUtils;
 import org.slf4j.Logger;
@@ -304,7 +304,7 @@ public class Task {
 				}
 			}
 			else {
-				throw new RuntimeException("unexpected state for cancelling: " + current);
+				throw new RuntimeException("unexpected state for failing the task: " + current);
 			}
 		}
 	}
@@ -361,7 +361,7 @@ public class Task {
 											Throwable optionalError) {
 		LOG.info("Update execution state of {} ({}) to {}.", this.getTaskName(),
 				this.getExecutionId(), executionState);
-		taskManager.tell(new JobManagerMessages.UpdateTaskExecutionState(
+		taskManager.tell(new TaskMessages.UpdateTaskExecutionState(
 				new TaskExecutionState(jobId, executionId, executionState, optionalError)),
 				ActorRef.noSender());
 
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/TaskInputSplitProvider.java b/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/TaskInputSplitProvider.java
index 6e446defaec..1bdc34683bb 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/TaskInputSplitProvider.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/TaskInputSplitProvider.java
@@ -28,12 +28,10 @@ import org.apache.flink.api.common.JobID;
 import org.apache.flink.runtime.jobgraph.JobVertexID;
 import org.apache.flink.runtime.jobgraph.tasks.InputSplitProvider;
 import org.apache.flink.runtime.messages.JobManagerMessages;
-import org.apache.flink.runtime.messages.TaskManagerMessages;
 import org.apache.flink.util.InstantiationUtil;
 
 import scala.concurrent.Await;
 import scala.concurrent.Future;
-import scala.concurrent.duration.FiniteDuration;
 
 public class TaskInputSplitProvider implements InputSplitProvider {
 
@@ -47,11 +45,11 @@ public class TaskInputSplitProvider implements InputSplitProvider {
 
 	private final ClassLoader usercodeClassLoader;
 	
-	private final FiniteDuration timeout;
+	private final Timeout timeout;
 	
 	public TaskInputSplitProvider(ActorRef jobManager, JobID jobId, JobVertexID vertexId,
 								ExecutionAttemptID executionID, ClassLoader userCodeClassLoader,
-								FiniteDuration timeout)
+								Timeout timeout)
 	{
 		this.jobManager = jobManager;
 		this.jobId = jobId;
@@ -66,20 +64,20 @@ public class TaskInputSplitProvider implements InputSplitProvider {
 		try {
 			final Future<Object> response = Patterns.ask(jobManager,
 					new JobManagerMessages.RequestNextInputSplit(jobId, vertexId, executionID),
-					new Timeout(timeout));
+					timeout);
 
-			final Object result = Await.result(response, timeout);
+			final Object result = Await.result(response, timeout.duration());
 
 			if (result == null) {
 				return null;
 			}
 
-			if(!(result instanceof TaskManagerMessages.NextInputSplit)){
+			if(!(result instanceof JobManagerMessages.NextInputSplit)){
 				throw new RuntimeException("RequestNextInputSplit requires a response of type " +
 						"NextInputSplit. Instead response is of type " + result.getClass() + ".");
 			} else {
-				final TaskManagerMessages.NextInputSplit nextInputSplit =
-						(TaskManagerMessages.NextInputSplit) result;
+				final JobManagerMessages.NextInputSplit nextInputSplit =
+						(JobManagerMessages.NextInputSplit) result;
 
 				byte[] serializedData = nextInputSplit.splitData();
 				Object deserialized = InstantiationUtil.deserializeObject(serializedData,
diff --git a/flink-runtime/src/main/scala/org/apache/flink/runtime/ActorLogMessages.scala b/flink-runtime/src/main/scala/org/apache/flink/runtime/ActorLogMessages.scala
index 5d4f89ca82b..acd434622c5 100644
--- a/flink-runtime/src/main/scala/org/apache/flink/runtime/ActorLogMessages.scala
+++ b/flink-runtime/src/main/scala/org/apache/flink/runtime/ActorLogMessages.scala
@@ -37,14 +37,14 @@ trait ActorLogMessages {
         _receiveWithLogMessages(x)
       }
       else {
-        log.debug(s"Received message $x at ${that.self.path} from ${that.sender}.")
+        log.debug(s"Received message $x at ${that.self.path} from ${that.sender()}.")
 
         val start = System.nanoTime()
 
         _receiveWithLogMessages(x)
 
         val duration = (System.nanoTime() - start) / 1000000
-        log.debug(s"Handled message $x in $duration ms from ${that.sender}.")
+        log.debug(s"Handled message $x in $duration ms from ${that.sender()}.")
       }
     }
   }
diff --git a/flink-runtime/src/main/scala/org/apache/flink/runtime/akka/AkkaUtils.scala b/flink-runtime/src/main/scala/org/apache/flink/runtime/akka/AkkaUtils.scala
index e898f44b06a..5b33017f083 100644
--- a/flink-runtime/src/main/scala/org/apache/flink/runtime/akka/AkkaUtils.scala
+++ b/flink-runtime/src/main/scala/org/apache/flink/runtime/akka/AkkaUtils.scala
@@ -43,6 +43,17 @@ object AkkaUtils {
 
   var globalExecutionContext: ExecutionContext = ExecutionContext.global
 
+  /**
+   * Creates a local actor system without remoting.
+   *
+   * @param configuration instance containing the user provided configuration values
+   * @return The created actor system
+   */
+  def createLocalActorSystem(configuration: Configuration): ActorSystem = {
+    val akkaConfig = getAkkaConfig(configuration, None)
+    createActorSystem(akkaConfig)
+  }
+
   /**
    * Creates an actor system. If a listening address is specified, then the actor system will listen
    * on that address for messages from a remote actor system. If not, then a local actor system
diff --git a/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManager.scala b/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManager.scala
index 4b0a55ba98b..49bb1d54b34 100644
--- a/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManager.scala
+++ b/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManager.scala
@@ -30,8 +30,10 @@ import org.apache.flink.runtime.client.{JobStatusMessage, JobSubmissionException
 import org.apache.flink.runtime.executiongraph.{ExecutionJobVertex, ExecutionGraph}
 import org.apache.flink.runtime.jobmanager.web.WebInfoServer
 import org.apache.flink.runtime.messages.ArchiveMessages.ArchiveExecutionGraph
+import org.apache.flink.runtime.messages.CheckpointingMessages.{StateBarrierAck, BarrierAck}
 import org.apache.flink.runtime.messages.ExecutionGraphMessages.JobStatusChanged
 import org.apache.flink.runtime.messages.Messages.{Disconnect, Acknowledge}
+import org.apache.flink.runtime.messages.TaskMessages.UpdateTaskExecutionState
 import org.apache.flink.runtime.process.ProcessReaper
 import org.apache.flink.runtime.security.SecurityUtils
 import org.apache.flink.runtime.security.SecurityUtils.FlinkSecuredRunner
@@ -46,9 +48,9 @@ import org.apache.flink.runtime.jobmanager.accumulators.AccumulatorManager
 import org.apache.flink.runtime.jobmanager.scheduler.{Scheduler => FlinkScheduler}
 import org.apache.flink.runtime.messages.JobManagerMessages._
 import org.apache.flink.runtime.messages.RegistrationMessages._
-import org.apache.flink.runtime.messages.TaskManagerMessages.{SendStackTrace, NextInputSplit, Heartbeat}
+import org.apache.flink.runtime.messages.TaskManagerMessages.{SendStackTrace, Heartbeat}
 import org.apache.flink.runtime.profiling.ProfilingUtils
-import org.apache.flink.util.InstantiationUtil
+import org.apache.flink.util.{ExceptionUtils, InstantiationUtil}
 
 import org.slf4j.LoggerFactory
 
@@ -76,7 +78,7 @@ import scala.collection.JavaConverters._
  *  is indicated by [[CancellationSuccess]] and a failure by [[CancellationFailure]]
  *
  * - [[UpdateTaskExecutionState]] is sent by a TaskManager to update the state of an
- * [[org.apache.flink.runtime.executiongraph.ExecutionVertex]] contained in the [[ExecutionGraph]].
+     ExecutionVertex contained in the [[ExecutionGraph]].
  * A successful update is acknowledged by true and otherwise false.
  *
  * - [[RequestNextInputSplit]] requests the next input split for a running task on a
@@ -103,17 +105,17 @@ class JobManager(val flinkConfiguration: Configuration,
 
   /** List of current jobs running jobs */
   val currentJobs = scala.collection.mutable.HashMap[JobID, (ExecutionGraph, JobInfo)]()
-  
+
 
   /**
    * Run when the job manager is started. Simply logs an informational message.
    */
   override def preStart(): Unit = {
-    LOG.info(s"Starting JobManager at ${self.path}.")
+    LOG.info(s"Starting JobManager at ${self.path.toSerializationFormat}.")
   }
 
   override def postStop(): Unit = {
-    log.info(s"Stopping job manager ${self.path}.")
+    log.info(s"Stopping JobManager ${self.path.toSerializationFormat}.")
 
     // disconnect the registered task managers
     instanceManager.getAllRegisteredInstances.asScala.foreach {
@@ -148,29 +150,38 @@ class JobManager(val flinkConfiguration: Configuration,
    */
   override def receiveWithLogMessages: Receive = {
 
-    case RegisterTaskManager(connectionInfo, hardwareInformation, numberOfSlots) =>
-      val taskManager = sender
+    case RegisterTaskManager(taskManager, connectionInfo, hardwareInformation, numberOfSlots) =>
 
       if (instanceManager.isRegistered(taskManager)) {
         val instanceID = instanceManager.getRegisteredInstance(taskManager).getId
-        taskManager ! AlreadyRegistered(instanceID, libraryCacheManager.getBlobServerPort, profiler)
-      } else {
 
-        val instanceID = try {
-           instanceManager.registerTaskManager(taskManager, connectionInfo,
+        // IMPORTANT: Send the response to the "sender", which is not the
+        //            TaskManager actor, but the ask future!
+        sender() ! AlreadyRegistered(self, instanceID, libraryCacheManager.getBlobServerPort)
+      }
+      else {
+        try {
+          val instanceID = instanceManager.registerTaskManager(taskManager, connectionInfo,
             hardwareInformation, numberOfSlots)
-        } catch {
+
+          // IMPORTANT: Send the response to the "sender", which is not the
+          //            TaskManager actor, but the ask future!
+          sender() ! AcknowledgeRegistration(self, instanceID,
+                                             libraryCacheManager.getBlobServerPort)
+
+          // to be notified when the taskManager is no longer reachable
+          context.watch(taskManager)
+        }
+        catch {
           // registerTaskManager throws an IllegalStateException if it is already shut down
           // let the actor crash and restart itself in this case
-          case ex: Exception => throw new RuntimeException(s"Could not register the task manager " +
-            s"${taskManager.path} at the instance manager.", ex)
-        }
-
-        // to be notified when the taskManager is no longer reachable
-        context.watch(taskManager)
+          case e: Exception =>
+            log.error(e, "Failed to register TaskManager at instance manager")
 
-        taskManager ! AcknowledgeRegistration(instanceID, libraryCacheManager.getBlobServerPort,
-          profiler)
+            // IMPORTANT: Send the response to the "sender", which is not the
+            //            TaskManager actor, but the ask future!
+            sender() ! RefuseRegistration(ExceptionUtils.stringifyException(e))
+        }
       }
 
     case RequestNumberRegisteredTaskManager =>
@@ -205,7 +216,7 @@ class JobManager(val flinkConfiguration: Configuration,
       } else {
         currentJobs.get(taskExecutionState.getJobID) match {
           case Some((executionGraph, _)) =>
-            val originalSender = sender
+            val originalSender = sender()
 
             Future {
               val result = executionGraph.updateState(taskExecutionState)
@@ -302,7 +313,7 @@ class JobManager(val flinkConfiguration: Configuration,
             }
 
             removeJob(jobID)
-            
+
           }
         case None =>
           removeJob(jobID)
@@ -320,7 +331,7 @@ class JobManager(val flinkConfiguration: Configuration,
           jobExecution._1.getStateCheckpointerActor forward  msg
         case None =>
       }
-      
+
     case ScheduleOrUpdateConsumers(jobId, partitionId) =>
       currentJobs.get(jobId) match {
         case Some((executionGraph, _)) =>
@@ -343,7 +354,7 @@ class JobManager(val flinkConfiguration: Configuration,
       } catch {
         case t: Throwable =>
           log.error(t, "Could not process accumulator event of job {} received from {}.",
-            accumulatorEvent.getJobID, sender.path)
+            accumulatorEvent.getJobID, sender().path)
       }
 
     case RequestAccumulatorResults(jobID) =>
@@ -396,9 +407,10 @@ class JobManager(val flinkConfiguration: Configuration,
 
     case Heartbeat(instanceID, metricsReport) =>
       try {
+        log.debug("Received hearbeat message from {}", instanceID)
         instanceManager.reportHeartBeat(instanceID, metricsReport)
       } catch {
-        case t: Throwable => log.error(t, "Could not report heart beat from {}.", sender.path)
+        case t: Throwable => log.error(t, "Could not report heart beat from {}.", sender().path)
       }
 
     case RequestStackTrace(instanceID) =>
@@ -414,10 +426,10 @@ class JobManager(val flinkConfiguration: Configuration,
       }
 
     case RequestJobManagerStatus =>
-      sender ! JobManagerStatusAlive
+      sender() ! JobManagerStatusAlive
 
     case Disconnect(msg) =>
-      val taskManager = sender
+      val taskManager = sender()
 
       if (instanceManager.isRegistered(taskManager)) {
         log.info("Task manager {} wants to disconnect, because {}.", taskManager.path, msg)
@@ -474,7 +486,7 @@ class JobManager(val flinkConfiguration: Configuration,
         executionGraph = currentJobs.getOrElseUpdate(jobGraph.getJobID,
           (new ExecutionGraph(jobGraph.getJobID, jobGraph.getName,
             jobGraph.getJobConfiguration, timeout, jobGraph.getUserJarBlobKeys, userCodeLoader),
-            JobInfo(sender, System.currentTimeMillis())))._1
+            JobInfo(sender(), System.currentTimeMillis())))._1
 
         // configure the execution graph
         val jobNumberRetries = if (jobGraph.getNumberOfExecutionRetries >= 0) {
@@ -486,7 +498,7 @@ class JobManager(val flinkConfiguration: Configuration,
         executionGraph.setDelayBeforeRetrying(delayBetweenRetries)
         executionGraph.setScheduleMode(jobGraph.getScheduleMode)
         executionGraph.setQueuedSchedulingAllowed(jobGraph.getAllowQueuedScheduling)
-        
+
         executionGraph.setCheckpointingEnabled(jobGraph.isCheckpointingEnabled)
         executionGraph.setCheckpointingInterval(jobGraph.getCheckpointingInterval)
 
@@ -532,15 +544,15 @@ class JobManager(val flinkConfiguration: Configuration,
         }
 
         // give an actorContext
-        executionGraph.setParentContext(context);
-        
+        executionGraph.setParentContext(context)
+
         // get notified about job status changes
         executionGraph.registerJobStatusListener(self)
 
         if (listenToEvents) {
           // the sender wants to be notified about state changes
-          executionGraph.registerExecutionListener(sender)
-          executionGraph.registerJobStatusListener(sender)
+          executionGraph.registerExecutionListener(sender())
+          executionGraph.registerJobStatusListener(sender())
         }
 
         // done with submitting the job
@@ -769,9 +781,12 @@ object JobManager {
       if (executionMode == JobManagerMode.LOCAL) {
         LOG.info("Starting embedded TaskManager for JobManager's LOCAL execution mode")
 
-        val taskManagerActor = TaskManager.startTaskManagerActor(
-          configuration, jobManagerSystem, listeningAddress,
-          TaskManager.TASK_MANAGER_NAME, true, true, classOf[TaskManager])
+        val taskManagerActor = TaskManager.startTaskManagerComponentsAndActor(
+                        configuration, jobManagerSystem,
+                        listeningAddress,
+                        Some(TaskManager.TASK_MANAGER_NAME),
+                        Some(jobManager.path.toString),
+                        true, classOf[TaskManager])
 
         LOG.debug("Starting TaskManager process reaper")
         jobManagerSystem.actorOf(
@@ -965,6 +980,25 @@ object JobManager {
   def startJobManagerActors(configuration: Configuration,
                             actorSystem: ActorSystem): (ActorRef, ActorRef) = {
 
+    startJobManagerActors(configuration,actorSystem, Some(JOB_MANAGER_NAME), Some(ARCHIVE_NAME))
+  }
+  /**
+   * Starts the JobManager and job archiver based on the given configuration, in the
+   * given actor system.
+   *
+   * @param configuration The configuration for the JobManager
+   * @param actorSystem The actor system running the JobManager
+   * @param jobMangerActorName Optionally the name of the JobManager actor. If none is given,
+   *                          the actor will have the name generated by the actor system.
+   * @param archiverActorName Optionally the name of the archive actor. If none is given,
+   *                          the actor will have the name generated by the actor system.
+   * @return A tuple of references (JobManager Ref, Archiver Ref)
+   */
+  def startJobManagerActors(configuration: Configuration,
+                            actorSystem: ActorSystem,
+                            jobMangerActorName: Option[String],
+                            archiverActorName: Option[String]): (ActorRef, ActorRef) = {
+
     val (instanceManager, scheduler, libraryCacheManager, archiveProps, accumulatorManager,
       profilerProps, executionRetries, delayBetweenRetries,
       timeout, _) = createJobManagerComponents(configuration)
@@ -972,13 +1006,20 @@ object JobManager {
     val profiler: Option[ActorRef] =
                  profilerProps.map( props => actorSystem.actorOf(props, PROFILER_NAME) )
 
-    val archiver: ActorRef = actorSystem.actorOf(archiveProps, JobManager.ARCHIVE_NAME)
+    // start the archiver wither with the given name, or without (avoid name conflicts)
+    val archiver: ActorRef = archiverActorName match {
+      case Some(actorName) => actorSystem.actorOf(archiveProps, actorName)
+      case None => actorSystem.actorOf(archiveProps)
+    }
 
     val jobManagerProps = Props(classOf[JobManager], configuration, instanceManager, scheduler,
         libraryCacheManager, archiver, accumulatorManager, profiler, executionRetries,
         delayBetweenRetries, timeout)
 
-    val jobManager = startActor(jobManagerProps, actorSystem)
+    val jobManager: ActorRef = jobMangerActorName match {
+      case Some(actorName) => actorSystem.actorOf(jobManagerProps, actorName)
+      case None => actorSystem.actorOf(jobManagerProps)
+    }
 
     (jobManager, archiver)
   }
diff --git a/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/StreamCheckpointCoordinator.scala b/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/StreamCheckpointCoordinator.scala
index f42d08ab557..48266e21872 100644
--- a/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/StreamCheckpointCoordinator.scala
+++ b/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/StreamCheckpointCoordinator.scala
@@ -18,19 +18,19 @@
 
 package org.apache.flink.runtime.jobmanager
 
-import java.lang.Long
+import java.lang.{Long => JLong}
 
 import akka.actor._
-import org.apache.flink.api.common.JobID
 import org.apache.flink.runtime.ActorLogMessages
-import org.apache.flink.runtime.executiongraph.{ExecutionAttemptID, ExecutionGraph, ExecutionVertex}
+import org.apache.flink.runtime.execution.ExecutionState
+import org.apache.flink.runtime.executiongraph.{ExecutionGraph, ExecutionVertex}
 import org.apache.flink.runtime.jobgraph.JobStatus._
 import org.apache.flink.runtime.jobgraph.JobVertexID
+import org.apache.flink.runtime.messages.CheckpointingMessages._
 import org.apache.flink.runtime.state.StateHandle
 
 import scala.collection.JavaConversions._
 import scala.collection.immutable.TreeMap
-import scala.concurrent.ExecutionContext.Implicits.global
 import scala.concurrent.duration.{FiniteDuration, _}
 
 /**
@@ -63,15 +63,18 @@ import scala.concurrent.duration.{FiniteDuration, _}
  */
 
 class StreamCheckpointCoordinator(val executionGraph: ExecutionGraph,
-                         val vertices: Iterable[ExecutionVertex],
-                         var acks: Map[(JobVertexID,Int),List[Long]],
-                         var states: Map[(JobVertexID, Integer, Long), 
-                                 StateHandle],
-                         val interval: FiniteDuration,var curId: Long,var ackId: Long)
-        extends Actor with ActorLogMessages with ActorLogging {
-  
+                                  val vertices: Iterable[ExecutionVertex],
+                                  var acks: Map[(JobVertexID,Int),List[JLong]],
+                                  var states: Map[(JobVertexID, Integer, JLong), StateHandle],
+                                  val interval: FiniteDuration,
+                                  var curId: JLong,
+                                  var ackId: JLong)
+extends Actor with ActorLogMessages with ActorLogging {
+
+  implicit private val executor = context.dispatcher
+
   override def receiveWithLogMessages: Receive = {
-    
+
     case InitBarrierScheduler =>
       context.system.scheduler.schedule(interval,interval,self,BarrierTimeout)
       context.system.scheduler.schedule(2 * interval,2 * interval,self,CompactAndUpdate)
@@ -87,7 +90,7 @@ class StreamCheckpointCoordinator(val executionGraph: ExecutionGraph,
           curId += 1
           log.debug("Sending Barrier to vertices of Job " + executionGraph.getJobName)
           vertices.filter(v => v.getJobVertex.getJobVertex.isInputVertex &&
-                  v.getExecutionState == RUNNING).foreach(vertex
+                  v.getExecutionState == ExecutionState.RUNNING).foreach(vertex
           => vertex.getCurrentAssignedResource.getInstance.getTaskManager
                     ! BarrierReq(vertex.getCurrentExecutionAttempt.getAttemptId,curId))
         case _ =>
@@ -105,16 +108,17 @@ class StreamCheckpointCoordinator(val executionGraph: ExecutionGraph,
               acks += (jobVertexID,instanceID) -> (checkpointID :: acklist)
             case None =>
           }
-          log.debug(acks.toString)
+          log.debug(acks.toString())
       
     case CompactAndUpdate =>
-      val barrierCount = acks.values.foldLeft(TreeMap[Long,Int]().withDefaultValue(0))((dict,myList)
+      val barrierCount =
+        acks.values.foldLeft(TreeMap[JLong,Int]().withDefaultValue(0))((dict,myList)
       => myList.foldLeft(dict)((dict2,elem) => dict2.updated(elem,dict2(elem) + 1)))
       val keysToKeep = barrierCount.filter(_._2 == acks.size).keys
-      ackId = if(!keysToKeep.isEmpty) keysToKeep.max else ackId
+      ackId = if(keysToKeep.nonEmpty) keysToKeep.max else ackId
       acks.keys.foreach(x => acks = acks.updated(x,acks(x).filter(_ >= ackId)))
       states = states.filterKeys(_._3 >= ackId)
-      log.debug("Last global barrier is " + ackId)
+      log.debug("[FT-MONITOR] Last global barrier is " + ackId)
       executionGraph.loadOperatorStates(states)
       
   }
@@ -128,7 +132,7 @@ object StreamCheckpointCoordinator {
     val vertices: Iterable[ExecutionVertex] = getExecutionVertices(executionGraph)
     val monitor = context.system.actorOf(Props(new StreamCheckpointCoordinator(executionGraph,
       vertices,vertices.map(x => ((x.getJobVertex.getJobVertexId,x.getParallelSubtaskIndex),
-              List.empty[Long])).toMap, Map() ,interval,0L,-1L)))
+              List.empty[JLong])).toMap, Map() ,interval,0L,-1L)))
     monitor ! InitBarrierScheduler
     monitor
   }
@@ -145,14 +149,3 @@ case class BarrierTimeout()
 case class InitBarrierScheduler()
 
 case class CompactAndUpdate()
-
-case class BarrierReq(attemptID: ExecutionAttemptID,checkpointID: Long)
-
-case class BarrierAck(jobID: JobID,jobVertexID: JobVertexID,instanceID: Int,checkpointID: Long)
-
-case class StateBarrierAck(jobID: JobID, jobVertexID: JobVertexID, instanceID: Integer,
-                           checkpointID: Long, states: StateHandle)
-       
-
-
-
diff --git a/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/CheckpointingMessages.scala b/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/CheckpointingMessages.scala
new file mode 100644
index 00000000000..9f6f51ae81f
--- /dev/null
+++ b/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/CheckpointingMessages.scala
@@ -0,0 +1,52 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.messages
+
+import org.apache.flink.api.common.JobID
+import org.apache.flink.runtime.executiongraph.ExecutionAttemptID
+import org.apache.flink.runtime.jobgraph.JobVertexID
+import org.apache.flink.runtime.state.StateHandle
+
+/**
+ * Actor messages specific to checkpoints (triggering, acknowledging,
+ * state transfer, ...)
+ */
+object CheckpointingMessages {
+
+  /**
+   * Abstract base trait for all checkpoint messages.
+   */
+  trait CheckpointingMessage
+
+  // --------------------------------------------------------------------------
+
+  case class BarrierReq(attemptID: ExecutionAttemptID,
+                        checkpointID: Long) extends CheckpointingMessage
+
+  case class BarrierAck(jobID: JobID,
+                        jobVertexID:JobVertexID,
+                        instanceID: Int,
+                        checkpointID: Long) extends CheckpointingMessage
+
+  case class StateBarrierAck(jobID: JobID,
+                             jobVertexID: JobVertexID,
+                             instanceID: Integer,
+                             checkpointID: java.lang.Long,
+                             states: StateHandle) extends CheckpointingMessage
+}
diff --git a/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/ExecutionGraphMessages.scala b/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/ExecutionGraphMessages.scala
index 6785c31e326..6bc59a26b81 100644
--- a/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/ExecutionGraphMessages.scala
+++ b/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/ExecutionGraphMessages.scala
@@ -64,7 +64,7 @@ object ExecutionGraphMessages {
   /**
    * Denotes the job state change of a job.
    *
-   * @param jobID identifying the correspong job
+   * @param jobID identifying the corresponding job
    * @param newJobStatus
    * @param timestamp
    * @param error
diff --git a/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/JobManagerMessages.scala b/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/JobManagerMessages.scala
index dab467155bb..73e20c01a21 100644
--- a/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/JobManagerMessages.scala
+++ b/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/JobManagerMessages.scala
@@ -25,7 +25,6 @@ import org.apache.flink.runtime.executiongraph.{ExecutionAttemptID, ExecutionGra
 import org.apache.flink.runtime.instance.{InstanceID, Instance}
 import org.apache.flink.runtime.io.network.partition.ResultPartitionID
 import org.apache.flink.runtime.jobgraph.{JobGraph, JobStatus, JobVertexID}
-import org.apache.flink.runtime.taskmanager.TaskExecutionState
 
 import scala.collection.JavaConverters._
 
@@ -52,14 +51,6 @@ object JobManagerMessages {
    */
   case class CancelJob(jobID: JobID)
 
-  /**
-   * Denotes a state change of a task at the JobManager. The update success is acknowledged by a
-   * boolean value which is sent back to the sender.
-   *
-   * @param taskExecutionState
-   */
-  case class UpdateTaskExecutionState(taskExecutionState: TaskExecutionState)
-
   /**
    * Requesting next input split for the
    * [[org.apache.flink.runtime.executiongraph.ExecutionJobVertex]]
@@ -72,6 +63,14 @@ object JobManagerMessages {
   case class RequestNextInputSplit(jobID: JobID, vertexID: JobVertexID, executionAttempt:
   ExecutionAttemptID)
 
+  /**
+   * Contains the next input split for a task. This message is a response to
+   * [[org.apache.flink.runtime.messages.JobManagerMessages.RequestNextInputSplit]].
+   *
+   * @param splitData
+   */
+  case class NextInputSplit(splitData: Array[Byte])
+
   /**
    * Notifies the [[org.apache.flink.runtime.jobmanager.JobManager]] about available data for a
    * produced partition.
diff --git a/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/Messages.scala b/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/Messages.scala
index 91c3b2d9e87..ab8b8c271a3 100644
--- a/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/Messages.scala
+++ b/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/Messages.scala
@@ -18,17 +18,34 @@
 
 package org.apache.flink.runtime.messages
 
+/**
+ * Generic messages between JobManager, TaskManager, JobClient.
+ */
 object Messages {
 
   /**
    * Message to signal the successful reception of another message
    */
-  case object Acknowledge
+  case object Acknowledge {
+
+    /**
+     * Accessor for the case object instance, to simplify Java interoperability.
+     *
+     * @return The Acknowledge case object instance.
+     */
+    def get(): Acknowledge.type = this
+  }
 
   /**
-   * Signals that the JobManager/TaskManager shall disconnect from the sender
-   * (TaskManager/JobManager)
-   * @param reason
+   * Signals that the receiver (JobManager/TaskManager) shall disconnect the sender.
+   *
+   * The TaskManager may send this on shutdown to let the JobManager realize the TaskManager
+   * loss more quickly.
+   *
+   * The JobManager may send this message to its TaskManagers to let them clean up their
+   * tasks that depend on the JobManager and go into a clean state.
+   *
+   * @param reason The reason for disconnecting, to be displayed in log and error messages.
    */
   case class Disconnect(reason: String)
 
diff --git a/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/RegistrationMessages.scala b/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/RegistrationMessages.scala
index 1a3479af4eb..3051d00509a 100644
--- a/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/RegistrationMessages.scala
+++ b/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/RegistrationMessages.scala
@@ -21,40 +21,67 @@ package org.apache.flink.runtime.messages
 import akka.actor.ActorRef
 import org.apache.flink.runtime.instance.{InstanceConnectionInfo, InstanceID, HardwareDescription}
 
+import scala.concurrent.duration.{Deadline, FiniteDuration}
+
+/**
+ * A set of messages from the between TaskManager and JobManager handle the
+ * registration of the TaskManager at the JobManager.
+ */
 object RegistrationMessages {
 
+  /**
+   * Marker trait for registration messages.
+   */
+  trait RegistrationMessage
+
+  /**
+   * Triggers the TaskManager to attempt a registration at the JobManager.
+   *
+   * @param jobManagerAkkaURL The actor URL of the JobManager.
+   * @param timeout The timeout for the message. The next retry will double this timeout.
+   * @param deadline Optional deadline until when the registration must be completed.
+   * @param attempt The attempt number, for logging.
+   */
+  case class TriggerTaskManagerRegistration(jobManagerAkkaURL: String,
+                                            timeout: FiniteDuration,
+                                            deadline: Option[Deadline],
+                                            attempt: Int)
+    extends RegistrationMessage
+
   /**
    * Registers a task manager at the job manager. A successful registration is acknowledged by
    * [[AcknowledgeRegistration]].
    *
-   * @param connectionInfo
-   * @param hardwareDescription
-   * @param numberOfSlots
+   * @param taskManager The TaskManager actor.
+   * @param connectionInfo The TaskManagers connection information.
+   * @param resources The TaskManagers resources.
+   * @param numberOfSlots The number of processing slots offered by the TaskManager.
    */
-  case class RegisterTaskManager(connectionInfo: InstanceConnectionInfo,
-                                 hardwareDescription: HardwareDescription,
+  case class RegisterTaskManager(taskManager: ActorRef,
+                                 connectionInfo: InstanceConnectionInfo,
+                                 resources: HardwareDescription,
                                  numberOfSlots: Int)
+    extends RegistrationMessage
 
   /**
    * Denotes the successful registration of a task manager at the job manager. This is the
    * response triggered by the [[RegisterTaskManager]] message.
    *
-   * @param instanceID
-   * @param blobPort
-   * @param profilerListener
+   * @param instanceID The instance ID under which the TaskManager is registered at the
+   *                   JobManager.
+   * @param blobPort The server port where the JobManager's BLOB service runs.
    */
-  case class AcknowledgeRegistration(instanceID: InstanceID, blobPort: Int,
-                                     profilerListener: Option[ActorRef])
+  case class AcknowledgeRegistration(jobManager: ActorRef, instanceID: InstanceID, blobPort: Int)
+    extends RegistrationMessage
 
   /**
    * Denotes that the TaskManager has already been registered at the JobManager.
    *
-   * @param instanceID
-   * @param blobPort
-   * @param profilerListener
+   * @param instanceID The instance ID under which the TaskManager is registered.
+   * @param blobPort The server port where the JobManager's BLOB service runs.
    */
-  case class AlreadyRegistered(instanceID: InstanceID, blobPort: Int,
-                                profilerListener: Option[ActorRef])
+  case class AlreadyRegistered(jobManager: ActorRef, instanceID: InstanceID, blobPort: Int)
+    extends RegistrationMessage
 
   /**
    * Denotes the unsuccessful registration of a task manager at the job manager. This is the
@@ -63,5 +90,5 @@ object RegistrationMessages {
    * @param reason Reason why the task manager registration was refused
    */
   case class RefuseRegistration(reason: String)
-
+    extends RegistrationMessage
 }
diff --git a/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/TaskManagerMessages.scala b/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/TaskManagerMessages.scala
index d27885b90f2..c81830c36b1 100644
--- a/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/TaskManagerMessages.scala
+++ b/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/TaskManagerMessages.scala
@@ -18,116 +18,67 @@
 
 package org.apache.flink.runtime.messages
 
-import org.apache.flink.core.io.InputSplit
-import org.apache.flink.runtime.deployment.{InputChannelDeploymentDescriptor, TaskDeploymentDescriptor}
-import org.apache.flink.runtime.executiongraph.ExecutionAttemptID
 import org.apache.flink.runtime.instance.InstanceID
-import org.apache.flink.runtime.jobgraph.IntermediateDataSetID
 
+/**
+ * Miscellaneous actor messages exchanged with the TaskManager.
+ */
 object TaskManagerMessages {
 
   /**
-   * Cancels the task associated with [[attemptID]]. The result is sent back to the sender as a
-   * [[TaskOperationResult]] message.
-   *
-   * @param attemptID
-   */
-  case class CancelTask(attemptID: ExecutionAttemptID)
-
-  /**
-   * Submits a task to the task manager. The submission result is sent back to the sender as a
-   * [[TaskOperationResult]] message.
-   *
-   * @param tasks task deployment descriptor which contains the task relevant information
+   * Tells the task manager to send a heartbeat message to the job manager.
    */
-  case class SubmitTask(tasks: TaskDeploymentDescriptor)
+  case object SendHeartbeat {
 
-  /**
-   * Contains the next input split for a task. This message is a response to
-   * [[org.apache.flink.runtime.messages.JobManagerMessages.RequestNextInputSplit]].
-   *
-   * @param splitData
-   */
-  case class NextInputSplit(splitData: Array[Byte])
+    /**
+     * Accessor for the case object instance, to simplify Java interoperability.
+     * @return The SendHeartbeat case object instance.
+     */
+    def get() : SendHeartbeat.type = SendHeartbeat
+  }
 
   /**
-   * Unregisters the task identified by [[executionID]] from the task manager.
+   * Reports liveliness of the TaskManager instance with the given instance ID to the
+   * This message is sent to the job. This message reports the TaskManagers
+   * metrics, as a byte array.
    *
-   * @param executionID
+   * @param instanceID The instance ID of the reporting TaskManager.
+   * @param metricsReport utf-8 encoded JSON metrics report from the metricRegistry.
    */
-  case class UnregisterTask(executionID: ExecutionAttemptID)
+  case class Heartbeat(instanceID: InstanceID, metricsReport: Array[Byte])
 
-  /**
-   * Updates the reader of the task identified by
-   * [[executionID]] from the task manager.
-   */
-  sealed trait UpdateTask{
-    def executionID: ExecutionAttemptID
-  }
 
-  case class UpdateTaskSinglePartitionInfo(
-    executionID: ExecutionAttemptID,
-    resultId: IntermediateDataSetID,
-    partitionInfo: InputChannelDeploymentDescriptor)
-    extends UpdateTask
-
-  case class UpdateTaskMultiplePartitionInfos(
-    executionID: ExecutionAttemptID,
-    partitionInfos: Seq[(IntermediateDataSetID, InputChannelDeploymentDescriptor)])
-    extends UpdateTask
-
-  def createUpdateTaskMultiplePartitionInfos(
-    executionID: ExecutionAttemptID,
-    resultIDs: java.util.List[IntermediateDataSetID],
-    partitionInfos: java.util.List[InputChannelDeploymentDescriptor]):
-  UpdateTaskMultiplePartitionInfos = {
-    require(resultIDs.size() == partitionInfos.size(), "ResultIDs must have the same length as" +
-      "partitionInfos.")
-
-    import scala.collection.JavaConverters.asScalaBufferConverter
-    new UpdateTaskMultiplePartitionInfos(executionID,
-      resultIDs.asScala.zip(partitionInfos.asScala))
-  }
+  // --------------------------------------------------------------------------
+  //  Utility messages used for notifications during TaskManager startup
+  // --------------------------------------------------------------------------
 
   /**
-   * Fails all intermediate result partitions identified by [[executionID]] from the task manager.
-   *
-   * @param executionID
+   * Tells the TaskManager to send a stack trace of all threads to the sender.
+   * The response to this message is the [[StackTrace]] message.
    */
-  case class FailIntermediateResultPartitions(executionID: ExecutionAttemptID)
+  case object SendStackTrace {
 
-  /**
-   * Reports whether a task manager operation has been successful or not. This message will be
-   * sent to the sender as a response to [[SubmitTask]] and [[CancelTask]].
-   *
-   * @param executionID identifying the respective task
-   * @param success indicating whether the operation has been successful
-   * @param description
-   */
-  case class TaskOperationResult(executionID: ExecutionAttemptID, success: Boolean,
-                                 description: String = ""){
-    def this(executionID: ExecutionAttemptID, success: Boolean) = this(executionID, success, "")
+    /**
+     * Accessor for the case object instance, to simplify Java interoperability.
+     * @return The SendStackTrace case object instance.
+     */
+    def get() : SendStackTrace.type = SendStackTrace
   }
 
   /**
-   * Reports liveliness of an instance with [[instanceID]] to the
-   * [[org.apache.flink.runtime.instance.InstanceManager]]. This message is sent to the job
-   * manager which forwards it to the InstanceManager.
-   *
-   * @param instanceID
-   * @param metricsReport utf-8 encoded JSON report from the metricRegistry.
-   */
-  case class Heartbeat(instanceID: InstanceID, metricsReport: Array[Byte])
-
-  /**
-   * Sends StackTrace Message of an instance with [[instanceID]]. This message is a response to
-   * [[org.apache.flink.runtime.messages.TaskManagerMessages.SendStackTrace]].
+   * Communicates the stack trace of the TaskManager with the given ID.
+   * This message is the response to [[SendStackTrace]].
    *
-   * @param instanceID
-   * @param stackTrace
+   * @param instanceID The ID of the responding task manager.
+   * @param stackTrace The stack trace, as a string.
    */
   case class StackTrace(instanceID: InstanceID, stackTrace: String)
 
+
+  // --------------------------------------------------------------------------
+  //  Utility messages used for notifications during TaskManager startup
+  // --------------------------------------------------------------------------
+
   /**
    * Requests a notification from the task manager as soon as the task manager has been
    * registered at the job manager. Once the task manager is registered at the job manager a
@@ -141,55 +92,23 @@ object TaskManagerMessages {
    */
   case object RegisteredAtJobManager
 
-  /**
-   * Registers the sender as task manager at the job manager.
-   */
-  case object RegisterAtJobManager
-
-  /**
-   * Makes the task manager sending a heartbeat message to the job manager.
-   */
-  case object SendHeartbeat
 
-  /**
-   * Logs the current memory usage as debug level output.
-   */
-  case object LogMemoryUsage
+  // --------------------------------------------------------------------------
+  //  Utility getters for case objects to simplify access from Java
+  // --------------------------------------------------------------------------
 
   /**
-   * Makes the task manager sending a stack trace message to the sender.
+   * Accessor for the case object instance, to simplify Java interoperability.
+   * @return The NotifyWhenRegisteredAtJobManager case object instance.
    */
-  case object SendStackTrace
+  def getNotifyWhenRegisteredAtJobManagerMessage:
+            NotifyWhenRegisteredAtJobManager.type = NotifyWhenRegisteredAtJobManager
 
   /**
-   * Fail the specified task externally
-   *
-   * @param executionID identifying the task to fail
-   * @param cause reason for the external failure
+   * Accessor for the case object instance, to simplify Java interoperability.
+   * @return The RegisteredAtJobManager case object instance.
    */
-  case class FailTask(executionID: ExecutionAttemptID, cause: Throwable)
-  
-  // --------------------------------------------------------------------------
-  // Utility methods to allow simpler case object access from Java
-  // --------------------------------------------------------------------------
-  
-  def getNotifyWhenRegisteredAtJobManagerMessage : AnyRef = {
-    NotifyWhenRegisteredAtJobManager
-  }
-  
-  def getRegisteredAtJobManagerMessage : AnyRef = {
-    RegisteredAtJobManager
-  }
-  
-  def getRegisterAtJobManagerMessage : AnyRef = {
-    RegisterAtJobManager
-  }
+  def getRegisteredAtJobManagerMessage:
+            RegisteredAtJobManager.type = RegisteredAtJobManager
 
-  def getSendHeartbeatMessage : AnyRef = {
-    SendHeartbeat
-  }
-
-  def getLogMemoryUsageMessage : AnyRef = {
-    RegisteredAtJobManager
-  }
 }
diff --git a/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/TaskMessages.scala b/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/TaskMessages.scala
new file mode 100644
index 00000000000..c8c57265cd5
--- /dev/null
+++ b/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/TaskMessages.scala
@@ -0,0 +1,171 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.messages
+
+import org.apache.flink.runtime.deployment.{TaskDeploymentDescriptor, InputChannelDeploymentDescriptor}
+import org.apache.flink.runtime.executiongraph.ExecutionAttemptID
+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID
+import org.apache.flink.runtime.taskmanager.TaskExecutionState
+
+/**
+ * A set of messages that control the deployment and the state of Tasks executed
+ * on the TaskManager
+ */
+object TaskMessages {
+
+  /**
+   * Marker trait for task messages.
+   */
+  trait TaskMessage
+
+  // --------------------------------------------------------------------------
+  //  Starting and stopping Tasks
+  // --------------------------------------------------------------------------
+
+  /**
+   * Submits a task to the task manager. The result is to this message is a
+   * [[TaskOperationResult]] message.
+   *
+   * @param tasks Descriptor which contains the information to start the task.
+   */
+  case class SubmitTask(tasks: TaskDeploymentDescriptor)
+    extends TaskMessage
+
+  /**
+   * Cancels the task associated with [[attemptID]]. The result is sent back to the sender as a
+   * [[TaskOperationResult]] message.
+   *
+   * @param attemptID The task's execution attempt ID.
+   */
+  case class CancelTask(attemptID: ExecutionAttemptID)
+    extends TaskMessage
+
+  /**
+   * Triggers a fail of specified task from the outside (as opposed to the task throwing
+   * an exception itself) with the given exception as the cause.
+   *
+   * @param executionID The task's execution attempt ID.
+   * @param cause The reason for the external failure.
+   */
+  case class FailTask(executionID: ExecutionAttemptID, cause: Throwable)
+    extends TaskMessage
+
+  /**
+   * Unregister the task identified by [[executionID]] from the TaskManager.
+   * Sent to the TaskManager by futures and callbacks.
+   *
+   * @param executionID The task's execution attempt ID.
+   */
+  case class UnregisterTask(executionID: ExecutionAttemptID)
+    extends TaskMessage
+
+
+  // --------------------------------------------------------------------------
+  //  Updates to Intermediate Results
+  // --------------------------------------------------------------------------
+
+  /**
+   * Base class for messages that update the information about location of input partitions
+   */
+  abstract sealed class UpdatePartitionInfo extends TaskMessage {
+    def executionID: ExecutionAttemptID
+  }
+
+  /**
+   *
+   * @param executionID The task's execution attempt ID.
+   * @param resultId The input reader to update.
+   * @param partitionInfo The partition info update.
+   */
+  case class UpdateTaskSinglePartitionInfo(executionID: ExecutionAttemptID,
+                                           resultId: IntermediateDataSetID,
+                                           partitionInfo: InputChannelDeploymentDescriptor)
+    extends UpdatePartitionInfo
+
+  /**
+   *
+   * @param executionID The task's execution attempt ID.
+   * @param partitionInfos List of input gates with channel descriptors to update.
+   */
+  case class UpdateTaskMultiplePartitionInfos(
+                    executionID: ExecutionAttemptID,
+                    partitionInfos: Seq[(IntermediateDataSetID, InputChannelDeploymentDescriptor)])
+    extends UpdatePartitionInfo
+
+  /**
+   * Fails (and releases) all intermediate result partitions identified by
+   * [[executionID]] from the task manager.
+   *
+   * @param executionID The task's execution attempt ID.
+   */
+  case class FailIntermediateResultPartitions(executionID: ExecutionAttemptID)
+    extends TaskMessage
+
+
+  // --------------------------------------------------------------------------
+  //  Report Messages
+  // --------------------------------------------------------------------------
+
+  /**
+   * Denotes a state change of a task at the JobManager. The update success is acknowledged by a
+   * boolean value which is sent back to the sender.
+   *
+   * @param taskExecutionState The changed task state
+   */
+  case class UpdateTaskExecutionState(taskExecutionState: TaskExecutionState)
+    extends TaskMessage
+
+  /**
+   * Response message to updates in the task state. Send for example as a response to
+   *
+   *  - [[SubmitTask]]
+   *  - [[CancelTask]]
+   *
+   * @param executionID identifying the respective task
+   * @param success indicating whether the operation has been successful
+   * @param description Optional description for unsuccessful results.
+   */
+  case class TaskOperationResult(executionID: ExecutionAttemptID,
+                                 success: Boolean,
+                                 description: String)
+    extends TaskMessage
+  {
+    def this(executionID: ExecutionAttemptID, success: Boolean) = this(executionID, success, "")
+  }
+
+
+  // --------------------------------------------------------------------------
+  //  Utility Functions
+  // --------------------------------------------------------------------------
+
+  def createUpdateTaskMultiplePartitionInfos(
+                               executionID: ExecutionAttemptID,
+                               resultIDs: java.util.List[IntermediateDataSetID],
+                               partitionInfos: java.util.List[InputChannelDeploymentDescriptor]):
+  UpdateTaskMultiplePartitionInfos = {
+
+    require(resultIDs.size() == partitionInfos.size(),
+      "ResultIDs must have the same length as partitionInfos.")
+
+    import scala.collection.JavaConverters.asScalaBufferConverter
+
+    new UpdateTaskMultiplePartitionInfos(executionID,
+      resultIDs.asScala.zip(partitionInfos.asScala))
+  }
+}
diff --git a/flink-runtime/src/main/scala/org/apache/flink/runtime/minicluster/LocalFlinkMiniCluster.scala b/flink-runtime/src/main/scala/org/apache/flink/runtime/minicluster/LocalFlinkMiniCluster.scala
index cb79fbc7b90..13e1ccd088d 100644
--- a/flink-runtime/src/main/scala/org/apache/flink/runtime/minicluster/LocalFlinkMiniCluster.scala
+++ b/flink-runtime/src/main/scala/org/apache/flink/runtime/minicluster/LocalFlinkMiniCluster.scala
@@ -100,8 +100,18 @@ class LocalFlinkMiniCluster(userConfiguration: Configuration, singleActorSystem:
       TaskManager.TASK_MANAGER_NAME
     }
 
-    TaskManager.startTaskManagerActor(config, system, HOSTNAME, taskManagerActorName,
-      singleActorSystem, localExecution, classOf[TaskManager])
+    val jobManagerPath: Option[String] = if (singleActorSystem) {
+      Some(jobManagerActor.path.toString)
+    } else {
+      None
+    }
+
+    TaskManager.startTaskManagerComponentsAndActor(config, system,
+                                                   HOSTNAME, // network interface to bind to
+                                                   Some(taskManagerActorName), // actor name
+                                                   jobManagerPath, // job manager akka URL
+                                                   localExecution, // start network stack?
+                                                   classOf[TaskManager])
   }
 
   def getJobClient(): ActorRef = {
diff --git a/flink-runtime/src/main/scala/org/apache/flink/runtime/taskmanager/TaskManager.scala b/flink-runtime/src/main/scala/org/apache/flink/runtime/taskmanager/TaskManager.scala
index f558d482a79..af13b74ff74 100644
--- a/flink-runtime/src/main/scala/org/apache/flink/runtime/taskmanager/TaskManager.scala
+++ b/flink-runtime/src/main/scala/org/apache/flink/runtime/taskmanager/TaskManager.scala
@@ -15,26 +15,31 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.flink.runtime.taskmanager
 
 import java.io.{File, IOException}
 import java.net.{InetAddress, InetSocketAddress}
 import java.util
 import java.util.concurrent.{TimeUnit, FutureTask}
-import management.{GarbageCollectorMXBean, ManagementFactory, MemoryMXBean}
+import java.lang.management.{GarbageCollectorMXBean, ManagementFactory, MemoryMXBean}
 
 import akka.actor._
 import akka.pattern.ask
+import akka.util.Timeout
+
 import com.codahale.metrics.{Gauge, MetricFilter, MetricRegistry}
 import com.codahale.metrics.json.MetricsModule
 import com.codahale.metrics.jvm.{MemoryUsageGaugeSet, GarbageCollectorMetricSet}
+
 import com.fasterxml.jackson.databind.ObjectMapper
+
 import org.apache.flink.api.common.cache.DistributedCache
-import org.apache.flink.configuration.{ConfigConstants, Configuration, GlobalConfiguration}
+import org.apache.flink.configuration._
 import org.apache.flink.core.fs.Path
 import org.apache.flink.runtime.ActorLogMessages
 import org.apache.flink.runtime.akka.AkkaUtils
-import org.apache.flink.runtime.blob.BlobCache
+import org.apache.flink.runtime.blob.{BlobService, BlobCache}
 import org.apache.flink.runtime.broadcast.BroadcastVariableManager
 import org.apache.flink.runtime.deployment.{InputChannelDeploymentDescriptor, TaskDeploymentDescriptor}
 import org.apache.flink.runtime.execution.librarycache.{BlobLibraryCacheManager, FallbackLibraryCacheManager, LibraryCacheManager}
@@ -43,25 +48,25 @@ import org.apache.flink.runtime.executiongraph.ExecutionAttemptID
 import org.apache.flink.runtime.filecache.FileCache
 import org.apache.flink.runtime.instance.{HardwareDescription, InstanceConnectionInfo, InstanceID}
 import org.apache.flink.runtime.io.disk.iomanager.IOManager.IOMode
-import org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync
+import org.apache.flink.runtime.io.disk.iomanager.{IOManager, IOManagerAsync}
 import org.apache.flink.runtime.io.network.NetworkEnvironment
 import org.apache.flink.runtime.io.network.netty.NettyConfig
 import org.apache.flink.runtime.jobgraph.IntermediateDataSetID
 import org.apache.flink.runtime.jobgraph.tasks.{OperatorStateCarrier,BarrierTransceiver}
-import org.apache.flink.runtime.jobmanager.{BarrierReq,JobManager}
-import org.apache.flink.runtime.memorymanager.DefaultMemoryManager
-import org.apache.flink.runtime.messages.JobManagerMessages.UpdateTaskExecutionState
-import org.apache.flink.runtime.messages.Messages.{Disconnect, Acknowledge}
-import org.apache.flink.runtime.messages.RegistrationMessages.{AlreadyRegistered, RefuseRegistration, AcknowledgeRegistration, RegisterTaskManager}
+import org.apache.flink.runtime.jobmanager.JobManager
+import org.apache.flink.runtime.memorymanager.{MemoryManager, DefaultMemoryManager}
+import org.apache.flink.runtime.messages.CheckpointingMessages.{CheckpointingMessage, BarrierReq}
+import org.apache.flink.runtime.messages.Messages._
+import org.apache.flink.runtime.messages.RegistrationMessages._
 import org.apache.flink.runtime.messages.TaskManagerMessages._
-import org.apache.flink.runtime.messages.TaskManagerProfilerMessages.{UnregisterProfilingListener, UnmonitorTask, MonitorTask, RegisterProfilingListener}
+import org.apache.flink.runtime.messages.TaskMessages._
 import org.apache.flink.runtime.net.NetUtils
 import org.apache.flink.runtime.process.ProcessReaper
-import org.apache.flink.runtime.profiling.ProfilingUtils
 import org.apache.flink.runtime.security.SecurityUtils
 import org.apache.flink.runtime.security.SecurityUtils.FlinkSecuredRunner
 import org.apache.flink.runtime.util.{MathUtils, EnvironmentInformation}
 import org.apache.flink.util.ExceptionUtils
+
 import org.slf4j.LoggerFactory
 
 import scala.concurrent._
@@ -75,331 +80,696 @@ import scala.language.postfixOps
  * The TaskManager is responsible for executing the individual tasks of a Flink job. It is
  * implemented as an actor. The TaskManager has the following phases:
  *
- * - Waiting to be registered with its JobManager. In that phase, it periodically sends
- * [[RegisterAtJobManager]] messages to itself, which trigger the sending of
- * a [[RegisterTaskManager]] message to the JobManager.
+ * - "Waiting to be registered": In that phase, it periodically
+ *   sends a [[RegisterTaskManager]] message to the JobManager.
+ *   Upon successful registration, the JobManager replies with an [[AcknowledgeRegistration]]
+ *   message. This stops the registration messages and initializes all fields
+ *   that require the JobManager's actor reference.
+ *
+ * - "Operational": Here the TaskManager accepts and processes task messages, like
+ *   [[SubmitTask]], [[CancelTask]], [[FailTask]].
+ *   If the TaskManager disconnects from the JobManager (because the JobManager is no longer
+ *   reachable), the TaskManager gets back to the "waiting to be registered" state.
  *
- * - Upon successful registration, the JobManager replies with an [[AcknowledgeRegistration]]
- * message. This stops the registration messages and initializes all fields
- * that require the JobManager's actor reference
  *
- *  - [[SubmitTask]] is sent from the JobManager and contains the next Task to be executed on this
- *  TaskManager
+ *  ========== Failure model of the TaskManager ==========
  *
- *  - [[CancelTask]] requests to cancel the corresponding task
+ *  The TaskManager tries to compensate for task failures as far as possible by marking
+ *  the task as failed and removing all its resources. This causes the JobManager to
+ *  restart the task (on this same TaskManager or on a different TaskManager).
  *
- *  - [[FailTask]] requests to fail the corresponding task
+ *  In certain cases, exceptions indicate that the TaskManager is unable to proceed.
+ *  The most robust way to clean up is letting the OS/kernel do it, so we will trigger
+ *  killing the process. In case of YARN (or resilient standalone mode), the process
+ *  will be restarted, producing a clean state.
+ *  To achieve this, we kill the TaskManager actor. The watch dog actor (process reaper)
+ *  will recognize that and kill the TaskManager process.
  *
- * - ...
+ *  Fatal errors that require TaskManager JVM restart include:
+ *
+ *    - Errors bringing up the Network Stack or Library Cache after the TaskManager
+ *      has registered at the JobManager. The TaskManager cannot operate without.
+ *
+ *    - Exceptions while releasing the task resources from the network stack, intermediate
+ *      results, or memory manager. Those situations indicate a critical leak in the
+ *      resource management, which can only be reliably fixed through a JVM restart.
+ *
+ *    - Exceptions releasing intermediate result resources. Critical resource leak,
+ *      requires a clean JVM.
  */
-class TaskManager(val connectionInfo: InstanceConnectionInfo,
-                  val jobManagerAkkaURL: String,
-                  val taskManagerConfig: TaskManagerConfiguration,
-                  val networkConfig: NetworkEnvironmentConfiguration)
-  extends Actor with ActorLogMessages with ActorLogging {
-
-  import context._
-  import taskManagerConfig.{timeout => tmTimeout, _}
-
-
-
-  implicit val timeout = tmTimeout
-
-  log.info("Starting task manager at {}.", self.path)
-  log.info("Creating {} task slot(s).", numberOfSlots)
-  log.info("TaskManager connection information: {}", connectionInfo)
-
-  val HEARTBEAT_INTERVAL = 5000 millisecond
-
-  var registrationDelay = 50 milliseconds
-  var registrationDuration = 0 seconds
-  var registrationAttempts: Int = 0
-
-  val ioManager = new IOManagerAsync(tmpDirPaths)
-  val memoryManager = new DefaultMemoryManager(memorySize, numberOfSlots, pageSize)
-  val bcVarManager = new BroadcastVariableManager()
-  val hardwareDescription = HardwareDescription.extractFromSystem(memoryManager.getMemorySize)
-  val fileCache = new FileCache()
-  val runningTasks = scala.collection.mutable.HashMap[ExecutionAttemptID, Task]()
-  val metricRegistry = new MetricRegistry
-  // register metrics
-  metricRegistry.register("gc", new GarbageCollectorMetricSet)
-  metricRegistry.register("memory", new MemoryUsageGaugeSet)
-  metricRegistry.register("load", new Gauge[Double] {
-    override def getValue: Double =
-      ManagementFactory.getOperatingSystemMXBean().getSystemLoadAverage()
-  })
-  // register metric serialization
-  val metricRegistryMapper: ObjectMapper =
-    new ObjectMapper().registerModule(new MetricsModule(TimeUnit.SECONDS, TimeUnit.MILLISECONDS,
-      false, MetricFilter.ALL))
-
-  // Actors which want to be notified once this task manager has been registered at the job manager
-  val waitForRegistration = scala.collection.mutable.Set[ActorRef]()
-
-  val profiler = profilingInterval match {
-    case Some(interval) =>
-      log.info("Profiling of jobs is enabled.")
-      Some(TaskManager.startProfiler(self.path.toSerializationFormat, interval, context.system))
-    case None =>
-      log.info("Profiling of jobs is disabled.")
-      None
-  }
+class TaskManager(protected val config: TaskManagerConfiguration,
+                  protected val connectionInfo: InstanceConnectionInfo,
+                  protected val jobManagerAkkaURL: String,
+                  protected val memoryManager: MemoryManager,
+                  protected val ioManager: IOManager,
+                  protected val network: NetworkEnvironment,
+                  protected val numberOfSlots: Int)
 
-  if (log.isInfoEnabled) {
-    log.info(TaskManager.getMemoryUsageStatsAsString(ManagementFactory.getMemoryMXBean))
-  }
+extends Actor with ActorLogMessages with ActorLogging {
+
+  /** The log for all synchronous logging calls */
+  private val LOG = TaskManager.LOG
+
+  /** The timeout for all actor ask futures */
+  protected val askTimeout = new Timeout(config.timeout)
+
+  /** The TaskManager's physical execution resources */
+  protected val resources = HardwareDescription.extractFromSystem(memoryManager.getMemorySize)
+
+  /** Registry of all tasks currently executed by this TaskManager */
+  protected val runningTasks = scala.collection.mutable.HashMap[ExecutionAttemptID, Task]()
+
+  /** Handler for shared broadcast variables (shared between multiple Tasks) */
+  protected val bcVarManager = new BroadcastVariableManager()
+
+  /** Handler for distributed files cached by this TaskManager */
+  protected val fileCache = new FileCache()
+
+  /** Registry of metrics periodically transmitted to the JobManager */
+  private val metricRegistry = TaskManager.createMetricsRegistry()
+
+  /** Metric serialization */
+  private val metricRegistryMapper: ObjectMapper = new ObjectMapper()
+        .registerModule(new MetricsModule(TimeUnit.SECONDS,
+                                          TimeUnit.MILLISECONDS,
+                                          false,
+                                          MetricFilter.ALL))
+
+  /** Actors which want to be notified once this task manager has been
+      registered at the job manager */
+  private val waitForRegistration = scala.collection.mutable.Set[ActorRef]()
+
+  private var blobService: Option[BlobService] = None
+  private var libraryCacheManager: Option[LibraryCacheManager] = None
+  private var currentJobManager: Option[ActorRef] = None
+
+  private var instanceID: InstanceID = null
 
-  var libraryCacheManager: Option[LibraryCacheManager] = None
-  var networkEnvironment: Option[NetworkEnvironment] = None
-  var currentJobManager: Option[ActorRef] = None
-  var profilerListener: Option[ActorRef] = None
-  var instanceID: InstanceID = null
-  var heartbeatScheduler: Option[Cancellable] = None
+  private var heartbeatScheduler: Option[Cancellable] = None
 
 
+  // --------------------------------------------------------------------------
+  //  Actor messages and life cycle
+  // --------------------------------------------------------------------------
+
+  /**
+   * Called prior to the actor receiving any messages.
+   * Logs some context info and triggers the initial attempt to register at the
+   * JobManager.
+   */
   override def preStart(): Unit = {
-    tryJobManagerRegistration()
+    LOG.info("Starting TaskManager actor at {}.", self.path.toSerializationFormat)
+    LOG.info("TaskManager data connection information: {}", connectionInfo)
+    LOG.info("TaskManager has {} task slot(s).", numberOfSlots)
+
+    // log the initial memory utilization
+    if (LOG.isInfoEnabled) {
+      LOG.info(TaskManager.getMemoryUsageStatsAsString(ManagementFactory.getMemoryMXBean))
+    }
+
+    // kick off the registration
+    val deadline: Option[Deadline] = config.maxRegistrationDuration.map(_.fromNow)
+
+    self.tell(TriggerTaskManagerRegistration(jobManagerAkkaURL,
+                   TaskManager.INITIAL_REGISTRATION_TIMEOUT, deadline,1), ActorRef.noSender)
   }
 
+  /**
+   * Called after the actor is stopped.
+   * Makes sure all currently running tasks are cancelled and all components
+   * (like network stack, library cache, memory manager, ...) are properly shut down.
+   */
   override def postStop(): Unit = {
-    log.info("Stopping task manager {}.", self.path)
+    LOG.info("Stopping TaskManager {}.", self.path.toSerializationFormat)
 
-    currentJobManager foreach {
-      _ ! Disconnect(s"TaskManager ${self.path} is shutting down.")
+    cancelAndClearEverything(new Exception("TaskManager is shutting down."))
+
+    if (isConnected) {
+      try {
+        disassociateFromJobManager()
+      } catch {
+        case t: Exception => LOG.error("Could not cleanly disassociate from JobManager", t)
+      }
     }
 
-    cancelAndClearEverything(new Exception("Task Manager is shutting down."))
+    try {
+      ioManager.shutdown()
+    } catch {
+      case t: Exception => LOG.error("I/O manager did not shutdown properly.", t)
+    }
 
-    cleanupTaskManager()
+    try {
+      memoryManager.shutdown()
+    } catch {
+      case t: Exception => LOG.error("Memory manager did not shutdown properly.", t)
+    }
 
-    ioManager.shutdown()
-    memoryManager.shutdown()
+    try {
+      network.shutdown()
+    } catch {
+      case t: Exception => LOG.error("Network environment did not shutdown properly.", t)
+    }
 
     try {
       fileCache.shutdown()
     } catch {
-      case t: Throwable => log.error(t, "FileCache did not shutdown properly.")
+      case t: Exception => LOG.error("FileCache did not shutdown properly.", t)
     }
 
-    if(log.isDebugEnabled){
-      log.debug("Task manager {} is completely stopped.", self.path)
-    }
-  }
-  
-  private def tryJobManagerRegistration(): Unit = {
-    context.system.scheduler.scheduleOnce(registrationDelay, self, RegisterAtJobManager)
+    LOG.info("Task manager {} is completely shut down.", self.path)
   }
 
+  /**
+   * Central handling of actor messages. This method delegates to the more specialized
+   * methods for handling certain classes of messages.
+   */
   override def receiveWithLogMessages: Receive = {
-    case RegisterAtJobManager =>
-      if(currentJobManager.isEmpty) {
-        registrationDuration += registrationDelay
-        // double delay for exponential backoff
-        registrationDelay *= 2
-        registrationAttempts += 1
-
-        if (registrationDuration > maxRegistrationDuration) {
-          log.error("TaskManager could not register at JobManager {} after {}.",
-            jobManagerAkkaURL,
-            maxRegistrationDuration)
 
-          self ! PoisonPill
-        } else {
-          log.info("Try to register at master {}. {}. Attempt", jobManagerAkkaURL,
-            registrationAttempts)
-          val jobManager = context.actorSelection(jobManagerAkkaURL)
+    // task messages are most common and critical, we handle them first
+    case message: TaskMessage => handleTaskMessage(message)
 
-          jobManager ! RegisterTaskManager(connectionInfo, hardwareDescription, numberOfSlots)
+    // messages for coordinating checkpoints
+    case message: CheckpointingMessage => handleCheckpointingMessage(message)
 
-          context.system.scheduler.scheduleOnce(registrationDelay, self, RegisterAtJobManager)
-        }
-      }
+    // registration messages for connecting and disconnecting from / to the JobManager
+    case message: RegistrationMessage => handleRegistrationMessage(message)
 
-    case AcknowledgeRegistration(id, blobPort, profilerListener) =>
-      if(currentJobManager.isEmpty) {
-        finishRegistration(sender, id, blobPort, profilerListener)
-      } else {
-        log.info("The TaskManager {} is already registered at the JobManager {}, but received " +
-          "another AcknowledgeRegistration message.", self.path, sender.path)
-      }
+    // ----- miscellaneous messages ----
+
+    // periodic heart beats that transport metrics
+    case SendHeartbeat => sendHeartbeatToJobManager()
 
-    case AlreadyRegistered(id, blobPort, profilerListener) =>
-      if(currentJobManager.isEmpty) {
-        log.warning("The TaskManager {} seems to be already registered at the JobManager {} even" +
-          "though it has not yet finished the registration process.", self.path, sender.path)
+    // sends the stack trace of this TaskManager to the sender
+    case SendStackTrace => sendStackTrace(sender())
 
-        finishRegistration(sender, id, blobPort, profilerListener)
+    // registers the message sender to be notified once this TaskManager has completed
+    // its registration at the JobManager
+    case NotifyWhenRegisteredAtJobManager =>
+      if (isConnected) {
+        sender ! RegisteredAtJobManager
       } else {
-        // ignore AlreadyRegistered messages which arrived after AcknowledgeRegistration
-        log.info("The TaskManager {} has already been registered at the JobManager {}.",
-          self.path, sender.path)
+        waitForRegistration += sender
       }
 
-    case RefuseRegistration(reason) =>
-      if(currentJobManager.isEmpty) {
-        log.error("The registration of task manager {} was refused by the job manager {} " +
-          "because {}.", self.path, jobManagerAkkaURL, reason)
-
-        // Shut task manager down
-        self ! PoisonPill
-      } else {
-        // ignore RefuseRegistration messages which arrived after AcknowledgeRegistration
-        log.info("Received RefuseRegistration from the JobManager even though being already " +
-          "registered")
+    // this message indicates that some actor watched by this TaskManager has died
+    case Terminated(actor: ActorRef) =>
+      if (isConnected && actor == currentJobManager.orNull) {
+        handleJobManagerDisconnect(sender(), "JobManager is no longer reachable")
+      }
+      else {
+        LOG.warn("Received unrecognized disconnect message from {}",
+          if (actor == null) null else actor.path)
       }
 
-    case SubmitTask(tdd) =>
-      submitTask(tdd)
+    case Disconnect(msg) =>
+      handleJobManagerDisconnect(sender(), "JobManager requested disconnect: " + msg)
+  }
 
-    case updateMsg:UpdateTask =>
-      updateMsg match {
-        case UpdateTaskSinglePartitionInfo(executionID, resultID, partitionInfo) =>
-          updateTask(executionID, List((resultID, partitionInfo)))
-        case UpdateTaskMultiplePartitionInfos(executionID, partitionInfos) =>
-          updateTask(executionID, partitionInfos)
-      }
+  /**
+   * Handle unmatched messages with an exception.
+   */
+  override def unhandled(message: Any): Unit = {
+    val errorMessage = "Received unknown message " + message
+    val error = new RuntimeException(errorMessage)
+    LOG.error(errorMessage)
 
-    case CancelTask(executionID) =>
-      runningTasks.get(executionID) match {
-        case Some(task) =>
-          // execute cancel operation concurrently
-          Future {
-            task.cancelExecution()
-          }.onFailure{
-            case t: Throwable => log.error(t, "Could not cancel task {}.", task)
-          }
+    // terminate all we are currently running (with a dedicated message)
+    // before the actor is stopped
+    cancelAndClearEverything(error)
 
-          sender ! new TaskOperationResult(executionID, true)
-        case None =>
-          sender ! new TaskOperationResult(executionID, false,
-            "No task with that execution ID was found.")
-      }
+    // let the actor crash
+    throw error
+  }
 
-    case UnregisterTask(executionID) =>
-      unregisterTaskAndNotifyFinalState(executionID)
+  /**
+   * Handler for messages concerning the deployment and status updates of
+   * tasks.
+   *
+   * @param message The task message.
+   */
+  private def handleTaskMessage(message: TaskMessage): Unit = {
 
-    case updateMsg:UpdateTaskExecutionState =>
-      currentJobManager foreach {
-        jobManager => {
-          val futureResponse = (jobManager ? updateMsg)(timeout)
-
-          val jobID = updateMsg.taskExecutionState.getJobID
-          val executionID = updateMsg.taskExecutionState.getID
-          val executionState = updateMsg.taskExecutionState.getExecutionState
-
-          futureResponse.mapTo[Boolean].onComplete {
-            case Success(result) =>
-              if (!result) {
-                self ! FailTask(executionID,
-                  new IllegalStateException("Task has been disposed on JobManager."))
-              }
+    // at very first, check that we are actually currently associated with a JobManager
+    if (!isConnected) {
+      LOG.debug("Dropping message {} because the TaskManager is currently " +
+        "not connected to a JobManager", message)
+    }
+
+    // we order the messages by frequency, to make sure the code paths for matching
+    // are as short as possible
+    message match {
+
+      // tell the task about the availability of a new input partition
+      case UpdateTaskSinglePartitionInfo(executionID, resultID, partitionInfo) =>
+        updateTaskInputPartitions(executionID, List((resultID, partitionInfo)))
+
+      // tell the task about the availability of some new input partitions
+      case UpdateTaskMultiplePartitionInfos(executionID, partitionInfos) =>
+        updateTaskInputPartitions(executionID, partitionInfos)
+
+      // discards intermediate result partitions of a task execution on this TaskManager
+      case FailIntermediateResultPartitions(executionID) =>
+        LOG.info("Discarding the results produced by task execution " + executionID)
+        if (network.isAssociated) {
+          try {
+            network.getPartitionManager.releasePartitionsProducedBy(executionID)
+          } catch {
+            case t: Throwable => killTaskManagerFatal(
+                "Fatal leak: Unable to release intermediate result partition data", t)
+          }
+        }
+
+      // notifies the TaskManager that the state of a task has changed.
+      // the TaskManager informs the JobManager and cleans up in case the transition
+      // was into a terminal state, or in case the JobManager cannot be informed of the
+      // state transition
+
+      case updateMsg @ UpdateTaskExecutionState(taskExecutionState: TaskExecutionState) =>
+        currentJobManager foreach {
+          jobManager => {
+            val futureResponse = (jobManager ? updateMsg)(askTimeout)
+
+            val executionID = taskExecutionState.getID
+            val executionState = taskExecutionState.getExecutionState
+
+            futureResponse.mapTo[Boolean].onComplete {
+              // IMPORTANT: In the future callback, we cannot directly modify state
+              //            but only send messages to the TaskManager to do those changes
+              case Success(result) =>
+                if (!result) {
+                  self ! FailTask(executionID,
+                    new Exception("Task has been cancelled on the JobManager."))
+                }
+
+                if (!result || executionState.isTerminal) {
+                  self ! UnregisterTask(executionID)
+                }
+              case Failure(t) =>
+                self ! FailTask(executionID, new Exception(
+                  "Failed to send ExecutionStateChange notification to JobManager"))
 
-              if (!result || executionState == ExecutionState.FINISHED || executionState ==
-                ExecutionState.CANCELED || executionState == ExecutionState.FAILED) {
                 self ! UnregisterTask(executionID)
-              }
-            case Failure(t) =>
-              log.error(t, "Execution state change notification failed for task {}" +
-                s"of job {}.", executionID, jobID)
-              self ! UnregisterTask(executionID)
+            }(context.dispatcher)
           }
         }
-      }
 
-    case SendHeartbeat =>
-      var report: Array[Byte] = null
-      try {
-        report = metricRegistryMapper.writeValueAsBytes(metricRegistry)
-      } catch {
-        case all: Throwable => log.warning("Error turning the report into JSON", all)
-      }
+      // removes the task from the TaskManager and frees all its resources
+      case UnregisterTask(executionID) =>
+        unregisterTaskAndNotifyFinalState(executionID)
+
+      // starts a new task on the TaskManager
+      case SubmitTask(tdd) =>
+        submitTask(tdd)
+
+      // marks a task as failed for an external reason
+      // external reasons are reasons other than the task code itself throwing an exception
+      case FailTask(executionID, cause) =>
+        runningTasks.get(executionID) match {
+          case Some(task) =>
+
+            // execute failing operation concurrently
+            implicit val executor = context.dispatcher
+            Future {
+              task.failExternally(cause)
+            }.onFailure{
+              case t: Throwable => LOG.error(s"Could not fail task ${task} externally.", t)
+            }
+          case None =>
+        }
 
-      currentJobManager foreach {
-        _ ! Heartbeat(instanceID, report)
-      }
+      // cancels a task
+      case CancelTask(executionID) =>
+        runningTasks.get(executionID) match {
+          case Some(task) =>
+            // execute cancel operation concurrently
+            implicit val executor = context.dispatcher
+            Future {
+              task.cancelExecution()
+            }.onFailure{
+              case t: Throwable => LOG.error("Could not cancel task " + task, t)
+            }
 
-    case SendStackTrace =>
-      val traces = Thread.getAllStackTraces.asScala
-      val stackTraceStr = traces.map((trace: (Thread, Array[StackTraceElement])) => {
-        val (thread, elements) = trace
-        "Thread: " + thread.getName + '\n' + elements.mkString("\n")
-      }).mkString("\n\n")
+            sender ! new TaskOperationResult(executionID, true)
 
-      sender ! StackTrace(instanceID, stackTraceStr)
+          case None =>
+            sender ! new TaskOperationResult(executionID, false,
+              "No task with that execution ID was found.")
+        }
+    }
+  }
 
-    case NotifyWhenRegisteredAtJobManager =>
-      if (currentJobManager.isDefined) {
-        sender ! RegisteredAtJobManager
-      } else {
-        waitForRegistration += sender
-      }
+  /**
+   * Handler for messages related to checkpoints.
+   *
+   * @param message The checkpoint message.
+   */
+  private def handleCheckpointingMessage(message: CheckpointingMessage): Unit = {
 
-    case FailTask(executionID, cause) =>
-      runningTasks.get(executionID) match {
-        case Some(task) =>
-          // execute failing operation concurrently
-          Future {
-            task.failExternally(cause)
-          }.onFailure{
-            case t: Throwable => log.error(t, "Could not fail task {} externally.", task)
-          }
-        case None =>
-      }
+    message match {
 
-    case Terminated(jobManager) =>
-      log.info("Job manager {} is no longer reachable. Cancelling all tasks and trying to " +
-        "reregister.", jobManagerAkkaURL)
+      case BarrierReq(attemptID, checkpointID) =>
+        LOG.debug("[FT-TaskManager] Barrier {} request received for attempt {}",
+          checkpointID, attemptID)
 
-      cancelAndClearEverything(new Throwable("Lost connection to JobManager"))
+        runningTasks.get(attemptID) match {
+          case Some(i) =>
+            if (i.getExecutionState == ExecutionState.RUNNING) {
+              i.getEnvironment.getInvokable match {
+                case barrierTransceiver: BarrierTransceiver =>
+                  new Thread(new Runnable {
+                    override def run(): Unit =
+                      barrierTransceiver.broadcastBarrierFromSource(checkpointID)
+                  }).start()
+
+                case _ => LOG.error(
+                  "Taskmanager received a checkpoint request for non-checkpointing task {}",
+                  attemptID)
+              }
+            }
 
-      cleanupTaskManager()
+          case None =>
+            // may always happen in case of canceled/finished tasks
+            LOG.debug("Taskmanager received a checkpoint request for unknown task {}",
+                      attemptID)
+        }
 
-      tryJobManagerRegistration()
+      // unknown checkpoint message
+      case _ => unhandled(message)
+    }
+  }
 
-    case Disconnect(msg) =>
-      log.info("Job manager {} wants {} to disconnect. Reason {}.", jobManagerAkkaURL,
-        self.path, msg)
+  /**
+   * Handler for messages concerning the registration of the TaskManager at
+   * the JobManager.
+   *
+   * Errors must not propagate out of the handler, but need to be handled internally.
+   *
+   * @param message The registration message.
+   */
+  private def handleRegistrationMessage(message: RegistrationMessage): Unit = {
 
-      cancelAndClearEverything(new Throwable("Job manager wants me to disconnect."))
+    message match {
 
-      cleanupTaskManager()
+      case TriggerTaskManagerRegistration(jobManagerURL, timeout, deadline, attempt) =>
+        if (isConnected) {
+          // this may be the case, if we queue another attempt and
+          // in the meantime, the registration is acknowledged
+          LOG.debug(
+            "TaskManager was triggered to register at JobManager, but is already registered")
+        }
+        else if (deadline.exists(_.isOverdue())) {
+          // we failed to register in time. that means we should quit
+          LOG.error("Failed to register at the JobManager withing the defined maximum " +
+            "connect time. Shutting down ...")
 
-      tryJobManagerRegistration()
+          // terminate ourselves (hasta la vista)
+          self ! PoisonPill
+        }
+        else {
+          LOG.info(s"Trying to register at JobManager ${jobManagerURL} " +
+            s"(attempt ${attempt}, timeout: ${timeout})")
 
-    case FailIntermediateResultPartitions(executionID) =>
-      log.info("Fail intermediate result partitions associated with execution {}.", executionID)
-      networkEnvironment foreach {
-        _.getPartitionManager.releasePartitionsProducedBy(executionID)
-      }
+          val jobManager = context.actorSelection(jobManagerAkkaURL)
+          jobManager ! RegisterTaskManager(self, connectionInfo, resources, numberOfSlots)
 
-    case BarrierReq(attemptID, checkpointID) =>
-      log.debug("[FT-TaskManager] Barrier {} request received for attempt {}", 
-          checkpointID, attemptID)
-      runningTasks.get(attemptID) match {
-        case Some(i) =>
-          if (i.getExecutionState == ExecutionState.RUNNING) {
-            i.getEnvironment.getInvokable match {
-              case barrierTransceiver: BarrierTransceiver =>
-                new Thread(new Runnable {
-                  override def run(): Unit =  
-                    barrierTransceiver.broadcastBarrierFromSource(checkpointID);
-                }).start()
-              case _ => log.error("[FT-TaskManager] Received a barrier for the wrong vertex")
+          // the next timeout computes via exponential backoff with cap
+          val nextTimeout = (timeout * 2).min(TaskManager.MAX_REGISTRATION_TIMEOUT)
+
+          // schedule (with our timeout s delay) a check triggers a new registration
+          // attempt, if we are not registered by then
+          context.system.scheduler.scheduleOnce(timeout) {
+            if (!isConnected) {
+              self.tell(TriggerTaskManagerRegistration(jobManagerURL,
+                                 nextTimeout, deadline, attempt + 1), ActorRef.noSender)
             }
+          }(context.dispatcher)
+        }
+
+      // successful registration. associate with the JobManager
+      // we disambiguate duplicate or erroneous messages, to simplify debugging
+      case AcknowledgeRegistration(jobManager, id, blobPort) =>
+        if (isConnected) {
+          if (jobManager == currentJobManager.orNull) {
+            LOG.debug("Ignoring duplicate registration acknowledgement.")
+          } else {
+            LOG.warn(s"Ignoring 'AcknowledgeRegistration' message from ${jobManager.path} , " +
+              s"because the TaskManager is already registered at ${currentJobManager.orNull}")
+          }
+        }
+        else {
+          // not yet connected, so let's associate with that JobManager
+          try {
+            associateWithJobManager(jobManager, id, blobPort)
+          } catch {
+            case t: Throwable =>
+              killTaskManagerFatal(
+                "Unable to start TaskManager components after registering at JobManager", t)
+          }
+        }
+
+      // we are already registered at that specific JobManager - duplicate answer, rare cases
+      case AlreadyRegistered(jobManager, id, blobPort) =>
+        if (isConnected) {
+          if (jobManager == currentJobManager.orNull) {
+            LOG.debug("Ignoring duplicate registration acknowledgement.")
+          } else {
+            LOG.warn(s"Received 'AlreadyRegistered' message from JobManager ${jobManager.path}, " +
+              s"even through TaskManager is currently registered at ${currentJobManager.orNull}")
           }
-        case None => log.error("[FT-TaskManager] Received a barrier for an unknown vertex")
+        }
+        else {
+          // not connected, yet, to let's associate
+          LOG.info("Received 'AlreadyRegistered' message before 'AcknowledgeRegistration'")
+
+          try {
+            associateWithJobManager(jobManager, id, blobPort)
+          } catch {
+            case t: Throwable =>
+              killTaskManagerFatal(
+                "Unable to start TaskManager components after registering at JobManager", t)
+          }
+        }
+
+      case RefuseRegistration(reason) =>
+        if (currentJobManager.isEmpty) {
+          LOG.error(s"The registration at JobManager ${jobManagerAkkaURL} was refused, " +
+                    s"because: ${reason}. Retrying later...")
+
+          // try the registration again after some time
+
+          val delay: FiniteDuration = TaskManager.DELAY_AFTER_REFUSED_REGISTRATION
+          val deadline: Option[Deadline] = config.maxRegistrationDuration.map {
+            timeout => timeout + delay fromNow
+          }
+
+          context.system.scheduler.scheduleOnce(delay) {
+            self.tell(TriggerTaskManagerRegistration(jobManagerAkkaURL,
+                  TaskManager.INITIAL_REGISTRATION_TIMEOUT, deadline, 1), ActorRef.noSender)
+          }(context.dispatcher)
+        }
+        else {
+          // ignore RefuseRegistration messages which arrived after AcknowledgeRegistration
+          if (sender() == currentJobManager.orNull) {
+            LOG.warn(s"Received 'RefuseRegistration' from the JobManager (${sender().path})" +
+                     s" even though this TaskManager is already registered there.")
+          }
+          else {
+            LOG.warn(s"Ignoring 'RefuseRegistration' from unrelated JobManager (${sender().path})")
+          }
+        }
+
+      case _ => unhandled(message)
+    }
+  }
+
+
+  // --------------------------------------------------------------------------
+  //  Task Manager / JobManager association and initialization
+  // --------------------------------------------------------------------------
+
+  /**
+   * Checks whether the TaskManager is currently connected to its JobManager.
+   *
+   * @return True, if the TaskManager is currently connected to a JobManager, false otherwise.
+   */
+  private def isConnected : Boolean = currentJobManager.isDefined
+
+  /**
+   * Associates the TaskManager with the given JobManager. After this
+   * method has completed, the TaskManager is ready to receive work
+   * from the given JobManager.
+   *
+   * @param jobManager The JobManager to associate with.
+   * @param id The instanceID under which the TaskManager is registered
+   *           at the JobManager.
+   * @param blobPort The JobManager's port for the BLOB server.
+   */
+  private def associateWithJobManager(jobManager: ActorRef,
+                                      id: InstanceID,
+                                      blobPort: Int): Unit = {
+
+    // sanity check that we are not currently registered with a different JobManager
+    if (isConnected) {
+      if (currentJobManager.get == jobManager) {
+        LOG.warn("Received call to finish registration with JobManager " +
+          jobManager.path + " even though TaskManager is already registered.")
+        return
+      }
+      else {
+        throw new IllegalStateException("Attempt to register with JobManager " +
+          jobManager.path + " even though TaskManager is currently registered with JobManager " +
+          currentJobManager.get.path)
       }
+    }
+
+    // not yet associated, so associate
+    LOG.info("Successful registration at JobManager ({}), " +
+      "starting network stack and library cache.", jobManager.path)
+
+    // sanity check that the JobManager dependent components are not set up currently
+    if (network.isAssociated || blobService.isDefined) {
+      throw new IllegalStateException("JobManager-specific components are already initialized.")
+    }
+
+    // start the network stack, now that we have the JobManager actor reference
+    try {
+      network.associateWithTaskManagerAndJobManager(jobManager, self)
+    }
+    catch {
+      case e: Exception =>
+        val message = "Could not start network environment."
+        LOG.error(message, e)
+        throw new RuntimeException(message, e)
+    }
+
+    // start a blob service, if a blob server is specified
+    if (blobPort > 0) {
+      val address = new InetSocketAddress(
+        currentJobManager.flatMap(_.path.address.host).getOrElse("localhost"),
+        blobPort)
+
+      LOG.info("Determined BLOB server address to be {}. Starting BLOB cache.", address)
+
+      try {
+        val blobcache = new BlobCache(address, config.configuration)
+        blobService = Option(blobcache)
+        libraryCacheManager = Some(new BlobLibraryCacheManager(blobcache, config.cleanupInterval))
+      }
+      catch {
+        case e: Exception =>
+          val message = "Could not create BLOB cache or library cache."
+          LOG.error(message, e)
+          throw new RuntimeException(message, e)
+      }
+    }
+    else {
+      libraryCacheManager = Some(new FallbackLibraryCacheManager)
+    }
+
+    currentJobManager = Some(jobManager)
+    instanceID = id
+
+    // watch job manager to detect when it dies
+    context.watch(jobManager)
+
+    // schedule regular heartbeat message for oneself
+    heartbeatScheduler = Some(context.system.scheduler.schedule(
+      TaskManager.HEARTBEAT_INTERVAL, TaskManager.HEARTBEAT_INTERVAL, self, SendHeartbeat)
+       (context.dispatcher))
+
+    // notify all the actors that listen for a successful registration
+    for (listener <- waitForRegistration) {
+      listener ! RegisteredAtJobManager
+    }
+    waitForRegistration.clear()
   }
 
   /**
-   * Handle unmatched messages with an exception.
+   * Disassociates the TaskManager from the JobManager. This cleans
+   * removes all tasks currently running, discards all intermediate
+   * results and all cached libraries.
    */
-  override def unhandled(message: Any): Unit = {
-    // let the actor crash
-    throw new RuntimeException("Received unknown message " + message)
+  private def disassociateFromJobManager(): Unit = {
+    if (!isConnected) {
+      LOG.warn("TaskManager received message to disassociate from JobManager, even though " +
+        "it is not currently associated with a JobManager")
+      return
+    }
+
+    LOG.info("Disassociating from JobManager")
+
+    // stop the periodic heartbeats
+    heartbeatScheduler foreach {
+      _.cancel()
+    }
+    heartbeatScheduler = None
+
+    // stop the monitoring of the JobManager
+    currentJobManager foreach {
+      jm => context.unwatch(jm)
+    }
+
+    // de-register from the JobManager (faster detection of disconnect)
+    currentJobManager foreach {
+      _ ! Disconnect(s"TaskManager ${self.path} is shutting down.")
+    }
+
+    currentJobManager = None
+    instanceID = null
+
+    // shut down BLOB and library cache
+    libraryCacheManager foreach {
+      manager => manager.shutdown()
+    }
+    libraryCacheManager = None
+
+    blobService foreach {
+      service => service.shutdown()
+    }
+    blobService = None
+
+    // disassociate the network environment
+    network.disassociate()
   }
 
+  private def handleJobManagerDisconnect(jobManager: ActorRef, msg: String): Unit = {
+    if (isConnected && jobManager != null) {
+
+      // check if it comes from our JobManager
+      if (jobManager == currentJobManager.orNull) {
+        try {
+          val message = "Disconnecting from JobManager: " + msg
+          LOG.info(message)
+
+          // cancel all our tasks with a proper error message
+          cancelAndClearEverything(new Exception(message))
+
+          // reset our state to disassociated
+          disassociateFromJobManager()
+
+          // begin attempts to reconnect
+          val deadline: Option[Deadline] = config.maxRegistrationDuration.map(_.fromNow)
+          self ! TriggerTaskManagerRegistration(jobManagerAkkaURL,
+                               TaskManager.INITIAL_REGISTRATION_TIMEOUT, deadline, 1)
+        }
+        catch {
+          // this is pretty bad, it leaves the TaskManager in a state where it cannot
+          // cleanly reconnect
+          case t: Throwable =>
+            killTaskManagerFatal("Failed to disassociate from the JobManager", t)
+        }
+      }
+      else {
+        LOG.warn("Received erroneous JobManager disconnect message for {}", jobManager.path)
+      }
+    }
+  }
+
+
+  // --------------------------------------------------------------------------
+  //  Task Operations
+  // --------------------------------------------------------------------------
+
   /**
    * Receives a [[TaskDeploymentDescriptor]] describing the task to be executed. Sets up a
    * [[RuntimeEnvironment]] for the task and starts its execution in a separate thread.
@@ -412,21 +782,31 @@ class TaskManager(val connectionInfo: InstanceConnectionInfo,
     val executionID = tdd.getExecutionId
     val taskIndex = tdd.getIndexInSubtaskGroup
     val numSubtasks = tdd.getNumberOfSubtasks
+    val slot = tdd.getTargetSlotNumber
     var startRegisteringTask = 0L
     var task: Task = null
 
+    // all operations are in a try / catch block to make sure we send a result upon any failure
     try {
+      // check that we are already registered
+      if (!isConnected) {
+        throw new IllegalStateException("TaskManager is not associated with a JobManager")
+      }
+      if (slot < 0 || slot >= numberOfSlots) {
+        throw new Exception(s"Target slot ${slot} does not exist on TaskManager.")
+      }
+
       val userCodeClassLoader = libraryCacheManager match {
         case Some(manager) =>
-          if (log.isDebugEnabled) {
+          if (LOG.isDebugEnabled) {
             startRegisteringTask = System.currentTimeMillis()
           }
 
           // triggers the download of all missing jar files from the job manager
           manager.registerTask(jobID, executionID, tdd.getRequiredJarFiles)
 
-          if (log.isDebugEnabled) {
-            log.debug("Register task {} at library cache manager took {}s", executionID,
+          if (LOG.isDebugEnabled) {
+            LOG.debug("Register task {} at library cache manager took {}s", executionID,
               (System.currentTimeMillis() - startRegisteringTask) / 1000.0)
           }
 
@@ -448,266 +828,135 @@ class TaskManager(val connectionInfo: InstanceConnectionInfo,
       }
 
       val env = currentJobManager match {
-        case Some(jobManager) =>
-          val splitProvider = new TaskInputSplitProvider(jobManager, jobID, vertexID,
-            executionID, userCodeClassLoader, timeout)
-
-          new RuntimeEnvironment(jobManager, task, tdd, userCodeClassLoader,
-            memoryManager, ioManager, splitProvider, bcVarManager, networkEnvironment.get)
-
-        case None => throw new IllegalStateException("TaskManager has not yet been registered at " +
-          "a JobManager.")
-      }
-
-      task.setEnvironment(env)
-
-      //inject operator state
-      if(tdd.getOperatorStates != null)
-      {
-        val vertex = task.getEnvironment.getInvokable match {
-          case opStateCarrier: OperatorStateCarrier =>
-            opStateCarrier.injectState(tdd.getOperatorStates)
-        }
-      }
-      
-      // register the task with the network stack and profiles
-      networkEnvironment match {
-        case Some(ne) =>
-          log.info("Register task {} on {}.", task, connectionInfo)
-          ne.registerTask(task)
-        case None => throw new RuntimeException(
-          "Network environment has not been properly instantiated.")
-      }
-
-      val jobConfig = tdd.getJobConfiguration
-
-      if (jobConfig.getBoolean(ProfilingUtils.PROFILE_JOB_KEY, true)) {
-        profiler match {
-          case Some(profilerActorRef) => profilerActorRef ! MonitorTask(task)
-          case None => // no log message here - floods the log
-        }
-      }
-
-      val cpTasks = new util.HashMap[String, FutureTask[Path]]()
-
-      for (entry <- DistributedCache.readFileInfoFromConfig(tdd.getJobConfiguration).asScala) {
-        val cp = fileCache.createTmpFile(entry.getKey, entry.getValue, jobID)
-        cpTasks.put(entry.getKey, cp)
-      }
-      env.addCopyTasksForCacheFile(cpTasks)
-
-      if (!task.startExecution()) {
-        throw new RuntimeException("Cannot start task. Task was canceled or failed.")
-      }
-
-      sender ! TaskOperationResult(executionID, success = true)
-    } catch {
-      case t: Throwable =>
-        val message = if (t.isInstanceOf[CancelTaskException]) {
-          "Task was canceled"
-        } else {
-          log.error(t, "Could not instantiate task with execution ID {}.", executionID)
-          ExceptionUtils.stringifyException(t)
-        }
-
-        try {
-          if (task != null) {
-            task.failExternally(t)
-            removeAllTaskResources(task)
-          }
-
-          libraryCacheManager foreach { _.unregisterTask(jobID, executionID) }
-        } catch {
-          case t: Throwable => log.error(t, "Error during cleanup of task deployment.")
-        }
-
-        sender ! new TaskOperationResult(executionID, false, message)
-    }
-  }
-
-  private def cleanupTaskManager(): Unit = {
-    currentJobManager foreach {
-      context.unwatch(_)
-    }
-
-    networkEnvironment foreach {
-      ne =>
-        try {
-          ne.shutdown()
-        } catch {
-          case t: Throwable => log.error(t, "ChannelManager did not shutdown properly.")
-        }
-    }
-
-    networkEnvironment = None
-
-    libraryCacheManager foreach {
-      manager =>
-        try {
-          manager.shutdown()
-        } catch {
-          case t: Throwable => log.error(t, "Could not shut down the library cache manager.")
-        }
-    }
-
-    libraryCacheManager = None
-
-    heartbeatScheduler foreach {
-      _.cancel()
-    }
-
-    heartbeatScheduler = None
-
-    profilerListener foreach {
-      listener =>
-        profiler foreach {
-          _.tell(UnregisterProfilingListener, listener)
-        }
-    }
-
-    profilerListener = None
-    currentJobManager = None
-    instanceID = null
-    registrationAttempts = 0
-    registrationDuration = 0 seconds
-  }
-
-  private def updateTask(
-    executionId: ExecutionAttemptID,
-    partitionInfos: Seq[(IntermediateDataSetID, InputChannelDeploymentDescriptor)]): Unit = {
-
-    runningTasks.get(executionId) match {
-      case Some(task) =>
-        val errors = partitionInfos flatMap {
-          case (resultID, partitionInfo) =>
-            Option(task.getEnvironment.getInputGateById(resultID)) match {
-              case Some(reader) =>
-                Future {
-                  try {
-                    reader.updateInputChannel(partitionInfo)
-                  } catch {
-                    case t: Throwable =>
-                      log.error(t, "Could not update task {}. Trying to cancel task.",
-                       task.getTaskName)
-
-                      try {
-                        task.markFailed(t)
-                      } catch {
-                        case t: Throwable =>
-                          log.error(t, "Failed canceling task with execution ID {} after task" +
-                            "update failure.", executionId)
-                      }
-                  }
-                }
-                None
-              case None => Some(s"No reader with ID $resultID for task $executionId was found.")
-            }
-        }
-
-        if(errors.isEmpty) {
-          sender ! Acknowledge
-        } else {
-          sender ! Failure(new IllegalStateException(errors.mkString("\n")))
-        }
-      case None =>
-        log.info("Could not update task with ID {}, because it is no longer running.",
-          executionId)
-        sender ! Acknowledge
-    }
-  }
-
-  private def finishRegistration(jobManager: ActorRef, id: InstanceID, blobPort: Int,
-                                  profilerListener: Option[ActorRef]): Unit = {
-    setupTaskManager(jobManager, id, blobPort, profilerListener)
+        case Some(jobManager) =>
+          val splitProvider = new TaskInputSplitProvider(jobManager, jobID, vertexID,
+            executionID, userCodeClassLoader, askTimeout)
 
-    for (listener <- waitForRegistration) {
-      listener ! RegisteredAtJobManager
-    }
+          new RuntimeEnvironment(jobManager, task, tdd, userCodeClassLoader,
+            memoryManager, ioManager, splitProvider, bcVarManager, network)
 
-    waitForRegistration.clear()
-  }
+        case None => throw new IllegalStateException(
+          "TaskManager has not yet been registered at a JobManager.")
+      }
 
-  private def setupTaskManager(jobManager: ActorRef, id: InstanceID, blobPort: Int,
-                                profilerListener: Option[ActorRef]): Unit = {
+      task.setEnvironment(env)
 
-    currentJobManager = Some(jobManager)
-    this.profilerListener = profilerListener
-    instanceID = id
+      //inject operator state
+      if (tdd.getOperatorStates != null) {
+        task.getEnvironment.getInvokable match {
+          case opStateCarrier: OperatorStateCarrier =>
+            opStateCarrier.injectState(tdd.getOperatorStates)
+        }
+      }
+      
+      // register the task with the network stack and profiles
+      LOG.info("Register task {}", task)
+      network.registerTask(task)
 
-    // watch job manager to detect when it dies
-    context.watch(jobManager)
+      val cpTasks = new util.HashMap[String, FutureTask[Path]]()
 
-    setupNetworkEnvironment(jobManager)
-    setupLibraryCacheManager(blobPort)
+      for (entry <- DistributedCache.readFileInfoFromConfig(tdd.getJobConfiguration).asScala) {
+        val cp = fileCache.createTmpFile(entry.getKey, entry.getValue, jobID)
+        cpTasks.put(entry.getKey, cp)
+      }
+      env.addCopyTasksForCacheFile(cpTasks)
 
-    // schedule regular heartbeat message for oneself
-    heartbeatScheduler = Some(context.system.scheduler.schedule(
-      TaskManager.HEARTBEAT_INTERVAL, TaskManager.HEARTBEAT_INTERVAL, self, SendHeartbeat))
+      if (!task.startExecution()) {
+        throw new RuntimeException("Cannot start task. Task was canceled or failed.")
+      }
 
-    profilerListener foreach {
-      listener =>
-        profiler foreach {
-          _.tell(RegisterProfilingListener, listener)
-        }
+      sender ! new TaskOperationResult(executionID, true)
     }
-  }
+    catch {
+      case t: Throwable =>
+        val message = if (t.isInstanceOf[CancelTaskException]) {
+          "Task was canceled"
+        } else {
+          LOG.error("Could not instantiate task with execution ID " + executionID, t)
+          ExceptionUtils.stringifyException(t)
+        }
 
-  private def setupNetworkEnvironment(jobManager: ActorRef): Unit = {
-    //shutdown existing network environment
-    networkEnvironment foreach {
-      ne =>
         try {
-          ne.shutdown()
+          if (task != null) {
+            task.failExternally(t)
+            removeAllTaskResources(task)
+          }
+
+          libraryCacheManager foreach { _.unregisterTask(jobID, executionID) }
         } catch {
-          case t: Throwable => log.error(t, "Network environment did not shutdown properly.")
+          case t: Throwable => LOG.error("Error during cleanup of task deployment.", t)
         }
-    }
 
-    try {
-      val env: NetworkEnvironment = new NetworkEnvironment(timeout, networkConfig)
-      env.associateWithTaskManagerAndJobManager(jobManager, self)
-      networkEnvironment = Some(env)
-    } catch {
-      case ioe: IOException =>
-        log.error(ioe, "Failed to instantiate network environment.")
-        throw new RuntimeException("Failed to instantiate ChannelManager.", ioe)
+        sender ! new TaskOperationResult(executionID, false, message)
     }
   }
 
-  private def setupLibraryCacheManager(blobPort: Int): Unit = {
-    // shutdown existing library cache manager first
-    libraryCacheManager foreach {
-      manager => {
-        try{
-          manager.shutdown()
-        } catch {
-          case t: Throwable => log.error(t, "Could not properly shut down LibraryCacheManager.")
-        }
-      }
-    }
+  /**
+   * Informs a task about additional locations of its input data partitions.
+   *
+   * @param executionId The execution attempt ID of the task.
+   * @param partitionInfos The descriptor of the intermediate result partitions.
+   */
+  private def updateTaskInputPartitions(
+         executionId: ExecutionAttemptID,
+         partitionInfos: Seq[(IntermediateDataSetID, InputChannelDeploymentDescriptor)]) : Unit = {
 
-    // Check if a blob server is specified
-    if (blobPort > 0) {
+    runningTasks.get(executionId) match {
+      case Some(task) =>
 
-      val address = new InetSocketAddress(
-        currentJobManager.flatMap(_.path.address.host).getOrElse("localhost"),
-        blobPort)
+        val errors: Seq[String] = partitionInfos.flatMap { info =>
 
-      log.info("Determined BLOB server address to be {}.", address)
+          val (resultID, partitionInfo) = info
+          val reader = task.getEnvironment.getInputGateById(resultID)
 
-      libraryCacheManager = Some(new BlobLibraryCacheManager(
-                                     new BlobCache(address, configuration), cleanupInterval))
-    } else {
-      libraryCacheManager = Some(new FallbackLibraryCacheManager)
+          if (reader != null) {
+            Future {
+              try {
+                reader.updateInputChannel(partitionInfo)
+              } catch {
+                case t: Throwable =>
+                  LOG.error(s"Could not update input data location for task " +
+                    s"${task.getTaskName}. Trying to fail  task.", t)
+
+                  try {
+                    task.markFailed(t)
+                  }
+                  catch {
+                    case t: Throwable =>
+                      LOG.error("Failed canceling task with execution ID " + executionId +
+                        " after task update failure.", t)
+                  }
+              }
+            }(context.dispatcher)
+            None
+          }
+          else {
+            Some(s"No reader with ID $resultID for task $executionId was found.")
+          }
+        }
+
+        if (errors.isEmpty) {
+          sender ! Acknowledge
+        } else {
+          sender ! Failure(new Exception(errors.mkString("\n")))
+        }
+
+      case None =>
+        LOG.debug("Discard update for input partitions of task {} : task is no longer running.",
+          executionId)
+        sender ! Acknowledge
     }
   }
 
   /**
-   * Removes all tasks from this TaskManager.
+   * Marks all tasks currently registered as failed (with the given
+   * cause) and removes them.
+   *
+   * @param cause The exception given to the tasks as the failure reason.
    */
   private def cancelAndClearEverything(cause: Throwable) {
     if (runningTasks.size > 0) {
-      log.info("Cancelling all computations and discarding all cached data.")
+      LOG.info("Cancelling all computations and discarding all cached data.")
 
       for (t <- runningTasks.values) {
         t.failExternally(cause)
@@ -719,24 +968,65 @@ class TaskManager(val connectionInfo: InstanceConnectionInfo,
   private def unregisterTaskAndNotifyFinalState(executionID: ExecutionAttemptID): Unit = {
     runningTasks.remove(executionID) match {
       case Some(task) =>
-        log.info("Unregister task with execution ID {}.", executionID)
+
+        // mark the task as failed if it is not yet in a final state
+        if (!task.getExecutionState.isTerminal) {
+          try {
+            task.failExternally(new Exception("Task is being removed from TaskManager"))
+          } catch {
+            case e: Exception => LOG.error("Could not properly fail task", e)
+          }
+        }
+
+        LOG.info("Unregister task with execution ID {}.", executionID)
         removeAllTaskResources(task)
         libraryCacheManager foreach { _.unregisterTask(task.getJobID, executionID) }
 
-        log.info("Updating FINAL execution state of {} ({}) to {}.", task.getTaskName,
-          task.getExecutionId, task.getExecutionState)
+        LOG.info("Updating FINAL execution state of {} ({}) to {}.",
+          task.getTaskName, task.getExecutionId, task.getExecutionState)
 
         self ! UpdateTaskExecutionState(new TaskExecutionState(
           task.getJobID, task.getExecutionId, task.getExecutionState, task.getFailureCause))
 
       case None =>
-        if (log.isDebugEnabled) {
-          log.debug("Cannot find task with ID {} to unregister.", executionID)
-        }
+        LOG.debug("Cannot find task with ID {} to unregister.", executionID)
     }
   }
 
+  /**
+   * This method cleans up the resources of a task in the distributed cache,
+   * network stack and the memory manager.
+   *
+   * If the cleanup in the network stack or memory manager fails, this is considered
+   * a fatal problem (critical resource leak) and causes the TaskManager to quit.
+   * A TaskManager JVM restart is the best safe way to fix that error.
+   *
+   * @param task The Task whose resources should be cleared.
+   */
   private def removeAllTaskResources(task: Task): Unit = {
+
+    // release the critical things first, and fail fatally if it does not work
+
+    // this releases all task resources, like buffer pools and intermediate result
+    // partitions being built. If this fails, the TaskManager is in serious trouble,
+    // as this is a massive resource leak. We kill the TaskManager in that case,
+    // to recover through a clean JVM start
+    try {
+      network.unregisterTask(task)
+    } catch {
+      case t: Throwable =>
+        killTaskManagerFatal("Failed to unregister task resources from network stack", t)
+    }
+
+    // safety net to release all the task's memory
+    try {
+      task.unregisterMemoryManager(memoryManager)
+    } catch {
+      case t: Throwable =>
+        killTaskManagerFatal("Failed to unregister task memory from memory manager", t)
+    }
+
+    // release temp files from the distributed cache
     if (task.getEnvironment != null) {
       try {
         for (entry <- DistributedCache.readFileInfoFromConfig(
@@ -744,20 +1034,74 @@ class TaskManager(val connectionInfo: InstanceConnectionInfo,
           fileCache.deleteTmpFile(entry.getKey, entry.getValue, task.getJobID)
         }
       } catch {
-        case t: Throwable => log.error(
-          "Error cleaning up local files from the distributed cache.", t)
+        // this is pretty unpleasant, but not a reason to give up immediately
+        case e: Exception => LOG.error(
+          "Error cleaning up local temp files from the distributed cache.", e)
+      }
+    }
+  }
+
+  // --------------------------------------------------------------------------
+  //  Miscellaneous actions
+  // --------------------------------------------------------------------------
+
+  /**
+   * Sends a heartbeat message to the JobManager (if connected) with the current
+   * metrics report.
+   */
+  private def sendHeartbeatToJobManager(): Unit = {
+    try {
+      LOG.debug("Sending heartbeat to JobManager")
+      val report: Array[Byte] = metricRegistryMapper.writeValueAsBytes(metricRegistry)
+      currentJobManager foreach {
+        jm => jm ! Heartbeat(instanceID, report)
       }
     }
+    catch {
+      case e: Exception => LOG.warn("Error sending the metric heartbeat to the JobManager", e)
+    }
+  }
 
-    networkEnvironment foreach {
-      _.unregisterTask(task)
+  /**
+   * Sends a message with the stack trace of all threads to the given recipient.
+   *
+   * @param recipient The target of the stack trace message
+   */
+  private def sendStackTrace(recipient: ActorRef): Unit = {
+    if (recipient == null) {
+      return
     }
 
-    profiler foreach {
-      _ ! UnmonitorTask(task.getExecutionId)
+    try {
+      val traces = Thread.getAllStackTraces.asScala
+      val stackTraceStr = traces.map(
+        (trace: (Thread, Array[StackTraceElement])) => {
+          val (thread, elements) = trace
+          "Thread: " + thread.getName + '\n' + elements.mkString("\n")
+          })
+        .mkString("\n\n")
+
+      recipient ! StackTrace(instanceID, stackTraceStr)
+    }
+    catch {
+      case e: Exception => LOG.error("Failed to send stack trace to " + recipient.path, e)
     }
+  }
 
-    task.unregisterMemoryManager(memoryManager)
+  /**
+   * Prints a big error message in the log and kills the TaskManager actor.
+   *
+   * @param cause The exception that caused the fatal problem.
+   */
+  private def killTaskManagerFatal(message: String, cause: Throwable): Unit = {
+    LOG.error("\n" +
+      "==============================================================\n" +
+      "======================      FATAL      =======================\n" +
+      "==============================================================\n" +
+      "\n" +
+      "A fatal error occurred, forcing the TaskManager to shut down: " + message, cause)
+
+    self ! Kill
   }
 }
 
@@ -767,18 +1111,32 @@ class TaskManager(val connectionInfo: InstanceConnectionInfo,
  */
 object TaskManager {
 
+  /** TaskManager logger for synchronous logging (not through the logging actor) */
   val LOG = LoggerFactory.getLogger(classOf[TaskManager])
 
+  /** Return code for unsuccessful TaskManager startup */
   val STARTUP_FAILURE_RETURN_CODE = 1
+
+  /** Return code for critical errors during the runtime */
   val RUNTIME_FAILURE_RETURN_CODE = 2
 
   val TASK_MANAGER_NAME = "taskmanager"
   val PROFILER_NAME = "profiler"
 
-  val REGISTRATION_DELAY = 0 seconds
-  val REGISTRATION_INTERVAL = 10 seconds
-  val MAX_REGISTRATION_ATTEMPTS = 10
-  val HEARTBEAT_INTERVAL = 5000 millisecond
+  /** Maximum time (msecs) that the TaskManager will spend searching for a
+    * suitable network interface to use for communication */
+  val MAX_STARTUP_CONNECT_TIME = 120000L
+
+  /** Time (msecs) after which the TaskManager will start logging failed
+    * connection attempts */
+  val STARTUP_CONNECT_LOG_SUPPRESS = 10000L
+
+  val INITIAL_REGISTRATION_TIMEOUT: FiniteDuration = 500 milliseconds
+  val MAX_REGISTRATION_TIMEOUT: FiniteDuration = 30 seconds
+
+  val DELAY_AFTER_REFUSED_REGISTRATION: FiniteDuration = 10 seconds
+
+  val HEARTBEAT_INTERVAL: FiniteDuration = 5000 milliseconds
 
 
   // --------------------------------------------------------------------------
@@ -807,19 +1165,19 @@ object TaskManager {
       }
     }
 
-    // run the TaskManager (is requested in an authentication enabled context)
+    // run the TaskManager (if requested in an authentication enabled context)
     try {
       if (SecurityUtils.isSecurityEnabled) {
         LOG.info("Security is enabled. Starting secure TaskManager.")
         SecurityUtils.runSecured(new FlinkSecuredRunner[Unit] {
           override def run(): Unit = {
-            runTaskManager(configuration, classOf[TaskManager])
+            selectNetworkInterfaceAndRunTaskManager(configuration, classOf[TaskManager])
           }
         })
       }
       else {
         LOG.info("Security is not enabled. Starting non-authenticated TaskManager.")
-        runTaskManager(configuration, classOf[TaskManager])
+        selectNetworkInterfaceAndRunTaskManager(configuration, classOf[TaskManager])
       }
     }
     catch {
@@ -831,7 +1189,7 @@ object TaskManager {
   }
 
   /**
-   * Parse the command line arguments of the [[TaskManager]] and loads the configuration.
+   * Parse the command line arguments of the TaskManager and loads the configuration.
    *
    * @param args Command line arguments
    * @return The parsed configuration.
@@ -869,22 +1227,43 @@ object TaskManager {
   // --------------------------------------------------------------------------
 
   /**
-   * Starts and runs the TaskManager. Brings up an actor system for the TaskManager and its
-   * actors, starts the TaskManager's services (library cache, shuffle network stack, ...),
-   * and starts the TaskManager itself.
+   * Starts and runs the TaskManager.
+   *
+   * This method first tries to select the network interface to use for the TaskManager
+   * communication. The network interface is used both for the actor communication
+   * (coordination) as well as for the data exchange between task managers. Unless
+   * the hostname/interface is explicitly configured in the configuration, this
+   * method will try out various interfaces and methods to connect to the JobManager
+   * and select the one where the connection attempt is successful.
+   *
+   * After selecting the network interface, this method brings up an actor system
+   * for the TaskManager and its actors, starts the TaskManager's services
+   * (library cache, shuffle network stack, ...), and starts the TaskManager itself.
 
    * @param configuration The configuration for the TaskManager.
-   * @param taskManagerClass The actor class to instantiate. Allows to use TaskManager subclasses
-   *                         for example for YARN.
+   * @param taskManagerClass The actor class to instantiate.
+   *                         Allows to use TaskManager subclasses for example for YARN.
    */
   @throws(classOf[Exception])
-  def runTaskManager(configuration: Configuration,
-                     taskManagerClass: Class[_ <: TaskManager]) : Unit = {
+  def selectNetworkInterfaceAndRunTaskManager(configuration: Configuration,
+                                              taskManagerClass: Class[_ <: TaskManager]) : Unit = {
 
     val (jobManagerHostname, jobManagerPort) = getAndCheckJobManagerAddress(configuration)
 
+    val (taskManagerHostname, actorSystemPort) =
+       selectNetworkInterfaceAndPort(configuration, jobManagerHostname, jobManagerPort)
+
+    runTaskManager(taskManagerHostname, actorSystemPort, configuration, taskManagerClass)
+  }
+
+  @throws(classOf[IOException])
+  @throws(classOf[IllegalConfigurationException])
+  def selectNetworkInterfaceAndPort(configuration: Configuration,
+                                    jobManagerHostname: String,
+                                    jobManagerPort: Int) : (String, Int) = {
+
     var taskManagerHostname = configuration.getString(
-                                       ConfigConstants.TASK_MANAGER_HOSTNAME_KEY, null)
+      ConfigConstants.TASK_MANAGER_HOSTNAME_KEY, null)
 
     if (taskManagerHostname != null) {
       LOG.info("Using configured hostname/address for TaskManager: " + taskManagerHostname)
@@ -893,31 +1272,38 @@ object TaskManager {
       // try to find out the hostname of the interface from which the TaskManager
       // can connect to the JobManager. This involves a reverse name lookup
       LOG.info("Trying to select the network interface and address to use " +
-        "by connecting to the configured JobManager")
+        "by connecting to the configured JobManager.")
+
+      LOG.info("TaskManager will try to connect for {} seconds before falling back to heuristics",
+        MAX_STARTUP_CONNECT_TIME)
 
       val jobManagerAddress = new InetSocketAddress(jobManagerHostname, jobManagerPort)
-      taskManagerHostname = try {
+      val taskManagerAddress = try {
         // try to get the address for up to two minutes and start
         // logging only after ten seconds
-        NetUtils.findConnectingAddress(jobManagerAddress, 120000, 10000).getHostName()
+        NetUtils.findConnectingAddress(jobManagerAddress,
+          MAX_STARTUP_CONNECT_TIME, STARTUP_CONNECT_LOG_SUPPRESS)
       }
       catch {
-        case t: Throwable => throw new Exception("TaskManager cannot find a network interface " +
+        case t: Throwable => throw new IOException("TaskManager cannot find a network interface " +
           "that can communicate with the JobManager (" + jobManagerAddress + ")", t)
       }
 
-      LOG.info("TaskManager will use hostname/address '{}' for communication.", taskManagerHostname)
+      taskManagerHostname = taskManagerAddress.getHostName()
+      LOG.info(s"TaskManager will use hostname/address '${taskManagerHostname}' " +
+        s"(${taskManagerAddress.getHostAddress()}) for communication.")
     }
 
     // if no task manager port has been configured, use 0 (system will pick any free port)
     val actorSystemPort = configuration.getInteger(ConfigConstants.TASK_MANAGER_IPC_PORT_KEY, 0)
-    if (actorSystemPort < 0) {
-      throw new Exception("Invalid value for '" + ConfigConstants.TASK_MANAGER_IPC_PORT_KEY  +
+    if (actorSystemPort < 0 || actorSystemPort > 65535) {
+      throw new IllegalConfigurationException("Invalid value for '" +
+        ConfigConstants.TASK_MANAGER_IPC_PORT_KEY +
         "' (port for the TaskManager actor system) : " + actorSystemPort +
         " - Leave config parameter empty or use 0 to let the system choose a port automatically.")
     }
 
-    runTaskManager(taskManagerHostname, actorSystemPort, configuration, taskManagerClass)
+    (taskManagerHostname, actorSystemPort)
   }
 
   /**
@@ -965,7 +1351,8 @@ object TaskManager {
     LOG.info("Starting TaskManager")
 
     // Bring up the TaskManager actor system first, bind it to the given address.
-    LOG.info("Starting TaskManager actor system")
+
+    LOG.info("Starting TaskManager actor system at {}:{}", taskManagerHostname, actorSystemPort)
 
     val taskManagerSystem = try {
       val akkaConfig = AkkaUtils.getAkkaConfig(configuration,
@@ -993,8 +1380,12 @@ object TaskManager {
     // and the TaskManager actor
     try {
       LOG.info("Starting TaskManager actor")
-      val taskManager = startTaskManagerActor(configuration, taskManagerSystem, taskManagerHostname,
-        TASK_MANAGER_NAME, false, false, taskManagerClass)
+      val taskManager = startTaskManagerComponentsAndActor(configuration,
+                                                           taskManagerSystem,
+                                                           taskManagerHostname,
+                                                           Some(TASK_MANAGER_NAME),
+                                                           None, false,
+                                                           taskManagerClass)
 
       // start a process reaper that watches the JobManager. If the JobManager actor dies,
       // the process reaper will kill the JVM process (to ensure easy failure detection)
@@ -1007,7 +1398,8 @@ object TaskManager {
       // memory usage information
       if (LOG.isInfoEnabled && configuration.getBoolean(
         ConfigConstants.TASK_MANAGER_DEBUG_MEMORY_USAGE_START_LOG_THREAD,
-        ConfigConstants.DEFAULT_TASK_MANAGER_DEBUG_MEMORY_USAGE_START_LOG_THREAD)) {
+        ConfigConstants.DEFAULT_TASK_MANAGER_DEBUG_MEMORY_USAGE_START_LOG_THREAD))
+      {
         LOG.info("Starting periodic memory usage logger")
 
         val interval = configuration.getLong(
@@ -1032,6 +1424,7 @@ object TaskManager {
           }
         }
         logger.setDaemon(true)
+        logger.setPriority(Thread.MIN_PRIORITY)
         logger.start()
       }
 
@@ -1051,38 +1444,118 @@ object TaskManager {
     }
   }
 
-  @throws(classOf[Exception])
-  def startTaskManagerActor(configuration: Configuration,
-                            actorSystem: ActorSystem,
-                            taskManagerHostname: String,
-                            taskManagerActorName: String,
-                            localAkkaCommunication: Boolean,
-                            localTaskManagerCommunication: Boolean,
-                            taskManagerClass: Class[_ <: TaskManager]): ActorRef = {
-
-    val (tmConfig, netConfig, connectionInfo, jmAkkaURL) =  parseTaskManagerConfiguration(
-      configuration, taskManagerHostname, localAkkaCommunication, localTaskManagerCommunication)
-
-    val tmProps = Props(taskManagerClass, connectionInfo, jmAkkaURL, tmConfig, netConfig)
-    actorSystem.actorOf(tmProps, taskManagerActorName)
-  }
-
   /**
-   * Starts the profiler actor.
    *
-   * @param instanceActorPath The actor path of the taskManager that is profiled.
-   * @param reportInterval The interval in which the profiler runs.
-   * @param actorSystem The actor system for the profiler actor
-   * @return The profiler actor ref.
+   * @param configuration The configuration for the TaskManager.
+   * @param actorSystem The actor system that should run the TaskManager actor.
+   * @param taskManagerHostname The hostname/address that describes the TaskManager's data location.
+   * @param taskManagerActorName Optionally the name of the TaskManager actor. If none is given,
+   *                             the actor will use a random name.
+   * @param jobManagerPath Optionally, the JobManager actor path may be provided. If none is
+   *                       provided, the method will construct it automatically from the
+   *                       JobManager hostname an port specified in the configuration.
+   * @param localTaskManagerCommunication If true, the TaskManager will not initiate the
+   *                                      TCP network stack.
+   * @param taskManagerClass The class of the TaskManager actor. May be used to give
+   *                         subclasses that understand additional actor messages.
+   *
+   * @throws org.apache.flink.configuration.IllegalConfigurationException
+   *                              Thrown, if the given config contains illegal values.
+   *
+   * @throws java.io.IOException Thrown, if any of the I/O components (such as buffer pools,
+   *                             I/O manager, ...) cannot be properly started.
+   * @throws java.lang.Exception Thrown is some other error occurs while parsing the configuration
+   *                             or starting the TaskManager components.
+   *
+   * @return An ActorRef to the TaskManager actor.
    */
-  private def startProfiler(instanceActorPath: String,
-                            reportInterval: Long,
-                            actorSystem: ActorSystem): ActorRef = {
+  @throws(classOf[IllegalConfigurationException])
+  @throws(classOf[IOException])
+  @throws(classOf[Exception])
+  def startTaskManagerComponentsAndActor(configuration: Configuration,
+                                         actorSystem: ActorSystem,
+                                         taskManagerHostname: String,
+                                         taskManagerActorName: Option[String],
+                                         jobManagerPath: Option[String],
+                                         localTaskManagerCommunication: Boolean,
+                                         taskManagerClass: Class[_ <: TaskManager]): ActorRef = {
+
+    // get and check the JobManager config
+    val jobManagerAkkaUrl: String = jobManagerPath.getOrElse {
+      val (jobManagerHostname, jobManagerPort) = getAndCheckJobManagerAddress(configuration)
+      val hostPort = new InetSocketAddress(jobManagerHostname, jobManagerPort)
+      JobManager.getRemoteJobManagerAkkaURL(hostPort)
+    }
+
+    val (taskManagerConfig : TaskManagerConfiguration,
+         netConfig: NetworkEnvironmentConfiguration,
+         connectionInfo: InstanceConnectionInfo)
+
+         = parseTaskManagerConfiguration(configuration, taskManagerHostname,
+                                         localTaskManagerCommunication)
+
+    // pre-start checks
+    checkTempDirs(taskManagerConfig.tmpDirPaths)
+
+    // we start the network first, to make sure it can allocate its buffers first
+    val network = new NetworkEnvironment(taskManagerConfig.timeout, netConfig)
+
+    // computing the amount of memory to use depends on how much memory is available
+    // it strictly needs to happen AFTER the network stack has been initialized
+
+    // check if a value has been configured
+    val configuredMemory = configuration.getLong(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, -1L)
+    checkConfigParameter(configuredMemory == -1 || configuredMemory > 0, configuredMemory,
+      ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY,
+      "MemoryManager needs at least one MB of memory. " +
+        "If you leave this config parameter empty, the system automatically " +
+        "pick a fraction of the available memory.")
 
-    val profilerProps = Props(classOf[TaskManagerProfiler], instanceActorPath, reportInterval)
-    actorSystem.actorOf(profilerProps, PROFILER_NAME)
+    val memorySize = if (configuredMemory > 0) {
+      LOG.info("Using {} MB for Flink managed memory.", configuredMemory)
+      configuredMemory << 20 // megabytes to bytes
+    }
+    else {
+      val fraction = configuration.getFloat(ConfigConstants.TASK_MANAGER_MEMORY_FRACTION_KEY,
+                                            ConfigConstants.DEFAULT_MEMORY_MANAGER_MEMORY_FRACTION)
+      checkConfigParameter(fraction > 0.0f && fraction < 1.0f, fraction,
+                           ConfigConstants.TASK_MANAGER_MEMORY_FRACTION_KEY,
+                           "MemoryManager fraction of the free memory must be between 0.0 and 1.0")
+
+      val relativeMemSize = (EnvironmentInformation.getSizeOfFreeHeapMemoryWithDefrag() *
+                                                                                   fraction).toLong
+
+      LOG.info("Using {} of the currently free heap space for Flink managed memory ({} MB).",
+        fraction, relativeMemSize >> 20)
+
+      relativeMemSize
+    }
+
+    // now start the memory manager
+    val memoryManager = new DefaultMemoryManager(memorySize,
+                                                 taskManagerConfig.numberOfSlots,
+                                                 netConfig.networkBufferSize)
+
+    // start the I/O manager last, it will create some temp directories.
+    val ioManager: IOManager = new IOManagerAsync(taskManagerConfig.tmpDirPaths)
+
+    // create the actor properties (which define the actor constructor parameters)
+    val tmProps = Props(taskManagerClass,
+                        taskManagerConfig,
+                        connectionInfo,
+                        jobManagerAkkaUrl,
+                        memoryManager,
+                        ioManager,
+                        network,
+                        taskManagerConfig.numberOfSlots)
+
+    taskManagerActorName match {
+      case Some(actorName) => actorSystem.actorOf(tmProps, actorName)
+      case None => actorSystem.actorOf(tmProps)
+    }
   }
 
+
   // --------------------------------------------------------------------------
   //  Resolving the TaskManager actor
   // --------------------------------------------------------------------------
@@ -1098,8 +1571,8 @@ object TaskManager {
    */
   @throws(classOf[IOException])
   def getTaskManagerRemoteReference(taskManagerUrl: String,
-                                   system: ActorSystem,
-                                   timeout: FiniteDuration): ActorRef = {
+                                    system: ActorSystem,
+                                    timeout: FiniteDuration): ActorRef = {
     try {
       val future = AkkaUtils.getReference(taskManagerUrl, system, timeout)
       Await.result(future, timeout)
@@ -1116,7 +1589,7 @@ object TaskManager {
   }
 
   // --------------------------------------------------------------------------
-  //  Miscellaneous Utilities
+  //  Parsing and checking the TaskManager Configuration
   // --------------------------------------------------------------------------
 
   /**
@@ -1125,19 +1598,18 @@ object TaskManager {
    *
    * @param configuration The configuration.
    * @param taskManagerHostname The host name under which the TaskManager communicates.
-   * @param localAkkaCommunication True, if the TaskManager runs in the same actor
-   *                               system as its JobManager.
    * @param localTaskManagerCommunication True, to skip initializing the network stack.
-   *                                      Use only when only one task manager is used.
+   *                                      Use only in cases where only one task manager runs.
    * @return A tuple (TaskManagerConfiguration, network configuration,
    *                  InstanceConnectionInfo, JobManager actor Akka URL).
    */
-  @throws(classOf[Exception])
+  @throws(classOf[IllegalArgumentException])
   def parseTaskManagerConfiguration(configuration: Configuration,
                                     taskManagerHostname: String,
-                                    localAkkaCommunication: Boolean,
                                     localTaskManagerCommunication: Boolean):
-  (TaskManagerConfiguration, NetworkEnvironmentConfiguration, InstanceConnectionInfo, String) = {
+    (TaskManagerConfiguration,
+     NetworkEnvironmentConfiguration,
+     InstanceConnectionInfo) = {
 
     // ------- read values from the config and check them ---------
     //                      (a lot of them)
@@ -1156,17 +1628,6 @@ object TaskManager {
     val taskManagerAddress = InetAddress.getByName(taskManagerHostname)
     val connectionInfo = new InstanceConnectionInfo(taskManagerAddress, dataport)
 
-    val jobManagerActorURL = if (localAkkaCommunication) {
-      // JobManager and TaskManager are in the same ActorSystem -> Use local Akka URL
-      JobManager.getLocalJobManagerAkkaURL
-    }
-    else {
-      // both run in different actor system
-      val (jobManagerHostname, jobManagerPort) = getAndCheckJobManagerAddress(configuration)
-      val hostPort = new InetSocketAddress(jobManagerHostname, jobManagerPort)
-      JobManager.getRemoteJobManagerAkkaURL(hostPort)
-    }
-
     // ----> memory / network stack (shuffles/broadcasts), task slots, temp directories
 
     // we need this because many configs have been written with a "-1" entry
@@ -1181,8 +1642,6 @@ object TaskManager {
       ConfigConstants.TASK_MANAGER_NETWORK_NUM_BUFFERS_KEY,
       ConfigConstants.DEFAULT_TASK_MANAGER_NETWORK_NUM_BUFFERS)
 
-    val configuredMemory = configuration.getLong(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, -1L)
-
     checkConfigParameter(slots >= 1, slots, ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS,
       "Number of task slots must be at least one.")
 
@@ -1197,19 +1656,11 @@ object TaskManager {
       ConfigConstants.TASK_MANAGER_NETWORK_BUFFER_SIZE_KEY,
       "Buffer size must be a power of 2.")
 
-    checkConfigParameter(configuredMemory == -1 || configuredMemory > 0, configuredMemory,
-      ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY,
-      "MemoryManager needs at least one MB of memory. " +
-        "Leave this config parameter empty to let the system automatically " +
-        "pick a fraction of the available memory.")
-
     val tmpDirs = configuration.getString(
       ConfigConstants.TASK_MANAGER_TMP_DIR_KEY,
       ConfigConstants.DEFAULT_TASK_MANAGER_TMP_PATH)
       .split(",|" + File.pathSeparator)
 
-    checkTempDirs(tmpDirs)
-
     val nettyConfig = if (localTaskManagerCommunication) {
       None
     } else {
@@ -1221,37 +1672,10 @@ object TaskManager {
     val syncOrAsync = configuration.getString(ConfigConstants.TASK_MANAGER_NETWORK_DEFAULT_IO_MODE,
       ConfigConstants.DEFAULT_TASK_MANAGER_NETWORK_DEFAULT_IO_MODE)
 
-    val ioMode : IOMode = if (syncOrAsync == "async") {
-      IOMode.ASYNC
-    }
-    else {
-      IOMode.SYNC
-    }
-
-    val networkConfig = NetworkEnvironmentConfiguration(numNetworkBuffers, pageSize, ioMode,
-      nettyConfig)
-
-    val networkBufferMem = numNetworkBuffers * pageSize
-
-    val memorySize = if (configuredMemory > 0) {
-      LOG.info("Using {} MB for Flink managed memory.", configuredMemory)
-      configuredMemory << 20 // megabytes to bytes
-    }
-    else {
-      val fraction = configuration.getFloat(ConfigConstants.TASK_MANAGER_MEMORY_FRACTION_KEY,
-        ConfigConstants.DEFAULT_MEMORY_MANAGER_MEMORY_FRACTION)
-      checkConfigParameter(fraction > 0.0f, fraction,
-        ConfigConstants.TASK_MANAGER_MEMORY_FRACTION_KEY,
-        "MemoryManager fraction of the free memory must be positive.")
-
-      val relativeMemSize = ((EnvironmentInformation.getSizeOfFreeHeapMemoryWithDefrag() -
-        networkBufferMem) * fraction).toLong
-
-      LOG.info("Using {} of the currently free heap space for Flink managed memory ({} MB).",
-        fraction, relativeMemSize >> 20)
+    val ioMode : IOMode = if (syncOrAsync == "async") IOMode.ASYNC else IOMode.SYNC
 
-      relativeMemSize
-    }
+    val networkConfig = NetworkEnvironmentConfiguration(numNetworkBuffers, pageSize,
+                                                        ioMode, nettyConfig)
 
     // ----> timeouts, library caching, profiling
 
@@ -1259,35 +1683,37 @@ object TaskManager {
       AkkaUtils.getTimeout(configuration)
     }
     catch {
-      case e: Exception => throw new Exception(
+      case e: Exception => throw new IllegalArgumentException(
         s"Invalid format for '${ConfigConstants.AKKA_ASK_TIMEOUT}'. " +
           s"Use formats like '50 s' or '1 min' to specify the timeout.")
     }
     LOG.info("Messages between TaskManager and JobManager have a max timeout of " + timeout)
 
-    val profilingInterval =
-      if (configuration.getBoolean(ProfilingUtils.ENABLE_PROFILING_KEY, false)) {
-        Some(configuration.getLong(ProfilingUtils.TASKMANAGER_REPORTINTERVAL_KEY,
-          ProfilingUtils.DEFAULT_TASKMANAGER_REPORTINTERVAL))
-      } else {
-        None
-      }
-
     val cleanupInterval = configuration.getLong(
       ConfigConstants.LIBRARY_CACHE_MANAGER_CLEANUP_INTERVAL,
       ConfigConstants.DEFAULT_LIBRARY_CACHE_MANAGER_CLEANUP_INTERVAL) * 1000
 
+    val finiteRegistratioDuration = try {
+      val maxRegistrationDuration = Duration(configuration.getString(
+        ConfigConstants.TASK_MANAGER_MAX_REGISTRATION_DURATION,
+        ConfigConstants.DEFAULT_TASK_MANAGER_MAX_REGISTRATION_DURATION))
 
+      if (maxRegistrationDuration.isFinite()) {
+        Some(maxRegistrationDuration.asInstanceOf[FiniteDuration])
+      } else {
+        None
+      }
+    } catch {
+      case e: NumberFormatException => throw new IllegalArgumentException(
+        "Invalid format for parameter " + ConfigConstants.TASK_MANAGER_MAX_REGISTRATION_DURATION,
+        e)
+    }
 
-    val maxRegistrationDuration = Duration(configuration.getString(
-      ConfigConstants.TASK_MANAGER_MAX_REGISTRATION_DURATION,
-      ConfigConstants.DEFAULT_TASK_MANAGER_MAX_REGISTRATION_DURATION))
-
-    val taskManagerConfig = TaskManagerConfiguration(slots, memorySize, pageSize,
-      tmpDirs, cleanupInterval, profilingInterval, timeout, maxRegistrationDuration,
+    val taskManagerConfig = TaskManagerConfiguration(tmpDirs, cleanupInterval, timeout,
+      finiteRegistratioDuration, slots,
       configuration)
 
-    (taskManagerConfig, networkConfig, connectionInfo, jobManagerActorURL)
+    (taskManagerConfig, networkConfig, connectionInfo)
   }
 
   /**
@@ -1318,30 +1744,59 @@ object TaskManager {
     (hostname, port)
   }
 
+
+  // --------------------------------------------------------------------------
+  //  Miscellaneous Utilities
+  // --------------------------------------------------------------------------
+
+  /**
+   * Validates a condition for a config parameter and displays a standard exception, if the
+   * the condition does not hold.
+   *
+   * @param condition The condition that must hold. If the condition is false, an
+   *                  exception is thrown.
+   * @param parameter The parameter value. Will be shown in the exception message.
+   * @param name The name of the config parameter. Will be shown in the exception message.
+   * @param errorMessage The optional custom error message to append to the exception message.
+   *
+   * @throws IllegalConfigurationException Thrown if the condition is violated.
+   */
+  @throws(classOf[IllegalConfigurationException])
   private def checkConfigParameter(condition: Boolean,
                                    parameter: Any,
                                    name: String,
                                    errorMessage: String = ""): Unit = {
     if (!condition) {
-      throw new Exception(
+      throw new IllegalConfigurationException(
         s"Invalid configuration value for '${name}' : ${parameter} - ${errorMessage}")
     }
   }
 
+  /**
+   * Validates that all the directories denoted by the strings do actually exist, are proper
+   * directories (not files), and are writable.
+   *
+   * @param tmpDirs The array of directory paths to check.
+   * @throws Exception Thrown if any of the directories doe not exist or is not writable
+   *                   or is a file, rather than a directory.
+   */
+  @throws(classOf[IOException])
   private def checkTempDirs(tmpDirs: Array[String]): Unit = {
     tmpDirs.zipWithIndex.foreach {
       case (dir: String, _) =>
         val file = new File(dir)
 
         if (!file.exists) {
-          throw new Exception(s"Temporary file directory ${file.getAbsolutePath} does not exist.")
+          throw new IOException(
+            s"Temporary file directory ${file.getAbsolutePath} does not exist.")
         }
         if (!file.isDirectory) {
-          throw new Exception(s"Temporary file directory ${file.getAbsolutePath} is not a " +
-            s"directory.")
+          throw new IOException(
+            s"Temporary file directory ${file.getAbsolutePath} is not a directory.")
         }
         if (!file.canWrite) {
-          throw new Exception(s"Temporary file directory ${file.getAbsolutePath} is not writable.")
+          throw new IOException(
+            s"Temporary file directory ${file.getAbsolutePath} is not writable.")
         }
 
         if (LOG.isInfoEnabled) {
@@ -1354,7 +1809,7 @@ object TaskManager {
           LOG.info(f"Temporary file directory '$path': total $totalSpaceGb GB, " +
             f"usable $usableSpaceGb GB ($usablePercentage%.2f%% usable)")
         }
-      case (_, id) => throw new Exception(s"Temporary file directory #$id is null.")
+      case (_, id) => throw new IllegalArgumentException(s"Temporary file directory #$id is null.")
     }
   }
 
@@ -1396,4 +1851,23 @@ object TaskManager {
 
     "Garbage collector stats: " + beans
   }
+
+  /**
+   * Creates the registry of default metrics, including stats about garbage collection, memory
+   * usage, and system CPU load.
+   *
+   * @return The registry with the default metrics.
+   */
+  private def createMetricsRegistry() : MetricRegistry = {
+    val metricRegistry = new MetricRegistry()
+
+    // register default metrics
+    metricRegistry.register("gc", new GarbageCollectorMetricSet)
+    metricRegistry.register("memory", new MemoryUsageGaugeSet)
+    metricRegistry.register("load", new Gauge[Double] {
+      override def getValue: Double =
+        ManagementFactory.getOperatingSystemMXBean().getSystemLoadAverage()
+    })
+    metricRegistry
+  }
 }
diff --git a/flink-runtime/src/main/scala/org/apache/flink/runtime/taskmanager/TaskManagerConfiguration.scala b/flink-runtime/src/main/scala/org/apache/flink/runtime/taskmanager/TaskManagerConfiguration.scala
index 19d55f7110c..05ede9962c4 100644
--- a/flink-runtime/src/main/scala/org/apache/flink/runtime/taskmanager/TaskManagerConfiguration.scala
+++ b/flink-runtime/src/main/scala/org/apache/flink/runtime/taskmanager/TaskManagerConfiguration.scala
@@ -20,13 +20,12 @@ package org.apache.flink.runtime.taskmanager
 
 import org.apache.flink.configuration.Configuration
 
-import scala.concurrent.duration.{Duration, FiniteDuration}
+import scala.concurrent.duration.FiniteDuration
 
-case class TaskManagerConfiguration(numberOfSlots: Int,
-                                    memorySize: Long, pageSize: Int,
-                                    tmpDirPaths: Array[String],
+
+case class TaskManagerConfiguration(tmpDirPaths: Array[String],
                                     cleanupInterval: Long,
-                                    profilingInterval: Option[Long],
                                     timeout: FiniteDuration,
-                                    maxRegistrationDuration: Duration,
+                                    maxRegistrationDuration: Option[FiniteDuration],
+                                    numberOfSlots: Int,
                                     configuration: Configuration)
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphTestUtils.java b/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphTestUtils.java
index 5dd10c0301c..7380b36bef7 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphTestUtils.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphTestUtils.java
@@ -43,8 +43,10 @@ import org.apache.flink.api.common.JobID;
 import org.apache.flink.runtime.jobgraph.JobStatus;
 import org.apache.flink.runtime.jobgraph.JobVertexID;
 import org.apache.flink.runtime.jobgraph.tasks.AbstractInvokable;
-import org.apache.flink.runtime.messages.TaskManagerMessages;
-import org.apache.flink.runtime.messages.TaskManagerMessages.TaskOperationResult;
+import org.apache.flink.runtime.messages.TaskMessages.SubmitTask;
+import org.apache.flink.runtime.messages.TaskMessages.FailIntermediateResultPartitions;
+import org.apache.flink.runtime.messages.TaskMessages.CancelTask;
+import org.apache.flink.runtime.messages.TaskMessages.TaskOperationResult;
 import org.mockito.Matchers;
 import org.mockito.invocation.InvocationOnMock;
 import org.mockito.stubbing.Answer;
@@ -113,16 +115,16 @@ public class ExecutionGraphTestUtils {
 		public TaskDeploymentDescriptor lastTDD;
 		@Override
 		public void onReceive(Object msg) throws Exception {
-			if (msg instanceof TaskManagerMessages.SubmitTask) {
-				TaskManagerMessages.SubmitTask submitTask = (TaskManagerMessages.SubmitTask) msg;
+			if (msg instanceof SubmitTask) {
+				SubmitTask submitTask = (SubmitTask) msg;
 				lastTDD = submitTask.tasks();
 
 				getSender().tell(new TaskOperationResult(submitTask.tasks().getExecutionId(), true), getSelf());
-			} else if (msg instanceof TaskManagerMessages.CancelTask) {
-				TaskManagerMessages.CancelTask cancelTask = (TaskManagerMessages.CancelTask) msg;
+			} else if (msg instanceof CancelTask) {
+				CancelTask cancelTask = (CancelTask) msg;
 				getSender().tell(new TaskOperationResult(cancelTask.attemptID(), true), getSelf());
 			}
-			else if (msg instanceof TaskManagerMessages.FailIntermediateResultPartitions) {
+			else if (msg instanceof FailIntermediateResultPartitions) {
 				getSender().tell(new Object(), getSelf());
 			}
 		}
@@ -133,13 +135,13 @@ public class ExecutionGraphTestUtils {
 	public static class SimpleFailingTaskManager extends UntypedActor {
 		@Override
 		public void onReceive(Object msg) throws Exception {
-			if (msg instanceof TaskManagerMessages.SubmitTask) {
-				TaskManagerMessages.SubmitTask submitTask = (TaskManagerMessages.SubmitTask) msg;
+			if (msg instanceof SubmitTask) {
+				SubmitTask submitTask = (SubmitTask) msg;
 
 				getSender().tell(new TaskOperationResult(submitTask.tasks().getExecutionId(),
 						false, ERROR_MESSAGE),	getSelf());
-			} else if (msg instanceof TaskManagerMessages.CancelTask) {
-				TaskManagerMessages.CancelTask cancelTask = (TaskManagerMessages.CancelTask) msg;
+			} else if (msg instanceof CancelTask) {
+				CancelTask cancelTask = (CancelTask) msg;
 				getSender().tell(new TaskOperationResult(cancelTask.attemptID(), true), getSelf());
 			}
 		}
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionVertexCancelTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionVertexCancelTest.java
index e19935321ae..226b256c4ae 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionVertexCancelTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionVertexCancelTest.java
@@ -42,8 +42,8 @@ import org.apache.flink.runtime.instance.Instance;
 import org.apache.flink.api.common.JobID;
 import org.apache.flink.runtime.jobgraph.JobVertexID;
 import org.apache.flink.runtime.jobmanager.scheduler.Scheduler;
-import org.apache.flink.runtime.messages.TaskManagerMessages;
-import org.apache.flink.runtime.messages.TaskManagerMessages.TaskOperationResult;
+import org.apache.flink.runtime.messages.TaskMessages;
+import org.apache.flink.runtime.messages.TaskMessages.TaskOperationResult;
 import org.apache.flink.runtime.testingUtils.TestingUtils;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
@@ -644,10 +644,10 @@ public class ExecutionVertexCancelTest {
 
 		@Override
 		public void onReceive(Object message) throws Exception {
-			if(message instanceof TaskManagerMessages.SubmitTask){
-				TaskDeploymentDescriptor tdd = ((TaskManagerMessages.SubmitTask) message).tasks();
+			if(message instanceof TaskMessages.SubmitTask){
+				TaskDeploymentDescriptor tdd = ((TaskMessages.SubmitTask) message).tasks();
 				getSender().tell(new TaskOperationResult(tdd.getExecutionId(), true), getSelf());
-			}else if(message instanceof TaskManagerMessages.CancelTask){
+			}else if(message instanceof TaskMessages.CancelTask){
 				index++;
 				if(index >= results.length){
 					getSender().tell(new Status.Failure(new IOException("RPC call failed.")), getSelf());
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionVertexDeploymentTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionVertexDeploymentTest.java
index 5713c1080bb..7b72669a7c4 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionVertexDeploymentTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionVertexDeploymentTest.java
@@ -33,7 +33,7 @@ import org.apache.flink.runtime.instance.Instance;
 import org.apache.flink.runtime.instance.SimpleSlot;
 import org.apache.flink.api.common.JobID;
 import org.apache.flink.runtime.jobgraph.JobVertexID;
-import org.apache.flink.runtime.messages.TaskManagerMessages.TaskOperationResult;
+import org.apache.flink.runtime.messages.TaskMessages.TaskOperationResult;
 import org.apache.flink.runtime.testingUtils.TestingUtils;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/RegistrationTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/RegistrationTest.java
new file mode 100644
index 00000000000..1b4f5f38ba4
--- /dev/null
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/RegistrationTest.java
@@ -0,0 +1,379 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.taskmanager;
+
+import akka.actor.ActorRef;
+import akka.actor.ActorSystem;
+import akka.actor.Kill;
+import akka.actor.Props;
+import akka.actor.UntypedActor;
+import akka.pattern.Patterns;
+import akka.testkit.JavaTestKit;
+import org.apache.flink.configuration.ConfigConstants;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.runtime.akka.AkkaUtils;
+import org.apache.flink.runtime.instance.InstanceID;
+import org.apache.flink.runtime.jobmanager.JobManager;
+import org.apache.flink.runtime.messages.JobManagerMessages;
+import org.apache.flink.runtime.messages.RegistrationMessages.AcknowledgeRegistration;
+import org.apache.flink.runtime.messages.RegistrationMessages.RegisterTaskManager;
+import org.apache.flink.runtime.messages.RegistrationMessages.RefuseRegistration;
+import org.apache.flink.runtime.messages.TaskManagerMessages;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import scala.Option;
+import scala.Some;
+import scala.concurrent.Await;
+import scala.concurrent.Future;
+import scala.concurrent.duration.FiniteDuration;
+
+import java.util.concurrent.TimeUnit;
+
+import static org.junit.Assert.*;
+
+/**
+ * The tests in this class verify the behavior of the TaskManager
+ * when connecting to the JobManager, and when the JobManager
+ * is unreachable.
+ */
+public class RegistrationTest {
+
+	private static final Option<String> NONE_STRING = Option.empty();
+
+	// use one actor system throughout all tests
+	private static ActorSystem actorSystem;
+
+	@BeforeClass
+	public static void startActorSystem() {
+		Configuration config = new Configuration();
+		config.getString(ConfigConstants.AKKA_ASK_TIMEOUT, "5 s");
+		config.getString(ConfigConstants.AKKA_WATCH_HEARTBEAT_INTERVAL, "200 ms");
+		config.getString(ConfigConstants.AKKA_WATCH_HEARTBEAT_PAUSE, "2 s");
+		config.getDouble(ConfigConstants.AKKA_WATCH_THRESHOLD, 2.0);
+
+		actorSystem = AkkaUtils.createLocalActorSystem(new Configuration());
+	}
+
+	@AfterClass
+	public static void shutdownActorSystem() {
+		if (actorSystem != null) {
+			actorSystem.shutdown();
+		}
+	}
+
+	/**
+	 * A test that verifies that two TaskManagers correctly register at the
+	 * JobManager.
+	 */
+	@Test
+	public void testSimpleRegistration() {
+		new JavaTestKit(actorSystem) {{
+			try {
+				// a simple JobManager
+				ActorRef jobManager = startJobManager();
+
+				// start two TaskManagers. it will automatically try to register
+				final ActorRef taskManager1 = startTaskManager(jobManager);
+				final ActorRef taskManager2 = startTaskManager(jobManager);
+
+				// check that the TaskManagers are registered
+				Future<Object> responseFuture1 = Patterns.ask(
+						taskManager1,
+						TaskManagerMessages.getNotifyWhenRegisteredAtJobManagerMessage(),
+						5000);
+
+				Future<Object> responseFuture2 = Patterns.ask(
+						taskManager2,
+						TaskManagerMessages.getNotifyWhenRegisteredAtJobManagerMessage(),
+						5000);
+
+				Object response1 = Await.result(responseFuture1, new FiniteDuration(5, TimeUnit.SECONDS));
+				Object response2 = Await.result(responseFuture2, new FiniteDuration(5, TimeUnit.SECONDS));
+
+				// this is a hack to work around the way Java can interact with scala case objects
+				Class<?> confirmClass = TaskManagerMessages.getRegisteredAtJobManagerMessage().getClass();
+				assertTrue(response1 != null && confirmClass.isAssignableFrom(response1.getClass()));
+				assertTrue(response2 != null && confirmClass.isAssignableFrom(response2.getClass()));
+
+				// check that the JobManager has 2 TaskManagers registered
+				Future<Object> numTaskManagersFuture = Patterns.ask(
+						jobManager,
+						JobManagerMessages.getRequestNumberRegisteredTaskManager(),
+						1000);
+
+				Integer count = (Integer) Await.result(numTaskManagersFuture, new FiniteDuration(1, TimeUnit.SECONDS));
+				assertEquals(2, count.intValue());
+
+				stopActor(taskManager1);
+				stopActor(taskManager2);
+				stopActor(jobManager);
+			}
+			catch (Exception e) {
+				e.printStackTrace();
+				fail(e.getMessage());
+			}
+		}};
+	}
+
+	/**
+	 * A test that verifies that two TaskManagers correctly register at the
+	 * JobManager.
+	 */
+	@Test
+	public void testDelayedRegistration() {
+		new JavaTestKit(actorSystem) {{
+			try {
+				// start a TaskManager that tries to register at the JobManager before the JobManager is
+				// available. we give it the regular JobManager akka URL
+				final ActorRef taskManager = startTaskManager(JobManager.getLocalJobManagerAkkaURL(),
+																new Configuration());
+				// let it try for a bit
+				Thread.sleep(6000);
+
+				// now start the JobManager, with the regular akka URL
+				final ActorRef jobManager = JobManager.startJobManagerActors(new Configuration(), actorSystem)._1();
+
+				// check that the TaskManagers are registered
+				Future<Object> responseFuture = Patterns.ask(
+						taskManager,
+						TaskManagerMessages.getNotifyWhenRegisteredAtJobManagerMessage(),
+						30000);
+
+				Object response = Await.result(responseFuture, new FiniteDuration(30, TimeUnit.SECONDS));
+
+				// this is a hack to work around the way Java can interact with scala case objects
+				Class<?> confirmClass = TaskManagerMessages.getRegisteredAtJobManagerMessage().getClass();
+				assertTrue(response != null && confirmClass.isAssignableFrom(response.getClass()));
+
+				stopActor(taskManager);
+				stopActor(jobManager);
+			}
+			catch (Exception e) {
+				e.printStackTrace();
+				fail(e.getMessage());
+			}
+		}};
+	}
+
+	/**
+	 * Tests that the TaskManager shuts down when it cannot register at the
+	 * JobManager within the given maximum duration.
+	 *
+	 * Unfortunately, this test does not give good error messages.
+	 * (I have not figured out how to get any better message out of the
+	 * Akka TestKit than "ask timeout exception".)
+	 *
+	 * Anyways: An "ask timeout exception" here means that the TaskManager
+	 * did not shut down after its registration timeout expired.
+	 */
+	@Test
+	public void testShutdownAfterRegistrationDurationExpired() {
+		new JavaTestKit(actorSystem) {{
+			try {
+				// registration timeout of 1 second
+				Configuration tmConfig = new Configuration();
+				tmConfig.setString(ConfigConstants.TASK_MANAGER_MAX_REGISTRATION_DURATION, "500 ms");
+
+				// start the taskManager actor
+				final ActorRef taskManager = startTaskManager(JobManager.getLocalJobManagerAkkaURL(), tmConfig);
+
+				// make sure it terminates in time, since it cannot register at a JobManager
+				watch(taskManager);
+				new Within(new FiniteDuration(10, TimeUnit.SECONDS)) {
+
+					@Override
+					protected void run() {
+						expectTerminated(taskManager);
+					}
+				};
+
+				stopActor(taskManager);
+			}
+			catch (Throwable e) {
+				e.printStackTrace();
+				fail(e.getMessage());
+			}
+		}};
+	}
+
+	/**
+	 * Make sure that the TaskManager keeps trying to register, even after
+	 * registration attempts have been refused.
+	 */
+	@Test
+	public void testTaskManagerResumesConnectAfterRefusedRegistration() {
+		new JavaTestKit(actorSystem) {{
+			try {
+				// we make the test actor (the test kit) the JobManager to intercept
+				// the messages
+				final ActorRef taskManager = startTaskManager(getTestActor());
+
+				// check and decline initial registration
+				new Within(new FiniteDuration(2, TimeUnit.SECONDS)) {
+
+					@Override
+					protected void run() {
+						// the TaskManager should try to register
+						expectMsgClass(RegisterTaskManager.class);
+
+						// we decline the registration
+						getLastSender().tell(new RefuseRegistration("test reason"), getTestActor());
+					}
+				};
+
+				// the TaskManager should wait a bit an retry...
+				FiniteDuration maxDelay = (FiniteDuration) TaskManager.DELAY_AFTER_REFUSED_REGISTRATION().$times(2.0);
+				new Within(maxDelay) {
+
+					@Override
+					protected void run() {
+						expectMsgClass(RegisterTaskManager.class);
+					}
+				};
+
+				stopActor(taskManager);
+			}
+			catch (Throwable e) {
+				e.printStackTrace();
+				fail(e.getMessage());
+			}
+		}};
+	}
+
+	/**
+	 * Validate that the TaskManager attempts to re-connect after it lost the connection
+	 * to the JobManager.
+	 */
+	@Test
+	public void testTaskManagerResumesConnectAfterJobManagerFailure() {
+		new JavaTestKit(actorSystem) {{
+			try {
+				final Props fakeJmProps = Props.create(ForwardingActor.class, getTestActor());
+				final String jobManagerName = "FAKE_JOB_MANAGER";
+
+				final ActorRef fakeJobManager1 = actorSystem.actorOf(fakeJmProps, jobManagerName);
+
+
+				// we make the test actor (the test kit) the JobManager to intercept
+				// the messages
+				final ActorRef taskManager = startTaskManager(fakeJobManager1);
+
+				// validate initial registration
+				new Within(new FiniteDuration(2, TimeUnit.SECONDS)) {
+
+					@Override
+					protected void run() {
+						// the TaskManager should try to register
+						expectMsgClass(RegisterTaskManager.class);
+
+						// we accept the registration
+						taskManager.tell(new AcknowledgeRegistration(fakeJobManager1, new InstanceID(), 45234),
+										fakeJobManager1);
+					}
+				};
+
+				// kill the first forwarding JobManager
+				watch(fakeJobManager1);
+				stopActor(fakeJobManager1);
+
+				// wait for the killing to be completed
+				new Within(new FiniteDuration(2, TimeUnit.SECONDS)) {
+
+					@Override
+					protected void run() {
+						expectTerminated(fakeJobManager1);
+					}
+				};
+
+				// now start the second fake JobManager and expect that
+				// the TaskManager registers again
+				// the second fake JM needs to have the same actor URL
+				final ActorRef fakeJobManager2 = actorSystem.actorOf(fakeJmProps, jobManagerName);
+
+				// expect the next registration
+				new Within(new FiniteDuration(10, TimeUnit.SECONDS)) {
+
+					@Override
+					protected void run() {
+						expectMsgClass(RegisterTaskManager.class);
+
+						// we accept the registration
+						taskManager.tell(new AcknowledgeRegistration(fakeJobManager2, new InstanceID(), 45234),
+										fakeJobManager2);
+					}
+				};
+
+				stopActor(taskManager);
+				stopActor(fakeJobManager2);
+			}
+			catch (Throwable e) {
+				e.printStackTrace();
+				fail(e.getMessage());
+			}
+		}};
+	}
+
+	// --------------------------------------------------------------------------------------------
+	//  Utility Functions
+	// --------------------------------------------------------------------------------------------
+
+	private static ActorRef startJobManager() throws Exception {
+		// start the actors. don't give names, so they get generated names and we
+		// avoid conflicts with the actor names
+		return JobManager.startJobManagerActors(new Configuration(), actorSystem, NONE_STRING, NONE_STRING)._1();
+	}
+
+	private static ActorRef startTaskManager(ActorRef jobManager) throws Exception {
+		return startTaskManager(jobManager.path().toString(), new Configuration());
+	}
+
+	private static ActorRef startTaskManager(String jobManagerUrl, Configuration config) throws Exception {
+		config.setInteger(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, 1);
+
+		return TaskManager.startTaskManagerComponentsAndActor(
+				config, actorSystem, "localhost",
+				NONE_STRING, // no actor name -> random
+				new Some<String>(jobManagerUrl), // job manager path
+				true, // local network stack only
+				TaskManager.class);
+	}
+
+	private static void stopActor(ActorRef actor) {
+		actor.tell(Kill.getInstance(), ActorRef.noSender());
+	}
+
+	// --------------------------------------------------------------------------------------------
+	//  Utility Actor that only forwards messages
+	// --------------------------------------------------------------------------------------------
+
+	public static class ForwardingActor extends UntypedActor {
+
+		private final ActorRef target;
+
+		public ForwardingActor(ActorRef target) {
+			this.target = target;
+		}
+
+		@Override
+		public void onReceive(Object message) throws Exception {
+			target.forward(message, context());
+		}
+	}
+}
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerComponentsStartupShutdownTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerComponentsStartupShutdownTest.java
new file mode 100644
index 00000000000..131549b045c
--- /dev/null
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerComponentsStartupShutdownTest.java
@@ -0,0 +1,141 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.taskmanager;
+
+import static org.junit.Assert.*;
+
+import akka.actor.ActorRef;
+import akka.actor.ActorSystem;
+import akka.actor.Kill;
+import akka.actor.Props;
+import akka.testkit.JavaTestKit;
+import org.apache.flink.configuration.ConfigConstants;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.runtime.akka.AkkaUtils;
+import org.apache.flink.runtime.instance.InstanceConnectionInfo;
+import org.apache.flink.runtime.io.disk.iomanager.IOManager;
+import org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync;
+import org.apache.flink.runtime.io.network.NetworkEnvironment;
+import org.apache.flink.runtime.io.network.netty.NettyConfig;
+import org.apache.flink.runtime.jobmanager.JobManager;
+import org.apache.flink.runtime.memorymanager.DefaultMemoryManager;
+import org.apache.flink.runtime.memorymanager.MemoryManager;
+
+import org.apache.flink.runtime.messages.TaskManagerMessages;
+import org.junit.Test;
+import scala.Option;
+import scala.concurrent.duration.FiniteDuration;
+
+import java.net.InetAddress;
+import java.util.concurrent.TimeUnit;
+
+public class TaskManagerComponentsStartupShutdownTest {
+
+	/**
+	 * Makes sure that all components are shut down when the TaskManager
+	 * actor is shut down.
+	 */
+	@Test
+	public void testComponentsStartupShutdown() {
+
+		final String[] TMP_DIR = new String[] { ConfigConstants.DEFAULT_TASK_MANAGER_TMP_PATH };
+		final FiniteDuration timeout = new FiniteDuration(100, TimeUnit.SECONDS);
+		final int BUFFER_SIZE = 32 * 1024;
+
+		Configuration config = new Configuration();
+		config.setString(ConfigConstants.AKKA_WATCH_HEARTBEAT_INTERVAL, "200 ms");
+		config.setString(ConfigConstants.AKKA_WATCH_HEARTBEAT_PAUSE, "1 s");
+		config.setInteger(ConfigConstants.AKKA_WATCH_THRESHOLD, 1);
+
+		ActorSystem actorSystem = null;
+		try {
+			actorSystem = AkkaUtils.createLocalActorSystem(config);
+
+			final ActorRef jobManager = JobManager.startJobManagerActors(config, actorSystem)._1();
+
+			// create the components for the TaskManager manually
+			final TaskManagerConfiguration tmConfig = new TaskManagerConfiguration(
+					TMP_DIR,
+					1000000,
+					timeout,
+					Option.<FiniteDuration>empty(),
+					1,
+					config);
+
+			final NetworkEnvironmentConfiguration netConf = new NetworkEnvironmentConfiguration(
+					32, BUFFER_SIZE, IOManager.IOMode.SYNC, Option.<NettyConfig>empty());
+
+			final InstanceConnectionInfo connectionInfo = new InstanceConnectionInfo(InetAddress.getLocalHost(), 10000);
+
+			final MemoryManager memManager = new DefaultMemoryManager(32 * BUFFER_SIZE, 1, BUFFER_SIZE);
+			final IOManager ioManager = new IOManagerAsync(TMP_DIR);
+			final NetworkEnvironment network = new NetworkEnvironment(timeout, netConf);
+			final int numberOfSlots = 1;
+
+			// create the task manager
+			final Props tmProps = Props.create(TaskManager.class,
+					tmConfig, connectionInfo, jobManager.path().toString(),
+					memManager, ioManager, network, numberOfSlots);
+
+			final ActorRef taskManager = actorSystem.actorOf(tmProps);
+
+			new JavaTestKit(actorSystem) {{
+
+				// wait for the TaskManager to be registered
+				new Within(new FiniteDuration(5000, TimeUnit.SECONDS)) {
+					@Override
+					protected void run() {
+						taskManager.tell(TaskManagerMessages.getNotifyWhenRegisteredAtJobManagerMessage(),
+								getTestActor());
+
+						expectMsgEquals(TaskManagerMessages.getRegisteredAtJobManagerMessage());
+					}
+				};
+			}};
+
+			// the components should now all be initialized
+			assertTrue(network.isAssociated());
+
+			// shut down all actors and the actor system
+			// Kill the Task down the JobManager
+			taskManager.tell(Kill.getInstance(), ActorRef.noSender());
+			jobManager.tell(Kill.getInstance(), ActorRef.noSender());
+
+			// shut down the actors and the actor system
+			actorSystem.shutdown();
+			actorSystem.awaitTermination();
+			actorSystem = null;
+
+			// now that the TaskManager is shut down, the components should be shut down as well
+			assertFalse(network.isAssociated());
+			assertTrue(network.isShutdown());
+			assertTrue(ioManager.isProperlyShutDown());
+			assertTrue(memManager.isShutdown());
+		}
+		catch (Exception e) {
+			e.printStackTrace();
+			fail(e.getMessage());
+		}
+		finally {
+			if (actorSystem != null) {
+				actorSystem.shutdown();
+			}
+		}
+	}
+}
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerConfigurationTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerConfigurationTest.java
new file mode 100644
index 00000000000..d243fbfdc18
--- /dev/null
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerConfigurationTest.java
@@ -0,0 +1,140 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.taskmanager;
+
+import org.apache.flink.configuration.ConfigConstants;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.configuration.IllegalConfigurationException;
+import org.junit.Test;
+
+import scala.Tuple2;
+
+import java.io.IOException;
+import java.net.InetAddress;
+import java.net.ServerSocket;
+import java.net.UnknownHostException;
+
+import static org.junit.Assert.*;
+
+/**
+ * Validates that the TaskManager startup properly obeys the configuration
+ * values.
+ */
+public class TaskManagerConfigurationTest {
+
+	@Test
+	public void testUsePreconfiguredNetworkInterface() {
+		try {
+			final String TEST_HOST_NAME = "testhostname";
+
+			Configuration config = new Configuration();
+			config.setString(ConfigConstants.TASK_MANAGER_HOSTNAME_KEY, TEST_HOST_NAME);
+
+			Tuple2<String, Object> address = TaskManager.selectNetworkInterfaceAndPort(config, "localhost", 7891);
+
+			// validate the configured test host name
+			assertEquals(TEST_HOST_NAME, address._1());
+		}
+		catch (Exception e) {
+			e.printStackTrace();
+			fail(e.getMessage());
+		}
+	}
+
+	@Test
+	public void testActorSystemPortConfig() {
+		try {
+			// config with pre-configured hostname to speed up tests (no interface selection)
+			Configuration config = new Configuration();
+			config.setString(ConfigConstants.TASK_MANAGER_HOSTNAME_KEY, "localhost");
+
+			// auto port
+			assertEquals(0, TaskManager.selectNetworkInterfaceAndPort(config, "localhost", 7891)._2());
+
+			// pre-defined port
+			final int testPort = 22551;
+			config.setInteger(ConfigConstants.TASK_MANAGER_IPC_PORT_KEY, testPort);
+			assertEquals(testPort, TaskManager.selectNetworkInterfaceAndPort(config, "localhost", 7891)._2());
+
+			// invalid port
+			try {
+				config.setInteger(ConfigConstants.TASK_MANAGER_IPC_PORT_KEY, -1);
+				TaskManager.selectNetworkInterfaceAndPort(config, "localhost", 7891);
+				fail("should fail with an exception");
+			}
+			catch (IllegalConfigurationException e) {
+				// bam!
+			}
+
+			// invalid port
+			try {
+				config.setInteger(ConfigConstants.TASK_MANAGER_IPC_PORT_KEY, 100000);
+				TaskManager.selectNetworkInterfaceAndPort(config, "localhost", 7891);
+				fail("should fail with an exception");
+			}
+			catch (IllegalConfigurationException e) {
+				// bam!
+			}
+		}
+		catch (Exception e) {
+			e.printStackTrace();
+			fail(e.getMessage());
+		}
+	}
+
+	@Test
+	public void testNetworkInterfaceSelection() {
+		ServerSocket server;
+		String hostname = "localhost";
+
+		try {
+			InetAddress localhostAddress = InetAddress.getByName(hostname);
+			server = new ServerSocket(0, 50, localhostAddress);
+		}
+		catch (UnknownHostException e) {
+			// may happen if disconnected. skip test.
+			System.err.println("Skipping 'testNetworkInterfaceSelection' test.");
+			return;
+		}
+		catch (IOException e) {
+			// may happen in certain test setups, skip test.
+			System.err.println("Skipping 'testNetworkInterfaceSelection' test.");
+			return;
+		}
+
+		try {
+			// open a server port to allow the system to connect
+			Configuration config = new Configuration();
+
+			assertNotNull(TaskManager.selectNetworkInterfaceAndPort(config, hostname, server.getLocalPort())._1());
+		}
+		catch (Exception e) {
+			e.printStackTrace();
+			fail(e.getMessage());
+		}
+		finally {
+			try {
+				server.close();
+			} catch (IOException e) {
+				// ignore shutdown errors
+			}
+		}
+	}
+
+}
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerProcessReapingTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerProcessReapingTest.java
index f4ee52fa68e..3e659168384 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerProcessReapingTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerProcessReapingTest.java
@@ -139,7 +139,7 @@ public class TaskManagerProcessReapingTest {
 			// wait for max 5 seconds for the process to terminate
 			{
 				long now = System.currentTimeMillis();
-				long deadline = now + 5000;
+				long deadline = now + 10000;
 
 				while (now < deadline && isProcessAlive(taskManagerProcess)) {
 					Thread.sleep(100);
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerTest.java
index 640ccc3f471..e736a5549d5 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerTest.java
@@ -20,6 +20,7 @@ package org.apache.flink.runtime.taskmanager;
 
 import akka.actor.ActorRef;
 import akka.actor.ActorSystem;
+import akka.actor.Kill;
 import akka.actor.Props;
 import akka.actor.UntypedActor;
 import akka.japi.Creator;
@@ -48,14 +49,17 @@ import org.apache.flink.runtime.jobmanager.Tasks;
 import org.apache.flink.runtime.messages.JobManagerMessages;
 import org.apache.flink.runtime.messages.RegistrationMessages;
 import org.apache.flink.runtime.messages.TaskManagerMessages;
-import org.apache.flink.runtime.messages.TaskManagerMessages.CancelTask;
-import org.apache.flink.runtime.messages.TaskManagerMessages.SubmitTask;
-import org.apache.flink.runtime.messages.TaskManagerMessages.TaskOperationResult;
+import org.apache.flink.runtime.messages.TaskMessages;
+import org.apache.flink.runtime.messages.TaskMessages.CancelTask;
+import org.apache.flink.runtime.messages.TaskMessages.SubmitTask;
+import org.apache.flink.runtime.messages.TaskMessages.TaskOperationResult;
+import org.apache.flink.runtime.testingUtils.TestingTaskManager;
 import org.apache.flink.runtime.testingUtils.TestingTaskManagerMessages;
 import org.apache.flink.runtime.testingUtils.TestingUtils;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 import org.junit.Test;
+
 import scala.Option;
 import scala.concurrent.Await;
 import scala.concurrent.Future;
@@ -93,10 +97,12 @@ public class TaskManagerTest {
 	@Test
 	public void testSetupTaskManager() {
 		new JavaTestKit(system){{
+			ActorRef jobManager = null;
+			ActorRef taskManager = null;
 			try {
-				ActorRef jobManager = system.actorOf(Props.create(SimpleJobManager.class));
+				jobManager = system.actorOf(Props.create(SimpleJobManager.class));
 
-				final ActorRef tm = createTaskManager(jobManager);
+				taskManager = createTaskManager(jobManager);
 
 				JobID jid = new JobID();
 				JobVertexID vid = new JobVertexID();
@@ -108,11 +114,12 @@ public class TaskManagerTest {
 						Collections.<InputGateDeploymentDescriptor>emptyList(),
 					new ArrayList<BlobKey>(), 0);
 
-				new Within(duration("1 seconds")){
+				final ActorRef tmClosure = taskManager;
+				new Within(duration("2 seconds")) {
 
 					@Override
 					protected void run() {
-						tm.tell(new SubmitTask(tdd), getRef());
+						tmClosure.tell(new SubmitTask(tdd), getRef());
 						expectMsgEquals(new TaskOperationResult(eid, true));
 					}
 				};
@@ -121,16 +128,27 @@ public class TaskManagerTest {
 				e.printStackTrace();
 				fail(e.getMessage());
 			}
+			finally {
+				// shut down the actors
+				if (taskManager != null) {
+					taskManager.tell(Kill.getInstance(), ActorRef.noSender());
+				}
+				if (jobManager != null) {
+					jobManager.tell(Kill.getInstance(), ActorRef.noSender());
+				}
+			}
 		}};
 	}
 	
 	@Test
 	public void testJobSubmissionAndCanceling() {
 		new JavaTestKit(system){{
-			try {
 
-				ActorRef jobManager = system.actorOf(Props.create(SimpleJobManager.class));
-				final ActorRef tm = createTaskManager(jobManager);
+			ActorRef jobManager = null;
+			ActorRef taskManager = null;
+			try {
+				jobManager = system.actorOf(Props.create(SimpleJobManager.class));
+				taskManager = createTaskManager(jobManager);
 
 				final JobID jid1 = new JobID();
 				final JobID jid2 = new JobID();
@@ -153,6 +171,7 @@ public class TaskManagerTest {
 						Collections.<InputGateDeploymentDescriptor>emptyList(),
 					new ArrayList<BlobKey>(), 0);
 
+				final ActorRef tm = taskManager;
 				final FiniteDuration d = duration("1 second");
 
 				new Within(d) {
@@ -220,21 +239,34 @@ public class TaskManagerTest {
 						}
 					}
 				};
-			}catch(Exception e){
+			}
+			catch(Exception e) {
 				e.printStackTrace();
 				fail(e.getMessage());
 			}
-
+			finally {
+				// shut down the actors
+				if (taskManager != null) {
+					taskManager.tell(Kill.getInstance(), ActorRef.noSender());
+				}
+				if (jobManager != null) {
+					jobManager.tell(Kill.getInstance(), ActorRef.noSender());
+				}
+			}
 		}};
 	}
 	
 	@Test
 	public void testGateChannelEdgeMismatch() {
 		new JavaTestKit(system){{
+
+			ActorRef jobManager = null;
+			ActorRef taskManager = null;
 			try {
-				ActorRef jm = system.actorOf(Props.create(SimpleJobManager.class));
+				jobManager = system.actorOf(Props.create(SimpleJobManager.class));
 
-				final ActorRef tm = createTaskManager(jm);
+				taskManager = createTaskManager(jobManager);
+				final ActorRef tm = taskManager;
 
 				final JobID jid = new JobID();
 
@@ -260,127 +292,152 @@ public class TaskManagerTest {
 
 					@Override
 					protected void run() {
-						try {
-							tm.tell(new SubmitTask(tdd1), getRef());
-							tm.tell(new SubmitTask(tdd2), getRef());
-							TaskOperationResult result = expectMsgClass(TaskOperationResult.class);
-							assertFalse(result.success());
-							assertEquals(eid1, result.executionID());
+						tm.tell(new SubmitTask(tdd1), getRef());
+						tm.tell(new SubmitTask(tdd2), getRef());
 
-							result = expectMsgClass(TaskOperationResult.class);
-							assertFalse(result.success());
-							assertEquals(eid2, result.executionID());
+						TaskOperationResult result = expectMsgClass(TaskOperationResult.class);
+						assertFalse(result.success());
+						assertEquals(eid1, result.executionID());
 
-							tm.tell(new TestingTaskManagerMessages.NotifyWhenTaskRemoved(eid1),
-									getRef());
-							tm.tell(new TestingTaskManagerMessages.NotifyWhenTaskRemoved(eid2),
-									getRef());
+						result = expectMsgClass(TaskOperationResult.class);
+						assertFalse(result.success());
+						assertEquals(eid2, result.executionID());
 
-							expectMsgEquals(true);
-							expectMsgEquals(true);
+						tm.tell(new TestingTaskManagerMessages.NotifyWhenTaskRemoved(eid1),
+								getRef());
+						tm.tell(new TestingTaskManagerMessages.NotifyWhenTaskRemoved(eid2),
+								getRef());
 
-							tm.tell(TestingTaskManagerMessages.getRequestRunningTasksMessage(), getRef());
-							Map<ExecutionAttemptID, Task> tasks = expectMsgClass(TestingTaskManagerMessages
-									.ResponseRunningTasks.class).asJava();
+						expectMsgEquals(true);
+						expectMsgEquals(true);
 
-							assertEquals(0, tasks.size());
-						}catch (Exception e) {
-							e.printStackTrace();
-							fail(e.getMessage());
-						}
+						tm.tell(TestingTaskManagerMessages.getRequestRunningTasksMessage(), getRef());
+						Map<ExecutionAttemptID, Task> tasks = expectMsgClass(TestingTaskManagerMessages
+								.ResponseRunningTasks.class).asJava();
+
+						assertEquals(0, tasks.size());
 					}
 				};
-			}catch (Exception e) {
+			}
+			catch (Exception e) {
 				e.printStackTrace();
 				fail(e.getMessage());
 			}
+			finally {
+				// shut down the actors
+				if (taskManager != null) {
+					taskManager.tell(Kill.getInstance(), ActorRef.noSender());
+				}
+				if (jobManager != null) {
+					jobManager.tell(Kill.getInstance(), ActorRef.noSender());
+				}
+			}
 		}};
 	}
 	
 	@Test
 	public void testRunJobWithForwardChannel() {
 		new JavaTestKit(system){{
-			final JobID jid = new JobID();
 
-			JobVertexID vid1 = new JobVertexID();
-			JobVertexID vid2 = new JobVertexID();
-
-			final ExecutionAttemptID eid1 = new ExecutionAttemptID();
-			final ExecutionAttemptID eid2 = new ExecutionAttemptID();
+			ActorRef jobManager = null;
+			ActorRef taskManager = null;
+			try {
+				final JobID jid = new JobID();
 
-			ActorRef jm = system.actorOf(Props.create(new SimpleLookupJobManagerCreator()));
-			final ActorRef tm = createTaskManager(jm);
+				JobVertexID vid1 = new JobVertexID();
+				JobVertexID vid2 = new JobVertexID();
 
-			IntermediateResultPartitionID partitionId = new IntermediateResultPartitionID();
+				final ExecutionAttemptID eid1 = new ExecutionAttemptID();
+				final ExecutionAttemptID eid2 = new ExecutionAttemptID();
 
-			List<ResultPartitionDeploymentDescriptor> irpdd = new ArrayList<ResultPartitionDeploymentDescriptor>();
-			irpdd.add(new ResultPartitionDeploymentDescriptor(new IntermediateDataSetID(), partitionId, ResultPartitionType.PIPELINED, 1));
+				jobManager = system.actorOf(Props.create(new SimpleLookupJobManagerCreator()));
 
-			InputGateDeploymentDescriptor ircdd =
-					new InputGateDeploymentDescriptor(
-							new IntermediateDataSetID(),
-							0, new InputChannelDeploymentDescriptor[]{
-									new InputChannelDeploymentDescriptor(new ResultPartitionID(partitionId, eid1), ResultPartitionLocation.createLocal())
-							}
-					);
+				taskManager = createTaskManager(jobManager);
+				final ActorRef tm = taskManager;
 
-			final TaskDeploymentDescriptor tdd1 = new TaskDeploymentDescriptor(jid, vid1, eid1, "Sender", 0, 1,
-					new Configuration(), new Configuration(), Tasks.Sender.class.getName(),
-					irpdd, Collections.<InputGateDeploymentDescriptor>emptyList(), new ArrayList<BlobKey>(), 0);
+				IntermediateResultPartitionID partitionId = new IntermediateResultPartitionID();
 
-			final TaskDeploymentDescriptor tdd2 = new TaskDeploymentDescriptor(jid, vid2, eid2, "Receiver", 2, 7,
-					new Configuration(), new Configuration(), Tasks.Receiver.class.getName(),
-					Collections.<ResultPartitionDeploymentDescriptor>emptyList(),
-					Collections.singletonList(ircdd),
-					new ArrayList<BlobKey>(), 0);
+				List<ResultPartitionDeploymentDescriptor> irpdd = new ArrayList<ResultPartitionDeploymentDescriptor>();
+				irpdd.add(new ResultPartitionDeploymentDescriptor(new IntermediateDataSetID(), partitionId, ResultPartitionType.PIPELINED, 1));
 
-			final FiniteDuration d = duration("1 second");
+				InputGateDeploymentDescriptor ircdd =
+						new InputGateDeploymentDescriptor(
+								new IntermediateDataSetID(),
+								0, new InputChannelDeploymentDescriptor[]{
+										new InputChannelDeploymentDescriptor(new ResultPartitionID(partitionId, eid1), ResultPartitionLocation.createLocal())
+								}
+						);
 
-			new Within(d) {
+				final TaskDeploymentDescriptor tdd1 = new TaskDeploymentDescriptor(jid, vid1, eid1, "Sender", 0, 1,
+						new Configuration(), new Configuration(), Tasks.Sender.class.getName(),
+						irpdd, Collections.<InputGateDeploymentDescriptor>emptyList(), new ArrayList<BlobKey>(), 0);
 
-				@Override
-				protected void run() {
-					try {
-						tm.tell(new SubmitTask(tdd1), getRef());
-						expectMsgEquals(new TaskOperationResult(eid1, true));
-						tm.tell(new SubmitTask(tdd2), getRef());
-						expectMsgEquals(new TaskOperationResult(eid2, true));
+				final TaskDeploymentDescriptor tdd2 = new TaskDeploymentDescriptor(jid, vid2, eid2, "Receiver", 2, 7,
+						new Configuration(), new Configuration(), Tasks.Receiver.class.getName(),
+						Collections.<ResultPartitionDeploymentDescriptor>emptyList(),
+						Collections.singletonList(ircdd),
+						new ArrayList<BlobKey>(), 0);
 
-						tm.tell(TestingTaskManagerMessages.getRequestRunningTasksMessage(), getRef());
-						Map<ExecutionAttemptID, Task> tasks = expectMsgClass(TestingTaskManagerMessages.ResponseRunningTasks
-								.class).asJava();
+				final FiniteDuration d = duration("1 second");
 
-						Task t1 = tasks.get(eid1);
-						Task t2 = tasks.get(eid2);
+				new Within(d) {
 
-						// wait until the tasks are done. rare thread races may cause the tasks to be done before
-						// we get to the check, so we need to guard the check
-						if (t1 != null) {
-							Future<Object> response = Patterns.ask(tm, new TestingTaskManagerMessages.NotifyWhenTaskRemoved(eid1),
-									timeout);
-							Await.ready(response, d);
-						}
+					@Override
+					protected void run() {
+						try {
+							tm.tell(new SubmitTask(tdd1), getRef());
+							expectMsgEquals(new TaskOperationResult(eid1, true));
+							tm.tell(new SubmitTask(tdd2), getRef());
+							expectMsgEquals(new TaskOperationResult(eid2, true));
 
-						if (t2 != null) {
-							Future<Object> response = Patterns.ask(tm, new TestingTaskManagerMessages.NotifyWhenTaskRemoved(eid2),
-									timeout);
-							Await.ready(response, d);
-							assertEquals(ExecutionState.FINISHED, t2.getExecutionState());
-						}
+							tm.tell(TestingTaskManagerMessages.getRequestRunningTasksMessage(), getRef());
+							Map<ExecutionAttemptID, Task> tasks = expectMsgClass(TestingTaskManagerMessages.ResponseRunningTasks
+									.class).asJava();
+
+							Task t1 = tasks.get(eid1);
+							Task t2 = tasks.get(eid2);
+
+							// wait until the tasks are done. rare thread races may cause the tasks to be done before
+							// we get to the check, so we need to guard the check
+							if (t1 != null) {
+								Future<Object> response = Patterns.ask(tm, new TestingTaskManagerMessages.NotifyWhenTaskRemoved(eid1),
+										timeout);
+								Await.ready(response, d);
+							}
 
-						tm.tell(TestingTaskManagerMessages.getRequestRunningTasksMessage(), getRef());
-						tasks = expectMsgClass(TestingTaskManagerMessages.ResponseRunningTasks
-								.class).asJava();
+							if (t2 != null) {
+								Future<Object> response = Patterns.ask(tm, new TestingTaskManagerMessages.NotifyWhenTaskRemoved(eid2),
+										timeout);
+								Await.ready(response, d);
+								assertEquals(ExecutionState.FINISHED, t2.getExecutionState());
+							}
 
-						assertEquals(0, tasks.size());
-					}
-					catch (Exception e) {
-						e.printStackTrace();
-						fail(e.getMessage());
+							tm.tell(TestingTaskManagerMessages.getRequestRunningTasksMessage(), getRef());
+							tasks = expectMsgClass(TestingTaskManagerMessages.ResponseRunningTasks
+									.class).asJava();
 
+							assertEquals(0, tasks.size());
+						}
+						catch (Exception e) {
+							e.printStackTrace();
+							fail(e.getMessage());
+						}
 					}
+				};
+			}
+			catch (Exception e) {
+				e.printStackTrace();
+				fail(e.getMessage());
+			}
+			finally {
+				// shut down the actors
+				if (taskManager != null) {
+					taskManager.tell(Kill.getInstance(), ActorRef.noSender());
 				}
-			};
+				if (jobManager != null) {
+					jobManager.tell(Kill.getInstance(), ActorRef.noSender());
+				}
+			}
 		}};
 	}
 	
@@ -390,109 +447,128 @@ public class TaskManagerTest {
 		// this tests creates two tasks. the sender sends data, and fails to send the
 		// state update back to the job manager
 		// the second one blocks to be canceled
-
 		new JavaTestKit(system){{
-			final JobID jid = new JobID();
 
-			JobVertexID vid1 = new JobVertexID();
-			JobVertexID vid2 = new JobVertexID();
+			ActorRef jobManager = null;
+			ActorRef taskManager = null;
+			try {
+				final JobID jid = new JobID();
 
-			final ExecutionAttemptID eid1 = new ExecutionAttemptID();
-			final ExecutionAttemptID eid2 = new ExecutionAttemptID();
+				JobVertexID vid1 = new JobVertexID();
+				JobVertexID vid2 = new JobVertexID();
 
-			ActorRef jm = system.actorOf(Props.create(new SimpleLookupFailingUpdateJobManagerCreator()));
-			final ActorRef tm = createTaskManager(jm);
+				final ExecutionAttemptID eid1 = new ExecutionAttemptID();
+				final ExecutionAttemptID eid2 = new ExecutionAttemptID();
 
-			IntermediateResultPartitionID partitionId = new IntermediateResultPartitionID();
+				jobManager = system.actorOf(Props.create(new SimpleLookupFailingUpdateJobManagerCreator()));
+				taskManager = createTaskManager(jobManager);
+				final ActorRef tm = taskManager;
 
-			List<ResultPartitionDeploymentDescriptor> irpdd = new ArrayList<ResultPartitionDeploymentDescriptor>();
-			irpdd.add(new ResultPartitionDeploymentDescriptor(new IntermediateDataSetID(), partitionId, ResultPartitionType.PIPELINED, 1));
+				IntermediateResultPartitionID partitionId = new IntermediateResultPartitionID();
 
-			InputGateDeploymentDescriptor ircdd =
-					new InputGateDeploymentDescriptor(
-							new IntermediateDataSetID(),
-							0, new InputChannelDeploymentDescriptor[]{
-									new InputChannelDeploymentDescriptor(new ResultPartitionID(partitionId, eid1), ResultPartitionLocation.createLocal())
-							}
-					);
+				List<ResultPartitionDeploymentDescriptor> irpdd = new ArrayList<ResultPartitionDeploymentDescriptor>();
+				irpdd.add(new ResultPartitionDeploymentDescriptor(new IntermediateDataSetID(), partitionId, ResultPartitionType.PIPELINED, 1));
 
-			final TaskDeploymentDescriptor tdd1 = new TaskDeploymentDescriptor(jid, vid1, eid1, "Sender", 0, 1,
-					new Configuration(), new Configuration(), Tasks.Sender.class.getName(),
-					irpdd, Collections.<InputGateDeploymentDescriptor>emptyList(),
-					new ArrayList<BlobKey>(), 0);
+				InputGateDeploymentDescriptor ircdd =
+						new InputGateDeploymentDescriptor(
+								new IntermediateDataSetID(),
+								0, new InputChannelDeploymentDescriptor[]{
+										new InputChannelDeploymentDescriptor(new ResultPartitionID(partitionId, eid1), ResultPartitionLocation.createLocal())
+								}
+						);
 
-			final TaskDeploymentDescriptor tdd2 = new TaskDeploymentDescriptor(jid, vid2, eid2, "Receiver", 2, 7,
-					new Configuration(), new Configuration(), Tasks.BlockingReceiver.class.getName(),
-					Collections.<ResultPartitionDeploymentDescriptor>emptyList(),
-					Collections.singletonList(ircdd),
-					new ArrayList<BlobKey>(), 0);
+				final TaskDeploymentDescriptor tdd1 = new TaskDeploymentDescriptor(jid, vid1, eid1, "Sender", 0, 1,
+						new Configuration(), new Configuration(), Tasks.Sender.class.getName(),
+						irpdd, Collections.<InputGateDeploymentDescriptor>emptyList(),
+						new ArrayList<BlobKey>(), 0);
 
-			final FiniteDuration d = duration("1 second");
+				final TaskDeploymentDescriptor tdd2 = new TaskDeploymentDescriptor(jid, vid2, eid2, "Receiver", 2, 7,
+						new Configuration(), new Configuration(), Tasks.BlockingReceiver.class.getName(),
+						Collections.<ResultPartitionDeploymentDescriptor>emptyList(),
+						Collections.singletonList(ircdd),
+						new ArrayList<BlobKey>(), 0);
 
-			new Within(d){
+				final FiniteDuration d = duration("1 second");
 
-				@Override
-				protected void run() {
-					try {
-						// deploy sender before receiver, so the target is online when the sender requests the connection info
-						tm.tell(new SubmitTask(tdd2), getRef());
-						tm.tell(new SubmitTask(tdd1), getRef());
+				new Within(d){
 
-						expectMsgEquals(new TaskOperationResult(eid2, true));
-						expectMsgEquals(new TaskOperationResult(eid1, true));
+					@Override
+					protected void run() {
+						try {
+							// deploy sender before receiver, so the target is online when the sender requests the connection info
+							tm.tell(new SubmitTask(tdd2), getRef());
+							tm.tell(new SubmitTask(tdd1), getRef());
 
-						tm.tell(TestingTaskManagerMessages.getRequestRunningTasksMessage(), getRef());
-						Map<ExecutionAttemptID, Task> tasks = expectMsgClass(TestingTaskManagerMessages
-								.ResponseRunningTasks.class).asJava();
+							expectMsgEquals(new TaskOperationResult(eid2, true));
+							expectMsgEquals(new TaskOperationResult(eid1, true));
 
-						Task t1 = tasks.get(eid1);
-						Task t2 = tasks.get(eid2);
+							tm.tell(TestingTaskManagerMessages.getRequestRunningTasksMessage(), getRef());
+							Map<ExecutionAttemptID, Task> tasks = expectMsgClass(TestingTaskManagerMessages
+									.ResponseRunningTasks.class).asJava();
 
-						tm.tell(new CancelTask(eid2), getRef());
-						expectMsgEquals(new TaskOperationResult(eid2, true));
+							Task t1 = tasks.get(eid1);
+							Task t2 = tasks.get(eid2);
 
-						if (t2 != null) {
-							Future<Object> response = Patterns.ask(tm, new TestingTaskManagerMessages.NotifyWhenTaskRemoved(eid2),
-									timeout);
-							Await.ready(response, d);
-						}
+							tm.tell(new CancelTask(eid2), getRef());
+							expectMsgEquals(new TaskOperationResult(eid2, true));
 
-						if (t1 != null) {
-							if (t1.getExecutionState() == ExecutionState.RUNNING) {
-								tm.tell(new CancelTask(eid1), getRef());
-								expectMsgEquals(new TaskOperationResult(eid1, true));
+							if (t2 != null) {
+								Future<Object> response = Patterns.ask(tm, new TestingTaskManagerMessages.NotifyWhenTaskRemoved(eid2),
+										timeout);
+								Await.ready(response, d);
 							}
-							Future<Object> response = Patterns.ask(tm, new TestingTaskManagerMessages.NotifyWhenTaskRemoved(eid1),
-									timeout);
-							Await.ready(response, d);
-						}
 
-						tm.tell(TestingTaskManagerMessages.getRequestRunningTasksMessage(), getRef());
-						tasks = expectMsgClass(TestingTaskManagerMessages
-								.ResponseRunningTasks.class).asJava();
+							if (t1 != null) {
+								if (t1.getExecutionState() == ExecutionState.RUNNING) {
+									tm.tell(new CancelTask(eid1), getRef());
+									expectMsgEquals(new TaskOperationResult(eid1, true));
+								}
+								Future<Object> response = Patterns.ask(tm, new TestingTaskManagerMessages.NotifyWhenTaskRemoved(eid1),
+										timeout);
+								Await.ready(response, d);
+							}
 
-						assertEquals(0, tasks.size());
-					}catch(Exception e){
-						e.printStackTrace();
-						fail(e.getMessage());
+							tm.tell(TestingTaskManagerMessages.getRequestRunningTasksMessage(), getRef());
+							tasks = expectMsgClass(TestingTaskManagerMessages
+									.ResponseRunningTasks.class).asJava();
+
+							assertEquals(0, tasks.size());
+						}
+						catch(Exception e) {
+							e.printStackTrace();
+							fail(e.getMessage());
+						}
 					}
+				};
+			}
+			catch(Exception e) {
+				e.printStackTrace();
+				fail(e.getMessage());
+			}
+			finally {
+				// shut down the actors
+				if (taskManager != null) {
+					taskManager.tell(Kill.getInstance(), ActorRef.noSender());
 				}
-			};
+				if (jobManager != null) {
+					jobManager.tell(Kill.getInstance(), ActorRef.noSender());
+				}
+			}
 		}};
 	}
 
 	// --------------------------------------------------------------------------------------------
 
-	public static class SimpleJobManager extends UntypedActor{
+	public static class SimpleJobManager extends UntypedActor {
 
 		@Override
 		public void onReceive(Object message) throws Exception {
-			if(message instanceof RegistrationMessages.RegisterTaskManager){
+			if (message instanceof RegistrationMessages.RegisterTaskManager) {
 				final InstanceID iid = new InstanceID();
-				getSender().tell(new RegistrationMessages.AcknowledgeRegistration(iid, -1,
-								Option.<ActorRef>apply(null)),
-						getSelf());
-			}else if(message instanceof JobManagerMessages.UpdateTaskExecutionState){
+				final ActorRef self = getSelf();
+				getSender().tell(new RegistrationMessages.AcknowledgeRegistration(self, iid, -1), self);
+			}
+			else if(message instanceof TaskMessages.UpdateTaskExecutionState){
 				getSender().tell(true, getSelf());
 			}
 		}
@@ -514,7 +590,7 @@ public class TaskManagerTest {
 
 		@Override
 		public void onReceive(Object message) throws Exception{
-			if (message instanceof JobManagerMessages.UpdateTaskExecutionState) {
+			if (message instanceof TaskMessages.UpdateTaskExecutionState) {
 				getSender().tell(false, getSelf());
 			} else {
 				super.onReceive(message);
@@ -538,14 +614,24 @@ public class TaskManagerTest {
 		}
 	}
 
-	public static ActorRef createTaskManager(ActorRef jm) {
-		Configuration cfg = new Configuration();
-		cfg.setInteger(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, 10);
+	public static ActorRef createTaskManager(ActorRef jobManager) {
+		ActorRef taskManager = null;
+		try {
+			Configuration cfg = new Configuration();
+			cfg.setInteger(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, 10);
 
-		String jobManagerURL = jm.path().toString();
+			Option<String> jobMangerUrl = Option.apply(jobManager.path().toString());
 
-		ActorRef taskManager = TestingUtils.startTestingTaskManagerWithConfiguration("localhost",
-				jobManagerURL, cfg, system);
+			taskManager = TaskManager.startTaskManagerComponentsAndActor(
+					cfg, system, "localhost",
+					Option.<String>empty(),
+					jobMangerUrl,
+					true, TestingTaskManager.class);
+		}
+		catch (Exception e) {
+			e.printStackTrace();
+			fail("Could not create test TaskManager: " + e.getMessage());
+		}
 
 		Future<Object> response = Patterns.ask(taskManager, 
 				TaskManagerMessages.getNotifyWhenRegisteredAtJobManagerMessage(), timeout);
@@ -554,8 +640,9 @@ public class TaskManagerTest {
 			FiniteDuration d = new FiniteDuration(20, TimeUnit.SECONDS);
 			Await.ready(response, d);
 		}
-		catch(Exception e) {
-			throw new RuntimeException("Exception while waiting for the task manager registration.", e);
+		catch (Exception e) {
+			e.printStackTrace();
+			fail("Exception while waiting for the task manager registration: " + e.getMessage());
 		}
 
 		return taskManager;
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskTest.java
index 689b22dc490..3a8fcd893c8 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskTest.java
@@ -46,7 +46,6 @@ import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertTrue;
 import static org.junit.Assert.fail;
-import static org.mockito.Matchers.any;
 import static org.mockito.Mockito.doNothing;
 import static org.mockito.Mockito.mock;
 import static org.mockito.Mockito.spy;
diff --git a/flink-runtime/src/test/scala/org/apache/flink/runtime/jobmanager/JobManagerConnectionTest.scala b/flink-runtime/src/test/scala/org/apache/flink/runtime/jobmanager/JobManagerConnectionTest.scala
index 9c329d1cf10..8c8ce06bbbb 100644
--- a/flink-runtime/src/test/scala/org/apache/flink/runtime/jobmanager/JobManagerConnectionTest.scala
+++ b/flink-runtime/src/test/scala/org/apache/flink/runtime/jobmanager/JobManagerConnectionTest.scala
@@ -31,14 +31,14 @@ import org.junit.Test
 
 import scala.concurrent.duration.Duration
 
+/**
+ * Tests that a lookup of a local JobManager fails within a given timeout if the JobManager
+ * actor is not reachable.
+ */
 class JobManagerConnectionTest {
 
   private val timeout = 1000
 
-  /**
-   * Tests that a lookup of a local JobManager fails within a given timeout if the JobManager
-   * actor is not reachable.
-   */
   @Test
   def testResolveUnreachableActorLocalHost() : Unit = {
     // startup a test actor system listening at an arbitrary address
@@ -54,7 +54,7 @@ class JobManagerConnectionTest {
       }
 
       val endpoint = new InetSocketAddress(InetAddress.getByName("127.0.0.1"), freePort)
-      val config = getConfigWithLowTimeout()
+      val config = createConfigWithLowTimeout()
 
       mustReturnWithinTimeout(Duration(5*timeout, TimeUnit.MILLISECONDS)) {
         () => {
@@ -90,7 +90,7 @@ class JobManagerConnectionTest {
     try {
       // some address that is not running a JobManager
       val endpoint = new InetSocketAddress(InetAddress.getByName("10.254.254.254"), 2)
-      val config = getConfigWithLowTimeout()
+      val config = createConfigWithLowTimeout()
 
       mustReturnWithinTimeout(Duration(5*timeout, TimeUnit.MILLISECONDS)) {
         () => {
@@ -114,7 +114,7 @@ class JobManagerConnectionTest {
     }
   }
 
-  private def getConfigWithLowTimeout() : Configuration = {
+  private def createConfigWithLowTimeout() : Configuration = {
     val config = new Configuration()
     config.setString(ConfigConstants.AKKA_LOOKUP_TIMEOUT,
                      Duration(timeout, TimeUnit.MILLISECONDS).toSeconds + " s")
diff --git a/flink-runtime/src/test/scala/org/apache/flink/runtime/jobmanager/TaskManagerRegistrationITCase.scala b/flink-runtime/src/test/scala/org/apache/flink/runtime/jobmanager/TaskManagerRegistrationITCase.scala
deleted file mode 100644
index 58905ef21b8..00000000000
--- a/flink-runtime/src/test/scala/org/apache/flink/runtime/jobmanager/TaskManagerRegistrationITCase.scala
+++ /dev/null
@@ -1,132 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.runtime.jobmanager
-
-import java.net.InetAddress
-
-import akka.actor._
-import akka.testkit.{TestKit, ImplicitSender}
-import org.apache.flink.configuration.{ConfigConstants, Configuration}
-import org.apache.flink.runtime.instance.{InstanceID, HardwareDescription, InstanceConnectionInfo}
-import org.apache.flink.runtime.messages.RegistrationMessages.{AlreadyRegistered,
-RefuseRegistration, AcknowledgeRegistration, RegisterTaskManager}
-import org.apache.flink.runtime.messages.TaskManagerMessages.Heartbeat
-import org.apache.flink.runtime.testingUtils.TestingUtils
-import org.scalatest.{BeforeAndAfterAll, Matchers, WordSpecLike}
-import scala.concurrent.duration._
-
-import scala.language.postfixOps
-
-class TaskManagerRegistrationITCase(_system: ActorSystem) extends TestKit(_system) with
-ImplicitSender with WordSpecLike with Matchers with BeforeAndAfterAll {
-
-  def this() = this(ActorSystem("TestingActorSystem", TestingUtils.testConfig))
-
-  override def afterAll(): Unit = {
-    TestKit.shutdownActorSystem(system)
-  }
-
-  "The JobManager" should {
-    "notify already registered TaskManagers" in {
-
-      val jm = TestingUtils.startTestingJobManager(_system)
-
-      val connectionInfo = new InstanceConnectionInfo(InetAddress.getLocalHost,1)
-      val hardwareDescription = HardwareDescription.extractFromSystem(10)
-
-      try {
-        within(TestingUtils.TESTING_DURATION) {
-          jm ! RegisterTaskManager(connectionInfo, hardwareDescription, 1)
-          jm ! RegisterTaskManager(connectionInfo, hardwareDescription, 1)
-
-          expectMsgType[AcknowledgeRegistration]
-          expectMsgType[AlreadyRegistered]
-        }
-      } finally {
-        jm ! Kill
-      }
-    }
-  }
-
-  "The TaskManager" should {
-    "shutdown if its registration is refused by the JobManager" in {
-
-      val tm = TestingUtils.startTestingTaskManager(self, _system)
-
-      watch(tm)
-
-      try{
-        within(TestingUtils.TESTING_DURATION) {
-          expectMsgType[RegisterTaskManager]
-          tm ! RefuseRegistration("Testing connection refusal")
-
-          expectTerminated(tm)
-        }
-      }
-    }
-
-    "ignore RefuseRegistration messages after it has been successfully registered" in {
-
-      val tm = TestingUtils.startTestingTaskManager(self, _system)
-
-      try {
-        ignoreMsg{
-          case _: Heartbeat => true
-        }
-        within(TestingUtils.TESTING_DURATION) {
-          expectMsgType[RegisterTaskManager]
-
-          tm ! AcknowledgeRegistration(new InstanceID(), 42, None)
-
-          tm ! RefuseRegistration("Should be ignored")
-
-          // Check if the TaskManager is still alive
-          tm ! Identify(1)
-
-          expectMsgType[ActorIdentity]
-
-        }
-      } finally {
-        tm ! Kill
-      }
-    }
-
-    "shutdown after the maximum registration duration has been exceeded" in {
-
-      val config = new Configuration()
-      config.setString(ConfigConstants.TASK_MANAGER_MAX_REGISTRATION_DURATION, "1 second")
-
-      val tm = TestingUtils.startTestingTaskManagerWithConfiguration("localhost",
-        self.path.toString, config, _system)
-
-      watch(tm)
-
-      try {
-        ignoreMsg{
-          case _: RegisterTaskManager => true
-        }
-        within(2 seconds) {
-          expectTerminated(tm)
-        }
-      } finally {
-        tm ! Kill
-      }
-    }
-  }
-}
diff --git a/flink-runtime/src/test/scala/org/apache/flink/runtime/jobmanager/TaskManagerRegistrationTest.scala b/flink-runtime/src/test/scala/org/apache/flink/runtime/jobmanager/TaskManagerRegistrationTest.scala
new file mode 100644
index 00000000000..409e98d03f0
--- /dev/null
+++ b/flink-runtime/src/test/scala/org/apache/flink/runtime/jobmanager/TaskManagerRegistrationTest.scala
@@ -0,0 +1,139 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.jobmanager
+
+import java.net.InetAddress
+
+import akka.actor._
+import akka.testkit.{ImplicitSender, TestKit}
+import org.apache.flink.configuration.Configuration
+import org.apache.flink.runtime.akka.AkkaUtils
+import org.apache.flink.runtime.instance.{HardwareDescription, InstanceConnectionInfo, InstanceID}
+import org.apache.flink.runtime.messages.RegistrationMessages.{AcknowledgeRegistration, AlreadyRegistered, RegisterTaskManager}
+import org.junit.Assert.{assertNotEquals, assertNotNull}
+import org.scalatest.{BeforeAndAfterAll, Matchers, WordSpecLike}
+
+import scala.concurrent.duration._
+import scala.language.postfixOps
+
+/**
+ * Tests for the JobManager's behavior when a TaskManager solicits registration.
+ * It also tests the JobManager's response to heartbeats from TaskManagers it does
+ * not know.
+ */
+class TaskManagerRegistrationTest(_system: ActorSystem) extends TestKit(_system) with
+ImplicitSender with WordSpecLike with Matchers with BeforeAndAfterAll {
+
+  def this() = this(AkkaUtils.createLocalActorSystem(new Configuration()))
+
+  override def afterAll(): Unit = {
+    TestKit.shutdownActorSystem(system)
+  }
+
+  "The JobManager" should {
+
+    "assign a TaskManager a unique instance ID" in {
+      val jm = startTestingJobManager(_system)
+
+      val tmDummy1 = _system.actorOf(Props(classOf[TaskManagerRegistrationTest.DummyActor]))
+      val tmDummy2 = _system.actorOf(Props(classOf[TaskManagerRegistrationTest.DummyActor]))
+
+      try {
+        val connectionInfo1 = new InstanceConnectionInfo(InetAddress.getLocalHost, 10000)
+        val connectionInfo2 = new InstanceConnectionInfo(InetAddress.getLocalHost, 10001)
+
+        val hardwareDescription = HardwareDescription.extractFromSystem(10)
+
+        var id1: InstanceID = null
+        var id2: InstanceID = null
+
+        // task manager 1
+        within(1 second) {
+          jm ! RegisterTaskManager(tmDummy1, connectionInfo1, hardwareDescription, 1)
+
+          val response = receiveOne(1 second)
+          response match {
+            case AcknowledgeRegistration(_, id, _) => id1 = id
+            case _ => fail("Wrong response message: " + response)
+          }
+        }
+
+        // task manager 2
+        within(1 second) {
+          jm ! RegisterTaskManager(tmDummy2, connectionInfo2, hardwareDescription, 1)
+
+          val response = receiveOne(1 second)
+          response match {
+            case AcknowledgeRegistration(_, id, _) => id2 = id
+            case _ => fail("Wrong response message: " + response)
+          }
+        }
+
+        assertNotNull(id1)
+        assertNotNull(id2)
+        assertNotEquals(id1, id2)
+      }
+      finally {
+        tmDummy1 ! Kill
+        tmDummy2 ! Kill
+        jm ! Kill
+      }
+    }
+
+    "handle repeated registration calls" in {
+
+      val jm = startTestingJobManager(_system)
+      val tmDummy = _system.actorOf(Props(classOf[TaskManagerRegistrationTest.DummyActor]))
+
+      try {
+        val connectionInfo = new InstanceConnectionInfo(InetAddress.getLocalHost,1)
+        val hardwareDescription = HardwareDescription.extractFromSystem(10)
+        
+        within(1 second) {
+          jm ! RegisterTaskManager(tmDummy, connectionInfo, hardwareDescription, 1)
+          jm ! RegisterTaskManager(tmDummy, connectionInfo, hardwareDescription, 1)
+          jm ! RegisterTaskManager(tmDummy, connectionInfo, hardwareDescription, 1)
+
+          expectMsgType[AcknowledgeRegistration]
+          expectMsgType[AlreadyRegistered]
+          expectMsgType[AlreadyRegistered]
+        }
+      } finally {
+        tmDummy ! Kill
+        jm ! Kill
+      }
+    }
+  }
+
+  private def startTestingJobManager(system: ActorSystem): ActorRef = {
+    val (jm: ActorRef, _) = JobManager.startJobManagerActors(
+                                        new Configuration(), _system, None, None)
+    jm
+  }
+}
+
+object TaskManagerRegistrationTest {
+
+  /** Simply dummy actor that swallows all messages */
+  class DummyActor extends Actor {
+    override def receive: Receive = {
+      case _ =>
+    }
+  }
+}
diff --git a/flink-runtime/src/test/scala/org/apache/flink/runtime/testingUtils/TestingCluster.scala b/flink-runtime/src/test/scala/org/apache/flink/runtime/testingUtils/TestingCluster.scala
index 4a726941559..f87e151baaf 100644
--- a/flink-runtime/src/test/scala/org/apache/flink/runtime/testingUtils/TestingCluster.scala
+++ b/flink-runtime/src/test/scala/org/apache/flink/runtime/testingUtils/TestingCluster.scala
@@ -67,7 +67,17 @@ class TestingCluster(userConfiguration: Configuration, singleActorSystem: Boolea
 
     val tmActorName = TaskManager.TASK_MANAGER_NAME + "_" + (index + 1)
 
-    TaskManager.startTaskManagerActor(configuration, system, HOSTNAME, tmActorName,
-      singleActorSystem, numTaskManagers == 1, classOf[TestingTaskManager])
+    val jobManagerPath: Option[String] = if (singleActorSystem) {
+      Some(jobManagerActor.path.toString)
+    } else {
+      None
+    }
+
+    TaskManager.startTaskManagerComponentsAndActor(configuration, system,
+                                                   HOSTNAME,
+                                                   Some(tmActorName),
+                                                   jobManagerPath,
+                                                   numTaskManagers == 1,
+                                                   classOf[TestingTaskManager])
   }
 }
diff --git a/flink-runtime/src/test/scala/org/apache/flink/runtime/testingUtils/TestingTaskManager.scala b/flink-runtime/src/test/scala/org/apache/flink/runtime/testingUtils/TestingTaskManager.scala
index a47e4e74ef1..702e34c15c0 100644
--- a/flink-runtime/src/test/scala/org/apache/flink/runtime/testingUtils/TestingTaskManager.scala
+++ b/flink-runtime/src/test/scala/org/apache/flink/runtime/testingUtils/TestingTaskManager.scala
@@ -22,9 +22,12 @@ import akka.actor.{Terminated, ActorRef}
 import org.apache.flink.api.common.JobID
 import org.apache.flink.runtime.executiongraph.ExecutionAttemptID
 import org.apache.flink.runtime.instance.InstanceConnectionInfo
+import org.apache.flink.runtime.io.disk.iomanager.IOManager
+import org.apache.flink.runtime.io.network.NetworkEnvironment
+import org.apache.flink.runtime.memorymanager.DefaultMemoryManager
 import org.apache.flink.runtime.messages.Messages.Disconnect
-import org.apache.flink.runtime.messages.TaskManagerMessages.UnregisterTask
-import org.apache.flink.runtime.taskmanager.{NetworkEnvironmentConfiguration, TaskManagerConfiguration, TaskManager}
+import org.apache.flink.runtime.messages.TaskMessages.UnregisterTask
+import org.apache.flink.runtime.taskmanager.{TaskManagerConfiguration, TaskManager}
 import org.apache.flink.runtime.testingUtils.TestingJobManagerMessages.NotifyWhenJobRemoved
 import org.apache.flink.runtime.testingUtils.TestingMessages.DisableDisconnect
 import org.apache.flink.runtime.testingUtils.TestingTaskManagerMessages._
@@ -35,11 +38,15 @@ import scala.language.postfixOps
 /**
  * Subclass of the [[TaskManager]] to support testing messages
  */
-class TestingTaskManager(connectionInfo: InstanceConnectionInfo,
+class TestingTaskManager(config: TaskManagerConfiguration,
+                         connectionInfo: InstanceConnectionInfo,
                          jobManagerAkkaURL: String,
-                         taskManagerConfig: TaskManagerConfiguration,
-                         networkConfig: NetworkEnvironmentConfiguration)
-  extends TaskManager(connectionInfo, jobManagerAkkaURL, taskManagerConfig, networkConfig) {
+                         memoryManager: DefaultMemoryManager,
+                         ioManager: IOManager,
+                         network: NetworkEnvironment,
+                         numberOfSlots: Int)
+  extends TaskManager(config, connectionInfo, jobManagerAkkaURL,
+                      memoryManager, ioManager, network, numberOfSlots) {
 
 
   val waitForRemoval = scala.collection.mutable.HashMap[ExecutionAttemptID, Set[ActorRef]]()
@@ -81,12 +88,12 @@ class TestingTaskManager(connectionInfo: InstanceConnectionInfo,
         bcVarManager.getNumberOfVariablesWithReferences)
 
     case RequestNumActiveConnections =>
-      networkEnvironment match {
-        case Some(ne) => sender ! ResponseNumActiveConnections(
-          ne.getConnectionManager.getNumberOfActiveConnections)
-
-        case None => sender ! ResponseNumActiveConnections(0)
-      }
+      val numActive = if (network.isAssociated) {
+                        network.getConnectionManager.getNumberOfActiveConnections
+                      } else {
+                        0
+                      }
+      sender ! ResponseNumActiveConnections(numActive)
 
     case NotifyWhenJobRemoved(jobID) =>
       if(runningTasks.values.exists(_.getJobID == jobID)){
@@ -129,7 +136,7 @@ class TestingTaskManager(connectionInfo: InstanceConnectionInfo,
       if (!disconnectDisabled) {
         super.receiveWithLogMessages(msg)
 
-        val jobManager = sender
+        val jobManager = sender()
 
         waitForJobManagerToBeTerminated.remove(jobManager.path.name) foreach {
           _ foreach {
diff --git a/flink-runtime/src/test/scala/org/apache/flink/runtime/testingUtils/TestingUtils.scala b/flink-runtime/src/test/scala/org/apache/flink/runtime/testingUtils/TestingUtils.scala
index 9bb3d0b2ea1..63dce317c00 100644
--- a/flink-runtime/src/test/scala/org/apache/flink/runtime/testingUtils/TestingUtils.scala
+++ b/flink-runtime/src/test/scala/org/apache/flink/runtime/testingUtils/TestingUtils.scala
@@ -18,13 +18,12 @@
 
 package org.apache.flink.runtime.testingUtils
 
-import akka.actor.{Props, ActorRef, ActorSystem}
+import akka.actor.{ActorRef, ActorSystem}
 import akka.testkit.CallingThreadDispatcher
 import com.typesafe.config.ConfigFactory
 import org.apache.flink.configuration.{ConfigConstants, Configuration}
 import org.apache.flink.runtime.akka.AkkaUtils
 import org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.ActionQueue
-import org.apache.flink.runtime.jobmanager.{MemoryArchivist, JobManager}
 import org.apache.flink.runtime.taskmanager.TaskManager
 import scala.concurrent.duration._
 
@@ -57,34 +56,18 @@ object TestingUtils {
 
   def getDefaultTestingActorSystemConfig = testConfig
 
-  def startTestingJobManager(system: ActorSystem): ActorRef = {
-    val config = new Configuration()
-
-    val (instanceManager, scheduler, libraryCacheManager, _, accumulatorManager, _ ,
-        executionRetries, delayBetweenRetries,
-        timeout, archiveCount) = JobManager.createJobManagerComponents(config)
-
-    val testArchiveProps = Props(new MemoryArchivist(archiveCount) with TestingMemoryArchivist)
-    val archive = system.actorOf(testArchiveProps, JobManager.ARCHIVE_NAME)
-
-    val jobManagerProps = Props(new JobManager(config, instanceManager, scheduler,
-      libraryCacheManager, archive, accumulatorManager, None, executionRetries,
-      delayBetweenRetries, timeout) with TestingJobManager)
-
-    system.actorOf(jobManagerProps, JobManager.JOB_MANAGER_NAME)
-  }
 
   def startTestingTaskManagerWithConfiguration(hostname: String,
                                                jobManagerURL: String,
                                                config: Configuration,
-                                               system: ActorSystem) = {
+                                               system: ActorSystem) : ActorRef = {
 
-    val (tmConfig, netConfig, connectionInfo, _) =
-      TaskManager.parseTaskManagerConfiguration(config, hostname, true, false)
 
-    val tmProps = Props(classOf[TestingTaskManager], connectionInfo,
-                        jobManagerURL, tmConfig, netConfig)
-    system.actorOf(tmProps)
+    TaskManager.startTaskManagerComponentsAndActor(config, system,
+                                                   hostname,
+                                                   None, // random actor name
+                                                   Some(jobManagerURL), // job manager
+                                                   true, classOf[TestingTaskManager])
   }
 
   def startTestingTaskManager(jobManager: ActorRef, system: ActorSystem): ActorRef = {
@@ -92,11 +75,7 @@ object TestingUtils {
     val jmURL = jobManager.path.toString
     val config = new Configuration()
 
-    val (tmConfig, netConfig, connectionInfo, _) =
-      TaskManager.parseTaskManagerConfiguration(config,  "localhost", true, true)
-
-    val tmProps = Props(classOf[TestingTaskManager], connectionInfo, jmURL, tmConfig, netConfig)
-    system.actorOf(tmProps)
+    startTestingTaskManagerWithConfiguration("localhost", jmURL, config, system)
   }
 
   def startTestingCluster(numSlots: Int, numTMs: Int = 1,
diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/streamvertex/StreamVertex.java b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/streamvertex/StreamVertex.java
index 2641bc15551..5b805313dc7 100644
--- a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/streamvertex/StreamVertex.java
+++ b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/streamvertex/StreamVertex.java
@@ -26,8 +26,7 @@ import org.apache.flink.runtime.execution.Environment;
 import org.apache.flink.runtime.jobgraph.tasks.AbstractInvokable;
 import org.apache.flink.runtime.jobgraph.tasks.BarrierTransceiver;
 import org.apache.flink.runtime.jobgraph.tasks.OperatorStateCarrier;
-import org.apache.flink.runtime.jobmanager.BarrierAck;
-import org.apache.flink.runtime.jobmanager.StateBarrierAck;
+import org.apache.flink.runtime.messages.CheckpointingMessages;
 import org.apache.flink.runtime.state.LocalStateHandle;
 import org.apache.flink.runtime.state.OperatorState;
 import org.apache.flink.runtime.state.StateHandle;
@@ -116,12 +115,12 @@ public class StreamVertex<IN, OUT> extends AbstractInvokable implements StreamTa
 
 		if (configuration.getStateMonitoring() && !states.isEmpty()) {
 			getEnvironment().getJobManager().tell(
-					new StateBarrierAck(getEnvironment().getJobID(), getEnvironment()
+					new CheckpointingMessages.StateBarrierAck(getEnvironment().getJobID(), getEnvironment()
 							.getJobVertexId(), context.getIndexOfThisSubtask(), barrierID,
 							new LocalStateHandle(states)), ActorRef.noSender());
 		} else {
 			getEnvironment().getJobManager().tell(
-					new BarrierAck(getEnvironment().getJobID(), getEnvironment().getJobVertexId(),
+					new CheckpointingMessages.BarrierAck(getEnvironment().getJobID(), getEnvironment().getJobVertexId(),
 							context.getIndexOfThisSubtask(), barrierID), ActorRef.noSender());
 		}
 
diff --git a/flink-test-utils/src/main/scala/org/apache/flink/test/util/ForkableFlinkMiniCluster.scala b/flink-test-utils/src/main/scala/org/apache/flink/test/util/ForkableFlinkMiniCluster.scala
index 6df8099c64f..ddfffeec952 100644
--- a/flink-test-utils/src/main/scala/org/apache/flink/test/util/ForkableFlinkMiniCluster.scala
+++ b/flink-test-utils/src/main/scala/org/apache/flink/test/util/ForkableFlinkMiniCluster.scala
@@ -110,8 +110,14 @@ class ForkableFlinkMiniCluster(userConfiguration: Configuration, singleActorSyst
 
     val localExecution = numTaskManagers == 1
 
-    TaskManager.startTaskManagerActor(config, system, HOSTNAME,
-        TaskManager.TASK_MANAGER_NAME + index, singleActorSystem, localExecution,
+    val jobManagerAkkaUrl: Option[String] = if (singleActorSystem) {
+      Some(jobManagerActor.path.toString)
+    } else {
+      None
+    }
+
+    TaskManager.startTaskManagerComponentsAndActor(config, system, HOSTNAME,
+        Some(TaskManager.TASK_MANAGER_NAME + index), jobManagerAkkaUrl, localExecution,
          classOf[TestingTaskManager])
   }
 
diff --git a/flink-tests/src/test/java/org/apache/flink/test/recovery/AbstractProcessFailureRecoveryTest.java b/flink-tests/src/test/java/org/apache/flink/test/recovery/AbstractProcessFailureRecoveryTest.java
index 4311e9c8cfb..2901bf81991 100644
--- a/flink-tests/src/test/java/org/apache/flink/test/recovery/AbstractProcessFailureRecoveryTest.java
+++ b/flink-tests/src/test/java/org/apache/flink/test/recovery/AbstractProcessFailureRecoveryTest.java
@@ -139,7 +139,7 @@ public abstract class AbstractProcessFailureRecoveryTest {
 
 			// we wait for the JobManager to have the two TaskManagers available
 			// wait for at most 20 seconds
-			waitUntilNumTaskManagersAreRegistered(jmActor, 2, 20000);
+			waitUntilNumTaskManagersAreRegistered(jmActor, 2, 30000);
 
 			// the program will set a marker file in each of its parallel tasks once they are ready, so that
 			// this coordinating code is aware of this.
@@ -174,7 +174,7 @@ public abstract class AbstractProcessFailureRecoveryTest {
 			new PipeForwarder(taskManagerProcess3.getErrorStream(), processOutput3);
 
 			// we wait for the third TaskManager to register (20 seconds max)
-			waitUntilNumTaskManagersAreRegistered(jmActor, 3, 20000);
+			waitUntilNumTaskManagersAreRegistered(jmActor, 3, 30000);
 
 			// kill one of the previous TaskManagers, triggering a failure and recovery
 			taskManagerProcess1.destroy();
@@ -369,7 +369,7 @@ public abstract class AbstractProcessFailureRecoveryTest {
 				cfg.setInteger(ConfigConstants.TASK_MANAGER_NETWORK_NUM_BUFFERS_KEY, 100);
 				cfg.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 2);
 
-				TaskManager.runTaskManager(cfg, TaskManager.class);
+				TaskManager.selectNetworkInterfaceAndRunTaskManager(cfg, TaskManager.class);
 
 				// wait forever
 				Object lock = new Object();
diff --git a/flink-yarn/src/main/java/org/apache/flink/yarn/appMaster/YarnTaskManagerRunner.java b/flink-yarn/src/main/java/org/apache/flink/yarn/appMaster/YarnTaskManagerRunner.java
index 6fa7a61094b..3556ec1e73e 100644
--- a/flink-yarn/src/main/java/org/apache/flink/yarn/appMaster/YarnTaskManagerRunner.java
+++ b/flink-yarn/src/main/java/org/apache/flink/yarn/appMaster/YarnTaskManagerRunner.java
@@ -87,7 +87,7 @@ public class YarnTaskManagerRunner {
 			@Override
 			public Object run() {
 				try {
-					TaskManager.runTaskManager(configuration, YarnTaskManager.class);
+					TaskManager.selectNetworkInterfaceAndRunTaskManager(configuration, YarnTaskManager.class);
 				}
 				catch (Throwable t) {
 					LOG.error("Error while starting the TaskManager", t);
diff --git a/flink-yarn/src/main/scala/org/apache/flink/yarn/YarnTaskManager.scala b/flink-yarn/src/main/scala/org/apache/flink/yarn/YarnTaskManager.scala
index 095b0e30ccb..de827168116 100644
--- a/flink-yarn/src/main/scala/org/apache/flink/yarn/YarnTaskManager.scala
+++ b/flink-yarn/src/main/scala/org/apache/flink/yarn/YarnTaskManager.scala
@@ -19,6 +19,9 @@
 package org.apache.flink.yarn
 
 import org.apache.flink.runtime.instance.InstanceConnectionInfo
+import org.apache.flink.runtime.io.disk.iomanager.IOManager
+import org.apache.flink.runtime.io.network.NetworkEnvironment
+import org.apache.flink.runtime.memorymanager.DefaultMemoryManager
 import org.apache.flink.runtime.taskmanager.{NetworkEnvironmentConfiguration, TaskManagerConfiguration, TaskManager}
 import org.apache.flink.yarn.Messages.StopYarnSession
 
@@ -26,11 +29,15 @@ import org.apache.flink.yarn.Messages.StopYarnSession
  * An extension of the TaskManager that listens for additional YARN related
  * messages.
  */
-class YarnTaskManager(connectionInfo: InstanceConnectionInfo, jobManagerAkkaURL: String,
-                  taskManagerConfig: TaskManagerConfiguration,
-                  networkConfig: NetworkEnvironmentConfiguration)
-
-  extends TaskManager(connectionInfo, jobManagerAkkaURL, taskManagerConfig, networkConfig) {
+class YarnTaskManager(config: TaskManagerConfiguration,
+                      connectionInfo: InstanceConnectionInfo,
+                      jobManagerAkkaURL: String,
+                      memoryManager: DefaultMemoryManager,
+                      ioManager: IOManager,
+                      network: NetworkEnvironment,
+                      numberOfSlots: Int)
+  extends TaskManager(config, connectionInfo, jobManagerAkkaURL,
+                      memoryManager, ioManager, network, numberOfSlots) {
 
 
   override def receiveWithLogMessages: Receive = {
