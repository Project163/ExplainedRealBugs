diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/SinkCodeGenerator.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/SinkCodeGenerator.scala
index 9f1a8103999..9a807f112a7 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/SinkCodeGenerator.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/SinkCodeGenerator.scala
@@ -18,27 +18,25 @@
 
 package org.apache.flink.table.planner.codegen
 
-import org.apache.flink.api.common.typeinfo.{AtomicType, TypeInformation}
-import org.apache.flink.api.common.typeutils.CompositeType
+import org.apache.flink.api.common.ExecutionConfig
+import org.apache.flink.api.common.typeinfo.{TypeInformation, Types}
 import org.apache.flink.api.java.tuple.{Tuple2 => JTuple2}
-import org.apache.flink.api.java.typeutils.{GenericTypeInfo, PojoTypeInfo, RowTypeInfo, TupleTypeInfo, TupleTypeInfoBase, TypeExtractor}
+import org.apache.flink.api.java.typeutils.{PojoTypeInfo, TupleTypeInfo}
+import org.apache.flink.api.java.typeutils.runtime.TupleSerializerBase
 import org.apache.flink.api.scala.createTuple2TypeInformation
-import org.apache.flink.api.scala.typeutils.CaseClassTypeInfo
-import org.apache.flink.streaming.api.datastream.DataStream
-import org.apache.flink.table.api.{Table, TableConfig, TableException, Types}
+import org.apache.flink.table.api.{TableConfig, TableException}
 import org.apache.flink.table.dataformat.util.BaseRowUtil
 import org.apache.flink.table.dataformat.{BaseRow, GenericRow}
 import org.apache.flink.table.planner.codegen.CodeGenUtils.genToExternal
+import org.apache.flink.table.planner.codegen.GeneratedExpression.NO_CODE
 import org.apache.flink.table.planner.codegen.OperatorCodeGenerator.generateCollect
+import org.apache.flink.table.planner.sinks.TableSinkUtils
 import org.apache.flink.table.runtime.operators.CodeGenOperatorFactory
-import org.apache.flink.table.runtime.types.PlannerTypeUtils
-import org.apache.flink.table.runtime.types.TypeInfoLogicalTypeConverter.fromTypeInfoToLogicalType
-import org.apache.flink.table.runtime.typeutils.BaseRowTypeInfo
+import org.apache.flink.table.runtime.types.TypeInfoDataTypeConverter.fromDataTypeToTypeInfo
 import org.apache.flink.table.sinks.TableSink
-import org.apache.flink.table.types.logical.utils.LogicalTypeChecks.areTypesCompatible
-import org.apache.flink.table.types.utils.TypeConversions.fromLegacyInfoToDataType
-import org.apache.flink.table.typeutils.TimeIndicatorTypeInfo
-import org.apache.flink.types.Row
+import org.apache.flink.table.types.logical.RowType
+
+import scala.collection.JavaConverters._
 
 object SinkCodeGenerator {
 
@@ -46,118 +44,94 @@ object SinkCodeGenerator {
   def generateRowConverterOperator[OUT](
       ctx: CodeGeneratorContext,
       config: TableConfig,
-      inputTypeInfo: BaseRowTypeInfo,
-      operatorName: String,
-      rowtimeField: Option[Int],
+      inputRowType: RowType,
+      sink: TableSink[_],
       withChangeFlag: Boolean,
-      resultType: TypeInformation[_],
-      sink: TableSink[_]): (CodeGenOperatorFactory[OUT], TypeInformation[OUT]) = {
-
-    val requestedTypeInfo = if (withChangeFlag) {
-      resultType match {
-        // Scala tuple
-        case t: CaseClassTypeInfo[_]
-          if t.getTypeClass == classOf[(_, _)] && t.getTypeAt(0) == Types.BOOLEAN =>
-          t.getTypeAt[Any](1)
-        // Java tuple
-        case t: TupleTypeInfo[_]
-          if t.getTypeClass == classOf[JTuple2[_, _]] && t.getTypeAt(0) == Types.BOOLEAN =>
-          t.getTypeAt[Any](1)
-        case _ => throw new TableException(
-          "Don't support " + resultType + " conversion for the retract sink")
-      }
-    } else {
-      resultType
-    }
+      operatorName: String): (CodeGenOperatorFactory[OUT], TypeInformation[OUT]) = {
 
-    /**
-      * The tpe may been inferred by invoking [[TypeExtractor.createTypeInfo]] based the class of
-      * the resulting type. For example, converts the given [[Table]] into an append [[DataStream]].
-      * If the class is Row, then the return type only is [[GenericTypeInfo[Row]]. So it should
-      * convert to the [[RowTypeInfo]] in order to better serialize performance.
-      *
-      */
-    val convertOutputType = requestedTypeInfo match {
-      case gt: GenericTypeInfo[Row] if gt.getTypeClass == classOf[Row] =>
-        new RowTypeInfo(
-          inputTypeInfo.getFieldTypes,
-          inputTypeInfo.getFieldNames)
-      case gt: GenericTypeInfo[BaseRow] if gt.getTypeClass == classOf[BaseRow] =>
-        new BaseRowTypeInfo(
-          inputTypeInfo.getLogicalTypes,
-          inputTypeInfo.getFieldNames)
-      case _ => requestedTypeInfo
-    }
-
-    checkRowConverterValid(inputTypeInfo, convertOutputType)
+    val physicalOutputType = TableSinkUtils.inferSinkPhysicalDataType(
+      sink.getConsumedDataType,
+      inputRowType,
+      withChangeFlag)
 
-    //update out put type info
     val outputTypeInfo = if (withChangeFlag) {
-      resultType match {
-        // Scala tuple
-        case t: CaseClassTypeInfo[_]
-          if t.getTypeClass == classOf[(_, _)] && t.getTypeAt(0) == Types.BOOLEAN =>
-          createTuple2TypeInformation(t.getTypeAt(0), convertOutputType)
-        // Java tuple
-        case t: TupleTypeInfo[_]
-          if t.getTypeClass == classOf[JTuple2[_, _]] && t.getTypeAt(0) == Types.BOOLEAN =>
-          new TupleTypeInfo(t.getTypeAt(0), convertOutputType)
+      val typeInfo = fromDataTypeToTypeInfo(physicalOutputType)
+      val consumedClass = sink.getConsumedDataType.getConversionClass
+      if (consumedClass == classOf[(_, _)]) {
+        createTuple2TypeInformation(Types.BOOLEAN, typeInfo)
+      } else if (consumedClass == classOf[JTuple2[_, _]]) {
+        new TupleTypeInfo(Types.BOOLEAN, typeInfo)
       }
     } else {
-      convertOutputType
+      fromDataTypeToTypeInfo(physicalOutputType)
     }
 
     val inputTerm = CodeGenUtils.DEFAULT_INPUT1_TERM
     var afterIndexModify = inputTerm
-    val fieldIndexProcessCode =
-      if (!resultType.isInstanceOf[PojoTypeInfo[_]]) {
-        ""
-      } else {
-        // field index may change (pojo)
-        val mapping = convertOutputType match {
-          case ct: CompositeType[_] => ct.getFieldNames.map {
-            name =>
-              val index = inputTypeInfo.getFieldIndex(name)
-              if (index < 0) {
-                throw new TableException(
-                  s"$name is not found in ${inputTypeInfo.getFieldNames.mkString(", ")}")
-              }
-              index
+    val fieldIndexProcessCode = outputTypeInfo match {
+      case pojo: PojoTypeInfo[_] =>
+        val mapping = pojo.getFieldNames.map { name =>
+          val index = inputRowType.getFieldIndex(name)
+          if (index < 0) {
+            throw new TableException(
+              s"$name is not found in ${inputRowType.getFieldNames.asScala.mkString(", ")}")
           }
-          case _ => Array(0)
+          index
         }
-
-        val resultGenerator = new ExprCodeGenerator(ctx, false).bindInput(
-          fromTypeInfoToLogicalType(inputTypeInfo),
-          inputTerm,
-          inputFieldMapping = Option(mapping))
-        val outputBaseRowType = new BaseRowTypeInfo(
-            getCompositeTypes(convertOutputType).map(fromTypeInfoToLogicalType): _*)
+        val resultGenerator = new ExprCodeGenerator(ctx, false)
+          .bindInput(
+            inputRowType,
+            inputTerm,
+            inputFieldMapping = Option(mapping))
         val conversion = resultGenerator.generateConverterResultExpression(
-          outputBaseRowType.toRowType,
+          inputRowType,
           classOf[GenericRow])
         afterIndexModify = CodeGenUtils.newName("afterIndexModify")
         s"""
            |${conversion.code}
            |${classOf[BaseRow].getCanonicalName} $afterIndexModify = ${conversion.resultTerm};
            |""".stripMargin
-      }
+      case _ =>
+        NO_CODE
+    }
 
-    val retractProcessCode = if (!withChangeFlag) {
-      generateCollect(
-        genToExternal(ctx, fromLegacyInfoToDataType(outputTypeInfo), afterIndexModify))
-    } else {
+    val consumedDataType = sink.getConsumedDataType
+    val outTerm = genToExternal(ctx, physicalOutputType, afterIndexModify)
+    val retractProcessCode = if (withChangeFlag) {
       val flagResultTerm =
         s"${classOf[BaseRowUtil].getCanonicalName}.isAccumulateMsg($afterIndexModify)"
       val resultTerm = CodeGenUtils.newName("result")
-      val genericRowField = classOf[GenericRow].getCanonicalName
-      s"""
-         |$genericRowField $resultTerm = new $genericRowField(2);
-         |$resultTerm.setField(0, $flagResultTerm);
-         |$resultTerm.setField(1, $afterIndexModify);
-         |${generateCollect(
-        genToExternal(ctx, fromLegacyInfoToDataType(outputTypeInfo), resultTerm))}
-          """.stripMargin
+      if (consumedDataType.getConversionClass == classOf[JTuple2[_, _]]) {
+        // Java Tuple2
+        val tupleClass = consumedDataType.getConversionClass.getCanonicalName
+        s"""
+           |$tupleClass $resultTerm = new $tupleClass();
+           |$resultTerm.setField($flagResultTerm, 0);
+           |$resultTerm.setField($outTerm, 1);
+           |${generateCollect(resultTerm)}
+         """.stripMargin
+      } else {
+        // Scala Case Class
+        val tupleClass = consumedDataType.getConversionClass.getCanonicalName
+        val scalaTupleSerializer = fromDataTypeToTypeInfo(consumedDataType)
+          .createSerializer(new ExecutionConfig)
+          .asInstanceOf[TupleSerializerBase[_]]
+        val serializerTerm = ctx.addReusableObject(
+          scalaTupleSerializer,
+          "serializer",
+          classOf[TupleSerializerBase[_]].getCanonicalName)
+        val fieldsTerm = CodeGenUtils.newName("fields")
+
+        s"""
+           |Object[] $fieldsTerm = new Object[2];
+           |$fieldsTerm[0] = $flagResultTerm;
+           |$fieldsTerm[1] = $outTerm;
+           |$tupleClass $resultTerm = ($tupleClass) $serializerTerm.createInstance($fieldsTerm);
+           |${generateCollect(resultTerm)}
+         """.stripMargin
+      }
+    } else {
+      generateCollect(outTerm)
     }
 
     val generated = OperatorCodeGenerator.generateOneInputStreamOperator[BaseRow, OUT](
@@ -167,96 +141,7 @@ object SinkCodeGenerator {
          |$fieldIndexProcessCode
          |$retractProcessCode
          |""".stripMargin,
-      fromTypeInfoToLogicalType(inputTypeInfo))
+      inputRowType)
     (new CodeGenOperatorFactory[OUT](generated), outputTypeInfo.asInstanceOf[TypeInformation[OUT]])
   }
-
-  private def checkRowConverterValid[OUT](
-      inputTypeInfo: BaseRowTypeInfo,
-      requestedTypeInfo: TypeInformation[OUT]): Unit = {
-
-    val fieldTypes = inputTypeInfo.getFieldTypes
-    val fieldNames = inputTypeInfo.getFieldNames
-
-    // check for valid type info
-    if (!requestedTypeInfo.isInstanceOf[GenericTypeInfo[_]] &&
-        requestedTypeInfo.getArity != fieldTypes.length) {
-      throw new TableException(
-        s"Arity [${fieldTypes.length}] of result [$fieldTypes] does not match " +
-            s"the number[${requestedTypeInfo.getArity}] of requested type [$requestedTypeInfo].")
-    }
-
-    // check requested types
-
-    def validateFieldType(fieldType: TypeInformation[_]): Unit = fieldType match {
-      case _: TimeIndicatorTypeInfo =>
-        throw new TableException("The time indicator type is an internal type only.")
-      case _ => // ok
-    }
-
-    requestedTypeInfo match {
-      // POJO type requested
-      case pt: PojoTypeInfo[_] =>
-        fieldNames.zip(fieldTypes) foreach {
-          case (fName, fType) =>
-            val pojoIdx = pt.getFieldIndex(fName)
-            if (pojoIdx < 0) {
-              throw new TableException(s"POJO does not define field name: $fName")
-            }
-            val requestedTypeInfo = pt.getTypeAt(pojoIdx)
-            validateFieldType(requestedTypeInfo)
-            if (fType != requestedTypeInfo) {
-              throw new TableException(s"Result field '$fName' does not match requested type. " +
-                  s"Requested: $requestedTypeInfo; Actual: $fType")
-            }
-        }
-
-      // Tuple/Case class/Row type requested
-      case tt: TupleTypeInfoBase[_] =>
-        fieldTypes.zipWithIndex foreach {
-          case (fieldTypeInfo: GenericTypeInfo[_], i) =>
-            val requestedTypeInfo = tt.getTypeAt(i)
-            if (!requestedTypeInfo.isInstanceOf[GenericTypeInfo[Object]]) {
-              throw new TableException(
-                s"Result field '${fieldNames(i)}' does not match requested type. " +
-                    s"Requested: $requestedTypeInfo; Actual: $fieldTypeInfo")
-            }
-          case (fieldTypeInfo, i) =>
-            val requestedTypeInfo = tt.getTypeAt(i)
-            validateFieldType(requestedTypeInfo)
-            // it's safe to be only assignable, because the conversion from internal type (Decimal)
-            // to external type (BigDecimal) doesn't loose precision, the internal type already
-            // matches to the expected type defined in DDL.
-            if (!PlannerTypeUtils.isAssignable(
-              fromTypeInfoToLogicalType(fieldTypeInfo),
-              fromTypeInfoToLogicalType(requestedTypeInfo)) &&
-              !requestedTypeInfo.isInstanceOf[GenericTypeInfo[Object]]) {
-              val fieldNames = tt.getFieldNames
-              throw new TableException(s"Result field '${fieldNames(i)}' does not match requested" +
-                  s" type. Requested: $requestedTypeInfo; Actual: $fieldTypeInfo")
-            }
-        }
-
-      // Atomic type requested
-      case at: AtomicType[_] =>
-        if (fieldTypes.size != 1) {
-          throw new TableException(s"Requested result type is an atomic type but " +
-              s"result[$fieldTypes] has more or less than a single field.")
-        }
-        val requestedTypeInfo = fieldTypes.head
-        validateFieldType(requestedTypeInfo)
-        if (requestedTypeInfo != at) {
-          throw new TableException(s"Result field does not match requested type. " +
-              s"Requested: $at; Actual: $requestedTypeInfo")
-        }
-
-      case _ =>
-        throw new TableException(s"Unsupported result type: $requestedTypeInfo")
-    }
-  }
-
-  def getCompositeTypes(t: TypeInformation[_]): Array[TypeInformation[_]] = t match {
-    case ct: CompositeType[_] => (0 until ct.getArity).map(ct.getTypeAt).toArray
-    case _ => Array[TypeInformation[_]](t)
-  }
 }
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/delegation/PlannerBase.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/delegation/PlannerBase.scala
index 0baca5c3e67..ab084e5bc88 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/delegation/PlannerBase.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/delegation/PlannerBase.scala
@@ -20,7 +20,6 @@ package org.apache.flink.table.planner.delegation
 
 import org.apache.flink.annotation.VisibleForTesting
 import org.apache.flink.api.dag.Transformation
-import org.apache.flink.configuration.Configuration
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment
 import org.apache.flink.table.api.config.ExecutionConfigOptions
 import org.apache.flink.table.api.{TableConfig, TableEnvironment, TableException}
@@ -38,10 +37,12 @@ import org.apache.flink.table.planner.plan.nodes.physical.FlinkPhysicalRel
 import org.apache.flink.table.planner.plan.optimize.Optimizer
 import org.apache.flink.table.planner.plan.reuse.SubplanReuser
 import org.apache.flink.table.planner.plan.utils.SameRelObjectShuttle
-import org.apache.flink.table.planner.sinks.{DataStreamTableSink, TableSinkUtils}
+import org.apache.flink.table.planner.sinks.DataStreamTableSink
+import org.apache.flink.table.planner.sinks.TableSinkUtils.{inferSinkPhysicalSchema, validateLogicalPhysicalTypesCompatible, validateSchemaAndApplyImplicitCast, validateTableSink}
 import org.apache.flink.table.planner.utils.JavaScalaConversionUtil
 import org.apache.flink.table.sinks.{OverwritableTableSink, TableSink}
 import org.apache.flink.table.types.utils.LegacyTypeInfoDataTypeConverter
+import org.apache.flink.table.utils.TableSchemaUtils
 
 import org.apache.calcite.jdbc.CalciteSchemaBuilder.asRootSchema
 import org.apache.calcite.plan.{RelTrait, RelTraitDef}
@@ -174,23 +175,33 @@ abstract class PlannerBase(
     modifyOperation match {
       case s: UnregisteredSinkModifyOperation[_] =>
         val input = getRelBuilder.queryOperation(s.getChild).build()
-        LogicalSink.create(input, s.getSink, "UnregisteredSink")
+        val sinkSchema = s.getSink.getTableSchema
+        // validate query schema and sink schema, and apply cast if possible
+        val query = validateSchemaAndApplyImplicitCast(input, sinkSchema, getTypeFactory)
+        LogicalSink.create(
+          query,
+          s.getSink,
+          "UnregisteredSink",
+          ConnectorCatalogTable.sink(s.getSink, !isStreamingMode))
 
       case catalogSink: CatalogSinkModifyOperation =>
         val input = getRelBuilder.queryOperation(modifyOperation.getChild).build()
         val identifier = catalogSink.getTableIdentifier
         getTableSink(identifier).map { case (table, sink) =>
-          TableSinkUtils.validateSink(catalogSink, identifier, sink, table.getPartitionKeys)
-          sink match {
-            case overwritableTableSink: OverwritableTableSink =>
-              overwritableTableSink.setOverwrite(catalogSink.isOverwrite)
-            case _ =>
-              assert(!catalogSink.isOverwrite, "INSERT OVERWRITE requires " +
-                s"${classOf[OverwritableTableSink].getSimpleName} but actually got " +
-                sink.getClass.getName)
-          }
-          LogicalSink.create(
+          // check the logical field type and physical field type are compatible
+          val queryLogicalType = FlinkTypeFactory.toLogicalRowType(input.getRowType)
+          // validate logical schema and physical schema are compatible
+          validateLogicalPhysicalTypesCompatible(table, sink, queryLogicalType)
+          // validate TableSink
+          validateTableSink(catalogSink, identifier, sink, table.getPartitionKeys)
+          // validate query schema and sink schema, and apply cast if possible
+          val query = validateSchemaAndApplyImplicitCast(
             input,
+            TableSchemaUtils.getPhysicalSchema(table.getSchema),
+            getTypeFactory,
+            Some(catalogSink.getTableIdentifier.asSummaryString()))
+          LogicalSink.create(
+            query,
             sink,
             identifier.toString,
             table,
@@ -209,9 +220,23 @@ abstract class PlannerBase(
           case UpdateMode.UPSERT => (false, true)
         }
         val typeInfo = LegacyTypeInfoDataTypeConverter.toLegacyTypeInfo(outputConversion.getType)
+        val inputLogicalType = FlinkTypeFactory.toLogicalRowType(input.getRowType)
+        val sinkPhysicalSchema = inferSinkPhysicalSchema(
+          outputConversion.getType,
+          inputLogicalType,
+          withChangeFlag)
+        // validate query schema and sink schema, and apply cast if possible
+        val query = validateSchemaAndApplyImplicitCast(input, sinkPhysicalSchema, getTypeFactory)
         val tableSink = new DataStreamTableSink(
-          outputConversion.getChild, typeInfo, updatesAsRetraction, withChangeFlag)
-        LogicalSink.create(input, tableSink, "DataStreamTableSink")
+          FlinkTypeFactory.toTableSchema(query.getRowType),
+          typeInfo,
+          updatesAsRetraction,
+          withChangeFlag)
+        LogicalSink.create(
+          query,
+          tableSink,
+          "DataStreamTableSink",
+          ConnectorCatalogTable.sink(tableSink, !isStreamingMode))
 
       case _ =>
         throw new TableException(s"Unsupported ModifyOperation: $modifyOperation")
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdUniqueKeys.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdUniqueKeys.scala
index 7d47093404c..4efddb402f0 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdUniqueKeys.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdUniqueKeys.scala
@@ -18,7 +18,9 @@
 
 package org.apache.flink.table.planner.plan.metadata
 
+import org.apache.flink.table.planner._
 import org.apache.flink.table.planner.calcite.FlinkRelBuilder.PlannerNamedWindowProperty
+import org.apache.flink.table.planner.calcite.FlinkTypeFactory
 import org.apache.flink.table.planner.plan.nodes.calcite.{Expand, Rank, WindowAggregate}
 import org.apache.flink.table.planner.plan.nodes.common.CommonLookupJoin
 import org.apache.flink.table.planner.plan.nodes.logical._
@@ -26,11 +28,10 @@ import org.apache.flink.table.planner.plan.nodes.physical.batch._
 import org.apache.flink.table.planner.plan.nodes.physical.stream._
 import org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase
 import org.apache.flink.table.planner.plan.utils.{FlinkRelMdUtil, RankUtil}
-import org.apache.flink.table.planner.{JArrayList, JBoolean, JHashMap, JHashSet, JList, JSet}
 import org.apache.flink.table.runtime.operators.rank.RankType
 import org.apache.flink.table.sources.TableSource
+import org.apache.flink.table.types.logical.utils.LogicalTypeCasts
 
-import com.google.common.collect.ImmutableSet
 import org.apache.calcite.plan.RelOptTable
 import org.apache.calcite.plan.volcano.RelSubset
 import org.apache.calcite.rel.`type`.RelDataType
@@ -42,6 +43,8 @@ import org.apache.calcite.sql.SqlKind
 import org.apache.calcite.sql.fun.SqlStdOperatorTable
 import org.apache.calcite.util.{Bug, BuiltInMethod, ImmutableBitSet, Util}
 
+import com.google.common.collect.ImmutableSet
+
 import java.util
 
 import scala.collection.JavaConversions._
@@ -135,8 +138,8 @@ class FlinkRelMdUniqueKeys private extends MetadataHandler[BuiltInMetadata.Uniqu
                 }
               case _ => // ignore
             }
-          //rename
-          case a: RexCall if a.getKind.equals(SqlKind.AS) &&
+          //rename or cast
+          case a: RexCall if (a.getKind.equals(SqlKind.AS) || isFidelityCast(a)) &&
             a.getOperands.get(0).isInstanceOf[RexInputRef] =>
             appendMapInToOutPos(a.getOperands.get(0).asInstanceOf[RexInputRef].getIndex, i)
           case _ => // ignore
@@ -173,6 +176,18 @@ class FlinkRelMdUniqueKeys private extends MetadataHandler[BuiltInMetadata.Uniqu
     projUniqueKeySet
   }
 
+  /**
+    * Whether the [[RexCall]] is a cast that doesn't lose any information.
+    */
+  private def isFidelityCast(call: RexCall): Boolean = {
+    if (call.getKind != SqlKind.CAST) {
+      return false
+    }
+    val originalType = FlinkTypeFactory.toLogicalType(call.getOperands.get(0).getType)
+    val newType = FlinkTypeFactory.toLogicalType(call.getType)
+    LogicalTypeCasts.supportsImplicitCast(originalType, newType)
+  }
+
   def getUniqueKeys(
       rel: Expand,
       mq: RelMetadataQuery,
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSink.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSink.scala
index 1f20d819d76..aeeba515d84 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSink.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSink.scala
@@ -30,7 +30,6 @@ import org.apache.flink.table.planner.plan.nodes.calcite.Sink
 import org.apache.flink.table.planner.plan.nodes.exec.{BatchExecNode, ExecNode}
 import org.apache.flink.table.planner.sinks.DataStreamTableSink
 import org.apache.flink.table.runtime.types.ClassLogicalTypeConverter
-import org.apache.flink.table.runtime.types.TypeInfoDataTypeConverter.fromDataTypeToTypeInfo
 import org.apache.flink.table.runtime.typeutils.BaseRowTypeInfo
 import org.apache.flink.table.sinks.{RetractStreamTableSink, StreamTableSink, TableSink, UpsertStreamTableSink}
 import org.apache.flink.table.types.DataType
@@ -117,7 +116,6 @@ class BatchExecSink[T](
       planner: BatchPlanner): Transformation[T] = {
     val config = planner.getTableConfig
     val resultDataType = sink.getConsumedDataType
-    val resultType = fromDataTypeToTypeInfo(resultDataType)
     validateType(resultDataType)
     val inputNode = getInputNodes.get(0)
     inputNode match {
@@ -130,16 +128,14 @@ class BatchExecSink[T](
           val (converterOperator, outputTypeInfo) = generateRowConverterOperator[T](
             CodeGeneratorContext(config),
             config,
-            plan.getOutputType.asInstanceOf[BaseRowTypeInfo],
-            "SinkConversion",
-            None,
+            plan.getOutputType.asInstanceOf[BaseRowTypeInfo].toRowType,
+            sink,
             withChangeFlag,
-            resultType,
-            sink
+            "SinkConversion"
           )
           ExecNode.createOneInputTransformation(
             plan,
-            s"SinkConversionTo${resultType.getTypeClass.getSimpleName}",
+            s"SinkConversionTo${resultDataType.getConversionClass.getSimpleName}",
             converterOperator,
             outputTypeInfo,
             plan.getParallelism)
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecSink.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecSink.scala
index 9140644ebbe..96883a85760 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecSink.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecSink.scala
@@ -32,11 +32,9 @@ import org.apache.flink.table.planner.plan.nodes.calcite.Sink
 import org.apache.flink.table.planner.plan.nodes.exec.{ExecNode, StreamExecNode}
 import org.apache.flink.table.planner.plan.utils.UpdatingPlanChecker
 import org.apache.flink.table.planner.sinks.DataStreamTableSink
-import org.apache.flink.table.runtime.types.TypeInfoDataTypeConverter
 import org.apache.flink.table.runtime.typeutils.{BaseRowTypeInfo, TypeCheckUtils}
 import org.apache.flink.table.sinks._
 import org.apache.flink.table.types.logical.TimestampType
-
 import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}
 import org.apache.calcite.rel.RelNode
 
@@ -210,24 +208,22 @@ class StreamExecSink[T](
     } else {
       parTransformation.getOutputType
     }
+
     val resultDataType = sink.getConsumedDataType
-    val resultType = TypeInfoDataTypeConverter.fromDataTypeToTypeInfo(resultDataType)
     if (CodeGenUtils.isInternalClass(resultDataType)) {
       parTransformation.asInstanceOf[Transformation[T]]
     } else {
       val (converterOperator, outputTypeInfo) = generateRowConverterOperator[T](
         CodeGeneratorContext(config),
         config,
-        convType.asInstanceOf[BaseRowTypeInfo],
-        "SinkConversion",
-        None,
+        convType.asInstanceOf[BaseRowTypeInfo].toRowType,
+        sink,
         withChangeFlag,
-        resultType,
-        sink
+        "SinkConversion"
       )
       new OneInputTransformation(
         parTransformation,
-        s"SinkConversionTo${resultType.getTypeClass.getSimpleName}",
+        s"SinkConversionTo${resultDataType.getConversionClass.getSimpleName}",
         converterOperator,
         outputTypeInfo,
         parTransformation.getParallelism)
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/logical/BatchLogicalWindowAggregateRule.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/logical/BatchLogicalWindowAggregateRule.scala
index 68d20d4ad08..cb810acc80f 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/logical/BatchLogicalWindowAggregateRule.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/logical/BatchLogicalWindowAggregateRule.scala
@@ -28,6 +28,7 @@ import org.apache.flink.table.runtime.types.LogicalTypeDataTypeConverter.fromLog
 import org.apache.calcite.rel.`type`.RelDataType
 import org.apache.calcite.rel.logical.{LogicalAggregate, LogicalProject}
 import org.apache.calcite.rex._
+import org.apache.calcite.sql.SqlKind
 
 import _root_.java.math.{BigDecimal => JBigDecimal}
 
@@ -57,6 +58,8 @@ class BatchLogicalWindowAggregateRule
       windowExprIdx: Int,
       rowType: RelDataType): FieldReferenceExpression = {
     operand match {
+      case c: RexCall if c.getKind == SqlKind.CAST =>
+        getTimeFieldReference(c.getOperands.get(0), windowExprIdx, rowType)
       // match TUMBLE_ROWTIME and TUMBLE_PROCTIME
       case c: RexCall if c.getOperands.size() == 1 &&
         FlinkTypeFactory.isTimeIndicatorType(c.getType) =>
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/sinks/DataStreamTableSink.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/sinks/DataStreamTableSink.scala
index 3d5ed84f0b0..a2e1ffe7429 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/sinks/DataStreamTableSink.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/sinks/DataStreamTableSink.scala
@@ -21,15 +21,12 @@ package org.apache.flink.table.planner.sinks
 import org.apache.flink.annotation.Internal
 import org.apache.flink.api.common.typeinfo.TypeInformation
 import org.apache.flink.streaming.api.datastream.DataStream
-import org.apache.flink.table.api.{Table, TableException}
-import org.apache.flink.table.operations.QueryOperation
-import org.apache.flink.table.runtime.types.TypeInfoDataTypeConverter.fromDataTypeToTypeInfo
+import org.apache.flink.table.api.{Table, TableException, TableSchema}
 import org.apache.flink.table.sinks.TableSink
 
 /**
   * A [[DataStreamTableSink]] specifies how to emit a [[Table]] to an DataStream[T]
   *
-  * @param queryOperation The [[QueryOperation]] to emit.
   * @param outputType The [[TypeInformation]] that specifies the type of the [[DataStream]].
   * @param updatesAsRetraction Set to true to encode updates as retraction messages.
   * @param withChangeFlag Set to true to emit records with change flags.
@@ -37,13 +34,11 @@ import org.apache.flink.table.sinks.TableSink
   */
 @Internal
 class DataStreamTableSink[T](
-    queryOperation: QueryOperation,
+    tableSchema: TableSchema,
     outputType: TypeInformation[T],
     val updatesAsRetraction: Boolean,
     val withChangeFlag: Boolean) extends TableSink[T] {
 
-  private lazy val tableSchema = queryOperation.getTableSchema
-
   /**
     * Return the type expected by this [[TableSink]].
     *
@@ -53,12 +48,7 @@ class DataStreamTableSink[T](
     */
   override def getOutputType: TypeInformation[T] = outputType
 
-  /** Returns the types of the table fields. */
-  override def getFieldTypes: Array[TypeInformation[_]] =
-    Array(tableSchema.getFieldDataTypes.map(fromDataTypeToTypeInfo): _*)
-
-  /** Returns the names of the table fields. */
-  override def getFieldNames: Array[String] = tableSchema.getFieldNames
+  override def getTableSchema: TableSchema = tableSchema
 
   override def configure(
       fieldNames: Array[String],
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/sinks/TableSinkUtils.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/sinks/TableSinkUtils.scala
index 7ea93a76049..63e6bbe21b8 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/sinks/TableSinkUtils.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/sinks/TableSinkUtils.scala
@@ -18,21 +18,98 @@
 
 package org.apache.flink.table.planner.sinks
 
-import org.apache.flink.table.api.ValidationException
-import org.apache.flink.table.catalog.ObjectIdentifier
+import org.apache.flink.api.java.tuple.{Tuple2 => JTuple2}
+import org.apache.flink.api.java.typeutils.{GenericTypeInfo, TupleTypeInfo}
+import org.apache.flink.api.scala.typeutils.CaseClassTypeInfo
+import org.apache.flink.table.api._
+import org.apache.flink.table.catalog.{CatalogTable, ObjectIdentifier}
+import org.apache.flink.table.dataformat.BaseRow
 import org.apache.flink.table.operations.CatalogSinkModifyOperation
-import org.apache.flink.table.runtime.types.LogicalTypeDataTypeConverter.fromDataTypeToLogicalType
-import org.apache.flink.table.runtime.types.PlannerTypeUtils
-import org.apache.flink.table.sinks.{PartitionableTableSink, TableSink}
+import org.apache.flink.table.planner.calcite.FlinkTypeFactory
+import org.apache.flink.table.planner.plan.utils.RelOptUtils
+import org.apache.flink.table.runtime.typeutils.BaseRowTypeInfo
+import org.apache.flink.table.sinks._
+import org.apache.flink.table.types.DataType
+import org.apache.flink.table.types.inference.TypeTransformations.{legacyDecimalToDefaultDecimal, toNullable}
+import org.apache.flink.table.types.logical.utils.{LogicalTypeCasts, LogicalTypeChecks}
+import org.apache.flink.table.types.logical.{LegacyTypeInformationType, LogicalType, RowType}
+import org.apache.flink.table.types.utils.DataTypeUtils
+import org.apache.flink.table.types.utils.TypeConversions.{fromLegacyInfoToDataType, fromLogicalToDataType}
+import org.apache.flink.table.utils.{TableSchemaUtils, TypeMappingUtils}
+import org.apache.flink.types.Row
 
-import scala.collection.JavaConversions._
+import org.apache.calcite.rel.RelNode
+
+import _root_.scala.collection.JavaConversions._
 
 object TableSinkUtils {
 
   /**
-    * Checks if the given [[CatalogSinkModifyOperation]]'s query can be written to
-    * the given [[TableSink]]. It checks if the names & the field types match. If the table
-    * sink is a [[PartitionableTableSink]], also check that the partitions are valid.
+    * Checks if the given query can be written into the given sink. It checks the field types
+    * should be compatible (types should equal including precisions). If types are not compatible,
+    * but can be implicitly casted, a cast projection will be applied. Otherwise, an exception will
+    * be thrown.
+    *
+    * @param query the query to be checked
+    * @param sinkSchema the schema of sink to be checked
+    * @param typeFactory type factory
+    * @return the query RelNode which may be applied the implicitly cast projection.
+    */
+  def validateSchemaAndApplyImplicitCast(
+      query: RelNode,
+      sinkSchema: TableSchema,
+      typeFactory: FlinkTypeFactory,
+      sinkIdentifier: Option[String] = None): RelNode = {
+
+    val queryLogicalType = FlinkTypeFactory.toLogicalRowType(query.getRowType)
+    val sinkLogicalType = DataTypeUtils
+      // we recognize legacy decimal is the same to default decimal
+      .transform(sinkSchema.toRowDataType, legacyDecimalToDefaultDecimal)
+      .getLogicalType
+      .asInstanceOf[RowType]
+    if (LogicalTypeCasts.supportsImplicitCast(queryLogicalType, sinkLogicalType)) {
+      // the query can be written into sink
+      // but we may need to add a cast project if the types are not compatible
+      if (LogicalTypeChecks.areTypesCompatible(
+          nullableLogicalType(queryLogicalType), nullableLogicalType(sinkLogicalType))) {
+        // types are compatible excepts nullable, do not need cast project
+        // we ignores nullable to avoid cast project as cast non-null to nullable is redundant
+        query
+      } else {
+        // otherwise, add a cast project
+        val castedDataType = typeFactory.buildRelNodeRowType(
+          sinkLogicalType.getFieldNames,
+          sinkLogicalType.getFields.map(_.getType))
+        RelOptUtils.createCastRel(query, castedDataType)
+      }
+    } else {
+      // format query and sink schema strings
+      val srcSchema = queryLogicalType.getFields
+        .map(f => s"${f.getName}: ${f.getType}")
+        .mkString("[", ", ", "]")
+      val sinkSchema = sinkLogicalType.getFields
+        .map(f => s"${f.getName}: ${f.getType}")
+        .mkString("[", ", ", "]")
+
+      val sinkDesc: String = sinkIdentifier.getOrElse("")
+
+      throw new ValidationException(
+        s"Field types of query result and registered TableSink $sinkDesc do not match.\n" +
+          s"Query schema: $srcSchema\n" +
+          s"Sink schema: $sinkSchema")
+    }
+  }
+
+  /**
+    * Make the logical type nullable recursively.
+    */
+  private def nullableLogicalType(logicalType: LogicalType): LogicalType = {
+    DataTypeUtils.transform(fromLogicalToDataType(logicalType), toNullable).getLogicalType
+  }
+
+  /**
+    * It checks whether the [[TableSink]] is compatible to the INSERT INTO clause, e.g.
+    * whether the sink is a [[PartitionableTableSink]] and the partitions are valid.
     *
     * @param sinkOperation The sink operation with the query that is supposed to be written.
     * @param sinkIdentifier Tha path of the sink. It is needed just for logging. It does not
@@ -40,45 +117,11 @@ object TableSinkUtils {
     * @param sink     The sink that we want to write to.
     * @param partitionKeys The partition keys of this table.
     */
-  def validateSink(
+  def validateTableSink(
       sinkOperation: CatalogSinkModifyOperation,
       sinkIdentifier: ObjectIdentifier,
       sink: TableSink[_],
       partitionKeys: Seq[String]): Unit = {
-    val query = sinkOperation.getChild
-    // validate schema of source table and table sink
-    val srcFieldTypes = query.getTableSchema.getFieldDataTypes
-    val sinkFieldTypes = sink.getTableSchema.getFieldDataTypes
-
-    val srcLogicalTypes = srcFieldTypes.map(t => fromDataTypeToLogicalType(t))
-    val sinkLogicalTypes = sinkFieldTypes.map(t => fromDataTypeToLogicalType(t))
-
-    if (srcLogicalTypes.length != sinkLogicalTypes.length ||
-      srcLogicalTypes.zip(sinkLogicalTypes).exists {
-        case (srcType, sinkType) =>
-          // it's safe to be only assignable, because the conversion from internal type (Decimal)
-          // to external type (BigDecimal) doesn't loose precision, the internal type already
-          // matches to the expected type defined in DDL.
-          !PlannerTypeUtils.isAssignable(srcType, sinkType)
-      }) {
-
-      val srcFieldNames = query.getTableSchema.getFieldNames
-      val sinkFieldNames = sink.getTableSchema.getFieldNames
-
-      // format table and table sink schema strings
-      val srcSchema = srcFieldNames.zip(srcLogicalTypes)
-        .map { case (n, t) => s"$n: $t" }
-        .mkString("[", ", ", "]")
-      val sinkSchema = sinkFieldNames.zip(sinkLogicalTypes)
-        .map { case (n, t) => s"$n: $t" }
-        .mkString("[", ", ", "]")
-
-      throw new ValidationException(
-        s"Field types of query result and registered TableSink " +
-          s"$sinkIdentifier do not match.\n" +
-          s"Query result schema: $srcSchema\n" +
-          s"TableSink schema:    $sinkSchema")
-    }
 
     // check partitions are valid
     if (partitionKeys.nonEmpty) {
@@ -98,5 +141,155 @@ object TableSinkUtils {
         }
       }
     }
+
+    sink match {
+      case overwritableTableSink: OverwritableTableSink =>
+        overwritableTableSink.setOverwrite(sinkOperation.isOverwrite)
+      case _ =>
+        assert(!sinkOperation.isOverwrite, "INSERT OVERWRITE requires " +
+          s"${classOf[OverwritableTableSink].getSimpleName} but actually got " +
+          sink.getClass.getName)
+    }
+  }
+
+  /**
+    * Inferences the physical schema of [[TableSink]], the physical schema ignores change flag
+    * field and normalizes physical types (can be generic type or POJO type) into [[TableSchema]].
+    * @param queryLogicalType the logical type of query, will be used to full-fill sink physical
+    *                         schema if the sink physical type is not specified.
+    * @param sink the instance of [[TableSink]]
+    */
+  def inferSinkPhysicalSchema(
+      queryLogicalType: RowType,
+      sink: TableSink[_]): TableSchema = {
+    val withChangeFlag = sink match {
+      case _: RetractStreamTableSink[_] | _: UpsertStreamTableSink[_] => true
+      case _: StreamTableSink[_] => false
+      case dsts: DataStreamTableSink[_] => dsts.withChangeFlag
+    }
+    inferSinkPhysicalSchema(sink.getConsumedDataType, queryLogicalType, withChangeFlag)
+  }
+
+  /**
+    * Inferences the physical schema of [[TableSink]], the physical schema ignores change flag
+    * field and normalizes physical types (can be generic type or POJO type) into [[TableSchema]].
+    *
+    * @param consumedDataType the consumed data type of sink
+    * @param queryLogicalType the logical type of query, will be used to full-fill sink physical
+    *                         schema if the sink physical type is not specified.
+    * @param withChangeFlag true if the emitted records contains change flags.
+    */
+  def inferSinkPhysicalSchema(
+      consumedDataType: DataType,
+      queryLogicalType: RowType,
+      withChangeFlag: Boolean): TableSchema = {
+    // the requested output physical type which ignores the flag field
+    val requestedOutputType = inferSinkPhysicalDataType(
+      consumedDataType,
+      queryLogicalType,
+      withChangeFlag)
+    if (LogicalTypeChecks.isCompositeType(requestedOutputType.getLogicalType)) {
+      DataTypeUtils.expandCompositeTypeToSchema(requestedOutputType)
+    } else {
+      // atomic type
+      TableSchema.builder().field("f0", requestedOutputType).build()
+    }
+  }
+
+  /**
+    * Inferences the physical data type of [[TableSink]], the physical data type ignores
+    * the change flag field.
+    *
+    * @param consumedDataType the consumed data type of sink
+    * @param queryLogicalType the logical type of query, will be used to full-fill sink physical
+    *                         schema if the sink physical type is not specified.
+    * @param withChangeFlag true if the emitted records contains change flags.
+    */
+  def inferSinkPhysicalDataType(
+      consumedDataType: DataType,
+      queryLogicalType: RowType,
+      withChangeFlag: Boolean): DataType = {
+    consumedDataType.getLogicalType match {
+      case lt: LegacyTypeInformationType[_] =>
+        val requestedTypeInfo = if (withChangeFlag) {
+          lt.getTypeInformation match {
+            // Scala tuple
+            case t: CaseClassTypeInfo[_]
+              if t.getTypeClass == classOf[(_, _)] && t.getTypeAt(0) == Types.BOOLEAN =>
+              t.getTypeAt[Any](1)
+            // Java tuple
+            case t: TupleTypeInfo[_]
+              if t.getTypeClass == classOf[JTuple2[_, _]] && t.getTypeAt(0) == Types.BOOLEAN =>
+              t.getTypeAt[Any](1)
+            case _ => throw new TableException(
+              "Don't support " + consumedDataType + " conversion for the retract sink")
+          }
+        } else {
+          lt.getTypeInformation
+        }
+        // The tpe may been inferred by invoking [[TypeExtractor.createTypeInfo]] based the
+        // class of the resulting type. For example, converts the given [[Table]] into
+        // an append [[DataStream]]. If the class is Row, then the return type only is
+        // [[GenericTypeInfo[Row]]. So it should convert to the [[RowTypeInfo]] in order
+        // to better serialize performance.
+        requestedTypeInfo match {
+          case gt: GenericTypeInfo[Row] if gt.getTypeClass == classOf[Row] =>
+            fromLogicalToDataType(queryLogicalType).bridgedTo(classOf[Row])
+          case gt: GenericTypeInfo[BaseRow] if gt.getTypeClass == classOf[BaseRow] =>
+            fromLogicalToDataType(queryLogicalType).bridgedTo(classOf[BaseRow])
+          case bt: BaseRowTypeInfo =>
+            val fields = bt.getFieldNames.zip(bt.getLogicalTypes).map { case (n, t) =>
+              DataTypes.FIELD(n, fromLogicalToDataType(t))
+            }
+            DataTypes.ROW(fields: _*).bridgedTo(classOf[BaseRow])
+          case _ =>
+            fromLegacyInfoToDataType(requestedTypeInfo)
+        }
+
+      case _ =>
+        consumedDataType
+    }
+  }
+
+  /**
+    * Checks whether the logical schema (from DDL) and physical schema
+    * (from TableSink.getConsumedDataType()) of sink are compatible.
+    *
+    * @param catalogTable the catalog table of sink
+    * @param sink the instance of [[TableSink]]
+    * @param queryLogicalType the logical type of query
+    */
+  def validateLogicalPhysicalTypesCompatible(
+      catalogTable: CatalogTable,
+      sink: TableSink[_],
+      queryLogicalType: RowType): Unit = {
+    // there may be generated columns in DDL, only get the physical part of DDL
+    val logicalSchema = TableSchemaUtils.getPhysicalSchema(catalogTable.getSchema)
+    // infer the physical schema from TableSink#getConsumedDataType
+    val physicalSchema = TableSinkUtils.inferSinkPhysicalSchema(
+      queryLogicalType,
+      sink)
+    // check for valid type info
+    if (logicalSchema.getFieldCount != physicalSchema.getFieldCount) {
+      throw new ValidationException("The field count of logical schema of the table does" +
+        " not match with the field count of physical schema\n. " +
+        s"The logical schema: [${logicalSchema.getFieldDataTypes.mkString(",")}]\n" +
+        s"The physical schema: [${physicalSchema.getFieldDataTypes.mkString(",")}].")
+    }
+
+    for (i <- 0 until logicalSchema.getFieldCount) {
+      val logicalFieldType = DataTypeUtils.transform(
+        logicalSchema.getFieldDataTypes()(i), toNullable) // ignore nullabilities
+      val logicalFieldName = logicalSchema.getFieldNames()(i)
+      val physicalFieldType = DataTypeUtils.transform(
+        physicalSchema.getFieldDataTypes()(i), toNullable) // ignore nullabilities
+      val physicalFieldName = physicalSchema.getFieldNames()(i)
+      TypeMappingUtils.checkPhysicalLogicalTypeCompatible(
+        physicalFieldType.getLogicalType,
+        logicalFieldType.getLogicalType,
+        physicalFieldName,
+        logicalFieldName,
+        false)
+     }
   }
 }
diff --git a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/MiniBatchIntervalInferTest.xml b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/MiniBatchIntervalInferTest.xml
index 26c89b48fa0..4de4a98760c 100644
--- a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/MiniBatchIntervalInferTest.xml
+++ b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/MiniBatchIntervalInferTest.xml
@@ -222,7 +222,7 @@ LogicalProject(a=[$0], b=[$1])
     <Resource name="planAfter">
       <![CDATA[
 Calc(select=[a, b])
-+- Join(joinType=[InnerJoin], where=[=(a0, a1)], select=[a, b, a0, b0, a1], leftInputSpec=[HasUniqueKey], rightInputSpec=[HasUniqueKey])
++- Join(joinType=[InnerJoin], where=[=(a0, a1)], select=[a, b, a0, b0, a1], leftInputSpec=[JoinKeyContainsUniqueKey], rightInputSpec=[HasUniqueKey])
    :- Exchange(distribution=[hash[a0]])
    :  +- Calc(select=[a, b, CAST(a) AS a0])
    :     +- GlobalGroupAggregate(groupBy=[a], select=[a, COUNT(count$0) AS b])
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/catalog/CatalogTableITCase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/catalog/CatalogTableITCase.scala
index 050fc76432b..1665ea40aad 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/catalog/CatalogTableITCase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/catalog/CatalogTableITCase.scala
@@ -200,7 +200,8 @@ class CatalogTableITCase(isStreamingMode: Boolean) extends AbstractTestBase {
        |CREATE TABLE T1 (
        |  price DECIMAL(10, 2),
        |  currency STRING,
-       |  ts TIMESTAMP(3),
+       |  ts6 TIMESTAMP(6),
+       |  ts AS CAST(ts6 AS TIMESTAMP(3)),
        |  WATERMARK FOR ts AS ts
        |) WITH (
        |  'connector.type' = 'filesystem',
@@ -234,7 +235,7 @@ class CatalogTableITCase(isStreamingMode: Boolean) extends AbstractTestBase {
         |INSERT INTO T2
         |SELECT
         |  TUMBLE_END(ts, INTERVAL '5' SECOND),
-        |  MAX(ts),
+        |  MAX(ts6),
         |  COUNT(*),
         |  MAX(price)
         |FROM T1
@@ -244,8 +245,8 @@ class CatalogTableITCase(isStreamingMode: Boolean) extends AbstractTestBase {
     execJob("testJob")
 
     val expected =
-      "2019-12-12 00:00:05.0,2019-12-12 00:00:04.004,3,50.00\n" +
-      "2019-12-12 00:00:10.0,2019-12-12 00:00:06.006,2,5.33\n"
+      "2019-12-12 00:00:05.0,2019-12-12 00:00:04.004001,3,50.00\n" +
+      "2019-12-12 00:00:10.0,2019-12-12 00:00:06.006001,2,5.33\n"
     assertEquals(expected, FileUtils.readFileUtf8(new File(new URI(sinkFilePath))))
   }
   @Test
@@ -535,7 +536,7 @@ class CatalogTableITCase(isStreamingMode: Boolean) extends AbstractTestBase {
     tableEnv.sqlUpdate(query)
     expectedEx.expect(classOf[ValidationException])
     expectedEx.expectMessage("Field types of query result and registered TableSink "
-      + "`default_catalog`.`default_database`.`t2` do not match.")
+      + "default_catalog.default_database.t2 do not match.")
     execJob("testJob")
   }
 
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/factories/utils/TestCollectionTableFactory.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/factories/utils/TestCollectionTableFactory.scala
index 8970ef31316..db1a39ad43b 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/factories/utils/TestCollectionTableFactory.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/factories/utils/TestCollectionTableFactory.scala
@@ -124,7 +124,7 @@ object TestCollectionTableFactory {
     val properties = new DescriptorProperties()
     properties.putProperties(props)
     val schema = properties.getTableSchema(Schema.SCHEMA)
-    new CollectionTableSink(physicalSchema(schema).toRowType.asInstanceOf[RowTypeInfo])
+    new CollectionTableSink(physicalSchema(schema))
   }
 
   def physicalSchema(schema: TableSchema): TableSchema = {
@@ -181,27 +181,23 @@ object TestCollectionTableFactory {
   /**
     * Table sink of collection.
     */
-  class CollectionTableSink(val outputType: RowTypeInfo)
+  class CollectionTableSink(val schema: TableSchema)
     extends BatchTableSink[Row]
       with AppendStreamTableSink[Row] {
     override def emitDataSet(dataSet: DataSet[Row]): Unit = {
       dataSet.output(new LocalCollectionOutputFormat[Row](RESULT)).setParallelism(1)
     }
 
-    override def getOutputType: RowTypeInfo = outputType
+    override def getConsumedDataType: DataType = schema.toRowDataType
 
-    override def getFieldNames: Array[String] = outputType.getFieldNames
-
-    override def getFieldTypes: Array[TypeInformation[_]] = {
-      outputType.getFieldTypes
-    }
+    override def getTableSchema: TableSchema = schema
 
     override def emitDataStream(dataStream: DataStream[Row]): Unit = {
-      dataStream.addSink(new UnsafeMemorySinkFunction(outputType)).setParallelism(1)
+      dataStream.addSink(new UnsafeMemorySinkFunction(schema.toRowType)).setParallelism(1)
     }
 
     override def consumeDataStream(dataStream: DataStream[Row]): DataStreamSink[_] = {
-      dataStream.addSink(new UnsafeMemorySinkFunction(outputType)).setParallelism(1)
+      dataStream.addSink(new UnsafeMemorySinkFunction(schema.toRowType)).setParallelism(1)
     }
 
     override def configure(fieldNames: Array[String],
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/stream/table/validation/TableSinkValidationTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/stream/table/validation/TableSinkValidationTest.scala
index 21c3ea3caa6..a84a43e65d0 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/stream/table/validation/TableSinkValidationTest.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/stream/table/validation/TableSinkValidationTest.scala
@@ -89,10 +89,10 @@ class TableSinkValidationTest extends TableTestBase {
   def testValidateSink(): Unit = {
     expectedException.expect(classOf[ValidationException])
     expectedException.expectMessage(
-      "Field types of query result and registered TableSink `default_catalog`." +
-      "`default_database`.`testSink` do not match.\n" +
-      "Query result schema: [a: INT, b: BIGINT, c: STRING, d: BIGINT]\n" +
-      "TableSink schema:    [a: INT, b: BIGINT, c: STRING, d: INT]")
+      "Field types of query result and registered TableSink default_catalog." +
+      "default_database.testSink do not match.\n" +
+      "Query schema: [a: INT, b: BIGINT, c: STRING, d: BIGINT]\n" +
+      "Sink schema: [a: INT, b: BIGINT, c: STRING, d: INT]")
 
     val env = StreamExecutionEnvironment.getExecutionEnvironment
     val tEnv = StreamTableEnvironment.create(env, TableTestUtil.STREAM_SETTING)
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/table/TableSinkITCase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/table/TableSinkITCase.scala
index 75b339595f9..3c9e6b4f368 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/table/TableSinkITCase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/table/TableSinkITCase.scala
@@ -21,7 +21,7 @@ package org.apache.flink.table.planner.runtime.batch.table
 import org.apache.flink.table.api.scala._
 import org.apache.flink.table.api.{DataTypes, TableSchema}
 import org.apache.flink.table.planner.runtime.utils.BatchTestBase
-import org.apache.flink.table.planner.runtime.utils.TestData.{data3, nullablesOfData3, type3}
+import org.apache.flink.table.planner.runtime.utils.TestData._
 import org.apache.flink.table.planner.utils.MemoryTableSourceSinkUtil
 import org.apache.flink.table.planner.utils.MemoryTableSourceSinkUtil.{DataTypeAppendStreamTableSink, DataTypeOutputFormatTableSink}
 import org.apache.flink.test.util.TestBaseUtils
@@ -85,4 +85,35 @@ class TableSinkITCase extends BatchTestBase {
 
     TestBaseUtils.compareResultAsText(results, expected)
   }
+
+  @Test
+  def testDecimalForLegacyTypeTableSink(): Unit = {
+    MemoryTableSourceSinkUtil.clear()
+
+    val schema = TableSchema.builder()
+      .field("a", DataTypes.VARCHAR(5))
+      .field("b", DataTypes.DECIMAL(10, 0))
+      .build()
+    val sink = new MemoryTableSourceSinkUtil.UnsafeMemoryAppendTableSink
+    tEnv.registerTableSink("testSink", sink.configure(schema.getFieldNames, schema.getFieldTypes))
+
+    registerCollection("Table3", simpleData2, simpleType2, "a, b", nullableOfSimpleData2)
+
+    tEnv.from("Table3")
+      .select('a.cast(DataTypes.STRING()), 'b.cast(DataTypes.DECIMAL(10, 2)))
+      .distinct()
+      .insertInto("testSink")
+
+    tEnv.execute("")
+
+    val results = MemoryTableSourceSinkUtil.tableDataStrings.asJava
+    val expected = Seq("1,0.100000000000000000", "2,0.200000000000000000",
+      "3,0.300000000000000000", "3,0.400000000000000000", "4,0.500000000000000000",
+      "4,0.600000000000000000", "5,0.700000000000000000", "5,0.800000000000000000",
+      "5,0.900000000000000000").mkString("\n")
+
+    TestBaseUtils.compareResultAsText(results, expected)
+  }
+
+
 }
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/table/TableSinkITCase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/table/TableSinkITCase.scala
index c48cf9fd0fd..f092d513958 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/table/TableSinkITCase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/table/TableSinkITCase.scala
@@ -26,7 +26,7 @@ import org.apache.flink.table.api.scala._
 import org.apache.flink.table.api.{DataTypes, TableException, TableSchema, Tumble, Types}
 import org.apache.flink.table.planner.runtime.utils.TestData.{smallTupleData3, tupleData3, tupleData5}
 import org.apache.flink.table.planner.runtime.utils.{TestingAppendTableSink, TestingRetractTableSink, TestingUpsertTableSink}
-import org.apache.flink.table.planner.utils.MemoryTableSourceSinkUtil.{DataTypeAppendStreamTableSink, DataTypeOutputFormatTableSink}
+import org.apache.flink.table.planner.utils.MemoryTableSourceSinkUtil.DataTypeAppendStreamTableSink
 import org.apache.flink.table.planner.utils.{MemoryTableSourceSinkUtil, TableTestUtil}
 import org.apache.flink.table.sinks._
 import org.apache.flink.test.util.{AbstractTestBase, TestBaseUtils}
@@ -54,7 +54,8 @@ class TableSinkITCase extends AbstractTestBase {
     val input = env.fromCollection(tupleData3)
       .assignAscendingTimestamps(r => r._2)
     val fieldNames = Array("d", "e", "t")
-    val fieldTypes: Array[TypeInformation[_]] = Array(Types.STRING, Types.SQL_TIMESTAMP, Types.LONG)
+    val fieldTypes: Array[TypeInformation[_]] = Array(
+      Types.STRING, Types.SQL_TIMESTAMP, Types.DECIMAL())
     val sink = new MemoryTableSourceSinkUtil.UnsafeMemoryAppendTableSink
     tEnv.registerTableSink("targetTable", sink.configure(fieldNames, fieldTypes))
 
@@ -65,10 +66,10 @@ class TableSinkITCase extends AbstractTestBase {
     env.execute()
 
     val expected = Seq(
-      "Hi,1970-01-01 00:00:00.001,1",
-      "Hello,1970-01-01 00:00:00.002,2",
-      "Comment#14,1970-01-01 00:00:00.006,6",
-      "Comment#15,1970-01-01 00:00:00.006,6").mkString("\n")
+      "Hi,1970-01-01 00:00:00.001,1.000000000000000000",
+      "Hello,1970-01-01 00:00:00.002,2.000000000000000000",
+      "Comment#14,1970-01-01 00:00:00.006,6.000000000000000000",
+      "Comment#15,1970-01-01 00:00:00.006,6.000000000000000000").mkString("\n")
 
     TestBaseUtils.compareResultAsText(MemoryTableSourceSinkUtil.tableData.asJava, expected)
   }
@@ -227,7 +228,7 @@ class TableSinkITCase extends AbstractTestBase {
       "retractSink",
       sink.configure(
         Array[String]("len", "icnt", "nsum"),
-        Array[TypeInformation[_]](Types.INT, Types.LONG, Types.LONG)))
+        Array[TypeInformation[_]](Types.INT, Types.LONG, Types.DECIMAL())))
 
     t.select('id, 'num, 'text.charLength() as 'len)
       .groupBy('len)
@@ -238,13 +239,13 @@ class TableSinkITCase extends AbstractTestBase {
 
     val retracted = sink.getRetractResults.sorted
     val expected = List(
-      "2,1,1",
-      "5,1,2",
-      "11,1,2",
-      "25,1,3",
-      "10,7,39",
-      "14,1,3",
-      "9,9,41").sorted
+      "2,1,1.000000000000000000",
+      "5,1,2.000000000000000000",
+      "11,1,2.000000000000000000",
+      "25,1,3.000000000000000000",
+      "10,7,39.000000000000000000",
+      "14,1,3.000000000000000000",
+      "9,9,41.000000000000000000").sorted
     assertEquals(expected, retracted)
 
   }
@@ -308,7 +309,7 @@ class TableSinkITCase extends AbstractTestBase {
       "upsertSink",
       sink.configure(
         Array[String]("cnt", "lencnt", "cTrue"),
-        Array[TypeInformation[_]](Types.LONG, Types.LONG, Types.BOOLEAN)))
+        Array[TypeInformation[_]](Types.LONG, Types.DECIMAL(), Types.BOOLEAN)))
 
     t.select('id, 'num, 'text.charLength() as 'len, ('id > 0) as 'cTrue)
       .groupBy('len, 'cTrue)
@@ -325,9 +326,9 @@ class TableSinkITCase extends AbstractTestBase {
 
     val retracted = sink.getUpsertResults.sorted
     val expected = List(
-      "1,5,true",
-      "7,1,true",
-      "9,1,true").sorted
+      "1,5.000000000000000000,true",
+      "7,1.000000000000000000,true",
+      "9,1.000000000000000000,true").sorted
     assertEquals(expected, retracted)
 
   }
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/utils/StreamTestSink.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/utils/StreamTestSink.scala
index d44f3163681..b3ad30d26be 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/utils/StreamTestSink.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/utils/StreamTestSink.scala
@@ -284,7 +284,9 @@ final class TestingUpsertTableSink(val keys: Array[Int], val tz: TimeZone)
   }
 
   override def getRecordType: TypeInformation[BaseRow] =
-    new BaseRowTypeInfo(fTypes.map(TypeInfoLogicalTypeConverter.fromTypeInfoToLogicalType), fNames)
+    new BaseRowTypeInfo(
+      fTypes.map(TypeConversions.fromLegacyInfoToDataType(_).getLogicalType),
+      fNames)
 
   override def getFieldNames: Array[String] = fNames
 
