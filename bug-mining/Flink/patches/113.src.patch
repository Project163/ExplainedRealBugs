diff --git a/flink-compiler/src/main/java/org/apache/flink/compiler/dag/TwoInputNode.java b/flink-compiler/src/main/java/org/apache/flink/compiler/dag/TwoInputNode.java
index b329a6e44ca..32f0519b864 100644
--- a/flink-compiler/src/main/java/org/apache/flink/compiler/dag/TwoInputNode.java
+++ b/flink-compiler/src/main/java/org/apache/flink/compiler/dag/TwoInputNode.java
@@ -418,18 +418,25 @@ public abstract class TwoInputNode extends OptimizerNode {
 						 *       the pairs of global properties.
 						 * *******************************************************************/
 						
+						outer:
 						for (GlobalPropertiesPair gpp : allGlobalPairs) {
 							if (gpp.getProperties1().isMetBy(c1.getGlobalProperties()) && 
 								gpp.getProperties2().isMetBy(c2.getGlobalProperties()) )
 							{
-								Channel c1Clone = c1.clone();
-								c1Clone.setRequiredGlobalProps(gpp.getProperties1());
-								c2.setRequiredGlobalProps(gpp.getProperties2());
-								
-								// we form a valid combination, so create the local candidates
-								// for this
-								addLocalCandidates(c1Clone, c2, broadcastPlanChannels, igps1, igps2, outputPlans, allLocalPairs, estimator);
-								break;
+								for (OperatorDescriptorDual desc : getProperties()) {
+									if (desc.areCompatible(gpp.getProperties1(), gpp.getProperties2(), 
+											c1.getGlobalProperties(), c2.getGlobalProperties()))
+									{
+										Channel c1Clone = c1.clone();
+										c1Clone.setRequiredGlobalProps(gpp.getProperties1());
+										c2.setRequiredGlobalProps(gpp.getProperties2());
+										
+										// we form a valid combination, so create the local candidates
+										// for this
+										addLocalCandidates(c1Clone, c2, broadcastPlanChannels, igps1, igps2, outputPlans, allLocalPairs, estimator);
+										break outer;
+									}
+								}
 							}
 						}
 						
diff --git a/flink-compiler/src/main/java/org/apache/flink/compiler/dag/WorksetIterationNode.java b/flink-compiler/src/main/java/org/apache/flink/compiler/dag/WorksetIterationNode.java
index 0dd23bfbe2e..ae6bc6b7bd8 100644
--- a/flink-compiler/src/main/java/org/apache/flink/compiler/dag/WorksetIterationNode.java
+++ b/flink-compiler/src/main/java/org/apache/flink/compiler/dag/WorksetIterationNode.java
@@ -513,6 +513,12 @@ public class WorksetIterationNode extends TwoInputNode implements IterationNode
 				new RequestedLocalProperties(), new RequestedLocalProperties()));
 		}
 		
+		@Override
+		public boolean areCompatible(RequestedGlobalProperties requested1, RequestedGlobalProperties requested2,
+				GlobalProperties produced1, GlobalProperties produced2) {
+			return true;
+		}
+		
 		@Override
 		public boolean areCoFulfilled(RequestedLocalProperties requested1, RequestedLocalProperties requested2,
 				LocalProperties produced1, LocalProperties produced2) {
@@ -562,6 +568,5 @@ public class WorksetIterationNode extends TwoInputNode implements IterationNode
 		protected void computeOperatorSpecificDefaultEstimates(DataStatistics statistics) {
 			// no estimates are needed here
 		}
-
 	}
 }
diff --git a/flink-compiler/src/main/java/org/apache/flink/compiler/operators/AbstractJoinDescriptor.java b/flink-compiler/src/main/java/org/apache/flink/compiler/operators/AbstractJoinDescriptor.java
index 84af77c0212..cb0e61c308b 100644
--- a/flink-compiler/src/main/java/org/apache/flink/compiler/operators/AbstractJoinDescriptor.java
+++ b/flink-compiler/src/main/java/org/apache/flink/compiler/operators/AbstractJoinDescriptor.java
@@ -64,14 +64,14 @@ public abstract class AbstractJoinDescriptor extends OperatorDescriptorDual {
 			// partition both (hash or custom)
 			RequestedGlobalProperties partitioned1 = new RequestedGlobalProperties();
 			if (customPartitioner == null) {
-				partitioned1.setHashPartitioned(this.keys1);
+				partitioned1.setAnyPartitioning(this.keys1);
 			} else {
 				partitioned1.setCustomPartitioned(this.keys1, this.customPartitioner);
 			}
 			
 			RequestedGlobalProperties partitioned2 = new RequestedGlobalProperties();
 			if (customPartitioner == null) {
-				partitioned2.setHashPartitioned(this.keys2);
+				partitioned2.setAnyPartitioning(this.keys2);
 			} else {
 				partitioned2.setCustomPartitioned(this.keys2, this.customPartitioner);
 			}
@@ -96,6 +96,21 @@ public abstract class AbstractJoinDescriptor extends OperatorDescriptorDual {
 		}
 		return pairs;
 	}
+	
+	@Override
+	public boolean areCompatible(RequestedGlobalProperties requested1, RequestedGlobalProperties requested2,
+			GlobalProperties produced1, GlobalProperties produced2)
+	{
+		if (requested1.getPartitioning().isPartitionedOnKey() && requested2.getPartitioning().isPartitionedOnKey()) {
+			return produced1.getPartitioning() == produced2.getPartitioning() && 
+					(produced1.getCustomPartitioner() == null ? 
+						produced2.getCustomPartitioner() == null :
+						produced1.getCustomPartitioner().equals(produced2.getCustomPartitioner()));
+		} else {
+			return true;
+		}
+
+	}
 
 	@Override
 	public GlobalProperties computeGlobalProperties(GlobalProperties in1, GlobalProperties in2) {
diff --git a/flink-compiler/src/main/java/org/apache/flink/compiler/operators/BinaryUnionOpDescriptor.java b/flink-compiler/src/main/java/org/apache/flink/compiler/operators/BinaryUnionOpDescriptor.java
index a78f8e730eb..c393fc2a2b5 100644
--- a/flink-compiler/src/main/java/org/apache/flink/compiler/operators/BinaryUnionOpDescriptor.java
+++ b/flink-compiler/src/main/java/org/apache/flink/compiler/operators/BinaryUnionOpDescriptor.java
@@ -27,6 +27,7 @@ import org.apache.flink.compiler.dag.TwoInputNode;
 import org.apache.flink.compiler.dataproperties.GlobalProperties;
 import org.apache.flink.compiler.dataproperties.LocalProperties;
 import org.apache.flink.compiler.dataproperties.PartitioningProperty;
+import org.apache.flink.compiler.dataproperties.RequestedGlobalProperties;
 import org.apache.flink.compiler.dataproperties.RequestedLocalProperties;
 import org.apache.flink.compiler.plan.BinaryUnionPlanNode;
 import org.apache.flink.compiler.plan.Channel;
@@ -87,4 +88,10 @@ public class BinaryUnionOpDescriptor extends OperatorDescriptorDual {
 			LocalProperties produced1, LocalProperties produced2) {
 		return true;
 	}
+	
+	@Override
+	public boolean areCompatible(RequestedGlobalProperties requested1, RequestedGlobalProperties requested2,
+			GlobalProperties produced1, GlobalProperties produced2) {
+		return true;
+	}
 }
diff --git a/flink-compiler/src/main/java/org/apache/flink/compiler/operators/CartesianProductDescriptor.java b/flink-compiler/src/main/java/org/apache/flink/compiler/operators/CartesianProductDescriptor.java
index 5107486de52..fefd71ae5bd 100644
--- a/flink-compiler/src/main/java/org/apache/flink/compiler/operators/CartesianProductDescriptor.java
+++ b/flink-compiler/src/main/java/org/apache/flink/compiler/operators/CartesianProductDescriptor.java
@@ -78,6 +78,12 @@ public abstract class CartesianProductDescriptor extends OperatorDescriptorDual
 		return Collections.singletonList(new LocalPropertiesPair(
 			new RequestedLocalProperties(), new RequestedLocalProperties()));
 	}
+	
+	@Override
+	public boolean areCompatible(RequestedGlobalProperties requested1, RequestedGlobalProperties requested2,
+			GlobalProperties produced1, GlobalProperties produced2) {
+		return true;
+	}
 
 	@Override
 	public boolean areCoFulfilled(RequestedLocalProperties requested1, RequestedLocalProperties requested2,
diff --git a/flink-compiler/src/main/java/org/apache/flink/compiler/operators/CoGroupDescriptor.java b/flink-compiler/src/main/java/org/apache/flink/compiler/operators/CoGroupDescriptor.java
index 55db89d6e8e..90e4c3bf405 100644
--- a/flink-compiler/src/main/java/org/apache/flink/compiler/operators/CoGroupDescriptor.java
+++ b/flink-compiler/src/main/java/org/apache/flink/compiler/operators/CoGroupDescriptor.java
@@ -16,7 +16,6 @@
  * limitations under the License.
  */
 
-
 package org.apache.flink.compiler.operators;
 
 import java.util.Collections;
@@ -37,9 +36,6 @@ import org.apache.flink.compiler.plan.DualInputPlanNode;
 import org.apache.flink.compiler.util.Utils;
 import org.apache.flink.runtime.operators.DriverStrategy;
 
-/**
- * 
- */
 public class CoGroupDescriptor extends OperatorDescriptorDual {
 	
 	private final Ordering ordering1;		// ordering on the first input 
@@ -109,6 +105,16 @@ public class CoGroupDescriptor extends OperatorDescriptorDual {
 		return Collections.singletonList(new LocalPropertiesPair(sort1, sort2));
 	}
 	
+	@Override
+	public boolean areCompatible(RequestedGlobalProperties requested1, RequestedGlobalProperties requested2,
+			GlobalProperties produced1, GlobalProperties produced2)
+	{
+		return produced1.getPartitioning() == produced2.getPartitioning() && 
+				(produced1.getCustomPartitioner() == null ? 
+					produced2.getCustomPartitioner() == null :
+					produced1.getCustomPartitioner().equals(produced2.getCustomPartitioner()));
+	}
+	
 	@Override
 	public boolean areCoFulfilled(RequestedLocalProperties requested1, RequestedLocalProperties requested2,
 			LocalProperties produced1, LocalProperties produced2)
diff --git a/flink-compiler/src/main/java/org/apache/flink/compiler/operators/CrossBlockOuterFirstDescriptor.java b/flink-compiler/src/main/java/org/apache/flink/compiler/operators/CrossBlockOuterFirstDescriptor.java
index be14d80e66e..1ec190582b1 100644
--- a/flink-compiler/src/main/java/org/apache/flink/compiler/operators/CrossBlockOuterFirstDescriptor.java
+++ b/flink-compiler/src/main/java/org/apache/flink/compiler/operators/CrossBlockOuterFirstDescriptor.java
@@ -16,15 +16,12 @@
  * limitations under the License.
  */
 
-
 package org.apache.flink.compiler.operators;
 
 import org.apache.flink.compiler.dataproperties.LocalProperties;
 import org.apache.flink.runtime.operators.DriverStrategy;
 
-/**
- * 
- */
+
 public class CrossBlockOuterFirstDescriptor extends CartesianProductDescriptor {
 	
 	public CrossBlockOuterFirstDescriptor() {
diff --git a/flink-compiler/src/main/java/org/apache/flink/compiler/operators/CrossBlockOuterSecondDescriptor.java b/flink-compiler/src/main/java/org/apache/flink/compiler/operators/CrossBlockOuterSecondDescriptor.java
index 4fcfef18f9e..a10f1a4e3fb 100644
--- a/flink-compiler/src/main/java/org/apache/flink/compiler/operators/CrossBlockOuterSecondDescriptor.java
+++ b/flink-compiler/src/main/java/org/apache/flink/compiler/operators/CrossBlockOuterSecondDescriptor.java
@@ -16,15 +16,12 @@
  * limitations under the License.
  */
 
-
 package org.apache.flink.compiler.operators;
 
 import org.apache.flink.compiler.dataproperties.LocalProperties;
 import org.apache.flink.runtime.operators.DriverStrategy;
 
-/**
- * 
- */
+
 public class CrossBlockOuterSecondDescriptor extends CartesianProductDescriptor {
 	
 	public CrossBlockOuterSecondDescriptor() {
diff --git a/flink-compiler/src/main/java/org/apache/flink/compiler/operators/CrossStreamOuterFirstDescriptor.java b/flink-compiler/src/main/java/org/apache/flink/compiler/operators/CrossStreamOuterFirstDescriptor.java
index f80f40453e1..a0512d46d50 100644
--- a/flink-compiler/src/main/java/org/apache/flink/compiler/operators/CrossStreamOuterFirstDescriptor.java
+++ b/flink-compiler/src/main/java/org/apache/flink/compiler/operators/CrossStreamOuterFirstDescriptor.java
@@ -16,15 +16,12 @@
  * limitations under the License.
  */
 
-
 package org.apache.flink.compiler.operators;
 
 import org.apache.flink.compiler.dataproperties.LocalProperties;
 import org.apache.flink.runtime.operators.DriverStrategy;
 
-/**
- * 
- */
+
 public class CrossStreamOuterFirstDescriptor extends CartesianProductDescriptor {
 	
 	public CrossStreamOuterFirstDescriptor() {
diff --git a/flink-compiler/src/main/java/org/apache/flink/compiler/operators/CrossStreamOuterSecondDescriptor.java b/flink-compiler/src/main/java/org/apache/flink/compiler/operators/CrossStreamOuterSecondDescriptor.java
index 69757604b52..9b6b122d64a 100644
--- a/flink-compiler/src/main/java/org/apache/flink/compiler/operators/CrossStreamOuterSecondDescriptor.java
+++ b/flink-compiler/src/main/java/org/apache/flink/compiler/operators/CrossStreamOuterSecondDescriptor.java
@@ -16,15 +16,12 @@
  * limitations under the License.
  */
 
-
 package org.apache.flink.compiler.operators;
 
 import org.apache.flink.compiler.dataproperties.LocalProperties;
 import org.apache.flink.runtime.operators.DriverStrategy;
 
-/**
- * 
- */
+
 public class CrossStreamOuterSecondDescriptor extends CartesianProductDescriptor {
 	
 	public CrossStreamOuterSecondDescriptor() {
diff --git a/flink-compiler/src/main/java/org/apache/flink/compiler/operators/OperatorDescriptorDual.java b/flink-compiler/src/main/java/org/apache/flink/compiler/operators/OperatorDescriptorDual.java
index 11224eca967..8eca16e5880 100644
--- a/flink-compiler/src/main/java/org/apache/flink/compiler/operators/OperatorDescriptorDual.java
+++ b/flink-compiler/src/main/java/org/apache/flink/compiler/operators/OperatorDescriptorDual.java
@@ -70,6 +70,9 @@ public abstract class OperatorDescriptorDual implements AbstractOperatorDescript
 	
 	protected abstract List<LocalPropertiesPair> createPossibleLocalProperties();
 	
+	public abstract boolean areCompatible(RequestedGlobalProperties requested1, RequestedGlobalProperties requested2,
+			GlobalProperties produced1, GlobalProperties produced2);
+	
 	public abstract boolean areCoFulfilled(RequestedLocalProperties requested1, RequestedLocalProperties requested2,
 			LocalProperties produced1, LocalProperties produced2);
 	
diff --git a/flink-compiler/src/main/java/org/apache/flink/compiler/operators/UtilSinkJoinOpDescriptor.java b/flink-compiler/src/main/java/org/apache/flink/compiler/operators/UtilSinkJoinOpDescriptor.java
index 7ccdda033a9..a17096f3f63 100644
--- a/flink-compiler/src/main/java/org/apache/flink/compiler/operators/UtilSinkJoinOpDescriptor.java
+++ b/flink-compiler/src/main/java/org/apache/flink/compiler/operators/UtilSinkJoinOpDescriptor.java
@@ -58,10 +58,15 @@ public class UtilSinkJoinOpDescriptor extends OperatorDescriptorDual {
 			new RequestedLocalProperties(), new RequestedLocalProperties()));
 	}
 	
+	@Override
+	public boolean areCompatible(RequestedGlobalProperties requested1, RequestedGlobalProperties requested2,
+			GlobalProperties produced1, GlobalProperties produced2) {
+		return true;
+	}
+	
 	@Override
 	public boolean areCoFulfilled(RequestedLocalProperties requested1, RequestedLocalProperties requested2,
-			LocalProperties produced1, LocalProperties produced2)
-	{
+			LocalProperties produced1, LocalProperties produced2) {
 		return true;
 	}
 
diff --git a/flink-compiler/src/test/java/org/apache/flink/compiler/custompartition/BinaryCustomPartitioningCompatibilityTest.java b/flink-compiler/src/test/java/org/apache/flink/compiler/custompartition/BinaryCustomPartitioningCompatibilityTest.java
new file mode 100644
index 00000000000..ff3d78ea90e
--- /dev/null
+++ b/flink-compiler/src/test/java/org/apache/flink/compiler/custompartition/BinaryCustomPartitioningCompatibilityTest.java
@@ -0,0 +1,84 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.compiler.custompartition;
+
+import static org.junit.Assert.*;
+
+import org.apache.flink.api.common.Plan;
+import org.apache.flink.api.common.functions.Partitioner;
+import org.apache.flink.api.java.DataSet;
+import org.apache.flink.api.java.ExecutionEnvironment;
+import org.apache.flink.api.java.tuple.Tuple2;
+import org.apache.flink.api.java.tuple.Tuple3;
+import org.apache.flink.compiler.CompilerTestBase;
+import org.apache.flink.compiler.plan.DualInputPlanNode;
+import org.apache.flink.compiler.plan.OptimizedPlan;
+import org.apache.flink.compiler.plan.SingleInputPlanNode;
+import org.apache.flink.compiler.plan.SinkPlanNode;
+import org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator;
+import org.apache.flink.runtime.operators.shipping.ShipStrategyType;
+import org.junit.Test;
+
+@SuppressWarnings({"serial","unchecked"})
+public class BinaryCustomPartitioningCompatibilityTest extends CompilerTestBase {
+
+	@Test
+	public void testCompatiblePartitioning() {
+		try {
+			final Partitioner<Long> partitioner = new Partitioner<Long>() {
+				@Override
+				public int partition(Long key, int numPartitions) {
+					return 0;
+				}
+			};
+			
+			ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
+			
+			DataSet<Tuple2<Long, Long>> input1 = env.fromElements(new Tuple2<Long, Long>(0L, 0L));
+			DataSet<Tuple3<Long, Long, Long>> input2 = env.fromElements(new Tuple3<Long, Long, Long>(0L, 0L, 0L));
+			
+			input1.partitionCustom(partitioner, 1)
+				.join(input2.partitionCustom(partitioner, 0))
+				.where(1).equalTo(0)
+				.print();
+			
+			Plan p = env.createProgramPlan();
+			OptimizedPlan op = compileNoStats(p);
+			
+			SinkPlanNode sink = op.getDataSinks().iterator().next();
+			DualInputPlanNode join = (DualInputPlanNode) sink.getInput().getSource();
+			SingleInputPlanNode partitioner1 = (SingleInputPlanNode) join.getInput1().getSource();
+			SingleInputPlanNode partitioner2 = (SingleInputPlanNode) join.getInput2().getSource();
+
+			assertEquals(ShipStrategyType.FORWARD, join.getInput1().getShipStrategy());
+			assertEquals(ShipStrategyType.FORWARD, join.getInput2().getShipStrategy());
+			
+			assertEquals(ShipStrategyType.PARTITION_CUSTOM, partitioner1.getInput().getShipStrategy());
+			assertEquals(ShipStrategyType.PARTITION_CUSTOM, partitioner2.getInput().getShipStrategy());
+			assertEquals(partitioner, partitioner1.getInput().getPartitioner());
+			assertEquals(partitioner, partitioner2.getInput().getPartitioner());
+			
+			new NepheleJobGraphGenerator().compileJobGraph(op);
+		}
+		catch (Exception e) {
+			e.printStackTrace();
+			fail(e.getMessage());
+		}
+	}
+}
diff --git a/flink-compiler/src/test/java/org/apache/flink/compiler/java/PartitioningOperatorTest.java b/flink-compiler/src/test/java/org/apache/flink/compiler/java/PartitioningOperatorTest.java
new file mode 100644
index 00000000000..26185c72bd7
--- /dev/null
+++ b/flink-compiler/src/test/java/org/apache/flink/compiler/java/PartitioningOperatorTest.java
@@ -0,0 +1,70 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.compiler.java;
+
+import static org.junit.Assert.*;
+
+import java.util.Collections;
+
+import org.apache.flink.api.common.Plan;
+import org.apache.flink.api.common.functions.Partitioner;
+import org.apache.flink.api.java.DataSet;
+import org.apache.flink.api.java.ExecutionEnvironment;
+import org.apache.flink.api.java.tuple.Tuple2;
+import org.apache.flink.compiler.CompilerTestBase;
+import org.apache.flink.compiler.plan.OptimizedPlan;
+import org.apache.flink.compiler.plan.SingleInputPlanNode;
+import org.apache.flink.compiler.plan.SinkPlanNode;
+import org.apache.flink.compiler.testfunctions.IdentityGroupReducer;
+import org.apache.flink.runtime.operators.shipping.ShipStrategyType;
+import org.junit.Test;
+
+@SuppressWarnings("serial")
+public class PartitioningOperatorTest extends CompilerTestBase {
+
+	@Test
+	public void testPartitiongOperatorPreservesFields() {
+		try {
+			ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
+			
+			DataSet<Tuple2<Long, Long>> data = env.fromCollection(Collections.singleton(new Tuple2<Long, Long>(0L, 0L)));
+			
+			data.partitionCustom(new Partitioner<Long>() {
+					public int partition(Long key, int numPartitions) { return key.intValue(); }
+				}, 1)
+				.groupBy(1)
+				.reduceGroup(new IdentityGroupReducer<Tuple2<Long,Long>>())
+				.print();
+			
+			Plan p = env.createProgramPlan();
+			OptimizedPlan op = compileNoStats(p);
+			
+			SinkPlanNode sink = op.getDataSinks().iterator().next();
+			SingleInputPlanNode reducer = (SingleInputPlanNode) sink.getInput().getSource();
+			SingleInputPlanNode partitioner = (SingleInputPlanNode) reducer.getInput().getSource();
+
+			assertEquals(ShipStrategyType.FORWARD, reducer.getInput().getShipStrategy());
+			assertEquals(ShipStrategyType.PARTITION_CUSTOM, partitioner.getInput().getShipStrategy());
+		}
+		catch (Exception e) {
+			e.printStackTrace();
+			fail(e.getMessage());
+		}
+	}
+}
diff --git a/flink-compiler/src/test/java/org/apache/flink/compiler/operators/CoGroupCompatibilityTest.java b/flink-compiler/src/test/java/org/apache/flink/compiler/operators/CoGroupCompatibilityTest.java
new file mode 100644
index 00000000000..ae650941aec
--- /dev/null
+++ b/flink-compiler/src/test/java/org/apache/flink/compiler/operators/CoGroupCompatibilityTest.java
@@ -0,0 +1,161 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.compiler.operators;
+
+import static org.junit.Assert.*;
+
+import org.apache.flink.api.common.functions.Partitioner;
+import org.apache.flink.api.common.operators.util.FieldList;
+import org.apache.flink.compiler.dataproperties.GlobalProperties;
+import org.apache.flink.compiler.dataproperties.RequestedGlobalProperties;
+import org.junit.Test;
+
+@SuppressWarnings("serial")
+public class CoGroupCompatibilityTest {
+
+	@Test
+	public void checkCompatiblePartitionings() {
+		try {
+			final FieldList keysLeft = new FieldList(1, 4);
+			final FieldList keysRight = new FieldList(3, 1);
+			
+			CoGroupDescriptor descr = new CoGroupDescriptor(keysLeft, keysRight);
+			
+			// test compatible hash partitioning
+			{
+				RequestedGlobalProperties reqLeft = new RequestedGlobalProperties();
+				reqLeft.setHashPartitioned(keysLeft);
+				RequestedGlobalProperties reqRight = new RequestedGlobalProperties();
+				reqRight.setHashPartitioned(keysRight);
+				
+				GlobalProperties propsLeft = new GlobalProperties();
+				propsLeft.setHashPartitioned(keysLeft);
+				GlobalProperties propsRight = new GlobalProperties();
+				propsRight.setHashPartitioned(keysRight);
+				
+				assertTrue(descr.areCompatible(reqLeft, reqRight, propsLeft, propsRight));
+			}
+			
+			// test compatible custom partitioning
+			{
+				Partitioner<Object> part = new Partitioner<Object>() {
+					@Override
+					public int partition(Object key, int numPartitions) {
+						return 0;
+					}
+				};
+				
+				RequestedGlobalProperties reqLeft = new RequestedGlobalProperties();
+				reqLeft.setCustomPartitioned(keysLeft, part);
+				RequestedGlobalProperties reqRight = new RequestedGlobalProperties();
+				reqRight.setCustomPartitioned(keysRight, part);
+				
+				GlobalProperties propsLeft = new GlobalProperties();
+				propsLeft.setCustomPartitioned(keysLeft, part);
+				GlobalProperties propsRight = new GlobalProperties();
+				propsRight.setCustomPartitioned(keysRight, part);
+				
+				assertTrue(descr.areCompatible(reqLeft, reqRight, propsLeft, propsRight));
+			}
+			
+			// test custom partitioning matching any partitioning
+			{
+				Partitioner<Object> part = new Partitioner<Object>() {
+					@Override
+					public int partition(Object key, int numPartitions) {
+						return 0;
+					}
+				};
+				
+				RequestedGlobalProperties reqLeft = new RequestedGlobalProperties();
+				reqLeft.setAnyPartitioning(keysLeft);
+				RequestedGlobalProperties reqRight = new RequestedGlobalProperties();
+				reqRight.setAnyPartitioning(keysRight);
+				
+				GlobalProperties propsLeft = new GlobalProperties();
+				propsLeft.setCustomPartitioned(keysLeft, part);
+				GlobalProperties propsRight = new GlobalProperties();
+				propsRight.setCustomPartitioned(keysRight, part);
+				
+				assertTrue(descr.areCompatible(reqLeft, reqRight, propsLeft, propsRight));
+			}
+		}
+		catch (Exception e) {
+			e.printStackTrace();
+			fail(e.getMessage());
+		}
+	}
+	
+	@Test
+	public void checkInompatiblePartitionings() {
+		try {
+			final FieldList keysLeft = new FieldList(1);
+			final FieldList keysRight = new FieldList(3);
+			
+			final Partitioner<Object> part = new Partitioner<Object>() {
+				@Override
+				public int partition(Object key, int numPartitions) {
+					return 0;
+				}
+			};
+			final Partitioner<Object> part2 = new Partitioner<Object>() {
+				@Override
+				public int partition(Object key, int numPartitions) {
+					return 0;
+				}
+			};
+			
+			CoGroupDescriptor descr = new CoGroupDescriptor(keysLeft, keysRight);
+			
+			// test incompatible hash with custom partitioning
+			{
+				RequestedGlobalProperties reqLeft = new RequestedGlobalProperties();
+				reqLeft.setAnyPartitioning(keysLeft);
+				RequestedGlobalProperties reqRight = new RequestedGlobalProperties();
+				reqRight.setAnyPartitioning(keysRight);
+				
+				GlobalProperties propsLeft = new GlobalProperties();
+				propsLeft.setHashPartitioned(keysLeft);
+				GlobalProperties propsRight = new GlobalProperties();
+				propsRight.setCustomPartitioned(keysRight, part);
+				
+				assertFalse(descr.areCompatible(reqLeft, reqRight, propsLeft, propsRight));
+			}
+			
+			// test incompatible custom partitionings
+			{
+				RequestedGlobalProperties reqLeft = new RequestedGlobalProperties();
+				reqLeft.setAnyPartitioning(keysLeft);
+				RequestedGlobalProperties reqRight = new RequestedGlobalProperties();
+				reqRight.setAnyPartitioning(keysRight);
+				
+				GlobalProperties propsLeft = new GlobalProperties();
+				propsLeft.setCustomPartitioned(keysLeft, part);
+				GlobalProperties propsRight = new GlobalProperties();
+				propsRight.setCustomPartitioned(keysRight, part2);
+				
+				assertFalse(descr.areCompatible(reqLeft, reqRight, propsLeft, propsRight));
+			}
+		}
+		catch (Exception e) {
+			e.printStackTrace();
+			fail(e.getMessage());
+		}
+	}
+}
diff --git a/flink-compiler/src/test/java/org/apache/flink/compiler/operators/JoinGlobalPropertiesCompatibilityTest.java b/flink-compiler/src/test/java/org/apache/flink/compiler/operators/JoinGlobalPropertiesCompatibilityTest.java
new file mode 100644
index 00000000000..13f4102264b
--- /dev/null
+++ b/flink-compiler/src/test/java/org/apache/flink/compiler/operators/JoinGlobalPropertiesCompatibilityTest.java
@@ -0,0 +1,161 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.compiler.operators;
+
+import static org.junit.Assert.*;
+
+import org.apache.flink.api.common.functions.Partitioner;
+import org.apache.flink.api.common.operators.util.FieldList;
+import org.apache.flink.compiler.dataproperties.GlobalProperties;
+import org.apache.flink.compiler.dataproperties.RequestedGlobalProperties;
+import org.junit.Test;
+
+@SuppressWarnings("serial")
+public class JoinGlobalPropertiesCompatibilityTest {
+
+	@Test
+	public void checkCompatiblePartitionings() {
+		try {
+			final FieldList keysLeft = new FieldList(1, 4);
+			final FieldList keysRight = new FieldList(3, 1);
+			
+			SortMergeJoinDescriptor descr = new SortMergeJoinDescriptor(keysLeft, keysRight);
+			
+			// test compatible hash partitioning
+			{
+				RequestedGlobalProperties reqLeft = new RequestedGlobalProperties();
+				reqLeft.setHashPartitioned(keysLeft);
+				RequestedGlobalProperties reqRight = new RequestedGlobalProperties();
+				reqRight.setHashPartitioned(keysRight);
+				
+				GlobalProperties propsLeft = new GlobalProperties();
+				propsLeft.setHashPartitioned(keysLeft);
+				GlobalProperties propsRight = new GlobalProperties();
+				propsRight.setHashPartitioned(keysRight);
+				
+				assertTrue(descr.areCompatible(reqLeft, reqRight, propsLeft, propsRight));
+			}
+			
+			// test compatible custom partitioning
+			{
+				Partitioner<Object> part = new Partitioner<Object>() {
+					@Override
+					public int partition(Object key, int numPartitions) {
+						return 0;
+					}
+				};
+				
+				RequestedGlobalProperties reqLeft = new RequestedGlobalProperties();
+				reqLeft.setCustomPartitioned(keysLeft, part);
+				RequestedGlobalProperties reqRight = new RequestedGlobalProperties();
+				reqRight.setCustomPartitioned(keysRight, part);
+				
+				GlobalProperties propsLeft = new GlobalProperties();
+				propsLeft.setCustomPartitioned(keysLeft, part);
+				GlobalProperties propsRight = new GlobalProperties();
+				propsRight.setCustomPartitioned(keysRight, part);
+				
+				assertTrue(descr.areCompatible(reqLeft, reqRight, propsLeft, propsRight));
+			}
+			
+			// test custom partitioning matching any partitioning
+			{
+				Partitioner<Object> part = new Partitioner<Object>() {
+					@Override
+					public int partition(Object key, int numPartitions) {
+						return 0;
+					}
+				};
+				
+				RequestedGlobalProperties reqLeft = new RequestedGlobalProperties();
+				reqLeft.setAnyPartitioning(keysLeft);
+				RequestedGlobalProperties reqRight = new RequestedGlobalProperties();
+				reqRight.setAnyPartitioning(keysRight);
+				
+				GlobalProperties propsLeft = new GlobalProperties();
+				propsLeft.setCustomPartitioned(keysLeft, part);
+				GlobalProperties propsRight = new GlobalProperties();
+				propsRight.setCustomPartitioned(keysRight, part);
+				
+				assertTrue(descr.areCompatible(reqLeft, reqRight, propsLeft, propsRight));
+			}
+		}
+		catch (Exception e) {
+			e.printStackTrace();
+			fail(e.getMessage());
+		}
+	}
+	
+	@Test
+	public void checkInompatiblePartitionings() {
+		try {
+			final FieldList keysLeft = new FieldList(1);
+			final FieldList keysRight = new FieldList(3);
+			
+			final Partitioner<Object> part = new Partitioner<Object>() {
+				@Override
+				public int partition(Object key, int numPartitions) {
+					return 0;
+				}
+			};
+			final Partitioner<Object> part2 = new Partitioner<Object>() {
+				@Override
+				public int partition(Object key, int numPartitions) {
+					return 0;
+				}
+			};
+			
+			SortMergeJoinDescriptor descr = new SortMergeJoinDescriptor(keysLeft, keysRight);
+			
+			// test incompatible hash with custom partitioning
+			{
+				RequestedGlobalProperties reqLeft = new RequestedGlobalProperties();
+				reqLeft.setAnyPartitioning(keysLeft);
+				RequestedGlobalProperties reqRight = new RequestedGlobalProperties();
+				reqRight.setAnyPartitioning(keysRight);
+				
+				GlobalProperties propsLeft = new GlobalProperties();
+				propsLeft.setHashPartitioned(keysLeft);
+				GlobalProperties propsRight = new GlobalProperties();
+				propsRight.setCustomPartitioned(keysRight, part);
+				
+				assertFalse(descr.areCompatible(reqLeft, reqRight, propsLeft, propsRight));
+			}
+			
+			// test incompatible custom partitionings
+			{
+				RequestedGlobalProperties reqLeft = new RequestedGlobalProperties();
+				reqLeft.setAnyPartitioning(keysLeft);
+				RequestedGlobalProperties reqRight = new RequestedGlobalProperties();
+				reqRight.setAnyPartitioning(keysRight);
+				
+				GlobalProperties propsLeft = new GlobalProperties();
+				propsLeft.setCustomPartitioned(keysLeft, part);
+				GlobalProperties propsRight = new GlobalProperties();
+				propsRight.setCustomPartitioned(keysRight, part2);
+				
+				assertFalse(descr.areCompatible(reqLeft, reqRight, propsLeft, propsRight));
+			}
+		}
+		catch (Exception e) {
+			e.printStackTrace();
+			fail(e.getMessage());
+		}
+	}
+}
diff --git a/flink-compiler/src/test/java/org/apache/flink/compiler/operators/JoinOnConflictingPartitioningsTest.java b/flink-compiler/src/test/java/org/apache/flink/compiler/operators/JoinOnConflictingPartitioningsTest.java
new file mode 100644
index 00000000000..c8713e67164
--- /dev/null
+++ b/flink-compiler/src/test/java/org/apache/flink/compiler/operators/JoinOnConflictingPartitioningsTest.java
@@ -0,0 +1,65 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.compiler.operators;
+
+import static org.junit.Assert.*;
+
+import org.apache.flink.api.common.Plan;
+import org.apache.flink.api.java.DataSet;
+import org.apache.flink.api.java.ExecutionEnvironment;
+import org.apache.flink.api.java.tuple.Tuple2;
+import org.apache.flink.compiler.CompilerException;
+import org.apache.flink.compiler.CompilerTestBase;
+import org.apache.flink.compiler.PactCompiler;
+import org.apache.flink.configuration.Configuration;
+import org.junit.Test;
+
+@SuppressWarnings({"serial", "unchecked"})
+public class JoinOnConflictingPartitioningsTest extends CompilerTestBase {
+
+	@Test
+	public void testRejectJoinOnHashAndRangePartitioning() {
+		try {
+			ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
+			
+			DataSet<Tuple2<Long, Long>> input = env.fromElements(new Tuple2<Long, Long>(0L, 0L));
+			
+			Configuration cfg = new Configuration();
+			cfg.setString(PactCompiler.HINT_SHIP_STRATEGY_FIRST_INPUT, PactCompiler.HINT_SHIP_STRATEGY_REPARTITION_HASH);
+			cfg.setString(PactCompiler.HINT_SHIP_STRATEGY_SECOND_INPUT, PactCompiler.HINT_SHIP_STRATEGY_REPARTITION_RANGE);
+			
+			input.join(input).where(0).equalTo(0)
+				.withParameters(cfg)
+				.print();
+			
+			Plan p = env.createProgramPlan();
+			try {
+				compileNoStats(p);
+				fail("This should fail with an exception");
+			}
+			catch (CompilerException e) {
+				// expected
+			}
+		}
+		catch (Exception e) {
+			e.printStackTrace();
+			fail(e.getMessage());
+		}
+	}
+}
diff --git a/flink-core/src/main/java/org/apache/flink/api/common/operators/SingleInputOperator.java b/flink-core/src/main/java/org/apache/flink/api/common/operators/SingleInputOperator.java
index f1bf2ad8d1a..5b491bfba0d 100644
--- a/flink-core/src/main/java/org/apache/flink/api/common/operators/SingleInputOperator.java
+++ b/flink-core/src/main/java/org/apache/flink/api/common/operators/SingleInputOperator.java
@@ -35,19 +35,13 @@ import org.apache.flink.util.Visitor;
  */
 public abstract class SingleInputOperator<IN, OUT, FT extends Function> extends AbstractUdfOperator<OUT, FT> {
 	
-	/**
-	 * The input which produces the data consumed by this operator.
-	 */
+	/** The input which produces the data consumed by this operator. */
 	protected Operator<IN> input;
 	
-	/**
-	 * The positions of the keys in the tuple.
-	 */
+	/** The positions of the keys in the tuple. */
 	private final int[] keyFields;
 	
-	/**
-	 * Semantic properties of the associated function.
-	 */
+	/** Semantic properties of the associated function. */
 	private SingleInputSemanticProperties semanticProperties;
 	
 	// --------------------------------------------------------------------------------------------
diff --git a/flink-core/src/main/java/org/apache/flink/api/common/operators/SingleInputSemanticProperties.java b/flink-core/src/main/java/org/apache/flink/api/common/operators/SingleInputSemanticProperties.java
index 2de53eb10a2..45f020ac45b 100644
--- a/flink-core/src/main/java/org/apache/flink/api/common/operators/SingleInputSemanticProperties.java
+++ b/flink-core/src/main/java/org/apache/flink/api/common/operators/SingleInputSemanticProperties.java
@@ -16,7 +16,6 @@
  * limitations under the License.
  */
 
-
 package org.apache.flink.api.common.operators;
 
 import java.util.HashMap;
@@ -149,5 +148,63 @@ public class SingleInputSemanticProperties extends SemanticProperties {
 		this.forwardedFields = new HashMap<Integer,FieldSet>();
 		this.readFields = null;
 	}
+	
+	// --------------------------------------------------------------------------------------------
+	
+	public static class AllFieldsConstantProperties extends SingleInputSemanticProperties {
+		
+		private static final long serialVersionUID = 1L;
+
+		@Override
+		public FieldSet getReadFields() {
+			return FieldSet.EMPTY_SET;
+		}
+		
+		@Override
+		public FieldSet getWrittenFields() {
+			return FieldSet.EMPTY_SET;
+		}
+
+		@Override
+		public FieldSet getForwardedField(int sourceField) {
+			return new FieldSet(sourceField);
+		}
+		
+		// ----- all mutating operations are unsupported -----
+		
+		@Override
+		public void addForwardedField(int sourceField, FieldSet destinationFields) {
+			throw new UnsupportedOperationException();
+		}
 		
+		@Override
+		public void addForwardedField(int sourceField, int destinationField) {
+			throw new UnsupportedOperationException();
+		}
+		
+		@Override
+		public void setForwardedField(int sourceField, FieldSet destinationFields) {
+			throw new UnsupportedOperationException();
+		}
+		
+		@Override
+		public void addReadFields(FieldSet readFields) {
+			throw new UnsupportedOperationException();
+		}
+		
+		@Override
+		public void setReadFields(FieldSet readFields) {
+			throw new UnsupportedOperationException();
+		}
+
+		@Override
+		public void addWrittenFields(FieldSet writtenFields) {
+			throw new UnsupportedOperationException();
+		}
+
+		@Override
+		public void setWrittenFields(FieldSet writtenFields) {
+			throw new UnsupportedOperationException();
+		}
+	}
 }
diff --git a/flink-core/src/main/java/org/apache/flink/api/common/operators/base/PartitionOperatorBase.java b/flink-core/src/main/java/org/apache/flink/api/common/operators/base/PartitionOperatorBase.java
index ee3b259f9a3..673529825bd 100644
--- a/flink-core/src/main/java/org/apache/flink/api/common/operators/base/PartitionOperatorBase.java
+++ b/flink-core/src/main/java/org/apache/flink/api/common/operators/base/PartitionOperatorBase.java
@@ -24,11 +24,11 @@ import org.apache.flink.api.common.functions.Partitioner;
 import org.apache.flink.api.common.functions.RuntimeContext;
 import org.apache.flink.api.common.functions.util.NoOpFunction;
 import org.apache.flink.api.common.operators.SingleInputOperator;
+import org.apache.flink.api.common.operators.SingleInputSemanticProperties;
 import org.apache.flink.api.common.operators.UnaryOperatorInformation;
 import org.apache.flink.api.common.operators.util.UserCodeObjectWrapper;
 
 /**
- *
  * @param <IN> The input and result type.
  */
 public class PartitionOperatorBase<IN> extends SingleInputOperator<IN, IN, NoOpFunction> {
@@ -79,6 +79,11 @@ public class PartitionOperatorBase<IN> extends SingleInputOperator<IN, IN, NoOpF
 		}
 		this.customPartitioner = customPartitioner;
 	}
+	
+	@Override
+	public SingleInputSemanticProperties getSemanticProperties() {
+		return new SingleInputSemanticProperties.AllFieldsConstantProperties();
+	}
 
 	// --------------------------------------------------------------------------------------------
 	
diff --git a/flink-tests/src/test/scala/org/apache/flink/api/scala/compiler/PartitionOperatorTranslationTest.scala b/flink-tests/src/test/scala/org/apache/flink/api/scala/compiler/PartitionOperatorTranslationTest.scala
new file mode 100644
index 00000000000..1e44413fe8e
--- /dev/null
+++ b/flink-tests/src/test/scala/org/apache/flink/api/scala/compiler/PartitionOperatorTranslationTest.scala
@@ -0,0 +1,60 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.api.scala.compiler
+
+import org.junit.Test
+import org.junit.Assert._
+import org.apache.flink.api.scala._
+import org.apache.flink.runtime.operators.shipping.ShipStrategyType
+import org.apache.flink.test.compiler.util.CompilerTestBase
+import org.apache.flink.compiler.plan.SingleInputPlanNode
+import org.apache.flink.api.common.functions.Partitioner
+
+class PartitionOperatorTranslationTest extends CompilerTestBase {
+
+  @Test
+  def testPartitiongOperatorPreservesFields() {
+    try {
+      val env = ExecutionEnvironment.getExecutionEnvironment
+      
+      val data = env.fromElements( (0L, 0L) )
+      data.partitionCustom(new Partitioner[Long]() {
+          def partition(key: Long, numPartitions: Int): Int = key.intValue()
+        }, 1)
+        .groupBy(1).reduceGroup( x => x)
+        .print()
+      
+      val p = env.createProgramPlan()
+      val op = compileNoStats(p)
+      
+      val sink = op.getDataSinks.iterator().next()
+      val reducer = sink.getInput.getSource.asInstanceOf[SingleInputPlanNode]
+      val partitioner = reducer.getInput.getSource.asInstanceOf[SingleInputPlanNode]
+
+      assertEquals(ShipStrategyType.FORWARD, reducer.getInput.getShipStrategy)
+      assertEquals(ShipStrategyType.PARTITION_CUSTOM, partitioner.getInput.getShipStrategy)
+    }
+    catch {
+      case e: Exception => {
+        e.printStackTrace()
+        fail(e.getMessage)
+      }
+    }
+  }
+}
