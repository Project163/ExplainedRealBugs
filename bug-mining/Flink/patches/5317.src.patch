diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdFilteredColumnInterval.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdFilteredColumnInterval.scala
index 2cb5a36b0f5..e16f86dda1a 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdFilteredColumnInterval.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdFilteredColumnInterval.scala
@@ -22,13 +22,14 @@ import org.apache.flink.table.planner.plan.nodes.calcite.TableAggregate
 import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalGroupAggregateBase
 import org.apache.flink.table.planner.plan.nodes.physical.stream.{StreamPhysicalGlobalGroupAggregate, StreamPhysicalGroupAggregate, StreamPhysicalGroupTableAggregate, StreamPhysicalGroupWindowAggregate, StreamPhysicalGroupWindowTableAggregate, StreamPhysicalLocalGroupAggregate}
 import org.apache.flink.table.planner.plan.stats.ValueInterval
-import org.apache.flink.table.planner.plan.utils.ColumnIntervalUtil
+import org.apache.flink.table.planner.plan.utils.{ColumnIntervalUtil, FlinkRelOptUtil}
 import org.apache.flink.util.Preconditions.checkArgument
 
 import org.apache.calcite.plan.volcano.RelSubset
 import org.apache.calcite.rel.RelNode
 import org.apache.calcite.rel.core._
 import org.apache.calcite.rel.metadata._
+import org.apache.calcite.rex.{RexBuilder, RexInputRef, RexLiteral, RexNode}
 import org.apache.calcite.sql.`type`.SqlTypeUtil
 import org.apache.calcite.util.Util
 
@@ -40,6 +41,9 @@ import scala.collection.JavaConversions._
   *
   * The [[FlinkRelMdFilteredColumnInterval]] is almost depend on the implementation of
   * [[FlinkRelMdColumnInterval]], except to handle filter argument in Calc RelNode.
+  *
+  * Refactor this meta data to a utility class later because it only serves for calculating the
+  * monotonicity of some specific aggregate functions.
   */
 class FlinkRelMdFilteredColumnInterval private extends MetadataHandler[FilteredColumnInterval] {
 
@@ -67,14 +71,49 @@ class FlinkRelMdFilteredColumnInterval private extends MetadataHandler[FilteredC
     } else {
       val condition = project.getProjects.get(filterArg)
       checkArgument(SqlTypeUtil.inBooleanFamily(condition.getType))
-      ColumnIntervalUtil.getColumnIntervalWithFilter(
+      intersectColumnIntervalWithPredicate(
+        project.getProjects.get(columnIndex),
         Option(columnInterval),
         condition,
-        columnIndex,
         project.getCluster.getRexBuilder)
     }
   }
 
+  /**
+   * Calculate the value interval of the given column (which may carry derived column interval) and
+   * additional predicate expression, if the given column is a RexInputRef then the final interval
+   * is intersect with the predicate, if the given column is a RexLiteral then the final interval
+   * is the literal itself, otherwise return null. This can be improved (e.g., for RexCall) later.
+   *
+   * @param column original column
+   * @param origColumnInterval original column interval
+   * @param predicate the predicate expression
+   * @param rexBuilder RexBuilder instance to analyze the predicate expression
+   * @return
+   */
+  private def intersectColumnIntervalWithPredicate(
+      column: RexNode,
+      origColumnInterval: Option[ValueInterval],
+      predicate: RexNode,
+      rexBuilder: RexBuilder): ValueInterval = {
+    column match {
+      case inputRef: RexInputRef =>
+        ColumnIntervalUtil.getColumnIntervalWithFilter(
+          origColumnInterval,
+          predicate,
+          inputRef.getIndex,
+          rexBuilder)
+      case literal: RexLiteral =>
+        val literalValue = FlinkRelOptUtil.getLiteralValueByBroadType(literal)
+        if (literalValue == null) {
+          ValueInterval.empty
+        } else {
+          ValueInterval(literalValue, literalValue)
+        }
+      case _ => null
+    }
+  }
+
   /**
     * Gets the column interval of the given column with filter argument on Filter
     *
@@ -133,10 +172,11 @@ class FlinkRelMdFilteredColumnInterval private extends MetadataHandler[FilteredC
       val filterRef = calc.getProgram.getProjectList.get(filterArg)
       val condition = calc.getProgram.expandLocalRef(filterRef)
       checkArgument(SqlTypeUtil.inBooleanFamily(condition.getType))
-      ColumnIntervalUtil.getColumnIntervalWithFilter(
+      val column = calc.getProgram.expandLocalRef(calc.getProgram.getProjectList.get(columnIndex))
+      intersectColumnIntervalWithPredicate(
+        column,
         Option(columnInterval),
         condition,
-        columnIndex,
         calc.getCluster.getRexBuilder)
     }
   }
@@ -236,7 +276,9 @@ class FlinkRelMdFilteredColumnInterval private extends MetadataHandler[FilteredC
       mq: RelMetadataQuery,
       columnIndex: Int,
       filterArg: Int): ValueInterval = {
-    checkArgument(filterArg == -1)
+    if (filterArg != -1) {
+      return null
+    }
     val fmq = FlinkRelMetadataQuery.reuseOrCreate(mq)
     fmq.getColumnInterval(rel, columnIndex)
   }
@@ -256,6 +298,10 @@ class FlinkRelMdFilteredColumnInterval private extends MetadataHandler[FilteredC
       columnIndex: Int,
       filterArg: Int): ValueInterval = {
     val fmq = FlinkRelMetadataQuery.reuseOrCreate(mq)
+    // do not support `union`
+    if (!union.all) {
+      return null
+    }
     val subIntervals = union
       .getInputs
       .map(fmq.getFilteredColumnInterval(_, columnIndex, filterArg))
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/ColumnIntervalUtil.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/ColumnIntervalUtil.scala
index ee1703da520..870ac5db2d9 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/ColumnIntervalUtil.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/ColumnIntervalUtil.scala
@@ -183,7 +183,7 @@ object ColumnIntervalUtil {
     * the interval of $1 is originInterval intersect with [-1, 2]
     *
     * for condition: $1 <= 2 and not ($1 < -1 or $2 is true),
-    * the interval of $1 is originInterval intersect with (-Inf, -1]
+    * the interval of $1 is originInterval intersect with [-1, 2]
     *
     * for condition $1 <= 2 or $1 > -1
     * the interval of $1 is (originInterval intersect with (-Inf, 2]) union
diff --git a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/agg/AggregateTest.xml b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/agg/AggregateTest.xml
index cab018becae..c78b8861a44 100644
--- a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/agg/AggregateTest.xml
+++ b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/agg/AggregateTest.xml
@@ -233,6 +233,40 @@ GroupAggregate(groupBy=[b], select=[b, SUM(a) AS EXPR$1])
 +- Exchange(distribution=[hash[b]])
    +- Calc(select=[a, b], where=[SEARCH(a, Sarg[(0.1:DECIMAL(11, 1)..10:DECIMAL(11, 1))]:DECIMAL(11, 1))])
       +- LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c, proctime, rowtime)]]], fields=[a, b, c, proctime, rowtime])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testFilteredColumnIntervalValidation">
+    <Resource name="sql">
+      <![CDATA[
+SELECT
+  SUM(uv) FILTER (WHERE c = 'all') AS all_uv
+FROM (
+  SELECT
+    c, COUNT(1) AS uv
+  FROM T
+  GROUP BY c
+) t
+]]>
+    </Resource>
+    <Resource name="ast">
+      <![CDATA[
+LogicalAggregate(group=[{}], all_uv=[SUM($0) FILTER $1])
++- LogicalProject(uv=[$1], $f1=[IS TRUE(=($0, _UTF-16LE'all'))])
+   +- LogicalAggregate(group=[{0}], uv=[COUNT()])
+      +- LogicalProject(c=[$2])
+         +- LogicalTableScan(table=[[default_catalog, default_database, T, source: [TestTableSource(a, b, c, d)]]])
+]]>
+    </Resource>
+    <Resource name="optimized exec plan">
+      <![CDATA[
+GroupAggregate(select=[SUM_RETRACT(uv) FILTER $f1 AS all_uv])
++- Exchange(distribution=[single])
+   +- Calc(select=[uv, (c = _UTF-16LE'all':VARCHAR(2147483647) CHARACTER SET "UTF-16LE") IS TRUE AS $f1])
+      +- GroupAggregate(groupBy=[c], select=[c, COUNT(*) AS uv])
+         +- Exchange(distribution=[hash[c]])
+            +- Calc(select=[c])
+               +- LegacyTableSourceScan(table=[[default_catalog, default_database, T, source: [TestTableSource(a, b, c, d)]]], fields=[a, b, c, d])
 ]]>
     </Resource>
   </TestCase>
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdColumnOriginNullCountTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdColumnOriginNullCountTest.scala
index 0f2530bf171..f4581fdc328 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdColumnOriginNullCountTest.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdColumnOriginNullCountTest.scala
@@ -115,7 +115,8 @@ class FlinkRelMdColumnOriginNullCountTest extends FlinkRelMdHandlerTestBase {
 
   @Test
   def testGetColumnOriginNullCountOnJoin(): Unit = {
-    val innerJoin1 = relBuilder.scan("MyTable3").scan("MyTable4")
+    val innerJoin1 = relBuilder.scan("MyTable3").project(relBuilder.fields().subList(0, 2))
+      .scan("MyTable4")
       .join(JoinRelType.INNER,
         relBuilder.call(EQUALS, relBuilder.field(2, 0, 1), relBuilder.field(2, 1, 1)))
       .build
@@ -124,7 +125,8 @@ class FlinkRelMdColumnOriginNullCountTest extends FlinkRelMdHandlerTestBase {
     assertEquals(0.0, mq.getColumnOriginNullCount(innerJoin1, 2))
     assertEquals(0.0, mq.getColumnOriginNullCount(innerJoin1, 3))
 
-    val innerJoin2 = relBuilder.scan("MyTable3").scan("MyTable4")
+    val innerJoin2 = relBuilder.scan("MyTable3").project(relBuilder.fields().subList(0, 2))
+      .scan("MyTable4")
       .join(JoinRelType.INNER,
         relBuilder.call(EQUALS, relBuilder.field(2, 0, 0), relBuilder.field(2, 1, 0)))
       .build
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdFilteredColumnIntervalTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdFilteredColumnIntervalTest.scala
index c9839001b31..54486521998 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdFilteredColumnIntervalTest.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdFilteredColumnIntervalTest.scala
@@ -22,7 +22,7 @@ import org.apache.flink.table.types.logical._
 
 import org.apache.calcite.rel.RelNode
 import org.apache.calcite.rex.RexNode
-import org.apache.calcite.sql.fun.SqlStdOperatorTable.{DIVIDE, EQUALS, GREATER_THAN, IS_FALSE, IS_TRUE, LESS_THAN, LESS_THAN_OR_EQUAL}
+import org.apache.calcite.sql.fun.SqlStdOperatorTable.{DIVIDE, EQUALS, GREATER_THAN, IS_FALSE, IS_NOT_NULL, IS_TRUE, LESS_THAN, LESS_THAN_OR_EQUAL, NOT_EQUALS}
 import org.junit.Assert.{assertEquals, assertNull}
 import org.junit.{Before, Test}
 
@@ -30,7 +30,8 @@ import scala.collection.JavaConversions._
 
 class FlinkRelMdFilteredColumnIntervalTest extends FlinkRelMdHandlerTestBase {
   private var ts: RelNode = _
-  private var expr1, expr2, expr3, expr4, expr5, expr6, expr7, expr8, expr9: RexNode = _
+  private var expr1, expr2, expr3, expr4, expr5, expr6, expr7, expr8, expr9, expr10,
+    expr11, expr12, expr13: RexNode = _
   private var projects: List[RexNode] = _
 
   @Before
@@ -57,6 +58,15 @@ class FlinkRelMdFilteredColumnIntervalTest extends FlinkRelMdHandlerTestBase {
     expr8 = relBuilder.call(IS_TRUE, expr4)
     // (b < 1.1) is false
     expr9 = relBuilder.call(IS_FALSE, expr4)
+    // c is not null
+    expr10 = relBuilder.call(IS_NOT_NULL, relBuilder.field(2))
+    // c in ('all', 'none')
+    expr11 = relBuilder
+      .in(relBuilder.field(2), relBuilder.literal("all"), relBuilder.literal("none"))
+    // c = 'all'
+    expr12 = relBuilder.call(EQUALS, relBuilder.field(2), relBuilder.literal("all"))
+    // c != 'all'
+    expr13 = relBuilder.call(NOT_EQUALS, relBuilder.field(2), relBuilder.literal("all"))
 
     // a, b, true, a = 1, a <= 2, a > -1, (a /2 ) > 3, b < 1.1, a > 90, a <= -1, b > 1.9,
     // (b < 1.1) is true, (b < 1.1) is false
@@ -65,7 +75,20 @@ class FlinkRelMdFilteredColumnIntervalTest extends FlinkRelMdHandlerTestBase {
       relBuilder.field(1),
       relBuilder.literal(true),
       relBuilder.call(EQUALS, relBuilder.field(0), relBuilder.literal(1)),
-      expr1, expr2, expr3, expr4, expr5, expr6, expr7, expr8, expr9)
+      expr1,
+      expr2,
+      expr3,
+      expr4,
+      expr5,
+      expr6,
+      expr7,
+      expr8,
+      expr9,
+      relBuilder.field(2),
+      expr10,
+      expr11,
+      expr12,
+      expr13)
   }
 
   @Test
@@ -88,6 +111,22 @@ class FlinkRelMdFilteredColumnIntervalTest extends FlinkRelMdHandlerTestBase {
     assertEquals(
       ValueInterval(bd(0D), bd(1.1D), includeUpper = false), mq.getFilteredColumnInterval(p, 1, 11))
     assertEquals(ValueInterval(bd(1.1D), bd(6.1D)), mq.getFilteredColumnInterval(p, 1, 12))
+    assertEquals(ValueInterval(bd(0.0D), bd(6.1D)), mq.getFilteredColumnInterval(p, 1, 14))
+    assertEquals(ValueInterval(bd(0.0D), bd(6.1D)), mq.getFilteredColumnInterval(p, 1, 15))
+    assertEquals(ValueInterval(bd(0.0D), bd(6.1D)), mq.getFilteredColumnInterval(p, 1, 16))
+    assertEquals(ValueInterval(bd(0.0D), bd(6.1D)), mq.getFilteredColumnInterval(p, 1, 17))
+    assertEquals(
+      ValueInterval(String.valueOf(""), String.valueOf("zzzzz")),
+      mq.getFilteredColumnInterval(p, 13, 14))
+    assertEquals(
+      ValueInterval(String.valueOf("all"), String.valueOf("none")),
+      mq.getFilteredColumnInterval(p, 13, 15))
+    assertEquals(
+      ValueInterval(String.valueOf("all"), String.valueOf("all")),
+      mq.getFilteredColumnInterval(p, 13, 16))
+    assertEquals(
+      ValueInterval(String.valueOf(""), String.valueOf("zzzzz")),
+      mq.getFilteredColumnInterval(p, 13, 17))
   }
 
   @Test
@@ -118,11 +157,13 @@ class FlinkRelMdFilteredColumnIntervalTest extends FlinkRelMdHandlerTestBase {
   @Test
   def testGetColumnIntervalOnCalc(): Unit = {
     val outputRowType = typeFactory.buildRelNodeRowType(
-      Array("f0", "f1", "f2", "f3", "f4", "f5", "f6", "f7", "f8", "f9", "f10", "f11", "f12"),
+      Array("f0", "f1", "f2", "f3", "f4", "f5", "f6", "f7", "f8", "f9", "f10", "f11", "f12", "f13",
+        "f14", "f15", "f16", "f17"),
       Array(new IntType(), new DoubleType(), new BooleanType(), new BooleanType(),
         new BooleanType(), new BooleanType(), new BooleanType(), new BooleanType(),
         new BooleanType(), new BooleanType(), new BooleanType(), new BooleanType(),
-        new BooleanType()))
+        new BooleanType(), new VarCharType(), new BooleanType(), new BooleanType(),
+        new BooleanType(), new BooleanType()))
     val calc = createLogicalCalc(ts, outputRowType, projects, List(expr1))
     assertEquals(ValueInterval(bd(-5), bd(2)), mq.getFilteredColumnInterval(calc, 0, -1))
     assertEquals(ValueInterval(bd(0D), bd(6.1D)), mq.getFilteredColumnInterval(calc, 1, -1))
@@ -169,6 +210,58 @@ class FlinkRelMdFilteredColumnIntervalTest extends FlinkRelMdHandlerTestBase {
     }
   }
 
+  @Test
+  def testGetColumnIntervalOnAggregateWithFilter(): Unit = {
+    // FilteredColumnInterval only used for calculating MonotonicityOnAggCall, and always via its
+    // input
+    Array(
+      logicalAggWithFilter, flinkLogicalAggWithFilter, batchGlobalAggWithoutLocalWithFilter,
+      streamGlobalAggWithoutLocalWithFilter).foreach { agg =>
+      assertEquals(ValueInterval(bd(12), bd(18)), mq.getFilteredColumnInterval(agg, 0, -1))
+      assertNull(mq.getFilteredColumnInterval(agg, 1, -1))
+      assertEquals(ValueInterval(bd(2.7), null), mq.getFilteredColumnInterval(agg, 4, -1))
+      // column interval of max is null
+      assertNull(mq.getFilteredColumnInterval(agg, 7, -1))
+      assertEquals(ValueInterval(bd(0), null), mq.getFilteredColumnInterval(agg, 13, -1))
+
+      // test aggregate's input column interval with filter `sex = 'M'` and `class > 3`
+      assertEquals(ValueInterval(bd(0), null), mq.getFilteredColumnInterval(agg.getInput, 0, 7))
+      assertEquals(
+        ValueInterval(bd(2.7), bd(4.8)),
+        mq.getFilteredColumnInterval(agg.getInput, 2, 7))
+      assertEquals(ValueInterval(bd(12), bd(18)), mq.getFilteredColumnInterval(agg.getInput, 3, 8))
+      assertEquals(ValueInterval("M", "M"), mq.getFilteredColumnInterval(agg.getInput, 5, 7))
+      assertEquals(
+        ValueInterval(bd(3), null, false),
+        mq.getFilteredColumnInterval(agg.getInput, 6, 8))
+    }
+
+    Array(batchGlobalAggWithLocalWithFilter, streamGlobalAggWithLocalWithFilter).foreach { agg =>
+      assertEquals(ValueInterval(bd(12), bd(18)), mq.getFilteredColumnInterval(agg, 0, -1))
+      assertNull(mq.getFilteredColumnInterval(agg, 1, -1))
+      assertNull(mq.getFilteredColumnInterval(agg.getInput, 2, 7))
+      assertNull(mq.getFilteredColumnInterval(agg.getInput, 3, 8))
+      assertNull(mq.getFilteredColumnInterval(agg, 4, -1))
+      assertNull(mq.getFilteredColumnInterval(agg.getInput, 5, 7))
+      assertNull(mq.getFilteredColumnInterval(agg.getInput, 6, 8))
+    }
+
+    Array(streamLocalAggWithFilter).foreach { agg =>
+      assertEquals(ValueInterval(bd(12), bd(18)), mq.getFilteredColumnInterval(agg, 0, -1))
+      assertNull(mq.getFilteredColumnInterval(agg, 1, -1))
+      assertEquals(
+        ValueInterval(bd(2.7), bd(4.8)),
+        mq.getFilteredColumnInterval(agg.getInput, 2, 7))
+      assertEquals(ValueInterval(bd(12), bd(18)), mq.getFilteredColumnInterval(agg.getInput, 3, 8))
+      // local aggregate's output is not equal to logical or global agg
+      assertNull(mq.getFilteredColumnInterval(agg, 4, -1))
+      assertEquals(ValueInterval("M", "M"), mq.getFilteredColumnInterval(agg.getInput, 5, 7))
+      assertEquals(
+        ValueInterval(bd(3), null, false),
+        mq.getFilteredColumnInterval(agg.getInput, 6, 8))
+    }
+  }
+
   @Test
   def testGetColumnIntervalOnTableAggregate(): Unit = {
     Array(logicalTableAgg, flinkLogicalTableAgg, streamExecTableAgg).foreach {
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdHandlerTestBase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdHandlerTestBase.scala
index bf91ebf20e2..69a2d182ea8 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdHandlerTestBase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdHandlerTestBase.scala
@@ -61,7 +61,7 @@ import org.apache.calcite.rel.logical._
 import org.apache.calcite.rel.metadata.{JaninoRelMetadataProvider, RelMetadataQuery, RelMetadataQueryBase}
 import org.apache.calcite.rex._
 import org.apache.calcite.schema.SchemaPlus
-import org.apache.calcite.sql.SqlWindow
+import org.apache.calcite.sql.{SqlAggFunction, SqlWindow}
 import org.apache.calcite.sql.`type`.SqlTypeName._
 import org.apache.calcite.sql.`type`.{BasicSqlType, SqlTypeName}
 import org.apache.calcite.sql.fun.SqlStdOperatorTable._
@@ -1052,6 +1052,243 @@ class FlinkRelMdHandlerTestBase {
       streamLocalAgg, streamGlobalAgg, streamGlobalAggWithoutLocal)
   }
 
+  // equivalent SQL is
+  // select age,
+  //        avg(score) as avg_score,
+  //        avg(score) filter(where sex = 'M') as m_avg_score,
+  //        avg(score) filter(where class > 3) as c3_avg_score,
+  //        sum(score) as sum_score,
+  //        sum(score) filter(where sex = 'M') as m_sum_score,
+  //        sum(score) filter(where class > 3) as c3_sum_score,
+  //        max(height) as max_height,
+  //        max(height) filter(where sex = 'M') as m_max_height,
+  //        max(height) filter(where class > 3) as c3_max_height,
+  //        min(height) as min_height,
+  //        min(height) filter(where sex = 'M') as m_min_height,
+  //        min(height) filter(where class > 3) as c3_min_height,
+  //        count(id) as cnt,
+  //        count(id) filter(where sex = 'M') as m_cnt,
+  //        count(id) filter(where class > 3) as c3_cnt
+  // from student group by age
+  protected lazy val (
+    logicalAggWithFilter,
+    flinkLogicalAggWithFilter,
+    batchLocalAggWithFilter,
+    batchGlobalAggWithLocalWithFilter,
+    batchGlobalAggWithoutLocalWithFilter,
+    streamLocalAggWithFilter,
+    streamGlobalAggWithLocalWithFilter,
+    streamGlobalAggWithoutLocalWithFilter) = {
+
+    relBuilder.push(studentLogicalScan)
+    val projects = List(
+      relBuilder.field(0),
+      relBuilder.field(1),
+      relBuilder.field(2),
+      relBuilder.field(3),
+      relBuilder.field(4),
+      relBuilder.field(5),
+      relBuilder.field(6),
+      // sex is not null and sex = 'M'
+      relBuilder.call(IS_TRUE,
+        relBuilder.call(EQUALS, relBuilder.field(5), relBuilder.literal("M"))),
+      // class is not null and class > 3
+      relBuilder.call(IS_TRUE,
+        relBuilder.call(GREATER_THAN, relBuilder.field(6), relBuilder.literal(3))))
+    val outputRowType = typeFactory.buildRelNodeRowType(
+      Array("id", "name", "score", "age", "height", "sex", "class", "f7", "f8"),
+      Array(new BigIntType, new VarCharType, new DoubleType, new IntType, new DoubleType,
+        new VarCharType, new IntType, new BooleanType(false), new BooleanType(false)))
+    val calcOnStudentScan = createLogicalCalc(studentLogicalScan, outputRowType, projects, null)
+    relBuilder.push(calcOnStudentScan)
+
+    def createSingleArgAggWithFilter(
+        aggFunction: SqlAggFunction,
+        argIndex: Int,
+        filterArg: Int,
+        name: String): AggregateCall = {
+      AggregateCall.create(
+        aggFunction,
+        false,
+        false,
+        false,
+        List(Integer.valueOf(argIndex)),
+        filterArg,
+        RelCollations.EMPTY,
+        1,
+        calcOnStudentScan,
+        null,
+        name)
+    }
+
+    val aggCallList = List(
+      createSingleArgAggWithFilter(AVG, 2, -1, "avg_score"),
+      createSingleArgAggWithFilter(AVG, 2, 7, "m_avg_score"),
+      createSingleArgAggWithFilter(AVG, 2, 8, "c3_avg_score"),
+      createSingleArgAggWithFilter(SUM, 2, -1, "sum_score"),
+      createSingleArgAggWithFilter(SUM, 2, 7, "m_sum_score"),
+      createSingleArgAggWithFilter(SUM, 2, 8, "c3_sum_score"),
+      createSingleArgAggWithFilter(MAX, 4, -1, "max_height"),
+      createSingleArgAggWithFilter(MAX, 4, 7, "m_max_height"),
+      createSingleArgAggWithFilter(MAX, 4, 8, "c3_max_height"),
+      createSingleArgAggWithFilter(MIN, 4, -1, "min_height"),
+      createSingleArgAggWithFilter(MIN, 4, 7, "c3_min_height"),
+      createSingleArgAggWithFilter(MIN, 4, 8, "c3_min_height"),
+      createSingleArgAggWithFilter(COUNT, 0, -1, "cnt"),
+      createSingleArgAggWithFilter(COUNT, 0, 7, "m_cnt"),
+      createSingleArgAggWithFilter(COUNT, 0, 8, "c3_cnt"))
+
+    val logicalAggWithFilter = LogicalAggregate.create(
+      calcOnStudentScan,
+      List(),
+      ImmutableBitSet.of(3),
+      List(ImmutableBitSet.of(3)),
+      aggCallList)
+
+    val flinkLogicalAggWithFilter = new FlinkLogicalAggregate(
+      cluster,
+      flinkLogicalTraits,
+      calcOnStudentScan,
+      logicalAggWithFilter.getGroupSet,
+      logicalAggWithFilter.getGroupSets,
+      logicalAggWithFilter.getAggCallList)
+
+    val aggCalls = logicalAggWithFilter.getAggCallList
+    val aggFunctionFactory = new AggFunctionFactory(
+      FlinkTypeFactory.toLogicalRowType(calcOnStudentScan.getRowType),
+      Array.empty[Int],
+      Array.fill(aggCalls.size())(false))
+    val aggCallToAggFunction = aggCalls.zipWithIndex.map {
+      case (call, index) => (call, aggFunctionFactory.createAggFunction(call, index))
+    }
+    val rowTypeOfLocalAgg = typeFactory.builder
+      .add("age", intType)
+      .add("sum$0", doubleType)
+      .add("count$1", longType)
+      .add("sum$2", doubleType)
+      .add("count$3", longType)
+      .add("sum$4", doubleType)
+      .add("count$5", longType)
+      .add("sum$6", doubleType)
+      .add("sum$7", doubleType)
+      .add("sum$8", doubleType)
+      .add("max$9", doubleType)
+      .add("max$10", doubleType)
+      .add("max$11", doubleType)
+      .add("min$12", doubleType)
+      .add("min$13", doubleType)
+      .add("min$14", doubleType)
+      .add("count$15", longType)
+      .add("count$16", longType)
+      .add("count$17", longType).build()
+
+    val rowTypeOfGlobalAgg = typeFactory.builder
+      .add("age", intType)
+      .add("avg_score", doubleType)
+      .add("m_avg_score", doubleType)
+      .add("c3_avg_score", doubleType)
+      .add("sum_score", doubleType)
+      .add("m_sum_score", doubleType)
+      .add("c3_sum_score", doubleType)
+      .add("max_height", doubleType)
+      .add("m_max_height", doubleType)
+      .add("c3_max_height", doubleType)
+      .add("min_height", doubleType)
+      .add("m_min_height", doubleType)
+      .add("c3_min_height", doubleType)
+      .add("cnt", longType)
+      .add("m_cnt", longType)
+      .add("c3_cnt", longType).build()
+
+    val hash0 = FlinkRelDistribution.hash(Array(0), requireStrict = true)
+    val hash3 = FlinkRelDistribution.hash(Array(3), requireStrict = true)
+
+    val batchLocalAggWithFilter = new BatchPhysicalLocalHashAggregate(
+      cluster,
+      batchPhysicalTraits,
+      calcOnStudentScan,
+      rowTypeOfLocalAgg,
+      calcOnStudentScan.getRowType,
+      Array(3),
+      auxGrouping = Array(),
+      aggCallToAggFunction)
+
+    val batchExchange1 = new BatchPhysicalExchange(
+      cluster, batchLocalAggWithFilter.getTraitSet.replace(hash0), batchLocalAgg, hash0)
+    val batchGlobalAgg = new BatchPhysicalHashAggregate(
+      cluster,
+      batchPhysicalTraits,
+      batchExchange1,
+      rowTypeOfGlobalAgg,
+      batchExchange1.getRowType,
+      batchLocalAggWithFilter.getInput.getRowType,
+      Array(0),
+      auxGrouping = Array(),
+      aggCallToAggFunction,
+      isMerge = true)
+
+    val batchExchange2 = new BatchPhysicalExchange(
+      cluster,
+      calcOnStudentScan.getTraitSet.replace(hash3),
+      calcOnStudentScan,
+      hash3)
+    val batchGlobalAggWithoutLocalWithFilter = new BatchPhysicalHashAggregate(
+      cluster,
+      batchPhysicalTraits,
+      batchExchange2,
+      rowTypeOfGlobalAgg,
+      batchExchange2.getRowType,
+      batchExchange2.getRowType,
+      Array(3),
+      auxGrouping = Array(),
+      aggCallToAggFunction,
+      isMerge = false)
+
+    val aggCallNeedRetractions = AggregateUtil.deriveAggCallNeedRetractions(
+      1, aggCalls, needRetraction = false, null)
+    val streamLocalAggWithFilter = new StreamPhysicalLocalGroupAggregate(
+      cluster,
+      streamPhysicalTraits,
+      calcOnStudentScan,
+      Array(3),
+      aggCalls,
+      aggCallNeedRetractions,
+      false,
+      PartialFinalType.NONE)
+
+    val streamExchange1 = new StreamPhysicalExchange(
+      cluster, streamLocalAggWithFilter.getTraitSet.replace(hash0), streamLocalAgg, hash0)
+    val streamGlobalAgg = new StreamPhysicalGlobalGroupAggregate(
+      cluster,
+      streamPhysicalTraits,
+      streamExchange1,
+      rowTypeOfGlobalAgg,
+      Array(0),
+      aggCalls,
+      aggCallNeedRetractions,
+      streamLocalAggWithFilter.getInput.getRowType,
+      AggregateUtil.needRetraction(streamLocalAggWithFilter),
+      PartialFinalType.NONE)
+
+    val streamExchange2 = new StreamPhysicalExchange(
+      cluster,
+      calcOnStudentScan.getTraitSet.replace(hash3),
+      calcOnStudentScan,
+      hash3)
+    val streamGlobalAggWithoutLocalWithFilter = new StreamPhysicalGroupAggregate(
+      cluster,
+      streamPhysicalTraits,
+      streamExchange2,
+      rowTypeOfGlobalAgg,
+      Array(3),
+      aggCalls)
+
+    (logicalAggWithFilter, flinkLogicalAggWithFilter,
+      batchLocalAggWithFilter, batchGlobalAgg, batchGlobalAggWithoutLocalWithFilter,
+      streamLocalAggWithFilter, streamGlobalAgg, streamGlobalAggWithoutLocalWithFilter)
+  }
+
+  // equivalent SQL is
   // equivalent SQL is
   // select avg(score) as avg_score,
   //        sum(score) as sum_score,
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdSelectivityTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdSelectivityTest.scala
index 95b2f534614..e453dba0c7e 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdSelectivityTest.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdSelectivityTest.scala
@@ -150,7 +150,7 @@ class FlinkRelMdSelectivityTest extends FlinkRelMdHandlerTestBase {
 
   @Test
   def testGetSelectivityOnExpand(): Unit = {
-    val ts = relBuilder.scan("MyTable3").build()
+    val ts = relBuilder.scan("MyTable3").project(relBuilder.fields().subList(0, 2)).build()
     val expandProjects = ExpandUtil.createExpandProjects(
       ts.getCluster.getRexBuilder,
       ts.getRowType,
@@ -464,7 +464,7 @@ class FlinkRelMdSelectivityTest extends FlinkRelMdHandlerTestBase {
 
   @Test
   def testGetSelectivityOnJoin(): Unit = {
-    val ts = relBuilder.scan("MyTable3").build()
+    val ts = relBuilder.scan("MyTable3").project(relBuilder.fields().subList(0, 2)).build()
     // right is $0 <= 2 and $1 < 1.1
     val right = relBuilder.push(ts).filter(
       relBuilder.call(LESS_THAN_OR_EQUAL, relBuilder.field(0), relBuilder.literal(2)),
@@ -507,7 +507,7 @@ class FlinkRelMdSelectivityTest extends FlinkRelMdHandlerTestBase {
   def testGetSelectivityOnUnion(): Unit = {
     val union = relBuilder
       .scan("MyTable4").project(relBuilder.fields().subList(0, 2))
-      .scan("MyTable3")
+      .scan("MyTable3").project(relBuilder.fields().subList(0, 2))
       .union(true).build()
     // a <= 2
     val pred = relBuilder.push(union).call(
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/MetadataTestUtil.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/MetadataTestUtil.scala
index 1b78562625e..88ee8850fd6 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/MetadataTestUtil.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/MetadataTestUtil.scala
@@ -20,7 +20,7 @@ package org.apache.flink.table.planner.plan.metadata
 
 import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, SqlTimeTypeInfo}
 import org.apache.flink.table.api.{DataTypes, TableException, TableSchema}
-import org.apache.flink.table.catalog.{CatalogTable, CatalogTableImpl, Column, ObjectIdentifier, ResolvedCatalogTable, ResolvedSchema, UniqueConstraint}
+import org.apache.flink.table.catalog.{CatalogTable, Column, ObjectIdentifier, ResolvedCatalogTable, ResolvedSchema, UniqueConstraint}
 import org.apache.flink.table.connector.ChangelogMode
 import org.apache.flink.table.connector.source.{DynamicTableSource, ScanTableSource}
 import org.apache.flink.table.plan.stats.{ColumnStats, TableStats}
@@ -29,6 +29,7 @@ import org.apache.flink.table.planner.plan.schema.{FlinkPreparingTableBase, Tabl
 import org.apache.flink.table.planner.plan.stats.FlinkStatistic
 import org.apache.flink.table.runtime.types.TypeInfoLogicalTypeConverter.fromTypeInfoToLogicalType
 import org.apache.flink.table.types.logical.{BigIntType, DoubleType, IntType, LocalZonedTimestampType, LogicalType, TimestampKind, TimestampType, VarCharType}
+
 import org.apache.calcite.config.CalciteConnectionConfig
 import org.apache.calcite.jdbc.CalciteSchema
 import org.apache.calcite.rel.`type`.{RelDataType, RelDataTypeFactory}
@@ -150,12 +151,18 @@ object MetadataTestUtil {
 
   private def createMyTable3(): Table = {
     val schema = new TableSchema(
-      Array("a", "b"),
-      Array(BasicTypeInfo.INT_TYPE_INFO, BasicTypeInfo.DOUBLE_TYPE_INFO))
+      Array("a", "b", "c"),
+      Array(
+        BasicTypeInfo.INT_TYPE_INFO,
+        BasicTypeInfo.DOUBLE_TYPE_INFO,
+        BasicTypeInfo.STRING_TYPE_INFO))
 
     val colStatsMap = Map[String, ColumnStats](
       "a" -> new ColumnStats(10L, 1L, 4D, 4, 5, -5),
-      "b" -> new ColumnStats(5L, 0L, 8D, 8, 6.1D, 0D)
+      "b" -> new ColumnStats(5L, 0L, 8D, 8, 6.1D, 0D),
+      "c" ->
+        ColumnStats.Builder.builder().setNdv(100L).setNullCount(1L).setAvgLen(16D).setMaxLen(128)
+          .setMax("zzzzz").setMin("").build()
     )
 
     val tableStats = new TableStats(100L, colStatsMap)
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/stream/sql/agg/AggregateTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/stream/sql/agg/AggregateTest.scala
index a386378d7e3..54a01be905c 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/stream/sql/agg/AggregateTest.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/stream/sql/agg/AggregateTest.scala
@@ -25,10 +25,10 @@ import org.apache.flink.table.api.config.ExecutionConfigOptions
 import org.apache.flink.table.planner.utils.{StreamTableTestUtil, TableTestBase}
 import org.apache.flink.table.runtime.typeutils.DecimalDataTypeInfo
 
-import java.time.Duration
-
 import org.junit.Test
 
+import java.time.Duration
+
 class AggregateTest extends TableTestBase {
 
   private val util: StreamTableTestUtil = streamTestUtil()
@@ -280,4 +280,20 @@ class AggregateTest extends TableTestBase {
     // test for FLINK-16577
     util.verifyExecPlan("SELECT b, SUM(a) FROM MyTable WHERE a > 0.1 and a < 10 GROUP BY b")
   }
+
+  @Test
+  def testFilteredColumnIntervalValidation(): Unit = {
+    // test for FLINK-22303
+    util.verifyExecPlan(
+      s"""
+         |SELECT
+         |  SUM(uv) FILTER (WHERE c = 'all') AS all_uv
+         |FROM (
+         |  SELECT
+         |    c, COUNT(1) AS uv
+         |  FROM T
+         |  GROUP BY c
+         |) t
+         |""".stripMargin)
+  }
 }
