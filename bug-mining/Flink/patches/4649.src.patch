diff --git a/flink-formats/flink-orc/src/main/java/org/apache/flink/orc/writer/OrcBulkWriterFactory.java b/flink-formats/flink-orc/src/main/java/org/apache/flink/orc/writer/OrcBulkWriterFactory.java
index b5069d621ee..2c5e0d1f034 100644
--- a/flink-formats/flink-orc/src/main/java/org/apache/flink/orc/writer/OrcBulkWriterFactory.java
+++ b/flink-formats/flink-orc/src/main/java/org/apache/flink/orc/writer/OrcBulkWriterFactory.java
@@ -19,6 +19,7 @@
 package org.apache.flink.orc.writer;
 
 import org.apache.flink.annotation.PublicEvolving;
+import org.apache.flink.annotation.VisibleForTesting;
 import org.apache.flink.api.common.serialization.BulkWriter;
 import org.apache.flink.core.fs.FSDataOutputStream;
 import org.apache.flink.orc.vector.Vectorizer;
@@ -32,6 +33,7 @@ import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
 import java.util.Properties;
+import java.util.UUID;
 
 import static org.apache.flink.util.Preconditions.checkNotNull;
 
@@ -45,15 +47,6 @@ import static org.apache.flink.util.Preconditions.checkNotNull;
 @PublicEvolving
 public class OrcBulkWriterFactory<T> implements BulkWriter.Factory<T> {
 
-	/*
-	A dummy Hadoop Path to work around the current implementation of ORC WriterImpl which
-	works on the basis of a Hadoop FileSystem and Hadoop Path but since we use a customised
-	ORC PhysicalWriter implementation that uses Flink's own FSDataOutputStream as the
-	underlying/internal stream instead of Hadoop's FSDataOutputStream, we don't have to worry
-	about this usage.
-	 */
-	private static final Path FIXED_PATH = new Path(".");
-
 	private final Vectorizer<T> vectorizer;
 	private final Properties writerProperties;
 	private final Map<String, String> confMap;
@@ -106,10 +99,16 @@ public class OrcBulkWriterFactory<T> implements BulkWriter.Factory<T> {
 		OrcFile.WriterOptions opts = getWriterOptions();
 		opts.physicalWriter(new PhysicalWriterImpl(out, opts));
 
-		return new OrcBulkWriter<>(vectorizer, new WriterImpl(null, FIXED_PATH, opts));
+		// The path of the Writer is not used to indicate the destination file
+		// in this case since we have used a dedicated physical writer to write
+		// to the give output stream directly. However, the path would be used as
+		// the key of writer in the ORC memory manager, thus we need to make it unique.
+		Path unusedPath = new Path(UUID.randomUUID().toString());
+		return new OrcBulkWriter<>(vectorizer, new WriterImpl(null, unusedPath, opts));
 	}
 
-	private OrcFile.WriterOptions getWriterOptions() {
+	@VisibleForTesting
+	protected OrcFile.WriterOptions getWriterOptions() {
 		if (null == writerOptions) {
 			Configuration conf = new ThreadLocalClassLoaderConfiguration();
 			for (Map.Entry<String, String> entry : confMap.entrySet()) {
diff --git a/flink-formats/flink-orc/src/test/java/org/apache/flink/orc/writer/OrcBulkWriterFactoryTest.java b/flink-formats/flink-orc/src/test/java/org/apache/flink/orc/writer/OrcBulkWriterFactoryTest.java
new file mode 100644
index 00000000000..2b6676ce5ee
--- /dev/null
+++ b/flink-formats/flink-orc/src/test/java/org/apache/flink/orc/writer/OrcBulkWriterFactoryTest.java
@@ -0,0 +1,102 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.orc.writer;
+
+import org.apache.flink.core.fs.local.LocalDataOutputStream;
+import org.apache.flink.orc.data.Record;
+import org.apache.flink.orc.vector.RecordVectorizer;
+import org.apache.flink.orc.vector.Vectorizer;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.orc.MemoryManager;
+import org.apache.orc.OrcFile;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import static org.apache.flink.util.Preconditions.checkNotNull;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertNotEquals;
+
+/**
+ * Tests the behavior of {@link OrcBulkWriterFactory}.
+ */
+public class OrcBulkWriterFactoryTest  {
+
+	@Rule
+	public TemporaryFolder temporaryFolder = new TemporaryFolder();
+
+	@Test
+	public void testNotOverrideInMemoryManager() throws IOException {
+		TestMemoryManager memoryManager = new TestMemoryManager();
+		OrcBulkWriterFactory<Record> factory = new TestOrcBulkWriterFactory<>(
+			new RecordVectorizer("struct<_col0:string,_col1:int>"),
+			memoryManager);
+		factory.create(new LocalDataOutputStream(temporaryFolder.newFile()));
+		factory.create(new LocalDataOutputStream(temporaryFolder.newFile()));
+
+		List<Path> addedWriterPath = memoryManager.getAddedWriterPath();
+		assertEquals(2, addedWriterPath.size());
+		assertNotEquals(addedWriterPath.get(0), addedWriterPath.get(1));
+	}
+
+	private static class TestOrcBulkWriterFactory<T> extends OrcBulkWriterFactory<T> {
+
+		private final MemoryManager memoryManager;
+
+		public TestOrcBulkWriterFactory(Vectorizer<T> vectorizer, MemoryManager memoryManager) {
+			super(vectorizer);
+			this.memoryManager = checkNotNull(memoryManager);
+		}
+
+		@Override
+		protected OrcFile.WriterOptions getWriterOptions() {
+			OrcFile.WriterOptions options = super.getWriterOptions();
+			options.memory(memoryManager);
+			return options;
+		}
+	}
+
+	private static class TestMemoryManager implements MemoryManager {
+		private final List<Path> addedWriterPath = new ArrayList<>();
+
+		@Override
+		public void addWriter(Path path, long requestedAllocation, Callback callback) {
+			addedWriterPath.add(path);
+		}
+
+		public List<Path> getAddedWriterPath() {
+			return addedWriterPath;
+		}
+
+		@Override
+		public void removeWriter(Path path) {
+
+		}
+
+		@Override
+		public void addedRow(int rows) {
+
+		}
+	}
+}
