diff --git a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicSink.java b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicSink.java
index f46951e69da..950c5805423 100644
--- a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicSink.java
+++ b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicSink.java
@@ -306,10 +306,13 @@ public class KafkaDynamicSink implements DynamicTableSink, SupportsWritingMetada
 	enum WritableMetadata {
 
 		HEADERS(
-				"headers",
-				// key and value of the map are nullable to make handling easier in queries
-				DataTypes.MAP(DataTypes.STRING().nullable(), DataTypes.BYTES().nullable()).nullable(),
-				(row, pos) -> {
+			"headers",
+			// key and value of the map are nullable to make handling easier in queries
+			DataTypes.MAP(DataTypes.STRING().nullable(), DataTypes.BYTES().nullable()).nullable(),
+			new MetadataConverter() {
+				private static final long serialVersionUID = 1L;
+				@Override
+				public Object read(RowData row, int pos) {
 					if (row.isNullAt(pos)) {
 						return null;
 					}
@@ -326,17 +329,23 @@ public class KafkaDynamicSink implements DynamicTableSink, SupportsWritingMetada
 					}
 					return headers;
 				}
+			}
 		),
 
 		TIMESTAMP(
-				"timestamp",
-				DataTypes.TIMESTAMP_WITH_LOCAL_TIME_ZONE(3).nullable(),
-				(row, pos) -> {
+			"timestamp",
+			DataTypes.TIMESTAMP_WITH_LOCAL_TIME_ZONE(3).nullable(),
+			new MetadataConverter() {
+				private static final long serialVersionUID = 1L;
+				@Override
+				public Object read(RowData row, int pos) {
 					if (row.isNullAt(pos)) {
 						return null;
 					}
 					return row.getTimestamp(pos, 3).getMillisecond();
-				});
+				}
+			}
+		);
 
 		final String key;
 
diff --git a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicSource.java b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicSource.java
index 203db92da5e..463ff554502 100644
--- a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicSource.java
+++ b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicSource.java
@@ -409,48 +409,90 @@ public class KafkaDynamicSource implements ScanTableSource, SupportsReadingMetad
 		TOPIC(
 			"topic",
 			DataTypes.STRING().notNull(),
-			record -> StringData.fromString(record.topic())
+			new MetadataConverter() {
+				private static final long serialVersionUID = 1L;
+				@Override
+				public Object read(ConsumerRecord<?, ?> record) {
+					return StringData.fromString(record.topic());
+				}
+			}
 		),
 
 		PARTITION(
 			"partition",
 			DataTypes.INT().notNull(),
-			ConsumerRecord::partition
+			new MetadataConverter() {
+				private static final long serialVersionUID = 1L;
+				@Override
+				public Object read(ConsumerRecord<?, ?> record) {
+					return record.partition();
+				}
+			}
 		),
 
 		HEADERS(
 			"headers",
 			// key and value of the map are nullable to make handling easier in queries
 			DataTypes.MAP(DataTypes.STRING().nullable(), DataTypes.BYTES().nullable()).notNull(),
-			record -> {
-				final Map<StringData, byte[]> map = new HashMap<>();
-				for (Header header : record.headers()) {
-					map.put(StringData.fromString(header.key()), header.value());
+			new MetadataConverter() {
+				private static final long serialVersionUID = 1L;
+				@Override
+				public Object read(ConsumerRecord<?, ?> record) {
+					final Map<StringData, byte[]> map = new HashMap<>();
+					for (Header header : record.headers()) {
+						map.put(StringData.fromString(header.key()), header.value());
+					}
+					return new GenericMapData(map);
 				}
-				return new GenericMapData(map);
 			}
 		),
 
 		LEADER_EPOCH(
 			"leader-epoch",
 			DataTypes.INT().nullable(),
-			record -> record.leaderEpoch().orElse(null)
+			new MetadataConverter() {
+				private static final long serialVersionUID = 1L;
+				@Override
+				public Object read(ConsumerRecord<?, ?> record) {
+					return record.leaderEpoch().orElse(null);
+				}
+			}
 		),
 
 		OFFSET(
 			"offset",
 			DataTypes.BIGINT().notNull(),
-			ConsumerRecord::offset),
+			new MetadataConverter() {
+				private static final long serialVersionUID = 1L;
+				@Override
+				public Object read(ConsumerRecord<?, ?> record) {
+					return record.offset();
+				}
+			}
+		),
 
 		TIMESTAMP(
 			"timestamp",
 			DataTypes.TIMESTAMP_WITH_LOCAL_TIME_ZONE(3).notNull(),
-			record -> TimestampData.fromEpochMillis(record.timestamp())),
+			new MetadataConverter() {
+				private static final long serialVersionUID = 1L;
+				@Override
+				public Object read(ConsumerRecord<?, ?> record) {
+					return TimestampData.fromEpochMillis(record.timestamp());
+				}
+			}
+		),
 
 		TIMESTAMP_TYPE(
 			"timestamp-type",
 			DataTypes.STRING().notNull(),
-			record -> StringData.fromString(record.timestampType().toString())
+			new MetadataConverter() {
+				private static final long serialVersionUID = 1L;
+				@Override
+				public Object read(ConsumerRecord<?, ?> record) {
+					return StringData.fromString(record.timestampType().toString());
+				}
+			}
 		);
 
 		final String key;
