diff --git a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/catalog/hive/HiveCatalogITCase.java b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/catalog/hive/HiveCatalogITCase.java
index e1c27dfd940..406405748af 100644
--- a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/catalog/hive/HiveCatalogITCase.java
+++ b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/catalog/hive/HiveCatalogITCase.java
@@ -44,6 +44,7 @@ import com.klarna.hiverunner.HiveShell;
 import com.klarna.hiverunner.annotations.HiveSQL;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.junit.AfterClass;
+import org.junit.Assert;
 import org.junit.BeforeClass;
 import org.junit.Rule;
 import org.junit.Test;
@@ -56,6 +57,7 @@ import java.io.FileReader;
 import java.net.URI;
 import java.nio.file.Path;
 import java.nio.file.Paths;
+import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Comparator;
 import java.util.HashSet;
@@ -260,4 +262,49 @@ public class HiveCatalogITCase {
 				"2019-12-12 00:00:10.0,2019-12-12 00:00:06.006001,2,5.33\n";
 		assertEquals(expected, FileUtils.readFileUtf8(new File(new URI(sinkPath))));
 	}
+
+	@Test
+	public void testBatchReadWriteCsvWithProctime() {
+		testReadWriteCsvWithProctime(false);
+	}
+
+	@Test
+	public void testStreamReadWriteCsvWithProctime() {
+		testReadWriteCsvWithProctime(true);
+	}
+
+	private void testReadWriteCsvWithProctime(boolean isStreaming) {
+		EnvironmentSettings.Builder builder = EnvironmentSettings.newInstance().useBlinkPlanner();
+		if (isStreaming) {
+			builder = builder.inStreamingMode();
+		} else {
+			builder = builder.inBatchMode();
+		}
+		EnvironmentSettings settings = builder.build();
+		TableEnvironment tableEnv = TableEnvironment.create(settings);
+		tableEnv.getConfig().getConfiguration().setInteger(
+				ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM, 1);
+
+		tableEnv.registerCatalog("myhive", hiveCatalog);
+		tableEnv.useCatalog("myhive");
+
+		String srcPath = this.getClass().getResource("/csv/test3.csv").getPath();
+
+		tableEnv.executeSql("CREATE TABLE proctime_src (" +
+				"price DECIMAL(10, 2)," +
+				"currency STRING," +
+				"ts6 TIMESTAMP(6)," +
+				"ts AS CAST(ts6 AS TIMESTAMP(3))," +
+				"WATERMARK FOR ts AS ts," +
+				"l_proctime AS PROCTIME( )) " + // test " " in proctime()
+				String.format("WITH (" +
+						"'connector.type' = 'filesystem'," +
+						"'connector.path' = 'file://%s'," +
+						"'format.type' = 'csv')", srcPath));
+
+		ArrayList<Row> rows = Lists.newArrayList(
+				tableEnv.executeSql("SELECT * FROM proctime_src").collect());
+		Assert.assertEquals(5, rows.size());
+		tableEnv.executeSql("DROP TABLE proctime_src");
+	}
 }
diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/catalog/CatalogCalciteSchema.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/catalog/CatalogCalciteSchema.java
index f869f46d6b4..a2160973096 100644
--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/catalog/CatalogCalciteSchema.java
+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/catalog/CatalogCalciteSchema.java
@@ -20,6 +20,7 @@ package org.apache.flink.table.planner.catalog;
 
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.table.catalog.CatalogManager;
+import org.apache.flink.table.planner.calcite.SqlExprToRexConverterFactory;
 
 import org.apache.calcite.linq4j.tree.Expression;
 import org.apache.calcite.schema.Schema;
@@ -42,10 +43,18 @@ public class CatalogCalciteSchema extends FlinkSchema {
 	private final CatalogManager catalogManager;
 	// Flag that tells if the current planner should work in a batch or streaming mode.
 	private final boolean isStreamingMode;
+	// The SQL expression converter factory is used to derive correct result type of computed column,
+	// because the date type of computed column from catalog table is not trusted.
+	private final SqlExprToRexConverterFactory converterFactory;
 
-	public CatalogCalciteSchema(String catalogName, CatalogManager catalog, boolean isStreamingMode) {
+	public CatalogCalciteSchema(
+			String catalogName,
+			CatalogManager catalog,
+			SqlExprToRexConverterFactory converterFactory,
+			boolean isStreamingMode) {
 		this.catalogName = catalogName;
 		this.catalogManager = catalog;
+		this.converterFactory = converterFactory;
 		this.isStreamingMode = isStreamingMode;
 	}
 
@@ -58,7 +67,8 @@ public class CatalogCalciteSchema extends FlinkSchema {
 	@Override
 	public Schema getSubSchema(String schemaName) {
 		if (catalogManager.schemaExists(catalogName, schemaName)) {
-			return new DatabaseCalciteSchema(schemaName, catalogName, catalogManager, isStreamingMode);
+			return new DatabaseCalciteSchema(
+					schemaName, catalogName, catalogManager, converterFactory, isStreamingMode);
 		} else {
 			return null;
 		}
diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/catalog/CatalogManagerCalciteSchema.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/catalog/CatalogManagerCalciteSchema.java
index 0afd2a89d5e..84e7671d275 100644
--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/catalog/CatalogManagerCalciteSchema.java
+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/catalog/CatalogManagerCalciteSchema.java
@@ -21,6 +21,7 @@ package org.apache.flink.table.planner.catalog;
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.table.catalog.Catalog;
 import org.apache.flink.table.catalog.CatalogManager;
+import org.apache.flink.table.planner.calcite.SqlExprToRexConverterFactory;
 
 import org.apache.calcite.linq4j.tree.Expression;
 import org.apache.calcite.schema.Schema;
@@ -44,10 +45,17 @@ public class CatalogManagerCalciteSchema extends FlinkSchema {
 	private final CatalogManager catalogManager;
 	// Flag that tells if the current planner should work in a batch or streaming mode.
 	private final boolean isStreamingMode;
+	// The SQL expression converter factory is used to derive correct result type of computed column,
+	// because the date type of computed column from catalog table is not trusted.
+	private final SqlExprToRexConverterFactory converterFactory;
 
-	public CatalogManagerCalciteSchema(CatalogManager catalogManager, boolean isStreamingMode) {
+	public CatalogManagerCalciteSchema(
+			CatalogManager catalogManager,
+			SqlExprToRexConverterFactory converterFactory,
+			boolean isStreamingMode) {
 		this.catalogManager = catalogManager;
 		this.isStreamingMode = isStreamingMode;
+		this.converterFactory = converterFactory;
 	}
 
 	@Override
@@ -63,7 +71,7 @@ public class CatalogManagerCalciteSchema extends FlinkSchema {
 	@Override
 	public Schema getSubSchema(String name) {
 		if (catalogManager.schemaExists(name)) {
-			return new CatalogCalciteSchema(name, catalogManager, isStreamingMode);
+			return new CatalogCalciteSchema(name, catalogManager, converterFactory, isStreamingMode);
 		} else {
 			return null;
 		}
diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/catalog/CatalogSchemaTable.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/catalog/CatalogSchemaTable.java
index 693e6ae92c9..e0b2217286b 100644
--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/catalog/CatalogSchemaTable.java
+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/catalog/CatalogSchemaTable.java
@@ -34,6 +34,7 @@ import org.apache.flink.table.factories.TableFactoryUtil;
 import org.apache.flink.table.factories.TableSourceFactory;
 import org.apache.flink.table.factories.TableSourceFactoryContextImpl;
 import org.apache.flink.table.planner.calcite.FlinkTypeFactory;
+import org.apache.flink.table.planner.calcite.SqlExprToRexConverterFactory;
 import org.apache.flink.table.planner.plan.stats.FlinkStatistic;
 import org.apache.flink.table.planner.sources.TableSourceUtil;
 import org.apache.flink.table.sources.StreamTableSource;
@@ -69,6 +70,7 @@ public class CatalogSchemaTable extends AbstractTable implements TemporalTable {
 	private final ObjectIdentifier tableIdentifier;
 	private final CatalogBaseTable catalogBaseTable;
 	private final FlinkStatistic statistic;
+	private final SqlExprToRexConverterFactory converterFactory;
 	private final boolean isStreamingMode;
 	private final boolean isTemporary;
 	private final Catalog catalog;
@@ -82,6 +84,9 @@ public class CatalogSchemaTable extends AbstractTable implements TemporalTable {
 	 * @param catalogBaseTable CatalogBaseTable instance which exists in the catalog
 	 * @param statistic Table statistics
 	 * @param catalog The catalog which the schema table belongs to
+	 * @param converterFactory The SQL expression converter factory is used to derive correct result
+	 *                         type of computed column, because the date type of computed column
+	 *                         from catalog table is not trusted.
 	 * @param isStreaming If the table is for streaming mode
 	 * @param isTemporary If the table is temporary
 	 */
@@ -90,12 +95,14 @@ public class CatalogSchemaTable extends AbstractTable implements TemporalTable {
 			CatalogBaseTable catalogBaseTable,
 			FlinkStatistic statistic,
 			Catalog catalog,
+			SqlExprToRexConverterFactory converterFactory,
 			boolean isStreaming,
 			boolean isTemporary) {
 		this.tableIdentifier = tableIdentifier;
 		this.catalogBaseTable = catalogBaseTable;
 		this.statistic = statistic;
 		this.catalog = catalog;
+		this.converterFactory = converterFactory;
 		this.isStreamingMode = isStreaming;
 		this.isTemporary = isTemporary;
 	}
@@ -179,10 +186,11 @@ public class CatalogSchemaTable extends AbstractTable implements TemporalTable {
 			}
 		}
 
-		return TableSourceUtil.getSourceRowType(flinkTypeFactory,
-			tableSchema,
-			scala.Option.empty(),
-			isStreamingMode);
+		return TableSourceUtil.getSourceRowTypeFromSchema(
+				converterFactory,
+				flinkTypeFactory,
+				tableSchema,
+				isStreamingMode);
 	}
 
 	@Override
diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/catalog/DatabaseCalciteSchema.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/catalog/DatabaseCalciteSchema.java
index d4f351e95c4..51102af5aa7 100644
--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/catalog/DatabaseCalciteSchema.java
+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/catalog/DatabaseCalciteSchema.java
@@ -32,6 +32,7 @@ import org.apache.flink.table.catalog.exceptions.TableNotExistException;
 import org.apache.flink.table.catalog.stats.CatalogColumnStatistics;
 import org.apache.flink.table.catalog.stats.CatalogTableStatistics;
 import org.apache.flink.table.plan.stats.TableStats;
+import org.apache.flink.table.planner.calcite.SqlExprToRexConverterFactory;
 import org.apache.flink.table.planner.plan.stats.FlinkStatistic;
 
 import org.apache.calcite.linq4j.tree.Expression;
@@ -55,13 +56,22 @@ class DatabaseCalciteSchema extends FlinkSchema {
 	private final String databaseName;
 	private final String catalogName;
 	private final CatalogManager catalogManager;
+	// The SQL expression converter factory is used to derive correct result type of computed column,
+	// because the date type of computed column from catalog table is not trusted.
+	private final SqlExprToRexConverterFactory converterFactory;
 	// Flag that tells if the current planner should work in a batch or streaming mode.
 	private final boolean isStreamingMode;
 
-	public DatabaseCalciteSchema(String databaseName, String catalogName, CatalogManager catalog, boolean isStreamingMode) {
+	public DatabaseCalciteSchema(
+			String databaseName,
+			String catalogName,
+			CatalogManager catalog,
+			SqlExprToRexConverterFactory converterFactory,
+			boolean isStreamingMode) {
 		this.databaseName = databaseName;
 		this.catalogName = catalogName;
 		this.catalogManager = catalog;
+		this.converterFactory = converterFactory;
 		this.isStreamingMode = isStreamingMode;
 	}
 
@@ -72,10 +82,12 @@ class DatabaseCalciteSchema extends FlinkSchema {
 			.map(result -> {
 				CatalogBaseTable table = result.getTable();
 				FlinkStatistic statistic = getStatistic(result.isTemporary(), table, identifier);
-				return new CatalogSchemaTable(identifier,
+				return new CatalogSchemaTable(
+					identifier,
 					table,
 					statistic,
 					catalogManager.getCatalog(catalogName).orElseThrow(IllegalStateException::new),
+					converterFactory,
 					isStreamingMode,
 					result.isTemporary());
 			})
diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/delegation/PlannerContext.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/delegation/PlannerContext.java
index f9be0cb8222..fde13a44825 100644
--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/delegation/PlannerContext.java
+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/delegation/PlannerContext.java
@@ -120,7 +120,7 @@ public class PlannerContext {
 		this.cluster = FlinkRelOptClusterFactory.create(planner, new RexBuilder(typeFactory));
 	}
 
-	private SqlExprToRexConverter createSqlExprToRexConverter(RelDataType rowType) {
+	public SqlExprToRexConverter createSqlExprToRexConverter(RelDataType rowType) {
 		return new SqlExprToRexConverterImpl(
 				checkNotNull(frameworkConfig),
 				checkNotNull(typeFactory),
diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/QueryOperationConverter.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/QueryOperationConverter.java
index fbb139c0af2..04dc531fd76 100644
--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/QueryOperationConverter.java
+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/QueryOperationConverter.java
@@ -462,9 +462,9 @@ public class QueryOperationConverter extends QueryOperationDefaultVisitor<RelNod
 				tableIdentifier = catalogManager.qualifyIdentifier(UnresolvedIdentifier.of(refId));
 			}
 
-			RelDataType rowType = TableSourceUtil.getSourceRowType(relBuilder.getTypeFactory(),
-				tableSourceOperation.getTableSchema(),
-				scala.Option.apply(tableSource),
+			RelDataType rowType = TableSourceUtil.getSourceRowTypeFromSource(
+				relBuilder.getTypeFactory(),
+				tableSource,
 				!isBatch);
 			LegacyTableSourceTable<?> tableSourceTable = new LegacyTableSourceTable<>(
 				relBuilder.getRelOptSchema(),
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/delegation/PlannerBase.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/delegation/PlannerBase.scala
index e6d820c8c3b..2fe00012741 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/delegation/PlannerBase.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/delegation/PlannerBase.scala
@@ -31,7 +31,7 @@ import org.apache.flink.table.factories.{FactoryUtil, TableFactoryUtil}
 import org.apache.flink.table.operations.OutputConversionModifyOperation.UpdateMode
 import org.apache.flink.table.operations._
 import org.apache.flink.table.planner.JMap
-import org.apache.flink.table.planner.calcite.{CalciteParser, FlinkPlannerImpl, FlinkRelBuilder, FlinkTypeFactory}
+import org.apache.flink.table.planner.calcite.{CalciteParser, FlinkPlannerImpl, FlinkRelBuilder, FlinkTypeFactory, SqlExprToRexConverter, SqlExprToRexConverterFactory}
 import org.apache.flink.table.planner.catalog.CatalogManagerCalciteSchema
 import org.apache.flink.table.planner.expressions.PlannerTypeInferenceUtilImpl
 import org.apache.flink.table.planner.hint.FlinkHints
@@ -51,6 +51,7 @@ import org.apache.flink.table.utils.TableSchemaUtils
 import org.apache.calcite.jdbc.CalciteSchemaBuilder.asRootSchema
 import org.apache.calcite.plan.{RelTrait, RelTraitDef}
 import org.apache.calcite.rel.RelNode
+import org.apache.calcite.rel.`type`.RelDataType
 import org.apache.calcite.tools.FrameworkConfig
 
 import java.util
@@ -83,13 +84,19 @@ abstract class PlannerBase(
   // temporary utility until we don't use planner expressions anymore
   functionCatalog.setPlannerTypeInferenceUtil(PlannerTypeInferenceUtilImpl.INSTANCE)
 
+  private val sqlExprToRexConverterFactory = new SqlExprToRexConverterFactory {
+    override def create(tableRowType: RelDataType): SqlExprToRexConverter =
+      plannerContext.createSqlExprToRexConverter(tableRowType)
+  }
+
   @VisibleForTesting
   private[flink] val plannerContext: PlannerContext =
     new PlannerContext(
       config,
       functionCatalog,
       catalogManager,
-      asRootSchema(new CatalogManagerCalciteSchema(catalogManager, isStreamingMode)),
+      asRootSchema(new CatalogManagerCalciteSchema(
+        catalogManager, sqlExprToRexConverterFactory, isStreamingMode)),
       getTraitDefs.toList
     )
 
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/sources/TableSourceUtil.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/sources/TableSourceUtil.scala
index e51a59db382..9d8880ebcfe 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/sources/TableSourceUtil.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/sources/TableSourceUtil.scala
@@ -18,12 +18,14 @@
 
 package org.apache.flink.table.planner.sources
 
-import org.apache.flink.table.api.{DataTypes, TableSchema, ValidationException, WatermarkSpec}
+import org.apache.flink.table.api.{DataTypes, TableColumn, TableSchema, ValidationException, WatermarkSpec}
 import org.apache.flink.table.expressions.ApiExpressionUtils.{typeLiteral, valueLiteral}
 import org.apache.flink.table.expressions.{CallExpression, Expression, ResolvedExpression, ResolvedFieldReference}
 import org.apache.flink.table.functions.BuiltInFunctionDefinitions
-import org.apache.flink.table.planner.calcite.FlinkTypeFactory
+import org.apache.flink.table.planner.calcite.{FlinkTypeFactory, SqlExprToRexConverterFactory}
 import org.apache.flink.table.planner.expressions.converter.ExpressionConverter
+import org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable
+import org.apache.flink.table.planner.utils.JavaScalaConversionUtil.toScala
 import org.apache.flink.table.runtime.types.DataTypePrecisionFixer
 import org.apache.flink.table.runtime.types.LogicalTypeDataTypeConverter.fromDataTypeToLogicalType
 import org.apache.flink.table.runtime.types.TypeInfoLogicalTypeConverter.fromTypeInfoToLogicalType
@@ -37,7 +39,7 @@ import org.apache.calcite.plan.RelOptCluster
 import org.apache.calcite.rel.RelNode
 import org.apache.calcite.rel.`type`.RelDataType
 import org.apache.calcite.rel.logical.LogicalValues
-import org.apache.calcite.rex.RexNode
+import org.apache.calcite.rex.{RexCall, RexNode}
 import org.apache.calcite.tools.RelBuilder
 
 import _root_.java.sql.Timestamp
@@ -117,90 +119,63 @@ object TableSourceUtil {
   }
 
   /**
-    * Returns schema of the selected fields of the given [[TableSource]].
-    *
-    * @param typeFactory    Type factory to create the type
-    * @param fieldNameArray Field names to build the row type
-    * @param fieldDataTypeArray Field data types to build the row type
-    * @param tableSource    The [[TableSource]] to derive time attributes
-    * @param streaming      Flag to determine whether the schema of
-    *                       a stream or batch table is created
-    * @return The schema for the selected fields of the given [[TableSource]]
-    */
-  def getSourceRowType(
-      typeFactory: FlinkTypeFactory,
-      fieldNameArray: Array[String],
-      fieldDataTypeArray: Array[DataType],
-      tableSource: TableSource[_],
-      streaming: Boolean): RelDataType = {
-
-    val fieldNames = fieldNameArray
-    var fieldTypes = fieldDataTypeArray
-      .map(fromDataTypeToLogicalType)
-
-    if (streaming) {
-      // adjust the type of time attributes for streaming tables
-      val rowtimeAttributes = getRowtimeAttributes(tableSource)
-      val proctimeAttributes = getProctimeAttribute(tableSource)
-
-      // patch rowtime fields with time indicator type
-      rowtimeAttributes.foreach { rowtimeField =>
-        val idx = fieldNames.indexOf(rowtimeField)
-        val rowtimeType = new TimestampType(
-          true, TimestampKind.ROWTIME, 3)
-        fieldTypes = fieldTypes.patch(idx, Seq(rowtimeType), 1)
-      }
-      // patch proctime field with time indicator type
-      proctimeAttributes.foreach { proctimeField =>
-        val idx = fieldNames.indexOf(proctimeField)
-        val proctimeType = new TimestampType(
-          true, TimestampKind.PROCTIME, 3)
-        fieldTypes = fieldTypes.patch(idx, Seq(proctimeType), 1)
-      }
-    }
-    typeFactory.buildRelNodeRowType(fieldNames, fieldTypes)
-  }
-
-  /**
-    * Returns schema of the selected fields of the given [[TableSource]].
+    * Returns schema of the selected fields of the given [[TableSchema]].
     *
+    * @param converterFactory converter to convert computed columns.
     * @param typeFactory Type factory to create the type
-    * @param fieldNameArray Field names to build the row type
-    * @param fieldDataTypeArray Field data types to build the row type
-    * @param watermarkSpec Watermark specifications defined through DDL
+    * @param tableSchema Table schema to derive table field names and data types
     * @param streaming Flag to determine whether the schema of a stream or batch table is created
-    * @return The row type for the selected fields of the given [[TableSource]], this type would
+    * @return The row type for the selected fields of the given [[TableSchema]], this type would
     *         also be patched with time attributes defined in the give [[WatermarkSpec]]
     */
-  def getSourceRowType(
+  def getSourceRowTypeFromSchema(
+      converterFactory: SqlExprToRexConverterFactory,
       typeFactory: FlinkTypeFactory,
-      fieldNameArray: Array[String],
-      fieldDataTypeArray: Array[DataType],
-      watermarkSpec: WatermarkSpec,
+      tableSchema: TableSchema,
       streaming: Boolean): RelDataType = {
+    val fieldNames = tableSchema.getFieldNames
+    var fieldTypes = tableSchema.getFieldDataTypes.map(fromDataTypeToLogicalType)
 
-    val fieldNames = fieldNameArray
-    var fieldTypes = fieldDataTypeArray
-      .map(fromDataTypeToLogicalType)
-
-    // patch rowtime field according to WatermarkSpec
-    fieldTypes = if (streaming) {
-      // TODO: [FLINK-14473] we only support top-level rowtime attribute right now
-      val rowtime = watermarkSpec.getRowtimeAttribute
+    // TODO: [FLINK-14473] we only support top-level rowtime attribute right now
+    val rowTimeIdx = if (tableSchema.getWatermarkSpecs.nonEmpty) {
+      val rowtime = tableSchema.getWatermarkSpecs.head.getRowtimeAttribute
       if (rowtime.contains(".")) {
         throw new ValidationException(
           s"Nested field '$rowtime' as rowtime attribute is not supported right now.")
       }
-      val idx = fieldNames.indexOf(rowtime)
-      val originalType = fieldTypes(idx).asInstanceOf[TimestampType]
-      val rowtimeType = new TimestampType(
-        originalType.isNullable,
-        TimestampKind.ROWTIME,
-        originalType.getPrecision)
-      fieldTypes.patch(idx, Seq(rowtimeType), 1)
+      Some(fieldNames.indexOf(rowtime))
     } else {
-      fieldTypes
+      None
+    }
+
+    val converter = converterFactory.create(
+      typeFactory.buildRelNodeRowType(fieldNames, fieldTypes))
+    def isProctime(column: TableColumn): Boolean = {
+      toScala(column.getExpr).exists { expr =>
+        converter.convertToRexNode(expr) match {
+          case call: RexCall => call.getOperator == FlinkSqlOperatorTable.PROCTIME
+          case _ => false
+        }
+      }
+    }
+
+    fieldTypes = fieldTypes.zipWithIndex.map {
+      case (originalType, i) =>
+        if (streaming && rowTimeIdx.exists(_.equals(i))) {
+          new TimestampType(
+            originalType.isNullable,
+            TimestampKind.ROWTIME,
+            originalType.asInstanceOf[TimestampType].getPrecision)
+        } else if (isProctime(tableSchema.getTableColumn(i).get())) {
+          new TimestampType(
+            originalType.isNullable,
+            TimestampKind.PROCTIME,
+            originalType.asInstanceOf[TimestampType].getPrecision)
+        } else {
+          originalType
+        }
     }
+
     typeFactory.buildRelNodeRowType(fieldNames, fieldTypes)
   }
 
@@ -211,31 +186,41 @@ object TableSourceUtil {
     * or [[TableSource]].
     *
     * @param typeFactory Type factory to create the type
-    * @param tableSchema Table schema to derive table field names and data types
     * @param tableSource Table source to derive watermark strategies
     * @param streaming Flag to determine whether the schema of a stream or batch table is created
     * @return The row type for the selected fields of the given [[TableSource]], this type would
     *         also be patched with time attributes defined in the give [[WatermarkSpec]]
     */
-  def getSourceRowType(
+  def getSourceRowTypeFromSource(
       typeFactory: FlinkTypeFactory,
-      tableSchema: TableSchema,
-      tableSource: Option[TableSource[_]],
+      tableSource: TableSource[_],
       streaming: Boolean): RelDataType = {
-
+    val tableSchema = tableSource.getTableSchema
     val fieldNames = tableSchema.getFieldNames
     val fieldDataTypes = tableSchema.getFieldDataTypes
+    var fieldTypes = fieldDataTypes.map(fromDataTypeToLogicalType)
 
-    if (tableSchema.getWatermarkSpecs.nonEmpty) {
-      getSourceRowType(typeFactory, fieldNames, fieldDataTypes, tableSchema.getWatermarkSpecs.head,
-        streaming)
-    } else if (tableSource.isDefined) {
-      getSourceRowType(typeFactory, fieldNames, fieldDataTypes, tableSource.get,
-        streaming)
-    } else {
-      val fieldTypes = fieldDataTypes.map(fromDataTypeToLogicalType)
-      typeFactory.buildRelNodeRowType(fieldNames, fieldTypes)
+    if (streaming) {
+      // adjust the type of time attributes for streaming tables
+      val rowtimeAttributes = getRowtimeAttributes(tableSource)
+      val proctimeAttributes = getProctimeAttribute(tableSource)
+
+      // patch rowtime fields with time indicator type
+      rowtimeAttributes.foreach { rowtimeField =>
+        val idx = fieldNames.indexOf(rowtimeField)
+        val rowtimeType = new TimestampType(
+          true, TimestampKind.ROWTIME, 3)
+        fieldTypes = fieldTypes.patch(idx, Seq(rowtimeType), 1)
+      }
+      // patch proctime field with time indicator type
+      proctimeAttributes.foreach { proctimeField =>
+        val idx = fieldNames.indexOf(proctimeField)
+        val proctimeType = new TimestampType(
+          true, TimestampKind.PROCTIME, 3)
+        fieldTypes = fieldTypes.patch(idx, Seq(proctimeType), 1)
+      }
     }
+    typeFactory.buildRelNodeRowType(fieldNames, fieldTypes)
   }
 
   /**
diff --git a/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/operations/SqlToOperationConverterTest.java b/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/operations/SqlToOperationConverterTest.java
index 8be154978b8..e9244ffb93e 100644
--- a/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/operations/SqlToOperationConverterTest.java
+++ b/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/operations/SqlToOperationConverterTest.java
@@ -56,6 +56,8 @@ import org.apache.flink.table.operations.ddl.CreateTableOperation;
 import org.apache.flink.table.operations.ddl.DropDatabaseOperation;
 import org.apache.flink.table.planner.calcite.CalciteParser;
 import org.apache.flink.table.planner.calcite.FlinkPlannerImpl;
+import org.apache.flink.table.planner.calcite.SqlExprToRexConverter;
+import org.apache.flink.table.planner.calcite.SqlExprToRexConverterFactory;
 import org.apache.flink.table.planner.catalog.CatalogManagerCalciteSchema;
 import org.apache.flink.table.planner.delegation.PlannerContext;
 import org.apache.flink.table.planner.expressions.utils.Func0$;
@@ -65,6 +67,7 @@ import org.apache.flink.table.planner.runtime.utils.JavaUserDefinedScalarFunctio
 import org.apache.flink.table.types.DataType;
 import org.apache.flink.table.utils.CatalogManagerMocks;
 
+import org.apache.calcite.rel.type.RelDataType;
 import org.apache.calcite.sql.SqlNode;
 import org.junit.After;
 import org.junit.Before;
@@ -110,13 +113,18 @@ public class SqlToOperationConverterTest {
 		tableConfig,
 		catalogManager,
 		moduleManager);
+	private SqlExprToRexConverterFactory sqlExprToRexConverterFactory = this::createSqlExprToRexConverter;
 	private final PlannerContext plannerContext =
 		new PlannerContext(tableConfig,
 			functionCatalog,
 			catalogManager,
-			asRootSchema(new CatalogManagerCalciteSchema(catalogManager, false)),
+			asRootSchema(new CatalogManagerCalciteSchema(catalogManager, sqlExprToRexConverterFactory, false)),
 			new ArrayList<>());
 
+	private SqlExprToRexConverter createSqlExprToRexConverter(RelDataType t) {
+		return plannerContext.createSqlExprToRexConverter(t);
+	}
+
 	@Rule
 	public ExpectedException thrown = ExpectedException.none();
 
diff --git a/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/FlinkCalciteCatalogReaderTest.java b/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/FlinkCalciteCatalogReaderTest.java
index 7c9337ba54c..296e7ce311f 100644
--- a/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/FlinkCalciteCatalogReaderTest.java
+++ b/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/FlinkCalciteCatalogReaderTest.java
@@ -23,6 +23,7 @@ import org.apache.flink.table.catalog.ConnectorCatalogTable;
 import org.apache.flink.table.catalog.ObjectIdentifier;
 import org.apache.flink.table.planner.calcite.FlinkTypeFactory;
 import org.apache.flink.table.planner.calcite.FlinkTypeSystem;
+import org.apache.flink.table.planner.calcite.SqlExprToRexConverter;
 import org.apache.flink.table.planner.catalog.CatalogSchemaTable;
 import org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase;
 import org.apache.flink.table.planner.plan.stats.FlinkStatistic;
@@ -33,6 +34,7 @@ import org.apache.calcite.config.CalciteConnectionProperty;
 import org.apache.calcite.jdbc.CalciteSchema;
 import org.apache.calcite.prepare.Prepare;
 import org.apache.calcite.rel.type.RelDataType;
+import org.apache.calcite.rex.RexNode;
 import org.apache.calcite.schema.SchemaPlus;
 import org.apache.calcite.schema.Table;
 import org.junit.Before;
@@ -79,6 +81,17 @@ public class FlinkCalciteCatalogReaderTest {
 				true),
 			FlinkStatistic.UNKNOWN(),
 			null,
+			tableRowType -> new SqlExprToRexConverter() {
+				@Override
+				public RexNode convertToRexNode(String expr) {
+					return null;
+				}
+
+				@Override
+				public RexNode[] convertToRexNodes(String[] exprs) {
+					return new RexNode[0];
+				}
+			},
 			true,
 			false);
 
diff --git a/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/utils/PlannerMocks.java b/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/utils/PlannerMocks.java
index f7e31a8901c..e3f15b13681 100644
--- a/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/utils/PlannerMocks.java
+++ b/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/utils/PlannerMocks.java
@@ -28,6 +28,7 @@ import org.apache.flink.table.planner.delegation.PlannerContext;
 import org.apache.flink.table.utils.CatalogManagerMocks;
 
 import java.util.ArrayList;
+import java.util.concurrent.atomic.AtomicReference;
 
 import static org.apache.calcite.jdbc.CalciteSchemaBuilder.asRootSchema;
 
@@ -43,12 +44,15 @@ public class PlannerMocks {
 			tableConfig,
 			catalogManager,
 			moduleManager);
+		AtomicReference<PlannerContext> reference = new AtomicReference<>();
 		PlannerContext plannerContext = new PlannerContext(
 			tableConfig,
 			functionCatalog,
 			catalogManager,
-			asRootSchema(new CatalogManagerCalciteSchema(catalogManager, false)),
+			asRootSchema(new CatalogManagerCalciteSchema(
+					catalogManager, t -> reference.get().createSqlExprToRexConverter(t), false)),
 			new ArrayList<>());
+		reference.set(plannerContext);
 		return plannerContext.createFlinkPlanner(
 			catalogManager.getCurrentCatalog(),
 			catalogManager.getCurrentDatabase());
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/codegen/WatermarkGeneratorCodeGenTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/codegen/WatermarkGeneratorCodeGenTest.scala
index 62b79b7c5da..95c6f34d721 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/codegen/WatermarkGeneratorCodeGenTest.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/codegen/WatermarkGeneratorCodeGenTest.scala
@@ -24,7 +24,7 @@ import org.apache.flink.table.api.TableConfig
 import org.apache.flink.table.catalog.{CatalogManager, FunctionCatalog, ObjectIdentifier}
 import org.apache.flink.table.data.{GenericRowData, TimestampData}
 import org.apache.flink.table.module.ModuleManager
-import org.apache.flink.table.planner.calcite.{FlinkContext, FlinkPlannerImpl, FlinkTypeFactory}
+import org.apache.flink.table.planner.calcite.{FlinkContext, FlinkPlannerImpl, FlinkTypeFactory, SqlExprToRexConverter, SqlExprToRexConverterFactory}
 import org.apache.flink.table.planner.catalog.CatalogManagerCalciteSchema
 import org.apache.flink.table.planner.delegation.PlannerContext
 import org.apache.flink.table.planner.runtime.utils.JavaUserDefinedScalarFunctions.JavaFunc5
@@ -34,7 +34,7 @@ import org.apache.flink.table.utils.CatalogManagerMocks
 
 import org.apache.calcite.jdbc.CalciteSchemaBuilder.asRootSchema
 import org.apache.calcite.plan.ConventionTraitDef
-
+import org.apache.calcite.rel.`type`.RelDataType
 import org.junit.Assert.{assertEquals, assertTrue}
 import org.junit.Test
 
@@ -50,11 +50,16 @@ class WatermarkGeneratorCodeGenTest {
   val config = new TableConfig
   val catalogManager: CatalogManager = CatalogManagerMocks.createEmptyCatalogManager()
   val functionCatalog = new FunctionCatalog(config, catalogManager, new ModuleManager)
+  private val sqlExprToRexConverterFactory = new SqlExprToRexConverterFactory {
+    override def create(tableRowType: RelDataType): SqlExprToRexConverter =
+      createSqlExprToRexConverter(tableRowType)
+  }
   val plannerContext = new PlannerContext(
     config,
     functionCatalog,
     catalogManager,
-    asRootSchema(new CatalogManagerCalciteSchema(catalogManager, false)),
+    asRootSchema(new CatalogManagerCalciteSchema(
+      catalogManager, sqlExprToRexConverterFactory, false)),
     Collections.singletonList(ConventionTraitDef.INSTANCE))
   val planner: FlinkPlannerImpl = plannerContext.createFlinkPlanner(
     catalogManager.getCurrentCatalog,
@@ -69,6 +74,9 @@ class WatermarkGeneratorCodeGenTest {
     GenericRowData.of(TimestampData.fromEpochMillis(6000L), JInt.valueOf(8))
   )
 
+  private def createSqlExprToRexConverter(tableRowType: RelDataType): SqlExprToRexConverter =
+    plannerContext.createSqlExprToRexConverter(tableRowType)
+
   @Test
   def testAscendingWatermark(): Unit = {
     val generator = generateWatermarkGenerator("ts - INTERVAL '0.001' SECOND")
