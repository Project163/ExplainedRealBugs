diff --git a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/codegen/LongHashJoinGenerator.scala b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/codegen/LongHashJoinGenerator.scala
index abea1abce62..9706a3aee55 100644
--- a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/codegen/LongHashJoinGenerator.scala
+++ b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/codegen/LongHashJoinGenerator.scala
@@ -319,6 +319,7 @@ object LongHashJoinGenerator {
     ctx.addReusableMember(s"""
                              |private void fallbackSMJProcessPartition() throws Exception {
                              |  if(!table.getPartitionsPendingForSMJ().isEmpty()) {
+                             |    table.releaseMemoryCacheForSMJ();
                              |    LOG.info(
                              |    "Fallback to sort merge join to process spilled partitions.");
                              |    initialSortMergeJoinFunction();
@@ -353,6 +354,7 @@ object LongHashJoinGenerator {
     ctx.addReusableMember(s"""
                              |private void initialSortMergeJoinFunction() throws Exception {
                              |  sortMergeJoinFunction.open(
+                             |    true,
                              |    this.getContainingTask(),
                              |    this.getOperatorConfig(),
                              |    new $collector<$ROW_DATA>(output),
diff --git a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/runtime/batch/sql/join/AdaptiveHashJoinITCase.java b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/runtime/batch/sql/join/AdaptiveHashJoinITCase.java
new file mode 100644
index 00000000000..6f975c7fb5d
--- /dev/null
+++ b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/runtime/batch/sql/join/AdaptiveHashJoinITCase.java
@@ -0,0 +1,186 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.planner.runtime.batch.sql.join;
+
+import org.apache.flink.api.common.BatchShuffleMode;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.configuration.ExecutionOptions;
+import org.apache.flink.configuration.MemorySize;
+import org.apache.flink.configuration.TaskManagerOptions;
+import org.apache.flink.runtime.testutils.MiniClusterResourceConfiguration;
+import org.apache.flink.table.api.EnvironmentSettings;
+import org.apache.flink.table.api.TableConfig;
+import org.apache.flink.table.api.TableEnvironment;
+import org.apache.flink.table.api.config.ExecutionConfigOptions;
+import org.apache.flink.table.planner.factories.TestValuesTableFactory;
+import org.apache.flink.table.planner.utils.TestingTableEnvironment;
+import org.apache.flink.test.util.MiniClusterWithClientResource;
+import org.apache.flink.types.Row;
+import org.apache.flink.util.TestLogger;
+
+import org.junit.After;
+import org.junit.Before;
+import org.junit.ClassRule;
+import org.junit.Test;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.concurrent.TimeUnit;
+
+import static org.assertj.core.api.Assertions.assertThat;
+
+/** Test for adaptive hash join. */
+public class AdaptiveHashJoinITCase extends TestLogger {
+
+    public static final int DEFAULT_PARALLELISM = 3;
+
+    @ClassRule
+    public static MiniClusterWithClientResource miniClusterResource =
+            new MiniClusterWithClientResource(
+                    new MiniClusterResourceConfiguration.Builder()
+                            .setConfiguration(getConfiguration())
+                            .setNumberTaskManagers(1)
+                            .setNumberSlotsPerTaskManager(DEFAULT_PARALLELISM)
+                            .build());
+
+    private static Configuration getConfiguration() {
+        Configuration config = new Configuration();
+        config.set(TaskManagerOptions.MANAGED_MEMORY_SIZE, MemorySize.parse("6m"));
+        return config;
+    }
+
+    private final TableEnvironment tEnv =
+            TestingTableEnvironment.create(
+                    EnvironmentSettings.newInstance().inBatchMode().build(),
+                    null,
+                    TableConfig.getDefault());
+
+    @Before
+    public void before() throws Exception {
+        tEnv.getConfig()
+                .getConfiguration()
+                .set(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM, 1);
+        tEnv.getConfig()
+                .set(ExecutionOptions.BATCH_SHUFFLE_MODE, BatchShuffleMode.ALL_EXCHANGES_PIPELINED);
+
+        JoinITCaseHelper.disableOtherJoinOpForJoin(tEnv, JoinType.HashJoin());
+
+        // prepare data
+        List<Row> data1 = new ArrayList<>();
+        data1.addAll(getRepeatedRow(2, 100000));
+        data1.addAll(getRepeatedRow(5, 100000));
+        data1.addAll(getRepeatedRow(10, 100000));
+        String dataId1 = TestValuesTableFactory.registerData(data1);
+
+        List<Row> data2 = new ArrayList<>();
+        data2.addAll(getRepeatedRow(5, 10));
+        data2.addAll(getRepeatedRow(10, 10));
+        data2.addAll(getRepeatedRow(20, 10));
+        String dataId2 = TestValuesTableFactory.registerData(data2);
+
+        tEnv.executeSql(
+                String.format(
+                        "CREATE TABLE t1 (\n"
+                                + "  x INT,\n"
+                                + "  y BIGINT,\n"
+                                + "  z VARCHAR\n"
+                                + ")  WITH (\n"
+                                + " 'connector' = 'values',\n"
+                                + " 'data-id' = '%s',\n"
+                                + " 'bounded' = 'true'\n"
+                                + ")",
+                        dataId1));
+
+        tEnv.executeSql(
+                String.format(
+                        "CREATE TABLE t2 (\n"
+                                + "  a INT,\n"
+                                + "  b BIGINT,\n"
+                                + "  c VARCHAR\n"
+                                + ")  WITH (\n"
+                                + " 'connector' = 'values',\n"
+                                + " 'data-id' = '%s',\n"
+                                + " 'bounded' = 'true'\n"
+                                + ")",
+                        dataId2));
+
+        tEnv.executeSql(
+                "CREATE TABLE sink (\n"
+                        + "  x INT,\n"
+                        + "  z VARCHAR,\n"
+                        + "  a INT,\n"
+                        + "  b BIGINT,\n"
+                        + "  c VARCHAR\n"
+                        + ")  WITH (\n"
+                        + " 'connector' = 'values',\n"
+                        + " 'bounded' = 'true'\n"
+                        + ")");
+    }
+
+    @After
+    public void after() {
+        TestValuesTableFactory.clearAllData();
+    }
+
+    @Test
+    public void testBuildLeftIntKeyAdaptiveHashJoin() throws Exception {
+        tEnv.executeSql("INSERT INTO sink SELECT x, z, a, b, c FROM t1 JOIN t2 ON t1.x=t2.a")
+                .await(60, TimeUnit.SECONDS);
+
+        asserResult("sink", 2000000);
+    }
+
+    @Test
+    public void testBuildRightIntKeyAdaptiveHashJoin() throws Exception {
+        tEnv.executeSql("INSERT INTO sink SELECT x, z, a, b, c FROM t2 JOIN t1 ON t1.x=t2.a")
+                .await(60, TimeUnit.SECONDS);
+
+        asserResult("sink", 2000000);
+    }
+
+    @Test
+    public void testBuildLeftStringKeyAdaptiveHashJoin() throws Exception {
+        tEnv.executeSql("INSERT INTO sink SELECT x, z, a, b, c FROM t1 JOIN t2 ON t1.z=t2.c")
+                .await(60, TimeUnit.SECONDS);
+
+        asserResult("sink", 2000000);
+    }
+
+    @Test
+    public void testBuildRightStringKeyAdaptiveHashJoin() throws Exception {
+        tEnv.executeSql("INSERT INTO sink SELECT x, z, a, b, c FROM t2 JOIN t1 ON t1.z=t2.c")
+                .await(60, TimeUnit.SECONDS);
+
+        asserResult("sink", 2000000);
+    }
+
+    private void asserResult(String sinkTableName, int resultSize) {
+        // Due to concern OOM and record value is same, here just assert result size
+        List<String> result = TestValuesTableFactory.getResults(sinkTableName);
+        assertThat(result.size()).isEqualTo(resultSize);
+    }
+
+    private List<Row> getRepeatedRow(int key, int nums) {
+        List<Row> rows = new ArrayList<>();
+        for (int i = 0; i < nums; i++) {
+            rows.add(Row.of(key, (long) key, String.valueOf(key)));
+        }
+        return rows;
+    }
+}
diff --git a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/hashtable/BaseHybridHashTable.java b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/hashtable/BaseHybridHashTable.java
index f7e691057c7..72dcc28267d 100644
--- a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/hashtable/BaseHybridHashTable.java
+++ b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/hashtable/BaseHybridHashTable.java
@@ -264,18 +264,22 @@ public abstract class BaseHybridHashTable implements MemorySegmentPool {
             this.buildSpillRetBufferNumbers--;
 
             // grab as many more buffers as are available directly
-            MemorySegment currBuff;
-            while (this.buildSpillRetBufferNumbers > 0
-                    && (currBuff = this.buildSpillReturnBuffers.poll()) != null) {
-                returnPage(currBuff);
-                this.buildSpillRetBufferNumbers--;
-            }
+            returnSpillBuffers();
             return toReturn;
         } else {
             return null;
         }
     }
 
+    private void returnSpillBuffers() {
+        MemorySegment currBuff;
+        while (this.buildSpillRetBufferNumbers > 0
+                && (currBuff = this.buildSpillReturnBuffers.poll()) != null) {
+            returnPage(currBuff);
+            this.buildSpillRetBufferNumbers--;
+        }
+    }
+
     /** Bulk memory acquisition. NOTE: Failure to get memory will throw an exception. */
     public MemorySegment[] getNextBuffers(int bufferSize) {
         MemorySegment[] memorySegments = new MemorySegment[bufferSize];
@@ -439,6 +443,19 @@ public abstract class BaseHybridHashTable implements MemorySegmentPool {
         internalPool.cleanCache();
     }
 
+    /**
+     * Due to adaptive hash join is introduced, the cached memory segments should be released to
+     * {@link MemoryManager} before switch to sort merge join. Otherwise, open sort merge join
+     * operator maybe fail because of insufficient memory.
+     *
+     * <p>Note: this method should only be invoked for sort merge join.
+     */
+    public void releaseMemoryCacheForSMJ() {
+        // return build spill buffer memory first
+        returnSpillBuffers();
+        freeCurrent();
+    }
+
     LazyMemorySegmentPool getInternalPool() {
         return this.internalPool;
     }
diff --git a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/hashtable/BinaryHashBucketArea.java b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/hashtable/BinaryHashBucketArea.java
index d5166de20ad..aa9e6a27ede 100644
--- a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/hashtable/BinaryHashBucketArea.java
+++ b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/hashtable/BinaryHashBucketArea.java
@@ -508,7 +508,7 @@ public class BinaryHashBucketArea {
         int posInSegment = bucketInSegmentOffset + BUCKET_HEADER_LENGTH;
         int countInBucket = bucket.getShort(bucketInSegmentOffset + HEADER_COUNT_OFFSET);
         int numInBucket = 0;
-        RandomAccessInputView view = partition.getBuildStateInputView();
+        RandomAccessInputView view = partition.getBuildStageInputView();
         while (countInBucket != 0) {
             while (numInBucket < countInBucket) {
 
diff --git a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/hashtable/BinaryHashPartition.java b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/hashtable/BinaryHashPartition.java
index eaea42361f1..a91ef265718 100644
--- a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/hashtable/BinaryHashPartition.java
+++ b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/hashtable/BinaryHashPartition.java
@@ -215,7 +215,7 @@ public class BinaryHashPartition extends AbstractPagedInputView implements Seeka
                 : this.partitionBuffers.length;
     }
 
-    RandomAccessInputView getBuildStateInputView() {
+    RandomAccessInputView getBuildStageInputView() {
         return this.buildSideWriteBuffer.getBuildStageInputView();
     }
 
diff --git a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/join/HashJoinOperator.java b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/join/HashJoinOperator.java
index 59b522a4521..65183a07f03 100644
--- a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/join/HashJoinOperator.java
+++ b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/join/HashJoinOperator.java
@@ -259,6 +259,8 @@ public abstract class HashJoinOperator extends TableStreamOperator<RowData>
      */
     private void fallbackSMJProcessPartition() throws Exception {
         if (!table.getPartitionsPendingForSMJ().isEmpty()) {
+            // release memory to MemoryManager first that is used to sort merge join operator
+            table.releaseMemoryCacheForSMJ();
             // initialize sort merge join operator
             LOG.info("Fallback to sort merge join to process spilled partitions.");
             initialSortMergeJoinFunction();
@@ -292,6 +294,7 @@ public abstract class HashJoinOperator extends TableStreamOperator<RowData>
 
     private void initialSortMergeJoinFunction() throws Exception {
         sortMergeJoinFunction.open(
+                true,
                 this.getContainingTask(),
                 this.getOperatorConfig(),
                 (StreamRecordCollector) this.collector,
diff --git a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/join/SortMergeJoinFunction.java b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/join/SortMergeJoinFunction.java
index 79a8fc19c56..616f9ebb71c 100644
--- a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/join/SortMergeJoinFunction.java
+++ b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/join/SortMergeJoinFunction.java
@@ -117,6 +117,7 @@ public class SortMergeJoinFunction implements Serializable {
     }
 
     public void open(
+            boolean adaptiveHashJoin,
             StreamTask<?, ?> taskContainer,
             StreamConfig operatorConfig,
             StreamRecordCollector collector,
@@ -132,11 +133,11 @@ public class SortMergeJoinFunction implements Serializable {
 
         ClassLoader cl = taskContainer.getUserCodeClassLoader();
         AbstractRowDataSerializer inputSerializer1 =
-                (AbstractRowDataSerializer) operatorConfig.getTypeSerializerIn1(cl);
+                getInputSerializer1(adaptiveHashJoin, leftIsSmaller, cl, operatorConfig);
         this.serializer1 = new BinaryRowDataSerializer(inputSerializer1.getArity());
 
         AbstractRowDataSerializer inputSerializer2 =
-                (AbstractRowDataSerializer) operatorConfig.getTypeSerializerIn2(cl);
+                getInputSerializer2(adaptiveHashJoin, leftIsSmaller, cl, operatorConfig);
         this.serializer2 = new BinaryRowDataSerializer(inputSerializer2.getArity());
 
         this.memManager = taskContainer.getEnvironment().getMemoryManager();
@@ -222,6 +223,30 @@ public class SortMergeJoinFunction implements Serializable {
                 (Gauge<Long>) () -> sorter1.getSpillInBytes() + sorter2.getSpillInBytes());
     }
 
+    private AbstractRowDataSerializer getInputSerializer1(
+            boolean adaptiveHashJoin,
+            boolean leftIsSmaller,
+            ClassLoader cl,
+            StreamConfig operatorConfig) {
+        if (adaptiveHashJoin && !leftIsSmaller) {
+            return (AbstractRowDataSerializer) operatorConfig.getTypeSerializerIn2(cl);
+        }
+
+        return (AbstractRowDataSerializer) operatorConfig.getTypeSerializerIn1(cl);
+    }
+
+    private AbstractRowDataSerializer getInputSerializer2(
+            boolean adaptiveHashJoin,
+            boolean leftIsSmaller,
+            ClassLoader cl,
+            StreamConfig operatorConfig) {
+        if (adaptiveHashJoin && !leftIsSmaller) {
+            return (AbstractRowDataSerializer) operatorConfig.getTypeSerializerIn1(cl);
+        }
+
+        return (AbstractRowDataSerializer) operatorConfig.getTypeSerializerIn2(cl);
+    }
+
     public void processElement1(RowData element) throws Exception {
         this.sorter1.write(element);
     }
diff --git a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/join/SortMergeJoinOperator.java b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/join/SortMergeJoinOperator.java
index 61c452a0f79..4cfabbfcff3 100644
--- a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/join/SortMergeJoinOperator.java
+++ b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/join/SortMergeJoinOperator.java
@@ -48,6 +48,7 @@ public class SortMergeJoinOperator extends TableStreamOperator<RowData>
 
         // initialize sort merge join function
         this.sortMergeJoinFunction.open(
+                false,
                 this.getContainingTask(),
                 this.getOperatorConfig(),
                 new StreamRecordCollector(output),
