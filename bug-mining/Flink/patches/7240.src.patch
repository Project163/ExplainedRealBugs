diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/connectors/DynamicSinkUtils.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/connectors/DynamicSinkUtils.java
index bdade68090e..f8eca5b2113 100644
--- a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/connectors/DynamicSinkUtils.java
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/connectors/DynamicSinkUtils.java
@@ -804,8 +804,10 @@ public final class DynamicSinkUtils {
             project.replaceInput(0, newTableScan);
         }
         // validate and apply metadata
+        // TODO FLINK-33083 we should not ignore the produced abilities but actually put those into
+        //  the table scan
         DynamicSourceUtils.validateAndApplyMetadata(
-                tableDebugName, resolvedSchema, newTableSourceTab.tableSource());
+                tableDebugName, resolvedSchema, newTableSourceTab.tableSource(), new ArrayList<>());
         return resolvedSchema;
     }
 
diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/connectors/DynamicSourceUtils.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/connectors/DynamicSourceUtils.java
index 1cc7da7bbf3..925d1a00b11 100644
--- a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/connectors/DynamicSourceUtils.java
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/connectors/DynamicSourceUtils.java
@@ -41,6 +41,7 @@ import org.apache.flink.table.connector.source.abilities.SupportsReadingMetadata
 import org.apache.flink.table.connector.source.abilities.SupportsRowLevelModificationScan;
 import org.apache.flink.table.planner.calcite.FlinkRelBuilder;
 import org.apache.flink.table.planner.expressions.converter.ExpressionConverter;
+import org.apache.flink.table.planner.plan.abilities.source.ReadingMetadataSpec;
 import org.apache.flink.table.planner.plan.abilities.source.SourceAbilitySpec;
 import org.apache.flink.table.planner.plan.schema.TableSourceTable;
 import org.apache.flink.table.planner.plan.stats.FlinkStatistic;
@@ -61,6 +62,7 @@ import org.apache.calcite.rel.type.RelDataType;
 import org.apache.calcite.rex.RexBuilder;
 import org.apache.calcite.rex.RexNode;
 
+import java.util.ArrayList;
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
@@ -121,12 +123,25 @@ public final class DynamicSourceUtils {
         final String tableDebugName = contextResolvedTable.getIdentifier().asSummaryString();
         final ResolvedCatalogTable resolvedCatalogTable = contextResolvedTable.getResolvedTable();
 
+        final List<SourceAbilitySpec> sourceAbilities = new ArrayList<>();
         // 1. prepare table source
         prepareDynamicSource(
-                tableDebugName, resolvedCatalogTable, tableSource, isBatchMode, config);
+                tableDebugName,
+                resolvedCatalogTable,
+                tableSource,
+                isBatchMode,
+                config,
+                sourceAbilities);
 
         // 2. push table scan
-        pushTableScan(isBatchMode, relBuilder, contextResolvedTable, statistic, hints, tableSource);
+        pushTableScan(
+                isBatchMode,
+                relBuilder,
+                contextResolvedTable,
+                statistic,
+                hints,
+                tableSource,
+                sourceAbilities);
 
         // 3. push project for non-physical columns
         final ResolvedSchema schema = contextResolvedTable.getResolvedSchema();
@@ -152,10 +167,11 @@ public final class DynamicSourceUtils {
             ResolvedCatalogTable table,
             DynamicTableSource source,
             boolean isBatchMode,
-            ReadableConfig config) {
+            ReadableConfig config,
+            List<SourceAbilitySpec> sourceAbilities) {
         final ResolvedSchema schema = table.getResolvedSchema();
 
-        validateAndApplyMetadata(tableDebugName, schema, source);
+        validateAndApplyMetadata(tableDebugName, schema, source, sourceAbilities);
 
         if (source instanceof ScanTableSource) {
             validateScanSource(
@@ -174,7 +190,7 @@ public final class DynamicSourceUtils {
      *
      * <p>This method assumes that source and schema have been validated via {@link
      * #prepareDynamicSource(String, ResolvedCatalogTable, DynamicTableSource, boolean,
-     * ReadableConfig)}.
+     * ReadableConfig, List)}.
      */
     public static List<MetadataColumn> createRequiredMetadataColumns(
             ResolvedSchema schema, DynamicTableSource source) {
@@ -357,7 +373,8 @@ public final class DynamicSourceUtils {
             ContextResolvedTable contextResolvedTable,
             FlinkStatistic statistic,
             List<RelHint> hints,
-            DynamicTableSource tableSource) {
+            DynamicTableSource tableSource,
+            List<SourceAbilitySpec> sourceAbilities) {
         final RowType producedType =
                 createProducedType(contextResolvedTable.getResolvedSchema(), tableSource);
         final RelDataType producedRelDataType =
@@ -373,7 +390,7 @@ public final class DynamicSourceUtils {
                         contextResolvedTable,
                         ShortcutUtils.unwrapContext(relBuilder),
                         ShortcutUtils.unwrapTypeFactory(relBuilder),
-                        new SourceAbilitySpec[0]);
+                        sourceAbilities.toArray(new SourceAbilitySpec[0]));
 
         final LogicalTableScan scan =
                 LogicalTableScan.create(relBuilder.getCluster(), tableSourceTable, hints);
@@ -395,7 +412,10 @@ public final class DynamicSourceUtils {
     }
 
     public static void validateAndApplyMetadata(
-            String tableDebugName, ResolvedSchema schema, DynamicTableSource source) {
+            String tableDebugName,
+            ResolvedSchema schema,
+            DynamicTableSource source,
+            List<SourceAbilitySpec> sourceAbilities) {
         final List<MetadataColumn> metadataColumns = extractMetadataColumns(schema);
 
         if (metadataColumns.isEmpty()) {
@@ -461,11 +481,15 @@ public final class DynamicSourceUtils {
                     }
                 });
 
-        metadataSource.applyReadableMetadata(
+        final List<String> metadataKeys =
                 createRequiredMetadataColumns(schema, source).stream()
                         .map(column -> column.getMetadataKey().orElse(column.getName()))
-                        .collect(Collectors.toList()),
-                TypeConversions.fromLogicalToDataType(createProducedType(schema, source)));
+                        .collect(Collectors.toList());
+        final DataType producedDataType =
+                TypeConversions.fromLogicalToDataType(createProducedType(schema, source));
+        sourceAbilities.add(
+                new ReadingMetadataSpec(metadataKeys, (RowType) producedDataType.getLogicalType()));
+        metadataSource.applyReadableMetadata(metadataKeys, producedDataType);
     }
 
     private static void validateScanSource(
diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/spec/DynamicTableSourceSpec.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/spec/DynamicTableSourceSpec.java
index 707e16bf580..21d0e313541 100644
--- a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/spec/DynamicTableSourceSpec.java
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/spec/DynamicTableSourceSpec.java
@@ -103,7 +103,7 @@ public class DynamicTableSourceSpec extends DynamicTableSpecBase {
                         (RowType)
                                 contextResolvedTable
                                         .getResolvedSchema()
-                                        .toSourceRowDataType()
+                                        .toPhysicalRowDataType()
                                         .getLogicalType();
                 for (SourceAbilitySpec spec : sourceAbilities) {
                     SourceAbilityContext sourceAbilityContext =
diff --git a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/schema/TableSourceTable.scala b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/schema/TableSourceTable.scala
index 913b598cf42..5fbf512c7a8 100644
--- a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/schema/TableSourceTable.scala
+++ b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/schema/TableSourceTable.scala
@@ -93,6 +93,13 @@ class TableSourceTable(
     builder.build()
   }
 
+  /** Adds the newSpec replacing any spec of the same class from existing ones. */
+  private def mergeSpecs(
+      original: Array[SourceAbilitySpec],
+      newSpec: Array[SourceAbilitySpec]): Array[SourceAbilitySpec] = {
+    original.filter(old => !newSpec.exists(n => old.getClass == n.getClass)) ++ newSpec
+  }
+
   /**
    * Creates a copy of this table with specified digest.
    *
@@ -116,7 +123,7 @@ class TableSourceTable(
       contextResolvedTable,
       flinkContext,
       flinkTypeFactory,
-      abilitySpecs ++ newAbilitySpecs
+      mergeSpecs(abilitySpecs, newAbilitySpecs)
     )
   }
 
@@ -146,7 +153,7 @@ class TableSourceTable(
       newResolveTable,
       flinkContext,
       flinkTypeFactory,
-      abilitySpecs ++ newAbilitySpecs
+      mergeSpecs(abilitySpecs, newAbilitySpecs)
     )
   }
 
@@ -198,7 +205,7 @@ class TableSourceTable(
       contextResolvedTable,
       flinkContext,
       flinkTypeFactory,
-      abilitySpecs ++ newAbilitySpecs)
+      mergeSpecs(abilitySpecs, newAbilitySpecs))
   }
 
   /**
diff --git a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/runtime/stream/jsonplan/TableSourceJsonPlanITCase.java b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/runtime/stream/jsonplan/TableSourceJsonPlanITCase.java
index 7bb82075ded..6a24ee079a5 100644
--- a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/runtime/stream/jsonplan/TableSourceJsonPlanITCase.java
+++ b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/runtime/stream/jsonplan/TableSourceJsonPlanITCase.java
@@ -64,6 +64,26 @@ public class TableSourceJsonPlanITCase extends JsonPlanTestBase {
         assertResult(Arrays.asList("1,Hi", "2,Hello", "3,Hello world"), sinkPath);
     }
 
+    @Test
+    public void testReadingMetadataWithProjectionPushDownDisabled() throws Exception {
+        createTestValuesSourceTable(
+                "MyTable",
+                JavaScalaConversionUtil.toJava(TestData.smallData3()),
+                new String[] {"a int", "b bigint", "m varchar metadata"},
+                new HashMap<String, String>() {
+                    {
+                        put("readable-metadata", "m:STRING");
+                        put("enable-projection-push-down", "false");
+                    }
+                });
+
+        File sinkPath = createTestCsvSinkTable("MySink", "a int", "m varchar");
+
+        compileSqlAndExecutePlan("insert into MySink select a, m from MyTable").await();
+
+        assertResult(Arrays.asList("1,Hi", "2,Hello", "3,Hello world"), sinkPath);
+    }
+
     @Test
     public void testFilterPushDown() throws Exception {
         List<String> data = Arrays.asList("1,1,hi", "2,1,hello", "3,2,hello world");
