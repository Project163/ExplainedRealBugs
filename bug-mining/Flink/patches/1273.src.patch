diff --git a/flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileMonitoringFunctionITCase.java b/flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingITCase.java
similarity index 53%
rename from flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileMonitoringFunctionITCase.java
rename to flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingITCase.java
index 079bf04b11e..3211a208a36 100644
--- a/flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileMonitoringFunctionITCase.java
+++ b/flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingITCase.java
@@ -19,9 +19,9 @@ package org.apache.flink.hdfstests;
 
 import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.api.java.io.TextInputFormat;
+import org.apache.flink.api.java.tuple.Tuple2;
 import org.apache.flink.api.java.typeutils.TypeExtractor;
 import org.apache.flink.configuration.Configuration;
-import org.apache.flink.core.fs.FileInputSplit;
 import org.apache.flink.core.fs.Path;
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
@@ -29,6 +29,7 @@ import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;
 import org.apache.flink.api.common.io.FilePathFilter;
 import org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction;
 import org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator;
+import org.apache.flink.streaming.api.functions.source.TimestampedFileInputSplit;
 import org.apache.flink.streaming.api.functions.source.FileProcessingMode;
 import org.apache.flink.streaming.util.StreamingProgramTestBase;
 import org.apache.hadoop.fs.FSDataOutputStream;
@@ -44,16 +45,19 @@ import java.util.ArrayList;
 import java.util.Collections;
 import java.util.Comparator;
 import java.util.HashMap;
+import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
+import java.util.Set;
 
 import static org.junit.Assert.assertEquals;
 
-public class ContinuousFileMonitoringFunctionITCase extends StreamingProgramTestBase {
+public class ContinuousFileProcessingITCase extends StreamingProgramTestBase {
 
-	private static final int NO_OF_FILES = 10;
-	private static final int LINES_PER_FILE = 10;
+	private static final int NO_OF_FILES = 5;
+	private static final int LINES_PER_FILE = 100;
 
+	private static final int PARALLELISM = 4;
 	private static final long INTERVAL = 100;
 
 	private File baseDir;
@@ -111,174 +115,189 @@ public class ContinuousFileMonitoringFunctionITCase extends StreamingProgramTest
 		* reader.
 		* */
 
-		FileCreator fileCreator = new FileCreator(INTERVAL);
-		Thread t = new Thread(fileCreator);
-		t.start();
-
 		TextInputFormat format = new TextInputFormat(new Path(hdfsURI));
 		format.setFilePath(hdfsURI);
-
-		try {
-			StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
-			env.setParallelism(4);
-
-			format.setFilesFilter(FilePathFilter.createDefaultFilter());
-			ContinuousFileMonitoringFunction<String> monitoringFunction =
-				new ContinuousFileMonitoringFunction<>(format, hdfsURI,
-					FileProcessingMode.PROCESS_CONTINUOUSLY,
-					env.getParallelism(), INTERVAL);
-
-			TypeInformation<String> typeInfo = TypeExtractor.getInputFormatTypes(format);
-			ContinuousFileReaderOperator<String, ?> reader = new ContinuousFileReaderOperator<>(format);
-			TestingSinkFunction sink = new TestingSinkFunction();
-
-			DataStream<FileInputSplit> splits = env.addSource(monitoringFunction);
-			splits.transform("FileSplitReader", typeInfo, reader).addSink(sink).setParallelism(1);
-			env.execute();
-
-		} catch (Exception e) {
-			Throwable th = e;
-			int depth = 0;
-
-			for (; depth < 20; depth++) {
-				if (th instanceof SuccessException) {
-					try {
-						postSubmit();
-					} catch (Exception e1) {
-						e1.printStackTrace();
+		format.setFilesFilter(FilePathFilter.createDefaultFilter());
+
+		// create the stream execution environment with a parallelism > 1 to test
+		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
+		env.setParallelism(PARALLELISM);
+
+		ContinuousFileMonitoringFunction<String> monitoringFunction =
+			new ContinuousFileMonitoringFunction<>(format, hdfsURI,
+				FileProcessingMode.PROCESS_CONTINUOUSLY,
+				env.getParallelism(), INTERVAL);
+
+		// the monitor has always DOP 1
+		DataStream<TimestampedFileInputSplit> splits = env.addSource(monitoringFunction);
+		Assert.assertEquals(1, splits.getParallelism());
+
+		ContinuousFileReaderOperator<String> reader = new ContinuousFileReaderOperator<>(format);
+		TypeInformation<String> typeInfo = TypeExtractor.getInputFormatTypes(format);
+
+		// the readers can be multiple
+		DataStream<String> content = splits.transform("FileSplitReader", typeInfo, reader);
+		Assert.assertEquals(PARALLELISM, content.getParallelism());
+
+		// finally for the sink we set the parallelism to 1 so that we can verify the output
+		TestingSinkFunction sink = new TestingSinkFunction();
+		content.addSink(sink).setParallelism(1);
+
+		Thread job = new Thread() {
+
+			@Override
+			public void run() {
+				try {
+					env.execute("ContinuousFileProcessingITCase Job.");
+				} catch (Exception e) {
+					Throwable th = e;
+					for (int depth = 0; depth < 20; depth++) {
+						if (th instanceof SuccessException) {
+							try {
+								postSubmit();
+							} catch (Exception e1) {
+								e1.printStackTrace();
+							}
+							return;
+						} else if (th.getCause() != null) {
+							th = th.getCause();
+						} else {
+							break;
+						}
 					}
-					return;
-				} else if (th.getCause() != null) {
-					th = th.getCause();
-				} else {
-					break;
+					e.printStackTrace();
+					Assert.fail(e.getMessage());
 				}
 			}
-			e.printStackTrace();
-			Assert.fail(e.getMessage());
+		};
+		job.start();
+
+		// The modification time of the last created file.
+		long lastCreatedModTime = Long.MIN_VALUE;
+
+		// create the files to be read
+		for (int i = 0; i < NO_OF_FILES; i++) {
+			Tuple2<org.apache.hadoop.fs.Path, String> tmpFile;
+			long modTime;
+			do {
+
+				// give it some time so that the files have
+				// different modification timestamps.
+				Thread.sleep(50);
+
+				tmpFile = fillWithData(hdfsURI, "file", i, "This is test line.");
+
+				modTime = hdfs.getFileStatus(tmpFile.f0).getModificationTime();
+				if (modTime <= lastCreatedModTime) {
+					// delete the last created file to recreate it with a different timestamp
+					hdfs.delete(tmpFile.f0, false);
+				}
+			} while (modTime <= lastCreatedModTime);
+			lastCreatedModTime = modTime;
+
+			// put the contents in the expected results list before the reader picks them
+			// this is to guarantee that they are in before the reader finishes (avoid race conditions)
+			expectedContents.put(i, tmpFile.f1);
+
+			org.apache.hadoop.fs.Path file =
+				new org.apache.hadoop.fs.Path(hdfsURI + "/file" + i);
+			hdfs.rename(tmpFile.f0, file);
+			Assert.assertTrue(hdfs.exists(file));
 		}
+
+		// wait for the job to finish.
+		job.join();
 	}
 
 	private static class TestingSinkFunction extends RichSinkFunction<String> {
 
 		private int elementCounter = 0;
-		private Map<Integer, Integer> elementCounters = new HashMap<>();
-		private Map<Integer, List<String>> collectedContent = new HashMap<>();
+		private Map<Integer, Set<String>> actualContent = new HashMap<>();
+
+		private transient Comparator<String> comparator;
 
 		@Override
 		public void open(Configuration parameters) throws Exception {
 			// this sink can only work with DOP 1
 			assertEquals(1, getRuntimeContext().getNumberOfParallelSubtasks());
+
+			comparator = new Comparator<String>() {
+				@Override
+				public int compare(String o1, String o2) {
+					return getLineNo(o1) - getLineNo(o2);
+				}
+			};
+		}
+
+		@Override
+		public void invoke(String value) throws Exception {
+			int fileIdx = getFileIdx(value);
+
+			Set<String> content = actualContent.get(fileIdx);
+			if (content == null) {
+				content = new HashSet<>();
+				actualContent.put(fileIdx, content);
+			}
+
+			if (!content.add(value + "\n")) {
+				Assert.fail("Duplicate line: "+ value);
+				System.exit(0);
+			}
+
+			elementCounter++;
+			if (elementCounter == NO_OF_FILES * LINES_PER_FILE) {
+				throw new SuccessException();
+			}
 		}
 
 		@Override
 		public void close() {
 			// check if the data that we collected are the ones they are supposed to be.
-
-			Assert.assertEquals(collectedContent.size(), expectedContents.size());
+			Assert.assertEquals(expectedContents.size(), actualContent.size());
 			for (Integer fileIdx: expectedContents.keySet()) {
-				Assert.assertTrue(collectedContent.keySet().contains(fileIdx));
+				Assert.assertTrue(actualContent.keySet().contains(fileIdx));
 
-				List<String> cntnt = collectedContent.get(fileIdx);
-				Collections.sort(cntnt, new Comparator<String>() {
-					@Override
-					public int compare(String o1, String o2) {
-						return getLineNo(o1) - getLineNo(o2);
-					}
-				});
+				List<String> cntnt = new ArrayList<>(actualContent.get(fileIdx));
+				Collections.sort(cntnt, comparator);
 
 				StringBuilder cntntStr = new StringBuilder();
 				for (String line: cntnt) {
 					cntntStr.append(line);
 				}
-				Assert.assertEquals(cntntStr.toString(), expectedContents.get(fileIdx));
+				Assert.assertEquals(expectedContents.get(fileIdx), cntntStr.toString());
 			}
 			expectedContents.clear();
 		}
 
 		private int getLineNo(String line) {
 			String[] tkns = line.split("\\s");
-			Assert.assertTrue(tkns.length == 6);
 			return Integer.parseInt(tkns[tkns.length - 1]);
 		}
 
-		@Override
-		public void invoke(String value) throws Exception {
-			int fileIdx = Character.getNumericValue(value.charAt(0));
-
-			Integer counter = elementCounters.get(fileIdx);
-			if (counter == null) {
-				counter = 0;
-			} else if (counter == LINES_PER_FILE) {
-				// ignore duplicate lines.
-				Assert.fail("Duplicate lines detected.");
-			}
-			elementCounters.put(fileIdx, ++counter);
-
-			List<String> content = collectedContent.get(fileIdx);
-			if (content == null) {
-				content = new ArrayList<>();
-				collectedContent.put(fileIdx, content);
-			}
-			content.add(value + "\n");
-
-			elementCounter++;
-			if (elementCounter == NO_OF_FILES * LINES_PER_FILE) {
-				throw new SuccessException();
-			}
+		private int getFileIdx(String line) {
+			String[] tkns = line.split(":");
+			return Integer.parseInt(tkns[0]);
 		}
 	}
 
-	/**
-	 * A separate thread creating {@link #NO_OF_FILES} files, one file every {@link #INTERVAL} milliseconds.
-	 * It serves for testing the file monitoring functionality of the {@link ContinuousFileMonitoringFunction}.
-	 * The files are filled with data by the {@link #fillWithData(String, String, int, String)} method.
-	 * */
-	private class FileCreator implements Runnable {
+	/** Create a file and fill it with content. */
+	private Tuple2<org.apache.hadoop.fs.Path, String> fillWithData(
+		String base, String fileName, int fileIdx, String sampleLine) throws IOException, InterruptedException {
 
-		private final long interval;
-
-		FileCreator(long interval) {
-			this.interval = interval;
-		}
-
-		public void run() {
-			try {
-				for (int i = 0; i < NO_OF_FILES; i++) {
-					fillWithData(hdfsURI, "file", i, "This is test line.");
-					Thread.sleep(interval);
-				}
-			} catch (IOException e) {
-				e.printStackTrace();
-			} catch (InterruptedException e) {
-				// we just close without any message.
-			}
-		}
-	}
-
-	/**
-	 * Fill the file with content.
-	 * */
-	private void fillWithData(String base, String fileName, int fileIdx, String sampleLine) throws IOException {
 		assert (hdfs != null);
 
-		org.apache.hadoop.fs.Path file = new org.apache.hadoop.fs.Path(base + "/" + fileName + fileIdx);
+		org.apache.hadoop.fs.Path tmp =
+			new org.apache.hadoop.fs.Path(base + "/." + fileName + fileIdx);
 
-		org.apache.hadoop.fs.Path tmp = new org.apache.hadoop.fs.Path(base + "/." + fileName + fileIdx);
 		FSDataOutputStream stream = hdfs.create(tmp);
 		StringBuilder str = new StringBuilder();
 		for (int i = 0; i < LINES_PER_FILE; i++) {
-			String line = fileIdx +": "+ sampleLine + " " + i +"\n";
+			String line = fileIdx + ": " + sampleLine + " " + i + "\n";
 			str.append(line);
 			stream.write(line.getBytes());
 		}
 		stream.close();
-
-		hdfs.rename(tmp, file);
-
-		expectedContents.put(fileIdx, str.toString());
-
-		Assert.assertTrue("No result file present", hdfs.exists(file));
+		return new Tuple2<>(tmp, str.toString());
 	}
 
 	public static class SuccessException extends Exception {
diff --git a/flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileMonitoringTest.java b/flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java
similarity index 53%
rename from flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileMonitoringTest.java
rename to flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java
index 198a621c27f..0283f681780 100644
--- a/flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileMonitoringTest.java
+++ b/flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java
@@ -27,9 +27,11 @@ import org.apache.flink.configuration.Configuration;
 import org.apache.flink.core.fs.FileInputSplit;
 import org.apache.flink.core.fs.Path;
 import org.apache.flink.api.common.io.FilePathFilter;
+import org.apache.flink.core.testutils.OneShotLatch;
 import org.apache.flink.streaming.api.TimeCharacteristic;
 import org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction;
 import org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator;
+import org.apache.flink.streaming.api.functions.source.TimestampedFileInputSplit;
 import org.apache.flink.streaming.api.functions.source.FileProcessingMode;
 import org.apache.flink.streaming.api.functions.source.SourceFunction;
 import org.apache.flink.streaming.api.watermark.Watermark;
@@ -53,10 +55,12 @@ import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
+import java.util.TreeSet;
+import java.util.concurrent.ConcurrentLinkedQueue;
 
-public class ContinuousFileMonitoringTest {
+public class ContinuousFileProcessingTest {
 
-	private static final int NO_OF_FILES = 10;
+	private static final int NO_OF_FILES = 5;
 	private static final int LINES_PER_FILE = 10;
 
 	private static final long INTERVAL = 100;
@@ -109,10 +113,11 @@ public class ContinuousFileMonitoringTest {
 	public void testFileReadingOperatorWithIngestionTime() throws Exception {
 		Set<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();
 		Map<Integer, String> expectedFileContents = new HashMap<>();
-
-		for(int i = 0; i < NO_OF_FILES; i++) {
-			Tuple2<org.apache.hadoop.fs.Path, String> file = fillWithData(hdfsURI, "file", i, "This is test line.");
+		Map<String, Long> modTimes = new HashMap<>();
+		for (int i = 0; i < NO_OF_FILES; i++) {
+			Tuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(hdfsURI, "file", i, "This is test line.");
 			filesCreated.add(file.f0);
+			modTimes.put(file.f0.getName(), hdfs.getFileStatus(file.f0).getModificationTime());
 			expectedFileContents.put(i, file.f1);
 		}
 
@@ -121,16 +126,13 @@ public class ContinuousFileMonitoringTest {
 
 		final long watermarkInterval = 10;
 
-		ContinuousFileReaderOperator<String, ?> reader = new ContinuousFileReaderOperator<>(format);
-		reader.setOutputType(typeInfo, new ExecutionConfig());
-
-		final OneInputStreamOperatorTestHarness<FileInputSplit, String> tester =
+		ContinuousFileReaderOperator<String> reader = new ContinuousFileReaderOperator<>(format);
+		final OneInputStreamOperatorTestHarness<TimestampedFileInputSplit, String> tester =
 			new OneInputStreamOperatorTestHarness<>(reader);
 
 		tester.getExecutionConfig().setAutoWatermarkInterval(watermarkInterval);
-
-
 		tester.setTimeCharacteristic(TimeCharacteristic.IngestionTime);
+		reader.setOutputType(typeInfo, tester.getExecutionConfig());
 
 		tester.open();
 
@@ -138,24 +140,25 @@ public class ContinuousFileMonitoringTest {
 
 		// test that watermarks are correctly emitted
 
+		ConcurrentLinkedQueue<Object> output = tester.getOutput();
+
 		tester.setProcessingTime(201);
+		Assert.assertTrue(output.peek() instanceof Watermark);
+		Assert.assertEquals(200, ((Watermark) output.poll()).getTimestamp());
+
 		tester.setProcessingTime(301);
+		Assert.assertTrue(output.peek() instanceof Watermark);
+		Assert.assertEquals(300, ((Watermark) output.poll()).getTimestamp());
+
 		tester.setProcessingTime(401);
-		tester.setProcessingTime(501);
+		Assert.assertTrue(output.peek() instanceof Watermark);
+		Assert.assertEquals(400, ((Watermark) output.poll()).getTimestamp());
 
-		int i = 0;
-		for(Object line: tester.getOutput()) {
-			if (!(line instanceof Watermark)) {
-				Assert.fail("Only watermarks are expected here ");
-			}
-			Watermark w = (Watermark) line;
-			Assert.assertEquals(200 + (i * 100), w.getTimestamp());
-			i++;
-		}
+		tester.setProcessingTime(501);
+		Assert.assertTrue(output.peek() instanceof Watermark);
+		Assert.assertEquals(500, ((Watermark) output.poll()).getTimestamp());
 
-		// clear the output to get the elements only and the final watermark
-		tester.getOutput().clear();
-		Assert.assertEquals(0, tester.getOutput().size());
+		Assert.assertTrue(output.isEmpty());
 
 		// create the necessary splits for the test
 		FileInputSplit[] splits = format.createInputSplits(
@@ -168,23 +171,31 @@ public class ContinuousFileMonitoringTest {
 		int lineCounter = 0;	// counter for the lines read from the splits
 		int watermarkCounter = 0;
 
-		for(FileInputSplit split: splits) {
+		for (FileInputSplit split: splits) {
 
 			// set the next "current processing time".
 			long nextTimestamp = tester.getProcessingTime() + watermarkInterval;
 			tester.setProcessingTime(nextTimestamp);
 
-			// send the next split to be read and wait until it is fully read.
-			tester.processElement(new StreamRecord<>(split));
-			synchronized (tester.getCheckpointLock()) {
-				while (tester.getOutput().isEmpty() || tester.getOutput().size() != (LINES_PER_FILE + 1)) {
-					tester.getCheckpointLock().wait(10);
-				}
+			// send the next split to be read and wait until it is fully read, the +1 is for the watermark.
+			tester.processElement(new StreamRecord<>(
+				new TimestampedFileInputSplit(modTimes.get(split.getPath().getName()),
+					split.getSplitNumber(), split.getPath(), split.getStart(),
+					split.getLength(), split.getHostnames())));
+
+			// NOTE: the following check works because each file fits in one split.
+			// In other case it would fail and wait forever.
+			// BUT THIS IS JUST FOR THIS TEST
+			while (tester.getOutput().isEmpty() || tester.getOutput().size() != (LINES_PER_FILE + 1)) {
+				Thread.sleep(10);
 			}
 
 			// verify that the results are the expected
-			for(Object line: tester.getOutput()) {
+			for (Object line: tester.getOutput()) {
+
 				if (line instanceof StreamRecord) {
+
+					@SuppressWarnings("unchecked")
 					StreamRecord<String> element = (StreamRecord<String>) line;
 					lineCounter++;
 
@@ -219,14 +230,14 @@ public class ContinuousFileMonitoringTest {
 		Assert.assertEquals(NO_OF_FILES * LINES_PER_FILE, lineCounter);
 
 		// because we expect one watermark per split.
-		Assert.assertEquals(NO_OF_FILES, watermarkCounter);
+		Assert.assertEquals(splits.length, watermarkCounter);
 
 		// then close the reader gracefully so that the Long.MAX watermark is emitted
 		synchronized (tester.getCheckpointLock()) {
 			tester.close();
 		}
 
-		for(org.apache.hadoop.fs.Path file: filesCreated) {
+		for (org.apache.hadoop.fs.Path file: filesCreated) {
 			hdfs.delete(file, false);
 		}
 
@@ -257,11 +268,13 @@ public class ContinuousFileMonitoringTest {
 	}
 
 	@Test
-	public void testFileReadingOperator() throws Exception {
+	public void testFileReadingOperatorWithEventTime() throws Exception {
 		Set<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();
+		Map<String, Long> modTimes = new HashMap<>();
 		Map<Integer, String> expectedFileContents = new HashMap<>();
-		for(int i = 0; i < NO_OF_FILES; i++) {
-			Tuple2<org.apache.hadoop.fs.Path, String> file = fillWithData(hdfsURI, "file", i, "This is test line.");
+		for (int i = 0; i < NO_OF_FILES; i++) {
+			Tuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(hdfsURI, "file", i, "This is test line.");
+			modTimes.put(file.f0.getName(), hdfs.getFileStatus(file.f0).getModificationTime());
 			filesCreated.add(file.f0);
 			expectedFileContents.put(i, file.f1);
 		}
@@ -269,10 +282,10 @@ public class ContinuousFileMonitoringTest {
 		TextInputFormat format = new TextInputFormat(new Path(hdfsURI));
 		TypeInformation<String> typeInfo = TypeExtractor.getInputFormatTypes(format);
 
-		ContinuousFileReaderOperator<String, ?> reader = new ContinuousFileReaderOperator<>(format);
+		ContinuousFileReaderOperator<String> reader = new ContinuousFileReaderOperator<>(format);
 		reader.setOutputType(typeInfo, new ExecutionConfig());
 
-		OneInputStreamOperatorTestHarness<FileInputSplit, String> tester =
+		OneInputStreamOperatorTestHarness<TimestampedFileInputSplit, String> tester =
 			new OneInputStreamOperatorTestHarness<>(reader);
 		tester.setTimeCharacteristic(TimeCharacteristic.EventTime);
 		tester.open();
@@ -282,8 +295,10 @@ public class ContinuousFileMonitoringTest {
 			reader.getRuntimeContext().getNumberOfParallelSubtasks());
 
 		// and feed them to the operator
-		for(FileInputSplit split: splits) {
-			tester.processElement(new StreamRecord<>(split));
+		for (FileInputSplit split: splits) {
+			tester.processElement(new StreamRecord<>(new TimestampedFileInputSplit(
+				modTimes.get(split.getPath().getName()), split.getSplitNumber(), split.getPath(),
+				split.getStart(), split.getLength(), split.getHostnames())));
 		}
 
 		// then close the reader gracefully (and wait to finish reading)
@@ -299,9 +314,12 @@ public class ContinuousFileMonitoringTest {
 
 		Map<Integer, List<String>> actualFileContents = new HashMap<>();
 		Object lastElement = null;
-		for(Object line: tester.getOutput()) {
+		for (Object line: tester.getOutput()) {
 			lastElement = line;
+
 			if (line instanceof StreamRecord) {
+
+				@SuppressWarnings("unchecked")
 				StreamRecord<String> element = (StreamRecord<String>) line;
 
 				int fileIdx = Character.getNumericValue(element.getValue().charAt(0));
@@ -337,240 +355,296 @@ public class ContinuousFileMonitoringTest {
 			Assert.assertEquals(expectedFileContents.get(fileIdx), cntntStr.toString());
 		}
 
-		for(org.apache.hadoop.fs.Path file: filesCreated) {
+		for (org.apache.hadoop.fs.Path file: filesCreated) {
 			hdfs.delete(file, false);
 		}
 	}
 
-	private static class PathFilter extends FilePathFilter {
-
-		@Override
-		public boolean filterPath(Path filePath) {
-			return filePath.getName().startsWith("**");
-		}
-	}
+	////				Monitoring Function Tests				//////
 
 	@Test
 	public void testFilePathFiltering() throws Exception {
-		Set<String> uniqFilesFound = new HashSet<>();
 		Set<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();
+		Set<String> filesKept = new TreeSet<>();
 
 		// create the files to be discarded
 		for (int i = 0; i < NO_OF_FILES; i++) {
-			Tuple2<org.apache.hadoop.fs.Path, String> file = fillWithData(hdfsURI, "**file", i, "This is test line.");
+			Tuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(hdfsURI, "**file", i, "This is test line.");
 			filesCreated.add(file.f0);
 		}
 
 		// create the files to be kept
 		for (int i = 0; i < NO_OF_FILES; i++) {
-			Tuple2<org.apache.hadoop.fs.Path, String> file = fillWithData(hdfsURI, "file", i, "This is test line.");
+			Tuple2<org.apache.hadoop.fs.Path, String> file =
+				createFileAndFillWithData(hdfsURI, "file", i, "This is test line.");
 			filesCreated.add(file.f0);
+			filesKept.add(file.f0.getName());
 		}
 
 		TextInputFormat format = new TextInputFormat(new Path(hdfsURI));
-		format.setFilesFilter(new PathFilter());
+		format.setFilesFilter(new FilePathFilter() {
+			@Override
+			public boolean filterPath(Path filePath) {
+				return filePath.getName().startsWith("**");
+			}
+		});
+
 		ContinuousFileMonitoringFunction<String> monitoringFunction =
 			new ContinuousFileMonitoringFunction<>(format, hdfsURI,
 				FileProcessingMode.PROCESS_ONCE, 1, INTERVAL);
 
+		final FileVerifyingSourceContext context =
+			new FileVerifyingSourceContext(new OneShotLatch(), monitoringFunction, 0, -1);
+
 		monitoringFunction.open(new Configuration());
-		monitoringFunction.run(new TestingSourceContext(monitoringFunction, uniqFilesFound));
+		monitoringFunction.run(context);
 
-		Assert.assertEquals(NO_OF_FILES, uniqFilesFound.size());
-		for(int i = 0; i < NO_OF_FILES; i++) {
-			org.apache.hadoop.fs.Path file = new org.apache.hadoop.fs.Path(hdfsURI + "/file" + i);
-			Assert.assertTrue(uniqFilesFound.contains(file.toString()));
-		}
+		Assert.assertArrayEquals(filesKept.toArray(), context.getSeenFiles().toArray());
 
-		for(org.apache.hadoop.fs.Path file: filesCreated) {
+		// finally delete the files created for the test.
+		for (org.apache.hadoop.fs.Path file: filesCreated) {
 			hdfs.delete(file, false);
 		}
 	}
 
 	@Test
-	public void testFileSplitMonitoringReprocessWithAppended() throws Exception {
-		final Set<String> uniqFilesFound = new HashSet<>();
+	public void testSortingOnModTime() throws Exception {
+		final long[] modTimes = new long[NO_OF_FILES];
+		final org.apache.hadoop.fs.Path[] filesCreated = new org.apache.hadoop.fs.Path[NO_OF_FILES];
+
+		for (int i = 0; i < NO_OF_FILES; i++) {
+			Tuple2<org.apache.hadoop.fs.Path, String> file =
+				createFileAndFillWithData(hdfsURI, "file", i, "This is test line.");
+			Thread.sleep(400);
+
+			filesCreated[i] = file.f0;
+			modTimes[i] = hdfs.getFileStatus(file.f0).getModificationTime();
+		}
+
+		TextInputFormat format = new TextInputFormat(new Path(hdfsURI));
+		format.setFilesFilter(FilePathFilter.createDefaultFilter());
+
+		// this is just to verify that all splits have been forwarded later.
+		FileInputSplit[] splits = format.createInputSplits(1);
+
+		ContinuousFileMonitoringFunction<String> monitoringFunction =
+			new ContinuousFileMonitoringFunction<>(format, hdfsURI,
+				FileProcessingMode.PROCESS_ONCE, 1, INTERVAL);
+
+		ModTimeVerifyingSourceContext context = new ModTimeVerifyingSourceContext(modTimes);
+
+		monitoringFunction.open(new Configuration());
+		monitoringFunction.run(context);
+		Assert.assertEquals(splits.length, context.getCounter());
+
+		// delete the created files.
+		for (int i = 0; i < NO_OF_FILES; i++) {
+			hdfs.delete(filesCreated[i], false);
+		}
+	}
+
+	@Test
+	public void testProcessOnce() throws Exception {
+		final OneShotLatch latch = new OneShotLatch();
+
+		// create a single file in the directory
+		Tuple2<org.apache.hadoop.fs.Path, String> bootstrap =
+			createFileAndFillWithData(hdfsURI, "file", NO_OF_FILES + 1, "This is test line.");
+		Assert.assertTrue(hdfs.exists(bootstrap.f0));
 
-		FileCreator fc = new FileCreator(INTERVAL, NO_OF_FILES);
-		fc.start();
+		// the source is supposed to read only this file.
+		final Set<String> filesToBeRead = new TreeSet<>();
+		filesToBeRead.add(bootstrap.f0.getName());
 
-		Thread t = new Thread(new Runnable() {
+		TextInputFormat format = new TextInputFormat(new Path(hdfsURI));
+		format.setFilesFilter(FilePathFilter.createDefaultFilter());
+
+		final ContinuousFileMonitoringFunction<String> monitoringFunction =
+			new ContinuousFileMonitoringFunction<>(format, hdfsURI,
+				FileProcessingMode.PROCESS_ONCE, 1, INTERVAL);
+
+		final FileVerifyingSourceContext context =
+			new FileVerifyingSourceContext(latch, monitoringFunction, 1, -1);
+
+		final Thread t = new Thread() {
 			@Override
 			public void run() {
-				TextInputFormat format = new TextInputFormat(new Path(hdfsURI));
-				format.setFilesFilter(FilePathFilter.createDefaultFilter());
-				ContinuousFileMonitoringFunction<String> monitoringFunction =
-					new ContinuousFileMonitoringFunction<>(format, hdfsURI,
-						FileProcessingMode.PROCESS_CONTINUOUSLY, 1, INTERVAL);
-
 				try {
 					monitoringFunction.open(new Configuration());
-					monitoringFunction.run(new TestingSourceContext(monitoringFunction, uniqFilesFound));
+					monitoringFunction.run(context);
 				} catch (Exception e) {
-					// do nothing as we interrupted the thread.
+					Assert.fail(e.getMessage());
 				}
 			}
-		});
+		};
 		t.start();
 
-		// wait until the sink also sees all the splits.
-		synchronized (uniqFilesFound) {
-			uniqFilesFound.wait();
+		if (!latch.isTriggered()) {
+			latch.await();
 		}
-		t.interrupt();
-		fc.join();
-
-		Assert.assertEquals(NO_OF_FILES, fc.getFilesCreated().size());
-		Assert.assertEquals(NO_OF_FILES, uniqFilesFound.size());
 
-		Set<org.apache.hadoop.fs.Path> filesCreated = fc.getFilesCreated();
-		Set<String> fileNamesCreated = new HashSet<>();
-		for (org.apache.hadoop.fs.Path path: fc.getFilesCreated()) {
-			fileNamesCreated.add(path.toString());
+		// create some additional files that should be processed in the case of PROCESS_CONTINUOUSLY
+		final org.apache.hadoop.fs.Path[] filesCreated = new org.apache.hadoop.fs.Path[NO_OF_FILES];
+		for (int i = 0; i < NO_OF_FILES; i++) {
+			Tuple2<org.apache.hadoop.fs.Path, String> ignoredFile =
+				createFileAndFillWithData(hdfsURI, "file", i, "This is test line.");
+			filesCreated[i] = ignoredFile.f0;
 		}
 
-		for(String file: uniqFilesFound) {
-			Assert.assertTrue(fileNamesCreated.contains(file));
-		}
+		// wait until the monitoring thread exits
+		t.join();
 
-		for(org.apache.hadoop.fs.Path file: filesCreated) {
-			hdfs.delete(file, false);
+		Assert.assertArrayEquals(filesToBeRead.toArray(), context.getSeenFiles().toArray());
+
+		// finally delete the files created for the test.
+		hdfs.delete(bootstrap.f0, false);
+		for (org.apache.hadoop.fs.Path path: filesCreated) {
+			hdfs.delete(path, false);
 		}
 	}
 
 	@Test
-	public void testFileSplitMonitoringProcessOnce() throws Exception {
-		Set<String> uniqFilesFound = new HashSet<>();
-
-		FileCreator fc = new FileCreator(INTERVAL, 1);
-		Set<org.apache.hadoop.fs.Path> filesCreated = fc.getFilesCreated();
-		fc.start();
-
-		// to make sure that at least one file is created
-		if (filesCreated.size() == 0) {
-			synchronized (filesCreated) {
-				if (filesCreated.size() == 0) {
-					filesCreated.wait();
-				}
-			}
-		}
-		Assert.assertTrue(fc.getFilesCreated().size() >= 1);
+	public void testProcessContinuously() throws Exception {
+		final OneShotLatch latch = new OneShotLatch();
+
+		// create a single file in the directory
+		Tuple2<org.apache.hadoop.fs.Path, String> bootstrap =
+			createFileAndFillWithData(hdfsURI, "file", NO_OF_FILES + 1, "This is test line.");
+		Assert.assertTrue(hdfs.exists(bootstrap.f0));
+
+		final Set<String> filesToBeRead = new TreeSet<>();
+		filesToBeRead.add(bootstrap.f0.getName());
 
 		TextInputFormat format = new TextInputFormat(new Path(hdfsURI));
 		format.setFilesFilter(FilePathFilter.createDefaultFilter());
-		ContinuousFileMonitoringFunction<String> monitoringFunction =
+
+		final ContinuousFileMonitoringFunction<String> monitoringFunction =
 			new ContinuousFileMonitoringFunction<>(format, hdfsURI,
-				FileProcessingMode.PROCESS_ONCE, 1, INTERVAL);
+				FileProcessingMode.PROCESS_CONTINUOUSLY, 1, INTERVAL);
 
-		monitoringFunction.open(new Configuration());
-		monitoringFunction.run(new TestingSourceContext(monitoringFunction, uniqFilesFound));
+		final int totalNoOfFilesToBeRead = NO_OF_FILES + 1; // 1 for the bootstrap + NO_OF_FILES
+		final FileVerifyingSourceContext context = new FileVerifyingSourceContext(latch,
+			monitoringFunction, 1, totalNoOfFilesToBeRead);
 
-		// wait until all the files are created
-		fc.join();
+		final Thread t = new Thread() {
 
-		Assert.assertEquals(NO_OF_FILES, filesCreated.size());
+			@Override
+			public void run() {
+				try {
+					monitoringFunction.open(new Configuration());
+					monitoringFunction.run(context);
+				} catch (Exception e) {
+					Assert.fail(e.getMessage());
+				}
+			}
+		};
+		t.start();
 
-		Set<String> fileNamesCreated = new HashSet<>();
-		for (org.apache.hadoop.fs.Path path: fc.getFilesCreated()) {
-			fileNamesCreated.add(path.toString());
+		if (!latch.isTriggered()) {
+			latch.await();
 		}
 
-		Assert.assertTrue(uniqFilesFound.size() >= 1 && uniqFilesFound.size() < fileNamesCreated.size());
-		for(String file: uniqFilesFound) {
-			Assert.assertTrue(fileNamesCreated.contains(file));
+		// create some additional files that will be processed in the case of PROCESS_CONTINUOUSLY
+		final org.apache.hadoop.fs.Path[] filesCreated = new org.apache.hadoop.fs.Path[NO_OF_FILES];
+		for (int i = 0; i < NO_OF_FILES; i++) {
+			Tuple2<org.apache.hadoop.fs.Path, String> file =
+				createFileAndFillWithData(hdfsURI, "file", i, "This is test line.");
+			filesCreated[i] = file.f0;
+			filesToBeRead.add(file.f0.getName());
 		}
 
-		for(org.apache.hadoop.fs.Path file: filesCreated) {
-			hdfs.delete(file, false);
+		// wait until the monitoring thread exits
+		t.join();
+
+		Assert.assertArrayEquals(filesToBeRead.toArray(), context.getSeenFiles().toArray());
+
+		// finally delete the files created for the test.
+		hdfs.delete(bootstrap.f0, false);
+		for (org.apache.hadoop.fs.Path path: filesCreated) {
+			hdfs.delete(path, false);
 		}
 	}
 
-	// -------------		End of Tests
+	///////////				Source Contexts Used by the tests				/////////////////
 
-	private int getLineNo(String line) {
-		String[] tkns = line.split("\\s");
-		Assert.assertEquals(6, tkns.length);
-		return Integer.parseInt(tkns[tkns.length - 1]);
-	}
+	private static class FileVerifyingSourceContext extends DummySourceContext {
 
-	/**
-	 * A separate thread creating {@link #NO_OF_FILES} files, one file every {@link #INTERVAL} milliseconds.
-	 * It serves for testing the file monitoring functionality of the {@link ContinuousFileMonitoringFunction}.
-	 * The files are filled with data by the {@link #fillWithData(String, String, int, String)} method.
-	 * */
-	private class FileCreator extends Thread {
+		private final ContinuousFileMonitoringFunction src;
+		private final OneShotLatch latch;
+		private final Set<String> seenFiles;
+		private final int elementsBeforeNotifying;
 
-		private final long interval;
-		private final int noOfFilesBeforeNotifying;
+		private int elementsBeforeCanceling = -1;
 
-		private final Set<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();
+		FileVerifyingSourceContext(OneShotLatch latch,
+								ContinuousFileMonitoringFunction src,
+								int elementsBeforeNotifying,
+								int elementsBeforeCanceling) {
+			this.latch = latch;
+			this.seenFiles = new TreeSet<>();
+			this.src = src;
+			this.elementsBeforeNotifying = elementsBeforeNotifying;
+			this.elementsBeforeCanceling = elementsBeforeCanceling;
+		}
 
-		FileCreator(long interval, int notificationLim) {
-			this.interval = interval;
-			this.noOfFilesBeforeNotifying = notificationLim;
+		Set<String> getSeenFiles() {
+			return this.seenFiles;
 		}
 
-		public void run() {
-			try {
-				for(int i = 0; i < NO_OF_FILES; i++) {
-					Tuple2<org.apache.hadoop.fs.Path, String> file =
-						fillWithData(hdfsURI, "file", i, "This is test line.");
-
-					synchronized (filesCreated) {
-						filesCreated.add(file.f0);
-						if (filesCreated.size() == noOfFilesBeforeNotifying) {
-							filesCreated.notifyAll();
-						}
-					}
-					Thread.sleep(interval);
-				}
-			} catch (IOException | InterruptedException e) {
-				e.printStackTrace();
+		@Override
+		public void collect(TimestampedFileInputSplit element) {
+			String seenFileName = element.getPath().getName();
+
+			this.seenFiles.add(seenFileName);
+			if (seenFiles.size() == elementsBeforeNotifying) {
+				latch.trigger();
 			}
-		}
 
-		Set<org.apache.hadoop.fs.Path> getFilesCreated() {
-			return this.filesCreated;
+			if (elementsBeforeCanceling != -1 && seenFiles.size() == elementsBeforeCanceling) {
+				src.cancel();
+			}
 		}
 	}
 
-	private class TestingSourceContext implements SourceFunction.SourceContext<FileInputSplit> {
+	private static class ModTimeVerifyingSourceContext extends DummySourceContext {
 
-		private final ContinuousFileMonitoringFunction src;
-		private final Set<String> filesFound;
-		private final Object lock = new Object();
+		final long[] expectedModificationTimes;
+		int splitCounter;
+		long lastSeenModTime;
+
+		ModTimeVerifyingSourceContext(long[] modTimes) {
+			this.expectedModificationTimes = modTimes;
+			this.splitCounter = 0;
+			this.lastSeenModTime = Long.MIN_VALUE;
+		}
 
-		TestingSourceContext(ContinuousFileMonitoringFunction monitoringFunction, Set<String> uniqFilesFound) {
-			this.filesFound = uniqFilesFound;
-			this.src = monitoringFunction;
+		int getCounter() {
+			return splitCounter;
 		}
 
 		@Override
-		public void collect(FileInputSplit element) {
+		public void collect(TimestampedFileInputSplit element) {
+			try {
+				long modTime = hdfs.getFileStatus(new org.apache.hadoop.fs.Path(element.getPath().getPath())).getModificationTime();
 
-			String filePath = element.getPath().toString();
-			if (filesFound.contains(filePath)) {
-				// check if we have duplicate splits that are open during the first time
-				// the monitor sees them, and they then close, so the modification time changes.
-				Assert.fail("Duplicate file: " + filePath);
-			}
+				Assert.assertTrue(modTime >= lastSeenModTime);
+				Assert.assertEquals(expectedModificationTimes[splitCounter], modTime);
 
-			synchronized (filesFound) {
-				filesFound.add(filePath);
-				try {
-					if (filesFound.size() == NO_OF_FILES) {
-						this.src.cancel();
-						this.src.close();
-						filesFound.notifyAll();
-					}
-				} catch (Exception e) {
-					e.printStackTrace();
-				}
+				lastSeenModTime = modTime;
+				splitCounter++;
+			} catch (IOException e) {
+				Assert.fail(e.getMessage());
 			}
 		}
+	}
+
+	private static abstract class DummySourceContext
+			implements SourceFunction.SourceContext<TimestampedFileInputSplit> {
+
+		private final Object lock = new Object();
 
 		@Override
-		public void collectWithTimestamp(FileInputSplit element, long timestamp) {
+		public void collectWithTimestamp(TimestampedFileInputSplit element, long timestamp) {
 		}
 
 		@Override
@@ -587,10 +661,21 @@ public class ContinuousFileMonitoringTest {
 		}
 	}
 
+	/////////				Auxiliary Methods				/////////////
+
+	private int getLineNo(String line) {
+		String[] tkns = line.split("\\s");
+		Assert.assertEquals(6, tkns.length);
+		return Integer.parseInt(tkns[tkns.length - 1]);
+	}
+
 	/**
-	 * Fill the file with content.
+	 * Create a file with pre-determined String format of the form:
+	 * {@code fileIdx +": "+ sampleLine +" "+ lineNo}.
 	 * */
-	private Tuple2<org.apache.hadoop.fs.Path, String> fillWithData(String base, String fileName, int fileIdx, String sampleLine) throws IOException {
+	private Tuple2<org.apache.hadoop.fs.Path, String> createFileAndFillWithData(
+				String base, String fileName, int fileIdx, String sampleLine) throws IOException {
+
 		assert (hdfs != null);
 
 		org.apache.hadoop.fs.Path file = new org.apache.hadoop.fs.Path(base + "/" + fileName + fileIdx);
@@ -599,7 +684,7 @@ public class ContinuousFileMonitoringTest {
 		org.apache.hadoop.fs.Path tmp = new org.apache.hadoop.fs.Path(base + "/." + fileName + fileIdx);
 		FSDataOutputStream stream = hdfs.create(tmp);
 		StringBuilder str = new StringBuilder();
-		for(int i = 0; i < LINES_PER_FILE; i++) {
+		for (int i = 0; i < LINES_PER_FILE; i++) {
 			String line = fileIdx +": "+ sampleLine + " " + i +"\n";
 			str.append(line);
 			stream.write(line.getBytes());
diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.java
index e1cfc60a7f5..08e17a1f5a7 100644
--- a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.java
+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.java
@@ -81,10 +81,6 @@ import java.util.Collection;
 import java.util.Iterator;
 import java.util.List;
 
-/**
- * An ExecutionEnvironment for streaming jobs. An instance of it is
- * necessary to construct streaming topologies.
- */
 /**
  * The StreamExecutionEnvironment is the context in which a streaming program is executed. A
  * {@link LocalStreamEnvironment} will cause execution in the current JVM, a
@@ -1350,18 +1346,20 @@ public abstract class StreamExecutionEnvironment {
 		Preconditions.checkNotNull(monitoringMode, "Unspecified monitoring mode.");
 
 		Preconditions.checkArgument(monitoringMode.equals(FileProcessingMode.PROCESS_ONCE) ||
-						interval >= ContinuousFileMonitoringFunction.MIN_MONITORING_INTERVAL,
-				"The path monitoring interval cannot be less than " +
-						ContinuousFileMonitoringFunction.MIN_MONITORING_INTERVAL + " ms.");
+				interval >= ContinuousFileMonitoringFunction.MIN_MONITORING_INTERVAL,
+			"The path monitoring interval cannot be less than " +
+					ContinuousFileMonitoringFunction.MIN_MONITORING_INTERVAL + " ms.");
 
-		ContinuousFileMonitoringFunction<OUT> monitoringFunction = new ContinuousFileMonitoringFunction<>(
+		ContinuousFileMonitoringFunction<OUT> monitoringFunction =
+			new ContinuousFileMonitoringFunction<>(
 				inputFormat, inputFormat.getFilePath().toString(),
 				monitoringMode, getParallelism(), interval);
 
-		ContinuousFileReaderOperator<OUT, ?> reader = new ContinuousFileReaderOperator<>(inputFormat);
+		ContinuousFileReaderOperator<OUT> reader =
+			new ContinuousFileReaderOperator<>(inputFormat);
 
 		SingleOutputStreamOperator<OUT> source = addSource(monitoringFunction, sourceName)
-				.transform("FileSplitReader_" + sourceName, typeInfo, reader);
+				.transform("Split Reader: " + sourceName, typeInfo, reader);
 
 		return new DataStreamSource<>(source);
 	}
diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/source/ContinuousFileMonitoringFunction.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/source/ContinuousFileMonitoringFunction.java
index f9ef565a30d..a6c5e4945e5 100644
--- a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/source/ContinuousFileMonitoringFunction.java
+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/source/ContinuousFileMonitoringFunction.java
@@ -19,13 +19,11 @@ package org.apache.flink.streaming.api.functions.source;
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.api.common.io.FileInputFormat;
 import org.apache.flink.api.common.io.FilePathFilter;
-import org.apache.flink.api.java.tuple.Tuple2;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.core.fs.FileInputSplit;
 import org.apache.flink.core.fs.FileStatus;
 import org.apache.flink.core.fs.FileSystem;
 import org.apache.flink.core.fs.Path;
-import org.apache.flink.runtime.JobException;
 import org.apache.flink.streaming.api.checkpoint.Checkpointed;
 import org.apache.flink.util.Preconditions;
 import org.slf4j.Logger;
@@ -35,52 +33,59 @@ import java.io.IOException;
 import java.net.URI;
 import java.util.ArrayList;
 import java.util.Collections;
-import java.util.Comparator;
 import java.util.HashMap;
-import java.util.Iterator;
-import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
+import java.util.TreeMap;
 
 /**
- * This is the single (non-parallel) task which takes a {@link FileInputFormat} and is responsible for
- * i) monitoring a user-provided path, ii) deciding which files should be further read and processed,
- * iii) creating the {@link FileInputSplit FileInputSplits} corresponding to those files, and iv) assigning
- * them to downstream tasks for further reading and processing. Which splits will be further processed
- * depends on the user-provided {@link FileProcessingMode} and the {@link FilePathFilter}.
- * The splits of the files to be read are then forwarded to the downstream
- * {@link ContinuousFileReaderOperator} which can have parallelism greater than one.
+ * This is the single (non-parallel) monitoring task which takes a {@link FileInputFormat}
+ * and, depending on the {@link FileProcessingMode} and the {@link FilePathFilter}, it is responsible for:
+ *
+ * <ol>
+ *     <li>Monitoring a user-provided path.</li>
+ *     <li>Deciding which files should be further read and processed.</li>
+ *     <li>Creating the {@link FileInputSplit splits} corresponding to those files.</li>
+ *     <li>Assigning them to downstream tasks for further processing.</li>
+ * </ol>
+ *
+ * The splits to be read are forwarded to the downstream {@link ContinuousFileReaderOperator}
+ * which can have parallelism greater than one.
+ *
+ * <b>IMPORTANT NOTE: </b> Splits are forwarded downstream for reading in ascending modification time order,
+ * based on the modification time of the files they belong to.
  */
 @Internal
 public class ContinuousFileMonitoringFunction<OUT>
-	extends RichSourceFunction<FileInputSplit> implements Checkpointed<Long> {
+	extends RichSourceFunction<TimestampedFileInputSplit> implements Checkpointed<Long> {
 
 	private static final long serialVersionUID = 1L;
 
 	private static final Logger LOG = LoggerFactory.getLogger(ContinuousFileMonitoringFunction.class);
 
 	/**
-	 * The minimum interval allowed between consecutive path scans. This is applicable if the
-	 * {@code watchType} is set to {@code PROCESS_CONTINUOUSLY}.
+	 * The minimum interval allowed between consecutive path scans.
+	 * <p><b>NOTE:</b> Only applicable to the {@code PROCESS_CONTINUOUSLY} mode.
 	 */
-	public static final long MIN_MONITORING_INTERVAL = 100l;
+	public static final long MIN_MONITORING_INTERVAL = 1L;
 
 	/** The path to monitor. */
 	private final String path;
 
-	/** The default parallelism for the job, as this is going to be the parallelism of the downstream readers. */
+	/** The parallelism of the downstream readers. */
 	private final int readerParallelism;
 
 	/** The {@link FileInputFormat} to be read. */
 	private FileInputFormat<OUT> format;
 
-	/** How often to monitor the state of the directory for new data. */
+	/** The interval between consecutive path scans. */
 	private final long interval;
 
 	/** Which new data to process (see {@link FileProcessingMode}. */
 	private final FileProcessingMode watchType;
 
-	private Long globalModificationTime;
+	/** The maximum file modification time seen so far. */
+	private volatile long globalModificationTime;
 
 	private transient Object checkpointLock;
 
@@ -91,10 +96,12 @@ public class ContinuousFileMonitoringFunction<OUT>
 		FileProcessingMode watchType,
 		int readerParallelism, long interval) {
 
-		if (watchType != FileProcessingMode.PROCESS_ONCE && interval < MIN_MONITORING_INTERVAL) {
-			throw new IllegalArgumentException("The specified monitoring interval (" + interval + " ms) is " +
-				"smaller than the minimum allowed one (100 ms).");
-		}
+		Preconditions.checkArgument(
+			watchType == FileProcessingMode.PROCESS_ONCE || interval >= MIN_MONITORING_INTERVAL,
+			"The specified monitoring interval (" + interval + " ms) is smaller than the minimum " +
+				"allowed one (" + MIN_MONITORING_INTERVAL + " ms)."
+		);
+
 		this.format = Preconditions.checkNotNull(format, "Unspecified File Input Format.");
 		this.path = Preconditions.checkNotNull(path, "Unspecified Path.");
 
@@ -105,16 +112,17 @@ public class ContinuousFileMonitoringFunction<OUT>
 	}
 
 	@Override
-	@SuppressWarnings("unchecked")
 	public void open(Configuration parameters) throws Exception {
-		LOG.info("Opening File Monitoring Source.");
-
 		super.open(parameters);
 		format.configure(parameters);
+
+		if (LOG.isDebugEnabled()) {
+			LOG.debug("Opened File Monitoring Source for path: " + path + ".");
+		}
 	}
 
 	@Override
-	public void run(SourceFunction.SourceContext<FileInputSplit> context) throws Exception {
+	public void run(SourceFunction.SourceContext<TimestampedFileInputSplit> context) throws Exception {
 		FileSystem fileSystem = FileSystem.get(new URI(path));
 
 		checkpointLock = context.getCheckpointLock();
@@ -146,104 +154,61 @@ public class ContinuousFileMonitoringFunction<OUT>
 		}
 	}
 
-	private void monitorDirAndForwardSplits(FileSystem fs, SourceContext<FileInputSplit> context) throws IOException, JobException {
-		assert (Thread.holdsLock(checkpointLock));
-
-		List<Tuple2<Long, List<FileInputSplit>>> splitsByModTime = getInputSplitSortedOnModTime(fs);
-
-		Iterator<Tuple2<Long, List<FileInputSplit>>> it = splitsByModTime.iterator();
-		while (it.hasNext()) {
-			forwardSplits(it.next(), context);
-			it.remove();
-		}
-	}
-
-	private void forwardSplits(Tuple2<Long, List<FileInputSplit>> splitsToFwd, SourceContext<FileInputSplit> context) {
+	private void monitorDirAndForwardSplits(FileSystem fs,
+											SourceContext<TimestampedFileInputSplit> context) throws IOException {
 		assert (Thread.holdsLock(checkpointLock));
 
-		Long modTime = splitsToFwd.f0;
-		List<FileInputSplit> splits = splitsToFwd.f1;
+		Map<Path, FileStatus> eligibleFiles = listEligibleFiles(fs);
+		Map<Long, List<TimestampedFileInputSplit>> splitsSortedByModTime = getInputSplitsSortedByModTime(eligibleFiles);
 
-		Iterator<FileInputSplit> it = splits.iterator();
-		while (it.hasNext()) {
-			FileInputSplit split = it.next();
-			processSplit(split, context);
-			it.remove();
-		}
-
-		// update the global modification time
-		if (modTime >= globalModificationTime) {
-			globalModificationTime = modTime;
-		}
-	}
-
-	private void processSplit(FileInputSplit split, SourceContext<FileInputSplit> context) {
-		LOG.info("Forwarding split: " + split);
-		context.collect(split);
-	}
-
-	private List<Tuple2<Long, List<FileInputSplit>>> getInputSplitSortedOnModTime(FileSystem fileSystem) throws IOException {
-		List<FileStatus> eligibleFiles = listEligibleFiles(fileSystem);
-		if (eligibleFiles.isEmpty()) {
-			return new ArrayList<>();
-		}
-
-		Map<Long, List<FileInputSplit>> splitsToForward = getInputSplits(eligibleFiles);
-		List<Tuple2<Long, List<FileInputSplit>>> sortedSplitsToForward = new ArrayList<>();
-
-		for (Map.Entry<Long, List<FileInputSplit>> entry : splitsToForward.entrySet()) {
-			sortedSplitsToForward.add(new Tuple2<>(entry.getKey(), entry.getValue()));
-		}
-
-		Collections.sort(sortedSplitsToForward, new Comparator<Tuple2<Long, List<FileInputSplit>>>() {
-			@Override
-			public int compare(Tuple2<Long, List<FileInputSplit>> o1, Tuple2<Long, List<FileInputSplit>> o2) {
-				return (int) (o1.f0 - o2.f0);
+		for (Map.Entry<Long, List<TimestampedFileInputSplit>> splits: splitsSortedByModTime.entrySet()) {
+			long modificationTime = splits.getKey();
+			for (TimestampedFileInputSplit split: splits.getValue()) {
+				LOG.info("Forwarding split: " + split);
+				context.collect(split);
 			}
-		});
-
-		return sortedSplitsToForward;
+			// update the global modification time
+			globalModificationTime = Math.max(globalModificationTime, modificationTime);
+		}
 	}
 
 	/**
-	 * Creates the input splits for the path to be forwarded to the downstream tasks of the
-	 * {@link ContinuousFileReaderOperator}. Those tasks are going to read their contents for further
-	 * processing. Splits belonging to files in the {@code eligibleFiles} list are the ones
-	 * that are shipped for further processing.
+	 * Creates the input splits to be forwarded to the downstream tasks of the
+	 * {@link ContinuousFileReaderOperator}. Splits are sorted <b>by modification time</b> before
+	 * being forwarded and only splits belonging to files in the {@code eligibleFiles}
+	 * list will be processed.
 	 * @param eligibleFiles The files to process.
 	 */
-	private Map<Long, List<FileInputSplit>> getInputSplits(List<FileStatus> eligibleFiles) throws IOException {
+	private Map<Long, List<TimestampedFileInputSplit>> getInputSplitsSortedByModTime(
+				Map<Path, FileStatus> eligibleFiles) throws IOException {
+
+		Map<Long, List<TimestampedFileInputSplit>> splitsByModTime = new TreeMap<>();
 		if (eligibleFiles.isEmpty()) {
-			return new HashMap<>();
+			return splitsByModTime;
 		}
 
-		FileInputSplit[] inputSplits = format.createInputSplits(readerParallelism);
-
-		Map<Long, List<FileInputSplit>> splitsPerFile = new HashMap<>();
-		for (FileInputSplit split: inputSplits) {
-			for (FileStatus file: eligibleFiles) {
-				if (file.getPath().equals(split.getPath())) {
-					Long modTime = file.getModificationTime();
-
-					List<FileInputSplit> splitsToForward = splitsPerFile.get(modTime);
-					if (splitsToForward == null) {
-						splitsToForward = new LinkedList<>();
-						splitsPerFile.put(modTime, splitsToForward);
-					}
-					splitsToForward.add(split);
-					break;
+		for (FileInputSplit split: format.createInputSplits(readerParallelism)) {
+			FileStatus fileStatus = eligibleFiles.get(split.getPath());
+			if (fileStatus != null) {
+				Long modTime = fileStatus.getModificationTime();
+				List<TimestampedFileInputSplit> splitsToForward = splitsByModTime.get(modTime);
+				if (splitsToForward == null) {
+					splitsToForward = new ArrayList<>();
+					splitsByModTime.put(modTime, splitsToForward);
 				}
+				splitsToForward.add(new TimestampedFileInputSplit(
+					modTime, split.getSplitNumber(), split.getPath(),
+					split.getStart(), split.getLength(), split.getHostnames()));
 			}
 		}
-		return splitsPerFile;
+		return splitsByModTime;
 	}
 
 	/**
-	 * Returns the files that have data to be processed. This method returns the
-	 * Paths to the aforementioned files. It is up to the {@link #processSplit(FileInputSplit, SourceContext)}
-	 * method to decide which parts of the file to be processed, and forward them downstream.
+	 * Returns the paths of the files not yet processed.
+	 * @param fileSystem The filesystem where the monitored directory resides.
 	 */
-	private List<FileStatus> listEligibleFiles(FileSystem fileSystem) throws IOException {
+	private Map<Path, FileStatus> listEligibleFiles(FileSystem fileSystem) throws IOException {
 
 		final FileStatus[] statuses;
 		try {
@@ -251,20 +216,20 @@ public class ContinuousFileMonitoringFunction<OUT>
 		} catch (IOException e) {
 			// we may run into an IOException if files are moved while listing their status
 			// delay the check for eligible files in this case
-			return Collections.emptyList();
+			return Collections.emptyMap();
 		}
 
 		if (statuses == null) {
 			LOG.warn("Path does not exist: {}", path);
-			return Collections.emptyList();
+			return Collections.emptyMap();
 		} else {
-			List<FileStatus> files = new ArrayList<>();
+			Map<Path, FileStatus> files = new HashMap<>();
 			// handle the new files
 			for (FileStatus status : statuses) {
 				Path filePath = status.getPath();
 				long modificationTime = status.getModificationTime();
 				if (!shouldIgnore(filePath, modificationTime)) {
-					files.add(status);
+					files.put(filePath, status);
 				}
 			}
 			return files;
@@ -273,19 +238,19 @@ public class ContinuousFileMonitoringFunction<OUT>
 
 	/**
 	 * Returns {@code true} if the file is NOT to be processed further.
-	 * This happens in the following cases:
-	 *
-	 * If the user-specified path filtering method returns {@code true} for the file,
-	 * or if the modification time of the file is smaller than the {@link #globalModificationTime}, which
-	 * is the time of the most recent modification found in any of the already processed files.
+	 * This happens if the modification time of the file is smaller than
+	 * the {@link #globalModificationTime}.
+	 * @param filePath the path of the file to check.
+	 * @param modificationTime the modification time of the file.
 	 */
 	private boolean shouldIgnore(Path filePath, long modificationTime) {
 		assert (Thread.holdsLock(checkpointLock));
 		boolean shouldIgnore = modificationTime <= globalModificationTime;
-		if (shouldIgnore) {
-			LOG.debug("Ignoring " + filePath + ", with mod time= " + modificationTime + " and global mod time= " + globalModificationTime);
+		if (shouldIgnore && LOG.isDebugEnabled()) {
+			LOG.debug("Ignoring " + filePath + ", with mod time= " + modificationTime +
+				" and global mod time= " + globalModificationTime);
 		}
-		return  shouldIgnore;
+		return shouldIgnore;
 	}
 
 	@Override
@@ -295,7 +260,10 @@ public class ContinuousFileMonitoringFunction<OUT>
 			globalModificationTime = Long.MAX_VALUE;
 			isRunning = false;
 		}
-		LOG.info("Closed File Monitoring Source.");
+
+		if (LOG.isDebugEnabled()) {
+			LOG.debug("Closed File Monitoring Source for path: " + path + ".");
+		}
 	}
 
 	@Override
@@ -316,7 +284,7 @@ public class ContinuousFileMonitoringFunction<OUT>
 
 	@Override
 	public Long snapshotState(long checkpointId, long checkpointTimestamp) throws Exception {
-		return globalModificationTime;
+		return this.globalModificationTime;
 	}
 
 	@Override
diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/source/ContinuousFileReaderOperator.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/source/ContinuousFileReaderOperator.java
index 2f0a16a57e8..c8e9846601a 100644
--- a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/source/ContinuousFileReaderOperator.java
+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/source/ContinuousFileReaderOperator.java
@@ -22,11 +22,9 @@ import org.apache.flink.api.common.io.CheckpointableInputFormat;
 import org.apache.flink.api.common.io.FileInputFormat;
 import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.api.common.typeutils.TypeSerializer;
-import org.apache.flink.api.java.tuple.Tuple3;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.core.fs.FSDataInputStream;
 import org.apache.flink.core.fs.FSDataOutputStream;
-import org.apache.flink.core.fs.FileInputSplit;
 import org.apache.flink.metrics.Counter;
 import org.apache.flink.streaming.api.operators.StreamCheckpointedOperator;
 import org.apache.flink.streaming.api.TimeCharacteristic;
@@ -43,44 +41,42 @@ import java.io.IOException;
 import java.io.ObjectInputStream;
 import java.io.ObjectOutputStream;
 import java.io.Serializable;
-import java.util.ArrayDeque;
 import java.util.ArrayList;
+import java.util.Comparator;
 import java.util.List;
+import java.util.PriorityQueue;
 import java.util.Queue;
 
+import static org.apache.flink.streaming.api.functions.source.TimestampedFileInputSplit.EOS;
 import static org.apache.flink.util.Preconditions.checkState;
 import static org.apache.flink.util.Preconditions.checkNotNull;
 
 /**
- * This is the operator that reads the {@link FileInputSplit FileInputSplits} received from
- * the preceding {@link ContinuousFileMonitoringFunction}. This operator can have parallelism
- * greater than 1, contrary to the {@link ContinuousFileMonitoringFunction} which has
- * a parallelism of 1.
+ * The operator that reads the {@link TimestampedFileInputSplit splits} received from the preceding
+ * {@link ContinuousFileMonitoringFunction}. Contrary to the {@link ContinuousFileMonitoringFunction}
+ * which has a parallelism of 1, this operator can have DOP > 1.
  * <p/>
- * This operator will receive the split descriptors, put them in a queue, and have another
- * thread read the actual data from the split. This architecture allows the separation of the
- * reading thread, from the one emitting the checkpoint barriers, thus removing any potential
+ * As soon as a split descriptor is received, it is put in a queue, and have another
+ * thread read the actual data of the split. This architecture allows the separation of the
+ * reading thread from the one emitting the checkpoint barriers, thus removing any potential
  * back-pressure.
  */
 @Internal
-public class ContinuousFileReaderOperator<OUT, S extends Serializable> extends AbstractStreamOperator<OUT>
-	implements OneInputStreamOperator<FileInputSplit, OUT>, OutputTypeConfigurable<OUT>, StreamCheckpointedOperator {
+public class ContinuousFileReaderOperator<OUT> extends AbstractStreamOperator<OUT>
+	implements OneInputStreamOperator<TimestampedFileInputSplit, OUT>, OutputTypeConfigurable<OUT>, StreamCheckpointedOperator {
 
 	private static final long serialVersionUID = 1L;
 
 	private static final Logger LOG = LoggerFactory.getLogger(ContinuousFileReaderOperator.class);
 
-	/** A value that serves as a kill-pill to stop the reading thread when no more splits remain. */
-	private static final FileInputSplit EOS = new FileInputSplit(-1, null, -1, -1, null);
-
 	private FileInputFormat<OUT> format;
 	private TypeSerializer<OUT> serializer;
 
 	private transient Object checkpointLock;
 
-	private transient SplitReader<S, OUT> reader;
+	private transient SplitReader<OUT> reader;
 	private transient SourceFunction.SourceContext<OUT> readerContext;
-	private Tuple3<List<FileInputSplit>, FileInputSplit, S> readerState;
+	private List<TimestampedFileInputSplit> restoredReaderState;
 
 	public ContinuousFileReaderOperator(FileInputFormat<OUT> format) {
 		this.format = checkNotNull(format);
@@ -110,13 +106,13 @@ public class ContinuousFileReaderOperator<OUT, S extends Serializable> extends A
 			timeCharacteristic, getProcessingTimeService(), checkpointLock, output, watermarkInterval);
 
 		// and initialize the split reading thread
-		this.reader = new SplitReader<>(format, serializer, readerContext, checkpointLock, readerState);
-		this.readerState = null;
+		this.reader = new SplitReader<>(format, serializer, readerContext, checkpointLock, restoredReaderState);
+		this.restoredReaderState = null;
 		this.reader.start();
 	}
 
 	@Override
-	public void processElement(StreamRecord<FileInputSplit> element) throws Exception {
+	public void processElement(StreamRecord<TimestampedFileInputSplit> element) throws Exception {
 		reader.addSplit(element.getValue());
 	}
 
@@ -157,7 +153,7 @@ public class ContinuousFileReaderOperator<OUT, S extends Serializable> extends A
 		}
 		reader = null;
 		readerContext = null;
-		readerState = null;
+		restoredReaderState = null;
 		format = null;
 		serializer = null;
 	}
@@ -190,7 +186,7 @@ public class ContinuousFileReaderOperator<OUT, S extends Serializable> extends A
 		output.close();
 	}
 
-	private class SplitReader<S extends Serializable, OT> extends Thread {
+	private class SplitReader<OT> extends Thread {
 
 		private volatile boolean isRunning;
 
@@ -200,44 +196,39 @@ public class ContinuousFileReaderOperator<OUT, S extends Serializable> extends A
 		private final Object checkpointLock;
 		private final SourceFunction.SourceContext<OT> readerContext;
 
-		private final Queue<FileInputSplit> pendingSplits;
-
-		private FileInputSplit currentSplit = null;
+		private final Queue<TimestampedFileInputSplit> pendingSplits;
 
-		private S restoredFormatState = null;
+		private TimestampedFileInputSplit currentSplit;
 
-		private volatile boolean isSplitOpen = false;
+		private volatile boolean isSplitOpen;
 
 		private SplitReader(FileInputFormat<OT> format,
 					TypeSerializer<OT> serializer,
 					SourceFunction.SourceContext<OT> readerContext,
 					Object checkpointLock,
-					Tuple3<List<FileInputSplit>, FileInputSplit, S> restoredState) {
+					List<TimestampedFileInputSplit> restoredState) {
 
 			this.format = checkNotNull(format, "Unspecified FileInputFormat.");
 			this.serializer = checkNotNull(serializer, "Unspecified Serializer.");
 			this.readerContext = checkNotNull(readerContext, "Unspecified Reader Context.");
 			this.checkpointLock = checkNotNull(checkpointLock, "Unspecified checkpoint lock.");
 
-			this.pendingSplits = new ArrayDeque<>();
 			this.isRunning = true;
 
-			// this is the case where a task recovers from a previous failed attempt
-			if (restoredState != null) {
-				List<FileInputSplit> pending = restoredState.f0;
-				FileInputSplit current = restoredState.f1;
-				S formatState = restoredState.f2;
-
-				for (FileInputSplit split : pending) {
-					pendingSplits.add(split);
+			this.pendingSplits = new PriorityQueue<>(10, new Comparator<TimestampedFileInputSplit>() {
+				@Override
+				public int compare(TimestampedFileInputSplit o1, TimestampedFileInputSplit o2) {
+					return o1.compareTo(o2);
 				}
+			});
 
-				this.currentSplit = current;
-				this.restoredFormatState = formatState;
+			// this is the case where a task recovers from a previous failed attempt
+			if (restoredState != null) {
+				this.pendingSplits.addAll(restoredState);
 			}
 		}
 
-		private void addSplit(FileInputSplit split) {
+		private void addSplit(TimestampedFileInputSplit split) {
 			checkNotNull(split, "Cannot insert a null value in the pending splits queue.");
 			synchronized (checkpointLock) {
 				this.pendingSplits.add(split);
@@ -259,43 +250,32 @@ public class ContinuousFileReaderOperator<OUT, S extends Serializable> extends A
 
 					synchronized (checkpointLock) {
 
-						if (this.currentSplit != null) {
-
-							if (currentSplit.equals(EOS)) {
-								isRunning = false;
-								break;
-							}
-
-							if (this.format instanceof CheckpointableInputFormat && restoredFormatState != null) {
-
-								@SuppressWarnings("unchecked")
-								CheckpointableInputFormat<FileInputSplit, S> checkpointableFormat =
-										(CheckpointableInputFormat<FileInputSplit, S>) this.format;
-
-								checkpointableFormat.reopen(currentSplit, restoredFormatState);
-							} else {
-								// this is the case of a non-checkpointable input format that will reprocess the last split.
-								LOG.info("Format " + this.format.getClass().getName() + " does not support checkpointing.");
-								format.open(currentSplit);
-							}
-							// reset the restored state to null for the next iteration
-							this.restoredFormatState = null;
-						} else {
-
-							// get the next split to read.
+						if (currentSplit == null) {
 							currentSplit = this.pendingSplits.poll();
-
 							if (currentSplit == null) {
 								checkpointLock.wait(50);
 								continue;
 							}
+						}
 
-							if (currentSplit.equals(EOS)) {
-								isRunning = false;
-								break;
-							}
+						if (currentSplit.equals(EOS)) {
+							isRunning = false;
+							break;
+						}
+
+						if (this.format instanceof CheckpointableInputFormat && currentSplit.getSplitState() != null) {
+							// recovering after a node failure with an input
+							// format that supports resetting the offset
+							((CheckpointableInputFormat<TimestampedFileInputSplit, Serializable>) this.format).
+								reopen(currentSplit, currentSplit.getSplitState());
+						} else {
+							// we either have a new split, or we recovered from a node
+							// failure but the input format does not support resetting the offset.
 							this.format.open(currentSplit);
 						}
+
+						// reset the restored state to null for the next iteration
+						this.currentSplit.resetSplitState();
 						this.isSplitOpen = true;
 					}
 
@@ -348,34 +328,17 @@ public class ContinuousFileReaderOperator<OUT, S extends Serializable> extends A
 			}
 		}
 
-		private Tuple3<List<FileInputSplit>, FileInputSplit, S> getReaderState() throws IOException {
-			List<FileInputSplit> snapshot = new ArrayList<>(this.pendingSplits.size());
-			for (FileInputSplit split: this.pendingSplits) {
-				snapshot.add(split);
-			}
-
-			// remove the current split from the list if inside.
-			if (this.currentSplit != null && this.currentSplit.equals(pendingSplits.peek())) {
-				this.pendingSplits.remove();
-			}
-
-			if (this.currentSplit != null) {
-				if (this.format instanceof CheckpointableInputFormat) {
-					@SuppressWarnings("unchecked")
-					CheckpointableInputFormat<FileInputSplit, S> checkpointableFormat =
-							(CheckpointableInputFormat<FileInputSplit, S>) this.format;
-
-					S formatState = this.isSplitOpen ?
-							checkpointableFormat.getCurrentState() :
-							restoredFormatState;
-					return new Tuple3<>(snapshot, currentSplit, formatState);
-				} else {
-					LOG.info("The format does not support checkpointing. The current input split will be re-read from start upon recovery.");
-					return new Tuple3<>(snapshot, currentSplit, null);
+		private List<TimestampedFileInputSplit> getReaderState() throws IOException {
+			List<TimestampedFileInputSplit> snapshot = new ArrayList<>(this.pendingSplits.size());
+			if (currentSplit != null ) {
+				if (this.format instanceof CheckpointableInputFormat && this.isSplitOpen) {
+					Serializable formatState = ((CheckpointableInputFormat<TimestampedFileInputSplit, Serializable>) this.format).getCurrentState();
+					this.currentSplit.setSplitState(formatState);
 				}
-			} else {
-				return new Tuple3<>(snapshot, null, null);
+				snapshot.add(this.currentSplit);
 			}
+			snapshot.addAll(this.pendingSplits);
+			return snapshot;
 		}
 
 		public void cancel() {
@@ -389,45 +352,27 @@ public class ContinuousFileReaderOperator<OUT, S extends Serializable> extends A
 	public void snapshotState(FSDataOutputStream os, long checkpointId, long timestamp) throws Exception {
 		final ObjectOutputStream oos = new ObjectOutputStream(os);
 
-		Tuple3<List<FileInputSplit>, FileInputSplit, S> readerState = this.reader.getReaderState();
-		List<FileInputSplit> pendingSplits = readerState.f0;
-		FileInputSplit currSplit = readerState.f1;
-		S formatState = readerState.f2;
-
-		// write the current split
-		oos.writeObject(currSplit);
-		oos.writeInt(pendingSplits.size());
-		for (FileInputSplit split : pendingSplits) {
+		List<TimestampedFileInputSplit> readerState = this.reader.getReaderState();
+		oos.writeInt(readerState.size());
+		for (TimestampedFileInputSplit split : readerState) {
 			oos.writeObject(split);
 		}
-
-		// write the state of the reading channel
-		oos.writeObject(formatState);
 		oos.flush();
 	}
 
 	@Override
 	public void restoreState(FSDataInputStream is) throws Exception {
-		final ObjectInputStream ois = new ObjectInputStream(is);
 
-		// read the split that was being read
-		FileInputSplit currSplit = (FileInputSplit) ois.readObject();
+		checkState(this.restoredReaderState == null,
+			"The reader state has already been initialized.");
+
+		final ObjectInputStream ois = new ObjectInputStream(is);
 
-		// read the pending splits list
-		List<FileInputSplit> pendingSplits = new ArrayList<>();
 		int noOfSplits = ois.readInt();
+		List<TimestampedFileInputSplit> pendingSplits = new ArrayList<>(noOfSplits);
 		for (int i = 0; i < noOfSplits; i++) {
-			FileInputSplit split = (FileInputSplit) ois.readObject();
-			pendingSplits.add(split);
+			pendingSplits.add((TimestampedFileInputSplit) ois.readObject());
 		}
-
-		// read the state of the format
-		@SuppressWarnings("unchecked")
-		S formatState = (S) ois.readObject();
-
-		// set the whole reader state for the open() to find.
-		checkState(this.readerState == null, "The reader state has already been initialized.");
-
-		this.readerState = new Tuple3<>(pendingSplits, currSplit, formatState);
+		this.restoredReaderState = pendingSplits;
 	}
 }
diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/source/FileProcessingMode.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/source/FileProcessingMode.java
index cdbeb2b98a1..f8c4fba3c4c 100644
--- a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/source/FileProcessingMode.java
+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/source/FileProcessingMode.java
@@ -20,12 +20,15 @@ package org.apache.flink.streaming.api.functions.source;
 import org.apache.flink.annotation.PublicEvolving;
 
 /**
- * Specifies when the computation of the {@link ContinuousFileMonitoringFunction}
- * will be triggered.
+ * The mode in which the {@link ContinuousFileMonitoringFunction} operates.
+ * This can be either {@link #PROCESS_ONCE} or {@link #PROCESS_CONTINUOUSLY}.
  */
 @PublicEvolving
 public enum FileProcessingMode {
 
-	PROCESS_ONCE,				// Processes the current content of a file/path only ONCE, and stops monitoring.
-	PROCESS_CONTINUOUSLY		// Reprocesses the whole file when new data is appended.
+	/** Processes the current contents of the path and exits. */
+	PROCESS_ONCE,
+
+	/** Periodically scans the path for new data. */
+	PROCESS_CONTINUOUSLY
 }
diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/source/TimestampedFileInputSplit.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/source/TimestampedFileInputSplit.java
new file mode 100644
index 00000000000..323b3aba6d0
--- /dev/null
+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/source/TimestampedFileInputSplit.java
@@ -0,0 +1,137 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.api.functions.source;
+
+import org.apache.flink.core.fs.FileInputSplit;
+import org.apache.flink.core.fs.Path;
+import org.apache.flink.util.Preconditions;
+
+import java.io.Serializable;
+
+/**
+ * An extended {@link FileInputSplit} that also includes information about:
+ * <ul>
+ *     <li>The modification time of the file this split belongs to.</li>
+ *     <li>When checkpointing, the state of the split at the moment of the checkpoint.</li>
+ * </ul>
+ * This class is used by the {@link ContinuousFileMonitoringFunction} and the
+ * {@link ContinuousFileReaderOperator} to perform continuous file processing.
+ * */
+public class TimestampedFileInputSplit extends FileInputSplit implements Comparable<TimestampedFileInputSplit>{
+
+	/** The modification time of the file this split belongs to. */
+	private final long modificationTime;
+
+	/**
+	 * The state of the split. This information is used when
+	 * restoring from a checkpoint and allows to resume reading the
+	 * underlying file from the point we left off.
+	 * */
+	private Serializable splitState;
+
+	/** A special {@link TimestampedFileInputSplit} signaling the end of the stream of splits.*/
+	public static final TimestampedFileInputSplit EOS =
+		new TimestampedFileInputSplit(Long.MIN_VALUE, -1, null, -1, -1, null);
+
+	/**
+	 * Creates a {@link TimestampedFileInputSplit} based on the file modification time and
+	 * the rest of the information of the {@link FileInputSplit}, as returned by the
+	 * underlying filesystem.
+	 *
+	 * @param modificationTime the modification file of the file this split belongs to
+	 * @param num    the number of this input split
+	 * @param file   the file name
+	 * @param start  the position of the first byte in the file to process
+	 * @param length the number of bytes in the file to process (-1 is flag for "read whole file")
+	 * @param hosts  the list of hosts containing the block, possibly {@code null}
+	 */
+	public TimestampedFileInputSplit(long modificationTime, int num, Path file, long start, long length, String[] hosts) {
+		super(num, file, start, length, hosts);
+
+		Preconditions.checkArgument(modificationTime >= 0 || modificationTime == Long.MIN_VALUE,
+			"Invalid File Split Modification Time: "+ modificationTime +".");
+
+		this.modificationTime = modificationTime;
+	}
+
+	/**
+	 * Sets the state of the split. This information is used when
+	 * restoring from a checkpoint and allows to resume reading the
+	 * underlying file from the point we left off.
+	 * <p>
+	 * This is applicable to {@link org.apache.flink.api.common.io.FileInputFormat FileInputFormats}
+	 * that implement the {@link org.apache.flink.api.common.io.CheckpointableInputFormat
+	 * CheckpointableInputFormat} interface.
+	 * */
+	public void setSplitState(Serializable state) {
+		this.splitState = state;
+	}
+
+	/**
+	 * Sets the state of the split to {@code null}.
+	 */
+	public void resetSplitState() {
+		this.setSplitState(null);
+	}
+
+	/** @return the state of the split. */
+	public Serializable getSplitState() {
+		return this.splitState;
+	}
+
+	/** @return The modification time of the file this split belongs to. */
+	public long getModificationTime() {
+		return this.modificationTime;
+	}
+
+	@Override
+	public int compareTo(TimestampedFileInputSplit o) {
+		long modTimeComp = this.modificationTime - o.modificationTime;
+		if (modTimeComp != 0L) {
+			// we cannot just cast the modTimeComp to int
+			// because it may overflow
+			return modTimeComp > 0 ? 1 : -1;
+		}
+
+		int pathComp = this.getPath().compareTo(o.getPath());
+		return pathComp != 0 ? pathComp : this.getSplitNumber() - o.getSplitNumber();
+	}
+
+	@Override
+	public boolean equals(Object o) {
+		if (this == o) {
+			return true;
+		} else if (o != null && o instanceof TimestampedFileInputSplit && super.equals(o)) {
+			TimestampedFileInputSplit that = (TimestampedFileInputSplit) o;
+			return this.modificationTime == that.modificationTime;
+		}
+		return false;
+	}
+
+	@Override
+	public int hashCode() {
+		int res = 37 * (int)(this.modificationTime ^ (this.modificationTime >>> 32));
+		return 37 * res + super.hashCode();
+	}
+
+	@Override
+	public String toString() {
+		return "[" + getSplitNumber() + "] " + getPath() +" mod@ "+
+			modificationTime + " : " + getStart() + " + " + getLength();
+	}
+}
diff --git a/flink-tests/src/test/java/org/apache/flink/test/checkpointing/ContinuousFileProcessingCheckpointITCase.java b/flink-tests/src/test/java/org/apache/flink/test/checkpointing/ContinuousFileProcessingCheckpointITCase.java
index a265c0a45d0..0e9b054af64 100644
--- a/flink-tests/src/test/java/org/apache/flink/test/checkpointing/ContinuousFileProcessingCheckpointITCase.java
+++ b/flink-tests/src/test/java/org/apache/flink/test/checkpointing/ContinuousFileProcessingCheckpointITCase.java
@@ -57,18 +57,17 @@ import static org.junit.Assert.fail;
 
 public class ContinuousFileProcessingCheckpointITCase extends StreamFaultToleranceTestBase {
 
-	private static final int NO_OF_FILES = 9;
-	private static final int LINES_PER_FILE = 200;
+	private static final int NO_OF_FILES = 5;
+	private static final int LINES_PER_FILE = 150;
 	private static final int NO_OF_RETRIES = 3;
-	private static final int PARALLELISM = 4;
-	private static final long INTERVAL = 2000;
+	private static final long INTERVAL = 100;
 
 	private static File baseDir;
-	private static org.apache.hadoop.fs.FileSystem fs;
+	private static org.apache.hadoop.fs.FileSystem localFs;
 	private static String localFsURI;
 	private FileCreator fc;
 
-	private static  Map<Integer, List<String>> finalCollectedContent = new HashMap<>();
+	private static  Map<Integer, Set<String>> actualCollectedContent = new HashMap<>();
 
 	@BeforeClass
 	public static void createHDFS() {
@@ -79,7 +78,7 @@ public class ContinuousFileProcessingCheckpointITCase extends StreamFaultToleran
 			org.apache.hadoop.conf.Configuration hdConf = new org.apache.hadoop.conf.Configuration();
 
 			localFsURI = "file:///" + baseDir +"/";
-			fs = new org.apache.hadoop.fs.Path(localFsURI).getFileSystem(hdConf);
+			localFs = new org.apache.hadoop.fs.Path(localFsURI).getFileSystem(hdConf);
 
 		} catch(Throwable e) {
 			e.printStackTrace();
@@ -100,22 +99,22 @@ public class ContinuousFileProcessingCheckpointITCase extends StreamFaultToleran
 	public void testProgram(StreamExecutionEnvironment env) {
 
 		// set the restart strategy.
-		env.getConfig().setRestartStrategy(
-			RestartStrategies.fixedDelayRestart(NO_OF_RETRIES, 0));
-		env.enableCheckpointing(20);
-		env.setParallelism(PARALLELISM);
+		env.getConfig().setRestartStrategy(RestartStrategies.fixedDelayRestart(NO_OF_RETRIES, 0));
+		env.enableCheckpointing(10);
 
 		// create and start the file creating thread.
 		fc = new FileCreator();
 		fc.start();
 
 		// create the monitoring source along with the necessary readers.
-		TestingSinkFunction sink = new TestingSinkFunction();
 		TextInputFormat format = new TextInputFormat(new org.apache.flink.core.fs.Path(localFsURI));
 		format.setFilesFilter(FilePathFilter.createDefaultFilter());
+
 		DataStream<String> inputStream = env.readFile(format, localFsURI,
 			FileProcessingMode.PROCESS_CONTINUOUSLY, INTERVAL);
 
+		TestingSinkFunction sink = new TestingSinkFunction();
+
 		inputStream.flatMap(new FlatMapFunction<String, String>() {
 			@Override
 			public void flatMap(String value, Collector<String> out) throws Exception {
@@ -126,12 +125,17 @@ public class ContinuousFileProcessingCheckpointITCase extends StreamFaultToleran
 
 	@Override
 	public void postSubmit() throws Exception {
-		Map<Integer, List<String>> collected = finalCollectedContent;
+
+		// be sure that the file creating thread is done.
+		fc.join();
+
+		Map<Integer, Set<String>> collected = actualCollectedContent;
 		Assert.assertEquals(collected.size(), fc.getFileContent().size());
+
 		for (Integer fileIdx: fc.getFileContent().keySet()) {
 			Assert.assertTrue(collected.keySet().contains(fileIdx));
 
-			List<String> cntnt = collected.get(fileIdx);
+			List<String> cntnt = new ArrayList<>(collected.get(fileIdx));
 			Collections.sort(cntnt, new Comparator<String>() {
 				@Override
 				public int compare(String o1, String o2) {
@@ -147,105 +151,34 @@ public class ContinuousFileProcessingCheckpointITCase extends StreamFaultToleran
 		}
 
 		collected.clear();
-		finalCollectedContent.clear();
+		actualCollectedContent.clear();
 		fc.clean();
 	}
 
 	private int getLineNo(String line) {
 		String[] tkns = line.split("\\s");
-		Assert.assertTrue(tkns.length == 6);
 		return Integer.parseInt(tkns[tkns.length - 1]);
 	}
 
-	// --------------------------------------------------------------------------------------------
-	//  Custom Functions
-	// --------------------------------------------------------------------------------------------
-
-	// -------------------------			FILE CREATION			-------------------------------
-
-	/**
-	 * A separate thread creating {@link #NO_OF_FILES} files, one file every {@link #INTERVAL} milliseconds.
-	 * It serves for testing the file monitoring functionality of the {@link ContinuousFileMonitoringFunction}.
-	 * The files are filled with data by the {@link #fillWithData(String, String, int, String)} method.
-	 * */
-	private class FileCreator extends Thread {
-
-		private final Set<Path> filesCreated = new HashSet<>();
-		private final Map<Integer, String> fileContents = new HashMap<>();
-
-		public void run() {
-			try {
-				for(int i = 0; i < NO_OF_FILES; i++) {
-					Tuple2<org.apache.hadoop.fs.Path, String> file =
-						fillWithData(localFsURI, "file", i, "This is test line.");
-					filesCreated.add(file.f0);
-					fileContents.put(i, file.f1);
-
-					Thread.sleep((int) (INTERVAL / (3.0/2)));
-				}
-			} catch (IOException | InterruptedException e) {
-				e.printStackTrace();
-			}
-		}
-
-		void clean() throws IOException {
-			assert (fs != null);
-			for (org.apache.hadoop.fs.Path path: filesCreated) {
-				fs.delete(path, false);
-			}
-			fileContents.clear();
-		}
-
-		Map<Integer, String> getFileContent() {
-			return this.fileContents;
-		}
-	}
-
-	/**
-	 * Fill the file with content and put the content in the {@code hdPathContents} list.
-	 * */
-	private Tuple2<Path, String> fillWithData(
-		String base, String fileName, int fileIdx, String sampleLine) throws IOException {
-
-		assert (fs != null);
-
-		org.apache.hadoop.fs.Path file = new org.apache.hadoop.fs.Path(base + "/" + fileName + fileIdx);
-
-		org.apache.hadoop.fs.Path tmp = new org.apache.hadoop.fs.Path(base + "/." + fileName + fileIdx);
-		FSDataOutputStream stream = fs.create(tmp);
-		StringBuilder str = new StringBuilder();
-		for(int i = 0; i < LINES_PER_FILE; i++) {
-			String line = fileIdx +": "+ sampleLine + " " + i +"\n";
-			str.append(line);
-			stream.write(line.getBytes());
-		}
-		stream.close();
-
-		Assert.assertTrue("Result file present", !fs.exists(file));
-		fs.rename(tmp, file);
-		Assert.assertTrue("No result file present", fs.exists(file));
-		return new Tuple2<>(file, str.toString());
-	}
-
 	// --------------------------			Task Sink			------------------------------
 
 	private static class TestingSinkFunction extends RichSinkFunction<String>
 		implements Checkpointed<Tuple2<Long, Map<Integer, Set<String>>>>, CheckpointListener {
 
-		private static volatile boolean hasFailed = false;
+		private boolean hasFailed;
 
-		private volatile int numSuccessfulCheckpoints;
-
-		private long count;
+		private volatile boolean hasSuccessfulCheckpoints;
 
 		private long elementsToFailure;
 
-		private long elementCounter = 0;
+		private long elementCounter;
 
-		private  Map<Integer, Set<String>> collectedContent = new HashMap<>();
+		private Map<Integer, Set<String>> actualContent = new HashMap<>();
 
 		TestingSinkFunction() {
 			hasFailed = false;
+			elementCounter = 0;
+			hasSuccessfulCheckpoints = false;
 		}
 
 		@Override
@@ -257,74 +190,157 @@ public class ContinuousFileProcessingCheckpointITCase extends StreamFaultToleran
 			long failurePosMax = (long) (0.7 * LINES_PER_FILE);
 
 			elementsToFailure = (new Random().nextLong() % (failurePosMax - failurePosMin)) + failurePosMin;
-
-			if (elementCounter >= NO_OF_FILES * LINES_PER_FILE) {
-				finalCollectedContent = new HashMap<>();
-				for (Map.Entry<Integer, Set<String>> result: collectedContent.entrySet()) {
-					finalCollectedContent.put(result.getKey(), new ArrayList<>(result.getValue()));
-				}
-				throw new SuccessException();
-			}
-		}
-
-		@Override
-		public void close() {
-			try {
-				super.close();
-			} catch (Exception e) {
-				e.printStackTrace();
-			}
 		}
 
 		@Override
 		public void invoke(String value) throws Exception {
-			int fileIdx = Character.getNumericValue(value.charAt(0));
+			int fileIdx = getFileIdx(value);
 
-			Set<String> content = collectedContent.get(fileIdx);
+			Set<String> content = actualContent.get(fileIdx);
 			if (content == null) {
 				content = new HashSet<>();
-				collectedContent.put(fileIdx, content);
+				actualContent.put(fileIdx, content);
 			}
 
+			// detect duplicate lines.
 			if (!content.add(value + "\n")) {
 				fail("Duplicate line: " + value);
 				System.exit(0);
 			}
 
-
 			elementCounter++;
+
+			// this is termination
 			if (elementCounter >= NO_OF_FILES * LINES_PER_FILE) {
-				finalCollectedContent = new HashMap<>();
-				for (Map.Entry<Integer, Set<String>> result: collectedContent.entrySet()) {
-					finalCollectedContent.put(result.getKey(), new ArrayList<>(result.getValue()));
-				}
+				actualCollectedContent = actualContent;
 				throw new SuccessException();
 			}
 
-			count++;
-			if (!hasFailed) {
-				Thread.sleep(2);
-				if (numSuccessfulCheckpoints >= 1 && count >= elementsToFailure) {
-					hasFailed = true;
-					throw new Exception("Task Failure");
-				}
+			// add some latency so that we have at least one checkpoint in
+			if (!hasFailed && !hasSuccessfulCheckpoints) {
+				Thread.sleep(5);
+			}
+
+			// simulate a node failure
+			if (!hasFailed && hasSuccessfulCheckpoints && elementCounter >= elementsToFailure) {
+				throw new Exception("Task Failure @ elem: " + elementCounter + " / " + elementsToFailure);
+			}
+		}
+
+		@Override
+		public void close() {
+			try {
+				super.close();
+			} catch (Exception e) {
+				e.printStackTrace();
 			}
 		}
 
 		@Override
 		public Tuple2<Long, Map<Integer, Set<String>>> snapshotState(long checkpointId, long checkpointTimestamp) throws Exception {
-			return new Tuple2<>(elementCounter, collectedContent);
+			return new Tuple2<>(elementCounter, actualContent);
 		}
 
 		@Override
 		public void restoreState(Tuple2<Long, Map<Integer, Set<String>>> state) throws Exception {
+			this.hasFailed = true;
 			this.elementCounter = state.f0;
-			this.collectedContent = state.f1;
+			this.actualContent = state.f1;
 		}
 
 		@Override
 		public void notifyCheckpointComplete(long checkpointId) throws Exception {
-			numSuccessfulCheckpoints++;
+			hasSuccessfulCheckpoints = true;
 		}
+
+		private int getFileIdx(String line) {
+			String[] tkns = line.split(":");
+			return Integer.parseInt(tkns[0]);
+		}
+	}
+
+	// -------------------------			FILE CREATION			-------------------------------
+
+	/**
+	 * A separate thread creating {@link #NO_OF_FILES} files, one file every {@link #INTERVAL} milliseconds.
+	 * It serves for testing the file monitoring functionality of the {@link ContinuousFileMonitoringFunction}.
+	 * The files are filled with data by the {@link #fillWithData(String, String, int, String)} method.
+	 * */
+	private class FileCreator extends Thread {
+
+		private final Set<Path> filesCreated = new HashSet<>();
+		private final Map<Integer, String> fileContents = new HashMap<>();
+
+		/** The modification time of the last created file. */
+		private long lastCreatedModTime = Long.MIN_VALUE;
+
+		public void run() {
+			try {
+				for(int i = 0; i < NO_OF_FILES; i++) {
+					Tuple2<org.apache.hadoop.fs.Path, String> tmpFile;
+					long modTime;
+					do {
+
+						// give it some time so that the files have
+						// different modification timestamps.
+						Thread.sleep(50);
+
+						tmpFile = fillWithData(localFsURI, "file", i, "This is test line.");
+
+						modTime = localFs.getFileStatus(tmpFile.f0).getModificationTime();
+						if (modTime <= lastCreatedModTime) {
+							// delete the last created file to recreate it with a different timestamp
+							localFs.delete(tmpFile.f0, false);
+						}
+					} while (modTime <= lastCreatedModTime);
+					lastCreatedModTime = modTime;
+
+					// rename the file
+					org.apache.hadoop.fs.Path file =
+						new org.apache.hadoop.fs.Path(localFsURI + "/file" + i);
+					localFs.rename(tmpFile.f0, file);
+					Assert.assertTrue(localFs.exists(file));
+
+					filesCreated.add(file);
+					fileContents.put(i, tmpFile.f1);
+				}
+			} catch (IOException | InterruptedException e) {
+				e.printStackTrace();
+			}
+		}
+
+		void clean() throws IOException {
+			assert (localFs != null);
+			for (org.apache.hadoop.fs.Path path: filesCreated) {
+				localFs.delete(path, false);
+			}
+			fileContents.clear();
+		}
+
+		Map<Integer, String> getFileContent() {
+			return this.fileContents;
+		}
+	}
+
+	/**
+	 * Fill the file with content and put the content in the {@code hdPathContents} list.
+	 * */
+	private Tuple2<Path, String> fillWithData(
+		String base, String fileName, int fileIdx, String sampleLine) throws IOException, InterruptedException {
+
+		assert (localFs != null);
+
+		org.apache.hadoop.fs.Path tmp =
+			new org.apache.hadoop.fs.Path(base + "/." + fileName + fileIdx);
+
+		FSDataOutputStream stream = localFs.create(tmp);
+		StringBuilder str = new StringBuilder();
+		for(int i = 0; i < LINES_PER_FILE; i++) {
+			String line = fileIdx +": "+ sampleLine + " " + i +"\n";
+			str.append(line);
+			stream.write(line.getBytes());
+		}
+		stream.close();
+		return new Tuple2<>(tmp, str.toString());
 	}
 }
diff --git a/flink-tests/src/test/java/org/apache/flink/test/checkpointing/TimestampedFileInputSplitTest.java b/flink-tests/src/test/java/org/apache/flink/test/checkpointing/TimestampedFileInputSplitTest.java
new file mode 100644
index 00000000000..88bd822bb48
--- /dev/null
+++ b/flink-tests/src/test/java/org/apache/flink/test/checkpointing/TimestampedFileInputSplitTest.java
@@ -0,0 +1,93 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.flink.test.checkpointing;
+
+import org.apache.flink.core.fs.FileInputSplit;
+import org.apache.flink.core.fs.Path;
+import org.apache.flink.streaming.api.functions.source.TimestampedFileInputSplit;
+import org.junit.Assert;
+import org.junit.Test;
+
+public class TimestampedFileInputSplitTest {
+
+	@Test
+	public void testSplitEquality() {
+
+		TimestampedFileInputSplit eos1 = TimestampedFileInputSplit.EOS;
+		TimestampedFileInputSplit eos2 = TimestampedFileInputSplit.EOS;
+
+		Assert.assertEquals(eos1, eos2);
+
+		TimestampedFileInputSplit richFirstSplit =
+			new TimestampedFileInputSplit(10, 2, new Path("test"), 0, 100, null);
+		Assert.assertNotEquals(eos1, richFirstSplit);
+
+		TimestampedFileInputSplit richSecondSplit =
+			new TimestampedFileInputSplit(10, 2, new Path("test"), 0, 100, null);
+		Assert.assertEquals(richFirstSplit, richSecondSplit);
+
+		TimestampedFileInputSplit richModSecondSplit =
+			new TimestampedFileInputSplit(11, 2, new Path("test"), 0, 100, null);
+		Assert.assertNotEquals(richSecondSplit, richModSecondSplit);
+
+		TimestampedFileInputSplit richThirdSplit =
+			new TimestampedFileInputSplit(10, 2, new Path("test/test1"), 0, 100, null);
+		Assert.assertEquals(richThirdSplit.getModificationTime(), 10);
+		Assert.assertNotEquals(richFirstSplit, richThirdSplit);
+
+		TimestampedFileInputSplit richThirdSplitCopy =
+			new TimestampedFileInputSplit(10, 2, new Path("test/test1"), 0, 100, null);
+		Assert.assertEquals(richThirdSplitCopy, richThirdSplit);
+	}
+
+	@Test
+	public void testSplitComparison() {
+		TimestampedFileInputSplit richFirstSplit =
+			new TimestampedFileInputSplit(10, 3, new Path("test/test1"), 0, 100, null);
+
+		TimestampedFileInputSplit richSecondSplit =
+			new TimestampedFileInputSplit(10, 2, new Path("test/test2"), 0, 100, null);
+
+		TimestampedFileInputSplit richThirdSplit =
+			new TimestampedFileInputSplit(10, 1, new Path("test/test2"), 0, 100, null);
+
+		TimestampedFileInputSplit richForthSplit =
+			new TimestampedFileInputSplit(11, 0, new Path("test/test3"), 0, 100, null);
+
+		// lexicographically on the path order
+		Assert.assertTrue(richFirstSplit.compareTo(richSecondSplit) < 0);
+		Assert.assertTrue(richFirstSplit.compareTo(richThirdSplit) < 0);
+
+		// same mod time, same file so smaller split number first
+		Assert.assertTrue(richThirdSplit.compareTo(richSecondSplit) < 0);
+
+		// smaller modification time first
+		Assert.assertTrue(richThirdSplit.compareTo(richForthSplit) < 0);
+	}
+
+	@Test
+	public void testIllegalArgument() {
+		try {
+			new TimestampedFileInputSplit(-10, 2, new Path("test"), 0, 100, null); // invalid modification time
+		} catch (Exception e) {
+			if (!(e instanceof IllegalArgumentException)) {
+				Assert.fail(e.getMessage());
+			}
+		}
+	}
+}
