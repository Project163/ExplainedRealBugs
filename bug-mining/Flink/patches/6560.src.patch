diff --git a/flink-core/src/main/java/org/apache/flink/api/common/BatchShuffleMode.java b/flink-core/src/main/java/org/apache/flink/api/common/BatchShuffleMode.java
index 1f8d0d05006..f6575d8ebb9 100644
--- a/flink-core/src/main/java/org/apache/flink/api/common/BatchShuffleMode.java
+++ b/flink-core/src/main/java/org/apache/flink/api/common/BatchShuffleMode.java
@@ -69,11 +69,27 @@ public enum BatchShuffleMode implements DescribedEnum {
      * <p>Downstream can start running anytime, as long as the upstream has started.
      *
      * <p>This adapts the resource usage to whatever is available.
+     *
+     * <p>This type will spill all data to disk to support re-consume.
+     */
+    // TODO remove the annotation and rename this enum constant when hybrid shuffle effort is
+    // finished.
+    @Documentation.ExcludeFromDocumentation
+    WIP_ALL_EXCHANGES_HYBRID_FULL(text("DO NOT USE - This feature is in progress.")),
+
+    /**
+     * *DO NOT USE* - This feature is in progress.
+     *
+     * <p>Downstream can start running anytime, as long as the upstream has started.
+     *
+     * <p>This adapts the resource usage to whatever is available.
+     *
+     * <p>This type will selective spilling data to reduce disk writes as much as possible.
      */
     // TODO remove the annotation and rename this enum constant when hybrid shuffle effort is
     // finished.
     @Documentation.ExcludeFromDocumentation
-    WIP_ALL_EXCHANGES_HYBRID(text("DO NOT USE - This feature is in progress."));
+    WIP_ALL_EXCHANGES_HYBRID_SELECTIVE(text("DO NOT USE - This feature is in progress."));
 
     private final InlineElement description;
 
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResultPartitionFactory.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResultPartitionFactory.java
index 94cbf004109..ec5c69ad477 100755
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResultPartitionFactory.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResultPartitionFactory.java
@@ -214,7 +214,8 @@ public class ResultPartitionFactory {
 
                 partition = blockingPartition;
             }
-        } else if (type == ResultPartitionType.HYBRID) {
+        } else if (type == ResultPartitionType.HYBRID_FULL
+                || type == ResultPartitionType.HYBRID_SELECTIVE) {
             partition =
                     new HsResultPartition(
                             taskNameWithSubtaskAndId,
@@ -231,6 +232,12 @@ public class ResultPartitionFactory {
                             HybridShuffleConfiguration.builder(
                                             numberOfSubpartitions,
                                             batchShuffleReadBufferPool.getNumBuffersPerRequest())
+                                    .setSpillingStrategyType(
+                                            type == ResultPartitionType.HYBRID_FULL
+                                                    ? HybridShuffleConfiguration
+                                                            .SpillingStrategyType.FULL
+                                                    : HybridShuffleConfiguration
+                                                            .SpillingStrategyType.SELECTIVE)
                                     .build(),
                             bufferCompressor,
                             bufferPoolFactory);
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResultPartitionType.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResultPartitionType.java
index 830a2b30687..29489f654dd 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResultPartitionType.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResultPartitionType.java
@@ -90,8 +90,18 @@ public enum ResultPartitionType {
      * simultaneous reading and writing shuffle data.
      *
      * <p>Hybrid partitions can be consumed any time, whether fully produced or not.
+     *
+     * <p>HYBRID_FULL partitions is re-consumable, so double calculation can be avoided during
+     * failover.
+     */
+    HYBRID_FULL(true, false, false, ConsumingConstraint.CAN_BE_PIPELINED, ReleaseBy.SCHEDULER),
+
+    /**
+     * HYBRID_SELECTIVE partitions are similar to {@link #HYBRID_FULL} partitions, but it is not
+     * re-consumable.
      */
-    HYBRID(false, false, false, ConsumingConstraint.CAN_BE_PIPELINED, ReleaseBy.SCHEDULER);
+    HYBRID_SELECTIVE(
+            false, false, false, ConsumingConstraint.CAN_BE_PIPELINED, ReleaseBy.SCHEDULER);
 
     /**
      * Can this result partition be consumed by multiple downstream consumers for multiple times.
@@ -207,7 +217,9 @@ public enum ResultPartitionType {
     }
 
     public boolean supportCompression() {
-        return isBlockingOrBlockingPersistentResultPartition() || this == HYBRID;
+        return isBlockingOrBlockingPersistentResultPartition()
+                || this == HYBRID_FULL
+                || this == HYBRID_SELECTIVE;
     }
 
     public boolean isReconsumable() {
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResultPartitionFactoryTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResultPartitionFactoryTest.java
index cd22abb953e..6a22484f0b0 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResultPartitionFactoryTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResultPartitionFactoryTest.java
@@ -84,8 +84,15 @@ public class ResultPartitionFactoryTest extends TestLogger {
     }
 
     @Test
-    public void testHybridResultPartitionCreated() {
-        ResultPartition resultPartition = createResultPartition(ResultPartitionType.HYBRID);
+    public void testHybridFullResultPartitionCreated() {
+        ResultPartition resultPartition = createResultPartition(ResultPartitionType.HYBRID_FULL);
+        assertTrue(resultPartition instanceof HsResultPartition);
+    }
+
+    @Test
+    public void testHybridSelectiveResultPartitionCreated() {
+        ResultPartition resultPartition =
+                createResultPartition(ResultPartitionType.HYBRID_SELECTIVE);
         assertTrue(resultPartition instanceof HsResultPartition);
     }
 
@@ -109,8 +116,19 @@ public class ResultPartitionFactoryTest extends TestLogger {
     }
 
     @Test
-    public void testNoReleaseOnConsumptionForHybridPartition() {
-        final ResultPartition resultPartition = createResultPartition(ResultPartitionType.HYBRID);
+    public void testNoReleaseOnConsumptionForHybridFullPartition() {
+        final ResultPartition resultPartition =
+                createResultPartition(ResultPartitionType.HYBRID_FULL);
+
+        resultPartition.onConsumedSubpartition(0);
+
+        assertFalse(resultPartition.isReleased());
+    }
+
+    @Test
+    public void testNoReleaseOnConsumptionForHybridSelectivePartition() {
+        final ResultPartition resultPartition =
+                createResultPartition(ResultPartitionType.HYBRID_SELECTIVE);
 
         resultPartition.onConsumedSubpartition(0);
 
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/hybrid/HsResultPartitionTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/hybrid/HsResultPartitionTest.java
index 6c7d7b66b34..0973fa5052a 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/hybrid/HsResultPartitionTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/hybrid/HsResultPartitionTest.java
@@ -352,7 +352,7 @@ class HsResultPartitionTest {
                         "HsResultPartitionTest",
                         0,
                         new ResultPartitionID(),
-                        ResultPartitionType.HYBRID,
+                        ResultPartitionType.HYBRID_FULL,
                         numSubpartitions,
                         numSubpartitions,
                         readBufferPool,
@@ -378,7 +378,7 @@ class HsResultPartitionTest {
                         "HsResultPartitionTest",
                         0,
                         new ResultPartitionID(),
-                        ResultPartitionType.HYBRID,
+                        ResultPartitionType.HYBRID_FULL,
                         numSubpartitions,
                         numSubpartitions,
                         readBufferPool,
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/strategy/PipelinedRegionSchedulingStrategyTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/strategy/PipelinedRegionSchedulingStrategyTest.java
index c99b816d532..9d2ec3a5380 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/strategy/PipelinedRegionSchedulingStrategyTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/strategy/PipelinedRegionSchedulingStrategyTest.java
@@ -65,6 +65,8 @@ class PipelinedRegionSchedulingStrategyTest {
 
     private List<TestingSchedulingExecutionVertex> map2;
 
+    private List<TestingSchedulingExecutionVertex> map3;
+
     private List<TestingSchedulingExecutionVertex> sink;
 
     @BeforeEach
@@ -93,6 +95,13 @@ class PipelinedRegionSchedulingStrategyTest {
                         .addExecutionVertices()
                         .withParallelism(PARALLELISM)
                         .finish();
+
+        map3 =
+                testingSchedulingTopology
+                        .addExecutionVertices()
+                        .withParallelism(PARALLELISM)
+                        .finish();
+
         sink =
                 testingSchedulingTopology
                         .addExecutionVertices()
@@ -107,10 +116,15 @@ class PipelinedRegionSchedulingStrategyTest {
         testingSchedulingTopology
                 .connectPointwise(map1, map2)
                 .withResultPartitionState(ResultPartitionState.CREATED)
-                .withResultPartitionType(ResultPartitionType.HYBRID)
+                .withResultPartitionType(ResultPartitionType.HYBRID_FULL)
+                .finish();
+        testingSchedulingTopology
+                .connectPointwise(map2, map3)
+                .withResultPartitionState(ResultPartitionState.CREATED)
+                .withResultPartitionType(ResultPartitionType.HYBRID_SELECTIVE)
                 .finish();
         testingSchedulingTopology
-                .connectAllToAll(map2, sink)
+                .connectAllToAll(map3, sink)
                 .withResultPartitionState(ResultPartitionState.CREATED)
                 .withResultPartitionType(ResultPartitionType.BLOCKING)
                 .finish();
@@ -126,6 +140,8 @@ class PipelinedRegionSchedulingStrategyTest {
         expectedScheduledVertices.add(Arrays.asList(source.get(1), map1.get(1)));
         expectedScheduledVertices.add(Arrays.asList(map2.get(0)));
         expectedScheduledVertices.add(Arrays.asList(map2.get(1)));
+        expectedScheduledVertices.add(Arrays.asList(map3.get(0)));
+        expectedScheduledVertices.add(Arrays.asList(map3.get(1)));
         assertLatestScheduledVerticesAreEqualTo(
                 expectedScheduledVertices, testingSchedulerOperation);
     }
@@ -136,7 +152,7 @@ class PipelinedRegionSchedulingStrategyTest {
                 startScheduling(testingSchedulingTopology);
 
         final Set<ExecutionVertexID> verticesToRestart =
-                Stream.of(source, map1, map2, sink)
+                Stream.of(source, map1, map2, map3, sink)
                         .flatMap(List::stream)
                         .map(TestingSchedulingExecutionVertex::getId)
                         .collect(Collectors.toSet());
@@ -149,6 +165,8 @@ class PipelinedRegionSchedulingStrategyTest {
         expectedScheduledVertices.add(Arrays.asList(source.get(1), map1.get(1)));
         expectedScheduledVertices.add(Arrays.asList(map2.get(0)));
         expectedScheduledVertices.add(Arrays.asList(map2.get(1)));
+        expectedScheduledVertices.add(Arrays.asList(map3.get(0)));
+        expectedScheduledVertices.add(Arrays.asList(map3.get(1)));
         assertLatestScheduledVerticesAreEqualTo(
                 expectedScheduledVertices, testingSchedulerOperation);
     }
@@ -158,18 +176,18 @@ class PipelinedRegionSchedulingStrategyTest {
         final PipelinedRegionSchedulingStrategy schedulingStrategy =
                 startScheduling(testingSchedulingTopology);
 
-        final TestingSchedulingExecutionVertex upstream1 = map2.get(0);
+        final TestingSchedulingExecutionVertex upstream1 = map3.get(0);
         upstream1.getProducedResults().iterator().next().markFinished();
         schedulingStrategy.onExecutionStateChange(upstream1.getId(), ExecutionState.FINISHED);
 
         // sinks' inputs are not all consumable yet so they are not scheduled
-        assertThat(testingSchedulerOperation.getScheduledVertices()).hasSize(4);
+        assertThat(testingSchedulerOperation.getScheduledVertices()).hasSize(6);
 
-        final TestingSchedulingExecutionVertex upstream2 = map2.get(1);
+        final TestingSchedulingExecutionVertex upstream2 = map3.get(1);
         upstream2.getProducedResults().iterator().next().markFinished();
         schedulingStrategy.onExecutionStateChange(upstream2.getId(), ExecutionState.FINISHED);
 
-        assertThat(testingSchedulerOperation.getScheduledVertices()).hasSize(6);
+        assertThat(testingSchedulerOperation.getScheduledVertices()).hasSize(8);
 
         final List<List<TestingSchedulingExecutionVertex>> expectedScheduledVertices =
                 new ArrayList<>();
@@ -347,7 +365,8 @@ class PipelinedRegionSchedulingStrategyTest {
 
         v2.connectNewDataSetAsInput(
                 v1, DistributionPattern.POINTWISE, ResultPartitionType.PIPELINED);
-        v3.connectNewDataSetAsInput(v2, DistributionPattern.POINTWISE, ResultPartitionType.HYBRID);
+        v3.connectNewDataSetAsInput(
+                v2, DistributionPattern.POINTWISE, ResultPartitionType.HYBRID_FULL);
         v3.connectNewDataSetAsInput(
                 v1, DistributionPattern.POINTWISE, ResultPartitionType.PIPELINED);
 
@@ -441,10 +460,14 @@ class PipelinedRegionSchedulingStrategyTest {
     void testFinishHybridPartitionWillNotRescheduleDownstream() throws Exception {
         final JobVertex v1 = createJobVertex("v1", 1);
         final JobVertex v2 = createJobVertex("v2", 1);
+        final JobVertex v3 = createJobVertex("v3", 1);
 
-        v2.connectNewDataSetAsInput(v1, DistributionPattern.POINTWISE, ResultPartitionType.HYBRID);
+        v2.connectNewDataSetAsInput(
+                v1, DistributionPattern.POINTWISE, ResultPartitionType.HYBRID_FULL);
+        v3.connectNewDataSetAsInput(
+                v1, DistributionPattern.POINTWISE, ResultPartitionType.HYBRID_SELECTIVE);
 
-        final List<JobVertex> ordered = new ArrayList<>(Arrays.asList(v1, v2));
+        final List<JobVertex> ordered = new ArrayList<>(Arrays.asList(v1, v2, v3));
         final JobGraph jobGraph =
                 JobGraphBuilder.newBatchJobGraphBuilder().addJobVertices(ordered).build();
         final ExecutionGraph executionGraph =
@@ -457,12 +480,12 @@ class PipelinedRegionSchedulingStrategyTest {
         PipelinedRegionSchedulingStrategy schedulingStrategy = startScheduling(schedulingTopology);
 
         // all regions will be scheduled
-        assertThat(testingSchedulerOperation.getScheduledVertices()).hasSize(2);
+        assertThat(testingSchedulerOperation.getScheduledVertices()).hasSize(3);
 
         final ExecutionVertex v11 = executionGraph.getJobVertex(v1.getID()).getTaskVertices()[0];
         schedulingStrategy.onExecutionStateChange(v11.getID(), ExecutionState.FINISHED);
 
-        assertThat(testingSchedulerOperation.getScheduledVertices()).hasSize(2);
+        assertThat(testingSchedulerOperation.getScheduledVertices()).hasSize(3);
     }
 
     /**
@@ -482,10 +505,12 @@ class PipelinedRegionSchedulingStrategyTest {
         final JobVertex v3 = createJobVertex("v3", 1);
         final JobVertex v4 = createJobVertex("v4", 1);
 
-        v2.connectNewDataSetAsInput(v1, DistributionPattern.POINTWISE, ResultPartitionType.HYBRID);
+        v2.connectNewDataSetAsInput(
+                v1, DistributionPattern.POINTWISE, ResultPartitionType.HYBRID_FULL);
         v3.connectNewDataSetAsInput(
                 v2, DistributionPattern.POINTWISE, ResultPartitionType.BLOCKING);
-        v4.connectNewDataSetAsInput(v3, DistributionPattern.POINTWISE, ResultPartitionType.HYBRID);
+        v4.connectNewDataSetAsInput(
+                v3, DistributionPattern.POINTWISE, ResultPartitionType.HYBRID_SELECTIVE);
 
         final List<JobVertex> ordered = new ArrayList<>(Arrays.asList(v1, v2, v3, v4));
         final JobGraph jobGraph =
@@ -526,6 +551,7 @@ class PipelinedRegionSchedulingStrategyTest {
         final JobVertex v2 = createJobVertex("v2", 1);
         final JobVertex v3 = createJobVertex("v3", 1);
         final JobVertex v4 = createJobVertex("v4", 1);
+        final JobVertex v5 = createJobVertex("v5", 1);
 
         v2.connectNewDataSetAsInput(
                 v1, DistributionPattern.POINTWISE, ResultPartitionType.PIPELINED);
@@ -533,11 +559,16 @@ class PipelinedRegionSchedulingStrategyTest {
                 v2, DistributionPattern.POINTWISE, ResultPartitionType.PIPELINED);
         v4.connectNewDataSetAsInput(
                 v2, DistributionPattern.POINTWISE, ResultPartitionType.PIPELINED);
-        v3.connectNewDataSetAsInput(v1, DistributionPattern.POINTWISE, ResultPartitionType.HYBRID);
+        v5.connectNewDataSetAsInput(
+                v2, DistributionPattern.POINTWISE, ResultPartitionType.PIPELINED);
+        v3.connectNewDataSetAsInput(
+                v1, DistributionPattern.POINTWISE, ResultPartitionType.HYBRID_FULL);
+        v4.connectNewDataSetAsInput(
+                v1, DistributionPattern.POINTWISE, ResultPartitionType.HYBRID_SELECTIVE);
         v4.connectNewDataSetAsInput(
                 v1, DistributionPattern.POINTWISE, ResultPartitionType.BLOCKING);
 
-        final List<JobVertex> ordered = new ArrayList<>(Arrays.asList(v1, v2, v3, v4));
+        final List<JobVertex> ordered = new ArrayList<>(Arrays.asList(v1, v2, v3, v4, v5));
         final JobGraph jobGraph =
                 JobGraphBuilder.newBatchJobGraphBuilder().addJobVertices(ordered).build();
         final ExecutionGraph executionGraph =
@@ -552,7 +583,7 @@ class PipelinedRegionSchedulingStrategyTest {
         assertThat(testingSchedulerOperation.getScheduledVertices()).hasSize(1);
         List<ExecutionVertexID> executionVertexIds =
                 testingSchedulerOperation.getScheduledVertices().get(0);
-        assertThat(executionVertexIds).hasSize(4);
+        assertThat(executionVertexIds).hasSize(5);
     }
 
     /**
@@ -564,12 +595,16 @@ class PipelinedRegionSchedulingStrategyTest {
         final JobVertex v1 = createJobVertex("v1", 1);
         final JobVertex v2 = createJobVertex("v2", 1);
         final JobVertex v3 = createJobVertex("v3", 1);
+        final JobVertex v4 = createJobVertex("v4", 1);
 
-        v3.connectNewDataSetAsInput(
+        v4.connectNewDataSetAsInput(
                 v1, DistributionPattern.POINTWISE, ResultPartitionType.BLOCKING);
-        v3.connectNewDataSetAsInput(v2, DistributionPattern.POINTWISE, ResultPartitionType.HYBRID);
+        v4.connectNewDataSetAsInput(
+                v2, DistributionPattern.POINTWISE, ResultPartitionType.HYBRID_FULL);
+        v4.connectNewDataSetAsInput(
+                v3, DistributionPattern.POINTWISE, ResultPartitionType.HYBRID_SELECTIVE);
 
-        final List<JobVertex> ordered = new ArrayList<>(Arrays.asList(v1, v2, v3));
+        final List<JobVertex> ordered = new ArrayList<>(Arrays.asList(v1, v2, v3, v4));
         final JobGraph jobGraph =
                 JobGraphBuilder.newBatchJobGraphBuilder().addJobVertices(ordered).build();
         final ExecutionGraph executionGraph =
@@ -582,12 +617,12 @@ class PipelinedRegionSchedulingStrategyTest {
         final PipelinedRegionSchedulingStrategy schedulingStrategy =
                 startScheduling(schedulingTopology);
 
-        assertThat(testingSchedulerOperation.getScheduledVertices()).hasSize(2);
+        assertThat(testingSchedulerOperation.getScheduledVertices()).hasSize(3);
 
         final ExecutionVertex v11 = executionGraph.getJobVertex(v1.getID()).getTaskVertices()[0];
         v11.finishAllBlockingPartitions();
         schedulingStrategy.onExecutionStateChange(v11.getID(), ExecutionState.FINISHED);
-        assertThat(testingSchedulerOperation.getScheduledVertices()).hasSize(3);
+        assertThat(testingSchedulerOperation.getScheduledVertices()).hasSize(4);
     }
 
     private static JobVertex createJobVertex(String vertexName, int parallelism) {
diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/graph/GlobalStreamExchangeMode.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/graph/GlobalStreamExchangeMode.java
index d2f43acd16a..bdd3d065779 100644
--- a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/graph/GlobalStreamExchangeMode.java
+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/graph/GlobalStreamExchangeMode.java
@@ -53,6 +53,9 @@ public enum GlobalStreamExchangeMode {
     /** Set all job edges {@link ResultPartitionType#PIPELINED_APPROXIMATE}. */
     ALL_EDGES_PIPELINED_APPROXIMATE,
 
-    /** Set all job edges {@link ResultPartitionType#HYBRID}. */
-    ALL_EDGES_HYBRID
+    /** Set all job edges {@link ResultPartitionType#HYBRID_FULL}. */
+    ALL_EDGES_HYBRID_FULL,
+
+    /** Set all job edges {@link ResultPartitionType#HYBRID_SELECTIVE}. */
+    ALL_EDGES_HYBRID_SELECTIVE
 }
diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/graph/StreamGraphGenerator.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/graph/StreamGraphGenerator.java
index 85d635a9c8c..f075704e010 100644
--- a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/graph/StreamGraphGenerator.java
+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/graph/StreamGraphGenerator.java
@@ -400,8 +400,10 @@ public class StreamGraphGenerator {
                 return GlobalStreamExchangeMode.ALL_EDGES_PIPELINED;
             case ALL_EXCHANGES_BLOCKING:
                 return GlobalStreamExchangeMode.ALL_EDGES_BLOCKING;
-            case WIP_ALL_EXCHANGES_HYBRID:
-                return GlobalStreamExchangeMode.ALL_EDGES_HYBRID;
+            case WIP_ALL_EXCHANGES_HYBRID_FULL:
+                return GlobalStreamExchangeMode.ALL_EDGES_HYBRID_FULL;
+            case WIP_ALL_EXCHANGES_HYBRID_SELECTIVE:
+                return GlobalStreamExchangeMode.ALL_EDGES_HYBRID_SELECTIVE;
             default:
                 throw new IllegalArgumentException(
                         String.format(
diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/graph/StreamingJobGraphGenerator.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/graph/StreamingJobGraphGenerator.java
index e44ed175626..c50e7437959 100644
--- a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/graph/StreamingJobGraphGenerator.java
+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/graph/StreamingJobGraphGenerator.java
@@ -1122,7 +1122,8 @@ public class StreamingJobGraphGenerator {
         StreamPartitioner<?> partitioner = output.getPartitioner();
         ResultPartitionType resultPartitionType = output.getPartitionType();
 
-        if (resultPartitionType == ResultPartitionType.HYBRID) {
+        if (resultPartitionType == ResultPartitionType.HYBRID_FULL
+                || resultPartitionType == ResultPartitionType.HYBRID_SELECTIVE) {
             hasHybridResultPartition = true;
         }
 
@@ -1187,8 +1188,10 @@ public class StreamingJobGraphGenerator {
                 return ResultPartitionType.PIPELINED_BOUNDED;
             case BATCH:
                 return ResultPartitionType.BLOCKING;
-            case HYBRID:
-                return ResultPartitionType.HYBRID;
+            case HYBRID_FULL:
+                return ResultPartitionType.HYBRID_FULL;
+            case HYBRID_SELECTIVE:
+                return ResultPartitionType.HYBRID_SELECTIVE;
             case UNDEFINED:
                 return determineUndefinedResultPartitionType(edge.getPartitioner());
             default:
@@ -1218,8 +1221,10 @@ public class StreamingJobGraphGenerator {
                 return ResultPartitionType.PIPELINED_BOUNDED;
             case ALL_EDGES_PIPELINED_APPROXIMATE:
                 return ResultPartitionType.PIPELINED_APPROXIMATE;
-            case ALL_EDGES_HYBRID:
-                return ResultPartitionType.HYBRID;
+            case ALL_EDGES_HYBRID_FULL:
+                return ResultPartitionType.HYBRID_FULL;
+            case ALL_EDGES_HYBRID_SELECTIVE:
+                return ResultPartitionType.HYBRID_SELECTIVE;
             default:
                 throw new RuntimeException(
                         "Unrecognized global data exchange mode "
diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/transformations/StreamExchangeMode.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/transformations/StreamExchangeMode.java
index 18bb7f88ea9..f7270c05b44 100644
--- a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/transformations/StreamExchangeMode.java
+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/transformations/StreamExchangeMode.java
@@ -38,8 +38,17 @@ public enum StreamExchangeMode {
 
     /**
      * The consumer can start consuming data anytime as long as the producer has started producing.
+     *
+     * <p>This exchange mode is re-consumable.
      */
-    HYBRID,
+    HYBRID_FULL,
+
+    /**
+     * The consumer can start consuming data anytime as long as the producer has started producing.
+     *
+     * <p>This exchange mode is not re-consumable.
+     */
+    HYBRID_SELECTIVE,
 
     /**
      * The exchange mode is undefined. It leaves it up to the framework to decide the exchange mode.
diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/graph/StreamGraphGeneratorBatchExecutionTest.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/graph/StreamGraphGeneratorBatchExecutionTest.java
index bd4fd6ede22..eb149bac04b 100644
--- a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/graph/StreamGraphGeneratorBatchExecutionTest.java
+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/graph/StreamGraphGeneratorBatchExecutionTest.java
@@ -107,8 +107,13 @@ public class StreamGraphGeneratorBatchExecutionTest extends TestLogger {
 
         testGlobalStreamExchangeMode(
                 RuntimeExecutionMode.BATCH,
-                BatchShuffleMode.WIP_ALL_EXCHANGES_HYBRID,
-                GlobalStreamExchangeMode.ALL_EDGES_HYBRID);
+                BatchShuffleMode.WIP_ALL_EXCHANGES_HYBRID_FULL,
+                GlobalStreamExchangeMode.ALL_EDGES_HYBRID_FULL);
+
+        testGlobalStreamExchangeMode(
+                RuntimeExecutionMode.BATCH,
+                BatchShuffleMode.WIP_ALL_EXCHANGES_HYBRID_SELECTIVE,
+                GlobalStreamExchangeMode.ALL_EDGES_HYBRID_SELECTIVE);
     }
 
     @Test
diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/graph/StreamingJobGraphGeneratorTest.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/graph/StreamingJobGraphGeneratorTest.java
index 4b6dd73ca7c..3c663ddfb20 100644
--- a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/graph/StreamingJobGraphGeneratorTest.java
+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/graph/StreamingJobGraphGeneratorTest.java
@@ -735,9 +735,9 @@ public class StreamingJobGraphGeneratorTest extends TestLogger {
                 .isEqualTo(ResultPartitionType.PIPELINED_BOUNDED);
     }
 
-    /** Test setting exchange mode to {@link StreamExchangeMode#HYBRID}. */
+    /** Test setting exchange mode to {@link StreamExchangeMode#HYBRID_FULL}. */
     @Test
-    public void testExchangeModeHybrid() {
+    void testExchangeModeHybridFull() {
         StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
         // fromElements -> Map -> Print
         DataStream<Integer> sourceDataStream = env.fromElements(1, 2, 3);
@@ -748,7 +748,7 @@ public class StreamingJobGraphGeneratorTest extends TestLogger {
                         new PartitionTransformation<>(
                                 sourceDataStream.getTransformation(),
                                 new ForwardPartitioner<>(),
-                                StreamExchangeMode.HYBRID));
+                                StreamExchangeMode.HYBRID_FULL));
         DataStream<Integer> mapDataStream =
                 partitionAfterSourceDataStream.map(value -> value).setParallelism(1);
 
@@ -758,7 +758,7 @@ public class StreamingJobGraphGeneratorTest extends TestLogger {
                         new PartitionTransformation<>(
                                 mapDataStream.getTransformation(),
                                 new RescalePartitioner<>(),
-                                StreamExchangeMode.HYBRID));
+                                StreamExchangeMode.HYBRID_FULL));
         partitionAfterMapDataStream.print().setParallelism(2);
 
         JobGraph jobGraph = StreamingJobGraphGenerator.createJobGraph(env.getStreamGraph());
@@ -766,12 +766,51 @@ public class StreamingJobGraphGeneratorTest extends TestLogger {
         List<JobVertex> verticesSorted = jobGraph.getVerticesSortedTopologicallyFromSources();
         Assertions.assertThat(verticesSorted.size()).isEqualTo(2);
 
-        // it can be chained with Hybrid exchange mode
+        // it can be chained with HYBRID_FULL exchange mode
         JobVertex sourceAndMapVertex = verticesSorted.get(0);
 
-        // Hybrid exchange mode is translated into Hybrid result partition
+        // HYBRID_FULL exchange mode is translated into HYBRID_FULL result partition
         Assertions.assertThat(sourceAndMapVertex.getProducedDataSets().get(0).getResultType())
-                .isEqualTo(ResultPartitionType.HYBRID);
+                .isEqualTo(ResultPartitionType.HYBRID_FULL);
+    }
+
+    /** Test setting exchange mode to {@link StreamExchangeMode#HYBRID_SELECTIVE}. */
+    @Test
+    void testExchangeModeHybridSelective() {
+        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
+        // fromElements -> Map -> Print
+        DataStream<Integer> sourceDataStream = env.fromElements(1, 2, 3);
+
+        DataStream<Integer> partitionAfterSourceDataStream =
+                new DataStream<>(
+                        env,
+                        new PartitionTransformation<>(
+                                sourceDataStream.getTransformation(),
+                                new ForwardPartitioner<>(),
+                                StreamExchangeMode.HYBRID_SELECTIVE));
+        DataStream<Integer> mapDataStream =
+                partitionAfterSourceDataStream.map(value -> value).setParallelism(1);
+
+        DataStream<Integer> partitionAfterMapDataStream =
+                new DataStream<>(
+                        env,
+                        new PartitionTransformation<>(
+                                mapDataStream.getTransformation(),
+                                new RescalePartitioner<>(),
+                                StreamExchangeMode.HYBRID_SELECTIVE));
+        partitionAfterMapDataStream.print().setParallelism(2);
+
+        JobGraph jobGraph = StreamingJobGraphGenerator.createJobGraph(env.getStreamGraph());
+
+        List<JobVertex> verticesSorted = jobGraph.getVerticesSortedTopologicallyFromSources();
+        Assertions.assertThat(verticesSorted.size()).isEqualTo(2);
+
+        // it can be chained with HYBRID_SELECTIVE exchange mode
+        JobVertex sourceAndMapVertex = verticesSorted.get(0);
+
+        // HYBRID_SELECTIVE exchange mode is translated into HYBRID_SELECTIVE result partition
+        Assertions.assertThat(sourceAndMapVertex.getProducedDataSets().get(0).getResultType())
+                .isEqualTo(ResultPartitionType.HYBRID_SELECTIVE);
     }
 
     @Test
@@ -1278,11 +1317,12 @@ public class StreamingJobGraphGeneratorTest extends TestLogger {
     }
 
     @Test
-    public void testSetNonDefaultSlotSharingInHybridMode() {
+    void testSetNonDefaultSlotSharingInHybridMode() {
         Configuration configuration = new Configuration();
-        // set all edge to HYBRID result partition type.
+        // set all edge to HYBRID_FULL result partition type.
         configuration.set(
-                ExecutionOptions.BATCH_SHUFFLE_MODE, BatchShuffleMode.WIP_ALL_EXCHANGES_HYBRID);
+                ExecutionOptions.BATCH_SHUFFLE_MODE,
+                BatchShuffleMode.WIP_ALL_EXCHANGES_HYBRID_FULL);
 
         final StreamGraph streamGraph = createStreamGraphForSlotSharingTest(configuration);
         // specify slot sharing group for map1
@@ -1295,6 +1335,23 @@ public class StreamingJobGraphGeneratorTest extends TestLogger {
                 .isInstanceOf(IllegalStateException.class)
                 .hasMessage(
                         "hybrid shuffle mode currently does not support setting non-default slot sharing group.");
+
+        // set all edge to HYBRID_SELECTIVE result partition type.
+        configuration.set(
+                ExecutionOptions.BATCH_SHUFFLE_MODE,
+                BatchShuffleMode.WIP_ALL_EXCHANGES_HYBRID_SELECTIVE);
+
+        final StreamGraph streamGraph2 = createStreamGraphForSlotSharingTest(configuration);
+        // specify slot sharing group for map1
+        streamGraph2.getStreamNodes().stream()
+                .filter(n -> "map1".equals(n.getOperatorName()))
+                .findFirst()
+                .get()
+                .setSlotSharingGroup("testSlotSharingGroup");
+        assertThatThrownBy(() -> StreamingJobGraphGenerator.createJobGraph(streamGraph2))
+                .isInstanceOf(IllegalStateException.class)
+                .hasMessage(
+                        "hybrid shuffle mode currently does not support setting non-default slot sharing group.");
     }
 
     @Test
