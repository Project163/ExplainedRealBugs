diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSourceDynamicFileEnumerator.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSourceDynamicFileEnumerator.java
index 1915e365721..530e0f61e52 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSourceDynamicFileEnumerator.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSourceDynamicFileEnumerator.java
@@ -32,6 +32,7 @@ import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.data.StringData;
 import org.apache.flink.table.data.util.DataFormatConverters.LocalDateConverter;
 import org.apache.flink.table.data.util.DataFormatConverters.LocalDateTimeConverter;
+import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.table.types.logical.LogicalTypeRoot;
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.util.Preconditions;
@@ -115,7 +116,7 @@ public class HiveSourceDynamicFileEnumerator implements DynamicFileEnumerator {
         Preconditions.checkArgument(rowType.getFieldCount() == dynamicFilterPartitionKeys.size());
         for (HiveTablePartition partition : allPartitions) {
             RowData partitionRow = createRowData(rowType, partition.getPartitionSpec());
-            if (data.contains(partitionRow)) {
+            if (partitionRow != null && data.contains(partitionRow)) {
                 finalPartitions.add(partition);
             }
         }
@@ -131,6 +132,7 @@ public class HiveSourceDynamicFileEnumerator implements DynamicFileEnumerator {
         GenericRowData rowData = new GenericRowData(rowType.getFieldCount());
         for (int i = 0; i < rowType.getFieldCount(); ++i) {
             String value = partitionSpec.get(dynamicFilterPartitionKeys.get(i));
+            LogicalType fieldType = rowType.getTypeAt(i);
             Object convertedValue =
                     HivePartitionUtils.restorePartitionValueFromType(
                             hiveShim, value, rowType.getTypeAt(i), defaultPartitionName);
@@ -159,6 +161,9 @@ public class HiveSourceDynamicFileEnumerator implements DynamicFileEnumerator {
                     throw new UnsupportedOperationException(
                             "Unsupported type for dynamic filtering:" + rowType.getTypeAt(i));
             }
+            if (!fieldType.isNullable() && convertedValue == null) {
+                return null;
+            }
             rowData.setField(i, convertedValue);
         }
         return rowData;
diff --git a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveSourceDynamicFileEnumeratorTest.java b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveSourceDynamicFileEnumeratorTest.java
index 3d390d448ca..c886692f425 100644
--- a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveSourceDynamicFileEnumeratorTest.java
+++ b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveSourceDynamicFileEnumeratorTest.java
@@ -60,6 +60,7 @@ import java.util.Map;
 import java.util.Properties;
 import java.util.stream.Collectors;
 
+import static org.apache.flink.table.data.StringData.fromString;
 import static org.assertj.core.api.Assertions.assertThat;
 
 /** Tests for HiveSourceDynamicFileEnumerator. */
@@ -158,6 +159,48 @@ class HiveSourceDynamicFileEnumeratorTest {
         }
     }
 
+    @Test
+    void testNonNullFieldTypeWithDefaultPartitionName() {
+        String defaultPartitionName = HiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal;
+        List<String> keys = Arrays.asList("NonNullString", "NonNullInt");
+        Map<String, String> partitionSpec = new HashMap<>();
+        partitionSpec.put("NonNullString", defaultPartitionName);
+        partitionSpec.put("NonNullInt", "0");
+
+        Map<String, String> partitionSpec2 = new HashMap<>();
+        partitionSpec2.put("NonNullString", "");
+        partitionSpec2.put("NonNullInt", defaultPartitionName);
+
+        List<Map<String, String>> partitionSpecs = Arrays.asList(partitionSpec, partitionSpec2);
+        HiveSourceDynamicFileEnumerator enumerator = createTestEnumerator(keys, partitionSpecs);
+
+        assertThat(enumerator.getFinalPartitions()).hasSize(2);
+        assertThat(
+                        enumerator.getFinalPartitions().stream()
+                                .map(HiveTablePartition::getPartitionSpec)
+                                .collect(Collectors.toList()))
+                .containsExactlyInAnyOrder(partitionSpecs.toArray(new Map[0]));
+
+        RowType rowType = RowType.of(new VarCharType(false, 32), new IntType(false));
+        TypeInformation<RowData> rowTypeInfo = InternalTypeInfo.of(rowType);
+        DynamicFilteringData data =
+                new DynamicFilteringData(
+                        InternalTypeInfo.of(rowType),
+                        rowType,
+                        Arrays.asList(
+                                serialize(
+                                        rowTypeInfo,
+                                        GenericRowData.of(fromString(defaultPartitionName), 0)),
+                                serialize(rowTypeInfo, GenericRowData.of(fromString(""), 0))),
+                        true);
+        enumerator.setDynamicFilteringData(data);
+
+        // No exception should be thrown and partitionSpec2 should not be retained
+        assertThat(enumerator.getFinalPartitions()).hasSize(1);
+        assertThat(enumerator.getFinalPartitions().get(0).getPartitionSpec())
+                .isEqualTo(partitionSpec);
+    }
+
     private HiveSourceDynamicFileEnumerator createTestEnumerator(
             List<String> keys, List<Map<String, String>> partitionSpecs) {
         return new HiveSourceDynamicFileEnumerator(
