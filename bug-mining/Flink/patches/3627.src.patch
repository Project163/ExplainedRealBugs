diff --git a/docs/_includes/generated/rocks_db_configurable_configuration.html b/docs/_includes/generated/rocks_db_configurable_configuration.html
index f46fc273868..154d9ac306a 100644
--- a/docs/_includes/generated/rocks_db_configurable_configuration.html
+++ b/docs/_includes/generated/rocks_db_configurable_configuration.html
@@ -56,6 +56,12 @@
             <td>Integer</td>
             <td>The maximum number of concurrent background flush and compaction jobs (per TaskManager). RocksDB has default configuration as '1'.</td>
         </tr>
+        <tr>
+            <td><h5>state.backend.rocksdb.write-batch-size</h5></td>
+            <td style="word-wrap: break-word;">2097152 bytes</td>
+            <td>MemorySize</td>
+            <td>The max size of the consumed memory for RocksDB batch write, will flush just based on item count if this config set to 0.</td>
+        </tr>
         <tr>
             <td><h5>state.backend.rocksdb.writebuffer.count</h5></td>
             <td style="word-wrap: break-word;">(none)</td>
diff --git a/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBConfigurableOptions.java b/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBConfigurableOptions.java
index e4dda8b22b1..e725bbdee7f 100644
--- a/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBConfigurableOptions.java
+++ b/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBConfigurableOptions.java
@@ -134,4 +134,10 @@ public class RocksDBConfigurableOptions implements Serializable {
 			.withDescription("The amount of the cache for data blocks in RocksDB. " +
 				"RocksDB has default block-cache size as '8MB'.");
 
+	public static final ConfigOption<MemorySize> WRITE_BATCH_SIZE =
+		key("state.backend.rocksdb.write-batch-size")
+		.memoryType()
+		.defaultValue(MemorySize.parse("2mb"))
+		.withDescription("The max size of the consumed memory for RocksDB batch write, " +
+			"will flush just based on item count if this config set to 0.");
 }
diff --git a/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBIncrementalCheckpointUtils.java b/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBIncrementalCheckpointUtils.java
index 6af14f10b4c..1f43dd0a905 100644
--- a/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBIncrementalCheckpointUtils.java
+++ b/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBIncrementalCheckpointUtils.java
@@ -77,7 +77,8 @@ public class RocksDBIncrementalCheckpointUtils {
 		@Nonnull List<ColumnFamilyHandle> columnFamilyHandles,
 		@Nonnull KeyGroupRange targetKeyGroupRange,
 		@Nonnull KeyGroupRange currentKeyGroupRange,
-		@Nonnegative int keyGroupPrefixBytes) throws RocksDBException {
+		@Nonnegative int keyGroupPrefixBytes,
+		@Nonnegative long writeBatchSize) throws RocksDBException {
 
 		final byte[] beginKeyGroupBytes = new byte[keyGroupPrefixBytes];
 		final byte[] endKeyGroupBytes = new byte[keyGroupPrefixBytes];
@@ -87,7 +88,7 @@ public class RocksDBIncrementalCheckpointUtils {
 				currentKeyGroupRange.getStartKeyGroup(), beginKeyGroupBytes);
 			RocksDBKeySerializationUtils.serializeKeyGroup(
 				targetKeyGroupRange.getStartKeyGroup(), endKeyGroupBytes);
-			deleteRange(db, columnFamilyHandles, beginKeyGroupBytes, endKeyGroupBytes);
+			deleteRange(db, columnFamilyHandles, beginKeyGroupBytes, endKeyGroupBytes, writeBatchSize);
 		}
 
 		if (currentKeyGroupRange.getEndKeyGroup() > targetKeyGroupRange.getEndKeyGroup()) {
@@ -95,7 +96,7 @@ public class RocksDBIncrementalCheckpointUtils {
 				targetKeyGroupRange.getEndKeyGroup() + 1, beginKeyGroupBytes);
 			RocksDBKeySerializationUtils.serializeKeyGroup(
 				currentKeyGroupRange.getEndKeyGroup() + 1, endKeyGroupBytes);
-			deleteRange(db, columnFamilyHandles, beginKeyGroupBytes, endKeyGroupBytes);
+			deleteRange(db, columnFamilyHandles, beginKeyGroupBytes, endKeyGroupBytes, writeBatchSize);
 		}
 	}
 
@@ -111,11 +112,12 @@ public class RocksDBIncrementalCheckpointUtils {
 		RocksDB db,
 		List<ColumnFamilyHandle> columnFamilyHandles,
 		byte[] beginKeyBytes,
-		byte[] endKeyBytes) throws RocksDBException {
+		byte[] endKeyBytes,
+		@Nonnegative long writeBatchSize) throws RocksDBException {
 
 		for (ColumnFamilyHandle columnFamilyHandle : columnFamilyHandles) {
 			try (RocksIteratorWrapper iteratorWrapper = RocksDBOperationUtils.getRocksIterator(db, columnFamilyHandle);
-				RocksDBWriteBatchWrapper writeBatchWrapper = new RocksDBWriteBatchWrapper(db)) {
+				RocksDBWriteBatchWrapper writeBatchWrapper = new RocksDBWriteBatchWrapper(db, writeBatchSize)) {
 
 				iteratorWrapper.seek(beginKeyBytes);
 
diff --git a/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBKeyedStateBackend.java b/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBKeyedStateBackend.java
index 2f83f62cbf7..6f27f111aea 100644
--- a/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBKeyedStateBackend.java
+++ b/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBKeyedStateBackend.java
@@ -73,6 +73,7 @@ import org.rocksdb.WriteOptions;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import javax.annotation.Nonnegative;
 import javax.annotation.Nonnull;
 
 import java.io.File;
@@ -90,6 +91,7 @@ import java.util.stream.Stream;
 import java.util.stream.StreamSupport;
 
 import static org.apache.flink.contrib.streaming.state.RocksDBSnapshotTransformFactoryAdaptor.wrapStateSnapshotTransformFactory;
+import static org.apache.flink.util.Preconditions.checkArgument;
 import static org.apache.flink.util.Preconditions.checkState;
 
 /**
@@ -147,6 +149,11 @@ public class RocksDBKeyedStateBackend<K> extends AbstractKeyedStateBackend<K> {
 	 */
 	private final WriteOptions writeOptions;
 
+	/**
+	 * The max memory size for one batch in {@link RocksDBWriteBatchWrapper}.
+	 */
+	private final long writeBatchSize;
+
 	/**
 	 * Information about the k/v states, maintained in the order as we create them. This is used to retrieve the
 	 * column family that is used for a state and also for sanity checks when restoring.
@@ -218,7 +225,8 @@ public class RocksDBKeyedStateBackend<K> extends AbstractKeyedStateBackend<K> {
 		RocksDBSerializedCompositeKeyBuilder<K> sharedRocksKeyBuilder,
 		PriorityQueueSetFactory priorityQueueFactory,
 		RocksDbTtlCompactFiltersManager ttlCompactFiltersManager,
-		InternalKeyContext<K> keyContext) {
+		InternalKeyContext<K> keyContext,
+		@Nonnegative long writeBatchSize) {
 
 		super(
 			kvStateRegistry,
@@ -243,6 +251,8 @@ public class RocksDBKeyedStateBackend<K> extends AbstractKeyedStateBackend<K> {
 		this.kvStateInformation = kvStateInformation;
 
 		this.writeOptions = new WriteOptions().setDisableWAL(true);
+		checkArgument(writeBatchSize >= 0, "Write batch size have to be no negative value.");
+		this.writeBatchSize = writeBatchSize;
 		this.db = db;
 		this.rocksDBResourceGuard = rocksDBResourceGuard;
 		this.checkpointSnapshotStrategy = checkpointSnapshotStrategy;
@@ -590,7 +600,7 @@ public class RocksDBKeyedStateBackend<K> extends AbstractKeyedStateBackend<K> {
 		Snapshot rocksDBSnapshot = db.getSnapshot();
 		try (
 			RocksIteratorWrapper iterator = RocksDBOperationUtils.getRocksIterator(db, stateMetaInfo.f0);
-			RocksDBWriteBatchWrapper batchWriter = new RocksDBWriteBatchWrapper(db, getWriteOptions())
+			RocksDBWriteBatchWrapper batchWriter = new RocksDBWriteBatchWrapper(db, getWriteOptions(), getWriteBatchSize())
 		) {
 			iterator.seekToFirst();
 
@@ -705,4 +715,9 @@ public class RocksDBKeyedStateBackend<K> extends AbstractKeyedStateBackend<K> {
 		RocksDbKvStateInfo kvStateInfo = kvStateInformation.get(stateDesc.getName());
 		db.compactRange(kvStateInfo.columnFamilyHandle);
 	}
+
+	@Nonnegative
+	long getWriteBatchSize() {
+		return writeBatchSize;
+	}
 }
diff --git a/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBKeyedStateBackendBuilder.java b/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBKeyedStateBackendBuilder.java
index a0818acafa9..70c438e29ee 100644
--- a/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBKeyedStateBackendBuilder.java
+++ b/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBKeyedStateBackendBuilder.java
@@ -74,6 +74,8 @@ import java.util.TreeMap;
 import java.util.UUID;
 import java.util.function.Function;
 
+import static org.apache.flink.util.Preconditions.checkArgument;
+
 /**
  * Builder class for {@link RocksDBKeyedStateBackend} which handles all necessary initializations and clean ups.
  *
@@ -110,6 +112,7 @@ public class RocksDBKeyedStateBackendBuilder<K> extends AbstractKeyedStateBacken
 	private boolean enableTtlCompactionFilter;
 	private RocksDBNativeMetricOptions nativeMetricOptions;
 	private int numberOfTransferingThreads;
+	private long writeBatchSize = RocksDBConfigurableOptions.WRITE_BATCH_SIZE.defaultValue().getBytes();
 
 	private RocksDB injectedTestDB; // for testing
 	private ColumnFamilyHandle injectedDefaultColumnFamilyHandle; // for testing
@@ -223,6 +226,12 @@ public class RocksDBKeyedStateBackendBuilder<K> extends AbstractKeyedStateBacken
 		return this;
 	}
 
+	RocksDBKeyedStateBackendBuilder<K> setWriteBatchSize(long writeBatchSize) {
+		checkArgument(writeBatchSize >= 0, "Write batch size should be non negative.");
+		this.writeBatchSize = writeBatchSize;
+		return this;
+	}
+
 	private static void checkAndCreateDirectory(File directory) throws IOException {
 		if (directory.exists()) {
 			if (!directory.isDirectory()) {
@@ -278,9 +287,8 @@ public class RocksDBKeyedStateBackendBuilder<K> extends AbstractKeyedStateBacken
 				}
 			}
 
-			// Init after db instantiated thus native library loaded. We disable write ahead logging.
 			writeOptions = new WriteOptions().setDisableWAL(true);
-			writeBatchWrapper = new RocksDBWriteBatchWrapper(db, writeOptions);
+			writeBatchWrapper = new RocksDBWriteBatchWrapper(db, writeOptions, writeBatchSize);
 			// it is important that we only create the key builder after the restore, and not before;
 			// restore operations may reconfigure the key serializer, so accessing the key serializer
 			// only now we can be certain that the key serializer used in the builder is final.
@@ -355,7 +363,8 @@ public class RocksDBKeyedStateBackendBuilder<K> extends AbstractKeyedStateBacken
 			sharedRocksKeyBuilder,
 			priorityQueueFactory,
 			ttlCompactFiltersManager,
-			keyContext);
+			keyContext,
+			writeBatchSize);
 	}
 
 	private AbstractRocksDBRestoreOperation<K> getRocksDBRestoreOperation(
@@ -400,7 +409,8 @@ public class RocksDBKeyedStateBackendBuilder<K> extends AbstractKeyedStateBacken
 				nativeMetricOptions,
 				metricGroup,
 				restoreStateHandles,
-				ttlCompactFiltersManager);
+				ttlCompactFiltersManager,
+				writeBatchSize);
 		} else {
 			return new RocksDBFullRestoreOperation<>(
 				keyGroupRange,
@@ -417,7 +427,8 @@ public class RocksDBKeyedStateBackendBuilder<K> extends AbstractKeyedStateBacken
 				nativeMetricOptions,
 				metricGroup,
 				restoreStateHandles,
-				ttlCompactFiltersManager);
+				ttlCompactFiltersManager,
+				writeBatchSize);
 		}
 	}
 
diff --git a/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBMapState.java b/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBMapState.java
index e7e1d2519f2..ad9281c4a19 100644
--- a/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBMapState.java
+++ b/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBMapState.java
@@ -138,7 +138,7 @@ class RocksDBMapState<K, N, UK, UV>
 			return;
 		}
 
-		try (RocksDBWriteBatchWrapper writeBatchWrapper = new RocksDBWriteBatchWrapper(backend.db, writeOptions)) {
+		try (RocksDBWriteBatchWrapper writeBatchWrapper = new RocksDBWriteBatchWrapper(backend.db, writeOptions, backend.getWriteBatchSize())) {
 			for (Map.Entry<UK, UV> entry : map.entrySet()) {
 				byte[] rawKeyBytes = serializeCurrentKeyWithGroupAndNamespacePlusUserKey(entry.getKey(), userKeySerializer);
 				byte[] rawValueBytes = serializeValueNullSensitive(entry.getValue(), userValueSerializer);
@@ -255,7 +255,7 @@ class RocksDBMapState<K, N, UK, UV>
 	public void clear() {
 		try {
 			try (RocksIteratorWrapper iterator = RocksDBOperationUtils.getRocksIterator(backend.db, columnFamily);
-				RocksDBWriteBatchWrapper rocksDBWriteBatchWrapper = new RocksDBWriteBatchWrapper(backend.db, backend.getWriteOptions())) {
+				RocksDBWriteBatchWrapper rocksDBWriteBatchWrapper = new RocksDBWriteBatchWrapper(backend.db, backend.getWriteOptions(), backend.getWriteBatchSize())) {
 
 				final byte[] keyPrefixBytes = serializeCurrentKeyWithGroupAndNamespace();
 				iterator.seek(keyPrefixBytes);
diff --git a/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBStateBackend.java b/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBStateBackend.java
index 954ad36cfe9..529acd67ca3 100644
--- a/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBStateBackend.java
+++ b/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBStateBackend.java
@@ -71,9 +71,11 @@ import java.util.List;
 import java.util.Random;
 import java.util.UUID;
 
+import static org.apache.flink.contrib.streaming.state.RocksDBConfigurableOptions.WRITE_BATCH_SIZE;
 import static org.apache.flink.contrib.streaming.state.RocksDBOptions.CHECKPOINT_TRANSFER_THREAD_NUM;
 import static org.apache.flink.contrib.streaming.state.RocksDBOptions.TIMER_SERVICE_FACTORY;
 import static org.apache.flink.contrib.streaming.state.RocksDBOptions.TTL_COMPACT_FILTER_ENABLED;
+import static org.apache.flink.util.Preconditions.checkArgument;
 import static org.apache.flink.util.Preconditions.checkNotNull;
 
 /**
@@ -110,6 +112,7 @@ public class RocksDBStateBackend extends AbstractStateBackend implements Configu
 	private static boolean rocksDbInitialized = false;
 
 	private static final int UNDEFINED_NUMBER_OF_TRANSFER_THREADS = -1;
+	private static final long UNDEFINED_WRITE_BATCH_SIZE = -1;
 
 	// ------------------------------------------------------------------------
 
@@ -169,6 +172,11 @@ public class RocksDBStateBackend extends AbstractStateBackend implements Configu
 	/** Whether we already lazily initialized our local storage directories. */
 	private transient boolean isInitialized;
 
+	/**
+	 * Max consumed memory size for one batch in {@link RocksDBWriteBatchWrapper}, default value 2mb.
+	 */
+	private long writeBatchSize;
+
 	// ------------------------------------------------------------------------
 
 	/**
@@ -271,6 +279,7 @@ public class RocksDBStateBackend extends AbstractStateBackend implements Configu
 		this.defaultMetricOptions = new RocksDBNativeMetricOptions();
 		this.enableTtlCompactionFilter = TernaryBoolean.UNDEFINED;
 		this.memoryConfiguration = new RocksDBMemoryConfiguration();
+		this.writeBatchSize = UNDEFINED_WRITE_BATCH_SIZE;
 	}
 
 	/**
@@ -313,6 +322,11 @@ public class RocksDBStateBackend extends AbstractStateBackend implements Configu
 			this.numberOfTransferThreads = original.numberOfTransferThreads;
 		}
 
+		if (original.writeBatchSize == UNDEFINED_WRITE_BATCH_SIZE) {
+			this.writeBatchSize = config.get(WRITE_BATCH_SIZE).getBytes();
+		} else {
+			this.writeBatchSize = original.writeBatchSize;
+		}
 		this.enableTtlCompactionFilter = original.enableTtlCompactionFilter
 			.resolveUndefined(config.getBoolean(TTL_COMPACT_FILTER_ENABLED));
 
@@ -529,7 +543,8 @@ public class RocksDBStateBackend extends AbstractStateBackend implements Configu
 			.setEnableIncrementalCheckpointing(isIncrementalCheckpointsEnabled())
 			.setEnableTtlCompactionFilter(isTtlCompactionFilterEnabled())
 			.setNumberOfTransferingThreads(getNumberOfTransferThreads())
-			.setNativeMetricOptions(resourceContainer.getMemoryWatcherOptions(defaultMetricOptions));
+			.setNativeMetricOptions(resourceContainer.getMemoryWatcherOptions(defaultMetricOptions))
+			.setWriteBatchSize(getWriteBatchSize());
 		return builder.build();
 	}
 
@@ -862,6 +877,24 @@ public class RocksDBStateBackend extends AbstractStateBackend implements Configu
 		setNumberOfTransferThreads(numberOfTransferingThreads);
 	}
 
+	/**
+	 * Gets the max batch size will be used in {@link RocksDBWriteBatchWrapper}.
+	 */
+	public long getWriteBatchSize() {
+		return writeBatchSize == UNDEFINED_WRITE_BATCH_SIZE ?
+			WRITE_BATCH_SIZE.defaultValue().getBytes() : writeBatchSize;
+	}
+
+	/**
+	 * Sets the max batch size will be used in {@link RocksDBWriteBatchWrapper},
+	 * no positive value will disable memory size controller, just use item count controller.
+	 * @param writeBatchSize The size will used to be used in {@link RocksDBWriteBatchWrapper}.
+	 */
+	public void setWriteBatchSize(long writeBatchSize) {
+		checkArgument(writeBatchSize >= 0, "Write batch size have to be no negative.");
+		this.writeBatchSize = writeBatchSize;
+	}
+
 	// ------------------------------------------------------------------------
 	//  utilities
 	// ------------------------------------------------------------------------
@@ -888,6 +921,7 @@ public class RocksDBStateBackend extends AbstractStateBackend implements Configu
 				", localRocksDbDirectories=" + Arrays.toString(localRocksDbDirectories) +
 				", enableIncrementalCheckpointing=" + enableIncrementalCheckpointing +
 				", numberOfTransferThreads=" + numberOfTransferThreads +
+				", writeBatchSize=" + writeBatchSize +
 				'}';
 	}
 
diff --git a/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBWriteBatchWrapper.java b/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBWriteBatchWrapper.java
index 05adf4f787e..4e0048efc05 100644
--- a/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBWriteBatchWrapper.java
+++ b/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBWriteBatchWrapper.java
@@ -18,6 +18,7 @@
 
 package org.apache.flink.contrib.streaming.state;
 
+import org.apache.flink.annotation.VisibleForTesting;
 import org.apache.flink.util.IOUtils;
 import org.apache.flink.util.Preconditions;
 
@@ -27,6 +28,7 @@ import org.rocksdb.RocksDBException;
 import org.rocksdb.WriteBatch;
 import org.rocksdb.WriteOptions;
 
+import javax.annotation.Nonnegative;
 import javax.annotation.Nonnull;
 import javax.annotation.Nullable;
 
@@ -40,6 +42,8 @@ public class RocksDBWriteBatchWrapper implements AutoCloseable {
 	private static final int MIN_CAPACITY = 100;
 	private static final int MAX_CAPACITY = 1000;
 	private static final int PER_RECORD_BYTES = 100;
+	// default 0 for disable memory size based flush
+	private static final long DEFAULT_BATCH_SIZE = 0;
 
 	private final RocksDB db;
 
@@ -49,22 +53,35 @@ public class RocksDBWriteBatchWrapper implements AutoCloseable {
 
 	private final int capacity;
 
-	public RocksDBWriteBatchWrapper(@Nonnull RocksDB rocksDB) {
-		this(rocksDB, null, 500);
+	@Nonnegative
+	private final long batchSize;
+
+	public RocksDBWriteBatchWrapper(@Nonnull RocksDB rocksDB, long writeBatchSize) {
+		this(rocksDB, null, 500, writeBatchSize);
 	}
 
 	public RocksDBWriteBatchWrapper(@Nonnull RocksDB rocksDB, @Nullable WriteOptions options) {
-		this(rocksDB, options, 500);
+		this(rocksDB, options, 500, DEFAULT_BATCH_SIZE);
+	}
+
+	public RocksDBWriteBatchWrapper(@Nonnull RocksDB rocksDB, @Nullable WriteOptions options, long batchSize) {
+		this(rocksDB, options, 500, batchSize);
 	}
 
-	public RocksDBWriteBatchWrapper(@Nonnull RocksDB rocksDB, @Nullable WriteOptions options, int capacity) {
+	public RocksDBWriteBatchWrapper(@Nonnull RocksDB rocksDB, @Nullable WriteOptions options, int capacity, long batchSize) {
 		Preconditions.checkArgument(capacity >= MIN_CAPACITY && capacity <= MAX_CAPACITY,
 			"capacity should be between " + MIN_CAPACITY + " and " + MAX_CAPACITY);
+		Preconditions.checkArgument(batchSize >= 0, "Max batch size have to be no negative.");
 
 		this.db = rocksDB;
 		this.options = options;
 		this.capacity = capacity;
-		this.batch = new WriteBatch(this.capacity * PER_RECORD_BYTES);
+		this.batchSize = batchSize;
+		if (this.batchSize > 0) {
+			this.batch = new WriteBatch((int) Math.min(this.batchSize, this.capacity * PER_RECORD_BYTES));
+		} else {
+			this.batch = new WriteBatch(this.capacity * PER_RECORD_BYTES);
+		}
 	}
 
 	public void put(
@@ -74,9 +91,7 @@ public class RocksDBWriteBatchWrapper implements AutoCloseable {
 
 		batch.put(handle, key, value);
 
-		if (batch.count() == capacity) {
-			flush();
-		}
+		flushIfNeeded();
 	}
 
 	public void remove(
@@ -85,9 +100,7 @@ public class RocksDBWriteBatchWrapper implements AutoCloseable {
 
 		batch.remove(handle, key);
 
-		if (batch.count() == capacity) {
-			flush();
-		}
+		flushIfNeeded();
 	}
 
 	public void flush() throws RocksDBException {
@@ -113,4 +126,16 @@ public class RocksDBWriteBatchWrapper implements AutoCloseable {
 		}
 		IOUtils.closeQuietly(batch);
 	}
+
+	private void flushIfNeeded() throws RocksDBException {
+		boolean needFlush = batch.count() == capacity || (batchSize > 0 && getDataSize() >= batchSize);
+		if (needFlush) {
+			flush();
+		}
+	}
+
+	@VisibleForTesting
+	long getDataSize() {
+		return batch.getDataSize();
+	}
 }
diff --git a/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/restore/RocksDBFullRestoreOperation.java b/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/restore/RocksDBFullRestoreOperation.java
index 289ebdcf74a..72709210f2b 100644
--- a/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/restore/RocksDBFullRestoreOperation.java
+++ b/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/restore/RocksDBFullRestoreOperation.java
@@ -47,6 +47,7 @@ import org.rocksdb.ColumnFamilyOptions;
 import org.rocksdb.DBOptions;
 import org.rocksdb.RocksDBException;
 
+import javax.annotation.Nonnegative;
 import javax.annotation.Nonnull;
 
 import java.io.File;
@@ -61,6 +62,7 @@ import java.util.function.Function;
 import static org.apache.flink.contrib.streaming.state.snapshot.RocksSnapshotUtil.END_OF_KEY_GROUP_MARK;
 import static org.apache.flink.contrib.streaming.state.snapshot.RocksSnapshotUtil.clearMetaDataFollowsFlag;
 import static org.apache.flink.contrib.streaming.state.snapshot.RocksSnapshotUtil.hasMetaDataFollowsFlag;
+import static org.apache.flink.util.Preconditions.checkArgument;
 
 /**
  * Encapsulates the process of restoring a RocksDB instance from a full snapshot.
@@ -87,6 +89,11 @@ public class RocksDBFullRestoreOperation<K> extends AbstractRocksDBRestoreOperat
 	 */
 	private StreamCompressionDecorator keygroupStreamCompressionDecorator;
 
+	/**
+	 * Write batch size used in {@link RocksDBWriteBatchWrapper}.
+	 */
+	private final long writeBatchSize;
+
 	public RocksDBFullRestoreOperation(
 		KeyGroupRange keyGroupRange,
 		int keyGroupPrefixBytes,
@@ -102,7 +109,8 @@ public class RocksDBFullRestoreOperation<K> extends AbstractRocksDBRestoreOperat
 		RocksDBNativeMetricOptions nativeMetricOptions,
 		MetricGroup metricGroup,
 		@Nonnull Collection<KeyedStateHandle> restoreStateHandles,
-		@Nonnull RocksDbTtlCompactFiltersManager ttlCompactFiltersManager) {
+		@Nonnull RocksDbTtlCompactFiltersManager ttlCompactFiltersManager,
+		@Nonnegative long writeBatchSize) {
 		super(
 			keyGroupRange,
 			keyGroupPrefixBytes,
@@ -119,6 +127,8 @@ public class RocksDBFullRestoreOperation<K> extends AbstractRocksDBRestoreOperat
 			metricGroup,
 			restoreStateHandles,
 			ttlCompactFiltersManager);
+		checkArgument(writeBatchSize >= 0, "Write batch size have to be no negative.");
+		this.writeBatchSize = writeBatchSize;
 	}
 
 	/**
@@ -188,7 +198,7 @@ public class RocksDBFullRestoreOperation<K> extends AbstractRocksDBRestoreOperat
 	 */
 	private void restoreKVStateData() throws IOException, RocksDBException {
 		//for all key-groups in the current state handle...
-		try (RocksDBWriteBatchWrapper writeBatchWrapper = new RocksDBWriteBatchWrapper(db)) {
+		try (RocksDBWriteBatchWrapper writeBatchWrapper = new RocksDBWriteBatchWrapper(db, writeBatchSize)) {
 			for (Tuple2<Integer, Long> keyGroupOffset : currentKeyGroupsStateHandle.getGroupRangeOffsets()) {
 				int keyGroup = keyGroupOffset.f0;
 
diff --git a/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/restore/RocksDBIncrementalRestoreOperation.java b/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/restore/RocksDBIncrementalRestoreOperation.java
index 908800676cb..35b6babebca 100644
--- a/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/restore/RocksDBIncrementalRestoreOperation.java
+++ b/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/restore/RocksDBIncrementalRestoreOperation.java
@@ -59,6 +59,7 @@ import org.rocksdb.RocksDBException;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import javax.annotation.Nonnegative;
 import javax.annotation.Nonnull;
 
 import java.io.File;
@@ -77,6 +78,7 @@ import java.util.UUID;
 import java.util.function.Function;
 
 import static org.apache.flink.contrib.streaming.state.snapshot.RocksSnapshotUtil.SST_FILE_SUFFIX;
+import static org.apache.flink.util.Preconditions.checkArgument;
 
 /**
  * Encapsulates the process of restoring a RocksDB instance from an incremental snapshot.
@@ -88,6 +90,7 @@ public class RocksDBIncrementalRestoreOperation<K> extends AbstractRocksDBRestor
 	private final SortedMap<Long, Set<StateHandleID>> restoredSstFiles;
 	private long lastCompletedCheckpointId;
 	private UUID backendUID;
+	private final long writeBatchSize;
 
 	public RocksDBIncrementalRestoreOperation(
 		String operatorIdentifier,
@@ -105,7 +108,8 @@ public class RocksDBIncrementalRestoreOperation<K> extends AbstractRocksDBRestor
 		RocksDBNativeMetricOptions nativeMetricOptions,
 		MetricGroup metricGroup,
 		@Nonnull Collection<KeyedStateHandle> restoreStateHandles,
-		@Nonnull RocksDbTtlCompactFiltersManager ttlCompactFiltersManager) {
+		@Nonnull RocksDbTtlCompactFiltersManager ttlCompactFiltersManager,
+		@Nonnegative long writeBatchSize) {
 		super(keyGroupRange,
 			keyGroupPrefixBytes,
 			numberOfTransferringThreads,
@@ -125,6 +129,8 @@ public class RocksDBIncrementalRestoreOperation<K> extends AbstractRocksDBRestor
 		this.restoredSstFiles = new TreeMap<>();
 		this.lastCompletedCheckpointId = -1L;
 		this.backendUID = UUID.randomUUID();
+		checkArgument(writeBatchSize >= 0, "Write batch size have to be no negative.");
+		this.writeBatchSize = writeBatchSize;
 	}
 
 	/**
@@ -294,7 +300,7 @@ public class RocksDBIncrementalRestoreOperation<K> extends AbstractRocksDBRestor
 			try (RestoredDBInstance tmpRestoreDBInfo = restoreDBInstanceFromStateHandle(
 				(IncrementalRemoteKeyedStateHandle) rawStateHandle,
 				temporaryRestoreInstancePath);
-				RocksDBWriteBatchWrapper writeBatchWrapper = new RocksDBWriteBatchWrapper(this.db)) {
+				RocksDBWriteBatchWrapper writeBatchWrapper = new RocksDBWriteBatchWrapper(this.db, writeBatchSize)) {
 
 				List<ColumnFamilyDescriptor> tmpColumnFamilyDescriptors = tmpRestoreDBInfo.columnFamilyDescriptors;
 				List<ColumnFamilyHandle> tmpColumnFamilyHandles = tmpRestoreDBInfo.columnFamilyHandles;
@@ -345,7 +351,8 @@ public class RocksDBIncrementalRestoreOperation<K> extends AbstractRocksDBRestor
 				columnFamilyHandles,
 				keyGroupRange,
 				initialHandle.getKeyGroupRange(),
-				keyGroupPrefixBytes);
+				keyGroupPrefixBytes,
+				writeBatchSize);
 		} catch (RocksDBException e) {
 			String errMsg = "Failed to clip DB after initialization.";
 			LOG.error(errMsg, e);
diff --git a/flink-state-backends/flink-statebackend-rocksdb/src/test/java/org/apache/flink/contrib/streaming/state/RocksDBIncrementalCheckpointUtilsTest.java b/flink-state-backends/flink-statebackend-rocksdb/src/test/java/org/apache/flink/contrib/streaming/state/RocksDBIncrementalCheckpointUtilsTest.java
index 942d85cf5ea..5da795c8ad8 100644
--- a/flink-state-backends/flink-statebackend-rocksdb/src/test/java/org/apache/flink/contrib/streaming/state/RocksDBIncrementalCheckpointUtilsTest.java
+++ b/flink-state-backends/flink-statebackend-rocksdb/src/test/java/org/apache/flink/contrib/streaming/state/RocksDBIncrementalCheckpointUtilsTest.java
@@ -145,7 +145,8 @@ public class RocksDBIncrementalCheckpointUtilsTest extends TestLogger {
 				Collections.singletonList(columnFamilyHandle),
 				targetGroupRange,
 				currentGroupRange,
-				keyGroupPrefixBytes);
+				keyGroupPrefixBytes,
+				RocksDBConfigurableOptions.WRITE_BATCH_SIZE.defaultValue().getBytes());
 
 			for (int i = currentGroupRangeStart; i <= currentGroupRangeEnd; ++i) {
 				for (int j = 0; j < 100; ++j) {
diff --git a/flink-state-backends/flink-statebackend-rocksdb/src/test/java/org/apache/flink/contrib/streaming/state/RocksDBWriteBatchWrapperTest.java b/flink-state-backends/flink-statebackend-rocksdb/src/test/java/org/apache/flink/contrib/streaming/state/RocksDBWriteBatchWrapperTest.java
index 47983fbbef9..56c1f8979d5 100644
--- a/flink-state-backends/flink-statebackend-rocksdb/src/test/java/org/apache/flink/contrib/streaming/state/RocksDBWriteBatchWrapperTest.java
+++ b/flink-state-backends/flink-statebackend-rocksdb/src/test/java/org/apache/flink/contrib/streaming/state/RocksDBWriteBatchWrapperTest.java
@@ -31,6 +31,10 @@ import org.rocksdb.WriteOptions;
 
 import java.util.ArrayList;
 import java.util.List;
+import java.util.concurrent.ThreadLocalRandom;
+
+import static org.apache.flink.contrib.streaming.state.RocksDBConfigurableOptions.WRITE_BATCH_SIZE;
+import static org.junit.Assert.assertEquals;
 
 /**
  * Tests to guard {@link RocksDBWriteBatchWrapper}.
@@ -51,7 +55,7 @@ public class RocksDBWriteBatchWrapperTest {
 		try (RocksDB db = RocksDB.open(folder.newFolder().getAbsolutePath());
 			WriteOptions options = new WriteOptions().setDisableWAL(true);
 			ColumnFamilyHandle handle = db.createColumnFamily(new ColumnFamilyDescriptor("test".getBytes()));
-			RocksDBWriteBatchWrapper writeBatchWrapper = new RocksDBWriteBatchWrapper(db, options, 200)) {
+			RocksDBWriteBatchWrapper writeBatchWrapper = new RocksDBWriteBatchWrapper(db, options, 200, WRITE_BATCH_SIZE.defaultValue().getBytes())) {
 
 			// insert data
 			for (Tuple2<byte[], byte[]> item : data) {
@@ -65,4 +69,52 @@ public class RocksDBWriteBatchWrapperTest {
 			}
 		}
 	}
+
+	/**
+	 * Tests that {@link RocksDBWriteBatchWrapper} flushes after the memory consumed exceeds the preconfigured value.
+	 */
+	@Test
+	public void testWriteBatchWrapperFlushAfterMemorySizeExceed() throws Exception {
+		try (RocksDB db = RocksDB.open(folder.newFolder().getAbsolutePath());
+			WriteOptions options = new WriteOptions().setDisableWAL(true);
+			ColumnFamilyHandle handle = db.createColumnFamily(new ColumnFamilyDescriptor("test".getBytes()));
+			RocksDBWriteBatchWrapper writeBatchWrapper = new RocksDBWriteBatchWrapper(db, options, 200, 50)) {
+
+			long initBatchSize = writeBatchWrapper.getDataSize();
+			byte[] dummy = new byte[6];
+			ThreadLocalRandom.current().nextBytes(dummy);
+			// will add 1 + 1 + 1 + 6 + 1 + 6 = 16 bytes for each KV
+			// format is [handleType|kvType|keyLen|key|valueLen|value]
+			// more information please ref write_batch.cc in RocksDB
+			writeBatchWrapper.put(handle, dummy, dummy);
+			assertEquals(initBatchSize + 16, writeBatchWrapper.getDataSize());
+			writeBatchWrapper.put(handle, dummy, dummy);
+			assertEquals(initBatchSize + 32, writeBatchWrapper.getDataSize());
+			writeBatchWrapper.put(handle, dummy, dummy);
+			// will flush all, then an empty write batch
+			assertEquals(initBatchSize, writeBatchWrapper.getDataSize());
+		}
+	}
+
+	/**
+	 * Tests that {@link RocksDBWriteBatchWrapper} flushes after the kv count exceeds the preconfigured value.
+	 */
+	@Test
+	public void testWriteBatchWrapperFlushAfterCountExceed() throws Exception {
+		try (RocksDB db = RocksDB.open(folder.newFolder().getAbsolutePath());
+			WriteOptions options = new WriteOptions().setDisableWAL(true);
+			ColumnFamilyHandle handle = db.createColumnFamily(new ColumnFamilyDescriptor("test".getBytes()));
+			RocksDBWriteBatchWrapper writeBatchWrapper = new RocksDBWriteBatchWrapper(db, options, 100, 50000)) {
+			long initBatchSize = writeBatchWrapper.getDataSize();
+			byte[] dummy = new byte[2];
+			ThreadLocalRandom.current().nextBytes(dummy);
+			for (int i = 1; i < 100; ++i) {
+				writeBatchWrapper.put(handle, dummy, dummy);
+				// each kv consumes 8 bytes
+				assertEquals(initBatchSize + 8 * i, writeBatchWrapper.getDataSize());
+			}
+			writeBatchWrapper.put(handle, dummy, dummy);
+			assertEquals(initBatchSize, writeBatchWrapper.getDataSize());
+		}
+	}
 }
