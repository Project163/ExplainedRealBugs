diff --git a/flink-dist/src/main/assemblies/bin.xml b/flink-dist/src/main/assemblies/bin.xml
index 08c8fa7e35e..bc8bb4f4373 100644
--- a/flink-dist/src/main/assemblies/bin.xml
+++ b/flink-dist/src/main/assemblies/bin.xml
@@ -125,6 +125,9 @@ under the License.
 			<directory>../flink-python/bin/</directory>
 			<outputDirectory>bin</outputDirectory>
 			<fileMode>0755</fileMode>
+			<excludes>
+				<exclude>pyflink-udf-runner.*</exclude>
+			</excludes>
 		</fileSet>
 
 		<!-- copy SQL client -->
diff --git a/flink-python/src/main/resources/pyflink-udf-runner.bat b/flink-python/bin/pyflink-udf-runner.bat
similarity index 100%
rename from flink-python/src/main/resources/pyflink-udf-runner.bat
rename to flink-python/bin/pyflink-udf-runner.bat
diff --git a/flink-python/src/main/resources/pyflink-udf-runner.sh b/flink-python/bin/pyflink-udf-runner.sh
similarity index 100%
rename from flink-python/src/main/resources/pyflink-udf-runner.sh
rename to flink-python/bin/pyflink-udf-runner.sh
diff --git a/flink-python/pyflink/fn_execution/boot.py b/flink-python/pyflink/fn_execution/boot.py
index d3743971642..1286c91cea2 100644
--- a/flink-python/pyflink/fn_execution/boot.py
+++ b/flink-python/pyflink/fn_execution/boot.py
@@ -28,7 +28,6 @@ process mode. It downloads and installs users' python artifacts, then launches t
 harness of Apache Beam.
 """
 import argparse
-import hashlib
 import os
 from subprocess import call
 
@@ -38,17 +37,10 @@ import sys
 
 from apache_beam.portability.api.beam_provision_api_pb2_grpc import ProvisionServiceStub
 from apache_beam.portability.api.beam_provision_api_pb2 import GetProvisionInfoRequest
-from apache_beam.portability.api.beam_artifact_api_pb2_grpc import ArtifactRetrievalServiceStub
-from apache_beam.portability.api.beam_artifact_api_pb2 import (GetManifestRequest,
-                                                               GetArtifactRequest)
 from apache_beam.portability.api.endpoints_pb2 import ApiServiceDescriptor
 
-from distutils.dist import Distribution
-
 from google.protobuf import json_format, text_format
 
-from pkg_resources import get_distribution, parse_version
-
 
 def check_not_empty(check_str, error_message):
     if check_str == "":
@@ -58,81 +50,6 @@ def check_not_empty(check_str, error_message):
 
 python_exec = sys.executable
 
-PYTHON_REQUIREMENTS_FILE = "_PYTHON_REQUIREMENTS_FILE"
-PYTHON_REQUIREMENTS_CACHE = "_PYTHON_REQUIREMENTS_CACHE"
-PYTHON_REQUIREMENTS_INSTALL_DIR = "_PYTHON_REQUIREMENTS_INSTALL_DIR"
-
-
-def append_path_to_env(env, name, value):
-    if name in env:
-        env[name] = os.pathsep.join([value, env[name]])
-    else:
-        env[name] = value
-
-
-def get_site_packages_paths(prefix):
-    install_obj = Distribution().get_command_obj('install', create=True)
-    install_obj.prefix = prefix
-    install_obj.finalize_options()
-    installed_dir = [install_obj.install_purelib]
-    if install_obj.install_purelib != install_obj.install_platlib:
-        installed_dir.append(install_obj.install_platlib)
-    return installed_dir
-
-
-def get_prefix_option(requirements_install_path):
-    pip_version = get_distribution("pip").version
-    # since '--prefix' option is only supported for pip 8.0+, so here we fallback to
-    # use '--install-option' when the pip version is lower than 8.0.0.
-    if parse_version(pip_version) >= parse_version('8.0.0'):
-        return ["--prefix", requirements_install_path]
-    else:
-        return ['--install-option', '--prefix=' + requirements_install_path]
-
-
-def pip_install_requirements():
-    if (PYTHON_REQUIREMENTS_FILE in os.environ
-            and PYTHON_REQUIREMENTS_INSTALL_DIR in os.environ):
-        requirements_file_path = os.environ[PYTHON_REQUIREMENTS_FILE]
-        requirements_install_path = os.environ[PYTHON_REQUIREMENTS_INSTALL_DIR]
-        if PYTHON_REQUIREMENTS_CACHE in os.environ:
-            requirements_cache_path = os.environ[PYTHON_REQUIREMENTS_CACHE]
-        else:
-            requirements_cache_path = None
-
-        env = dict(os.environ)
-        installed_python_path = os.pathsep.join(get_site_packages_paths(requirements_install_path))
-        installed_python_script_path = os.path.join(requirements_install_path, "bin")
-        append_path_to_env(env, "PYTHONPATH", installed_python_path)
-        append_path_to_env(env, "PATH", installed_python_script_path)
-
-        pip_install_commands = [python_exec, "-m", "pip", "install", "--ignore-installed", "-r",
-                                requirements_file_path]
-        pip_install_commands.extend(get_prefix_option(requirements_install_path))
-        if requirements_cache_path is not None:
-            pip_install_commands.extend(["--find-links", requirements_cache_path])
-
-        max_retry_times = 3
-        cur_retry = 0
-        while cur_retry < max_retry_times:
-            cur_retry += 1
-            logging.info("Run command: %s with retry (%d/%d)\n" % (" ".join(pip_install_commands),
-                                                                   cur_retry, max_retry_times))
-            exit_code = call(
-                pip_install_commands, stdout=sys.stdout, stderr=sys.stderr, env=env)
-            if exit_code != 0:
-                if cur_retry < max_retry_times:
-                    logging.error("Run command: %s error! exit code: %d. Retry to run again!" %
-                                  (" ".join(pip_install_commands), exit_code))
-                else:
-                    raise Exception(
-                        "Run command: %s error! exit code: %d. Max retry times exhausted!" %
-                        (" ".join(pip_install_commands), exit_code))
-            else:
-                break
-        os.environ["PYTHONPATH"] = env["PYTHONPATH"]
-        os.environ["PATH"] = env["PATH"]
-
 
 if __name__ == "__main__":
     # print INFO and higher level messages
@@ -143,8 +60,6 @@ if __name__ == "__main__":
     parser.add_argument("--id", default="", help="Local identifier (required).")
     parser.add_argument("--logging_endpoint", default="",
                         help="Logging endpoint (required).")
-    parser.add_argument("--artifact_endpoint", default="",
-                        help="Artifact endpoint (required).")
     parser.add_argument("--provision_endpoint", default="",
                         help="Provision endpoint (required).")
     parser.add_argument("--control_endpoint", default="",
@@ -152,18 +67,16 @@ if __name__ == "__main__":
     parser.add_argument("--semi_persist_dir", default="/tmp",
                         help="Local semi-persistent directory (optional).")
 
-    args = parser.parse_args()
+    args = parser.parse_known_args()[0]
 
     worker_id = args.id
     logging_endpoint = args.logging_endpoint
-    artifact_endpoint = args.artifact_endpoint
     provision_endpoint = args.provision_endpoint
     control_endpoint = args.control_endpoint
     semi_persist_dir = args.semi_persist_dir
 
     check_not_empty(worker_id, "No id provided.")
     check_not_empty(logging_endpoint, "No logging endpoint provided.")
-    check_not_empty(artifact_endpoint, "No artifact endpoint provided.")
     check_not_empty(provision_endpoint, "No provision endpoint provided.")
     check_not_empty(control_endpoint, "No control endpoint provided.")
 
@@ -177,51 +90,6 @@ if __name__ == "__main__":
         info = client.GetProvisionInfo(GetProvisionInfoRequest(), metadata=metadata).info
         options = json_format.MessageToJson(info.pipeline_options)
 
-    staged_dir = os.path.join(semi_persist_dir, "staged")
-
-    # download files
-    with grpc.insecure_channel(artifact_endpoint) as channel:
-        client = ArtifactRetrievalServiceStub(channel=channel)
-        # get file list via retrieval token
-        response = client.GetManifest(GetManifestRequest(retrieval_token=info.retrieval_token),
-                                      metadata=metadata)
-        artifacts = response.manifest.artifact
-        # download files and check hash values
-        for artifact in artifacts:
-            name = artifact.name
-            permissions = artifact.permissions
-            sha256 = artifact.sha256
-            file_path = os.path.join(staged_dir, name)
-            if os.path.exists(file_path):
-                with open(file_path, "rb") as f:
-                    sha256obj = hashlib.sha256()
-                    sha256obj.update(f.read())
-                    hash_value = sha256obj.hexdigest()
-                if hash_value == sha256:
-                    logging.info("The file: %s already exists and its sha256 hash value: %s is the "
-                                 "same as the expected hash value, skipped." % (file_path, sha256))
-                    continue
-                else:
-                    os.remove(file_path)
-            if not os.path.exists(os.path.dirname(file_path)):
-                os.makedirs(os.path.dirname(file_path), 0o755)
-            stream = client.GetArtifact(
-                GetArtifactRequest(name=name, retrieval_token=info.retrieval_token),
-                metadata=metadata)
-            with open(file_path, "wb") as f:
-                sha256obj = hashlib.sha256()
-                for artifact_chunk in stream:
-                    sha256obj.update(artifact_chunk.data)
-                    f.write(artifact_chunk.data)
-                hash_value = sha256obj.hexdigest()
-            if hash_value != sha256:
-                raise Exception("The sha256 hash value: %s of the downloaded file: %s is not the"
-                                " same as the expected hash value: %s" %
-                                (hash_value, file_path, sha256))
-            os.chmod(file_path, int(str(permissions), 8))
-
-    pip_install_requirements()
-
     os.environ["WORKER_ID"] = worker_id
     os.environ["PIPELINE_OPTIONS"] = options
     os.environ["SEMI_PERSISTENT_DIRECTORY"] = semi_persist_dir
diff --git a/flink-python/pyflink/fn_execution/tests/process_mode_test_data.py b/flink-python/pyflink/fn_execution/tests/process_mode_test_data.py
deleted file mode 100644
index bd8b87e3910..00000000000
--- a/flink-python/pyflink/fn_execution/tests/process_mode_test_data.py
+++ /dev/null
@@ -1,114 +0,0 @@
-################################################################################
-#  Licensed to the Apache Software Foundation (ASF) under one
-#  or more contributor license agreements.  See the NOTICE file
-#  distributed with this work for additional information
-#  regarding copyright ownership.  The ASF licenses this file
-#  to you under the Apache License, Version 2.0 (the
-#  "License"); you may not use this file except in compliance
-#  with the License.  You may obtain a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-#  Unless required by applicable law or agreed to in writing, software
-#  distributed under the License is distributed on an "AS IS" BASIS,
-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-#  See the License for the specific language governing permissions and
-# limitations under the License.
-################################################################################
-
-# "manifest" stores the list of the python artifacts.
-# "name" is the artifact file name.
-# "permissions" is the permission of the file.
-# "sha256" is the sha256 hash value of the file content.
-manifest = """
-{
-  "manifest": {
-    "artifact": [{
-      "name": "requirements.txt",
-      "permissions": 444,
-      "sha256": "899f0fe6b9c337652484952d174c32b7adf63b9ddf47e8b84322f8673a14b6d8"
-    }, {
-      "name": "python-package1-0.0.0.tar.gz",
-      "permissions": 444,
-      "sha256": "52e324d57abe35c97ba85c6c8e01c575aec71d9f820577b1b2dfbcb9ca6c986c"
-    }, {
-      "name": "python-package2-0.0.0.tar.gz",
-      "permissions": 444,
-      "sha256": "0fc7f2401046f530c58670fba53ba01e55d1616523f6f084c3d3ffa03eea817f"
-    }, {
-      "name": "workflow.tar.gz",
-      "permissions": 444,
-      "sha256": "50087afe6c813eee62a87a817fc6faf410ea38508b64b4bbd43d8f1251d2f32d"
-    }, {
-      "name": "python-package3-0.0.0.tar.gz",
-      "permissions": 444,
-      "sha256": "e0a74121cdf1fc12e58fe14224c5d0fb074e1bbcfcee2791321712874c263897"
-    }, {
-      "name": "apache_beam-mocked-py2.py3-none-any.whl",
-      "permissions": 444,
-      "sha256": "b15fc95bfc64363402c57a5c92d8963ea1e03ce8ad4efb93f61ad49e54dcfdce"
-    }, {
-      "name": "extra_packages.txt",
-      "permissions": 444,
-      "sha256": "e78582559e40b381912289d3686b63fa9af3a409d27cf767c247c938c8e9443d"
-    }, {
-      "name": "dataflow_python_sdk.tar",
-      "permissions": 444,
-      "sha256": "834c8c8a2a498b10d562add48511a4dca2ba1b3b83edd6f28478fdaf6b8a688f"
-    }]
-  }
-}
-"""
-
-# "file_data" stores the actual contents of the python artifacts.
-# Its keys are file names of the artifacts.
-# Its values are the file contents encoded in base64 format.
-file_data = dict()
-
-file_data["workflow.tar.gz"] = """
-{
-  "data": "H4sICM1rXl0C/y92YXIvZm9sZGVycy9qdC93YjZrejZmMTFqMTg4NXczempnbjR2NjQwMDAwZ3AvVC90bXBIZ3ltUzYvVU5LTk9XTi0wLjAuMC50YXIA7Zlbb5swGIa55ldw2V5AsDlJlXoxdUd1S6pl2S6mCTEwFBUwArMt+/UjJBkJW9KmImSH9+HCYBsR6/Xrz/4yG1+PJx/Gqq7V10g6CnqNY1nL0rabUqfmslwiEcOijmlbDqn7EcM2DUmxpAGoSuEViiJ9v+VZ9JXFu/rV3cLwnkEuBrIu/xJmW/rfXL9QX42fT/rX3zbNnfob1OnobzomlRQd+h+dN0x4gSc89T0ryphnFwrRdHnspexCWc0N+WdTM0nkaZWmXjFv21/ylKm5F2288qQSt7zoPqss9eKkrX0d+ywrN157ykq/iHNRf0294plgmVDfzfPf92grbxJPhLxI2xoJPMb/JRNVrvlhNKj/ddPu+N8ybRP+H4KPLIrcOAv5J1l4kfu5ipNAuVSah3pdYPW9LsNO/5X/8/mw8Z+QZv+nO7S5Fv43EP+HIU5zXgilEV5wnpSy3N5rze1Z7vl3dXQvLzdawjgL3HXD2fk5nPQv+H/1pNVBQV0EhVFv/j/s/GfXBwCc//4A/QOWsyxgmT93kzi7KzXxTfS//9PJtv6UWlj/hwFbO6z/e/zfS0Lo8PyPYzkG/I/8D/I/J/b/dDJ7e/Vs+qiw/2D/U6vjf6obyP8Mw/rEL+9c+uV9k0J+0I7x116C527CvrCkaa5KVrg+D9jIdeMsFq67+EVtbcqDKmGk98QEuM//Wzody/86oR3/E0oc+H+Y4a9MhpAJ/4/aJbfPbxye/7Fsgv//T6r/RiDuRf/9+Z+N/L9pL85/DtGx/p9S/z63XAfpbznN+Z8S6A8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQJcfyOnXdABQAAA="
-}
-"""
-file_data["extra_packages.txt"] = """
-{
-  "data": "cHl0aG9uLXBhY2thZ2UzLTAuMC4wLnRhci5negphcGFjaGVfYmVhbS1tb2NrZWQtcHkyLnB5My1ub25lLWFueS53aGwK"
-}
-"""
-file_data["requirements.txt"] = """
-{
-  "data": "cHl0aG9uLXBhY2thZ2UxPT0wLjAuMApweXRob24tcGFja2FnZTI9PTAuMC4wCg=="
-}
-"""
-file_data["python-package1-0.0.0.tar.gz"] = """
-{
-  "data": "H4sICNefrV0C/2Rpc3QvcHl0aG9uLXBhY2thZ2UxLTAuMC4wLnRhcgDtmVtv2jAYhnPtX2H1CrRCY+ckIXEx7axuUA11u5imyICTRc1JiVnHfv1MKKWjYxwKEdPehws7xkmUfH5f+3PyqfqWpa1cjG5EKFnLbOvfhXFQTI3nOPPSdavS5Pa8nGMwy3Esi3ke9wyTObbnGNQxamBSKlFQavzUryG8ldG6frpbEGx4yNmDLMp/hPyP8b+6fNN613vdP1z8XdteG3+ug/17/F3Hcw1qIv5H54NUYiyUaH2SRRllaYeytkl6IpEdujI2yH2XapCQwSRJRDHt0OveZa//uUfeZonUvUO5bHo+0ZcoVo9bMhFRvGx9H41kWj447aUsR0WUq+pui8arWKggK5JliwGOo/95q79ovXi6/nfyf246Dof/n078fT9KI+X77Xx6BP83bX4Xf5NxT7dz7toO/L8OxjKgeTwpG+KcDpsdQjWFVJMipYI+o0MCk4X/t2UYtqI0yPabCHb3f861XcD/Ty/+Y5nLdCzT0dSPo/SmbKsf6un+b7KV+LsW4/D/OoC9w/930P9eGwM75//csrD+Q/6P/P/k9D/oX3988Wqw1bS/tf6tR+s/m3EG/ddBqXO9XKf15C8pP9k4HZBtBgzZaVW5vrfKcj+W32W82ygEB9D/Xu9+4/qfP9L/rBv0X1v87yONKRX61/qfzwqjIDzIPTbv/7or3/88i0H/tfBFW7s/s/avRInQH06ieEy7tDrQeYHUdRN7wP+n/vf62LOH/pld7f9xz7a5Pfufedy0oP86iJI8KxStAq6yLC4JWdbbVbWRikR2z1ZGytk5vauW3QdnBFE6XqwmykazCesAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOBw/AJw5CHBAFAAAA=="
-}
-"""
-file_data["python-package2-0.0.0.tar.gz"] = """
-{
-  "data": "H4sICO1VXl0C/2Rpc3QvcHl0aG9uLXBhY2thZ2UyLTAuMC4wLnRhcgDtmF1vmzAUhrnmV1i9aqVB+UaKlItp3+pGqkXtLqYJeWCYVb6ETVb262dgKS1alJAlLJvOE0U2J4cgcvy+PqSo+bc8Uwoc3OGYGIqmiteldFA0gWvb3eg47agZVjd2SLppG64l3k2ebtsihGxpAirGcYmQ9EP8DPF3QjflibQo2nKTzY2sx3+E4rf1v756o7zzXi8OV3/HsjbWXxR9UH/H0XQJaVD/o/OBcBxijpVbUjKaZzOkq5rs4ZTM0GBtyA8p7SKRl1Wa4rKeoRvvylt88uS3eUpEdkz60PNKfEU5PFZIimnSR9/TgGTs0WkvCQtKWvD2auvgdYJ5lJdpH5GA4+i/i/rrqEriWKFZlF/urf9R/m8Yhg3+f4r1D0lBspBkQe0nNLtjKr/nf+z/mjD7p/V3TN0B/58C8FDw/xH636sxHN3/GaZpQ/8H/R/0f6em/+Xi5uOLV8udtv3d9S8+e6p/Szct0P8UMMKrQi1qeavly7ssCnlU57g5m+eFn5AVScatNODI+t+rLuP7f8t1Yf+H/h/4S/rvdoUgig9yja36d83B/7+uaYD+J+GzsHa/sfYvMsex/7WiSYjmqD0QzwVEzLX2gK0yvyQr2jwBtEFwjv9d/0UtTaJ/3er3f8u1Gv0bGvT/k0DTIi85agvO8zxhstzP1XZ6nuGUzM8GK+XsGfo1ZfNHZ0Q0C9d9Izu/uACXAAAAAAAAOC1+AsFp+00AKAAA"
-}
-"""
-file_data["python-package3-0.0.0.tar.gz"] = """
-{
-  "data": "H4sICABWXl0C/2Rpc3QvcHl0aG9uLXBhY2thZ2UzLTAuMC4wLnRhcgDtmF1vmzAUhrnmV1i9aqVBwWCQIuVi2re6kWpR14tpQh4YZpUvgcnGfv0caJoONUrIEpZN58mF7RMTIMfv6wNFI77lmVbQ4I7GzNIMXX4ulYNiSFxCutZx2tbAdtd2KKZFsGsTa9k3TGI7poKIMgJ1JWiJkPJT/g3xd8Y3zZPTomjLTbYXf9/+IxRP5v/66o32zns9O1z+HdvemH/skl7+HeJgBRmQ/6PzgQkaUkG1T6yseJ5NkKkbqkdTNkG9taE+TGkXiTqv05SWzQTdeFfe7NZT3+Ypk7Njtg49r+VPlP2xxlLKk3X0PQ9YVj067CWrgpIXoj3bKnidUBHlZbqOKMBx9N9F/VVUZ3Gs8SzKL/fW/yD/xxhbGPz/BPMfsoJlIcuCxk94dlfp4of4Y/83DLOXf0e24P9jAB4K/j9A/3sVhoPrP2xJQP9Q/0H9d2L6n89uPr54Nd9p299d//K73/Vvm6YD+h+Diom60ItG3Wr56i6LQh1UOW6eLfLCT9iCJcNWGnBk/e+Vl+H1v+0S2P+h/gf+kv67XSGI4oOcY6v+Xav3/te1DHj/OwqfpbX7S2v/ogoa+19rnoRoitqBfC5gsm+0g2qR+SVb8OUTQBsE5/jf9V80yij6N+2H/R8Tkyz1j2VJAPofAZ4WeSlQm3CR50mlquu+3nbPM5qy6VlvpZw9Q/fdavroiIhn4apurM4vLsAlAAAAAAAATotfRtJCXwAoAAA="
-}
-"""
-file_data["apache_beam-mocked-py2.py3-none-any.whl"] = """
-{
-  "data": "UEsDBBQAAAAIADYyF08hin2WcQAAALAAAAAlAAAAYXBhY2hlX2JlYW0tbW9ja2VkLmRpc3QtaW5mby9NRVRBREFUQV3LMQ7CMBQD0P2fIhdIJRizsSEBAQkBs0kNjehvqjQduH23BrHZz/KJBS0K7J15imlwZttsxEPpDEaEjvZJqKyzpvBhK9dZFfnrzM0f/PnhZZ+UdsSblXZz6VL+75aK2Fc9xsBh+rldepRXylpF1iALUEsDBBQAAAAIADYyF08LJgBBXwAAAG4AAAAiAAAAYXBhY2hlX2JlYW0tbW9ja2VkLmRpc3QtaW5mby9XSEVFTAvPSE3N0Q1LLSrOzM+zUjDUM+ByT81LLUosyS+yUkhKySwuiS8HqVHQMNAzNtYz0eQKys8v0fUs1g0oLUrNyUyyUigpKk3lCklMt1IoqDTSzcvPS9VNzKuEiRgjRLgAUEsDBBQAAAAIADYyF0+TBtcyAwAAAAEAAAAqAAAAYXBhY2hlX2JlYW0tbW9ja2VkLmRpc3QtaW5mby90b3BfbGV2ZWwudHh04wIAUEsDBBQAAAAIADYyF09fYh5q0AAAAD0BAAAjAAAAYXBhY2hlX2JlYW0tbW9ja2VkLmRpc3QtaW5mby9SRUNPUkR9zUtygjAAANC9Zwk2oEA3XQSIIH4yk1Eo3TDhYwMBkikBO5zelVsu8B5TrOR1XtSsN3pZirraVs2ojWZ4yI8LvqEA3RAYObNs5ytMWmvSR9lAEiTp2CAnsKWcD3dSqaGLJl+3i/u8zuwEgek6G7aGpxHG57fMc5YMNvGsI6b39lI0lprKnubLksZa7OAu+545d+A1/gSmCddlLVXe1XPdbfW/fg+oUFFY/p4LQ9AxDofnAQst3GlRP8QvvczdG5lX/Z2ScATmuk+xT2gAwOYFUEsBAhQDFAAAAAgANjIXTyGKfZZxAAAAsAAAACUAAAAAAAAAAAAAAKSBAAAAAGFwYWNoZV9iZWFtLW1vY2tlZC5kaXN0LWluZm8vTUVUQURBVEFQSwECFAMUAAAACAA2MhdPCyYAQV8AAABuAAAAIgAAAAAAAAAAAAAApIG0AAAAYXBhY2hlX2JlYW0tbW9ja2VkLmRpc3QtaW5mby9XSEVFTFBLAQIUAxQAAAAIADYyF0+TBtcyAwAAAAEAAAAqAAAAAAAAAAAAAACkgVMBAABhcGFjaGVfYmVhbS1tb2NrZWQuZGlzdC1pbmZvL3RvcF9sZXZlbC50eHRQSwECFAMUAAAACAA2MhdPX2IeatAAAAA9AQAAIwAAAAAAAAAAAAAAtAGeAQAAYXBhY2hlX2JlYW0tbW9ja2VkLmRpc3QtaW5mby9SRUNPUkRQSwUGAAAAAAQABABMAQAArwIAAAAA"
-}
-"""
-file_data["dataflow_python_sdk.tar"] = """
-{
-  "data": "python-package5-0.0.0/                                                                              0000755 0000766 0000024 00000000000 13527453063 015474  5                                                                                                    ustar   zhongwei                        staff                           0000000 0000000                                                                                                                                                                        python-package5-0.0.0/PKG-INFO                                                                      0000644 0000766 0000024 00000000275 13527453063 016575  0                                                                                                    ustar   zhongwei                        staff                           0000000 0000000                                                                                                                                                                        Metadata-Version: 1.0
Name: python-package5
Version: 0.0.0
Summary: UNKNOWN
Home-page: UNKNOWN
Author: UNKNOWN
Author-email: UNKNOWN
License: UNKNOWN
Description: UNKNOWN
Platform: UNKNOWN
                                                                                                                                                                                                                                                                                                                                   python-package5-0.0.0/python_package5.egg-info/                                                     0000755 0000766 0000024 00000000000 13527453063 022247  5                                                                                                    ustar   zhongwei                        staff                           0000000 0000000                                                                                                                                                                        python-package5-0.0.0/python_package5.egg-info/dependency_links.txt                                 0000644 0000766 0000024 00000000001 13527453063 026315  0                                                                                                    ustar   zhongwei                        staff                           0000000 0000000                                                                                                                                                                        
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               python-package5-0.0.0/python_package5.egg-info/PKG-INFO                                             0000644 0000766 0000024 00000000275 13527453063 023350  0                                                                                                    ustar   zhongwei                        staff                           0000000 0000000                                                                                                                                                                        Metadata-Version: 1.0
Name: python-package5
Version: 0.0.0
Summary: UNKNOWN
Home-page: UNKNOWN
Author: UNKNOWN
Author-email: UNKNOWN
License: UNKNOWN
Description: UNKNOWN
Platform: UNKNOWN
                                                                                                                                                                                                                                                                                                                                   python-package5-0.0.0/python_package5.egg-info/SOURCES.txt                                          0000644 0000766 0000024 00000000244 13527453063 024133  0                                                                                                    ustar   zhongwei                        staff                           0000000 0000000                                                                                                                                                                        setup.py
python_package5.egg-info/PKG-INFO
python_package5.egg-info/SOURCES.txt
python_package5.egg-info/dependency_links.txt
python_package5.egg-info/top_level.txt                                                                                                                                                                                                                                                                                                                                                            python-package5-0.0.0/python_package5.egg-info/top_level.txt                                        0000644 0000766 0000024 00000000001 13527453063 024770  0                                                                                                    ustar   zhongwei                        staff                           0000000 0000000                                                                                                                                                                        
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               python-package5-0.0.0/setup.cfg                                                                     0000644 0000766 0000024 00000000073 13527453063 017315  0                                                                                                    ustar   zhongwei                        staff                           0000000 0000000                                                                                                                                                                        [egg_info]
tag_build = 
tag_date = 0
tag_svn_revision = 0

                                                                                                                                                                                                                                                                                                                                                                                                                                                                     python-package5-0.0.0/setup.py                                                                      0000644 0000766 0000024 00000000141 13527452537 017207  0                                                                                                    ustar   zhongwei                        staff                           0000000 0000000                                                                                                                                                                        import setuptools

setuptools.setup(name="python-package5", packages=setuptools.find_packages())
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               "
-}
-"""
-
-# Mocked provision info, the "retrievalToken" could be any non-empty value.
-test_provision_info_json = '''
-{
-  "retrievalToken": "test_token"
-}
-'''
diff --git a/flink-python/pyflink/fn_execution/tests/test_process_mode_boot.py b/flink-python/pyflink/fn_execution/tests/test_process_mode_boot.py
index 6d2a0a2b012..b3ed713f106 100644
--- a/flink-python/pyflink/fn_execution/tests/test_process_mode_boot.py
+++ b/flink-python/pyflink/fn_execution/tests/test_process_mode_boot.py
@@ -15,8 +15,6 @@
 #  See the License for the specific language governing permissions and
 # limitations under the License.
 ################################################################################
-import hashlib
-import json
 import os
 import shutil
 import socket
@@ -25,13 +23,8 @@ import sys
 import tempfile
 import time
 import unittest
-import uuid
-from stat import ST_MODE
 
 import grpc
-from apache_beam.portability.api.beam_artifact_api_pb2 import GetManifestResponse, ArtifactChunk
-from apache_beam.portability.api.beam_artifact_api_pb2_grpc import (
-    ArtifactRetrievalServiceServicer, add_ArtifactRetrievalServiceServicer_to_server)
 from apache_beam.portability.api.beam_provision_api_pb2 import (ProvisionInfo,
                                                                 GetProvisionInfoResponse)
 from apache_beam.portability.api.beam_provision_api_pb2_grpc import (
@@ -39,11 +32,6 @@ from apache_beam.portability.api.beam_provision_api_pb2_grpc import (
 from concurrent import futures
 from google.protobuf import json_format
 
-from pyflink.fn_execution.boot import (PYTHON_REQUIREMENTS_FILE,
-                                       PYTHON_REQUIREMENTS_CACHE,
-                                       PYTHON_REQUIREMENTS_INSTALL_DIR)
-from pyflink.fn_execution.tests.process_mode_test_data import (manifest, file_data,
-                                                               test_provision_info_json)
 from pyflink.java_gateway import get_gateway
 from pyflink.pyflink_gateway_server import on_windows
 from pyflink.testing.test_case_utils import PyFlinkTestCase
@@ -52,11 +40,7 @@ from pyflink.testing.test_case_utils import PyFlinkTestCase
 class PythonBootTests(PyFlinkTestCase):
 
     def setUp(self):
-        manifest_response = json_format.Parse(manifest, GetManifestResponse())
-        artifact_chunks = dict()
-        for file_name in file_data:
-            artifact_chunks[file_name] = json_format.Parse(file_data[file_name], ArtifactChunk())
-        provision_info = json_format.Parse(test_provision_info_json, ProvisionInfo())
+        provision_info = json_format.Parse('{"retrievalToken": "test_token"}', ProvisionInfo())
         response = GetProvisionInfoResponse(info=provision_info)
 
         def get_unused_port():
@@ -66,21 +50,6 @@ class PythonBootTests(PyFlinkTestCase):
             sock.close()
             return port
 
-        class ArtifactService(ArtifactRetrievalServiceServicer):
-            def GetManifest(self, request, context):
-                return manifest_response
-
-            def GetArtifact(self, request, context):
-                yield artifact_chunks[request.name]
-
-        def start_test_artifact_server():
-            server = grpc.server(futures.ThreadPoolExecutor(max_workers=1))
-            add_ArtifactRetrievalServiceServicer_to_server(ArtifactService(), server)
-            port = get_unused_port()
-            server.add_insecure_port('[::]:' + str(port))
-            server.start()
-            return server, port
-
         class ProvisionService(ProvisionServiceServicer):
             def GetProvisionInfo(self, request, context):
                 return response
@@ -93,7 +62,6 @@ class PythonBootTests(PyFlinkTestCase):
             server.start()
             return server, port
 
-        self.artifact_server, self.artifact_port = start_test_artifact_server()
         self.provision_server, self.provision_port = start_test_provision_server()
 
         self.env = dict(os.environ)
@@ -108,55 +76,21 @@ class PythonBootTests(PyFlinkTestCase):
         runner_script = "pyflink-udf-runner.bat" if on_windows() else \
             "pyflink-udf-runner.sh"
         self.runner_path = os.path.join(
-            flink_python_source_root, "src", "main", "resources", runner_script)
+            flink_python_source_root, "bin", runner_script)
 
     def run_boot_py(self):
         args = [self.runner_path, "--id", "1",
                 "--logging_endpoint", "localhost:0000",
-                "--artifact_endpoint", "localhost:%d" % self.artifact_port,
+                "--artifact_endpoint", "whatever",
                 "--provision_endpoint", "localhost:%d" % self.provision_port,
                 "--control_endpoint", "localhost:0000",
                 "--semi_persist_dir", self.tmp_dir]
 
         return subprocess.call(args, env=self.env)
 
-    def check_downloaded_files(self, staged_dir, manifest):
-        expected_files_info = json.loads(manifest)["manifest"]["artifact"]
-        files = os.listdir(staged_dir)
-        self.assertEqual(len(expected_files_info), len(files))
-        checked = 0
-        for file_name in files:
-            for file_info in expected_files_info:
-                if file_name == file_info["name"]:
-                    self.assertEqual(
-                        oct(os.stat(os.path.join(staged_dir, file_name))[ST_MODE])[-3:],
-                        str(file_info["permissions"]))
-                    with open(os.path.join(staged_dir, file_name), "rb") as f:
-                        sha256obj = hashlib.sha256()
-                        sha256obj.update(f.read())
-                        hash_value = sha256obj.hexdigest()
-                    self.assertEqual(hash_value, file_info["sha256"])
-                    checked += 1
-                    break
-        self.assertEqual(checked, len(files))
-
-    def check_installed_files(self, prefix_dir, package_list):
-        from distutils.dist import Distribution
-        install_obj = Distribution().get_command_obj('install', create=True)
-        install_obj.prefix = prefix_dir
-        install_obj.finalize_options()
-        installed_dir = [install_obj.install_purelib]
-        if install_obj.install_purelib != install_obj.install_platlib:
-            installed_dir.append(install_obj.install_platlib)
-        for package_name in package_list:
-            self.assertTrue(any([os.path.exists(os.path.join(package_dir, package_name))
-                                 for package_dir in installed_dir]))
-
     def test_python_boot(self):
-
         exit_code = self.run_boot_py()
         self.assertTrue(exit_code == 0, "the boot.py exited with non-zero code.")
-        self.check_downloaded_files(os.path.join(self.tmp_dir, "staged"), manifest)
 
     @unittest.skipIf(on_windows(), "'subprocess.check_output' in Windows always return empty "
                                    "string, skip this test.")
@@ -169,33 +103,17 @@ class PythonBootTests(PyFlinkTestCase):
         exit_message = subprocess.check_output(args, env=self.env).decode("utf-8")
         self.assertIn("No logging endpoint provided.", exit_message)
 
-        args = [self.runner_path, "--id", "1", "--logging_endpoint", "localhost:0000"]
-        exit_message = subprocess.check_output(args, env=self.env).decode("utf-8")
-        self.assertIn("No artifact endpoint provided.", exit_message)
-
         args = [self.runner_path, "--id", "1",
-                "--logging_endpoint", "localhost:0000",
-                "--artifact_endpoint", "localhost:%d" % self.artifact_port]
+                "--logging_endpoint", "localhost:0000"]
         exit_message = subprocess.check_output(args, env=self.env).decode("utf-8")
         self.assertIn("No provision endpoint provided.", exit_message)
 
         args = [self.runner_path, "--id", "1",
                 "--logging_endpoint", "localhost:0000",
-                "--artifact_endpoint", "localhost:%d" % self.artifact_port,
                 "--provision_endpoint", "localhost:%d" % self.provision_port]
         exit_message = subprocess.check_output(args, env=self.env).decode("utf-8")
         self.assertIn("No control endpoint provided.", exit_message)
 
-    def test_constant_consistency(self):
-        JProcessPythonEnvironmentManager = \
-            get_gateway().jvm.org.apache.flink.python.env.ProcessPythonEnvironmentManager
-        self.assertEqual(PYTHON_REQUIREMENTS_FILE,
-                         JProcessPythonEnvironmentManager.PYTHON_REQUIREMENTS_FILE)
-        self.assertEqual(PYTHON_REQUIREMENTS_CACHE,
-                         JProcessPythonEnvironmentManager.PYTHON_REQUIREMENTS_CACHE)
-        self.assertEqual(PYTHON_REQUIREMENTS_INSTALL_DIR,
-                         JProcessPythonEnvironmentManager.PYTHON_REQUIREMENTS_INSTALL_DIR)
-
     def test_set_working_directory(self):
         JProcessPythonEnvironmentManager = \
             get_gateway().jvm.org.apache.flink.python.env.ProcessPythonEnvironmentManager
@@ -227,37 +145,7 @@ class PythonBootTests(PyFlinkTestCase):
                          process_cwd,
                          "setting working directory variable is not work!")
 
-    def test_install_requirements_without_cached_dir(self):
-        requirements_txt_path = os.path.join(self.tmp_dir, "requirements_txt_" + str(uuid.uuid4()))
-        with open(requirements_txt_path, 'w') as f:
-            f.write("#test line continuation\ncloudpickle\\\n==1.2.2\npy4j==0.10.8.1")
-
-        self.env[PYTHON_REQUIREMENTS_FILE] = requirements_txt_path
-        requirements_target_dir_path = \
-            os.path.join(self.tmp_dir, "requirements_target_dir_" + str(uuid.uuid4()))
-        self.env[PYTHON_REQUIREMENTS_INSTALL_DIR] = requirements_target_dir_path
-
-        exit_code = self.run_boot_py()
-        self.assertTrue(exit_code == 0, "the boot.py exited with non-zero code.")
-        self.check_installed_files(requirements_target_dir_path, ["cloudpickle", "py4j"])
-
-    def test_install_requirements_with_cached_dir(self):
-        requirements_txt_path = os.path.join(self.tmp_dir, "requirements_txt_" + str(uuid.uuid4()))
-        with open(requirements_txt_path, 'w') as f:
-            f.write("python-package1==0.0.0")
-
-        self.env[PYTHON_REQUIREMENTS_FILE] = requirements_txt_path
-        self.env[PYTHON_REQUIREMENTS_CACHE] = os.path.join(self.tmp_dir, "staged")
-        requirements_target_dir_path = \
-            os.path.join(self.tmp_dir, "requirements_target_dir_" + str(uuid.uuid4()))
-        self.env[PYTHON_REQUIREMENTS_INSTALL_DIR] = requirements_target_dir_path
-
-        exit_code = self.run_boot_py()
-        self.assertTrue(exit_code == 0, "the boot.py exited with non-zero code.")
-        self.check_installed_files(requirements_target_dir_path, ["python_package1"])
-
     def tearDown(self):
-        self.artifact_server.stop(0)
         self.provision_server.stop(0)
         try:
             if self.tmp_dir is not None:
diff --git a/flink-python/pyflink/pyflink_gateway_server.py b/flink-python/pyflink/pyflink_gateway_server.py
index 3b407a1d02b..9f0adfc19e3 100644
--- a/flink-python/pyflink/pyflink_gateway_server.py
+++ b/flink-python/pyflink/pyflink_gateway_server.py
@@ -190,6 +190,9 @@ def launch_gateway_server_process(env, args):
     if "FLINK_TESTING" in env:
         download_apache_avro()
         classpath = os.pathsep.join([classpath, construct_test_classpath()])
+        # use the script in the directory of flink-python/bin in case of testing
+        env["PYFLINK_UDF_RUNNER_DIR"] = os.path.join(
+            os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "bin")
     program_args = construct_program_args(args)
     if program_args.cluster_type == "local":
         command = [java_executable] + log_settings + ["-cp", classpath, program_args.main_class] \
diff --git a/flink-python/pyflink/table/tests/test_dependency.py b/flink-python/pyflink/table/tests/test_dependency.py
index 2c856eea448..4b63784b522 100644
--- a/flink-python/pyflink/table/tests/test_dependency.py
+++ b/flink-python/pyflink/table/tests/test_dependency.py
@@ -15,7 +15,6 @@
 #  See the License for the specific language governing permissions and
 # limitations under the License.
 ################################################################################
-import json
 import os
 import shutil
 import sys
@@ -130,9 +129,24 @@ class BlinkStreamDependencyTests(DependencyTests, PyFlinkBlinkStreamTableTestCas
         os.mkdir(requirements_dir_path)
         package_file_name = "python-package1-0.0.0.tar.gz"
         with open(os.path.join(requirements_dir_path, package_file_name), 'wb') as f:
-            from pyflink.fn_execution.tests.process_mode_test_data import file_data
             import base64
-            f.write(base64.b64decode(json.loads(file_data[package_file_name])["data"]))
+            # This base64 data is encoded from a python package file which includes a
+            # "python_package1" module. The module contains a "plus(a, b)" function.
+            # The base64 can be recomputed by following code:
+            # base64.b64encode(open("python-package1-0.0.0.tar.gz", "rb").read()).decode("utf-8")
+            f.write(base64.b64decode(
+                "H4sICNefrV0C/2Rpc3QvcHl0aG9uLXBhY2thZ2UxLTAuMC4wLnRhcgDtmVtv2jAYhnPtX2H1CrRCY+ckI"
+                "XEx7axuUA11u5imyICTRc1JiVnHfv1MKKWjYxwKEdPehws7xkmUfH5f+3PyqfqWpa1cjG5EKFnLbOvfhX"
+                "FQTI3nOPPSdavS5Pa8nGMwy3Esi3ke9wyTObbnGNQxamBSKlFQavzUryG8ldG6frpbEGx4yNmDLMp/hPy"
+                "P8b+6fNN613vdP1z8XdteG3+ug/17/F3Hcw1qIv5H54NUYiyUaH2SRRllaYeytkl6IpEdujI2yH2XapCQ"
+                "wSRJRDHt0OveZa//uUfeZonUvUO5bHo+0ZcoVo9bMhFRvGx9H41kWj447aUsR0WUq+pui8arWKggK5Jli"
+                "wGOo/95q79ovXi6/nfyf246Dof/n078fT9KI+X77Xx6BP83bX4Xf5NxT7dz7toO/L8OxjKgeTwpG+KcDp"
+                "sdQjWFVJMipYI+o0MCk4X/t2UYtqI0yPabCHb3f861XcD/Ty/+Y5nLdCzT0dSPo/SmbKsf6un+b7KV+Ls"
+                "W4/D/OoC9w/930P9eGwM75//csrD+Q/6P/P/k9D/oX3988Wqw1bS/tf6tR+s/m3EG/ddBqXO9XKf15C8p"
+                "P9k4HZBtBgzZaVW5vrfKcj+W32W82ygEB9D/Xu9+4/qfP9L/rBv0X1v87yONKRX61/qfzwqjIDzIPTbv/"
+                "7or3/88i0H/tfBFW7s/s/avRInQH06ieEy7tDrQeYHUdRN7wP+n/vf62LOH/pld7f9xz7a5Pfufedy0oP"
+                "86iJI8KxStAq6yLC4JWdbbVbWRikR2z1ZGytk5vauW3QdnBFE6XqwmykazCesAAAAAAAAAAAAAAAAAAAA"
+                "AAAAAAAAAAAAAAOBw/AJw5CHBAFAAAA=="))
         self.t_env.set_python_requirements(requirements_txt_path, requirements_dir_path)
 
         def add_one(i):
diff --git a/flink-python/setup.py b/flink-python/setup.py
index 76c8da861e1..22ca08a8a1e 100644
--- a/flink-python/setup.py
+++ b/flink-python/setup.py
@@ -115,6 +115,8 @@ SCRIPTS_TEMP_PATH = os.path.join(TEMP_PATH, "bin")
 LICENSE_FILE_TEMP_PATH = os.path.join(this_directory, "LICENSE")
 NOTICE_FILE_TEMP_PATH = os.path.join(this_directory, "NOTICE")
 README_FILE_TEMP_PATH = os.path.join("pyflink", "README.txt")
+PYFLINK_UDF_RUNNER_SH = "pyflink-udf-runner.sh"
+PYFLINK_UDF_RUNNER_BAT = "pyflink-udf-runner.bat"
 
 in_flink_source = os.path.isfile("../flink-java/src/main/java/org/apache/flink/api/java/"
                                  "ExecutionEnvironment.java")
@@ -182,7 +184,6 @@ run sdist.
             os.symlink(CONF_PATH, CONF_TEMP_PATH)
             os.symlink(EXAMPLES_PATH, EXAMPLES_TEMP_PATH)
             os.symlink(PLUGINS_PATH, PLUGINS_TEMP_PATH)
-            os.symlink(SCRIPTS_PATH, SCRIPTS_TEMP_PATH)
             os.symlink(LICENSE_FILE_PATH, LICENSE_FILE_TEMP_PATH)
             os.symlink(README_FILE_PATH, README_FILE_TEMP_PATH)
         else:
@@ -194,13 +195,20 @@ run sdist.
             copytree(CONF_PATH, CONF_TEMP_PATH)
             copytree(EXAMPLES_PATH, EXAMPLES_TEMP_PATH)
             copytree(PLUGINS_PATH, PLUGINS_TEMP_PATH)
-            copytree(SCRIPTS_PATH, SCRIPTS_TEMP_PATH)
             copy(LICENSE_FILE_PATH, LICENSE_FILE_TEMP_PATH)
             copy(README_FILE_PATH, README_FILE_TEMP_PATH)
         os.mkdir(LOG_TEMP_PATH)
         with open(os.path.join(LOG_TEMP_PATH, "empty.txt"), 'w') as f:
             f.write("This file is used to force setuptools to include the log directory. "
                     "You can delete it at any time after installation.")
+
+        # copy the udf runner scripts
+        copytree(SCRIPTS_PATH, SCRIPTS_TEMP_PATH)
+        copy(os.path.join(this_directory, "bin", PYFLINK_UDF_RUNNER_SH),
+             os.path.join(SCRIPTS_TEMP_PATH, PYFLINK_UDF_RUNNER_SH))
+        copy(os.path.join(this_directory, "bin", PYFLINK_UDF_RUNNER_BAT),
+             os.path.join(SCRIPTS_TEMP_PATH, PYFLINK_UDF_RUNNER_BAT))
+
         if exist_licenses and platform.system() != "Windows":
             # regenerate the licenses directory and NOTICE file as we only copy part of the
             # flink binary distribution.
diff --git a/flink-python/src/main/java/org/apache/flink/python/env/ProcessPythonEnvironmentManager.java b/flink-python/src/main/java/org/apache/flink/python/env/ProcessPythonEnvironmentManager.java
index a0c54948b50..e1ec2978dec 100644
--- a/flink-python/src/main/java/org/apache/flink/python/env/ProcessPythonEnvironmentManager.java
+++ b/flink-python/src/main/java/org/apache/flink/python/env/ProcessPythonEnvironmentManager.java
@@ -20,8 +20,8 @@ package org.apache.flink.python.env;
 
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.annotation.VisibleForTesting;
-import org.apache.flink.python.util.ResourceUtil;
-import org.apache.flink.python.util.ZipUtil;
+import org.apache.flink.python.util.PythonEnvironmentManagerUtils;
+import org.apache.flink.python.util.ZipUtils;
 import org.apache.flink.util.FileUtils;
 import org.apache.flink.util.ShutdownHookUtil;
 
@@ -164,14 +164,24 @@ public final class ProcessPythonEnvironmentManager implements PythonEnvironmentM
 	}
 
 	@Override
-	public RunnerApi.Environment createEnvironment() throws IOException, InterruptedException {
+	public RunnerApi.Environment createEnvironment() throws IOException {
 		Map<String, String> env = constructEnvironmentVariables();
-		File runnerScript = ResourceUtil.extractUdfRunner(baseDirectory);
+
+		if (dependencyInfo.getRequirementsFilePath().isPresent()) {
+			LOG.info("Trying to pip install the Python requirements...");
+			PythonEnvironmentManagerUtils.pipInstallRequirements(
+				dependencyInfo.getRequirementsFilePath().get(),
+				dependencyInfo.getRequirementsCacheDir().orElse(null),
+				requirementsDirectory,
+				dependencyInfo.getPythonExec(),
+				env);
+		}
+		String runnerScript = PythonEnvironmentManagerUtils.getPythonUdfRunnerScript(dependencyInfo.getPythonExec(), env);
 
 		return Environments.createProcessEnvironment(
 			"",
 			"",
-			runnerScript.getPath(),
+			runnerScript,
 			env);
 	}
 
@@ -206,7 +216,7 @@ public final class ProcessPythonEnvironmentManager implements PythonEnvironmentM
 	 */
 	@VisibleForTesting
 	Map<String, String> constructEnvironmentVariables()
-			throws IOException, IllegalArgumentException, InterruptedException {
+			throws IOException, IllegalArgumentException {
 		Map<String, String> env = new HashMap<>(this.systemEnv);
 
 		constructFilesDirectory(env);
@@ -221,7 +231,7 @@ public final class ProcessPythonEnvironmentManager implements PythonEnvironmentM
 		// disable the launching of gateway server to prevent from this dead loop:
 		// launch UDF worker -> import udf -> import job code
 		//        ^                                    | (If the job code is not enclosed in a
-		//        									   |  if name == 'main' statement)
+		//        |                                    |  if name == 'main' statement)
 		//        |                                    V
 		// execute job in local mode <- launch gateway server and submit job to local executor
 		env.put(PYFLINK_GATEWAY_DISABLED, "true");
@@ -283,7 +293,7 @@ public final class ProcessPythonEnvironmentManager implements PythonEnvironmentM
 
 			// extract archives to archives directory
 			for (Map.Entry<String, String> entry : dependencyInfo.getArchives().entrySet()) {
-				ZipUtil.extractZipFileWithPermissions(
+				ZipUtils.extractZipFileWithPermissions(
 					entry.getKey(), String.join(File.separator, archivesDirectory, entry.getValue()));
 			}
 		}
diff --git a/flink-python/src/main/java/org/apache/flink/python/util/PythonDependencyUtils.java b/flink-python/src/main/java/org/apache/flink/python/util/PythonDependencyUtils.java
index 3790fe7ee80..f71fe2cb341 100644
--- a/flink-python/src/main/java/org/apache/flink/python/util/PythonDependencyUtils.java
+++ b/flink-python/src/main/java/org/apache/flink/python/util/PythonDependencyUtils.java
@@ -17,6 +17,7 @@
 
 package org.apache.flink.python.util;
 
+import org.apache.flink.annotation.Internal;
 import org.apache.flink.api.common.cache.DistributedCache;
 import org.apache.flink.api.java.tuple.Tuple2;
 import org.apache.flink.configuration.ConfigOption;
@@ -51,6 +52,7 @@ import static org.apache.flink.python.PythonOptions.PYTHON_EXECUTABLE;
  * Utility class for Python dependency management. The dependencies will be registered at the distributed
  * cache.
  */
+@Internal
 public class PythonDependencyUtils {
 
 	public static final String FILE = "file";
diff --git a/flink-python/src/main/java/org/apache/flink/python/util/PythonEnvironmentManagerUtils.java b/flink-python/src/main/java/org/apache/flink/python/util/PythonEnvironmentManagerUtils.java
new file mode 100644
index 00000000000..7ef9275e016
--- /dev/null
+++ b/flink-python/src/main/java/org/apache/flink/python/util/PythonEnvironmentManagerUtils.java
@@ -0,0 +1,210 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.python.util;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.annotation.VisibleForTesting;
+import org.apache.flink.util.OperatingSystem;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import javax.annotation.Nullable;
+
+import java.io.BufferedInputStream;
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Map;
+
+/**
+ * Utils used to prepare the python environment.
+ */
+@Internal
+public class PythonEnvironmentManagerUtils {
+
+	private static final Logger LOG = LoggerFactory.getLogger(PythonEnvironmentManagerUtils.class);
+
+	private static final int MAX_RETRY_TIMES = 3;
+
+	private static final String PYFLINK_UDF_RUNNER_SH = "pyflink-udf-runner.sh";
+	private static final String PYFLINK_UDF_RUNNER_BAT = "pyflink-udf-runner.bat";
+
+	@VisibleForTesting
+	public static final String PYFLINK_UDF_RUNNER_DIR = "PYFLINK_UDF_RUNNER_DIR";
+
+	private static final String GET_SITE_PACKAGES_PATH_SCRIPT =
+		"import sys;" +
+		"from distutils.dist import Distribution;" +
+		"install_obj = Distribution().get_command_obj('install', create=True);" +
+		"install_obj.prefix = sys.argv[1];" +
+		"install_obj.finalize_options();" +
+		"installed_dir = [install_obj.install_purelib];" +
+		"install_obj.install_purelib != install_obj.install_platlib and " +
+			"installed_dir.append(install_obj.install_platlib);" +
+		"print(installed_dir[0]);" +
+		"len(installed_dir) > 1 and " +
+			"print(installed_dir[1])";
+
+	private static final String CHECK_PIP_VERSION_SCRIPT =
+		"import sys;" +
+		"from pkg_resources import get_distribution, parse_version;" +
+		"pip_version = get_distribution('pip').version;" +
+		"print(parse_version(pip_version) >= parse_version(sys.argv[1]))";
+
+	private static final String GET_RUNNER_DIR_SCRIPT =
+		"import pyflink;" +
+		"import os;" +
+		"print(os.path.join(os.path.abspath(os.path.dirname(pyflink.__file__)), 'bin'))";
+
+	/**
+	 * Installs the 3rd party libraries listed in the user-provided requirements file. An optional
+	 * requirements cached directory can be provided to support offline installation. In order not
+	 * to populate the public environment, the libraries will be installed to the specified
+	 * directory, and added to the PYTHONPATH of the UDF workers.
+	 *
+	 * @param requirementsFilePath The path of the requirements file.
+	 * @param requirementsCacheDir The path of the requirements cached directory.
+	 * @param requirementsInstallDir The target directory of the installation.
+	 * @param pythonExecutable The python interpreter used to launch the pip program.
+	 * @param environmentVariables The environment variables used to launch the pip program.
+	 */
+	public static void pipInstallRequirements(
+			String requirementsFilePath,
+			@Nullable String requirementsCacheDir,
+			String requirementsInstallDir,
+			String pythonExecutable,
+			Map<String, String> environmentVariables) throws IOException {
+		String sitePackagesPath = getSitePackagesPath(requirementsInstallDir, pythonExecutable, environmentVariables);
+		String path = String.join(File.pathSeparator, requirementsInstallDir, "bin");
+		appendToEnvironmentVariable("PYTHONPATH", sitePackagesPath, environmentVariables);
+		appendToEnvironmentVariable("PATH", path, environmentVariables);
+
+		List<String> commands = new ArrayList<>(Arrays.asList(
+			pythonExecutable, "-m", "pip", "install", "--ignore-installed", "-r", requirementsFilePath));
+		if (isPipVersionGreaterEqual("8.0.0", pythonExecutable, environmentVariables)) {
+			commands.addAll(Arrays.asList("--prefix", requirementsInstallDir));
+		} else {
+			commands.addAll(Arrays.asList("--install-option", "--prefix=" + requirementsInstallDir));
+		}
+		if (requirementsCacheDir != null) {
+			commands.addAll(Arrays.asList("--find-links", requirementsCacheDir));
+		}
+
+		int retries = 0;
+		while (true) {
+			try {
+				execute(commands.toArray(new String[0]), environmentVariables, true);
+				break;
+			} catch (Throwable t) {
+				retries++;
+				if (retries < MAX_RETRY_TIMES) {
+					LOG.warn(String.format("Pip install failed, retrying... (%d/%d)", retries, MAX_RETRY_TIMES), t);
+				} else {
+					LOG.error(String.format("Pip install failed, already retried %d time...", retries));
+					throw new IOException(t);
+				}
+			}
+		}
+	}
+
+	public static String getPythonUdfRunnerScript(
+			String pythonExecutable,
+			Map<String, String> environmentVariables) throws IOException {
+		String runnerDir;
+		if (environmentVariables.containsKey(PYFLINK_UDF_RUNNER_DIR)) {
+			runnerDir = environmentVariables.get(PYFLINK_UDF_RUNNER_DIR);
+		} else {
+			String[] commands = new String[] { pythonExecutable, "-c", GET_RUNNER_DIR_SCRIPT};
+			String out = execute(commands, environmentVariables, false);
+			runnerDir = out.trim();
+		}
+		String runnerScriptPath;
+		if (OperatingSystem.isWindows()) {
+			runnerScriptPath = String.join(File.separator, runnerDir, PYFLINK_UDF_RUNNER_BAT);
+		} else {
+			runnerScriptPath = String.join(File.separator, runnerDir, PYFLINK_UDF_RUNNER_SH);
+		}
+		return runnerScriptPath;
+	}
+
+	private static String getSitePackagesPath(
+			String prefix,
+			String pythonExecutable,
+			Map<String, String> environmentVariables) throws IOException {
+		String[] commands = new String[] { pythonExecutable, "-c", GET_SITE_PACKAGES_PATH_SCRIPT, prefix };
+		String out = execute(commands, environmentVariables, false);
+		return String.join(File.pathSeparator, out.trim().split("\n"));
+	}
+
+	private static boolean isPipVersionGreaterEqual(
+			String pipVersion,
+			String pythonExecutable,
+			Map<String, String> environmentVariables) throws IOException {
+		String[] commands = new String[] { pythonExecutable, "-c", CHECK_PIP_VERSION_SCRIPT, pipVersion };
+		String out = execute(commands, environmentVariables, false);
+		return Boolean.parseBoolean(out.trim());
+	}
+
+	private static String execute(
+			String[] commands,
+			Map<String, String> environmentVariables,
+			boolean logDetails) throws IOException {
+		ProcessBuilder pb = new ProcessBuilder(commands);
+		pb.environment().putAll(environmentVariables);
+		pb.redirectErrorStream(true);
+		Process p = pb.start();
+		InputStream in = new BufferedInputStream(p.getInputStream());
+		StringBuilder out = new StringBuilder();
+		String s;
+		if (logDetails) {
+			LOG.info(String.format("Executing command: %s", String.join(" ", commands)));
+		}
+		try (BufferedReader br = new BufferedReader(new InputStreamReader(in))) {
+			while ((s = br.readLine()) != null) {
+				out.append(s).append("\n");
+				if (logDetails) {
+					LOG.info(s);
+				}
+			}
+		}
+		try {
+			if (p.waitFor() != 0) {
+				throw new IOException(String.format(
+					"Failed to execute the command: %s\noutput: %s", String.join(" ", commands), out));
+			}
+		} catch (InterruptedException e) {
+			// Ignored. The subprocess is dead after "br.readLine()" returns null, so the call of
+			// "waitFor" should return intermediately.
+		}
+		return out.toString();
+	}
+
+	private static void appendToEnvironmentVariable(String key, String value, Map<String, String> env) {
+		if (env.containsKey(key)) {
+			env.put(key, String.join(File.pathSeparator, value, env.get(key)));
+		} else {
+			env.put(key, value);
+		}
+	}
+}
diff --git a/flink-python/src/main/java/org/apache/flink/python/util/ResourceUtil.java b/flink-python/src/main/java/org/apache/flink/python/util/ResourceUtil.java
deleted file mode 100644
index 2c4ab74285b..00000000000
--- a/flink-python/src/main/java/org/apache/flink/python/util/ResourceUtil.java
+++ /dev/null
@@ -1,78 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.python.util;
-
-import org.apache.flink.util.OperatingSystem;
-
-import java.io.File;
-import java.io.IOException;
-import java.nio.file.Files;
-import java.nio.file.Paths;
-
-/**
- * Utils for building the most basic environment for running python udf workers. The basic environment does not include
- * python part of Apache Beam. Users need to prepare it themselves.
- */
-public class ResourceUtil {
-
-	public static final String PYFLINK_UDF_RUNNER_SH = "pyflink-udf-runner.sh";
-	public static final String PYFLINK_UDF_RUNNER_BAT = "pyflink-udf-runner.bat";
-
-	public static File extractUdfRunner(String tmpdir) throws IOException, InterruptedException {
-		File file;
-		if (OperatingSystem.isWindows()) {
-			file = new File(tmpdir, PYFLINK_UDF_RUNNER_BAT);
-		} else {
-			file = new File(tmpdir, PYFLINK_UDF_RUNNER_SH);
-		}
-		// TODO: This is a hacky solution to prevent subprocesses to hold the file descriptor of shell scripts,
-		// which will cause the execution of shell scripts failed with the exception "test file is busy"
-		// randomly. It's a bug of JDK, see https://bugs.openjdk.java.net/browse/JDK-8068370. After moving flink
-		// python jar to lib directory, we can solve this problem elegantly by extracting these files only once.
-		String javaExecutable = String.join(File.separator, System.getProperty("java.home"), "bin", "java");
-		String classPath = new File(
-			ResourceUtil.class.getProtectionDomain().getCodeSource().getLocation().getPath()).getAbsolutePath();
-		new ProcessBuilder(
-			javaExecutable,
-			"-cp",
-			classPath,
-			ResourceUtil.class.getName(),
-			tmpdir,
-			file.getName()).inheritIO().start().waitFor();
-		return file;
-	}
-
-	/**
-	 * This main method is used to create the shell script in a subprocess, see the "TODO" hints in method
-	 * {@link ResourceUtil#extractUdfRunner}.
-	 * @param args First argument is the directory where shell script will be created. Second argument is the prefix of
-	 *             the shell script. Third argument is the fileName of the shell script.
-	 * @throws IOException
-	 */
-	public static void main(String[] args) throws IOException {
-		String tmpdir = args[0];
-		String fileName = args[1];
-		File file = new File(tmpdir, fileName);
-
-		Files.copy(
-			ResourceUtil.class.getClassLoader().getResourceAsStream(fileName), Paths.get(file.getAbsolutePath()));
-
-		file.setExecutable(true);
-	}
-}
diff --git a/flink-python/src/main/java/org/apache/flink/python/util/ZipUtil.java b/flink-python/src/main/java/org/apache/flink/python/util/ZipUtils.java
similarity index 98%
rename from flink-python/src/main/java/org/apache/flink/python/util/ZipUtil.java
rename to flink-python/src/main/java/org/apache/flink/python/util/ZipUtils.java
index 8b24985b1ad..f1a100f5e46 100644
--- a/flink-python/src/main/java/org/apache/flink/python/util/ZipUtil.java
+++ b/flink-python/src/main/java/org/apache/flink/python/util/ZipUtils.java
@@ -18,6 +18,7 @@
 
 package org.apache.flink.python.util;
 
+import org.apache.flink.annotation.Internal;
 import org.apache.flink.util.IOUtils;
 import org.apache.flink.util.OperatingSystem;
 
@@ -39,7 +40,8 @@ import java.util.Set;
 /**
  * Utils used to extract zip files and try to restore the origin permissions of files.
  */
-public class ZipUtil {
+@Internal
+public class ZipUtils {
 
 	public static void extractZipFileWithPermissions(String zipFilePath, String targetPath) throws IOException {
 		try (ZipFile zipFile = new ZipFile(zipFilePath)) {
diff --git a/flink-python/src/test/java/org/apache/flink/python/env/ProcessPythonEnvironmentManagerTest.java b/flink-python/src/test/java/org/apache/flink/python/env/ProcessPythonEnvironmentManagerTest.java
index ecc40e25036..b4f9035a2b8 100644
--- a/flink-python/src/test/java/org/apache/flink/python/env/ProcessPythonEnvironmentManagerTest.java
+++ b/flink-python/src/test/java/org/apache/flink/python/env/ProcessPythonEnvironmentManagerTest.java
@@ -21,7 +21,6 @@ package org.apache.flink.python.env;
 import org.apache.flink.util.FileUtils;
 import org.apache.flink.util.OperatingSystem;
 
-import org.apache.beam.model.pipeline.v1.RunnerApi;
 import org.apache.commons.compress.archivers.zip.ZipArchiveEntry;
 import org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream;
 import org.junit.AfterClass;
@@ -255,24 +254,6 @@ public class ProcessPythonEnvironmentManagerTest {
 		}
 	}
 
-	@Test
-	public void testCreateEnvironment() throws Exception {
-		PythonDependencyInfo dependencyInfo = new PythonDependencyInfo(
-			new HashMap<>(), null, null, new HashMap<>(), "python");
-
-		try (ProcessPythonEnvironmentManager environmentManager = createBasicPythonEnvironmentManager(dependencyInfo)) {
-			environmentManager.open();
-			RunnerApi.Environment environment = environmentManager.createEnvironment();
-			RunnerApi.ProcessPayload payload = RunnerApi.ProcessPayload.parseFrom(environment.getPayload());
-
-			assertEquals(
-				String.join(File.separator, environmentManager.getBaseDirectory(), "pyflink-udf-runner.sh"),
-				payload.getCommand());
-			Map<String, String> expectedEnv = getBasicExpectedEnv(environmentManager);
-			assertEquals(expectedEnv, payload.getEnvMap());
-		}
-	}
-
 	@Test
 	public void testCreateRetrievalToken() throws Exception {
 		PythonDependencyInfo dependencyInfo = new PythonDependencyInfo(
diff --git a/flink-python/src/test/java/org/apache/flink/python/util/ResourceUtilTest.java b/flink-python/src/test/java/org/apache/flink/python/util/ResourceUtilTest.java
deleted file mode 100644
index 273ea13dff2..00000000000
--- a/flink-python/src/test/java/org/apache/flink/python/util/ResourceUtilTest.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.python.util;
-
-import org.apache.flink.util.FileUtils;
-
-import org.junit.Test;
-
-import java.io.File;
-import java.io.IOException;
-import java.util.UUID;
-
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertTrue;
-
-/**
- * Tests for {@link ResourceUtil}.
- */
-public class ResourceUtilTest {
-
-	@Test
-	public void testExtractUdfRunnerFromResource() throws IOException, InterruptedException {
-		File tmpdir = File.createTempFile(UUID.randomUUID().toString(), null);
-		tmpdir.delete();
-		tmpdir.mkdirs();
-		Thread hook = new Thread(() -> {
-			try {
-				FileUtils.deleteDirectory(tmpdir);
-			} catch (IOException e) {
-				throw new RuntimeException(e);
-			}
-		});
-		Runtime.getRuntime().addShutdownHook(hook);
-		try {
-			File file = ResourceUtil.extractUdfRunner(tmpdir.getAbsolutePath());
-			assertEquals(file, new File(tmpdir, "pyflink-udf-runner.sh"));
-			assertTrue(file.canExecute());
-		} finally {
-			hook.run();
-			Runtime.getRuntime().removeShutdownHook(hook);
-		}
-	}
-}
diff --git a/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/PythonScalarFunctionOperatorTest.java b/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/PythonScalarFunctionOperatorTest.java
index cc6c8be924e..aad8a3fc702 100644
--- a/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/PythonScalarFunctionOperatorTest.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/PythonScalarFunctionOperatorTest.java
@@ -99,7 +99,7 @@ public class PythonScalarFunctionOperatorTest extends PythonScalarFunctionOperat
 				getRuntimeContext().getTaskName(),
 				resultReceiver,
 				scalarFunctions,
-				pythonEnvironmentManager,
+				PythonTestUtils.createTestEnvironmentManager(),
 				userDefinedFunctionInputType,
 				userDefinedFunctionOutputType,
 				jobOptions,
diff --git a/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/RowDataPythonScalarFunctionOperatorTest.java b/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/RowDataPythonScalarFunctionOperatorTest.java
index 99ab1bd7852..25244857758 100644
--- a/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/RowDataPythonScalarFunctionOperatorTest.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/RowDataPythonScalarFunctionOperatorTest.java
@@ -125,7 +125,7 @@ public class RowDataPythonScalarFunctionOperatorTest
 				getRuntimeContext().getTaskName(),
 				resultReceiver,
 				scalarFunctions,
-				pythonEnvironmentManager,
+				PythonTestUtils.createTestEnvironmentManager(),
 				userDefinedFunctionInputType,
 				userDefinedFunctionOutputType,
 				jobOptions,
diff --git a/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/arrow/ArrowPythonScalarFunctionOperatorTest.java b/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/arrow/ArrowPythonScalarFunctionOperatorTest.java
index 4316bc98533..c6f5515db4c 100644
--- a/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/arrow/ArrowPythonScalarFunctionOperatorTest.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/arrow/ArrowPythonScalarFunctionOperatorTest.java
@@ -97,7 +97,7 @@ public class ArrowPythonScalarFunctionOperatorTest extends PythonScalarFunctionO
 				getRuntimeContext().getTaskName(),
 				resultReceiver,
 				scalarFunctions,
-				pythonEnvironmentManager,
+				PythonTestUtils.createTestEnvironmentManager(),
 				userDefinedFunctionInputType,
 				userDefinedFunctionOutputType,
 				getPythonConfig().getMaxArrowBatchSize(),
diff --git a/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/arrow/RowDataArrowPythonScalarFunctionOperatorTest.java b/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/arrow/RowDataArrowPythonScalarFunctionOperatorTest.java
index fdf25d83542..e1742c5fd07 100644
--- a/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/arrow/RowDataArrowPythonScalarFunctionOperatorTest.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/arrow/RowDataArrowPythonScalarFunctionOperatorTest.java
@@ -121,7 +121,7 @@ public class RowDataArrowPythonScalarFunctionOperatorTest
 				getRuntimeContext().getTaskName(),
 				resultReceiver,
 				scalarFunctions,
-				pythonEnvironmentManager,
+				PythonTestUtils.createTestEnvironmentManager(),
 				userDefinedFunctionInputType,
 				userDefinedFunctionOutputType,
 				getPythonConfig().getMaxArrowBatchSize(),
diff --git a/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/scalar/AbstractPythonScalarFunctionRunnerTest.java b/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/scalar/AbstractPythonScalarFunctionRunnerTest.java
index 515c8d08e2a..b372111ddc5 100644
--- a/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/scalar/AbstractPythonScalarFunctionRunnerTest.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/scalar/AbstractPythonScalarFunctionRunnerTest.java
@@ -32,7 +32,7 @@ import java.util.Collections;
  *
  * @param <IN> Type of the input elements.
  */
-public abstract class AbstractPythonScalarFunctionRunnerTest<IN>  {
+public abstract class AbstractPythonScalarFunctionRunnerTest<IN> {
 
 	protected AbstractPythonScalarFunctionRunner<IN> createSingleUDFRunner() throws Exception {
 		PythonFunctionInfo[] pythonFunctionInfos = new PythonFunctionInfo[] {
diff --git a/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/scalar/PythonScalarFunctionRunnerTest.java b/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/scalar/PythonScalarFunctionRunnerTest.java
index 9eba787ff4b..0b8b5d52050 100644
--- a/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/scalar/PythonScalarFunctionRunnerTest.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/scalar/PythonScalarFunctionRunnerTest.java
@@ -22,8 +22,6 @@ import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.api.java.typeutils.runtime.RowSerializer;
 import org.apache.flink.core.memory.DataOutputViewStreamWrapper;
 import org.apache.flink.fnexecution.v1.FlinkFnApi;
-import org.apache.flink.python.env.ProcessPythonEnvironmentManager;
-import org.apache.flink.python.env.PythonDependencyInfo;
 import org.apache.flink.python.env.PythonEnvironmentManager;
 import org.apache.flink.table.functions.python.PythonFunctionInfo;
 import org.apache.flink.table.runtime.typeutils.PythonTypeUtils;
@@ -46,6 +44,7 @@ import java.util.HashMap;
 import java.util.Map;
 import java.util.Objects;
 
+import static org.apache.flink.table.runtime.utils.PythonTestUtils.createTestEnvironmentManager;
 import static org.junit.Assert.assertEquals;
 import static org.mockito.ArgumentMatchers.any;
 import static org.mockito.ArgumentMatchers.argThat;
@@ -199,11 +198,7 @@ public class PythonScalarFunctionRunnerTest extends AbstractPythonScalarFunction
 			// ignore the execution results
 		};
 
-		final PythonEnvironmentManager environmentManager =
-			new ProcessPythonEnvironmentManager(
-				new PythonDependencyInfo(new HashMap<>(), null, null, new HashMap<>(), "python"),
-				new String[] {System.getProperty("java.io.tmpdir")},
-				new HashMap<>());
+		final PythonEnvironmentManager environmentManager = createTestEnvironmentManager();
 
 		return new PythonScalarFunctionRunner(
 			"testPythonRunner",
@@ -226,11 +221,7 @@ public class PythonScalarFunctionRunnerTest extends AbstractPythonScalarFunction
 
 		RowType rowType = new RowType(Collections.singletonList(new RowType.RowField("f1", new BigIntType())));
 
-		final PythonEnvironmentManager environmentManager =
-			new ProcessPythonEnvironmentManager(
-				new PythonDependencyInfo(new HashMap<>(), null, null, new HashMap<>(), "python"),
-				new String[] {System.getProperty("java.io.tmpdir")},
-				new HashMap<>());
+		final PythonEnvironmentManager environmentManager = createTestEnvironmentManager();
 
 		return new PassThroughPythonScalarFunctionRunner<Row>(
 			"testPythonRunner",
diff --git a/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/scalar/RowDataPythonScalarFunctionRunnerTest.java b/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/scalar/RowDataPythonScalarFunctionRunnerTest.java
index 3c67b14d611..cd10954ba6e 100644
--- a/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/scalar/RowDataPythonScalarFunctionRunnerTest.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/scalar/RowDataPythonScalarFunctionRunnerTest.java
@@ -19,8 +19,6 @@
 package org.apache.flink.table.runtime.runners.python.scalar;
 
 import org.apache.flink.api.common.typeutils.TypeSerializer;
-import org.apache.flink.python.env.ProcessPythonEnvironmentManager;
-import org.apache.flink.python.env.PythonDependencyInfo;
 import org.apache.flink.python.env.PythonEnvironmentManager;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.functions.python.PythonFunctionInfo;
@@ -32,8 +30,8 @@ import org.apache.beam.sdk.fn.data.FnDataReceiver;
 import org.junit.Test;
 
 import java.util.Collections;
-import java.util.HashMap;
 
+import static org.apache.flink.table.runtime.utils.PythonTestUtils.createTestEnvironmentManager;
 import static org.junit.Assert.assertEquals;
 
 /**
@@ -78,11 +76,7 @@ public class RowDataPythonScalarFunctionRunnerTest extends AbstractPythonScalarF
 			// ignore the execution results
 		};
 
-		final PythonEnvironmentManager environmentManager =
-			new ProcessPythonEnvironmentManager(
-				new PythonDependencyInfo(new HashMap<>(), null, null, new HashMap<>(), "python"),
-				new String[] {System.getProperty("java.io.tmpdir")},
-				new HashMap<>());
+		final PythonEnvironmentManager environmentManager = createTestEnvironmentManager();
 
 		return new RowDataPythonScalarFunctionRunner(
 			"testPythonRunner",
diff --git a/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/scalar/arrow/ArrowPythonScalarFunctionRunnerTest.java b/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/scalar/arrow/ArrowPythonScalarFunctionRunnerTest.java
index 3d6b36eec66..2e0451cfdf5 100644
--- a/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/scalar/arrow/ArrowPythonScalarFunctionRunnerTest.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/scalar/arrow/ArrowPythonScalarFunctionRunnerTest.java
@@ -18,8 +18,6 @@
 
 package org.apache.flink.table.runtime.runners.python.scalar.arrow;
 
-import org.apache.flink.python.env.ProcessPythonEnvironmentManager;
-import org.apache.flink.python.env.PythonDependencyInfo;
 import org.apache.flink.python.env.PythonEnvironmentManager;
 import org.apache.flink.table.functions.python.PythonFunctionInfo;
 import org.apache.flink.table.runtime.arrow.ArrowUtils;
@@ -44,6 +42,7 @@ import java.util.Collections;
 import java.util.HashMap;
 import java.util.Map;
 
+import static org.apache.flink.table.runtime.utils.PythonTestUtils.createTestEnvironmentManager;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertTrue;
 import static org.mockito.ArgumentMatchers.any;
@@ -160,11 +159,7 @@ public class ArrowPythonScalarFunctionRunnerTest extends AbstractPythonScalarFun
 		int maxArrowBatchSize,
 		JobBundleFactory jobBundleFactory) {
 
-		final PythonEnvironmentManager environmentManager =
-			new ProcessPythonEnvironmentManager(
-				new PythonDependencyInfo(new HashMap<>(), null, null, new HashMap<>(), "python"),
-				new String[] {System.getProperty("java.io.tmpdir")},
-				new HashMap<>());
+		final PythonEnvironmentManager environmentManager = createTestEnvironmentManager();
 
 		return new PassThroughArrowPythonScalarFunctionRunner<Row>(
 			"testPythonRunner",
diff --git a/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/table/PythonTableFunctionRunnerTest.java b/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/table/PythonTableFunctionRunnerTest.java
index 6cfce8bb47d..2f443feb03c 100644
--- a/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/table/PythonTableFunctionRunnerTest.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/table/PythonTableFunctionRunnerTest.java
@@ -21,8 +21,6 @@ package org.apache.flink.table.runtime.runners.python.table;
 import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.api.java.typeutils.runtime.RowSerializer;
 import org.apache.flink.fnexecution.v1.FlinkFnApi;
-import org.apache.flink.python.env.ProcessPythonEnvironmentManager;
-import org.apache.flink.python.env.PythonDependencyInfo;
 import org.apache.flink.python.env.PythonEnvironmentManager;
 import org.apache.flink.python.metric.FlinkMetricContainer;
 import org.apache.flink.table.functions.python.PythonFunctionInfo;
@@ -39,8 +37,8 @@ import org.junit.Test;
 
 import java.io.IOException;
 import java.util.Collections;
-import java.util.HashMap;
 
+import static org.apache.flink.table.runtime.utils.PythonTestUtils.createTestEnvironmentManager;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertTrue;
 
@@ -86,11 +84,7 @@ public class PythonTableFunctionRunnerTest extends AbstractPythonTableFunctionRu
 			// ignore the execution results
 		};
 
-		final PythonEnvironmentManager environmentManager =
-			new ProcessPythonEnvironmentManager(
-				new PythonDependencyInfo(new HashMap<>(), null, null, new HashMap<>(), "python"),
-				new String[]{System.getProperty("java.io.tmpdir")},
-				new HashMap<>());
+		final PythonEnvironmentManager environmentManager = createTestEnvironmentManager();
 
 		return new PythonTableFunctionRunner(
 			"testPythonRunner",
@@ -111,11 +105,7 @@ public class PythonTableFunctionRunnerTest extends AbstractPythonTableFunctionRu
 
 		RowType rowType = new RowType(Collections.singletonList(new RowType.RowField("f1", new BigIntType())));
 
-		final PythonEnvironmentManager environmentManager =
-			new ProcessPythonEnvironmentManager(
-				new PythonDependencyInfo(new HashMap<>(), null, null, new HashMap<>(), "python"),
-				new String[]{System.getProperty("java.io.tmpdir")},
-				new HashMap<>());
+		final PythonEnvironmentManager environmentManager = createTestEnvironmentManager();
 
 		return new PythonTableFunctionRunnerTestHarness(
 			"testPythonRunner",
diff --git a/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/table/RowDataPythonTableFunctionRunnerTest.java b/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/table/RowDataPythonTableFunctionRunnerTest.java
index b31c655e914..0f4d84ef406 100644
--- a/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/table/RowDataPythonTableFunctionRunnerTest.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/table/RowDataPythonTableFunctionRunnerTest.java
@@ -19,8 +19,6 @@
 package org.apache.flink.table.runtime.runners.python.table;
 
 import org.apache.flink.api.common.typeutils.TypeSerializer;
-import org.apache.flink.python.env.ProcessPythonEnvironmentManager;
-import org.apache.flink.python.env.PythonDependencyInfo;
 import org.apache.flink.python.env.PythonEnvironmentManager;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.functions.python.PythonFunctionInfo;
@@ -32,8 +30,8 @@ import org.apache.beam.sdk.fn.data.FnDataReceiver;
 import org.junit.Test;
 
 import java.util.Collections;
-import java.util.HashMap;
 
+import static org.apache.flink.table.runtime.utils.PythonTestUtils.createTestEnvironmentManager;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertTrue;
 
@@ -63,11 +61,7 @@ public class RowDataPythonTableFunctionRunnerTest extends AbstractPythonTableFun
 			// ignore the execution results
 		};
 
-		final PythonEnvironmentManager environmentManager =
-			new ProcessPythonEnvironmentManager(
-				new PythonDependencyInfo(new HashMap<>(), null, null, new HashMap<>(), "python"),
-				new String[]{System.getProperty("java.io.tmpdir")},
-				new HashMap<>());
+		final PythonEnvironmentManager environmentManager = createTestEnvironmentManager();
 
 		return new RowDataPythonTableFunctionRunner(
 			"testPythonRunner",
diff --git a/flink-python/src/test/java/org/apache/flink/table/runtime/utils/PythonTestUtils.java b/flink-python/src/test/java/org/apache/flink/table/runtime/utils/PythonTestUtils.java
index df8edccecca..31772805210 100644
--- a/flink-python/src/test/java/org/apache/flink/table/runtime/utils/PythonTestUtils.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/utils/PythonTestUtils.java
@@ -18,7 +18,11 @@
 
 package org.apache.flink.table.runtime.utils;
 
+import org.apache.flink.python.env.ProcessPythonEnvironmentManager;
+import org.apache.flink.python.env.PythonDependencyInfo;
+import org.apache.flink.python.env.PythonEnvironmentManager;
 import org.apache.flink.python.metric.FlinkMetricContainer;
+import org.apache.flink.python.util.PythonEnvironmentManagerUtils;
 import org.apache.flink.runtime.metrics.NoOpMetricRegistry;
 import org.apache.flink.runtime.metrics.groups.GenericMetricGroup;
 import org.apache.flink.runtime.metrics.groups.MetricGroupTest;
@@ -65,4 +69,13 @@ public final class PythonTestUtils {
 				new MetricGroupTest.DummyAbstractMetricGroup(NoOpMetricRegistry.INSTANCE),
 				"root"));
 	}
+
+	public static PythonEnvironmentManager createTestEnvironmentManager() {
+		Map<String, String> env = new HashMap<>();
+		env.put(PythonEnvironmentManagerUtils.PYFLINK_UDF_RUNNER_DIR, "");
+		return new ProcessPythonEnvironmentManager(
+			new PythonDependencyInfo(new HashMap<>(), null, null, new HashMap<>(), "python"),
+			new String[] {System.getProperty("java.io.tmpdir")},
+			env);
+	}
 }
