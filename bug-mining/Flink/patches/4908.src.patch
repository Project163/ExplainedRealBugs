diff --git a/docs/dev/batch/hadoop_compatibility.md b/docs/dev/batch/hadoop_compatibility.md
index 78248e9bc9c..be26996e963 100644
--- a/docs/dev/batch/hadoop_compatibility.md
+++ b/docs/dev/batch/hadoop_compatibility.md
@@ -64,7 +64,17 @@ and Reducers.
 </dependency>
 {% endhighlight %}
 
-See also **[how to configure hadoop dependencies]({{ site.baseurl }}/deployment/resource-providers/hadoop.html#add-hadoop-classpaths)**.
+If you want to run your Flink application locally (from your IDE), you also need to add 
+a `hadoop-client` dependency such as:
+
+{% highlight xml %}
+<dependency>
+    <groupId>org.apache.hadoop</groupId>
+    <artifactId>hadoop-client</artifactId>
+    <version>2.8.3</version>
+    <scope>provided</scope>
+</dependency>
+{% endhighlight %}
 
 ### Using Hadoop InputFormats
 
diff --git a/docs/dev/batch/hadoop_compatibility.zh.md b/docs/dev/batch/hadoop_compatibility.zh.md
index f098d9ae186..a85cbf09c2f 100644
--- a/docs/dev/batch/hadoop_compatibility.zh.md
+++ b/docs/dev/batch/hadoop_compatibility.zh.md
@@ -64,7 +64,14 @@ and Reducers.
 </dependency>
 {% endhighlight %}
 
-See also **[how to configure hadoop dependencies]({{ site.baseurl }}/deployment/resource-providers/hadoop.html#add-hadoop-classpaths)**.
+{% highlight xml %}
+<dependency>
+    <groupId>org.apache.hadoop</groupId>
+    <artifactId>hadoop-client</artifactId>
+    <version>2.8.3</version>
+    <scope>provided</scope>
+</dependency>
+{% endhighlight %}
 
 ### Using Hadoop InputFormats
 
diff --git a/docs/dev/project-configuration.md b/docs/dev/project-configuration.md
index 7757978091e..0a81ccbef4f 100644
--- a/docs/dev/project-configuration.md
+++ b/docs/dev/project-configuration.md
@@ -152,8 +152,8 @@ for details on how to build Flink for a specific Scala version.
 *(The only exception being when using existing Hadoop input-/output formats with Flink's Hadoop compatibility wrappers)*
 
 If you want to use Flink with Hadoop, you need to have a Flink setup that includes the Hadoop dependencies, rather than
-adding Hadoop as an application dependency. Please refer to the [Hadoop Setup Guide]({{ site.baseurl }}/deployment/resource-providers/hadoop.html)
-for details.
+adding Hadoop as an application dependency. Flink will use the Hadoop dependencies specified by the `HADOOP_CLASSPATH`
+environment variable, which can usually be set by calling `export HADOOP_CLASSPATH=``hadoop classpath```
 
 There are two main reasons for that design:
 
diff --git a/docs/dev/project-configuration.zh.md b/docs/dev/project-configuration.zh.md
index 7757978091e..0a81ccbef4f 100644
--- a/docs/dev/project-configuration.zh.md
+++ b/docs/dev/project-configuration.zh.md
@@ -152,8 +152,8 @@ for details on how to build Flink for a specific Scala version.
 *(The only exception being when using existing Hadoop input-/output formats with Flink's Hadoop compatibility wrappers)*
 
 If you want to use Flink with Hadoop, you need to have a Flink setup that includes the Hadoop dependencies, rather than
-adding Hadoop as an application dependency. Please refer to the [Hadoop Setup Guide]({{ site.baseurl }}/deployment/resource-providers/hadoop.html)
-for details.
+adding Hadoop as an application dependency. Flink will use the Hadoop dependencies specified by the `HADOOP_CLASSPATH`
+environment variable, which can usually be set by calling `export HADOOP_CLASSPATH=``hadoop classpath```
 
 There are two main reasons for that design:
 
diff --git a/docs/dev/table/connectors/hive/index.md b/docs/dev/table/connectors/hive/index.md
index a9949266797..9e13c7d60f6 100644
--- a/docs/dev/table/connectors/hive/index.md
+++ b/docs/dev/table/connectors/hive/index.md
@@ -92,8 +92,11 @@ to make the integration work in Table API program or SQL in SQL Client.
 Alternatively, you can put these dependencies in a dedicated folder, and add them to classpath with the `-C`
 or `-l` option for Table API program or SQL Client respectively.
 
-Apache Hive is built on Hadoop, so you need Hadoop dependency first, please refer to
-[Providing Hadoop classes]({{ site.baseurl }}/deployment/resource-providers/hadoop.html#providing-hadoop-classes).
+Apache Hive is built on Hadoop, so you need to provide Hadoop dependenies, by setting the `HADOOP_CLASSPATH` 
+environment variable:
+```
+export HADOOP_CLASSPATH=`hadoop classpath`
+```
 
 There are two ways to add Hive dependencies. First is to use Flink's bundled Hive jars. You can choose a bundled Hive jar according to the version of the metastore you use. Second is to add each of the required jars separately. The second way can be useful if the Hive version you're using is not listed here.
 
diff --git a/docs/dev/table/connectors/hive/index.zh.md b/docs/dev/table/connectors/hive/index.zh.md
index 8343d0ba8a7..41f83c946d4 100644
--- a/docs/dev/table/connectors/hive/index.zh.md
+++ b/docs/dev/table/connectors/hive/index.zh.md
@@ -92,7 +92,10 @@ Flink 支持一下的 Hive 版本。
 或者，您可以将这些依赖项放在专用文件夹中，并分别使用 Table API 程序或 SQL Client 的`-C`或`-l`选项将它们添加到 classpath 中。
 
 Apache Hive 是基于 Hadoop 之上构建的, 首先您需要 Hadoop 的依赖，请参考
-[Providing Hadoop classes]({{ site.baseurl }}/zh/deployment/resource-providers/hadoop.html#providing-hadoop-classes).
+Providing Hadoop classes:
+```
+export HADOOP_CLASSPATH=`hadoop classpath`
+```
 
 有两种添加 Hive 依赖项的方法。第一种是使用 Flink 提供的 Hive Jar包。您可以根据使用的 Metastore 的版本来选择对应的 Hive jar。第二个方式是分别添加每个所需的 jar 包。如果您使用的 Hive 版本尚未在此处列出，则第二种方法会更适合。
 
