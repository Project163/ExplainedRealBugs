diff --git a/docs/layouts/shortcodes/generated/execution_config_configuration.html b/docs/layouts/shortcodes/generated/execution_config_configuration.html
index 128a05351bf..770f2f5b524 100644
--- a/docs/layouts/shortcodes/generated/execution_config_configuration.html
+++ b/docs/layouts/shortcodes/generated/execution_config_configuration.html
@@ -160,6 +160,12 @@ By default no operator is disabled.</td>
             <td><p>Enum</p></td>
             <td>In order to minimize the distributed disorder problem when writing data into table with primary keys that many users suffers. FLINK will auto add a keyed shuffle by default when the sink parallelism differs from upstream operator and sink parallelism is not 1. This works only when the upstream ensures the multi-records' order on the primary key, if not, the added shuffle can not solve the problem (In this situation, a more proper way is to consider the deduplicate operation for the source firstly or use an upsert source with primary key definition which truly reflect the records evolution).<br />By default, the keyed shuffle will be added when the sink's parallelism differs from upstream operator. You can set to no shuffle(NONE) or force shuffle(FORCE).<br /><br />Possible values:<ul><li>"NONE"</li><li>"AUTO"</li><li>"FORCE"</li></ul></td>
         </tr>
+        <tr>
+            <td><h5>table.exec.sink.nested-constraint-enforcer</h5><br> <span class="label label-primary">Batch</span> <span class="label label-primary">Streaming</span></td>
+            <td style="word-wrap: break-word;">IGNORE</td>
+            <td><p>Enum</p></td>
+            <td>Determines if constraints should be enforced for nested fields. Beware that enforcing constraints for nested fields adds computational overhead especially when iterating through collections<br /><br />Possible values:<ul><li>"IGNORE": Don't perform check on nested types in ROWS/ARRAYS/MAPS</li><li>"ROWS": Perform checks on nested types in ROWS.</li><li>"ROWS_AND_COLLECTIONS": Perform checks on nested types in ROWS/ARRAYS/MAPS. Be aware that the more checks the more performance impact. Especially checking types in ARRAYS/MAPS can be expensive.</li></ul></td>
+        </tr>
         <tr>
             <td><h5>table.exec.sink.not-null-enforcer</h5><br> <span class="label label-primary">Batch</span> <span class="label label-primary">Streaming</span></td>
             <td style="word-wrap: break-word;">ERROR</td>
@@ -176,7 +182,7 @@ By default no operator is disabled.</td>
             <td><h5>table.exec.sink.type-length-enforcer</h5><br> <span class="label label-primary">Batch</span> <span class="label label-primary">Streaming</span></td>
             <td style="word-wrap: break-word;">IGNORE</td>
             <td><p>Enum</p></td>
-            <td>Determines whether values for columns with CHAR(&lt;length&gt;)/VARCHAR(&lt;length&gt;)/BINARY(&lt;length&gt;)/VARBINARY(&lt;length&gt;) types will be trimmed or padded (only for CHAR(&lt;length&gt;)/BINARY(&lt;length&gt;)), so that their length will match the one defined by the length of their respective CHAR/VARCHAR/BINARY/VARBINARY column type.<br /><br />Possible values:<ul><li>"IGNORE": Don't apply any trimming and padding, and instead ignore the CHAR/VARCHAR/BINARY/VARBINARY length directive.</li><li>"TRIM_PAD": Trim and pad string and binary values to match the length defined by the CHAR/VARCHAR/BINARY/VARBINARY length.</li></ul></td>
+            <td>Determines whether values for columns with CHAR(&lt;length&gt;)/VARCHAR(&lt;length&gt;)/BINARY(&lt;length&gt;)/VARBINARY(&lt;length&gt;) types will be trimmed or padded (only for CHAR(&lt;length&gt;)/BINARY(&lt;length&gt;)), so that their length will match the one defined by the length of their respective CHAR/VARCHAR/BINARY/VARBINARY column type.<br /><br />Possible values:<ul><li>"IGNORE": Don't apply any trimming and padding, and instead ignore the CHAR/VARCHAR/BINARY/VARBINARY length directive.</li><li>"TRIM_PAD": Trim and pad string and binary values to match the length defined by the CHAR/VARCHAR/BINARY/VARBINARY length.</li><li>"ERROR": Throw a runtime exception when writing data into a CHAR/VARCHAR/BINARY/VARBINARY column which does not match the length constraint</li></ul></td>
         </tr>
         <tr>
             <td><h5>table.exec.sink.upsert-materialize</h5><br> <span class="label label-primary">Streaming</span></td>
diff --git a/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/config/ExecutionConfigOptions.java b/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/config/ExecutionConfigOptions.java
index 99bb67af5d2..a1847e93b9e 100644
--- a/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/config/ExecutionConfigOptions.java
+++ b/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/config/ExecutionConfigOptions.java
@@ -130,6 +130,16 @@ public class ExecutionConfigOptions {
                                     + "will match the one defined by the length of their respective "
                                     + "CHAR/VARCHAR/BINARY/VARBINARY column type.");
 
+    @Documentation.TableOption(execMode = Documentation.ExecMode.BATCH_STREAMING)
+    public static final ConfigOption<NestedEnforcer> TABLE_EXEC_SINK_NESTED_CONSTRAINT_ENFORCER =
+            key("table.exec.sink.nested-constraint-enforcer")
+                    .enumType(NestedEnforcer.class)
+                    .defaultValue(NestedEnforcer.IGNORE)
+                    .withDescription(
+                            "Determines if constraints should be enforced for nested fields. Beware that"
+                                    + " enforcing constraints for nested fields adds computational"
+                                    + " overhead especially when iterating through collections");
+
     @Documentation.TableOption(execMode = Documentation.ExecMode.STREAMING)
     public static final ConfigOption<UpsertMaterialize> TABLE_EXEC_SINK_UPSERT_MATERIALIZE =
             key("table.exec.sink.upsert-materialize")
@@ -665,7 +675,12 @@ public class ExecutionConfigOptions {
         TRIM_PAD(
                 text(
                         "Trim and pad string and binary values to match the length "
-                                + "defined by the CHAR/VARCHAR/BINARY/VARBINARY length."));
+                                + "defined by the CHAR/VARCHAR/BINARY/VARBINARY length.")),
+        ERROR(
+                text(
+                        "Throw a runtime exception when writing data into a "
+                                + "CHAR/VARCHAR/BINARY/VARBINARY column which does not match the length"
+                                + " constraint"));
 
         private final InlineElement description;
 
@@ -680,6 +695,30 @@ public class ExecutionConfigOptions {
         }
     }
 
+    /** The enforcer to check the constraints on nested types. */
+    @PublicEvolving
+    public enum NestedEnforcer implements DescribedEnum {
+        IGNORE(text("Don't perform check on nested types in ROWS/ARRAYS/MAPS")),
+        ROWS(text("Perform checks on nested types in ROWS.")),
+        ROWS_AND_COLLECTIONS(
+                text(
+                        "Perform checks on nested types in ROWS/ARRAYS/MAPS. Be aware that the"
+                                + " more checks the more performance impact. Especially checking"
+                                + " types in ARRAYS/MAPS can be expensive."));
+
+        private final InlineElement description;
+
+        NestedEnforcer(InlineElement description) {
+            this.description = description;
+        }
+
+        @Internal
+        @Override
+        public InlineElement getDescription() {
+            return description;
+        }
+    }
+
     /** Upsert materialize strategy before sink. */
     @PublicEvolving
     public enum UpsertMaterialize {
diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/common/CommonExecSink.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/common/CommonExecSink.java
index 82917fce8da..68b9f89040e 100644
--- a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/common/CommonExecSink.java
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/common/CommonExecSink.java
@@ -70,28 +70,24 @@ import org.apache.flink.table.planner.plan.nodes.exec.utils.TransformationMetada
 import org.apache.flink.table.planner.plan.utils.KeySelectorUtil;
 import org.apache.flink.table.runtime.connector.sink.SinkRuntimeProviderContext;
 import org.apache.flink.table.runtime.keyselector.RowDataKeySelector;
-import org.apache.flink.table.runtime.operators.sink.ConstraintEnforcer;
 import org.apache.flink.table.runtime.operators.sink.RowKindSetter;
 import org.apache.flink.table.runtime.operators.sink.SinkOperator;
 import org.apache.flink.table.runtime.operators.sink.StreamRecordTimestampInserter;
+import org.apache.flink.table.runtime.operators.sink.constraint.ConstraintEnforcer;
+import org.apache.flink.table.runtime.operators.sink.constraint.ConstraintEnforcerExecutor;
 import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
-import org.apache.flink.table.types.logical.BinaryType;
-import org.apache.flink.table.types.logical.CharType;
 import org.apache.flink.table.types.logical.LogicalType;
-import org.apache.flink.table.types.logical.LogicalTypeRoot;
 import org.apache.flink.table.types.logical.RowType;
-import org.apache.flink.table.types.logical.utils.LogicalTypeChecks;
 import org.apache.flink.types.RowKind;
 import org.apache.flink.util.TemporaryClassLoaderContext;
 
 import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.annotation.JsonProperty;
 
-import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.List;
+import java.util.Objects;
 import java.util.Optional;
 import java.util.stream.Collectors;
-import java.util.stream.IntStream;
 
 /**
  * Base {@link ExecNode} to write data to an external sink defined by a {@link DynamicTableSink}.
@@ -259,112 +255,38 @@ public abstract class CommonExecSink extends ExecNodeBase<Object>
             Transformation<RowData> inputTransform,
             ExecNodeConfig config,
             RowType physicalRowType) {
-        final ConstraintEnforcer.Builder validatorBuilder = ConstraintEnforcer.newBuilder();
-        final String[] fieldNames = physicalRowType.getFieldNames().toArray(new String[0]);
-
-        // Build NOT NULL enforcer
-        final int[] notNullFieldIndices = getNotNullFieldIndices(physicalRowType);
-        if (notNullFieldIndices.length > 0) {
-            final ExecutionConfigOptions.NotNullEnforcer notNullEnforcer =
-                    config.get(ExecutionConfigOptions.TABLE_EXEC_SINK_NOT_NULL_ENFORCER);
-            final List<String> notNullFieldNames =
-                    Arrays.stream(notNullFieldIndices)
-                            .mapToObj(idx -> fieldNames[idx])
-                            .collect(Collectors.toList());
-
-            validatorBuilder.addNotNullConstraint(
-                    notNullEnforcer, notNullFieldIndices, notNullFieldNames, fieldNames);
-        }
-
-        final ExecutionConfigOptions.TypeLengthEnforcer typeLengthEnforcer =
-                config.get(ExecutionConfigOptions.TABLE_EXEC_SINK_TYPE_LENGTH_ENFORCER);
-
-        // Build CHAR/VARCHAR length enforcer
-        final List<ConstraintEnforcer.FieldInfo> charFieldInfo =
-                getFieldInfoForLengthEnforcer(physicalRowType, LengthEnforcerType.CHAR);
-        if (!charFieldInfo.isEmpty()) {
-            final List<String> charFieldNames =
-                    charFieldInfo.stream()
-                            .map(cfi -> fieldNames[cfi.fieldIdx()])
-                            .collect(Collectors.toList());
-
-            validatorBuilder.addCharLengthConstraint(
-                    typeLengthEnforcer, charFieldInfo, charFieldNames, fieldNames);
-        }
-
-        // Build BINARY/VARBINARY length enforcer
-        final List<ConstraintEnforcer.FieldInfo> binaryFieldInfo =
-                getFieldInfoForLengthEnforcer(physicalRowType, LengthEnforcerType.BINARY);
-        if (!binaryFieldInfo.isEmpty()) {
-            final List<String> binaryFieldNames =
-                    binaryFieldInfo.stream()
-                            .map(cfi -> fieldNames[cfi.fieldIdx()])
-                            .collect(Collectors.toList());
-
-            validatorBuilder.addBinaryLengthConstraint(
-                    typeLengthEnforcer, binaryFieldInfo, binaryFieldNames, fieldNames);
-        }
-
-        ConstraintEnforcer constraintEnforcer = validatorBuilder.build();
-        if (constraintEnforcer != null) {
-            return ExecNodeUtil.createOneInputTransformation(
-                    inputTransform,
-                    createTransformationMeta(
-                            CONSTRAINT_VALIDATOR_TRANSFORMATION,
-                            constraintEnforcer.getOperatorName(),
-                            "ConstraintEnforcer",
-                            config),
-                    constraintEnforcer,
-                    getInputTypeInfo(),
-                    inputTransform.getParallelism(),
-                    false);
-        } else {
-            // there are no not-null fields, just skip adding the enforcer operator
-            return inputTransform;
-        }
-    }
-
-    private int[] getNotNullFieldIndices(RowType physicalType) {
-        return IntStream.range(0, physicalType.getFieldCount())
-                .filter(pos -> !physicalType.getTypeAt(pos).isNullable())
-                .toArray();
-    }
-
-    /**
-     * Returns a List of {@link ConstraintEnforcer.FieldInfo}, each containing the info needed to
-     * determine whether a string or binary value needs trimming and/or padding.
-     */
-    private List<ConstraintEnforcer.FieldInfo> getFieldInfoForLengthEnforcer(
-            RowType physicalType, LengthEnforcerType enforcerType) {
-        LogicalTypeRoot staticType = null;
-        LogicalTypeRoot variableType = null;
-        int maxLength = 0;
-        switch (enforcerType) {
-            case CHAR:
-                staticType = LogicalTypeRoot.CHAR;
-                variableType = LogicalTypeRoot.VARCHAR;
-                maxLength = CharType.MAX_LENGTH;
-                break;
-            case BINARY:
-                staticType = LogicalTypeRoot.BINARY;
-                variableType = LogicalTypeRoot.VARBINARY;
-                maxLength = BinaryType.MAX_LENGTH;
-        }
-        final List<ConstraintEnforcer.FieldInfo> fieldsAndLengths = new ArrayList<>();
-        for (int i = 0; i < physicalType.getFieldCount(); i++) {
-            LogicalType type = physicalType.getTypeAt(i);
-            boolean isStatic = type.is(staticType);
-            // Should trim and possibly pad
-            if ((isStatic && (LogicalTypeChecks.getLength(type) < maxLength))
-                    || (type.is(variableType) && (LogicalTypeChecks.getLength(type) < maxLength))) {
-                fieldsAndLengths.add(
-                        new ConstraintEnforcer.FieldInfo(
-                                i, LogicalTypeChecks.getLength(type), isStatic));
-            } else if (isStatic) { // Should pad
-                fieldsAndLengths.add(new ConstraintEnforcer.FieldInfo(i, null, isStatic));
-            }
-        }
-        return fieldsAndLengths;
+        final Optional<ConstraintEnforcerExecutor> enforcerExecutor =
+                ConstraintEnforcerExecutor.create(
+                        physicalRowType,
+                        config.get(ExecutionConfigOptions.TABLE_EXEC_SINK_NOT_NULL_ENFORCER),
+                        config.get(ExecutionConfigOptions.TABLE_EXEC_SINK_TYPE_LENGTH_ENFORCER),
+                        config.get(
+                                ExecutionConfigOptions.TABLE_EXEC_SINK_NESTED_CONSTRAINT_ENFORCER));
+
+        return enforcerExecutor
+                .map(
+                        executor -> {
+                            final String operatorName =
+                                    "ConstraintEnforcer["
+                                            + Arrays.stream(executor.getConstraints())
+                                                    .map(Objects::toString)
+                                                    .collect(Collectors.joining(", "))
+                                            + "]";
+
+                            return (Transformation<RowData>)
+                                    ExecNodeUtil.createOneInputTransformation(
+                                            inputTransform,
+                                            createTransformationMeta(
+                                                    CONSTRAINT_VALIDATOR_TRANSFORMATION,
+                                                    operatorName,
+                                                    "ConstraintEnforcer",
+                                                    config),
+                                            new ConstraintEnforcer(executor, operatorName),
+                                            getInputTypeInfo(),
+                                            inputTransform.getParallelism(),
+                                            false);
+                        })
+                .orElse(inputTransform);
     }
 
     /**
@@ -620,11 +542,6 @@ public abstract class CommonExecSink extends ExecNodeBase<Object>
         return (RowType) schema.toPhysicalRowDataType().getLogicalType();
     }
 
-    private enum LengthEnforcerType {
-        CHAR,
-        BINARY
-    }
-
     /**
      * Get the target row-kind that the row data should change to, assuming the current row kind is
      * RowKind.INSERT. Return Optional.empty() if it doesn't need to change. Currently, it'll only
diff --git a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/nodes/exec/stream/ConstraintEnforcerSemanticTests.java b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/nodes/exec/stream/ConstraintEnforcerSemanticTests.java
new file mode 100644
index 00000000000..499770c480b
--- /dev/null
+++ b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/nodes/exec/stream/ConstraintEnforcerSemanticTests.java
@@ -0,0 +1,45 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.planner.plan.nodes.exec.stream;
+
+import org.apache.flink.table.planner.plan.nodes.exec.testutils.SemanticTestBase;
+import org.apache.flink.table.runtime.operators.sink.constraint.ConstraintEnforcer;
+import org.apache.flink.table.test.program.TableTestProgram;
+
+import java.util.List;
+
+/** Semantic tests for {@link ConstraintEnforcer}. */
+public class ConstraintEnforcerSemanticTests extends SemanticTestBase {
+
+    @Override
+    public List<TableTestProgram> programs() {
+        return List.of(
+                ConstraintEnforcerTestPrograms.NOT_NULL_DROP_NESTED_ROWS,
+                ConstraintEnforcerTestPrograms.NOT_NULL_ERROR_NESTED_ROWS,
+                ConstraintEnforcerTestPrograms.LENGTH_TRIM_PAD_NESTED_ROWS,
+                ConstraintEnforcerTestPrograms.LENGTH_ERROR_NESTED_ROWS,
+                ConstraintEnforcerTestPrograms.NOT_NULL_DROP_NESTED_ARRAYS,
+                ConstraintEnforcerTestPrograms.NOT_NULL_ERROR_NESTED_ARRAYS,
+                ConstraintEnforcerTestPrograms.NOT_NULL_DROP_NESTED_MAPS,
+                ConstraintEnforcerTestPrograms.NOT_NULL_ERROR_NESTED_MAPS,
+                ConstraintEnforcerTestPrograms.LENGTH_TRIM_PAD_WITH_NESTED_COLLECTIONS,
+                ConstraintEnforcerTestPrograms.LENGTH_ERROR_WITH_NESTED_ARRAYS,
+                ConstraintEnforcerTestPrograms.LENGTH_ERROR_WITH_NESTED_MAPS);
+    }
+}
diff --git a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/nodes/exec/stream/ConstraintEnforcerTestPrograms.java b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/nodes/exec/stream/ConstraintEnforcerTestPrograms.java
new file mode 100644
index 00000000000..9609a3f3d01
--- /dev/null
+++ b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/nodes/exec/stream/ConstraintEnforcerTestPrograms.java
@@ -0,0 +1,823 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.planner.plan.nodes.exec.stream;
+
+import org.apache.flink.table.api.TableRuntimeException;
+import org.apache.flink.table.api.ValidationException;
+import org.apache.flink.table.api.config.ExecutionConfigOptions.NestedEnforcer;
+import org.apache.flink.table.api.config.ExecutionConfigOptions.NotNullEnforcer;
+import org.apache.flink.table.api.config.ExecutionConfigOptions.TypeLengthEnforcer;
+import org.apache.flink.table.runtime.operators.sink.constraint.ConstraintEnforcer;
+import org.apache.flink.table.test.program.SinkTestStep;
+import org.apache.flink.table.test.program.SourceTestStep;
+import org.apache.flink.table.test.program.TableTestProgram;
+import org.apache.flink.types.Row;
+
+import javax.annotation.Nullable;
+
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.apache.flink.table.api.config.ExecutionConfigOptions.TABLE_EXEC_SINK_NESTED_CONSTRAINT_ENFORCER;
+import static org.apache.flink.table.api.config.ExecutionConfigOptions.TABLE_EXEC_SINK_NOT_NULL_ENFORCER;
+import static org.apache.flink.table.api.config.ExecutionConfigOptions.TABLE_EXEC_SINK_TYPE_LENGTH_ENFORCER;
+
+/** Tests for {@link ConstraintEnforcer}. */
+public class ConstraintEnforcerTestPrograms {
+
+    public static final String SCHEMA_NOT_NULL =
+            "a ROW<"
+                    + "id BIGINT NOT NULL, "
+                    + "ab ROW<"
+                    + "     aba BIGINT NOT NULL,"
+                    // extra column to make it easier to create a row with null as `aba`, without
+                    // the
+                    // column Row.of(null) can not differentiate if the vararg is null or the column
+                    // is null
+                    + "     ignore BIGINT"
+                    + "> NOT NULL> NOT NULL";
+    static final TableTestProgram NOT_NULL_DROP_NESTED_ROWS =
+            TableTestProgram.of(
+                            "constraint-enforcer-drop-not-null-nested-rows",
+                            "validates"
+                                    + " constraint enforcer drops records with nulls for NOT NULL"
+                                    + " columns in nested rows")
+                    .setupConfig(TABLE_EXEC_SINK_NESTED_CONSTRAINT_ENFORCER, NestedEnforcer.ROWS)
+                    .setupConfig(TABLE_EXEC_SINK_NOT_NULL_ENFORCER, NotNullEnforcer.DROP)
+                    .setupTableSource(
+                            SourceTestStep.newBuilder("source_t")
+                                    .addSchema(SCHEMA_NOT_NULL)
+                                    .producedValues(
+                                            // should be dropped because a value in a nested ROW is
+                                            // null
+                                            Row.of(
+                                                    Row.of( // a
+                                                            1L, // id
+                                                            Row.of( // ab
+                                                                    null, // aba,
+                                                                    1L // ignore
+                                                                    ))),
+                                            // should be dropped because the entire nested ROW is
+                                            // null
+                                            Row.of(
+                                                    Row.of( // a
+                                                            2L, // id
+                                                            null)), // ab
+                                            // a valid record, which should be kept
+                                            Row.of(
+                                                    Row.of( // a
+                                                            3L, // id
+                                                            Row.of( // ab
+                                                                    1L, // aba,
+                                                                    1L // ignore
+                                                                    ))))
+                                    .build())
+                    .setupTableSink(
+                            SinkTestStep.newBuilder("sink_t")
+                                    .addSchema(SCHEMA_NOT_NULL)
+                                    .consumedValues(
+                                            Row.of(
+                                                    Row.of( // a
+                                                            3L, // id
+                                                            Row.of( // ab
+                                                                    1L, // aba,
+                                                                    1L // ignore
+                                                                    ))))
+                                    .build())
+                    .runSql("INSERT INTO sink_t SELECT * FROM source_t")
+                    .build();
+
+    static final TableTestProgram NOT_NULL_ERROR_NESTED_ROWS =
+            TableTestProgram.of(
+                            "constraint-enforcer-error-not-null-nested-rows",
+                            "validates"
+                                    + " constraint enforcer throws an exception when trying to"
+                                    + " write records null into NOT NULL columns in nested rows")
+                    .setupConfig(TABLE_EXEC_SINK_NESTED_CONSTRAINT_ENFORCER, NestedEnforcer.ROWS)
+                    .setupConfig(TABLE_EXEC_SINK_NOT_NULL_ENFORCER, NotNullEnforcer.ERROR)
+                    .setupTableSource(
+                            SourceTestStep.newBuilder("source_t")
+                                    .addSchema(SCHEMA_NOT_NULL)
+                                    .producedValues(
+                                            // should be dropped because a value in a nested ROW is
+                                            // null
+                                            Row.of(
+                                                    Row.of( // a
+                                                            1L, // id
+                                                            Row.of( // ab
+                                                                    null, // aba,
+                                                                    1L // ignore
+                                                                    ))),
+                                            // should be dropped because the entire nested ROW is
+                                            // null
+                                            Row.of(
+                                                    Row.of( // a
+                                                            2L, // id
+                                                            null)) // ab
+                                            )
+                                    .build())
+                    .setupTableSink(
+                            SinkTestStep.newBuilder("sink_t")
+                                    .addSchema(SCHEMA_NOT_NULL)
+                                    .consumedValues(new Row[0])
+                                    .build())
+                    .runFailingSql(
+                            "INSERT INTO sink_t SELECT * FROM source_t WHERE `a`.`id` = 1",
+                            TableRuntimeException.class,
+                            "Column 'a.ab.aba' is NOT NULL, however, a null"
+                                    + " value is being written into it. You can set job"
+                                    + " configuration 'table.exec.sink.not-null-enforcer'='DROP'"
+                                    + " to suppress this exception and drop such records silently.")
+                    .runFailingSql(
+                            "INSERT INTO sink_t SELECT * FROM source_t WHERE `a`.`id` = 2",
+                            TableRuntimeException.class,
+                            "Column 'a.ab' is NOT NULL, however, a null"
+                                    + " value is being written into it. You can set job"
+                                    + " configuration 'table.exec.sink.not-null-enforcer'='DROP'"
+                                    + " to suppress this exception and drop such records silently.")
+                    .build();
+
+    public static final String SCHEMA_LENGTH =
+            "a ROW<"
+                    + "id BIGINT NOT NULL, "
+                    + "ab ROW<"
+                    + "     aba CHAR(2) NOT NULL,"
+                    + "     abb VARCHAR(2) NOT NULL,"
+                    + "     abc BINARY(2) NOT NULL,"
+                    + "     abd VARBINARY(2) NOT NULL"
+                    + "> NOT NULL> NOT NULL";
+    static final TableTestProgram LENGTH_TRIM_PAD_NESTED_ROWS =
+            TableTestProgram.of(
+                            "constraint-enforcer-trim-pad-length-nested-rows",
+                            "validates"
+                                    + " constraint enforcer trim or pads character and binary strings"
+                                    + " in nested rows")
+                    .setupConfig(TABLE_EXEC_SINK_NESTED_CONSTRAINT_ENFORCER, NestedEnforcer.ROWS)
+                    .setupConfig(TABLE_EXEC_SINK_TYPE_LENGTH_ENFORCER, TypeLengthEnforcer.TRIM_PAD)
+                    .setupTableSource(
+                            SourceTestStep.newBuilder("source_t")
+                                    .addSchema(SCHEMA_LENGTH)
+                                    .producedValues(
+                                            // should be padded
+                                            Row.of(
+                                                    Row.of( // a
+                                                            1L, // aa
+                                                            Row.of( // ab
+                                                                    "a", // aba
+                                                                    "a", // abb
+                                                                    new byte[] {1}, // abc
+                                                                    new byte[] {1} // abd
+                                                                    ))),
+                                            // should be trimmed
+                                            Row.of(
+                                                    Row.of( // a
+                                                            2L, // aa
+                                                            Row.of( // ab
+                                                                    "abc", // aba,
+                                                                    "abc", // abb
+                                                                    new byte[] {1, 2, 3}, // abc
+                                                                    new byte[] {1, 2, 3} // abd
+                                                                    ))), // ab
+                                            // a valid record, which should be kept
+                                            Row.of(
+                                                    Row.of( // a
+                                                            3L, // aa
+                                                            Row.of( // ab
+                                                                    "ab", // aba,
+                                                                    "ab", // abb
+                                                                    new byte[] {1, 2}, // abc
+                                                                    new byte[] {1, 2} // abd
+                                                                    ))))
+                                    .build())
+                    .setupTableSink(
+                            SinkTestStep.newBuilder("sink_t")
+                                    .addSchema(SCHEMA_LENGTH)
+                                    .consumedValues(
+                                            // should be padded
+                                            Row.of(
+                                                    Row.of( // a
+                                                            1L, // aa
+                                                            Row.of( // ab
+                                                                    "a ", // aba
+                                                                    "a", // abb
+                                                                    new byte[] {1, 0}, // abc
+                                                                    new byte[] {1} // abd
+                                                                    ))),
+                                            // should be trimmed
+                                            Row.of(
+                                                    Row.of( // a
+                                                            2L, // aa
+                                                            Row.of( // ab
+                                                                    "ab", // aba,
+                                                                    "ab", // abb
+                                                                    new byte[] {1, 2}, // abc
+                                                                    new byte[] {1, 2} // abd
+                                                                    ))), // ab
+                                            // a valid record, which should be kept
+                                            Row.of(
+                                                    Row.of( // a
+                                                            3L, // aa
+                                                            Row.of( // ab
+                                                                    "ab", // aba,
+                                                                    "ab", // abb
+                                                                    new byte[] {1, 2}, // abc
+                                                                    new byte[] {1, 2} // abd
+                                                                    ))))
+                                    .build())
+                    .runSql("INSERT INTO sink_t SELECT * FROM source_t")
+                    .build();
+
+    static final TableTestProgram LENGTH_ERROR_NESTED_ROWS =
+            TableTestProgram.of(
+                            "constraint-enforcer-error-length-nested-rows",
+                            "validates"
+                                    + " constraint enforcer errors or invalid length of character"
+                                    + " and binary strings in nested rows")
+                    .setupConfig(TABLE_EXEC_SINK_NESTED_CONSTRAINT_ENFORCER, NestedEnforcer.ROWS)
+                    .setupConfig(TABLE_EXEC_SINK_TYPE_LENGTH_ENFORCER, TypeLengthEnforcer.ERROR)
+                    .setupTableSource(
+                            SourceTestStep.newBuilder("source_t")
+                                    .addSchema(SCHEMA_LENGTH)
+                                    .producedValues(
+                                            // should be padded CHAR
+                                            Row.of(
+                                                    Row.of( // a
+                                                            1L, // id
+                                                            Row.of( // ab
+                                                                    "a", // aba
+                                                                    "a", // abb
+                                                                    new byte[] {1, 2}, // abc
+                                                                    new byte[] {1} // abd
+                                                                    ))),
+                                            // should be padded BINARY
+                                            Row.of(
+                                                    Row.of( // a
+                                                            2L, // id
+                                                            Row.of( // ab
+                                                                    "ab", // aba
+                                                                    "a", // abb
+                                                                    new byte[] {1}, // abc
+                                                                    new byte[] {1} // abd
+                                                                    ))),
+                                            // should be trimmed CHAR
+                                            Row.of(
+                                                    Row.of( // a
+                                                            3L, // id
+                                                            Row.of( // ab
+                                                                    "abc", // aba,
+                                                                    "ab", // abb
+                                                                    new byte[] {1, 2}, // abc
+                                                                    new byte[] {1, 2} // abd
+                                                                    ))), // ab
+                                            // should be trimmed BINARY
+                                            Row.of(
+                                                    Row.of( // a
+                                                            4L, // id
+                                                            Row.of( // ab
+                                                                    "ab", // aba,
+                                                                    "ab", // abb
+                                                                    new byte[] {1, 2, 3}, // abc
+                                                                    new byte[] {1, 2} // abd
+                                                                    ))), // ab
+                                            // should be trimmed VARCHAR
+                                            Row.of(
+                                                    Row.of( // a
+                                                            5L, // id
+                                                            Row.of( // ab
+                                                                    "ab", // aba,
+                                                                    "abc", // abb
+                                                                    new byte[] {1, 2}, // abc
+                                                                    new byte[] {1, 2} // abd
+                                                                    ))), // ab
+                                            // should be trimmed VARBINARY
+                                            Row.of(
+                                                    Row.of( // a
+                                                            6L, // id
+                                                            Row.of( // ab
+                                                                    "ab", // aba,
+                                                                    "ab", // abb
+                                                                    new byte[] {1, 2}, // abc
+                                                                    new byte[] {1, 2, 3} // abd
+                                                                    ))) // ab
+                                            )
+                                    .build())
+                    .setupTableSink(
+                            SinkTestStep.newBuilder("sink_t")
+                                    .addSchema(SCHEMA_LENGTH)
+                                    .consumedValues(new Row[0])
+                                    .build())
+                    .runFailingSql(
+                            "INSERT INTO sink_t SELECT * FROM source_t WHERE `a`.`id` = 1",
+                            TableRuntimeException.class,
+                            "Column 'a.ab.aba' is CHAR(2), however, a string of"
+                                    + " length 1 is being written into it. You can set job"
+                                    + " configuration 'table.exec.sink.type-length-enforcer' to"
+                                    + " control this behaviour.")
+                    .runFailingSql(
+                            "INSERT INTO sink_t SELECT * FROM source_t WHERE `a`.`id` = 2",
+                            TableRuntimeException.class,
+                            "Column 'a.ab.abc' is BINARY(2), however, a string of"
+                                    + " length 1 is being written into it. You can set job"
+                                    + " configuration 'table.exec.sink.type-length-enforcer' to"
+                                    + " control this behaviour.")
+                    .runFailingSql(
+                            "INSERT INTO sink_t SELECT * FROM source_t WHERE `a`.`id` = 3",
+                            TableRuntimeException.class,
+                            "Column 'a.ab.aba' is CHAR(2), however, a string of"
+                                    + " length 3 is being written into it. You can set job"
+                                    + " configuration 'table.exec.sink.type-length-enforcer' to"
+                                    + " control this behaviour.")
+                    .runFailingSql(
+                            "INSERT INTO sink_t SELECT * FROM source_t WHERE `a`.`id` = 4",
+                            TableRuntimeException.class,
+                            "Column 'a.ab.abc' is BINARY(2), however, a string of"
+                                    + " length 3 is being written into it. You can set job"
+                                    + " configuration 'table.exec.sink.type-length-enforcer' to"
+                                    + " control this behaviour.")
+                    .runFailingSql(
+                            "INSERT INTO sink_t SELECT * FROM source_t WHERE `a`.`id` = 5",
+                            TableRuntimeException.class,
+                            "Column 'a.ab.abb' is VARCHAR(2), however, a string of"
+                                    + " length 3 is being written into it. You can set job"
+                                    + " configuration 'table.exec.sink.type-length-enforcer' to"
+                                    + " control this behaviour.")
+                    .runFailingSql(
+                            "INSERT INTO sink_t SELECT * FROM source_t WHERE `a`.`id` = 6",
+                            TableRuntimeException.class,
+                            "Column 'a.ab.abd' is VARBINARY(2), however, a string of"
+                                    + " length 3 is being written into it. You can set job"
+                                    + " configuration 'table.exec.sink.type-length-enforcer' to"
+                                    + " control this behaviour.")
+                    .build();
+
+    public static final String SCHEMA_ARRAYS_NOT_NULL =
+            "a ROW<"
+                    + "id BIGINT NOT NULL, "
+                    + "ab ARRAY<"
+                    + " ARRAY<BIGINT NOT NULL> NOT NULL"
+                    + "> NOT NULL> NOT NULL";
+    static final TableTestProgram NOT_NULL_DROP_NESTED_ARRAYS =
+            TableTestProgram.of(
+                            "constraint-enforcer-drop-not-null-nested-arrays",
+                            "validates"
+                                    + " constraint enforcer drops records with nulls for NOT NULL"
+                                    + " columns in nested arrays")
+                    .setupConfig(
+                            TABLE_EXEC_SINK_NESTED_CONSTRAINT_ENFORCER,
+                            NestedEnforcer.ROWS_AND_COLLECTIONS)
+                    .setupConfig(TABLE_EXEC_SINK_NOT_NULL_ENFORCER, NotNullEnforcer.DROP)
+                    .setupTableSource(
+                            SourceTestStep.newBuilder("source_t")
+                                    .addSchema(SCHEMA_ARRAYS_NOT_NULL)
+                                    .producedValues(
+                                            // should be dropped because a value in a nested ARRAY
+                                            // is null
+                                            Row.of(
+                                                    Row.of( // a
+                                                            1L, // id
+                                                            new Long[][] {new Long[] {null}} // ab
+                                                            )),
+                                            // should be dropped because the entire nested ARRAY is
+                                            // null
+                                            Row.of(
+                                                    Row.of( // a
+                                                            2L, // id
+                                                            new Long[][] {null})), // ab
+                                            // a valid record, which should be kept
+                                            Row.of(
+                                                    Row.of( // a
+                                                            3L, // id
+                                                            new Long[][] {new Long[] {1L}} // ab
+                                                            )))
+                                    .build())
+                    .setupTableSink(
+                            SinkTestStep.newBuilder("sink_t")
+                                    .addSchema(SCHEMA_ARRAYS_NOT_NULL)
+                                    .consumedValues(
+                                            Row.of(
+                                                    Row.of( // a
+                                                            3L, // id
+                                                            new Long[][] {new Long[] {1L}} // ab
+                                                            )))
+                                    .build())
+                    .runSql("INSERT INTO sink_t SELECT * FROM source_t")
+                    .build();
+
+    static final TableTestProgram NOT_NULL_ERROR_NESTED_ARRAYS =
+            TableTestProgram.of(
+                            "constraint-enforcer-error-not-null-nested-arrays",
+                            "validates"
+                                    + " constraint enforcer throws an exception for records with"
+                                    + " nulls for NOT NULL elements in nested arrays")
+                    .setupConfig(
+                            TABLE_EXEC_SINK_NESTED_CONSTRAINT_ENFORCER,
+                            NestedEnforcer.ROWS_AND_COLLECTIONS)
+                    .setupConfig(TABLE_EXEC_SINK_NOT_NULL_ENFORCER, NotNullEnforcer.ERROR)
+                    .setupTableSource(
+                            SourceTestStep.newBuilder("source_t")
+                                    .addSchema(SCHEMA_ARRAYS_NOT_NULL)
+                                    .producedValues(
+                                            // should be dropped because a value in a nested ARRAY
+                                            // is null
+                                            Row.of(
+                                                    Row.of( // a
+                                                            1L, // id
+                                                            new Long[][] {new Long[] {null}} // ab
+                                                            )),
+                                            // should be dropped because the entire nested ARRAY is
+                                            // null
+                                            Row.of(
+                                                    Row.of( // a
+                                                            2L, // id
+                                                            new Long[][] {null})) // ab
+                                            )
+                                    .build())
+                    .setupTableSink(
+                            SinkTestStep.newBuilder("sink_t")
+                                    .addSchema(SCHEMA_ARRAYS_NOT_NULL)
+                                    .consumedValues(new Row[0])
+                                    .build())
+                    .runFailingSql(
+                            "INSERT INTO sink_t SELECT * FROM source_t WHERE `a`.`id` = 1",
+                            TableRuntimeException.class,
+                            "Column 'a.ab[0][0]' is NOT NULL, however, a null"
+                                    + " value is being written into it. You can set job"
+                                    + " configuration 'table.exec.sink.not-null-enforcer'='DROP'"
+                                    + " to suppress this exception and drop such records silently.")
+                    .runFailingSql(
+                            "INSERT INTO sink_t SELECT * FROM source_t WHERE `a`.`id` = 2",
+                            TableRuntimeException.class,
+                            "Column 'a.ab[0]' is NOT NULL, however, a null"
+                                    + " value is being written into it. You can set job"
+                                    + " configuration 'table.exec.sink.not-null-enforcer'='DROP'"
+                                    + " to suppress this exception and drop such records silently.")
+                    .build();
+
+    public static final String SCHEMA_MAPS_NOT_NULL =
+            "a ROW<"
+                    + "id BIGINT NOT NULL, "
+                    + "ab MAP<BIGINT NOT NULL, BIGINT NOT NULL> NOT NULL"
+                    + "> NOT NULL";
+    static final TableTestProgram NOT_NULL_DROP_NESTED_MAPS =
+            TableTestProgram.of(
+                            "constraint-enforcer-drop-not-null-nested-maps",
+                            "validates"
+                                    + " constraint enforcer drops records with nulls for NOT NULL"
+                                    + " columns in nested maps")
+                    .setupConfig(
+                            TABLE_EXEC_SINK_NESTED_CONSTRAINT_ENFORCER,
+                            NestedEnforcer.ROWS_AND_COLLECTIONS)
+                    .setupConfig(TABLE_EXEC_SINK_NOT_NULL_ENFORCER, NotNullEnforcer.DROP)
+                    .setupTableSource(
+                            SourceTestStep.newBuilder("source_t")
+                                    .addSchema(SCHEMA_MAPS_NOT_NULL)
+                                    .producedValues(
+                                            // should be dropped because the key in a nested MAP
+                                            // is null
+                                            Row.of(
+                                                    Row.of( // a
+                                                            1L, // id
+                                                            mapOfNullable(null, 1L) // ab
+                                                            )),
+                                            // should be dropped because the value in a nested MAP
+                                            // is null
+                                            Row.of(
+                                                    Row.of( // a
+                                                            2L, // id
+                                                            mapOfNullable(1L, null) // ab
+                                                            )),
+                                            // a valid record, which should be kept
+                                            Row.of(
+                                                    Row.of( // a
+                                                            3L, // id
+                                                            mapOfNullable(1L, 1L) // ab
+                                                            )))
+                                    .build())
+                    .setupTableSink(
+                            SinkTestStep.newBuilder("sink_t")
+                                    .addSchema(SCHEMA_MAPS_NOT_NULL)
+                                    .consumedValues(
+                                            Row.of(
+                                                    Row.of( // a
+                                                            3L, // id
+                                                            mapOfNullable(1L, 1L) // ab
+                                                            )))
+                                    .build())
+                    .runSql("INSERT INTO sink_t SELECT * FROM source_t")
+                    .build();
+
+    static final TableTestProgram NOT_NULL_ERROR_NESTED_MAPS =
+            TableTestProgram.of(
+                            "constraint-enforcer-error-not-null-nested-maps",
+                            "validates"
+                                    + " constraint enforcer throws an exception when writing nulls"
+                                    + " into in nested maps which require NOT NULL values and/or"
+                                    + " keys")
+                    .setupConfig(
+                            TABLE_EXEC_SINK_NESTED_CONSTRAINT_ENFORCER,
+                            NestedEnforcer.ROWS_AND_COLLECTIONS)
+                    .setupConfig(TABLE_EXEC_SINK_NOT_NULL_ENFORCER, NotNullEnforcer.ERROR)
+                    .setupTableSource(
+                            SourceTestStep.newBuilder("source_t")
+                                    .addSchema(SCHEMA_MAPS_NOT_NULL)
+                                    .producedValues(
+                                            // should be dropped because the key in a nested MAP
+                                            // is null
+                                            Row.of(
+                                                    Row.of( // a
+                                                            1L, // id
+                                                            mapOfNullable(null, 1L) // ab
+                                                            )),
+                                            // should be dropped because the value in a nested MAP
+                                            // is null
+                                            Row.of(
+                                                    Row.of( // a
+                                                            2L, // id
+                                                            mapOfNullable(1L, null) // ab
+                                                            )))
+                                    .build())
+                    .setupTableSink(
+                            SinkTestStep.newBuilder("sink_t")
+                                    .addSchema(SCHEMA_MAPS_NOT_NULL)
+                                    .consumedValues(new Row[0])
+                                    .build())
+                    .runFailingSql(
+                            "INSERT INTO sink_t SELECT * FROM source_t WHERE `a`.`id` = 1",
+                            TableRuntimeException.class,
+                            "Column 'a.ab[key]' is NOT NULL, however, a null"
+                                    + " value is being written into it. You can set job"
+                                    + " configuration 'table.exec.sink.not-null-enforcer'='DROP'"
+                                    + " to suppress this exception and drop such records silently.")
+                    .runFailingSql(
+                            "INSERT INTO sink_t SELECT * FROM source_t WHERE `a`.`id` = 2",
+                            TableRuntimeException.class,
+                            "Column 'a.ab[value]' is NOT NULL, however, a null"
+                                    + " value is being written into it. You can set job"
+                                    + " configuration 'table.exec.sink.not-null-enforcer'='DROP'"
+                                    + " to suppress this exception and drop such records silently.")
+                    .build();
+
+    public static final String SCHEMA_ARRAYS_LENGTH =
+            "a ROW<"
+                    + "id BIGINT NOT NULL, "
+                    + "ab ARRAY<"
+                    + " ROW<"
+                    + "     c CHAR(2) NOT NULL,"
+                    + "     vc VARCHAR(2) NOT NULL,"
+                    + "     b BINARY(2) NOT NULL,"
+                    + "     vb VARBINARY(2) NOT NULL"
+                    + " > NOT NULL"
+                    + "> NOT NULL> NOT NULL";
+    static final TableTestProgram LENGTH_TRIM_PAD_WITH_NESTED_COLLECTIONS =
+            TableTestProgram.of(
+                            "constraint-enforcer-trim-pad-nested-collections",
+                            "validates"
+                                    + " constraint enforcer does not work with trim padding if"
+                                    + " checks for nested collections is enabled")
+                    .setupConfig(
+                            TABLE_EXEC_SINK_NESTED_CONSTRAINT_ENFORCER,
+                            NestedEnforcer.ROWS_AND_COLLECTIONS)
+                    .setupConfig(TABLE_EXEC_SINK_TYPE_LENGTH_ENFORCER, TypeLengthEnforcer.TRIM_PAD)
+                    .setupTableSource(
+                            SourceTestStep.newBuilder("source_t")
+                                    .addSchema(SCHEMA_ARRAYS_LENGTH)
+                                    .producedValues(
+                                            Row.of(
+                                                    Row.of( // a
+                                                            1L, // id
+                                                            new Row[] {} // ab
+                                                            )))
+                                    .build())
+                    .setupTableSink(
+                            SinkTestStep.newBuilder("sink_t")
+                                    .addSchema(SCHEMA_ARRAYS_LENGTH)
+                                    .consumedValues(new Row[0])
+                                    .build())
+                    .runFailingSql(
+                            "INSERT INTO sink_t SELECT * FROM source_t",
+                            ValidationException.class,
+                            "Trimming and/or padding is not supported if"
+                                    + " constraint checking is enabled on types nested"
+                                    + " in collections.")
+                    .build();
+
+    static final TableTestProgram LENGTH_ERROR_WITH_NESTED_ARRAYS =
+            TableTestProgram.of(
+                            "constraint-enforcer-error-nested-arrays",
+                            "validates"
+                                    + " constraint enforcer throws an exception if character or"
+                                    + " binary string of invalid lengths are written into elements"
+                                    + " of an array")
+                    .setupConfig(
+                            TABLE_EXEC_SINK_NESTED_CONSTRAINT_ENFORCER,
+                            NestedEnforcer.ROWS_AND_COLLECTIONS)
+                    .setupConfig(TABLE_EXEC_SINK_TYPE_LENGTH_ENFORCER, TypeLengthEnforcer.ERROR)
+                    .setupTableSource(
+                            SourceTestStep.newBuilder("source_t")
+                                    .addSchema(SCHEMA_ARRAYS_LENGTH)
+                                    .producedValues(
+                                            Row.of(
+                                                    Row.of( // a
+                                                            1L, // id
+                                                            new Row[] {
+                                                                Row.of(
+                                                                        "a", // c
+                                                                        "a", // vc
+                                                                        new byte[] {1, 2}, // b
+                                                                        new byte[] {1} // vb
+                                                                        )
+                                                            } // ab
+                                                            )),
+                                            Row.of(
+                                                    Row.of( // a
+                                                            2L, // id
+                                                            new Row[] {
+                                                                Row.of(
+                                                                        "ab", // c
+                                                                        "abc", // vc
+                                                                        new byte[] {1, 2}, // b
+                                                                        new byte[] {1} // vb
+                                                                        )
+                                                            } // ab
+                                                            )),
+                                            Row.of(
+                                                    Row.of( // a
+                                                            3L, // id
+                                                            new Row[] {
+                                                                Row.of(
+                                                                        "ab", // c
+                                                                        "a", // vc
+                                                                        new byte[] {1}, // b
+                                                                        new byte[] {1} // vb
+                                                                        )
+                                                            } // ab
+                                                            )),
+                                            Row.of(
+                                                    Row.of( // a
+                                                            4L, // id
+                                                            new Row[] {
+                                                                Row.of(
+                                                                        "ab", // c
+                                                                        "a", // vc
+                                                                        new byte[] {1, 2}, // b
+                                                                        new byte[] {1, 2, 3} // vb
+                                                                        )
+                                                            } // ab
+                                                            )))
+                                    .build())
+                    .setupTableSink(
+                            SinkTestStep.newBuilder("sink_t")
+                                    .addSchema(SCHEMA_ARRAYS_LENGTH)
+                                    .consumedValues(new Row[0])
+                                    .build())
+                    .runFailingSql(
+                            "INSERT INTO sink_t SELECT * FROM source_t WHERE id = 1",
+                            TableRuntimeException.class,
+                            "Column 'a.ab[0].c' is CHAR(2), however, a string"
+                                    + " of length 1 is being written into it. You can set job"
+                                    + " configuration 'table.exec.sink.type-length-enforcer' to"
+                                    + " control this behaviour")
+                    .runFailingSql(
+                            "INSERT INTO sink_t SELECT * FROM source_t WHERE id = 2",
+                            TableRuntimeException.class,
+                            "Column 'a.ab[0].vc' is VARCHAR(2), however, a string"
+                                    + " of length 3 is being written into it. You can set job"
+                                    + " configuration 'table.exec.sink.type-length-enforcer' to"
+                                    + " control this behaviour")
+                    .runFailingSql(
+                            "INSERT INTO sink_t SELECT * FROM source_t WHERE id = 3",
+                            TableRuntimeException.class,
+                            "Column 'a.ab[0].b' is BINARY(2), however, a string"
+                                    + " of length 1 is being written into it. You can set job"
+                                    + " configuration 'table.exec.sink.type-length-enforcer' to"
+                                    + " control this behaviour")
+                    .runFailingSql(
+                            "INSERT INTO sink_t SELECT * FROM source_t WHERE id = 4",
+                            TableRuntimeException.class,
+                            "Column 'a.ab[0].vb' is VARBINARY(2), however, a string"
+                                    + " of length 3 is being written into it. You can set job"
+                                    + " configuration 'table.exec.sink.type-length-enforcer' to"
+                                    + " control this behaviour")
+                    .build();
+
+    public static final String SCHEMA_MAPS_LENGTH =
+            "a ROW<"
+                    + "id BIGINT NOT NULL, "
+                    + "ac MAP<"
+                    + "     CHAR(2) NOT NULL,"
+                    + "     VARCHAR(2) NOT NULL"
+                    + " > NOT NULL, "
+                    + "ab MAP<"
+                    + "     BINARY(2) NOT NULL,"
+                    + "     VARBINARY(2) NOT NULL"
+                    + " > NOT NULL "
+                    + "> NOT NULL";
+    static final TableTestProgram LENGTH_ERROR_WITH_NESTED_MAPS =
+            TableTestProgram.of(
+                            "constraint-enforcer-error-nested-maps",
+                            "validates"
+                                    + " constraint enforcer throws an exception if character or"
+                                    + " binary string of invalid lengths are written into elements"
+                                    + " of a map")
+                    .setupConfig(
+                            TABLE_EXEC_SINK_NESTED_CONSTRAINT_ENFORCER,
+                            NestedEnforcer.ROWS_AND_COLLECTIONS)
+                    .setupConfig(TABLE_EXEC_SINK_TYPE_LENGTH_ENFORCER, TypeLengthEnforcer.ERROR)
+                    .setupTableSource(
+                            SourceTestStep.newBuilder("source_t")
+                                    .addSchema(SCHEMA_MAPS_LENGTH)
+                                    .producedValues(
+                                            // key which is BINARY is too short
+                                            Row.of(
+                                                    Row.of( // a
+                                                            1L, // id
+                                                            Map.of("a", "a"), // ac
+                                                            Map.of(
+                                                                    new byte[] {1, 2},
+                                                                    new byte[] {1}) // ab
+                                                            )),
+                                            // value which is VARCHAR is too long
+                                            Row.of(
+                                                    Row.of( // a
+                                                            2L, // id
+                                                            Map.of("ab", "abc"), // ac
+                                                            Map.of(
+                                                                    new byte[] {1, 2},
+                                                                    new byte[] {1}) // ab
+                                                            )),
+                                            // key which is BINARY is too short
+                                            Row.of(
+                                                    Row.of( // a
+                                                            3L, // id
+                                                            Map.of("ab", "a"), // ac
+                                                            Map.of(
+                                                                    new byte[] {1},
+                                                                    new byte[] {1}) // ab
+                                                            )),
+                                            // value which is VARBINARY is too long
+                                            Row.of(
+                                                    Row.of( // a
+                                                            4L, // id
+                                                            Map.of("ab", "a"), // ac
+                                                            Map.of(
+                                                                    new byte[] {1, 2},
+                                                                    new byte[] {1, 2, 3}) // ab
+                                                            )))
+                                    .build())
+                    .setupTableSink(
+                            SinkTestStep.newBuilder("sink_t")
+                                    .addSchema(SCHEMA_MAPS_LENGTH)
+                                    .consumedValues(new Row[0])
+                                    .build())
+                    .runFailingSql(
+                            "INSERT INTO sink_t SELECT * FROM source_t WHERE id = 1",
+                            TableRuntimeException.class,
+                            "Column 'a.ac[key]' is CHAR(2), however, a string"
+                                    + " of length 1 is being written into it. You can set job"
+                                    + " configuration 'table.exec.sink.type-length-enforcer' to"
+                                    + " control this behaviour")
+                    .runFailingSql(
+                            "INSERT INTO sink_t SELECT * FROM source_t WHERE id = 2",
+                            TableRuntimeException.class,
+                            "Column 'a.ac[value]' is VARCHAR(2), however, a string"
+                                    + " of length 3 is being written into it. You can set job"
+                                    + " configuration 'table.exec.sink.type-length-enforcer' to"
+                                    + " control this behaviour")
+                    .runFailingSql(
+                            "INSERT INTO sink_t SELECT * FROM source_t WHERE id = 3",
+                            TableRuntimeException.class,
+                            "Column 'a.ab[key]' is BINARY(2), however, a string"
+                                    + " of length 1 is being written into it. You can set job"
+                                    + " configuration 'table.exec.sink.type-length-enforcer' to"
+                                    + " control this behaviour")
+                    .runFailingSql(
+                            "INSERT INTO sink_t SELECT * FROM source_t WHERE id = 4",
+                            TableRuntimeException.class,
+                            "Column 'a.ab[value]' is VARBINARY(2), however, a string"
+                                    + " of length 3 is being written into it. You can set job"
+                                    + " configuration 'table.exec.sink.type-length-enforcer' to"
+                                    + " control this behaviour")
+                    .build();
+
+    private static Map<Long, Long> mapOfNullable(@Nullable Long key, @Nullable Long value) {
+        final Map<Long, Long> map = new HashMap<>();
+        map.put(key, value);
+        return map;
+    }
+}
diff --git a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/ConstraintEnforcer.java b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/ConstraintEnforcer.java
deleted file mode 100644
index 35c9888c397..00000000000
--- a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/ConstraintEnforcer.java
+++ /dev/null
@@ -1,361 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.table.runtime.operators.sink;
-
-import org.apache.flink.annotation.Internal;
-import org.apache.flink.streaming.api.operators.OneInputStreamOperator;
-import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
-import org.apache.flink.table.api.TableException;
-import org.apache.flink.table.api.config.ExecutionConfigOptions.NotNullEnforcer;
-import org.apache.flink.table.data.RowData;
-import org.apache.flink.table.data.StringData;
-import org.apache.flink.table.data.UpdatableRowData;
-import org.apache.flink.table.data.binary.BinaryStringData;
-import org.apache.flink.table.runtime.operators.TableStreamOperator;
-import org.apache.flink.table.runtime.util.SegmentsUtil;
-import org.apache.flink.table.runtime.util.StreamRecordCollector;
-
-import javax.annotation.Nullable;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.BitSet;
-import java.util.List;
-
-import static org.apache.flink.table.api.config.ExecutionConfigOptions.TABLE_EXEC_SINK_NOT_NULL_ENFORCER;
-import static org.apache.flink.table.api.config.ExecutionConfigOptions.TypeLengthEnforcer;
-import static org.apache.flink.util.Preconditions.checkArgument;
-
-/**
- * Processes {@link RowData} to enforce the following constraints:
- *
- * <ul>
- *   <li>{@code NOT NULL} column constraint of a sink table
- *   <li>{@code CHAR(length)}/@{code VARCHAR(length)}: trim string values to comply with the {@code
- *       length} defined in their corresponding types.
- * </ul>
- */
-@Internal
-public class ConstraintEnforcer extends TableStreamOperator<RowData>
-        implements OneInputStreamOperator<RowData, RowData> {
-
-    private static final long serialVersionUID = 1L;
-
-    private final NotNullEnforcer notNullEnforcer;
-    private final int[] notNullFieldIndices;
-    private final String[] allFieldNames;
-
-    private final TypeLengthEnforcer typeLengthEnforcer;
-    private final int[] charFieldIndices;
-    private final int[] charFieldLengths;
-    private final BitSet charFieldCouldPad;
-    private final int[] binaryFieldIndices;
-    private final int[] binaryFieldLengths;
-    private final BitSet binaryFieldCouldPad;
-
-    private final String operatorName;
-
-    private transient StreamRecordCollector<RowData> collector;
-
-    private ConstraintEnforcer(
-            NotNullEnforcer notNullEnforcer,
-            int[] notNullFieldIndices,
-            TypeLengthEnforcer typeLengthEnforcer,
-            int[] charFieldIndices,
-            int[] charFieldLengths,
-            BitSet charFieldCouldPad,
-            int[] binaryFieldIndices,
-            int[] binaryFieldLengths,
-            BitSet binaryFieldCouldPad,
-            String[] allFieldNames,
-            String operatorName) {
-        this.notNullEnforcer = notNullEnforcer;
-        this.notNullFieldIndices = notNullFieldIndices;
-        this.typeLengthEnforcer = typeLengthEnforcer;
-        this.charFieldIndices = charFieldIndices;
-        this.charFieldLengths = charFieldLengths;
-        this.charFieldCouldPad = charFieldCouldPad;
-        this.binaryFieldIndices = binaryFieldIndices;
-        this.binaryFieldLengths = binaryFieldLengths;
-        this.binaryFieldCouldPad = binaryFieldCouldPad;
-        this.allFieldNames = allFieldNames;
-        this.operatorName = operatorName;
-    }
-
-    @Override
-    public String getOperatorName() {
-        return operatorName;
-    }
-
-    @Override
-    public void open() throws Exception {
-        super.open();
-        collector = new StreamRecordCollector<>(output);
-    }
-
-    public static Builder newBuilder() {
-        return new Builder();
-    }
-
-    /**
-     * Helper builder, so that the {@link ConstraintEnforcer} can be instantiated with only the NOT
-     * NULL constraint validation, only the CHAR/VARCHAR length validation, only the
-     * BINARY/VARBINARY length validation or combinations of them, or all of them.
-     */
-    public static class Builder {
-
-        private NotNullEnforcer notNullEnforcer;
-        private int[] notNullFieldIndices;
-
-        private TypeLengthEnforcer typeLengthEnforcer;
-        private List<FieldInfo> charFieldInfo;
-        private List<FieldInfo> binaryFieldInfo;
-        private String[] allFieldNames;
-
-        private final List<String> operatorNames = new ArrayList<>();
-
-        private boolean isConfigured = false;
-
-        public void addNotNullConstraint(
-                NotNullEnforcer notNullEnforcer,
-                int[] notNullFieldIndices,
-                List<String> notNullFieldNames,
-                String[] allFieldNames) {
-            checkArgument(
-                    notNullFieldIndices.length > 0,
-                    "ConstraintValidator requires that there are not-null fields.");
-            this.notNullFieldIndices = notNullFieldIndices;
-            this.notNullEnforcer = notNullEnforcer;
-            this.allFieldNames = allFieldNames;
-            if (notNullEnforcer != null) {
-                operatorNames.add(
-                        String.format(
-                                "NotNullEnforcer(fields=[%s])",
-                                String.join(", ", notNullFieldNames)));
-                this.isConfigured = true;
-            }
-        }
-
-        public void addCharLengthConstraint(
-                TypeLengthEnforcer typeLengthEnforcer,
-                List<FieldInfo> charFieldInfo,
-                List<String> charFieldNames,
-                String[] allFieldNames) {
-            this.typeLengthEnforcer = typeLengthEnforcer;
-            if (this.typeLengthEnforcer == TypeLengthEnforcer.TRIM_PAD) {
-                checkArgument(
-                        charFieldInfo.size() > 0,
-                        "ConstraintValidator requires that there are CHAR/VARCHAR fields.");
-                this.charFieldInfo = charFieldInfo;
-                this.allFieldNames = allFieldNames;
-
-                operatorNames.add(
-                        String.format(
-                                "LengthEnforcer(fields=[%s])", String.join(", ", charFieldNames)));
-                this.isConfigured = true;
-            }
-        }
-
-        public void addBinaryLengthConstraint(
-                TypeLengthEnforcer typeLengthEnforcer,
-                List<FieldInfo> binaryFieldInfo,
-                List<String> binaryFieldNames,
-                String[] allFieldNames) {
-            this.typeLengthEnforcer = typeLengthEnforcer;
-            if (this.typeLengthEnforcer == TypeLengthEnforcer.TRIM_PAD) {
-                checkArgument(
-                        binaryFieldInfo.size() > 0,
-                        "ConstraintValidator requires that there are BINARY/VARBINARY fields.");
-                this.binaryFieldInfo = binaryFieldInfo;
-                this.allFieldNames = allFieldNames;
-
-                operatorNames.add(
-                        String.format(
-                                "LengthEnforcer(fields=[%s])",
-                                String.join(", ", binaryFieldNames)));
-                this.isConfigured = true;
-            }
-        }
-
-        /**
-         * If neither of NOT NULL or CHAR/VARCHAR length or BINARY/VARBINARY enforcers are
-         * configured, null is returned.
-         */
-        public ConstraintEnforcer build() {
-            if (isConfigured) {
-                String operatorName =
-                        "ConstraintEnforcer[" + String.join(", ", operatorNames) + "]";
-                return new ConstraintEnforcer(
-                        notNullEnforcer,
-                        notNullFieldIndices,
-                        typeLengthEnforcer,
-                        charFieldInfo != null
-                                ? charFieldInfo.stream().mapToInt(fi -> fi.fieldIdx).toArray()
-                                : null,
-                        charFieldInfo != null
-                                ? charFieldInfo.stream().mapToInt(fi -> fi.length).toArray()
-                                : null,
-                        charFieldInfo != null ? buildCouldPad(charFieldInfo) : null,
-                        binaryFieldInfo != null
-                                ? binaryFieldInfo.stream().mapToInt(fi -> fi.fieldIdx).toArray()
-                                : null,
-                        binaryFieldInfo != null
-                                ? binaryFieldInfo.stream().mapToInt(fi -> fi.length).toArray()
-                                : null,
-                        binaryFieldInfo != null ? buildCouldPad(binaryFieldInfo) : null,
-                        allFieldNames,
-                        operatorName);
-            }
-            return null;
-        }
-    }
-
-    private static BitSet buildCouldPad(List<FieldInfo> charFieldInfo) {
-        BitSet couldPad = new BitSet(charFieldInfo.size());
-        for (int i = 0; i < charFieldInfo.size(); i++) {
-            if (charFieldInfo.get(i).couldPad) {
-                couldPad.set(i);
-            }
-        }
-        return couldPad;
-    }
-
-    @Override
-    public void processElement(StreamRecord<RowData> element) throws Exception {
-        RowData processedRowData = processNotNullConstraint(element.getValue());
-        // If NOT NULL constraint is not respected don't proceed to process the other constraints,
-        // simply drop the record.
-        if (processedRowData != null) {
-            processedRowData = processCharConstraint(processedRowData);
-            processedRowData = processBinaryConstraint(processedRowData);
-            collector.collect(processedRowData);
-        }
-    }
-
-    private @Nullable RowData processNotNullConstraint(RowData rowData) {
-        if (notNullEnforcer == null) {
-            return rowData;
-        }
-
-        for (int index : notNullFieldIndices) {
-            if (rowData.isNullAt(index)) {
-                switch (notNullEnforcer) {
-                    case ERROR:
-                        throw new TableException(
-                                String.format(
-                                        "Column '%s' is NOT NULL, however, a null value is being written into it. "
-                                                + "You can set job configuration '%s'='%s' "
-                                                + "to suppress this exception and drop such records silently.",
-                                        allFieldNames[index],
-                                        TABLE_EXEC_SINK_NOT_NULL_ENFORCER.key(),
-                                        NotNullEnforcer.DROP.name()));
-                    case DROP:
-                        return null;
-                }
-            }
-        }
-        return rowData;
-    }
-
-    private RowData processCharConstraint(RowData rowData) {
-        if (typeLengthEnforcer == null
-                || typeLengthEnforcer == TypeLengthEnforcer.IGNORE
-                || charFieldIndices == null) {
-            return rowData;
-        }
-
-        UpdatableRowData updatedRowData = null;
-
-        for (int i = 0; i < charFieldIndices.length; i++) {
-            final int fieldIdx = charFieldIndices[i];
-            final int length = charFieldLengths[i];
-            final BinaryStringData stringData = (BinaryStringData) rowData.getString(fieldIdx);
-            final int sourceStrLength = stringData.numChars();
-
-            if (charFieldCouldPad.get(i) && sourceStrLength < length) {
-                if (updatedRowData == null) {
-                    updatedRowData = new UpdatableRowData(rowData, allFieldNames.length);
-                }
-                final int srcSizeInBytes = stringData.getSizeInBytes();
-                final byte[] newString = new byte[srcSizeInBytes + length - sourceStrLength];
-                for (int j = srcSizeInBytes; j < newString.length; j++) {
-                    newString[j] = (byte) 32; // space
-                }
-                SegmentsUtil.copyToBytes(stringData.getSegments(), 0, newString, 0, srcSizeInBytes);
-                updatedRowData.setField(fieldIdx, StringData.fromBytes(newString));
-            } else if (sourceStrLength > length) {
-                if (updatedRowData == null) {
-                    updatedRowData = new UpdatableRowData(rowData, allFieldNames.length);
-                }
-                updatedRowData.setField(fieldIdx, stringData.substring(0, length));
-            }
-        }
-
-        return updatedRowData != null ? updatedRowData : rowData;
-    }
-
-    private RowData processBinaryConstraint(RowData rowData) {
-        if (typeLengthEnforcer == null
-                || typeLengthEnforcer == TypeLengthEnforcer.IGNORE
-                || binaryFieldIndices == null) {
-            return rowData;
-        }
-
-        UpdatableRowData updatedRowData = null;
-
-        for (int i = 0; i < binaryFieldLengths.length; i++) {
-            final int fieldIdx = binaryFieldIndices[i];
-            final int length = binaryFieldLengths[i];
-            final byte[] binaryData = rowData.getBinary(fieldIdx);
-            final int sourceLength = binaryData.length;
-
-            // Trimming takes places because of the shorter length used in `Arrays.copyOf` and
-            // padding because of the longer length, as implicitly the trailing bytes are 0.
-            if ((sourceLength > length) || (binaryFieldCouldPad.get(i) && sourceLength < length)) {
-                if (updatedRowData == null) {
-                    updatedRowData = new UpdatableRowData(rowData, allFieldNames.length);
-                }
-                updatedRowData.setField(fieldIdx, Arrays.copyOf(binaryData, length));
-            }
-        }
-
-        return updatedRowData != null ? updatedRowData : rowData;
-    }
-
-    /**
-     * Helper POJO to keep info about CHAR/VARCHAR/BINARY/VARBINARY fields, used to determine if
-     * trimming or padding is needed.
-     */
-    @Internal
-    public static class FieldInfo {
-        private final int fieldIdx;
-        private final Integer length;
-        private final boolean couldPad;
-
-        public FieldInfo(int fieldIdx, @Nullable Integer length, boolean couldPad) {
-            this.fieldIdx = fieldIdx;
-            this.length = length;
-            this.couldPad = couldPad;
-        }
-
-        public int fieldIdx() {
-            return fieldIdx;
-        }
-    }
-}
diff --git a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/BinaryLengthConstraint.java b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/BinaryLengthConstraint.java
new file mode 100644
index 00000000000..692462dd773
--- /dev/null
+++ b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/BinaryLengthConstraint.java
@@ -0,0 +1,117 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.runtime.operators.sink.constraint;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.table.data.RowData;
+import org.apache.flink.table.data.UpdatableRowData;
+
+import javax.annotation.Nullable;
+
+import java.util.Arrays;
+import java.util.BitSet;
+
+import static org.apache.flink.table.api.config.ExecutionConfigOptions.TABLE_EXEC_SINK_TYPE_LENGTH_ENFORCER;
+
+/** Enforces length constraints on the input {@link RowData}. */
+@Internal
+final class BinaryLengthConstraint implements Constraint {
+
+    private final TypeLengthEnforcementStrategy typeLengthEnforcementStrategy;
+    private final int[] fieldIndices;
+    private final int[] fieldLengths;
+    private final String[] fieldNames;
+    private final BitSet fieldCouldPad;
+
+    BinaryLengthConstraint(
+            final TypeLengthEnforcementStrategy typeLengthEnforcementStrategy,
+            final int[] fieldIndices,
+            final int[] fieldLengths,
+            final String[] fieldNames,
+            final BitSet fieldCouldPad) {
+        this.typeLengthEnforcementStrategy = typeLengthEnforcementStrategy;
+        this.fieldIndices = fieldIndices;
+        this.fieldLengths = fieldLengths;
+        this.fieldNames = fieldNames;
+        this.fieldCouldPad = fieldCouldPad;
+    }
+
+    @Nullable
+    @Override
+    public RowData enforce(RowData rowData) {
+        UpdatableRowData updatedRowData = null;
+
+        for (int i = 0; i < fieldLengths.length; i++) {
+            final int fieldIdx = fieldIndices[i];
+            final int expectedLength = fieldLengths[i];
+            final byte[] binaryData = rowData.getBinary(fieldIdx);
+            final int actualLength = binaryData.length;
+            final boolean shouldPad = fieldCouldPad.get(i);
+
+            // Trimming takes places because of the shorter length used in `Arrays.copyOf` and
+            // padding because of the longer length, as implicitly the trailing bytes are 0.
+            if ((actualLength > expectedLength) || (shouldPad && actualLength < expectedLength)) {
+                switch (typeLengthEnforcementStrategy) {
+                    case TRIM_PAD:
+                        updatedRowData =
+                                trimOrPad(
+                                        rowData,
+                                        updatedRowData,
+                                        fieldIdx,
+                                        binaryData,
+                                        expectedLength);
+                        break;
+                    case THROW:
+                        throw new EnforcerException(
+                                "Column '%s'"
+                                        + String.format(
+                                                " is %s, however, a string of length %s is being written into it. "
+                                                        + "You can set job configuration '%s' "
+                                                        + "to control this behaviour.",
+                                                (shouldPad ? "BINARY(" : "VARBINARY(")
+                                                        + expectedLength
+                                                        + ")",
+                                                actualLength,
+                                                TABLE_EXEC_SINK_TYPE_LENGTH_ENFORCER.key()),
+                                fieldNames[i]);
+                }
+            }
+        }
+
+        return updatedRowData != null ? updatedRowData : rowData;
+    }
+
+    private static UpdatableRowData trimOrPad(
+            RowData rowData,
+            UpdatableRowData updatedRowData,
+            int fieldIdx,
+            byte[] binaryData,
+            int length) {
+        if (updatedRowData == null) {
+            updatedRowData = new UpdatableRowData(rowData, rowData.getArity());
+        }
+        updatedRowData.setField(fieldIdx, Arrays.copyOf(binaryData, length));
+        return updatedRowData;
+    }
+
+    @Override
+    public String toString() {
+        return String.format("LengthEnforcer(fields=[%s])", String.join(", ", fieldNames));
+    }
+}
diff --git a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/CharLengthConstraint.java b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/CharLengthConstraint.java
new file mode 100644
index 00000000000..641be2f4ebe
--- /dev/null
+++ b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/CharLengthConstraint.java
@@ -0,0 +1,137 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.runtime.operators.sink.constraint;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.table.data.RowData;
+import org.apache.flink.table.data.StringData;
+import org.apache.flink.table.data.UpdatableRowData;
+import org.apache.flink.table.data.binary.BinaryStringData;
+import org.apache.flink.table.runtime.util.SegmentsUtil;
+
+import javax.annotation.Nullable;
+
+import java.util.BitSet;
+
+import static org.apache.flink.table.api.config.ExecutionConfigOptions.TABLE_EXEC_SINK_TYPE_LENGTH_ENFORCER;
+
+/** Enforces length constraints on the input {@link RowData}. */
+@Internal
+final class CharLengthConstraint implements Constraint {
+
+    private final TypeLengthEnforcementStrategy enforcementStrategy;
+    private final int[] fieldIndices;
+    private final int[] fieldLengths;
+    private final String[] fieldNames;
+    private final BitSet fieldCouldPad;
+
+    CharLengthConstraint(
+            final TypeLengthEnforcementStrategy enforcementStrategy,
+            final int[] charFieldIndices,
+            final int[] charFieldLengths,
+            final String[] charFieldNames,
+            final BitSet charFieldCouldPad) {
+        this.enforcementStrategy = enforcementStrategy;
+        this.fieldIndices = charFieldIndices;
+        this.fieldLengths = charFieldLengths;
+        this.fieldNames = charFieldNames;
+        this.fieldCouldPad = charFieldCouldPad;
+    }
+
+    @Nullable
+    @Override
+    public RowData enforce(RowData rowData) {
+        UpdatableRowData updatedRowData = null;
+
+        for (int i = 0; i < fieldIndices.length; i++) {
+            final int fieldIdx = fieldIndices[i];
+            final int expectedLength = fieldLengths[i];
+            final BinaryStringData stringData = (BinaryStringData) rowData.getString(fieldIdx);
+            final int actualLength = stringData.numChars();
+            final boolean shouldPad = fieldCouldPad.get(i);
+
+            switch (enforcementStrategy) {
+                case TRIM_PAD:
+                    updatedRowData =
+                            trimOrPad(
+                                    rowData,
+                                    actualLength,
+                                    expectedLength,
+                                    updatedRowData,
+                                    stringData,
+                                    fieldIdx,
+                                    shouldPad);
+                    break;
+                case THROW:
+                    if (actualLength > expectedLength
+                            || shouldPad && actualLength < expectedLength) {
+                        throw new EnforcerException(
+                                "Column '%s'"
+                                        + String.format(
+                                                " is %s, however, a string of length %s is being written into it. "
+                                                        + "You can set job configuration '%s' "
+                                                        + "to control this behaviour.",
+                                                (shouldPad ? "CHAR(" : "VARCHAR(")
+                                                        + expectedLength
+                                                        + ")",
+                                                actualLength,
+                                                TABLE_EXEC_SINK_TYPE_LENGTH_ENFORCER.key()),
+                                fieldNames[i]);
+                    }
+                    break;
+            }
+        }
+
+        return updatedRowData != null ? updatedRowData : rowData;
+    }
+
+    private UpdatableRowData trimOrPad(
+            RowData rowData,
+            int actualLength,
+            int expectedLength,
+            UpdatableRowData updatedRowData,
+            BinaryStringData stringData,
+            int fieldIdx,
+            boolean canPad) {
+        if (canPad && actualLength < expectedLength) {
+            if (updatedRowData == null) {
+                updatedRowData = new UpdatableRowData(rowData, rowData.getArity());
+            }
+            final int srcSizeInBytes = stringData.getSizeInBytes();
+            final byte[] newString = new byte[srcSizeInBytes + expectedLength - actualLength];
+            for (int j = srcSizeInBytes; j < newString.length; j++) {
+                newString[j] = (byte) 32; // space
+            }
+            SegmentsUtil.copyToBytes(
+                    stringData.getSegments(), stringData.getOffset(), newString, 0, srcSizeInBytes);
+            updatedRowData.setField(fieldIdx, StringData.fromBytes(newString));
+        } else if (actualLength > expectedLength) {
+            if (updatedRowData == null) {
+                updatedRowData = new UpdatableRowData(rowData, rowData.getArity());
+            }
+            updatedRowData.setField(fieldIdx, stringData.substring(0, expectedLength));
+        }
+        return updatedRowData;
+    }
+
+    @Override
+    public String toString() {
+        return String.format("LengthEnforcer(fields=[%s])", String.join(", ", fieldNames));
+    }
+}
diff --git a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/Constraint.java b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/Constraint.java
new file mode 100644
index 00000000000..813c4c4bcd3
--- /dev/null
+++ b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/Constraint.java
@@ -0,0 +1,39 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.runtime.operators.sink.constraint;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.table.data.RowData;
+
+import javax.annotation.Nullable;
+
+import java.io.Serializable;
+
+/** Interface for constraints to be enforced on the input {@link RowData}. */
+@Internal
+public interface Constraint extends Serializable {
+
+    /**
+     * If a null value is returned, the input row is considered to be invalid and should be dropped.
+     * This applies to all nested columns. If at any level a null value is returned, the incoming
+     * row is dropped from further processing.
+     */
+    @Nullable
+    RowData enforce(RowData input);
+}
diff --git a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/ConstraintEnforcer.java b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/ConstraintEnforcer.java
new file mode 100644
index 00000000000..1a62bcb7b52
--- /dev/null
+++ b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/ConstraintEnforcer.java
@@ -0,0 +1,71 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.runtime.operators.sink.constraint;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;
+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
+import org.apache.flink.table.data.RowData;
+import org.apache.flink.table.runtime.operators.TableStreamOperator;
+import org.apache.flink.table.runtime.util.StreamRecordCollector;
+
+/**
+ * Processes {@link RowData} to enforce the following constraints:
+ *
+ * <ul>
+ *   <li>{@code NOT NULL} column constraint of a sink table
+ *   <li>{@code CHAR(length)}/@{code VARCHAR(length)}: trim string values to comply with the {@code
+ *       length} defined in their corresponding types.
+ * </ul>
+ */
+@Internal
+public class ConstraintEnforcer extends TableStreamOperator<RowData>
+        implements OneInputStreamOperator<RowData, RowData> {
+
+    private static final long serialVersionUID = 1L;
+
+    private final String operatorName;
+    private final ConstraintEnforcerExecutor executor;
+
+    private transient StreamRecordCollector<RowData> collector;
+
+    public ConstraintEnforcer(ConstraintEnforcerExecutor executor, String operatorName) {
+        this.executor = executor;
+        this.operatorName = operatorName;
+    }
+
+    @Override
+    public String getOperatorName() {
+        return operatorName;
+    }
+
+    @Override
+    public void open() throws Exception {
+        super.open();
+        collector = new StreamRecordCollector<>(output);
+    }
+
+    @Override
+    public void processElement(StreamRecord<RowData> element) throws Exception {
+        RowData processedRowData = executor.enforce(element.getValue());
+        if (processedRowData != null) {
+            collector.collect(processedRowData);
+        }
+    }
+}
diff --git a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/ConstraintEnforcerExecutor.java b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/ConstraintEnforcerExecutor.java
new file mode 100644
index 00000000000..b8377af468e
--- /dev/null
+++ b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/ConstraintEnforcerExecutor.java
@@ -0,0 +1,552 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.runtime.operators.sink.constraint;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.table.api.ValidationException;
+import org.apache.flink.table.api.config.ExecutionConfigOptions.NestedEnforcer;
+import org.apache.flink.table.api.config.ExecutionConfigOptions.NotNullEnforcer;
+import org.apache.flink.table.api.config.ExecutionConfigOptions.TypeLengthEnforcer;
+import org.apache.flink.table.data.ArrayData;
+import org.apache.flink.table.data.RowData;
+import org.apache.flink.table.runtime.operators.TableStreamOperator;
+import org.apache.flink.table.types.logical.ArrayType;
+import org.apache.flink.table.types.logical.BinaryType;
+import org.apache.flink.table.types.logical.CharType;
+import org.apache.flink.table.types.logical.LogicalType;
+import org.apache.flink.table.types.logical.LogicalTypeRoot;
+import org.apache.flink.table.types.logical.MapType;
+import org.apache.flink.table.types.logical.RowType;
+import org.apache.flink.table.types.logical.StructuredType;
+import org.apache.flink.table.types.logical.utils.LogicalTypeChecks;
+
+import javax.annotation.Nullable;
+
+import java.io.Serializable;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.BitSet;
+import java.util.List;
+import java.util.Optional;
+import java.util.stream.Collectors;
+import java.util.stream.IntStream;
+import java.util.stream.Stream;
+
+/**
+ * Logic extracted from {@link ConstraintEnforcer} in order to use it outside of {@link
+ * TableStreamOperator}.
+ */
+@Internal
+public class ConstraintEnforcerExecutor implements Serializable {
+
+    private static final long serialVersionUID = 1L;
+    private final Constraint[] constraints;
+
+    private ConstraintEnforcerExecutor(Constraint[] constraints) {
+        this.constraints = constraints;
+    }
+
+    public Constraint[] getConstraints() {
+        return constraints;
+    }
+
+    public static Optional<ConstraintEnforcerExecutor> create(
+            final RowType physicalType,
+            final NotNullEnforcer notNullEnforcer,
+            final TypeLengthEnforcer typeLengthEnforcer,
+            final NestedEnforcer nestedConstraints) {
+        final Constraint[] topLevelConstraints =
+                createConstraints(
+                        physicalType, notNullEnforcer, typeLengthEnforcer, nestedConstraints);
+
+        return create(topLevelConstraints);
+    }
+
+    private static Constraint[] createConstraints(
+            final RowType physicalType,
+            final NotNullEnforcer notNullEnforcer,
+            final TypeLengthEnforcer typeLengthEnforcer,
+            final NestedEnforcer nestedEnforcer) {
+        final String[] fieldNames = physicalType.getFieldNames().toArray(new String[0]);
+        final List<Constraint> constraints = new ArrayList<>();
+
+        // Build NOT NULL enforcer
+        final int[] notNullFieldIndices = getNotNullFieldIndices(physicalType);
+        if (notNullFieldIndices.length > 0) {
+            final String[] notNullFieldNames =
+                    Arrays.stream(notNullFieldIndices)
+                            .mapToObj(idx -> fieldNames[idx])
+                            .toArray(String[]::new);
+
+            constraints.add(
+                    new NotNullConstraint(
+                            NotNullEnforcementStrategy.of(notNullEnforcer),
+                            notNullFieldIndices,
+                            notNullFieldNames));
+        }
+
+        if (typeLengthEnforcer != TypeLengthEnforcer.IGNORE) {
+            // Build CHAR/VARCHAR length enforcer
+            addLengthEnforcers(physicalType, typeLengthEnforcer, fieldNames, constraints);
+        }
+
+        // Build nested row constraints
+        if (nestedEnforcer != NestedEnforcer.IGNORE) {
+            addNestedConstraints(
+                    physicalType,
+                    notNullEnforcer,
+                    typeLengthEnforcer,
+                    nestedEnforcer,
+                    fieldNames,
+                    constraints);
+        }
+
+        return constraints.toArray(Constraint[]::new);
+    }
+
+    private static void addNestedConstraints(
+            final RowType physicalType,
+            final NotNullEnforcer notNullEnforcer,
+            final TypeLengthEnforcer typeLengthEnforcer,
+            final NestedEnforcer nestedEnforcer,
+            final String[] fieldNames,
+            final List<Constraint> constraints) {
+        final List<NestedRowInfo> nestedRowInfo = getNestedRowInfos(physicalType);
+        if (!nestedRowInfo.isEmpty()) {
+            final String[] nestedRowFieldNames =
+                    nestedRowInfo.stream()
+                            .map(r -> fieldNames[r.getFieldIdx()])
+                            .toArray(String[]::new);
+
+            final int[] nestedRowFieldIndices =
+                    nestedRowInfo.stream().mapToInt(NestedRowInfo::getFieldIdx).toArray();
+
+            final int[] nestedRowFieldArities =
+                    nestedRowInfo.stream().mapToInt(NestedRowInfo::getArity).toArray();
+
+            final Constraint[][] nestedConstraints =
+                    nestedRowInfo.stream()
+                            .map(
+                                    r ->
+                                            createConstraints(
+                                                    r.getFieldType(),
+                                                    notNullEnforcer,
+                                                    typeLengthEnforcer,
+                                                    nestedEnforcer))
+                            .toArray(Constraint[][]::new);
+
+            constraints.add(
+                    new NestedRowConstraint(
+                            nestedRowFieldIndices,
+                            nestedRowFieldArities,
+                            nestedRowFieldNames,
+                            nestedConstraints));
+        }
+
+        if (nestedEnforcer == NestedEnforcer.ROWS) {
+            return;
+        }
+
+        if (typeLengthEnforcer == TypeLengthEnforcer.TRIM_PAD) {
+            throw new ValidationException(
+                    "Trimming and/or padding is not supported if constraint checking is enabled on"
+                            + " types nested in collections.");
+        }
+
+        // add nested array constraints
+        final List<NestedArrayInfo> nestedArrayInfos = getNestedArrayInfos(physicalType);
+        if (!nestedArrayInfos.isEmpty()) {
+            final String[] nestedRowFieldNames =
+                    nestedArrayInfos.stream()
+                            .map(r -> fieldNames[r.getFieldIdx()])
+                            .toArray(String[]::new);
+
+            final int[] nestedRowFieldIndices =
+                    nestedArrayInfos.stream().mapToInt(NestedArrayInfo::getFieldIdx).toArray();
+
+            final Constraint[][] elementConstraints =
+                    nestedArrayInfos.stream()
+                            .map(
+                                    r1 ->
+                                            createConstraints(
+                                                    new RowType(
+                                                            List.of(
+                                                                    new RowType.RowField(
+                                                                            "element",
+                                                                            r1.getElementType()))),
+                                                    notNullEnforcer,
+                                                    typeLengthEnforcer,
+                                                    nestedEnforcer))
+                            .toArray(Constraint[][]::new);
+
+            final ArrayData.ElementGetter[] elementGetters =
+                    nestedArrayInfos.stream()
+                            .map(r -> ArrayData.createElementGetter(r.getElementType()))
+                            .toArray(ArrayData.ElementGetter[]::new);
+
+            constraints.add(
+                    new NestedArrayConstraint(
+                            nestedRowFieldIndices,
+                            nestedRowFieldNames,
+                            elementConstraints,
+                            elementGetters));
+        }
+
+        // add nested map constraints
+        final List<NestedMapInfo> nestedMapInfos = getNestedMapInfos(physicalType);
+        if (!nestedMapInfos.isEmpty()) {
+            final String[] nestedRowFieldNames =
+                    nestedMapInfos.stream()
+                            .map(r -> fieldNames[r.getFieldIdx()])
+                            .toArray(String[]::new);
+
+            final int[] nestedRowFieldIndices =
+                    nestedMapInfos.stream().mapToInt(NestedMapInfo::getFieldIdx).toArray();
+
+            final Constraint[][] elementConstraints =
+                    nestedMapInfos.stream()
+                            .map(
+                                    r1 ->
+                                            createConstraints(
+                                                    new RowType(
+                                                            List.of(
+                                                                    new RowType.RowField(
+                                                                            "key", r1.getKeyType()),
+                                                                    new RowType.RowField(
+                                                                            "value",
+                                                                            r1.getValueType()))),
+                                                    notNullEnforcer,
+                                                    typeLengthEnforcer,
+                                                    nestedEnforcer))
+                            .toArray(Constraint[][]::new);
+
+            final ArrayData.ElementGetter[] keyGetters =
+                    nestedMapInfos.stream()
+                            .map(r -> ArrayData.createElementGetter(r.getKeyType()))
+                            .toArray(ArrayData.ElementGetter[]::new);
+
+            final ArrayData.ElementGetter[] valueGetters =
+                    nestedMapInfos.stream()
+                            .map(r -> ArrayData.createElementGetter(r.getValueType()))
+                            .toArray(ArrayData.ElementGetter[]::new);
+
+            constraints.add(
+                    new NestedMapConstraint(
+                            nestedRowFieldIndices,
+                            nestedRowFieldNames,
+                            elementConstraints,
+                            keyGetters,
+                            valueGetters));
+        }
+    }
+
+    private static void addLengthEnforcers(
+            final RowType physicalType,
+            final TypeLengthEnforcer typeLengthEnforcer,
+            final String[] fieldNames,
+            final List<Constraint> constraints) {
+        final List<FieldInfo> charFieldInfo =
+                getFieldInfoForLengthEnforcer(physicalType, LengthEnforcerKind.CHAR);
+        if (!charFieldInfo.isEmpty()) {
+            final String[] charFieldNames =
+                    charFieldInfo.stream()
+                            .map(cfi -> fieldNames[cfi.fieldIdx()])
+                            .toArray(String[]::new);
+
+            final int[] charFieldIndices =
+                    charFieldInfo.stream().mapToInt(FieldInfo::fieldIdx).toArray();
+
+            final int[] charFieldLengths =
+                    charFieldInfo.stream().mapToInt(FieldInfo::getLength).toArray();
+
+            constraints.add(
+                    new CharLengthConstraint(
+                            TypeLengthEnforcementStrategy.of(typeLengthEnforcer),
+                            charFieldIndices,
+                            charFieldLengths,
+                            charFieldNames,
+                            buildCouldPad(charFieldInfo)));
+        }
+
+        // Build BINARY/VARBINARY length enforcer
+        final List<FieldInfo> binaryFieldInfo =
+                getFieldInfoForLengthEnforcer(physicalType, LengthEnforcerKind.BINARY);
+        if (!binaryFieldInfo.isEmpty()) {
+            final String[] binaryFieldNames =
+                    binaryFieldInfo.stream()
+                            .map(cfi -> fieldNames[cfi.fieldIdx()])
+                            .toArray(String[]::new);
+
+            final int[] binaryFieldIndices =
+                    binaryFieldInfo.stream().mapToInt(FieldInfo::fieldIdx).toArray();
+
+            final int[] binaryFieldLengths =
+                    binaryFieldInfo.stream().mapToInt(FieldInfo::getLength).toArray();
+
+            constraints.add(
+                    new BinaryLengthConstraint(
+                            TypeLengthEnforcementStrategy.of(typeLengthEnforcer),
+                            binaryFieldIndices,
+                            binaryFieldLengths,
+                            binaryFieldNames,
+                            buildCouldPad(binaryFieldInfo)));
+        }
+    }
+
+    private static Optional<ConstraintEnforcerExecutor> create(final Constraint[] constraints) {
+        if (constraints.length == 0) {
+            return Optional.empty();
+        }
+        return Optional.of(new ConstraintEnforcerExecutor(constraints));
+    }
+
+    private static int[] getNotNullFieldIndices(final RowType physicalType) {
+        return IntStream.range(0, physicalType.getFieldCount())
+                .filter(pos -> !physicalType.getTypeAt(pos).isNullable())
+                .toArray();
+    }
+
+    private static List<NestedRowInfo> getNestedRowInfos(final RowType physicalType) {
+        return IntStream.range(0, physicalType.getFieldCount())
+                .boxed()
+                .flatMap(
+                        pos -> {
+                            final LogicalType nestedType = physicalType.getTypeAt(pos);
+                            if (nestedType.is(LogicalTypeRoot.ROW)) {
+                                final RowType rowType = (RowType) nestedType;
+                                return Stream.of(
+                                        new NestedRowInfo(pos, (rowType).getFieldCount(), rowType));
+                            } else if (nestedType.is(LogicalTypeRoot.STRUCTURED_TYPE)) {
+                                // for constraint extraction, convert the STRUCTURED_TYPE to a ROW
+                                final StructuredType structuredType = (StructuredType) nestedType;
+                                final List<StructuredType.StructuredAttribute> attributes =
+                                        structuredType.getAttributes();
+                                final List<RowType.RowField> fields =
+                                        attributes.stream()
+                                                .map(
+                                                        attr ->
+                                                                new RowType.RowField(
+                                                                        attr.getName(),
+                                                                        attr.getType()))
+                                                .collect(Collectors.toList());
+                                return Stream.of(
+                                        new NestedRowInfo(
+                                                pos,
+                                                attributes.size(),
+                                                new RowType(structuredType.isNullable(), fields)));
+                            } else {
+                                return Stream.empty();
+                            }
+                        })
+                .collect(Collectors.toList());
+    }
+
+    private static List<NestedArrayInfo> getNestedArrayInfos(final RowType physicalType) {
+        return IntStream.range(0, physicalType.getFieldCount())
+                .boxed()
+                .flatMap(
+                        pos -> {
+                            final LogicalType nestedType = physicalType.getTypeAt(pos);
+                            if (nestedType.is(LogicalTypeRoot.ARRAY)) {
+                                final ArrayType arrayType = (ArrayType) nestedType;
+                                final LogicalType elementType = arrayType.getElementType();
+
+                                return Stream.of(new NestedArrayInfo(pos, elementType));
+                            } else {
+                                return Stream.empty();
+                            }
+                        })
+                .collect(Collectors.toList());
+    }
+
+    private static List<NestedMapInfo> getNestedMapInfos(final RowType physicalType) {
+        return IntStream.range(0, physicalType.getFieldCount())
+                .boxed()
+                .flatMap(
+                        pos -> {
+                            final LogicalType nestedType = physicalType.getTypeAt(pos);
+                            if (nestedType.is(LogicalTypeRoot.MAP)) {
+                                final MapType mapType = (MapType) nestedType;
+
+                                return Stream.of(
+                                        new NestedMapInfo(
+                                                pos, mapType.getKeyType(), mapType.getValueType()));
+                            } else {
+                                return Stream.empty();
+                            }
+                        })
+                .collect(Collectors.toList());
+    }
+
+    /**
+     * Returns a List of {@link FieldInfo}, each containing the info needed to determine whether a
+     * string or binary value needs trimming and/or padding.
+     */
+    private static List<FieldInfo> getFieldInfoForLengthEnforcer(
+            final RowType physicalType, final LengthEnforcerKind enforcerType) {
+        LogicalTypeRoot staticType = null;
+        LogicalTypeRoot variableType = null;
+        int maxLength = 0;
+        switch (enforcerType) {
+            case CHAR:
+                staticType = LogicalTypeRoot.CHAR;
+                variableType = LogicalTypeRoot.VARCHAR;
+                maxLength = CharType.MAX_LENGTH;
+                break;
+            case BINARY:
+                staticType = LogicalTypeRoot.BINARY;
+                variableType = LogicalTypeRoot.VARBINARY;
+                maxLength = BinaryType.MAX_LENGTH;
+        }
+        final List<FieldInfo> fieldsAndLengths = new ArrayList<>();
+        for (int i = 0; i < physicalType.getFieldCount(); i++) {
+            LogicalType type = physicalType.getTypeAt(i);
+            boolean isStatic = type.is(staticType);
+            // Should trim and possibly pad
+            if ((isStatic && (LogicalTypeChecks.getLength(type) < maxLength))
+                    || (type.is(variableType) && (LogicalTypeChecks.getLength(type) < maxLength))) {
+                fieldsAndLengths.add(new FieldInfo(i, LogicalTypeChecks.getLength(type), isStatic));
+            } else if (isStatic) { // Should pad
+                fieldsAndLengths.add(new FieldInfo(i, -1, isStatic));
+            }
+        }
+        return fieldsAndLengths;
+    }
+
+    private static BitSet buildCouldPad(final List<FieldInfo> charFieldInfo) {
+        BitSet couldPad = new BitSet(charFieldInfo.size());
+        for (int i = 0; i < charFieldInfo.size(); i++) {
+            if (charFieldInfo.get(i).couldPad()) {
+                couldPad.set(i);
+            }
+        }
+        return couldPad;
+    }
+
+    public @Nullable RowData enforce(final RowData row) {
+        RowData enforcedRow = row;
+        for (Constraint constraint : constraints) {
+            enforcedRow = constraint.enforce(enforcedRow);
+            if (enforcedRow == null) {
+                return null; // drop row
+            }
+        }
+        return enforcedRow;
+    }
+
+    private static class NestedRowInfo {
+        private final int fieldIdx;
+        private final int arity;
+        private final RowType fieldType;
+
+        public NestedRowInfo(int fieldIdx, int arity, RowType fieldType) {
+            this.fieldIdx = fieldIdx;
+            this.arity = arity;
+            this.fieldType = fieldType;
+        }
+
+        public int getFieldIdx() {
+            return fieldIdx;
+        }
+
+        public int getArity() {
+            return arity;
+        }
+
+        public RowType getFieldType() {
+            return fieldType;
+        }
+    }
+
+    private static class NestedArrayInfo {
+        private final int fieldIdx;
+        private final LogicalType elementType;
+
+        private NestedArrayInfo(final int fieldIdx, final LogicalType elementType) {
+            this.fieldIdx = fieldIdx;
+            this.elementType = elementType;
+        }
+
+        public int getFieldIdx() {
+            return fieldIdx;
+        }
+
+        public LogicalType getElementType() {
+            return elementType;
+        }
+    }
+
+    private static class NestedMapInfo {
+        private final int fieldIdx;
+        private final LogicalType valueType;
+        private final LogicalType keyType;
+
+        private NestedMapInfo(final int fieldIdx, LogicalType keyType, LogicalType valueType) {
+            this.fieldIdx = fieldIdx;
+            this.valueType = valueType;
+            this.keyType = keyType;
+        }
+
+        public int getFieldIdx() {
+            return fieldIdx;
+        }
+
+        public LogicalType getValueType() {
+            return valueType;
+        }
+
+        public LogicalType getKeyType() {
+            return keyType;
+        }
+    }
+
+    /**
+     * Helper POJO to keep info about CHAR/VARCHAR/BINARY/VARBINARY fields, used to determine if
+     * trimming or padding is needed.
+     */
+    @Internal
+    private static class FieldInfo {
+        private final int fieldIdx;
+        private final int length;
+        private final boolean couldPad;
+
+        public FieldInfo(int fieldIdx, int length, boolean couldPad) {
+            this.fieldIdx = fieldIdx;
+            this.length = length;
+            this.couldPad = couldPad;
+        }
+
+        public int fieldIdx() {
+            return fieldIdx;
+        }
+
+        public boolean couldPad() {
+            return couldPad;
+        }
+
+        public int getLength() {
+            return length;
+        }
+    }
+
+    enum LengthEnforcerKind {
+        CHAR,
+        BINARY
+    }
+}
diff --git a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/EnforcerException.java b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/EnforcerException.java
new file mode 100644
index 00000000000..ecd60fbbcef
--- /dev/null
+++ b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/EnforcerException.java
@@ -0,0 +1,47 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.runtime.operators.sink.constraint;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.table.api.TableRuntimeException;
+
+/**
+ * Exception thrown when a constraint is violated. Specific exception so that we can construct a
+ * nested column path from {@link NestedRowConstraint}.
+ */
+@Internal
+public final class EnforcerException extends TableRuntimeException {
+
+    private final String columnName;
+    private final String format;
+
+    public EnforcerException(String format, String columnName) {
+        super(String.format(format, columnName));
+        this.columnName = columnName;
+        this.format = format;
+    }
+
+    public String getColumnName() {
+        return columnName;
+    }
+
+    public String getFormat() {
+        return format;
+    }
+}
diff --git a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/NestedArrayConstraint.java b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/NestedArrayConstraint.java
new file mode 100644
index 00000000000..2d2eced91ff
--- /dev/null
+++ b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/NestedArrayConstraint.java
@@ -0,0 +1,102 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.runtime.operators.sink.constraint;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.table.data.ArrayData;
+import org.apache.flink.table.data.GenericRowData;
+import org.apache.flink.table.data.RowData;
+import org.apache.flink.util.StringUtils;
+
+import javax.annotation.Nullable;
+
+import java.util.Arrays;
+import java.util.stream.Collectors;
+import java.util.stream.IntStream;
+
+/** Checks constraints on nested arrays. */
+@Internal
+final class NestedArrayConstraint implements Constraint {
+
+    private final int[] nestedArrayFieldIndices;
+    private final String[] nestedArrayFieldNames;
+    private final Constraint[][] nestedElementsConstraints;
+    private final ArrayData.ElementGetter[] elementGetters;
+
+    NestedArrayConstraint(
+            final int[] nestedArrayFieldIndices,
+            final String[] nestedArrayFieldNames,
+            final Constraint[][] nestedElementsConstraints,
+            final ArrayData.ElementGetter[] elementGetters) {
+        this.nestedArrayFieldIndices = nestedArrayFieldIndices;
+        this.nestedArrayFieldNames = nestedArrayFieldNames;
+        this.nestedElementsConstraints = nestedElementsConstraints;
+        this.elementGetters = elementGetters;
+    }
+
+    @Nullable
+    @Override
+    public RowData enforce(RowData input) {
+        for (int i = 0; i < nestedArrayFieldIndices.length; i++) {
+            final int index = nestedArrayFieldIndices[i];
+            if (!input.isNullAt(index)) {
+                ArrayData nestedArray = input.getArray(index);
+                final Constraint[] nestedConstraints = nestedElementsConstraints[i];
+                final ArrayData.ElementGetter elementGetter = elementGetters[i];
+                for (int entryIdx = 0; entryIdx < nestedArray.size(); entryIdx++) {
+                    final Object element = elementGetter.getElementOrNull(nestedArray, entryIdx);
+                    for (Constraint nestedConstraint : nestedConstraints) {
+                        if (enforce(nestedConstraint, element, i, entryIdx) == null) {
+                            // the record is invalid
+                            return null;
+                        }
+                    }
+                }
+            }
+        }
+        return input;
+    }
+
+    private RowData enforce(Constraint nestedConstraint, Object element, int i, int entryIdx) {
+        try {
+            return nestedConstraint.enforce(GenericRowData.of(element));
+        } catch (EnforcerException e) {
+            final String nestedColumnName = e.getColumnName().replace("element", "");
+            String columnName = String.format("%s[%d]", nestedArrayFieldNames[i], entryIdx);
+            if (!StringUtils.isNullOrWhitespaceOnly(nestedColumnName)) {
+                columnName += nestedColumnName;
+            }
+            throw new EnforcerException(e.getFormat(), columnName);
+        }
+    }
+
+    @Override
+    public String toString() {
+        return String.format(
+                "NestedArrayEnforcer(constraints=[%s])",
+                IntStream.range(0, nestedArrayFieldIndices.length)
+                        .mapToObj(
+                                idx ->
+                                        String.format(
+                                                "{%s=%s}",
+                                                nestedArrayFieldNames[idx],
+                                                Arrays.toString(nestedElementsConstraints[idx])))
+                        .collect(Collectors.joining(", ")));
+    }
+}
diff --git a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/NestedMapConstraint.java b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/NestedMapConstraint.java
new file mode 100644
index 00000000000..73d5592238a
--- /dev/null
+++ b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/NestedMapConstraint.java
@@ -0,0 +1,121 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.runtime.operators.sink.constraint;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.table.data.ArrayData;
+import org.apache.flink.table.data.GenericRowData;
+import org.apache.flink.table.data.MapData;
+import org.apache.flink.table.data.RowData;
+
+import javax.annotation.Nullable;
+
+import java.util.Arrays;
+import java.util.stream.Collectors;
+import java.util.stream.IntStream;
+
+/** Checks constraints on nested maps. */
+@Internal
+final class NestedMapConstraint implements Constraint {
+
+    private final int[] nestedMapFieldIndices;
+    private final String[] nestedMapFieldNames;
+    private final Constraint[][] nestedElementsConstraints;
+    private final ArrayData.ElementGetter[] valueGetters;
+    private final ArrayData.ElementGetter[] keyGetters;
+
+    NestedMapConstraint(
+            final int[] nestedMapFieldIndices,
+            final String[] nestedMapFieldNames,
+            final Constraint[][] nestedElementsConstraints,
+            final ArrayData.ElementGetter[] valueGetters,
+            final ArrayData.ElementGetter[] keyGetters) {
+        this.nestedMapFieldIndices = nestedMapFieldIndices;
+        this.nestedMapFieldNames = nestedMapFieldNames;
+        this.nestedElementsConstraints = nestedElementsConstraints;
+        this.valueGetters = valueGetters;
+        this.keyGetters = keyGetters;
+    }
+
+    @Nullable
+    @Override
+    public RowData enforce(RowData input) {
+        for (int i = 0; i < nestedMapFieldIndices.length; i++) {
+            final int index = nestedMapFieldIndices[i];
+            if (!input.isNullAt(index)) {
+                MapData nestedMap = input.getMap(index);
+                final Constraint[] nestedConstraints = nestedElementsConstraints[i];
+                final ArrayData.ElementGetter keyGetter = keyGetters[i];
+                final ArrayData.ElementGetter valueGetter = valueGetters[i];
+                final ArrayData keys = nestedMap.keyArray();
+                final ArrayData values = nestedMap.valueArray();
+                for (int entryIdx = 0; entryIdx < nestedMap.size(); entryIdx++) {
+                    final Object key = keyGetter.getElementOrNull(keys, entryIdx);
+                    final Object value = valueGetter.getElementOrNull(values, entryIdx);
+                    for (Constraint nestedConstraint : nestedConstraints) {
+                        if (enforce(nestedConstraint, key, value, i) == null) {
+                            return null;
+                        }
+                    }
+                }
+            }
+        }
+        return input;
+    }
+
+    private RowData enforce(Constraint nestedConstraint, Object key, Object value, int i) {
+        try {
+            return nestedConstraint.enforce(GenericRowData.of(key, value));
+        } catch (EnforcerException e) {
+            final String adjustedNestedColumnName = getAdjustedNestedColumnName(e);
+            throw new EnforcerException(
+                    e.getFormat(),
+                    String.format("%s[%s", nestedMapFieldNames[i], adjustedNestedColumnName));
+        }
+    }
+
+    private static String getAdjustedNestedColumnName(EnforcerException e) {
+        final String adjustedNestedColumnName;
+        if ("key".equals(e.getColumnName()) || "value".equals(e.getColumnName())) {
+            adjustedNestedColumnName = e.getColumnName() + "]";
+        } else {
+            adjustedNestedColumnName =
+                    e.getColumnName()
+                            // the column will start with either "key." or "value."
+                            // replace so that we
+                            // get <column_name>[key/value].<nested_column_name>
+                            .replaceFirst("\\.", "]");
+        }
+        return adjustedNestedColumnName;
+    }
+
+    @Override
+    public String toString() {
+        return String.format(
+                "NestedMapEnforcer(constraints=[%s])",
+                IntStream.range(0, nestedMapFieldIndices.length)
+                        .mapToObj(
+                                idx ->
+                                        String.format(
+                                                "{%s==%s}",
+                                                nestedMapFieldNames[idx],
+                                                Arrays.toString(nestedElementsConstraints[idx])))
+                        .collect(Collectors.joining(", ")));
+    }
+}
diff --git a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/NestedRowConstraint.java b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/NestedRowConstraint.java
new file mode 100644
index 00000000000..e91c37873b1
--- /dev/null
+++ b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/NestedRowConstraint.java
@@ -0,0 +1,99 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.runtime.operators.sink.constraint;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.table.data.RowData;
+import org.apache.flink.table.data.UpdatableRowData;
+
+import javax.annotation.Nullable;
+
+import java.util.Arrays;
+import java.util.stream.Collectors;
+import java.util.stream.IntStream;
+
+/** Checks constraints on nested rows and structured types. */
+@Internal
+final class NestedRowConstraint implements Constraint {
+
+    private final int[] nestedRowFieldIndices;
+    private final int[] nestedRowFieldArities;
+    private final String[] nestedRowFieldNames;
+    private final Constraint[][] nestedRowConstraints;
+
+    NestedRowConstraint(
+            int[] nestedRowFieldIndices,
+            int[] nestedRowFieldArities,
+            String[] nestedRowFieldNames,
+            Constraint[][] nestedRowConstraints) {
+        this.nestedRowFieldIndices = nestedRowFieldIndices;
+        this.nestedRowFieldArities = nestedRowFieldArities;
+        this.nestedRowFieldNames = nestedRowFieldNames;
+        this.nestedRowConstraints = nestedRowConstraints;
+    }
+
+    @Nullable
+    @Override
+    public RowData enforce(RowData input) {
+        UpdatableRowData updatableRowData = null;
+        for (int i = 0; i < nestedRowFieldIndices.length; i++) {
+            final int index = nestedRowFieldIndices[i];
+            if (!input.isNullAt(index)) {
+                RowData nestedRow = input.getRow(index, nestedRowFieldArities[i]);
+                for (Constraint constraint : nestedRowConstraints[i]) {
+                    RowData enforcedRow = enforce(constraint, nestedRow, i);
+                    if (enforcedRow == null) {
+                        return null;
+                    }
+                    if (enforcedRow != nestedRow) {
+                        if (updatableRowData == null) {
+                            updatableRowData = new UpdatableRowData(input, input.getArity());
+                        }
+                        updatableRowData.setField(index, enforcedRow);
+                        nestedRow = enforcedRow;
+                    }
+                }
+            }
+        }
+        return updatableRowData != null ? updatableRowData : input;
+    }
+
+    private RowData enforce(Constraint constraint, RowData nestedRow, int index) {
+        try {
+            return constraint.enforce(nestedRow);
+        } catch (EnforcerException e) {
+            throw new EnforcerException(
+                    e.getFormat(), nestedRowFieldNames[index] + "." + e.getColumnName());
+        }
+    }
+
+    @Override
+    public String toString() {
+        return String.format(
+                "NestedRowEnforcer(constraints=[%s])",
+                IntStream.range(0, nestedRowFieldIndices.length)
+                        .mapToObj(
+                                idx ->
+                                        String.format(
+                                                "{%s=%s}",
+                                                nestedRowFieldNames[idx],
+                                                Arrays.toString(nestedRowConstraints[idx])))
+                        .collect(Collectors.joining(", ")));
+    }
+}
diff --git a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/NotNullConstraint.java b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/NotNullConstraint.java
new file mode 100644
index 00000000000..f2ae509c5b5
--- /dev/null
+++ b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/NotNullConstraint.java
@@ -0,0 +1,73 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.runtime.operators.sink.constraint;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.table.api.config.ExecutionConfigOptions;
+import org.apache.flink.table.data.RowData;
+
+import javax.annotation.Nullable;
+
+import static org.apache.flink.table.api.config.ExecutionConfigOptions.TABLE_EXEC_SINK_NOT_NULL_ENFORCER;
+
+/** Enforces NOT NULL constraints on the input {@link RowData}. */
+@Internal
+final class NotNullConstraint implements Constraint {
+    private final NotNullEnforcementStrategy enforcementStrategy;
+    private final int[] notNullFieldIndices;
+    private final String[] notNullFieldNames;
+
+    NotNullConstraint(
+            NotNullEnforcementStrategy enforcementStrategy,
+            int[] notNullFieldIndices,
+            String[] notNullFieldNames) {
+        this.enforcementStrategy = enforcementStrategy;
+        this.notNullFieldIndices = notNullFieldIndices;
+        this.notNullFieldNames = notNullFieldNames;
+    }
+
+    @Nullable
+    @Override
+    public RowData enforce(RowData input) {
+        for (int i = 0; i < notNullFieldIndices.length; i++) {
+            final int index = notNullFieldIndices[i];
+            if (input.isNullAt(index)) {
+                switch (enforcementStrategy) {
+                    case ERROR:
+                        throw new EnforcerException(
+                                "Column '%s' is NOT NULL, however, a null value is being written into it. "
+                                        + String.format(
+                                                "You can set job configuration '%s'='%s' "
+                                                        + "to suppress this exception and drop such records silently.",
+                                                TABLE_EXEC_SINK_NOT_NULL_ENFORCER.key(),
+                                                ExecutionConfigOptions.NotNullEnforcer.DROP.name()),
+                                notNullFieldNames[i]);
+                    case DROP:
+                        return null;
+                }
+            }
+        }
+        return input;
+    }
+
+    @Override
+    public String toString() {
+        return String.format("NotNullEnforcer(fields=[%s])", String.join(", ", notNullFieldNames));
+    }
+}
diff --git a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/NotNullEnforcementStrategy.java b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/NotNullEnforcementStrategy.java
new file mode 100644
index 00000000000..63a5a70ec37
--- /dev/null
+++ b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/NotNullEnforcementStrategy.java
@@ -0,0 +1,42 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.runtime.operators.sink.constraint;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.table.api.config.ExecutionConfigOptions;
+
+import java.io.Serializable;
+
+/** Keep in sync with {@link ExecutionConfigOptions.NotNullEnforcer}. */
+@Internal
+enum NotNullEnforcementStrategy implements Serializable {
+    ERROR,
+    DROP;
+
+    public static NotNullEnforcementStrategy of(ExecutionConfigOptions.NotNullEnforcer option) {
+        switch (option) {
+            case ERROR:
+                return ERROR;
+            case DROP:
+                return DROP;
+            default:
+                throw new IllegalArgumentException("Unknown type length enforcer: " + option);
+        }
+    }
+}
diff --git a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/TypeLengthEnforcementStrategy.java b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/TypeLengthEnforcementStrategy.java
new file mode 100644
index 00000000000..34ae2e4c272
--- /dev/null
+++ b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sink/constraint/TypeLengthEnforcementStrategy.java
@@ -0,0 +1,45 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.runtime.operators.sink.constraint;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.table.api.config.ExecutionConfigOptions;
+
+import java.io.Serializable;
+
+/** Keep in sync with {@link ExecutionConfigOptions.TypeLengthEnforcer}. */
+@Internal
+enum TypeLengthEnforcementStrategy implements Serializable {
+    TRIM_PAD,
+    THROW;
+
+    public static TypeLengthEnforcementStrategy of(
+            ExecutionConfigOptions.TypeLengthEnforcer option) {
+        switch (option) {
+            case TRIM_PAD:
+                return TRIM_PAD;
+            case ERROR:
+                return THROW;
+            case IGNORE:
+            // We should not create a constraint for this case.
+            default:
+                throw new IllegalArgumentException("Unknown type length enforcer: " + option);
+        }
+    }
+}
