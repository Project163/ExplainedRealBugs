diff --git a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/TableEnvHiveConnectorITCase.java b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/TableEnvHiveConnectorITCase.java
index 8cdf001c46c..ea7a596534a 100644
--- a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/TableEnvHiveConnectorITCase.java
+++ b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/TableEnvHiveConnectorITCase.java
@@ -130,10 +130,7 @@ public class TableEnvHiveConnectorITCase {
 	public void testDifferentFormats() throws Exception {
 		String[] formats = new String[]{"orc", "parquet", "sequencefile", "csv", "avro"};
 		for (String format : formats) {
-			if (format.equals("orc") && HiveShimLoader.getHiveVersion().startsWith("2.0")) {
-				// Ignore orc test for Hive version 2.0.x for now due to FLINK-13998
-				continue;
-			} else if (format.equals("avro") && !HiveVersionTestUtil.HIVE_110_OR_LATER) {
+			if (format.equals("avro") && !HiveVersionTestUtil.HIVE_110_OR_LATER) {
 				// timestamp is not supported for avro tables before 1.1.0
 				continue;
 			}
@@ -190,11 +187,14 @@ public class TableEnvHiveConnectorITCase {
 
 		verifyFlinkQueryResult(tableEnv.sqlQuery("select * from db1.src"), expected);
 
-		// populate dest table with source table
-		TableEnvUtil.execInsertSqlAndWaitResult(tableEnv, "insert into db1.dest select * from db1.src");
+		// Ignore orc write test for Hive version 2.0.x for now due to FLINK-13998
+		if (!format.equals("orc") || !HiveShimLoader.getHiveVersion().startsWith("2.0")) {
+			// populate dest table with source table
+			TableEnvUtil.execInsertSqlAndWaitResult(tableEnv, "insert into db1.dest select * from db1.src");
 
-		// verify data on hive side
-		verifyHiveQueryResult("select * from db1.dest", expected);
+			// verify data on hive side
+			verifyHiveQueryResult("select * from db1.dest", expected);
+		}
 
 		tableEnv.executeSql("drop database db1 cascade");
 	}
diff --git a/flink-formats/flink-orc/src/main/java/org/apache/flink/orc/OrcSplitReaderUtil.java b/flink-formats/flink-orc/src/main/java/org/apache/flink/orc/OrcSplitReaderUtil.java
index 7f1cf034f76..30bc96c24fa 100644
--- a/flink-formats/flink-orc/src/main/java/org/apache/flink/orc/OrcSplitReaderUtil.java
+++ b/flink-formats/flink-orc/src/main/java/org/apache/flink/orc/OrcSplitReaderUtil.java
@@ -80,7 +80,7 @@ public class OrcSplitReaderUtil {
 				LogicalType type = fullFieldTypes[selectedFields[i]].getLogicalType();
 				vectors[i] = partitionSpec.containsKey(name) ?
 						createFlinkVectorFromConstant(type, partitionSpec.get(name), batchSize) :
-						createFlinkVector(rowBatch.cols[nonPartNames.indexOf(name)]);
+						createFlinkVector(rowBatch.cols[nonPartNames.indexOf(name)], type);
 			}
 			return new VectorizedColumnBatch(vectors);
 		};
diff --git a/flink-formats/flink-orc/src/main/java/org/apache/flink/orc/TimestampUtil.java b/flink-formats/flink-orc/src/main/java/org/apache/flink/orc/TimestampUtil.java
new file mode 100644
index 00000000000..f338e8ce34a
--- /dev/null
+++ b/flink-formats/flink-orc/src/main/java/org/apache/flink/orc/TimestampUtil.java
@@ -0,0 +1,62 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.orc;
+
+import org.apache.flink.orc.vector.OrcLegacyTimestampColumnVector;
+import org.apache.flink.orc.vector.OrcTimestampColumnVector;
+
+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * Util class to handle timestamp vectors. It's responsible for deciding whether new or legacy (Hive 2.0.x) timestamp
+ * column vector should be used.
+ */
+public class TimestampUtil {
+
+	private TimestampUtil() {
+	}
+
+	private static final Logger LOG = LoggerFactory.getLogger(OrcLegacyTimestampColumnVector.class);
+
+	private static Class hiveTSColVectorClz = null;
+
+	static {
+		try {
+			hiveTSColVectorClz = Class.forName("org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector");
+		} catch (ClassNotFoundException e) {
+			LOG.debug("Hive TimestampColumnVector not available", e);
+		}
+	}
+
+	// whether a ColumnVector is the new TimestampColumnVector
+	public static boolean isHiveTimestampColumnVector(ColumnVector vector) {
+		return hiveTSColVectorClz != null && hiveTSColVectorClz.isAssignableFrom(vector.getClass());
+	}
+
+	// creates a Hive ColumnVector of constant timestamp value
+	public static ColumnVector createVectorFromConstant(int batchSize, Object value) {
+		if (hiveTSColVectorClz != null) {
+			return OrcTimestampColumnVector.createFromConstant(batchSize, value);
+		} else {
+			return OrcLegacyTimestampColumnVector.createFromConstant(batchSize, value);
+		}
+	}
+}
diff --git a/flink-formats/flink-orc/src/main/java/org/apache/flink/orc/vector/AbstractOrcColumnVector.java b/flink-formats/flink-orc/src/main/java/org/apache/flink/orc/vector/AbstractOrcColumnVector.java
index d12ed2ebd31..1b9b8cc086b 100644
--- a/flink-formats/flink-orc/src/main/java/org/apache/flink/orc/vector/AbstractOrcColumnVector.java
+++ b/flink-formats/flink-orc/src/main/java/org/apache/flink/orc/vector/AbstractOrcColumnVector.java
@@ -18,8 +18,10 @@
 
 package org.apache.flink.orc.vector;
 
+import org.apache.flink.orc.TimestampUtil;
 import org.apache.flink.table.types.logical.DecimalType;
 import org.apache.flink.table.types.logical.LogicalType;
+import org.apache.flink.table.types.logical.LogicalTypeRoot;
 
 import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
@@ -27,14 +29,11 @@ import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
 
 import java.math.BigDecimal;
 import java.nio.charset.StandardCharsets;
 import java.sql.Date;
-import java.sql.Timestamp;
 import java.time.LocalDate;
-import java.time.LocalDateTime;
 
 import static org.apache.flink.table.runtime.functions.SqlDateTimeUtils.dateToInternal;
 
@@ -56,17 +55,21 @@ public abstract class AbstractOrcColumnVector implements
 	}
 
 	public static org.apache.flink.table.data.vector.ColumnVector createFlinkVector(
-			ColumnVector vector) {
+			ColumnVector vector, LogicalType logicalType) {
 		if (vector instanceof LongColumnVector) {
-			return new OrcLongColumnVector((LongColumnVector) vector);
+			if (logicalType.getTypeRoot() == LogicalTypeRoot.TIMESTAMP_WITHOUT_TIME_ZONE) {
+				return new OrcLegacyTimestampColumnVector((LongColumnVector) vector);
+			} else {
+				return new OrcLongColumnVector((LongColumnVector) vector);
+			}
 		} else if (vector instanceof DoubleColumnVector) {
 			return new OrcDoubleColumnVector((DoubleColumnVector) vector);
 		} else if (vector instanceof BytesColumnVector) {
 			return new OrcBytesColumnVector((BytesColumnVector) vector);
 		} else if (vector instanceof DecimalColumnVector) {
 			return new OrcDecimalColumnVector((DecimalColumnVector) vector);
-		} else if (vector instanceof TimestampColumnVector) {
-			return new OrcTimestampColumnVector((TimestampColumnVector) vector);
+		} else if (TimestampUtil.isHiveTimestampColumnVector(vector)) {
+			return new OrcTimestampColumnVector(vector);
 		} else {
 			throw new UnsupportedOperationException("Unsupport vector: " + vector.getClass().getName());
 		}
@@ -77,7 +80,7 @@ public abstract class AbstractOrcColumnVector implements
 	 */
 	public static org.apache.flink.table.data.vector.ColumnVector createFlinkVectorFromConstant(
 			LogicalType type, Object value, int batchSize) {
-		return createFlinkVector(createHiveVectorFromConstant(type, value, batchSize));
+		return createFlinkVector(createHiveVectorFromConstant(type, value, batchSize), type);
 	}
 
 	/**
@@ -112,7 +115,7 @@ public abstract class AbstractOrcColumnVector implements
 				}
 				return createLongVector(batchSize, dateToInternal((Date) value));
 			case TIMESTAMP_WITHOUT_TIME_ZONE:
-				return createTimestampVector(batchSize, value);
+				return TimestampUtil.createVectorFromConstant(batchSize, value);
 			default:
 				throw new UnsupportedOperationException("Unsupported type: " + type);
 		}
@@ -176,19 +179,4 @@ public abstract class AbstractOrcColumnVector implements
 		}
 		return dcv;
 	}
-
-	private static TimestampColumnVector createTimestampVector(int batchSize, Object value) {
-		TimestampColumnVector lcv = new TimestampColumnVector(batchSize);
-		if (value == null) {
-			lcv.noNulls = false;
-			lcv.isNull[0] = true;
-			lcv.isRepeating = true;
-		} else {
-			Timestamp timestamp = value instanceof LocalDateTime ?
-				Timestamp.valueOf((LocalDateTime) value) : (Timestamp) value;
-			lcv.fill(timestamp);
-			lcv.isNull[0] = false;
-		}
-		return lcv;
-	}
 }
diff --git a/flink-formats/flink-orc/src/main/java/org/apache/flink/orc/vector/OrcLegacyTimestampColumnVector.java b/flink-formats/flink-orc/src/main/java/org/apache/flink/orc/vector/OrcLegacyTimestampColumnVector.java
new file mode 100644
index 00000000000..05c268fbfd9
--- /dev/null
+++ b/flink-formats/flink-orc/src/main/java/org/apache/flink/orc/vector/OrcLegacyTimestampColumnVector.java
@@ -0,0 +1,84 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.orc.vector;
+
+import org.apache.flink.table.data.TimestampData;
+
+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
+
+import java.sql.Timestamp;
+import java.time.LocalDateTime;
+
+/**
+ * This class is used to adapt to Hive's legacy (2.0.x) timestamp column vector which is a LongColumnVector.
+ */
+public class OrcLegacyTimestampColumnVector extends AbstractOrcColumnVector implements
+		org.apache.flink.table.data.vector.TimestampColumnVector {
+
+	private final LongColumnVector hiveVector;
+
+	OrcLegacyTimestampColumnVector(LongColumnVector vector) {
+		super(vector);
+		this.hiveVector = vector;
+	}
+
+	@Override
+	public TimestampData getTimestamp(int i, int precision) {
+		int index = hiveVector.isRepeating ? 0 : i;
+		Timestamp timestamp = toTimestamp(hiveVector.vector[index]);
+		return TimestampData.fromTimestamp(timestamp);
+	}
+
+	// creates a Hive ColumnVector of constant timestamp value
+	public static ColumnVector createFromConstant(int batchSize, Object value) {
+		LongColumnVector res = new LongColumnVector(batchSize);
+		if (value == null) {
+			res.noNulls = false;
+			res.isNull[0] = true;
+			res.isRepeating = true;
+		} else {
+			Timestamp timestamp = value instanceof LocalDateTime ?
+					Timestamp.valueOf((LocalDateTime) value) : (Timestamp) value;
+			res.fill(fromTimestamp(timestamp));
+			res.isNull[0] = false;
+		}
+		return res;
+	}
+
+	// converting from/to Timestamp is copied from Hive 2.0.0 TimestampUtils
+	private static long fromTimestamp(Timestamp timestamp) {
+		long time = timestamp.getTime();
+		int nanos = timestamp.getNanos();
+		return (time * 1000000) + (nanos % 1000000);
+	}
+
+	private static Timestamp toTimestamp(long timeInNanoSec) {
+		long integralSecInMillis = (timeInNanoSec / 1000000000) * 1000; // Full seconds converted to millis.
+		long nanos = timeInNanoSec % 1000000000; // The nanoseconds.
+		if (nanos < 0) {
+			nanos = 1000000000 + nanos; // The positive nano-part that will be added to milliseconds.
+			integralSecInMillis = ((timeInNanoSec / 1000000000) - 1) * 1000; // Reduce by one second.
+		}
+		Timestamp res = new Timestamp(0);
+		res.setTime(integralSecInMillis);
+		res.setNanos((int) nanos);
+		return res;
+	}
+}
diff --git a/flink-formats/flink-orc/src/main/java/org/apache/flink/orc/vector/OrcTimestampColumnVector.java b/flink-formats/flink-orc/src/main/java/org/apache/flink/orc/vector/OrcTimestampColumnVector.java
index 40ecc09ac33..f87784507f8 100644
--- a/flink-formats/flink-orc/src/main/java/org/apache/flink/orc/vector/OrcTimestampColumnVector.java
+++ b/flink-formats/flink-orc/src/main/java/org/apache/flink/orc/vector/OrcTimestampColumnVector.java
@@ -20,9 +20,11 @@ package org.apache.flink.orc.vector;
 
 import org.apache.flink.table.data.TimestampData;
 
+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
 
 import java.sql.Timestamp;
+import java.time.LocalDateTime;
 
 /**
  * This column vector is used to adapt hive's TimestampColumnVector to
@@ -33,9 +35,9 @@ public class OrcTimestampColumnVector extends AbstractOrcColumnVector implements
 
 	private TimestampColumnVector vector;
 
-	public OrcTimestampColumnVector(TimestampColumnVector vector) {
+	public OrcTimestampColumnVector(ColumnVector vector) {
 		super(vector);
-		this.vector = vector;
+		this.vector = (TimestampColumnVector) vector;
 	}
 
 	@Override
@@ -45,4 +47,19 @@ public class OrcTimestampColumnVector extends AbstractOrcColumnVector implements
 		timestamp.setNanos(vector.nanos[index]);
 		return TimestampData.fromTimestamp(timestamp);
 	}
+
+	public static ColumnVector createFromConstant(int batchSize, Object value) {
+		TimestampColumnVector res = new TimestampColumnVector(batchSize);
+		if (value == null) {
+			res.noNulls = false;
+			res.isNull[0] = true;
+			res.isRepeating = true;
+		} else {
+			Timestamp timestamp = value instanceof LocalDateTime ?
+					Timestamp.valueOf((LocalDateTime) value) : (Timestamp) value;
+			res.fill(timestamp);
+			res.isNull[0] = false;
+		}
+		return res;
+	}
 }
