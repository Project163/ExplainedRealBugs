diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/api/TableEnvironment.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/api/TableEnvironment.scala
index ed750b890d3..48e33ccb441 100644
--- a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/api/TableEnvironment.scala
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/api/TableEnvironment.scala
@@ -358,20 +358,32 @@ abstract class TableEnvironment(val config: TableConfig) {
     * Registers an [[AggregateFunction]] under a unique name. Replaces already existing
     * user-defined functions under this name.
     */
-  private[flink] def registerAggregateFunctionInternal[T: TypeInformation, ACC](
+  private[flink] def registerAggregateFunctionInternal[T: TypeInformation, ACC: TypeInformation](
       name: String, function: AggregateFunction[T, ACC]): Unit = {
     // check if class not Scala object
     checkNotSingleton(function.getClass)
     // check if class could be instantiated
     checkForInstantiation(function.getClass)
 
-    val typeInfo: TypeInformation[_] = implicitly[TypeInformation[T]]
+    val resultTypeInfo: TypeInformation[_] = getResultTypeOfAggregateFunction(
+      function,
+      implicitly[TypeInformation[T]])
+
+    val accTypeInfo: TypeInformation[_] = getAccumulatorTypeOfAggregateFunction(
+      function,
+      implicitly[TypeInformation[ACC]])
 
     // register in Table API
     functionCatalog.registerFunction(name, function.getClass)
 
     // register in SQL API
-    val sqlFunctions = createAggregateSqlFunction(name, function, typeInfo, typeFactory)
+    val sqlFunctions = createAggregateSqlFunction(
+      name,
+      function,
+      resultTypeInfo,
+      accTypeInfo,
+      typeFactory)
+
     functionCatalog.registerSqlFunction(sqlFunctions)
   }
 
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/api/java/BatchTableEnvironment.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/api/java/BatchTableEnvironment.scala
index 03fb77ee2b3..44910b6d4f8 100644
--- a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/api/java/BatchTableEnvironment.scala
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/api/java/BatchTableEnvironment.scala
@@ -196,6 +196,10 @@ class BatchTableEnvironment(
       .createTypeInfo(f, classOf[AggregateFunction[T, ACC]], f.getClass, 0)
       .asInstanceOf[TypeInformation[T]]
 
+    implicit val accTypeInfo: TypeInformation[ACC] = TypeExtractor
+      .createTypeInfo(f, classOf[AggregateFunction[T, ACC]], f.getClass, 1)
+      .asInstanceOf[TypeInformation[ACC]]
+
     registerAggregateFunctionInternal[T, ACC](name, f)
   }
 }
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/api/java/StreamTableEnvironment.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/api/java/StreamTableEnvironment.scala
index be94df912f8..2da08faa12e 100644
--- a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/api/java/StreamTableEnvironment.scala
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/api/java/StreamTableEnvironment.scala
@@ -482,6 +482,10 @@ class StreamTableEnvironment(
       .createTypeInfo(f, classOf[AggregateFunction[T, ACC]], f.getClass, 0)
       .asInstanceOf[TypeInformation[T]]
 
+    implicit val accTypeInfo: TypeInformation[ACC] = TypeExtractor
+      .createTypeInfo(f, classOf[AggregateFunction[T, ACC]], f.getClass, 1)
+      .asInstanceOf[TypeInformation[ACC]]
+
     registerAggregateFunctionInternal[T, ACC](name, f)
   }
 }
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/api/scala/BatchTableEnvironment.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/api/scala/BatchTableEnvironment.scala
index 0dd7ca06095..5def9f2abbb 100644
--- a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/api/scala/BatchTableEnvironment.scala
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/api/scala/BatchTableEnvironment.scala
@@ -161,7 +161,7 @@ class BatchTableEnvironment(
     * @tparam T The type of the output value.
     * @tparam ACC The type of aggregate accumulator.
     */
-  def registerFunction[T: TypeInformation, ACC](
+  def registerFunction[T: TypeInformation, ACC: TypeInformation](
       name: String,
       f: AggregateFunction[T, ACC])
   : Unit = {
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/api/scala/StreamTableEnvironment.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/api/scala/StreamTableEnvironment.scala
index bfd443a576e..14ea84ae202 100644
--- a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/api/scala/StreamTableEnvironment.scala
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/api/scala/StreamTableEnvironment.scala
@@ -274,7 +274,7 @@ class StreamTableEnvironment(
     * @tparam T The type of the output value.
     * @tparam ACC The type of aggregate accumulator.
     */
-  def registerFunction[T: TypeInformation, ACC](
+  def registerFunction[T: TypeInformation, ACC: TypeInformation](
       name: String,
       f: AggregateFunction[T, ACC])
   : Unit = {
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/api/scala/expressionDsl.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/api/scala/expressionDsl.scala
index 9370c572d09..563dc31f2cd 100644
--- a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/api/scala/expressionDsl.scala
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/api/scala/expressionDsl.scala
@@ -792,7 +792,7 @@ trait ImplicitExpressionConversions {
   implicit def sqlTimestamp2Literal(sqlTimestamp: Timestamp): Expression =
     Literal(sqlTimestamp)
   implicit def array2ArrayConstructor(array: Array[_]): Expression = convertArray(array)
-  implicit def userDefinedAggFunctionConstructor[T: TypeInformation, ACC]
+  implicit def userDefinedAggFunctionConstructor[T: TypeInformation, ACC: TypeInformation]
       (udagg: AggregateFunction[T, ACC]): UDAGGExpression[T, ACC] = UDAGGExpression(udagg)
 }
 
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/expressions/UDAGGExpression.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/expressions/UDAGGExpression.scala
index c0e213df4cc..583f745b677 100644
--- a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/expressions/UDAGGExpression.scala
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/expressions/UDAGGExpression.scala
@@ -19,11 +19,13 @@ package org.apache.flink.table.expressions
 
 import org.apache.flink.api.common.typeinfo.TypeInformation
 import org.apache.flink.table.functions.AggregateFunction
+import org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.{getAccumulatorTypeOfAggregateFunction, getResultTypeOfAggregateFunction}
 
 /**
   * A class which creates a call to an aggregateFunction
   */
-case class UDAGGExpression[T: TypeInformation, ACC](aggregateFunction: AggregateFunction[T, ACC]) {
+case class UDAGGExpression[T: TypeInformation, ACC: TypeInformation](
+  aggregateFunction: AggregateFunction[T, ACC]) {
 
   /**
     * Creates a call to an [[AggregateFunction]].
@@ -31,6 +33,15 @@ case class UDAGGExpression[T: TypeInformation, ACC](aggregateFunction: Aggregate
     * @param params actual parameters of function
     * @return a [[AggFunctionCall]]
     */
-  def apply(params: Expression*): AggFunctionCall =
-    AggFunctionCall(aggregateFunction, params)
+  def apply(params: Expression*): AggFunctionCall = {
+    val resultTypeInfo: TypeInformation[_] = getResultTypeOfAggregateFunction(
+      aggregateFunction,
+      implicitly[TypeInformation[T]])
+
+    val accTypeInfo: TypeInformation[_] = getAccumulatorTypeOfAggregateFunction(
+      aggregateFunction,
+      implicitly[TypeInformation[ACC]])
+
+    AggFunctionCall(aggregateFunction, resultTypeInfo, accTypeInfo, params)
+  }
 }
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/expressions/aggregations.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/expressions/aggregations.scala
index 6d906b9ea2e..a9901a3d943 100644
--- a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/expressions/aggregations.scala
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/expressions/aggregations.scala
@@ -228,12 +228,14 @@ case class VarSamp(child: Expression) extends Aggregation {
 
 case class AggFunctionCall(
     aggregateFunction: AggregateFunction[_, _],
+    resultTypeInfo: TypeInformation[_],
+    accTypeInfo: TypeInformation[_],
     args: Seq[Expression])
   extends Aggregation {
 
   override private[flink] def children: Seq[Expression] = args
 
-  override def resultType: TypeInformation[_] = getResultTypeOfAggregateFunction(aggregateFunction)
+  override def resultType: TypeInformation[_] = resultTypeInfo
 
   override def validateInput(): ValidationResult = {
     val signature = children.map(_.resultType)
@@ -258,12 +260,13 @@ case class AggFunctionCall(
 
   override private[flink] def getSqlAggFunction()(implicit relBuilder: RelBuilder) = {
     val typeFactory = relBuilder.getTypeFactory.asInstanceOf[FlinkTypeFactory]
-    val sqlAgg = AggSqlFunction(aggregateFunction.getClass.getSimpleName,
-                   aggregateFunction,
-                   resultType,
-                   typeFactory,
-                   aggregateFunction.requiresOver)
-    sqlAgg
+    AggSqlFunction(
+      aggregateFunction.getClass.getSimpleName,
+      aggregateFunction,
+      resultType,
+      accTypeInfo,
+      typeFactory,
+      aggregateFunction.requiresOver)
   }
 
   override private[flink] def toRexNode(implicit relBuilder: RelBuilder): RexNode = {
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/functions/utils/AggSqlFunction.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/functions/utils/AggSqlFunction.scala
index be5501a9270..41977604ce0 100644
--- a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/functions/utils/AggSqlFunction.scala
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/functions/utils/AggSqlFunction.scala
@@ -37,12 +37,14 @@ import org.apache.flink.table.functions.utils.UserDefinedFunctionUtils._
   * @param name function name (used by SQL parser)
   * @param aggregateFunction aggregate function to be called
   * @param returnType the type information of returned value
+  * @param accType the type information of the accumulator
   * @param typeFactory type factory for converting Flink's between Calcite's types
   */
 class AggSqlFunction(
     name: String,
     aggregateFunction: AggregateFunction[_, _],
-    returnType: TypeInformation[_],
+    val returnType: TypeInformation[_],
+    val accType: TypeInformation[_],
     typeFactory: FlinkTypeFactory,
     requiresOver: Boolean)
   extends SqlUserDefinedAggFunction(
@@ -67,10 +69,11 @@ object AggSqlFunction {
       name: String,
       aggregateFunction: AggregateFunction[_, _],
       returnType: TypeInformation[_],
+      accType: TypeInformation[_],
       typeFactory: FlinkTypeFactory,
       requiresOver: Boolean): AggSqlFunction = {
 
-    new AggSqlFunction(name, aggregateFunction, returnType, typeFactory, requiresOver)
+    new AggSqlFunction(name, aggregateFunction, returnType, accType, typeFactory, requiresOver)
   }
 
   private[flink] def createOperandTypeInference(
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/functions/utils/UserDefinedFunctionUtils.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/functions/utils/UserDefinedFunctionUtils.scala
index 89b31f29a7e..5e34586ad17 100644
--- a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/functions/utils/UserDefinedFunctionUtils.scala
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/functions/utils/UserDefinedFunctionUtils.scala
@@ -286,13 +286,20 @@ object UserDefinedFunctionUtils {
   def createAggregateSqlFunction(
       name: String,
       aggFunction: AggregateFunction[_, _],
-      typeInfo: TypeInformation[_],
+      resultType: TypeInformation[_],
+      accTypeInfo: TypeInformation[_],
       typeFactory: FlinkTypeFactory)
   : SqlFunction = {
     //check if a qualified accumulate method exists before create Sql function
     checkAndExtractMethods(aggFunction, "accumulate")
-    val resultType: TypeInformation[_] = getResultTypeOfAggregateFunction(aggFunction, typeInfo)
-    AggSqlFunction(name, aggFunction, resultType, typeFactory, aggFunction.requiresOver)
+
+    AggSqlFunction(
+      name,
+      aggFunction,
+      resultType,
+      accTypeInfo,
+      typeFactory,
+      aggFunction.requiresOver)
   }
 
   // ----------------------------------------------------------------------------------------------
@@ -307,9 +314,29 @@ object UserDefinedFunctionUtils {
       aggregateFunction: AggregateFunction[_, _],
       extractedType: TypeInformation[_] = null)
     : TypeInformation[_] = {
+    getParameterTypeOfAggregateFunction(aggregateFunction, "getResultType", 0, extractedType)
+  }
+
+  /**
+    * Internal method of AggregateFunction#getAccumulatorType() that does some pre-checking
+    * and uses [[TypeExtractor]] as default return type inference.
+    */
+  def getAccumulatorTypeOfAggregateFunction(
+    aggregateFunction: AggregateFunction[_, _],
+    extractedType: TypeInformation[_] = null)
+  : TypeInformation[_] = {
+    getParameterTypeOfAggregateFunction(aggregateFunction, "getAccumulatorType", 1, extractedType)
+  }
+
+  private def getParameterTypeOfAggregateFunction(
+    aggregateFunction: AggregateFunction[_, _],
+    getTypeMethod: String,
+    parameterTypePos: Int,
+    extractedType: TypeInformation[_] = null)
+  : TypeInformation[_] = {
 
     val resultType = try {
-      val method: Method = aggregateFunction.getClass.getMethod("getResultType")
+      val method: Method = aggregateFunction.getClass.getMethod(getTypeMethod)
       method.invoke(aggregateFunction).asInstanceOf[TypeInformation[_]]
     } catch {
       case _: NoSuchMethodException => null
@@ -317,15 +344,23 @@ object UserDefinedFunctionUtils {
     }
     if (resultType != null) {
       resultType
-    } else if(extractedType != null) {
+    } else if (extractedType != null) {
       extractedType
     } else {
-      TypeExtractor
+      try {
+        TypeExtractor
         .createTypeInfo(aggregateFunction,
                         classOf[AggregateFunction[_, _]],
                         aggregateFunction.getClass,
-                        0)
+                        parameterTypePos)
         .asInstanceOf[TypeInformation[_]]
+      } catch {
+        case ite: InvalidTypesException =>
+          throw new TableException(
+            s"Cannot infer generic type of ${aggregateFunction.getClass}. " +
+              s"You can override AggregateFunction.$getTypeMethod() to specify the type.",
+            ite)
+      }
     }
   }
 
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/ProjectionTranslator.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/ProjectionTranslator.scala
index b3799d17826..dfb44b10617 100644
--- a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/ProjectionTranslator.scala
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/ProjectionTranslator.scala
@@ -316,28 +316,28 @@ object ProjectionTranslator {
       identifyFieldReferences(b.right, l)
 
     // Functions calls
-    case c @ Call(name, args) =>
+    case Call(_, args: Seq[Expression]) =>
       args.foldLeft(fieldReferences) {
         (fieldReferences, expr) => identifyFieldReferences(expr, fieldReferences)
       }
-    case sfc @ ScalarFunctionCall(clazz, args) =>
+    case ScalarFunctionCall(_, args: Seq[Expression]) =>
       args.foldLeft(fieldReferences) {
         (fieldReferences, expr) => identifyFieldReferences(expr, fieldReferences)
       }
 
-    case aggfc @ AggFunctionCall(clazz, args) =>
+    case AggFunctionCall(_, _, _, args) =>
       args.foldLeft(fieldReferences) {
         (fieldReferences, expr) => identifyFieldReferences(expr, fieldReferences)
       }
 
     // array constructor
-    case c @ ArrayConstructor(args) =>
+    case ArrayConstructor(args) =>
       args.foldLeft(fieldReferences) {
         (fieldReferences, expr) => identifyFieldReferences(expr, fieldReferences)
       }
 
     // ignore fields from window property
-    case w : WindowProperty =>
+    case _: WindowProperty =>
       fieldReferences
 
     // keep this case after all unwanted unary expressions
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/aggregate/AggregateUtil.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/aggregate/AggregateUtil.scala
index f4ead48b494..82f005155aa 100644
--- a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/aggregate/AggregateUtil.scala
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/aggregate/AggregateUtil.scala
@@ -17,7 +17,6 @@
  */
 package org.apache.flink.table.runtime.aggregate
 
-import java.lang.reflect.Method
 import java.util
 
 import org.apache.calcite.rel.`type`._
@@ -83,13 +82,13 @@ object AggregateUtil {
       isRowsClause: Boolean)
     : ProcessFunction[CRow, CRow] = {
 
-    val (aggFields, aggregates) =
+    val (aggFields, aggregates, accTypes) =
       transformToAggregateFunctions(
         namedAggregates.map(_.getKey),
         inputType,
         needRetraction = false)
 
-    val aggregationStateType: RowTypeInfo = createAccumulatorRowType(aggregates)
+    val aggregationStateType: RowTypeInfo = new RowTypeInfo(accTypes: _*)
 
     val forwardMapping = (0 until inputType.getFieldCount).toArray
     val aggMapping = aggregates.indices.map(x => x + inputType.getFieldCount).toArray
@@ -160,7 +159,7 @@ object AggregateUtil {
       generateRetraction: Boolean,
       consumeRetraction: Boolean): ProcessFunction[CRow, CRow] = {
 
-    val (aggFields, aggregates) =
+    val (aggFields, aggregates, accTypes) =
       transformToAggregateFunctions(
         namedAggregates.map(_.getKey),
         inputRowType,
@@ -170,7 +169,7 @@ object AggregateUtil {
 
     val outputArity = groupings.length + aggregates.length
 
-    val aggregationStateType: RowTypeInfo = createAccumulatorRowType(aggregates)
+    val aggregationStateType: RowTypeInfo = new RowTypeInfo(accTypes: _*)
 
     val genFunction = generator.generateAggregations(
       "NonWindowedAggregationHelper",
@@ -224,13 +223,13 @@ object AggregateUtil {
     : ProcessFunction[CRow, CRow] = {
 
     val needRetract = true
-    val (aggFields, aggregates) =
+    val (aggFields, aggregates, accTypes) =
       transformToAggregateFunctions(
         namedAggregates.map(_.getKey),
         inputType,
         needRetract)
 
-    val aggregationStateType: RowTypeInfo = createAccumulatorRowType(aggregates)
+    val aggregationStateType: RowTypeInfo = new RowTypeInfo(accTypes: _*)
     val inputRowType = CRowTypeInfo(inputTypeInfo)
 
     val forwardMapping = (0 until inputType.getFieldCount).toArray
@@ -323,7 +322,7 @@ object AggregateUtil {
   : MapFunction[Row, Row] = {
 
     val needRetract = false
-    val (aggFieldIndexes, aggregates) = transformToAggregateFunctions(
+    val (aggFieldIndexes, aggregates, accTypes) = transformToAggregateFunctions(
       namedAggregates.map(_.getKey),
       inputType,
       needRetract)
@@ -332,6 +331,7 @@ object AggregateUtil {
       createRowTypeForKeysAndAggregates(
         groupings,
         aggregates,
+        accTypes,
         inputType,
         Some(Array(BasicTypeInfo.LONG_TYPE_INFO)))
 
@@ -428,7 +428,7 @@ object AggregateUtil {
     : RichGroupReduceFunction[Row, Row] = {
 
     val needRetract = false
-    val (aggFieldIndexes, aggregates) = transformToAggregateFunctions(
+    val (aggFieldIndexes, aggregates, accTypes) = transformToAggregateFunctions(
       namedAggregates.map(_.getKey),
       physicalInputRowType,
       needRetract)
@@ -436,6 +436,7 @@ object AggregateUtil {
     val returnType: RowTypeInfo = createRowTypeForKeysAndAggregates(
       groupings,
       aggregates,
+      accTypes,
       physicalInputRowType,
       Some(Array(BasicTypeInfo.LONG_TYPE_INFO)))
 
@@ -541,7 +542,7 @@ object AggregateUtil {
     : RichGroupReduceFunction[Row, Row] = {
 
     val needRetract = false
-    val (aggFieldIndexes, aggregates) = transformToAggregateFunctions(
+    val (aggFieldIndexes, aggregates, _) = transformToAggregateFunctions(
       namedAggregates.map(_.getKey),
       physicalInputRowType,
       needRetract)
@@ -688,7 +689,7 @@ object AggregateUtil {
     groupings: Array[Int]): MapPartitionFunction[Row, Row] = {
 
     val needRetract = false
-    val (aggFieldIndexes, aggregates) = transformToAggregateFunctions(
+    val (aggFieldIndexes, aggregates, accTypes) = transformToAggregateFunctions(
       namedAggregates.map(_.getKey),
       physicalInputRowType,
       needRetract)
@@ -703,6 +704,7 @@ object AggregateUtil {
           createRowTypeForKeysAndAggregates(
             groupings,
             aggregates,
+            accTypes,
             physicalInputRowType,
             Option(Array(BasicTypeInfo.LONG_TYPE_INFO, BasicTypeInfo.LONG_TYPE_INFO)))
 
@@ -761,7 +763,7 @@ object AggregateUtil {
     : GroupCombineFunction[Row, Row] = {
 
     val needRetract = false
-    val (aggFieldIndexes, aggregates) = transformToAggregateFunctions(
+    val (aggFieldIndexes, aggregates, accTypes) = transformToAggregateFunctions(
       namedAggregates.map(_.getKey),
       physicalInputRowType,
       needRetract)
@@ -777,6 +779,7 @@ object AggregateUtil {
           createRowTypeForKeysAndAggregates(
             groupings,
             aggregates,
+            accTypes,
             physicalInputRowType,
             Option(Array(BasicTypeInfo.LONG_TYPE_INFO, BasicTypeInfo.LONG_TYPE_INFO)))
 
@@ -827,7 +830,7 @@ object AggregateUtil {
         RichGroupReduceFunction[Row, Row]) = {
 
     val needRetract = false
-    val (aggInFields, aggregates) = transformToAggregateFunctions(
+    val (aggInFields, aggregates, accTypes) = transformToAggregateFunctions(
       namedAggregates.map(_.getKey),
       inputType,
       needRetract)
@@ -858,7 +861,7 @@ object AggregateUtil {
       // compute preaggregation type
       val preAggFieldTypes = gkeyOutMapping.map(_._2)
         .map(inputType.getFieldList.get(_).getType)
-        .map(FlinkTypeFactory.toTypeInfo) ++ createAccumulatorType(aggregates)
+        .map(FlinkTypeFactory.toTypeInfo) ++ accTypes
       val preAggRowType = new RowTypeInfo(preAggFieldTypes: _*)
 
       val genPreAggFunction = generator.generateAggregations(
@@ -999,7 +1002,7 @@ object AggregateUtil {
     : (DataStreamAggFunction[CRow, Row, Row], RowTypeInfo, RowTypeInfo) = {
 
     val needRetract = false
-    val (aggFields, aggregates) =
+    val (aggFields, aggregates, accTypes) =
       transformToAggregateFunctions(
         namedAggregates.map(_.getKey),
         inputType,
@@ -1027,7 +1030,7 @@ object AggregateUtil {
 
     val aggResultTypes = namedAggregates.map(a => FlinkTypeFactory.toTypeInfo(a.left.getType))
 
-    val accumulatorRowType = createAccumulatorRowType(aggregates)
+    val accumulatorRowType = new RowTypeInfo(accTypes: _*)
     val aggResultRowType = new RowTypeInfo(aggResultTypes: _*)
     val aggFunction = new AggregateAggFunction(genFunction)
 
@@ -1158,11 +1161,12 @@ object AggregateUtil {
       aggregateCalls: Seq[AggregateCall],
       inputType: RelDataType,
       needRetraction: Boolean)
-  : (Array[Array[Int]], Array[TableAggregateFunction[_ <: Any, _ <: Any]]) = {
+  : (Array[Array[Int]], Array[TableAggregateFunction[_, _]], Array[TypeInformation[_]]) = {
 
     // store the aggregate fields of each aggregate function, by the same order of aggregates.
     val aggFieldIndexes = new Array[Array[Int]](aggregateCalls.size)
     val aggregates = new Array[TableAggregateFunction[_ <: Any, _ <: Any]](aggregateCalls.size)
+    val accTypes = new Array[TypeInformation[_]](aggregateCalls.size)
 
     // create aggregate function instances by function type and aggregate field data type.
     aggregateCalls.zipWithIndex.foreach { case (aggregateCall, index) =>
@@ -1388,50 +1392,27 @@ object AggregateUtil {
 
         case udagg: AggSqlFunction =>
           aggregates(index) = udagg.getFunction
+          accTypes(index) = udagg.accType
 
         case unSupported: SqlAggFunction =>
           throw new TableException(s"unsupported Function: '${unSupported.getName}'")
       }
     }
 
-    (aggFieldIndexes, aggregates)
-  }
-
-  private def createAccumulatorType(
-      aggregates: Array[TableAggregateFunction[_, _]]): Seq[TypeInformation[_]] = {
-
-    val aggTypes: Seq[TypeInformation[_]] =
-      aggregates.map {
-        agg =>
-          val accType = try {
-            val method: Method = agg.getClass.getMethod("getAccumulatorType")
-            method.invoke(agg).asInstanceOf[TypeInformation[_]]
-          } catch {
-            case _: NoSuchMethodException => null
-            case ite: Throwable => throw new TableException("Unexpected exception:", ite)
-          }
-          if (accType != null) {
-            accType
-          } else {
-            val accumulator = agg.createAccumulator()
-            try {
-              TypeInformation.of(accumulator.getClass)
-            } catch {
-              case ite: InvalidTypesException =>
-                throw new TableException(
-                  "Cannot infer type of accumulator. " +
-                    "You can override AggregateFunction.getAccumulatorType() to specify the type.",
-                  ite)
-            }
-          }
+    // create accumulator type information for every aggregate function
+    aggregates.zipWithIndex.foreach { case (agg, index) =>
+      if (null == accTypes(index)) {
+        accTypes(index) = getAccumulatorTypeOfAggregateFunction(agg)
       }
+    }
 
-    aggTypes
+    (aggFieldIndexes, aggregates, accTypes)
   }
 
   private def createRowTypeForKeysAndAggregates(
       groupings: Array[Int],
       aggregates: Array[TableAggregateFunction[_, _]],
+      aggTypes: Array[TypeInformation[_]],
       inputType: RelDataType,
       windowKeyTypes: Option[Array[TypeInformation[_]]] = None): RowTypeInfo = {
 
@@ -1441,9 +1422,6 @@ object AggregateUtil {
         .map(inputType.getFieldList.get(_).getType)
         .map(FlinkTypeFactory.toTypeInfo)
 
-    // get all field data types of all intermediate aggregates
-    val aggTypes: Seq[TypeInformation[_]] = createAccumulatorType(aggregates)
-
     // concat group key types, aggregation types, and window key types
     val allFieldTypes: Seq[TypeInformation[_]] = windowKeyTypes match {
       case None => groupingTypes ++: aggTypes
@@ -1452,13 +1430,6 @@ object AggregateUtil {
     new RowTypeInfo(allFieldTypes: _*)
   }
 
-  private[flink] def createAccumulatorRowType(
-      aggregates: Array[TableAggregateFunction[_, _]]): RowTypeInfo = {
-
-    val aggTypes: Seq[TypeInformation[_]] = createAccumulatorType(aggregates)
-
-    new RowTypeInfo(aggTypes: _*)
-  }
 
   private def getTimeFieldPosition(
     timeField: Expression,
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/validate/FunctionCatalog.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/validate/FunctionCatalog.scala
index df77441e400..b7bfa9b3527 100644
--- a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/validate/FunctionCatalog.scala
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/validate/FunctionCatalog.scala
@@ -105,7 +105,9 @@ class FunctionCatalog {
           .getOrElse(throw ValidationException(s"Undefined table function: $name"))
           .asInstanceOf[AggSqlFunction]
         val function = aggregateFunction.getFunction
-        AggFunctionCall(function, children)
+        val returnType = aggregateFunction.returnType
+        val accType = aggregateFunction.accType
+        AggFunctionCall(function, returnType, accType, children)
 
       // general expression call
       case expression if classOf[Expression].isAssignableFrom(expression) =>
diff --git a/flink-libraries/flink-table/src/test/java/org/apache/flink/table/api/java/utils/UserDefinedAggFunctions.java b/flink-libraries/flink-table/src/test/java/org/apache/flink/table/api/java/utils/UserDefinedAggFunctions.java
index 94c5c90c9ef..a32d6190d01 100644
--- a/flink-libraries/flink-table/src/test/java/org/apache/flink/table/api/java/utils/UserDefinedAggFunctions.java
+++ b/flink-libraries/flink-table/src/test/java/org/apache/flink/table/api/java/utils/UserDefinedAggFunctions.java
@@ -59,7 +59,7 @@ public class UserDefinedAggFunctions {
 	/**
 	 * Accumulator for WeightedAvg.
 	 */
-	public static class WeightedAvgAccum extends Tuple2<Long, Integer> {
+	public static class WeightedAvgAccum {
 		public long sum = 0;
 		public int count = 0;
 	}
diff --git a/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/api/scala/stream/sql/AggregationsTest.scala b/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/api/scala/stream/sql/AggregationsTest.scala
index 585c390909f..b6a0185db06 100644
--- a/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/api/scala/stream/sql/AggregationsTest.scala
+++ b/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/api/scala/stream/sql/AggregationsTest.scala
@@ -17,12 +17,20 @@
  */
 package org.apache.flink.table.api.scala.stream.sql
 
+import org.apache.flink.api.common.typeinfo.TypeInformation
+import org.apache.flink.api.java.typeutils.RowTypeInfo
 import org.apache.flink.api.scala._
-import org.apache.flink.table.api.ValidationException
+import org.apache.flink.api.scala.typeutils.CaseClassTypeInfo
+import org.apache.flink.table.api.{Types, ValidationException}
 import org.apache.flink.table.api.java.utils.UserDefinedAggFunctions.OverAgg0
 import org.apache.flink.table.api.scala._
+import org.apache.flink.table.expressions.AggFunctionCall
+import org.apache.flink.table.functions.AggregateFunction
 import org.apache.flink.table.utils.{StreamTableTestUtil, TableTestBase}
+import org.apache.flink.types.Row
 import org.junit.Test
+import org.junit.Assert.{assertEquals, assertTrue}
+
 
 class AggregationsTest extends TableTestBase {
   private val streamUtil: StreamTableTestUtil = streamTestUtil()
@@ -39,4 +47,59 @@ class AggregationsTest extends TableTestBase {
 
     streamUtil.tEnv.sql(sqlQuery)
   }
+
+  @Test
+  def testUserDefinedAggregateFunctionWithScalaAccumulator(): Unit = {
+    streamUtil.addFunction("udag", new MyAgg)
+    val call = streamUtil
+      .tEnv
+      .functionCatalog
+      .lookupFunction("udag", Seq())
+      .asInstanceOf[AggFunctionCall]
+
+    val typeInfo = call.accTypeInfo
+    assertTrue(typeInfo.isInstanceOf[CaseClassTypeInfo[_]])
+    assertEquals(2, typeInfo.getTotalFields)
+    val caseTypeInfo = typeInfo.asInstanceOf[CaseClassTypeInfo[_]]
+    assertEquals(Types.LONG, caseTypeInfo.getTypeAt(0))
+    assertEquals(Types.LONG, caseTypeInfo.getTypeAt(1))
+
+    streamUtil.addFunction("udag2", new MyAgg2)
+    val call2 = streamUtil
+      .tEnv
+      .functionCatalog
+      .lookupFunction("udag2", Seq())
+      .asInstanceOf[AggFunctionCall]
+
+    val typeInfo2 = call2.accTypeInfo
+    assertTrue(s"actual type: $typeInfo2", typeInfo2.isInstanceOf[RowTypeInfo])
+    assertEquals(2, typeInfo2.getTotalFields)
+    val rowTypeInfo = typeInfo2.asInstanceOf[RowTypeInfo]
+    assertEquals(Types.LONG, rowTypeInfo.getTypeAt(0))
+    assertEquals(Types.INT, rowTypeInfo.getTypeAt(1))
+  }
+}
+
+case class MyAccumulator(var sum: Long, var count: Long)
+
+class MyAgg extends AggregateFunction[Long, MyAccumulator] {
+
+  //Overloaded accumulate method
+  def accumulate(acc: MyAccumulator, value: Long): Unit = {
+  }
+
+  override def createAccumulator(): MyAccumulator = MyAccumulator(0, 0)
+
+  override def getValue(accumulator: MyAccumulator): Long = 1L
+}
+
+class MyAgg2 extends AggregateFunction[Long, Row] {
+
+  def accumulate(acc: Row, value: Long): Unit = {}
+
+  override def createAccumulator(): Row = new Row(2)
+
+  override def getValue(accumulator: Row): Long = 1L
+
+  def getAccumulatorType: TypeInformation[_] = new RowTypeInfo(Types.LONG, Types.INT)
 }
diff --git a/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/harness/HarnessTestBase.scala b/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/harness/HarnessTestBase.scala
index 0b3373ceadd..7ffc5f9eec2 100644
--- a/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/harness/HarnessTestBase.scala
+++ b/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/harness/HarnessTestBase.scala
@@ -30,8 +30,8 @@ import org.apache.flink.streaming.util.{KeyedOneInputStreamOperatorTestHarness,
 import org.apache.flink.table.codegen.GeneratedAggregationsFunction
 import org.apache.flink.table.functions.AggregateFunction
 import org.apache.flink.table.functions.utils.UserDefinedFunctionUtils
-import org.apache.flink.table.functions.aggfunctions.{LongMaxWithRetractAggFunction, LongMinWithRetractAggFunction, IntSumWithRetractAggFunction}
-import org.apache.flink.table.runtime.aggregate.AggregateUtil
+import org.apache.flink.table.functions.aggfunctions.{IntSumWithRetractAggFunction, LongMaxWithRetractAggFunction, LongMinWithRetractAggFunction}
+import org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.getAccumulatorTypeOfAggregateFunction
 import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
 
 class HarnessTestBase {
@@ -70,10 +70,10 @@ class HarnessTestBase {
     Array(new IntSumWithRetractAggFunction).asInstanceOf[Array[AggregateFunction[_, _]]]
 
   protected val minMaxAggregationStateType: RowTypeInfo =
-    AggregateUtil.createAccumulatorRowType(minMaxAggregates)
+    new RowTypeInfo(minMaxAggregates.map(getAccumulatorTypeOfAggregateFunction(_)): _*)
 
   protected val sumAggregationStateType: RowTypeInfo =
-    AggregateUtil.createAccumulatorRowType(sumAggregates)
+    new RowTypeInfo(sumAggregates.map(getAccumulatorTypeOfAggregateFunction(_)): _*)
 
   val minMaxCode: String =
     s"""
