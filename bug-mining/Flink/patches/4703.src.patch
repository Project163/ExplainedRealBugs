diff --git a/flink-formats/flink-avro-confluent-registry/src/test/java/org/apache/flink/formats/avro/registry/confluent/RegistryAvroRowDataSeDeSchemaTest.java b/flink-formats/flink-avro-confluent-registry/src/test/java/org/apache/flink/formats/avro/registry/confluent/RegistryAvroRowDataSeDeSchemaTest.java
index dbcec71cdd4..ce3d630b536 100644
--- a/flink-formats/flink-avro-confluent-registry/src/test/java/org/apache/flink/formats/avro/registry/confluent/RegistryAvroRowDataSeDeSchemaTest.java
+++ b/flink-formats/flink-avro-confluent-registry/src/test/java/org/apache/flink/formats/avro/registry/confluent/RegistryAvroRowDataSeDeSchemaTest.java
@@ -139,7 +139,9 @@ public class RegistryAvroRowDataSeDeSchemaTest {
 		RowType rowType = (RowType) dataType.getLogicalType();
 
 		AvroRowDataSerializationSchema serializer = getSerializationSchema(rowType, schema);
-		AvroRowDataDeserializationSchema deserializer = getDeserializationSchema(rowType, schema);
+		Schema writeSchema = AvroSchemaConverter.convertToSchema(dataType.getLogicalType());
+		AvroRowDataDeserializationSchema deserializer =
+				getDeserializationSchema(rowType, writeSchema);
 
 		serializer.open(null);
 		deserializer.open(null);
diff --git a/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverter.java b/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverter.java
index 2418b8321d3..924fa55b5c6 100644
--- a/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverter.java
+++ b/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverter.java
@@ -300,29 +300,44 @@ public class AvroSchemaConverter {
 		return convertToSchema(logicalType, "record");
 	}
 
+	/**
+	 * Converts Flink SQL {@link LogicalType} (can be nested) into an Avro schema.
+	 *
+	 * @param logicalType logical type
+	 * @param rowName     the record name
+	 * @return Avro's {@link Schema} matching this logical type.
+	 */
 	public static Schema convertToSchema(LogicalType logicalType, String rowName) {
 		int precision;
+		boolean nullable = logicalType.isNullable();
 		switch (logicalType.getTypeRoot()) {
 			case NULL:
 				return SchemaBuilder.builder().nullType();
 			case BOOLEAN:
-				return getNullableBuilder(logicalType).booleanType();
+				Schema bool = SchemaBuilder.builder().booleanType();
+				return nullable ? nullableSchema(bool) : bool;
 			case TINYINT:
 			case SMALLINT:
 			case INTEGER:
-				return getNullableBuilder(logicalType).intType();
+				Schema integer = SchemaBuilder.builder().intType();
+				return nullable ? nullableSchema(integer) : integer;
 			case BIGINT:
-				return getNullableBuilder(logicalType).longType();
+				Schema bigint = SchemaBuilder.builder().longType();
+				return nullable ? nullableSchema(bigint) : bigint;
 			case FLOAT:
-				return getNullableBuilder(logicalType).floatType();
+				Schema f = SchemaBuilder.builder().floatType();
+				return nullable ? nullableSchema(f) : f;
 			case DOUBLE:
-				return getNullableBuilder(logicalType).doubleType();
+				Schema d = SchemaBuilder.builder().doubleType();
+				return nullable ? nullableSchema(d) : d;
 			case CHAR:
 			case VARCHAR:
-				return getNullableBuilder(logicalType).stringType();
+				Schema str = SchemaBuilder.builder().stringType();
+				return nullable ? nullableSchema(str) : str;
 			case BINARY:
 			case VARBINARY:
-				return getNullableBuilder(logicalType).bytesType();
+				Schema binary = SchemaBuilder.builder().bytesType();
+				return nullable ? nullableSchema(binary) : binary;
 			case TIMESTAMP_WITHOUT_TIME_ZONE:
 				// use long to represents Timestamp
 				final TimestampType timestampType = (TimestampType) logicalType;
@@ -357,18 +372,18 @@ public class AvroSchemaConverter {
 				RowType rowType = (RowType) logicalType;
 				List<String> fieldNames = rowType.getFieldNames();
 				// we have to make sure the record name is different in a Schema
-				SchemaBuilder.FieldAssembler<Schema> builder = SchemaBuilder
-					.builder()
-					.record(rowName)
-					.fields();
+				SchemaBuilder.FieldAssembler<Schema> builder = SchemaBuilder.builder()
+						.record(rowName)
+						.fields();
 				for (int i = 0; i < rowType.getFieldCount(); i++) {
-					String fieldName = rowName + "_" + fieldNames.get(i);
+					String fieldName = fieldNames.get(i);
 					builder = builder
 						.name(fieldName)
-						.type(convertToSchema(rowType.getTypeAt(i), fieldName))
+						.type(convertToSchema(rowType.getTypeAt(i), rowName + "_" + fieldName))
 						.noDefault();
 				}
-				return builder.endRecord();
+				Schema record = builder.endRecord();
+				return nullable ? nullableSchema(record) : record;
 			case MULTISET:
 			case MAP:
 				return SchemaBuilder
@@ -410,11 +425,10 @@ public class AvroSchemaConverter {
 		return valueType;
 	}
 
-	private static SchemaBuilder.BaseTypeBuilder<Schema> getNullableBuilder(LogicalType logicalType) {
-		SchemaBuilder.TypeBuilder<Schema> builder = SchemaBuilder.builder();
-		if (logicalType.isNullable()) {
-			return builder.nullable();
-		}
-		return builder;
+	/** Returns schema with nullable true. */
+	private static Schema nullableSchema(Schema schema) {
+		return schema.isNullable()
+				? schema
+				: Schema.createUnion(schema, SchemaBuilder.builder().nullType());
 	}
 }
diff --git a/flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverterTest.java b/flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverterTest.java
index cfcb309f2c2..f673a6a9680 100644
--- a/flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverterTest.java
+++ b/flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverterTest.java
@@ -104,48 +104,122 @@ public class AvroSchemaConverterTest {
 				DataTypes.FIELD("row3", DataTypes.ROW(DataTypes.FIELD("c", DataTypes.STRING())))))
 			.build().toRowDataType().getLogicalType();
 		Schema schema = AvroSchemaConverter.convertToSchema(rowType);
-		assertEquals("{\n" +
-			"  \"type\" : \"record\",\n" +
-			"  \"name\" : \"record\",\n" +
-			"  \"fields\" : [ {\n" +
-			"    \"name\" : \"record_row1\",\n" +
-			"    \"type\" : {\n" +
-			"      \"type\" : \"record\",\n" +
-			"      \"name\" : \"record_row1\",\n" +
-			"      \"fields\" : [ {\n" +
-			"        \"name\" : \"record_row1_a\",\n" +
-			"        \"type\" : [ \"string\", \"null\" ]\n" +
-			"      } ]\n" +
-			"    }\n" +
-			"  }, {\n" +
-			"    \"name\" : \"record_row2\",\n" +
-			"    \"type\" : {\n" +
-			"      \"type\" : \"record\",\n" +
-			"      \"name\" : \"record_row2\",\n" +
-			"      \"fields\" : [ {\n" +
-			"        \"name\" : \"record_row2_b\",\n" +
-			"        \"type\" : [ \"string\", \"null\" ]\n" +
-			"      } ]\n" +
-			"    }\n" +
-			"  }, {\n" +
-			"    \"name\" : \"record_row3\",\n" +
-			"    \"type\" : {\n" +
-			"      \"type\" : \"record\",\n" +
-			"      \"name\" : \"record_row3\",\n" +
-			"      \"fields\" : [ {\n" +
-			"        \"name\" : \"record_row3_row3\",\n" +
-			"        \"type\" : {\n" +
-			"          \"type\" : \"record\",\n" +
-			"          \"name\" : \"record_row3_row3\",\n" +
-			"          \"fields\" : [ {\n" +
-			"            \"name\" : \"record_row3_row3_c\",\n" +
-			"            \"type\" : [ \"string\", \"null\" ]\n" +
-			"          } ]\n" +
-			"        }\n" +
-			"      } ]\n" +
-			"    }\n" +
-			"  } ]\n" +
-			"}", schema.toString(true));
+		assertEquals("[ {\n" +
+				"  \"type\" : \"record\",\n" +
+				"  \"name\" : \"record\",\n" +
+				"  \"fields\" : [ {\n" +
+				"    \"name\" : \"row1\",\n" +
+				"    \"type\" : [ {\n" +
+				"      \"type\" : \"record\",\n" +
+				"      \"name\" : \"record_row1\",\n" +
+				"      \"fields\" : [ {\n" +
+				"        \"name\" : \"a\",\n" +
+				"        \"type\" : [ \"string\", \"null\" ]\n" +
+				"      } ]\n" +
+				"    }, \"null\" ]\n" +
+				"  }, {\n" +
+				"    \"name\" : \"row2\",\n" +
+				"    \"type\" : [ {\n" +
+				"      \"type\" : \"record\",\n" +
+				"      \"name\" : \"record_row2\",\n" +
+				"      \"fields\" : [ {\n" +
+				"        \"name\" : \"b\",\n" +
+				"        \"type\" : [ \"string\", \"null\" ]\n" +
+				"      } ]\n" +
+				"    }, \"null\" ]\n" +
+				"  }, {\n" +
+				"    \"name\" : \"row3\",\n" +
+				"    \"type\" : [ {\n" +
+				"      \"type\" : \"record\",\n" +
+				"      \"name\" : \"record_row3\",\n" +
+				"      \"fields\" : [ {\n" +
+				"        \"name\" : \"row3\",\n" +
+				"        \"type\" : [ {\n" +
+				"          \"type\" : \"record\",\n" +
+				"          \"name\" : \"record_row3_row3\",\n" +
+				"          \"fields\" : [ {\n" +
+				"            \"name\" : \"c\",\n" +
+				"            \"type\" : [ \"string\", \"null\" ]\n" +
+				"          } ]\n" +
+				"        }, \"null\" ]\n" +
+				"      } ]\n" +
+				"    }, \"null\" ]\n" +
+				"  } ]\n" +
+				"}, \"null\" ]", schema.toString(true));
+	}
+
+	/**
+	 * Test convert nullable data type to Avro schema then converts back.
+	 */
+	@Test
+	public void testConversionIntegralityNullable() {
+		DataType dataType = DataTypes.ROW(
+				DataTypes.FIELD("f_null", DataTypes.NULL()),
+				DataTypes.FIELD("f_boolean", DataTypes.BOOLEAN()),
+				// tinyint and smallint all convert to int
+				DataTypes.FIELD("f_int", DataTypes.INT()),
+				DataTypes.FIELD("f_bigint", DataTypes.BIGINT()),
+				DataTypes.FIELD("f_float", DataTypes.FLOAT()),
+				DataTypes.FIELD("f_double", DataTypes.DOUBLE()),
+				// char converts to string
+				DataTypes.FIELD("f_string", DataTypes.STRING()),
+				// binary converts to bytes
+				DataTypes.FIELD("f_varbinary", DataTypes.BYTES()),
+				DataTypes.FIELD("f_timestamp", DataTypes.TIMESTAMP(3)),
+				DataTypes.FIELD("f_date", DataTypes.DATE()),
+				DataTypes.FIELD("f_time", DataTypes.TIME(3)),
+				DataTypes.FIELD("f_decimal", DataTypes.DECIMAL(10, 0)),
+				DataTypes.FIELD("f_row", DataTypes.ROW(
+					DataTypes.FIELD("f0", DataTypes.INT()),
+					DataTypes.FIELD("f1", DataTypes.TIMESTAMP(3)))),
+				// multiset converts to map
+				// map key is always not null
+				DataTypes.FIELD("f_map",
+						DataTypes.MAP(DataTypes.STRING().notNull(), DataTypes.INT())),
+				DataTypes.FIELD("f_array", DataTypes.ARRAY(DataTypes.INT())))
+				.notNull();
+		Schema schema = AvroSchemaConverter.convertToSchema(dataType.getLogicalType());
+		DataType converted = AvroSchemaConverter.convertToDataType(schema.toString());
+		assertEquals(dataType, converted);
+	}
+
+	/**
+	 * Test convert non-nullable data type to Avro schema then converts back.
+	 */
+	@Test
+	public void testConversionIntegralityNonNullable() {
+		DataType dataType = DataTypes.ROW(
+				DataTypes.FIELD("f_boolean", DataTypes.BOOLEAN().notNull()),
+				// tinyint and smallint all convert to int
+				DataTypes.FIELD("f_int", DataTypes.INT().notNull()),
+				DataTypes.FIELD("f_bigint", DataTypes.BIGINT().notNull()),
+				DataTypes.FIELD("f_float", DataTypes.FLOAT().notNull()),
+				DataTypes.FIELD("f_double", DataTypes.DOUBLE().notNull()),
+				// char converts to string
+				DataTypes.FIELD("f_string", DataTypes.STRING().notNull()),
+				// binary converts to bytes
+				DataTypes.FIELD("f_varbinary", DataTypes.BYTES().notNull()),
+				DataTypes.FIELD("f_timestamp", DataTypes.TIMESTAMP(3).notNull()),
+				DataTypes.FIELD("f_date", DataTypes.DATE().notNull()),
+				DataTypes.FIELD("f_time", DataTypes.TIME(3).notNull()),
+				DataTypes.FIELD("f_decimal",
+						DataTypes.DECIMAL(10, 0).notNull()),
+				DataTypes.FIELD("f_row", DataTypes.ROW(
+						DataTypes.FIELD("f0", DataTypes.INT().notNull()),
+						DataTypes.FIELD("f1", DataTypes.TIMESTAMP(3).notNull()))
+						.notNull()),
+				// multiset converts to map
+				// map key is always not null
+				DataTypes.FIELD("f_map",
+						DataTypes.MAP(
+								DataTypes.STRING().notNull(),
+								DataTypes.INT().notNull())
+								.notNull()),
+				DataTypes.FIELD("f_array",
+						DataTypes.ARRAY(DataTypes.INT().notNull()).notNull()));
+		Schema schema = AvroSchemaConverter.convertToSchema(dataType.getLogicalType());
+		DataType converted = AvroSchemaConverter.convertToDataType(schema.toString());
+		assertEquals(dataType, converted);
 	}
 
 	private void validateUserSchema(TypeInformation<?> actual) {
