diff --git a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/FlinkKafkaInternalProducer.java b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/FlinkKafkaInternalProducer.java
index 4856222dd02..aec1edfff13 100644
--- a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/FlinkKafkaInternalProducer.java
+++ b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/FlinkKafkaInternalProducer.java
@@ -84,12 +84,16 @@ class FlinkKafkaInternalProducer<K, V> extends KafkaProducer<K, V> {
 
     @Override
     public void abortTransaction() throws ProducerFencedException {
+        LOG.debug("abortTransaction {}", transactionalId);
+        checkState(inTransaction, "Transaction was not started");
         super.abortTransaction();
         inTransaction = false;
     }
 
     @Override
     public void commitTransaction() throws ProducerFencedException {
+        LOG.debug("commitTransaction {}", transactionalId);
+        checkState(inTransaction, "Transaction was not started");
         super.commitTransaction();
         inTransaction = false;
     }
@@ -156,6 +160,7 @@ class FlinkKafkaInternalProducer<K, V> extends KafkaProducer<K, V> {
     public void setTransactionId(String transactionalId) {
         if (!transactionalId.equals(this.transactionalId)) {
             checkState(!inTransaction);
+            LOG.debug("Change transaction id from {} to {}", this.transactionalId, transactionalId);
             Object transactionManager = getTransactionManager();
             synchronized (transactionManager) {
                 setField(transactionManager, "transactionalId", transactionalId);
@@ -274,6 +279,7 @@ class FlinkKafkaInternalProducer<K, V> extends KafkaProducer<K, V> {
      * https://github.com/apache/kafka/commit/5d2422258cb975a137a42a4e08f03573c49a387e#diff-f4ef1afd8792cd2a2e9069cd7ddea630
      */
     public void resumeTransaction(long producerId, short epoch) {
+        checkState(!inTransaction, "Already in transaction %s", transactionalId);
         checkState(
                 producerId >= 0 && epoch >= 0,
                 "Incorrect values for producerId %s and epoch %s",
@@ -302,6 +308,7 @@ class FlinkKafkaInternalProducer<K, V> extends KafkaProducer<K, V> {
 
             transitionTransactionManagerStateTo(transactionManager, "IN_TRANSACTION");
             setField(transactionManager, "transactionStarted", true);
+            this.inTransaction = true;
         }
     }
 
@@ -358,4 +365,16 @@ class FlinkKafkaInternalProducer<K, V> extends KafkaProducer<K, V> {
             Object transactionManager, String state) {
         invoke(transactionManager, "transitionTo", getTransactionManagerState(state));
     }
+
+    @Override
+    public String toString() {
+        return "FlinkKafkaInternalProducer{"
+                + "transactionalId='"
+                + transactionalId
+                + "', inTransaction="
+                + inTransaction
+                + ", closed="
+                + closed
+                + '}';
+    }
 }
diff --git a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaWriter.java b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaWriter.java
index b13a51f92e8..dfe17f63493 100644
--- a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaWriter.java
+++ b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaWriter.java
@@ -26,6 +26,7 @@ import org.apache.flink.connector.kafka.MetricUtil;
 import org.apache.flink.metrics.Counter;
 import org.apache.flink.metrics.MetricGroup;
 import org.apache.flink.metrics.groups.SinkWriterMetricGroup;
+import org.apache.flink.runtime.checkpoint.CheckpointIDCounter;
 import org.apache.flink.streaming.connectors.kafka.internals.metrics.KafkaMetricMutableWrapper;
 import org.apache.flink.util.FlinkRuntimeException;
 
@@ -157,7 +158,10 @@ class KafkaWriter<IN> implements SinkWriter<IN, KafkaCommittable, KafkaWriterSta
         }
 
         this.kafkaWriterState = new KafkaWriterState(transactionalIdPrefix);
-        this.lastCheckpointId = sinkInitContext.getRestoredCheckpointId().orElse(-1);
+        this.lastCheckpointId =
+                sinkInitContext
+                        .getRestoredCheckpointId()
+                        .orElse(CheckpointIDCounter.INITIAL_CHECKPOINT_ID - 1);
         if (deliveryGuarantee == DeliveryGuarantee.EXACTLY_ONCE) {
             abortLingeringTransactions(
                     checkNotNull(recoveredStates, "recoveredStates"), lastCheckpointId + 1);
@@ -191,7 +195,7 @@ class KafkaWriter<IN> implements SinkWriter<IN, KafkaCommittable, KafkaWriterSta
             final List<KafkaCommittable> committables =
                     Collections.singletonList(
                             KafkaCommittable.of(currentProducer, producerPool::add));
-            LOG.info("Committing {} committables.", committables);
+            LOG.debug("Committing {} committables, final commit={}.", committables, flush);
             return committables;
         }
         return Collections.emptyList();
@@ -209,6 +213,7 @@ class KafkaWriter<IN> implements SinkWriter<IN, KafkaCommittable, KafkaWriterSta
     @Override
     public void close() throws Exception {
         closed = true;
+        LOG.debug("Closing writer with {}", currentProducer);
         closeAll(
                 this::abortCurrentProducer,
                 closer,
diff --git a/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/sink/KafkaWriterITCase.java b/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/sink/KafkaWriterITCase.java
index 62b382a5263..5f8d552bc9c 100644
--- a/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/sink/KafkaWriterITCase.java
+++ b/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/sink/KafkaWriterITCase.java
@@ -191,9 +191,9 @@ public class KafkaWriterITCase extends TestLogger {
 
         // create two lingering transactions
         failedWriter.prepareCommit(false);
-        failedWriter.snapshotState(0);
-        failedWriter.prepareCommit(false);
         failedWriter.snapshotState(1);
+        failedWriter.prepareCommit(false);
+        failedWriter.snapshotState(2);
 
         try (final KafkaWriter<Integer> recoveredWriter =
                 createWriterWithConfiguration(
@@ -201,7 +201,7 @@ public class KafkaWriterITCase extends TestLogger {
             recoveredWriter.write(1, SINK_WRITER_CONTEXT);
 
             List<KafkaCommittable> committables = recoveredWriter.prepareCommit(false);
-            recoveredWriter.snapshotState(0);
+            recoveredWriter.snapshotState(1);
             assertThat(committables, hasSize(1));
             assertThat(committables.get(0).getProducer().isPresent(), equalTo(true));
 
@@ -248,7 +248,7 @@ public class KafkaWriterITCase extends TestLogger {
             assertThat(writer.getProducerPool(), hasSize(0));
 
             List<KafkaCommittable> committables0 = writer.prepareCommit(false);
-            writer.snapshotState(0);
+            writer.snapshotState(1);
             assertThat(committables0, hasSize(1));
             assertThat(committables0.get(0).getProducer().isPresent(), equalTo(true));
 
@@ -266,7 +266,7 @@ public class KafkaWriterITCase extends TestLogger {
             assertThat(writer.getProducerPool(), hasSize(1));
 
             List<KafkaCommittable> committables1 = writer.prepareCommit(false);
-            writer.snapshotState(1);
+            writer.snapshotState(2);
             assertThat(committables1, hasSize(1));
             assertThat(committables1.get(0).getProducer().isPresent(), equalTo(true));
 
