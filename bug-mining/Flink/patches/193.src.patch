diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/deployment/PartitionInfo.java b/flink-runtime/src/main/java/org/apache/flink/runtime/deployment/PartitionInfo.java
index cdaf2895f63..dd2c063caba 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/deployment/PartitionInfo.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/deployment/PartitionInfo.java
@@ -26,7 +26,7 @@ import org.apache.flink.runtime.executiongraph.Execution;
 import org.apache.flink.runtime.executiongraph.ExecutionAttemptID;
 import org.apache.flink.runtime.executiongraph.ExecutionEdge;
 import org.apache.flink.runtime.executiongraph.IntermediateResultPartition;
-import org.apache.flink.runtime.instance.AllocatedSlot;
+import org.apache.flink.runtime.instance.SimpleSlot;
 import org.apache.flink.runtime.io.network.RemoteAddress;
 import org.apache.flink.runtime.jobgraph.IntermediateResultPartitionID;
 
@@ -114,7 +114,7 @@ public class PartitionInfo implements IOReadableWritable, Serializable {
 
 	// ------------------------------------------------------------------------
 
-	public static PartitionInfo fromEdge(ExecutionEdge edge, AllocatedSlot consumerSlot) {
+	public static PartitionInfo fromEdge(ExecutionEdge edge, SimpleSlot consumerSlot) {
 		IntermediateResultPartition partition = edge.getSource();
 		IntermediateResultPartitionID partitionId = partition.getPartitionId();
 
@@ -125,7 +125,7 @@ public class PartitionInfo implements IOReadableWritable, Serializable {
 		RemoteAddress producerAddress = null;
 		PartitionLocation producerLocation = PartitionLocation.UNKNOWN;
 
-		AllocatedSlot producerSlot = producer.getAssignedResource();
+		SimpleSlot producerSlot = producer.getAssignedResource();
 		ExecutionState producerState = producer.getState();
 
 		// The producer needs to be running, otherwise the consumer might request a partition,
@@ -145,7 +145,7 @@ public class PartitionInfo implements IOReadableWritable, Serializable {
 		return new PartitionInfo(partitionId, producerExecutionId, producerLocation, producerAddress);
 	}
 
-	public static PartitionInfo[] fromEdges(ExecutionEdge[] edges, AllocatedSlot consumerSlot) {
+	public static PartitionInfo[] fromEdges(ExecutionEdge[] edges, SimpleSlot consumerSlot) {
 		// Every edge consumes a different result partition, which might be of
 		// local, remote, or unknown location.
 		PartitionInfo[] partitions = new PartitionInfo[edges.length];
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java b/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java
index a705231caf7..e1a24c45b31 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java
@@ -101,8 +101,8 @@ public class Execution implements Serializable {
 
 
 	private volatile ExecutionState state = CREATED;
-
-	private volatile AllocatedSlot assignedResource;  // once assigned, never changes
+	
+	private volatile SimpleSlot assignedResource;  // once assigned, never changes
 	
 	private volatile Throwable failureCause;          // once assigned, never changes
 	
@@ -141,7 +141,7 @@ public class Execution implements Serializable {
 		return state;
 	}
 	
-	public AllocatedSlot getAssignedResource() {
+	public SimpleSlot getAssignedResource() {
 		return assignedResource;
 	}
 	
@@ -185,7 +185,7 @@ public class Execution implements Serializable {
 
 		// sanity check
 		if (locationConstraint != null && sharingGroup == null) {
-			throw new RuntimeException("Trying to schedule with co-location constraint but without slot sharing allowed.");
+			throw new RuntimeException("Trying to schedule with co-location constraint but without slot sharing not allowed.");
 		}
 
 		if (transitionState(CREATED, SCHEDULED)) {
@@ -201,7 +201,7 @@ public class Execution implements Serializable {
 
 				future.setFutureAction(new SlotAllocationFutureAction() {
 					@Override
-					public void slotAllocated(AllocatedSlot slot) {
+					public void slotAllocated(SimpleSlot slot) {
 						try {
 							deployToSlot(slot);
 						}
@@ -216,7 +216,7 @@ public class Execution implements Serializable {
 				});
 			}
 			else {
-				AllocatedSlot slot = scheduler.scheduleImmediately(toSchedule);
+				SimpleSlot slot = scheduler.scheduleImmediately(toSchedule);
 				try {
 					deployToSlot(slot);
 				}
@@ -237,7 +237,7 @@ public class Execution implements Serializable {
 		}
 	}
 
-	public void deployToSlot(final AllocatedSlot slot) throws JobException {
+	public void deployToSlot(final SimpleSlot slot) throws JobException {
 		// sanity checks
 		if (slot == null) {
 			throw new NullPointerException();
@@ -406,7 +406,7 @@ public class Execution implements Serializable {
 				}
 			}
 			else if (consumerState == RUNNING) {
-				AllocatedSlot consumerSlot = consumerVertex.getCurrentAssignedResource();
+				SimpleSlot consumerSlot = consumerVertex.getCurrentAssignedResource();
 				ExecutionAttemptID consumerExecutionId = consumerVertex.getCurrentExecutionAttempt().getAttemptId();
 
 				PartitionInfo partitionInfo = PartitionInfo.fromEdge(edge, consumerSlot);
@@ -635,7 +635,7 @@ public class Execution implements Serializable {
 	}
 
 	private void sendCancelRpcCall() {
-		final AllocatedSlot slot = this.assignedResource;
+		final SimpleSlot slot = this.assignedResource;
 		if (slot == null) {
 			return;
 		}
@@ -662,7 +662,7 @@ public class Execution implements Serializable {
 	}
 
 	private void sendFailIntermediateResultPartitionsRPCCall() {
-		final AllocatedSlot slot = this.assignedResource;
+		final SimpleSlot slot = this.assignedResource;
 		if (slot == null) {
 			return;
 		}
@@ -680,7 +680,7 @@ public class Execution implements Serializable {
 		}
 	}
 
-	private boolean sendUpdateTaskRpcCall(final AllocatedSlot consumerSlot, final ExecutionAttemptID executionId, final IntermediateDataSetID resultId, final PartitionInfo partitionInfo) throws Exception {
+	private boolean sendUpdateTaskRpcCall(final SimpleSlot consumerSlot, final ExecutionAttemptID executionId, final IntermediateDataSetID resultId, final PartitionInfo partitionInfo) throws Exception {
 		final Instance instance = consumerSlot.getInstance();
 
 		final TaskManagerMessages.TaskOperationResult result = AkkaUtils.ask(
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionVertex.java b/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionVertex.java
index 881256932ba..d3993bb9c19 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionVertex.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionVertex.java
@@ -18,6 +18,7 @@
 
 package org.apache.flink.runtime.executiongraph;
 
+import org.apache.flink.runtime.instance.SimpleSlot;
 import org.apache.flink.runtime.JobException;
 import org.apache.flink.runtime.blob.BlobKey;
 import org.apache.flink.runtime.deployment.PartitionConsumerDeploymentDescriptor;
@@ -25,7 +26,6 @@ import org.apache.flink.runtime.deployment.PartitionDeploymentDescriptor;
 import org.apache.flink.runtime.deployment.PartitionInfo;
 import org.apache.flink.runtime.deployment.TaskDeploymentDescriptor;
 import org.apache.flink.runtime.execution.ExecutionState;
-import org.apache.flink.runtime.instance.AllocatedSlot;
 import org.apache.flink.runtime.instance.Instance;
 import org.apache.flink.runtime.jobgraph.DistributionPattern;
 import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;
@@ -178,7 +178,7 @@ public class ExecutionVertex implements Serializable {
 		return currentExecution.getFailureCause();
 	}
 	
-	public AllocatedSlot getCurrentAssignedResource() {
+	public SimpleSlot getCurrentAssignedResource() {
 		return currentExecution.getAssignedResource();
 	}
 	
@@ -304,7 +304,7 @@ public class ExecutionVertex implements Serializable {
 			ExecutionEdge[] sources = inputEdges[i];
 			if (sources != null) {
 				for (int k = 0; k < sources.length; k++) {
-					AllocatedSlot sourceSlot = sources[k].getSource().getProducer().getCurrentAssignedResource();
+					SimpleSlot sourceSlot = sources[k].getSource().getProducer().getCurrentAssignedResource();
 					if (sourceSlot != null) {
 						locations.add(sourceSlot.getInstance());
 						if (locations.size() > MAX_DISTINCT_LOCATIONS_TO_CONSIDER) {
@@ -346,7 +346,7 @@ public class ExecutionVertex implements Serializable {
 		return this.currentExecution.scheduleForExecution(scheduler, queued);
 	}
 
-	public void deployToSlot(AllocatedSlot slot) throws JobException {
+	public void deployToSlot(SimpleSlot slot) throws JobException {
 		this.currentExecution.deployToSlot(slot);
 	}
 
@@ -397,7 +397,7 @@ public class ExecutionVertex implements Serializable {
 		getExecutionGraph().notifyExecutionChange(getJobvertexId(), subTaskIndex, executionId, newState, error);
 	}
 	
-	TaskDeploymentDescriptor createDeploymentDescriptor(ExecutionAttemptID executionId, AllocatedSlot slot) {
+	TaskDeploymentDescriptor createDeploymentDescriptor(ExecutionAttemptID executionId, SimpleSlot slot) {
 		// Produced intermediate results
 		List<PartitionDeploymentDescriptor> producedPartitions = new ArrayList<PartitionDeploymentDescriptor>(resultPartitions.length);
 
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/instance/AllocatedSlot.java b/flink-runtime/src/main/java/org/apache/flink/runtime/instance/AllocatedSlot.java
deleted file mode 100644
index f1481f39369..00000000000
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/instance/AllocatedSlot.java
+++ /dev/null
@@ -1,192 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.runtime.instance;
-
-import java.io.Serializable;
-import java.util.concurrent.atomic.AtomicIntegerFieldUpdater;
-import java.util.concurrent.atomic.AtomicReferenceFieldUpdater;
-
-import org.apache.flink.runtime.executiongraph.Execution;
-import org.apache.flink.runtime.jobgraph.JobID;
-import org.apache.flink.runtime.jobmanager.scheduler.Locality;
-
-/**
- * An allocated slot is the unit in which resources are allocated on instances.
- */
-public class AllocatedSlot implements Serializable {
-
-	static final long serialVersionUID = 42L;
-	
-	private static final AtomicIntegerFieldUpdater<AllocatedSlot> STATUS_UPDATER = 
-			AtomicIntegerFieldUpdater.newUpdater(AllocatedSlot.class, "status");
-	
-	private static final AtomicReferenceFieldUpdater<AllocatedSlot, Execution> VERTEX_UPDATER =
-			AtomicReferenceFieldUpdater.newUpdater(AllocatedSlot.class, Execution.class, "executedTask");
-	
-	private static final int ALLOCATED_AND_ALIVE = 0;		// tasks may be added and might be running
-	private static final int CANCELLED = 1;					// no more tasks may run
-	private static final int RELEASED = 2;					// has been given back to the instance
-
-	
-	/** The ID of the job this slice belongs to. */
-	private final JobID jobID;
-	
-	/** The instance on which the slot is allocated */
-	private final Instance instance;
-	
-	/** The number of the slot on which the task is deployed */
-	private final int slotNumber;
-	
-	/** Task being executed in the slot. Volatile to force a memory barrier and allow for correct double-checking */
-	private volatile Execution executedTask;
-	
-	/** The state of the vertex, only atomically updated */
-	private volatile int status = ALLOCATED_AND_ALIVE;
-	
-	private Locality locality = Locality.UNCONSTRAINED;
-	
-
-	public AllocatedSlot(JobID jobID, Instance instance, int slotNumber) {
-		if (jobID == null || instance == null || slotNumber < 0) {
-			throw new IllegalArgumentException();
-		}
-		
-		this.jobID = jobID;
-		this.instance = instance;
-		this.slotNumber = slotNumber;
-	}
-
-	// --------------------------------------------------------------------------------------------
-	
-	/**
-	 * Returns the ID of the job this allocated slot belongs to.
-	 * 
-	 * @return the ID of the job this allocated slot belongs to
-	 */
-	public JobID getJobID() {
-		return this.jobID;
-	}
-	
-	public Instance getInstance() {
-		return instance;
-	}
-	
-	public int getSlotNumber() {
-		return slotNumber;
-	}
-	
-	public Execution getExecutedVertex() {
-		return executedTask;
-	}
-	
-	public Locality getLocality() {
-		return locality;
-	}
-	
-	public void setLocality(Locality locality) {
-		this.locality = locality;
-	}
-	
-	public boolean setExecutedVertex(Execution executedVertex) {
-		if (executedVertex == null) {
-			throw new NullPointerException();
-		}
-		
-		// check that we can actually run in this slot
-		if (status != ALLOCATED_AND_ALIVE) {
-			return false;
-		}
-		
-		// atomically assign the vertex
-		if (!VERTEX_UPDATER.compareAndSet(this, null, executedVertex)) {
-			return false;
-		}
-
-		// we need to do a double check that we were not cancelled in the meantime
-		if (status != ALLOCATED_AND_ALIVE) {
-			this.executedTask = null;
-			return false;
-		}
-		
-		return true;
-	}
-	
-	// --------------------------------------------------------------------------------------------
-	//  Status and life cycle
-	// --------------------------------------------------------------------------------------------
-	
-	public boolean isAlive() {
-		return status == ALLOCATED_AND_ALIVE;
-	}
-	
-	public boolean isCanceled() {
-		return status != ALLOCATED_AND_ALIVE;
-	}
-	
-	public boolean isReleased() {
-		return status == RELEASED;
-	}
-	
-	
-	public void cancel() {
-		if (STATUS_UPDATER.compareAndSet(this, ALLOCATED_AND_ALIVE, CANCELLED)) {
-			// kill all tasks currently running in this slot
-			Execution exec = this.executedTask;
-			if (exec != null && !exec.isFinished()) {
-				exec.fail(new Exception("The slot in which the task was scheduled has been killed (probably loss of TaskManager)."));
-			}
-		}
-	}
-	
-	public void releaseSlot() {
-		// cancel everything, if there is something. since this is atomically status based,
-		// it will not happen twice if another attempt happened before or concurrently
-		try {
-			cancel();
-		} finally {
-			this.instance.returnAllocatedSlot(this);
-		}
-	}
-	
-	protected boolean markReleased() {
-		return STATUS_UPDATER.compareAndSet(this, CANCELLED, RELEASED);
-	}
-	
-	// --------------------------------------------------------------------------------------------
-	//  Utilities
-	// --------------------------------------------------------------------------------------------
-	
-	@Override
-	public String toString() {
-		return instance.getId() + " (" + slotNumber + ") - " + getStateName(status);
-	}
-	
-	private static final String getStateName(int state) {
-		switch (state) {
-		case ALLOCATED_AND_ALIVE:
-			return "ALLOCATED/ALIVE";
-		case CANCELLED:
-			return "CANCELLED";
-		case RELEASED:
-			return "RELEASED";
-		default:
-			return "(unknown)";
-		}
-	}
-}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/instance/Instance.java b/flink-runtime/src/main/java/org/apache/flink/runtime/instance/Instance.java
index abbbc342021..4f9dc7f4881 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/instance/Instance.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/instance/Instance.java
@@ -27,8 +27,10 @@ import java.util.Queue;
 import java.util.Set;
 
 import akka.actor.ActorRef;
+import org.apache.flink.runtime.AbstractID;
 import org.apache.flink.runtime.jobgraph.JobID;
 import org.apache.flink.runtime.jobmanager.scheduler.SlotAvailabilityListener;
+import org.apache.flink.runtime.jobmanager.scheduler.SlotSharingGroupAssignment;
 
 /**
  * An taskManager represents a resource a {@link org.apache.flink.runtime.taskmanager.TaskManager} runs on.
@@ -59,7 +61,7 @@ public class Instance implements Serializable {
 	private transient final Queue<Integer> availableSlots;
 	
 	/** Allocated slots on this taskManager */
-	private final Set<AllocatedSlot> allocatedSlots = new HashSet<AllocatedSlot>();
+	private final Set<Slot> allocatedSlots = new HashSet<Slot>();
 
 	
 	/** A listener to be notified upon new slot availability */
@@ -121,20 +123,27 @@ public class Instance implements Serializable {
 	}
 
 	public void markDead() {
-		if (isDead) {
-			return;
-		}
-		
-		isDead = true;
-		
 		synchronized (instanceLock) {
-			
+			if (isDead) {
+				return;
+			}
+
+			isDead = true;
+
 			// no more notifications for the slot releasing
 			this.slotAvailabilityListener = null;
-			
-			for (AllocatedSlot slot : allocatedSlots) {
-				slot.releaseSlot();
-			}
+		}
+
+		/*
+		 * releaseSlot must not own the instanceLock in order to avoid dead locks where a slot
+		 * owning the assignment group lock wants to give itself back to the instance which requires
+		 * the instance lock
+		 */
+		for (Slot slot : allocatedSlots) {
+			slot.releaseSlot();
+		}
+
+		synchronized (instanceLock) {
 			allocatedSlots.clear();
 			availableSlots.clear();
 		}
@@ -176,8 +185,12 @@ public class Instance implements Serializable {
 	// --------------------------------------------------------------------------------------------
 	// Resource allocation
 	// --------------------------------------------------------------------------------------------
+
+	public SimpleSlot allocateSimpleSlot(JobID jobID) throws InstanceDiedException {
+		return allocateSimpleSlot(jobID, jobID);
+	}
 	
-	public AllocatedSlot allocateSlot(JobID jobID) throws InstanceDiedException {
+	public SimpleSlot allocateSimpleSlot(JobID jobID, AbstractID groupID) throws InstanceDiedException {
 		if (jobID == null) {
 			throw new IllegalArgumentException();
 		}
@@ -191,15 +204,38 @@ public class Instance implements Serializable {
 			if (nextSlot == null) {
 				return null;
 			} else {
-				AllocatedSlot slot = new AllocatedSlot(jobID, this, nextSlot);
+				SimpleSlot slot = new SimpleSlot(jobID, this, nextSlot, null, groupID);
 				allocatedSlots.add(slot);
 				return slot;
 			}
 		}
 	}
-	
-	public boolean returnAllocatedSlot(AllocatedSlot slot) {
+
+	public SharedSlot allocateSharedSlot(JobID jobID, SlotSharingGroupAssignment sharingGroupAssignment, AbstractID groupID) throws
+	InstanceDiedException {
 		// the slot needs to be in the returned to taskManager state
+		if (jobID == null) {
+			throw new IllegalArgumentException();
+		}
+
+		synchronized (instanceLock) {
+			if (isDead) {
+				throw new InstanceDiedException(this);
+			}
+
+			Integer nextSlot = availableSlots.poll();
+			if (nextSlot == null) {
+				return null;
+			} else {
+				SharedSlot slot = new SharedSlot(jobID, this, nextSlot,
+						sharingGroupAssignment, null, groupID);
+				allocatedSlots.add(slot);
+				return slot;
+			}
+		}
+	}
+
+	public boolean returnAllocatedSlot(Slot slot) {
 		if (slot == null || slot.getInstance() != this) {
 			throw new IllegalArgumentException("Slot is null or belongs to the wrong taskManager.");
 		}
@@ -231,14 +267,15 @@ public class Instance implements Serializable {
 	}
 	
 	public void cancelAndReleaseAllSlots() {
+		List<Slot> copy = null;
+
 		synchronized (instanceLock) {
 			// we need to do this copy because of concurrent modification exceptions
-			List<AllocatedSlot> copy = new ArrayList<AllocatedSlot>(this.allocatedSlots);
+			copy = new ArrayList<Slot>(this.allocatedSlots);
+		}
 			
-			for (AllocatedSlot slot : copy) {
-				slot.releaseSlot();
-			}
-			allocatedSlots.clear();
+		for (Slot slot : copy) {
+			slot.releaseSlot();
 		}
 	}
 
@@ -293,6 +330,6 @@ public class Instance implements Serializable {
 	@Override
 	public String toString() {
 		return instanceId + " @" + (taskManager != null ? taskManager.path() : "ActorRef.noSender") + " " +
-				numberOfSlots + " slots";
+				numberOfSlots + " slots" + " - " + hashCode();
 	}
 }
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/instance/SharedSlot.java b/flink-runtime/src/main/java/org/apache/flink/runtime/instance/SharedSlot.java
new file mode 100644
index 00000000000..2efcf6ca039
--- /dev/null
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/instance/SharedSlot.java
@@ -0,0 +1,154 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.instance;
+
+import org.apache.flink.runtime.AbstractID;
+import org.apache.flink.runtime.jobgraph.JobID;
+import org.apache.flink.runtime.jobmanager.scheduler.SlotSharingGroupAssignment;
+
+import java.util.HashSet;
+import java.util.Set;
+
+/**
+ * This class represents a shared slot. A shared slot can have multiple
+ * {@link org.apache.flink.runtime.instance.SimpleSlot} instances within itself. This allows to
+ * schedule multiple tasks simultaneously, enabling Flink's streaming capabilities.
+ *
+ * IMPORTANT: This class contains no synchronization. Thus, the caller has to guarantee proper
+ * synchronization. In the current implementation, all concurrently modifying operations are
+ * passed through a {@link SlotSharingGroupAssignment} object which is responsible for
+ * synchronization.
+ *
+ */
+public class SharedSlot extends Slot {
+
+	private final SlotSharingGroupAssignment assignmentGroup;
+
+	private final Set<Slot> subSlots;
+
+	public SharedSlot(JobID jobID, Instance instance, int slotNumber,
+					SlotSharingGroupAssignment assignmentGroup, SharedSlot parent,
+					AbstractID groupID) {
+		super(jobID, instance, slotNumber, parent, groupID);
+
+		this.assignmentGroup = assignmentGroup;
+		this.subSlots = new HashSet<Slot>();
+	}
+
+	public Set<Slot> getSubSlots() {
+		return subSlots;
+	}
+
+	/**
+	 * Removes the simple slot from the {@link org.apache.flink.runtime.instance.SharedSlot}. Should
+	 * only be called through the
+	 * {@link org.apache.flink.runtime.jobmanager.scheduler.SlotSharingGroupAssignment} attribute
+	 * assignmnetGroup.
+	 *
+	 * @param slot slot to be removed from the set of sub slots.
+	 * @return Number of remaining sub slots
+	 */
+	public int freeSubSlot(Slot slot){
+		if(!subSlots.remove(slot)){
+			throw new IllegalArgumentException("Wrong shared slot for sub slot.");
+		}
+
+		return subSlots.size();
+	}
+
+	@Override
+	public int getNumberLeaves() {
+		int result = 0;
+
+		for(Slot slot: subSlots){
+			result += slot.getNumberLeaves();
+		}
+
+		return result;
+	}
+
+	@Override
+	public void cancel() {
+		// Guarantee that the operation is only executed once
+		if (markCancelled()) {
+			assignmentGroup.releaseSharedSlot(this);
+		}
+	}
+
+	/**
+	 * Release this shared slot. In order to do this:
+	 *
+	 * 1. Cancel and release all sub slots atomically with respect to the assigned assignment group.
+	 * 2. Set the state of the shared slot to be cancelled.
+	 * 3. Dispose the shared slot (returning the slot to the instance).
+	 *
+	 * After cancelAndReleaseSubSlots, the shared slot is marked to be dead. This prevents further
+	 * sub slot creation by the scheduler.
+	 */
+	@Override
+	public void releaseSlot() {
+		assignmentGroup.releaseSharedSlot(this);
+	}
+
+	/**
+	 * Creates a new sub slot if the slot is not dead, yet. This method should only be called from
+	 * the assignment group instance to guarantee synchronization.
+	 *
+	 * @param jID id to identify tasks which can be deployed in this sub slot
+	 * @return new sub slot if the shared slot is still alive, otherwise null
+	 */
+	public SimpleSlot allocateSubSlot(AbstractID jID){
+		if(isDead()){
+			return null;
+		} else {
+			SimpleSlot slot = new SimpleSlot(jobID, instance, subSlots.size(), this, jID);
+			subSlots.add(slot);
+
+			return slot;
+		}
+	}
+
+	public SharedSlot allocateSharedSlot(AbstractID jID){
+		if(isDead()){
+			return null;
+		} else {
+			SharedSlot slot = new SharedSlot(jobID, instance, subSlots.size(), assignmentGroup, this, jID);
+			subSlots.add(slot);
+
+			return slot;
+		}
+	}
+
+	/**
+	 * Disposes the given sub slot. This
+	 * is done by the means of the assignmentGroup in order to synchronize the method. If the
+	 * disposed slot was the last sub slot, then the shared slot is marked to be cancelled and is
+	 * disposed/returned to the owning instance.
+	 *
+	 * @param slot sub slot which shall be removed from the shared slot
+	 */
+	public void disposeChild(SimpleSlot slot){
+		assignmentGroup.releaseSimpleSlot(slot);
+	}
+
+	@Override
+	public String toString() {
+		return "Shared " + super.toString();
+	}
+}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/instance/SimpleSlot.java b/flink-runtime/src/main/java/org/apache/flink/runtime/instance/SimpleSlot.java
new file mode 100644
index 00000000000..5b1af579e19
--- /dev/null
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/instance/SimpleSlot.java
@@ -0,0 +1,124 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.instance;
+
+import org.apache.flink.runtime.AbstractID;
+import org.apache.flink.runtime.executiongraph.Execution;
+import org.apache.flink.runtime.jobgraph.JobID;
+import org.apache.flink.runtime.jobmanager.scheduler.Locality;
+
+import java.util.concurrent.atomic.AtomicReferenceFieldUpdater;
+
+/**
+ * Class which represents a single slot on a machine or within a shared slot. If this slot is part
+ * of a [[SharedSlot]], then its parent attribute is set to this instance. If not, then the parent
+ * attribute is null.
+ *
+ * IMPORTANT: This class has no synchronization. Thus it has to be synchronized by the calling
+ * object.
+ */
+public class SimpleSlot extends Slot {
+
+	private static final AtomicReferenceFieldUpdater<SimpleSlot, Execution> VERTEX_UPDATER =
+			AtomicReferenceFieldUpdater.newUpdater(SimpleSlot.class, Execution.class, "executedTask");
+
+	/** Task being executed in the slot. Volatile to force a memory barrier and allow for correct double-checking */
+	private volatile Execution executedTask;
+
+	private Locality locality = Locality.UNCONSTRAINED;
+
+	public SimpleSlot(JobID jobID, Instance instance, int slotNumber, SharedSlot parent, AbstractID groupID){
+		super(jobID, instance, slotNumber, parent, groupID);
+	}
+
+	@Override
+	public int getNumberLeaves() {
+		return 1;
+	}
+
+
+	public Execution getExecution() {
+		return executedTask;
+	}
+
+	public Locality getLocality() {
+		return locality;
+	}
+
+	public void setLocality(Locality locality) {
+		this.locality = locality;
+	}
+
+	public boolean setExecutedVertex(Execution executedVertex) {
+		if (executedVertex == null) {
+			throw new NullPointerException();
+		}
+
+		// check that we can actually run in this slot
+		if (status != ALLOCATED_AND_ALIVE) {
+			return false;
+		}
+
+		// atomically assign the vertex
+		if (!VERTEX_UPDATER.compareAndSet(this, null, executedVertex)) {
+			return false;
+		}
+
+		// we need to do a double check that we were not cancelled in the meantime
+		if (status != ALLOCATED_AND_ALIVE) {
+			this.executedTask = null;
+			return false;
+		}
+
+		return true;
+	}
+
+	@Override
+	public void cancel() {
+		if (markCancelled()) {
+			// kill all tasks currently running in this slot
+			Execution exec = this.executedTask;
+			if (exec != null && !exec.isFinished()) {
+				exec.fail(new Exception("The slot in which the task was scheduled has been killed (probably loss of TaskManager)."));
+			}
+		}
+	}
+
+	@Override
+	public void releaseSlot() {
+		// cancel everything, if there is something. since this is atomically status based,
+		// it will not happen twice if another attempt happened before or concurrently
+		try {
+			cancel();
+		} finally {
+			if (getParent() != null) {
+				// we have to ask our parent to dispose us
+				getParent().disposeChild(this);
+			} else {
+				// we have to give back the slot to the owning instance
+				instance.returnAllocatedSlot(this);
+			}
+		}
+	}
+
+	@Override
+	public String toString() {
+		return "SimpleSlot " + super.toString();
+	}
+}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/instance/Slot.java b/flink-runtime/src/main/java/org/apache/flink/runtime/instance/Slot.java
new file mode 100644
index 00000000000..fb62c4c55c9
--- /dev/null
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/instance/Slot.java
@@ -0,0 +1,191 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.instance;
+
+import org.apache.flink.runtime.AbstractID;
+import org.apache.flink.runtime.jobgraph.JobID;
+
+import java.util.concurrent.atomic.AtomicIntegerFieldUpdater;
+
+/**
+ * Base class for slots.
+ */
+public abstract class Slot {
+	protected static final AtomicIntegerFieldUpdater<Slot> STATUS_UPDATER =
+			AtomicIntegerFieldUpdater.newUpdater(Slot.class, "status");
+
+	protected static final int ALLOCATED_AND_ALIVE = 0;		// tasks may be added and might be running
+	protected static final int CANCELLED = 1;					// no more tasks may run
+	protected static final int RELEASED = 2;					// has been given back to the instance
+
+	/** The ID of the job this slice belongs to. */
+	protected final JobID jobID;
+
+	/** The instance on which the slot is allocated */
+	protected final Instance instance;
+
+	/** The number of the slot on which the task is deployed */
+	protected final int slotNumber;
+
+	/** The state of the vertex, only atomically updated */
+	protected volatile int status = ALLOCATED_AND_ALIVE;
+
+	/** Indicates whether this slot was marked dead by the system */
+	private boolean dead = false;
+
+	private final AbstractID groupID;
+
+	private final SharedSlot parent;
+
+	private boolean disposed = false;
+
+
+	public Slot(JobID jobID, Instance instance, int slotNumber, SharedSlot parent, AbstractID groupID) {
+		if (jobID == null || instance == null || slotNumber < 0) {
+			throw new IllegalArgumentException();
+		}
+
+		this.jobID = jobID;
+		this.instance = instance;
+		this.slotNumber = slotNumber;
+		this.parent = parent;
+		this.groupID = groupID;
+
+	}
+	// --------------------------------------------------------------------------------------------
+
+	/**
+	 * Returns the ID of the job this allocated slot belongs to.
+	 *
+	 * @return the ID of the job this allocated slot belongs to
+	 */
+	public JobID getJobID() {
+		return this.jobID;
+	}
+
+	public Instance getInstance() {
+		return instance;
+	}
+
+	public int getSlotNumber() {
+		return slotNumber;
+	}
+
+	public AbstractID getGroupID() {
+		return groupID;
+	}
+
+	public SharedSlot getParent() {
+		return parent;
+	}
+
+	public Slot getRoot() {
+		if(parent == null){
+			return this;
+		} else {
+			return parent.getRoot();
+		}
+	}
+
+	public abstract int getNumberLeaves();
+
+	// --------------------------------------------------------------------------------------------
+	//  Status and life cycle
+	// --------------------------------------------------------------------------------------------
+
+	public boolean isAlive() {
+		return status == ALLOCATED_AND_ALIVE;
+	}
+
+	public boolean isCanceled() {
+		return status != ALLOCATED_AND_ALIVE;
+	}
+
+	public boolean isReleased() {
+		return status == RELEASED;
+	}
+
+	public abstract void cancel();
+
+	public abstract void releaseSlot();
+
+	public boolean markReleased() {
+		return STATUS_UPDATER.compareAndSet(this, CANCELLED, RELEASED);
+	}
+
+	public boolean markCancelled() {
+		return STATUS_UPDATER.compareAndSet(this, ALLOCATED_AND_ALIVE, CANCELLED);
+	}
+
+	/**
+	 * Marks this shared slot to be dead. Returns if the slot was alive before. Should only
+	 * be called through the {@link org.apache.flink.runtime.jobmanager.scheduler.SlotSharingGroupAssignment} attribute assignmentGroup.
+	 *
+	 * @return if the slot was alive before
+	 */
+	public boolean markDead() {
+		boolean result = !dead;
+
+		dead = true;
+
+		return result;
+	}
+
+	public boolean isDead() {
+		return dead;
+	}
+
+	public boolean markDisposed() {
+		boolean result = !disposed;
+
+		disposed = true;
+
+		return result;
+	}
+
+	public boolean isDisposed() {
+		return disposed;
+	}
+
+	// --------------------------------------------------------------------------------------------
+	//  Utilities
+	// --------------------------------------------------------------------------------------------
+
+	@Override
+	public String toString() {
+		return hierarchy() + " - " + instance.getId() + " - " + getStateName(status);
+	}
+
+	protected String hierarchy() {
+		return "(" + slotNumber + ")" + (getParent() != null ? getParent().hierarchy() : "");
+	}
+
+	private static final String getStateName(int state) {
+		switch (state) {
+			case ALLOCATED_AND_ALIVE:
+				return "ALLOCATED/ALIVE";
+			case CANCELLED:
+				return "CANCELLED";
+			case RELEASED:
+				return "RELEASED";
+			default:
+				return "(unknown)";
+		}
+	}
+}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/CoLocationConstraint.java b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/CoLocationConstraint.java
index 739ec09d773..8ef61b9263a 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/CoLocationConstraint.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/CoLocationConstraint.java
@@ -22,6 +22,7 @@ import org.apache.flink.runtime.AbstractID;
 import org.apache.flink.runtime.instance.Instance;
 
 import com.google.common.base.Preconditions;
+import org.apache.flink.runtime.instance.SharedSlot;
 
 import java.io.Serializable;
 
@@ -46,7 +47,7 @@ public class CoLocationConstraint implements Serializable {
 	
 	public Instance getLocation() {
 		if (sharedSlot != null) {
-			return sharedSlot.getAllocatedSlot().getInstance();
+			return sharedSlot.getInstance();
 		} else {
 			throw new IllegalStateException("Not assigned");
 		}
@@ -56,7 +57,7 @@ public class CoLocationConstraint implements Serializable {
 		if (this.sharedSlot == sharedSlot) {
 			return;
 		}
-		else if (this.sharedSlot == null || this.sharedSlot.isDisposed()) {
+		else if (this.sharedSlot == null || this.sharedSlot.isDead()) {
 			this.sharedSlot = sharedSlot;
 		} else {
 			throw new IllegalStateException("Overriding shared slot that is still alive.");
@@ -68,12 +69,17 @@ public class CoLocationConstraint implements Serializable {
 	}
 	
 	public boolean isUnassignedOrDisposed() {
-		return this.sharedSlot == null || this.sharedSlot.isDisposed();
+		return this.sharedSlot == null || this.sharedSlot.isDead();
 	}
 	
 	public AbstractID getGroupId() {
 		return this.group.getId();
 	}
+
+	@Override
+	public String toString() {
+		return "CoLocation constraint id " + getGroupId() + " shared slot " + sharedSlot;
+	}
 	
 
 }
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/NoResourceAvailableException.java b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/NoResourceAvailableException.java
index 730952b1e9c..93b4541a5c0 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/NoResourceAvailableException.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/NoResourceAvailableException.java
@@ -36,14 +36,19 @@ public class NoResourceAvailableException extends JobException {
 				+ ". You can decrease the operator parallelism or increase the number of slots per TaskManager in the configuration.");
 	}
 	
-	public NoResourceAvailableException(int numInstances, int numSlotsTotal) {
-		super(String.format("%s Resources available to scheduler: Number of instances=%d, total number of slots=%d", 
-				BASE_MESSAGE, numInstances, numSlotsTotal));
+	public NoResourceAvailableException(int numInstances, int numSlotsTotal, int availableSlots) {
+		super(String.format("%s Resources available to scheduler: Number of instances=%d, total number of slots=%d, available slots=%d",
+				BASE_MESSAGE, numInstances, numSlotsTotal, availableSlots));
 	}
 	
-	NoResourceAvailableException(ScheduledUnit task, int numInstances, int numSlotsTotal) {
-		super(String.format("%s Task to schedule: < %s > in sharing group < %s >. Resources available to scheduler: Number of instances=%d, total number of slots=%d", 
-				BASE_MESSAGE, task.getTaskToExecute(), task.getSlotSharingGroup(), numInstances, numSlotsTotal));
+	NoResourceAvailableException(ScheduledUnit task, int numInstances, int numSlotsTotal, int availableSlots) {
+		super(String.format("%s Task to schedule: < %s > with groupID < %s > in sharing group < %s >. Resources available to scheduler: Number of instances=%d, total number of slots=%d, available slots=%d",
+				BASE_MESSAGE, task.getTaskToExecute(),
+				task.getLocationConstraint() == null ? task.getTaskToExecute().getVertex().getJobvertexId() : task.getLocationConstraint().getGroupId(),
+				task.getSlotSharingGroup(),
+				numInstances,
+				numSlotsTotal,
+				availableSlots));
 	}
 
 	public NoResourceAvailableException(String message) {
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/Scheduler.java b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/Scheduler.java
index 6495abad921..c237aa55f43 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/Scheduler.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/Scheduler.java
@@ -29,11 +29,15 @@ import java.util.concurrent.Callable;
 import java.util.concurrent.LinkedBlockingQueue;
 
 import akka.dispatch.Futures;
+import org.apache.commons.lang3.tuple.ImmutablePair;
+import org.apache.commons.lang3.tuple.Pair;
 import org.apache.flink.runtime.akka.AkkaUtils;
+import org.apache.flink.runtime.AbstractID;
+import org.apache.flink.runtime.instance.SharedSlot;
+import org.apache.flink.runtime.instance.SimpleSlot;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import org.apache.flink.runtime.executiongraph.ExecutionVertex;
-import org.apache.flink.runtime.instance.AllocatedSlot;
 import org.apache.flink.runtime.instance.Instance;
 import org.apache.flink.runtime.instance.InstanceDiedException;
 import org.apache.flink.runtime.instance.InstanceListener;
@@ -124,10 +128,10 @@ public class Scheduler implements InstanceListener, SlotAvailabilityListener {
 	//  Scheduling
 	// --------------------------------------------------------------------------------------------
 	
-	public AllocatedSlot scheduleImmediately(ScheduledUnit task) throws NoResourceAvailableException {
+	public SimpleSlot scheduleImmediately(ScheduledUnit task) throws NoResourceAvailableException {
 		Object ret = scheduleTask(task, false);
-		if (ret instanceof AllocatedSlot) {
-			return (AllocatedSlot) ret;
+		if (ret instanceof SimpleSlot) {
+			return (SimpleSlot) ret;
 		}
 		else {
 			throw new RuntimeException();
@@ -136,8 +140,8 @@ public class Scheduler implements InstanceListener, SlotAvailabilityListener {
 	
 	public SlotAllocationFuture scheduleQueued(ScheduledUnit task) throws NoResourceAvailableException {
 		Object ret = scheduleTask(task, true);
-		if (ret instanceof AllocatedSlot) {
-			return new SlotAllocationFuture((AllocatedSlot) ret);
+		if (ret instanceof SimpleSlot) {
+			return new SlotAllocationFuture((SimpleSlot) ret);
 		}
 		if (ret instanceof SlotAllocationFuture) {
 			return (SlotAllocationFuture) ret;
@@ -148,7 +152,7 @@ public class Scheduler implements InstanceListener, SlotAvailabilityListener {
 	}
 	
 	/**
-	 * Returns either an {@link AllocatedSlot}, or an {@link SlotAllocationFuture}.
+	 * Returns either an {@link org.apache.flink.runtime.instance.SimpleSlot}, or an {@link SlotAllocationFuture}.
 	 */
 	private Object scheduleTask(ScheduledUnit task, boolean queueIfNoResource) throws NoResourceAvailableException {
 		if (task == null) {
@@ -158,7 +162,7 @@ public class Scheduler implements InstanceListener, SlotAvailabilityListener {
 		if (LOG.isDebugEnabled()) {
 			LOG.debug("Scheduling task " + task);
 		}
-		
+
 		final ExecutionVertex vertex = task.getTaskToExecute().getVertex();
 	
 		synchronized (globalLock) {
@@ -176,15 +180,15 @@ public class Scheduler implements InstanceListener, SlotAvailabilityListener {
 				final CoLocationConstraint constraint = task.getLocationConstraint();
 				
 				// get a slot from the group, if the group has one for us (and can fulfill the constraint)
-				SubSlot slotFromGroup;
+				SimpleSlot slotFromGroup;
 				if (constraint == null) {
 					slotFromGroup = assignment.getSlotForTask(vertex);
 				}
 				else {
 					slotFromGroup = assignment.getSlotForTask(vertex, constraint);
 				}
-				
-				AllocatedSlot newSlot = null;
+
+				SimpleSlot newSlot = null;
 				
 				// the following needs to make sure any allocated slot is released in case of an error
 				try {
@@ -202,15 +206,15 @@ public class Scheduler implements InstanceListener, SlotAvailabilityListener {
 							vertex.getPreferredLocations() : Collections.singleton(constraint.getLocation());
 					
 					// get a new slot, since we could not place it into the group, or we could not place it locally
-					newSlot = getFreeSlotForTask(vertex, locations);
-					
-					SubSlot toUse;
+					newSlot = getFreeSubSlotForTask(vertex, locations, assignment, constraint);
+
+					SimpleSlot toUse;
 					
 					if (newSlot == null) {
 						if (slotFromGroup == null) {
 							// both null
 							if (constraint == null || constraint.isUnassigned()) {
-								throw new NoResourceAvailableException(task, getNumberOfAvailableInstances(), getTotalNumberOfSlots());
+								throw new NoResourceAvailableException(task, getNumberOfAvailableInstances(), getTotalNumberOfSlots(), getNumberOfAvailableSlots());
 							} else {
 								throw new NoResourceAvailableException("Could not allocate a slot on instance " + 
 											constraint.getLocation() + ", as required by the co-location constraint.");
@@ -226,11 +230,7 @@ public class Scheduler implements InstanceListener, SlotAvailabilityListener {
 							slotFromGroup.releaseSlot();
 						}
 						
-						if (constraint == null) {
-							toUse = assignment.addNewSlotWithTask(newSlot, vertex);
-						} else {
-							toUse = assignment.addNewSlotWithTask(newSlot, vertex, constraint);
-						}
+						toUse = newSlot;
 					}
 					else {
 						// both are available and usable. neither is local
@@ -242,7 +242,7 @@ public class Scheduler implements InstanceListener, SlotAvailabilityListener {
 					// if it was assigned before and the new one is not local, it is a fail
 					if (constraint != null) {
 						if (constraint.isUnassigned() || toUse.getLocality() == Locality.LOCAL) {
-							constraint.setSharedSlot(toUse.getSharedSlot());
+							constraint.setSharedSlot(toUse.getParent());
 						} else {
 							// the fail
 							throw new NoResourceAvailableException("Could not allocate a slot on instance " + 
@@ -270,7 +270,7 @@ public class Scheduler implements InstanceListener, SlotAvailabilityListener {
 		
 			// 2) === schedule without hints and sharing ===
 			
-			AllocatedSlot slot = getFreeSlotForTask(vertex, vertex.getPreferredLocations());
+			SimpleSlot slot = getFreeSlotForTask(vertex, vertex.getPreferredLocations());
 			if (slot != null) {
 				updateLocalityCounters(slot.getLocality());
 				return slot;
@@ -283,7 +283,7 @@ public class Scheduler implements InstanceListener, SlotAvailabilityListener {
 					return future;
 				}
 				else {
-					throw new NoResourceAvailableException(getNumberOfAvailableInstances(), getTotalNumberOfSlots());
+					throw new NoResourceAvailableException(getNumberOfAvailableInstances(), getTotalNumberOfSlots(), getNumberOfAvailableSlots());
 				}
 			}
 		}
@@ -297,69 +297,96 @@ public class Scheduler implements InstanceListener, SlotAvailabilityListener {
 	 * @param vertex The task to run. 
 	 * @return The instance to run the vertex on, it {@code null}, if no instance is available.
 	 */
-	protected AllocatedSlot getFreeSlotForTask(ExecutionVertex vertex, Iterable<Instance> requestedLocations) {
+	protected SimpleSlot getFreeSlotForTask(ExecutionVertex vertex, Iterable<Instance> requestedLocations) {
 		
 		// we need potentially to loop multiple times, because there may be false positives
 		// in the set-with-available-instances
 		while (true) {
-			if (this.instancesWithAvailableResources.isEmpty()) {
-				// check if the asynchronous calls did not yet return the queues
-				Instance queuedInstance = this.newlyAvailableInstances.poll();
-				if (queuedInstance == null) {
-					return null;
-				} else {
-					this.instancesWithAvailableResources.add(queuedInstance);
+			Pair<Instance, Locality> instanceLocalityPair = findInstance(requestedLocations);
+
+			if(instanceLocalityPair == null){
+				return null;
+			}
+
+			Instance instanceToUse = instanceLocalityPair.getLeft();
+			Locality locality = instanceLocalityPair.getRight();
+
+			if(LOG.isDebugEnabled()){
+				if(locality == Locality.LOCAL){
+					LOG.debug("Local assignment: " + vertex.getSimpleName() + " --> " + instanceToUse);
+				}else if(locality == Locality.NON_LOCAL){
+					LOG.debug("Non-local assignment: " + vertex.getSimpleName() + " --> " + instanceToUse);
+				}else if(locality == Locality.UNCONSTRAINED) {
+					LOG.debug("Unconstrained assignment: " + vertex.getSimpleName() + " --> " + instanceToUse);
 				}
 			}
-			
-			Iterator<Instance> locations = requestedLocations == null ? null : requestedLocations.iterator();
-			
-			Instance instanceToUse = null;
-			Locality locality = Locality.UNCONSTRAINED;
-			
-			if (locations != null && locations.hasNext()) {
-				// we have a locality preference
+
+			try {
+				SimpleSlot slot = instanceToUse.allocateSimpleSlot(vertex.getJobId(), vertex.getJobvertexId());
 				
-				while (locations.hasNext()) {
-					Instance location = locations.next();
-					
-					if (location != null && this.instancesWithAvailableResources.remove(location)) {
-						instanceToUse = location;
-						locality = Locality.LOCAL;
-						
-						if (LOG.isDebugEnabled()) {
-							LOG.debug("Local assignment: " + vertex.getSimpleName() + " --> " + location);
-						}
-						
-						break;
-					}
+				// if the instance has further available slots, re-add it to the set of available resources.
+				if (instanceToUse.hasResourcesAvailable()) {
+					this.instancesWithAvailableResources.add(instanceToUse);
 				}
 				
-				if (instanceToUse == null) {
-					instanceToUse = this.instancesWithAvailableResources.poll();
-					locality = Locality.NON_LOCAL;
-					if (LOG.isDebugEnabled()) {
-						LOG.debug("Non-local assignment: " + vertex.getSimpleName() + " --> " + instanceToUse);
-					}
+				if (slot != null) {
+					slot.setLocality(locality);
+					return slot;
 				}
 			}
-			else {
-				instanceToUse = this.instancesWithAvailableResources.poll();
-				if (LOG.isDebugEnabled()) {
+			catch (InstanceDiedException e) {
+				// the instance died it has not yet been propagated to this scheduler
+				// remove the instance from the set of available instances
+				this.allInstances.remove(instanceToUse);
+				this.instancesWithAvailableResources.remove(instanceToUse);
+			}
+			
+			// if we failed to get a slot, fall through the loop
+		}
+	}
+
+	protected SimpleSlot getFreeSubSlotForTask(ExecutionVertex vertex,
+											Iterable<Instance> requestedLocations,
+											SlotSharingGroupAssignment groupAssignment,
+											CoLocationConstraint constraint) {
+		// we need potentially to loop multiple times, because there may be false positives
+		// in the set-with-available-instances
+		while (true) {
+			Pair<Instance, Locality> instanceLocalityPair = findInstance(requestedLocations);
+
+			if(instanceLocalityPair == null){
+				return null;
+			}
+
+			Instance instanceToUse = instanceLocalityPair.getLeft();
+			Locality locality = instanceLocalityPair.getRight();
+
+			if(LOG.isDebugEnabled()){
+				if(locality == Locality.LOCAL){
+					LOG.debug("Local assignment: " + vertex.getSimpleName() + " --> " + instanceToUse);
+				}else if(locality == Locality.NON_LOCAL){
+					LOG.debug("Non-local assignment: " + vertex.getSimpleName() + " --> " + instanceToUse);
+				}else if(locality == Locality.UNCONSTRAINED) {
 					LOG.debug("Unconstrained assignment: " + vertex.getSimpleName() + " --> " + instanceToUse);
 				}
 			}
-			
+
 			try {
-				AllocatedSlot slot = instanceToUse.allocateSlot(vertex.getJobId());
-				
+				AbstractID groupID = constraint == null ? vertex.getJobvertexId() : constraint.getGroupId();
+
+				// root SharedSlot
+				SharedSlot sharedSlot = instanceToUse.allocateSharedSlot(vertex.getJobId(), groupAssignment, groupID);
+
+				// If constraint != null, then slot nested in a SharedSlot nested in sharedSlot
+				// If constraint == null, then slot nested in sharedSlot
+				SimpleSlot slot = groupAssignment.addSharedSlotAndAllocateSubSlot(sharedSlot, locality, groupID, constraint);
+
 				// if the instance has further available slots, re-add it to the set of available resources.
 				if (instanceToUse.hasResourcesAvailable()) {
 					this.instancesWithAvailableResources.add(instanceToUse);
 				}
-				
+
 				if (slot != null) {
-					slot.setLocality(locality);
 					return slot;
 				}
 			}
@@ -369,10 +396,61 @@ public class Scheduler implements InstanceListener, SlotAvailabilityListener {
 				this.allInstances.remove(instanceToUse);
 				this.instancesWithAvailableResources.remove(instanceToUse);
 			}
-			
+
 			// if we failed to get a slot, fall through the loop
 		}
 	}
+
+	/**
+	 * NOTE: This method is not thread-safe, it needs to be synchronized by the caller.
+	 *
+	 * Tries to find a requested instance. If no such instance is available it will return a non-
+	 * local instance. If no such instance exists (all slots occupied), then return null.
+	 *
+	 * @param requestedLocations
+	 * @return
+	 */
+	private Pair<Instance, Locality> findInstance(Iterable<Instance> requestedLocations){
+		if (this.instancesWithAvailableResources.isEmpty()) {
+			// check if the asynchronous calls did not yet return the queues
+			Instance queuedInstance = this.newlyAvailableInstances.poll();
+			if (queuedInstance == null) {
+				return null;
+			} else {
+				this.instancesWithAvailableResources.add(queuedInstance);
+			}
+		}
+
+		Iterator<Instance> locations = requestedLocations == null ? null : requestedLocations.iterator();
+
+		Instance instanceToUse = null;
+		Locality locality = Locality.UNCONSTRAINED;
+
+		if (locations != null && locations.hasNext()) {
+			// we have a locality preference
+
+			while (locations.hasNext()) {
+				Instance location = locations.next();
+
+				if (location != null && this.instancesWithAvailableResources.remove(location)) {
+					instanceToUse = location;
+					locality = Locality.LOCAL;
+
+					break;
+				}
+			}
+
+			if (instanceToUse == null) {
+				instanceToUse = this.instancesWithAvailableResources.poll();
+				locality = Locality.NON_LOCAL;
+			}
+		}
+		else {
+			instanceToUse = this.instancesWithAvailableResources.poll();
+		}
+
+		return new ImmutablePair<Instance, Locality>(instanceToUse, locality);
+	}
 	
 	@Override
 	public void newSlotAvailable(final Instance instance) {
@@ -386,7 +464,7 @@ public class Scheduler implements InstanceListener, SlotAvailabilityListener {
 		//                             (2) scheduler (to check whether to take a new task item
 		// 
 		// that leads with a high probability to deadlocks, when scheduling fast
-		
+
 		this.newlyAvailableInstances.add(instance);
 
 		Futures.future(new Callable<Object>() {
@@ -416,7 +494,7 @@ public class Scheduler implements InstanceListener, SlotAvailabilityListener {
 				ExecutionVertex vertex = task.getTaskToExecute().getVertex();
 				
 				try {
-					AllocatedSlot newSlot = instance.allocateSlot(vertex.getJobId());
+					SimpleSlot newSlot = instance.allocateSimpleSlot(vertex.getJobId(), vertex.getJobvertexId());
 					if (newSlot != null) {
 						
 						// success, remove from the task queue and notify the future
@@ -524,7 +602,16 @@ public class Scheduler implements InstanceListener, SlotAvailabilityListener {
 	// --------------------------------------------------------------------------------------------
 
 	public int getNumberOfAvailableInstances() {
-		return allInstances.size();
+		int numberAvailableInstances = 0;
+		synchronized (this.globalLock) {
+			for(Instance instance: allInstances){
+				if(instance.isAlive()){
+					numberAvailableInstances++;
+				}
+			}
+		}
+
+		return numberAvailableInstances;
 	}
 	
 	public int getNumberOfInstancesWithAvailableSlots() {
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/SharedSlot.java b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/SharedSlot.java
deleted file mode 100644
index 1ce8465fdaf..00000000000
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/SharedSlot.java
+++ /dev/null
@@ -1,114 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.runtime.jobmanager.scheduler;
-
-import java.io.Serializable;
-import java.util.HashSet;
-import java.util.Set;
-
-import org.apache.flink.runtime.instance.AllocatedSlot;
-import org.apache.flink.runtime.jobgraph.JobVertexID;
-
-/**
- * 
- * NOTE: This class does no synchronization by itself and its mutating
- *       methods may only be called from within the synchronization scope of
- *       it associated SlotSharingGroupAssignment.
- */
-class SharedSlot implements Serializable {
-
-	static final long serialVersionUID = 42L;
-
-	private final AllocatedSlot allocatedSlot;
-	
-	private final SlotSharingGroupAssignment assignmentGroup;
-	
-	private final Set<SubSlot> subSlots;
-	
-	private int subSlotNumber;
-	
-	private volatile boolean disposed;
-	
-	// --------------------------------------------------------------------------------------------
-	
-	public SharedSlot(AllocatedSlot allocatedSlot, SlotSharingGroupAssignment assignmentGroup) {
-		if (allocatedSlot == null || assignmentGroup == null) {
-			throw new NullPointerException();
-		}
-		
-		this.allocatedSlot = allocatedSlot;
-		this.assignmentGroup = assignmentGroup;
-		this.subSlots = new HashSet<SubSlot>();
-	}
-	
-	// --------------------------------------------------------------------------------------------
-	
-	AllocatedSlot getAllocatedSlot() {
-		return this.allocatedSlot;
-	}
-	
-	boolean isDisposed() {
-		return disposed;
-	}
-	
-	int getNumberOfAllocatedSubSlots() {
-		return this.subSlots.size();
-	}
-	
-	SubSlot allocateSubSlot(JobVertexID jid) {
-		if (disposed) {
-			return null;
-		} else {
-			SubSlot ss = new SubSlot(this, subSlotNumber++, jid);
-			this.subSlots.add(ss);
-			return ss;
-		}
-	}
-	
-	void returnAllocatedSlot(SubSlot slot) {
-		if (!slot.isReleased()) {
-			throw new IllegalArgumentException("SubSlot is not released.");
-		}
-		
-		this.assignmentGroup.releaseSubSlot(slot, this);
-	}
-	
-	int releaseSlot(SubSlot slot) {
-		if (!this.subSlots.remove(slot)) {
-			throw new IllegalArgumentException("Wrong shared slot for subslot.");
-		}
-		return subSlots.size();
-	}
-	
-	void dispose() {
-		if (subSlots.isEmpty()) {
-			disposed = true;
-			this.allocatedSlot.releaseSlot();
-		} else {
-			throw new IllegalStateException("Cannot dispose while subslots are still alive.");
-		}
-	}
-	
-	// --------------------------------------------------------------------------------------------
-	
-	@Override
-	public String toString() {
-		return "Shared " + allocatedSlot.toString();
-	}
-}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/SlotAllocationFuture.java b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/SlotAllocationFuture.java
index eb5f9fb7af7..31bd3417f84 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/SlotAllocationFuture.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/SlotAllocationFuture.java
@@ -18,13 +18,13 @@
 
 package org.apache.flink.runtime.jobmanager.scheduler;
 
-import org.apache.flink.runtime.instance.AllocatedSlot;
+import org.apache.flink.runtime.instance.SimpleSlot;
 
 public class SlotAllocationFuture {
 	
 	private final Object monitor = new Object();
 	
-	private volatile AllocatedSlot slot;
+	private volatile SimpleSlot slot;
 	
 	private volatile SlotAllocationFutureAction action;
 	
@@ -32,17 +32,17 @@ public class SlotAllocationFuture {
 
 	public SlotAllocationFuture() {}
 	
-	public SlotAllocationFuture(AllocatedSlot slot) {
+	public SlotAllocationFuture(SimpleSlot slot) {
 		this.slot = slot;
 	}
 	
 	// --------------------------------------------------------------------------------------------
 	
-	public AllocatedSlot waitTillAllocated() throws InterruptedException {
+	public SimpleSlot waitTillAllocated() throws InterruptedException {
 		return waitTillAllocated(0);
 	}
 	
-	public AllocatedSlot waitTillAllocated(long timeout) throws InterruptedException {
+	public SimpleSlot waitTillAllocated(long timeout) throws InterruptedException {
 		synchronized (monitor) {
 			while (slot == null) {
 				monitor.wait(timeout);
@@ -66,7 +66,7 @@ public class SlotAllocationFuture {
 		}
 	}
 	
-	public void setSlot(AllocatedSlot slot) {
+	public void setSlot(SimpleSlot slot) {
 		if (slot == null) {
 			throw new NullPointerException();
 		}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/SlotAllocationFutureAction.java b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/SlotAllocationFutureAction.java
index 11137fd6d4d..f9d032f34d6 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/SlotAllocationFutureAction.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/SlotAllocationFutureAction.java
@@ -18,7 +18,7 @@
 
 package org.apache.flink.runtime.jobmanager.scheduler;
 
-import org.apache.flink.runtime.instance.AllocatedSlot;
+import org.apache.flink.runtime.instance.SimpleSlot;
 
 /**
  * An action that is invoked once a {@link SlotAllocationFuture} is triggered.
@@ -30,5 +30,5 @@ public interface SlotAllocationFutureAction {
 	 * 
 	 * @param slot The slot that has been allocated.
 	 */
-	void slotAllocated(AllocatedSlot slot);
+	void slotAllocated(SimpleSlot slot);
 }
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/SlotAvailabilityListener.java b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/SlotAvailabilityListener.java
index 639d2b78200..f75f294105d 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/SlotAvailabilityListener.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/SlotAvailabilityListener.java
@@ -21,7 +21,7 @@ package org.apache.flink.runtime.jobmanager.scheduler;
 import org.apache.flink.runtime.instance.Instance;
 
 /**
- * A SlotAvailabilityListener can be notified when new {@link org.apache.flink.runtime.instance.AllocatedSlot}s become available
+ * A SlotAvailabilityListener can be notified when new {@link org.apache.flink.runtime.instance.AllocatedSlot2}s become available
  * on an {@link org.apache.flink.runtime.instance.Instance}.
  */
 public interface SlotAvailabilityListener {
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/SlotSharingGroupAssignment.java b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/SlotSharingGroupAssignment.java
index 7a0546f15d4..70d4510b30d 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/SlotSharingGroupAssignment.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/SlotSharingGroupAssignment.java
@@ -33,8 +33,10 @@ import org.apache.commons.lang3.tuple.ImmutablePair;
 import org.apache.commons.lang3.tuple.Pair;
 import org.apache.flink.runtime.AbstractID;
 import org.apache.flink.runtime.executiongraph.ExecutionVertex;
-import org.apache.flink.runtime.instance.AllocatedSlot;
 import org.apache.flink.runtime.instance.Instance;
+import org.apache.flink.runtime.instance.SharedSlot;
+import org.apache.flink.runtime.instance.SimpleSlot;
+import org.apache.flink.runtime.instance.Slot;
 import org.apache.flink.runtime.jobgraph.JobVertexID;
 import org.slf4j.Logger;
 
@@ -42,86 +44,82 @@ import org.slf4j.Logger;
 public class SlotSharingGroupAssignment implements Serializable {
 
 	static final long serialVersionUID = 42L;
-	
+
 	private static final Logger LOG = Scheduler.LOG;
-	
+
 	private transient final Object lock = new Object();
-	
+
 	/** All slots currently allocated to this sharing group */
 	private final Set<SharedSlot> allSlots = new LinkedHashSet<SharedSlot>();
-	
+
 	/** The slots available per vertex type (jid), keyed by instance, to make them locatable */
 	private final Map<AbstractID, Map<Instance, List<SharedSlot>>> availableSlotsPerJid = new LinkedHashMap<AbstractID, Map<Instance, List<SharedSlot>>>();
-	
-	
+
 	// --------------------------------------------------------------------------------------------
-	
-	
-	public SubSlot addNewSlotWithTask(AllocatedSlot slot, ExecutionVertex vertex) {
-		JobVertexID id = vertex.getJobvertexId();
-		return addNewSlotWithTask(slot, id, id);
-	}
-	
-	public SubSlot addNewSlotWithTask(AllocatedSlot slot, ExecutionVertex vertex, CoLocationConstraint constraint) {
-		AbstractID groupId = constraint.getGroupId();
-		return addNewSlotWithTask(slot, groupId, null);
-	}
-	
-	private SubSlot addNewSlotWithTask(AllocatedSlot slot, AbstractID groupId, JobVertexID vertexId) {
-		
-		final SharedSlot sharedSlot = new SharedSlot(slot, this);
-		final Instance location = slot.getInstance();
-		
+
+	public SimpleSlot addSharedSlotAndAllocateSubSlot(SharedSlot sharedSlot, Locality locality,
+													AbstractID groupId, CoLocationConstraint constraint) {
+
+		final Instance location = sharedSlot.getInstance();
+
 		synchronized (lock) {
 			// add to the total bookkeeping
 			allSlots.add(sharedSlot);
-			
-			// allocate us a sub slot to return
-			SubSlot subslot = sharedSlot.allocateSubSlot(vertexId);
-			
+
+			SimpleSlot subSlot = null;
+
+			if(constraint == null){
+				// allocate us a sub slot to return
+				subSlot = sharedSlot.allocateSubSlot(groupId);
+			} else {
+				// we need a colocation slot --> a SimpleSlot nested in a SharedSlot to host other colocated tasks
+				SharedSlot constraintGroupSlot = sharedSlot.allocateSharedSlot(groupId);
+				subSlot = constraintGroupSlot.allocateSubSlot(null);
+			}
+
 			// preserve the locality information
-			subslot.setLocality(slot.getLocality());
-			
+			subSlot.setLocality(locality);
+
 			boolean entryForNewJidExists = false;
-			
+
 			// let the other vertex types know about this one as well
 			for (Map.Entry<AbstractID, Map<Instance, List<SharedSlot>>> entry : availableSlotsPerJid.entrySet()) {
-				
+
 				if (entry.getKey().equals(groupId)) {
 					entryForNewJidExists = true;
 					continue;
 				}
-				
+
 				Map<Instance, List<SharedSlot>> available = entry.getValue();
 				putIntoMultiMap(available, location, sharedSlot);
 			}
-			
+
 			// make sure an empty entry exists for this group, if no other entry exists
 			if (!entryForNewJidExists) {
 				availableSlotsPerJid.put(groupId, new LinkedHashMap<Instance, List<SharedSlot>>());
 			}
-			
-			return subslot;
+
+			return subSlot;
 		}
 	}
-	
+
 	/**
 	 * Gets a slot suitable for the given task vertex. This method will prefer slots that are local
 	 * (with respect to {@link ExecutionVertex#getPreferredLocations()}), but will return non local
 	 * slots if no local slot is available. The method returns null, when no slot is available for the
 	 * given JobVertexID at all.
-	 * 
+	 *
 	 * @param vertex
-	 * 
+	 *
 	 * @return A task vertex for a task with the given JobVertexID, or null, if none is available.
 	 */
-	public SubSlot getSlotForTask(ExecutionVertex vertex) {
+	public SimpleSlot getSlotForTask(ExecutionVertex vertex) {
 		synchronized (lock) {
 			Pair<SharedSlot, Locality> p = getSlotForTaskInternal(vertex.getJobvertexId(), vertex, vertex.getPreferredLocations(), false);
-			
+
 			if (p != null) {
 				SharedSlot ss = p.getLeft();
-				SubSlot slot = ss.allocateSubSlot(vertex.getJobvertexId());
+				SimpleSlot slot = ss.allocateSubSlot(vertex.getJobvertexId());
 				slot.setLocality(p.getRight());
 				return slot;
 			}
@@ -129,17 +127,17 @@ public class SlotSharingGroupAssignment implements Serializable {
 				return null;
 			}
 		}
-		
+
 	}
-	
-	public SubSlot getSlotForTask(ExecutionVertex vertex, CoLocationConstraint constraint) {
-		
+
+	public SimpleSlot getSlotForTask(ExecutionVertex vertex, CoLocationConstraint constraint) {
+
 		synchronized (lock) {
 			SharedSlot shared = constraint.getSharedSlot();
-			
-			if (shared != null && !shared.isDisposed()) {
+
+			if (shared != null && !shared.isDead()) {
 				// initialized and set
-				SubSlot subslot = shared.allocateSubSlot(null);
+				SimpleSlot subslot = shared.allocateSubSlot(null);
 				subslot.setLocality(Locality.LOCAL);
 				return subslot;
 			}
@@ -147,85 +145,92 @@ public class SlotSharingGroupAssignment implements Serializable {
 				// not initialized, grab a new slot. preferred locations are defined by the vertex
 				// we only associate the slot with the constraint, if it was a local match
 				Pair<SharedSlot, Locality> p = getSlotForTaskInternal(constraint.getGroupId(), vertex, vertex.getPreferredLocations(), false);
+
 				if (p == null) {
 					return null;
 				} else {
 					shared = p.getLeft();
 					Locality l = p.getRight();
-					
-					SubSlot sub = shared.allocateSubSlot(null);
+
+					// we need a colocation slot --> SimpleSlot nested in a SharedSlot to host other colocated tasks
+					SharedSlot constraintGroupSlot = shared.allocateSharedSlot(constraint.getGroupId());
+					// Depth=3 => groupID==null
+					SimpleSlot sub = constraintGroupSlot.allocateSubSlot(null);
 					sub.setLocality(l);
-					
+
 					if (l != Locality.NON_LOCAL) {
-						constraint.setSharedSlot(shared);
+						constraint.setSharedSlot(constraintGroupSlot);
 					}
 					return sub;
 				}
 			}
 			else {
 				// disposed. get a new slot on the same instance
-				Instance location = shared.getAllocatedSlot().getInstance();
+				Instance location = shared.getInstance();
 				Pair<SharedSlot, Locality> p = getSlotForTaskInternal(constraint.getGroupId(), vertex, Collections.singleton(location), true);
+
 				if (p == null) {
 					return null;
 				} else {
 					shared = p.getLeft();
-					constraint.setSharedSlot(shared);
-					SubSlot subslot = shared.allocateSubSlot(null);
-					subslot.setLocality(Locality.LOCAL);
-					return subslot;
+					// we need colocation slot --> SimpleSlot nested in a SharedSlot to host other colocated tasks
+					SharedSlot constraintGroupSlot = shared.allocateSharedSlot(constraint.getGroupId());
+					constraint.setSharedSlot(constraintGroupSlot);
+					SimpleSlot subSlot = constraintGroupSlot.allocateSubSlot(null);
+					subSlot.setLocality(Locality.LOCAL);
+					return subSlot;
 				}
 			}
 		}
 	}
-	
+
 	/**
 	 * NOTE: This method is not synchronized by itself, needs to be synchronized externally.
-	 * 
+	 *
 	 * @return An allocated sub slot, or {@code null}, if no slot is available.
 	 */
 	private Pair<SharedSlot, Locality> getSlotForTaskInternal(AbstractID groupId, ExecutionVertex vertex, Iterable<Instance> preferredLocations, boolean localOnly) {
+		Map<Instance, List<SharedSlot>> slotsForGroup = availableSlotsPerJid.get(groupId);
+
 		if (allSlots.isEmpty()) {
 			return null;
 		}
-		
-		Map<Instance, List<SharedSlot>> slotsForGroup = availableSlotsPerJid.get(groupId);
-		
+
 		// get the available slots for the group
 		if (slotsForGroup == null) {
 			// no task is yet scheduled for that group, so all slots are available
 			slotsForGroup = new LinkedHashMap<Instance, List<SharedSlot>>();
 			availableSlotsPerJid.put(groupId, slotsForGroup);
-			
+
 			for (SharedSlot availableSlot : allSlots) {
-				putIntoMultiMap(slotsForGroup, availableSlot.getAllocatedSlot().getInstance(), availableSlot);
+				putIntoMultiMap(slotsForGroup, availableSlot.getInstance(), availableSlot);
 			}
 		}
 		else if (slotsForGroup.isEmpty()) {
 			return null;
 		}
-		
+
 		// check whether we can schedule the task to a preferred location
 		boolean didNotGetPreferred = false;
-		
+
 		if (preferredLocations != null) {
 			for (Instance location : preferredLocations) {
-				
+
 				// set the flag that we failed a preferred location. If one will be found,
 				// we return early anyways and skip the flag evaluation
 				didNotGetPreferred = true;
-				
+
 				SharedSlot slot = removeFromMultiMap(slotsForGroup, location);
-				if (slot != null && !slot.isDisposed()) {
+				if (slot != null && !slot.isDead()) {
 					if (LOG.isDebugEnabled()) {
 						LOG.debug("Local assignment in shared group : " + vertex + " --> " + slot);
 					}
-					
+
 					return new ImmutablePair<SharedSlot, Locality>(slot, Locality.LOCAL);
 				}
 			}
 		}
-		
+
 		// if we want only local assignments, exit now with a "not found" result
 		if (didNotGetPreferred && localOnly) {
 			if (LOG.isDebugEnabled()) {
@@ -233,84 +238,153 @@ public class SlotSharingGroupAssignment implements Serializable {
 			}
 			return null;
 		}
-		
+
 		// schedule the task to any available location
 		SharedSlot slot = pollFromMultiMap(slotsForGroup);
-		if (slot != null && !slot.isDisposed()) {
+		if (slot != null && !slot.isDead()) {
 			if (LOG.isDebugEnabled()) {
 				LOG.debug((didNotGetPreferred ? "Non-local" : "Unconstrained") + " assignment in shared group : " + vertex + " --> " + slot);
 			}
-			
+
 			return new ImmutablePair<SharedSlot, Locality>(slot, didNotGetPreferred ? Locality.NON_LOCAL : Locality.UNCONSTRAINED);
 		}
 		else {
 			return null;
 		}
 	}
-	
-	
-	void releaseSubSlot(SubSlot subslot, SharedSlot sharedSlot) {
-		
-		AbstractID groupId = subslot.getGroupId();
-		
+
+	/**
+	 * Removes the shared slot from the assignment group.
+	 *
+	 * @param sharedSlot
+	 */
+	private void removeSharedSlot(SharedSlot sharedSlot){
+		if (!allSlots.contains(sharedSlot)) {
+			throw new IllegalArgumentException("Slot was not associated with this SlotSharingGroup before.");
+		}
+
+		allSlots.remove(sharedSlot);
+
+		Instance location = sharedSlot.getInstance();
+
+		for(Map.Entry<AbstractID, Map<Instance, List<SharedSlot>>> mapEntry: availableSlotsPerJid.entrySet()){
+			Map<Instance, List<SharedSlot>> map = mapEntry.getValue();
+
+			List<SharedSlot> list = map.get(location);
+
+			if(list == null || !list.remove(sharedSlot)){
+				throw new IllegalStateException("Bug: SharedSlot was not available to another vertex type that it was not allocated for before.");
+			}
+
+			if(list.isEmpty()){
+				map.remove(location);
+			}
+		}
+
+		sharedSlot.markCancelled();
+
+		returnAllocatedSlot(sharedSlot);
+	}
+
+	/**
+	 * Releases the shared slot from the assignment group.
+	 * @param sharedSlot The SharedSlot to be released
+	 */
+	public void releaseSharedSlot(SharedSlot sharedSlot){
 		synchronized (lock) {
+			Set<Slot> subSlots = sharedSlot.getSubSlots();
 
-			if (!allSlots.contains(sharedSlot)) {
-				throw new IllegalArgumentException("Slot was not associated with this SlotSharingGroup before.");
+			for(Slot subSlot: subSlots) {
+
+				subSlot.markDisposed();
+
+				if(subSlot instanceof SharedSlot){
+					releaseSharedSlot((SharedSlot) subSlot);
+				}else if(subSlot instanceof SimpleSlot){
+					releaseSimpleSlot((SimpleSlot) subSlot);
+				}
 			}
-			
-			int slotsRemaining = sharedSlot.releaseSlot(subslot);
-			
-			if (slotsRemaining == 0) {
-				// this was the last sub slot. remove this from the availability list 
-				// and trigger disposal
-				try {
-					allSlots.remove(sharedSlot);
-					
-					Instance location = sharedSlot.getAllocatedSlot().getInstance();
-
-					if (groupId != null) {
-						for (Map.Entry<AbstractID, Map<Instance, List<SharedSlot>>> mapEntry : availableSlotsPerJid.entrySet()) {
-							AbstractID id = mapEntry.getKey();
-							
-							// hack: we identify co location hint entries by the fact that they are keyed
-							//       by an abstract id, rather than a job vertex id
-							if (id.getClass() == AbstractID.class || id.equals(groupId)) {
-								continue;
-							}
-							
-							Map<Instance, List<SharedSlot>> map = mapEntry.getValue();
-							List<SharedSlot> list = map.get(location);
-							if (list == null || !list.remove(sharedSlot)) {
-								throw new IllegalStateException("Bug: SharedSlot was not available to another vertex type that it was not allocated for before.");
-							}
-							if (list.isEmpty()) {
-								map.remove(location);
-							}
-						}
+
+			subSlots.clear();
+
+			returnSlot(sharedSlot);
+		}
+	}
+
+	/**
+	 * Releases the simple slot from the assignment group.
+	 * @param simpleSlot The SimpleSlot to be released
+	 */
+	public void releaseSimpleSlot(SimpleSlot simpleSlot){
+		synchronized (lock) {
+			simpleSlot.cancel();
+
+			returnSlot(simpleSlot);
+		}
+
+	}
+
+	/**
+	 * Removes the given slot from the assignment group. If the slot is a root object, then it has
+	 * to be a SharedSlot and it is removed from the availableSlotsPerJid field and the slot is
+	 * returned to the instance. If the slot is a sub slot of the root slot, then this sub slot
+	 * is marked available again for tasks of the same group. Otherwise, the slot is simply removed
+	 * from its parent if it is not already marked as disposed. If a slot is already marked to be
+	 * disposed, then the releasing was called from a parent slot which will take care of the
+	 * disposal.
+	 *
+	 * IMPORTANT: The method is not synchronized. The caller is responsible for that.
+	 *
+	 * @param slot The slot to be returned.
+	 */
+	private void returnSlot(Slot slot){
+		// each slot can only be returned once, if a slot is returned then it should no longer be used --> markDead
+		if(slot.markDead()) {
+			// slot is a root slot
+			if(slot.getParent() == null){
+				// only SharedSlots are allowed to be root slots in a SlotSharingGroupAssignment
+				if(slot instanceof SharedSlot){
+					removeSharedSlot((SharedSlot) slot);
+				} else {
+					throw new IllegalStateException("Simple slot cannot be returned from SlotSharingGroupAssignment.");
+				}
+			} else {
+				AbstractID groupID = slot.getGroupID();
+				SharedSlot parent = slot.getParent();
+
+				// Only colocation constraint slots (SimpleSlot nested in a SharedSlot nested in a SharedSlot) have a groupID==null
+				// One can also say, all nested slots deeper than 2 have a groupID==null
+				if(groupID != null){
+					if (!allSlots.contains(parent)) {
+						throw new IllegalArgumentException("Slot was not associated with this SlotSharingGroup before.");
+					}
+
+					// make the shared slot available to tasks within the group it available to
+					Map<Instance, List<SharedSlot>> slotsForJid = availableSlotsPerJid.get(groupID);
+
+					// sanity check
+					if (slotsForJid == null) {
+						throw new IllegalStateException("Trying to return a slot for group " + groupID +
+								" when available slots indicated that all slots were available.");
 					}
-				} finally {
-					sharedSlot.dispose();
+
+					putIntoMultiMap(slotsForJid, parent.getInstance(), parent);
 				}
-			}
-			else if (groupId != null) {
-				// make the shared slot available to tasks within the group it available to
-				Map<Instance, List<SharedSlot>> slotsForJid = availableSlotsPerJid.get(groupId);
-				
-				// sanity check
-				if (slotsForJid == null) {
-					throw new IllegalStateException("Trying to return a slot for group " + groupId + 
-							" when available slots indicated that all slots were available.");
+
+				// if no one else takes care of disposal, then remove the slot from the parent
+				if(slot.markDisposed()) {
+					if (slot.getParent().freeSubSlot(slot) == 0) {
+						releaseSharedSlot(slot.getParent());
+					}
 				}
-				
-				putIntoMultiMap(slotsForJid, sharedSlot.getAllocatedSlot().getInstance(), sharedSlot);
 			}
 		}
 	}
-	
-	
-	
-	
+
+	private void returnAllocatedSlot(SharedSlot slot){
+		slot.getInstance().returnAllocatedSlot(slot);
+	}
+
 	// --------------------------------------------------------------------------------------------
 	//  State
 	// --------------------------------------------------------------------------------------------
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/SubSlot.java b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/SubSlot.java
deleted file mode 100644
index 92d457bbf77..00000000000
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/SubSlot.java
+++ /dev/null
@@ -1,75 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.runtime.jobmanager.scheduler;
-
-import org.apache.flink.runtime.AbstractID;
-import org.apache.flink.runtime.instance.AllocatedSlot;
-
-public class SubSlot extends AllocatedSlot {
-
-	private static final long serialVersionUID = 1361615219044538497L;
-	
-
-	private final SharedSlot sharedSlot;
-	
-	private final AbstractID groupId;
-	
-	private final int subSlotNumber;
-	
-	
-	public SubSlot(SharedSlot sharedSlot, int subSlotNumber, AbstractID groupId) {
-		super(sharedSlot.getAllocatedSlot().getJobID(),
-				sharedSlot.getAllocatedSlot().getInstance(),
-				sharedSlot.getAllocatedSlot().getSlotNumber());
-		
-		this.sharedSlot = sharedSlot;
-		this.groupId = groupId;
-		this.subSlotNumber = subSlotNumber;
-	}
-	
-	// --------------------------------------------------------------------------------------------
-	
-	public void releaseSlot() {
-		// cancel everything, if there is something. since this is atomically status based,
-		// it will not happen twice if another attempt happened before or concurrently
-		try {
-			cancel();
-		}
-		finally {
-			if (markReleased()) {
-				this.sharedSlot.returnAllocatedSlot(this);
-			}
-		}
-	}
-	
-	public SharedSlot getSharedSlot() {
-		return this.sharedSlot;
-	}
-	
-	public AbstractID getGroupId() {
-		return groupId;
-	}
-	
-	// --------------------------------------------------------------------------------------------
-	
-	@Override
-	public String toString() {
-		return "SubSlot " + subSlotNumber + " (" + super.toString() + ')';
-	}
-}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/web/JobmanagerInfoServlet.java b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/web/JobManagerInfoServlet.java
similarity index 98%
rename from flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/web/JobmanagerInfoServlet.java
rename to flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/web/JobManagerInfoServlet.java
index b842a9b658b..1492ae1560e 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/web/JobmanagerInfoServlet.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/web/JobManagerInfoServlet.java
@@ -35,6 +35,7 @@ import javax.servlet.http.HttpServletResponse;
 
 import akka.actor.ActorRef;
 import org.apache.flink.runtime.akka.AkkaUtils;
+import org.apache.flink.runtime.instance.SimpleSlot;
 import org.apache.flink.runtime.messages.ArchiveMessages.ArchivedJobs;
 import org.apache.flink.runtime.messages.ArchiveMessages.RequestArchivedJobs$;
 import org.apache.flink.runtime.messages.JobManagerMessages.AccumulatorResultsResponse;
@@ -56,7 +57,6 @@ import org.apache.flink.runtime.executiongraph.Execution;
 import org.apache.flink.runtime.executiongraph.ExecutionGraph;
 import org.apache.flink.runtime.executiongraph.ExecutionJobVertex;
 import org.apache.flink.runtime.executiongraph.ExecutionVertex;
-import org.apache.flink.runtime.instance.AllocatedSlot;
 import org.apache.flink.runtime.jobgraph.JobID;
 import org.apache.flink.runtime.jobgraph.JobStatus;
 import org.apache.flink.runtime.jobgraph.JobVertexID;
@@ -66,12 +66,11 @@ import org.apache.flink.util.StringUtils;
 import org.eclipse.jetty.io.EofException;
 import scala.concurrent.duration.FiniteDuration;
 
-
-public class JobmanagerInfoServlet extends HttpServlet {
+public class JobManagerInfoServlet extends HttpServlet {
 	
 	private static final long serialVersionUID = 1L;
 	
-	private static final Logger LOG = LoggerFactory.getLogger(JobmanagerInfoServlet.class);
+	private static final Logger LOG = LoggerFactory.getLogger(JobManagerInfoServlet.class);
 	
 	/** Underlying JobManager */
 	private final ActorRef jobmanager;
@@ -79,7 +78,7 @@ public class JobmanagerInfoServlet extends HttpServlet {
 	private final FiniteDuration timeout;
 	
 	
-	public JobmanagerInfoServlet(ActorRef jobmanager, ActorRef archive, FiniteDuration timeout) {
+	public JobManagerInfoServlet(ActorRef jobmanager, ActorRef archive, FiniteDuration timeout) {
 		this.jobmanager = jobmanager;
 		this.archive = archive;
 		this.timeout = timeout;
@@ -293,7 +292,7 @@ public class JobmanagerInfoServlet extends HttpServlet {
 				boolean first = true;
 				for (ExecutionVertex vertex : graph.getAllExecutionVertices()) {
 					if (vertex.getExecutionState() == ExecutionState.FAILED) {
-						AllocatedSlot slot = vertex.getCurrentAssignedResource();
+						SimpleSlot slot = vertex.getCurrentAssignedResource();
 						Throwable failureCause = vertex.getFailureCause();
 						if (slot != null || failureCause != null) {
 							if (first) {
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/web/JsonFactory.java b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/web/JsonFactory.java
index 103c1beea46..8e466929820 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/web/JsonFactory.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/web/JsonFactory.java
@@ -22,7 +22,7 @@ import org.apache.flink.runtime.execution.ExecutionState;
 import org.apache.flink.runtime.executiongraph.ExecutionJobVertex;
 import org.apache.flink.runtime.executiongraph.ExecutionVertex;
 import org.apache.flink.runtime.executiongraph.IntermediateResult;
-import org.apache.flink.runtime.instance.AllocatedSlot;
+import org.apache.flink.runtime.instance.SimpleSlot;
 import org.apache.flink.util.StringUtils;
 
 import java.util.HashMap;
@@ -38,7 +38,7 @@ public class JsonFactory {
 		json.append("\"vertexname\": \"" + StringUtils.escapeHtml(vertex.getSimpleName()) + "\",");
 		json.append("\"vertexstatus\": \"" + vertex.getExecutionState() + "\",");
 		
-		AllocatedSlot slot = vertex.getCurrentAssignedResource();
+		SimpleSlot slot = vertex.getCurrentAssignedResource();
 		String instanceName = slot == null ? "(null)" : slot.getInstance()
 				.getInstanceConnectionInfo().getFQDNHostname();
 		
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/web/WebInfoServer.java b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/web/WebInfoServer.java
index 733cf5e6f7f..71347eef840 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/web/WebInfoServer.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/web/WebInfoServer.java
@@ -128,7 +128,7 @@ public class WebInfoServer {
 		// ----- the handlers for the servlets -----
 		ServletContextHandler servletContext = new ServletContextHandler(ServletContextHandler.SESSIONS);
 		servletContext.setContextPath("/");
-		servletContext.addServlet(new ServletHolder(new JobmanagerInfoServlet(jobmanager,
+		servletContext.addServlet(new ServletHolder(new JobManagerInfoServlet(jobmanager,
 				archive, timeout)), "/jobsInfo");
 		servletContext.addServlet(new ServletHolder(new LogfileInfoServlet(logDirFiles)), "/logInfo");
 		servletContext.addServlet(new ServletHolder(new SetupInfoServlet(config, jobmanager, timeout)),
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/profiling/impl/JobProfilingData.java b/flink-runtime/src/main/java/org/apache/flink/runtime/profiling/impl/JobProfilingData.java
index 3a53801e137..ce7db251f12 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/profiling/impl/JobProfilingData.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/profiling/impl/JobProfilingData.java
@@ -26,12 +26,11 @@ import java.util.Set;
 
 import org.apache.flink.runtime.executiongraph.ExecutionGraph;
 import org.apache.flink.runtime.executiongraph.ExecutionVertex;
-import org.apache.flink.runtime.instance.AllocatedSlot;
 import org.apache.flink.runtime.instance.Instance;
+import org.apache.flink.runtime.instance.SimpleSlot;
 import org.apache.flink.runtime.profiling.impl.types.InternalInstanceProfilingData;
 import org.apache.flink.runtime.profiling.types.InstanceSummaryProfilingEvent;
 
-
 public class JobProfilingData {
 
 	private final ExecutionGraph executionGraph;
@@ -59,7 +58,7 @@ public class JobProfilingData {
 	public boolean addIfInstanceIsAllocatedByJob(InternalInstanceProfilingData instanceProfilingData) {
 
 		for (ExecutionVertex executionVertex : this.executionGraph.getAllExecutionVertices()) {
-			AllocatedSlot slot = executionVertex.getCurrentAssignedResource();
+			SimpleSlot slot = executionVertex.getCurrentAssignedResource();
 			if (slot != null && slot.getInstance().getPath().equals(
 					instanceProfilingData.getInstancePath()))
 			{
@@ -76,7 +75,7 @@ public class JobProfilingData {
 		final Set<Instance> tempSet = new HashSet<Instance>();
 		
 		for (ExecutionVertex executionVertex : this.executionGraph.getAllExecutionVertices()) {
-			AllocatedSlot slot = executionVertex.getCurrentAssignedResource();
+			SimpleSlot slot = executionVertex.getCurrentAssignedResource();
 			if (slot != null) {
 				tempSet.add(slot.getInstance());
 			}
diff --git a/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManager.scala b/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManager.scala
index c780589cda1..532f7f8fb2b 100644
--- a/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManager.scala
+++ b/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManager.scala
@@ -437,6 +437,7 @@ class JobManager(val configuration: Configuration)
 
     case Terminated(taskManager) => {
       log.info("Task manager {} terminated.", taskManager.path)
+      JobManager.LOG.warn(s"Task manager ${taskManager.path} terminated.")
       instanceManager.unregisterTaskManager(taskManager)
       context.unwatch(taskManager)
     }
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphDeploymentTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphDeploymentTest.java
index f4c95c985c4..4ee28e68b68 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphDeploymentTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphDeploymentTest.java
@@ -42,8 +42,8 @@ import org.apache.flink.runtime.deployment.PartitionConsumerDeploymentDescriptor
 import org.apache.flink.runtime.deployment.PartitionDeploymentDescriptor;
 import org.apache.flink.runtime.deployment.TaskDeploymentDescriptor;
 import org.apache.flink.runtime.execution.ExecutionState;
-import org.apache.flink.runtime.instance.AllocatedSlot;
 import org.apache.flink.runtime.instance.Instance;
+import org.apache.flink.runtime.instance.SimpleSlot;
 import org.apache.flink.runtime.jobgraph.AbstractJobVertex;
 import org.apache.flink.runtime.jobgraph.DistributionPattern;
 import org.apache.flink.runtime.jobgraph.JobID;
@@ -120,7 +120,7 @@ public class ExecutionGraphDeploymentTest {
 
 			final Instance instance = getInstance(simpleTaskManager);
 
-			final AllocatedSlot slot = instance.allocateSlot(jobId);
+			final SimpleSlot slot = instance.allocateSimpleSlot(jobId);
 
 			assertEquals(ExecutionState.CREATED, vertex.getExecutionState());
 
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphTestUtils.java b/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphTestUtils.java
index 21caca090eb..e8e1f7e37f1 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphTestUtils.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphTestUtils.java
@@ -33,11 +33,11 @@ import org.apache.flink.runtime.JobException;
 import org.apache.flink.runtime.akka.AkkaUtils;
 import org.apache.flink.runtime.deployment.TaskDeploymentDescriptor;
 import org.apache.flink.runtime.execution.ExecutionState;
-import org.apache.flink.runtime.instance.AllocatedSlot;
 import org.apache.flink.runtime.instance.HardwareDescription;
 import org.apache.flink.runtime.instance.Instance;
 import org.apache.flink.runtime.instance.InstanceConnectionInfo;
 import org.apache.flink.runtime.instance.InstanceID;
+import org.apache.flink.runtime.instance.SimpleSlot;
 import org.apache.flink.runtime.jobgraph.AbstractJobVertex;
 import org.apache.flink.runtime.jobgraph.JobID;
 import org.apache.flink.runtime.jobgraph.JobStatus;
@@ -68,7 +68,7 @@ public class ExecutionGraphTestUtils {
 		}
 	}
 	
-	public static void setVertexResource(ExecutionVertex vertex, AllocatedSlot slot) {
+	public static void setVertexResource(ExecutionVertex vertex, SimpleSlot slot) {
 		try {
 			Execution exec = vertex.getCurrentExecutionAttempt();
 			
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionStateProgressTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionStateProgressTest.java
index 4a8c69b6635..2f1af705c9e 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionStateProgressTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionStateProgressTest.java
@@ -30,7 +30,7 @@ import akka.actor.Props;
 import akka.testkit.JavaTestKit;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.runtime.akka.AkkaUtils;
-import org.apache.flink.runtime.instance.AllocatedSlot;
+import org.apache.flink.runtime.instance.SimpleSlot;
 import org.apache.flink.runtime.jobgraph.AbstractJobVertex;
 import org.apache.flink.runtime.jobgraph.JobID;
 import org.apache.flink.runtime.jobgraph.JobStatus;
@@ -76,7 +76,7 @@ public class ExecutionStateProgressTest {
 			// mock resources and mock taskmanager
 			ActorRef taskManager = system.actorOf(Props.create(SimpleAcknowledgingTaskManager.class));
 			for (ExecutionVertex ee : ejv.getTaskVertices()) {
-				AllocatedSlot slot = getInstance(taskManager).allocateSlot(jid);
+				SimpleSlot slot = getInstance(taskManager).allocateSimpleSlot(jid);
 				ee.deployToSlot(slot);
 			}
 			
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionVertexCancelTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionVertexCancelTest.java
index 3c1b174ba0e..ee8995453d9 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionVertexCancelTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionVertexCancelTest.java
@@ -37,7 +37,7 @@ import akka.testkit.JavaTestKit;
 import org.apache.flink.runtime.akka.AkkaUtils;
 import org.apache.flink.runtime.deployment.TaskDeploymentDescriptor;
 import org.apache.flink.runtime.execution.ExecutionState;
-import org.apache.flink.runtime.instance.AllocatedSlot;
+import org.apache.flink.runtime.instance.SimpleSlot;
 import org.apache.flink.runtime.instance.Instance;
 import org.apache.flink.runtime.jobgraph.JobID;
 import org.apache.flink.runtime.jobgraph.JobVertexID;
@@ -148,7 +148,7 @@ public class ExecutionVertexCancelTest {
 						new TaskOperationResult(execId, false))));
 
 				Instance instance = getInstance(taskManager);
-				AllocatedSlot slot = instance.allocateSlot(new JobID());
+			SimpleSlot slot = instance.allocateSimpleSlot(new JobID());
 
 				vertex.deployToSlot(slot);
 
@@ -223,7 +223,7 @@ public class ExecutionVertexCancelTest {
 							TaskOperationResult(execId, false), new TaskOperationResult(execId, true))));
 
 					Instance instance = getInstance(taskManager);
-					AllocatedSlot slot = instance.allocateSlot(new JobID());
+			SimpleSlot slot = instance.allocateSimpleSlot(new JobID());
 
 					vertex.deployToSlot(slot);
 
@@ -296,7 +296,7 @@ public class ExecutionVertexCancelTest {
 									TaskOperationResult(execId, true))));
 
 					Instance instance = getInstance(taskManager);
-					AllocatedSlot slot = instance.allocateSlot(new JobID());
+			SimpleSlot slot = instance.allocateSimpleSlot(new JobID());
 
 					setVertexState(vertex, ExecutionState.RUNNING);
 					setVertexResource(vertex, slot);
@@ -344,7 +344,7 @@ public class ExecutionVertexCancelTest {
 							TaskOperationResult(execId, true))));
 
 					Instance instance = getInstance(taskManager);
-					AllocatedSlot slot = instance.allocateSlot(new JobID());
+			SimpleSlot slot = instance.allocateSimpleSlot(new JobID());
 
 					setVertexState(vertex, ExecutionState.RUNNING);
 					setVertexResource(vertex, slot);
@@ -400,7 +400,7 @@ public class ExecutionVertexCancelTest {
 							TaskOperationResult(execId, false))));
 
 					Instance instance = getInstance(taskManager);
-					AllocatedSlot slot = instance.allocateSlot(new JobID());
+			SimpleSlot slot = instance.allocateSimpleSlot(new JobID());
 
 					setVertexState(vertex, ExecutionState.RUNNING);
 					setVertexResource(vertex, slot);
@@ -441,7 +441,7 @@ public class ExecutionVertexCancelTest {
 							CancelSequenceTaskManagerCreator()));
 
 					Instance instance = getInstance(taskManager);
-					AllocatedSlot slot = instance.allocateSlot(new JobID());
+			SimpleSlot slot = instance.allocateSimpleSlot(new JobID());
 
 					setVertexState(vertex, ExecutionState.RUNNING);
 					setVertexResource(vertex, slot);
@@ -487,7 +487,7 @@ public class ExecutionVertexCancelTest {
 							)));
 
 					Instance instance = getInstance(taskManager);
-					AllocatedSlot slot = instance.allocateSlot(new JobID());
+			SimpleSlot slot = instance.allocateSimpleSlot(new JobID());
 
 					setVertexState(vertex, ExecutionState.RUNNING);
 					setVertexResource(vertex, slot);
@@ -544,7 +544,7 @@ public class ExecutionVertexCancelTest {
 			// the scheduler (or any caller) needs to know that the slot should be released
 			try {
 				Instance instance = getInstance(ActorRef.noSender());
-				AllocatedSlot slot = instance.allocateSlot(new JobID());
+				SimpleSlot slot = instance.allocateSimpleSlot(new JobID());
 				
 				vertex.deployToSlot(slot);
 				fail("Method should throw an exception");
@@ -587,7 +587,7 @@ public class ExecutionVertexCancelTest {
 				setVertexState(vertex, ExecutionState.CANCELING);
 				
 				Instance instance = getInstance(ActorRef.noSender());
-				AllocatedSlot slot = instance.allocateSlot(new JobID());
+				SimpleSlot slot = instance.allocateSimpleSlot(new JobID());
 				
 				vertex.deployToSlot(slot);
 				fail("Method should throw an exception");
@@ -601,7 +601,7 @@ public class ExecutionVertexCancelTest {
 						AkkaUtils.DEFAULT_TIMEOUT());
 				
 				Instance instance = getInstance(ActorRef.noSender());
-				AllocatedSlot slot = instance.allocateSlot(new JobID());
+				SimpleSlot slot = instance.allocateSimpleSlot(new JobID());
 				
 				setVertexResource(vertex, slot);
 				setVertexState(vertex, ExecutionState.CANCELING);
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionVertexDeploymentTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionVertexDeploymentTest.java
index 330292babda..c0d1db88d33 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionVertexDeploymentTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionVertexDeploymentTest.java
@@ -29,8 +29,8 @@ import akka.testkit.JavaTestKit;
 import akka.testkit.TestActorRef;
 import org.apache.flink.runtime.akka.AkkaUtils;
 import org.apache.flink.runtime.execution.ExecutionState;
-import org.apache.flink.runtime.instance.AllocatedSlot;
 import org.apache.flink.runtime.instance.Instance;
+import org.apache.flink.runtime.instance.SimpleSlot;
 import org.apache.flink.runtime.jobgraph.JobID;
 import org.apache.flink.runtime.jobgraph.JobVertexID;
 import org.apache.flink.runtime.messages.TaskManagerMessages.TaskOperationResult;
@@ -64,7 +64,7 @@ public class ExecutionVertexDeploymentTest {
 			// mock taskmanager to simply accept the call
 			Instance instance = getInstance(tm);
 
-			final AllocatedSlot slot = instance.allocateSlot(new JobID());
+			final SimpleSlot slot = instance.allocateSimpleSlot(new JobID());
 			
 			final ExecutionJobVertex ejv = getExecutionVertex(jid);
 			
@@ -106,8 +106,8 @@ public class ExecutionVertexDeploymentTest {
 					Props.create(SimpleAcknowledgingTaskManager.class));
 			
 			final Instance instance = getInstance(simpleTaskManager);
+			final SimpleSlot slot = instance.allocateSimpleSlot(new JobID());
 
-			final AllocatedSlot slot = instance.allocateSlot(new JobID());
 			
 			final ExecutionJobVertex ejv = getExecutionVertex(jid);
 			
@@ -151,7 +151,7 @@ public class ExecutionVertexDeploymentTest {
 			
 			final Instance instance = getInstance(simpleTaskManager);
 
-			final AllocatedSlot slot = instance.allocateSlot(new JobID());
+			final SimpleSlot slot = instance.allocateSimpleSlot(new JobID());
 			
 			final ExecutionJobVertex ejv = getExecutionVertex(jid);
 			
@@ -206,7 +206,7 @@ public class ExecutionVertexDeploymentTest {
 					Props.create(SimpleFailingTaskManager.class));
 			
 			final Instance instance = getInstance(simpleTaskManager);
-			final AllocatedSlot slot = instance.allocateSlot(new JobID());
+			final SimpleSlot slot = instance.allocateSimpleSlot(new JobID());
 			
 			final ExecutionJobVertex ejv = getExecutionVertex(jid);
 			
@@ -242,8 +242,8 @@ public class ExecutionVertexDeploymentTest {
 					Props.create(SimpleFailingTaskManager.class));
 			
 			final Instance instance = getInstance(simpleTaskManager);
+			final SimpleSlot slot = instance.allocateSimpleSlot(new JobID());
 
-			final AllocatedSlot slot = instance.allocateSlot(new JobID());
 
 			final ExecutionJobVertex ejv = getExecutionVertex(jid);
 			final ExecutionVertex vertex = new ExecutionVertex(ejv, 0, new IntermediateResult[0],
@@ -289,9 +289,8 @@ public class ExecutionVertexDeploymentTest {
 
 			final TestActorRef simpleTaskManager = TestActorRef.create(system,
 					Props.create(SimpleAcknowledgingTaskManager.class));
-
 			final Instance instance = getInstance(simpleTaskManager);
-			final AllocatedSlot slot = instance.allocateSlot(new JobID());
+			final SimpleSlot slot = instance.allocateSimpleSlot(new JobID());
 
 			final ExecutionJobVertex ejv = getExecutionVertex(jid);
 
@@ -343,8 +342,7 @@ public class ExecutionVertexDeploymentTest {
 					TaskOperationResult(eid, false), new TaskOperationResult(eid, true))));
 
 			final Instance instance = getInstance(simpleTaskManager);
-
-			final AllocatedSlot slot = instance.allocateSlot(new JobID());
+			final SimpleSlot slot = instance.allocateSimpleSlot(new JobID());
 
 			assertEquals(ExecutionState.CREATED, vertex.getExecutionState());
 
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionVertexSchedulingTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionVertexSchedulingTest.java
index 24ac44bb8f0..8230433b530 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionVertexSchedulingTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionVertexSchedulingTest.java
@@ -30,8 +30,8 @@ import akka.testkit.JavaTestKit;
 import akka.testkit.TestActorRef;
 import org.apache.flink.runtime.akka.AkkaUtils;
 import org.apache.flink.runtime.execution.ExecutionState;
-import org.apache.flink.runtime.instance.AllocatedSlot;
 import org.apache.flink.runtime.instance.Instance;
+import org.apache.flink.runtime.instance.SimpleSlot;
 import org.apache.flink.runtime.jobgraph.JobID;
 import org.apache.flink.runtime.jobgraph.JobVertexID;
 import org.apache.flink.runtime.jobmanager.scheduler.Scheduler;
@@ -65,7 +65,7 @@ public class ExecutionVertexSchedulingTest {
 		try {
 			// a slot than cannot be deployed to
 			final Instance instance = getInstance(ActorRef.noSender());
-			final AllocatedSlot slot = instance.allocateSlot(new JobID());
+			final SimpleSlot slot = instance.allocateSimpleSlot(new JobID());
 			slot.cancel();
 			assertFalse(slot.isReleased());
 			
@@ -96,7 +96,7 @@ public class ExecutionVertexSchedulingTest {
 		try {
 			// a slot than cannot be deployed to
 			final Instance instance = getInstance(ActorRef.noSender());
-			final AllocatedSlot slot = instance.allocateSlot(new JobID());
+			final SimpleSlot slot = instance.allocateSimpleSlot(new JobID());
 			slot.cancel();
 			assertFalse(slot.isReleased());
 			
@@ -136,7 +136,7 @@ public class ExecutionVertexSchedulingTest {
 					.SimpleAcknowledgingTaskManager.class));
 
 			final Instance instance = getInstance(tm);
-			final AllocatedSlot slot = instance.allocateSlot(new JobID());
+			final SimpleSlot slot = instance.allocateSimpleSlot(new JobID());
 			
 			final ExecutionJobVertex ejv = getExecutionVertex(new JobVertexID());
 			final ExecutionVertex vertex = new ExecutionVertex(ejv, 0, new IntermediateResult[0],
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/instance/AllocatedSlotTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/instance/AllocatedSlotTest.java
index 15cdefb002d..94bdef1219f 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/instance/AllocatedSlotTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/instance/AllocatedSlotTest.java
@@ -36,7 +36,7 @@ public class AllocatedSlotTest {
 		try {
 			// cancel, then release
 			{
-				AllocatedSlot slot = getSlot();
+				SimpleSlot slot = getSlot();
 				assertTrue(slot.isAlive());
 				
 				slot.cancel();
@@ -52,7 +52,7 @@ public class AllocatedSlotTest {
 			
 			// release immediately
 			{
-				AllocatedSlot slot = getSlot();
+				SimpleSlot slot = getSlot();
 				assertTrue(slot.isAlive());
 				
 				slot.releaseSlot();
@@ -75,32 +75,32 @@ public class AllocatedSlotTest {
 			
 			// assign to alive slot
 			{
-				AllocatedSlot slot = getSlot();
+				SimpleSlot slot = getSlot();
 				
 				assertTrue(slot.setExecutedVertex(ev));
-				assertEquals(ev, slot.getExecutedVertex());
+				assertEquals(ev, slot.getExecution());
 				
 				// try to add another one
 				assertFalse(slot.setExecutedVertex(ev_2));
-				assertEquals(ev, slot.getExecutedVertex());
+				assertEquals(ev, slot.getExecution());
 			}
 			
 			// assign to canceled slot
 			{
-				AllocatedSlot slot = getSlot();
+				SimpleSlot slot = getSlot();
 				slot.cancel();
 				
 				assertFalse(slot.setExecutedVertex(ev));
-				assertNull(slot.getExecutedVertex());
+				assertNull(slot.getExecution());
 			}
 			
 			// assign to released
 			{
-				AllocatedSlot slot = getSlot();
+				SimpleSlot slot = getSlot();
 				slot.releaseSlot();
 				
 				assertFalse(slot.setExecutedVertex(ev));
-				assertNull(slot.getExecutedVertex());
+				assertNull(slot.getExecution());
 			}
 		}
 		catch (Exception e) {
@@ -114,9 +114,9 @@ public class AllocatedSlotTest {
 		try {
 			Execution ev = mock(Execution.class);
 			
-			AllocatedSlot slot = getSlot();
+			SimpleSlot slot = getSlot();
 			assertTrue(slot.setExecutedVertex(ev));
-			assertEquals(ev, slot.getExecutedVertex());
+			assertEquals(ev, slot.getExecution());
 			
 			slot.cancel();
 			slot.releaseSlot();
@@ -130,12 +130,12 @@ public class AllocatedSlotTest {
 		}
 	}
 	
-	public static AllocatedSlot getSlot() throws Exception {
+	public static SimpleSlot getSlot() throws Exception {
 		HardwareDescription hardwareDescription = new HardwareDescription(4, 2L*1024*1024*1024, 1024*1024*1024, 512*1024*1024);
 		InetAddress address = InetAddress.getByName("127.0.0.1");
 		InstanceConnectionInfo connection = new InstanceConnectionInfo(address, 10001);
 		
 		Instance instance = new Instance(ActorRef.noSender(), connection, new InstanceID(), hardwareDescription, 1);
-		return instance.allocateSlot(new JobID());
+		return instance.allocateSimpleSlot(new JobID());
 	}
 }
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/instance/InstanceTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/instance/InstanceTest.java
index a7820a330f3..47f7a2b7c5f 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/instance/InstanceTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/instance/InstanceTest.java
@@ -45,10 +45,10 @@ public class InstanceTest {
 			assertEquals(4, instance.getNumberOfAvailableSlots());
 			assertEquals(0, instance.getNumberOfAllocatedSlots());
 			
-			AllocatedSlot slot1 = instance.allocateSlot(new JobID());
-			AllocatedSlot slot2 = instance.allocateSlot(new JobID());
-			AllocatedSlot slot3 = instance.allocateSlot(new JobID());
-			AllocatedSlot slot4 = instance.allocateSlot(new JobID());
+			SimpleSlot slot1 = instance.allocateSimpleSlot(new JobID());
+			SimpleSlot slot2 = instance.allocateSimpleSlot(new JobID());
+			SimpleSlot slot3 = instance.allocateSimpleSlot(new JobID());
+			SimpleSlot slot4 = instance.allocateSimpleSlot(new JobID());
 			
 			assertNotNull(slot1);
 			assertNotNull(slot2);
@@ -61,7 +61,7 @@ public class InstanceTest {
 					slot3.getSlotNumber() + slot4.getSlotNumber());
 			
 			// no more slots
-			assertNull(instance.allocateSlot(new JobID()));
+			assertNull(instance.allocateSimpleSlot(new JobID()));
 			try {
 				instance.returnAllocatedSlot(slot2);
 				fail("instance accepted a non-cancelled slot.");
@@ -109,9 +109,9 @@ public class InstanceTest {
 			
 			assertEquals(3, instance.getNumberOfAvailableSlots());
 			
-			AllocatedSlot slot1 = instance.allocateSlot(new JobID());
-			AllocatedSlot slot2 = instance.allocateSlot(new JobID());
-			AllocatedSlot slot3 = instance.allocateSlot(new JobID());
+			SimpleSlot slot1 = instance.allocateSimpleSlot(new JobID());
+			SimpleSlot slot2 = instance.allocateSimpleSlot(new JobID());
+			SimpleSlot slot3 = instance.allocateSimpleSlot(new JobID());
 			
 			instance.markDead();
 			
@@ -139,9 +139,9 @@ public class InstanceTest {
 			
 			assertEquals(3, instance.getNumberOfAvailableSlots());
 			
-			AllocatedSlot slot1 = instance.allocateSlot(new JobID());
-			AllocatedSlot slot2 = instance.allocateSlot(new JobID());
-			AllocatedSlot slot3 = instance.allocateSlot(new JobID());
+			SimpleSlot slot1 = instance.allocateSimpleSlot(new JobID());
+			SimpleSlot slot2 = instance.allocateSimpleSlot(new JobID());
+			SimpleSlot slot3 = instance.allocateSimpleSlot(new JobID());
 			
 			instance.cancelAndReleaseAllSlots();
 			
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/ScheduleWithCoLocationHintTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/ScheduleWithCoLocationHintTest.java
index b22ccd0a9a7..272a9115b6c 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/ScheduleWithCoLocationHintTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/ScheduleWithCoLocationHintTest.java
@@ -26,9 +26,9 @@ import static org.junit.Assert.assertNotNull;
 import static org.junit.Assert.assertTrue;
 import static org.junit.Assert.fail;
 
+import org.apache.flink.runtime.instance.SimpleSlot;
 import akka.actor.ActorSystem;
 import akka.testkit.JavaTestKit;
-import org.apache.flink.runtime.instance.AllocatedSlot;
 import org.apache.flink.runtime.instance.Instance;
 import org.apache.flink.runtime.jobgraph.JobVertexID;
 import org.apache.flink.runtime.testingUtils.TestingUtils;
@@ -77,18 +77,18 @@ public class ScheduleWithCoLocationHintTest {
 			CoLocationConstraint c6 = new CoLocationConstraint(ccg);
 			
 			// schedule 4 tasks from the first vertex group
-			AllocatedSlot s1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 0, 6), sharingGroup, c1));
-			AllocatedSlot s2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 1, 6), sharingGroup, c2));
-			AllocatedSlot s3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 2, 6), sharingGroup, c3));
-			AllocatedSlot s4 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 3, 6), sharingGroup, c4));
-			AllocatedSlot s5 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 0, 6), sharingGroup, c1));
-			AllocatedSlot s6 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 1, 6), sharingGroup, c2));
-			AllocatedSlot s7 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 2, 6), sharingGroup, c3));
-			AllocatedSlot s8 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 4, 6), sharingGroup, c5));
-			AllocatedSlot s9 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 5, 6), sharingGroup, c6));
-			AllocatedSlot s10 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 3, 6), sharingGroup, c4));
-			AllocatedSlot s11 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 4, 6), sharingGroup, c5));
-			AllocatedSlot s12 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 5, 6), sharingGroup, c6));
+			SimpleSlot s1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 0, 6), sharingGroup, c1));
+			SimpleSlot s2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 1, 6), sharingGroup, c2));
+			SimpleSlot s3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 2, 6), sharingGroup, c3));
+			SimpleSlot s4 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 3, 6), sharingGroup, c4));
+			SimpleSlot s5 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 0, 6), sharingGroup, c1));
+			SimpleSlot s6 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 1, 6), sharingGroup, c2));
+			SimpleSlot s7 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 2, 6), sharingGroup, c3));
+			SimpleSlot s8 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 4, 6), sharingGroup, c5));
+			SimpleSlot s9 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 5, 6), sharingGroup, c6));
+			SimpleSlot s10 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 3, 6), sharingGroup, c4));
+			SimpleSlot s11 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 4, 6), sharingGroup, c5));
+			SimpleSlot s12 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 5, 6), sharingGroup, c6));
 
 			assertNotNull(s1);
 			assertNotNull(s2);
@@ -104,18 +104,18 @@ public class ScheduleWithCoLocationHintTest {
 			assertNotNull(s12);
 			
 			// check that each slot got exactly two tasks
-			assertEquals(2, ((SubSlot) s1).getSharedSlot().getNumberOfAllocatedSubSlots());
-			assertEquals(2, ((SubSlot) s2).getSharedSlot().getNumberOfAllocatedSubSlots());
-			assertEquals(2, ((SubSlot) s3).getSharedSlot().getNumberOfAllocatedSubSlots());
-			assertEquals(2, ((SubSlot) s4).getSharedSlot().getNumberOfAllocatedSubSlots());
-			assertEquals(2, ((SubSlot) s5).getSharedSlot().getNumberOfAllocatedSubSlots());
-			assertEquals(2, ((SubSlot) s6).getSharedSlot().getNumberOfAllocatedSubSlots());
-			assertEquals(2, ((SubSlot) s7).getSharedSlot().getNumberOfAllocatedSubSlots());
-			assertEquals(2, ((SubSlot) s8).getSharedSlot().getNumberOfAllocatedSubSlots());
-			assertEquals(2, ((SubSlot) s9).getSharedSlot().getNumberOfAllocatedSubSlots());
-			assertEquals(2, ((SubSlot) s10).getSharedSlot().getNumberOfAllocatedSubSlots());
-			assertEquals(2, ((SubSlot) s11).getSharedSlot().getNumberOfAllocatedSubSlots());
-			assertEquals(2, ((SubSlot) s12).getSharedSlot().getNumberOfAllocatedSubSlots());
+			assertEquals(2, s1.getRoot().getNumberLeaves());
+			assertEquals(2, s2.getRoot().getNumberLeaves());
+			assertEquals(2, s3.getRoot().getNumberLeaves());
+			assertEquals(2, s4.getRoot().getNumberLeaves());
+			assertEquals(2, s5.getRoot().getNumberLeaves());
+			assertEquals(2, s6.getRoot().getNumberLeaves());
+			assertEquals(2, s7.getRoot().getNumberLeaves());
+			assertEquals(2, s8.getRoot().getNumberLeaves());
+			assertEquals(2, s9.getRoot().getNumberLeaves());
+			assertEquals(2, s10.getRoot().getNumberLeaves());
+			assertEquals(2, s11.getRoot().getNumberLeaves());
+			assertEquals(2, s12.getRoot().getNumberLeaves());
 			
 			assertEquals(s1.getInstance(), s5.getInstance());
 			assertEquals(s2.getInstance(), s6.getInstance());
@@ -150,7 +150,7 @@ public class ScheduleWithCoLocationHintTest {
 			s12.releaseSlot();
 			assertTrue(scheduler.getNumberOfAvailableSlots() >= 1);
 			
-			AllocatedSlot single = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(new JobVertexID(), 0, 1)));
+			SimpleSlot single = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(new JobVertexID(), 0, 1)));
 			assertNotNull(single);
 			
 			s1.releaseSlot();
@@ -197,10 +197,10 @@ public class ScheduleWithCoLocationHintTest {
 			SlotSharingGroup sharingGroup = new SlotSharingGroup();
 			CoLocationConstraint c1 = new CoLocationConstraint(new CoLocationGroup());
 			
-			AllocatedSlot s1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 0, 1), sharingGroup, c1));
-			AllocatedSlot s2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 0, 1), sharingGroup, c1));
+			SimpleSlot s1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 0, 1), sharingGroup, c1));
+			SimpleSlot s2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 0, 1), sharingGroup, c1));
 			
-			AllocatedSlot sSolo = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid4, 0, 1)));
+			SimpleSlot sSolo = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid4, 0, 1)));
 			
 			Instance loc = s1.getInstance();
 			
@@ -208,7 +208,7 @@ public class ScheduleWithCoLocationHintTest {
 			s2.releaseSlot();
 			sSolo.releaseSlot();
 			
-			AllocatedSlot sNew = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid3, 0, 1), sharingGroup, c1));
+			SimpleSlot sNew = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid3, 0, 1), sharingGroup, c1));
 			assertEquals(loc, sNew.getInstance());
 			
 			assertEquals(2, scheduler.getNumberOfLocalizedAssignments());
@@ -241,7 +241,7 @@ public class ScheduleWithCoLocationHintTest {
 			SlotSharingGroup sharingGroup = new SlotSharingGroup();
 			CoLocationConstraint c1 = new CoLocationConstraint(new CoLocationGroup());
 			
-			AllocatedSlot s1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 0, 1), sharingGroup, c1));
+			SimpleSlot s1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 0, 1), sharingGroup, c1));
 			s1.releaseSlot();
 			
 			scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 0, 1)));
@@ -296,16 +296,16 @@ public class ScheduleWithCoLocationHintTest {
 			scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 3, 4), shareGroup));
 			
 			// second wave
-			AllocatedSlot s21 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 0, 4), shareGroup, clc1));
-			AllocatedSlot s22 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 2, 4), shareGroup, clc2));
-			AllocatedSlot s23 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 1, 4), shareGroup, clc3));
-			AllocatedSlot s24 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 3, 4), shareGroup, clc4));
+			SimpleSlot s21 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 0, 4), shareGroup, clc1));
+			SimpleSlot s22 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 2, 4), shareGroup, clc2));
+			SimpleSlot s23 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 1, 4), shareGroup, clc3));
+			SimpleSlot s24 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 3, 4), shareGroup, clc4));
 			
 			// third wave
-			AllocatedSlot s31 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid3, 1, 4), shareGroup, clc2));
-			AllocatedSlot s32 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid3, 2, 4), shareGroup, clc3));
-			AllocatedSlot s33 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid3, 3, 4), shareGroup, clc4));
-			AllocatedSlot s34 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid3, 0, 4), shareGroup, clc1));
+			SimpleSlot s31 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid3, 1, 4), shareGroup, clc2));
+			SimpleSlot s32 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid3, 2, 4), shareGroup, clc3));
+			SimpleSlot s33 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid3, 3, 4), shareGroup, clc4));
+			SimpleSlot s34 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid3, 0, 4), shareGroup, clc1));
 			
 			scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid4, 0, 4), shareGroup));
 			scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid4, 1, 4), shareGroup));
@@ -352,24 +352,24 @@ public class ScheduleWithCoLocationHintTest {
 			CoLocationConstraint cc2 = new CoLocationConstraint(ccg);
 
 			// schedule something into the shared group so that both instances are in the sharing group
-			AllocatedSlot s1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid1, 0, 2, i1), sharingGroup));
-			AllocatedSlot s2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid1, 1, 2, i2), sharingGroup));
+			SimpleSlot s1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid1, 0, 2, i1), sharingGroup));
+			SimpleSlot s2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid1, 1, 2, i2), sharingGroup));
 			
 			// schedule one locally to instance 1
-			AllocatedSlot s3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid2, 0, 2, i1), sharingGroup, cc1));
+			SimpleSlot s3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid2, 0, 2, i1), sharingGroup, cc1));
 
 			// schedule with co location constraint (yet unassigned) and a preference for
 			// instance 1, but it can only get instance 2
-			AllocatedSlot s4 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid2, 1, 2, i1), sharingGroup, cc2));
+			SimpleSlot s4 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid2, 1, 2, i1), sharingGroup, cc2));
 			
 			// schedule something into the assigned co-location constraints and check that they override the
 			// other preferences
-			AllocatedSlot s5 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid3, 0, 2, i2), sharingGroup, cc1));
-			AllocatedSlot s6 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid3, 1, 2, i1), sharingGroup, cc2));
+			SimpleSlot s5 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid3, 0, 2, i2), sharingGroup, cc1));
+			SimpleSlot s6 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid3, 1, 2, i1), sharingGroup, cc2));
 			
 			// check that each slot got three
-			assertEquals(3, ((SubSlot) s1).getSharedSlot().getNumberOfAllocatedSubSlots());
-			assertEquals(3, ((SubSlot) s2).getSharedSlot().getNumberOfAllocatedSubSlots());
+			assertEquals(3, s1.getRoot().getNumberLeaves());
+			assertEquals(3, s2.getRoot().getNumberLeaves());
 			
 			assertEquals(s1.getInstance(), s3.getInstance());
 			assertEquals(s2.getInstance(), s4.getInstance());
@@ -420,8 +420,8 @@ public class ScheduleWithCoLocationHintTest {
 			CoLocationConstraint cc1 = new CoLocationConstraint(ccg);
 			CoLocationConstraint cc2 = new CoLocationConstraint(ccg);
 
-			AllocatedSlot s1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid1, 0, 2, i1), sharingGroup, cc1));
-			AllocatedSlot s2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid1, 1, 2, i2), sharingGroup, cc2));
+			SimpleSlot s1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid1, 0, 2, i1), sharingGroup, cc1));
+			SimpleSlot s2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid1, 1, 2, i2), sharingGroup, cc2));
 			
 			s1.releaseSlot();
 			s2.releaseSlot();
@@ -429,8 +429,8 @@ public class ScheduleWithCoLocationHintTest {
 			assertEquals(2, scheduler.getNumberOfAvailableSlots());
 			assertEquals(0, sharingGroup.getTaskAssignment().getNumberOfSlots());
 
-			AllocatedSlot s3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid2, 0, 2, i2), sharingGroup, cc1));
-			AllocatedSlot s4 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid2, 1, 2, i1), sharingGroup, cc2));
+			SimpleSlot s3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid2, 0, 2, i2), sharingGroup, cc1));
+			SimpleSlot s4 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid2, 1, 2, i1), sharingGroup, cc2));
 			
 			// still preserves the previous instance mapping)
 			assertEquals(i1, s3.getInstance());
@@ -474,8 +474,8 @@ public class ScheduleWithCoLocationHintTest {
 			CoLocationConstraint cc1 = new CoLocationConstraint(ccg);
 			CoLocationConstraint cc2 = new CoLocationConstraint(ccg);
 
-			AllocatedSlot s1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid1, 0, 2, i1), sharingGroup, cc1));
-			AllocatedSlot s2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid1, 1, 2, i2), sharingGroup, cc2));
+			SimpleSlot s1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid1, 0, 2, i1), sharingGroup, cc1));
+			SimpleSlot s2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid1, 1, 2, i2), sharingGroup, cc2));
 			
 			s1.releaseSlot();
 			s2.releaseSlot();
@@ -483,8 +483,8 @@ public class ScheduleWithCoLocationHintTest {
 			assertEquals(2, scheduler.getNumberOfAvailableSlots());
 			assertEquals(0, sharingGroup.getTaskAssignment().getNumberOfSlots());
 
-			AllocatedSlot sa = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jidx, 0, 2)));
-			AllocatedSlot sb = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jidx, 1, 2)));
+			SimpleSlot sa = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jidx, 0, 2)));
+			SimpleSlot sb = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jidx, 1, 2)));
 			
 			try {
 				scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid2, 0, 2, i2), sharingGroup, cc1));
@@ -535,15 +535,15 @@ public class ScheduleWithCoLocationHintTest {
 			// schedule something from the second job vertex id before the first is filled,
 			// and give locality preferences that hint at using the same shared slot for both
 			// co location constraints (which we seek to prevent)
-			AllocatedSlot s1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid1, 0, 2, i1), sharingGroup, cc1));
-			AllocatedSlot s2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid2, 0, 2, i1), sharingGroup, cc2));
+			SimpleSlot s1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid1, 0, 2, i1), sharingGroup, cc1));
+			SimpleSlot s2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid2, 0, 2, i1), sharingGroup, cc2));
 
-			AllocatedSlot s3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid2, 1, 2, i1), sharingGroup, cc1));
-			AllocatedSlot s4 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid1, 1, 2, i1), sharingGroup, cc2));
+			SimpleSlot s3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid2, 1, 2, i1), sharingGroup, cc1));
+			SimpleSlot s4 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid1, 1, 2, i1), sharingGroup, cc2));
 			
 			// check that each slot got three
-			assertEquals(2, ((SubSlot) s1).getSharedSlot().getNumberOfAllocatedSubSlots());
-			assertEquals(2, ((SubSlot) s2).getSharedSlot().getNumberOfAllocatedSubSlots());
+			assertEquals(2, s1.getRoot().getNumberLeaves());
+			assertEquals(2, s2.getRoot().getNumberLeaves());
 			
 			assertEquals(s1.getInstance(), s3.getInstance());
 			assertEquals(s2.getInstance(), s4.getInstance());
@@ -594,15 +594,15 @@ public class ScheduleWithCoLocationHintTest {
 			CoLocationConstraint cc1 = new CoLocationConstraint(ccg);
 			CoLocationConstraint cc2 = new CoLocationConstraint(ccg);
 
-			AllocatedSlot s1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid1, 0, 2, i1), sharingGroup, cc1));
-			AllocatedSlot s2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid1, 1, 2, i2), sharingGroup, cc2));
+			SimpleSlot s1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid1, 0, 2, i1), sharingGroup, cc1));
+			SimpleSlot s2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid1, 1, 2, i2), sharingGroup, cc2));
 
-			AllocatedSlot s3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid2, 0, 2, i1), sharingGroup));
-			AllocatedSlot s4 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid2, 1, 2, i1), sharingGroup));
+			SimpleSlot s3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid2, 0, 2, i1), sharingGroup));
+			SimpleSlot s4 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid2, 1, 2, i1), sharingGroup));
 			
-			// check that each slot got three
-			assertEquals(2, ((SubSlot) s1).getSharedSlot().getNumberOfAllocatedSubSlots());
-			assertEquals(2, ((SubSlot) s2).getSharedSlot().getNumberOfAllocatedSubSlots());
+			// check that each slot got two
+			assertEquals(2, s1.getRoot().getNumberLeaves());
+			assertEquals(2, s2.getRoot().getNumberLeaves());
 			
 			s1.releaseSlot();
 			s2.releaseSlot();
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SchedulerIsolatedTasksTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SchedulerIsolatedTasksTest.java
index 72098426d70..a7f0f0482f7 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SchedulerIsolatedTasksTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SchedulerIsolatedTasksTest.java
@@ -24,6 +24,7 @@ import static org.apache.flink.runtime.jobmanager.scheduler.SchedulerTestUtils.g
 import static org.apache.flink.runtime.jobmanager.scheduler.SchedulerTestUtils.getRandomInstance;
 import static org.junit.Assert.*;
 
+import org.apache.flink.runtime.instance.SimpleSlot;
 import akka.actor.ActorSystem;
 import akka.testkit.JavaTestKit;
 import org.apache.flink.runtime.testingUtils.TestingUtils;
@@ -40,7 +41,6 @@ import java.util.List;
 import java.util.Set;
 import java.util.concurrent.atomic.AtomicBoolean;
 
-import org.apache.flink.runtime.instance.AllocatedSlot;
 import org.apache.flink.runtime.instance.Instance;
 
 /**
@@ -137,11 +137,11 @@ public class SchedulerIsolatedTasksTest {
 			assertEquals(5, scheduler.getNumberOfAvailableSlots());
 			
 			// schedule something into all slots
-			AllocatedSlot s1 = scheduler.scheduleImmediately(new ScheduledUnit(getDummyTask()));
-			AllocatedSlot s2 = scheduler.scheduleImmediately(new ScheduledUnit(getDummyTask()));
-			AllocatedSlot s3 = scheduler.scheduleImmediately(new ScheduledUnit(getDummyTask()));
-			AllocatedSlot s4 = scheduler.scheduleImmediately(new ScheduledUnit(getDummyTask()));
-			AllocatedSlot s5 = scheduler.scheduleImmediately(new ScheduledUnit(getDummyTask()));
+			SimpleSlot s1 = scheduler.scheduleImmediately(new ScheduledUnit(getDummyTask()));
+			SimpleSlot s2 = scheduler.scheduleImmediately(new ScheduledUnit(getDummyTask()));
+			SimpleSlot s3 = scheduler.scheduleImmediately(new ScheduledUnit(getDummyTask()));
+			SimpleSlot s4 = scheduler.scheduleImmediately(new ScheduledUnit(getDummyTask()));
+			SimpleSlot s5 = scheduler.scheduleImmediately(new ScheduledUnit(getDummyTask()));
 			
 			// the slots should all be different
 			assertTrue(areAllDistinct(s1, s2, s3, s4, s5));
@@ -160,8 +160,8 @@ public class SchedulerIsolatedTasksTest {
 			assertEquals(2, scheduler.getNumberOfAvailableSlots());
 			
 			// now we can schedule some more slots
-			AllocatedSlot s6 = scheduler.scheduleImmediately(new ScheduledUnit(getDummyTask()));
-			AllocatedSlot s7 = scheduler.scheduleImmediately(new ScheduledUnit(getDummyTask()));
+			SimpleSlot s6 = scheduler.scheduleImmediately(new ScheduledUnit(getDummyTask()));
+			SimpleSlot s7 = scheduler.scheduleImmediately(new ScheduledUnit(getDummyTask()));
 			
 			assertTrue(areAllDistinct(s1, s2, s3, s4, s5, s6, s7));
 			
@@ -215,7 +215,7 @@ public class SchedulerIsolatedTasksTest {
 			List<SlotAllocationFuture> allAllocatedSlots = new ArrayList<SlotAllocationFuture>();
 			
 			// slots that need to be released
-			final Set<AllocatedSlot> toRelease = new HashSet<AllocatedSlot>();
+			final Set<SimpleSlot> toRelease = new HashSet<SimpleSlot>();
 			
 			// flag to track errors in the concurrent thread
 			final AtomicBoolean errored = new AtomicBoolean(false);
@@ -223,7 +223,7 @@ public class SchedulerIsolatedTasksTest {
 			
 			SlotAllocationFutureAction action = new SlotAllocationFutureAction() {
 				@Override
-				public void slotAllocated(AllocatedSlot slot) {
+				public void slotAllocated(SimpleSlot slot) {
 					synchronized (toRelease) {
 						toRelease.add(slot);
 						toRelease.notifyAll();
@@ -244,8 +244,8 @@ public class SchedulerIsolatedTasksTest {
 									toRelease.wait();
 								}
 								
-								Iterator<AllocatedSlot> iter = toRelease.iterator();
-								AllocatedSlot next = iter.next();
+								Iterator<SimpleSlot> iter = toRelease.iterator();
+								SimpleSlot next = iter.next();
 								iter.remove();
 								
 								next.releaseSlot();
@@ -272,7 +272,7 @@ public class SchedulerIsolatedTasksTest {
 			
 			assertFalse("The slot releasing thread caused an error.", errored.get());
 			
-			List<AllocatedSlot> slotsAfter = new ArrayList<AllocatedSlot>();
+			List<SimpleSlot> slotsAfter = new ArrayList<SimpleSlot>();
 			for (SlotAllocationFuture future : allAllocatedSlots) {
 				slotsAfter.add(future.waitTillAllocated());
 			}
@@ -303,7 +303,7 @@ public class SchedulerIsolatedTasksTest {
 			scheduler.newInstanceAvailable(i2);
 			scheduler.newInstanceAvailable(i3);
 			
-			List<AllocatedSlot> slots = new ArrayList<AllocatedSlot>();
+			List<SimpleSlot> slots = new ArrayList<SimpleSlot>();
 			slots.add(scheduler.scheduleImmediately(new ScheduledUnit(getDummyTask())));
 			slots.add(scheduler.scheduleImmediately(new ScheduledUnit(getDummyTask())));
 			slots.add(scheduler.scheduleImmediately(new ScheduledUnit(getDummyTask())));
@@ -312,7 +312,7 @@ public class SchedulerIsolatedTasksTest {
 			
 			i2.markDead();
 			
-			for (AllocatedSlot slot : slots) {
+			for (SimpleSlot slot : slots) {
 				if (slot.getInstance() == i2) {
 					assertTrue(slot.isCanceled());
 				} else {
@@ -364,7 +364,7 @@ public class SchedulerIsolatedTasksTest {
 			scheduler.newInstanceAvailable(i3);
 			
 			// schedule something on an arbitrary instance
-			AllocatedSlot s1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(Collections.<Instance>emptyList())));
+			SimpleSlot s1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(Collections.<Instance>emptyList())));
 			
 			// figure out how we use the location hints
 			Instance first = s1.getInstance();
@@ -372,28 +372,28 @@ public class SchedulerIsolatedTasksTest {
 			Instance third = first == i3 ? i2 : i3;
 			
 			// something that needs to go to the first instance again
-			AllocatedSlot s2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(Collections.singletonList(s1.getInstance()))));
+			SimpleSlot s2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(Collections.singletonList(s1.getInstance()))));
 			assertEquals(first, s2.getInstance());
 
 			// first or second --> second, because first is full
-			AllocatedSlot s3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(Arrays.asList(first, second))));
+			SimpleSlot s3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(Arrays.asList(first, second))));
 			assertEquals(second, s3.getInstance());
 			
 			// first or third --> third (because first is full)
-			AllocatedSlot s4 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(Arrays.asList(first, third))));
-			AllocatedSlot s5 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(Arrays.asList(first, third))));
+			SimpleSlot s4 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(Arrays.asList(first, third))));
+			SimpleSlot s5 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(Arrays.asList(first, third))));
 			assertEquals(third, s4.getInstance());
 			assertEquals(third, s5.getInstance());
 			
 			// first or third --> second, because all others are full
-			AllocatedSlot s6 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(Arrays.asList(first, third))));
+			SimpleSlot s6 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(Arrays.asList(first, third))));
 			assertEquals(second, s6.getInstance());
 			
 			// release something on the first and second instance
 			s2.releaseSlot();
 			s6.releaseSlot();
 			
-			AllocatedSlot s7 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(Arrays.asList(first, third))));
+			SimpleSlot s7 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(Arrays.asList(first, third))));
 			assertEquals(first, s7.getInstance());
 			
 			assertEquals(1, scheduler.getNumberOfUnconstrainedAssignments());
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SchedulerSlotSharingTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SchedulerSlotSharingTest.java
index de90701cfe1..d1c69386860 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SchedulerSlotSharingTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SchedulerSlotSharingTest.java
@@ -31,13 +31,13 @@ import java.util.concurrent.Executors;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicInteger;
 
+import org.apache.flink.runtime.instance.SimpleSlot;
 import akka.actor.ActorSystem;
 import akka.testkit.JavaTestKit;
 import org.apache.flink.runtime.testingUtils.TestingUtils;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 import org.junit.Test;
-import org.apache.flink.runtime.instance.AllocatedSlot;
 import org.apache.flink.runtime.instance.Instance;
 import org.apache.flink.runtime.jobgraph.JobVertexID;
 
@@ -74,10 +74,10 @@ public class SchedulerSlotSharingTest {
 			scheduler.newInstanceAvailable(i2);
 			
 			// schedule 4 tasks from the first vertex group
-			AllocatedSlot s1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 0, 8), sharingGroup));
-			AllocatedSlot s2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 1, 8), sharingGroup));
-			AllocatedSlot s3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 2, 8), sharingGroup));
-			AllocatedSlot s4 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 3, 8), sharingGroup));
+			SimpleSlot s1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 0, 8), sharingGroup));
+			SimpleSlot s2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 1, 8), sharingGroup));
+			SimpleSlot s3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 2, 8), sharingGroup));
+			SimpleSlot s4 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 3, 8), sharingGroup));
 			
 			assertNotNull(s1);
 			assertNotNull(s2);
@@ -102,7 +102,7 @@ public class SchedulerSlotSharingTest {
 			s3.releaseSlot();
 			
 			// allocate another slot from that group
-			AllocatedSlot s5 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 4, 8), sharingGroup));
+			SimpleSlot s5 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 4, 8), sharingGroup));
 			assertNotNull(s5);
 			
 			// release all old slots
@@ -110,9 +110,9 @@ public class SchedulerSlotSharingTest {
 			s2.releaseSlot();
 			s4.releaseSlot();
 			
-			AllocatedSlot s6 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 5, 8), sharingGroup));
-			AllocatedSlot s7 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 6, 8), sharingGroup));
-			AllocatedSlot s8 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 7, 8), sharingGroup));
+			SimpleSlot s6 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 5, 8), sharingGroup));
+			SimpleSlot s7 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 6, 8), sharingGroup));
+			SimpleSlot s8 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 7, 8), sharingGroup));
 			
 			assertNotNull(s6);
 			assertNotNull(s7);
@@ -159,10 +159,10 @@ public class SchedulerSlotSharingTest {
 			scheduler.newInstanceAvailable(getRandomInstance(2));
 			
 			// schedule 4 tasks from the first vertex group
-			AllocatedSlot s1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 0, 5), sharingGroup));
-			AllocatedSlot s2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 1, 5), sharingGroup));
-			AllocatedSlot s3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 2, 5), sharingGroup));
-			AllocatedSlot s4 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 3, 5), sharingGroup));
+			SimpleSlot s1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 0, 5), sharingGroup));
+			SimpleSlot s2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 1, 5), sharingGroup));
+			SimpleSlot s3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 2, 5), sharingGroup));
+			SimpleSlot s4 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 3, 5), sharingGroup));
 			
 			assertNotNull(s1);
 			assertNotNull(s2);
@@ -184,10 +184,10 @@ public class SchedulerSlotSharingTest {
 			}
 			
 			// schedule some tasks from the second ID group
-			AllocatedSlot s1_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 0, 5), sharingGroup));
-			AllocatedSlot s2_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 1, 5), sharingGroup));
-			AllocatedSlot s3_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 2, 5), sharingGroup));
-			AllocatedSlot s4_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 3, 5), sharingGroup));
+			SimpleSlot s1_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 0, 5), sharingGroup));
+			SimpleSlot s2_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 1, 5), sharingGroup));
+			SimpleSlot s3_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 2, 5), sharingGroup));
+			SimpleSlot s4_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 3, 5), sharingGroup));
 			
 			assertNotNull(s1_2);
 			assertNotNull(s2_2);
@@ -228,7 +228,7 @@ public class SchedulerSlotSharingTest {
 			}
 			
 			// we can schedule something from the first vertex group
-			AllocatedSlot s5 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 4, 5), sharingGroup));
+			SimpleSlot s5 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 4, 5), sharingGroup));
 			assertNotNull(s5);
 			
 			assertEquals(4, sharingGroup.getTaskAssignment().getNumberOfSlots());
@@ -238,7 +238,7 @@ public class SchedulerSlotSharingTest {
 			
 			// now we release a slot from the second vertex group and schedule another task from that group
 			s2_2.releaseSlot();
-			AllocatedSlot s5_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 4, 5), sharingGroup));
+			SimpleSlot s5_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 4, 5), sharingGroup));
 			assertNotNull(s5_2);
 			
 			// release all slots
@@ -279,10 +279,10 @@ public class SchedulerSlotSharingTest {
 			scheduler.newInstanceAvailable(getRandomInstance(2));
 			
 			// schedule 4 tasks from the first vertex group
-			AllocatedSlot s1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 0, 4), sharingGroup));
-			AllocatedSlot s2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 1, 4), sharingGroup));
-			AllocatedSlot s3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 2, 4), sharingGroup));
-			AllocatedSlot s4 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 3, 4), sharingGroup));
+			SimpleSlot s1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 0, 4), sharingGroup));
+			SimpleSlot s2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 1, 4), sharingGroup));
+			SimpleSlot s3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 2, 4), sharingGroup));
+			SimpleSlot s4 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 3, 4), sharingGroup));
 			
 			assertEquals(4, sharingGroup.getTaskAssignment().getNumberOfSlots());
 			assertEquals(0, sharingGroup.getTaskAssignment().getNumberOfAvailableSlotsForJid(jid1));
@@ -298,10 +298,10 @@ public class SchedulerSlotSharingTest {
 			assertEquals(0, sharingGroup.getTaskAssignment().getNumberOfAvailableSlotsForJid(jid2));
 			
 			// schedule some tasks from the second ID group
-			AllocatedSlot s1_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 0, 4), sharingGroup));
-			AllocatedSlot s2_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 1, 4), sharingGroup));
-			AllocatedSlot s3_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 2, 4), sharingGroup));
-			AllocatedSlot s4_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 3, 4), sharingGroup));
+			SimpleSlot s1_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 0, 4), sharingGroup));
+			SimpleSlot s2_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 1, 4), sharingGroup));
+			SimpleSlot s3_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 2, 4), sharingGroup));
+			SimpleSlot s4_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 3, 4), sharingGroup));
 
 			assertEquals(4, sharingGroup.getTaskAssignment().getNumberOfSlots());
 			assertEquals(4, sharingGroup.getTaskAssignment().getNumberOfAvailableSlotsForJid(jid1));
@@ -344,10 +344,10 @@ public class SchedulerSlotSharingTest {
 			scheduler.newInstanceAvailable(getRandomInstance(2));
 			
 			// schedule 4 tasks from the first vertex group
-			AllocatedSlot s1_1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 0, 4), sharingGroup));
-			AllocatedSlot s2_1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 1, 4), sharingGroup));
-			AllocatedSlot s3_1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 2, 4), sharingGroup));
-			AllocatedSlot s4_1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 3, 4), sharingGroup));
+			SimpleSlot s1_1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 0, 4), sharingGroup));
+			SimpleSlot s2_1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 1, 4), sharingGroup));
+			SimpleSlot s3_1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 2, 4), sharingGroup));
+			SimpleSlot s4_1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 3, 4), sharingGroup));
 			
 			assertNotNull(s1_1);
 			assertNotNull(s2_1);
@@ -357,10 +357,10 @@ public class SchedulerSlotSharingTest {
 			assertTrue(areAllDistinct(s1_1, s2_1, s3_1, s4_1));
 			
 			// schedule 4 tasks from the second vertex group
-			AllocatedSlot s1_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 0, 7), sharingGroup));
-			AllocatedSlot s2_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 1, 7), sharingGroup));
-			AllocatedSlot s3_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 2, 7), sharingGroup));
-			AllocatedSlot s4_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 3, 7), sharingGroup));
+			SimpleSlot s1_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 0, 7), sharingGroup));
+			SimpleSlot s2_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 1, 7), sharingGroup));
+			SimpleSlot s3_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 2, 7), sharingGroup));
+			SimpleSlot s4_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 3, 7), sharingGroup));
 			
 			assertNotNull(s1_2);
 			assertNotNull(s2_2);
@@ -370,10 +370,10 @@ public class SchedulerSlotSharingTest {
 			assertTrue(areAllDistinct(s1_2, s2_2, s3_2, s4_2));
 			
 			// schedule 4 tasks from the third vertex group
-			AllocatedSlot s1_3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid3, 0, 4), sharingGroup));
-			AllocatedSlot s2_3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid3, 1, 4), sharingGroup));
-			AllocatedSlot s3_3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid3, 2, 4), sharingGroup));
-			AllocatedSlot s4_3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid3, 3, 4), sharingGroup));
+			SimpleSlot s1_3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid3, 0, 4), sharingGroup));
+			SimpleSlot s2_3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid3, 1, 4), sharingGroup));
+			SimpleSlot s3_3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid3, 2, 4), sharingGroup));
+			SimpleSlot s4_3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid3, 3, 4), sharingGroup));
 			
 			assertNotNull(s1_3);
 			assertNotNull(s2_3);
@@ -401,9 +401,9 @@ public class SchedulerSlotSharingTest {
 			s3_2.releaseSlot();
 			s4_2.releaseSlot();
 			
-			AllocatedSlot s5_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 5, 7), sharingGroup));
-			AllocatedSlot s6_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 6, 7), sharingGroup));
-			AllocatedSlot s7_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 7, 7), sharingGroup));
+			SimpleSlot s5_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 5, 7), sharingGroup));
+			SimpleSlot s6_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 6, 7), sharingGroup));
+			SimpleSlot s7_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 7, 7), sharingGroup));
 			
 			assertNotNull(s5_2);
 			assertNotNull(s6_2);
@@ -454,9 +454,9 @@ public class SchedulerSlotSharingTest {
 			scheduler.newInstanceAvailable(getRandomInstance(2));
 			
 			// schedule 1 tasks from the first vertex group and 2 from the second
-			AllocatedSlot s1_1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 0, 2), sharingGroup));
-			AllocatedSlot s2_1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 0, 2), sharingGroup));
-			AllocatedSlot s2_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 1, 2), sharingGroup));
+			SimpleSlot s1_1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 0, 2), sharingGroup));
+			SimpleSlot s2_1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 0, 2), sharingGroup));
+			SimpleSlot s2_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 1, 2), sharingGroup));
 			
 			assertNotNull(s1_1);
 			assertNotNull(s2_1);
@@ -472,7 +472,7 @@ public class SchedulerSlotSharingTest {
 			
 			
 			// this should free one slot so we can allocate one non-shared
-			AllocatedSlot sx = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid3, 0, 1)));
+			SimpleSlot sx = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid3, 0, 1)));
 			assertNotNull(sx);
 			
 			assertEquals(1, sharingGroup.getTaskAssignment().getNumberOfSlots());
@@ -507,23 +507,23 @@ public class SchedulerSlotSharingTest {
 			scheduler.newInstanceAvailable(getRandomInstance(2));
 			
 			// schedule some individual vertices
-			AllocatedSlot sA1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jidA, 0, 2)));
-			AllocatedSlot sA2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jidA, 1, 2)));
+			SimpleSlot sA1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jidA, 0, 2)));
+			SimpleSlot sA2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jidA, 1, 2)));
 			assertNotNull(sA1);
 			assertNotNull(sA2);
 			
 			// schedule some vertices in the sharing group
-			AllocatedSlot s1_0 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 0, 4), sharingGroup));
-			AllocatedSlot s1_1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 1, 4), sharingGroup));
-			AllocatedSlot s2_0 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 0, 4), sharingGroup));
-			AllocatedSlot s2_1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 1, 4), sharingGroup));
+			SimpleSlot s1_0 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 0, 4), sharingGroup));
+			SimpleSlot s1_1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 1, 4), sharingGroup));
+			SimpleSlot s2_0 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 0, 4), sharingGroup));
+			SimpleSlot s2_1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 1, 4), sharingGroup));
 			assertNotNull(s1_0);
 			assertNotNull(s1_1);
 			assertNotNull(s2_0);
 			assertNotNull(s2_1);
 			
 			// schedule another isolated vertex
-			AllocatedSlot sB1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jidB, 1, 3)));
+			SimpleSlot sB1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jidB, 1, 3)));
 			assertNotNull(sB1);
 			
 			// should not be able to schedule more vertices
@@ -574,8 +574,8 @@ public class SchedulerSlotSharingTest {
 			// release some isolated task and check that the sharing group may grow
 			sA1.releaseSlot();
 			
-			AllocatedSlot s1_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 2, 4), sharingGroup));
-			AllocatedSlot s2_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 3, 4), sharingGroup));
+			SimpleSlot s1_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 2, 4), sharingGroup));
+			SimpleSlot s2_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 3, 4), sharingGroup));
 			assertNotNull(s1_2);
 			assertNotNull(s2_2);
 			
@@ -587,19 +587,19 @@ public class SchedulerSlotSharingTest {
 			assertEquals(1, scheduler.getNumberOfAvailableSlots());
 			
 			// schedule one more no-shared task
-			AllocatedSlot sB0 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jidB, 0, 3)));
+			SimpleSlot sB0 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jidB, 0, 3)));
 			assertNotNull(sB0);
 			
 			// release the last of the original shared slots and allocate one more non-shared slot
 			s2_1.releaseSlot();
-			AllocatedSlot sB2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jidB, 2, 3)));
+			SimpleSlot sB2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jidB, 2, 3)));
 			assertNotNull(sB2);
 			
 			
 			// release on non-shared and add some shared slots
 			sA2.releaseSlot();
-			AllocatedSlot s1_3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 3, 4), sharingGroup));
-			AllocatedSlot s2_3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 2, 4), sharingGroup));
+			SimpleSlot s1_3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 3, 4), sharingGroup));
+			SimpleSlot s2_3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 2, 4), sharingGroup));
 			assertNotNull(s1_3);
 			assertNotNull(s2_3);
 			
@@ -609,8 +609,8 @@ public class SchedulerSlotSharingTest {
 			s1_3.releaseSlot();
 			s2_3.releaseSlot();
 			
-			AllocatedSlot sC0 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jidC, 1, 2)));
-			AllocatedSlot sC1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jidC, 0, 2)));
+			SimpleSlot sC0 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jidC, 1, 2)));
+			SimpleSlot sC1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jidC, 0, 2)));
 			assertNotNull(sC0);
 			assertNotNull(sC1);
 			
@@ -655,8 +655,8 @@ public class SchedulerSlotSharingTest {
 			
 			
 			// schedule one to each instance
-			AllocatedSlot s1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid1, 0, 2, i1), sharingGroup));
-			AllocatedSlot s2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid1, 1, 2, i2), sharingGroup));
+			SimpleSlot s1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid1, 0, 2, i1), sharingGroup));
+			SimpleSlot s2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid1, 1, 2, i2), sharingGroup));
 			assertNotNull(s1);
 			assertNotNull(s2);
 			
@@ -665,8 +665,8 @@ public class SchedulerSlotSharingTest {
 			assertEquals(1, i2.getNumberOfAvailableSlots());
 			
 			// schedule one from the other group to each instance
-			AllocatedSlot s3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid2, 0, 2, i1), sharingGroup));
-			AllocatedSlot s4 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid2, 1, 2, i2), sharingGroup));
+			SimpleSlot s3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid2, 0, 2, i1), sharingGroup));
+			SimpleSlot s4 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid2, 1, 2, i2), sharingGroup));
 			assertNotNull(s3);
 			assertNotNull(s4);
 			
@@ -705,8 +705,8 @@ public class SchedulerSlotSharingTest {
 			
 			
 			// schedule one to each instance
-			AllocatedSlot s1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid1, 0, 2, i1), sharingGroup));
-			AllocatedSlot s2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid1, 1, 2, i1), sharingGroup));
+			SimpleSlot s1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid1, 0, 2, i1), sharingGroup));
+			SimpleSlot s2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid1, 1, 2, i1), sharingGroup));
 			assertNotNull(s1);
 			assertNotNull(s2);
 			
@@ -715,8 +715,8 @@ public class SchedulerSlotSharingTest {
 			assertEquals(2, i2.getNumberOfAvailableSlots());
 			
 			// schedule one from the other group to each instance
-			AllocatedSlot s3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid2, 0, 2, i2), sharingGroup));
-			AllocatedSlot s4 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid2, 1, 2, i2), sharingGroup));
+			SimpleSlot s3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid2, 0, 2, i2), sharingGroup));
+			SimpleSlot s4 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid2, 1, 2, i2), sharingGroup));
 			assertNotNull(s3);
 			assertNotNull(s4);
 			
@@ -754,14 +754,14 @@ public class SchedulerSlotSharingTest {
 			scheduler.newInstanceAvailable(i2);
 			
 			// schedule until the one instance is full
-			AllocatedSlot s1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid1, 0, 2, i1), sharingGroup));
-			AllocatedSlot s2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid1, 1, 2, i1), sharingGroup));
-			AllocatedSlot s3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid2, 0, 4, i1), sharingGroup));
-			AllocatedSlot s4 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid2, 1, 4, i1), sharingGroup));
+			SimpleSlot s1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid1, 0, 2, i1), sharingGroup));
+			SimpleSlot s2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid1, 1, 2, i1), sharingGroup));
+			SimpleSlot s3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid2, 0, 4, i1), sharingGroup));
+			SimpleSlot s4 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid2, 1, 4, i1), sharingGroup));
 
 			// schedule two more with preference of same instance --> need to go to other instance
-			AllocatedSlot s5 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid2, 3, 4, i1), sharingGroup));
-			AllocatedSlot s6 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid2, 4, 4, i1), sharingGroup));
+			SimpleSlot s5 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid2, 3, 4, i1), sharingGroup));
+			SimpleSlot s6 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertexWithLocation(jid2, 4, 4, i1), sharingGroup));
 			
 			assertNotNull(s1);
 			assertNotNull(s2);
@@ -808,19 +808,19 @@ public class SchedulerSlotSharingTest {
 			scheduler.newInstanceAvailable(getRandomInstance(4));
 			
 			// allocate something from group 1 and 2 interleaved with schedule for group 3
-			AllocatedSlot slot_1_1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 0, 4), sharingGroup));
-			AllocatedSlot slot_1_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 1, 4), sharingGroup));
+			SimpleSlot slot_1_1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 0, 4), sharingGroup));
+			SimpleSlot slot_1_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 1, 4), sharingGroup));
 
-			AllocatedSlot slot_2_1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 0, 4), sharingGroup));
-			AllocatedSlot slot_2_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 1, 4), sharingGroup));
+			SimpleSlot slot_2_1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 0, 4), sharingGroup));
+			SimpleSlot slot_2_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 1, 4), sharingGroup));
 			
-			AllocatedSlot slot_3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid3, 0, 1), sharingGroup));
+			SimpleSlot slot_3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid3, 0, 1), sharingGroup));
 			
-			AllocatedSlot slot_1_3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 2, 4), sharingGroup));
-			AllocatedSlot slot_1_4 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 3, 4), sharingGroup));
+			SimpleSlot slot_1_3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 2, 4), sharingGroup));
+			SimpleSlot slot_1_4 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 3, 4), sharingGroup));
 			
-			AllocatedSlot slot_2_3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 2, 4), sharingGroup));
-			AllocatedSlot slot_2_4 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 3, 4), sharingGroup));
+			SimpleSlot slot_2_3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 2, 4), sharingGroup));
+			SimpleSlot slot_2_4 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 3, 4), sharingGroup));
 			
 			// release groups 1 and 2
 			
@@ -836,10 +836,10 @@ public class SchedulerSlotSharingTest {
 			
 			// allocate group 4
 			
-			AllocatedSlot slot_4_1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid4, 0, 4), sharingGroup));
-			AllocatedSlot slot_4_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid4, 1, 4), sharingGroup));
-			AllocatedSlot slot_4_3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid4, 2, 4), sharingGroup));
-			AllocatedSlot slot_4_4 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid4, 3, 4), sharingGroup));
+			SimpleSlot slot_4_1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid4, 0, 4), sharingGroup));
+			SimpleSlot slot_4_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid4, 1, 4), sharingGroup));
+			SimpleSlot slot_4_3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid4, 2, 4), sharingGroup));
+			SimpleSlot slot_4_4 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid4, 3, 4), sharingGroup));
 			
 			// release groups 3 and 4
 			
@@ -892,11 +892,11 @@ public class SchedulerSlotSharingTest {
 					@Override
 					public void run() {
 						try {
-							AllocatedSlot slot = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid4, enumerator4.getAndIncrement(), 4), sharingGroup));
-							
+							SimpleSlot slot = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid4, enumerator4.getAndIncrement(), 4), sharingGroup));
+
 							sleepUninterruptibly(rnd.nextInt(5));
 							slot.releaseSlot();
-							
+
 							if (completed.incrementAndGet() == 13) {
 								synchronized (completed) {
 									completed.notifyAll();
@@ -915,7 +915,7 @@ public class SchedulerSlotSharingTest {
 					public void run() {
 						try {
 							if (flag3.compareAndSet(false, true)) {
-								AllocatedSlot slot = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid3, 0, 1), sharingGroup));
+								SimpleSlot slot = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid3, 0, 1), sharingGroup));
 								
 								sleepUninterruptibly(5);
 								
@@ -944,7 +944,7 @@ public class SchedulerSlotSharingTest {
 					@Override
 					public void run() {
 						try {
-							AllocatedSlot slot = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, enumerator2.getAndIncrement(), 4), sharingGroup));
+							SimpleSlot slot = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, enumerator2.getAndIncrement(), 4), sharingGroup));
 							
 							// wait a bit till scheduling the successor
 							sleepUninterruptibly(rnd.nextInt(5));
@@ -971,7 +971,7 @@ public class SchedulerSlotSharingTest {
 					@Override
 					public void run() {
 						try {
-							AllocatedSlot slot = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, enumerator1.getAndIncrement(), 4), sharingGroup));
+							SimpleSlot slot = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, enumerator1.getAndIncrement(), 4), sharingGroup));
 							
 							// wait a bit till scheduling the successor
 							sleepUninterruptibly(rnd.nextInt(5));
@@ -1049,24 +1049,24 @@ public class SchedulerSlotSharingTest {
 			scheduler.newInstanceAvailable(getRandomInstance(4));
 			
 			// schedule one task for the first and second vertex
-			AllocatedSlot s1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 0, 1), sharingGroup));
-			AllocatedSlot s2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 0, 1), sharingGroup));
+			SimpleSlot s1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid1, 0, 1), sharingGroup));
+			SimpleSlot s2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid2, 0, 1), sharingGroup));
 			
-			assertTrue( ((SubSlot) s1).getSharedSlot() == ((SubSlot) s2).getSharedSlot() );
+			assertTrue(  s1.getParent() == s2.getParent() );
 			assertEquals(3, scheduler.getNumberOfAvailableSlots());
 			
-			AllocatedSlot s3_0 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid3, 0, 5), sharingGroup));
-			AllocatedSlot s3_1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid3, 1, 5), sharingGroup));
-			AllocatedSlot s4_0 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid4, 0, 4), sharingGroup));
-			AllocatedSlot s4_1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid4, 1, 4), sharingGroup));
+			SimpleSlot s3_0 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid3, 0, 5), sharingGroup));
+			SimpleSlot s3_1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid3, 1, 5), sharingGroup));
+			SimpleSlot s4_0 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid4, 0, 4), sharingGroup));
+			SimpleSlot s4_1 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid4, 1, 4), sharingGroup));
 			
 			s1.releaseSlot();
 			s2.releaseSlot();
 			
-			AllocatedSlot s3_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid3, 2, 5), sharingGroup));
-			AllocatedSlot s3_3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid3, 3, 5), sharingGroup));
-			AllocatedSlot s4_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid4, 2, 4), sharingGroup));
-			AllocatedSlot s4_3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid4, 3, 4), sharingGroup));
+			SimpleSlot s3_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid3, 2, 5), sharingGroup));
+			SimpleSlot s3_3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid3, 3, 5), sharingGroup));
+			SimpleSlot s4_2 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid4, 2, 4), sharingGroup));
+			SimpleSlot s4_3 = scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid4, 3, 4), sharingGroup));
 			
 			try {
 				scheduler.scheduleImmediately(new ScheduledUnit(getTestVertex(jid3, 4, 5), sharingGroup));
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SharedSlotsTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SharedSlotsTest.java
index feb30053c03..fcb638fa3ec 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SharedSlotsTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SharedSlotsTest.java
@@ -23,6 +23,8 @@ import static org.mockito.Mockito.doAnswer;
 import static org.mockito.Matchers.any;
 import static org.junit.Assert.*;
 
+import org.apache.flink.runtime.instance.SharedSlot;
+import org.apache.flink.runtime.instance.SimpleSlot;
 import org.junit.Test;
 import org.mockito.invocation.InvocationOnMock;
 import org.mockito.stubbing.Answer;
@@ -39,41 +41,45 @@ public class SharedSlotsTest {
 			doAnswer(new Answer<Void>() {
 				@Override
 				public Void answer(InvocationOnMock invocation) throws Throwable {
-					final SubSlot sub = (SubSlot) invocation.getArguments()[0];
-					final SharedSlot shared = (SharedSlot) invocation.getArguments()[1];
-					shared.releaseSlot(sub);
+					final SimpleSlot simpleSlot = (SimpleSlot) invocation.getArguments()[0];
+					final SharedSlot sharedSlot = simpleSlot.getParent();
+
+					sharedSlot.freeSubSlot(simpleSlot);
+
 					return null;
 				}
 				
-			}).when(assignment).releaseSubSlot(any(SubSlot.class), any(SharedSlot.class));
+			}).when(assignment).releaseSimpleSlot(any(SimpleSlot.class));
+
+			JobVertexID id1 = new JobVertexID();
 			
 			Instance instance = SchedulerTestUtils.getRandomInstance(1);
 			
-			SharedSlot slot = new SharedSlot(instance.allocateSlot(new JobID()), assignment);
-			assertFalse(slot.isDisposed());
+			SharedSlot slot = instance.allocateSharedSlot(new JobID(), assignment, id1);
+			assertFalse(slot.isDead());
 			
-			SubSlot ss1 = slot.allocateSubSlot(new JobVertexID());
+			SimpleSlot ss1 = slot.allocateSubSlot(id1);
 			assertNotNull(ss1);
 			
 			// verify resources
 			assertEquals(instance, ss1.getInstance());
 			assertEquals(0, ss1.getSlotNumber());
-			assertEquals(slot.getAllocatedSlot().getJobID(), ss1.getJobID());
+			assertEquals(slot.getJobID(), ss1.getJobID());
 			
-			SubSlot ss2 = slot.allocateSubSlot(new JobVertexID());
+			SimpleSlot ss2 = slot.allocateSubSlot(new JobVertexID());
 			assertNotNull(ss2);
 			
-			assertEquals(2, slot.getNumberOfAllocatedSubSlots());
+			assertEquals(2, slot.getNumberLeaves());
 			
 			// release first slot, should not trigger release
 			ss1.releaseSlot();
-			assertFalse(slot.isDisposed());
+			assertFalse(slot.isDead());
 			
 			ss2.releaseSlot();
-			assertFalse(slot.isDisposed());
+			assertFalse(slot.isDead());
 			
 			// the shared slot should now dispose itself
-			assertEquals(0, slot.getNumberOfAllocatedSubSlots());
+			assertEquals(0, slot.getNumberLeaves());
 		}
 		catch (Exception e) {
 			e.printStackTrace();
@@ -85,46 +91,49 @@ public class SharedSlotsTest {
 	public void createAndRelease() {
 		try {
 			SlotSharingGroupAssignment assignment = mock(SlotSharingGroupAssignment.class);
-			doAnswer(new Answer<Void>() {
+			doAnswer(new Answer<Boolean>() {
 				@Override
-				public Void answer(InvocationOnMock invocation) throws Throwable {
-					final SubSlot sub = (SubSlot) invocation.getArguments()[0];
-					final SharedSlot shared = (SharedSlot) invocation.getArguments()[1];
-					if (shared.releaseSlot(sub) == 0) {
-						shared.dispose();
+				public Boolean answer(InvocationOnMock invocation) throws Throwable {
+					final SimpleSlot slot = (SimpleSlot) invocation.getArguments()[0];
+					final SharedSlot shared = slot.getParent();
+					if (shared.freeSubSlot(slot) == 0) {
+						shared.markDead();
+						return true;
 					}
-					return null;
+					return false;
 				}
-				
-			}).when(assignment).releaseSubSlot(any(SubSlot.class), any(SharedSlot.class));
-			
+
+			}).when(assignment).releaseSimpleSlot(any(SimpleSlot.class));
+
+			JobVertexID id1 = new JobVertexID();
+
 			Instance instance = SchedulerTestUtils.getRandomInstance(1);
 			
-			SharedSlot slot = new SharedSlot(instance.allocateSlot(new JobID()), assignment);
-			assertFalse(slot.isDisposed());
+			SharedSlot slot = instance.allocateSharedSlot(new JobID(), assignment, id1);
+			assertFalse(slot.isDead());
 			
-			SubSlot ss1 = slot.allocateSubSlot(new JobVertexID());
+			SimpleSlot ss1 = slot.allocateSubSlot(id1);
 			assertNotNull(ss1);
 			
 			// verify resources
 			assertEquals(instance, ss1.getInstance());
 			assertEquals(0, ss1.getSlotNumber());
-			assertEquals(slot.getAllocatedSlot().getJobID(), ss1.getJobID());
+			assertEquals(slot.getJobID(), ss1.getJobID());
 			
-			SubSlot ss2 = slot.allocateSubSlot(new JobVertexID());
+			SimpleSlot ss2 = slot.allocateSubSlot(new JobVertexID());
 			assertNotNull(ss2);
 			
-			assertEquals(2, slot.getNumberOfAllocatedSubSlots());
+			assertEquals(2, slot.getNumberLeaves());
 			
 			// release first slot, should not trigger release
 			ss1.releaseSlot();
-			assertFalse(slot.isDisposed());
+			assertFalse(slot.isDead());
 			
 			ss2.releaseSlot();
-			assertTrue(slot.isDisposed());
+			assertTrue(slot.isDead());
 			
 			// the shared slot should now dispose itself
-			assertEquals(0, slot.getNumberOfAllocatedSubSlots());
+			assertEquals(0, slot.getNumberLeaves());
 			
 			assertNull(slot.allocateSubSlot(new JobVertexID()));
 		}
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SlotAllocationFutureTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SlotAllocationFutureTest.java
index 47afacbe540..23a5c94c10c 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SlotAllocationFutureTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SlotAllocationFutureTest.java
@@ -23,7 +23,7 @@ import static org.junit.Assert.*;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicInteger;
 
-import org.apache.flink.runtime.instance.AllocatedSlot;
+import org.apache.flink.runtime.instance.SimpleSlot;
 import org.apache.flink.runtime.jobgraph.JobID;
 import org.junit.Test;
 
@@ -36,7 +36,7 @@ public class SlotAllocationFutureTest {
 			
 			SlotAllocationFutureAction action = new SlotAllocationFutureAction() {
 				@Override
-				public void slotAllocated(AllocatedSlot slot) {}
+				public void slotAllocated(SimpleSlot slot) {}
 			};
 			
 			future.setFutureAction(action);
@@ -47,8 +47,8 @@ public class SlotAllocationFutureTest {
 				// expected
 			}
 			
-			final AllocatedSlot slot1 = new AllocatedSlot(new JobID(), SchedulerTestUtils.getRandomInstance(1), 0);
-			final AllocatedSlot slot2 = new AllocatedSlot(new JobID(), SchedulerTestUtils.getRandomInstance(1), 0);
+			final SimpleSlot slot1 = new SimpleSlot(new JobID(), SchedulerTestUtils.getRandomInstance(1), 0, null, null);
+			final SimpleSlot slot2 = new SimpleSlot(new JobID(), SchedulerTestUtils.getRandomInstance(1), 0, null, null);
 			
 			future.setSlot(slot1);
 			try {
@@ -71,13 +71,13 @@ public class SlotAllocationFutureTest {
 			// action before the slot
 			{
 				final AtomicInteger invocations = new AtomicInteger();
-				final AllocatedSlot thisSlot = new AllocatedSlot(new JobID(), SchedulerTestUtils.getRandomInstance(1), 0);
+				final SimpleSlot thisSlot = new SimpleSlot(new JobID(), SchedulerTestUtils.getRandomInstance(1), 0, null, null);
 				
 				SlotAllocationFuture future = new SlotAllocationFuture();
 				
 				future.setFutureAction(new SlotAllocationFutureAction() {
 					@Override
-					public void slotAllocated(AllocatedSlot slot) {
+					public void slotAllocated(SimpleSlot slot) {
 						assertEquals(thisSlot, slot);
 						invocations.incrementAndGet();
 					}
@@ -91,14 +91,14 @@ public class SlotAllocationFutureTest {
 			// slot before action
 			{
 				final AtomicInteger invocations = new AtomicInteger();
-				final AllocatedSlot thisSlot = new AllocatedSlot(new JobID(), SchedulerTestUtils.getRandomInstance(1), 0);
+				final SimpleSlot thisSlot = new SimpleSlot(new JobID(), SchedulerTestUtils.getRandomInstance(1), 0, null, null);
 				
 				SlotAllocationFuture future = new SlotAllocationFuture();
 				future.setSlot(thisSlot);
 				
 				future.setFutureAction(new SlotAllocationFutureAction() {
 					@Override
-					public void slotAllocated(AllocatedSlot slot) {
+					public void slotAllocated(SimpleSlot slot) {
 						assertEquals(thisSlot, slot);
 						invocations.incrementAndGet();
 					}
@@ -121,7 +121,7 @@ public class SlotAllocationFutureTest {
 				final AtomicInteger invocations = new AtomicInteger();
 				final AtomicBoolean error = new AtomicBoolean();
 				
-				final AllocatedSlot thisSlot = new AllocatedSlot(new JobID(), SchedulerTestUtils.getRandomInstance(1), 0);
+				final SimpleSlot thisSlot = new SimpleSlot(new JobID(), SchedulerTestUtils.getRandomInstance(1), 0, null, null);
 				
 				final SlotAllocationFuture future = new SlotAllocationFuture();
 				
@@ -130,7 +130,7 @@ public class SlotAllocationFutureTest {
 					@Override
 					public void run() {
 						try {
-							AllocatedSlot syncSlot = future.waitTillAllocated();
+							SimpleSlot syncSlot = future.waitTillAllocated();
 							if (syncSlot == null || syncSlot != thisSlot) {
 								error.set(true);
 								return;
@@ -158,12 +158,12 @@ public class SlotAllocationFutureTest {
 			
 			// setting slot before syncing
 			{
-				final AllocatedSlot thisSlot = new AllocatedSlot(new JobID(), SchedulerTestUtils.getRandomInstance(1), 0);
+				final SimpleSlot thisSlot = new SimpleSlot(new JobID(), SchedulerTestUtils.getRandomInstance(1), 0, null, null);
 				final SlotAllocationFuture future = new SlotAllocationFuture();
 
 				future.setSlot(thisSlot);
 				
-				AllocatedSlot retrieved = future.waitTillAllocated();
+				SimpleSlot retrieved = future.waitTillAllocated();
 				
 				assertNotNull(retrieved);
 				assertEquals(thisSlot, retrieved);
diff --git a/flink-runtime/src/test/scala/org/apache/flink/runtime/jobmanager/JobManagerITCase.scala b/flink-runtime/src/test/scala/org/apache/flink/runtime/jobmanager/JobManagerITCase.scala
index 36f0f92e510..9bcd163a2f5 100644
--- a/flink-runtime/src/test/scala/org/apache/flink/runtime/jobmanager/JobManagerITCase.scala
+++ b/flink-runtime/src/test/scala/org/apache/flink/runtime/jobmanager/JobManagerITCase.scala
@@ -62,7 +62,7 @@ WordSpecLike with Matchers with BeforeAndAfterAll {
         within(1 second) {
           jm ! SubmitJob(jobGraph)
 
-          expectMsg(SubmissionFailure(jobGraph.getJobID, new NoResourceAvailableException(1,1)))
+          expectMsg(SubmissionFailure(jobGraph.getJobID, new NoResourceAvailableException(1,1,0)))
 
           expectNoMsg()
         }
diff --git a/flink-runtime/src/test/scala/org/apache/flink/runtime/jobmanager/TaskManagerFailsITCase.scala b/flink-runtime/src/test/scala/org/apache/flink/runtime/jobmanager/TaskManagerFailsITCase.scala
index 0f6eeca92c4..22ee2c1718c 100644
--- a/flink-runtime/src/test/scala/org/apache/flink/runtime/jobmanager/TaskManagerFailsITCase.scala
+++ b/flink-runtime/src/test/scala/org/apache/flink/runtime/jobmanager/TaskManagerFailsITCase.scala
@@ -18,7 +18,7 @@
 
 package org.apache.flink.runtime.jobmanager
 
-import akka.actor.{ActorSystem, PoisonPill}
+import akka.actor.{Kill, ActorSystem, PoisonPill}
 import akka.testkit.{ImplicitSender, TestKit}
 import org.apache.flink.runtime.jobgraph.{AbstractJobVertex, DistributionPattern, JobGraph}
 import org.apache.flink.runtime.jobmanager.Tasks.{BlockingReceiver, Sender}
@@ -41,7 +41,7 @@ with WordSpecLike with Matchers with BeforeAndAfterAll {
   }
 
   "The JobManager" should {
-    "handle failing task manager" in {
+    "handle gracefully failing task manager" in {
       val num_tasks = 31
       val sender = new AbstractJobVertex("Sender")
       val receiver = new AbstractJobVertex("Receiver")
@@ -78,6 +78,41 @@ with WordSpecLike with Matchers with BeforeAndAfterAll {
         cluster.stop()
       }
     }
+
+    "handle hard failing task manager" in {
+      val num_tasks = 31
+      val sender = new AbstractJobVertex("Sender")
+      val receiver = new AbstractJobVertex("Receiver")
+      sender.setInvokableClass(classOf[Sender])
+      receiver.setInvokableClass(classOf[BlockingReceiver])
+      sender.setParallelism(num_tasks)
+      receiver.setParallelism(num_tasks)
+      receiver.connectNewDataSetAsInput(sender, DistributionPattern.POINTWISE)
+
+      val jobGraph = new JobGraph("Pointwise Job", sender, receiver)
+      val jobID = jobGraph.getJobID
+
+      val cluster = TestingUtils.startTestingCluster(num_tasks, 2)
+
+      val taskManagers = cluster.getTaskManagers
+      val jm = cluster.getJobManager
+
+      try {
+        within(TestingUtils.TESTING_DURATION) {
+          jm ! SubmitJob(jobGraph)
+          expectMsg(SubmissionSuccess(jobGraph.getJobID))
+
+          jm ! WaitForAllVerticesToBeRunningOrFinished(jobID)
+          expectMsg(AllVerticesRunning(jobID))
+
+          // kill one task manager
+          taskManagers(0) ! Kill
+          expectMsgType[JobResultFailed]
+        }
+      }finally{
+        cluster.stop()
+      }
+    }
   }
 
 }
diff --git a/flink-runtime/src/test/scala/org/apache/flink/runtime/jobmanager/TaskManagerFailsWithSlotSharingITCase.scala b/flink-runtime/src/test/scala/org/apache/flink/runtime/jobmanager/TaskManagerFailsWithSlotSharingITCase.scala
index fba7c7690bd..626c51884eb 100644
--- a/flink-runtime/src/test/scala/org/apache/flink/runtime/jobmanager/TaskManagerFailsWithSlotSharingITCase.scala
+++ b/flink-runtime/src/test/scala/org/apache/flink/runtime/jobmanager/TaskManagerFailsWithSlotSharingITCase.scala
@@ -18,7 +18,7 @@
 
 package org.apache.flink.runtime.jobmanager
 
-import akka.actor.{ActorSystem, PoisonPill}
+import akka.actor.{Kill, ActorSystem, PoisonPill}
 import akka.testkit.{ImplicitSender, TestKit}
 import org.apache.flink.runtime.jobgraph.{AbstractJobVertex, DistributionPattern, JobGraph}
 import org.apache.flink.runtime.jobmanager.Tasks.{BlockingReceiver, Sender}
@@ -41,7 +41,7 @@ ImplicitSender with WordSpecLike with Matchers with BeforeAndAfterAll {
   }
 
   "The JobManager" should {
-    "handle task manager failures with slot sharing" in {
+    "handle gracefully failing task manager with slot sharing" in {
       val num_tasks = 20
 
       val sender = new AbstractJobVertex("Sender")
@@ -83,6 +83,48 @@ ImplicitSender with WordSpecLike with Matchers with BeforeAndAfterAll {
         cluster.stop()
       }
     }
+
+    "handle hard failing task manager with slot sharing" in {
+      val num_tasks = 20
+
+      val sender = new AbstractJobVertex("Sender")
+      val receiver = new AbstractJobVertex("Receiver")
+
+      sender.setInvokableClass(classOf[Sender])
+      receiver.setInvokableClass(classOf[BlockingReceiver])
+
+      sender.setParallelism(num_tasks)
+      receiver.setParallelism(num_tasks)
+      receiver.connectNewDataSetAsInput(sender, DistributionPattern.POINTWISE)
+
+      val sharingGroup = new SlotSharingGroup()
+      sender.setSlotSharingGroup(sharingGroup)
+      receiver.setSlotSharingGroup(sharingGroup)
+
+      val jobGraph = new JobGraph("Pointwise Job", sender, receiver)
+      val jobID = jobGraph.getJobID
+
+      val cluster = TestingUtils.startTestingCluster(num_tasks/2, 2)
+      val jm = cluster.getJobManager
+      val taskManagers = cluster.getTaskManagers
+
+      try{
+        within(TestingUtils.TESTING_DURATION) {
+          jm ! SubmitJob(jobGraph)
+          expectMsg(SubmissionSuccess(jobGraph.getJobID))
+
+          jm ! WaitForAllVerticesToBeRunningOrFinished(jobID)
+          expectMsg(AllVerticesRunning(jobID))
+
+          //kill task manager
+          taskManagers(0) ! Kill
+
+          expectMsgType[JobResultFailed]
+        }
+      }finally{
+        cluster.stop()
+      }
+    }
   }
 
 }
