diff --git a/flink-connectors/flink-connector-files/src/main/java/org/apache/flink/connector/file/src/FileSource.java b/flink-connectors/flink-connector-files/src/main/java/org/apache/flink/connector/file/src/FileSource.java
index f43b305f035..3e77e4282f3 100644
--- a/flink-connectors/flink-connector-files/src/main/java/org/apache/flink/connector/file/src/FileSource.java
+++ b/flink-connectors/flink-connector-files/src/main/java/org/apache/flink/connector/file/src/FileSource.java
@@ -142,8 +142,10 @@ public final class FileSource<T> extends AbstractFileSource<T, FileSourceSplit>
 	 * file stream.
 	 *
 	 * <p>When possible, stream-based formats are generally easier (preferable) to file-based formats,
-	 * because they support better default behavior around I/O batching, or better progress tracking to
-	 * avoid re-doing work on recovery.
+	 * because they support better default behavior around I/O batching or progress tracking (checkpoints).
+	 *
+	 * <p>Stream formats also automatically de-compress files based on the file extension. This supports
+	 * files ending in ".deflate" (Deflate), ".xz" (XZ), ".bz2" (BZip2), ".gz", ".gzip" (GZip).
 	 */
 	public static <T> FileSourceBuilder<T> forRecordStreamFormat(final StreamFormat<T> reader, final Path... paths) {
 		checkNotNull(reader, "reader");
diff --git a/flink-connectors/flink-connector-files/src/main/java/org/apache/flink/connector/file/src/impl/StreamFormatAdapter.java b/flink-connectors/flink-connector-files/src/main/java/org/apache/flink/connector/file/src/impl/StreamFormatAdapter.java
index 6bf2ff2a2a2..1bab75bb3cc 100644
--- a/flink-connectors/flink-connector-files/src/main/java/org/apache/flink/connector/file/src/impl/StreamFormatAdapter.java
+++ b/flink-connectors/flink-connector-files/src/main/java/org/apache/flink/connector/file/src/impl/StreamFormatAdapter.java
@@ -19,10 +19,13 @@
 package org.apache.flink.connector.file.src.impl;
 
 import org.apache.flink.annotation.Internal;
+import org.apache.flink.api.common.io.InputStreamFSInputWrapper;
+import org.apache.flink.api.common.io.compression.InflaterInputStreamFactory;
 import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.configuration.IllegalConfigurationException;
 import org.apache.flink.connector.file.src.FileSourceSplit;
+import org.apache.flink.connector.file.src.compression.StandardDeCompressors;
 import org.apache.flink.connector.file.src.reader.BulkFormat;
 import org.apache.flink.connector.file.src.reader.StreamFormat;
 import org.apache.flink.connector.file.src.util.CheckpointedPosition;
@@ -131,12 +134,17 @@ public final class StreamFormatAdapter<T> implements BulkFormat<T, FileSourceSpl
 							StreamFormat.FETCH_IO_SIZE.key(), fetchSize));
 		}
 
+		final InflaterInputStreamFactory<?> deCompressor =
+				StandardDeCompressors.getDecompressorForFileName(file.getPath());
+
 		final FSDataInputStream inStream = fs.open(file);
 		return doWithCleanupOnException(inStream, () -> {
-			inStream.seek(seekPosition);
-			return new TrackingFsDataInputStream(inStream, fileLength, fetchSize);
+			final FSDataInputStream in = deCompressor == null
+					? inStream
+					: new InputStreamFSInputWrapper(deCompressor.create(inStream));
+			in.seek(seekPosition);
+			return new TrackingFsDataInputStream(in, fileLength, fetchSize);
 		});
-
 	}
 
 	// ----------------------------------------------------------------------------------
diff --git a/flink-connectors/flink-connector-files/src/test/java/org/apache/flink/connector/file/src/FileSourceTextLinesITCase.java b/flink-connectors/flink-connector-files/src/test/java/org/apache/flink/connector/file/src/FileSourceTextLinesITCase.java
index cf382fea106..9fa2f8fd67a 100644
--- a/flink-connectors/flink-connector-files/src/test/java/org/apache/flink/connector/file/src/FileSourceTextLinesITCase.java
+++ b/flink-connectors/flink-connector-files/src/test/java/org/apache/flink/connector/file/src/FileSourceTextLinesITCase.java
@@ -28,18 +28,23 @@ import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 import org.apache.flink.streaming.api.operators.collect.ClientAndIterator;
 import org.apache.flink.test.util.MiniClusterWithClientResource;
 import org.apache.flink.util.TestLogger;
+import org.apache.flink.util.function.FunctionWithException;
 
 import org.junit.ClassRule;
 import org.junit.Test;
 import org.junit.rules.TemporaryFolder;
 
 import java.io.File;
-import java.io.FileWriter;
+import java.io.FileOutputStream;
 import java.io.IOException;
+import java.io.OutputStream;
+import java.io.OutputStreamWriter;
 import java.io.PrintWriter;
+import java.nio.charset.StandardCharsets;
 import java.time.Duration;
 import java.util.Arrays;
 import java.util.List;
+import java.util.zip.GZIPOutputStream;
 
 import static org.hamcrest.Matchers.equalTo;
 import static org.junit.Assert.assertThat;
@@ -246,9 +251,19 @@ public class FileSourceTextLinesITCase extends TestLogger {
 		writeFileAtomically(file, LINES_PER_FILE[num]);
 	}
 
+	private static void writeCompressedFile(File testDir, int num) throws IOException {
+		final File file = new File(testDir, FILE_PATHS[num] + ".gz");
+		writeFileAtomically(file, LINES_PER_FILE[num], GZIPOutputStream::new);
+	}
+
 	private static void writeAllFiles(File testDir) throws IOException {
 		for (int i = 0; i < FILE_PATHS.length; i++) {
-			writeFile(testDir, i);
+			// we write half of the files regularly, half compressed
+			if (i % 2 == 0) {
+				writeFile(testDir, i);
+			} else {
+				writeCompressedFile(testDir, i);
+			}
 		}
 	}
 
@@ -264,12 +279,24 @@ public class FileSourceTextLinesITCase extends TestLogger {
 		}
 	}
 
-	private static void writeFileAtomically(File file, String[] lines) throws IOException {
+	private static void writeFileAtomically(final File file, final String[] lines) throws IOException {
+		writeFileAtomically(file, lines, (v) -> v);
+	}
+
+	private static void writeFileAtomically(
+			final File file,
+			final String[] lines,
+			final FunctionWithException<OutputStream, OutputStream, IOException> streamEncoderFactory) throws IOException {
+
 		final File parent = file.getParentFile();
 		final File stagingFile = new File(parent, ".tmp-" + file.getName());
 		assertTrue(parent.mkdirs() || parent.exists());
 
-		try (PrintWriter writer = new PrintWriter(new FileWriter(stagingFile))) {
+		try (final FileOutputStream fileOut = new FileOutputStream(stagingFile);
+				final OutputStream out = streamEncoderFactory.apply(fileOut);
+				final OutputStreamWriter encoder = new OutputStreamWriter(out, StandardCharsets.UTF_8);
+				final PrintWriter writer = new PrintWriter(encoder)) {
+
 			for (String line : lines) {
 				writer.println(line);
 			}
