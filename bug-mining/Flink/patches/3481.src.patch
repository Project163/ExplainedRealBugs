diff --git a/flink-end-to-end-tests/flink-e2e-test-utils/pom.xml b/flink-end-to-end-tests/flink-e2e-test-utils/pom.xml
deleted file mode 100644
index 7e683b9595e..00000000000
--- a/flink-end-to-end-tests/flink-e2e-test-utils/pom.xml
+++ /dev/null
@@ -1,80 +0,0 @@
-<?xml version="1.0" encoding="UTF-8"?>
-<!--
-Licensed to the Apache Software Foundation (ASF) under one
-or more contributor license agreements.  See the NOTICE file
-distributed with this work for additional information
-regarding copyright ownership.  The ASF licenses this file
-to you under the Apache License, Version 2.0 (the
-"License"); you may not use this file except in compliance
-with the License.  You may obtain a copy of the License at
-
-  http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing,
-software distributed under the License is distributed on an
-"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-KIND, either express or implied.  See the License for the
-specific language governing permissions and limitations
-under the License.
--->
-<project xmlns="http://maven.apache.org/POM/4.0.0"
-		 xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-		 xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
-	<parent>
-		<artifactId>flink-end-to-end-tests</artifactId>
-		<groupId>org.apache.flink</groupId>
-		<version>1.10-SNAPSHOT</version>
-	</parent>
-	<modelVersion>4.0.0</modelVersion>
-
-	<artifactId>flink-e2e-test-utils</artifactId>
-
-	<dependencies>
-		<dependency>
-			<groupId>org.apache.flink</groupId>
-			<artifactId>flink-streaming-java_${scala.binary.version}</artifactId>
-			<version>${project.version}</version>
-		</dependency>
-		<dependency>
-			<groupId>com.amazonaws</groupId>
-			<artifactId>aws-java-sdk-s3</artifactId>
-			<version>1.11.437</version>
-		</dependency>
-	</dependencies>
-
-	<build>
-		<plugins>
-			<plugin>
-				<groupId>org.apache.maven.plugins</groupId>
-				<artifactId>maven-shade-plugin</artifactId>
-				<executions>
-					<execution>
-						<id>S3UtilProgram</id>
-						<phase>package</phase>
-						<goals>
-							<goal>shade</goal>
-						</goals>
-						<configuration>
-							<finalName>S3UtilProgram</finalName>
-							<transformers>
-								<transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
-									<mainClass>org.apache.flink.streaming.tests.util.s3.S3UtilProgram</mainClass>
-								</transformer>
-							</transformers>
-						</configuration>
-					</execution>
-				</executions>
-			</plugin>
-
-			<plugin>
-				<groupId>org.apache.maven.plugins</groupId>
-				<artifactId>maven-javadoc-plugin</artifactId>
-				<configuration>
-					<!-- Plugin fails since this module contains no public/protected classes -->
-					<skip>true</skip>
-				</configuration>
-			</plugin>
-		</plugins>
-	</build>
-
-</project>
diff --git a/flink-end-to-end-tests/flink-e2e-test-utils/src/main/java/org/apache/flink/streaming/tests/util/s3/S3QueryUtil.java b/flink-end-to-end-tests/flink-e2e-test-utils/src/main/java/org/apache/flink/streaming/tests/util/s3/S3QueryUtil.java
deleted file mode 100644
index 781a8edd20a..00000000000
--- a/flink-end-to-end-tests/flink-e2e-test-utils/src/main/java/org/apache/flink/streaming/tests/util/s3/S3QueryUtil.java
+++ /dev/null
@@ -1,92 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.streaming.tests.util.s3;
-
-import com.amazonaws.services.s3.AmazonS3;
-import com.amazonaws.services.s3.model.CSVInput;
-import com.amazonaws.services.s3.model.CSVOutput;
-import com.amazonaws.services.s3.model.CompressionType;
-import com.amazonaws.services.s3.model.ExpressionType;
-import com.amazonaws.services.s3.model.InputSerialization;
-import com.amazonaws.services.s3.model.OutputSerialization;
-import com.amazonaws.services.s3.model.SelectObjectContentEvent;
-import com.amazonaws.services.s3.model.SelectObjectContentEventStream;
-import com.amazonaws.services.s3.model.SelectObjectContentEventVisitor;
-import com.amazonaws.services.s3.model.SelectObjectContentRequest;
-import com.amazonaws.services.s3.model.SelectObjectContentResult;
-
-import java.io.ByteArrayOutputStream;
-import java.io.InputStream;
-import java.util.concurrent.atomic.AtomicBoolean;
-
-import static com.amazonaws.util.IOUtils.copy;
-
-class S3QueryUtil {
-	/** Run SQL query over non-compressed CSV file saved in s3 object. */
-	static String queryFile(
-			AmazonS3 s3client, String bucket, String s3file, @SuppressWarnings("SameParameterValue") String query) {
-		SelectObjectContentRequest request = generateBaseCSVRequest(bucket, s3file, query);
-		final AtomicBoolean isResultComplete = new AtomicBoolean(false);
-		String res;
-		try (SelectObjectContentResult result = s3client.selectObjectContent(request);
-			SelectObjectContentEventStream payload = result.getPayload();
-			ByteArrayOutputStream out = new ByteArrayOutputStream()) {
-			InputStream resultInputStream = payload.getRecordsInputStream(
-				new SelectObjectContentEventVisitor() {
-					@Override
-					public void visit(SelectObjectContentEvent.EndEvent event) {
-						isResultComplete.set(true);
-					}
-				}
-			);
-			copy(resultInputStream, out);
-			res = out.toString().trim();
-		} catch (Throwable e) {
-			System.out.println("SQL query failure");
-			throw new RuntimeException("SQL query failure", e);
-		}
-		/*
-		 * The End Event indicates all matching records have been transmitted.
-		 * If the End Event is not received, the results may be incomplete.
-		 */
-		if (!isResultComplete.get()) {
-			throw new RuntimeException("S3 Select request was incomplete as End Event was not received.");
-		}
-		return res;
-	}
-
-	private static SelectObjectContentRequest generateBaseCSVRequest(String bucket, String key, String query) {
-		SelectObjectContentRequest request = new SelectObjectContentRequest();
-		request.setBucketName(bucket);
-		request.setKey(key);
-		request.setExpression(query);
-		request.setExpressionType(ExpressionType.SQL);
-
-		InputSerialization inputSerialization = new InputSerialization();
-		inputSerialization.setCsv(new CSVInput());
-		inputSerialization.setCompressionType(CompressionType.NONE);
-		request.setInputSerialization(inputSerialization);
-
-		OutputSerialization outputSerialization = new OutputSerialization();
-		outputSerialization.setCsv(new CSVOutput());
-		request.setOutputSerialization(outputSerialization);
-
-		return request;
-	}
-}
diff --git a/flink-end-to-end-tests/flink-e2e-test-utils/src/main/java/org/apache/flink/streaming/tests/util/s3/S3UtilProgram.java b/flink-end-to-end-tests/flink-e2e-test-utils/src/main/java/org/apache/flink/streaming/tests/util/s3/S3UtilProgram.java
deleted file mode 100644
index 70cb55854c5..00000000000
--- a/flink-end-to-end-tests/flink-e2e-test-utils/src/main/java/org/apache/flink/streaming/tests/util/s3/S3UtilProgram.java
+++ /dev/null
@@ -1,225 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.streaming.tests.util.s3;
-
-import org.apache.flink.api.java.utils.ParameterTool;
-
-import com.amazonaws.services.s3.AmazonS3;
-import com.amazonaws.services.s3.AmazonS3ClientBuilder;
-import com.amazonaws.services.s3.model.DeleteObjectsRequest;
-import com.amazonaws.services.s3.model.S3ObjectSummary;
-import com.amazonaws.services.s3.transfer.KeyFilter;
-import com.amazonaws.services.s3.transfer.TransferManager;
-import com.amazonaws.services.s3.transfer.TransferManagerBuilder;
-
-import java.io.File;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.concurrent.CompletableFuture;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.Executors;
-import java.util.function.Consumer;
-import java.util.function.Predicate;
-import java.util.stream.Collectors;
-
-/**
- * S3 utilities.
- *
- * <p>Usage: java -jar S3UtilProgram.jar args.
- *
- * <p>Note: {@code S3UtilProgram.Action.lineNumber*} actions are applicable only
- * to valid non-compressed CSV comma separated files.
- *
- * <p>Program parameters:
- * <ul>
- *     <li>action (string, required): Action to perform, see {@link S3UtilProgram.Action}.</li>
- *     <li>bucket (string, required): Bucket where s3 objects reside.</li>
- *     <li>s3file (string, required for single object actions): s3 object key.</li>
- *     <li>s3prefix (string, required for actions over objects grouped by key prefix): s3 key prefix.</li>
- *     <li>s3filePrefix (string, optional for downloadByFullPathAndFileNamePrefix or numberOfLinesInFilesWithFullAndNamePrefix):
- *     s3 file name prefix w/o directory to filter files by name.</li>
- *     <li>localFile (string, required for single file actions): local file path.</li>
- *     <li>localFolder (string, required for actions over folders): local folder path.</li>
- *     <li>parallelism (int, default 10): parallelism for parallelizable actions
- *     (e.g. {@code numberOfLinesInFilesWithFullAndNamePrefix}).</li>
- * </ul>
- */
-class S3UtilProgram {
-	enum Action {
-		listByFullPathPrefix,
-		downloadFile,
-		downloadByFullPathAndFileNamePrefix,
-		deleteFile,
-		deleteByFullPathPrefix,
-		numberOfLinesInFile,
-		numberOfLinesInFilesWithFullAndNamePrefix
-	}
-
-	private static final Map<Action, Consumer<ParameterTool>> handlers;
-	static {
-		Map<Action, Consumer<ParameterTool>> handlersMutable = new HashMap<>();
-		handlersMutable.put(Action.listByFullPathPrefix, S3UtilProgram::listByFullPathPrefix);
-		handlersMutable.put(Action.downloadFile, S3UtilProgram::downloadFile);
-		handlersMutable.put(Action.downloadByFullPathAndFileNamePrefix,
-			S3UtilProgram::downloadByFullPathAndFileNamePrefix);
-		handlersMutable.put(Action.deleteFile, S3UtilProgram::deleteFile);
-		handlersMutable.put(Action.deleteByFullPathPrefix, S3UtilProgram::deleteByFullPathPrefix);
-		handlersMutable.put(Action.numberOfLinesInFile, S3UtilProgram::numberOfLinesInFile);
-		handlersMutable.put(Action.numberOfLinesInFilesWithFullAndNamePrefix,
-			S3UtilProgram::numberOfLinesInFilesWithFullAndNamePrefix);
-		handlers = Collections.unmodifiableMap(handlersMutable);
-	}
-
-	private static final String countQuery = "select count(*) from s3object";
-
-	public static void main(String[] args) {
-		final ParameterTool params = ParameterTool.fromArgs(args);
-		final Action action = Action.valueOf(params.getRequired("action"));
-		handlers.get(action).accept(params);
-	}
-
-	private static void listByFullPathPrefix(ParameterTool params) {
-		final String bucket = params.getRequired("bucket");
-		final String s3prefix = params.getRequired("s3prefix");
-		listByFullPathPrefix(bucket, s3prefix).forEach(System.out::println);
-	}
-
-	private static List<String> listByFullPathPrefix(final String bucket, final String s3prefix) {
-		return AmazonS3ClientBuilder.defaultClient().listObjects(bucket, s3prefix).getObjectSummaries()
-			.stream().map(S3ObjectSummary::getKey).collect(Collectors.toList());
-	}
-
-	private static void downloadFile(ParameterTool params) {
-		final String bucket = params.getRequired("bucket");
-		final String s3file = params.getRequired("s3file");
-		final String localFile = params.getRequired("localFile");
-		TransferManager tx = TransferManagerBuilder.defaultTransferManager();
-		try {
-			tx.download(bucket, s3file, new File(localFile)).waitForCompletion();
-		} catch (InterruptedException e) {
-			System.out.println("Transfer interrupted");
-		} finally {
-			tx.shutdownNow();
-		}
-	}
-
-	private static void downloadByFullPathAndFileNamePrefix(ParameterTool params) {
-		final String bucket = params.getRequired("bucket");
-		final String s3prefix = params.getRequired("s3prefix");
-		final String localFolder = params.getRequired("localFolder");
-		final String s3filePrefix = params.get("s3filePrefix", "");
-		TransferManager tx = TransferManagerBuilder.defaultTransferManager();
-		Predicate<String> keyPredicate = getKeyFilterByFileNamePrefix(s3filePrefix);
-		KeyFilter keyFilter = s3filePrefix.isEmpty() ? KeyFilter.INCLUDE_ALL :
-			objectSummary -> keyPredicate.test(objectSummary.getKey());
-		try {
-			tx.downloadDirectory(bucket, s3prefix, new File(localFolder), keyFilter).waitForCompletion();
-		} catch (InterruptedException e) {
-			System.out.println("Transfer interrupted");
-		} finally {
-			tx.shutdownNow();
-		}
-	}
-
-	private static Predicate<String> getKeyFilterByFileNamePrefix(String s3filePrefix) {
-		if (s3filePrefix.isEmpty()) {
-			return key -> true;
-		} else {
-			return key -> {
-				String[] parts = key.split("/");
-				String fileName = parts[parts.length - 1];
-				return fileName.startsWith(s3filePrefix);
-			};
-		}
-	}
-
-	private static void deleteFile(ParameterTool params) {
-		final String bucket = params.getRequired("bucket");
-		final String s3file = params.getRequired("s3file");
-		AmazonS3ClientBuilder.defaultClient().deleteObject(bucket, s3file);
-	}
-
-	private static void deleteByFullPathPrefix(ParameterTool params) {
-		final String bucket = params.getRequired("bucket");
-		final String s3prefix = params.getRequired("s3prefix");
-		String[] keys = listByFullPathPrefix(bucket, s3prefix).toArray(new String[] {});
-		if (keys.length > 0) {
-			DeleteObjectsRequest request = new DeleteObjectsRequest(bucket).withKeys(keys);
-			AmazonS3ClientBuilder.defaultClient().deleteObjects(request);
-		}
-	}
-
-	private static void numberOfLinesInFile(ParameterTool params) {
-		final String bucket = params.getRequired("bucket");
-		final String s3file = params.getRequired("s3file");
-		AmazonS3 s3client = AmazonS3ClientBuilder.defaultClient();
-		System.out.print(S3QueryUtil.queryFile(s3client, bucket, s3file, countQuery));
-		s3client.shutdown();
-	}
-
-	private static void numberOfLinesInFilesWithFullAndNamePrefix(ParameterTool params) {
-		final String bucket = params.getRequired("bucket");
-		final String s3prefix = params.getRequired("s3prefix");
-		final String s3filePrefix = params.get("s3filePrefix", "");
-		int parallelism = params.getInt("parallelism", 10);
-
-		List<String> files = listByFullPathPrefix(bucket, s3prefix);
-
-		ExecutorService executor = Executors.newFixedThreadPool(parallelism);
-		AmazonS3 s3client = AmazonS3ClientBuilder.defaultClient();
-		List<CompletableFuture<Integer>> requests =
-			submitLineCountingRequestsForFilesAsync(executor, s3client, bucket, files, s3filePrefix);
-		int count = waitAndComputeTotalLineCountResult(requests);
-
-		executor.shutdownNow();
-		s3client.shutdown();
-		System.out.print(count);
-	}
-
-	private static List<CompletableFuture<Integer>> submitLineCountingRequestsForFilesAsync(
-			ExecutorService executor, AmazonS3 s3client, String bucket, List<String> files, String s3filePrefix) {
-		List<CompletableFuture<Integer>> requests = new ArrayList<>();
-		Predicate<String> keyPredicate = getKeyFilterByFileNamePrefix(s3filePrefix);
-		files.forEach(file -> {
-			if (keyPredicate.test(file)) {
-				CompletableFuture<Integer> result = new CompletableFuture<>();
-				executor.execute(() ->
-					result.complete(Integer.parseInt(S3QueryUtil.queryFile(s3client, bucket, file, countQuery))));
-				requests.add(result);
-			}
-		});
-		return requests;
-	}
-
-	private static int waitAndComputeTotalLineCountResult(List<CompletableFuture<Integer>> requests) {
-		int count = 0;
-		for (CompletableFuture<Integer> result : requests) {
-			try {
-				count += result.get();
-			} catch (Throwable e) {
-				System.out.println("Failed count lines");
-				e.printStackTrace();
-			}
-		}
-		return count;
-	}
-}
diff --git a/flink-end-to-end-tests/pom.xml b/flink-end-to-end-tests/pom.xml
index c2c1bfc2aac..0c87bea9d8d 100644
--- a/flink-end-to-end-tests/pom.xml
+++ b/flink-end-to-end-tests/pom.xml
@@ -63,7 +63,6 @@ under the License.
 		<module>flink-sql-client-test</module>
 		<module>flink-streaming-file-sink-test</module>
 		<module>flink-state-evolution-test</module>
-		<module>flink-e2e-test-utils</module>
 		<module>flink-end-to-end-tests-common</module>
 		<module>flink-metrics-availability-test</module>
 		<module>flink-metrics-reporter-prometheus-test</module>
diff --git a/flink-end-to-end-tests/test-scripts/common_s3.sh b/flink-end-to-end-tests/test-scripts/common_s3.sh
index 90454ed7f10..0fce47a8328 100644
--- a/flink-end-to-end-tests/test-scripts/common_s3.sh
+++ b/flink-end-to-end-tests/test-scripts/common_s3.sh
@@ -48,8 +48,6 @@ AWS_SECRET_KEY=$IT_CASE_S3_SECRET_KEY
 
 S3_TEST_DATA_WORDS_URI="s3://$IT_CASE_S3_BUCKET/static/words"
 
-s3util="java -jar ${END_TO_END_DIR}/flink-e2e-test-utils/target/S3UtilProgram.jar"
-
 ###################################
 # Setup Flink s3 access.
 #
@@ -74,57 +72,4 @@ function s3_setup_with_provider {
   set_config_key "$2" "com.amazonaws.auth.EnvironmentVariableCredentialsProvider"
 }
 
-###################################
-# Download s3 objects to folder by full path prefix.
-#
-# Globals:
-#   IT_CASE_S3_BUCKET
-# Arguments:
-#   $1 - local path to save folder with files
-#   $2 - s3 key full path prefix
-#   $3 - s3 file name prefix w/o directory to filter files by name (optional)
-# Returns:
-#   None
-###################################
-function s3_get_by_full_path_and_filename_prefix {
-  local file_prefix="${3-}"
-  AWS_REGION=$AWS_REGION \
-  ${s3util} --action downloadByFullPathAndFileNamePrefix \
-    --localFolder "$1" --s3prefix "$2" --s3filePrefix "${file_prefix}" --bucket $IT_CASE_S3_BUCKET
-}
-
-###################################
-# Delete s3 objects by full path prefix.
-#
-# Globals:
-#   IT_CASE_S3_BUCKET
-# Arguments:
-#   $1 - s3 key full path prefix
-# Returns:
-#   None
-###################################
-function s3_delete_by_full_path_prefix {
-  AWS_REGION=$AWS_REGION \
-  ${s3util} --action deleteByFullPathPrefix --s3prefix "$1" --bucket $IT_CASE_S3_BUCKET
-}
-
-###################################
-# Count number of lines in files of s3 objects filtered by prefix.
-# The lines has to be simple to comply with CSV format
-# because SQL is used to query the s3 objects.
-#
-# Globals:
-#   IT_CASE_S3_BUCKET
-# Arguments:
-#   $1 - s3 key prefix
-#   $2 - s3 bucket
-#   $3 - s3 file name prefix w/o directory to filter files by name (optional)
-# Returns:
-#   None
-###################################
-function s3_get_number_of_lines_by_prefix {
-  local file_prefix="${3-}"
-  AWS_REGION=$AWS_REGION \
-  ${s3util} --action numberOfLinesInFilesWithFullAndNamePrefix \
-    --s3prefix "$1" --s3filePrefix "${file_prefix}" --bucket $IT_CASE_S3_BUCKET
-}
+source "$(dirname "$0")"/common_s3_operations.sh
\ No newline at end of file
diff --git a/flink-end-to-end-tests/test-scripts/common_s3_operations.sh b/flink-end-to-end-tests/test-scripts/common_s3_operations.sh
new file mode 100644
index 00000000000..ad4f9964378
--- /dev/null
+++ b/flink-end-to-end-tests/test-scripts/common_s3_operations.sh
@@ -0,0 +1,157 @@
+#!/usr/bin/env bash
+################################################################################
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+################################################################################
+
+###################################
+# Starts a docker container of the aws.
+#
+# To improve performance of s3_get_number_of_lines_by_prefix, one docker container will be reused for several aws
+# commands. An interactive python shell keeps the container busy such that it can be reused to issue several commands.
+#
+# Globals:
+#   TEST_INFRA_DIR
+# Exports:
+#   AWSCLI_CONTAINER_ID
+###################################
+function aws_cli_start() {
+  export AWSCLI_CONTAINER_ID=$(docker run -d \
+    --network host \
+    --mount type=bind,source="$TEST_INFRA_DIR",target=/hostdir \
+    -e AWS_REGION -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY \
+    --entrypoint python \
+    -it banst/awscli)
+
+  while [[ "$(docker inspect -f {{.State.Running}} "$AWSCLI_CONTAINER_ID")" -ne "true" ]]; do
+    sleep 0.1
+  done
+  on_exit aws_cli_stop
+}
+
+###################################
+# Stops the docker container of the aws cli.
+#
+# Globals:
+#   AWSCLI_CONTAINER_ID
+###################################
+function aws_cli_stop() {
+  docker kill "$AWSCLI_CONTAINER_ID"
+  docker rm "$AWSCLI_CONTAINER_ID"
+  export AWSCLI_CONTAINER_ID=
+}
+
+# always start it while sourcing, so that AWSCLI_CONTAINER_ID is available from parent script
+if [[ $AWSCLI_CONTAINER_ID ]]; then
+  aws_cli_stop
+fi
+aws_cli_start
+
+###################################
+# Runs an aws command on the previously started container.
+#
+# Globals:
+#   AWSCLI_CONTAINER_ID
+###################################
+function aws_cli() {
+  local endpoint=""
+  if [[ $S3_ENDPOINT ]]; then
+    endpoint="--endpoint-url $S3_ENDPOINT"
+  fi
+  docker exec "$AWSCLI_CONTAINER_ID" aws $endpoint "$@"
+}
+
+###################################
+# Download s3 objects to folder by full path prefix.
+#
+# Globals:
+#   IT_CASE_S3_BUCKET
+#   TEST_INFRA_DIR
+# Arguments:
+#   $1 - local path to save folder with files
+#   $2 - s3 key full path prefix
+#   $3 - s3 file name prefix w/o directory to filter files by name (optional)
+#   $4 - recursive?
+# Returns:
+#   None
+###################################
+function s3_get_by_full_path_and_filename_prefix() {
+  local args=
+  if [[ $3 ]]; then
+    args=" --exclude '*' --include '*/${3}[!/]*'"
+  fi
+  if [[ "$4" == true ]]; then
+    args="$args --recursive"
+  fi
+  local relative_dir=${1#$TEST_INFRA_DIR}
+  aws_cli s3 cp --quiet "s3://$IT_CASE_S3_BUCKET/$2" "/hostdir/${relative_dir}" $args
+}
+
+###################################
+# Delete s3 objects by full path prefix.
+#
+# Globals:
+#   IT_CASE_S3_BUCKET
+# Arguments:
+#   $1 - s3 key full path prefix
+# Returns:
+#   None
+###################################
+function s3_delete_by_full_path_prefix() {
+  aws_cli s3 rm --quiet "s3://$IT_CASE_S3_BUCKET/$1" --recursive
+}
+
+###################################
+# Count number of lines in files of s3 objects filtered by prefix.
+# The lines has to be simple to comply with CSV format
+# because SQL is used to query the s3 objects.
+#
+# Globals:
+#   IT_CASE_S3_BUCKET
+# Arguments:
+#   $1 - s3 key prefix
+#   $2 - s3 file name prefix w/o directory to filter files by name (optional)
+# Returns:
+#   line number in part files
+###################################
+function s3_get_number_of_lines_by_prefix() {
+  local file_prefix="${2-}"
+
+  # find all files that have the given prefix
+  parts=$(aws_cli s3api list-objects --bucket "$IT_CASE_S3_BUCKET" --prefix "$1" |
+    docker run -i stedolan/jq -r '[.Contents[].Key] | join(" ")')
+
+  # in parallel (N tasks), query the number of lines, store result in a file named lines
+  N=10
+  echo "0" >lines
+  # turn off job control, so that there is noise when starting/finishing bg tasks
+  old_state=$(set +o)
+  set +m
+  for part in $parts; do
+    if [[ $(basename "${part}") == $file_prefix* ]]; then
+      ((i = i % N))
+      ((i++ == 0)) && wait
+      aws_cli s3api select-object-content --bucket "$IT_CASE_S3_BUCKET" --key "$part" \
+        --expression "select count(*) from s3object" --expression-type "SQL" \
+        --input-serialization='{"CSV": {}}' --output-serialization='{"CSV": {}}' /dev/stdout >>lines &
+    fi
+  done
+  wait
+  # restore old settings
+  eval "$old_state"
+  # add number of lines of each part
+  paste -sd+ lines | bc
+}
