diff --git a/docs/content.zh/docs/dev/datastream/operators/overview.md b/docs/content.zh/docs/dev/datastream/operators/overview.md
index d16f89e2923..8032e9c59ec 100644
--- a/docs/content.zh/docs/dev/datastream/operators/overview.md
+++ b/docs/content.zh/docs/dev/datastream/operators/overview.md
@@ -604,7 +604,16 @@ env.execute()
 ```
 {{< /tab >}}
 {{< tab "Python" >}}
-Python 中尚不支持此特性。
+```python
+data_stream = ... # DataStream
+cached_data_stream = data_stream.cache()
+cached_data_stream.print()
+# ...
+env.execute() # Execute and create cache.
+
+cached_data_stream.print() # Consume cached result.
+env.execute()
+```
 {{< /tab >}}
 {{< /tabs>}}
 
diff --git a/docs/content/docs/dev/datastream/operators/overview.md b/docs/content/docs/dev/datastream/operators/overview.md
index a396a5af923..0fed52c1625 100644
--- a/docs/content/docs/dev/datastream/operators/overview.md
+++ b/docs/content/docs/dev/datastream/operators/overview.md
@@ -609,7 +609,16 @@ env.execute()
 ```
 {{< /tab >}}
 {{< tab "Python" >}}
-This feature is not yet supported in Python
+```python
+data_stream = ... # DataStream
+cached_data_stream = data_stream.cache()
+cached_data_stream.print()
+# ...
+env.execute() # Execute and create cache.
+
+cached_data_stream.print() # Consume cached result.
+env.execute()
+```
 {{< /tab >}}
 {{< /tabs>}}
 
diff --git a/flink-python/pyflink/datastream/data_stream.py b/flink-python/pyflink/datastream/data_stream.py
index 6938aa0d4da..3089bd691a2 100644
--- a/flink-python/pyflink/datastream/data_stream.py
+++ b/flink-python/pyflink/datastream/data_stream.py
@@ -895,6 +895,20 @@ class DataStream(object):
         """
         return DataStream(self._j_data_stream.getSideOutput(output_tag.get_java_output_tag()))
 
+    def cache(self) -> 'CachedDataStream':
+        """
+        Cache the intermediate result of the transformation. Only support bounded streams and
+        currently only block mode is supported. The cache is generated lazily at the first time the
+        intermediate result is computed. The cache will be clear when the StreamExecutionEnvironment
+        close.
+
+        :return: The cached DataStream that can use in later job to reuse the cached intermediate
+                 result.
+
+        .. versionadded:: 1.16.0
+        """
+        return CachedDataStream(self._j_data_stream.cache())
+
     def _apply_chaining_optimization(self):
         """
         Chain the Python operators if possible.
@@ -1756,6 +1770,63 @@ class KeyedStream(DataStream):
     def slot_sharing_group(self, slot_sharing_group: Union[str, SlotSharingGroup]) -> 'DataStream':
         raise Exception("Setting slot sharing group for KeyedStream is not supported.")
 
+    def cache(self) -> 'CachedDataStream':
+        raise Exception("Cache for KeyedStream is not supported.")
+
+
+class CachedDataStream(DataStream):
+    """
+    CachedDataStream represents a DataStream whose intermediate result will be cached at the first
+    time when it is computed. And the cached intermediate result can be used in later job that using
+    the same CachedDataStream to avoid re-computing the intermediate result.
+    """
+
+    def __init__(self, j_data_stream):
+        super(CachedDataStream, self).__init__(j_data_stream)
+
+    def invalidate(self):
+        """
+        Invalidate the cache intermediate result of this DataStream to release the physical
+        resources. Users are not required to invoke this method to release physical resources unless
+        they want to. Cache will be recreated if it is used after invalidated.
+
+        .. versionadded:: 1.16.0
+        """
+        self._j_data_stream.invalidate()
+
+    def set_parallelism(self, parallelism: int):
+        raise Exception("Set parallelism for CachedDataStream is not supported.")
+
+    def name(self, name: str):
+        raise Exception("Set name for CachedDataStream is not supported.")
+
+    def get_name(self) -> str:
+        raise Exception("Get name of CachedDataStream is not supported.")
+
+    def uid(self, uid: str):
+        raise Exception("Set uid for CachedDataStream is not supported.")
+
+    def set_uid_hash(self, uid_hash: str):
+        raise Exception("Set uid hash for CachedDataStream is not supported.")
+
+    def set_max_parallelism(self, max_parallelism: int):
+        raise Exception("Set max parallelism for CachedDataStream is not supported.")
+
+    def force_non_parallel(self):
+        raise Exception("Set force non-parallel for CachedDataStream is not supported.")
+
+    def set_buffer_timeout(self, timeout_millis: int):
+        raise Exception("Set buffer timeout for CachedDataStream is not supported.")
+
+    def start_new_chain(self) -> 'DataStream':
+        raise Exception("Start new chain for CachedDataStream is not supported.")
+
+    def disable_chaining(self) -> 'DataStream':
+        raise Exception("Disable chaining for CachedDataStream is not supported.")
+
+    def slot_sharing_group(self, slot_sharing_group: Union[str, SlotSharingGroup]) -> 'DataStream':
+        raise Exception("Setting slot sharing group for CachedDataStream is not supported.")
+
 
 class WindowedStream(object):
     """
diff --git a/flink-python/pyflink/datastream/stream_execution_environment.py b/flink-python/pyflink/datastream/stream_execution_environment.py
index 31cfaf38bd1..2a2f09ddf3d 100644
--- a/flink-python/pyflink/datastream/stream_execution_environment.py
+++ b/flink-python/pyflink/datastream/stream_execution_environment.py
@@ -1053,3 +1053,12 @@ class StreamExecutionEnvironment(object):
         Returns whether Unaligned Checkpoints are force-enabled.
         """
         return self._j_stream_execution_environment.isForceUnalignedCheckpoints()
+
+    def close(self):
+        """
+        Close and clean up the execution environment. All the cached intermediate results will be
+        released physically.
+
+        .. versionadded:: 1.16.0
+        """
+        self._j_stream_execution_environment.close()
