diff --git a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/KafkaSource.java b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/KafkaSource.java
index c24494cc117..b7dd7185f89 100644
--- a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/KafkaSource.java
+++ b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/KafkaSource.java
@@ -18,6 +18,7 @@
 
 package org.apache.flink.connector.kafka.source;
 
+import org.apache.flink.api.common.serialization.DeserializationSchema;
 import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.api.connector.source.Boundedness;
 import org.apache.flink.api.connector.source.Source;
@@ -42,6 +43,8 @@ import org.apache.flink.connector.kafka.source.reader.deserializer.KafkaRecordDe
 import org.apache.flink.connector.kafka.source.split.KafkaPartitionSplit;
 import org.apache.flink.connector.kafka.source.split.KafkaPartitionSplitSerializer;
 import org.apache.flink.core.io.SimpleVersionedSerializer;
+import org.apache.flink.metrics.MetricGroup;
+import org.apache.flink.util.UserCodeClassLoader;
 
 import javax.annotation.Nullable;
 
@@ -114,9 +117,23 @@ public class KafkaSource<OUT>
     }
 
     @Override
-    public SourceReader<OUT, KafkaPartitionSplit> createReader(SourceReaderContext readerContext) {
+    public SourceReader<OUT, KafkaPartitionSplit> createReader(SourceReaderContext readerContext)
+            throws Exception {
         FutureCompletingBlockingQueue<RecordsWithSplitIds<Tuple3<OUT, Long, Long>>> elementsQueue =
                 new FutureCompletingBlockingQueue<>();
+        deserializationSchema.open(
+                new DeserializationSchema.InitializationContext() {
+                    @Override
+                    public MetricGroup getMetricGroup() {
+                        return readerContext.metricGroup().addGroup("deserializer");
+                    }
+
+                    @Override
+                    public UserCodeClassLoader getUserCodeClassLoader() {
+                        return readerContext.getUserCodeClassLoader();
+                    }
+                });
+
         Supplier<KafkaPartitionSplitReader<OUT>> splitReaderSupplier =
                 () ->
                         new KafkaPartitionSplitReader<>(
diff --git a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/deserializer/KafkaDeserializationSchemaWrapper.java b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/deserializer/KafkaDeserializationSchemaWrapper.java
new file mode 100644
index 00000000000..94197e34714
--- /dev/null
+++ b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/deserializer/KafkaDeserializationSchemaWrapper.java
@@ -0,0 +1,65 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connector.kafka.source.reader.deserializer;
+
+import org.apache.flink.api.common.serialization.DeserializationSchema;
+import org.apache.flink.api.common.typeinfo.TypeInformation;
+import org.apache.flink.streaming.connectors.kafka.KafkaDeserializationSchema;
+import org.apache.flink.util.Collector;
+
+import org.apache.kafka.clients.consumer.ConsumerRecord;
+
+import java.io.IOException;
+
+/**
+ * A wrapper class that wraps a {@link
+ * org.apache.flink.streaming.connectors.kafka.KafkaDeserializationSchema} to deserialize {@link
+ * ConsumerRecord ConsumerRecords}.
+ *
+ * @param <T> the type of the deserialized records.
+ */
+class KafkaDeserializationSchemaWrapper<T> implements KafkaRecordDeserializationSchema<T> {
+    private static final long serialVersionUID = 1L;
+    private final KafkaDeserializationSchema<T> kafkaDeserializationSchema;
+
+    KafkaDeserializationSchemaWrapper(KafkaDeserializationSchema<T> kafkaDeserializationSchema) {
+        this.kafkaDeserializationSchema = kafkaDeserializationSchema;
+    }
+
+    @Override
+    public void open(DeserializationSchema.InitializationContext context) throws Exception {
+        kafkaDeserializationSchema.open(context);
+    }
+
+    @Override
+    public void deserialize(ConsumerRecord<byte[], byte[]> message, Collector<T> out)
+            throws IOException {
+        try {
+            kafkaDeserializationSchema.deserialize(message, out);
+        } catch (Exception exception) {
+            throw new IOException(
+                    String.format("Failed to deserialize consumer record %s.", message), exception);
+        }
+    }
+
+    @Override
+    public TypeInformation<T> getProducedType() {
+        return kafkaDeserializationSchema.getProducedType();
+    }
+}
diff --git a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/deserializer/KafkaRecordDeserializationSchema.java b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/deserializer/KafkaRecordDeserializationSchema.java
index 86eaa46b867..24410797044 100644
--- a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/deserializer/KafkaRecordDeserializationSchema.java
+++ b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/deserializer/KafkaRecordDeserializationSchema.java
@@ -18,13 +18,17 @@
 
 package org.apache.flink.connector.kafka.source.reader.deserializer;
 
+import org.apache.flink.annotation.PublicEvolving;
+import org.apache.flink.api.common.serialization.DeserializationSchema;
 import org.apache.flink.api.java.typeutils.ResultTypeQueryable;
+import org.apache.flink.streaming.connectors.kafka.KafkaDeserializationSchema;
 import org.apache.flink.util.Collector;
 
 import org.apache.kafka.clients.consumer.ConsumerRecord;
 import org.apache.kafka.common.Configurable;
 import org.apache.kafka.common.serialization.Deserializer;
 
+import java.io.IOException;
 import java.io.Serializable;
 import java.util.Collections;
 import java.util.Map;
@@ -33,13 +37,66 @@ import java.util.Map;
 public interface KafkaRecordDeserializationSchema<T> extends Serializable, ResultTypeQueryable<T> {
 
     /**
-     * Deserialize a consumer record into the given collector.
+     * Initialization method for the schema. It is called before the actual working methods {@link
+     * #deserialize} and thus suitable for one time setup work.
      *
-     * @param record the {@code ConsumerRecord} to deserialize.
-     * @throws Exception if the deserialization failed.
+     * <p>The provided {@link DeserializationSchema.InitializationContext} can be used to access
+     * additional features such as e.g. registering user metrics.
+     *
+     * @param context Contextual information that can be used during initialization.
+     */
+    @PublicEvolving
+    default void open(DeserializationSchema.InitializationContext context) throws Exception {}
+
+    /**
+     * Deserializes the byte message.
+     *
+     * <p>Can output multiple records through the {@link Collector}. Note that number and size of
+     * the produced records should be relatively small. Depending on the source implementation
+     * records can be buffered in memory or collecting records might delay emitting checkpoint
+     * barrier.
+     *
+     * @param record The ConsumerRecord to deserialize.
+     * @param out The collector to put the resulting messages.
      */
-    void deserialize(ConsumerRecord<byte[], byte[]> record, Collector<T> collector)
-            throws Exception;
+    @PublicEvolving
+    void deserialize(ConsumerRecord<byte[], byte[]> record, Collector<T> out) throws IOException;
+
+    /**
+     * Wraps a legacy {@link KafkaDeserializationSchema} as the deserializer of the {@link
+     * ConsumerRecord ConsumerRecords}.
+     *
+     * <p>Note that the {@link KafkaDeserializationSchema#isEndOfStream(Object)} method will no
+     * longer be used to determin the end of the stream.
+     *
+     * @param kafkaDeserializationSchema the legacy {@link KafkaDeserializationSchema} to use.
+     * @param <V> the return type of the deserialized record.
+     * @return A {@link KafkaRecordDeserializationSchema} that uses the given {@link
+     *     KafkaDeserializationSchema} to deserialize the {@link ConsumerRecord ConsumerRecords}.
+     */
+    static <V> KafkaRecordDeserializationSchema<V> of(
+            KafkaDeserializationSchema<V> kafkaDeserializationSchema) {
+        return new KafkaDeserializationSchemaWrapper<>(kafkaDeserializationSchema);
+    }
+
+    /**
+     * Wraps a {@link DeserializationSchema} as the value deserialization schema of the {@link
+     * ConsumerRecord ConsumerRecords}. The other fields such as key, headers, timestamp are
+     * ignored.
+     *
+     * <p>Note that the {@link DeserializationSchema#isEndOfStream(Object)} method will no longer be
+     * used to determine the end of the stream.
+     *
+     * @param valueDeserializationSchema the {@link DeserializationSchema} used to deserialized the
+     *     value of a {@link ConsumerRecord}.
+     * @param <V> the type of the deserialized record.
+     * @return A {@link KafkaRecordDeserializationSchema} that uses the given {@link
+     *     DeserializationSchema} to deserialize a {@link ConsumerRecord} from its value.
+     */
+    static <V> KafkaRecordDeserializationSchema<V> valueOnly(
+            DeserializationSchema<V> valueDeserializationSchema) {
+        return new KafkaValueOnlyDeserializationSchemaWrapper<>(valueDeserializationSchema);
+    }
 
     /**
      * Wraps a Kafka {@link Deserializer} to a {@link KafkaRecordDeserializationSchema}.
@@ -51,7 +108,8 @@ public interface KafkaRecordDeserializationSchema<T> extends Serializable, Resul
      */
     static <V> KafkaRecordDeserializationSchema<V> valueOnly(
             Class<? extends Deserializer<V>> valueDeserializerClass) {
-        return new ValueDeserializerWrapper<>(valueDeserializerClass, Collections.emptyMap());
+        return new KafkaValueOnlyDeserializerWrapper<>(
+                valueDeserializerClass, Collections.emptyMap());
     }
 
     /**
@@ -68,6 +126,6 @@ public interface KafkaRecordDeserializationSchema<T> extends Serializable, Resul
     static <V, D extends Configurable & Deserializer<V>>
             KafkaRecordDeserializationSchema<V> valueOnly(
                     Class<D> valueDeserializerClass, Map<String, String> config) {
-        return new ValueDeserializerWrapper<>(valueDeserializerClass, config);
+        return new KafkaValueOnlyDeserializerWrapper<>(valueDeserializerClass, config);
     }
 }
diff --git a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/deserializer/KafkaValueOnlyDeserializationSchemaWrapper.java b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/deserializer/KafkaValueOnlyDeserializationSchemaWrapper.java
new file mode 100644
index 00000000000..209f5e15c2c
--- /dev/null
+++ b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/deserializer/KafkaValueOnlyDeserializationSchemaWrapper.java
@@ -0,0 +1,58 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connector.kafka.source.reader.deserializer;
+
+import org.apache.flink.api.common.serialization.DeserializationSchema;
+import org.apache.flink.api.common.typeinfo.TypeInformation;
+import org.apache.flink.util.Collector;
+
+import org.apache.kafka.clients.consumer.ConsumerRecord;
+
+import java.io.IOException;
+
+/**
+ * A class that wraps a {@link DeserializationSchema} as the value deserializer for a {@link
+ * ConsumerRecord}.
+ *
+ * @param <T> the return type of the deserialization.
+ */
+class KafkaValueOnlyDeserializationSchemaWrapper<T> implements KafkaRecordDeserializationSchema<T> {
+    private static final long serialVersionUID = 1L;
+    private final DeserializationSchema<T> deserializationSchema;
+
+    KafkaValueOnlyDeserializationSchemaWrapper(DeserializationSchema<T> deserializationSchema) {
+        this.deserializationSchema = deserializationSchema;
+    }
+
+    @Override
+    public void open(DeserializationSchema.InitializationContext context) throws Exception {
+        deserializationSchema.open(context);
+    }
+
+    @Override
+    public void deserialize(ConsumerRecord<byte[], byte[]> message, Collector<T> out)
+            throws IOException {
+        deserializationSchema.deserialize(message.value(), out);
+    }
+
+    @Override
+    public TypeInformation<T> getProducedType() {
+        return deserializationSchema.getProducedType();
+    }
+}
diff --git a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/deserializer/ValueDeserializerWrapper.java b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/deserializer/KafkaValueOnlyDeserializerWrapper.java
similarity index 72%
rename from flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/deserializer/ValueDeserializerWrapper.java
rename to flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/deserializer/KafkaValueOnlyDeserializerWrapper.java
index 226ce7f47d9..0321c4bd68a 100644
--- a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/deserializer/ValueDeserializerWrapper.java
+++ b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/deserializer/KafkaValueOnlyDeserializerWrapper.java
@@ -18,10 +18,12 @@
 
 package org.apache.flink.connector.kafka.source.reader.deserializer;
 
+import org.apache.flink.api.common.serialization.DeserializationSchema;
 import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.api.java.typeutils.TypeExtractor;
 import org.apache.flink.util.Collector;
 import org.apache.flink.util.InstantiationUtil;
+import org.apache.flink.util.TemporaryClassLoaderContext;
 
 import org.apache.kafka.clients.consumer.ConsumerRecord;
 import org.apache.kafka.common.Configurable;
@@ -29,18 +31,20 @@ import org.apache.kafka.common.serialization.Deserializer;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import java.io.IOException;
 import java.util.Map;
 
 /** A package private class to wrap {@link Deserializer}. */
-class ValueDeserializerWrapper<T> implements KafkaRecordDeserializationSchema<T> {
+class KafkaValueOnlyDeserializerWrapper<T> implements KafkaRecordDeserializationSchema<T> {
     private static final long serialVersionUID = 5409547407386004054L;
-    private static final Logger LOG = LoggerFactory.getLogger(ValueDeserializerWrapper.class);
+    private static final Logger LOG =
+            LoggerFactory.getLogger(KafkaValueOnlyDeserializerWrapper.class);
     private final String deserializerClass;
     private final Map<String, String> config;
 
     private transient Deserializer<T> deserializer;
 
-    ValueDeserializerWrapper(
+    KafkaValueOnlyDeserializerWrapper(
             Class<? extends Deserializer<T>> deserializerClass, Map<String, String> config) {
         this.deserializerClass = deserializerClass.getName();
         this.config = config;
@@ -48,9 +52,10 @@ class ValueDeserializerWrapper<T> implements KafkaRecordDeserializationSchema<T>
 
     @Override
     @SuppressWarnings("unchecked")
-    public void deserialize(ConsumerRecord<byte[], byte[]> record, Collector<T> collector)
-            throws Exception {
-        if (deserializer == null) {
+    public void open(DeserializationSchema.InitializationContext context) throws Exception {
+        ClassLoader userCodeClassLoader = context.getUserCodeClassLoader().asClassLoader();
+        try (TemporaryClassLoaderContext ignored =
+                TemporaryClassLoaderContext.of(userCodeClassLoader)) {
             deserializer =
                     (Deserializer<T>)
                             InstantiationUtil.instantiate(
@@ -60,6 +65,19 @@ class ValueDeserializerWrapper<T> implements KafkaRecordDeserializationSchema<T>
             if (deserializer instanceof Configurable) {
                 ((Configurable) deserializer).configure(config);
             }
+        } catch (Exception e) {
+            throw new IOException(
+                    "Failed to instantiate the deserializer of class " + deserializerClass, e);
+        }
+    }
+
+    @Override
+    public void deserialize(ConsumerRecord<byte[], byte[]> record, Collector<T> collector)
+            throws IOException {
+        if (deserializer == null) {
+            throw new IllegalStateException(
+                    "The deserializer has not been created. Make sure the open() method has been "
+                            + "invoked.");
         }
 
         T value = deserializer.deserialize(record.topic(), record.value());
diff --git a/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/source/KafkaSourceITCase.java b/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/source/KafkaSourceITCase.java
index 0137dcb4b06..40fe4a8027d 100644
--- a/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/source/KafkaSourceITCase.java
+++ b/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/source/KafkaSourceITCase.java
@@ -37,6 +37,7 @@ import org.junit.AfterClass;
 import org.junit.BeforeClass;
 import org.junit.Test;
 
+import java.io.IOException;
 import java.io.Serializable;
 import java.util.ArrayList;
 import java.util.Arrays;
@@ -103,7 +104,7 @@ public class KafkaSourceITCase {
         @Override
         public void deserialize(
                 ConsumerRecord<byte[], byte[]> record, Collector<PartitionAndValue> collector)
-                throws Exception {
+                throws IOException {
             if (deserializer == null) {
                 deserializer = new IntegerDeserializer();
             }
diff --git a/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/source/reader/KafkaPartitionSplitReaderTest.java b/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/source/reader/KafkaPartitionSplitReaderTest.java
index 30991cd728d..e0f011d06c0 100644
--- a/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/source/reader/KafkaPartitionSplitReaderTest.java
+++ b/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/source/reader/KafkaPartitionSplitReaderTest.java
@@ -25,6 +25,7 @@ import org.apache.flink.connector.base.source.reader.splitreader.SplitsChange;
 import org.apache.flink.connector.kafka.source.KafkaSourceTestEnv;
 import org.apache.flink.connector.kafka.source.reader.deserializer.KafkaRecordDeserializationSchema;
 import org.apache.flink.connector.kafka.source.split.KafkaPartitionSplit;
+import org.apache.flink.connector.testutils.source.deserialization.TestingDeserializationContext;
 
 import org.apache.kafka.clients.consumer.ConsumerConfig;
 import org.apache.kafka.common.TopicPartition;
@@ -77,14 +78,14 @@ public class KafkaPartitionSplitReaderTest {
     }
 
     @Test
-    public void testHandleSplitChangesAndFetch() throws IOException {
+    public void testHandleSplitChangesAndFetch() throws Exception {
         KafkaPartitionSplitReader<Integer> reader = createReader();
         assignSplitsAndFetchUntilFinish(reader, 0);
         assignSplitsAndFetchUntilFinish(reader, 1);
     }
 
     @Test
-    public void testWakeUp() throws InterruptedException {
+    public void testWakeUp() throws Exception {
         KafkaPartitionSplitReader<Integer> reader = createReader();
         TopicPartition nonExistingTopicPartition = new TopicPartition("NotExist", 0);
         assignSplits(
@@ -170,12 +171,14 @@ public class KafkaPartitionSplitReaderTest {
 
     // ------------------
 
-    private KafkaPartitionSplitReader<Integer> createReader() {
+    private KafkaPartitionSplitReader<Integer> createReader() throws Exception {
         Properties props = new Properties();
         props.putAll(KafkaSourceTestEnv.getConsumerProperties(ByteArrayDeserializer.class));
         props.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "none");
-        return new KafkaPartitionSplitReader<>(
-                props, KafkaRecordDeserializationSchema.valueOnly(IntegerDeserializer.class), 0);
+        KafkaRecordDeserializationSchema<Integer> deserializationSchema =
+                KafkaRecordDeserializationSchema.valueOnly(IntegerDeserializer.class);
+        deserializationSchema.open(new TestingDeserializationContext());
+        return new KafkaPartitionSplitReader<>(props, deserializationSchema, 0);
     }
 
     private Map<String, KafkaPartitionSplit> assignSplits(
