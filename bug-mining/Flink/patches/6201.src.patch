diff --git a/flink-python/pyflink/table/table.py b/flink-python/pyflink/table/table.py
index c320c8201eb..484d7583a72 100644
--- a/flink-python/pyflink/table/table.py
+++ b/flink-python/pyflink/table/table.py
@@ -16,8 +16,6 @@
 # limitations under the License.
 ################################################################################
 
-import warnings
-
 from py4j.java_gateway import get_method
 from typing import Union
 
@@ -952,29 +950,6 @@ class Table(object):
                 func = func(with_columns(col("*")))
             return FlatAggregateTable(self._j_table.flatAggregate(func._j_expr), self._t_env)
 
-    def insert_into(self, table_path: str):
-        """
-        Writes the :class:`~pyflink.table.Table` to a :class:`~pyflink.table.TableSink` that was
-        registered under the specified name. For the path resolution algorithm see
-        :func:`~TableEnvironment.use_database`.
-
-        Example:
-        ::
-
-            >>> tab.insert_into("sink")
-
-        :param table_path: The path of the registered :class:`~pyflink.table.TableSink` to which
-               the :class:`~pyflink.table.Table` is written.
-
-        .. note:: Deprecated in 1.11. Use :func:`execute_insert` for single sink,
-                  use :class:`TableTableEnvironment`#:func:`create_statement_set`
-                  for multiple sinks.
-        """
-        warnings.warn("Deprecated in 1.11. Use execute_insert for single sink, "
-                      "use TableTableEnvironment#create_statement_set for multiple sinks.",
-                      DeprecationWarning)
-        self._j_table.insertInto(table_path)
-
     def to_pandas(self):
         """
         Converts the table to a pandas DataFrame. It will collect the content of the table to
diff --git a/flink-python/pyflink/table/table_environment.py b/flink-python/pyflink/table/table_environment.py
index fc9fcb37559..57e10eadc00 100644
--- a/flink-python/pyflink/table/table_environment.py
+++ b/flink-python/pyflink/table/table_environment.py
@@ -29,7 +29,6 @@ from pyflink.table.sources import TableSource
 from pyflink.common.typeinfo import TypeInformation
 from pyflink.datastream.data_stream import DataStream
 
-from pyflink.common import JobExecutionResult
 from pyflink.java_gateway import get_gateway
 from pyflink.serializers import BatchedSerializer, PickleSerializer
 from pyflink.table import Table, EnvironmentSettings, Expression, ExplainDetail, \
@@ -636,34 +635,6 @@ class TableEnvironment(object):
         """
         return Table(get_method(self._j_tenv, "from")(descriptor._j_table_descriptor), self)
 
-    def insert_into(self, target_path: str, table: Table):
-        """
-        Instructs to write the content of a :class:`~pyflink.table.Table` API object into a table.
-
-        See the documentation of :func:`use_database` or :func:`use_catalog` for the rules on the
-        path resolution.
-
-        Example:
-        ::
-
-            >>> tab = table_env.scan("tableName")
-            >>> table_env.insert_into("sink", tab)
-
-        :param target_path: The path of the registered :class:`~pyflink.table.TableSink` to which
-                            the :class:`~pyflink.table.Table` is written.
-        :param table: The Table to write to the sink.
-
-        .. versionchanged:: 1.10.0
-            The signature is changed, e.g. the parameter *table_path_continued* was removed and
-            the parameter *target_path* is moved before the parameter *table*.
-
-        .. note:: Deprecated in 1.11. Use :func:`execute_insert` for single sink,
-                  use :func:`create_statement_set` for multiple sinks.
-        """
-        warnings.warn("Deprecated in 1.11. Use Table#execute_insert for single sink,"
-                      "use create_statement_set for multiple sinks.", DeprecationWarning)
-        self._j_tenv.insertInto(target_path, table._j_table)
-
     def list_catalogs(self) -> List[str]:
         """
         Gets the names of all catalogs registered in this environment.
@@ -801,25 +772,6 @@ class TableEnvironment(object):
         """
         return self._j_tenv.dropTemporaryView(view_path)
 
-    def explain(self, table: Table = None, extended: bool = False) -> str:
-        """
-        Returns the AST of the specified Table API and SQL queries and the execution plan to compute
-        the result of the given :class:`~pyflink.table.Table` or multi-sinks plan.
-
-        :param table: The table to be explained. If table is None, explain for multi-sinks plan,
-                      else for given table.
-        :param extended: If the plan should contain additional properties.
-                         e.g. estimated cost, traits
-        :return: The table for which the AST and execution plan will be returned.
-
-        .. note:: Deprecated in 1.11. Use :class:`Table`#:func:`explain` instead.
-        """
-        warnings.warn("Deprecated in 1.11. Use Table#explain instead.", DeprecationWarning)
-        if table is None:
-            return self._j_tenv.explain(extended)
-        else:
-            return self._j_tenv.explain(table._j_table, extended)
-
     def explain_sql(self, stmt: str, *extra_details: ExplainDetail) -> str:
         """
         Returns the AST of the specified statement and the execution plan.
@@ -888,83 +840,6 @@ class TableEnvironment(object):
         _j_statement_set = self._j_tenv.createStatementSet()
         return StatementSet(_j_statement_set, self)
 
-    def sql_update(self, stmt: str):
-        """
-        Evaluates a SQL statement such as INSERT, UPDATE or DELETE or a DDL statement
-
-        .. note::
-
-            Currently only SQL INSERT statements and CREATE TABLE statements are supported.
-
-        All tables referenced by the query must be registered in the TableEnvironment.
-        A :class:`~pyflink.table.Table` is automatically registered when its
-        :func:`~Table.__str__` method is called, for example when it is embedded into a String.
-        Hence, SQL queries can directly reference a :class:`~pyflink.table.Table` as follows:
-        ::
-
-            # register the table sink into which the result is inserted.
-            >>> table_env.register_table_sink("sink_table", table_sink)
-            >>> source_table = ...
-            # source_table is not registered to the table environment
-            >>> table_env.sql_update("INSERT INTO sink_table SELECT * FROM %s" % source_table)
-
-        A DDL statement can also be executed to create/drop a table:
-        For example, the below DDL statement would create a CSV table named `tbl1`
-        into the current catalog::
-
-            create table tbl1(
-                a int,
-                b bigint,
-                c varchar
-            ) with (
-                'connector.type' = 'filesystem',
-                'format.type' = 'csv',
-                'connector.path' = 'xxx'
-            )
-
-        SQL queries can directly execute as follows:
-        ::
-
-            >>> source_ddl = \\
-            ... '''
-            ... create table sourceTable(
-            ...     a int,
-            ...     b varchar
-            ... ) with (
-            ...     'connector.type' = 'kafka',
-            ...     'update-mode' = 'append',
-            ...     'connector.topic' = 'xxx',
-            ...     'connector.properties.bootstrap.servers' = 'localhost:9092'
-            ... )
-            ... '''
-
-            >>> sink_ddl = \\
-            ... '''
-            ... create table sinkTable(
-            ...     a int,
-            ...     b varchar
-            ... ) with (
-            ...     'connector.type' = 'filesystem',
-            ...     'format.type' = 'csv',
-            ...     'connector.path' = 'xxx'
-            ... )
-            ... '''
-
-            >>> query = "INSERT INTO sinkTable SELECT FROM sourceTable"
-            >>> table_env.sql(source_ddl)
-            >>> table_env.sql(sink_ddl)
-            >>> table_env.sql(query)
-            >>> table_env.execute("MyJob")
-
-        :param stmt: The SQL statement to evaluate.
-
-        .. note:: Deprecated in 1.11. Use :func:`execute_sql` for single statement,
-                  use :func:`create_statement_set` for multiple DML statements.
-        """
-        warnings.warn("Deprecated in 1.11. Use execute_sql for single statement, "
-                      "use create_statement_set for multiple DML statements.", DeprecationWarning)
-        self._j_tenv.sqlUpdate(stmt)
-
     def get_current_catalog(self) -> str:
         """
         Gets the current default catalog name of the current session.
@@ -1432,34 +1307,6 @@ class TableEnvironment(object):
         self.get_config().get_configuration().set_string(
             jvm.PythonOptions.PYTHON_ARCHIVES.key(), python_files)
 
-    def execute(self, job_name: str) -> JobExecutionResult:
-        """
-        Triggers the program execution. The environment will execute all parts of
-        the program.
-
-        The program execution will be logged and displayed with the provided name.
-
-        .. note::
-
-            It is highly advised to set all parameters in the :class:`~pyflink.table.TableConfig`
-            on the very beginning of the program. It is undefined what configurations values will
-            be used for the execution if queries are mixed with config changes. It depends on
-            the characteristic of the particular parameter. For some of them the value from the
-            point in time of query construction (e.g. the current catalog) will be used. On the
-            other hand some values might be evaluated according to the state from the time when
-            this method is called (e.g. timezone).
-
-        :param job_name: Desired name of the job.
-        :return: The result of the job execution, containing elapsed time and accumulators.
-
-        .. note:: Deprecated in 1.11. Use :func:`execute_sql` for single sink,
-                  use :func:`create_statement_set` for multiple sinks.
-        """
-        warnings.warn("Deprecated in 1.11. Use execute_sql for single sink, "
-                      "use create_statement_set for multiple sinks.", DeprecationWarning)
-        self._before_execute()
-        return JobExecutionResult(self._j_tenv.execute(job_name))
-
     def from_elements(self, elements: Iterable, schema: Union[DataType, List[str]] = None,
                       verify_schema: bool = True) -> Table:
         """
diff --git a/flink-python/pyflink/table/tests/test_sql.py b/flink-python/pyflink/table/tests/test_sql.py
index dbaa6244aef..3ba0901438c 100644
--- a/flink-python/pyflink/table/tests/test_sql.py
+++ b/flink-python/pyflink/table/tests/test_sql.py
@@ -101,22 +101,6 @@ class StreamSqlTests(PyFlinkStreamTableTestCase):
         self.assertEqual(table_result.get_result_kind(), ResultKind.SUCCESS)
         table_result.print()
 
-    def test_sql_update(self):
-        t_env = self.t_env
-        source = t_env.from_elements([(1, "Hi", "Hello"), (2, "Hello", "Hello")], ["a", "b", "c"])
-        field_names = ["a", "b", "c"]
-        field_types = [DataTypes.BIGINT(), DataTypes.STRING(), DataTypes.STRING()]
-        t_env.register_table_sink(
-            "sinks",
-            source_sink_utils.TestAppendSink(field_names, field_types))
-
-        t_env.sql_update("insert into sinks select * from %s" % source)
-        self.t_env.execute("test_sql_job")
-
-        actual = source_sink_utils.results()
-        expected = ['+I[1, Hi, Hello]', '+I[2, Hello, Hello]']
-        self.assert_equals(actual, expected)
-
 
 class JavaSqlTests(PyFlinkTestCase):
     """
diff --git a/flink-python/pyflink/table/tests/test_table_environment_api.py b/flink-python/pyflink/table/tests/test_table_environment_api.py
index 4019e830cd7..3abbc8b8c02 100644
--- a/flink-python/pyflink/table/tests/test_table_environment_api.py
+++ b/flink-python/pyflink/table/tests/test_table_environment_api.py
@@ -284,8 +284,7 @@ class DataStreamConversionTestCases(PyFlinkTestCase):
         field_types = [DataTypes.INT(), DataTypes.STRING(), DataTypes.STRING()]
         t_env.register_table_sink("Sink",
                                   source_sink_utils.TestAppendSink(field_names, field_types))
-        t_env.insert_into("Sink", table)
-        t_env.execute("test_from_data_stream")
+        table.execute_insert("Sink").wait()
         result = source_sink_utils.results()
         expected = ['+I[1, Hi, Hello]', '+I[2, Hello, Hi]']
         self.assert_equals(result, expected)
@@ -295,8 +294,7 @@ class DataStreamConversionTestCases(PyFlinkTestCase):
         table = t_env.from_data_stream(ds, col('a'), col('b'), col('c'))
         t_env.register_table_sink("ExprSink",
                                   source_sink_utils.TestAppendSink(field_names, field_types))
-        t_env.insert_into("ExprSink", table)
-        t_env.execute("test_from_data_stream_with_expr")
+        table.execute_insert("ExprSink").wait()
         result = source_sink_utils.results()
         self.assert_equals(result, expected)
 
diff --git a/flink-python/pyflink/table/tests/test_udf.py b/flink-python/pyflink/table/tests/test_udf.py
index 057eb52295d..50589435b2e 100644
--- a/flink-python/pyflink/table/tests/test_udf.py
+++ b/flink-python/pyflink/table/tests/test_udf.py
@@ -203,8 +203,7 @@ class UserDefinedFunctionTests(object):
                              "cast ('2014-09-13' as DATE),"
                              "cast ('12:00:00' as TIME),"
                              "cast ('1999-9-10 05:20:10' as TIMESTAMP))"
-                             " from test_table").insert_into("Results")
-        self.t_env.execute("test")
+                             " from test_table").execute_insert("Results").wait()
         actual = source_sink_utils.results()
         self.assert_equals(actual, ["+I[3, 8]", "+I[3, 9]", "+I[3, 10]"])
 
diff --git a/flink-table/flink-table-api-java-bridge/src/main/java/org/apache/flink/table/api/bridge/java/StreamTableEnvironment.java b/flink-table/flink-table-api-java-bridge/src/main/java/org/apache/flink/table/api/bridge/java/StreamTableEnvironment.java
index 7e4bd4c48e0..f32172b0118 100644
--- a/flink-table/flink-table-api-java-bridge/src/main/java/org/apache/flink/table/api/bridge/java/StreamTableEnvironment.java
+++ b/flink-table/flink-table-api-java-bridge/src/main/java/org/apache/flink/table/api/bridge/java/StreamTableEnvironment.java
@@ -19,7 +19,6 @@
 package org.apache.flink.table.api.bridge.java;
 
 import org.apache.flink.annotation.PublicEvolving;
-import org.apache.flink.api.common.JobExecutionResult;
 import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.api.common.typeinfo.Types;
 import org.apache.flink.api.common.typeutils.CompositeType;
@@ -1132,23 +1131,4 @@ public interface StreamTableEnvironment extends TableEnvironment {
      */
     @Deprecated
     <T> DataStream<Tuple2<Boolean, T>> toRetractStream(Table table, TypeInformation<T> typeInfo);
-
-    /**
-     * Triggers the program execution. The environment will execute all parts of the program.
-     *
-     * <p>The program execution will be logged and displayed with the provided name
-     *
-     * <p>It calls the {@link StreamExecutionEnvironment#execute(String)} on the underlying {@link
-     * StreamExecutionEnvironment}. In contrast to the {@link TableEnvironment} this environment
-     * translates queries eagerly.
-     *
-     * @param jobName Desired name of the job
-     * @return The result of the job execution, containing elapsed time and accumulators.
-     * @throws Exception which occurs during job execution.
-     * @deprecated Use {@link StreamExecutionEnvironment#execute(String)} instead or directly call
-     *     the execute methods of the Table API such as {@link #executeSql(String)}.
-     */
-    @Deprecated
-    @Override
-    JobExecutionResult execute(String jobName) throws Exception;
 }
diff --git a/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/Table.java b/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/Table.java
index b727b2eba88..e2ca97aef86 100644
--- a/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/Table.java
+++ b/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/Table.java
@@ -27,7 +27,6 @@ import org.apache.flink.table.expressions.Expression;
 import org.apache.flink.table.functions.TableFunction;
 import org.apache.flink.table.functions.TemporalTableFunction;
 import org.apache.flink.table.operations.QueryOperation;
-import org.apache.flink.table.sinks.TableSink;
 import org.apache.flink.table.types.DataType;
 
 /**
@@ -942,19 +941,6 @@ public interface Table {
         return offset(offset).fetch(fetch);
     }
 
-    /**
-     * Writes the {@link Table} to a {@link DynamicTableSink} that was registered under the
-     * specified path. For the path resolution algorithm see {@link
-     * TableEnvironment#useDatabase(String)}.
-     *
-     * @param tablePath The path of the registered {@link TableSink} to which the {@link Table} is
-     *     written.
-     * @deprecated use {@link #executeInsert(String)} for single sink, use {@link
-     *     TableEnvironment#createStatementSet()} for multiple sinks.
-     */
-    @Deprecated
-    void insertInto(String tablePath);
-
     /**
      * Groups the records of a table by assigning them to windows defined by a time or row interval.
      *
diff --git a/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/TableEnvironment.java b/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/TableEnvironment.java
index dfd94dd3842..0a834497e48 100644
--- a/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/TableEnvironment.java
+++ b/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/TableEnvironment.java
@@ -20,7 +20,6 @@ package org.apache.flink.table.api;
 
 import org.apache.flink.annotation.Experimental;
 import org.apache.flink.annotation.PublicEvolving;
-import org.apache.flink.api.common.JobExecutionResult;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.table.api.config.TableConfigOptions;
 import org.apache.flink.table.api.internal.TableEnvironmentImpl;
@@ -31,8 +30,6 @@ import org.apache.flink.table.functions.ScalarFunction;
 import org.apache.flink.table.functions.UserDefinedFunction;
 import org.apache.flink.table.module.Module;
 import org.apache.flink.table.module.ModuleEntry;
-import org.apache.flink.table.sinks.TableSink;
-import org.apache.flink.table.sources.TableSource;
 import org.apache.flink.table.types.AbstractDataType;
 
 import java.io.Serializable;
@@ -380,14 +377,6 @@ public interface TableEnvironment {
      */
     Table fromValues(AbstractDataType<?> rowType, Iterable<?> values);
 
-    /**
-     * Creates a table from a table source.
-     *
-     * @param source table source used as table
-     */
-    @Deprecated
-    Table fromTableSource(TableSource<?> source);
-
     /**
      * Registers a {@link Catalog} under a unique name. All tables registered in the {@link Catalog}
      * can be accessed.
@@ -768,39 +757,6 @@ public interface TableEnvironment {
      */
     Table from(TableDescriptor descriptor);
 
-    /**
-     * Writes the {@link Table} to a {@link TableSink} that was registered under the specified name.
-     *
-     * <p>See the documentation of {@link TableEnvironment#useDatabase(String)} or {@link
-     * TableEnvironment#useCatalog(String)} for the rules on the path resolution.
-     *
-     * @param table The Table to write to the sink.
-     * @param sinkPath The first part of the path of the registered {@link TableSink} to which the
-     *     {@link Table} is written. This is to ensure at least the name of the {@link TableSink} is
-     *     provided.
-     * @param sinkPathContinued The remaining part of the path of the registered {@link TableSink}
-     *     to which the {@link Table} is written.
-     * @deprecated use {@link Table#executeInsert(String)} for single sink, use {@link
-     *     TableEnvironment#createStatementSet()} for multiple sinks.
-     */
-    @Deprecated
-    void insertInto(Table table, String sinkPath, String... sinkPathContinued);
-
-    /**
-     * Instructs to write the content of a {@link Table} API object into a table.
-     *
-     * <p>See the documentation of {@link TableEnvironment#useDatabase(String)} or {@link
-     * TableEnvironment#useCatalog(String)} for the rules on the path resolution.
-     *
-     * @param targetPath The path of the registered {@link TableSink} to which the {@link Table} is
-     *     written.
-     * @param table The Table to write to the sink.
-     * @deprecated use {@link Table#executeInsert(String)} for single sink, use {@link
-     *     TableEnvironment#createStatementSet()} for multiple sinks.
-     */
-    @Deprecated
-    void insertInto(String targetPath, Table table);
-
     /**
      * Gets the names of all catalogs registered in this environment.
      *
@@ -897,37 +853,6 @@ public interface TableEnvironment {
      */
     boolean dropTemporaryView(String path);
 
-    /**
-     * Returns the AST of the specified Table API and SQL queries and the execution plan to compute
-     * the result of the given {@link Table}.
-     *
-     * @param table The table for which the AST and execution plan will be returned.
-     * @deprecated use {@link Table#explain(ExplainDetail...)}.
-     */
-    @Deprecated
-    String explain(Table table);
-
-    /**
-     * Returns the AST of the specified Table API and SQL queries and the execution plan to compute
-     * the result of the given {@link Table}.
-     *
-     * @param table The table for which the AST and execution plan will be returned.
-     * @param extended if the plan should contain additional properties, e.g. estimated cost, traits
-     * @deprecated use {@link Table#explain(ExplainDetail...)}.
-     */
-    @Deprecated
-    String explain(Table table, boolean extended);
-
-    /**
-     * Returns the AST of the specified Table API and SQL queries and the execution plan to compute
-     * the result of multiple-sinks plan.
-     *
-     * @param extended if the plan should contain additional properties, e.g. estimated cost, traits
-     * @deprecated use {@link StatementSet#explain(ExplainDetail...)}.
-     */
-    @Deprecated
-    String explain(boolean extended);
-
     /**
      * Returns the AST of the specified statement and the execution plan to compute the result of
      * the given statement.
@@ -999,87 +924,6 @@ public interface TableEnvironment {
      */
     TableResult executeSql(String statement);
 
-    /**
-     * Evaluates a SQL statement such as INSERT, UPDATE or DELETE; or a DDL statement; NOTE:
-     * Currently only SQL INSERT statements and CREATE TABLE statements are supported.
-     *
-     * <p>All tables referenced by the query must be registered in the TableEnvironment. A {@link
-     * Table} is automatically registered when its {@link Table#toString()} method is called, for
-     * example when it is embedded into a String. Hence, SQL queries can directly reference a {@link
-     * Table} as follows:
-     *
-     * <pre>{@code
-     * // register the configured table sink into which the result is inserted.
-     * tEnv.registerTableSinkInternal("sinkTable", configuredSink);
-     * Table sourceTable = ...
-     * String tableName = sourceTable.toString();
-     * // sourceTable is not registered to the table environment
-     * tEnv.sqlUpdate(s"INSERT INTO sinkTable SELECT * FROM tableName");
-     * }</pre>
-     *
-     * <p>A DDL statement can also be executed to create a table: For example, the below DDL
-     * statement would create a CSV table named `tbl1` into the current catalog:
-     *
-     * <blockquote>
-     *
-     * <pre>
-     *    create table tbl1(
-     *      a int,
-     *      b bigint,
-     *      c varchar
-     *    ) with (
-     *      'connector.type' = 'filesystem',
-     *      'format.type' = 'csv',
-     *      'connector.path' = 'xxx'
-     *    )
-     * </pre>
-     *
-     * </blockquote>
-     *
-     * <p>SQL queries can directly execute as follows:
-     *
-     * <blockquote>
-     *
-     * <pre>
-     *    String sinkDDL = "create table sinkTable(
-     *                        a int,
-     *                        b varchar
-     *                      ) with (
-     *                        'connector.type' = 'filesystem',
-     *                        'format.type' = 'csv',
-     *                        'connector.path' = 'xxx'
-     *                      )";
-     *
-     *    String sourceDDL ="create table sourceTable(
-     *                        a int,
-     *                        b varchar
-     *                      ) with (
-     *                        'connector.type' = 'kafka',
-     *                        'update-mode' = 'append',
-     *                        'connector.topic' = 'xxx',
-     *                        'connector.properties.bootstrap.servers' = 'localhost:9092',
-     *                        ...
-     *                      )";
-     *
-     *    String query = "INSERT INTO sinkTable SELECT * FROM sourceTable";
-     *
-     *    tEnv.sqlUpdate(sourceDDL);
-     *    tEnv.sqlUpdate(sinkDDL);
-     *    tEnv.sqlUpdate(query);
-     *    tEnv.execute("MyJob");
-     * </pre>
-     *
-     * </blockquote>
-     *
-     * <p>This code snippet creates a job to read data from Kafka source into a CSV sink.
-     *
-     * @param stmt The SQL statement to evaluate.
-     * @deprecated use {@link #executeSql(String)} for single statement, use {@link
-     *     TableEnvironment#createStatementSet()} for multiple DML statements.
-     */
-    @Deprecated
-    void sqlUpdate(String stmt);
-
     /**
      * Gets the current default catalog name of the current session.
      *
@@ -1221,32 +1065,6 @@ public interface TableEnvironment {
     /** Returns the table config that defines the runtime behavior of the Table API. */
     TableConfig getConfig();
 
-    /**
-     * Triggers the program execution. The environment will execute all parts of the program.
-     *
-     * <p>The program execution will be logged and displayed with the provided name
-     *
-     * <p><b>NOTE:</b>It is highly advised to set all parameters in the {@link TableConfig} on the
-     * very beginning of the program. It is undefined what configurations values will be used for
-     * the execution if queries are mixed with config changes. It depends on the characteristic of
-     * the particular parameter. For some of them the value from the point in time of query
-     * construction (e.g. the currentCatalog) will be used. On the other hand some values might be
-     * evaluated according to the state from the time when this method is called (e.g. timeZone).
-     *
-     * <p>Once the execution finishes, any previously defined DMLs will be cleared, no matter
-     * whether the execution succeeds or not. Therefore, if you want to retry in case of failures,
-     * you have to re-define the DMLs, i.e. by calling {@link #sqlUpdate(String)}, before you call
-     * this method again.
-     *
-     * @param jobName Desired name of the job
-     * @return The result of the job execution, containing elapsed time and accumulators.
-     * @throws Exception which occurs during job execution.
-     * @deprecated use {@link #executeSql(String)} or {@link Table#executeInsert(String)} for single
-     *     sink, use {@link #createStatementSet()} for multiple sinks.
-     */
-    @Deprecated
-    JobExecutionResult execute(String jobName) throws Exception;
-
     /**
      * Returns a {@link StatementSet} that accepts pipelines defined by DML statements or {@link
      * Table} objects. The planner can optimize all added statements together and then submit them
diff --git a/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/internal/TableEnvironmentImpl.java b/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/internal/TableEnvironmentImpl.java
index cac88fd534e..fb241b4bac8 100644
--- a/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/internal/TableEnvironmentImpl.java
+++ b/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/internal/TableEnvironmentImpl.java
@@ -20,7 +20,6 @@ package org.apache.flink.table.api.internal;
 
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.annotation.VisibleForTesting;
-import org.apache.flink.api.common.JobExecutionResult;
 import org.apache.flink.api.dag.Pipeline;
 import org.apache.flink.api.dag.Transformation;
 import org.apache.flink.configuration.Configuration;
@@ -187,7 +186,6 @@ public class TableEnvironmentImpl implements TableEnvironmentInternal {
     private final CatalogManager catalogManager;
     private final ModuleManager moduleManager;
     private final OperationTreeBuilder operationTreeBuilder;
-    private final List<ModifyOperation> bufferedModifyOperations = new ArrayList<>();
 
     protected final TableConfig tableConfig;
     protected final Executor execEnv;
@@ -195,12 +193,6 @@ public class TableEnvironmentImpl implements TableEnvironmentInternal {
     protected final Planner planner;
     private final boolean isStreamingMode;
     private final ClassLoader userClassLoader;
-    private static final String UNSUPPORTED_QUERY_IN_SQL_UPDATE_MSG =
-            "Unsupported SQL query! sqlUpdate() only accepts a single SQL statement of type "
-                    + "INSERT, CREATE TABLE, DROP TABLE, ALTER TABLE, USE CATALOG, USE [CATALOG.]DATABASE, "
-                    + "CREATE DATABASE, DROP DATABASE, ALTER DATABASE, CREATE FUNCTION, DROP FUNCTION, ALTER FUNCTION, "
-                    + "CREATE CATALOG, DROP CATALOG, CREATE VIEW, DROP VIEW, LOAD MODULE, UNLOAD "
-                    + "MODULE, USE MODULES.";
     private static final String UNSUPPORTED_QUERY_IN_EXECUTE_SQL_MSG =
             "Unsupported SQL query! executeSql() only accepts a single SQL statement of type "
                     + "CREATE TABLE, DROP TABLE, ALTER TABLE, CREATE DATABASE, DROP DATABASE, ALTER DATABASE, "
@@ -555,34 +547,6 @@ public class TableEnvironmentImpl implements TableEnvironmentInternal {
         return createTable(queryOperation);
     }
 
-    @Override
-    public void insertInto(String targetPath, Table table) {
-        UnresolvedIdentifier unresolvedIdentifier = getParser().parseIdentifier(targetPath);
-        insertIntoInternal(unresolvedIdentifier, table);
-    }
-
-    @Override
-    public void insertInto(Table table, String sinkPath, String... sinkPathContinued) {
-        List<String> fullPath = new ArrayList<>(Arrays.asList(sinkPathContinued));
-        fullPath.add(0, sinkPath);
-        UnresolvedIdentifier unresolvedIdentifier = UnresolvedIdentifier.of(fullPath);
-
-        insertIntoInternal(unresolvedIdentifier, table);
-    }
-
-    private void insertIntoInternal(UnresolvedIdentifier unresolvedIdentifier, Table table) {
-        ObjectIdentifier objectIdentifier = catalogManager.qualifyIdentifier(unresolvedIdentifier);
-
-        ContextResolvedTable contextResolvedTable =
-                catalogManager.getTableOrError(objectIdentifier);
-
-        List<ModifyOperation> modifyOperations =
-                Collections.singletonList(
-                        new SinkModifyOperation(contextResolvedTable, table.getQueryOperation()));
-
-        buffer(modifyOperations);
-    }
-
     private Optional<SourceQueryOperation> scanInternal(UnresolvedIdentifier identifier) {
         ObjectIdentifier tableIdentifier = catalogManager.qualifyIdentifier(identifier);
 
@@ -671,26 +635,6 @@ public class TableEnvironmentImpl implements TableEnvironmentInternal {
         return functions;
     }
 
-    @Override
-    public String explain(Table table) {
-        return explain(table, false);
-    }
-
-    @Override
-    public String explain(Table table, boolean extended) {
-        return planner.explain(
-                Collections.singletonList(table.getQueryOperation()), getExplainDetails(extended));
-    }
-
-    @Override
-    public String explain(boolean extended) {
-        List<Operation> operations =
-                bufferedModifyOperations.stream()
-                        .map(o -> (Operation) o)
-                        .collect(Collectors.toList());
-        return planner.explain(operations, getExplainDetails(extended));
-    }
-
     @Override
     public String explainSql(String statement, ExplainDetail... extraDetails) {
         List<Operation> operations = getParser().parse(statement);
@@ -908,43 +852,6 @@ public class TableEnvironmentImpl implements TableEnvironmentInternal {
         }
     }
 
-    @Override
-    public void sqlUpdate(String stmt) {
-        List<Operation> operations = getParser().parse(stmt);
-
-        if (operations.size() != 1) {
-            throw new TableException(UNSUPPORTED_QUERY_IN_SQL_UPDATE_MSG);
-        }
-
-        Operation operation = operations.get(0);
-        if (operation instanceof ModifyOperation) {
-            buffer(Collections.singletonList((ModifyOperation) operation));
-        } else if (operation instanceof CreateTableOperation
-                || operation instanceof DropTableOperation
-                || operation instanceof AlterTableOperation
-                || operation instanceof CreateViewOperation
-                || operation instanceof DropViewOperation
-                || operation instanceof CreateDatabaseOperation
-                || operation instanceof DropDatabaseOperation
-                || operation instanceof AlterDatabaseOperation
-                || operation instanceof CreateCatalogFunctionOperation
-                || operation instanceof CreateTempSystemFunctionOperation
-                || operation instanceof DropCatalogFunctionOperation
-                || operation instanceof DropTempSystemFunctionOperation
-                || operation instanceof AlterCatalogFunctionOperation
-                || operation instanceof CreateCatalogOperation
-                || operation instanceof DropCatalogOperation
-                || operation instanceof UseCatalogOperation
-                || operation instanceof UseDatabaseOperation
-                || operation instanceof LoadModuleOperation
-                || operation instanceof UnloadModuleOperation
-                || operation instanceof NopOperation) {
-            executeInternal(operation);
-        } else {
-            throw new TableException(UNSUPPORTED_QUERY_IN_SQL_UPDATE_MSG);
-        }
-    }
-
     @Override
     public TableResultInternal executeInternal(Operation operation) {
         if (operation instanceof ModifyOperation) {
@@ -1677,14 +1584,6 @@ public class TableEnvironmentImpl implements TableEnvironmentInternal {
         return tableConfig;
     }
 
-    @Override
-    public JobExecutionResult execute(String jobName) throws Exception {
-        Pipeline pipeline =
-                execEnv.createPipeline(
-                        translateAndClearBuffer(), tableConfig.getConfiguration(), jobName);
-        return execEnv.execute(pipeline);
-    }
-
     @Override
     public Parser getParser() {
         return getPlanner().getParser();
@@ -1721,46 +1620,10 @@ public class TableEnvironmentImpl implements TableEnvironmentInternal {
         TableSourceValidation.validateTableSource(tableSource, tableSource.getTableSchema());
     }
 
-    /**
-     * Translate the buffered operations to Transformations, and clear the buffer.
-     *
-     * <p>The buffer will be clear even if the `translate` fails. In most cases, the failure is not
-     * retryable (e.g. type mismatch, can't generate physical plan). If the buffer is not clear
-     * after failure, the following `translate` will also fail.
-     */
-    protected List<Transformation<?>> translateAndClearBuffer() {
-        List<Transformation<?>> transformations;
-        try {
-            transformations = translate(bufferedModifyOperations);
-        } finally {
-            bufferedModifyOperations.clear();
-        }
-        return transformations;
-    }
-
     protected List<Transformation<?>> translate(List<ModifyOperation> modifyOperations) {
         return planner.translate(modifyOperations);
     }
 
-    private void buffer(List<ModifyOperation> modifyOperations) {
-        bufferedModifyOperations.addAll(modifyOperations);
-    }
-
-    @VisibleForTesting
-    protected ExplainDetail[] getExplainDetails(boolean extended) {
-        if (extended) {
-            if (isStreamingMode) {
-                return new ExplainDetail[] {
-                    ExplainDetail.ESTIMATED_COST, ExplainDetail.CHANGELOG_MODE
-                };
-            } else {
-                return new ExplainDetail[] {ExplainDetail.ESTIMATED_COST};
-            }
-        } else {
-            return new ExplainDetail[0];
-        }
-    }
-
     @Override
     public void registerTableSourceInternal(String name, TableSource<?> tableSource) {
         validateTableSource(tableSource);
diff --git a/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/internal/TableEnvironmentInternal.java b/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/internal/TableEnvironmentInternal.java
index 65e50830ff4..9e2f950fccc 100644
--- a/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/internal/TableEnvironmentInternal.java
+++ b/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/internal/TableEnvironmentInternal.java
@@ -22,6 +22,7 @@ import org.apache.flink.annotation.Experimental;
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.table.api.CompiledPlan;
 import org.apache.flink.table.api.ExplainDetail;
+import org.apache.flink.table.api.Table;
 import org.apache.flink.table.api.TableEnvironment;
 import org.apache.flink.table.catalog.CatalogManager;
 import org.apache.flink.table.delegation.Parser;
@@ -57,6 +58,14 @@ public interface TableEnvironmentInternal extends TableEnvironment {
     /** Returns a {@link OperationTreeBuilder} that can create {@link QueryOperation}s. */
     OperationTreeBuilder getOperationTreeBuilder();
 
+    /**
+     * Creates a table from a table source.
+     *
+     * @param source table source used as table
+     */
+    @Deprecated
+    Table fromTableSource(TableSource<?> source);
+
     /**
      * Execute the given modify operations and return the execution result.
      *
diff --git a/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/internal/TableImpl.java b/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/internal/TableImpl.java
index 7c755df8129..d6fcb58a4b0 100644
--- a/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/internal/TableImpl.java
+++ b/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/internal/TableImpl.java
@@ -430,11 +430,6 @@ public class TableImpl implements Table {
         return createTable(operationTreeBuilder.limitWithFetch(fetch, operationTree));
     }
 
-    @Override
-    public void insertInto(String tablePath) {
-        tableEnvironment.insertInto(tablePath, this);
-    }
-
     @Override
     public GroupWindowedTable window(GroupWindow groupWindow) {
         return new GroupWindowedTableImpl(this, groupWindow);
diff --git a/flink-table/flink-table-api-scala-bridge/src/main/scala/org/apache/flink/table/api/bridge/scala/StreamTableEnvironment.scala b/flink-table/flink-table-api-scala-bridge/src/main/scala/org/apache/flink/table/api/bridge/scala/StreamTableEnvironment.scala
index 8ccad406908..49d0b7a556f 100644
--- a/flink-table/flink-table-api-scala-bridge/src/main/scala/org/apache/flink/table/api/bridge/scala/StreamTableEnvironment.scala
+++ b/flink-table/flink-table-api-scala-bridge/src/main/scala/org/apache/flink/table/api/bridge/scala/StreamTableEnvironment.scala
@@ -844,26 +844,6 @@ trait StreamTableEnvironment extends TableEnvironment {
     */
   @deprecated
   def toRetractStream[T: TypeInformation](table: Table): DataStream[(Boolean, T)]
-
-  /**
-    * Triggers the program execution. The environment will execute all parts of
-    * the program.
-    *
-    * The program execution will be logged and displayed with the provided name
-    *
-    * It calls the StreamExecutionEnvironment#execute on the underlying
-    * [[StreamExecutionEnvironment]]. In contrast to the [[TableEnvironment]] this
-    * environment translates queries eagerly.
-    *
-    * @param jobName Desired name of the job
-    * @return The result of the job execution, containing elapsed time and accumulators.
-    * @throws Exception which occurs during job execution.
-    * @deprecated Use [[StreamExecutionEnvironment.execute(String)]] instead or directly call
-    *             the execute methods of the Table API such as [[executeSql(String)]].
-    */
-  @deprecated
-  @throws[Exception]
-  override def execute(jobName: String): JobExecutionResult
 }
 
 object StreamTableEnvironment {
diff --git a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/api/TableEnvironmentITCase.scala b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/api/TableEnvironmentITCase.scala
index 0a77de6d30d..9d5cb959cbe 100644
--- a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/api/TableEnvironmentITCase.scala
+++ b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/api/TableEnvironmentITCase.scala
@@ -43,11 +43,11 @@ import org.junit.runner.RunWith
 import org.junit.runners.Parameterized
 import org.junit.{Assert, Before, Rule, Test}
 
-import _root_.java.io.{File, FileFilter}
-import _root_.java.lang.{Long => JLong}
-import _root_.java.util
+import java.io.{File, FileFilter}
+import java.lang.{Long => JLong}
+import java.util
 
-import _root_.scala.collection.mutable
+import scala.collection.mutable
 
 @RunWith(classOf[Parameterized])
 class TableEnvironmentITCase(tableEnvName: String, isStreaming: Boolean) extends TestLogger {
@@ -136,8 +136,7 @@ class TableEnvironmentITCase(tableEnvName: String, isStreaming: Boolean) extends
     checkEmptyFile(sink2Path)
 
     val table1 = tEnv.sqlQuery("select first from MyTable")
-    tEnv.insertInto(table1, "MySink1")
-    tEnv.execute("test1")
+    table1.executeInsert("MySink1").await()
     assertFirstValues(sink1Path)
     checkEmptyFile(sink2Path)
 
@@ -146,8 +145,7 @@ class TableEnvironmentITCase(tableEnvName: String, isStreaming: Boolean) extends
     assertFalse(new File(sink1Path).exists())
 
     val table2 = tEnv.sqlQuery("select last from MyTable")
-    tEnv.insertInto(table2, "MySink2")
-    tEnv.execute("test2")
+    table2.executeInsert("MySink2").await()
     assertFalse(new File(sink1Path).exists())
     assertLastValues(sink2Path)
   }
@@ -158,52 +156,75 @@ class TableEnvironmentITCase(tableEnvName: String, isStreaming: Boolean) extends
       tEnv, new TableSchema(Array("first"), Array(STRING)), "MySink1")
 
     val table1 = tEnv.sqlQuery("select first from MyTable")
-    tEnv.insertInto(table1, "MySink1")
-
-    tEnv.explain(false)
-    tEnv.execute("test1")
+    table1.executeInsert("MySink1").await()
     assertFirstValues(sinkPath)
   }
 
   @Test
-  def testExplainAndExecuteMultipleSink(): Unit = {
-    val sink1Path = TestTableSourceSinks.createCsvTemporarySinkTable(
+  def testExecuteSqlWithInsertInto(): Unit = {
+    val sinkPath = TestTableSourceSinks.createCsvTemporarySinkTable(
       tEnv, new TableSchema(Array("first"), Array(STRING)), "MySink1")
+    checkEmptyFile(sinkPath)
+    val tableResult = tEnv.executeSql("insert into MySink1 select first from MyTable")
+    checkInsertTableResult(tableResult, "default_catalog.default_database.MySink1")
+    assertFirstValues(sinkPath)
+  }
 
-    val sink2Path = TestTableSourceSinks.createCsvTemporarySinkTable(
-      tEnv, new TableSchema(Array("first"), Array(STRING)), "MySink2")
+  @Test
+  def testExecuteSqlWithInsertOverwrite(): Unit = {
+    if(isStreaming) {
+      // Streaming mode not support overwrite for FileSystemTableSink.
+      return
+    }
 
-    val table1 = tEnv.sqlQuery("select first from MyTable")
-    tEnv.insertInto(table1, "MySink1")
-    val table2 = tEnv.sqlQuery("select last from MyTable")
-    tEnv.insertInto(table2, "MySink2")
+    val sinkPath = _tempFolder.newFolder().toString
+    tEnv.executeSql(
+      s"""
+         |create table MySink (
+         |  first string
+         |) with (
+         |  'connector' = 'filesystem',
+         |  'path' = '$sinkPath',
+         |  'format' = 'testcsv'
+         |)
+       """.stripMargin
+    )
 
-    tEnv.explain(false)
-    tEnv.execute("test1")
-    assertFirstValues(sink1Path)
-    assertLastValues(sink2Path)
+    val tableResult1 = tEnv.executeSql("insert overwrite MySink select first from MyTable")
+    checkInsertTableResult(tableResult1, "default_catalog.default_database.MySink")
+    assertFirstValues(sinkPath)
+
+    val tableResult2 =  tEnv.executeSql("insert overwrite MySink select first from MyTable")
+    checkInsertTableResult(tableResult2, "default_catalog.default_database.MySink")
+    assertFirstValues(sinkPath)
   }
 
   @Test
-  def testExplainTwice(): Unit = {
+  def testExecuteSqlAndExecuteInsert(): Unit = {
     val sink1Path = TestTableSourceSinks.createCsvTemporarySinkTable(
       tEnv, new TableSchema(Array("first"), Array(STRING)), "MySink1")
-
     val sink2Path = TestTableSourceSinks.createCsvTemporarySinkTable(
-      tEnv, new TableSchema(Array("first"), Array(STRING)), "MySink2")
+      tEnv, new TableSchema(Array("last"), Array(STRING)), "MySink2")
+    checkEmptyFile(sink1Path)
+    checkEmptyFile(sink2Path)
 
-    val table1 = tEnv.sqlQuery("select first from MyTable")
-    tEnv.insertInto(table1, "MySink1")
-    val table2 = tEnv.sqlQuery("select last from MyTable")
-    tEnv.insertInto(table2, "MySink2")
+    val tableResult = tEnv.executeSql("insert into MySink1 select first from MyTable")
+    checkInsertTableResult(tableResult, "default_catalog.default_database.MySink1")
+
+    assertFirstValues(sink1Path)
+    checkEmptyFile(sink2Path)
+
+    // delete first csv file
+    new File(sink1Path).delete()
+    assertFalse(new File(sink1Path).exists())
 
-    val result1 = tEnv.explain(false)
-    val result2 = tEnv.explain(false)
-    assertEquals(replaceStageId(result1), replaceStageId(result2))
+    tEnv.sqlQuery("select last from MyTable").executeInsert("MySink2").await()
+    assertFalse(new File(sink1Path).exists())
+    assertLastValues(sink2Path)
   }
 
   @Test
-  def testSqlUpdateAndToDataStream(): Unit = {
+  def testExecuteSqlAndToDataStream(): Unit = {
     if (!tableEnvName.equals("StreamTableEnvironment")) {
       return
     }
@@ -214,19 +235,13 @@ class TableEnvironmentITCase(tableEnvName: String, isStreaming: Boolean) extends
       streamTableEnv, new TableSchema(Array("first"), Array(STRING)), "MySink1")
     checkEmptyFile(sink1Path)
 
-    streamTableEnv.sqlUpdate("insert into MySink1 select first from MyTable")
-
     val table = streamTableEnv.sqlQuery("select last from MyTable where id > 0")
     val resultSet = streamTableEnv.toAppendStream(table, classOf[Row])
     val sink = new TestingAppendSink
     resultSet.addSink(sink)
 
-    val explain = streamTableEnv.explain(false)
-    assertEquals(
-      replaceStageId(readFromResource("/explain/testSqlUpdateAndToDataStream.out")),
-      replaceStageId(explain))
-
-    streamTableEnv.execute("test1")
+    val tableResult = streamTableEnv.executeSql("insert into MySink1 select first from MyTable")
+    checkInsertTableResult(tableResult, "default_catalog.default_database.MySink1")
     assertFirstValues(sink1Path)
 
     // the DataStream program is not executed
@@ -241,7 +256,7 @@ class TableEnvironmentITCase(tableEnvName: String, isStreaming: Boolean) extends
   }
 
   @Test
-  def testToDataStreamAndSqlUpdate(): Unit = {
+  def testToDataStreamAndExecuteSql(): Unit = {
     if (!tableEnvName.equals("StreamTableEnvironment")) {
       return
     }
@@ -257,9 +272,9 @@ class TableEnvironmentITCase(tableEnvName: String, isStreaming: Boolean) extends
     val sink = new TestingAppendSink
     resultSet.addSink(sink)
 
-    streamTableEnv.sqlUpdate("insert into MySink1 select first from MyTable")
+    val insertStmt = "insert into MySink1 select first from MyTable"
 
-    val explain = streamTableEnv.explain(false)
+    val explain = streamTableEnv.explainSql(insertStmt)
     assertEquals(
       replaceStageId(readFromResource("/explain/testSqlUpdateAndToDataStream.out")),
       replaceStageId(explain))
@@ -269,14 +284,14 @@ class TableEnvironmentITCase(tableEnvName: String, isStreaming: Boolean) extends
     checkEmptyFile(sink1Path)
     assertEquals(getExpectedLastValues.sorted, sink.getAppendResults.sorted)
 
-    streamTableEnv.execute("test1")
+    streamTableEnv.executeSql(insertStmt).await()
     assertFirstValues(sink1Path)
     // the DataStream program is not executed again because the result in sink is not changed
     assertEquals(getExpectedLastValues.sorted, sink.getAppendResults.sorted)
   }
 
   @Test
-  def testFromToDataStreamAndSqlUpdate(): Unit = {
+  def testFromToDataStreamAndExecuteSql(): Unit = {
     if (!tableEnvName.equals("StreamTableEnvironment")) {
       return
     }
@@ -294,9 +309,9 @@ class TableEnvironmentITCase(tableEnvName: String, isStreaming: Boolean) extends
     val sink = new TestingAppendSink
     resultSet.addSink(sink)
 
-    streamTableEnv.sqlUpdate("insert into MySink1 select first from MyTable")
+    val insertStmt = "insert into MySink1 select first from MyTable"
 
-    val explain = streamTableEnv.explain(false)
+    val explain = streamTableEnv.explainSql(insertStmt)
     assertEquals(
       replaceStageId(readFromResource("/explain/testFromToDataStreamAndSqlUpdate.out")),
       replaceStageId(explain))
@@ -306,109 +321,12 @@ class TableEnvironmentITCase(tableEnvName: String, isStreaming: Boolean) extends
     checkEmptyFile(sink1Path)
     assertEquals(getExpectedLastValues.sorted, sink.getAppendResults.sorted)
 
-    streamTableEnv.execute("test1")
+    streamTableEnv.executeSql(insertStmt).await()
     assertFirstValues(sink1Path)
     // the DataStream program is not executed again because the result in sink is not changed
     assertEquals(getExpectedLastValues.sorted, sink.getAppendResults.sorted)
   }
 
-  @Test
-  def testExecuteSqlWithInsertInto(): Unit = {
-    val sinkPath = TestTableSourceSinks.createCsvTemporarySinkTable(
-      tEnv, new TableSchema(Array("first"), Array(STRING)), "MySink1")
-    checkEmptyFile(sinkPath)
-    val tableResult = tEnv.executeSql("insert into MySink1 select first from MyTable")
-    checkInsertTableResult(tableResult, "default_catalog.default_database.MySink1")
-    assertFirstValues(sinkPath)
-  }
-
-  @Test
-  def testExecuteSqlWithInsertOverwrite(): Unit = {
-    if(isStreaming) {
-      // Streaming mode not support overwrite for FileSystemTableSink.
-      return
-    }
-
-    val sinkPath = _tempFolder.newFolder().toString
-    tEnv.executeSql(
-      s"""
-         |create table MySink (
-         |  first string
-         |) with (
-         |  'connector' = 'filesystem',
-         |  'path' = '$sinkPath',
-         |  'format' = 'testcsv'
-         |)
-       """.stripMargin
-    )
-
-    val tableResult1 = tEnv.executeSql("insert overwrite MySink select first from MyTable")
-    checkInsertTableResult(tableResult1, "default_catalog.default_database.MySink")
-    assertFirstValues(sinkPath)
-
-    val tableResult2 =  tEnv.executeSql("insert overwrite MySink select first from MyTable")
-    checkInsertTableResult(tableResult2, "default_catalog.default_database.MySink")
-    assertFirstValues(sinkPath)
-  }
-
-  @Test
-  def testExecuteSqlAndSqlUpdate(): Unit = {
-    val sink1Path = TestTableSourceSinks.createCsvTemporarySinkTable(
-      tEnv, new TableSchema(Array("first"), Array(STRING)), "MySink1")
-    val sink2Path = TestTableSourceSinks.createCsvTemporarySinkTable(
-      tEnv, new TableSchema(Array("last"), Array(STRING)), "MySink2")
-    checkEmptyFile(sink1Path)
-    checkEmptyFile(sink2Path)
-
-    val tableResult = tEnv.executeSql("insert into MySink1 select first from MyTable")
-    checkInsertTableResult(tableResult, "default_catalog.default_database.MySink1")
-
-    assertFirstValues(sink1Path)
-    checkEmptyFile(sink2Path)
-
-    // delete first csv file
-    new File(sink1Path).delete()
-    assertFalse(new File(sink1Path).exists())
-
-    val table2 = tEnv.sqlQuery("select last from MyTable")
-    tEnv.insertInto(table2, "MySink2")
-    tEnv.execute("test2")
-    assertFalse(new File(sink1Path).exists())
-    assertLastValues(sink2Path)
-  }
-
-  @Test
-  def testExecuteSqlAndToDataStream(): Unit = {
-    if (!tableEnvName.equals("StreamTableEnvironment")) {
-      return
-    }
-    val streamEnv = StreamExecutionEnvironment.getExecutionEnvironment
-    val streamTableEnv = StreamTableEnvironment.create(streamEnv, settings)
-    TestTableSourceSinks.createPersonCsvTemporaryTable(streamTableEnv, "MyTable")
-    val sink1Path = TestTableSourceSinks.createCsvTemporarySinkTable(
-      streamTableEnv, new TableSchema(Array("first"), Array(STRING)), "MySink1")
-    checkEmptyFile(sink1Path)
-
-    val table = streamTableEnv.sqlQuery("select last from MyTable where id > 0")
-    val resultSet = streamTableEnv.toAppendStream(table, classOf[Row])
-    val sink = new TestingAppendSink
-    resultSet.addSink(sink)
-
-    val tableResult = streamTableEnv.executeSql("insert into MySink1 select first from MyTable")
-    checkInsertTableResult(tableResult, "default_catalog.default_database.MySink1")
-    assertFirstValues(sink1Path)
-
-    // the DataStream program is not executed
-    assertFalse(sink.isInitialized)
-
-    deleteFile(sink1Path)
-
-    streamEnv.execute("test2")
-    assertEquals(getExpectedLastValues.sorted, sink.getAppendResults.sorted)
-    // the table program is not executed again
-    assertFileNotExist(sink1Path)
-  }
-
   @Test
   def testExecuteInsert(): Unit = {
     val sinkPath = TestTableSourceSinks.createCsvTemporarySinkTable(
diff --git a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/runtime/FileSystemITCaseBase.scala b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/runtime/FileSystemITCaseBase.scala
index 7bd0f2959c7..bded741d442 100644
--- a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/runtime/FileSystemITCaseBase.scala
+++ b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/runtime/FileSystemITCaseBase.scala
@@ -26,8 +26,8 @@ import org.apache.flink.table.api.config.ExecutionConfigOptions
 import org.apache.flink.table.planner.runtime.FileSystemITCaseBase._
 import org.apache.flink.table.planner.runtime.utils.BatchTableEnvUtil
 import org.apache.flink.table.planner.runtime.utils.BatchTestBase.row
-import org.apache.flink.table.utils.DateTimeUtils
 import org.apache.flink.types.Row
+
 import org.junit.Assert.{assertEquals, assertNotNull, assertTrue}
 import org.junit.rules.TemporaryFolder
 import org.junit.{Rule, Test}
@@ -36,6 +36,7 @@ import java.io.File
 import java.net.URI
 import java.nio.file.Paths
 import java.time.Instant
+
 import scala.collection.{JavaConverters, Seq}
 
 /**
@@ -467,11 +468,11 @@ trait FileSystemITCaseBase {
 
   @Test
   def testInsertAppend(): Unit = {
-    tableEnv.sqlUpdate("insert into partitionedTable select x, y, a, b from originalT")
-    tableEnv.execute("test1")
+    tableEnv.executeSql("insert into partitionedTable select x, y, a, b from originalT")
+      .await()
 
-    tableEnv.sqlUpdate("insert into partitionedTable select x, y, a, b from originalT")
-    tableEnv.execute("test2")
+    tableEnv.executeSql("insert into partitionedTable select x, y, a, b from originalT")
+      .await()
 
     check(
       "select y, b, x from partitionedTable where a=3",
@@ -487,11 +488,11 @@ trait FileSystemITCaseBase {
 
   @Test
   def testInsertOverwrite(): Unit = {
-    tableEnv.sqlUpdate("insert overwrite partitionedTable select x, y, a, b from originalT")
-    tableEnv.execute("test1")
+    tableEnv.executeSql("insert overwrite partitionedTable select x, y, a, b from originalT")
+      .await()
 
-    tableEnv.sqlUpdate("insert overwrite partitionedTable select x, y, a, b from originalT")
-    tableEnv.execute("test2")
+    tableEnv.executeSql("insert overwrite partitionedTable select x, y, a, b from originalT")
+      .await()
 
     check(
       "select y, b, x from partitionedTable where a=3",
