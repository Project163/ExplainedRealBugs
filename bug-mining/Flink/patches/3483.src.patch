diff --git a/flink-core/src/main/java/org/apache/flink/core/fs/FileSystem.java b/flink-core/src/main/java/org/apache/flink/core/fs/FileSystem.java
index b57f1f6b85f..912f1348095 100644
--- a/flink-core/src/main/java/org/apache/flink/core/fs/FileSystem.java
+++ b/flink-core/src/main/java/org/apache/flink/core/fs/FileSystem.java
@@ -35,6 +35,8 @@ import org.apache.flink.core.plugin.PluginManager;
 import org.apache.flink.core.plugin.TemporaryClassLoaderContext;
 import org.apache.flink.util.ExceptionUtils;
 
+import org.apache.flink.shaded.guava18.com.google.common.collect.Iterators;
+
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -294,7 +296,8 @@ public abstract class FileSystem {
 			factorySuppliers.add(() -> ServiceLoader.load(FileSystemFactory.class).iterator());
 
 			if (pluginManager != null) {
-				factorySuppliers.add(() -> pluginManager.load(FileSystemFactory.class));
+				factorySuppliers.add(() ->
+					Iterators.transform(pluginManager.load(FileSystemFactory.class), PluginFileSystemFactory::of));
 			}
 
 			final List<FileSystemFactory> fileSystemFactories = loadFileSystemFactories(factorySuppliers);
diff --git a/flink-core/src/main/java/org/apache/flink/core/fs/PluginFileSystemFactory.java b/flink-core/src/main/java/org/apache/flink/core/fs/PluginFileSystemFactory.java
new file mode 100644
index 00000000000..ee537c4b113
--- /dev/null
+++ b/flink-core/src/main/java/org/apache/flink/core/fs/PluginFileSystemFactory.java
@@ -0,0 +1,189 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.core.fs;
+
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.core.plugin.TemporaryClassLoaderContext;
+
+import java.io.IOException;
+import java.net.URI;
+
+/**
+ * A wrapper around {@link FileSystemFactory} that ensures the plugin classloader is used for all {@link FileSystem}
+ * operations.
+ */
+public class PluginFileSystemFactory implements FileSystemFactory {
+	private final FileSystemFactory inner;
+	private final ClassLoader loader;
+
+	private PluginFileSystemFactory(final FileSystemFactory inner, final ClassLoader loader) {
+		this.inner = inner;
+		this.loader = loader;
+	}
+
+	public static PluginFileSystemFactory of(final FileSystemFactory inner) {
+		return new PluginFileSystemFactory(inner, inner.getClass().getClassLoader());
+	}
+
+	@Override
+	public String getScheme() {
+		return inner.getScheme();
+	}
+
+	@Override
+	public ClassLoader getClassLoader() {
+		return inner.getClassLoader();
+	}
+
+	@Override
+	public void configure(final Configuration config) {
+		inner.configure(config);
+	}
+
+	@Override
+	public FileSystem create(final URI fsUri) throws IOException {
+		try (TemporaryClassLoaderContext ignored = new TemporaryClassLoaderContext(loader)) {
+			return new ClassLoaderFixingFileSystem(inner.create(fsUri), loader);
+		}
+	}
+
+	private static class ClassLoaderFixingFileSystem extends FileSystem {
+		private final FileSystem inner;
+		private final ClassLoader loader;
+
+		private ClassLoaderFixingFileSystem(final FileSystem inner, final ClassLoader loader) {
+			this.inner = inner;
+			this.loader = loader;
+		}
+
+		@Override
+		public Path getWorkingDirectory() {
+			try (TemporaryClassLoaderContext ignored = new TemporaryClassLoaderContext(loader)) {
+				return inner.getWorkingDirectory();
+			}
+		}
+
+		@Override
+		public Path getHomeDirectory() {
+			try (TemporaryClassLoaderContext ignored = new TemporaryClassLoaderContext(loader)) {
+				return inner.getHomeDirectory();
+			}
+		}
+
+		@Override
+		public URI getUri() {
+			try (TemporaryClassLoaderContext ignored = new TemporaryClassLoaderContext(loader)) {
+				return inner.getUri();
+			}
+		}
+
+		@Override
+		public FileStatus getFileStatus(final Path f) throws IOException {
+			try (TemporaryClassLoaderContext ignored = new TemporaryClassLoaderContext(loader)) {
+				return inner.getFileStatus(f);
+			}
+		}
+
+		@Override
+		public BlockLocation[] getFileBlockLocations(
+			final FileStatus file,
+			final long start,
+			final long len) throws IOException {
+			try (TemporaryClassLoaderContext ignored = new TemporaryClassLoaderContext(loader)) {
+				return inner.getFileBlockLocations(file, start, len);
+			}
+		}
+
+		@Override
+		public FSDataInputStream open(final Path f, final int bufferSize) throws IOException {
+			try (TemporaryClassLoaderContext ignored = new TemporaryClassLoaderContext(loader)) {
+				return inner.open(f, bufferSize);
+			}
+		}
+
+		@Override
+		public FSDataInputStream open(final Path f) throws IOException {
+			try (TemporaryClassLoaderContext ignored = new TemporaryClassLoaderContext(loader)) {
+				return inner.open(f);
+			}
+		}
+
+		@Override
+		public RecoverableWriter createRecoverableWriter() throws IOException {
+			try (TemporaryClassLoaderContext ignored = new TemporaryClassLoaderContext(loader)) {
+				return inner.createRecoverableWriter();
+			}
+		}
+
+		@Override
+		public FileStatus[] listStatus(final Path f) throws IOException {
+			try (TemporaryClassLoaderContext ignored = new TemporaryClassLoaderContext(loader)) {
+				return inner.listStatus(f);
+			}
+		}
+
+		@Override
+		public boolean exists(final Path f) throws IOException {
+			try (TemporaryClassLoaderContext ignored = new TemporaryClassLoaderContext(loader)) {
+				return inner.exists(f);
+			}
+		}
+
+		@Override
+		public boolean delete(final Path f, final boolean recursive) throws IOException {
+			try (TemporaryClassLoaderContext ignored = new TemporaryClassLoaderContext(loader)) {
+				return inner.delete(f, recursive);
+			}
+		}
+
+		@Override
+		public boolean mkdirs(final Path f) throws IOException {
+			try (TemporaryClassLoaderContext ignored = new TemporaryClassLoaderContext(loader)) {
+				return inner.mkdirs(f);
+			}
+		}
+
+		@Override
+		public FSDataOutputStream create(final Path f, final WriteMode overwriteMode) throws IOException {
+			try (TemporaryClassLoaderContext ignored = new TemporaryClassLoaderContext(loader)) {
+				return inner.create(f, overwriteMode);
+			}
+		}
+
+		@Override
+		public boolean isDistributedFS() {
+			try (TemporaryClassLoaderContext ignored = new TemporaryClassLoaderContext(loader)) {
+				return inner.isDistributedFS();
+			}
+		}
+
+		@Override
+		public FileSystemKind getKind() {
+			try (TemporaryClassLoaderContext ignored = new TemporaryClassLoaderContext(loader)) {
+				return inner.getKind();
+			}
+		}
+
+		@Override
+		public boolean rename(final Path src, final Path dst) throws IOException {
+			try (TemporaryClassLoaderContext ignored = new TemporaryClassLoaderContext(loader)) {
+				return inner.rename(src, dst);
+			}
+		}
+	}
+}
diff --git a/flink-end-to-end-tests/run-pre-commit-tests.sh b/flink-end-to-end-tests/run-pre-commit-tests.sh
index a34001fa606..56371a45e26 100755
--- a/flink-end-to-end-tests/run-pre-commit-tests.sh
+++ b/flink-end-to-end-tests/run-pre-commit-tests.sh
@@ -52,7 +52,9 @@ run_test "State Migration end-to-end test from 1.6" "$END_TO_END_DIR/test-script
 run_test "State Evolution end-to-end test" "$END_TO_END_DIR/test-scripts/test_state_evolution.sh"
 run_test "Wordcount end-to-end test" "$END_TO_END_DIR/test-scripts/test_batch_wordcount.sh file"
 run_test "Shaded Hadoop S3A end-to-end test" "$END_TO_END_DIR/test-scripts/test_batch_wordcount.sh hadoop"
+run_test "Shaded Hadoop S3A end-to-end test" "$END_TO_END_DIR/test-scripts/test_batch_wordcount.sh hadoop_minio"
 run_test "Shaded Presto S3 end-to-end test" "$END_TO_END_DIR/test-scripts/test_batch_wordcount.sh presto"
+run_test "Shaded Presto S3 end-to-end test" "$END_TO_END_DIR/test-scripts/test_batch_wordcount.sh presto_minio"
 run_test "Custom FS plugin end-to-end test" "$END_TO_END_DIR/test-scripts/test_batch_wordcount.sh dummy-fs"
 
 run_test "Kinesis end-to-end test" "$END_TO_END_DIR/test-scripts/test_streaming_kinesis.sh"
diff --git a/flink-end-to-end-tests/test-scripts/common_s3.sh b/flink-end-to-end-tests/test-scripts/common_s3.sh
index 564afbb71b6..b7d107c5694 100644
--- a/flink-end-to-end-tests/test-scripts/common_s3.sh
+++ b/flink-end-to-end-tests/test-scripts/common_s3.sh
@@ -77,4 +77,4 @@ function s3_setup_with_provider {
   set_config_key "$2" "com.amazonaws.auth.EnvironmentVariableCredentialsProvider"
 }
 
-source "$(dirname "$0")"/common_s3_operations.sh
\ No newline at end of file
+source "$(dirname "$0")"/common_s3_operations.sh
diff --git a/flink-end-to-end-tests/test-scripts/test_batch_wordcount.sh b/flink-end-to-end-tests/test-scripts/test_batch_wordcount.sh
index 9f57a433afe..e0cedf47a73 100755
--- a/flink-end-to-end-tests/test-scripts/test_batch_wordcount.sh
+++ b/flink-end-to-end-tests/test-scripts/test_batch_wordcount.sh
@@ -21,29 +21,55 @@ source "$(dirname "$0")"/common.sh
 
 INPUT_TYPE=${1:-file}
 RESULT_HASH="72a690412be8928ba239c2da967328a5"
+S3_PREFIX=temp/test_batch_wordcount-$(uuidgen)
+OUTPUT_PATH="${TEST_DATA_DIR}/out/wc_out"
+
+fetch_complete_result=()
+
 case $INPUT_TYPE in
     (file)
-        INPUT_ARGS="--input ${TEST_INFRA_DIR}/test-data/words"
+        ARGS="--input ${TEST_INFRA_DIR}/test-data/words --output ${OUTPUT_PATH}"
     ;;
     (hadoop)
         source "$(dirname "$0")"/common_s3.sh
         s3_setup hadoop
-        INPUT_ARGS="--input ${S3_TEST_DATA_WORDS_URI}"
+        ARGS="--input ${S3_TEST_DATA_WORDS_URI} --output s3://$IT_CASE_S3_BUCKET/$S3_PREFIX"
+        OUTPUT_PATH="$TEST_DATA_DIR/$S3_PREFIX"
+        on_exit "s3_delete_by_full_path_prefix '$S3_PREFIX'"
+        fetch_complete_result=(s3_get_by_full_path_and_filename_prefix "$OUTPUT_PATH" "${S3_PREFIX}" "" false)
+    ;;
+    (hadoop_minio)
+        source "$(dirname "$0")"/common_s3_minio.sh
+        s3_setup hadoop
+        ARGS="--input ${S3_TEST_DATA_WORDS_URI} --output s3://$IT_CASE_S3_BUCKET/$S3_PREFIX"
+        OUTPUT_PATH="$TEST_INFRA_DIR/$IT_CASE_S3_BUCKET/$S3_PREFIX"
     ;;
     (hadoop_with_provider)
         source "$(dirname "$0")"/common_s3.sh
         s3_setup_with_provider hadoop "fs.s3a.aws.credentials.provider"
-        INPUT_ARGS="--input ${S3_TEST_DATA_WORDS_URI}"
+        ARGS="--input ${S3_TEST_DATA_WORDS_URI} --output s3://$IT_CASE_S3_BUCKET/$S3_PREFIX"
+        OUTPUT_PATH="$TEST_DATA_DIR/$S3_PREFIX"
+        on_exit "s3_delete_by_full_path_prefix '$S3_PREFIX'"
+        fetch_complete_result=(s3_get_by_full_path_and_filename_prefix "$OUTPUT_PATH" "${S3_PREFIX}" "" false)
     ;;
     (presto)
         source "$(dirname "$0")"/common_s3.sh
         s3_setup presto
-        INPUT_ARGS="--input ${S3_TEST_DATA_WORDS_URI}"
+        ARGS="--input ${S3_TEST_DATA_WORDS_URI} --output s3://$IT_CASE_S3_BUCKET/$S3_PREFIX"
+        OUTPUT_PATH="$TEST_DATA_DIR/$S3_PREFIX"
+        on_exit "s3_delete_by_full_path_prefix '$S3_PREFIX'"
+        fetch_complete_result=(s3_get_by_full_path_and_filename_prefix "$OUTPUT_PATH" "${S3_PREFIX}" "" false)
+    ;;
+    (presto_minio)
+        source "$(dirname "$0")"/common_s3_minio.sh
+        s3_setup presto
+        ARGS="--input ${S3_TEST_DATA_WORDS_URI} --output s3://$IT_CASE_S3_BUCKET/$S3_PREFIX"
+        OUTPUT_PATH="$TEST_INFRA_DIR/$IT_CASE_S3_BUCKET/$S3_PREFIX"
     ;;
     (dummy-fs)
         source "$(dirname "$0")"/common_dummy_fs.sh
         dummy_fs_setup
-        INPUT_ARGS="--input dummy://localhost/words --input anotherDummy://localhost/words"
+        ARGS="--input dummy://localhost/words --input anotherDummy://localhost/words --output ${OUTPUT_PATH}"
         RESULT_HASH="0e5bd0a3dd7d5a7110aa85ff70adb54b"
     ;;
     (*)
@@ -52,13 +78,12 @@ case $INPUT_TYPE in
     ;;
 esac
 
-OUTPUT_LOCATION="${TEST_DATA_DIR}/out/wc_out"
-
-mkdir -p "${TEST_DATA_DIR}"
-
+mkdir -p "$(dirname $OUTPUT_PATH)"
 start_cluster
 
 # The test may run against different source types.
 # But the sources should provide the same test data, so the checksum stays the same for all tests.
-eval "${FLINK_DIR}/bin/flink run -p 1 ${FLINK_DIR}/examples/batch/WordCount.jar ${INPUT_ARGS} --output ${OUTPUT_LOCATION}"
-check_result_hash "WordCount (${INPUT_TYPE})" "${OUTPUT_LOCATION}" "${RESULT_HASH}"
+${FLINK_DIR}/bin/flink run -p 1 ${FLINK_DIR}/examples/batch/WordCount.jar ${ARGS}
+# Fetches result from AWS s3 to the OUTPUT_PATH, no-op for other filesystems and minio-based tests
+${fetch_complete_result[@]}
+check_result_hash "WordCount (${INPUT_TYPE})" "${OUTPUT_PATH}" "${RESULT_HASH}"
diff --git a/flink-end-to-end-tests/test-scripts/test_streaming_file_sink.sh b/flink-end-to-end-tests/test-scripts/test_streaming_file_sink.sh
index aa711f96fe2..b341b66c8a3 100755
--- a/flink-end-to-end-tests/test-scripts/test_streaming_file_sink.sh
+++ b/flink-end-to-end-tests/test-scripts/test_streaming_file_sink.sh
@@ -19,6 +19,9 @@
 
 OUT_TYPE="${1:-local}" # other type: s3
 
+S3_PREFIX=temp/test_streaming_file_sink-$(uuidgen)
+OUTPUT_PATH="$TEST_DATA_DIR/$S3_PREFIX"
+S3_OUTPUT_PATH="s3://$IT_CASE_S3_BUCKET/$S3_PREFIX"
 source "$(dirname "$0")"/common.sh
 source "$(dirname "$0")"/common_s3.sh
 
@@ -32,10 +35,6 @@ set_config_key "metrics.fetcher.update-interval" "2000"
 # this test relies on global failovers
 set_config_key "jobmanager.execution.failover-strategy" "full"
 
-OUT=temp/test_streaming_file_sink-$(uuidgen)
-OUTPUT_PATH="$TEST_DATA_DIR/$OUT"
-S3_OUTPUT_PATH="s3://$IT_CASE_S3_BUCKET/$OUT"
-
 mkdir -p $OUTPUT_PATH
 
 if [ "${OUT_TYPE}" == "local" ]; then
@@ -44,6 +43,8 @@ if [ "${OUT_TYPE}" == "local" ]; then
 elif [ "${OUT_TYPE}" == "s3" ]; then
   echo "Use s3 output"
   JOB_OUTPUT_PATH=${S3_OUTPUT_PATH}
+  set_config_key "state.checkpoints.dir" "s3://$IT_CASE_S3_BUCKET/$S3_PREFIX-chk"
+  mkdir -p "$OUTPUT_PATH-chk"
 else
   echo "Unknown output type: ${OUT_TYPE}"
   exit 1
@@ -51,7 +52,8 @@ fi
 
 # make sure we delete the file at the end
 function out_cleanup {
-  s3_delete_by_full_path_prefix $OUT
+  s3_delete_by_full_path_prefix "$S3_PREFIX"
+  s3_delete_by_full_path_prefix "${S3_PREFIX}-chk"
   rollback_openssl_lib
 }
 if [ "${OUT_TYPE}" == "s3" ]; then
@@ -72,8 +74,7 @@ TEST_PROGRAM_JAR="${END_TO_END_DIR}/flink-streaming-file-sink-test/target/Stream
 ###################################
 function get_complete_result {
   if [ "${OUT_TYPE}" == "s3" ]; then
-    rm -rf $OUTPUT_PATH; mkdir -p $OUTPUT_PATH
-    s3_get_by_full_path_and_filename_prefix ${TEST_DATA_DIR} "${OUT}" "part-"
+    s3_get_by_full_path_and_filename_prefix "$OUTPUT_PATH" "$S3_PREFIX" "part-" true
   fi
   find "${OUTPUT_PATH}" -type f \( -iname "part-*" \) -exec cat {} + | sort -g
 }
@@ -82,7 +83,7 @@ function get_complete_result {
 # Get total number of lines in part files.
 #
 # Globals:
-#   OUT
+#   S3_PREFIX
 # Arguments:
 #   None
 # Returns:
@@ -92,7 +93,7 @@ function get_total_number_of_valid_lines {
   if [ "${OUT_TYPE}" == "local" ]; then
     get_complete_result | wc -l | tr -d '[:space:]'
   elif [ "${OUT_TYPE}" == "s3" ]; then
-    s3_get_number_of_lines_by_prefix "${OUT}" "part-"
+    s3_get_number_of_lines_by_prefix "${S3_PREFIX}" "part-"
   fi
 }
 
