diff --git a/flink-formats/flink-orc/src/main/java/org/apache/flink/orc/OrcSplitReader.java b/flink-formats/flink-orc/src/main/java/org/apache/flink/orc/OrcSplitReader.java
index 4275a20dae3..85e3bb8c467 100644
--- a/flink-formats/flink-orc/src/main/java/org/apache/flink/orc/OrcSplitReader.java
+++ b/flink-formats/flink-orc/src/main/java/org/apache/flink/orc/OrcSplitReader.java
@@ -128,12 +128,12 @@ public abstract class OrcSplitReader<T, BATCH> implements Closeable {
 	private boolean ensureBatch() throws IOException {
 
 		if (nextRow >= rowsInBatch) {
-			// No more rows available in the Rows array.
-			nextRow = 0;
 			// Try to read the next batch if rows from the ORC file.
 			boolean moreRows = shim.nextBatch(orcRowsReader, rowBatchWrapper.getBatch());
 
 			if (moreRows) {
+				// No more rows available in the Rows array.
+				nextRow = 0;
 				// Load the data into the Rows array.
 				rowsInBatch = fillRows();
 			}
diff --git a/flink-formats/flink-orc/src/test/java/org/apache/flink/orc/OrcColumnarRowSplitReaderTest.java b/flink-formats/flink-orc/src/test/java/org/apache/flink/orc/OrcColumnarRowSplitReaderTest.java
index cc70325602c..cdeace6d732 100644
--- a/flink-formats/flink-orc/src/test/java/org/apache/flink/orc/OrcColumnarRowSplitReaderTest.java
+++ b/flink-formats/flink-orc/src/test/java/org/apache/flink/orc/OrcColumnarRowSplitReaderTest.java
@@ -58,6 +58,7 @@ import static org.apache.flink.table.runtime.functions.SqlDateTimeUtils.internal
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertNotNull;
+import static org.junit.Assert.assertTrue;
 
 /**
  * Test for {@link OrcColumnarRowSplitReader}.
@@ -388,6 +389,21 @@ public class OrcColumnarRowSplitReaderTest {
 		assertEquals(rowSize, cnt);
 	}
 
+	@Test
+	public void testReachEnd() throws Exception {
+		FileInputSplit[] splits = createSplits(testFileFlat, 1);
+		try (OrcColumnarRowSplitReader reader = createReader(
+				new int[]{0, 1},
+				testSchemaFlat,
+				new HashMap<>(),
+				splits[0])) {
+			while (!reader.reachedEnd()) {
+				reader.nextRecord(null);
+			}
+			assertTrue(reader.reachedEnd());
+		}
+	}
+
 	protected static Timestamp toTimestamp(int i) {
 		return new Timestamp(
 						i + 1000,
diff --git a/flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/vector/ParquetColumnarRowSplitReader.java b/flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/vector/ParquetColumnarRowSplitReader.java
index b13993c22a1..b39d953e278 100644
--- a/flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/vector/ParquetColumnarRowSplitReader.java
+++ b/flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/vector/ParquetColumnarRowSplitReader.java
@@ -266,10 +266,13 @@ public class ParquetColumnarRowSplitReader implements Closeable {
 	 */
 	private boolean ensureBatch() throws IOException {
 		if (nextRow >= rowsInBatch) {
-			// No more rows available in the Rows array.
-			nextRow = 0;
 			// Try to read the next batch if rows from the file.
-			return nextBatch();
+			if (nextBatch()) {
+				// No more rows available in the Rows array.
+				nextRow = 0;
+				return true;
+			}
+			return false;
 		}
 		// there is at least one Row left in the Rows array.
 		return true;
diff --git a/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/vector/ParquetColumnarRowSplitReaderTest.java b/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/vector/ParquetColumnarRowSplitReaderTest.java
index bea997ed0e4..4590fce2418 100644
--- a/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/vector/ParquetColumnarRowSplitReaderTest.java
+++ b/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/vector/ParquetColumnarRowSplitReaderTest.java
@@ -161,6 +161,29 @@ public class ParquetColumnarRowSplitReaderTest {
 		testNormalTypes(number, records, values);
 	}
 
+	@Test
+	public void testReachEnd() throws Exception {
+		// prepare parquet file
+		int number = 5;
+		List<Row> records = new ArrayList<>(number);
+		Random random = new Random();
+		for (int i = 0; i < number; i++) {
+			Integer v = random.nextInt(number / 2);
+			if (v % 10 == 0) {
+				records.add(new Row(FIELD_NUMBER));
+			} else {
+				records.add(newRow(v));
+			}
+		}
+		Path testPath = createTempParquetFile(
+				TEMPORARY_FOLDER.newFolder(), PARQUET_SCHEMA, records, rowGroupSize);
+		ParquetColumnarRowSplitReader reader = createReader(testPath, 0, testPath.getFileSystem().getFileStatus(testPath).getLen());
+		while (!reader.reachedEnd()) {
+			reader.nextRecord();
+		}
+		assertTrue(reader.reachedEnd());
+	}
+
 	private void testNormalTypes(int number, List<Row> records,
 			List<Integer> values) throws IOException {
 		Path testPath = createTempParquetFile(
@@ -179,13 +202,10 @@ public class ParquetColumnarRowSplitReaderTest {
 				readSplitAndCheck(number / 2, number / 2, testPath, 0, fileLen, values));
 	}
 
-	private int readSplitAndCheck(
-			int start,
-			long seekToRow,
+	private ParquetColumnarRowSplitReader createReader(
 			Path testPath,
 			long splitStart,
-			long splitLength,
-			List<Integer> values) throws IOException {
+			long splitLength) throws IOException {
 		LogicalType[] fieldTypes = new LogicalType[]{
 				new VarCharType(VarCharType.MAX_LENGTH),
 				new BooleanType(),
@@ -203,12 +223,12 @@ public class ParquetColumnarRowSplitReaderTest {
 				new DecimalType(15, 0),
 				new DecimalType(20, 0)};
 
-		ParquetColumnarRowSplitReader reader = new ParquetColumnarRowSplitReader(
+		return new ParquetColumnarRowSplitReader(
 				false,
 				true,
 				new Configuration(),
 				fieldTypes,
-				new String[] {
+				new String[]{
 						"f0", "f1", "f2", "f3", "f4", "f5", "f6", "f7",
 						"f8", "f9", "f10", "f11", "f12", "f13", "f14"},
 				VectorizedColumnBatch::new,
@@ -216,6 +236,16 @@ public class ParquetColumnarRowSplitReaderTest {
 				new org.apache.hadoop.fs.Path(testPath.getPath()),
 				splitStart,
 				splitLength);
+	}
+
+	private int readSplitAndCheck(
+			int start,
+			long seekToRow,
+			Path testPath,
+			long splitStart,
+			long splitLength,
+			List<Integer> values) throws IOException {
+		ParquetColumnarRowSplitReader reader = createReader(testPath, splitStart, splitLength);
 		reader.seekToRow(seekToRow);
 
 		int i = start;
