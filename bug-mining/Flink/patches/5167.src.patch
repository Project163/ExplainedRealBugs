diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/calcite/FlinkLogicalRelFactories.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/calcite/FlinkLogicalRelFactories.scala
index 72436722e85..f99f4c061df 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/calcite/FlinkLogicalRelFactories.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/calcite/FlinkLogicalRelFactories.scala
@@ -234,10 +234,9 @@ object FlinkLogicalRelFactories {
   class ExpandFactoryImpl extends ExpandFactory {
     def createExpand(
         input: RelNode,
-        rowType: RelDataType,
         projects: util.List[util.List[RexNode]],
         expandIdIndex: Int): RelNode = {
-      FlinkLogicalExpand.create(input, rowType, projects, expandIdIndex)
+      FlinkLogicalExpand.create(input, projects, expandIdIndex)
     }
   }
 
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/calcite/FlinkRelBuilder.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/calcite/FlinkRelBuilder.scala
index 9697252b891..c96039009de 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/calcite/FlinkRelBuilder.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/calcite/FlinkRelBuilder.scala
@@ -30,7 +30,7 @@ import org.apache.flink.table.runtime.operators.rank.{RankRange, RankType}
 import com.google.common.collect.ImmutableList
 import org.apache.calcite.plan._
 import org.apache.calcite.rel.RelCollation
-import org.apache.calcite.rel.`type`.{RelDataType, RelDataTypeField}
+import org.apache.calcite.rel.`type`.RelDataTypeField
 import org.apache.calcite.rel.logical.LogicalAggregate
 import org.apache.calcite.rex.RexNode
 import org.apache.calcite.sql.SqlKind
@@ -84,11 +84,10 @@ class FlinkRelBuilder(
   }
 
   def expand(
-      outputRowType: RelDataType,
       projects: util.List[util.List[RexNode]],
       expandIdIndex: Int): RelBuilder = {
     val input = build()
-    val expand = expandFactory.createExpand(input, outputRowType, projects, expandIdIndex)
+    val expand = expandFactory.createExpand(input, projects, expandIdIndex)
     push(expand)
   }
 
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/calcite/FlinkRelFactories.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/calcite/FlinkRelFactories.scala
index 62a1cd6bf21..ea4319136e3 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/calcite/FlinkRelFactories.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/calcite/FlinkRelFactories.scala
@@ -22,7 +22,7 @@ import org.apache.flink.table.planner.plan.nodes.calcite.{LogicalExpand, Logical
 import org.apache.flink.table.runtime.operators.rank.{RankRange, RankType}
 
 import org.apache.calcite.plan.Contexts
-import org.apache.calcite.rel.`type`.{RelDataType, RelDataTypeField}
+import org.apache.calcite.rel.`type`.RelDataTypeField
 import org.apache.calcite.rel.core.RelFactories
 import org.apache.calcite.rel.{RelCollation, RelNode}
 import org.apache.calcite.rex.RexNode
@@ -62,7 +62,6 @@ object FlinkRelFactories {
   trait ExpandFactory {
     def createExpand(
         input: RelNode,
-        rowType: RelDataType,
         projects: util.List[util.List[RexNode]],
         expandIdIndex: Int): RelNode
   }
@@ -73,9 +72,10 @@ object FlinkRelFactories {
   class ExpandFactoryImpl extends ExpandFactory {
     def createExpand(
         input: RelNode,
-        rowType: RelDataType,
         projects: util.List[util.List[RexNode]],
-        expandIdIndex: Int): RelNode = LogicalExpand.create(input, rowType, projects, expandIdIndex)
+        expandIdIndex: Int): RelNode = {
+      LogicalExpand.create(input, projects, expandIdIndex)
+    }
   }
 
   /**
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/calcite/Expand.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/calcite/Expand.scala
index d8f0a89a05c..2ad4f606d04 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/calcite/Expand.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/calcite/Expand.scala
@@ -18,17 +18,21 @@
 
 package org.apache.flink.table.planner.plan.nodes.calcite
 
+import org.apache.flink.table.api.DataTypes.NULL
+import org.apache.flink.table.api.TableException
+import org.apache.flink.table.planner.plan.utils.{ExpandUtil, RelExplainUtil}
+
 import org.apache.calcite.plan.{RelOptCluster, RelOptCost, RelOptPlanner, RelTraitSet}
 import org.apache.calcite.rel.`type`.RelDataType
 import org.apache.calcite.rel.metadata.RelMetadataQuery
 import org.apache.calcite.rel.{RelNode, RelWriter, SingleRel}
-import org.apache.calcite.rex.{RexLiteral, RexNode}
+import org.apache.calcite.rex.{RexInputRef, RexLiteral, RexNode}
 import org.apache.calcite.util.Litmus
-import org.apache.flink.table.planner.plan.utils.RelExplainUtil
 
 import java.util
 
 import scala.collection.JavaConversions._
+import scala.collection.mutable
 
 /**
   * Relational expression that apply a number of projects to every input row,
@@ -39,7 +43,6 @@ import scala.collection.JavaConversions._
   * @param cluster       cluster that this relational expression belongs to
   * @param traits        the traits of this rel
   * @param input         input relational expression
-  * @param outputRowType output row type
   * @param projects      all projects, each project contains list of expressions for
   *                      the output columns
   * @param expandIdIndex expand_id('$e') field index
@@ -48,7 +51,6 @@ abstract class Expand(
     cluster: RelOptCluster,
     traits: RelTraitSet,
     input: RelNode,
-    outputRowType: RelDataType,
     val projects: util.List[util.List[RexNode]],
     val expandIdIndex: Int)
   extends SingleRel(cluster, traits, input) {
@@ -59,10 +61,20 @@ abstract class Expand(
     if (projects.size() <= 1) {
       return litmus.fail("Expand should output more than one rows, otherwise use Project.")
     }
-    if (projects.exists(_.size != outputRowType.getFieldCount)) {
-      return litmus.fail("project filed count is not equal to output field count.")
+    val fieldLen = projects.get(0).size()
+    if (projects.exists(_.size != fieldLen)) {
+      return litmus.fail("all projects' field count should be equal.")
+    }
+
+    // do type check and derived row type info will be cached by framework
+    try {
+      deriveRowType()
+    } catch {
+      case exp: TableException =>
+        return litmus.fail(exp.getMessage)
     }
-    if (expandIdIndex < 0 || expandIdIndex >= outputRowType.getFieldCount) {
+
+    if (expandIdIndex < 0 || expandIdIndex >= fieldLen) {
       return litmus.fail(
         "expand_id field index should be greater than 0 and less than output field count.")
     }
@@ -79,7 +91,57 @@ abstract class Expand(
     litmus.succeed()
   }
 
-  override def deriveRowType(): RelDataType = outputRowType
+  override def deriveRowType(): RelDataType = {
+    val inputNames = input.getRowType.getFieldNames
+    val fieldNameSet = mutable.Set[String](inputNames: _*)
+    val rowTypes = mutable.ListBuffer[RelDataType]()
+    val outputNames = mutable.ListBuffer[String]()
+    val fieldLen = projects.get(0).size()
+    val inputNameRefCnt = mutable.Map[String, Int]()
+
+    for (fieldIndex <- 0 until fieldLen) {
+      val fieldTypes = mutable.ListBuffer[RelDataType]()
+      val fieldNames = mutable.ListBuffer[String]()
+      for (projectIndex <- 0 until projects.size()) {
+        val rexNode = projects.get(projectIndex).get(fieldIndex)
+        fieldTypes += rexNode.getType
+        rexNode match {
+          case ref: RexInputRef =>
+            fieldNames += inputNames.get(ref.getIndex)
+          case _: RexLiteral => // ignore
+          case exp@_ =>
+            throw new TableException(
+              "Expand node only support RexInputRef and RexLiteral, but got " + exp)
+        }
+      }
+      if (!fieldNames.isEmpty) {
+        val inputName = fieldNames(0)
+        val refCnt = inputNameRefCnt.getOrElse(inputName, 0) + 1
+        inputNameRefCnt.put(inputName, refCnt)
+        outputNames += ExpandUtil.buildDuplicateFieldName(
+          fieldNameSet,
+          inputName,
+          inputNameRefCnt.get(inputName).get)
+      } else if (fieldIndex == expandIdIndex) {
+        outputNames += ExpandUtil.buildUniqueFieldName(fieldNameSet, "$e")
+      } else {
+        outputNames += ExpandUtil.buildUniqueFieldName(fieldNameSet, "$f" + fieldIndex)
+      }
+
+      val leastRestrictive = input.getCluster.getTypeFactory.leastRestrictive(fieldTypes)
+      // if leastRestrictive type is null or type name is NULL means we can not support given
+      // projects with different column types (NULL type name is reserved for untyped literals only)
+      if (leastRestrictive == null || leastRestrictive.getSqlTypeName == NULL) {
+        throw new TableException(
+          "Expand node only support projects that have common types, but got a column with " +
+            "different types which can not derive a least restrictive common type: column index[" +
+            fieldIndex + "], column types[" + fieldTypes.mkString(",") + "]")
+      } else {
+        rowTypes += leastRestrictive
+      }
+    }
+    cluster.getTypeFactory.createStructType(rowTypes, outputNames)
+  }
 
   override def explainTerms(pw: RelWriter): RelWriter = {
     super.explainTerms(pw)
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/calcite/LogicalExpand.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/calcite/LogicalExpand.scala
index f9963b9140c..4995acb8322 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/calcite/LogicalExpand.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/calcite/LogicalExpand.scala
@@ -20,7 +20,6 @@ package org.apache.flink.table.planner.plan.nodes.calcite
 
 import org.apache.calcite.plan.{Convention, RelOptCluster, RelTraitSet}
 import org.apache.calcite.rel.RelNode
-import org.apache.calcite.rel.`type`.RelDataType
 import org.apache.calcite.rex.RexNode
 
 import java.util
@@ -34,13 +33,12 @@ final class LogicalExpand(
     cluster: RelOptCluster,
     traits: RelTraitSet,
     input: RelNode,
-    outputRowType: RelDataType,
     projects: util.List[util.List[RexNode]],
     expandIdIndex: Int)
-  extends Expand(cluster, traits, input, outputRowType, projects, expandIdIndex) {
+  extends Expand(cluster, traits, input, projects, expandIdIndex) {
 
   override def copy(traitSet: RelTraitSet, inputs: util.List[RelNode]): RelNode = {
-    new LogicalExpand(cluster, traitSet, inputs.get(0), outputRowType, projects, expandIdIndex)
+    new LogicalExpand(cluster, traitSet, inputs.get(0), projects, expandIdIndex)
   }
 
 }
@@ -48,11 +46,10 @@ final class LogicalExpand(
 object LogicalExpand {
   def create(
       input: RelNode,
-      outputRowType: RelDataType,
       projects: util.List[util.List[RexNode]],
       expandIdIndex: Int): LogicalExpand = {
     val traits = input.getCluster.traitSetOf(Convention.NONE)
-    new LogicalExpand(input.getCluster, traits, input, outputRowType, projects, expandIdIndex)
+    new LogicalExpand(input.getCluster, traits, input, projects, expandIdIndex)
   }
 }
 
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/logical/FlinkLogicalExpand.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/logical/FlinkLogicalExpand.scala
index 60b53f595ce..3ff2c06283e 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/logical/FlinkLogicalExpand.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/logical/FlinkLogicalExpand.scala
@@ -23,7 +23,6 @@ import org.apache.flink.table.planner.plan.nodes.calcite.{Expand, LogicalExpand}
 
 import org.apache.calcite.plan.{Convention, RelOptCluster, RelOptRule, RelTraitSet}
 import org.apache.calcite.rel.RelNode
-import org.apache.calcite.rel.`type`.RelDataType
 import org.apache.calcite.rel.convert.ConverterRule
 import org.apache.calcite.rex.RexNode
 
@@ -37,14 +36,18 @@ class FlinkLogicalExpand(
     cluster: RelOptCluster,
     traits: RelTraitSet,
     input: RelNode,
-    outputRowType: RelDataType,
     projects: util.List[util.List[RexNode]],
     expandIdIndex: Int)
-  extends Expand(cluster, traits, input, outputRowType, projects, expandIdIndex)
+  extends Expand(cluster, traits, input, projects, expandIdIndex)
   with FlinkLogicalRel {
 
   override def copy(traitSet: RelTraitSet, inputs: util.List[RelNode]): RelNode = {
-    new FlinkLogicalExpand(cluster, traitSet, inputs.get(0), outputRowType, projects, expandIdIndex)
+    new FlinkLogicalExpand(
+      cluster,
+      traitSet,
+      inputs.get(0),
+      projects,
+      expandIdIndex)
   }
 
 }
@@ -61,7 +64,6 @@ private class FlinkLogicalExpandConverter
     val newInput = RelOptRule.convert(expand.getInput, FlinkConventions.LOGICAL)
     FlinkLogicalExpand.create(
       newInput,
-      expand.getRowType,
       expand.projects,
       expand.expandIdIndex)
   }
@@ -72,11 +74,10 @@ object FlinkLogicalExpand {
 
   def create(
       input: RelNode,
-      outputRowType: RelDataType,
       projects: util.List[util.List[RexNode]],
       expandIdIndex: Int): FlinkLogicalExpand = {
     val cluster = input.getCluster
     val traitSet = cluster.traitSetOf(FlinkConventions.LOGICAL).simplify()
-    new FlinkLogicalExpand(cluster, traitSet, input, outputRowType, projects, expandIdIndex)
+    new FlinkLogicalExpand(cluster, traitSet, input, projects, expandIdIndex)
   }
 }
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchPhysicalExpand.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchPhysicalExpand.scala
index a530d38303f..237929433b7 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchPhysicalExpand.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchPhysicalExpand.scala
@@ -24,7 +24,6 @@ import org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecExpand
 import org.apache.flink.table.planner.plan.nodes.exec.{InputProperty, ExecNode}
 
 import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}
-import org.apache.calcite.rel.`type`.RelDataType
 import org.apache.calcite.rel.RelNode
 import org.apache.calcite.rex.RexNode
 
@@ -37,10 +36,9 @@ class BatchPhysicalExpand(
     cluster: RelOptCluster,
     traitSet: RelTraitSet,
     input: RelNode,
-    outputRowType: RelDataType,
     projects: util.List[util.List[RexNode]],
     expandIdIndex: Int)
-  extends Expand(cluster, traitSet, input, outputRowType, projects, expandIdIndex)
+  extends Expand(cluster, traitSet, input, projects, expandIdIndex)
   with BatchPhysicalRel {
 
   override def copy(traitSet: RelTraitSet, inputs: util.List[RelNode]): RelNode = {
@@ -48,7 +46,6 @@ class BatchPhysicalExpand(
       cluster,
       traitSet,
       inputs.get(0),
-      outputRowType,
       projects,
       expandIdIndex
     )
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamPhysicalExpand.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamPhysicalExpand.scala
index 8799258c266..9e0c414c439 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamPhysicalExpand.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamPhysicalExpand.scala
@@ -24,7 +24,6 @@ import org.apache.flink.table.planner.plan.nodes.exec.{InputProperty, ExecNode}
 
 import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}
 import org.apache.calcite.rel.RelNode
-import org.apache.calcite.rel.`type`.RelDataType
 import org.apache.calcite.rex.RexNode
 
 import java.util
@@ -36,17 +35,16 @@ class StreamPhysicalExpand(
     cluster: RelOptCluster,
     traitSet: RelTraitSet,
     inputRel: RelNode,
-    outputRowType: RelDataType,
     projects: util.List[util.List[RexNode]],
     expandIdIndex: Int)
-  extends Expand(cluster, traitSet, inputRel, outputRowType, projects, expandIdIndex)
+  extends Expand(cluster, traitSet, inputRel, projects, expandIdIndex)
   with StreamPhysicalRel {
 
   override def requireWatermark: Boolean = false
 
   override def copy(traitSet: RelTraitSet, inputs: util.List[RelNode]): RelNode = {
     new StreamPhysicalExpand(
-      cluster, traitSet, inputs.get(0), outputRowType, projects, expandIdIndex)
+      cluster, traitSet, inputs.get(0), projects, expandIdIndex)
   }
 
   override def translateToExecNode(): ExecNode[_] = {
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/logical/DecomposeGroupingSetsRule.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/logical/DecomposeGroupingSetsRule.scala
index 7f516afb723..ba07bfb6bd6 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/logical/DecomposeGroupingSetsRule.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/logical/DecomposeGroupingSetsRule.scala
@@ -235,7 +235,7 @@ class DecomposeGroupingSetsRule extends RelOptRule(
 
     val (newGroupSet, duplicateFieldMap) = if (needExpand) {
       val (duplicateFieldMap, expandIdIdxInExpand) = ExpandUtil.buildExpandNode(
-        cluster, relBuilder, agg.getAggCallList, agg.getGroupSet, agg.getGroupSets)
+        relBuilder, agg.getAggCallList, agg.getGroupSet, agg.getGroupSets)
 
       // new groupSet contains original groupSet and expand_id('$e') field
       val newGroupSet = agg.getGroupSet.union(ImmutableBitSet.of(expandIdIdxInExpand))
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/logical/SplitAggregateRule.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/logical/SplitAggregateRule.scala
index 8d8807b20d1..be94ba13b89 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/logical/SplitAggregateRule.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/logical/SplitAggregateRule.scala
@@ -230,7 +230,7 @@ class SplitAggregateRule extends RelOptRule(
     val needExpand = newGroupSetsNum > 1
     val duplicateFieldMap = if (needExpand) {
       val (duplicateFieldMap, _) = ExpandUtil.buildExpandNode(
-        cluster, relBuilder, partialAggCalls, fullGroupSet, groupSets)
+        relBuilder, partialAggCalls, fullGroupSet, groupSets)
       duplicateFieldMap
     } else {
       Map.empty[Integer, Integer]
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/batch/BatchPhysicalExpandRule.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/batch/BatchPhysicalExpandRule.scala
index e3e139114e5..eb4df5f4bce 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/batch/BatchPhysicalExpandRule.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/batch/BatchPhysicalExpandRule.scala
@@ -44,7 +44,6 @@ class BatchPhysicalExpandRule
       rel.getCluster,
       newTrait,
       newInput,
-      rel.getRowType,
       expand.projects,
       expand.expandIdIndex)
   }
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/stream/ExpandWindowTableFunctionTransposeRule.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/stream/ExpandWindowTableFunctionTransposeRule.scala
index 5880586a466..a1416f18555 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/stream/ExpandWindowTableFunctionTransposeRule.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/stream/ExpandWindowTableFunctionTransposeRule.scala
@@ -30,11 +30,9 @@ import org.apache.flink.table.planner.plan.utils.WindowUtil.buildNewProgramWitho
 import org.apache.calcite.plan.RelOptRule.{any, operand}
 import org.apache.calcite.plan.{RelOptRule, RelOptRuleCall}
 import org.apache.calcite.rel.RelNode
-import org.apache.calcite.rel.`type`.RelDataType
 import org.apache.calcite.rex.{RexInputRef, RexLiteral, RexNode, RexProgram}
 
 import scala.collection.JavaConverters._
-import scala.collection.mutable
 import scala.collection.mutable.ArrayBuffer
 
 /**
@@ -207,8 +205,6 @@ class ExpandWindowTableFunctionTransposeRule
     val newInputRowType = newCalc.getRowType
     val expandIdIndex = expand.expandIdIndex
     var newExpandIdIndex = -1
-    val newTypeList = mutable.HashMap[Int, RelDataType]()
-    val newFieldNameList = mutable.HashMap[Int, String]()
     val newProjects = expand.projects.asScala.map { exprs =>
       val newExprs = ArrayBuffer[RexNode]()
       var baseOffset = 0
@@ -219,8 +215,6 @@ class ExpandWindowTableFunctionTransposeRule
           val newInputIndex = inputFieldShifting(ref.getIndex)
           newExprs += RexInputRef.of(newInputIndex, newInputRowType)
           // we only use the type from input ref instead of literal
-          newTypeList(baseOffset) = newInputRowType.getFieldList.get(newInputIndex).getType
-          newFieldNameList(baseOffset) = newInputRowType.getFieldNames.get(newInputIndex)
           baseOffset += 1
         case (lit: RexLiteral, exprIndex) =>
           newExprs += lit
@@ -228,8 +222,6 @@ class ExpandWindowTableFunctionTransposeRule
             // this is the expand id, we should remember the new index of expand id
             // and update type for this expr
             newExpandIdIndex = baseOffset
-            newTypeList(baseOffset) = lit.getType
-            newFieldNameList(baseOffset) = expand.getRowType.getFieldNames.get(expandIdIndex)
           }
           baseOffset += 1
         case exp@_ =>
@@ -239,22 +231,14 @@ class ExpandWindowTableFunctionTransposeRule
       if (timeFieldAdded) {
         // append time attribute reference if needed
         newExprs += RexInputRef.of(newTimeField, newInputRowType)
-        newTypeList(baseOffset) = newInputRowType.getFieldList.get(newTimeField).getType
-        newFieldNameList(baseOffset) = newInputRowType.getFieldNames.get(newTimeField)
       }
       newExprs.asJava
     }
 
-    val typeFactory = expand.getCluster.getTypeFactory
-    val newOutputRowType = typeFactory.createStructType(
-      newTypeList.toList.sortBy(_._1).map(_._2).asJava,
-      newFieldNameList.toList.sortBy(_._1).map(_._2).asJava)
-
     new StreamPhysicalExpand(
       expand.getCluster,
       expand.getTraitSet,
       newCalc,
-      newOutputRowType,
       newProjects.asJava,
       newExpandIdIndex
     )
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/stream/StreamPhysicalExpandRule.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/stream/StreamPhysicalExpandRule.scala
index 2969b513389..6b6b90505cc 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/stream/StreamPhysicalExpandRule.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/stream/StreamPhysicalExpandRule.scala
@@ -44,7 +44,6 @@ class StreamPhysicalExpandRule
       rel.getCluster,
       newTrait,
       newInput,
-      rel.getRowType,
       expand.projects,
       expand.expandIdIndex)
   }
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/ExpandUtil.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/ExpandUtil.scala
index 6a76a8c09ab..2509ee40545 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/ExpandUtil.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/ExpandUtil.scala
@@ -19,21 +19,17 @@
 package org.apache.flink.table.planner.plan.utils
 
 import org.apache.flink.table.planner.calcite.FlinkRelBuilder
-import org.apache.flink.table.planner.plan.nodes.calcite.{Expand, LogicalExpand}
 
 import com.google.common.collect.ImmutableList
-import org.apache.calcite.plan.RelOptCluster
-import org.apache.calcite.rel.`type`.{RelDataType, RelDataTypeFactory}
+import org.apache.calcite.rel.`type`.RelDataType
 import org.apache.calcite.rel.core.AggregateCall
 import org.apache.calcite.rex.{RexBuilder, RexNode}
-import org.apache.calcite.sql.`type`.SqlTypeName
 import org.apache.calcite.util.ImmutableBitSet
 
 import java.math.BigDecimal
 import java.util
 
 import scala.collection.JavaConversions._
-import scala.collection.mutable
 
 object ExpandUtil {
 
@@ -43,7 +39,6 @@ object ExpandUtil {
     * and the created Expand node will be at the top of the stack of the RelBuilder.
     */
   def buildExpandNode(
-      cluster: RelOptCluster,
       relBuilder: FlinkRelBuilder,
       aggCalls: Seq[AggregateCall],
       groupSet: ImmutableBitSet,
@@ -78,22 +73,18 @@ object ExpandUtil {
 
     val inputType = relBuilder.peek().getRowType
 
-    // expand output fields: original input fields + expand_id field + duplicate fields
     val expandIdIdxInExpand = inputType.getFieldCount
     val duplicateFieldMap = buildDuplicateFieldMap(inputType, duplicateFieldIndexes)
 
-    val expandRowType = buildExpandRowType(
-      cluster.getTypeFactory, inputType, duplicateFieldIndexes)
+    // expand output fields: original input fields + expand_id field + duplicate fields
     val expandProjects = createExpandProjects(
       relBuilder.getRexBuilder,
       inputType,
-      expandRowType,
       groupSet,
       groupSets,
       duplicateFieldIndexes)
 
-    relBuilder.expand(
-      expandRowType, expandProjects, expandIdIdxInExpand)
+    relBuilder.expand(expandProjects, expandIdIdxInExpand)
 
     (duplicateFieldMap, expandIdIdxInExpand)
   }
@@ -117,52 +108,11 @@ object ExpandUtil {
     }.toMap[Integer, Integer]
   }
 
-
-  /**
-    * Build row type for [[LogicalExpand]].
-    *
-    * the order of fields are:
-    * first, the input fields,
-    * second, expand_id field(to distinguish different expanded rows),
-    * last, optional duplicate fields.
-    *
-    * @param typeFactory Type factory.
-    * @param inputType Input row type.
-    * @param duplicateFieldIndexes Fields indexes that will be output as duplicate.
-    * @return Row type for [[LogicalExpand]].
-    */
-  def buildExpandRowType(
-      typeFactory: RelDataTypeFactory,
-      inputType: RelDataType,
-      duplicateFieldIndexes: Array[Integer]): RelDataType = {
-
-    // 1. add original input fields
-    val typeList = mutable.ListBuffer(inputType.getFieldList.map(_.getType): _*)
-    val fieldNameList = mutable.ListBuffer(inputType.getFieldNames: _*)
-    val allFieldNames = mutable.Set[String](fieldNameList: _*)
-
-    // 2. add expand_id('$e') field
-    typeList += typeFactory.createTypeWithNullability(
-      typeFactory.createSqlType(SqlTypeName.BIGINT), false)
-    var expandIdFieldName = buildUniqueFieldName(allFieldNames, "$e")
-    fieldNameList += expandIdFieldName
-
-    // 3. add duplicate fields
-    duplicateFieldIndexes.foreach {
-      duplicateFieldIdx =>
-        typeList += inputType.getFieldList.get(duplicateFieldIdx).getType
-        fieldNameList += buildUniqueFieldName(
-          allFieldNames, inputType.getFieldNames.get(duplicateFieldIdx))
-    }
-
-    typeFactory.createStructType(typeList, fieldNameList)
-  }
-
   /**
     * Get unique field name based on existed `allFieldNames` collection.
     * NOTES: the new unique field name will be added to existed `allFieldNames` collection.
     */
-  private def buildUniqueFieldName(
+  def buildUniqueFieldName(
       allFieldNames: util.Set[String],
       toAddFieldName: String): String = {
     var name: String = toAddFieldName
@@ -175,13 +125,28 @@ object ExpandUtil {
     name
   }
 
+  /**
+   * Generate a new field name using given source input name and its reference count, the rule is:
+   * keep it is if refCnt <= 1, else use pattern $inputName_${refCnt - 2} as the targetName to call
+   * the buildUniqueFieldName function.
+   */
+  def buildDuplicateFieldName(
+      allFieldNames: util.Set[String],
+      inputName: String,
+      refCnt: Int): String = {
+    if (refCnt <= 1) {
+      inputName
+    } else {
+      buildUniqueFieldName(allFieldNames, s"${inputName}_${refCnt - 2}")
+    }
+  }
+
   /**
     * Create Project list for [[LogicalExpand]].
     * One input row will expand to multiple output rows, so multi projects will be created.
     *
     * @param rexBuilder Rex builder.
     * @param inputType Input row type.
-    * @param outputType Row type of [[LogicalExpand]].
     * @param groupSet The original groupSet of a aggregate before expanded.
     * @param groupSets The original groupSets of a aggregate before expanded.
     * @param duplicateFieldIndexes Fields indexes that will be output as duplicate.
@@ -190,15 +155,12 @@ object ExpandUtil {
   def createExpandProjects(
       rexBuilder: RexBuilder,
       inputType: RelDataType,
-      outputType: RelDataType,
       groupSet: ImmutableBitSet,
       groupSets: ImmutableList[ImmutableBitSet],
       duplicateFieldIndexes: Array[Integer]): util.List[util.List[RexNode]] = {
 
     val fullGroupList = groupSet.toArray
     require(!groupSets.isEmpty && fullGroupList.nonEmpty)
-    val fieldCount = inputType.getFieldCount + 1 + duplicateFieldIndexes.length
-    require(fieldCount == outputType.getFieldCount)
 
     // expand for each groupSet
     val expandProjects = groupSets.map { subGroupSet =>
diff --git a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/agg/DistinctAggregateTest.xml b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/agg/DistinctAggregateTest.xml
index 00d3e18abef..0fa93ea28fa 100644
--- a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/agg/DistinctAggregateTest.xml
+++ b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/agg/DistinctAggregateTest.xml
@@ -61,7 +61,7 @@ Calc(select=[a, CAST(EXPR$1) AS EXPR$1, EXPR$2])
 +- HashAggregate(isMerge=[true], groupBy=[a], select=[a, Final_MIN(min$0) AS EXPR$1, Final_COUNT(count$1) AS EXPR$2])
    +- Exchange(distribution=[hash[a]])
       +- LocalHashAggregate(groupBy=[a], select=[a, Partial_MIN(EXPR$1) FILTER $g_3 AS min$0, Partial_COUNT(d) FILTER $g_0 AS count$1])
-         +- Calc(select=[a, d, EXPR$1, ((CASE(($e = 0:BIGINT), 0:BIGINT, 3:BIGINT) = 0) AND $f2) AS $g_0, (CASE(($e = 0:BIGINT), 0:BIGINT, 3:BIGINT) = 3) AS $g_3])
+         +- Calc(select=[a, d, EXPR$1, ((CASE(($e = 0:BIGINT), 0:BIGINT, 3:BIGINT) = 0) AND $f2 IS TRUE) AS $g_0, (CASE(($e = 0:BIGINT), 0:BIGINT, 3:BIGINT) = 3) AS $g_3])
             +- HashAggregate(isMerge=[true], groupBy=[a, $f2, d, $e], select=[a, $f2, d, $e, Final_COUNT(count$0) AS EXPR$1])
                +- Exchange(distribution=[hash[a, $f2, d, $e]])
                   +- LocalHashAggregate(groupBy=[a, $f2, d, $e], select=[a, $f2, d, $e, Partial_COUNT(c) FILTER $f2_0 AS count$0])
@@ -168,7 +168,7 @@ LogicalAggregate(group=[{}], EXPR$0=[COUNT(DISTINCT $0)], EXPR$1=[SUM(DISTINCT $
 HashAggregate(isMerge=[true], select=[Final_COUNT(count$0) AS EXPR$0, Final_SUM(sum$1) AS EXPR$1, Final_COUNT(count$2) AS EXPR$2])
 +- Exchange(distribution=[single])
    +- LocalHashAggregate(select=[Partial_COUNT(a) FILTER $g_7 AS count$0, Partial_SUM(b) FILTER $g_11 AS sum$1, Partial_COUNT(c) FILTER $g_12 AS count$2])
-      +- Calc(select=[a, b, c, (CASE(($e = 7:BIGINT), 7:BIGINT, ($e = 11:BIGINT), 11:BIGINT, 12:BIGINT) = 7) AS $g_7, (CASE(($e = 7:BIGINT), 7:BIGINT, ($e = 11:BIGINT), 11:BIGINT, 12:BIGINT) = 11) AS $g_11, ((CASE(($e = 7:BIGINT), 7:BIGINT, ($e = 11:BIGINT), 11:BIGINT, 12:BIGINT) = 12) AND $f3) AS $g_12])
+      +- Calc(select=[a, b, c, (CASE(($e = 7:BIGINT), 7:BIGINT, ($e = 11:BIGINT), 11:BIGINT, 12:BIGINT) = 7) AS $g_7, (CASE(($e = 7:BIGINT), 7:BIGINT, ($e = 11:BIGINT), 11:BIGINT, 12:BIGINT) = 11) AS $g_11, ((CASE(($e = 7:BIGINT), 7:BIGINT, ($e = 11:BIGINT), 11:BIGINT, 12:BIGINT) = 12) AND $f3 IS TRUE) AS $g_12])
          +- HashAggregate(isMerge=[true], groupBy=[a, b, c, $f3, $e], select=[a, b, c, $f3, $e])
             +- Exchange(distribution=[hash[a, b, c, $f3, $e]])
                +- LocalHashAggregate(groupBy=[a, b, c, $f3, $e], select=[a, b, c, $f3, $e])
@@ -224,7 +224,7 @@ LogicalAggregate(group=[{}], EXPR$0=[COUNT(DISTINCT $0)], EXPR$1=[SUM(DISTINCT $
 HashAggregate(isMerge=[true], select=[Final_COUNT(count$0) AS EXPR$0, Final_SUM(sum$1) AS EXPR$1, Final_MIN(min$2) AS EXPR$2])
 +- Exchange(distribution=[single])
    +- LocalHashAggregate(select=[Partial_COUNT(a) FILTER $g_1 AS count$0, Partial_SUM(a) FILTER $g_0 AS sum$1, Partial_MIN(EXPR$2) FILTER $g_3 AS min$2])
-      +- Calc(select=[a, EXPR$2, ((CASE(($e = 0:BIGINT), 0:BIGINT, ($e = 1:BIGINT), 1:BIGINT, 3:BIGINT) = 0) AND $f1) AS $g_0, (CASE(($e = 0:BIGINT), 0:BIGINT, ($e = 1:BIGINT), 1:BIGINT, 3:BIGINT) = 1) AS $g_1, (CASE(($e = 0:BIGINT), 0:BIGINT, ($e = 1:BIGINT), 1:BIGINT, 3:BIGINT) = 3) AS $g_3])
+      +- Calc(select=[a, EXPR$2, ((CASE(($e = 0:BIGINT), 0:BIGINT, ($e = 1:BIGINT), 1:BIGINT, 3:BIGINT) = 0) AND $f1 IS TRUE) AS $g_0, (CASE(($e = 0:BIGINT), 0:BIGINT, ($e = 1:BIGINT), 1:BIGINT, 3:BIGINT) = 1) AS $g_1, (CASE(($e = 0:BIGINT), 0:BIGINT, ($e = 1:BIGINT), 1:BIGINT, 3:BIGINT) = 3) AS $g_3])
          +- HashAggregate(isMerge=[true], groupBy=[a, $f1, $e], select=[a, $f1, $e, Final_MAX(max$0) AS EXPR$2])
             +- Exchange(distribution=[hash[a, $f1, $e]])
                +- LocalHashAggregate(groupBy=[a, $f1, $e], select=[a, $f1, $e, Partial_MAX(a_0) AS max$0])
@@ -279,7 +279,7 @@ Calc(select=[EXPR$0, EXPR$1, CASE(EXPR$2 IS NOT NULL, EXPR$2, 0) AS EXPR$2])
 +- HashAggregate(isMerge=[true], select=[Final_COUNT(count$0) AS EXPR$0, Final_SUM(sum$1) AS EXPR$1, Final_MIN(min$2) AS EXPR$2])
    +- Exchange(distribution=[single])
       +- LocalHashAggregate(select=[Partial_COUNT(a) FILTER $g_1 AS count$0, Partial_SUM(b) FILTER $g_6 AS sum$1, Partial_MIN(EXPR$2) FILTER $g_7 AS min$2])
-         +- Calc(select=[a, b, EXPR$2, ((CASE(($e = 1:BIGINT), 1:BIGINT, ($e = 6:BIGINT), 6:BIGINT, 7:BIGINT) = 1) AND $f1) AS $g_1, (CASE(($e = 1:BIGINT), 1:BIGINT, ($e = 6:BIGINT), 6:BIGINT, 7:BIGINT) = 6) AS $g_6, (CASE(($e = 1:BIGINT), 1:BIGINT, ($e = 6:BIGINT), 6:BIGINT, 7:BIGINT) = 7) AS $g_7])
+         +- Calc(select=[a, b, EXPR$2, ((CASE(($e = 1:BIGINT), 1:BIGINT, ($e = 6:BIGINT), 6:BIGINT, 7:BIGINT) = 1) AND $f1 IS TRUE) AS $g_1, (CASE(($e = 1:BIGINT), 1:BIGINT, ($e = 6:BIGINT), 6:BIGINT, 7:BIGINT) = 6) AS $g_6, (CASE(($e = 1:BIGINT), 1:BIGINT, ($e = 6:BIGINT), 6:BIGINT, 7:BIGINT) = 7) AS $g_7])
             +- HashAggregate(isMerge=[true], groupBy=[a, $f1, b, $e], select=[a, $f1, b, $e, Final_COUNT(count$0) AS EXPR$2])
                +- Exchange(distribution=[hash[a, $f1, b, $e]])
                   +- LocalHashAggregate(groupBy=[a, $f1, b, $e], select=[a, $f1, b, $e, Partial_COUNT(c) AS count$0])
@@ -336,7 +336,7 @@ SortAggregate(isMerge=[true], groupBy=[d], select=[d, Final_MIN(min$0) AS EXPR$1
 +- Sort(orderBy=[d ASC])
    +- Exchange(distribution=[hash[d]])
       +- LocalSortAggregate(groupBy=[d], select=[d, Partial_MIN(EXPR$1) FILTER $g_15 AS min$0, Partial_MIN(EXPR$2) FILTER $g_15 AS min$1, Partial_COUNT(c) FILTER $g_7 AS count$2, Partial_COUNT(c) FILTER $g_3 AS count$3, Partial_COUNT(b) FILTER $g_12 AS count$4])
-         +- Calc(select=[d, c, b, EXPR$1, EXPR$2, ((CASE(($e = 3:BIGINT), 3:BIGINT, ($e = 7:BIGINT), 7:BIGINT, ($e = 12:BIGINT), 12:BIGINT, 15:BIGINT) = 3) AND $f4) AS $g_3, (CASE(($e = 3:BIGINT), 3:BIGINT, ($e = 7:BIGINT), 7:BIGINT, ($e = 12:BIGINT), 12:BIGINT, 15:BIGINT) = 7) AS $g_7, ((CASE(($e = 3:BIGINT), 3:BIGINT, ($e = 7:BIGINT), 7:BIGINT, ($e = 12:BIGINT), 12:BIGINT, 15:BIGINT) = 12) AND $f6) AS $g_12, (CASE(($e = 3:BIGINT), 3:BIGINT, ($e = 7:BIGINT), 7:BIGINT, ($e = 12:BIGINT), 12:BIGINT, 15:BIGINT) = 15) AS $g_15])
+         +- Calc(select=[d, c, b, EXPR$1, EXPR$2, ((CASE(($e = 3:BIGINT), 3:BIGINT, ($e = 7:BIGINT), 7:BIGINT, ($e = 12:BIGINT), 12:BIGINT, 15:BIGINT) = 3) AND $f4 IS TRUE) AS $g_3, (CASE(($e = 3:BIGINT), 3:BIGINT, ($e = 7:BIGINT), 7:BIGINT, ($e = 12:BIGINT), 12:BIGINT, 15:BIGINT) = 7) AS $g_7, ((CASE(($e = 3:BIGINT), 3:BIGINT, ($e = 7:BIGINT), 7:BIGINT, ($e = 12:BIGINT), 12:BIGINT, 15:BIGINT) = 12) AND $f6 IS TRUE) AS $g_12, (CASE(($e = 3:BIGINT), 3:BIGINT, ($e = 7:BIGINT), 7:BIGINT, ($e = 12:BIGINT), 12:BIGINT, 15:BIGINT) = 15) AS $g_15])
             +- Sort(orderBy=[d ASC])
                +- SortAggregate(isMerge=[false], groupBy=[d, c, $f4, b, $f6, $e], select=[d, c, $f4, b, $f6, $e, MAX(e) AS EXPR$1, MAX(e) FILTER $f2 AS EXPR$2])
                   +- Sort(orderBy=[d ASC, c ASC, $f4 ASC, b ASC, $f6 ASC, $e ASC])
@@ -364,7 +364,7 @@ LogicalAggregate(group=[{0}], EXPR$1=[COUNT(DISTINCT $1)], EXPR$2=[COUNT(DISTINC
 HashAggregate(isMerge=[true], groupBy=[d], select=[d, Final_COUNT(count$0) AS EXPR$1, Final_COUNT(count$1) AS EXPR$2, Final_COUNT(count$2) AS EXPR$3])
 +- Exchange(distribution=[hash[d]])
    +- LocalHashAggregate(groupBy=[d], select=[d, Partial_COUNT(c) FILTER $g_7 AS count$0, Partial_COUNT(c) FILTER $g_3 AS count$1, Partial_COUNT(b) FILTER $g_12 AS count$2])
-      +- Calc(select=[d, c, b, ((CASE(($e = 3:BIGINT), 3:BIGINT, ($e = 7:BIGINT), 7:BIGINT, 12:BIGINT) = 3) AND $f2) AS $g_3, (CASE(($e = 3:BIGINT), 3:BIGINT, ($e = 7:BIGINT), 7:BIGINT, 12:BIGINT) = 7) AS $g_7, ((CASE(($e = 3:BIGINT), 3:BIGINT, ($e = 7:BIGINT), 7:BIGINT, 12:BIGINT) = 12) AND $f4) AS $g_12])
+      +- Calc(select=[d, c, b, ((CASE(($e = 3:BIGINT), 3:BIGINT, ($e = 7:BIGINT), 7:BIGINT, 12:BIGINT) = 3) AND $f2 IS TRUE) AS $g_3, (CASE(($e = 3:BIGINT), 3:BIGINT, ($e = 7:BIGINT), 7:BIGINT, 12:BIGINT) = 7) AS $g_7, ((CASE(($e = 3:BIGINT), 3:BIGINT, ($e = 7:BIGINT), 7:BIGINT, 12:BIGINT) = 12) AND $f4 IS TRUE) AS $g_12])
          +- HashAggregate(isMerge=[true], groupBy=[d, c, $f2, b, $f4, $e], select=[d, c, $f2, b, $f4, $e])
             +- Exchange(distribution=[hash[d, c, $f2, b, $f4, $e]])
                +- LocalHashAggregate(groupBy=[d, c, $f2, b, $f4, $e], select=[d, c, $f2, b, $f4, $e])
@@ -391,7 +391,7 @@ LogicalAggregate(group=[{0}], EXPR$1=[COUNT(DISTINCT $1)], EXPR$2=[COUNT(DISTINC
 HashAggregate(isMerge=[true], groupBy=[d], select=[d, Final_COUNT(count$0) AS EXPR$1, Final_COUNT(count$1) AS EXPR$2, Final_COUNT(count$2) AS EXPR$3])
 +- Exchange(distribution=[hash[d]])
    +- LocalHashAggregate(groupBy=[d], select=[d, Partial_COUNT(c) FILTER $g_3 AS count$0, Partial_COUNT(c) FILTER $g_1 AS count$1, Partial_COUNT(c) FILTER $g_2 AS count$2])
-      +- Calc(select=[d, c, ((CASE(($e = 1:BIGINT), 1:BIGINT, ($e = 2:BIGINT), 2:BIGINT, 3:BIGINT) = 1) AND $f2) AS $g_1, ((CASE(($e = 1:BIGINT), 1:BIGINT, ($e = 2:BIGINT), 2:BIGINT, 3:BIGINT) = 2) AND $f3) AS $g_2, (CASE(($e = 1:BIGINT), 1:BIGINT, ($e = 2:BIGINT), 2:BIGINT, 3:BIGINT) = 3) AS $g_3])
+      +- Calc(select=[d, c, ((CASE(($e = 1:BIGINT), 1:BIGINT, ($e = 2:BIGINT), 2:BIGINT, 3:BIGINT) = 1) AND $f2 IS TRUE) AS $g_1, ((CASE(($e = 1:BIGINT), 1:BIGINT, ($e = 2:BIGINT), 2:BIGINT, 3:BIGINT) = 2) AND $f3 IS TRUE) AS $g_2, (CASE(($e = 1:BIGINT), 1:BIGINT, ($e = 2:BIGINT), 2:BIGINT, 3:BIGINT) = 3) AS $g_3])
          +- HashAggregate(isMerge=[true], groupBy=[d, c, $f2, $f3, $e], select=[d, c, $f2, $f3, $e])
             +- Exchange(distribution=[hash[d, c, $f2, $f3, $e]])
                +- LocalHashAggregate(groupBy=[d, c, $f2, $f3, $e], select=[d, c, $f2, $f3, $e])
@@ -419,7 +419,7 @@ SortAggregate(isMerge=[true], groupBy=[d], select=[d, Final_COUNT(count$0) AS EX
 +- Sort(orderBy=[d ASC])
    +- Exchange(distribution=[hash[d]])
       +- LocalSortAggregate(groupBy=[d], select=[d, Partial_COUNT(c) FILTER $g_1 AS count$0, Partial_COUNT(c) FILTER $g_0 AS count$1, Partial_MIN(EXPR$3) FILTER $g_3 AS min$2, Partial_MIN(EXPR$4) FILTER $g_3 AS min$3])
-         +- Calc(select=[d, c, EXPR$3, EXPR$4, ((CASE(($e = 0:BIGINT), 0:BIGINT, ($e = 1:BIGINT), 1:BIGINT, 3:BIGINT) = 0) AND $f2) AS $g_0, (CASE(($e = 0:BIGINT), 0:BIGINT, ($e = 1:BIGINT), 1:BIGINT, 3:BIGINT) = 1) AS $g_1, (CASE(($e = 0:BIGINT), 0:BIGINT, ($e = 1:BIGINT), 1:BIGINT, 3:BIGINT) = 3) AS $g_3])
+         +- Calc(select=[d, c, EXPR$3, EXPR$4, ((CASE(($e = 0:BIGINT), 0:BIGINT, ($e = 1:BIGINT), 1:BIGINT, 3:BIGINT) = 0) AND $f2 IS TRUE) AS $g_0, (CASE(($e = 0:BIGINT), 0:BIGINT, ($e = 1:BIGINT), 1:BIGINT, 3:BIGINT) = 1) AS $g_1, (CASE(($e = 0:BIGINT), 0:BIGINT, ($e = 1:BIGINT), 1:BIGINT, 3:BIGINT) = 3) AS $g_3])
             +- Sort(orderBy=[d ASC])
                +- SortAggregate(isMerge=[false], groupBy=[d, c, $f2, $e], select=[d, c, $f2, $e, MAX(e) AS EXPR$3, MIN(e) AS EXPR$4])
                   +- Sort(orderBy=[d ASC, c ASC, $f2 ASC, $e ASC])
@@ -470,7 +470,7 @@ LogicalAggregate(group=[{}], EXPR$0=[COUNT(DISTINCT $0) FILTER $1], EXPR$1=[SUM(
 HashAggregate(isMerge=[true], select=[Final_COUNT(count$0) AS EXPR$0, Final_MIN(min$1) AS EXPR$1])
 +- Exchange(distribution=[single])
    +- LocalHashAggregate(select=[Partial_COUNT(a) FILTER $g_0 AS count$0, Partial_MIN(EXPR$1) FILTER $g_3 AS min$1])
-      +- Calc(select=[a, EXPR$1, ((CASE(($e = 0:BIGINT), 0:BIGINT, 3:BIGINT) = 0) AND $f1) AS $g_0, (CASE(($e = 0:BIGINT), 0:BIGINT, 3:BIGINT) = 3) AS $g_3])
+      +- Calc(select=[a, EXPR$1, ((CASE(($e = 0:BIGINT), 0:BIGINT, 3:BIGINT) = 0) AND $f1 IS TRUE) AS $g_0, (CASE(($e = 0:BIGINT), 0:BIGINT, 3:BIGINT) = 3) AS $g_3])
          +- HashAggregate(isMerge=[true], groupBy=[a, $f1, $e], select=[a, $f1, $e, Final_SUM(sum$0) AS EXPR$1])
             +- Exchange(distribution=[hash[a, $f1, $e]])
                +- LocalHashAggregate(groupBy=[a, $f1, $e], select=[a, $f1, $e, Partial_SUM(b) AS sum$0])
diff --git a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/agg/GroupingSetsTest.xml b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/agg/GroupingSetsTest.xml
index 7a513109ace..f70de6b78e6 100644
--- a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/agg/GroupingSetsTest.xml
+++ b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/agg/GroupingSetsTest.xml
@@ -752,6 +752,36 @@ Calc(select=[c])
                   +- Expand(projects=[{deptno, 0 AS $e}, {null AS deptno, 1 AS $e}])
                      +- Calc(select=[deptno])
                         +- LegacyTableSourceScan(table=[[default_catalog, default_database, emp, source: [TestTableSource(ename, deptno, gender)]]], fields=[ename, deptno, gender])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testGroupingSetsOutputsNullability">
+    <Resource name="sql">
+      <![CDATA[
+SELECT
+ a,
+ b,
+ coalesce(c, 'empty'),
+ avg(d)
+FROM t1
+GROUP BY GROUPING SETS ((a, b), (a, b, c))
+      ]]>
+    </Resource>
+    <Resource name="ast">
+      <![CDATA[
+LogicalProject(a=[$0], b=[$1], EXPR$2=[CASE(IS NOT NULL($2), CAST($2):VARCHAR(2147483647) CHARACTER SET "UTF-16LE" NOT NULL, _UTF-16LE'empty':VARCHAR(2147483647) CHARACTER SET "UTF-16LE")], EXPR$3=[$3])
++- LogicalAggregate(group=[{0, 1, 2}], groups=[[{0, 1, 2}, {0, 1}]], EXPR$3=[AVG($3)])
+   +- LogicalTableScan(table=[[default_catalog, default_database, t1]])
+]]>
+    </Resource>
+    <Resource name="optimized exec plan">
+      <![CDATA[
+Calc(select=[a, b, CASE(c IS NOT NULL, CAST(c), _UTF-16LE'empty':VARCHAR(2147483647) CHARACTER SET "UTF-16LE") AS EXPR$2, EXPR$3])
++- HashAggregate(isMerge=[true], groupBy=[a, b, c, $e], select=[a, b, c, $e, Final_AVG(sum$0, count$1) AS EXPR$3])
+   +- Exchange(distribution=[hash[a, b, c, $e]])
+      +- LocalHashAggregate(groupBy=[a, b, c, $e], select=[a, b, c, $e, Partial_AVG(d) AS (sum$0, count$1)])
+         +- Expand(projects=[{a, b, c, d, 0 AS $e}, {a, b, null AS c, d, 1 AS $e}])
+            +- TableSourceScan(table=[[default_catalog, default_database, t1]], fields=[a, b, c, d])
 ]]>
     </Resource>
   </TestCase>
diff --git a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/nodes/exec/stream/ExpandJsonPlanTest_jsonplan/testExpand.out b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/nodes/exec/stream/ExpandJsonPlanTest_jsonplan/testExpand.out
index 4e3e514d3fa..cb8988f206b 100644
--- a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/nodes/exec/stream/ExpandJsonPlanTest_jsonplan/testExpand.out
+++ b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/nodes/exec/stream/ExpandJsonPlanTest_jsonplan/testExpand.out
@@ -267,7 +267,7 @@
       }, {
         "c" : "VARCHAR(2147483647)"
       }, {
-        "$f3" : "INT NOT NULL"
+        "$f3" : "INT"
       }, {
         "$f4" : "INT"
       }, {
@@ -304,7 +304,7 @@
       "inputIndex" : 3,
       "type" : {
         "typeName" : "INTEGER",
-        "nullable" : false
+        "nullable" : true
       }
     }, {
       "kind" : "INPUT_REF",
@@ -385,7 +385,7 @@
       }, {
         "c" : "VARCHAR(2147483647)"
       }, {
-        "$f3" : "INT NOT NULL"
+        "$f3" : "INT"
       }, {
         "$f4" : "INT"
       }, {
@@ -416,7 +416,7 @@
       }, {
         "c" : "VARCHAR(2147483647)"
       }, {
-        "$f3" : "INT NOT NULL"
+        "$f3" : "INT"
       }, {
         "$f4" : "INT"
       }, {
@@ -480,7 +480,7 @@
       "fields" : [ {
         "a" : "BIGINT"
       }, {
-        "$f3" : "INT NOT NULL"
+        "$f3" : "INT"
       }, {
         "$f4" : "INT"
       }, {
@@ -507,7 +507,7 @@
       "fields" : [ {
         "a" : "BIGINT"
       }, {
-        "$f3" : "INT NOT NULL"
+        "$f3" : "INT"
       }, {
         "$f4" : "INT"
       }, {
diff --git a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/FlinkAggregateExpandDistinctAggregatesRuleTest.xml b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/FlinkAggregateExpandDistinctAggregatesRuleTest.xml
index 96da0bba01f..4041bd6963e 100644
--- a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/FlinkAggregateExpandDistinctAggregatesRuleTest.xml
+++ b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/FlinkAggregateExpandDistinctAggregatesRuleTest.xml
@@ -55,7 +55,7 @@ LogicalAggregate(group=[{0}], EXPR$1=[COUNT($1) FILTER $2], EXPR$2=[COUNT(DISTIN
       <![CDATA[
 FlinkLogicalCalc(select=[a, CAST(EXPR$1) AS EXPR$1, EXPR$2])
 +- FlinkLogicalAggregate(group=[{0}], EXPR$1=[MIN($2) FILTER $4], EXPR$2=[COUNT($1) FILTER $3])
-   +- FlinkLogicalCalc(select=[a, d, EXPR$1, AND(=(CASE(=($e, 0:BIGINT), 0:BIGINT, 3:BIGINT), 0), $f2) AS $g_0, =(CASE(=($e, 0:BIGINT), 0:BIGINT, 3:BIGINT), 3) AS $g_3])
+   +- FlinkLogicalCalc(select=[a, d, EXPR$1, AND(=(CASE(=($e, 0:BIGINT), 0:BIGINT, 3:BIGINT), 0), IS TRUE($f2)) AS $g_0, =(CASE(=($e, 0:BIGINT), 0:BIGINT, 3:BIGINT), 3) AS $g_3])
       +- FlinkLogicalAggregate(group=[{0, 2, 3, 4}], EXPR$1=[COUNT($1) FILTER $5])
          +- FlinkLogicalExpand(projects=[{a, c, $f2, d, 0 AS $e, $f2 AS $f2_0}, {a, c, null AS $f2, null AS d, 3 AS $e, $f2 AS $f2_0}])
             +- FlinkLogicalCalc(select=[a, c, IS TRUE(>(b, 1)) AS $f2, d])
@@ -146,7 +146,7 @@ LogicalAggregate(group=[{}], EXPR$0=[COUNT(DISTINCT $0)], EXPR$1=[SUM(DISTINCT $
     <Resource name="optimized rel plan">
       <![CDATA[
 FlinkLogicalAggregate(group=[{}], EXPR$0=[COUNT($0) FILTER $3], EXPR$1=[SUM($1) FILTER $4], EXPR$2=[COUNT($2) FILTER $5])
-+- FlinkLogicalCalc(select=[a, b, c, =(CASE(=($e, 7:BIGINT), 7:BIGINT, =($e, 11:BIGINT), 11:BIGINT, 12:BIGINT), 7) AS $g_7, =(CASE(=($e, 7:BIGINT), 7:BIGINT, =($e, 11:BIGINT), 11:BIGINT, 12:BIGINT), 11) AS $g_11, AND(=(CASE(=($e, 7:BIGINT), 7:BIGINT, =($e, 11:BIGINT), 11:BIGINT, 12:BIGINT), 12), $f3) AS $g_12])
++- FlinkLogicalCalc(select=[a, b, c, =(CASE(=($e, 7:BIGINT), 7:BIGINT, =($e, 11:BIGINT), 11:BIGINT, 12:BIGINT), 7) AS $g_7, =(CASE(=($e, 7:BIGINT), 7:BIGINT, =($e, 11:BIGINT), 11:BIGINT, 12:BIGINT), 11) AS $g_11, AND(=(CASE(=($e, 7:BIGINT), 7:BIGINT, =($e, 11:BIGINT), 11:BIGINT, 12:BIGINT), 12), IS TRUE($f3)) AS $g_12])
    +- FlinkLogicalAggregate(group=[{0, 1, 2, 3, 4}])
       +- FlinkLogicalExpand(projects=[{a, null AS b, null AS c, null AS $f3, 7 AS $e}, {null AS a, b, null AS c, null AS $f3, 11 AS $e}, {null AS a, null AS b, c, $f3, 12 AS $e}])
          +- FlinkLogicalCalc(select=[a, b, c, IS TRUE(>(a, 5)) AS $f3])
@@ -194,7 +194,7 @@ LogicalAggregate(group=[{}], EXPR$0=[COUNT(DISTINCT $0)], EXPR$1=[SUM(DISTINCT $
     <Resource name="optimized rel plan">
       <![CDATA[
 FlinkLogicalAggregate(group=[{}], EXPR$0=[COUNT($0) FILTER $3], EXPR$1=[SUM($0) FILTER $2], EXPR$2=[MIN($1) FILTER $4])
-+- FlinkLogicalCalc(select=[a, EXPR$2, AND(=(CASE(=($e, 0:BIGINT), 0:BIGINT, =($e, 1:BIGINT), 1:BIGINT, 3:BIGINT), 0), $f1) AS $g_0, =(CASE(=($e, 0:BIGINT), 0:BIGINT, =($e, 1:BIGINT), 1:BIGINT, 3:BIGINT), 1) AS $g_1, =(CASE(=($e, 0:BIGINT), 0:BIGINT, =($e, 1:BIGINT), 1:BIGINT, 3:BIGINT), 3) AS $g_3])
++- FlinkLogicalCalc(select=[a, EXPR$2, AND(=(CASE(=($e, 0:BIGINT), 0:BIGINT, =($e, 1:BIGINT), 1:BIGINT, 3:BIGINT), 0), IS TRUE($f1)) AS $g_0, =(CASE(=($e, 0:BIGINT), 0:BIGINT, =($e, 1:BIGINT), 1:BIGINT, 3:BIGINT), 1) AS $g_1, =(CASE(=($e, 0:BIGINT), 0:BIGINT, =($e, 1:BIGINT), 1:BIGINT, 3:BIGINT), 3) AS $g_3])
    +- FlinkLogicalAggregate(group=[{0, 1, 2}], EXPR$2=[MAX($3)])
       +- FlinkLogicalExpand(projects=[{a, $f1, 0 AS $e, a AS a_0}, {a, null AS $f1, 1 AS $e, a AS a_0}, {null AS a, null AS $f1, 3 AS $e, a AS a_0}])
          +- FlinkLogicalCalc(select=[a, IS TRUE(>(b, 0)) AS $f1])
@@ -241,7 +241,7 @@ LogicalAggregate(group=[{}], EXPR$0=[COUNT(DISTINCT $0) FILTER $1], EXPR$1=[SUM(
       <![CDATA[
 FlinkLogicalCalc(select=[EXPR$0, EXPR$1, CASE(IS NOT NULL(EXPR$2), EXPR$2, 0) AS EXPR$2])
 +- FlinkLogicalAggregate(group=[{}], EXPR$0=[COUNT($0) FILTER $3], EXPR$1=[SUM($1) FILTER $4], EXPR$2=[MIN($2) FILTER $5])
-   +- FlinkLogicalCalc(select=[a, b, EXPR$2, AND(=(CASE(=($e, 1:BIGINT), 1:BIGINT, =($e, 6:BIGINT), 6:BIGINT, 7:BIGINT), 1), $f1) AS $g_1, =(CASE(=($e, 1:BIGINT), 1:BIGINT, =($e, 6:BIGINT), 6:BIGINT, 7:BIGINT), 6) AS $g_6, =(CASE(=($e, 1:BIGINT), 1:BIGINT, =($e, 6:BIGINT), 6:BIGINT, 7:BIGINT), 7) AS $g_7])
+   +- FlinkLogicalCalc(select=[a, b, EXPR$2, AND(=(CASE(=($e, 1:BIGINT), 1:BIGINT, =($e, 6:BIGINT), 6:BIGINT, 7:BIGINT), 1), IS TRUE($f1)) AS $g_1, =(CASE(=($e, 1:BIGINT), 1:BIGINT, =($e, 6:BIGINT), 6:BIGINT, 7:BIGINT), 6) AS $g_6, =(CASE(=($e, 1:BIGINT), 1:BIGINT, =($e, 6:BIGINT), 6:BIGINT, 7:BIGINT), 7) AS $g_7])
       +- FlinkLogicalAggregate(group=[{0, 1, 2, 4}], EXPR$2=[COUNT($3)])
          +- FlinkLogicalExpand(projects=[{a, $f1, null AS b, c, 1 AS $e}, {null AS a, null AS $f1, b, c, 6 AS $e}, {null AS a, null AS $f1, null AS b, c, 7 AS $e}])
             +- FlinkLogicalCalc(select=[a, IS TRUE(>(c, 0)) AS $f1, b, c])
@@ -289,7 +289,7 @@ LogicalAggregate(group=[{0}], EXPR$1=[MAX($1)], EXPR$2=[MAX($1) FILTER $2], EXPR
     <Resource name="optimized rel plan">
       <![CDATA[
 FlinkLogicalAggregate(group=[{0}], EXPR$1=[MIN($3) FILTER $8], EXPR$2=[MIN($4) FILTER $8], EXPR$3=[COUNT($1) FILTER $6], EXPR$4=[COUNT($1) FILTER $5], EXPR$5=[COUNT($2) FILTER $7])
-+- FlinkLogicalCalc(select=[d, c, b, EXPR$1, EXPR$2, AND(=(CASE(=($e, 3:BIGINT), 3:BIGINT, =($e, 7:BIGINT), 7:BIGINT, =($e, 12:BIGINT), 12:BIGINT, 15:BIGINT), 3), $f4) AS $g_3, =(CASE(=($e, 3:BIGINT), 3:BIGINT, =($e, 7:BIGINT), 7:BIGINT, =($e, 12:BIGINT), 12:BIGINT, 15:BIGINT), 7) AS $g_7, AND(=(CASE(=($e, 3:BIGINT), 3:BIGINT, =($e, 7:BIGINT), 7:BIGINT, =($e, 12:BIGINT), 12:BIGINT, 15:BIGINT), 12), $f6) AS $g_12, =(CASE(=($e, 3:BIGINT), 3:BIGINT, =($e, 7:BIGINT), 7:BIGINT, =($e, 12:BIGINT), 12:BIGINT, 15:BIGINT), 15) AS $g_15])
++- FlinkLogicalCalc(select=[d, c, b, EXPR$1, EXPR$2, AND(=(CASE(=($e, 3:BIGINT), 3:BIGINT, =($e, 7:BIGINT), 7:BIGINT, =($e, 12:BIGINT), 12:BIGINT, 15:BIGINT), 3), IS TRUE($f4)) AS $g_3, =(CASE(=($e, 3:BIGINT), 3:BIGINT, =($e, 7:BIGINT), 7:BIGINT, =($e, 12:BIGINT), 12:BIGINT, 15:BIGINT), 7) AS $g_7, AND(=(CASE(=($e, 3:BIGINT), 3:BIGINT, =($e, 7:BIGINT), 7:BIGINT, =($e, 12:BIGINT), 12:BIGINT, 15:BIGINT), 12), IS TRUE($f6)) AS $g_12, =(CASE(=($e, 3:BIGINT), 3:BIGINT, =($e, 7:BIGINT), 7:BIGINT, =($e, 12:BIGINT), 12:BIGINT, 15:BIGINT), 15) AS $g_15])
    +- FlinkLogicalAggregate(group=[{0, 3, 4, 5, 6, 7}], EXPR$1=[MAX($1)], EXPR$2=[MAX($1) FILTER $2])
       +- FlinkLogicalExpand(projects=[{d, e, $f2, c, $f4, null AS b, null AS $f6, 3 AS $e}, {d, e, $f2, c, null AS $f4, null AS b, null AS $f6, 7 AS $e}, {d, e, $f2, null AS c, null AS $f4, b, $f6, 12 AS $e}, {d, e, $f2, null AS c, null AS $f4, null AS b, null AS $f6, 15 AS $e}])
          +- FlinkLogicalCalc(select=[d, e, IS TRUE(<(a, 10)) AS $f2, c, IS TRUE(>(a, 5)) AS $f4, b, IS TRUE(>(b, 3)) AS $f6])
@@ -312,7 +312,7 @@ LogicalAggregate(group=[{0}], EXPR$1=[COUNT(DISTINCT $1)], EXPR$2=[COUNT(DISTINC
     <Resource name="optimized rel plan">
       <![CDATA[
 FlinkLogicalAggregate(group=[{0}], EXPR$1=[COUNT($1) FILTER $4], EXPR$2=[COUNT($1) FILTER $3], EXPR$3=[COUNT($2) FILTER $5])
-+- FlinkLogicalCalc(select=[d, c, b, AND(=(CASE(=($e, 3:BIGINT), 3:BIGINT, =($e, 7:BIGINT), 7:BIGINT, 12:BIGINT), 3), $f2) AS $g_3, =(CASE(=($e, 3:BIGINT), 3:BIGINT, =($e, 7:BIGINT), 7:BIGINT, 12:BIGINT), 7) AS $g_7, AND(=(CASE(=($e, 3:BIGINT), 3:BIGINT, =($e, 7:BIGINT), 7:BIGINT, 12:BIGINT), 12), $f4) AS $g_12])
++- FlinkLogicalCalc(select=[d, c, b, AND(=(CASE(=($e, 3:BIGINT), 3:BIGINT, =($e, 7:BIGINT), 7:BIGINT, 12:BIGINT), 3), IS TRUE($f2)) AS $g_3, =(CASE(=($e, 3:BIGINT), 3:BIGINT, =($e, 7:BIGINT), 7:BIGINT, 12:BIGINT), 7) AS $g_7, AND(=(CASE(=($e, 3:BIGINT), 3:BIGINT, =($e, 7:BIGINT), 7:BIGINT, 12:BIGINT), 12), IS TRUE($f4)) AS $g_12])
    +- FlinkLogicalAggregate(group=[{0, 1, 2, 3, 4, 5}])
       +- FlinkLogicalExpand(projects=[{d, c, $f2, null AS b, null AS $f4, 3 AS $e}, {d, c, null AS $f2, null AS b, null AS $f4, 7 AS $e}, {d, null AS c, null AS $f2, b, $f4, 12 AS $e}])
          +- FlinkLogicalCalc(select=[d, c, IS TRUE(>(a, 0)) AS $f2, b, IS TRUE(>(b, 1)) AS $f4])
@@ -335,7 +335,7 @@ LogicalAggregate(group=[{0}], EXPR$1=[COUNT(DISTINCT $1)], EXPR$2=[COUNT(DISTINC
     <Resource name="optimized rel plan">
       <![CDATA[
 FlinkLogicalAggregate(group=[{0}], EXPR$1=[COUNT($1) FILTER $4], EXPR$2=[COUNT($1) FILTER $2], EXPR$3=[COUNT($1) FILTER $3])
-+- FlinkLogicalCalc(select=[d, c, AND(=(CASE(=($e, 1:BIGINT), 1:BIGINT, =($e, 2:BIGINT), 2:BIGINT, 3:BIGINT), 1), $f2) AS $g_1, AND(=(CASE(=($e, 1:BIGINT), 1:BIGINT, =($e, 2:BIGINT), 2:BIGINT, 3:BIGINT), 2), $f3) AS $g_2, =(CASE(=($e, 1:BIGINT), 1:BIGINT, =($e, 2:BIGINT), 2:BIGINT, 3:BIGINT), 3) AS $g_3])
++- FlinkLogicalCalc(select=[d, c, AND(=(CASE(=($e, 1:BIGINT), 1:BIGINT, =($e, 2:BIGINT), 2:BIGINT, 3:BIGINT), 1), IS TRUE($f2)) AS $g_1, AND(=(CASE(=($e, 1:BIGINT), 1:BIGINT, =($e, 2:BIGINT), 2:BIGINT, 3:BIGINT), 2), IS TRUE($f3)) AS $g_2, =(CASE(=($e, 1:BIGINT), 1:BIGINT, =($e, 2:BIGINT), 2:BIGINT, 3:BIGINT), 3) AS $g_3])
    +- FlinkLogicalAggregate(group=[{0, 1, 2, 3, 4}])
       +- FlinkLogicalExpand(projects=[{d, c, $f2, null AS $f3, 1 AS $e}, {d, c, null AS $f2, $f3, 2 AS $e}, {d, c, null AS $f2, null AS $f3, 3 AS $e}])
          +- FlinkLogicalCalc(select=[d, c, IS TRUE(>(a, 10)) AS $f2, IS TRUE(<(a, 10)) AS $f3])
@@ -358,7 +358,7 @@ LogicalAggregate(group=[{0}], EXPR$1=[COUNT(DISTINCT $1)], EXPR$2=[COUNT(DISTINC
     <Resource name="optimized rel plan">
       <![CDATA[
 FlinkLogicalAggregate(group=[{0}], EXPR$1=[COUNT($1) FILTER $5], EXPR$2=[COUNT($1) FILTER $4], EXPR$3=[MIN($2) FILTER $6], EXPR$4=[MIN($3) FILTER $6])
-+- FlinkLogicalCalc(select=[d, c, EXPR$3, EXPR$4, AND(=(CASE(=($e, 0:BIGINT), 0:BIGINT, =($e, 1:BIGINT), 1:BIGINT, 3:BIGINT), 0), $f2) AS $g_0, =(CASE(=($e, 0:BIGINT), 0:BIGINT, =($e, 1:BIGINT), 1:BIGINT, 3:BIGINT), 1) AS $g_1, =(CASE(=($e, 0:BIGINT), 0:BIGINT, =($e, 1:BIGINT), 1:BIGINT, 3:BIGINT), 3) AS $g_3])
++- FlinkLogicalCalc(select=[d, c, EXPR$3, EXPR$4, AND(=(CASE(=($e, 0:BIGINT), 0:BIGINT, =($e, 1:BIGINT), 1:BIGINT, 3:BIGINT), 0), IS TRUE($f2)) AS $g_0, =(CASE(=($e, 0:BIGINT), 0:BIGINT, =($e, 1:BIGINT), 1:BIGINT, 3:BIGINT), 1) AS $g_1, =(CASE(=($e, 0:BIGINT), 0:BIGINT, =($e, 1:BIGINT), 1:BIGINT, 3:BIGINT), 3) AS $g_3])
    +- FlinkLogicalAggregate(group=[{0, 1, 2, 4}], EXPR$3=[MAX($3)], EXPR$4=[MIN($3)])
       +- FlinkLogicalExpand(projects=[{d, c, $f2, e, 0 AS $e}, {d, c, null AS $f2, e, 1 AS $e}, {d, null AS c, null AS $f2, e, 3 AS $e}])
          +- FlinkLogicalCalc(select=[d, c, IS TRUE(>(a, 0)) AS $f2, e])
@@ -400,7 +400,7 @@ LogicalAggregate(group=[{}], EXPR$0=[COUNT(DISTINCT $0) FILTER $1], EXPR$1=[SUM(
     <Resource name="optimized rel plan">
       <![CDATA[
 FlinkLogicalAggregate(group=[{}], EXPR$0=[COUNT($0) FILTER $2], EXPR$1=[MIN($1) FILTER $3])
-+- FlinkLogicalCalc(select=[a, EXPR$1, AND(=(CASE(=($e, 0:BIGINT), 0:BIGINT, 3:BIGINT), 0), $f1) AS $g_0, =(CASE(=($e, 0:BIGINT), 0:BIGINT, 3:BIGINT), 3) AS $g_3])
++- FlinkLogicalCalc(select=[a, EXPR$1, AND(=(CASE(=($e, 0:BIGINT), 0:BIGINT, 3:BIGINT), 0), IS TRUE($f1)) AS $g_0, =(CASE(=($e, 0:BIGINT), 0:BIGINT, 3:BIGINT), 3) AS $g_3])
    +- FlinkLogicalAggregate(group=[{0, 1, 3}], EXPR$1=[SUM($2)])
       +- FlinkLogicalExpand(projects=[{a, $f1, b, 0 AS $e}, {null AS a, null AS $f1, b, 3 AS $e}])
          +- FlinkLogicalCalc(select=[a, IS TRUE(>(a, 0)) AS $f1, b])
diff --git a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/physical/batch/RemoveRedundantLocalHashAggRuleTest.xml b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/physical/batch/RemoveRedundantLocalHashAggRuleTest.xml
index 07265fea7a0..17424124983 100644
--- a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/physical/batch/RemoveRedundantLocalHashAggRuleTest.xml
+++ b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/physical/batch/RemoveRedundantLocalHashAggRuleTest.xml
@@ -32,7 +32,7 @@ LogicalAggregate(group=[{0}], EXPR$1=[MAX($1)], EXPR$2=[MAX($1) FILTER $2], EXPR
     <Resource name="optimized rel plan">
       <![CDATA[
 HashAggregate(isMerge=[false], groupBy=[d], select=[d, MIN(EXPR$1) FILTER $g_15 AS EXPR$1, MIN(EXPR$2) FILTER $g_15 AS EXPR$2, COUNT(c) FILTER $g_7 AS EXPR$3, COUNT(c) FILTER $g_3 AS EXPR$4, COUNT(b) FILTER $g_12 AS EXPR$5])
-+- Calc(select=[d, c, b, EXPR$1, EXPR$2, AND(=(CASE(=($e, 3:BIGINT), 3:BIGINT, =($e, 7:BIGINT), 7:BIGINT, =($e, 12:BIGINT), 12:BIGINT, 15:BIGINT), 3), $f4) AS $g_3, =(CASE(=($e, 3:BIGINT), 3:BIGINT, =($e, 7:BIGINT), 7:BIGINT, =($e, 12:BIGINT), 12:BIGINT, 15:BIGINT), 7) AS $g_7, AND(=(CASE(=($e, 3:BIGINT), 3:BIGINT, =($e, 7:BIGINT), 7:BIGINT, =($e, 12:BIGINT), 12:BIGINT, 15:BIGINT), 12), $f6) AS $g_12, =(CASE(=($e, 3:BIGINT), 3:BIGINT, =($e, 7:BIGINT), 7:BIGINT, =($e, 12:BIGINT), 12:BIGINT, 15:BIGINT), 15) AS $g_15])
++- Calc(select=[d, c, b, EXPR$1, EXPR$2, AND(=(CASE(=($e, 3:BIGINT), 3:BIGINT, =($e, 7:BIGINT), 7:BIGINT, =($e, 12:BIGINT), 12:BIGINT, 15:BIGINT), 3), IS TRUE($f4)) AS $g_3, =(CASE(=($e, 3:BIGINT), 3:BIGINT, =($e, 7:BIGINT), 7:BIGINT, =($e, 12:BIGINT), 12:BIGINT, 15:BIGINT), 7) AS $g_7, AND(=(CASE(=($e, 3:BIGINT), 3:BIGINT, =($e, 7:BIGINT), 7:BIGINT, =($e, 12:BIGINT), 12:BIGINT, 15:BIGINT), 12), IS TRUE($f6)) AS $g_12, =(CASE(=($e, 3:BIGINT), 3:BIGINT, =($e, 7:BIGINT), 7:BIGINT, =($e, 12:BIGINT), 12:BIGINT, 15:BIGINT), 15) AS $g_15])
    +- HashAggregate(isMerge=[true], groupBy=[d, c, $f4, b, $f6, $e], select=[d, c, $f4, b, $f6, $e, Final_MAX(max$0) AS EXPR$1, Final_MAX(max$1) AS EXPR$2])
       +- Exchange(distribution=[hash[d]])
          +- LocalHashAggregate(groupBy=[d, c, $f4, b, $f6, $e], select=[d, c, $f4, b, $f6, $e, Partial_MAX(e) AS max$0, Partial_MAX(e) FILTER $f2 AS max$1])
diff --git a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/physical/batch/RemoveRedundantLocalSortAggRuleTest.xml b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/physical/batch/RemoveRedundantLocalSortAggRuleTest.xml
index ea65c729248..c2f32f80933 100644
--- a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/physical/batch/RemoveRedundantLocalSortAggRuleTest.xml
+++ b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/physical/batch/RemoveRedundantLocalSortAggRuleTest.xml
@@ -68,7 +68,7 @@ SortAggregate(isMerge=[true], groupBy=[d], select=[d, Final_MIN(min$0) AS EXPR$1
 +- Sort(orderBy=[d ASC])
    +- Exchange(distribution=[hash[d]])
       +- LocalSortAggregate(groupBy=[d], select=[d, Partial_MIN(EXPR$1) FILTER $g_15 AS min$0, Partial_MIN(EXPR$2) FILTER $g_15 AS min$1, Partial_COUNT(c) FILTER $g_7 AS count$2, Partial_COUNT(c) FILTER $g_3 AS count$3, Partial_COUNT(b) FILTER $g_12 AS count$4])
-         +- Calc(select=[d, c, b, EXPR$1, EXPR$2, AND(=(CASE(=($e, 3:BIGINT), 3:BIGINT, =($e, 7:BIGINT), 7:BIGINT, =($e, 12:BIGINT), 12:BIGINT, 15:BIGINT), 3), $f4) AS $g_3, =(CASE(=($e, 3:BIGINT), 3:BIGINT, =($e, 7:BIGINT), 7:BIGINT, =($e, 12:BIGINT), 12:BIGINT, 15:BIGINT), 7) AS $g_7, AND(=(CASE(=($e, 3:BIGINT), 3:BIGINT, =($e, 7:BIGINT), 7:BIGINT, =($e, 12:BIGINT), 12:BIGINT, 15:BIGINT), 12), $f6) AS $g_12, =(CASE(=($e, 3:BIGINT), 3:BIGINT, =($e, 7:BIGINT), 7:BIGINT, =($e, 12:BIGINT), 12:BIGINT, 15:BIGINT), 15) AS $g_15])
+         +- Calc(select=[d, c, b, EXPR$1, EXPR$2, AND(=(CASE(=($e, 3:BIGINT), 3:BIGINT, =($e, 7:BIGINT), 7:BIGINT, =($e, 12:BIGINT), 12:BIGINT, 15:BIGINT), 3), IS TRUE($f4)) AS $g_3, =(CASE(=($e, 3:BIGINT), 3:BIGINT, =($e, 7:BIGINT), 7:BIGINT, =($e, 12:BIGINT), 12:BIGINT, 15:BIGINT), 7) AS $g_7, AND(=(CASE(=($e, 3:BIGINT), 3:BIGINT, =($e, 7:BIGINT), 7:BIGINT, =($e, 12:BIGINT), 12:BIGINT, 15:BIGINT), 12), IS TRUE($f6)) AS $g_12, =(CASE(=($e, 3:BIGINT), 3:BIGINT, =($e, 7:BIGINT), 7:BIGINT, =($e, 12:BIGINT), 12:BIGINT, 15:BIGINT), 15) AS $g_15])
             +- Sort(orderBy=[d ASC])
                +- SortAggregate(isMerge=[false], groupBy=[d, c, $f4, b, $f6, $e], select=[d, c, $f4, b, $f6, $e, MAX(e) AS EXPR$1, MAX(e) FILTER $f2 AS EXPR$2])
                   +- Sort(orderBy=[d ASC, c ASC, $f4 ASC, b ASC, $f6 ASC, $e ASC])
diff --git a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/agg/GroupingSetsTest.xml b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/agg/GroupingSetsTest.xml
index 6995574958d..581a5a6017a 100644
--- a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/agg/GroupingSetsTest.xml
+++ b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/agg/GroupingSetsTest.xml
@@ -687,6 +687,35 @@ Calc(select=[c])
                +- Expand(projects=[{deptno, 0 AS $e}, {null AS deptno, 1 AS $e}])
                   +- Calc(select=[deptno])
                      +- LegacyTableSourceScan(table=[[default_catalog, default_database, emp, source: [TestTableSource(ename, deptno, gender)]]], fields=[ename, deptno, gender])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testGroupingSetsOutputsNullability">
+    <Resource name="sql">
+      <![CDATA[
+SELECT
+ a,
+ b,
+ coalesce(c, 'empty'),
+ avg(d)
+FROM t1
+GROUP BY GROUPING SETS ((a, b), (a, b, c))
+      ]]>
+    </Resource>
+    <Resource name="ast">
+      <![CDATA[
+LogicalProject(a=[$0], b=[$1], EXPR$2=[CASE(IS NOT NULL($2), CAST($2):VARCHAR(2147483647) CHARACTER SET "UTF-16LE" NOT NULL, _UTF-16LE'empty':VARCHAR(2147483647) CHARACTER SET "UTF-16LE")], EXPR$3=[$3])
++- LogicalAggregate(group=[{0, 1, 2}], groups=[[{0, 1, 2}, {0, 1}]], EXPR$3=[AVG($3)])
+   +- LogicalTableScan(table=[[default_catalog, default_database, t1]])
+]]>
+    </Resource>
+    <Resource name="optimized exec plan">
+      <![CDATA[
+Calc(select=[a, b, CASE(c IS NOT NULL, CAST(c), _UTF-16LE'empty':VARCHAR(2147483647) CHARACTER SET "UTF-16LE") AS EXPR$2, EXPR$3])
++- GroupAggregate(groupBy=[a, b, c, $e], select=[a, b, c, $e, AVG(d) AS EXPR$3])
+   +- Exchange(distribution=[hash[a, b, c, $e]])
+      +- Expand(projects=[{a, b, c, d, 0 AS $e}, {a, b, null AS c, d, 1 AS $e}])
+         +- TableSourceScan(table=[[default_catalog, default_database, t1]], fields=[a, b, c, d])
 ]]>
     </Resource>
   </TestCase>
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/batch/sql/agg/GroupingSetsTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/batch/sql/agg/GroupingSetsTest.scala
index 6ab7b1d59bf..10eab590113 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/batch/sql/agg/GroupingSetsTest.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/batch/sql/agg/GroupingSetsTest.scala
@@ -74,6 +74,37 @@ class GroupingSetsTest extends TableTestBase {
     util.verifyExecPlan(sqlQuery)
   }
 
+  @Test
+  def testGroupingSetsOutputsNullability(): Unit = {
+    // prepare a source table contains a non-null column
+    val sourceDDL =
+      s"""
+         |create table t1(
+         |  a int,
+         |  b varchar,
+         |  c varchar not null,
+         |  d bigint
+         |) with (
+         |  'connector' = 'filesystem',
+         |  'path' = '/to/my/path1',
+         |  'format' = 'testcsv'
+         |)
+      """.stripMargin
+    util.tableEnv.executeSql(sourceDDL)
+
+    val sqlQuery =
+      """
+        |SELECT
+        | a,
+        | b,
+        | coalesce(c, 'empty'),
+        | avg(d)
+        |FROM t1
+        |GROUP BY GROUPING SETS ((a, b), (a, b, c))
+      """.stripMargin
+    util.verifyExecPlan(sqlQuery)
+  }
+
   @Test
   def testCube(): Unit = {
     val sqlQuery =
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdColumnUniquenessTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdColumnUniquenessTest.scala
index d657d51ed95..31c8cf0c217 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdColumnUniquenessTest.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdColumnUniquenessTest.scala
@@ -155,12 +155,9 @@ class FlinkRelMdColumnUniquenessTest extends FlinkRelMdHandlerTestBase {
         }
     }
 
-    val expandOutputType = ExpandUtil.buildExpandRowType(
-      cluster.getTypeFactory, studentLogicalScan.getRowType, Array.empty[Integer])
     val expandProjects = ExpandUtil.createExpandProjects(
       studentLogicalScan.getCluster.getRexBuilder,
       studentLogicalScan.getRowType,
-      expandOutputType,
       ImmutableBitSet.of(0, 3, 5),
       ImmutableList.of(
         ImmutableBitSet.of(0, 3, 5),
@@ -168,7 +165,7 @@ class FlinkRelMdColumnUniquenessTest extends FlinkRelMdHandlerTestBase {
         ImmutableBitSet.of(3)),
       Array.empty[Integer])
     val logicalExpand2 = new LogicalExpand(cluster, studentLogicalScan.getTraitSet,
-      studentLogicalScan, expandOutputType, expandProjects, 7)
+      studentLogicalScan, expandProjects, 7)
     (0 until logicalExpand2.getRowType.getFieldCount - 1).foreach { idx =>
       assertFalse(mq.areColumnsUnique(logicalExpand2, ImmutableBitSet.of(idx, 7)))
     }
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdHandlerTestBase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdHandlerTestBase.scala
index 9a030bdabe4..d3a524a7f2e 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdHandlerTestBase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdHandlerTestBase.scala
@@ -261,12 +261,9 @@ class FlinkRelMdHandlerTestBase {
   // id, null, score, age, height, null, class, 5
   protected lazy val (logicalExpand, flinkLogicalExpand, batchExpand, streamExpand) = {
     val cluster = studentLogicalScan.getCluster
-    val expandOutputType = ExpandUtil.buildExpandRowType(
-      cluster.getTypeFactory, studentLogicalScan.getRowType, Array.empty[Integer])
     val expandProjects = ExpandUtil.createExpandProjects(
       studentLogicalScan.getCluster.getRexBuilder,
       studentLogicalScan.getRowType,
-      expandOutputType,
       ImmutableBitSet.of(1, 3, 5),
       ImmutableList.of(
         ImmutableBitSet.of(1, 3, 5),
@@ -274,16 +271,16 @@ class FlinkRelMdHandlerTestBase {
         ImmutableBitSet.of(3)),
       Array.empty[Integer])
     val logicalExpand = new LogicalExpand(cluster, studentLogicalScan.getTraitSet,
-      studentLogicalScan, expandOutputType, expandProjects, 7)
+      studentLogicalScan, expandProjects, 7)
 
     val flinkLogicalExpand = new FlinkLogicalExpand(cluster, flinkLogicalTraits,
-      studentFlinkLogicalScan, expandOutputType, expandProjects, 7)
+      studentFlinkLogicalScan, expandProjects, 7)
 
     val batchExpand = new BatchPhysicalExpand(cluster, batchPhysicalTraits,
-      studentBatchScan, expandOutputType, expandProjects, 7)
+      studentBatchScan, expandProjects, 7)
 
     val streamExecExpand = new StreamPhysicalExpand(cluster, streamPhysicalTraits,
-      studentStreamScan, expandOutputType, expandProjects, 7)
+      studentStreamScan, expandProjects, 7)
 
     (logicalExpand, flinkLogicalExpand, batchExpand, streamExecExpand)
   }
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdSelectivityTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdSelectivityTest.scala
index dab451cd797..95b2f534614 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdSelectivityTest.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdSelectivityTest.scala
@@ -151,16 +151,13 @@ class FlinkRelMdSelectivityTest extends FlinkRelMdHandlerTestBase {
   @Test
   def testGetSelectivityOnExpand(): Unit = {
     val ts = relBuilder.scan("MyTable3").build()
-    val expandOutputType = ExpandUtil.buildExpandRowType(
-      ts.getCluster.getTypeFactory, ts.getRowType, Array.empty[Integer])
     val expandProjects = ExpandUtil.createExpandProjects(
       ts.getCluster.getRexBuilder,
       ts.getRowType,
-      expandOutputType,
       ImmutableBitSet.of(0, 1),
       ImmutableList.of(ImmutableBitSet.of(0), ImmutableBitSet.of(1)), Array.empty[Integer])
     val expand = new FlinkLogicalExpand(
-      ts.getCluster, ts.getTraitSet, ts, expandOutputType, expandProjects, 2)
+      ts.getCluster, ts.getTraitSet, ts, expandProjects, 2)
 
     relBuilder.push(expand)
     val predicate1 = relBuilder
@@ -289,16 +286,13 @@ class FlinkRelMdSelectivityTest extends FlinkRelMdHandlerTestBase {
 
     relBuilder.clear()
     val ts = relBuilder.scan("MyTable4").build()
-    val expandOutputType = ExpandUtil.buildExpandRowType(
-      ts.getCluster.getTypeFactory, ts.getRowType, Array.empty[Integer])
     val expandProjects = ExpandUtil.createExpandProjects(
       ts.getCluster.getRexBuilder,
       ts.getRowType,
-      expandOutputType,
       ImmutableBitSet.of(0, 1, 2),
       ImmutableList.of(ImmutableBitSet.of(0, 1), ImmutableBitSet.of(0, 2)), Array.empty[Integer])
     val expand = new LogicalExpand(
-      ts.getCluster, ts.getTraitSet, ts, expandOutputType, expandProjects, 4)
+      ts.getCluster, ts.getTraitSet, ts, expandProjects, 4)
 
     // agg output type: a, $e, b, c, count(d)
     val aggWithAuxGroupAndExpand = relBuilder.push(expand).aggregate(
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdUniqueGroupsTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdUniqueGroupsTest.scala
index 269be8052eb..92ae3bf8463 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdUniqueGroupsTest.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdUniqueGroupsTest.scala
@@ -183,12 +183,9 @@ class FlinkRelMdUniqueGroupsTest extends FlinkRelMdHandlerTestBase {
   def testGetUniqueGroupsOnExpand(): Unit = {
     // column 0 is unique key
     val ts = studentLogicalScan
-    val expandOutputType = ExpandUtil.buildExpandRowType(
-      ts.getCluster.getTypeFactory, ts.getRowType, Array.empty[Integer])
     val expandProjects1 = ExpandUtil.createExpandProjects(
       ts.getCluster.getRexBuilder,
       ts.getRowType,
-      expandOutputType,
       ImmutableBitSet.of(0, 1, 2, 3),
       ImmutableList.of(
         ImmutableBitSet.of(0),
@@ -197,7 +194,7 @@ class FlinkRelMdUniqueGroupsTest extends FlinkRelMdHandlerTestBase {
         ImmutableBitSet.of(3)
       ), Array.empty[Integer])
     val expand1 = new FlinkLogicalExpand(
-      ts.getCluster, ts.getTraitSet, ts, expandOutputType, expandProjects1, 7)
+      ts.getCluster, ts.getTraitSet, ts, expandProjects1, 7)
     assertEquals(ImmutableBitSet.of(0), mq.getUniqueGroups(expand1, ImmutableBitSet.of(0)))
     assertEquals(ImmutableBitSet.of(1), mq.getUniqueGroups(expand1, ImmutableBitSet.of(1)))
     assertEquals(ImmutableBitSet.of(2), mq.getUniqueGroups(expand1, ImmutableBitSet.of(2)))
@@ -211,7 +208,6 @@ class FlinkRelMdUniqueGroupsTest extends FlinkRelMdHandlerTestBase {
     val expandProjects2 = ExpandUtil.createExpandProjects(
       ts.getCluster.getRexBuilder,
       ts.getRowType,
-      expandOutputType,
       ImmutableBitSet.of(0, 1, 2, 3),
       ImmutableList.of(
         ImmutableBitSet.of(0, 1),
@@ -219,7 +215,7 @@ class FlinkRelMdUniqueGroupsTest extends FlinkRelMdHandlerTestBase {
         ImmutableBitSet.of(0, 3)
       ), Array.empty[Integer])
     val expand2 = new FlinkLogicalExpand(
-      ts.getCluster, ts.getTraitSet, ts, expandOutputType, expandProjects2, 7)
+      ts.getCluster, ts.getTraitSet, ts, expandProjects2, 7)
     assertEquals(ImmutableSet.of(ImmutableBitSet.of(0, 7)), mq.getUniqueKeys(expand2))
     assertEquals(ImmutableBitSet.of(0), mq.getUniqueGroups(expand2, ImmutableBitSet.of(0)))
     assertEquals(ImmutableBitSet.of(1), mq.getUniqueGroups(expand2, ImmutableBitSet.of(1)))
@@ -236,14 +232,13 @@ class FlinkRelMdUniqueGroupsTest extends FlinkRelMdHandlerTestBase {
     val expandProjects3 = ExpandUtil.createExpandProjects(
       ts.getCluster.getRexBuilder,
       ts.getRowType,
-      expandOutputType,
       ImmutableBitSet.of(0, 1, 2, 3),
       ImmutableList.of(
         ImmutableBitSet.of(0, 1, 2),
         ImmutableBitSet.of(0, 1, 3)
       ), Array.empty[Integer])
     val expand3 = new FlinkLogicalExpand(
-      ts.getCluster, ts.getTraitSet, ts, expandOutputType, expandProjects3, 7)
+      ts.getCluster, ts.getTraitSet, ts, expandProjects3, 7)
     assertEquals(ImmutableSet.of(ImmutableBitSet.of(0, 7)), mq.getUniqueKeys(expand2))
     assertEquals(ImmutableBitSet.of(0), mq.getUniqueGroups(expand3, ImmutableBitSet.of(0)))
     assertEquals(ImmutableBitSet.of(1), mq.getUniqueGroups(expand3, ImmutableBitSet.of(1)))
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdUniqueKeysTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdUniqueKeysTest.scala
index 41656cdcf65..250b74ae2cd 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdUniqueKeysTest.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdUniqueKeysTest.scala
@@ -117,12 +117,9 @@ class FlinkRelMdUniqueKeysTest extends FlinkRelMdHandlerTestBase {
       expand => assertEquals(uniqueKeys(Array(0, 7)), mq.getUniqueKeys(expand).toSet)
     }
 
-    val expandOutputType = ExpandUtil.buildExpandRowType(
-      cluster.getTypeFactory, studentLogicalScan.getRowType, Array.empty[Integer])
     val expandProjects = ExpandUtil.createExpandProjects(
       studentLogicalScan.getCluster.getRexBuilder,
       studentLogicalScan.getRowType,
-      expandOutputType,
       ImmutableBitSet.of(0, 1, 2, 3),
       ImmutableList.of(
         ImmutableBitSet.of(0),
@@ -131,7 +128,7 @@ class FlinkRelMdUniqueKeysTest extends FlinkRelMdHandlerTestBase {
         ImmutableBitSet.of(3)),
       Array.empty[Integer])
     val expand = new LogicalExpand(cluster, studentLogicalScan.getTraitSet,
-      studentLogicalScan, expandOutputType, expandProjects, 7)
+      studentLogicalScan, expandProjects, 7)
     assertNull(mq.getUniqueKeys(expand))
   }
 
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/nodes/calcite/ExpandTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/nodes/calcite/ExpandTest.scala
new file mode 100644
index 00000000000..1aab8c1bdfa
--- /dev/null
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/nodes/calcite/ExpandTest.scala
@@ -0,0 +1,106 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.planner.plan.nodes.calcite
+
+import org.apache.flink.table.api.DataTypes
+import org.apache.flink.table.planner.JBigDecimal
+
+import org.apache.calcite.avatica.util.ByteString
+import org.apache.calcite.util.TimestampString
+import org.apache.calcite.rex.{RexInputRef, RexNode}
+import org.apache.calcite.sql.fun.SqlStdOperatorTable
+import org.junit.Test
+
+import java.util
+
+class ExpandTest extends RelNodeTestBase {
+
+  private val fieldNames = Array("string", "double", "long")
+  private val fieldTypes = Array(
+    DataTypes.STRING().getLogicalType,
+    DataTypes.DOUBLE().getLogicalType,
+    DataTypes.BIGINT().getLogicalType)
+  private lazy val logicalTableScan = buildLogicalTableScan(fieldNames, fieldTypes)
+
+  @Test(expected = classOf[AssertionError])
+  def testUnsupportedExpandProjectsWithDifferentLength(): Unit = {
+    val inputFields = logicalTableScan.getRowType.getFieldList
+    val expandProjects: util.List[util.List[RexNode]] = new util.ArrayList()
+    val rexBuilder = logicalTableScan.getCluster.getRexBuilder
+
+    val project: util.List[RexNode] = new util.ArrayList[RexNode]()
+    project.add(rexBuilder.makeInputRef(inputFields.get(0).getType, 0))
+    project.add(rexBuilder.makeInputRef(inputFields.get(1).getType, 1))
+    project.add(rexBuilder.makeNullLiteral(inputFields.get(2).getType))
+    expandProjects.add(project)
+
+    val project2: util.List[RexNode] = new util.ArrayList[RexNode]()
+    project2.add(rexBuilder.makeInputRef(inputFields.get(0).getType, 0))
+    project2.add(rexBuilder.makeBigintLiteral(new JBigDecimal(1)))
+    expandProjects.add(project2)
+
+    new LogicalExpand(cluster,  logicalTableScan.getTraitSet, logicalTableScan, expandProjects, 2)
+  }
+
+  @Test(expected = classOf[AssertionError])
+  def testUnsupportedExpandProjectsWithRexCall(): Unit = {
+    val inputFields = logicalTableScan.getRowType.getFieldList
+    val expandProjects: util.List[util.List[RexNode]] = new util.ArrayList()
+    val rexBuilder = logicalTableScan.getCluster.getRexBuilder
+
+    val project: util.List[RexNode] = new util.ArrayList[RexNode]()
+    project.add(rexBuilder.makeInputRef(inputFields.get(0).getType, 0))
+    project.add(rexBuilder.makeInputRef(inputFields.get(1).getType, 1))
+    project.add(rexBuilder.makeNullLiteral(inputFields.get(2).getType))
+    expandProjects.add(project)
+
+    val project2: util.List[RexNode] = new util.ArrayList[RexNode]()
+    project2.add(rexBuilder.makeInputRef(inputFields.get(0).getType, 0))
+    project2.add(
+      rexBuilder.makeCall(
+        SqlStdOperatorTable.EQUALS,
+        new RexInputRef(0, logicalTableScan.getRowType),
+        new RexInputRef(1, logicalTableScan.getRowType)))
+    project2.add(rexBuilder.makeBigintLiteral(new JBigDecimal(1)))
+    expandProjects.add(project2)
+
+    new LogicalExpand(cluster,  logicalTableScan.getTraitSet, logicalTableScan, expandProjects, 2)
+  }
+
+  @Test(expected = classOf[AssertionError])
+  def testUnsupportedExpandProjectsWithNoCommonTypes(): Unit = {
+    val inputFields = logicalTableScan.getRowType.getFieldList
+    val expandProjects: util.List[util.List[RexNode]] = new util.ArrayList()
+    val rexBuilder = logicalTableScan.getCluster.getRexBuilder
+
+    val project: util.List[RexNode] = new util.ArrayList[RexNode]()
+    project.add(rexBuilder.makeInputRef(inputFields.get(0).getType, 0))
+    project.add(rexBuilder.makeBinaryLiteral(ByteString.of("10001", 2)))
+    project.add(rexBuilder.makeNullLiteral(inputFields.get(2).getType))
+    expandProjects.add(project)
+
+    val project2: util.List[RexNode] = new util.ArrayList[RexNode]()
+    project2.add(rexBuilder.makeInputRef(inputFields.get(0).getType, 0))
+    project2.add(rexBuilder.makeTimestampLiteral(new TimestampString("2021-03-23 23:11:17.123"), 3))
+    project2.add(rexBuilder.makeBigintLiteral(new JBigDecimal(1)))
+    expandProjects.add(project2)
+
+    new LogicalExpand(cluster,  logicalTableScan.getTraitSet, logicalTableScan, expandProjects, 2)
+  }
+}
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/nodes/calcite/RelNodeTestBase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/nodes/calcite/RelNodeTestBase.scala
new file mode 100644
index 00000000000..d145627558c
--- /dev/null
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/nodes/calcite/RelNodeTestBase.scala
@@ -0,0 +1,93 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.planner.plan.nodes.calcite
+
+import org.apache.flink.table.api.TableConfig
+import org.apache.flink.table.catalog.{CatalogManager, FunctionCatalog}
+import org.apache.flink.table.module.ModuleManager
+import org.apache.flink.table.planner.calcite.{FlinkRelBuilder, FlinkTypeFactory, FlinkTypeSystem}
+import org.apache.flink.table.planner.delegation.PlannerContext
+import org.apache.flink.table.planner.plan.metadata.MockMetaTable
+import org.apache.flink.table.planner.plan.stats.FlinkStatistic
+import org.apache.flink.table.planner.plan.`trait`.FlinkRelDistributionTraitDef
+import org.apache.flink.table.types.logical.LogicalType
+import org.apache.flink.table.utils.CatalogManagerMocks
+
+import org.apache.calcite.jdbc.CalciteSchema
+import org.apache.calcite.plan.{Convention, ConventionTraitDef, RelOptCluster, RelTraitSet}
+import org.apache.calcite.rel.hint.RelHint
+import org.apache.calcite.rel.logical.LogicalTableScan
+import org.apache.calcite.rel.RelCollationTraitDef
+import org.apache.calcite.rex.RexBuilder
+import org.apache.calcite.schema.SchemaPlus
+import org.junit.Before
+
+import java.util
+
+/**
+ * A base class for rel node test.
+ * TODO refactor the metadata test to extract the common logic for all related tests.
+ */
+class RelNodeTestBase {
+  val tableConfig = new TableConfig()
+  val rootSchema: SchemaPlus = CalciteSchema.createRootSchema(true, false).plus()
+  val catalogManager: CatalogManager = CatalogManagerMocks.createEmptyCatalogManager()
+  val moduleManager = new ModuleManager
+
+  val plannerContext: PlannerContext = new PlannerContext(
+    tableConfig,
+    new FunctionCatalog(tableConfig, catalogManager, moduleManager),
+    catalogManager,
+    CalciteSchema.from(rootSchema),
+    util.Arrays.asList(
+      ConventionTraitDef.INSTANCE,
+      FlinkRelDistributionTraitDef.INSTANCE,
+      RelCollationTraitDef.INSTANCE
+      )
+    )
+
+  val typeFactory: FlinkTypeFactory = plannerContext.getTypeFactory
+  var relBuilder: FlinkRelBuilder = _
+  var rexBuilder: RexBuilder = _
+  var cluster: RelOptCluster = _
+  var logicalTraits: RelTraitSet = _
+
+  @Before
+  def setUp(): Unit = {
+    relBuilder = plannerContext.createRelBuilder("default_catalog", "default_database")
+    rexBuilder = relBuilder.getRexBuilder
+    cluster = relBuilder.getCluster
+    logicalTraits = cluster.traitSetOf(Convention.NONE)
+  }
+
+  /**
+   * Build a [[LogicalTableScan]] based on a [[MockMetaTable]] using given field names and types.
+    @param fieldNames String array
+   * @param fieldTypes [[LogicalType]] array
+   * @return a [[LogicalTableScan]]
+   */
+  def buildLogicalTableScan(
+      fieldNames: Array[String],
+      fieldTypes: Array[LogicalType]): LogicalTableScan = {
+    val flinkTypeFactory = new FlinkTypeFactory(new FlinkTypeSystem)
+    val rowType = flinkTypeFactory.buildRelNodeRowType(fieldNames, fieldTypes)
+    val table = new MockMetaTable(rowType, FlinkStatistic.UNKNOWN)
+    LogicalTableScan.create(cluster, table, new util.ArrayList[RelHint]())
+  }
+}
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/stream/sql/agg/GroupingSetsTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/stream/sql/agg/GroupingSetsTest.scala
index 738f1dd338c..0b989949c81 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/stream/sql/agg/GroupingSetsTest.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/stream/sql/agg/GroupingSetsTest.scala
@@ -23,11 +23,11 @@ import org.apache.flink.table.api._
 import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil
 import org.apache.flink.table.planner.utils.{TableTestBase, TableTestUtil}
 
+import java.sql.Date
+
 import org.junit.Assert.assertEquals
 import org.junit.Test
 
-import java.sql.Date
-
 class GroupingSetsTest extends TableTestBase {
 
   private val util = streamTestUtil()
@@ -74,6 +74,37 @@ class GroupingSetsTest extends TableTestBase {
     util.verifyExecPlan(sqlQuery)
   }
 
+  @Test
+  def testGroupingSetsOutputsNullability(): Unit = {
+    // prepare a source table contains a non-null column
+    val sourceDDL =
+      s"""
+         |create table t1(
+         |  a int,
+         |  b varchar,
+         |  c varchar not null,
+         |  d bigint
+         |) with (
+         |  'connector' = 'filesystem',
+         |  'path' = '/to/my/path1',
+         |  'format' = 'testcsv'
+         |)
+      """.stripMargin
+    util.tableEnv.executeSql(sourceDDL)
+
+    val sqlQuery =
+      """
+        |SELECT
+        | a,
+        | b,
+        | coalesce(c, 'empty'),
+        | avg(d)
+        |FROM t1
+        |GROUP BY GROUPING SETS ((a, b), (a, b, c))
+      """.stripMargin
+    util.verifyExecPlan(sqlQuery)
+  }
+
   @Test
   def testCube(): Unit = {
     val sqlQuery =
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/agg/GroupingSetsITCase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/agg/GroupingSetsITCase.scala
index a9c1be679c2..ce976a82bd8 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/agg/GroupingSetsITCase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/agg/GroupingSetsITCase.scala
@@ -147,6 +147,46 @@ class GroupingSetsITCase extends BatchTestBase {
     )
   }
 
+  @Test
+  def testBooleanColumnOnGroupingSets(): Unit = {
+    checkResult(
+      s"""
+         |select
+         |  gender, city, manager, count(*) as cnt
+         |from emps group by grouping sets ((city), (gender, city, manager))
+         |""".stripMargin,
+      Seq(
+        row("F", "Vancouver", true, 1),
+        row("F", null, true, 1),
+        row("M", "San Francisco", false, 1),
+        row("M", "Vancouver", true, 1),
+        row(null, "San Francisco", null, 1),
+        row(null, "Vancouver", null, 2),
+        row(null, null, false, 1),
+        row(null, null, null, 2)))
+  }
+
+  @Test
+  def testCoalesceOnGroupingSets(): Unit = {
+    checkResult(
+      s"""
+         |select
+         |  gender, city, coalesce(deptno, -1) as deptno, count(*) as cnt
+         |from emps group by grouping sets ((gender, city), (gender, city, deptno))
+         |""".stripMargin,
+      Seq(
+        row("F", "Vancouver", -1, 1),
+        row("F", "Vancouver", 40, 1),
+        row("F", null, -1, 1),
+        row("F", null, 20, 1),
+        row("M", "San Francisco", -1, 1),
+        row("M", "San Francisco", 20, 1),
+        row("M", "Vancouver", -1, 1),
+        row("M", "Vancouver", 40, 1),
+        row(null, null, -1, 1),
+        row(null, null, 10, 1)))
+  }
+
   @Test
   def testCube(): Unit = {
     checkResult(
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/AggregateITCase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/AggregateITCase.scala
index 46e5e2a6612..8aed33bc9a1 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/AggregateITCase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/AggregateITCase.scala
@@ -1376,4 +1376,70 @@ class AggregateITCase(
       "3,z")
     assertEquals(expected2.sorted, sink2.getRetractResults.sorted)
   }
+
+  @Test
+  def testCoalesceOnGroupingSets(): Unit = {
+    val empsData = List(
+      (100L, "Fred", 10, null, null, 40L, 25, true, false),
+      (110L, "Eric", 20, "M", "San Francisco", 3L, 80, null, false),
+      (110L, "John", 40, "M", "Vancouver", 2L, null, false, true),
+      (120L, "Wilma", 20, "F", null, 1L, 5, null, true),
+      (130L, "Alice", 40, "F", "Vancouver", 2L, null, false, true))
+    val tableA = failingDataSource(empsData)
+      .toTable(tEnv, 'empno, 'name, 'deptno, 'gender, 'city, 'empid, 'age, 'slacker, 'manager)
+    tEnv.registerTable("emps", tableA)
+    val sql =
+      s"""
+         |select
+         |  gender, city, coalesce(deptno, -1), count(*) as cnt
+         |from emps group by grouping sets ((gender, city), (gender, city, deptno))
+         |""".stripMargin
+    val sink = new TestingRetractSink
+    tEnv.sqlQuery(sql).toRetractStream[Row].addSink(sink).setParallelism(1)
+    env.execute()
+    val expected = List(
+      "F,Vancouver,-1,1",
+      "F,Vancouver,40,1",
+      "F,null,-1,1",
+      "F,null,20,1",
+      "M,San Francisco,-1,1",
+      "M,San Francisco,20,1",
+      "M,Vancouver,-1,1",
+      "M,Vancouver,40,1",
+      "null,null,-1,1",
+      "null,null,10,1")
+    assertEquals(expected.sorted, sink.getRetractResults.sorted)
+  }
+
+  @Test
+  def testBooleanColumnOnGroupingSets(): Unit = {
+    val empsData = List(
+      (100L, "Fred", 10, null, null, 40L, 25, true, false),
+      (110L, "Eric", 20, "M", "San Francisco", 3L, 80, null, false),
+      (110L, "John", 40, "M", "Vancouver", 2L, null, false, true),
+      (120L, "Wilma", 20, "F", null, 1L, 5, null, true),
+      (130L, "Alice", 40, "F", "Vancouver", 2L, null, false, true))
+    val tableA = failingDataSource(empsData)
+      .toTable(tEnv, 'empno, 'name, 'deptno, 'gender, 'city, 'empid, 'age, 'slacker, 'manager)
+    tEnv.registerTable("emps", tableA)
+    val sql =
+      s"""
+         |select
+         |  gender, city, manager, count(*) as cnt
+         |from emps group by grouping sets ((city), (gender, city, manager))
+         |""".stripMargin
+    val sink = new TestingRetractSink
+    tEnv.sqlQuery(sql).toRetractStream[Row].addSink(sink).setParallelism(1)
+    env.execute()
+    val expected = List(
+      "F,Vancouver,true,1",
+      "F,null,true,1",
+      "M,San Francisco,false,1",
+      "M,Vancouver,true,1",
+      "null,San Francisco,null,1",
+      "null,Vancouver,null,2",
+      "null,null,false,1",
+      "null,null,null,2")
+    assertEquals(expected.sorted, sink.getRetractResults.sorted)
+  }
 }
