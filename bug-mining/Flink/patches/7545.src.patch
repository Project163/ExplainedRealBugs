diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/AdaptiveBatchScheduler.java b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/AdaptiveBatchScheduler.java
index d9578e7be10..6111e69dc41 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/AdaptiveBatchScheduler.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/AdaptiveBatchScheduler.java
@@ -627,15 +627,8 @@ public class AdaptiveBatchScheduler extends DefaultScheduler {
             final ExecutionJobVertex jobVertex, List<BlockingResultInfo> inputs) {
         int vertexInitialParallelism = jobVertex.getParallelism();
         ForwardGroup forwardGroup = forwardGroupsByJobVertexId.get(jobVertex.getJobVertexId());
-        if (!jobVertex.isParallelismDecided()
-                && forwardGroup != null
-                && forwardGroup.isParallelismDecided()) {
-            vertexInitialParallelism = forwardGroup.getParallelism();
-            log.info(
-                    "Parallelism of JobVertex: {} ({}) is decided to be {} according to forward group's parallelism.",
-                    jobVertex.getName(),
-                    jobVertex.getJobVertexId(),
-                    vertexInitialParallelism);
+        if (!jobVertex.isParallelismDecided() && forwardGroup != null) {
+            checkState(!forwardGroup.isParallelismDecided());
         }
 
         int vertexMinParallelism = ExecutionConfig.PARALLELISM_DEFAULT;
@@ -674,6 +667,36 @@ public class AdaptiveBatchScheduler extends DefaultScheduler {
 
         if (forwardGroup != null && !forwardGroup.isParallelismDecided()) {
             forwardGroup.setParallelism(parallelismAndInputInfos.getParallelism());
+
+            // When the parallelism for a forward group is determined, we ensure that the
+            // parallelism for all job vertices within that group is also set.
+            // This approach ensures that each forward edge produces single subpartition.
+            //
+            // This setting is crucial because the Sink V2 committer relies on the interplay
+            // between the CommittableSummary and the CommittableWithLineage, which are sent by
+            // the upstream Sink V2 Writer. The committer expects to receive CommittableSummary
+            // before CommittableWithLineage.
+            //
+            // If the number of subpartitions produced by a forward edge is greater than one,
+            // the ordering of these elements received by the committer cannot be assured, which
+            // would break the assumption that CommittableSummary is received before
+            // CommittableWithLineage.
+            for (JobVertexID jobVertexId : forwardGroup.getJobVertexIds()) {
+                ExecutionJobVertex executionJobVertex = getExecutionJobVertex(jobVertexId);
+                if (!executionJobVertex.isParallelismDecided()) {
+                    log.info(
+                            "Parallelism of JobVertex: {} ({}) is decided to be {} according to forward group's parallelism.",
+                            executionJobVertex.getName(),
+                            executionJobVertex.getJobVertexId(),
+                            parallelismAndInputInfos.getParallelism());
+                    changeJobVertexParallelism(
+                            executionJobVertex, parallelismAndInputInfos.getParallelism());
+                } else {
+                    checkState(
+                            parallelismAndInputInfos.getParallelism()
+                                    == executionJobVertex.getParallelism());
+                }
+            }
         }
 
         return parallelismAndInputInfos;
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adaptivebatch/AdaptiveBatchSchedulerTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adaptivebatch/AdaptiveBatchSchedulerTest.java
index e1a0a4714f9..2a5da861067 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adaptivebatch/AdaptiveBatchSchedulerTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adaptivebatch/AdaptiveBatchSchedulerTest.java
@@ -186,20 +186,18 @@ class AdaptiveBatchSchedulerTest {
         // trigger source finished.
         transitionExecutionsState(scheduler, ExecutionState.FINISHED, source);
         assertThat(mapExecutionJobVertex.getParallelism()).isEqualTo(5);
-        assertThat(sinkExecutionJobVertex.getParallelism()).isEqualTo(-1);
+        assertThat(sinkExecutionJobVertex.getParallelism()).isEqualTo(5);
+        // check that the jobGraph is updated
+        assertThat(sink.getParallelism()).isEqualTo(5);
 
         // trigger map finished.
         transitionExecutionsState(scheduler, ExecutionState.FINISHED, map);
         assertThat(mapExecutionJobVertex.getParallelism()).isEqualTo(5);
         assertThat(sinkExecutionJobVertex.getParallelism()).isEqualTo(5);
 
-        // check that the jobGraph is updated
-        assertThat(sink.getParallelism()).isEqualTo(5);
-
         // check aggregatedInputDataBytes of each ExecutionVertex calculated. Total number of
-        // subpartitions of map is ceil(128 / 5) * 5 = 130, so total bytes sink consume is 130 *
-        // SUBPARTITION_BYTES = 13_000L.
-        checkAggregatedInputDataBytesIsCalculated(sinkExecutionJobVertex, 13_000L);
+        // subpartitions of map is 5, so total bytes sink consume is 5 * SUBPARTITION_BYTES = 500L.
+        checkAggregatedInputDataBytesIsCalculated(sinkExecutionJobVertex, 500L);
     }
 
     @Test
diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/translators/SinkTransformationTranslator.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/translators/SinkTransformationTranslator.java
index ffe0f3e28b1..5f6bec1fda2 100644
--- a/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/translators/SinkTransformationTranslator.java
+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/translators/SinkTransformationTranslator.java
@@ -357,7 +357,9 @@ public class SinkTransformationTranslator<Input, Output>
                     // In this case, the subTransformation does not contain any customized
                     // parallelism value and will therefore inherit the parallelism value
                     // from the sinkTransformation.
-                    subTransformation.setParallelism(transformation.getParallelism());
+                    subTransformation.setParallelism(
+                            transformation.getParallelism(),
+                            transformation.isParallelismConfigured());
                 }
 
                 if (subTransformation.getMaxParallelism() < 0
diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/graph/SinkTransformationTranslatorITCaseBase.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/graph/SinkTransformationTranslatorITCaseBase.java
index 62157911abe..fc706e351d8 100644
--- a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/graph/SinkTransformationTranslatorITCaseBase.java
+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/graph/SinkTransformationTranslatorITCaseBase.java
@@ -119,6 +119,24 @@ abstract class SinkTransformationTranslatorITCaseBase<SinkT> {
                 -1);
     }
 
+    @TestTemplate
+    void testParallelismConfigured() {
+        testParallelismConfiguredInternal(true);
+
+        testParallelismConfiguredInternal(false);
+    }
+
+    private void testParallelismConfiguredInternal(boolean setSinkParallelism) {
+        final StreamGraph streamGraph =
+                buildGraph(sinkWithCommitter(), runtimeExecutionMode, setSinkParallelism);
+
+        final StreamNode writerNode = findWriter(streamGraph);
+        final StreamNode committerNode = findCommitter(streamGraph);
+
+        assertThat(writerNode.isParallelismConfigured()).isEqualTo(setSinkParallelism);
+        assertThat(committerNode.isParallelismConfigured()).isEqualTo(setSinkParallelism);
+    }
+
     StreamNode findWriter(StreamGraph streamGraph) {
         return findNodeName(
                 streamGraph, name -> name.contains("Writer") && !name.contains("Committer"));
@@ -196,6 +214,11 @@ abstract class SinkTransformationTranslatorITCaseBase<SinkT> {
     }
 
     StreamGraph buildGraph(SinkT sink, RuntimeExecutionMode runtimeExecutionMode) {
+        return buildGraph(sink, runtimeExecutionMode, true);
+    }
+
+    StreamGraph buildGraph(
+            SinkT sink, RuntimeExecutionMode runtimeExecutionMode, boolean setSinkParallelism) {
         StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
 
         final Configuration config = new Configuration();
@@ -203,16 +226,19 @@ abstract class SinkTransformationTranslatorITCaseBase<SinkT> {
         env.configure(config, getClass().getClassLoader());
         final DataStreamSource<Integer> src = env.fromElements(1, 2);
         final DataStreamSink<Integer> dataStreamSink = sinkTo(src.rebalance(), sink);
-        setSinkProperty(dataStreamSink);
+        setSinkProperty(dataStreamSink, setSinkParallelism);
         // Trigger the plan generation but do not clear the transformations
         env.getExecutionPlan();
         return env.getStreamGraph();
     }
 
-    private void setSinkProperty(DataStreamSink<Integer> dataStreamSink) {
+    private void setSinkProperty(
+            DataStreamSink<Integer> dataStreamSink, boolean setSinkParallelism) {
         dataStreamSink.name(NAME);
         dataStreamSink.uid(UID);
-        dataStreamSink.setParallelism(SinkTransformationTranslatorITCaseBase.PARALLELISM);
+        if (setSinkParallelism) {
+            dataStreamSink.setParallelism(SinkTransformationTranslatorITCaseBase.PARALLELISM);
+        }
         dataStreamSink.slotSharingGroup(SLOT_SHARE_GROUP);
     }
 
