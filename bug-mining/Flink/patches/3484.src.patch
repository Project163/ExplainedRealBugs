diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShim.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShim.java
index f0edb503e0a..6b235a6b141 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShim.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShim.java
@@ -45,6 +45,7 @@ import org.apache.thrift.TException;
 
 import java.io.IOException;
 import java.io.Serializable;
+import java.time.LocalDate;
 import java.time.LocalDateTime;
 import java.util.List;
 import java.util.Map;
@@ -221,4 +222,14 @@ public interface HiveShim extends Serializable {
 	 * Converts a hive timestamp instance to LocalDateTime which is expected by DataFormatConverter.
 	 */
 	LocalDateTime toFlinkTimestamp(Object hiveTimestamp);
+
+	/**
+	 * Converts a Flink date instance to what's expected by Hive.
+	 */
+	Object toHiveDate(Object flinkDate);
+
+	/**
+	 * Converts a hive date instance to LocalDate which is expected by DataFormatConverter.
+	 */
+	LocalDate toFlinkDate(Object hiveDate);
 }
diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShimV100.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShimV100.java
index 02f20bce3c6..b754a955ec2 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShimV100.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShimV100.java
@@ -76,7 +76,9 @@ import java.io.IOException;
 import java.lang.reflect.Constructor;
 import java.lang.reflect.InvocationTargetException;
 import java.lang.reflect.Method;
+import java.sql.Date;
 import java.sql.Timestamp;
+import java.time.LocalDate;
 import java.time.LocalDateTime;
 import java.util.ArrayList;
 import java.util.Collections;
@@ -364,9 +366,29 @@ public class HiveShimV100 implements HiveShim {
 		return ((Timestamp) hiveTimestamp).toLocalDateTime();
 	}
 
+	@Override
+	public Object toHiveDate(Object flinkDate) {
+		ensureSupportedFlinkDate(flinkDate);
+		return flinkDate instanceof Date ? flinkDate : Date.valueOf((LocalDate) flinkDate);
+	}
+
+	@Override
+	public LocalDate toFlinkDate(Object hiveDate) {
+		Preconditions.checkArgument(hiveDate instanceof Date,
+				"Expecting Hive timestamp to be an instance of %s, but actually got %s",
+				Date.class.getName(), hiveDate.getClass().getName());
+		return ((Date) hiveDate).toLocalDate();
+	}
+
 	void ensureSupportedFlinkTimestamp(Object flinkTimestamp) {
 		Preconditions.checkArgument(flinkTimestamp instanceof Timestamp || flinkTimestamp instanceof LocalDateTime,
 				"Only support converting %s or %s to Hive timestamp, but not %s",
 				Timestamp.class.getName(), LocalDateTime.class.getName(), flinkTimestamp.getClass().getName());
 	}
+
+	void ensureSupportedFlinkDate(Object flinkDate) {
+		Preconditions.checkArgument(flinkDate instanceof Date || flinkDate instanceof LocalDate,
+				"Only support converting %s or %s to Hive date, but not %s",
+				Date.class.getName(), LocalDate.class.getName(), flinkDate.getClass().getName());
+	}
 }
diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShimV310.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShimV310.java
index 40772dbc7e6..86c702f5d6f 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShimV310.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShimV310.java
@@ -38,7 +38,9 @@ import java.lang.reflect.Constructor;
 import java.lang.reflect.Field;
 import java.lang.reflect.InvocationTargetException;
 import java.lang.reflect.Method;
+import java.sql.Date;
 import java.sql.Timestamp;
+import java.time.LocalDate;
 import java.time.LocalDateTime;
 import java.util.HashSet;
 import java.util.List;
@@ -50,25 +52,38 @@ import java.util.Set;
  */
 public class HiveShimV310 extends HiveShimV235 {
 
+	// timestamp classes
 	private static Class hiveTimestampClz;
 	private static Constructor hiveTimestampConstructor;
 	private static Field hiveTimestampLocalDateTime;
-	private static boolean hiveTimestampInited;
 
-	private static void initTimestampClz() {
-		if (!hiveTimestampInited) {
+	// date classes
+	private static Class hiveDateClz;
+	private static Constructor hiveDateConstructor;
+	private static Field hiveDateLocalDate;
+
+	private static boolean hiveClassesInited;
+
+	private static void initDateTimeClasses() {
+		if (!hiveClassesInited) {
 			synchronized (HiveShimV310.class) {
-				if (!hiveTimestampInited) {
+				if (!hiveClassesInited) {
 					try {
 						hiveTimestampClz = Class.forName("org.apache.hadoop.hive.common.type.Timestamp");
 						hiveTimestampConstructor = hiveTimestampClz.getDeclaredConstructor(LocalDateTime.class);
 						hiveTimestampConstructor.setAccessible(true);
 						hiveTimestampLocalDateTime = hiveTimestampClz.getDeclaredField("localDateTime");
 						hiveTimestampLocalDateTime.setAccessible(true);
+
+						hiveDateClz = Class.forName("org.apache.hadoop.hive.common.type.Date");
+						hiveDateConstructor = hiveDateClz.getDeclaredConstructor(LocalDate.class);
+						hiveDateConstructor.setAccessible(true);
+						hiveDateLocalDate = hiveDateClz.getDeclaredField("localDate");
+						hiveDateLocalDate.setAccessible(true);
 					} catch (ClassNotFoundException | NoSuchMethodException | NoSuchFieldException e) {
 						throw new FlinkHiveException("Failed to get Hive timestamp class and constructor", e);
 					}
-					hiveTimestampInited = true;
+					hiveClassesInited = true;
 				}
 			}
 		}
@@ -105,16 +120,13 @@ public class HiveShimV310 extends HiveShimV235 {
 
 	@Override
 	public Class<?> getDateDataTypeClass() {
-		try {
-			return Class.forName("org.apache.hadoop.hive.common.type.Date");
-		} catch (ClassNotFoundException e) {
-			throw new CatalogException("Failed to find class org.apache.hadoop.hive.common.type.Date", e);
-		}
+		initDateTimeClasses();
+		return hiveDateClz;
 	}
 
 	@Override
 	public Class<?> getTimestampDataTypeClass() {
-		initTimestampClz();
+		initDateTimeClasses();
 		return hiveTimestampClz;
 	}
 
@@ -171,7 +183,7 @@ public class HiveShimV310 extends HiveShimV235 {
 	@Override
 	public Object toHiveTimestamp(Object flinkTimestamp) {
 		ensureSupportedFlinkTimestamp(flinkTimestamp);
-		initTimestampClz();
+		initDateTimeClasses();
 		if (flinkTimestamp instanceof Timestamp) {
 			flinkTimestamp = ((Timestamp) flinkTimestamp).toLocalDateTime();
 		}
@@ -184,7 +196,7 @@ public class HiveShimV310 extends HiveShimV235 {
 
 	@Override
 	public LocalDateTime toFlinkTimestamp(Object hiveTimestamp) {
-		initTimestampClz();
+		initDateTimeClasses();
 		Preconditions.checkArgument(hiveTimestampClz.isAssignableFrom(hiveTimestamp.getClass()),
 				"Expecting Hive timestamp to be an instance of %s, but actually got %s",
 				hiveTimestampClz.getName(), hiveTimestamp.getClass().getName());
@@ -194,4 +206,31 @@ public class HiveShimV310 extends HiveShimV235 {
 			throw new FlinkHiveException("Failed to convert to Flink timestamp", e);
 		}
 	}
+
+	@Override
+	public Object toHiveDate(Object flinkDate) {
+		ensureSupportedFlinkDate(flinkDate);
+		initDateTimeClasses();
+		if (flinkDate instanceof Date) {
+			flinkDate = ((Date) flinkDate).toLocalDate();
+		}
+		try {
+			return hiveDateConstructor.newInstance(flinkDate);
+		} catch (InstantiationException | IllegalAccessException | InvocationTargetException e) {
+			throw new FlinkHiveException("Failed to convert to Hive date", e);
+		}
+	}
+
+	@Override
+	public LocalDate toFlinkDate(Object hiveDate) {
+		initDateTimeClasses();
+		Preconditions.checkArgument(hiveDateClz.isAssignableFrom(hiveDate.getClass()),
+				"Expecting Hive date to be an instance of %s, but actually got %s",
+				hiveDateClz.getName(), hiveDate.getClass().getName());
+		try {
+			return (LocalDate) hiveDateLocalDate.get(hiveDate);
+		} catch (IllegalAccessException e) {
+			throw new FlinkHiveException("Failed to convert to Flink date", e);
+		}
+	}
 }
diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveReflectionUtils.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveReflectionUtils.java
index 4ff42dad90c..401a750d29e 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveReflectionUtils.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveReflectionUtils.java
@@ -73,15 +73,6 @@ public class HiveReflectionUtils {
 		}
 	}
 
-	public static Object convertToHiveDate(HiveShim hiveShim, String s) throws FlinkHiveUDFException {
-		try {
-			Method method = hiveShim.getDateDataTypeClass().getMethod("valueOf", String.class);
-			return method.invoke(null, s);
-		} catch (NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {
-			throw new FlinkHiveUDFException("Failed to invoke Hive's Date.valueOf()", e);
-		}
-	}
-
 	public static Object invokeMethod(Class clz, Object obj, String methodName, Class[] argClz, Object[] args)
 			throws NoSuchMethodException, InvocationTargetException, IllegalAccessException {
 		Method method;
diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/functions/hive/conversion/HiveInspectors.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/functions/hive/conversion/HiveInspectors.java
index bbdc1d5dcf6..ec3407c1f36 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/functions/hive/conversion/HiveInspectors.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/functions/hive/conversion/HiveInspectors.java
@@ -139,9 +139,10 @@ public class HiveInspectors {
 					inspector instanceof LongObjectInspector ||
 					inspector instanceof FloatObjectInspector ||
 					inspector instanceof DoubleObjectInspector ||
-					inspector instanceof DateObjectInspector ||
 					inspector instanceof BinaryObjectInspector) {
 				conversion = IdentityConversion.INSTANCE;
+			} else if (inspector instanceof DateObjectInspector) {
+				conversion = hiveShim::toHiveDate;
 			} else if (inspector instanceof TimestampObjectInspector) {
 				conversion = hiveShim::toHiveTimestamp;
 			} else if (inspector instanceof HiveCharObjectInspector) {
@@ -239,11 +240,13 @@ public class HiveInspectors {
 					inspector instanceof LongObjectInspector ||
 					inspector instanceof FloatObjectInspector ||
 					inspector instanceof DoubleObjectInspector ||
-					inspector instanceof DateObjectInspector ||
 					inspector instanceof BinaryObjectInspector) {
 
 				PrimitiveObjectInspector poi = (PrimitiveObjectInspector) inspector;
 				return poi.getPrimitiveJavaObject(data);
+			} else if (inspector instanceof DateObjectInspector) {
+				PrimitiveObjectInspector poi = (PrimitiveObjectInspector) inspector;
+				return hiveShim.toFlinkDate(poi.getPrimitiveJavaObject(data));
 			} else if (inspector instanceof TimestampObjectInspector) {
 				PrimitiveObjectInspector poi = (PrimitiveObjectInspector) inspector;
 				return hiveShim.toFlinkTimestamp(poi.getPrimitiveJavaObject(data));
diff --git a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/TableEnvHiveConnectorTest.java b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/TableEnvHiveConnectorTest.java
index cdebe878966..6a3beedff1f 100644
--- a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/TableEnvHiveConnectorTest.java
+++ b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/TableEnvHiveConnectorTest.java
@@ -44,7 +44,9 @@ import org.junit.BeforeClass;
 import org.junit.Test;
 import org.junit.runner.RunWith;
 
+import java.sql.Date;
 import java.sql.Timestamp;
+import java.time.LocalDate;
 import java.time.LocalDateTime;
 import java.util.Arrays;
 import java.util.Collections;
@@ -142,19 +144,21 @@ public class TableEnvHiveConnectorTest {
 		} else {
 			suffix = "stored as " + format;
 		}
-		hiveShell.execute("create table db1.src (i int,s string,ts timestamp) " + suffix);
-		hiveShell.execute("create table db1.dest (i int,s string,ts timestamp) " + suffix);
+		hiveShell.execute("create table db1.src (i int,s string,ts timestamp,dt date) " + suffix);
+		hiveShell.execute("create table db1.dest (i int,s string,ts timestamp,dt date) " + suffix);
 
 		// prepare source data with Hive
 		// TABLE keyword in INSERT INTO is mandatory prior to 1.1.0
-		hiveShell.execute("insert into table db1.src values (1,'a','2018-08-20 00:00:00.1'),(2,'b','2019-08-26 00:00:00.1')");
+		hiveShell.execute("insert into table db1.src values " +
+				"(1,'a','2018-08-20 00:00:00.1','2018-08-20'),(2,'b','2019-08-26 00:00:00.1','2019-08-26')");
 
 		// populate dest table with source table
 		tableEnv.sqlUpdate("insert into db1.dest select * from db1.src");
 		tableEnv.execute("test_" + format);
 
 		// verify data on hive side
-		verifyHiveQueryResult("select * from db1.dest", Arrays.asList("1\ta\t2018-08-20 00:00:00.1", "2\tb\t2019-08-26 00:00:00.1"));
+		verifyHiveQueryResult("select * from db1.dest",
+				Arrays.asList("1\ta\t2018-08-20 00:00:00.1\t2018-08-20", "2\tb\t2019-08-26 00:00:00.1\t2019-08-26"));
 
 		hiveShell.execute("drop database db1 cascade");
 	}
@@ -394,6 +398,31 @@ public class TableEnvHiveConnectorTest {
 		}
 	}
 
+	@Test
+	public void testDate() throws Exception {
+		hiveShell.execute("create database db1");
+		try {
+			hiveShell.execute("create table db1.src (dt date)");
+			hiveShell.execute("create table db1.dest (dt date)");
+			HiveTestUtils.createTextTableInserter(hiveShell, "db1", "src")
+					.addRow(new Object[]{Date.valueOf("2019-12-09")})
+					.addRow(new Object[]{Date.valueOf("2019-12-12")})
+					.commit();
+			TableEnvironment tableEnv = getTableEnvWithHiveCatalog();
+			// test read date from hive
+			List<Row> results = HiveTestUtils.collectTable(tableEnv, tableEnv.sqlQuery("select * from db1.src"));
+			assertEquals(2, results.size());
+			assertEquals(LocalDate.of(2019, 12, 9), results.get(0).getField(0));
+			assertEquals(LocalDate.of(2019, 12, 12), results.get(1).getField(0));
+			// test write date to hive
+			tableEnv.sqlUpdate("insert into db1.dest select max(dt) from db1.src");
+			tableEnv.execute("write date to hive");
+			verifyHiveQueryResult("select * from db1.dest", Collections.singletonList("2019-12-12"));
+		} finally {
+			hiveShell.execute("drop database db1 cascade");
+		}
+	}
+
 	private TableEnvironment getTableEnvWithHiveCatalog() {
 		TableEnvironment tableEnv = HiveTestUtils.createTableEnv();
 		tableEnv.registerCatalog(hiveCatalog.getName(), hiveCatalog);
diff --git a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/catalog/hive/HiveCatalogUseBlinkITCase.java b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/catalog/hive/HiveCatalogUseBlinkITCase.java
index a40e45fe3e0..b8ee6aa19d4 100644
--- a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/catalog/hive/HiveCatalogUseBlinkITCase.java
+++ b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/catalog/hive/HiveCatalogUseBlinkITCase.java
@@ -49,6 +49,7 @@ import org.apache.flink.util.FileUtils;
 import com.klarna.hiverunner.HiveShell;
 import com.klarna.hiverunner.annotations.HiveSQL;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.udf.UDFMonth;
 import org.apache.hadoop.hive.ql.udf.UDFYear;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum;
 import org.junit.AfterClass;
@@ -63,6 +64,7 @@ import java.io.IOException;
 import java.nio.file.Files;
 import java.nio.file.Path;
 import java.nio.file.Paths;
+import java.sql.Date;
 import java.sql.Timestamp;
 import java.util.ArrayList;
 import java.util.Arrays;
@@ -250,17 +252,45 @@ public class HiveCatalogUseBlinkITCase extends AbstractTestBase {
 				false);
 
 		hiveShell.execute("create table src(ts timestamp)");
-		HiveTestUtils.createTextTableInserter(hiveShell, "default", "src")
-				.addRow(new Object[]{Timestamp.valueOf("2013-07-15 10:00:00")})
-				.addRow(new Object[]{Timestamp.valueOf("2019-05-23 17:32:55")})
-				.commit();
-		TableEnvironment tableEnv = HiveTestUtils.createTableEnv();
-		tableEnv.registerCatalog(hiveCatalog.getName(), hiveCatalog);
-		tableEnv.useCatalog(hiveCatalog.getName());
-
-		List<Row> results = HiveTestUtils.collectTable(tableEnv, tableEnv.sqlQuery("select myyear(ts) as y from src"));
-		Assert.assertEquals(2, results.size());
-		Assert.assertEquals("[2013, 2019]", results.toString());
+		try {
+			HiveTestUtils.createTextTableInserter(hiveShell, "default", "src")
+					.addRow(new Object[]{Timestamp.valueOf("2013-07-15 10:00:00")})
+					.addRow(new Object[]{Timestamp.valueOf("2019-05-23 17:32:55")})
+					.commit();
+			TableEnvironment tableEnv = HiveTestUtils.createTableEnv();
+			tableEnv.registerCatalog(hiveCatalog.getName(), hiveCatalog);
+			tableEnv.useCatalog(hiveCatalog.getName());
+
+			List<Row> results = HiveTestUtils.collectTable(tableEnv, tableEnv.sqlQuery("select myyear(ts) as y from src"));
+			Assert.assertEquals(2, results.size());
+			Assert.assertEquals("[2013, 2019]", results.toString());
+		} finally {
+			hiveShell.execute("drop table src");
+		}
+	}
+
+	@Test
+	public void testDateUDF() throws Exception {
+		hiveCatalog.createFunction(new ObjectPath("default", "mymonth"),
+				new CatalogFunctionImpl(UDFMonth.class.getCanonicalName()),
+				false);
+
+		hiveShell.execute("create table src(dt date)");
+		try {
+			HiveTestUtils.createTextTableInserter(hiveShell, "default", "src")
+					.addRow(new Object[]{Date.valueOf("2019-01-19")})
+					.addRow(new Object[]{Date.valueOf("2019-03-02")})
+					.commit();
+			TableEnvironment tableEnv = HiveTestUtils.createTableEnv();
+			tableEnv.registerCatalog(hiveCatalog.getName(), hiveCatalog);
+			tableEnv.useCatalog(hiveCatalog.getName());
+
+			List<Row> results = HiveTestUtils.collectTable(tableEnv, tableEnv.sqlQuery("select mymonth(dt) as m from src order by m"));
+			Assert.assertEquals(2, results.size());
+			Assert.assertEquals("[1, 3]", results.toString());
+		} finally {
+			hiveShell.execute("drop table src");
+		}
 	}
 
 	private static class JavaToScala implements MapFunction<Tuple2<Boolean, Row>, scala.Tuple2<Boolean, Row>> {
diff --git a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/functions/hive/HiveGenericUDFTest.java b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/functions/hive/HiveGenericUDFTest.java
index 3b315279acc..7af0262d26b 100644
--- a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/functions/hive/HiveGenericUDFTest.java
+++ b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/functions/hive/HiveGenericUDFTest.java
@@ -21,7 +21,6 @@ package org.apache.flink.table.functions.hive;
 import org.apache.flink.table.api.DataTypes;
 import org.apache.flink.table.catalog.hive.client.HiveShim;
 import org.apache.flink.table.catalog.hive.client.HiveShimLoader;
-import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;
 import org.apache.flink.table.functions.hive.util.TestGenericUDFArray;
 import org.apache.flink.table.functions.hive.util.TestGenericUDFStructSize;
 import org.apache.flink.table.types.DataType;
@@ -42,6 +41,7 @@ import org.junit.Test;
 
 import java.lang.reflect.InvocationTargetException;
 import java.math.BigDecimal;
+import java.sql.Date;
 import java.sql.Timestamp;
 import java.util.HashMap;
 
@@ -145,7 +145,7 @@ public class HiveGenericUDFTest {
 			}
 		);
 
-		assertEquals("8", udf.eval(HiveReflectionUtils.convertToHiveDate(hiveShim, "2019-08-31"), constMonth));
+		assertEquals("8", udf.eval(Date.valueOf("2019-08-31"), constMonth));
 	}
 
 	@Test
@@ -274,7 +274,7 @@ public class HiveGenericUDFTest {
 			}
 		);
 
-		assertEquals(-4182, udf.eval(HiveReflectionUtils.convertToHiveDate(hiveShim, d), Timestamp.valueOf(t2)));
+		assertEquals(-4182, udf.eval(Date.valueOf(d), Timestamp.valueOf(t2)));
 
 		// Test invalid char length
 		udf = init(
diff --git a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/functions/hive/HiveSimpleUDFTest.java b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/functions/hive/HiveSimpleUDFTest.java
index c3efa7af375..49cf403b69b 100644
--- a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/functions/hive/HiveSimpleUDFTest.java
+++ b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/functions/hive/HiveSimpleUDFTest.java
@@ -21,7 +21,6 @@ package org.apache.flink.table.functions.hive;
 import org.apache.flink.table.api.DataTypes;
 import org.apache.flink.table.catalog.hive.client.HiveShim;
 import org.apache.flink.table.catalog.hive.client.HiveShimLoader;
-import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;
 import org.apache.flink.table.functions.hive.util.TestHiveUDFArray;
 import org.apache.flink.table.types.DataType;
 
@@ -38,6 +37,7 @@ import org.junit.Test;
 
 import java.io.UnsupportedEncodingException;
 import java.math.BigDecimal;
+import java.sql.Date;
 import java.sql.Timestamp;
 
 import static org.junit.Assert.assertEquals;
@@ -133,8 +133,8 @@ public class HiveSimpleUDFTest {
 			});
 
 		assertEquals(29, udf.eval("1969-07-20"));
-		assertEquals(29, udf.eval(HiveReflectionUtils.convertToHiveDate(hiveShim, "1969-07-20")));
-		assertEquals(29, udf.eval(hiveShim.toHiveTimestamp(Timestamp.valueOf("1969-07-20 00:00:00"))));
+		assertEquals(29, udf.eval(Date.valueOf("1969-07-20")));
+		assertEquals(29, udf.eval(Timestamp.valueOf("1969-07-20 00:00:00")));
 		assertEquals(1, udf.eval("1980-12-31 12:59:59"));
 	}
 
