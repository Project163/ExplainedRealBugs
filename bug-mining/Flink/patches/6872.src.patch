diff --git a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/functions/table/lookup/CachingLookupFunction.java b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/functions/table/lookup/CachingLookupFunction.java
index fb0c3075685..7d034c12f93 100644
--- a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/functions/table/lookup/CachingLookupFunction.java
+++ b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/functions/table/lookup/CachingLookupFunction.java
@@ -20,7 +20,6 @@ package org.apache.flink.table.runtime.functions.table.lookup;
 
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.annotation.VisibleForTesting;
-import org.apache.flink.configuration.Configuration;
 import org.apache.flink.metrics.Counter;
 import org.apache.flink.metrics.SimpleCounter;
 import org.apache.flink.metrics.groups.CacheMetricGroup;
@@ -109,10 +108,6 @@ public class CachingLookupFunction extends LookupFunction {
         }
         // Initialize cache and the delegating function
         cache.open(cacheMetricGroup);
-        if (cache instanceof LookupFullCache) {
-            // TODO add Configuration into FunctionContext
-            ((LookupFullCache) cache).open(new Configuration());
-        }
         if (delegate != null) {
             delegate.open(context);
         }
diff --git a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/functions/table/lookup/fullcache/CacheLoader.java b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/functions/table/lookup/fullcache/CacheLoader.java
index 5b29a772bde..d76d1ab6cb4 100644
--- a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/functions/table/lookup/fullcache/CacheLoader.java
+++ b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/functions/table/lookup/fullcache/CacheLoader.java
@@ -18,7 +18,6 @@
 
 package org.apache.flink.table.runtime.functions.table.lookup.fullcache;
 
-import org.apache.flink.api.common.functions.AbstractRichFunction;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.metrics.Counter;
 import org.apache.flink.metrics.ThreadSafeSimpleCounter;
@@ -33,8 +32,13 @@ import org.slf4j.LoggerFactory;
 
 import java.io.Serializable;
 import java.util.Collection;
+import java.util.concurrent.CompletableFuture;
 import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
 import java.util.concurrent.locks.ReentrantLock;
 
 import static org.apache.flink.runtime.metrics.groups.InternalCacheMetricGroup.UNINITIALIZED;
@@ -42,31 +46,34 @@ import static org.apache.flink.runtime.metrics.groups.InternalCacheMetricGroup.U
 /**
  * Abstract task that loads data in Full cache from source provided by {@link ScanRuntimeProvider}.
  */
-public abstract class CacheLoader extends AbstractRichFunction implements Runnable, Serializable {
+public abstract class CacheLoader implements AutoCloseable, Serializable {
     private static final Logger LOG = LoggerFactory.getLogger(CacheLoader.class);
+    protected static final long TIMEOUT_AFTER_INTERRUPT_MS = 10000;
 
     protected transient volatile ConcurrentHashMap<RowData, Collection<RowData>> cache;
 
     // 2 reloads can't be executed simultaneously, so they are performed under lock
     private final ReentrantLock reloadLock = new ReentrantLock();
     // runtime waits for the first load to complete to start an execution lookup join
-    private CountDownLatch firstLoadLatch;
+    private transient CountDownLatch firstLoadLatch;
+    private transient ExecutorService reloadExecutor;
 
     // Cache metrics
     private transient Counter loadCounter;
     private transient Counter loadFailuresCounter;
-    private transient volatile long latestLoadTimeMs = UNINITIALIZED;
+    private volatile long latestLoadTimeMs = UNINITIALIZED;
 
     protected volatile boolean isStopped;
 
-    protected abstract void reloadCache() throws Exception;
+    /** @return whether reload was successful and was not interrupted. */
+    protected abstract boolean updateCache() throws Exception;
 
-    @Override
     public void open(Configuration parameters) throws Exception {
         firstLoadLatch = new CountDownLatch(1);
+        reloadExecutor = Executors.newSingleThreadExecutor();
     }
 
-    public void open(CacheMetricGroup cacheMetricGroup) {
+    public void initializeMetrics(CacheMetricGroup cacheMetricGroup) {
         if (loadCounter == null) {
             loadCounter = new ThreadSafeSimpleCounter();
         }
@@ -92,8 +99,11 @@ public abstract class CacheLoader extends AbstractRichFunction implements Runnab
         firstLoadLatch.await();
     }
 
-    @Override
-    public void run() {
+    public CompletableFuture<Void> reloadAsync() {
+        return CompletableFuture.runAsync(this::reload, reloadExecutor);
+    }
+
+    private void reload() {
         if (isStopped) {
             return;
         }
@@ -102,13 +112,17 @@ public abstract class CacheLoader extends AbstractRichFunction implements Runnab
         try {
             LOG.info("Lookup 'FULL' cache loading triggered.");
             long start = System.currentTimeMillis();
-            reloadCache();
+            boolean success = updateCache();
             latestLoadTimeMs = System.currentTimeMillis() - start;
-            loadCounter.inc();
-            LOG.info(
-                    "Lookup 'FULL' cache loading finished. Time elapsed - {} ms. Number of records - {}.",
-                    latestLoadTimeMs,
-                    cache.size());
+            if (success) {
+                loadCounter.inc();
+                LOG.info(
+                        "Lookup 'FULL' cache loading finished. Time elapsed - {} ms. Number of records - {}.",
+                        latestLoadTimeMs,
+                        cache.size());
+            } else {
+                LOG.info("Active lookup 'FULL' cache reload has been interrupted.");
+            }
             if (LOG.isDebugEnabled()) {
                 // 'if' guard statement prevents us from transforming cache to string
                 LOG.debug(
@@ -128,15 +142,18 @@ public abstract class CacheLoader extends AbstractRichFunction implements Runnab
     @Override
     public void close() throws Exception {
         isStopped = true;
-        // if reload is in progress, we will wait until it is over
-        // current reload should already be interrupted, so block won't take much time
-        reloadLock.lock();
-        try {
-            if (cache != null) {
-                cache.clear();
+        if (reloadExecutor != null) {
+            reloadExecutor.shutdownNow(); // interrupt active reload
+            if (!reloadExecutor.awaitTermination(
+                    TIMEOUT_AFTER_INTERRUPT_MS, TimeUnit.MILLISECONDS)) {
+                throw new TimeoutException(
+                        "Lookup 'FULL' cache reload thread was not terminated after closing in timeout of "
+                                + TIMEOUT_AFTER_INTERRUPT_MS
+                                + " ms.");
             }
-        } finally {
-            reloadLock.unlock();
+        }
+        if (cache != null) {
+            cache.clear();
         }
     }
 }
diff --git a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/functions/table/lookup/fullcache/LookupFullCache.java b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/functions/table/lookup/fullcache/LookupFullCache.java
index 20d317d4bcd..87950b62e8e 100644
--- a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/functions/table/lookup/fullcache/LookupFullCache.java
+++ b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/functions/table/lookup/fullcache/LookupFullCache.java
@@ -18,6 +18,7 @@
 
 package org.apache.flink.table.runtime.functions.table.lookup.fullcache;
 
+import org.apache.flink.annotation.Internal;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.metrics.Counter;
 import org.apache.flink.metrics.SimpleCounter;
@@ -33,6 +34,7 @@ import java.util.Collection;
 import java.util.Collections;
 
 /** Internal implementation of {@link LookupCache} for {@link LookupCacheType#FULL}. */
+@Internal
 public class LookupFullCache implements LookupCache {
     private static final long serialVersionUID = 1L;
 
@@ -57,25 +59,28 @@ public class LookupFullCache implements LookupCache {
         }
         metricGroup.hitCounter(hitCounter);
         metricGroup.missCounter(new SimpleCounter()); // always zero
-        cacheLoader.open(metricGroup);
-    }
+        cacheLoader.initializeMetrics(metricGroup);
 
-    public synchronized void open(Configuration parameters) throws Exception {
         if (reloadTriggerContext == null) {
-            cacheLoader.open(parameters);
-            reloadTriggerContext =
-                    new ReloadTriggerContext(
-                            cacheLoader,
-                            th -> {
-                                if (reloadFailCause == null) {
-                                    reloadFailCause = th;
-                                } else {
-                                    reloadFailCause.addSuppressed(th);
-                                }
-                            });
-
-            reloadTrigger.open(reloadTriggerContext);
-            cacheLoader.awaitFirstLoad();
+            try {
+                // TODO add Configuration into FunctionContext and pass in into LookupFullCache
+                cacheLoader.open(new Configuration());
+                reloadTriggerContext =
+                        new ReloadTriggerContext(
+                                cacheLoader::reloadAsync,
+                                th -> {
+                                    if (reloadFailCause == null) {
+                                        reloadFailCause = th;
+                                    } else {
+                                        reloadFailCause.addSuppressed(th);
+                                    }
+                                });
+
+                reloadTrigger.open(reloadTriggerContext);
+                cacheLoader.awaitFirstLoad();
+            } catch (Exception e) {
+                throw new RuntimeException("Failed to open lookup 'FULL' cache.", e);
+            }
         }
     }
 
@@ -109,7 +114,9 @@ public class LookupFullCache implements LookupCache {
 
     @Override
     public void close() throws Exception {
-        reloadTrigger.close(); // firstly try to interrupt reload thread
+        // stops scheduled thread pool that's responsible for scheduling cache updates
+        reloadTrigger.close();
+        // stops thread pool that's responsible for executing the actual cache update
         cacheLoader.close();
     }
 }
diff --git a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/functions/table/lookup/fullcache/ReloadTriggerContext.java b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/functions/table/lookup/fullcache/ReloadTriggerContext.java
index f1dad16113a..f1c4500f548 100644
--- a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/functions/table/lookup/fullcache/ReloadTriggerContext.java
+++ b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/functions/table/lookup/fullcache/ReloadTriggerContext.java
@@ -22,15 +22,17 @@ import org.apache.flink.table.connector.source.lookup.cache.trigger.CacheReloadT
 
 import java.util.concurrent.CompletableFuture;
 import java.util.function.Consumer;
+import java.util.function.Supplier;
 
 /** Runtime implementation of {@link CacheReloadTrigger.Context}. */
 public class ReloadTriggerContext implements CacheReloadTrigger.Context {
 
-    private final Runnable reloadTask;
+    private final Supplier<CompletableFuture<Void>> cacheLoader;
     private final Consumer<Throwable> reloadFailCallback;
 
-    public ReloadTriggerContext(Runnable reloadTask, Consumer<Throwable> reloadFailCallback) {
-        this.reloadTask = reloadTask;
+    public ReloadTriggerContext(
+            Supplier<CompletableFuture<Void>> cacheLoader, Consumer<Throwable> reloadFailCallback) {
+        this.cacheLoader = cacheLoader;
         this.reloadFailCallback = reloadFailCallback;
     }
 
@@ -49,7 +51,8 @@ public class ReloadTriggerContext implements CacheReloadTrigger.Context {
 
     @Override
     public CompletableFuture<Void> triggerReload() {
-        return CompletableFuture.runAsync(reloadTask)
+        return cacheLoader
+                .get()
                 .exceptionally(
                         th -> {
                             reloadFailCallback.accept(th);
diff --git a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/functions/table/lookup/fullcache/inputformat/InputFormatCacheLoader.java b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/functions/table/lookup/fullcache/inputformat/InputFormatCacheLoader.java
index 872929111f2..cbd06e0f028 100644
--- a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/functions/table/lookup/fullcache/inputformat/InputFormatCacheLoader.java
+++ b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/functions/table/lookup/fullcache/inputformat/InputFormatCacheLoader.java
@@ -32,13 +32,15 @@ import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 import java.io.IOException;
+import java.util.ArrayDeque;
 import java.util.Arrays;
 import java.util.Collection;
-import java.util.List;
+import java.util.Deque;
+import java.util.concurrent.CompletableFuture;
 import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.Executors;
-import java.util.concurrent.Future;
+import java.util.concurrent.TimeUnit;
 import java.util.stream.Collectors;
 
 /** {@link CacheLoader} that used {@link InputFormat} for loading data into the cache. */
@@ -50,7 +52,6 @@ public class InputFormatCacheLoader extends CacheLoader {
     private final GenericRowDataKeySelector keySelector;
     private final RowDataSerializer cacheEntriesSerializer;
 
-    private transient volatile List<InputSplitCacheLoadTask> cacheLoadTasks;
     private transient Configuration parameters;
 
     public InputFormatCacheLoader(
@@ -71,56 +72,56 @@ public class InputFormatCacheLoader extends CacheLoader {
     }
 
     @Override
-    protected void reloadCache() throws Exception {
+    protected boolean updateCache() throws Exception {
         InputSplit[] inputSplits = createInputSplits();
         int numSplits = inputSplits.length;
+        int concurrencyLevel = getConcurrencyLevel(numSplits);
         // load data into the another copy of cache
-        // notice: it requires twice more memory, but on the other hand we don't need any blocking
+        // notice: it requires twice more memory, but on the other hand we don't need any blocking;
         // cache has default initialCapacity and loadFactor, but overridden concurrencyLevel
         ConcurrentHashMap<RowData, Collection<RowData>> newCache =
-                new ConcurrentHashMap<>(16, 0.75f, getConcurrencyLevel(numSplits));
-        this.cacheLoadTasks =
+                new ConcurrentHashMap<>(16, 0.75f, concurrencyLevel);
+        Deque<InputSplitCacheLoadTask> cacheLoadTasks =
                 Arrays.stream(inputSplits)
                         .map(split -> createCacheLoadTask(split, newCache))
-                        .collect(Collectors.toList());
-        if (isStopped) {
-            // check for cases when #close was called during reload before creating cacheLoadTasks
-            return;
-        }
-        // run first task or create numSplits threads to run all tasks
+                        .collect(Collectors.toCollection(ArrayDeque::new));
+        // run first task and create concurrencyLevel - 1 threads to run remaining tasks
         ExecutorService cacheLoadTaskService = null;
+        boolean wasInterrupted = false;
         try {
-            if (numSplits > 1) {
-                int numThreads = getConcurrencyLevel(numSplits);
-                cacheLoadTaskService = Executors.newFixedThreadPool(numThreads);
-                ExecutorService finalCacheLoadTaskService = cacheLoadTaskService;
-                List<Future<?>> futures =
+            InputSplitCacheLoadTask firstTask = cacheLoadTasks.pop();
+            CompletableFuture<?> otherTasksFuture = null;
+            if (!cacheLoadTasks.isEmpty()) {
+                cacheLoadTaskService = Executors.newFixedThreadPool(concurrencyLevel - 1);
+                ExecutorService finalExecutor = cacheLoadTaskService;
+                CompletableFuture<?>[] futures =
                         cacheLoadTasks.stream()
-                                .map(finalCacheLoadTaskService::submit)
-                                .collect(Collectors.toList());
-                for (Future<?> future : futures) {
-                    future.get(); // if any exception occurs it will be thrown here
-                }
-            } else {
-                cacheLoadTasks.get(0).run();
+                                .map(task -> CompletableFuture.runAsync(task, finalExecutor))
+                                .toArray(CompletableFuture<?>[]::new);
+                otherTasksFuture = CompletableFuture.allOf(futures);
             }
-        } catch (InterruptedException ignored) { // we use interrupt to close reload thread
+            firstTask.run(); // normally finishes if interrupted and saves interrupted status
+            if (otherTasksFuture != null) {
+                otherTasksFuture.get(); // if any exception occurs it will be thrown here
+            }
+        } catch (InterruptedException ignored) { // we use interrupt to close reload threads
+            Thread.currentThread().interrupt(); // restore interrupted status
         } finally {
+            wasInterrupted = Thread.interrupted();
             if (cacheLoadTaskService != null) {
+                // if main cache reload thread encountered an exception,
+                // it interrupts underlying InputSplitCacheLoadTasks threads
                 cacheLoadTaskService.shutdownNow();
+                if (!cacheLoadTaskService.awaitTermination(
+                        TIMEOUT_AFTER_INTERRUPT_MS, TimeUnit.MILLISECONDS)) {
+                    LOG.error(
+                            "ExecutorService with InputSplit cache loading tasks was not terminated in timeout of {} ms.",
+                            TIMEOUT_AFTER_INTERRUPT_MS);
+                }
             }
         }
         cache = newCache; // reassigning cache field is safe, because it's volatile
-    }
-
-    @Override
-    public void close() throws Exception {
-        // firstly stop current reload in case when custom reloadTrigger didn't interrupt it
-        isStopped = true;
-        if (cacheLoadTasks != null) {
-            cacheLoadTasks.forEach(InputSplitCacheLoadTask::stopRunning);
-        }
-        super.close();
+        return !wasInterrupted;
     }
 
     private InputSplitCacheLoadTask createCacheLoadTask(
diff --git a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/functions/table/lookup/fullcache/inputformat/InputSplitCacheLoadTask.java b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/functions/table/lookup/fullcache/inputformat/InputSplitCacheLoadTask.java
index 3d28a844446..cc2f9782921 100644
--- a/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/functions/table/lookup/fullcache/inputformat/InputSplitCacheLoadTask.java
+++ b/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/functions/table/lookup/fullcache/inputformat/InputSplitCacheLoadTask.java
@@ -27,6 +27,9 @@ import org.apache.flink.table.runtime.keyselector.GenericRowDataKeySelector;
 import org.apache.flink.table.runtime.typeutils.RowDataSerializer;
 import org.apache.flink.types.RowKind;
 
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
 import java.io.IOException;
 import java.util.Collection;
 import java.util.concurrent.ConcurrentHashMap;
@@ -37,6 +40,7 @@ import java.util.concurrent.ConcurrentLinkedQueue;
  * InputSplit}.
  */
 public class InputSplitCacheLoadTask implements Runnable {
+    private static final Logger LOG = LoggerFactory.getLogger(InputSplitCacheLoadTask.class);
 
     private final ConcurrentHashMap<RowData, Collection<RowData>> cache;
     private final GenericRowDataKeySelector keySelector;
@@ -44,8 +48,6 @@ public class InputSplitCacheLoadTask implements Runnable {
     private final InputFormat<RowData, InputSplit> inputFormat;
     private final InputSplit inputSplit;
 
-    private volatile boolean isRunning = true;
-
     public InputSplitCacheLoadTask(
             ConcurrentHashMap<RowData, Collection<RowData>> cache,
             GenericRowDataKeySelector keySelector,
@@ -68,7 +70,7 @@ public class InputSplitCacheLoadTask implements Runnable {
             }
             inputFormat.open(inputSplit);
             RowData nextElement = new BinaryRowData(cacheEntriesSerializer.getArity());
-            while (isRunning && !inputFormat.reachedEnd() && !Thread.interrupted()) {
+            while (!inputFormat.reachedEnd() && !Thread.currentThread().isInterrupted()) {
                 nextElement = inputFormat.nextRecord(nextElement);
                 if (nextElement != null) {
                     if (nextElement.getRowKind() != RowKind.INSERT) {
@@ -103,16 +105,11 @@ public class InputSplitCacheLoadTask implements Runnable {
                     ((RichInputFormat<?, ?>) inputFormat).closeInputFormat();
                 }
             } catch (IOException e) {
-                // can't do try-with-resources
-                throw new RuntimeException("Failed to close InputFormat.", e);
+                LOG.error("Failed to close InputFormat.", e);
             }
         }
     }
 
-    public void stopRunning() {
-        isRunning = false;
-    }
-
     private static boolean hasNoNulls(RowData keys) {
         // in SQL comparison 'null = null' returns false
         // IS NOT DISTINCT FROM (which returns true) is not supported in lookup join
diff --git a/flink-table/flink-table-runtime/src/test/java/org/apache/flink/table/runtime/functions/table/fullcache/LookupFullCacheTest.java b/flink-table/flink-table-runtime/src/test/java/org/apache/flink/table/runtime/functions/table/fullcache/LookupFullCacheTest.java
index 8fa1a2d747a..bbda500889e 100644
--- a/flink-table/flink-table-runtime/src/test/java/org/apache/flink/table/runtime/functions/table/fullcache/LookupFullCacheTest.java
+++ b/flink-table/flink-table-runtime/src/test/java/org/apache/flink/table/runtime/functions/table/fullcache/LookupFullCacheTest.java
@@ -18,7 +18,6 @@
 
 package org.apache.flink.table.runtime.functions.table.fullcache;
 
-import org.apache.flink.configuration.Configuration;
 import org.apache.flink.metrics.groups.CacheMetricGroup;
 import org.apache.flink.metrics.groups.UnregisteredMetricsGroup;
 import org.apache.flink.table.connector.source.lookup.cache.InterceptingCacheMetricGroup;
@@ -164,7 +163,6 @@ public class LookupFullCacheTest {
                                 ThrowingRunnable.unchecked(
                                         () -> {
                                             cache.open(metricGroup);
-                                            cache.open(new Configuration());
                                         }),
                                 executor);
                 futures.add(future);
@@ -178,7 +176,6 @@ public class LookupFullCacheTest {
                         runAsync(
                                 () -> {
                                     RowData key = row(1);
-                                    cache.open(metricGroup);
                                     assertThat(cache.getIfPresent(key))
                                             .isEqualTo(TestCacheLoader.DATA.get(key));
                                 },
@@ -204,10 +201,9 @@ public class LookupFullCacheTest {
     private LookupFullCache createAndLoadCache(
             TestCacheLoader cacheLoader, CacheMetricGroup metricGroup) throws Exception {
         LookupFullCache fullCache = new LookupFullCache(cacheLoader, reloadTrigger);
-        fullCache.open(metricGroup);
         assertThat(cacheLoader.isAwaitTriggered()).isFalse();
         assertThat(cacheLoader.getNumLoads()).isZero();
-        fullCache.open(new Configuration());
+        fullCache.open(metricGroup);
         assertThat(cacheLoader.isAwaitTriggered()).isTrue();
         assertThat(cacheLoader.getNumLoads()).isEqualTo(1);
         assertThat(cacheLoader.getCache()).isEqualTo(TestCacheLoader.DATA);
diff --git a/flink-table/flink-table-runtime/src/test/java/org/apache/flink/table/runtime/functions/table/fullcache/TestCacheLoader.java b/flink-table/flink-table-runtime/src/test/java/org/apache/flink/table/runtime/functions/table/fullcache/TestCacheLoader.java
index 53077937467..0fa2a17b4f9 100644
--- a/flink-table/flink-table-runtime/src/test/java/org/apache/flink/table/runtime/functions/table/fullcache/TestCacheLoader.java
+++ b/flink-table/flink-table-runtime/src/test/java/org/apache/flink/table/runtime/functions/table/fullcache/TestCacheLoader.java
@@ -68,11 +68,12 @@ public class TestCacheLoader extends CacheLoader {
     }
 
     @Override
-    protected void reloadCache() throws Exception {
+    protected boolean updateCache() {
         cache = new ConcurrentHashMap<>(DATA);
         numLoads++;
         if (numLoads == 2) {
             secondLoadDataChange.accept(cache);
         }
+        return true;
     }
 }
diff --git a/flink-table/flink-table-runtime/src/test/java/org/apache/flink/table/runtime/functions/table/fullcache/inputformat/FullCacheTestInputFormat.java b/flink-table/flink-table-runtime/src/test/java/org/apache/flink/table/runtime/functions/table/fullcache/inputformat/FullCacheTestInputFormat.java
index fef9eac4123..ba1e1296995 100644
--- a/flink-table/flink-table-runtime/src/test/java/org/apache/flink/table/runtime/functions/table/fullcache/inputformat/FullCacheTestInputFormat.java
+++ b/flink-table/flink-table-runtime/src/test/java/org/apache/flink/table/runtime/functions/table/fullcache/inputformat/FullCacheTestInputFormat.java
@@ -41,18 +41,23 @@ import java.util.stream.IntStream;
 
 import static org.assertj.core.api.Assertions.assertThat;
 
-/** TestInputFormat that reads data from (2 + delta) splits which share the same {@code queue}. */
+/**
+ * TestInputFormat that reads data from {@link #numSplits} splits on first load and ({@link
+ * #numSplits} + {@link #deltaNumSplits}) splits on next load. Splits share the same {@code queue}.
+ */
 public class FullCacheTestInputFormat
         extends RichInputFormat<RowData, FullCacheTestInputFormat.QueueInputSplit> {
 
     public static final AtomicInteger OPEN_CLOSED_COUNTER = new AtomicInteger(0);
     private static final int DEFAULT_NUM_SPLITS = 2;
+    private static final int DEFAULT_DELTA_NUM_SPLITS = 0;
 
     // RowData is not serializable, so we store Rows
     private final Collection<Row> dataRows;
     private final DataFormatConverters.RowConverter rowConverter;
     private final GeneratedProjection generatedProjection;
     private final boolean projectable;
+    private final int numSplits;
     private final int deltaNumSplits;
 
     private transient ConcurrentLinkedQueue<RowData> queue;
@@ -68,12 +73,14 @@ public class FullCacheTestInputFormat
             Collection<Row> dataRows,
             Optional<GeneratedProjection> generatedProjection,
             DataFormatConverters.RowConverter rowConverter,
+            int numSplits,
             int deltaNumSplits) {
         // for unit tests
         this.dataRows = dataRows;
         this.projectable = generatedProjection.isPresent();
         this.generatedProjection = generatedProjection.orElse(null);
         this.rowConverter = rowConverter;
+        this.numSplits = numSplits;
         this.deltaNumSplits = deltaNumSplits;
     }
 
@@ -82,24 +89,25 @@ public class FullCacheTestInputFormat
             Optional<GeneratedProjection> generatedProjection,
             DataFormatConverters.RowConverter rowConverter) {
         // for integration tests
-        this.dataRows = dataRows;
-        this.projectable = generatedProjection.isPresent();
-        this.generatedProjection = generatedProjection.orElse(null);
-        this.rowConverter = rowConverter;
-        this.deltaNumSplits = 0;
+        this(
+                dataRows,
+                generatedProjection,
+                rowConverter,
+                DEFAULT_NUM_SPLITS,
+                DEFAULT_DELTA_NUM_SPLITS);
     }
 
     @Override
     public QueueInputSplit[] createInputSplits(int minNumSplits) throws IOException {
         int delta = loadCounter > 0 ? deltaNumSplits : 0;
-        int numSplits = DEFAULT_NUM_SPLITS + delta;
+        int currentSplits = numSplits + delta;
         ConcurrentLinkedQueue<RowData> queue = new ConcurrentLinkedQueue<>();
-        QueueInputSplit[] splits = new QueueInputSplit[numSplits];
-        IntStream.range(0, numSplits).forEach(i -> splits[i] = new QueueInputSplit(queue, i));
+        QueueInputSplit[] splits = new QueueInputSplit[currentSplits];
+        IntStream.range(0, currentSplits).forEach(i -> splits[i] = new QueueInputSplit(queue, i));
         dataRows.forEach(row -> queue.add(rowConverter.toInternal(row)));
         // divide data evenly between InputFormat copies
         loadCounter++;
-        maxReadRecords = (int) Math.ceil((double) queue.size() / numSplits);
+        maxReadRecords = (int) Math.ceil((double) queue.size() / currentSplits);
         return splits;
     }
 
diff --git a/flink-table/flink-table-runtime/src/test/java/org/apache/flink/table/runtime/functions/table/fullcache/inputformat/InputFormatCacheLoaderTest.java b/flink-table/flink-table-runtime/src/test/java/org/apache/flink/table/runtime/functions/table/fullcache/inputformat/InputFormatCacheLoaderTest.java
index dcb73ae0346..954092bc34b 100644
--- a/flink-table/flink-table-runtime/src/test/java/org/apache/flink/table/runtime/functions/table/fullcache/inputformat/InputFormatCacheLoaderTest.java
+++ b/flink-table/flink-table-runtime/src/test/java/org/apache/flink/table/runtime/functions/table/fullcache/inputformat/InputFormatCacheLoaderTest.java
@@ -19,12 +19,14 @@
 package org.apache.flink.table.runtime.functions.table.fullcache.inputformat;
 
 import org.apache.flink.configuration.Configuration;
+import org.apache.flink.core.testutils.OneShotLatch;
 import org.apache.flink.metrics.groups.UnregisteredMetricsGroup;
 import org.apache.flink.table.api.DataTypes;
 import org.apache.flink.table.connector.source.lookup.cache.InterceptingCacheMetricGroup;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.data.util.DataFormatConverters;
 import org.apache.flink.table.runtime.functions.table.fullcache.TestCacheLoader;
+import org.apache.flink.table.runtime.functions.table.lookup.fullcache.CacheLoader;
 import org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputFormatCacheLoader;
 import org.apache.flink.table.runtime.generated.GeneratedProjection;
 import org.apache.flink.table.runtime.generated.Projection;
@@ -35,7 +37,6 @@ import org.apache.flink.table.runtime.typeutils.RowDataSerializer;
 import org.apache.flink.table.types.DataType;
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.types.Row;
-import org.apache.flink.util.function.ThrowingRunnable;
 
 import org.junit.jupiter.api.AfterEach;
 import org.junit.jupiter.api.BeforeEach;
@@ -47,23 +48,22 @@ import org.junit.jupiter.params.provider.MethodSource;
 import java.util.Collection;
 import java.util.Map;
 import java.util.Optional;
+import java.util.concurrent.CompletableFuture;
 import java.util.concurrent.ConcurrentHashMap;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.Executors;
-import java.util.concurrent.Future;
-import java.util.concurrent.atomic.AtomicInteger;
 import java.util.stream.Collectors;
 import java.util.stream.Stream;
 
 import static org.apache.flink.runtime.metrics.groups.InternalCacheMetricGroup.UNINITIALIZED;
 import static org.apache.flink.table.runtime.util.StreamRecordUtils.row;
 import static org.assertj.core.api.Assertions.assertThat;
-import static org.assertj.core.api.Assertions.assertThatNoException;
 import static org.assertj.core.api.Assertions.assertThatThrownBy;
 
 /** Unit test for {@link InputFormatCacheLoader}. */
 class InputFormatCacheLoaderTest {
 
+    private static final int DEFAULT_NUM_SPLITS = 2;
+    private static final int DEFAULT_DELTA_NUM_SPLITS = 0;
+
     @BeforeEach
     void resetCounter() {
         FullCacheTestInputFormat.OPEN_CLOSED_COUNTER.set(0);
@@ -77,43 +77,48 @@ class InputFormatCacheLoaderTest {
     @ParameterizedTest
     @MethodSource("deltaNumSplits")
     void testReadWithDifferentSplits(int deltaNumSplits) throws Exception {
-        InputFormatCacheLoader cacheLoader = createCacheLoader(deltaNumSplits);
-        cacheLoader.open(UnregisteredMetricsGroup.createCacheMetricGroup());
-        cacheLoader.run();
-        ConcurrentHashMap<RowData, Collection<RowData>> cache = cacheLoader.getCache();
-        assertCacheContent(cache);
-        cacheLoader.run();
-        assertThat(cacheLoader.getCache()).isNotSameAs(cache); // new instance of cache after reload
-        cacheLoader.close();
-        assertThat(cacheLoader.getCache().size()).isZero(); // cache is cleared after close
+        ConcurrentHashMap<RowData, Collection<RowData>> cache;
+        try (InputFormatCacheLoader cacheLoader = createCacheLoader(deltaNumSplits)) {
+            cacheLoader.initializeMetrics(UnregisteredMetricsGroup.createCacheMetricGroup());
+            reloadSynchronously(cacheLoader);
+            cache = cacheLoader.getCache();
+            assertCacheContent(cache);
+            reloadSynchronously(cacheLoader);
+            assertThat(cacheLoader.getCache())
+                    .as("A new instance of cache should be present after reload.")
+                    .isNotSameAs(cache);
+            cache = cacheLoader.getCache();
+        }
+        assertThat(cache.size()).as("Cache should be cleared after close.").isZero();
     }
 
     @Test
     void testCacheMetrics() throws Exception {
-        InputFormatCacheLoader cacheLoader = createCacheLoader(0);
-        InterceptingCacheMetricGroup metricGroup = new InterceptingCacheMetricGroup();
-        cacheLoader.open(metricGroup);
-        // These metrics are registered
-        assertThat(metricGroup.loadCounter).isNotNull();
-        assertThat(metricGroup.loadCounter.getCount()).isEqualTo(0);
-        assertThat(metricGroup.numLoadFailuresCounter).isNotNull();
-        assertThat(metricGroup.numLoadFailuresCounter.getCount()).isEqualTo(0);
-        assertThat(metricGroup.numCachedRecordsGauge).isNotNull();
-        assertThat(metricGroup.numCachedRecordsGauge.getValue()).isEqualTo(0);
-        assertThat(metricGroup.latestLoadTimeGauge).isNotNull();
-        assertThat(metricGroup.latestLoadTimeGauge.getValue()).isEqualTo(UNINITIALIZED);
-
-        // These metrics are left blank
-        assertThat(metricGroup.hitCounter).isNull();
-        assertThat(metricGroup.missCounter).isNull();
-        assertThat(metricGroup.numCachedBytesGauge).isNull(); // not supported currently
-
-        cacheLoader.run();
-
-        assertThat(metricGroup.loadCounter.getCount()).isEqualTo(1);
-        assertThat(metricGroup.latestLoadTimeGauge.getValue()).isNotEqualTo(UNINITIALIZED);
-        assertThat(metricGroup.numCachedRecordsGauge.getValue())
-                .isEqualTo(TestCacheLoader.DATA.size());
+        try (InputFormatCacheLoader cacheLoader = createCacheLoader(DEFAULT_DELTA_NUM_SPLITS)) {
+            InterceptingCacheMetricGroup metricGroup = new InterceptingCacheMetricGroup();
+            cacheLoader.initializeMetrics(metricGroup);
+            // These metrics are registered
+            assertThat(metricGroup.loadCounter).isNotNull();
+            assertThat(metricGroup.loadCounter.getCount()).isEqualTo(0);
+            assertThat(metricGroup.numLoadFailuresCounter).isNotNull();
+            assertThat(metricGroup.numLoadFailuresCounter.getCount()).isEqualTo(0);
+            assertThat(metricGroup.numCachedRecordsGauge).isNotNull();
+            assertThat(metricGroup.numCachedRecordsGauge.getValue()).isEqualTo(0);
+            assertThat(metricGroup.latestLoadTimeGauge).isNotNull();
+            assertThat(metricGroup.latestLoadTimeGauge.getValue()).isEqualTo(UNINITIALIZED);
+
+            // These metrics are left blank
+            assertThat(metricGroup.hitCounter).isNull();
+            assertThat(metricGroup.missCounter).isNull();
+            assertThat(metricGroup.numCachedBytesGauge).isNull(); // not supported currently
+
+            reloadSynchronously(cacheLoader);
+
+            assertThat(metricGroup.loadCounter.getCount()).isEqualTo(1);
+            assertThat(metricGroup.latestLoadTimeGauge.getValue()).isNotEqualTo(UNINITIALIZED);
+            assertThat(metricGroup.numCachedRecordsGauge.getValue())
+                    .isEqualTo(TestCacheLoader.DATA.size());
+        }
     }
 
     @Test
@@ -123,46 +128,53 @@ class InputFormatCacheLoaderTest {
                 () -> {
                     throw exception;
                 };
-        InputFormatCacheLoader cacheLoader = createCacheLoader(0, reloadAction);
-        InterceptingCacheMetricGroup metricGroup = new InterceptingCacheMetricGroup();
-        cacheLoader.open(metricGroup);
-        assertThatThrownBy(cacheLoader::run).hasRootCause(exception);
-        assertThat(metricGroup.numLoadFailuresCounter.getCount()).isEqualTo(1);
+        try (InputFormatCacheLoader cacheLoader =
+                createCacheLoader(DEFAULT_NUM_SPLITS, DEFAULT_DELTA_NUM_SPLITS, reloadAction)) {
+            InterceptingCacheMetricGroup metricGroup = new InterceptingCacheMetricGroup();
+            cacheLoader.initializeMetrics(metricGroup);
+            assertThatThrownBy(() -> reloadSynchronously(cacheLoader)).hasRootCause(exception);
+            assertThat(metricGroup.loadCounter.getCount()).isEqualTo(0);
+            assertThat(metricGroup.numLoadFailuresCounter.getCount()).isEqualTo(1);
+        }
     }
 
-    @Test
-    void testCloseAndInterruptDuringReload() throws Exception {
-        AtomicInteger sleepCounter = new AtomicInteger(0);
-        int totalSleepCount = TestCacheLoader.DATA.size() + 1; // equals to number of all rows
+    /**
+     * Cache loader creates additional threads in case of multiple input splits. In both cases cache
+     * loader must correctly react on close and interrupt all threads.
+     */
+    @ParameterizedTest
+    @MethodSource("numSplits")
+    void testCloseDuringReload(int numSplits) throws Exception {
+        OneShotLatch reloadLatch = new OneShotLatch();
         Runnable reloadAction =
-                ThrowingRunnable.unchecked(
-                        () -> {
-                            sleepCounter.incrementAndGet();
-                            Thread.sleep(1000);
-                        });
-        InputFormatCacheLoader cacheLoader = createCacheLoader(0, reloadAction);
+                () -> {
+                    reloadLatch.trigger();
+                    assertThatThrownBy(() -> new OneShotLatch().await())
+                            .as("Wait should be interrupted if everything works ok")
+                            .isInstanceOf(InterruptedException.class);
+                    Thread.currentThread().interrupt(); // restore interrupted status
+                };
         InterceptingCacheMetricGroup metricGroup = new InterceptingCacheMetricGroup();
-        cacheLoader.open(metricGroup);
-
-        // check interruption
-        ExecutorService executorService = Executors.newSingleThreadExecutor();
-        Future<?> future = executorService.submit(cacheLoader);
-        executorService.shutdownNow(); // internally interrupts a thread
-        assertThatNoException().isThrownBy(future::get); // wait for the end
-        // check that we didn't process all elements, but reacted on interruption
-        assertThat(sleepCounter).hasValueLessThan(totalSleepCount);
+        CompletableFuture<Void> future;
+        try (InputFormatCacheLoader cacheLoader =
+                createCacheLoader(numSplits, DEFAULT_DELTA_NUM_SPLITS, reloadAction)) {
+            cacheLoader.initializeMetrics(metricGroup);
+            future = cacheLoader.reloadAsync();
+            reloadLatch.await();
+        }
+        // try-with-resources calls #close which will interrupt any running threads and wait for the
+        // end of reload
+        assertThat(future.isDone())
+                .as(
+                        "The reload future should still complete successfully indicating "
+                                + "that the reload was intentionally stopped without an error.")
+                .isTrue();
+        assertThat(metricGroup.loadCounter.getCount()).isEqualTo(0);
         assertThat(metricGroup.numLoadFailuresCounter.getCount()).isEqualTo(0);
+    }
 
-        sleepCounter.set(0);
-
-        // check closing
-        executorService = Executors.newSingleThreadExecutor();
-        future = executorService.submit(cacheLoader);
-        cacheLoader.close();
-        assertThatNoException().isThrownBy(future::get); // wait for the end
-        // check that we didn't process all elements, but reacted on closing
-        assertThat(sleepCounter).hasValueLessThan(totalSleepCount);
-        assertThat(metricGroup.numLoadFailuresCounter.getCount()).isEqualTo(0);
+    static Stream<Arguments> numSplits() {
+        return Stream.of(Arguments.of(1), Arguments.of(2));
     }
 
     static Stream<Arguments> deltaNumSplits() {
@@ -178,12 +190,16 @@ class InputFormatCacheLoaderTest {
                         assertThat(rows).containsExactlyInAnyOrderElementsOf(actual.get(key)));
     }
 
+    private void reloadSynchronously(CacheLoader cacheLoader) {
+        cacheLoader.reloadAsync().join();
+    }
+
     private InputFormatCacheLoader createCacheLoader(int deltaNumSplits) throws Exception {
-        return createCacheLoader(deltaNumSplits, () -> {});
+        return createCacheLoader(DEFAULT_NUM_SPLITS, deltaNumSplits, () -> {});
     }
 
-    private InputFormatCacheLoader createCacheLoader(int deltaNumSplits, Runnable reloadAction)
-            throws Exception {
+    private InputFormatCacheLoader createCacheLoader(
+            int numSplits, int deltaNumSplits, Runnable reloadAction) throws Exception {
         DataType rightRowDataType =
                 DataTypes.ROW(
                         DataTypes.FIELD("f0", DataTypes.INT()),
@@ -203,7 +219,8 @@ class InputFormatCacheLoaderTest {
                         .map(converter::toExternal)
                         .collect(Collectors.toList());
         FullCacheTestInputFormat inputFormat =
-                new FullCacheTestInputFormat(dataRows, Optional.empty(), converter, deltaNumSplits);
+                new FullCacheTestInputFormat(
+                        dataRows, Optional.empty(), converter, numSplits, deltaNumSplits);
         RowType keyType = (RowType) DataTypes.ROW(DataTypes.INT()).getLogicalType();
 
         // noinspection rawtypes
