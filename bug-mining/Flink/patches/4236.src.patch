diff --git a/docs/dev/table/connectors/filesystem.md b/docs/dev/table/connectors/filesystem.md
index 266453cf64f..99d9d5c67f2 100644
--- a/docs/dev/table/connectors/filesystem.md
+++ b/docs/dev/table/connectors/filesystem.md
@@ -146,6 +146,8 @@ After writing a partition, it is often necessary to notify downstream applicatio
 - Trigger: The timing of the commit of the partition can be determined by the watermark with the time extracted from the partition, or by processing time.
 - Policy: How to commit a partition, built-in policies support for the commit of success files and metastore, you can also implement your own policies, such as triggering hive's analysis to generate statistics, or merging small files, etc.
 
+**NOTE:** Partition Commit only works in dynamic partition inserting.
+
 #### Partition commit trigger
 
 To define when to commit a partition, providing partition commit trigger:
diff --git a/docs/dev/table/connectors/filesystem.zh.md b/docs/dev/table/connectors/filesystem.zh.md
index 266453cf64f..99d9d5c67f2 100644
--- a/docs/dev/table/connectors/filesystem.zh.md
+++ b/docs/dev/table/connectors/filesystem.zh.md
@@ -146,6 +146,8 @@ After writing a partition, it is often necessary to notify downstream applicatio
 - Trigger: The timing of the commit of the partition can be determined by the watermark with the time extracted from the partition, or by processing time.
 - Policy: How to commit a partition, built-in policies support for the commit of success files and metastore, you can also implement your own policies, such as triggering hive's analysis to generate statistics, or merging small files, etc.
 
+**NOTE:** Partition Commit only works in dynamic partition inserting.
+
 #### Partition commit trigger
 
 To define when to commit a partition, providing partition commit trigger:
diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java
index 1cac9e08587..c49905cb781 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java
@@ -127,6 +127,7 @@ public class HiveTableSink implements AppendStreamTableSink, PartitionableTableS
 			StorageDescriptor sd = table.getSd();
 			HiveTableMetaStoreFactory msFactory = new HiveTableMetaStoreFactory(
 					jobConf, hiveVersion, dbName, tableName);
+			HadoopFileSystemFactory fsFactory = new HadoopFileSystemFactory(jobConf);
 
 			Class hiveOutputFormatClz = hiveShim.getHiveOutputFormatClass(
 					Class.forName(sd.getOutputFormat()));
@@ -158,7 +159,7 @@ public class HiveTableSink implements AppendStreamTableSink, PartitionableTableS
 						partitionColumns));
 				builder.setDynamicGrouped(dynamicGrouping);
 				builder.setPartitionColumns(partitionColumns);
-				builder.setFileSystemFactory(new HadoopFileSystemFactory(jobConf));
+				builder.setFileSystemFactory(fsFactory);
 				builder.setFormatFactory(new HiveOutputFormatFactory(recordWriterFactory));
 				builder.setMetaStoreFactory(
 						msFactory);
@@ -213,7 +214,8 @@ public class HiveTableSink implements AppendStreamTableSink, PartitionableTableS
 						overwrite,
 						dataStream,
 						builder,
-						msFactory);
+						msFactory,
+						fsFactory);
 			}
 		} catch (TException e) {
 			throw new CatalogException("Failed to query Hive metaStore", e);
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java
index b6127461f6e..d943ee941d6 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java
@@ -131,6 +131,7 @@ public class FileSystemTableSink implements
 		OutputFileConfig outputFileConfig = OutputFileConfig.builder()
 				.withPartPrefix("part-" + UUID.randomUUID().toString())
 				.build();
+		FileSystemFactory fsFactory = FileSystem::get;
 
 		if (isBounded) {
 			FileSystemOutputFormat.Builder<RowData> builder = new FileSystemOutputFormat.Builder<>();
@@ -139,6 +140,7 @@ public class FileSystemTableSink implements
 			builder.setPartitionColumns(partitionKeys.toArray(new String[0]));
 			builder.setFormatFactory(createOutputFormatFactory());
 			builder.setMetaStoreFactory(metaStoreFactory);
+			builder.setFileSystemFactory(fsFactory);
 			builder.setOverwrite(overwrite);
 			builder.setStaticPartitions(staticPartitions);
 			builder.setTempPath(toStagingPath());
@@ -179,7 +181,8 @@ public class FileSystemTableSink implements
 					overwrite,
 					dataStream,
 					bucketsBuilder,
-					metaStoreFactory);
+					metaStoreFactory,
+					fsFactory);
 		}
 	}
 
@@ -191,7 +194,8 @@ public class FileSystemTableSink implements
 			boolean overwrite,
 			DataStream<RowData> inputStream,
 			BucketsBuilder<RowData, String, ? extends BucketsBuilder<RowData, ?, ?>> bucketsBuilder,
-			TableMetaStoreFactory msFactory) {
+			TableMetaStoreFactory msFactory,
+			FileSystemFactory fsFactory) {
 		if (overwrite) {
 			throw new IllegalStateException("Streaming mode not support overwrite.");
 		}
@@ -208,7 +212,7 @@ public class FileSystemTableSink implements
 		// save committer when we don't need it.
 		if (partitionKeys.size() > 0 && conf.contains(SINK_PARTITION_COMMIT_POLICY_KIND)) {
 			StreamingFileCommitter committer = new StreamingFileCommitter(
-					path, tableIdentifier, partitionKeys, msFactory, conf);
+					path, tableIdentifier, partitionKeys, msFactory, fsFactory, conf);
 			returnStream = writerStream
 					.transform(StreamingFileCommitter.class.getSimpleName(), Types.VOID, committer)
 					.setParallelism(1)
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/PartitionCommitPolicy.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/PartitionCommitPolicy.java
index 9074b7f5990..d2c2c7c479e 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/PartitionCommitPolicy.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/PartitionCommitPolicy.java
@@ -26,6 +26,7 @@ import org.apache.flink.table.api.ValidationException;
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.List;
+import java.util.function.Supplier;
 import java.util.stream.Collectors;
 
 /**
@@ -99,7 +100,7 @@ public interface PartitionCommitPolicy {
 			String policyKind,
 			String customClass,
 			String successFileName,
-			FileSystem fileSystem) {
+			Supplier<FileSystem> fsSupplier) {
 		if (policyKind == null) {
 			return Collections.emptyList();
 		}
@@ -109,7 +110,7 @@ public interface PartitionCommitPolicy {
 				case METASTORE:
 					return new MetastoreCommitPolicy();
 				case SUCCESS_FILE:
-					return new SuccessFileCommitPolicy(successFileName, fileSystem);
+					return new SuccessFileCommitPolicy(successFileName, fsSupplier.get());
 				case CUSTOM:
 					try {
 						return (PartitionCommitPolicy) cl.loadClass(customClass).newInstance();
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/PartitionCommitTrigger.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/PartitionCommitTrigger.java
index 1f4b294194d..8761fa06917 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/PartitionCommitTrigger.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/PartitionCommitTrigger.java
@@ -18,93 +18,52 @@
 
 package org.apache.flink.table.filesystem.stream;
 
-import org.apache.flink.api.common.state.ListState;
-import org.apache.flink.api.common.state.ListStateDescriptor;
 import org.apache.flink.api.common.state.OperatorStateStore;
-import org.apache.flink.api.common.typeutils.base.ListSerializer;
-import org.apache.flink.api.common.typeutils.base.StringSerializer;
 import org.apache.flink.configuration.Configuration;
-import org.apache.flink.core.fs.FileSystem;
-import org.apache.flink.core.fs.Path;
 import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;
-import org.apache.flink.util.StringUtils;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashSet;
 import java.util.List;
-import java.util.Set;
 
 import static org.apache.flink.table.filesystem.FileSystemOptions.SINK_PARTITION_COMMIT_TRIGGER;
 
 /**
- * Abstract commit trigger, store partitions in state and provide {@link #committablePartitions}
- * for trigger.
+ * Partition commit trigger.
  * See {@link PartitionTimeCommitTigger}.
  * See {@link ProcTimeCommitTigger}.
  */
-public abstract class PartitionCommitTrigger {
+public interface PartitionCommitTrigger {
 
-	public static final String PARTITION_TIME = "partition-time";
-	public static final String PROCESS_TIME = "process-time";
-
-	private static final ListStateDescriptor<List<String>> PENDING_PARTITIONS_STATE_DESC =
-			new ListStateDescriptor<>(
-					"pending-partitions",
-					new ListSerializer<>(StringSerializer.INSTANCE));
-
-	protected final ListState<List<String>> pendingPartitionsState;
-	protected final Set<String> pendingPartitions;
-
-	protected PartitionCommitTrigger(
-			boolean isRestored, OperatorStateStore stateStore) throws Exception {
-		this.pendingPartitionsState = stateStore.getListState(PENDING_PARTITIONS_STATE_DESC);
-		this.pendingPartitions = new HashSet<>();
-		if (isRestored) {
-			pendingPartitions.addAll(pendingPartitionsState.get().iterator().next());
-		}
-	}
+	String PARTITION_TIME = "partition-time";
+	String PROCESS_TIME = "process-time";
 
 	/**
 	 * Add a pending partition.
 	 */
-	public void addPartition(String partition) {
-		if (!StringUtils.isNullOrWhitespaceOnly(partition)) {
-			this.pendingPartitions.add(partition);
-		}
-	}
+	void addPartition(String partition);
 
 	/**
 	 * Get committable partitions, and cleanup useless watermarks and partitions.
 	 */
-	public abstract List<String> committablePartitions(long checkpointId) throws IOException;
+	List<String> committablePartitions(long checkpointId) throws IOException;
 
 	/**
 	 * End input, return committable partitions and clear.
 	 */
-	public List<String> endInput() {
-		ArrayList<String> partitions = new ArrayList<>(pendingPartitions);
-		pendingPartitions.clear();
-		return partitions;
-	}
+	List<String> endInput();
 
 	/**
 	 * Snapshot state.
 	 */
-	public void snapshotState(long checkpointId, long watermark) throws Exception {
-		pendingPartitionsState.clear();
-		pendingPartitionsState.add(new ArrayList<>(pendingPartitions));
-	}
+	void snapshotState(long checkpointId, long watermark) throws Exception;
 
-	public static PartitionCommitTrigger create(
+	static PartitionCommitTrigger create(
 			boolean isRestored,
 			OperatorStateStore stateStore,
 			Configuration conf,
 			ClassLoader cl,
 			List<String> partitionKeys,
-			ProcessingTimeService procTimeService,
-			FileSystem fileSystem,
-			Path locationPath) throws Exception {
+			ProcessingTimeService procTimeService) throws Exception {
 		String trigger = conf.get(SINK_PARTITION_COMMIT_TRIGGER);
 		switch (trigger) {
 			case PARTITION_TIME:
@@ -112,7 +71,7 @@ public abstract class PartitionCommitTrigger {
 						isRestored, stateStore, conf, cl, partitionKeys);
 			case PROCESS_TIME:
 				return new ProcTimeCommitTigger(
-						isRestored, stateStore, conf, procTimeService, fileSystem, locationPath);
+						isRestored, stateStore, conf, procTimeService);
 			default:
 				throw new UnsupportedOperationException(
 						"Unsupported partition commit trigger: " + trigger);
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/PartitionTimeCommitTigger.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/PartitionTimeCommitTigger.java
index 27312b56542..390a89561bf 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/PartitionTimeCommitTigger.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/PartitionTimeCommitTigger.java
@@ -21,18 +21,23 @@ package org.apache.flink.table.filesystem.stream;
 import org.apache.flink.api.common.state.ListState;
 import org.apache.flink.api.common.state.ListStateDescriptor;
 import org.apache.flink.api.common.state.OperatorStateStore;
+import org.apache.flink.api.common.typeutils.base.ListSerializer;
 import org.apache.flink.api.common.typeutils.base.LongSerializer;
 import org.apache.flink.api.common.typeutils.base.MapSerializer;
+import org.apache.flink.api.common.typeutils.base.StringSerializer;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.core.fs.Path;
 import org.apache.flink.table.filesystem.PartitionTimeExtractor;
+import org.apache.flink.util.StringUtils;
 
 import java.time.LocalDateTime;
 import java.util.ArrayList;
 import java.util.HashMap;
+import java.util.HashSet;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
+import java.util.Set;
 import java.util.TreeMap;
 
 import static org.apache.flink.table.filesystem.DefaultPartTimeExtractor.toMills;
@@ -49,13 +54,21 @@ import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionVa
  * <p>Compares watermark, and watermark is related to records and checkpoint, so we need store
  * watermark information for checkpoint.
  */
-public class PartitionTimeCommitTigger extends PartitionCommitTrigger {
+public class PartitionTimeCommitTigger implements PartitionCommitTrigger {
+
+	private static final ListStateDescriptor<List<String>> PENDING_PARTITIONS_STATE_DESC =
+			new ListStateDescriptor<>(
+					"pending-partitions",
+					new ListSerializer<>(StringSerializer.INSTANCE));
 
 	private static final ListStateDescriptor<Map<Long, Long>> WATERMARKS_STATE_DESC =
 			new ListStateDescriptor<>(
 					"checkpoint-id-to-watermark",
 					new MapSerializer<>(LongSerializer.INSTANCE, LongSerializer.INSTANCE));
 
+	private final ListState<List<String>> pendingPartitionsState;
+	private final Set<String> pendingPartitions;
+
 	private final ListState<Map<Long, Long>> watermarksState;
 	private final TreeMap<Long, Long> watermarks;
 	private final PartitionTimeExtractor extractor;
@@ -68,7 +81,12 @@ public class PartitionTimeCommitTigger extends PartitionCommitTrigger {
 			Configuration conf,
 			ClassLoader cl,
 			List<String> partitionKeys) throws Exception {
-		super(isRestored, stateStore);
+		this.pendingPartitionsState = stateStore.getListState(PENDING_PARTITIONS_STATE_DESC);
+		this.pendingPartitions = new HashSet<>();
+		if (isRestored) {
+			pendingPartitions.addAll(pendingPartitionsState.get().iterator().next());
+		}
+
 		this.partitionKeys = partitionKeys;
 		this.commitDelay = conf.get(SINK_PARTITION_COMMIT_DELAY).toMillis();
 		this.extractor = PartitionTimeExtractor.create(
@@ -84,6 +102,13 @@ public class PartitionTimeCommitTigger extends PartitionCommitTrigger {
 		}
 	}
 
+	@Override
+	public void addPartition(String partition) {
+		if (!StringUtils.isNullOrWhitespaceOnly(partition)) {
+			this.pendingPartitions.add(partition);
+		}
+	}
+
 	@Override
 	public List<String> committablePartitions(long checkpointId) {
 		if (!watermarks.containsKey(checkpointId)) {
@@ -111,9 +136,18 @@ public class PartitionTimeCommitTigger extends PartitionCommitTrigger {
 
 	@Override
 	public void snapshotState(long checkpointId, long watermark) throws Exception {
-		super.snapshotState(checkpointId, watermark);
+		pendingPartitionsState.clear();
+		pendingPartitionsState.add(new ArrayList<>(pendingPartitions));
+
 		watermarks.put(checkpointId, watermark);
 		watermarksState.clear();
 		watermarksState.add(new HashMap<>(watermarks));
 	}
+
+	@Override
+	public List<String> endInput() {
+		ArrayList<String> partitions = new ArrayList<>(pendingPartitions);
+		pendingPartitions.clear();
+		return partitions;
+	}
 }
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/ProcTimeCommitTigger.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/ProcTimeCommitTigger.java
index 7d6ff74a9b5..f230d7d667a 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/ProcTimeCommitTigger.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/ProcTimeCommitTigger.java
@@ -18,60 +18,88 @@
 
 package org.apache.flink.table.filesystem.stream;
 
+import org.apache.flink.api.common.state.ListState;
+import org.apache.flink.api.common.state.ListStateDescriptor;
 import org.apache.flink.api.common.state.OperatorStateStore;
+import org.apache.flink.api.common.typeutils.base.LongSerializer;
+import org.apache.flink.api.common.typeutils.base.MapSerializer;
+import org.apache.flink.api.common.typeutils.base.StringSerializer;
 import org.apache.flink.configuration.Configuration;
-import org.apache.flink.core.fs.FileSystem;
-import org.apache.flink.core.fs.Path;
 import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;
+import org.apache.flink.util.StringUtils;
 
-import java.io.IOException;
 import java.util.ArrayList;
+import java.util.HashMap;
 import java.util.Iterator;
 import java.util.List;
+import java.util.Map;
 
 import static org.apache.flink.table.filesystem.FileSystemOptions.SINK_PARTITION_COMMIT_DELAY;
 
 /**
- * Partition commit trigger by path creation time and processing time service,
- * if 'current processing time' > 'partition directory creation time' + 'delay', will commit the partition.
- *
- * <p>It is hard to get partition start time from writer, so just get creation time from file system.
+ * Partition commit trigger by creation time and processing time service,
+ * if 'current processing time' > 'partition creation time' + 'delay', will commit the partition.
  */
-public class ProcTimeCommitTigger extends PartitionCommitTrigger {
+public class ProcTimeCommitTigger implements PartitionCommitTrigger {
+
+	private static final ListStateDescriptor<Map<String, Long>> PENDING_PARTITIONS_STATE_DESC =
+			new ListStateDescriptor<>(
+					"pending-partitions-with-time",
+					new MapSerializer<>(StringSerializer.INSTANCE, LongSerializer.INSTANCE));
 
+	private final ListState<Map<String, Long>> pendingPartitionsState;
+	private final Map<String, Long> pendingPartitions;
 	private final long commitDelay;
 	private final ProcessingTimeService procTimeService;
-	private final FileSystem fileSystem;
-	private final Path locationPath;
 
 	public ProcTimeCommitTigger(
 			boolean isRestored,
 			OperatorStateStore stateStore,
 			Configuration conf,
-			ProcessingTimeService procTimeService,
-			FileSystem fileSystem,
-			Path locationPath) throws Exception {
-		super(isRestored, stateStore);
+			ProcessingTimeService procTimeService) throws Exception {
+		this.pendingPartitionsState = stateStore.getListState(PENDING_PARTITIONS_STATE_DESC);
+		this.pendingPartitions = new HashMap<>();
+		if (isRestored) {
+			pendingPartitions.putAll(pendingPartitionsState.get().iterator().next());
+		}
+
 		this.procTimeService = procTimeService;
-		this.fileSystem = fileSystem;
-		this.locationPath = locationPath;
 		this.commitDelay = conf.get(SINK_PARTITION_COMMIT_DELAY).toMillis();
 	}
 
 	@Override
-	public List<String> committablePartitions(long checkpointId) throws IOException {
+	public void addPartition(String partition) {
+		if (!StringUtils.isNullOrWhitespaceOnly(partition)) {
+			this.pendingPartitions.putIfAbsent(partition, procTimeService.getCurrentProcessingTime());
+		}
+	}
+
+	@Override
+	public List<String> committablePartitions(long checkpointId) {
 		List<String> needCommit = new ArrayList<>();
 		long currentProcTime = procTimeService.getCurrentProcessingTime();
-		Iterator<String> iter = pendingPartitions.iterator();
+		Iterator<Map.Entry<String, Long>> iter = pendingPartitions.entrySet().iterator();
 		while (iter.hasNext()) {
-			String partition = iter.next();
-			long creationTime = fileSystem.getFileStatus(new Path(locationPath, partition))
-					.getModificationTime();
-			if (currentProcTime > creationTime + commitDelay) {
-				needCommit.add(partition);
+			Map.Entry<String, Long> entry = iter.next();
+			long creationTime = entry.getValue();
+			if (commitDelay == 0 || currentProcTime > creationTime + commitDelay) {
+				needCommit.add(entry.getKey());
 				iter.remove();
 			}
 		}
 		return needCommit;
 	}
+
+	@Override
+	public void snapshotState(long checkpointId, long watermark) throws Exception {
+		pendingPartitionsState.clear();
+		pendingPartitionsState.add(new HashMap<>(pendingPartitions));
+	}
+
+	@Override
+	public List<String> endInput() {
+		ArrayList<String> partitions = new ArrayList<>(pendingPartitions.keySet());
+		pendingPartitions.clear();
+		return partitions;
+	}
 }
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/StreamingFileCommitter.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/StreamingFileCommitter.java
index d8aa2831ce8..ec63faf62ab 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/StreamingFileCommitter.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/StreamingFileCommitter.java
@@ -19,7 +19,6 @@
 package org.apache.flink.table.filesystem.stream;
 
 import org.apache.flink.configuration.Configuration;
-import org.apache.flink.core.fs.FileSystem;
 import org.apache.flink.core.fs.Path;
 import org.apache.flink.runtime.state.StateInitializationContext;
 import org.apache.flink.runtime.state.StateSnapshotContext;
@@ -29,10 +28,12 @@ import org.apache.flink.streaming.api.watermark.Watermark;
 import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
 import org.apache.flink.table.catalog.ObjectIdentifier;
 import org.apache.flink.table.filesystem.EmptyMetaStoreFactory;
+import org.apache.flink.table.filesystem.FileSystemFactory;
 import org.apache.flink.table.filesystem.MetastoreCommitPolicy;
 import org.apache.flink.table.filesystem.PartitionCommitPolicy;
 import org.apache.flink.table.filesystem.TableMetaStoreFactory;
 
+import java.io.IOException;
 import java.io.Serializable;
 import java.util.ArrayList;
 import java.util.HashSet;
@@ -75,6 +76,8 @@ public class StreamingFileCommitter extends AbstractStreamOperator<Void>
 
 	private final TableMetaStoreFactory metaStoreFactory;
 
+	private final FileSystemFactory fsFactory;
+
 	private transient PartitionCommitTrigger trigger;
 
 	private transient TaskTracker taskTracker;
@@ -88,11 +91,13 @@ public class StreamingFileCommitter extends AbstractStreamOperator<Void>
 			ObjectIdentifier tableIdentifier,
 			List<String> partitionKeys,
 			TableMetaStoreFactory metaStoreFactory,
+			FileSystemFactory fsFactory,
 			Configuration conf) {
 		this.locationPath = locationPath;
 		this.tableIdentifier = tableIdentifier;
 		this.partitionKeys = partitionKeys;
 		this.metaStoreFactory = metaStoreFactory;
+		this.fsFactory = fsFactory;
 		this.conf = conf;
 		PartitionCommitPolicy.validatePolicyChain(
 				metaStoreFactory instanceof EmptyMetaStoreFactory,
@@ -103,22 +108,25 @@ public class StreamingFileCommitter extends AbstractStreamOperator<Void>
 	public void initializeState(StateInitializationContext context) throws Exception {
 		super.initializeState(context);
 		currentWatermark = Long.MIN_VALUE;
-		FileSystem fileSystem = locationPath.getFileSystem();
 		this.trigger = PartitionCommitTrigger.create(
 				context.isRestored(),
 				context.getOperatorStateStore(),
 				conf,
 				getUserCodeClassloader(),
 				partitionKeys,
-				getProcessingTimeService(),
-				fileSystem,
-				locationPath);
+				getProcessingTimeService());
 		this.policies = PartitionCommitPolicy.createPolicyChain(
 				getUserCodeClassloader(),
 				conf.get(SINK_PARTITION_COMMIT_POLICY_KIND),
 				conf.get(SINK_PARTITION_COMMIT_POLICY_CLASS),
 				conf.get(SINK_PARTITION_COMMIT_SUCCESS_FILE_NAME),
-				fileSystem);
+				() -> {
+					try {
+						return fsFactory.create(locationPath.toUri());
+					} catch (IOException e) {
+						throw new RuntimeException(e);
+					}
+				});
 	}
 
 	@Override
