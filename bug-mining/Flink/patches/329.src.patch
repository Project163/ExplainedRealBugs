diff --git a/flink-clients/src/main/java/org/apache/flink/client/LocalExecutor.java b/flink-clients/src/main/java/org/apache/flink/client/LocalExecutor.java
index b1705df7d83..22e17d0402b 100644
--- a/flink-clients/src/main/java/org/apache/flink/client/LocalExecutor.java
+++ b/flink-clients/src/main/java/org/apache/flink/client/LocalExecutor.java
@@ -30,6 +30,7 @@ import org.apache.flink.api.common.Program;
 import org.apache.flink.configuration.ConfigConstants;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.runtime.client.JobClient;
+import org.apache.flink.runtime.client.SerializedJobExecutionResult;
 import org.apache.flink.runtime.jobgraph.JobGraph;
 import org.apache.flink.api.java.ExecutionEnvironment;
 import org.apache.flink.optimizer.DataStatistics;
@@ -182,8 +183,10 @@ public class LocalExecutor extends PlanExecutor {
 
 				ActorRef jobClient = flink.getJobClient();
 
-				return JobClient.submitJobAndWait(jobGraph, printStatusDuringExecution,
-						jobClient, flink.timeout());
+				SerializedJobExecutionResult result =
+						JobClient.submitJobAndWait(jobGraph, printStatusDuringExecution, jobClient, flink.timeout());
+
+				return result.toJobExecutionResult(ClassLoader.getSystemClassLoader());
 			}
 			finally {
 				if (shutDownAtEnd) {
diff --git a/flink-clients/src/main/java/org/apache/flink/client/program/Client.java b/flink-clients/src/main/java/org/apache/flink/client/program/Client.java
index 2bbbe91a95f..80bdcb8bb8b 100644
--- a/flink-clients/src/main/java/org/apache/flink/client/program/Client.java
+++ b/flink-clients/src/main/java/org/apache/flink/client/program/Client.java
@@ -47,6 +47,7 @@ import org.apache.flink.configuration.GlobalConfiguration;
 import org.apache.flink.core.fs.Path;
 import org.apache.flink.runtime.client.JobClient;
 import org.apache.flink.runtime.client.JobExecutionException;
+import org.apache.flink.runtime.client.SerializedJobExecutionResult;
 import org.apache.flink.runtime.jobgraph.JobGraph;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -74,7 +75,7 @@ public class Client {
 
 	/**
 	 * If != -1, this field specifies the total number of available slots on the cluster
-	 * conntected to the client.
+	 * connected to the client.
 	 */
 	private int maxSlots = -1;
 
@@ -83,7 +84,7 @@ public class Client {
 	 */
 	private JobID lastJobId = null;
 
-	private ClassLoader userCodeClassLoader; // TODO: use userCodeClassloader to deserialize accumulator results.
+	private ClassLoader userCodeClassLoader;
 	
 	// ------------------------------------------------------------------------
 	//                            Construction
@@ -343,7 +344,15 @@ public class Client {
 
 		try{
 			if (wait) {
-				return JobClient.submitJobAndWait(jobGraph, printStatusDuringExecution, client, timeout);
+				SerializedJobExecutionResult result =
+						JobClient.submitJobAndWait(jobGraph, printStatusDuringExecution, client, timeout);
+				try {
+					return result.toJobExecutionResult(this.userCodeClassLoader);
+				}
+				catch (Exception e) {
+					throw new ProgramInvocationException(
+							"Failed to deserialize the accumulator result after the job execution", e);
+				}
 			}
 			else {
 				JobClient.submitJobDetached(jobGraph, client, timeout);
diff --git a/flink-core/src/main/java/org/apache/flink/api/common/JobExecutionResult.java b/flink-core/src/main/java/org/apache/flink/api/common/JobExecutionResult.java
index 68506ae9c73..62848f75b95 100644
--- a/flink-core/src/main/java/org/apache/flink/api/common/JobExecutionResult.java
+++ b/flink-core/src/main/java/org/apache/flink/api/common/JobExecutionResult.java
@@ -27,12 +27,13 @@ import java.util.Map;
 public class JobExecutionResult extends JobSubmissionResult {
 
 	private long netRuntime;
+
 	private Map<String, Object> accumulatorResults;
 
 	/**
 	 * Creates a new JobExecutionResult.
 	 *
-	 * @param jobID
+	 * @param jobID The job's ID.
 	 * @param netRuntime The net runtime of the job (excluding pre-flight phase like the optimizer)
 	 * @param accumulators A map of all accumulators produced by the job.
 	 */
@@ -93,7 +94,4 @@ public class JobExecutionResult extends JobSubmissionResult {
 		}
 		return (Integer) result;
 	}
-
-
-	// TODO Create convenience methods for the other shipped accumulator types
 }
diff --git a/flink-core/src/main/java/org/apache/flink/api/common/accumulators/Accumulator.java b/flink-core/src/main/java/org/apache/flink/api/common/accumulators/Accumulator.java
index f1aac4da7cd..123d956ead5 100644
--- a/flink-core/src/main/java/org/apache/flink/api/common/accumulators/Accumulator.java
+++ b/flink-core/src/main/java/org/apache/flink/api/common/accumulators/Accumulator.java
@@ -16,20 +16,18 @@
  * limitations under the License.
  */
 
-
 package org.apache.flink.api.common.accumulators;
 
-import java.io.IOException;
-import java.io.ObjectInputStream;
-import java.io.ObjectOutputStream;
 import java.io.Serializable;
 
-
 /**
- * Interface for custom accumulator objects. Data are written to in a UDF and
- * merged by the system at the end of the job. The result can be read at the end
- * of the job from the calling client. Inspired by Hadoop/MapReduce counters.<br>
- * <br>
+ * Accumulators collect distributed statistics or aggregates in a from user functions
+ * and operators. Each parallel instance creates and updates its own accumulator object,
+ * and the different parallel instances of the accumulator are later merged.
+ * merged by the system at the end of the job. The result can be obtained from the
+ * result of a job execution, or from teh web runtime monitor.
+ *
+ * The accumulators are inspired by the Hadoop/MapReduce counters.
  * 
  * The type added to the accumulator might differ from the type returned. This
  * is the case e.g. for a set-accumulator: We add single objects, but the result
@@ -41,7 +39,7 @@ import java.io.Serializable;
  *            Type of the accumulator result as it will be reported to the
  *            client
  */
-public interface Accumulator<V, R extends Serializable> extends Serializable, Cloneable{
+public interface Accumulator<V, R extends Serializable> extends Serializable, Cloneable {
 
 	/**
 	 * @param value
@@ -68,20 +66,10 @@ public interface Accumulator<V, R extends Serializable> extends Serializable, Cl
 	void merge(Accumulator<V, R> other);
 
 	/**
-	 * Serialization method of accumulators
-	 *
-	 * @param oos
-	 * @throws IOException
-	 */
-	void write(ObjectOutputStream oos) throws IOException;
-
-	/**
-	 * Deserialization method of accumulators
+	 * Duplicates the accumulator. All subclasses need to properly implement
+	 * cloning and cannot throw a {@link java.lang.CloneNotSupportedException}
 	 *
-	 * @param ois
-	 * @throws IOException
+	 * @return The duplicated accumulator.
 	 */
-	void read(ObjectInputStream ois) throws IOException;
-
 	Accumulator<V, R> clone();
 }
diff --git a/flink-core/src/main/java/org/apache/flink/api/common/accumulators/AccumulatorHelper.java b/flink-core/src/main/java/org/apache/flink/api/common/accumulators/AccumulatorHelper.java
index da871efe45c..9b0e01947c3 100644
--- a/flink-core/src/main/java/org/apache/flink/api/common/accumulators/AccumulatorHelper.java
+++ b/flink-core/src/main/java/org/apache/flink/api/common/accumulators/AccumulatorHelper.java
@@ -16,7 +16,6 @@
  * limitations under the License.
  */
 
-
 package org.apache.flink.api.common.accumulators;
 
 import java.io.Serializable;
@@ -69,13 +68,27 @@ public class AccumulatorHelper {
 	 * Compare both classes and throw {@link UnsupportedOperationException} if
 	 * they differ
 	 */
+	@SuppressWarnings("rawtypes")
 	public static void compareAccumulatorTypes(Object name,
-			@SuppressWarnings("rawtypes") Class<? extends Accumulator> first,
-			@SuppressWarnings("rawtypes") Class<? extends Accumulator> second)
-			throws UnsupportedOperationException {
+												Class<? extends Accumulator> first,
+												Class<? extends Accumulator> second)
+			throws UnsupportedOperationException
+	{
+		if (first == null || second == null) {
+			throw new NullPointerException();
+		}
+
 		if (first != second) {
-			throw new UnsupportedOperationException("The accumulator object '" + name
-					+ "' was created with two different types: " + first + " and " + second);
+			if (!first.getName().equals(second.getName())) {
+				throw new UnsupportedOperationException("The accumulator object '" + name
+					+ "' was created with two different types: " + first.getName() + " and " + second.getName());
+			} else {
+				// damn, name is the same, but different classloaders
+				throw new UnsupportedOperationException("The accumulator object '" + name
+						+ "' was created with two different classes: " + first + " and " + second
+						+ " Both have the same type (" + first.getName() + ") but different classloaders: "
+						+ first.getClassLoader() + " and " + second.getClassLoader());
+			}
 		}
 	}
 
diff --git a/flink-core/src/main/java/org/apache/flink/api/common/accumulators/DoubleCounter.java b/flink-core/src/main/java/org/apache/flink/api/common/accumulators/DoubleCounter.java
index 6f02650549b..85d187c59ed 100644
--- a/flink-core/src/main/java/org/apache/flink/api/common/accumulators/DoubleCounter.java
+++ b/flink-core/src/main/java/org/apache/flink/api/common/accumulators/DoubleCounter.java
@@ -16,13 +16,11 @@
  * limitations under the License.
  */
 
-
 package org.apache.flink.api.common.accumulators;
 
-import java.io.IOException;
-import java.io.ObjectInputStream;
-import java.io.ObjectOutputStream;
-
+/**
+ * An accumulator that sums up {@code double} values.
+ */
 public class DoubleCounter implements SimpleAccumulator<Double> {
 
 	private static final long serialVersionUID = 1L;
@@ -41,7 +39,7 @@ public class DoubleCounter implements SimpleAccumulator<Double> {
 
 	@Override
 	public void merge(Accumulator<Double, Double> other) {
-		this.localValue += ((DoubleCounter) other).getLocalValue();
+		this.localValue += other.getLocalValue();
 	}
 
 	@Override
@@ -49,27 +47,15 @@ public class DoubleCounter implements SimpleAccumulator<Double> {
 		this.localValue = 0;
 	}
 
-	@Override
-	public void write(ObjectOutputStream out) throws IOException {
-		out.writeDouble(localValue);
-	}
-
-	@Override
-	public void read(ObjectInputStream in) throws IOException {
-		this.localValue = in.readDouble();
-	}
-
 	@Override
 	public Accumulator<Double, Double> clone() {
 		DoubleCounter result = new DoubleCounter();
 		result.localValue = localValue;
-
 		return result;
 	}
 
 	@Override
 	public String toString() {
-		return "DoubleCounter object. Local value: " + this.localValue;
+		return "DoubleCounter " + this.localValue;
 	}
-
 }
diff --git a/flink-core/src/main/java/org/apache/flink/api/common/accumulators/Histogram.java b/flink-core/src/main/java/org/apache/flink/api/common/accumulators/Histogram.java
index e204b818a78..f5e959a03f5 100644
--- a/flink-core/src/main/java/org/apache/flink/api/common/accumulators/Histogram.java
+++ b/flink-core/src/main/java/org/apache/flink/api/common/accumulators/Histogram.java
@@ -16,22 +16,18 @@
  * limitations under the License.
  */
 
-
 package org.apache.flink.api.common.accumulators;
 
-import java.io.IOException;
-import java.io.ObjectInputStream;
-import java.io.ObjectOutputStream;
 import java.util.Map;
 import java.util.TreeMap;
 
 /**
- * Histogram for discrete-data. Let's you populate a histogram distributedly.
+ * Histogram accumulator, which builds a histogram in a distributed manner.
  * Implemented as a Integer->Integer TreeMap, so that the entries are sorted
  * according to the values.
  * 
- * Could be extended to continuous values later, but then we need to dynamically
- * decide about the bin size in an online algorithm (or ask the user)
+ * This class does not extend to continuous values later, because it makes no
+ * attempt to put the data in bins.
  */
 public class Histogram implements Accumulator<Integer, TreeMap<Integer, Integer>> {
 
@@ -54,8 +50,7 @@ public class Histogram implements Accumulator<Integer, TreeMap<Integer, Integer>
 	@Override
 	public void merge(Accumulator<Integer, TreeMap<Integer, Integer>> other) {
 		// Merge the values into this map
-		for (Map.Entry<Integer, Integer> entryFromOther : ((Histogram) other).getLocalValue()
-				.entrySet()) {
+		for (Map.Entry<Integer, Integer> entryFromOther : other.getLocalValue().entrySet()) {
 			Integer ownValue = this.treeMap.get(entryFromOther.getKey());
 			if (ownValue == null) {
 				this.treeMap.put(entryFromOther.getKey(), entryFromOther.getValue());
@@ -75,30 +70,10 @@ public class Histogram implements Accumulator<Integer, TreeMap<Integer, Integer>
 		return this.treeMap.toString();
 	}
 
-	@Override
-	public void write(ObjectOutputStream out) throws IOException {
-		out.writeInt(treeMap.size());
-		for (Map.Entry<Integer, Integer> entry : treeMap.entrySet()) {
-			out.writeInt(entry.getKey());
-			out.writeInt(entry.getValue());
-		}
-	}
-
-	@Override
-	public void read(ObjectInputStream in) throws IOException {
-		int size = in.readInt();
-		for (int i = 0; i < size; ++i) {
-			treeMap.put(in.readInt(), in.readInt());
-		}
-	}
-
 	@Override
 	public Accumulator<Integer, TreeMap<Integer, Integer>> clone() {
 		Histogram result = new Histogram();
-
 		result.treeMap = new TreeMap<Integer, Integer>(treeMap);
-
 		return result;
 	}
-
 }
diff --git a/flink-core/src/main/java/org/apache/flink/api/common/accumulators/IntCounter.java b/flink-core/src/main/java/org/apache/flink/api/common/accumulators/IntCounter.java
index f1c45a1fe18..c88c73d068c 100644
--- a/flink-core/src/main/java/org/apache/flink/api/common/accumulators/IntCounter.java
+++ b/flink-core/src/main/java/org/apache/flink/api/common/accumulators/IntCounter.java
@@ -19,10 +19,9 @@
 
 package org.apache.flink.api.common.accumulators;
 
-import java.io.IOException;
-import java.io.ObjectInputStream;
-import java.io.ObjectOutputStream;
-
+/**
+ * An accumulator that sums up {@code Integer} values.
+ */
 public class IntCounter implements SimpleAccumulator<Integer> {
 
 	private static final long serialVersionUID = 1L;
@@ -41,7 +40,7 @@ public class IntCounter implements SimpleAccumulator<Integer> {
 
 	@Override
 	public void merge(Accumulator<Integer, Integer> other) {
-		this.localValue += ((IntCounter) other).getLocalValue();
+		this.localValue += other.getLocalValue();
 	}
 
 	@Override
@@ -49,27 +48,15 @@ public class IntCounter implements SimpleAccumulator<Integer> {
 		this.localValue = 0;
 	}
 
-	@Override
-	public void write(ObjectOutputStream out) throws IOException {
-		out.writeInt(localValue);
-	}
-
-	@Override
-	public void read(ObjectInputStream in) throws IOException {
-		localValue = in.readInt();
-	}
-
 	@Override
 	public Accumulator<Integer, Integer> clone() {
 		IntCounter result = new IntCounter();
 		result.localValue = localValue;
-
 		return result;
 	}
 
 	@Override
 	public String toString() {
-		return "IntCounter object. Local value: " + this.localValue;
+		return "IntCounter " + this.localValue;
 	}
-
 }
diff --git a/flink-core/src/main/java/org/apache/flink/api/common/accumulators/ListAccumulator.java b/flink-core/src/main/java/org/apache/flink/api/common/accumulators/ListAccumulator.java
index 5a973ac3887..3af785cfde9 100644
--- a/flink-core/src/main/java/org/apache/flink/api/common/accumulators/ListAccumulator.java
+++ b/flink-core/src/main/java/org/apache/flink/api/common/accumulators/ListAccumulator.java
@@ -16,42 +16,29 @@
  * limitations under the License.
  */
 
-
 package org.apache.flink.api.common.accumulators;
 
-import java.io.IOException;
-import java.io.ObjectInputStream;
-import java.io.ObjectOutputStream;
-import java.io.Serializable;
 import java.util.ArrayList;
 
-import org.apache.commons.lang3.SerializationUtils;
-
 /**
- * * This accumulator stores a collection of objects which are immediately serialized to cope with object reuse.
- * * When the objects are requested again, they are deserialized.
+ * This accumulator stores a collection of objects.
+ *
  * @param <T> The type of the accumulated objects
  */
 public class ListAccumulator<T> implements Accumulator<T, ArrayList<T>> {
 
 	private static final long serialVersionUID = 1L;
 
-	private ArrayList<byte[]> localValue = new ArrayList<byte[]>();
+	private ArrayList<T> localValue = new ArrayList<T>();
 	
 	@Override
 	public void add(T value) {
-		byte[] byteArray = SerializationUtils.serialize((Serializable) value);
-		localValue.add(byteArray);
+		localValue.add(value);
 	}
 
 	@Override
 	public ArrayList<T> getLocalValue() {
-		ArrayList<T> arrList = new ArrayList<T>();
-		for (byte[] byteArr : localValue) {
-			T item = SerializationUtils.deserialize(byteArr);
-			arrList.add(item);
-		}
-		return arrList;
+		return localValue;
 	}
 
 	@Override
@@ -61,37 +48,18 @@ public class ListAccumulator<T> implements Accumulator<T, ArrayList<T>> {
 
 	@Override
 	public void merge(Accumulator<T, ArrayList<T>> other) {
-		localValue.addAll(((ListAccumulator<T>) other).localValue);
+		localValue.addAll(other.getLocalValue());
 	}
 
 	@Override
 	public Accumulator<T, ArrayList<T>> clone() {
 		ListAccumulator<T> newInstance = new ListAccumulator<T>();
-		for (byte[] item : localValue) {
-			newInstance.localValue.add(item.clone());
-		}
+		newInstance.localValue = new ArrayList<T>(localValue);
 		return newInstance;
 	}
 
 	@Override
-	public void write(ObjectOutputStream out) throws IOException {
-		int numItems = localValue.size();
-		out.writeInt(numItems);
-		for (byte[] item : localValue) {
-			out.writeInt(item.length);
-			out.write(item);
-		}
+	public String toString() {
+		return "List Accumulator " + localValue;
 	}
-
-	@Override
-	public void read(ObjectInputStream in) throws IOException {
-		int numItems = in.readInt();
-		for (int i = 0; i < numItems; i++) {
-			int len = in.readInt();
-			byte[] obj = new byte[len];
-			in.read(obj);
-			localValue.add(obj);
-		}
-	}
-
 }
diff --git a/flink-core/src/main/java/org/apache/flink/api/common/accumulators/LongCounter.java b/flink-core/src/main/java/org/apache/flink/api/common/accumulators/LongCounter.java
index dfa35264cec..70f2417b0c3 100644
--- a/flink-core/src/main/java/org/apache/flink/api/common/accumulators/LongCounter.java
+++ b/flink-core/src/main/java/org/apache/flink/api/common/accumulators/LongCounter.java
@@ -16,13 +16,11 @@
  * limitations under the License.
  */
 
-
 package org.apache.flink.api.common.accumulators;
 
-import java.io.IOException;
-import java.io.ObjectInputStream;
-import java.io.ObjectOutputStream;
-
+/**
+ * An accumulator that sums up {@code long} values.
+ */
 public class LongCounter implements SimpleAccumulator<Long> {
 
 	private static final long serialVersionUID = 1L;
@@ -41,7 +39,7 @@ public class LongCounter implements SimpleAccumulator<Long> {
 	
 	@Override
 	public void merge(Accumulator<Long, Long> other) {
-		this.localValue += ((LongCounter)other).getLocalValue();
+		this.localValue += other.getLocalValue();
 	}
 	
 	@Override
@@ -50,26 +48,14 @@ public class LongCounter implements SimpleAccumulator<Long> {
 	}
 
 	@Override
-	public void write(ObjectOutputStream out) throws IOException {
-		out.writeLong(this.localValue);
-	}
-
-	@Override
-	public void read(ObjectInputStream in) throws IOException {
-		this.localValue = in.readLong();
-	}
-
-	@Override
-	public Accumulator<Long, Long> clone() {
+	public LongCounter clone() {
 		LongCounter result = new LongCounter();
 		result.localValue = localValue;
-
 		return result;
 	}
 
 	@Override
 	public String toString() {
-		return "LongCounter object. Local value: " + this.localValue;
+		return "LongCounter " + this.localValue;
 	}
-
 }
diff --git a/flink-core/src/main/java/org/apache/flink/api/common/accumulators/SerializedListAccumulator.java b/flink-core/src/main/java/org/apache/flink/api/common/accumulators/SerializedListAccumulator.java
new file mode 100644
index 00000000000..4ab339b1a6c
--- /dev/null
+++ b/flink-core/src/main/java/org/apache/flink/api/common/accumulators/SerializedListAccumulator.java
@@ -0,0 +1,103 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.api.common.accumulators;
+
+import org.apache.flink.util.InstantiationUtil;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+/**
+ * This accumulator stores a collection of objects in serialized form, so that the stored objects
+ * are not affected by modifications to the original objects.
+ *
+ * Objects may be deserialized on demand with a specific classloader.
+ *
+ * @param <T> The type of the accumulated objects
+ */
+public class SerializedListAccumulator<T> implements Accumulator<T, ArrayList<byte[]>> {
+
+	private static final long serialVersionUID = 1L;
+
+	private ArrayList<byte[]> localValue = new ArrayList<byte[]>();
+
+	@Override
+	public void add(T value) {
+		if (value == null) {
+			throw new NullPointerException("Value to accumulate must nor be null");
+		}
+
+		try {
+			byte[] byteArray = InstantiationUtil.serializeObject(value);
+			localValue.add(byteArray);
+		}
+		catch (IOException e) {
+			throw new RuntimeException("Serialization of accumulated value failed", e);
+		}
+	}
+
+	@Override
+	public ArrayList<byte[]> getLocalValue() {
+		return localValue;
+	}
+
+	public ArrayList<T> deserializeLocalValue(ClassLoader classLoader) {
+		try {
+			ArrayList<T> arrList = new ArrayList<T>(localValue.size());
+			for (byte[] byteArr : localValue) {
+				@SuppressWarnings("unchecked")
+				T item = (T) InstantiationUtil.deserializeObject(byteArr, classLoader);
+				arrList.add(item);
+			}
+			return arrList;
+		}
+		catch (Exception e) {
+			throw new RuntimeException("Cannot deserialize accumulator list element", e);
+		}
+	}
+
+	@Override
+	public void resetLocal() {
+		localValue.clear();
+	}
+
+	@Override
+	public void merge(Accumulator<T, ArrayList<byte[]>> other) {
+		localValue.addAll(other.getLocalValue());
+	}
+
+	@Override
+	public SerializedListAccumulator<T> clone() {
+		SerializedListAccumulator<T> newInstance = new SerializedListAccumulator<T>();
+		newInstance.localValue = new ArrayList<byte[]>(localValue);
+		return newInstance;
+	}
+
+	@SuppressWarnings("unchecked")
+	public static <T> List<T> deserializeList(ArrayList<byte[]> data, ClassLoader loader)
+			throws IOException, ClassNotFoundException
+	{
+		List<T> result = new ArrayList<T>(data.size());
+		for (byte[] bytes : data) {
+			result.add((T) InstantiationUtil.deserializeObject(bytes, loader));
+		}
+		return result;
+	}
+}
diff --git a/flink-core/src/main/java/org/apache/flink/api/common/accumulators/SimpleAccumulator.java b/flink-core/src/main/java/org/apache/flink/api/common/accumulators/SimpleAccumulator.java
index 596bf63c1a3..6614e137ea5 100644
--- a/flink-core/src/main/java/org/apache/flink/api/common/accumulators/SimpleAccumulator.java
+++ b/flink-core/src/main/java/org/apache/flink/api/common/accumulators/SimpleAccumulator.java
@@ -16,7 +16,6 @@
  * limitations under the License.
  */
 
-
 package org.apache.flink.api.common.accumulators;
 
 import java.io.Serializable;
diff --git a/flink-core/src/main/java/org/apache/flink/api/common/functions/util/AbstractRuntimeUDFContext.java b/flink-core/src/main/java/org/apache/flink/api/common/functions/util/AbstractRuntimeUDFContext.java
index c04548c686d..ff531c73dd4 100644
--- a/flink-core/src/main/java/org/apache/flink/api/common/functions/util/AbstractRuntimeUDFContext.java
+++ b/flink-core/src/main/java/org/apache/flink/api/common/functions/util/AbstractRuntimeUDFContext.java
@@ -110,7 +110,7 @@ public abstract class AbstractRuntimeUDFContext implements RuntimeContext {
 	@Override
 	public <V, A extends Serializable> void addAccumulator(String name, Accumulator<V, A> accumulator) {
 		if (accumulators.containsKey(name)) {
-			throw new UnsupportedOperationException("The counter '" + name
+			throw new UnsupportedOperationException("The accumulator '" + name
 					+ "' already exists and cannot be added.");
 		}
 		accumulators.put(name, accumulator);
@@ -141,8 +141,8 @@ public abstract class AbstractRuntimeUDFContext implements RuntimeContext {
 	
 	@SuppressWarnings("unchecked")
 	private <V, A extends Serializable> Accumulator<V, A> getAccumulator(String name,
-			Class<? extends Accumulator<V, A>> accumulatorClass) {
-
+			Class<? extends Accumulator<V, A>> accumulatorClass)
+	{
 		Accumulator<?, ?> accumulator = accumulators.get(name);
 
 		if (accumulator != null) {
@@ -151,10 +151,9 @@ public abstract class AbstractRuntimeUDFContext implements RuntimeContext {
 			// Create new accumulator
 			try {
 				accumulator = accumulatorClass.newInstance();
-			} catch (InstantiationException e) {
-				e.printStackTrace();
-			} catch (IllegalAccessException e) {
-				e.printStackTrace();
+			}
+			catch (Exception e) {
+				throw new RuntimeException("Cannot create accumulator " + accumulatorClass.getName());
 			}
 			accumulators.put(name, accumulator);
 		}
diff --git a/flink-examples/flink-java-examples/src/main/java/org/apache/flink/examples/java/relational/EmptyFieldsCountAccumulator.java b/flink-examples/flink-java-examples/src/main/java/org/apache/flink/examples/java/relational/EmptyFieldsCountAccumulator.java
index 09b88fa88a1..2016eaa8530 100644
--- a/flink-examples/flink-java-examples/src/main/java/org/apache/flink/examples/java/relational/EmptyFieldsCountAccumulator.java
+++ b/flink-examples/flink-java-examples/src/main/java/org/apache/flink/examples/java/relational/EmptyFieldsCountAccumulator.java
@@ -15,11 +15,9 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.flink.examples.java.relational;
 
-import java.io.IOException;
-import java.io.ObjectInputStream;
-import java.io.ObjectOutputStream;
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.List;
@@ -206,14 +204,14 @@ public class EmptyFieldsCountAccumulator {
 		 * Increases the result vector component at the specified position by 1.
 		 */
 		@Override
-		public void add(final Integer position) {
+		public void add(Integer position) {
 			updateResultVector(position, 1);
 		}
 
 		/**
 		 * Increases the result vector component at the specified position by the specified delta.
 		 */
-		private void updateResultVector(final int position, final int delta) {
+		private void updateResultVector(int position, int delta) {
 			// inflate the vector to contain the given position
 			while (this.resultVector.size() <= position) {
 				this.resultVector.add(0);
@@ -244,32 +242,9 @@ public class EmptyFieldsCountAccumulator {
 			}
 		}
 
-		@Override
-		public void write(final ObjectOutputStream out) throws IOException {
-			// binary serialization of the result vector:
-			// [number of components, component 0, component 1, ...]
-			out.writeInt(this.resultVector.size());
-			for (final Integer component : this.resultVector) {
-				out.writeInt(component);
-			}
-		}
-
-		@Override
-		public void read(final ObjectInputStream in) throws IOException {
-			// binary deserialization of the result vector
-			final int size = in.readInt();
-			for (int numReadComponents = 0; numReadComponents < size; numReadComponents++) {
-				final int component = in.readInt();
-				this.resultVector.add(component);
-			}
-		}
-
 		@Override
 		public Accumulator<Integer, ArrayList<Integer>> clone() {
-			VectorAccumulator result = new VectorAccumulator(new ArrayList<Integer>(resultVector));
-
-			return result;
+			return new VectorAccumulator(new ArrayList<Integer>(resultVector));
 		}
-
 	}
 }
diff --git a/flink-java/src/main/java/org/apache/flink/api/java/DataSet.java b/flink-java/src/main/java/org/apache/flink/api/java/DataSet.java
index 1e91eeb0b22..7cee323bef4 100644
--- a/flink-java/src/main/java/org/apache/flink/api/java/DataSet.java
+++ b/flink-java/src/main/java/org/apache/flink/api/java/DataSet.java
@@ -18,11 +18,14 @@
 
 package org.apache.flink.api.java;
 
+import java.io.IOException;
+import java.util.ArrayList;
 import java.util.List;
 
 import org.apache.commons.lang3.Validate;
 import org.apache.flink.api.common.InvalidProgramException;
 import org.apache.flink.api.common.JobExecutionResult;
+import org.apache.flink.api.common.accumulators.SerializedListAccumulator;
 import org.apache.flink.api.common.functions.FilterFunction;
 import org.apache.flink.api.common.functions.GroupCombineFunction;
 import org.apache.flink.api.common.functions.FlatMapFunction;
@@ -399,7 +402,8 @@ public abstract class DataSet<T> {
 	}
 
 
-	 /* Convenience method to get the elements of a DataSet as a List
+	/**
+	 * Convenience method to get the elements of a DataSet as a List
 	 * As DataSet can contain a lot of data, this method should be used with caution.
 	 *
 	 * @return A List containing the elements of the DataSet
@@ -407,15 +411,31 @@ public abstract class DataSet<T> {
 	 * @see org.apache.flink.api.java.Utils.CollectHelper
 	 */
 	public List<T> collect() throws Exception {
+		// validate that our type is actually serializable
+		Class<?> typeClass = getType().getTypeClass();
+		ClassLoader cl = typeClass.getClassLoader() == null ? ClassLoader.getSystemClassLoader()
+															: typeClass.getClassLoader();
 
-		final String id = new AbstractID().toString();
+		if (!java.io.Serializable.class.isAssignableFrom(typeClass)) {
+			throw new UnsupportedOperationException("collect() can only be used with serializable data types. "
+					+ "The DataSet type '" + typeClass.getName() + "' does not implement java.io.Serializable.");
+		}
 
-		this.flatMap(new Utils.CollectHelper<T>(id)).output(
-				new DiscardingOutputFormat<T>());
+		final String id = new AbstractID().toString();
 
-		JobExecutionResult res = this.getExecutionEnvironment().execute();
+		this.flatMap(new Utils.CollectHelper<T>(id)).output(new DiscardingOutputFormat<T>());
+		JobExecutionResult res = getExecutionEnvironment().execute();
 
-		return (List<T>) res.getAccumulatorResult(id);
+		ArrayList<byte[]> accResult = res.getAccumulatorResult(id);
+		try {
+			return SerializedListAccumulator.deserializeList(accResult, cl);
+		}
+		catch (ClassNotFoundException e) {
+			throw new RuntimeException("Cannot find type class of collected data type.", e);
+		}
+		catch (IOException e) {
+			throw new RuntimeException("Serialization error while deserializing collected data", e);
+		}
 	}
 
 	/**
diff --git a/flink-java/src/main/java/org/apache/flink/api/java/Utils.java b/flink-java/src/main/java/org/apache/flink/api/java/Utils.java
index b860b77465b..5351484d72f 100644
--- a/flink-java/src/main/java/org/apache/flink/api/java/Utils.java
+++ b/flink-java/src/main/java/org/apache/flink/api/java/Utils.java
@@ -19,6 +19,7 @@
 package org.apache.flink.api.java;
 
 import org.apache.commons.lang3.StringUtils;
+import org.apache.flink.api.common.accumulators.SerializedListAccumulator;
 import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.api.common.typeutils.CompositeType;
 import org.apache.flink.api.java.typeutils.GenericTypeInfo;
@@ -26,7 +27,6 @@ import org.apache.flink.api.java.typeutils.GenericTypeInfo;
 import java.lang.reflect.Field;
 import java.lang.reflect.Modifier;
 import java.util.List;
-import org.apache.flink.api.common.accumulators.ListAccumulator;
 import org.apache.flink.api.common.functions.RichFlatMapFunction;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.util.Collector;
@@ -97,11 +97,11 @@ public class Utils {
 		private static final long serialVersionUID = 1L;
 
 		private final String id;
-		private final ListAccumulator<T> accumulator;
+		private final SerializedListAccumulator<T> accumulator;
 
 		public CollectHelper(String id) {
 			this.id = id;
-			this.accumulator = new ListAccumulator<T>();
+			this.accumulator = new SerializedListAccumulator<T>();
 		}
 
 		@Override
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/accumulators/AccumulatorEvent.java b/flink-runtime/src/main/java/org/apache/flink/runtime/accumulators/AccumulatorEvent.java
index f345d6af013..ad7fabd96e0 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/accumulators/AccumulatorEvent.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/accumulators/AccumulatorEvent.java
@@ -18,179 +18,32 @@
 
 package org.apache.flink.runtime.accumulators;
 
-import java.io.ByteArrayInputStream;
-import java.io.ByteArrayOutputStream;
 import java.io.IOException;
-import java.io.ObjectInputStream;
-import java.io.ObjectOutputStream;
-import java.io.Serializable;
-import java.util.Collections;
-import java.util.HashMap;
 import java.util.Map;
 
 import org.apache.flink.api.common.accumulators.Accumulator;
 import org.apache.flink.api.common.JobID;
-import org.apache.flink.util.InstantiationUtil;
+import org.apache.flink.runtime.util.SerializedValue;
 
 /**
  * This class encapsulates a map of accumulators for a single job. It is used
  * for the transfer from TaskManagers to the JobManager and from the JobManager
  * to the Client.
  */
-public class AccumulatorEvent implements Serializable {
+public class AccumulatorEvent extends SerializedValue<Map<String, Accumulator<?, ?>>> {
 
 	private static final long serialVersionUID = 8965894516006882735L;
 
-	private JobID jobID;
-	
-	private Map<String, Accumulator<?, ?>> accumulators;
-	
-	// staging deserialized data until the classloader is available
-	private byte[] serializedData;
+	/** JobID for the target job */
+	private final JobID jobID;
 
 
-	// Removing this causes an EOFException in the RPC service. The RPC should
-	// be improved in this regard (error message is very unspecific).
-	public AccumulatorEvent() {
-		this.accumulators = Collections.emptyMap();
-	}
-
-	public AccumulatorEvent(JobID jobID, Map<String, Accumulator<?, ?>> accumulators) {
-		this.accumulators = accumulators;
+	public AccumulatorEvent(JobID jobID, Map<String, Accumulator<?, ?>> accumulators) throws IOException {
+		super(accumulators);
 		this.jobID = jobID;
 	}
 
 	public JobID getJobID() {
 		return this.jobID;
 	}
-
-	public Map<String, Accumulator<?, ?>> getAccumulators(ClassLoader loader) {
-		if (loader == null) {
-			throw new NullPointerException();
-		}
-		
-		if (this.accumulators == null) {
-			// deserialize
-			// we have read the binary data, but not yet turned into the objects
-			ByteArrayInputStream bais = new ByteArrayInputStream(serializedData);
-			ObjectInputStream ios = null;
-			try {
-				ios = new ObjectInputStream(bais);
-			} catch (IOException e) {
-				throw new RuntimeException("Error while creating the object input stream.");
-			}
-
-			int numAccumulators = 0;
-			try {
-				numAccumulators = ios.readInt();
-			} catch (IOException e) {
-				throw new RuntimeException("Error while reading the number of serialized " +
-						"accumulators.");
-			}
-
-			this.accumulators = new HashMap<String, Accumulator<?, ?>>(numAccumulators);
-
-			for (int i = 0; i < numAccumulators; i++) {
-				String accumulatorName;
-				try {
-					accumulatorName = ios.readUTF();
-				} catch (IOException e) {
-					throw new RuntimeException("Error while reading the "+ i +"th accumulator " +
-							"name.");
-				}
-
-				String className;
-				try {
-					className = ios.readUTF();
-				} catch (IOException e) {
-					throw new RuntimeException("Error while reading the "+ i +"th accumulator " +
-							"class name.");
-				}
-				Accumulator<?, ?> accumulator;
-
-				try {
-					@SuppressWarnings("unchecked")
-					Class<? extends Accumulator<?, ?>> valClass = (Class<? extends Accumulator<?,
-							?>>) Class.forName(className, true, loader);
-					accumulator = InstantiationUtil.instantiate(valClass, Accumulator.class);
-				} catch (ClassNotFoundException e) {
-					throw new RuntimeException("Could not load user-defined class '" +
-							className + "'.", e);
-				} catch (ClassCastException e) {
-					throw new RuntimeException("User-defined accumulator class is not an Accumulator sublass.");
-				}
-
-				try {
-					accumulator.read(ios);
-				} catch (IOException e) {
-					throw new RuntimeException("Error while deserializing the user-defined aggregate class.", e);
-				}
-
-				accumulators.put(accumulatorName, accumulator);
-
-			}
-
-			try {
-				ios.close();
-			} catch (IOException e) {
-				throw new RuntimeException("Error while closing the InputObjectStream.");
-			}
-
-			try {
-				bais.close();
-			} catch (IOException e) {
-				throw new RuntimeException("Error while closing the ByteArrayInputStream.");
-			}
-
-		}
-
-		return accumulators;
-	}
-
-	private void writeObject(java.io.ObjectOutputStream out) throws IOException{
-		out.writeObject(jobID);
-
-		byte[] buffer = null;
-
-		if(accumulators != null) {
-			ByteArrayOutputStream baos = new ByteArrayOutputStream();
-			ObjectOutputStream oos = new ObjectOutputStream(baos);
-
-			oos.writeInt(accumulators.size());
-
-			for (Map.Entry<String, Accumulator<?, ?>> entry : this.accumulators.entrySet()) {
-				oos.writeUTF(entry.getKey());
-				oos.writeUTF(entry.getValue().getClass().getName());
-
-				entry.getValue().write(oos);
-			}
-
-			oos.flush();
-			oos.close();
-			baos.close();
-
-			buffer = baos.toByteArray();
-		} else if(serializedData != null) {
-			buffer = serializedData;
-		} else {
-			throw new RuntimeException("The AccumulatorEvent's accumulator is null and there is " +
-					"no serialized data attached to it.");
-		}
-
-		out.writeInt(buffer.length);
-		out.write(buffer);
-	}
-
-	private void readObject(java.io.ObjectInputStream in) throws IOException,
-			ClassNotFoundException{
-		this.accumulators = null; // this makes sure we deserialize
-
-		jobID = (JobID) in.readObject();
-
-		int bufferLength = in.readInt();
-
-		serializedData = new byte[bufferLength];
-
-		in.readFully(serializedData);
-	}
 }
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/accumulators/StringifiedAccumulatorResult.java b/flink-runtime/src/main/java/org/apache/flink/runtime/accumulators/StringifiedAccumulatorResult.java
new file mode 100644
index 00000000000..3b25fe0bdf6
--- /dev/null
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/accumulators/StringifiedAccumulatorResult.java
@@ -0,0 +1,49 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.accumulators;
+
+/**
+ * Container class that transports the result of an accumulator as set of strings.
+ */
+public class StringifiedAccumulatorResult implements java.io.Serializable{
+
+	private static final long serialVersionUID = -4642311296836822611L;
+
+	private final String name;
+	private final String type;
+	private final String value;
+
+	public StringifiedAccumulatorResult(String name, String type, String value) {
+		this.name = name;
+		this.type = type;
+		this.value = value;
+	}
+
+	public String getName() {
+		return name;
+	}
+
+	public String getType() {
+		return type;
+	}
+
+	public String getValue() {
+		return value;
+	}
+}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/client/SerializedJobExecutionResult.java b/flink-runtime/src/main/java/org/apache/flink/runtime/client/SerializedJobExecutionResult.java
new file mode 100644
index 00000000000..e0b1ad149d4
--- /dev/null
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/client/SerializedJobExecutionResult.java
@@ -0,0 +1,85 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.client;
+
+import org.apache.flink.api.common.JobExecutionResult;
+import org.apache.flink.api.common.JobID;
+import org.apache.flink.runtime.util.SerializedValue;
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+
+/**
+ * A variant of the {@link org.apache.flink.api.common.JobExecutionResult} that holds
+ * its accumulator data in serialized form.
+ */
+public class SerializedJobExecutionResult implements java.io.Serializable {
+
+	private static final long serialVersionUID = -6301865617099921789L;
+
+	private final JobID jobId;
+
+	private final Map<String, SerializedValue<Object>> accumulatorResults;
+
+	private final long netRuntime;
+
+	/**
+	 * Creates a new SerializedJobExecutionResult.
+	 *
+	 * @param jobID The job's ID.
+	 * @param netRuntime The net runtime of the job (excluding pre-flight phase like the optimizer)
+	 * @param accumulators A map of all accumulator results produced by the job, in serialized form
+	 */
+	public SerializedJobExecutionResult(JobID jobID, long netRuntime,
+										Map<String, SerializedValue<Object>> accumulators) {
+		this.jobId = jobID;
+		this.netRuntime = netRuntime;
+		this.accumulatorResults = accumulators;
+	}
+
+	public JobID getJobId() {
+		return jobId;
+	}
+
+	public long getNetRuntime() {
+		return netRuntime;
+	}
+
+	public Map<String, SerializedValue<Object>> getSerializedAccumulatorResults() {
+		return this.accumulatorResults;
+	}
+
+	public JobExecutionResult toJobExecutionResult(ClassLoader loader) throws IOException, ClassNotFoundException {
+		Map<String, Object> accumulators = null;
+		if (accumulatorResults != null) {
+			accumulators = accumulatorResults.isEmpty() ?
+									Collections.<String, Object>emptyMap() :
+									new HashMap<String, Object>(this.accumulatorResults.size());
+
+			for (Map.Entry<String, SerializedValue<Object>> entry : this.accumulatorResults.entrySet()) {
+				Object o = entry.getValue() == null ? null : entry.getValue().deserializeValue(loader);
+				accumulators.put(entry.getKey(), o);
+			}
+		}
+
+		return new JobExecutionResult(jobId, netRuntime, accumulators);
+	}
+}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/execution/RuntimeEnvironment.java b/flink-runtime/src/main/java/org/apache/flink/runtime/execution/RuntimeEnvironment.java
index 5416f483faf..d5671565229 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/execution/RuntimeEnvironment.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/execution/RuntimeEnvironment.java
@@ -20,7 +20,6 @@ package org.apache.flink.runtime.execution;
 
 import akka.actor.ActorRef;
 import org.apache.flink.api.common.accumulators.Accumulator;
-import org.apache.flink.api.common.accumulators.AccumulatorHelper;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.core.fs.Path;
 import org.apache.flink.runtime.accumulators.AccumulatorEvent;
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionGraph.java b/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionGraph.java
index 5ce89b3efc6..08e44f96e03 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionGraph.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionGraph.java
@@ -176,6 +176,8 @@ public class ExecutionGraph implements Serializable {
 	 * Once this value has reached the number of vertices, the job is done. */
 	private int nextVertexToFinish;
 
+
+
 	private ActorContext parentContext;
 
 	private  ActorRef stateCheckpointerActor;
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/accumulators/AccumulatorManager.java b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/accumulators/AccumulatorManager.java
index 5bf8ebe1566..c8242321f3b 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/accumulators/AccumulatorManager.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/accumulators/AccumulatorManager.java
@@ -16,15 +16,18 @@
  * limitations under the License.
  */
 
-
 package org.apache.flink.runtime.jobmanager.accumulators;
 
+import java.io.IOException;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.LinkedList;
 import java.util.Map;
 
 import org.apache.flink.api.common.accumulators.Accumulator;
 import org.apache.flink.api.common.JobID;
+import org.apache.flink.runtime.accumulators.StringifiedAccumulatorResult;
+import org.apache.flink.runtime.util.SerializedValue;
 
 /**
  * This class manages the accumulators for different jobs. Either the jobs are
@@ -38,12 +41,13 @@ import org.apache.flink.api.common.JobID;
  */
 public class AccumulatorManager {
 
-	// Map of accumulators belonging to recently started jobs
+	/** Map of accumulators belonging to recently started jobs */
 	private final Map<JobID, JobAccumulators> jobAccumulators = new HashMap<JobID, JobAccumulators>();
 
 	private final LinkedList<JobID> lru = new LinkedList<JobID>();
 	private int maxEntries;
 
+
 	public AccumulatorManager(int maxEntries) {
 		this.maxEntries = maxEntries;
 	}
@@ -69,11 +73,13 @@ public class AccumulatorManager {
 	public Map<String, Object> getJobAccumulatorResults(JobID jobID) {
 		Map<String, Object> result = new HashMap<String, Object>();
 
-		JobAccumulators jobAccumulator = jobAccumulators.get(jobID);
+		JobAccumulators acc;
+		synchronized (jobAccumulators) {
+			acc = jobAccumulators.get(jobID);
+		}
 
-		if(jobAccumulator != null) {
-			for (Map.Entry<String, Accumulator<?, ?>> entry : jobAccumulator.getAccumulators().
-					entrySet()) {
+		if (acc != null) {
+			for (Map.Entry<String, Accumulator<?, ?>> entry : acc.getAccumulators().entrySet()) {
 				result.put(entry.getKey(), entry.getValue().getLocalValue());
 			}
 		}
@@ -81,9 +87,50 @@ public class AccumulatorManager {
 		return result;
 	}
 
+	public Map<String, SerializedValue<Object>> getJobAccumulatorResultsSerialized(JobID jobID) throws IOException {
+		JobAccumulators acc;
+		synchronized (jobAccumulators) {
+			acc = jobAccumulators.get(jobID);
+		}
+
+		if (acc == null || acc.getAccumulators().isEmpty()) {
+			return Collections.emptyMap();
+		}
+
+		Map<String, SerializedValue<Object>> result = new HashMap<String, SerializedValue<Object>>();
+		for (Map.Entry<String, Accumulator<?, ?>> entry : acc.getAccumulators().entrySet()) {
+			result.put(entry.getKey(), new SerializedValue<Object>(entry.getValue().getLocalValue()));
+		}
+
+		return result;
+	}
+
+	public StringifiedAccumulatorResult[] getJobAccumulatorResultsStringified(JobID jobID) throws IOException {
+		JobAccumulators acc;
+		synchronized (jobAccumulators) {
+			acc = jobAccumulators.get(jobID);
+		}
+
+		if (acc == null || acc.getAccumulators().isEmpty()) {
+			return new StringifiedAccumulatorResult[0];
+		}
+
+		Map<String, Accumulator<?, ?>> accMap = acc.getAccumulators();
+
+		StringifiedAccumulatorResult[] result = new StringifiedAccumulatorResult[accMap.size()];
+		int i = 0;
+		for (Map.Entry<String, Accumulator<?, ?>> entry : accMap.entrySet()) {
+			String type = entry.getValue() == null ? "(null)" : entry.getValue().getClass().getSimpleName();
+			String value = entry.getValue() == null ? "(null)" : entry.getValue().toString();
+			result[i++] = new StringifiedAccumulatorResult(entry.getKey(), type, value);
+		}
+		return result;
+	}
+
 	/**
-	 * Cleanup data for the oldest jobs if the maximum number of entries is
-	 * reached.
+	 * Cleanup data for the oldest jobs if the maximum number of entries is reached.
+	 *
+	 * @param jobId The (potentially new) JobId.
 	 */
 	private void cleanup(JobID jobId) {
 		if (!lru.contains(jobId)) {
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/accumulators/JobAccumulators.java b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/accumulators/JobAccumulators.java
index 2835b30c91d..970d99330c7 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/accumulators/JobAccumulators.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/accumulators/JobAccumulators.java
@@ -16,7 +16,6 @@
  * limitations under the License.
  */
 
-
 package org.apache.flink.runtime.jobmanager.accumulators;
 
 import java.util.HashMap;
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/web/JobManagerInfoServlet.java b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/web/JobManagerInfoServlet.java
index 008be65d97b..144c477c46c 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/web/JobManagerInfoServlet.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/web/JobManagerInfoServlet.java
@@ -25,8 +25,6 @@ import java.util.Collections;
 import java.util.Comparator;
 import java.util.Iterator;
 import java.util.List;
-import java.util.Map;
-import java.util.Map.Entry;
 
 import javax.servlet.ServletException;
 import javax.servlet.http.HttpServlet;
@@ -37,18 +35,20 @@ import akka.actor.ActorRef;
 
 import akka.pattern.Patterns;
 import akka.util.Timeout;
+import org.apache.flink.runtime.accumulators.StringifiedAccumulatorResult;
 import org.apache.flink.runtime.instance.InstanceConnectionInfo;
 import org.apache.flink.runtime.messages.ArchiveMessages.ArchivedJobs;
 import org.apache.flink.runtime.messages.ArchiveMessages;
 import org.apache.flink.runtime.messages.JobManagerMessages;
-import org.apache.flink.runtime.messages.JobManagerMessages.AccumulatorResultsResponse;
-import org.apache.flink.runtime.messages.JobManagerMessages.AccumulatorResultsFound;
 import org.apache.flink.runtime.messages.JobManagerMessages.RunningJobs;
 import org.apache.flink.runtime.messages.JobManagerMessages.CancelJob;
-import org.apache.flink.runtime.messages.JobManagerMessages.RequestAccumulatorResults;
 import org.apache.flink.runtime.messages.JobManagerMessages.RequestJob;
 import org.apache.flink.runtime.messages.JobManagerMessages.JobResponse;
 import org.apache.flink.runtime.messages.JobManagerMessages.JobFound;
+import org.apache.flink.runtime.messages.accumulators.AccumulatorResultStringsFound;
+import org.apache.flink.runtime.messages.accumulators.AccumulatorResultsErroneous;
+import org.apache.flink.runtime.messages.accumulators.AccumulatorResultsNotFound;
+import org.apache.flink.runtime.messages.accumulators.RequestAccumulatorResultsStringified;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import org.apache.flink.runtime.execution.ExecutionState;
@@ -69,34 +69,34 @@ import scala.concurrent.Future;
 import scala.concurrent.duration.FiniteDuration;
 
 public class JobManagerInfoServlet extends HttpServlet {
-	
+
 	private static final long serialVersionUID = 1L;
-	
+
 	private static final Logger LOG = LoggerFactory.getLogger(JobManagerInfoServlet.class);
-	
+
 	/** Underlying JobManager */
 	private final ActorRef jobmanager;
 	private final ActorRef archive;
 	private final FiniteDuration timeout;
-	
-	
+
+
 	public JobManagerInfoServlet(ActorRef jobmanager, ActorRef archive, FiniteDuration timeout) {
 		this.jobmanager = jobmanager;
 		this.archive = archive;
 		this.timeout = timeout;
 	}
-	
-	
+
+
 	@Override
 	protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException,
 			IOException {
-			
+
 		resp.setStatus(HttpServletResponse.SC_OK);
 		resp.setContentType("application/json");
 
 		Future<Object> response;
 		Object result;
-		
+
 		try {
 			if("archive".equals(req.getParameter("get"))) {
 				response = Patterns.ask(archive, ArchiveMessages.getRequestArchivedJobs(),
@@ -132,7 +132,7 @@ public class JobManagerInfoServlet extends HttpServlet {
 					if(jobResponse instanceof JobFound){
 						ExecutionGraph archivedJob = ((JobFound)result).executionGraph();
 						writeJsonForArchivedJob(resp.getWriter(), archivedJob);
-				} else {
+					} else {
 						LOG.warn("DoGet:job: Could not find job for job ID " + jobId);
 					}
 				}
@@ -157,7 +157,7 @@ public class JobManagerInfoServlet extends HttpServlet {
 
 						writeJsonForArchivedJobGroupvertex(resp.getWriter(), archivedJob,
 								JobVertexID.fromHexString(groupvertexId));
-				} else {
+					} else {
 						LOG.warn("DoGet:groupvertex: Could not find job for job ID " + jobId);
 					}
 				}
@@ -226,7 +226,7 @@ public class JobManagerInfoServlet extends HttpServlet {
 					writeJsonForJobs(resp.getWriter(), runningJobs);
 				}
 			}
-			
+
 		} catch (Exception e) {
 			resp.setStatus(HttpServletResponse.SC_BAD_REQUEST);
 			resp.getWriter().print(e.getMessage());
@@ -235,10 +235,10 @@ public class JobManagerInfoServlet extends HttpServlet {
 			}
 		}
 	}
-	
+
 	/**
 	 * Writes ManagementGraph as Json for all recent jobs
-	 * 
+	 *
 	 * @param wrt
 	 * @param graphs
 	 */
@@ -250,23 +250,23 @@ public class JobManagerInfoServlet extends HttpServlet {
 			// Loop Jobs
 			while(it.hasNext()){
 				ExecutionGraph graph = it.next();
-	
+
 				writeJsonForJob(wrt, graph);
-	
+
 				//Write seperator between json objects
 				if(it.hasNext()) {
 					wrt.write(",");
 				}
 			}
 			wrt.write("]");
-		
+
 		} catch (EofException eof) { // Connection closed by client
 			LOG.info("Info server for jobmanager: Connection closed by client, EofException");
-		} catch (IOException ioe) { // Connection closed by client	
+		} catch (IOException ioe) { // Connection closed by client
 			LOG.info("Info server for jobmanager: Connection closed by client, IOException");
-		} 
+		}
 	}
-	
+
 	private void writeJsonForJob(PrintWriter wrt, ExecutionGraph graph) throws IOException {
 		//Serialize job to json
 		wrt.write("{");
@@ -274,35 +274,35 @@ public class JobManagerInfoServlet extends HttpServlet {
 		wrt.write("\"jobname\": \"" + graph.getJobName()+"\",");
 		wrt.write("\"status\": \""+ graph.getState() + "\",");
 		wrt.write("\"time\": " + graph.getStatusTimestamp(graph.getState())+",");
-		
+
 		// Serialize ManagementGraph to json
 		wrt.write("\"groupvertices\": [");
 		boolean first = true;
-		
+
 		for (ExecutionJobVertex groupVertex : graph.getVerticesTopologically()) {
 			//Write seperator between json objects
 			if(first) {
 				first = false;
 			} else {
 				wrt.write(","); }
-			
+
 			wrt.write(JsonFactory.toJson(groupVertex));
 		}
 		wrt.write("]");
 		wrt.write("}");
-			
+
 	}
-	
+
 	/**
 	 * Writes Json with a list of currently archived jobs, sorted by time
-	 * 
+	 *
 	 * @param wrt
 	 * @param graphs
 	 */
 	private void writeJsonForArchive(PrintWriter wrt, List<ExecutionGraph> graphs) {
-		
+
 		wrt.write("[");
-		
+
 		// sort jobs by time
 		Collections.sort(graphs,  new Comparator<ExecutionGraph>() {
 			@Override
@@ -313,41 +313,39 @@ public class JobManagerInfoServlet extends HttpServlet {
 					return -1;
 				}
 			}
-			
+
 		});
-		
+
 		// Loop Jobs
 		for (int i = 0; i < graphs.size(); i++) {
 			ExecutionGraph graph = graphs.get(i);
-			
+
 			//Serialize job to json
 			wrt.write("{");
 			wrt.write("\"jobid\": \"" + graph.getJobID() + "\",");
 			wrt.write("\"jobname\": \"" + graph.getJobName()+"\",");
 			wrt.write("\"status\": \""+ graph.getState() + "\",");
 			wrt.write("\"time\": " + graph.getStatusTimestamp(graph.getState()));
-			
+
 			wrt.write("}");
-			
+
 			//Write seperator between json objects
 			if(i != graphs.size() - 1) {
 				wrt.write(",");
 			}
 		}
 		wrt.write("]");
-		
+
 	}
-	
+
 	/**
 	 * Writes infos about archived job in Json format, including groupvertices and groupverticetimes
-	 * 
+	 *
 	 * @param wrt
 	 * @param graph
 	 */
 	private void writeJsonForArchivedJob(PrintWriter wrt, ExecutionGraph graph) {
-		
 		try {
-
 			wrt.write("[");
 
 			//Serialize job to json
@@ -402,113 +400,108 @@ public class JobManagerInfoServlet extends HttpServlet {
 
 			// write accumulators
 			final Future<Object> response = Patterns.ask(jobmanager,
-					new RequestAccumulatorResults(graph.getJobID()), new Timeout(timeout));
-
-			Object result = null;
+					new RequestAccumulatorResultsStringified(graph.getJobID()), new Timeout(timeout));
 
+			Object result;
 			try {
 				result = Await.result(response, timeout);
 			} catch (Exception ex) {
-				throw new IOException("Could not retrieve the accumulator results from the " +
-						"job manager.", ex);
+				throw new IOException("Could not retrieve the accumulator results from the job manager.", ex);
 			}
 
-			if (!(result instanceof AccumulatorResultsResponse)) {
+			if (result instanceof AccumulatorResultStringsFound) {
+				StringifiedAccumulatorResult[] accumulators = ((AccumulatorResultStringsFound) result).result();
+
+				wrt.write("\n\"accumulators\": [");
+				int i = 0;
+				for (StringifiedAccumulatorResult accumulator : accumulators) {
+					wrt.write("{ \"name\": \"" + accumulator.getName() + " (" + accumulator.getType() + ")\","
+							+ " \"value\": \"" + accumulator.getValue() + "\"}\n");
+					if (++i < accumulators.length) {
+						wrt.write(",");
+					}
+				}
+				wrt.write("],\n");
+			}
+			else if (result instanceof AccumulatorResultsNotFound) {
+				wrt.write("\n\"accumulators\": [],");
+			}
+			else if (result instanceof AccumulatorResultsErroneous) {
+				LOG.error("Could not obtain accumulators for job " + graph.getJobID(),
+						((AccumulatorResultsErroneous) result).cause());
+			}
+			else {
 				throw new RuntimeException("RequestAccumulatorResults requires a response of type " +
-						"AccumulatorResultsReponse. Instead the response is of type " +
+						"AccumulatorResultStringsFound. Instead the response is of type " +
 						result.getClass() + ".");
-			} else {
-				final AccumulatorResultsResponse accumulatorResponse =
-						(AccumulatorResultsResponse) result;
-
-				if (accumulatorResponse instanceof AccumulatorResultsFound) {
-					Map<String, Object> accMap = ((AccumulatorResultsFound) accumulatorResponse).
-							asJavaMap();
-
-					wrt.write("\n\"accumulators\": [");
-					int i = 0;
-					for (Entry<String, Object> accumulator : accMap.entrySet()) {
-						wrt.write("{ \"name\": \"" + accumulator.getKey() + " (" + accumulator.getValue().getClass().getName() + ")\","
-								+ " \"value\": \"" + accumulator.getValue().toString() + "\"}\n");
-						if (++i < accMap.size()) {
-							wrt.write(",");
-						}
-					}
-					wrt.write("],\n");
-
-					wrt.write("\"groupverticetimes\": {");
-					first = true;
-					for (ExecutionJobVertex groupVertex : graph.getVerticesTopologically()) {
-
-						if (first) {
-							first = false;
-						} else {
-							wrt.write(",");
-						}
-
-						// Calculate start and end time for groupvertex
-						long started = Long.MAX_VALUE;
-						long ended = 0;
+			}
 
-						// Take earliest running state and latest endstate of groupmembers
-						for (ExecutionVertex vertex : groupVertex.getTaskVertices()) {
+			wrt.write("\"groupverticetimes\": {");
+			first = true;
 
-							long running = vertex.getStateTimestamp(ExecutionState.RUNNING);
-							if (running != 0 && running < started) {
-								started = running;
-							}
+			for (ExecutionJobVertex groupVertex : graph.getVerticesTopologically()) {
+				if (first) {
+					first = false;
+				} else {
+					wrt.write(",");
+				}
 
-							long finished = vertex.getStateTimestamp(ExecutionState.FINISHED);
-							long canceled = vertex.getStateTimestamp(ExecutionState.CANCELED);
-							long failed = vertex.getStateTimestamp(ExecutionState.FAILED);
+				// Calculate start and end time for groupvertex
+				long started = Long.MAX_VALUE;
+				long ended = 0;
 
-							if (finished != 0 && finished > ended) {
-								ended = finished;
-							}
+				// Take earliest running state and latest endstate of groupmembers
+				for (ExecutionVertex vertex : groupVertex.getTaskVertices()) {
 
-							if (canceled != 0 && canceled > ended) {
-								ended = canceled;
-							}
+					long running = vertex.getStateTimestamp(ExecutionState.RUNNING);
+					if (running != 0 && running < started) {
+						started = running;
+					}
 
-							if (failed != 0 && failed > ended) {
-								ended = failed;
-							}
+					long finished = vertex.getStateTimestamp(ExecutionState.FINISHED);
+					long canceled = vertex.getStateTimestamp(ExecutionState.CANCELED);
+					long failed = vertex.getStateTimestamp(ExecutionState.FAILED);
 
-						}
+					if (finished != 0 && finished > ended) {
+						ended = finished;
+					}
 
-						wrt.write("\"" + groupVertex.getJobVertexId() + "\": {");
-						wrt.write("\"groupvertexid\": \"" + groupVertex.getJobVertexId() + "\",");
-						wrt.write("\"groupvertexname\": \"" + groupVertex + "\",");
-						wrt.write("\"STARTED\": " + started + ",");
-						wrt.write("\"ENDED\": " + ended);
-						wrt.write("}");
+					if (canceled != 0 && canceled > ended) {
+						ended = canceled;
+					}
 
+					if (failed != 0 && failed > ended) {
+						ended = failed;
 					}
-			} else {
-					LOG.warn("Could not find accumulator results for job ID " + graph.getJobID());
-				}
 
-				wrt.write("}");
+				}
 
+				wrt.write("\"" + groupVertex.getJobVertexId() + "\": {");
+				wrt.write("\"groupvertexid\": \"" + groupVertex.getJobVertexId() + "\",");
+				wrt.write("\"groupvertexname\": \"" + groupVertex + "\",");
+				wrt.write("\"STARTED\": " + started + ",");
+				wrt.write("\"ENDED\": " + ended);
 				wrt.write("}");
 
-
-				wrt.write("]");
 			}
-		} catch (Exception ex) { // Connection closed by client
-			LOG.info("Info server for jobmanager: Failed to write json for archived jobs, " +
-					"because {}.", StringUtils.stringifyException(ex));
+
+			wrt.write("}");
+			wrt.write("}");
+			wrt.write("]");
+		}
+		catch (Exception ex) { // Connection closed by client
+			LOG.error("Info server for JobManager: Failed to write json for archived jobs", ex);
 		}
 	}
 
 	/**
 	 * Writes all updates (events) for a given job since a given time
-	 * 
+	 *
 	 * @param wrt
 	 * @param jobId
 	 */
 	private void writeJsonUpdatesForJob(PrintWriter wrt, JobID jobId) {
-		
+
 		try {
 			final Future<Object> responseArchivedJobs = Patterns.ask(jobmanager,
 					JobManagerMessages.getRequestRunningJobs(),
@@ -539,9 +532,9 @@ public class JobManagerInfoServlet extends HttpServlet {
 				boolean first = true;
 
 				for(ExecutionGraph g : graphs){
-				if (first) {
+					if (first) {
 						first = false;
-				} else {
+					} else {
 						wrt.write(",");
 					}
 
@@ -559,7 +552,7 @@ public class JobManagerInfoServlet extends HttpServlet {
 					resultJob = Await.result(responseJob, timeout);
 				} catch (Exception ex){
 					throw new IOException("Could not retrieve the job with jobID " + jobId +
-						"from the job manager.", ex);
+							"from the job manager.", ex);
 				}
 
 				if(!(resultJob instanceof JobResponse)) {
@@ -602,7 +595,7 @@ public class JobManagerInfoServlet extends HttpServlet {
 						wrt.write("]");
 
 						wrt.write("}");
-			} else {
+					} else {
 						wrt.write("\"vertexevents\": [],");
 						wrt.write("\"jobevents\": [");
 						wrt.write("{");
@@ -615,14 +608,14 @@ public class JobManagerInfoServlet extends HttpServlet {
 					}
 				}
 			}
-			
+
 		} catch (Exception exception) { // Connection closed by client
 			LOG.info("Info server for jobmanager: Failed to write json updates for job {}, " +
 					"because {}.", jobId, StringUtils.stringifyException(exception));
-		} 
-		
+		}
+
 	}
-	
+
 	/**
 	 * Writes info about one particular archived JobVertex in a job, including all member execution vertices, their times and statuses.
 	 */
@@ -663,10 +656,10 @@ public class JobManagerInfoServlet extends HttpServlet {
 		}
 		wrt.write("}}");
 	}
-	
+
 	/**
 	 * Writes the version and the revision of Flink.
-	 * 
+	 *
 	 * @param wrt
 	 */
 	private void writeJsonForVersion(PrintWriter wrt) {
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/util/SerializedValue.java b/flink-runtime/src/main/java/org/apache/flink/runtime/util/SerializedValue.java
new file mode 100644
index 00000000000..f5e897b7c75
--- /dev/null
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/util/SerializedValue.java
@@ -0,0 +1,84 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.util;
+
+import org.apache.flink.util.InstantiationUtil;
+
+import java.io.IOException;
+import java.util.Arrays;
+
+/**
+ * This class is used to transfer (via serialization) objects whose classes are not available
+ * in the system class loader. When those objects are deserialized without access to their
+ * special class loader, the deserialization fails with a {@code ClassNotFoundException}.
+ *
+ * To work around that issue, the SerializedValue serialized data immediately into a byte array.
+ * When send through RPC or another service that uses serialization, the only the byte array is
+ * transferred. The object is deserialized later (upon access) and requires the accessor to
+ * provide the corresponding class loader.
+ *
+ * @param <T> The type of the value held.
+ */
+public class SerializedValue<T> implements java.io.Serializable {
+
+	private static final long serialVersionUID = -3564011643393683761L;
+
+	/** The serialized data */
+	private final byte[] serializedData;
+
+
+	public SerializedValue(T value) throws IOException {
+		this.serializedData = value == null ? null : InstantiationUtil.serializeObject(value);
+	}
+
+
+	@SuppressWarnings("unchecked")
+	public T deserializeValue(ClassLoader loader) throws IOException, ClassNotFoundException {
+		if (loader == null) {
+			throw new NullPointerException();
+		}
+
+		return serializedData == null ? null : (T) InstantiationUtil.deserializeObject(serializedData, loader);
+	}
+
+	// --------------------------------------------------------------------------------------------
+
+
+	@Override
+	public int hashCode() {
+		return serializedData == null ? 0 : Arrays.hashCode(serializedData);
+	}
+
+	@Override
+	public boolean equals(Object obj) {
+		if (obj instanceof SerializedValue) {
+			SerializedValue<?> other = (SerializedValue<?>) obj;
+			return this.serializedData == null ? other.serializedData == null :
+					(other.serializedData != null && Arrays.equals(this.serializedData, other.serializedData));
+		}
+		else {
+			return false;
+		}
+	}
+
+	@Override
+	public String toString() {
+		return "SerializedValue";
+	}
+}
diff --git a/flink-runtime/src/main/scala/org/apache/flink/runtime/client/JobClient.scala b/flink-runtime/src/main/scala/org/apache/flink/runtime/client/JobClient.scala
index d153dcc2ded..013fe4c8578 100644
--- a/flink-runtime/src/main/scala/org/apache/flink/runtime/client/JobClient.scala
+++ b/flink-runtime/src/main/scala/org/apache/flink/runtime/client/JobClient.scala
@@ -24,7 +24,6 @@ import java.net.{InetAddress, InetSocketAddress}
 import akka.actor.Status.{Success, Failure}
 import akka.actor._
 import akka.pattern.{Patterns, ask}
-import org.apache.flink.api.common.JobExecutionResult
 import org.apache.flink.configuration.{ConfigConstants, Configuration}
 import org.apache.flink.runtime.ActorLogMessages
 import org.apache.flink.runtime.akka.AkkaUtils
@@ -46,6 +45,7 @@ class JobClient(jobManager: ActorRef) extends
 Actor with ActorLogMessages with ActorLogging {
 
   override def receiveWithLogMessages: Receive = {
+
     case SubmitJobDetached(jobGraph) =>
       jobManager forward SubmitJob(jobGraph, registerForEvents = false)
 
@@ -53,7 +53,7 @@ Actor with ActorLogMessages with ActorLogging {
       jobManager forward cancelJob
 
     case SubmitJobAndWait(jobGraph, listen) =>
-      val listener = context.actorOf(Props(classOf[JobClientListener], sender))
+      val listener = context.actorOf(Props(classOf[JobClientListener], sender()))
       jobManager.tell(SubmitJob(jobGraph, registerForEvents = listen), listener)
 
     case RequestBlobManagerPort =>
@@ -88,8 +88,8 @@ ActorLogging {
 
     case Success(_) =>
 
-    case JobResultSuccess(jobId, duration, accumulatorResults) =>
-      jobSubmitter ! new JobExecutionResult(jobId, duration, accumulatorResults)
+    case JobResultSuccess(result) =>
+      jobSubmitter ! result
       self ! PoisonPill
 
     case msg =>
@@ -191,8 +191,8 @@ object JobClient {
   /**
    * Sends a [[JobGraph]] to the JobClient actor specified by jobClient which submits it then to
    * the JobManager. The method blocks until the job has finished or the JobManager is no longer
-   * alive. In the former case, the [[JobExecutionResult]] is returned and in the latter case a
-   * [[JobExecutionException]] is thrown.
+   * alive. In the former case, the [[SerializedJobExecutionResult]] is returned and in the latter
+   * case a [[JobExecutionException]] is thrown.
    *
    * @param jobGraph JobGraph describing the Flink job
    * @param listenToStatusEvents true if the JobClient shall print status events of the
@@ -204,14 +204,16 @@ object JobClient {
    * @return The job execution result
    */
   @throws(classOf[JobExecutionException])
-  def submitJobAndWait(jobGraph: JobGraph, listenToStatusEvents: Boolean, jobClient: ActorRef)
-                      (implicit timeout: FiniteDuration): JobExecutionResult = {
+  def submitJobAndWait(jobGraph: JobGraph,
+                       listenToStatusEvents: Boolean,
+                       jobClient: ActorRef,
+                       timeout: FiniteDuration): SerializedJobExecutionResult = {
 
     var waitForAnswer = true
-    var answer: JobExecutionResult = null
+    var answer: SerializedJobExecutionResult = null
 
     val result = (jobClient ? SubmitJobAndWait(jobGraph, listenToEvents = listenToStatusEvents))(
-      AkkaUtils.INF_TIMEOUT).mapTo[JobExecutionResult]
+      AkkaUtils.INF_TIMEOUT).mapTo[SerializedJobExecutionResult]
 
     while (waitForAnswer) {
       try {
@@ -241,12 +243,13 @@ object JobClient {
    *
    * @param jobGraph Flink job
    * @param jobClient ActorRef to the JobClient
-   * @param timeout Tiemout for futures
+   * @param timeout Timeout for futures
    * @return The submission response
    */
   @throws(classOf[JobExecutionException])
-  def submitJobDetached(jobGraph: JobGraph, jobClient: ActorRef)(implicit timeout: FiniteDuration):
-  Unit = {
+  def submitJobDetached(jobGraph: JobGraph,
+                        jobClient: ActorRef,
+                        timeout: FiniteDuration): Unit = {
 
     val response = (jobClient ? SubmitJobDetached(jobGraph))(timeout)
 
@@ -255,7 +258,7 @@ object JobClient {
     } catch {
       case timeout: TimeoutException =>
         throw new JobTimeoutException(jobGraph.getJobID,
-          "Timeout while waiting for the submission result.", timeout);
+          "Timeout while submitting the job to the JobManager.", timeout);
     }
   }
 
diff --git a/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManager.scala b/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManager.scala
index 49bb1d54b34..6870d9510d4 100644
--- a/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManager.scala
+++ b/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManager.scala
@@ -20,13 +20,15 @@ package org.apache.flink.runtime.jobmanager
 
 import java.io.{IOException, File}
 import java.net.InetSocketAddress
+import java.util.Collections
 
 import akka.actor.Status.{Success, Failure}
 import org.apache.flink.api.common.{JobID, ExecutionConfig}
 import org.apache.flink.configuration.{ConfigConstants, GlobalConfiguration, Configuration}
 import org.apache.flink.core.io.InputSplitAssigner
+import org.apache.flink.runtime.accumulators.StringifiedAccumulatorResult
 import org.apache.flink.runtime.blob.BlobServer
-import org.apache.flink.runtime.client.{JobStatusMessage, JobSubmissionException, JobExecutionException, JobCancellationException}
+import org.apache.flink.runtime.client._
 import org.apache.flink.runtime.executiongraph.{ExecutionJobVertex, ExecutionGraph}
 import org.apache.flink.runtime.jobmanager.web.WebInfoServer
 import org.apache.flink.runtime.messages.ArchiveMessages.ArchiveExecutionGraph
@@ -34,11 +36,12 @@ import org.apache.flink.runtime.messages.CheckpointingMessages.{StateBarrierAck,
 import org.apache.flink.runtime.messages.ExecutionGraphMessages.JobStatusChanged
 import org.apache.flink.runtime.messages.Messages.{Disconnect, Acknowledge}
 import org.apache.flink.runtime.messages.TaskMessages.UpdateTaskExecutionState
+import org.apache.flink.runtime.messages.accumulators._
 import org.apache.flink.runtime.process.ProcessReaper
 import org.apache.flink.runtime.security.SecurityUtils
 import org.apache.flink.runtime.security.SecurityUtils.FlinkSecuredRunner
 import org.apache.flink.runtime.taskmanager.TaskManager
-import org.apache.flink.runtime.util.EnvironmentInformation
+import org.apache.flink.runtime.util.{SerializedValue, EnvironmentInformation}
 import org.apache.flink.runtime.ActorLogMessages
 import org.apache.flink.runtime.akka.AkkaUtils
 import org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManager
@@ -297,14 +300,25 @@ class JobManager(val flinkConfiguration: Configuration,
           // is the client waiting for the job result?
             newJobStatus match {
               case JobStatus.FINISHED =>
-                val accumulatorResults = accumulatorManager.getJobAccumulatorResults(jobID)
-                jobInfo.client ! JobResultSuccess(jobID, jobInfo.duration, accumulatorResults)
+                val accumulatorResults: java.util.Map[String, SerializedValue[AnyRef]] = try {
+                  accumulatorManager.getJobAccumulatorResultsSerialized(jobID)
+                } catch {
+                  case e: Exception =>
+                    log.error(e, "Cannot fetch serialized accumulators for job {}", jobID)
+                    Collections.emptyMap()
+                }
+                val result = new SerializedJobExecutionResult(jobID, jobInfo.duration,
+                                                              accumulatorResults)
+                jobInfo.client ! JobResultSuccess(result)
+
               case JobStatus.CANCELED =>
                 jobInfo.client ! Failure(new JobCancellationException(jobID,
                   "Job was cancelled.", error))
+
               case JobStatus.FAILED =>
                 jobInfo.client ! Failure(new JobExecutionException(jobID,
                   "Job execution failed.", error))
+
               case x =>
                 val exception = new JobExecutionException(jobID, s"$x is not a " +
                   "terminal state.")
@@ -344,24 +358,6 @@ class JobManager(val flinkConfiguration: Configuration,
             s"$jobId to schedule or update consumers."))
       }
 
-    case ReportAccumulatorResult(accumulatorEvent) =>
-      try {
-        accumulatorManager.processIncomingAccumulators(accumulatorEvent.getJobID,
-          accumulatorEvent.getAccumulators(
-            libraryCacheManager.getClassLoader(accumulatorEvent.getJobID)
-          )
-        )
-      } catch {
-        case t: Throwable =>
-          log.error(t, "Could not process accumulator event of job {} received from {}.",
-            accumulatorEvent.getJobID, sender().path)
-      }
-
-    case RequestAccumulatorResults(jobID) =>
-      import scala.collection.JavaConverters._
-      sender ! AccumulatorResultsFound(jobID, accumulatorManager.getJobAccumulatorResults
-        (jobID).asScala.toMap)
-
     case RequestJobStatus(jobID) =>
       currentJobs.get(jobID) match {
         case Some((executionGraph,_)) => sender ! CurrentJobStatus(jobID, executionGraph.getState)
@@ -413,6 +409,8 @@ class JobManager(val flinkConfiguration: Configuration,
         case t: Throwable => log.error(t, "Could not report heart beat from {}.", sender().path)
       }
 
+    case message: AccumulatorMessage => handleAccumulatorMessage(message)
+
     case RequestStackTrace(instanceID) =>
       val taskManager = instanceManager.getRegisteredInstanceById(instanceID).getTaskManager
       taskManager forward SendStackTrace
@@ -607,6 +605,65 @@ class JobManager(val flinkConfiguration: Configuration,
     throw new RuntimeException("Received unknown message " + message)
   }
 
+  /**
+   * Handle messages that request or report accumulators.
+   *
+   * @param message The accumulator message.
+   */
+  private def handleAccumulatorMessage(message: AccumulatorMessage): Unit = {
+
+    message match {
+      case ReportAccumulatorResult(jobId, _, accumulatorEvent) =>
+        val classLoader = try {
+          libraryCacheManager.getClassLoader(jobId)
+        } catch {
+          case e: Exception =>
+            log.error(e, "Dropping accumulators. No class loader available for job " + jobId)
+            return
+        }
+
+        if (classLoader != null) {
+          try {
+            val accumulators = accumulatorEvent.deserializeValue(classLoader)
+            accumulatorManager.processIncomingAccumulators(jobId, accumulators)
+          }
+          catch {
+            case e: Exception => log.error(e, "Cannot update accumulators for job " + jobId)
+          }
+        } else {
+          log.error("Dropping accumulators. No class loader available for job " + jobId)
+        }
+
+      case RequestAccumulatorResults(jobID) =>
+        try {
+          val accumulatorValues: java.util.Map[String, SerializedValue[Object]] =
+            accumulatorManager.getJobAccumulatorResultsSerialized(jobID)
+
+          sender() ! AccumulatorResultsFound(jobID, accumulatorValues)
+        }
+        catch {
+          case e: Exception =>
+            log.error(e, "Cannot serialize accumulator result")
+            sender() ! AccumulatorResultsErroneous(jobID, e)
+        }
+
+      case RequestAccumulatorResultsStringified(jobId) =>
+        try {
+          val accumulatorValues: Array[StringifiedAccumulatorResult] =
+            accumulatorManager.getJobAccumulatorResultsStringified(jobId)
+
+          sender() ! AccumulatorResultStringsFound(jobId, accumulatorValues)
+        }
+        catch {
+          case e: Exception =>
+            log.error(e, "Cannot fetch accumulator result")
+            sender() ! AccumulatorResultsErroneous(jobId, e)
+        }
+
+      case x => unhandled(x)
+    }
+  }
+
   /**
    * Removes the job and sends it to the MemoryArchivist
    * @param jobID ID of the job to remove and archive
diff --git a/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/JobManagerMessages.scala b/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/JobManagerMessages.scala
index 73e20c01a21..5b70294aed3 100644
--- a/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/JobManagerMessages.scala
+++ b/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/JobManagerMessages.scala
@@ -19,8 +19,7 @@
 package org.apache.flink.runtime.messages
 
 import org.apache.flink.api.common.JobID
-import org.apache.flink.runtime.accumulators.AccumulatorEvent
-import org.apache.flink.runtime.client.JobStatusMessage
+import org.apache.flink.runtime.client.{SerializedJobExecutionResult, JobStatusMessage}
 import org.apache.flink.runtime.executiongraph.{ExecutionAttemptID, ExecutionGraph}
 import org.apache.flink.runtime.instance.{InstanceID, Instance}
 import org.apache.flink.runtime.io.network.partition.ResultPartitionID
@@ -90,46 +89,6 @@ object JobManagerMessages {
 
   case class ConsumerNotificationResult(success: Boolean, error: Option[Throwable] = None)
 
-  /**
-   * Reports the accumulator results of the individual tasks to the job manager.
-   *
-   * @param accumulatorEvent
-   */
-  case class ReportAccumulatorResult(accumulatorEvent: AccumulatorEvent)
-
-  /**
-   * Requests the accumulator results of the job identified by [[jobID]] from the job manager.
-   * The result is sent back to the sender as a [[AccumulatorResultsResponse]] message.
-   *
-   * @param jobID
-   */
-  case class RequestAccumulatorResults(jobID: JobID)
-
-  sealed trait AccumulatorResultsResponse{
-    val jobID: JobID
-  }
-
-  /**
-   * Contains the retrieved accumulator results from the job manager. This response is triggered
-   * by [[RequestAccumulatorResults]].
-   *
-   * @param jobID
-   * @param results
-   */
-  case class AccumulatorResultsFound(jobID: JobID, results: Map[String,
-    Object]) extends AccumulatorResultsResponse{
-    def asJavaMap: java.util.Map[String, Object] = {
-      import scala.collection.JavaConverters._
-      results.asJava
-    }
-  }
-
-  /**
-   * Denotes that no accumulator results for [[jobID]] could be found at the job manager.
-   * @param jobID
-   */
-  case class AccumulatorResultsNotFound(jobID: JobID) extends AccumulatorResultsResponse
-
   /**
    * Requests the current [[JobStatus]] of the job identified by [[jobID]]. This message triggers
    * as response a [[JobStatusResponse]] message.
@@ -170,13 +129,9 @@ object JobManagerMessages {
 
   /**
    * Denotes a successful job execution.
-   *
-   * @param jobID
-   * @param runtime
-   * @param accumulatorResults
    */
-  case class JobResultSuccess(jobID: JobID, runtime: Long,
-                              accumulatorResults: java.util.Map[String, AnyRef]) {}
+  case class JobResultSuccess(result: SerializedJobExecutionResult)
+
 
   sealed trait CancellationResponse{
     def jobID: JobID
diff --git a/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/accumulators/AccumulatorMessages.scala b/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/accumulators/AccumulatorMessages.scala
new file mode 100644
index 00000000000..82c4ab6bdc6
--- /dev/null
+++ b/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/accumulators/AccumulatorMessages.scala
@@ -0,0 +1,108 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.messages.accumulators
+
+import org.apache.flink.api.common.JobID
+import org.apache.flink.runtime.accumulators.{StringifiedAccumulatorResult, AccumulatorEvent}
+import org.apache.flink.runtime.executiongraph.ExecutionAttemptID
+import org.apache.flink.runtime.util.SerializedValue
+
+/**
+ * Base trait of all accumulator messages
+ */
+sealed trait AccumulatorMessage {
+
+  /** ID of the job that the accumulator belongs to */
+  val jobID: JobID
+}
+
+/**
+ * Base trait of responses to [[RequestAccumulatorResults]]
+ */
+sealed trait AccumulatorResultsResponse extends AccumulatorMessage
+
+/**
+ * Reports the accumulator results of the individual tasks to the job manager.
+ *
+ * @param jobID The ID of the job the accumulator belongs to
+ * @param executionId The ID of the task execution that the accumulator belongs to.
+ * @param accumulatorEvent The serialized accumulators
+ */
+case class ReportAccumulatorResult(jobID: JobID,
+                                   executionId: ExecutionAttemptID,
+                                   accumulatorEvent: AccumulatorEvent)
+  extends AccumulatorMessage
+
+/**
+ * Requests the accumulator results of the job identified by [[jobID]] from the job manager.
+ * The result is sent back to the sender as a [[AccumulatorResultsResponse]] message.
+ *
+ * @param jobID Job Id of the job that the accumulator belongs to
+ */
+case class RequestAccumulatorResults(jobID: JobID)
+  extends AccumulatorMessage
+
+/**
+ * Requests the accumulator results of the job as strings. This is used to request accumulators to
+ * be displayed for example in the web frontend. Because it transports only
+ * Strings, not custom objects, it is safe for user-defined types and class loading.
+ *
+ * @param jobID Job Id of the job that the accumulator belongs to
+ */
+case class RequestAccumulatorResultsStringified(jobID: JobID)
+  extends AccumulatorMessage
+
+/**
+ * Contains the retrieved accumulator results from the job manager. This response is triggered
+ * by [[RequestAccumulatorResults]].
+ *
+ * @param jobID Job Id of the job that the accumulator belongs to
+ * @param result The accumulator result values, in serialized form.
+ */
+case class AccumulatorResultsFound(jobID: JobID,
+                                   result: java.util.Map[String, SerializedValue[Object]])
+  extends AccumulatorResultsResponse
+
+/**
+ * Contains the retrieved accumulator result strings from the job manager.
+ *
+ * @param jobID Job Id of the job that the accumulator belongs to
+ * @param result The accumulator result values, in string form.
+ */
+case class AccumulatorResultStringsFound(jobID: JobID,
+                                         result: Array[StringifiedAccumulatorResult])
+  extends AccumulatorResultsResponse
+
+/**
+ * Denotes that no accumulator results for [[jobID]] could be found at the job manager.
+ *
+ * @param jobID Job Id of the job that the accumulators were requested for.
+ */
+case class AccumulatorResultsNotFound(jobID: JobID)
+  extends AccumulatorResultsResponse
+
+/**
+ * Denotes that the accumulator results for [[jobID]] could be obtained from
+ * the JobManager because of an exception.
+ *
+ * @param jobID Job Id of the job that the accumulators were requested for.
+ * @param cause Reason why the accumulators were not found.
+ */
+case class AccumulatorResultsErroneous(jobID: JobID, cause: Exception)
+  extends AccumulatorResultsResponse
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/client/SerializedJobExecutionResultTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/client/SerializedJobExecutionResultTest.java
new file mode 100644
index 00000000000..f58bbe18560
--- /dev/null
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/client/SerializedJobExecutionResultTest.java
@@ -0,0 +1,99 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.client;
+
+import org.apache.flink.api.common.JobExecutionResult;
+import org.apache.flink.api.common.JobID;
+import org.apache.flink.runtime.testutils.CommonTestUtils;
+import org.apache.flink.runtime.util.SerializedValue;
+import org.junit.Test;
+
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.junit.Assert.*;
+
+/**
+ * Tests for the SerializedJobExecutionResult
+ */
+public class SerializedJobExecutionResultTest {
+
+	@Test
+	public void testSerialization() {
+		try {
+			final ClassLoader classloader = getClass().getClassLoader();
+
+			JobID origJobId = new JobID();
+			long origTime = 65927436589267L;
+
+			Map<String, SerializedValue<Object>> origMap = new HashMap<String, SerializedValue<Object>>();
+			origMap.put("name1", new SerializedValue<Object>(723L));
+			origMap.put("name2", new SerializedValue<Object>("peter"));
+
+			SerializedJobExecutionResult result = new SerializedJobExecutionResult(origJobId, origTime, origMap);
+
+			// serialize and deserialize the object
+			SerializedJobExecutionResult cloned = CommonTestUtils.createCopySerializable(result);
+
+			assertEquals(origJobId, cloned.getJobId());
+			assertEquals(origTime, cloned.getNetRuntime());
+			assertEquals(origMap, cloned.getSerializedAccumulatorResults());
+
+			// convert to deserialized result
+			JobExecutionResult jResult = result.toJobExecutionResult(classloader);
+			JobExecutionResult jResultCopied = result.toJobExecutionResult(classloader);
+
+			assertEquals(origJobId, jResult.getJobID());
+			assertEquals(origJobId, jResultCopied.getJobID());
+			assertEquals(origTime, jResult.getNetRuntime());
+			assertEquals(origTime, jResultCopied.getNetRuntime());
+
+			for (Map.Entry<String, SerializedValue<Object>> entry : origMap.entrySet()) {
+				String name = entry.getKey();
+				Object value = entry.getValue().deserializeValue(classloader);
+				assertEquals(value, jResult.getAccumulatorResult(name));
+				assertEquals(value, jResultCopied.getAccumulatorResult(name));
+			}
+		}
+		catch (Exception e) {
+			e.printStackTrace();
+			fail(e.getMessage());
+		}
+	}
+
+	@Test
+	public void testSerializationWithNullValues() {
+		try {
+			SerializedJobExecutionResult result = new SerializedJobExecutionResult(null, 0L, null);
+			SerializedJobExecutionResult cloned = CommonTestUtils.createCopySerializable(result);
+
+			assertNull(cloned.getJobId());
+			assertEquals(0L, cloned.getNetRuntime());
+			assertNull(cloned.getSerializedAccumulatorResults());
+
+			JobExecutionResult jResult = result.toJobExecutionResult(getClass().getClassLoader());
+			assertNull(jResult.getJobID());
+			assertNull(jResult.getAllAccumulatorResults());
+		}
+		catch (Exception e) {
+			e.printStackTrace();
+			fail(e.getMessage());
+		}
+	}
+}
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/operators/testutils/MockEnvironment.java b/flink-runtime/src/test/java/org/apache/flink/runtime/operators/testutils/MockEnvironment.java
index 7cbb59b184f..319a37c626d 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/operators/testutils/MockEnvironment.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/operators/testutils/MockEnvironment.java
@@ -20,6 +20,7 @@
 package org.apache.flink.runtime.operators.testutils;
 
 import akka.actor.ActorRef;
+import org.apache.flink.api.common.accumulators.Accumulator;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.core.fs.Path;
 import org.apache.flink.core.memory.MemorySegment;
@@ -255,4 +256,9 @@ public class MockEnvironment implements Environment {
 	public BroadcastVariableManager getBroadcastVariableManager() {
 		return this.bcVarManager;
 	}
+
+	@Override
+	public void reportAccumulators(Map<String, Accumulator<?, ?>> accumulators) {
+		// discard, this is only for testing
+	}
 }
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/util/SerializedValueTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/util/SerializedValueTest.java
new file mode 100644
index 00000000000..01543344933
--- /dev/null
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/util/SerializedValueTest.java
@@ -0,0 +1,72 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.util;
+
+import org.apache.flink.runtime.testutils.CommonTestUtils;
+import org.junit.Test;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertNotNull;
+import static org.junit.Assert.assertNull;
+import static org.junit.Assert.fail;
+
+public class SerializedValueTest {
+
+	@Test
+	public void testSimpleValue() {
+		try {
+			final String value = "teststring";
+
+			SerializedValue<String> v = new SerializedValue<String>(value);
+			SerializedValue<String> copy = CommonTestUtils.createCopySerializable(v);
+
+			assertEquals(value, v.deserializeValue(getClass().getClassLoader()));
+			assertEquals(value, copy.deserializeValue(getClass().getClassLoader()));
+
+			assertEquals(v, copy);
+			assertEquals(v.hashCode(), copy.hashCode());
+
+			assertNotNull(v.toString());
+			assertNotNull(copy.toString());
+
+		}
+		catch (Exception e) {
+			e.printStackTrace();
+			fail(e.getMessage());
+		}
+	}
+
+	@Test
+	public void testNullValue() {
+		try {
+			SerializedValue<Object> v = new SerializedValue<Object>(null);
+			SerializedValue<Object> copy = CommonTestUtils.createCopySerializable(v);
+
+			assertNull(copy.deserializeValue(getClass().getClassLoader()));
+
+			assertEquals(v, copy);
+			assertEquals(v.hashCode(), copy.hashCode());
+			assertEquals(v.toString(), copy.toString());
+		}
+		catch (Exception e) {
+			e.printStackTrace();
+			fail(e.getMessage());
+		}
+	}
+}
diff --git a/flink-runtime/src/test/scala/org/apache/flink/runtime/jobmanager/JobManagerITCase.scala b/flink-runtime/src/test/scala/org/apache/flink/runtime/jobmanager/JobManagerITCase.scala
index fc67b464308..0fa3d5a00c9 100644
--- a/flink-runtime/src/test/scala/org/apache/flink/runtime/jobmanager/JobManagerITCase.scala
+++ b/flink-runtime/src/test/scala/org/apache/flink/runtime/jobmanager/JobManagerITCase.scala
@@ -121,7 +121,7 @@ WordSpecLike with Matchers with BeforeAndAfterAll {
           expectMsg(Success(jobGraph.getJobID))
           val result = expectMsgType[JobResultSuccess]
 
-          result.jobID should equal(jobGraph.getJobID)
+          result.result.getJobId() should equal(jobGraph.getJobID)
         }
 
         jm ! NotifyWhenJobRemoved(jobGraph.getJobID)
@@ -152,7 +152,7 @@ WordSpecLike with Matchers with BeforeAndAfterAll {
 
           val result = expectMsgType[JobResultSuccess]
 
-          result.jobID should equal(jobGraph.getJobID)
+          result.result.getJobId() should equal(jobGraph.getJobID)
         }
         jm ! NotifyWhenJobRemoved(jobGraph.getJobID)
         expectMsg(true)
@@ -187,7 +187,7 @@ WordSpecLike with Matchers with BeforeAndAfterAll {
 
           val result = expectMsgType[JobResultSuccess]
 
-          result.jobID should equal(jobGraph.getJobID)
+          result.result.getJobId() should equal(jobGraph.getJobID)
         }
         jm ! NotifyWhenJobRemoved(jobGraph.getJobID)
         expectMsg(true)
diff --git a/flink-runtime/src/test/scala/org/apache/flink/runtime/jobmanager/RecoveryITCase.scala b/flink-runtime/src/test/scala/org/apache/flink/runtime/jobmanager/RecoveryITCase.scala
index c201d083504..53bc70c0fbd 100644
--- a/flink-runtime/src/test/scala/org/apache/flink/runtime/jobmanager/RecoveryITCase.scala
+++ b/flink-runtime/src/test/scala/org/apache/flink/runtime/jobmanager/RecoveryITCase.scala
@@ -84,7 +84,7 @@ WordSpecLike with Matchers with BeforeAndAfterAll {
 
           val result = expectMsgType[JobResultSuccess]
 
-          result.jobID should equal(jobGraph.getJobID)
+          result.result.getJobId() should equal(jobGraph.getJobID)
         }
       } catch {
         case t: Throwable =>
@@ -127,7 +127,7 @@ WordSpecLike with Matchers with BeforeAndAfterAll {
 
           val result = expectMsgType[JobResultSuccess]
 
-          result.jobID should equal(jobGraph.getJobID)
+          result.result.getJobId() should equal(jobGraph.getJobID)
         }
       } catch {
         case t: Throwable =>
@@ -189,7 +189,7 @@ WordSpecLike with Matchers with BeforeAndAfterAll {
 
           val result = expectMsgType[JobResultSuccess]
 
-          result.jobID should equal(jobGraph.getJobID)
+          result.result.getJobId() should equal(jobGraph.getJobID)
         }
       } catch {
         case t: Throwable =>
diff --git a/flink-scala/src/main/scala/org/apache/flink/api/scala/DataSet.scala b/flink-scala/src/main/scala/org/apache/flink/api/scala/DataSet.scala
index 00b019b99a5..48a5285d438 100644
--- a/flink-scala/src/main/scala/org/apache/flink/api/scala/DataSet.scala
+++ b/flink-scala/src/main/scala/org/apache/flink/api/scala/DataSet.scala
@@ -17,9 +17,10 @@
  */
 package org.apache.flink.api.scala
 
-import java.lang
 import org.apache.commons.lang3.Validate
+
 import org.apache.flink.api.common.InvalidProgramException
+import org.apache.flink.api.common.accumulators.SerializedListAccumulator
 import org.apache.flink.api.common.aggregators.Aggregator
 import org.apache.flink.api.common.functions._
 import org.apache.flink.api.common.io.{FileOutputFormat, OutputFormat}
@@ -39,11 +40,10 @@ import org.apache.flink.configuration.Configuration
 import org.apache.flink.core.fs.{FileSystem, Path}
 import org.apache.flink.api.common.typeinfo.TypeInformation
 import org.apache.flink.util.{AbstractID, Collector}
+
 import scala.collection.JavaConverters._
-import scala.collection.mutable
 import scala.reflect.ClassTag
 
-
 /**
  * The DataSet, the basic abstraction of Flink. This represents a collection of elements of a
  * specific type `T`. The operations in this class can be used to create new DataSets and to combine
@@ -522,7 +522,7 @@ class DataSet[T: ClassTag](set: JavaDataSet[T]) {
   @throws(classOf[Exception])
   def count(): Long = {
     val id = new AbstractID().toString
-    javaSet.flatMap(new CountHelper[T](id)).output(new DiscardingOutputFormat[lang.Long])
+    javaSet.flatMap(new CountHelper[T](id)).output(new DiscardingOutputFormat[java.lang.Long])
     val res = getExecutionEnvironment.execute()
     res.getAccumulatorResult[Long](id)
   }
@@ -537,11 +537,33 @@ class DataSet[T: ClassTag](set: JavaDataSet[T]) {
    */
   @throws(classOf[Exception])
   def collect(): Seq[T] = {
+    val typeClass: Class[_] = getType().getTypeClass()
+    val cl: ClassLoader = if (typeClass.getClassLoader == null) ClassLoader.getSystemClassLoader
+                            else typeClass.getClassLoader
+
+    if (typeClass != null && !classOf[java.io.Serializable].isAssignableFrom(typeClass)) {
+      throw new UnsupportedOperationException(
+        "collect() can only be used with serializable data types. " +
+        "The DataSet type '" + typeClass.getName + "' does not implement java.io.Serializable.")
+    }
+
     val id = new AbstractID().toString
     javaSet.flatMap(new Utils.CollectHelper[T](id)).output(new DiscardingOutputFormat[T])
     val res = getExecutionEnvironment.execute()
 
-    res.getAccumulatorResult(id).asInstanceOf[java.util.List[T]].asScala
+    val accResult: java.util.ArrayList[Array[Byte]] = res.getAccumulatorResult(id)
+
+    try {
+      SerializedListAccumulator.deserializeList(accResult, cl).asScala
+    }
+    catch {
+      case e: ClassNotFoundException => {
+        throw new RuntimeException("Cannot find type class of collected data type.", e)
+      }
+      case e: java.io.IOException => {
+        throw new RuntimeException("Serialization error while deserializing collected data", e)
+      }
+    }
   }
 
   /**
diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/util/ClusterUtil.java b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/util/ClusterUtil.java
index 77ac0c5a3b0..85563e9f031 100644
--- a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/util/ClusterUtil.java
+++ b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/util/ClusterUtil.java
@@ -21,6 +21,7 @@ import org.apache.flink.api.common.JobExecutionResult;
 import org.apache.flink.configuration.ConfigConstants;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.runtime.client.JobClient;
+import org.apache.flink.runtime.client.SerializedJobExecutionResult;
 import org.apache.flink.runtime.jobgraph.JobGraph;
 import org.apache.flink.runtime.minicluster.LocalFlinkMiniCluster;
 import org.slf4j.Logger;
@@ -61,11 +62,11 @@ public class ClusterUtil {
 			exec = new LocalFlinkMiniCluster(configuration, true);
 			ActorRef jobClient = exec.getJobClient();
 
-			return JobClient.submitJobAndWait(jobGraph, true, jobClient, exec.timeout());
-
-		} catch (Exception e) {
-			throw e;
-		} finally {
+			SerializedJobExecutionResult result =
+					JobClient.submitJobAndWait(jobGraph, true, jobClient, exec.timeout());
+			return result.toJobExecutionResult(ClusterUtil.class.getClassLoader());
+		}
+		finally {
 			if (exec != null) {
 				exec.stop();
 			}
diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/util/TestStreamEnvironment.java b/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/util/TestStreamEnvironment.java
index f7843cf4757..4dac9804c62 100644
--- a/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/util/TestStreamEnvironment.java
+++ b/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/util/TestStreamEnvironment.java
@@ -24,6 +24,7 @@ import org.apache.flink.configuration.ConfigConstants;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.runtime.client.JobClient;
 import org.apache.flink.runtime.client.JobExecutionException;
+import org.apache.flink.runtime.client.SerializedJobExecutionResult;
 import org.apache.flink.runtime.jobgraph.JobGraph;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironmentFactory;
@@ -70,9 +71,14 @@ public class TestStreamEnvironment extends StreamExecutionEnvironment {
 		}
 		try {
 			ActorRef client = executor.getJobClient();
-			latestResult = JobClient.submitJobAndWait(jobGraph, false, client, executor.timeout());
+
+			SerializedJobExecutionResult result =
+					JobClient.submitJobAndWait(jobGraph, false, client, executor.timeout());
+
+			latestResult = result.toJobExecutionResult(getClass().getClassLoader());
 			return latestResult;
-		} catch(JobExecutionException e) {
+		}
+		catch(JobExecutionException e) {
 			if (e.getMessage().contains("GraphConversionException")) {
 				throw new Exception(CANNOT_EXECUTE_EMPTY_JOB, e);
 			} else {
diff --git a/flink-test-utils/src/main/java/org/apache/flink/test/util/RecordAPITestBase.java b/flink-test-utils/src/main/java/org/apache/flink/test/util/RecordAPITestBase.java
index 5d4cf4d030c..6acd5b0cbd7 100644
--- a/flink-test-utils/src/main/java/org/apache/flink/test/util/RecordAPITestBase.java
+++ b/flink-test-utils/src/main/java/org/apache/flink/test/util/RecordAPITestBase.java
@@ -29,6 +29,7 @@ import org.apache.flink.optimizer.plandump.PlanJSONDumpGenerator;
 import org.apache.flink.optimizer.plantranslate.JobGraphGenerator;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.runtime.client.JobClient;
+import org.apache.flink.runtime.client.SerializedJobExecutionResult;
 import org.apache.flink.runtime.jobgraph.JobGraph;
 import org.junit.Assert;
 import org.junit.Test;
@@ -120,9 +121,10 @@ public abstract class RecordAPITestBase extends AbstractTestBase {
 			Assert.assertNotNull("Obtained null JobGraph", jobGraph);
 
 			try {
-			ActorRef client = this.executor.getJobClient();
-			this.jobExecutionResult = JobClient.submitJobAndWait(jobGraph, false, client,
-					executor.timeout());
+				ActorRef client = this.executor.getJobClient();
+				SerializedJobExecutionResult result =
+						JobClient.submitJobAndWait(jobGraph, false, client, executor.timeout());
+				this.jobExecutionResult = result.toJobExecutionResult(getClass().getClassLoader());
 			}
 			catch (Exception e) {
 				System.err.println(e.getMessage());
diff --git a/flink-test-utils/src/main/java/org/apache/flink/test/util/TestEnvironment.java b/flink-test-utils/src/main/java/org/apache/flink/test/util/TestEnvironment.java
index b3423cbc3fb..eaf9854f314 100644
--- a/flink-test-utils/src/main/java/org/apache/flink/test/util/TestEnvironment.java
+++ b/flink-test-utils/src/main/java/org/apache/flink/test/util/TestEnvironment.java
@@ -29,6 +29,7 @@ import org.apache.flink.optimizer.plan.OptimizedPlan;
 import org.apache.flink.optimizer.plandump.PlanJSONDumpGenerator;
 import org.apache.flink.optimizer.plantranslate.JobGraphGenerator;
 import org.apache.flink.runtime.client.JobClient;
+import org.apache.flink.runtime.client.SerializedJobExecutionResult;
 import org.apache.flink.runtime.jobgraph.JobGraph;
 import org.junit.Assert;
 
@@ -53,11 +54,11 @@ public class TestEnvironment extends ExecutionEnvironment {
 			JobGraph jobGraph = jgg.compileJobGraph(op);
 
 			ActorRef client = this.executor.getJobClient();
-			JobExecutionResult result = JobClient.submitJobAndWait(jobGraph, false, client,
-					executor.timeout());
+			SerializedJobExecutionResult result =
+					JobClient.submitJobAndWait(jobGraph, false, client, executor.timeout());
 
-			this.latestResult = result;
-			return result;
+			this.latestResult = result.toJobExecutionResult(getClass().getClassLoader());
+			return this.latestResult;
 		}
 		catch (Exception e) {
 			System.err.println(e.getMessage());
diff --git a/flink-tests/src/test/java/org/apache/flink/test/accumulators/AccumulatorITCase.java b/flink-tests/src/test/java/org/apache/flink/test/accumulators/AccumulatorITCase.java
index 1d7d2066cc6..11d6b8365a4 100644
--- a/flink-tests/src/test/java/org/apache/flink/test/accumulators/AccumulatorITCase.java
+++ b/flink-tests/src/test/java/org/apache/flink/test/accumulators/AccumulatorITCase.java
@@ -18,9 +18,7 @@
 
 package org.apache.flink.test.accumulators;
 
-import java.io.IOException;
-import java.io.ObjectInputStream;
-import java.io.ObjectOutputStream;
+import java.util.HashSet;
 import java.util.Map;
 import java.util.Set;
 
@@ -36,10 +34,6 @@ import org.apache.flink.api.java.DataSet;
 import org.apache.flink.api.java.ExecutionEnvironment;
 import org.apache.flink.api.java.tuple.Tuple2;
 import org.apache.flink.configuration.Configuration;
-import org.apache.flink.core.io.IOReadableWritable;
-import org.apache.flink.core.memory.InputViewObjectInputStreamWrapper;
-import org.apache.flink.core.memory.OutputViewObjectOutputStreamWrapper;
-import org.apache.flink.runtime.util.SerializableHashSet;
 import org.apache.flink.test.util.JavaProgramTestBase;
 import org.apache.flink.types.StringValue;
 import org.apache.flink.util.Collector;
@@ -155,7 +149,9 @@ public class AccumulatorITCase extends JavaProgramTestBase {
 				// getRuntimeContext().getAggregator("custom",
 				// DoubleSumAggregator.class);
 				Assert.fail("Should not be able to obtain previously created counter with different type");
-			} catch (UnsupportedOperationException ex) {
+			}
+			catch (UnsupportedOperationException ex) {
+				// expected!
 			}
 
 			// Test counter used in open() and closed()
@@ -222,11 +218,11 @@ public class AccumulatorITCase extends JavaProgramTestBase {
 	/**
 	 * Custom accumulator
 	 */
-	public static class SetAccumulator<T extends IOReadableWritable> implements Accumulator<T, SerializableHashSet<T>> {
+	public static class SetAccumulator<T> implements Accumulator<T, HashSet<T>> {
 
 		private static final long serialVersionUID = 1L;
 
-		private SerializableHashSet<T> set = new SerializableHashSet<T>();
+		private HashSet<T> set = new HashSet<T>();
 
 		@Override
 		public void add(T value) {
@@ -234,7 +230,7 @@ public class AccumulatorITCase extends JavaProgramTestBase {
 		}
 
 		@Override
-		public SerializableHashSet<T> getLocalValue() {
+		public HashSet<T> getLocalValue() {
 			return this.set;
 		}
 
@@ -244,27 +240,15 @@ public class AccumulatorITCase extends JavaProgramTestBase {
 		}
 
 		@Override
-		public void merge(Accumulator<T, SerializableHashSet<T>> other) {
+		public void merge(Accumulator<T, HashSet<T>> other) {
 			// build union
-			this.set.addAll(((SetAccumulator<T>) other).getLocalValue());
+			this.set.addAll(other.getLocalValue());
 		}
 
 		@Override
-		public void write(ObjectOutputStream out) throws IOException {
-			this.set.write(new OutputViewObjectOutputStreamWrapper(out));
-		}
-
-		@Override
-		public void read(ObjectInputStream in) throws IOException {
-			this.set.read(new InputViewObjectInputStreamWrapper(in));
-		}
-
-		@Override
-		public Accumulator<T, SerializableHashSet<T>> clone() {
+		public Accumulator<T, HashSet<T>> clone() {
 			SetAccumulator<T> result = new SetAccumulator<T>();
-
 			result.set.addAll(set);
-
 			return result;
 		}
 	}
diff --git a/flink-tests/src/test/java/org/apache/flink/test/failingPrograms/JobSubmissionFailsITCase.java b/flink-tests/src/test/java/org/apache/flink/test/failingPrograms/JobSubmissionFailsITCase.java
index 5690475c9fc..a54bd97261a 100644
--- a/flink-tests/src/test/java/org/apache/flink/test/failingPrograms/JobSubmissionFailsITCase.java
+++ b/flink-tests/src/test/java/org/apache/flink/test/failingPrograms/JobSubmissionFailsITCase.java
@@ -26,6 +26,7 @@ import org.apache.flink.runtime.akka.AkkaUtils;
 import org.apache.flink.runtime.client.JobClient;
 import org.apache.flink.runtime.client.JobExecutionException;
 import org.apache.flink.runtime.client.JobSubmissionException;
+import org.apache.flink.runtime.client.SerializedJobExecutionResult;
 import org.apache.flink.runtime.jobgraph.AbstractJobVertex;
 import org.apache.flink.runtime.jobgraph.JobGraph;
 import org.apache.flink.runtime.jobmanager.Tasks;
@@ -80,11 +81,14 @@ public class JobSubmissionFailsITCase {
 	}
 
 	private JobExecutionResult submitJob(JobGraph jobGraph, ActorRef jobClient) throws Exception {
-		if(detached) {
+		if (detached) {
 			JobClient.submitJobDetached(jobGraph, jobClient, TestingUtils.TESTING_DURATION());
 			return null;
-		} else {
-			return JobClient.submitJobAndWait(jobGraph, false, jobClient, TestingUtils.TESTING_DURATION());
+		}
+		else {
+			SerializedJobExecutionResult result =
+					JobClient.submitJobAndWait(jobGraph, false, jobClient, TestingUtils.TESTING_DURATION());
+			return result.toJobExecutionResult(getClass().getClassLoader());
 		}
 	}
 
diff --git a/flink-tests/src/test/java/org/apache/flink/test/localDistributed/PackagedProgramEndToEndITCase.java b/flink-tests/src/test/java/org/apache/flink/test/localDistributed/PackagedProgramEndToEndITCase.java
index 40d3d96fffb..f2517c6b8c6 100644
--- a/flink-tests/src/test/java/org/apache/flink/test/localDistributed/PackagedProgramEndToEndITCase.java
+++ b/flink-tests/src/test/java/org/apache/flink/test/localDistributed/PackagedProgramEndToEndITCase.java
@@ -58,8 +58,6 @@ public class PackagedProgramEndToEndITCase {
 			fwClusters.write(KMeansData.INITIAL_CENTERS);
 			fwClusters.close();
 
-			
-
 			// run KMeans
 			Configuration config = new Configuration();
 			config.setInteger(ConfigConstants.LOCAL_INSTANCE_MANAGER_NUMBER_TASK_MANAGER, 2);
@@ -70,11 +68,10 @@ public class PackagedProgramEndToEndITCase {
 
 			ex.executeJar(JAR_PATH,
 					"org.apache.flink.test.util.testjar.KMeansForTest",
-					new String[] {
-							points.toURI().toString(),
-							clusters.toURI().toString(),
-							outFile.toURI().toString(),
-							"25"});
+					points.toURI().toString(),
+					clusters.toURI().toString(),
+					outFile.toURI().toString(),
+					"25");
 
 		} catch (Exception e) {
 			e.printStackTrace();
diff --git a/flink-tests/src/test/java/org/apache/flink/test/recordJobs/wordcount/WordCountAccumulators.java b/flink-tests/src/test/java/org/apache/flink/test/recordJobs/wordcount/WordCountAccumulators.java
index 5f4fe149158..780db582b38 100644
--- a/flink-tests/src/test/java/org/apache/flink/test/recordJobs/wordcount/WordCountAccumulators.java
+++ b/flink-tests/src/test/java/org/apache/flink/test/recordJobs/wordcount/WordCountAccumulators.java
@@ -18,10 +18,8 @@
 
 package org.apache.flink.test.recordJobs.wordcount;
 
-import java.io.IOException;
-import java.io.ObjectInputStream;
-import java.io.ObjectOutputStream;
 import java.io.Serializable;
+import java.util.HashSet;
 import java.util.Iterator;
 import java.util.StringTokenizer;
 
@@ -44,9 +42,6 @@ import org.apache.flink.api.java.record.operators.ReduceOperator;
 import org.apache.flink.api.java.record.operators.ReduceOperator.Combinable;
 import org.apache.flink.client.LocalExecutor;
 import org.apache.flink.configuration.Configuration;
-import org.apache.flink.core.memory.InputViewObjectInputStreamWrapper;
-import org.apache.flink.core.memory.OutputViewObjectOutputStreamWrapper;
-import org.apache.flink.runtime.util.SerializableHashSet;
 import org.apache.flink.types.IntValue;
 import org.apache.flink.types.Record;
 import org.apache.flink.types.StringValue;
@@ -193,11 +188,11 @@ public class WordCountAccumulators implements Program, ProgramDescription {
 	/**
 	 * Custom accumulator
 	 */
-	public static class SetAccumulator<T extends Value> implements Accumulator<T, SerializableHashSet<T>> {
+	public static class SetAccumulator<T extends Value> implements Accumulator<T, HashSet<T>> {
 
 		private static final long serialVersionUID = 1L;
 
-		private SerializableHashSet<T> set = new SerializableHashSet<T>();
+		private HashSet<T> set = new HashSet<T>();
 
 		@Override
 		public void add(T value) {
@@ -205,7 +200,7 @@ public class WordCountAccumulators implements Program, ProgramDescription {
 		}
 
 		@Override
-		public SerializableHashSet<T> getLocalValue() {
+		public HashSet<T> getLocalValue() {
 			return this.set;
 		}
 
@@ -215,27 +210,15 @@ public class WordCountAccumulators implements Program, ProgramDescription {
 		}
 
 		@Override
-		public void merge(Accumulator<T, SerializableHashSet<T>> other) {
+		public void merge(Accumulator<T, HashSet<T>> other) {
 			// build union
-			this.set.addAll(((SetAccumulator<T>) other).getLocalValue());
+			this.set.addAll(other.getLocalValue());
 		}
 
 		@Override
-		public void write(ObjectOutputStream out) throws IOException {
-			this.set.write(new OutputViewObjectOutputStreamWrapper(out));
-		}
-
-		@Override
-		public void read(ObjectInputStream in) throws IOException {
-			this.set.read(new InputViewObjectInputStreamWrapper(in));
-		}
-
-		@Override
-		public Accumulator<T, SerializableHashSet<T>> clone() {
+		public Accumulator<T, HashSet<T>> clone() {
 			SetAccumulator<T> result = new SetAccumulator<T>();
-
 			result.set.addAll(set);
-
 			return result;
 		}
 	}
diff --git a/flink-tests/src/test/resources/log4j-test.properties b/flink-tests/src/test/resources/log4j-test.properties
index 85897b37b49..0845c812e0c 100644
--- a/flink-tests/src/test/resources/log4j-test.properties
+++ b/flink-tests/src/test/resources/log4j-test.properties
@@ -18,7 +18,7 @@
 
 # Set root logger level to OFF to not flood build logs
 # set manually to INFO for debugging purposes
-log4j.rootLogger=OFF, testlogger
+log4j.rootLogger=INFO, testlogger
 
 # A1 is set to be a ConsoleAppender.
 log4j.appender.testlogger=org.apache.log4j.ConsoleAppender
diff --git a/flink-tests/src/test/scala/org/apache/flink/api/scala/actions/CountCollectITCase.scala b/flink-tests/src/test/scala/org/apache/flink/api/scala/actions/CountCollectITCase.scala
index f9c744d6973..2d2cdd31b74 100644
--- a/flink-tests/src/test/scala/org/apache/flink/api/scala/actions/CountCollectITCase.scala
+++ b/flink-tests/src/test/scala/org/apache/flink/api/scala/actions/CountCollectITCase.scala
@@ -39,7 +39,7 @@ class CountCollectITCase(mode: TestExecutionMode) extends MultipleProgramsTestBa
     val inputDS = env.fromElements(input: _*)
 
     // count
-    val numEntries = inputDS.count
+    val numEntries = inputDS.count()
     assertEquals(input.length, numEntries)
 
     // collect
@@ -60,7 +60,7 @@ class CountCollectITCase(mode: TestExecutionMode) extends MultipleProgramsTestBa
 
     val result = inputDS1 cross inputDS2
 
-    val numEntries = result.count
+    val numEntries = result.count()
     assertEquals(input1.length * input2.length, numEntries)
 
     val list = result.collect()
diff --git a/flink-tests/src/test/scala/org/apache/flink/api/scala/operators/SumMinMaxITCase.scala b/flink-tests/src/test/scala/org/apache/flink/api/scala/operators/SumMinMaxITCase.scala
index 180486bb26d..d94f09921dd 100644
--- a/flink-tests/src/test/scala/org/apache/flink/api/scala/operators/SumMinMaxITCase.scala
+++ b/flink-tests/src/test/scala/org/apache/flink/api/scala/operators/SumMinMaxITCase.scala
@@ -47,7 +47,7 @@ class SumMinMaxITCase(mode: TestExecutionMode) extends MultipleProgramsTestBase(
       .map{ t => (t._1, t._2) }
 
 
-    val result: Seq[(Int, Long)] = aggregateDs.collect
+    val result: Seq[(Int, Long)] = aggregateDs.collect()
 
     assertEquals(1, result.size)
     assertEquals((231, 6L), result.head)
@@ -66,7 +66,7 @@ class SumMinMaxITCase(mode: TestExecutionMode) extends MultipleProgramsTestBase(
       .filter(_._3 != null)
       .map { t => (t._2, t._1) }
 
-    val result : Seq[(Long, Int)] = aggregateDs.collect.sortWith((a, b) => a._1 < b._1)
+    val result : Seq[(Long, Int)] = aggregateDs.collect().sortWith((a, b) => a._1 < b._1)
 
     val expected : Seq[(Long, Int)] = Seq((1L, 1), (2L, 5), (3L, 15), (4L, 34), (5L, 65), (6L, 111))
     assertEquals(expected, result)
@@ -86,7 +86,7 @@ class SumMinMaxITCase(mode: TestExecutionMode) extends MultipleProgramsTestBase(
       .filter(_._3 != null)
       .map { t => t._1 }
 
-    val result: Seq[Int] = aggregateDs.collect
+    val result: Seq[Int] = aggregateDs.collect()
 
     assertEquals(1, result.size)
     assertEquals(Seq(1), result)
diff --git a/flink-tests/src/test/scala/org/apache/flink/api/scala/runtime/jobmanager/JobManagerFailsITCase.scala b/flink-tests/src/test/scala/org/apache/flink/api/scala/runtime/jobmanager/JobManagerFailsITCase.scala
index 625ca074c4b..12e0e5b1d1e 100644
--- a/flink-tests/src/test/scala/org/apache/flink/api/scala/runtime/jobmanager/JobManagerFailsITCase.scala
+++ b/flink-tests/src/test/scala/org/apache/flink/api/scala/runtime/jobmanager/JobManagerFailsITCase.scala
@@ -123,7 +123,7 @@ with WordSpecLike with Matchers with BeforeAndAfterAll {
 
           val result = expectMsgType[JobResultSuccess]
 
-          result.jobID should equal(jobGraph2.getJobID)
+          result.result.getJobId() should equal(jobGraph2.getJobID)
         }
       } finally {
         cluster.stop()
diff --git a/flink-tests/src/test/scala/org/apache/flink/api/scala/runtime/taskmanager/TaskManagerFailsITCase.scala b/flink-tests/src/test/scala/org/apache/flink/api/scala/runtime/taskmanager/TaskManagerFailsITCase.scala
index 659262cffca..74492154a2b 100644
--- a/flink-tests/src/test/scala/org/apache/flink/api/scala/runtime/taskmanager/TaskManagerFailsITCase.scala
+++ b/flink-tests/src/test/scala/org/apache/flink/api/scala/runtime/taskmanager/TaskManagerFailsITCase.scala
@@ -218,7 +218,7 @@ with WordSpecLike with Matchers with BeforeAndAfterAll {
           expectMsgType[Success]
 
           val result = expectMsgType[JobResultSuccess]
-          result.jobID should equal(jobGraph2.getJobID)
+          result.result.getJobId() should equal(jobGraph2.getJobID)
         }
       } finally {
         cluster.stop()
