diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/AdaptiveBatchScheduler.java b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/AdaptiveBatchScheduler.java
index e05df56b84f..160f080c0fc 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/AdaptiveBatchScheduler.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/AdaptiveBatchScheduler.java
@@ -62,7 +62,6 @@ import org.apache.flink.runtime.scheduler.ExecutionSlotAllocatorFactory;
 import org.apache.flink.runtime.scheduler.ExecutionVertexVersioner;
 import org.apache.flink.runtime.scheduler.VertexParallelismStore;
 import org.apache.flink.runtime.scheduler.adaptivebatch.forwardgroup.ForwardGroup;
-import org.apache.flink.runtime.scheduler.adaptivebatch.forwardgroup.ForwardGroupComputeUtil;
 import org.apache.flink.runtime.scheduler.strategy.ExecutionVertexID;
 import org.apache.flink.runtime.scheduler.strategy.SchedulingStrategyFactory;
 import org.apache.flink.runtime.shuffle.ShuffleMaster;
@@ -127,7 +126,8 @@ public class AdaptiveBatchScheduler extends DefaultScheduler {
             final Time rpcTimeout,
             final VertexParallelismAndInputInfosDecider vertexParallelismAndInputInfosDecider,
             int defaultMaxParallelism,
-            HybridPartitionDataConsumeConstraint hybridPartitionDataConsumeConstraint)
+            final HybridPartitionDataConsumeConstraint hybridPartitionDataConsumeConstraint,
+            final Map<JobVertexID, ForwardGroup> forwardGroupsByJobVertexId)
             throws Exception {
 
         super(
@@ -162,10 +162,7 @@ public class AdaptiveBatchScheduler extends DefaultScheduler {
         this.vertexParallelismAndInputInfosDecider =
                 checkNotNull(vertexParallelismAndInputInfosDecider);
 
-        this.forwardGroupsByJobVertexId =
-                ForwardGroupComputeUtil.computeForwardGroups(
-                        jobGraph.getVerticesSortedTopologicallyFromSources(),
-                        getExecutionGraph()::getJobVertex);
+        this.forwardGroupsByJobVertexId = checkNotNull(forwardGroupsByJobVertexId);
 
         this.blockingResultInfos = new HashMap<>();
 
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/AdaptiveBatchSchedulerFactory.java b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/AdaptiveBatchSchedulerFactory.java
index 37a7d1801e5..8bfe00f591e 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/AdaptiveBatchSchedulerFactory.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/AdaptiveBatchSchedulerFactory.java
@@ -43,6 +43,7 @@ import org.apache.flink.runtime.jobgraph.IntermediateDataSet;
 import org.apache.flink.runtime.jobgraph.JobGraph;
 import org.apache.flink.runtime.jobgraph.JobType;
 import org.apache.flink.runtime.jobgraph.JobVertex;
+import org.apache.flink.runtime.jobgraph.JobVertexID;
 import org.apache.flink.runtime.jobmaster.ExecutionDeploymentTracker;
 import org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotProvider;
 import org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotProviderImpl;
@@ -59,6 +60,8 @@ import org.apache.flink.runtime.scheduler.ExecutionVertexVersioner;
 import org.apache.flink.runtime.scheduler.SchedulerNG;
 import org.apache.flink.runtime.scheduler.SchedulerNGFactory;
 import org.apache.flink.runtime.scheduler.SimpleExecutionSlotAllocator;
+import org.apache.flink.runtime.scheduler.adaptivebatch.forwardgroup.ForwardGroup;
+import org.apache.flink.runtime.scheduler.adaptivebatch.forwardgroup.ForwardGroupComputeUtil;
 import org.apache.flink.runtime.scheduler.strategy.AllFinishedInputConsumableDecider;
 import org.apache.flink.runtime.scheduler.strategy.DefaultInputConsumableDecider;
 import org.apache.flink.runtime.scheduler.strategy.InputConsumableDecider;
@@ -75,6 +78,7 @@ import org.slf4j.LoggerFactory;
 
 import java.util.ArrayList;
 import java.util.List;
+import java.util.Map;
 import java.util.concurrent.Executor;
 import java.util.concurrent.ScheduledExecutorService;
 import java.util.function.Consumer;
@@ -176,6 +180,10 @@ public class AdaptiveBatchSchedulerFactory implements SchedulerNGFactory {
         int defaultMaxParallelism =
                 getDefaultMaxParallelism(jobMasterConfiguration, executionConfig);
 
+        final Map<JobVertexID, ForwardGroup> forwardGroupsByJobVertexId =
+                ForwardGroupComputeUtil.computeForwardGroupsAndSetVertexParallelismsIfNecessary(
+                        jobGraph.getVerticesSortedTopologicallyFromSources());
+
         if (enableSpeculativeExecution) {
             return new SpeculativeScheduler(
                     log,
@@ -205,7 +213,8 @@ public class AdaptiveBatchSchedulerFactory implements SchedulerNGFactory {
                             defaultMaxParallelism, jobMasterConfiguration),
                     defaultMaxParallelism,
                     blocklistOperations,
-                    hybridPartitionDataConsumeConstraint);
+                    hybridPartitionDataConsumeConstraint,
+                    forwardGroupsByJobVertexId);
         } else {
             return new AdaptiveBatchScheduler(
                     log,
@@ -234,7 +243,8 @@ public class AdaptiveBatchSchedulerFactory implements SchedulerNGFactory {
                     DefaultVertexParallelismAndInputInfosDecider.from(
                             defaultMaxParallelism, jobMasterConfiguration),
                     defaultMaxParallelism,
-                    hybridPartitionDataConsumeConstraint);
+                    hybridPartitionDataConsumeConstraint,
+                    forwardGroupsByJobVertexId);
         }
     }
 
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/SpeculativeScheduler.java b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/SpeculativeScheduler.java
index d8b18a92b3d..35400a824d4 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/SpeculativeScheduler.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/SpeculativeScheduler.java
@@ -44,12 +44,14 @@ import org.apache.flink.runtime.executiongraph.failover.flip1.FailureHandlingRes
 import org.apache.flink.runtime.executiongraph.failover.flip1.RestartBackoffTimeStrategy;
 import org.apache.flink.runtime.io.network.partition.PartitionException;
 import org.apache.flink.runtime.jobgraph.JobGraph;
+import org.apache.flink.runtime.jobgraph.JobVertexID;
 import org.apache.flink.runtime.metrics.MetricNames;
 import org.apache.flink.runtime.metrics.groups.JobManagerJobMetricGroup;
 import org.apache.flink.runtime.scheduler.ExecutionGraphFactory;
 import org.apache.flink.runtime.scheduler.ExecutionOperations;
 import org.apache.flink.runtime.scheduler.ExecutionSlotAllocatorFactory;
 import org.apache.flink.runtime.scheduler.ExecutionVertexVersioner;
+import org.apache.flink.runtime.scheduler.adaptivebatch.forwardgroup.ForwardGroup;
 import org.apache.flink.runtime.scheduler.slowtaskdetector.ExecutionTimeBasedSlowTaskDetector;
 import org.apache.flink.runtime.scheduler.slowtaskdetector.SlowTaskDetector;
 import org.apache.flink.runtime.scheduler.slowtaskdetector.SlowTaskDetectorListener;
@@ -124,7 +126,8 @@ public class SpeculativeScheduler extends AdaptiveBatchScheduler
             final VertexParallelismAndInputInfosDecider vertexParallelismAndInputInfosDecider,
             final int defaultMaxParallelism,
             final BlocklistOperations blocklistOperations,
-            final HybridPartitionDataConsumeConstraint hybridPartitionDataConsumeConstraint)
+            final HybridPartitionDataConsumeConstraint hybridPartitionDataConsumeConstraint,
+            final Map<JobVertexID, ForwardGroup> forwardGroupsByJobVertexId)
             throws Exception {
 
         super(
@@ -152,7 +155,8 @@ public class SpeculativeScheduler extends AdaptiveBatchScheduler
                 rpcTimeout,
                 vertexParallelismAndInputInfosDecider,
                 defaultMaxParallelism,
-                hybridPartitionDataConsumeConstraint);
+                hybridPartitionDataConsumeConstraint,
+                forwardGroupsByJobVertexId);
 
         this.maxConcurrentExecutions =
                 jobMasterConfiguration.getInteger(
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/forwardgroup/ForwardGroup.java b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/forwardgroup/ForwardGroup.java
index fa5dffc5c2a..8b05fd675b1 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/forwardgroup/ForwardGroup.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/forwardgroup/ForwardGroup.java
@@ -20,7 +20,7 @@
 package org.apache.flink.runtime.scheduler.adaptivebatch.forwardgroup;
 
 import org.apache.flink.api.common.ExecutionConfig;
-import org.apache.flink.runtime.executiongraph.ExecutionJobVertex;
+import org.apache.flink.runtime.jobgraph.JobVertex;
 import org.apache.flink.runtime.jobgraph.JobVertexID;
 
 import java.util.HashSet;
@@ -40,17 +40,17 @@ public class ForwardGroup {
 
     private final Set<JobVertexID> jobVertexIds = new HashSet<>();
 
-    public ForwardGroup(final Set<ExecutionJobVertex> jobVertices) {
+    public ForwardGroup(final Set<JobVertex> jobVertices) {
         checkNotNull(jobVertices);
 
         Set<Integer> decidedParallelisms =
                 jobVertices.stream()
                         .filter(
                                 jobVertex -> {
-                                    jobVertexIds.add(jobVertex.getJobVertexId());
-                                    return jobVertex.isParallelismDecided();
+                                    jobVertexIds.add(jobVertex.getID());
+                                    return jobVertex.getParallelism() > 0;
                                 })
-                        .map(ExecutionJobVertex::getParallelism)
+                        .map(JobVertex::getParallelism)
                         .collect(Collectors.toSet());
 
         checkState(decidedParallelisms.size() <= 1);
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/forwardgroup/ForwardGroupComputeUtil.java b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/forwardgroup/ForwardGroupComputeUtil.java
index 28491cf32f1..a45304b4021 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/forwardgroup/ForwardGroupComputeUtil.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptivebatch/forwardgroup/ForwardGroupComputeUtil.java
@@ -19,7 +19,7 @@
 
 package org.apache.flink.runtime.scheduler.adaptivebatch.forwardgroup;
 
-import org.apache.flink.runtime.executiongraph.ExecutionJobVertex;
+import org.apache.flink.api.common.ExecutionConfig;
 import org.apache.flink.runtime.executiongraph.VertexGroupComputeUtil;
 import org.apache.flink.runtime.jobgraph.JobEdge;
 import org.apache.flink.runtime.jobgraph.JobVertex;
@@ -30,15 +30,31 @@ import java.util.HashSet;
 import java.util.IdentityHashMap;
 import java.util.Map;
 import java.util.Set;
-import java.util.function.Function;
 import java.util.stream.Collectors;
 
 /** Common utils for computing forward groups. */
 public class ForwardGroupComputeUtil {
 
-    public static Map<JobVertexID, ForwardGroup> computeForwardGroups(
-            final Iterable<JobVertex> topologicallySortedVertices,
-            Function<JobVertexID, ExecutionJobVertex> executionJobVertexRetriever) {
+    public static Map<JobVertexID, ForwardGroup>
+            computeForwardGroupsAndSetVertexParallelismsIfNecessary(
+                    final Iterable<JobVertex> topologicallySortedVertices) {
+        final Map<JobVertexID, ForwardGroup> forwardGroupsByJobVertexId =
+                computeForwardGroups(topologicallySortedVertices);
+        // set parallelism for vertices in parallelism-decided forward groups
+        topologicallySortedVertices.forEach(
+                jobVertex -> {
+                    ForwardGroup forwardGroup = forwardGroupsByJobVertexId.get(jobVertex.getID());
+                    if (jobVertex.getParallelism() == ExecutionConfig.PARALLELISM_DEFAULT
+                            && forwardGroup != null
+                            && forwardGroup.isParallelismDecided()) {
+                        jobVertex.setParallelism(forwardGroup.getParallelism());
+                    }
+                });
+        return forwardGroupsByJobVertexId;
+    }
+
+    static Map<JobVertexID, ForwardGroup> computeForwardGroups(
+            final Iterable<JobVertex> topologicallySortedVertices) {
 
         final Map<JobVertex, Set<JobVertex>> vertexToGroup = new IdentityHashMap<>();
 
@@ -74,14 +90,7 @@ public class ForwardGroupComputeUtil {
         for (Set<JobVertex> vertexGroup :
                 VertexGroupComputeUtil.uniqueVertexGroups(vertexToGroup)) {
             if (vertexGroup.size() > 1) {
-                ForwardGroup forwardGroup =
-                        new ForwardGroup(
-                                vertexGroup.stream()
-                                        .map(
-                                                vertex ->
-                                                        executionJobVertexRetriever.apply(
-                                                                vertex.getID()))
-                                        .collect(Collectors.toSet()));
+                ForwardGroup forwardGroup = new ForwardGroup(vertexGroup);
                 for (JobVertexID jobVertexId : forwardGroup.getJobVertexIds()) {
                     ret.put(jobVertexId, forwardGroup);
                 }
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/DefaultSchedulerBuilder.java b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/DefaultSchedulerBuilder.java
index 4ac71011b80..374e10ea8b6 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/DefaultSchedulerBuilder.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/DefaultSchedulerBuilder.java
@@ -41,12 +41,14 @@ import org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRe
 import org.apache.flink.runtime.io.network.partition.JobMasterPartitionTracker;
 import org.apache.flink.runtime.io.network.partition.NoOpJobMasterPartitionTracker;
 import org.apache.flink.runtime.jobgraph.JobGraph;
+import org.apache.flink.runtime.jobgraph.JobVertexID;
 import org.apache.flink.runtime.jobmaster.DefaultExecutionDeploymentTracker;
 import org.apache.flink.runtime.metrics.groups.JobManagerJobMetricGroup;
 import org.apache.flink.runtime.metrics.groups.UnregisteredMetricGroups;
 import org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchScheduler;
 import org.apache.flink.runtime.scheduler.adaptivebatch.SpeculativeScheduler;
 import org.apache.flink.runtime.scheduler.adaptivebatch.VertexParallelismAndInputInfosDecider;
+import org.apache.flink.runtime.scheduler.adaptivebatch.forwardgroup.ForwardGroupComputeUtil;
 import org.apache.flink.runtime.scheduler.strategy.AllFinishedInputConsumableDecider;
 import org.apache.flink.runtime.scheduler.strategy.InputConsumableDecider;
 import org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy;
@@ -63,6 +65,7 @@ import org.slf4j.LoggerFactory;
 import java.util.Collections;
 import java.util.concurrent.Executor;
 import java.util.concurrent.ScheduledExecutorService;
+import java.util.function.Function;
 
 import static org.apache.flink.runtime.scheduler.SchedulerBase.computeVertexParallelismStore;
 
@@ -331,7 +334,9 @@ public class DefaultSchedulerBuilder {
                 rpcTimeout,
                 vertexParallelismAndInputInfosDecider,
                 defaultMaxParallelism,
-                hybridPartitionDataConsumeConstraint);
+                hybridPartitionDataConsumeConstraint,
+                ForwardGroupComputeUtil.computeForwardGroupsAndSetVertexParallelismsIfNecessary(
+                        jobGraph.getVerticesSortedTopologicallyFromSources()));
     }
 
     public SpeculativeScheduler buildSpeculativeScheduler() throws Exception {
@@ -361,7 +366,9 @@ public class DefaultSchedulerBuilder {
                 vertexParallelismAndInputInfosDecider,
                 defaultMaxParallelism,
                 blocklistOperations,
-                HybridPartitionDataConsumeConstraint.ALL_PRODUCERS_FINISHED);
+                HybridPartitionDataConsumeConstraint.ALL_PRODUCERS_FINISHED,
+                ForwardGroupComputeUtil.computeForwardGroupsAndSetVertexParallelismsIfNecessary(
+                        jobGraph.getVerticesSortedTopologicallyFromSources()));
     }
 
     private ExecutionGraphFactory createExecutionGraphFactory(boolean isDynamicGraph) {
@@ -390,8 +397,16 @@ public class DefaultSchedulerBuilder {
 
     public static VertexParallelismAndInputInfosDecider createCustomParallelismDecider(
             int expectParallelism) {
+        return createCustomParallelismDecider(ignore -> expectParallelism);
+    }
+
+    public static VertexParallelismAndInputInfosDecider createCustomParallelismDecider(
+            Function<JobVertexID, Integer> parallelismFunction) {
         return (jobVertexId, consumedResults, initialParallelism) -> {
-            int parallelism = initialParallelism > 0 ? initialParallelism : expectParallelism;
+            int parallelism =
+                    initialParallelism > 0
+                            ? initialParallelism
+                            : parallelismFunction.apply(jobVertexId);
             return new ParallelismAndInputInfos(
                     parallelism,
                     consumedResults.isEmpty()
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adaptivebatch/AdaptiveBatchSchedulerTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adaptivebatch/AdaptiveBatchSchedulerTest.java
index 48f4b602b4a..559028683e7 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adaptivebatch/AdaptiveBatchSchedulerTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adaptivebatch/AdaptiveBatchSchedulerTest.java
@@ -93,7 +93,7 @@ class AdaptiveBatchSchedulerTest {
 
     @Test
     void testAdaptiveBatchScheduler() throws Exception {
-        JobGraph jobGraph = createJobGraph(false);
+        JobGraph jobGraph = createJobGraph();
         Iterator<JobVertex> jobVertexIterator = jobGraph.getVertices().iterator();
         JobVertex source1 = jobVertexIterator.next();
         JobVertex source2 = jobVertexIterator.next();
@@ -118,44 +118,68 @@ class AdaptiveBatchSchedulerTest {
         // check that the jobGraph is updated
         assertThat(sink.getParallelism()).isEqualTo(10);
 
-        // check aggregatedInputDataBytes of each ExecutionVertex calculated.
-        checkAggregatedInputDataBytesIsCalculated(sinkExecutionJobVertex);
+        // check aggregatedInputDataBytes of each ExecutionVertex calculated. Total number of
+        // subpartitions of source1 is ceil(128 / 6) * 6 = 132, total number of subpartitions of
+        // source2 is ceil(128 / 4) * 4 = 128, so total bytes is (132 + 128) * SUBPARTITION_BYTES =
+        // 26_000L.
+        checkAggregatedInputDataBytesIsCalculated(sinkExecutionJobVertex, 26_000L);
     }
 
     @Test
     void testDecideParallelismForForwardTarget() throws Exception {
-        JobGraph jobGraph = createJobGraph(true);
-        Iterator<JobVertex> jobVertexIterator = jobGraph.getVertices().iterator();
-        JobVertex source1 = jobVertexIterator.next();
-        JobVertex source2 = jobVertexIterator.next();
-        JobVertex sink = jobVertexIterator.next();
+        final JobVertex source = createJobVertex("source", SOURCE_PARALLELISM_1);
+        final JobVertex map = createJobVertex("map", -1);
+        final JobVertex sink = createJobVertex("sink", -1);
 
-        SchedulerBase scheduler = createScheduler(jobGraph);
+        map.connectNewDataSetAsInput(
+                source, DistributionPattern.POINTWISE, ResultPartitionType.BLOCKING);
+        sink.connectNewDataSetAsInput(
+                map, DistributionPattern.POINTWISE, ResultPartitionType.BLOCKING);
+        map.getProducedDataSets().get(0).getConsumers().get(0).setForward(true);
+
+        SchedulerBase scheduler =
+                createScheduler(
+                        new JobGraph(new JobID(), "test job", source, map, sink),
+                        createCustomParallelismDecider(
+                                jobVertexId -> {
+                                    if (jobVertexId.equals(map.getID())) {
+                                        return 5;
+                                    } else {
+                                        return 10;
+                                    }
+                                }),
+                        128);
 
         final DefaultExecutionGraph graph = (DefaultExecutionGraph) scheduler.getExecutionGraph();
+        final ExecutionJobVertex mapExecutionJobVertex = graph.getJobVertex(map.getID());
         final ExecutionJobVertex sinkExecutionJobVertex = graph.getJobVertex(sink.getID());
 
         scheduler.startScheduling();
+        assertThat(mapExecutionJobVertex.getParallelism()).isEqualTo(-1);
         assertThat(sinkExecutionJobVertex.getParallelism()).isEqualTo(-1);
 
-        // trigger source1 finished.
-        transitionExecutionsState(scheduler, ExecutionState.FINISHED, source1);
+        // trigger source finished.
+        transitionExecutionsState(scheduler, ExecutionState.FINISHED, source);
+        assertThat(mapExecutionJobVertex.getParallelism()).isEqualTo(5);
         assertThat(sinkExecutionJobVertex.getParallelism()).isEqualTo(-1);
 
-        // trigger source2 finished.
-        transitionExecutionsState(scheduler, ExecutionState.FINISHED, source2);
-        assertThat(sinkExecutionJobVertex.getParallelism()).isEqualTo(SOURCE_PARALLELISM_1);
+        // trigger map finished.
+        transitionExecutionsState(scheduler, ExecutionState.FINISHED, map);
+        assertThat(mapExecutionJobVertex.getParallelism()).isEqualTo(5);
+        assertThat(sinkExecutionJobVertex.getParallelism()).isEqualTo(5);
 
         // check that the jobGraph is updated
-        assertThat(sink.getParallelism()).isEqualTo(SOURCE_PARALLELISM_1);
+        assertThat(sink.getParallelism()).isEqualTo(5);
 
-        // check aggregatedInputDataBytes of each ExecutionVertex calculated.
-        checkAggregatedInputDataBytesIsCalculated(sinkExecutionJobVertex);
+        // check aggregatedInputDataBytes of each ExecutionVertex calculated. Total number of
+        // subpartitions of map is ceil(128 / 5) * 5 = 130, so total bytes sink consume is 130 *
+        // SUBPARTITION_BYTES = 13_000L.
+        checkAggregatedInputDataBytesIsCalculated(sinkExecutionJobVertex, 13_000L);
     }
 
     @Test
     void testUpdateBlockingResultInfoWhileScheduling() throws Exception {
-        JobGraph jobGraph = createJobGraph(false);
+        JobGraph jobGraph = createJobGraph();
         Iterator<JobVertex> jobVertexIterator = jobGraph.getVertices().iterator();
         JobVertex source1 = jobVertexIterator.next();
         JobVertex source2 = jobVertexIterator.next();
@@ -281,7 +305,7 @@ class AdaptiveBatchSchedulerTest {
     }
 
     private void checkAggregatedInputDataBytesIsCalculated(
-            ExecutionJobVertex sinkExecutionJobVertex) {
+            ExecutionJobVertex sinkExecutionJobVertex, long expectedTotalBytes) {
         final ExecutionVertex[] executionVertices = sinkExecutionJobVertex.getTaskVertices();
         long totalInputBytes = 0;
         for (ExecutionVertex ev : executionVertices) {
@@ -290,7 +314,7 @@ class AdaptiveBatchSchedulerTest {
             totalInputBytes += executionInputBytes;
         }
 
-        assertThat(totalInputBytes).isEqualTo(26_000L);
+        assertThat(totalInputBytes).isEqualTo(expectedTotalBytes);
     }
 
     private void triggerFailedByPartitionNotFound(
@@ -375,7 +399,7 @@ class AdaptiveBatchSchedulerTest {
         return jobVertex;
     }
 
-    public JobGraph createJobGraph(boolean withForwardEdge) {
+    public JobGraph createJobGraph() {
         final JobVertex source1 = createJobVertex("source1", SOURCE_PARALLELISM_1);
         final JobVertex source2 = createJobVertex("source2", SOURCE_PARALLELISM_2);
         final JobVertex sink = createJobVertex("sink", -1);
@@ -383,9 +407,6 @@ class AdaptiveBatchSchedulerTest {
                 source1, DistributionPattern.POINTWISE, ResultPartitionType.BLOCKING);
         sink.connectNewDataSetAsInput(
                 source2, DistributionPattern.POINTWISE, ResultPartitionType.BLOCKING);
-        if (withForwardEdge) {
-            source1.getProducedDataSets().get(0).getConsumers().get(0).setForward(true);
-        }
         return new JobGraph(new JobID(), "test job", source1, source2, sink);
     }
 
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adaptivebatch/forwardgroup/ForwardGroupComputeUtilTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adaptivebatch/forwardgroup/ForwardGroupComputeUtilTest.java
index e9b0da1509d..7ef152ba30f 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adaptivebatch/forwardgroup/ForwardGroupComputeUtilTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adaptivebatch/forwardgroup/ForwardGroupComputeUtilTest.java
@@ -18,15 +18,9 @@
 
 package org.apache.flink.runtime.scheduler.adaptivebatch.forwardgroup;
 
-import org.apache.flink.api.common.JobID;
-import org.apache.flink.runtime.executiongraph.DefaultExecutionGraph;
-import org.apache.flink.runtime.executiongraph.ExecutionGraph;
-import org.apache.flink.runtime.executiongraph.TestingDefaultExecutionGraphBuilder;
 import org.apache.flink.runtime.io.network.partition.ResultPartitionType;
 import org.apache.flink.runtime.jobgraph.DistributionPattern;
-import org.apache.flink.runtime.jobgraph.JobGraph;
 import org.apache.flink.runtime.jobgraph.JobVertex;
-import org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchScheduler;
 import org.apache.flink.runtime.testtasks.NoOpInvokable;
 import org.apache.flink.testutils.TestingUtils;
 import org.apache.flink.testutils.executor.TestExecutorExtension;
@@ -182,31 +176,62 @@ class ForwardGroupComputeUtilTest {
         checkGroupSize(groups, 1, 3);
     }
 
-    private static Set<ForwardGroup> computeForwardGroups(JobVertex... vertices) throws Exception {
+    /**
+     * Tests whether the parallelism of job vertices in forward group are correctly set.
+     *
+     * <pre>
+     *
+     *     (v1) -> (v2)
+     *
+     *     (v3) -> (v4)
+     *
+     * </pre>
+     */
+    @Test
+    void testComputeForwardGroupsAndSetVertexParallelismsIfNecessary() throws Exception {
+        JobVertex v1 = new JobVertex("v1");
+        JobVertex v2 = new JobVertex("v2");
+        JobVertex v3 = new JobVertex("v3");
+        JobVertex v4 = new JobVertex("v4");
+
+        v2.setParallelism(8);
+
+        v2.connectNewDataSetAsInput(
+                v1, DistributionPattern.ALL_TO_ALL, ResultPartitionType.BLOCKING);
+        v4.connectNewDataSetAsInput(
+                v3, DistributionPattern.POINTWISE, ResultPartitionType.BLOCKING);
+
+        v1.getProducedDataSets().get(0).getConsumers().get(0).setForward(true);
+        v3.getProducedDataSets().get(0).getConsumers().get(0).setForward(true);
+
+        Set<ForwardGroup> groups =
+                computeForwardGroupsAndSetVertexParallelismsIfNecessary(v1, v2, v3, v4);
+        checkGroupSize(groups, 2, 2, 2);
+        assertThat(v1.getParallelism()).isEqualTo(8);
+        assertThat(v2.getParallelism()).isEqualTo(8);
+        assertThat(v3.getParallelism()).isEqualTo(-1);
+        assertThat(v4.getParallelism()).isEqualTo(-1);
+    }
+
+    private static Set<ForwardGroup> computeForwardGroupsAndSetVertexParallelismsIfNecessary(
+            JobVertex... vertices) throws Exception {
         Arrays.asList(vertices).forEach(vertex -> vertex.setInvokableClass(NoOpInvokable.class));
-        ExecutionGraph executionGraph = createDynamicGraph(vertices);
         return new HashSet<>(
-                ForwardGroupComputeUtil.computeForwardGroups(
-                                Arrays.asList(vertices), executionGraph::getJobVertex)
+                ForwardGroupComputeUtil.computeForwardGroupsAndSetVertexParallelismsIfNecessary(
+                                Arrays.asList(vertices))
                         .values());
     }
 
+    private static Set<ForwardGroup> computeForwardGroups(JobVertex... vertices) throws Exception {
+        Arrays.asList(vertices).forEach(vertex -> vertex.setInvokableClass(NoOpInvokable.class));
+        return new HashSet<>(
+                ForwardGroupComputeUtil.computeForwardGroups(Arrays.asList(vertices)).values());
+    }
+
     private static void checkGroupSize(
             Set<ForwardGroup> groups, int numOfGroups, Integer... sizes) {
         assertThat(groups.size()).isEqualTo(numOfGroups);
         assertThat(groups.stream().map(ForwardGroup::size).collect(Collectors.toList()))
                 .contains(sizes);
     }
-
-    private static DefaultExecutionGraph createDynamicGraph(JobVertex... vertices)
-            throws Exception {
-
-        TestingDefaultExecutionGraphBuilder builder =
-                TestingDefaultExecutionGraphBuilder.newBuilder()
-                        .setJobGraph(new JobGraph(new JobID(), "TestJob", vertices))
-                        .setVertexParallelismStore(
-                                AdaptiveBatchScheduler.computeVertexParallelismStoreForDynamicGraph(
-                                        Arrays.asList(vertices), 10));
-        return builder.buildDynamicGraph(EXECUTOR_RESOURCE.getExecutor());
-    }
 }
diff --git a/flink-tests/src/test/java/org/apache/flink/test/scheduling/AdaptiveBatchSchedulerITCase.java b/flink-tests/src/test/java/org/apache/flink/test/scheduling/AdaptiveBatchSchedulerITCase.java
index f7c42f9d50f..2dcc33585cd 100644
--- a/flink-tests/src/test/java/org/apache/flink/test/scheduling/AdaptiveBatchSchedulerITCase.java
+++ b/flink-tests/src/test/java/org/apache/flink/test/scheduling/AdaptiveBatchSchedulerITCase.java
@@ -78,6 +78,24 @@ class AdaptiveBatchSchedulerITCase {
         testScheduling(true);
     }
 
+    @Test
+    void testParallelismOfForwardGroupLargerThanGlobalMaxParallelism() throws Exception {
+        final Configuration configuration = createConfiguration();
+        final StreamExecutionEnvironment env =
+                StreamExecutionEnvironment.createLocalEnvironment(configuration);
+        env.setRuntimeMode(RuntimeExecutionMode.BATCH);
+        env.setParallelism(4);
+
+        final DataStream<Long> source =
+                env.fromSequence(0, NUMBERS_TO_PRODUCE - 1)
+                        .setParallelism(4)
+                        .name("source")
+                        .slotSharingGroup("group1");
+
+        source.forward().map(new NumberCounter()).name("map").slotSharingGroup("group2");
+        env.execute();
+    }
+
     private void testScheduling(Boolean isFineGrained) throws Exception {
         executeJob(isFineGrained);
 
@@ -99,18 +117,7 @@ class AdaptiveBatchSchedulerITCase {
     }
 
     private void executeJob(Boolean isFineGrained) throws Exception {
-        final Configuration configuration = new Configuration();
-        configuration.setString(RestOptions.BIND_PORT, "0");
-        configuration.setLong(JobManagerOptions.SLOT_REQUEST_TIMEOUT, 5000L);
-        configuration.setInteger(
-                BatchExecutionOptions.ADAPTIVE_AUTO_PARALLELISM_MAX_PARALLELISM,
-                DEFAULT_MAX_PARALLELISM);
-        configuration.set(
-                BatchExecutionOptions.ADAPTIVE_AUTO_PARALLELISM_AVG_DATA_VOLUME_PER_TASK,
-                MemorySize.parse("150kb"));
-        configuration.set(TaskManagerOptions.MEMORY_SEGMENT_SIZE, MemorySize.parse("4kb"));
-        configuration.set(TaskManagerOptions.NUM_TASK_SLOTS, 1);
-
+        final Configuration configuration = createConfiguration();
         if (isFineGrained) {
             configuration.set(ClusterOptions.ENABLE_FINE_GRAINED_RESOURCE_MANAGEMENT, true);
             configuration.set(ClusterOptions.FINE_GRAINED_SHUFFLE_MODE_ALL_BLOCKING, true);
@@ -157,6 +164,21 @@ class AdaptiveBatchSchedulerITCase {
         env.execute();
     }
 
+    private static Configuration createConfiguration() {
+        final Configuration configuration = new Configuration();
+        configuration.setString(RestOptions.BIND_PORT, "0");
+        configuration.setLong(JobManagerOptions.SLOT_REQUEST_TIMEOUT, 5000L);
+        configuration.setInteger(
+                BatchExecutionOptions.ADAPTIVE_AUTO_PARALLELISM_MAX_PARALLELISM, 2);
+        configuration.set(
+                BatchExecutionOptions.ADAPTIVE_AUTO_PARALLELISM_AVG_DATA_VOLUME_PER_TASK,
+                MemorySize.parse("150kb"));
+        configuration.set(TaskManagerOptions.MEMORY_SEGMENT_SIZE, MemorySize.parse("4kb"));
+        configuration.set(TaskManagerOptions.NUM_TASK_SLOTS, 1);
+
+        return configuration;
+    }
+
     private static class NumberCounter extends RichMapFunction<Long, Long> {
 
         private final Map<Long, Long> numberCountResult = new HashMap<>();
