diff --git a/docs/content.zh/docs/deployment/filesystems/overview.md b/docs/content.zh/docs/deployment/filesystems/overview.md
index 1636e8076f8..6d4c37e2ad3 100644
--- a/docs/content.zh/docs/deployment/filesystems/overview.md
+++ b/docs/content.zh/docs/deployment/filesystems/overview.md
@@ -47,7 +47,7 @@ Apache Flink 支持下列文件系统：
 
   - **MapR FS** 文件系统适配器已在 Flink 的主发行版中通过 *maprfs://* URI Scheme 支持。MapR 库需要在 classpath 中指定（例如在 `lib` 目录中）。
 
-  - **[阿里云对象存储]({{< ref "docs/deployment/filesystems/oss" >}})**由 `flink-oss-fs-hadoop` 支持，并通过 *oss://* URI scheme 使用。该实现基于 [Hadoop Project](https://hadoop.apache.org/)，但其是独立的，没有依赖项。
+  - **[阿里云对象存储]({{< ref "docs/deployment/filesystems/oss" >}})** 由 `flink-oss-fs-hadoop` 支持，并通过 *oss://* URI scheme 使用。该实现基于 [Hadoop Project](https://hadoop.apache.org/)，但其是独立的，没有依赖项。
 
   - **[Azure Blob Storage]({{< ref "docs/deployment/filesystems/azure" >}})** 由`flink-azure-fs-hadoop` 支持，并通过 *wasb(s)://* URI scheme 使用。该实现基于 [Hadoop Project](https://hadoop.apache.org/)，但其是独立的，没有依赖项。
 
diff --git a/docs/content.zh/docs/deployment/filesystems/s3.md b/docs/content.zh/docs/deployment/filesystems/s3.md
index 07c48bcbdbf..506836433b1 100644
--- a/docs/content.zh/docs/deployment/filesystems/s3.md
+++ b/docs/content.zh/docs/deployment/filesystems/s3.md
@@ -56,7 +56,9 @@ env.setStateBackend(new FsStateBackend("s3://<your-bucket>/<endpoint>"));
 
 ### Hadoop/Presto S3 文件系统插件
 
-{% panel **注意:** 如果您在使用 [Flink on EMR](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-flink.html)，您无需手动对此进行配置。 %}
+{{< hint info >}}
+如果您在使用 [Flink on EMR](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-flink.html)，您无需手动对此进行配置。
+{{< /hint >}}
 
 Flink 提供两种文件系统用来与 S3 交互：`flink-s3-fs-presto` 和 `flink-s3-fs-hadoop`。两种实现都是独立的且没有依赖项，因此使用时无需将 Hadoop 添加至 classpath。
 
@@ -125,7 +127,9 @@ s3.path.style.access: true
 **这仅在使用熵注入选项创建文件时启用！**
 否则将完全删除文件路径中的 entropy key。更多细节请参见 [FileSystem.create(Path, WriteOption)](https://ci.apache.org/projects/flink/flink-docs-release-1.6/api/java/org/apache/flink/core/fs/FileSystem.html#create-org.apache.flink.core.fs.Path-org.apache.flink.core.fs.FileSystem.WriteOptions-)。
 
-{% panel **注意:** 目前 Flink 运行时仅对 checkpoint 数据文件使用熵注入选项。所有其他文件包括 chekcpoint 元数据与外部 URI 都不使用熵注入，以保证 checkpoint URI 的可预测性。 %}
+{{< hint info >}}
+目前 Flink 运行时仅对 checkpoint 数据文件使用熵注入选项。所有其他文件包括 chekcpoint 元数据与外部 URI 都不使用熵注入，以保证 checkpoint URI 的可预测性。
+{{< /hint >}}
 
 配置 *entropy key* 与 *entropy length* 参数以启用熵注入：
 
diff --git a/docs/content.zh/docs/dev/datastream/fault-tolerance/serialization/schema_evolution.md b/docs/content.zh/docs/dev/datastream/fault-tolerance/serialization/schema_evolution.md
index 7a842ba64a1..3f3371c253b 100644
--- a/docs/content.zh/docs/dev/datastream/fault-tolerance/serialization/schema_evolution.md
+++ b/docs/content.zh/docs/dev/datastream/fault-tolerance/serialization/schema_evolution.md
@@ -95,11 +95,15 @@ Flink 完全支持 Avro 状态类型的升级，只要数据结构的修改是
 
 一个例外是如果新的 Avro 数据 schema 生成的类无法被重定位或者使用了不同的命名空间，在作业恢复时状态数据会被认为是不兼容的。
 
-{% warn Attention %} Schema evolution of keys is not supported.
+{{< hint warning >}}
+Schema evolution of keys is not supported.
+{{< /hint >}}
 
 Example: RocksDB state backend relies on binary objects identity, rather than `hashCode` method implementation. Any changes to the keys object structure could lead to non deterministic behaviour.
 
-{% warn Attention %} **Kryo** cannot be used for schema evolution.
+{{< hint warning >}}
+**Kryo** cannot be used for schema evolution.
+{{< /hint >}}
 
 When Kryo is used, there is no possibility for the framework to verify if any incompatible changes have been made.
 
diff --git a/docs/content.zh/docs/dev/datastream/operators/asyncio.md b/docs/content.zh/docs/dev/datastream/operators/asyncio.md
index d35ef3edc49..8e6b15c5a2d 100644
--- a/docs/content.zh/docs/dev/datastream/operators/asyncio.md
+++ b/docs/content.zh/docs/dev/datastream/operators/asyncio.md
@@ -42,7 +42,7 @@ under the License.
 
 {{< img src="/fig/async_io.svg" width="50%" >}}
 
-*注意：*仅仅提高 `MapFunction` 的并行度（parallelism）在有些情况下也可以提升吞吐量，但是这样做通常会导致非常高的资源消耗：更多的并行 `MapFunction` 实例意味着更多的 Task、更多的线程、更多的 Flink 内部网络连接、 更多的与数据库的网络连接、更多的缓冲和更多程序内部协调的开销。
+*注意：* 仅仅提高 `MapFunction` 的并行度（parallelism）在有些情况下也可以提升吞吐量，但是这样做通常会导致非常高的资源消耗：更多的并行 `MapFunction` 实例意味着更多的 Task、更多的线程、更多的 Flink 内部网络连接、 更多的与数据库的网络连接、更多的缓冲和更多程序内部协调的开销。
 
 
 ## 先决条件
diff --git a/docs/content.zh/docs/dev/datastream/sources.md b/docs/content.zh/docs/dev/datastream/sources.md
index 8358cf17037..8dfefa18634 100644
--- a/docs/content.zh/docs/dev/datastream/sources.md
+++ b/docs/content.zh/docs/dev/datastream/sources.md
@@ -117,13 +117,13 @@ Source 实现应该是可序列化的，因为 Source 实例会在运行时被
 <a name="SplitEnumerator"></a>
 
 ### SplitEnumerator
-SplitEnumerator 被认为是整个 Source 的”大脑“。SplitEnumerator 的典型实现如下：
+SplitEnumerator 被认为是整个 Source 的“大脑”。SplitEnumerator 的典型实现如下：
 
   - `SourceReader` 的注册处理
   - `SourceReader` 的失败处理
     - `SourceReader` 失败时会调用 `addSplitsBack()` 方法。SplitEnumerator应当收回已经被分配，但尚未被该 `SourceReader` 确认（acknowledged）的分片。
   - `SourceEvent` 的处理
-    - `SourceEvent`s 是 `SplitEnumerator` 和 `SourceReader` 之间来回传递的自定义事件。可以利用此机制来执行复杂的协调任务。
+    - `SourceEvents` 是 `SplitEnumerator` 和 `SourceReader` 之间来回传递的自定义事件。可以利用此机制来执行复杂的协调任务。
   - 分片的发现以及分配
     - `SplitEnumerator` 可以将分片分配到 `SourceReader` 从而响应各种事件，包括发现新的分片，新 `SourceReader` 的注册，`SourceReader` 的失败处理等
 
diff --git a/docs/content.zh/docs/dev/table/concepts/versioned_tables.md b/docs/content.zh/docs/dev/table/concepts/versioned_tables.md
index 77642ca30f8..3823b32898a 100644
--- a/docs/content.zh/docs/dev/table/concepts/versioned_tables.md
+++ b/docs/content.zh/docs/dev/table/concepts/versioned_tables.md
@@ -203,7 +203,7 @@ WHERE rowNum = 1;
 +(UPDATE_AFTER)  10:45:00      Euro       116
 +(UPDATE_AFTER)  11:15:00      Euro       119
 +(INSERT)        11:49:00      Pounds     108
-{% endhighlight sql %}
+```
 
 行 `(1)` 保留了事件时间作为视图 `versioned_rates` 的事件时间，行 `(2)` 使得视图 `versioned_rates` 有了主键, 因此视图 `versioned_rates` 是一个版本视图。
 
diff --git a/docs/content.zh/docs/dev/table/sql/queries/match_recognize.md b/docs/content.zh/docs/dev/table/sql/queries/match_recognize.md
index fb7ce391f6b..8b064b4bbcc 100644
--- a/docs/content.zh/docs/dev/table/sql/queries/match_recognize.md
+++ b/docs/content.zh/docs/dev/table/sql/queries/match_recognize.md
@@ -60,8 +60,9 @@ FROM MyTable
 
 本页将更详细地解释每个关键字，并演示说明更复杂的示例。
 
-{% info 注意 %}  Flink 的 `MATCH_RECOGNIZE` 子句实现是一个完整标准子集。仅支持以下部分中记录的功能。基于社区反馈，可能会支持其他功能，请查看[已知的局限](#known-limitations)。
-
+{{< hint info >}}
+Flink 的 `MATCH_RECOGNIZE` 子句实现是一个完整标准子集。仅支持以下部分中记录的功能。基于社区反馈，可能会支持其他功能，请查看[已知的局限](#known-limitations)。
+{{< /hint >}}
 
 
 <a name="introduction-and-examples"></a>
diff --git a/docs/content.zh/docs/flinkDev/building.md b/docs/content.zh/docs/flinkDev/building.md
index 61f6b2eba99..f7b78b1de96 100644
--- a/docs/content.zh/docs/flinkDev/building.md
+++ b/docs/content.zh/docs/flinkDev/building.md
@@ -138,7 +138,9 @@ mvn clean install
 
 ## Scala 版本
 
-{% info %} 只是用 Java 库和 API 的用户可以 *忽略* 这一部分。
+{{< hint info >}}
+只是用 Java 库和 API 的用户可以*忽略*这一部分。
+{{< /hint >}}
 
 Flink 有使用 [Scala](http://scala-lang.org) 来写的 API，库和运行时模块。使用 Scala API 和库的同学必须配置 Flink 的 Scala 版本和自己的 Flink 版本（因为 Scala 
 并不严格的向后兼容）。
diff --git a/docs/content.zh/docs/libs/cep.md b/docs/content.zh/docs/libs/cep.md
index 72cdbbcee3e..08ccb604753 100644
--- a/docs/content.zh/docs/libs/cep.md
+++ b/docs/content.zh/docs/libs/cep.md
@@ -50,12 +50,16 @@ FlinkCEP是在Flink上层实现的复杂事件处理库。
 {{< /tab >}}
 {{< /tabs >}}
 
-{% info 提示 %} FlinkCEP不是二进制发布包的一部分。在集群上执行如何链接它可以看[这里]({{< ref "docs/dev/datastream/project-configuration" >}})。
+{{< hint info >}}
+FlinkCEP不是二进制发布包的一部分。在集群上执行如何链接它可以看[这里]({{< ref "docs/dev/datastream/project-configuration" >}})。
+{{< /hint >}}
 
 现在可以开始使用Pattern API写你的第一个CEP程序了。
 
-{% warn 注意 %} `DataStream`中的事件，如果你想在上面进行模式匹配的话，必须实现合适的 `equals()`和`hashCode()`方法，
+{{< hint warning >}} 
+`DataStream`中的事件，如果你想在上面进行模式匹配的话，必须实现合适的 `equals()`和`hashCode()`方法，
 因为FlinkCEP使用它们来比较和匹配事件。
+{{< /hint >}}
 
 {{< tabs "4fef83d9-e4c5-4073-9607-4c8cde1ebf1e" >}}
 {{< tab "Java" >}}
@@ -131,9 +135,13 @@ val result: DataStream[Alert] = patternStream.process(
 这些模式基于用户指定的**条件**从一个转换到另外一个，比如 `event.getName().equals("end")`。
 一个**匹配**是输入事件的一个序列，这些事件通过一系列有效的模式转换，能够访问到复杂模式图中的所有模式。
 
-{% warn 注意 %} 每个模式必须有一个独一无二的名字，你可以在后面使用它来识别匹配到的事件。
+{{< hint warning >}}
+每个模式必须有一个独一无二的名字，你可以在后面使用它来识别匹配到的事件。
+{{< /hint >}}
 
-{% warn 注意 %} 模式的名字不能包含字符`":"`.
+{{< hint danger >}}
+模式的名字不能包含字符`":"`.
+{{< /hint >}}
 
 这一节的剩余部分我们会先讲述如何定义[单个模式](#单个模式)，然后讲如何将单个模式组合成[复杂模式](#组合模式)。
 
@@ -294,8 +302,10 @@ middle.oneOrMore()
 {{< /tab >}}
 {{< /tabs >}}
 
-{% warn 注意 %} 调用`ctx.getEventsForPattern(...)`可以获得所有前面已经接受作为可能匹配的事件。
+{{< hint info >}}
+调用`ctx.getEventsForPattern(...)`可以获得所有前面已经接受作为可能匹配的事件。
 调用这个操作的代价可能很小也可能很大，所以在实现你的条件时，尽量少使用它。
+{{< /hint >}}
 
 描述的上下文提供了获取事件时间属性的方法。更多细节可以看[时间上下文](#时间上下文)。
 
@@ -669,9 +679,13 @@ val start : Pattern[Event, _] = Pattern.begin("start")
 1. `notNext()`，如果不想后面直接连着一个特定事件
 2. `notFollowedBy()`，如果不想一个特定事件发生在两个事件之间的任何地方。
 
-{% warn 注意 %} 模式序列不能以`notFollowedBy()`结尾。
+{{< hint warning >}}
+模式序列不能以`notFollowedBy()`结尾。
+{{< /hint >}}
 
-{% warn 注意 %} 一个`NOT`模式前面不能是可选的模式。
+{{< hint warning >}}
+一个 **NOT**　模式前面不能是可选的模式。
+{{< /hint >}}
 
 {{< tabs "c5c3a1fe-8ab4-45ae-8a97-c2c2e96b6bb3" >}}
 {{< tab "Java" >}}
@@ -729,7 +743,9 @@ val relaxedNot: Pattern[Event, _] = start.notFollowedBy("not").where(...)
 例如，你可以通过`pattern.within()`方法指定一个模式应该在10秒内发生。
 这种时间模式支持[处理时间和事件时间]({{< ref "docs/concepts/time" >}}).
 
-{% warn 注意 %} 一个模式序列只能有一个时间限制。如果限制了多个时间在不同的单个模式上，会使用最小的那个时间限制。
+{{< hint info >}}
+一个模式序列只能有一个时间限制。如果限制了多个时间在不同的单个模式上，会使用最小的那个时间限制。
+{{< /hint >}}
 
 {{< tabs "8228f5b0-b6b3-4ca6-a56b-e5a8fd5fdc3b" >}}
 {{< tab "Java" >}}
@@ -1374,9 +1390,11 @@ Pattern.begin("patternName", skipStrategy)
 {{< /tab >}}
 {{< /tabs >}}
 
-{% warn 注意 %} 使用SKIP_TO_FIRST/LAST时，有两个选项可以用来处理没有事件可以映射到对应的变量名上的情况。
+{{< hint info >}}
+使用SKIP_TO_FIRST/LAST时，有两个选项可以用来处理没有事件可以映射到对应的变量名上的情况。
 默认情况下会使用NO_SKIP策略，另外一个选项是抛出异常。
 可以使用如下的选项：
+{{< /hint >}}
 
 {{< tabs "48a6f23b-1861-4350-894d-0404d070cfb2" >}}
 {{< tab "Java" >}}
@@ -1420,7 +1438,9 @@ val patternStream: PatternStream[Event] = CEP.pattern(input, pattern, comparator
 
 输入流根据你的使用场景可以是*keyed*或者*non-keyed*。
 
-{% warn 注意 %} 在*non-keyed*流上使用模式将会使你的作业并发度被设为1。
+{{< hint info >}}
+在 *non-keyed* 流上使用模式将会使你的作业并发度被设为1。
+{{< /hint >}}
 
 ### 从模式中选取
 
@@ -1536,7 +1556,9 @@ val timeoutResult: DataStream[TimeoutEvent] = result.getSideOutput(outputTag)
 在`CEP`中，事件的处理顺序很重要。在使用事件时间时，为了保证事件按照正确的顺序被处理，一个事件到来后会先被放到一个缓冲区中，
 在缓冲区里事件都按照时间戳从小到大排序，当水位线到达后，缓冲区中所有小于水位线的事件被处理。这意味着水位线之间的数据都按照时间戳被顺序处理。
 
-{% warn 注意 %} 这个库假定按照事件时间时水位线一定是正确的。
+{{< hint info >}}
+这个库假定按照事件时间时水位线一定是正确的。
+{{< /hint >}}
 
 为了保证跨水位线的事件按照事件时间处理，Flink CEP库假定*水位线一定是正确的*，并且把时间戳小于最新水位线的事件看作是*晚到*的。
 晚到的事件不会被处理。你也可以指定一个侧输出标志来收集比最新水位线晚到的事件，你可以这样做：
diff --git a/docs/content.zh/docs/try-flink/datastream.md b/docs/content.zh/docs/try-flink/datastream.md
index 2e7b3cb8006..dd270fe2806 100644
--- a/docs/content.zh/docs/try-flink/datastream.md
+++ b/docs/content.zh/docs/try-flink/datastream.md
@@ -70,8 +70,9 @@ Flink 支持对状态和时间的细粒度控制，以此来实现复杂的事
 一个准备好的 Flink Maven Archetype 能够快速创建一个包含了必要依赖的 Flink 程序骨架，基于此，你可以把精力集中在编写业务逻辑上即可。
 这些已包含的依赖包括 `flink-streaming-java`、`flink-walkthrough-common` 等，他们分别是 Flink 应用程序的核心依赖项和这个代码练习需要的数据生成器，当然还包括其他本代码练习所依赖的类。
 
-{% panel **说明:** 为简洁起见，本练习中的代码块中可能不包含完整的类路径。完整的类路径可以在文档底部 [链接](#final-application) 中找到。 %}
-
+{{< hint info >}}
+**说明：** 为简洁起见，本练习中的代码块中可能不包含完整的类路径。完整的类路径可以在文档底部 [链接](#final-application) 中找到。
+{{< /hint >}}
 
 {{< tabs "archetype" >}}
 {{< tab "Java" >}}
@@ -104,7 +105,7 @@ $ mvn archetype:generate \
 
 {{< unstable >}}
 {{< hint warning >}}
-    **注意** Maven 3.0 及更高版本，不再支持通过命令行指定仓库（-DarchetypeCatalog）。有关这个改动的详细信息，
+    Maven 3.0 及更高版本，不再支持通过命令行指定仓库（-DarchetypeCatalog）。有关这个改动的详细信息，
     请参阅 [Maven 官方文档](http://maven.apache.org/archetype/maven-archetype-plugin/archetype-repository.html)
     如果你希望使用快照仓库，则需要在 settings.xml 文件中添加一个仓库条目。例如：
 ```xml
