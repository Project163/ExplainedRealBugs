diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSourceFileEnumerator.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSourceFileEnumerator.java
index 047ec8867df..54a21893603 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSourceFileEnumerator.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSourceFileEnumerator.java
@@ -28,8 +28,10 @@ import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
 import org.apache.hadoop.mapred.FileSplit;
 import org.apache.hadoop.mapred.InputFormat;
+import org.apache.hadoop.mapred.InputSplit;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hadoop.util.StringUtils;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -65,32 +67,8 @@ public class HiveSourceFileEnumerator implements FileEnumerator {
             throws IOException {
         List<HiveSourceSplit> hiveSplits = new ArrayList<>();
         for (HiveTablePartition partition : partitions) {
-            StorageDescriptor sd = partition.getStorageDescriptor();
-            org.apache.hadoop.fs.Path inputPath = new org.apache.hadoop.fs.Path(sd.getLocation());
-            FileSystem fs = inputPath.getFileSystem(jobConf);
-            // it's possible a partition exists in metastore but the data has been removed
-            if (!fs.exists(inputPath)) {
-                continue;
-            }
-            InputFormat format;
-            try {
-                format =
-                        (InputFormat)
-                                Class.forName(
-                                                sd.getInputFormat(),
-                                                true,
-                                                Thread.currentThread().getContextClassLoader())
-                                        .newInstance();
-            } catch (Exception e) {
-                throw new FlinkHiveException("Unable to instantiate the hadoop input format", e);
-            }
-            ReflectionUtils.setConf(format, jobConf);
-            jobConf.set(INPUT_DIR, sd.getLocation());
-            // TODO: we should consider how to calculate the splits according to minNumSplits in the
-            // future.
-            org.apache.hadoop.mapred.InputSplit[] splitArray =
-                    format.getSplits(jobConf, minNumSplits);
-            for (org.apache.hadoop.mapred.InputSplit inputSplit : splitArray) {
+            for (InputSplit inputSplit :
+                    createMRSplits(minNumSplits, partition.getStorageDescriptor(), jobConf)) {
                 Preconditions.checkState(
                         inputSplit instanceof FileSplit,
                         "Unsupported InputSplit type: " + inputSplit.getClass().getName());
@@ -101,6 +79,34 @@ public class HiveSourceFileEnumerator implements FileEnumerator {
         return hiveSplits;
     }
 
+    public static InputSplit[] createMRSplits(
+            int minNumSplits, StorageDescriptor sd, JobConf jobConf) throws IOException {
+        org.apache.hadoop.fs.Path inputPath = new org.apache.hadoop.fs.Path(sd.getLocation());
+        FileSystem fs = inputPath.getFileSystem(jobConf);
+        // it's possible a partition exists in metastore but the data has been removed
+        if (!fs.exists(inputPath)) {
+            return new InputSplit[0];
+        }
+        InputFormat format;
+        try {
+            format =
+                    (InputFormat)
+                            Class.forName(
+                                            sd.getInputFormat(),
+                                            true,
+                                            Thread.currentThread().getContextClassLoader())
+                                    .newInstance();
+        } catch (Exception e) {
+            throw new FlinkHiveException("Unable to instantiate the hadoop input format", e);
+        }
+        ReflectionUtils.setConf(format, jobConf);
+        // need to escape comma in the location path
+        jobConf.set(INPUT_DIR, StringUtils.escapeString(sd.getLocation()));
+        // TODO: we should consider how to calculate the splits according to minNumSplits in the
+        // future.
+        return format.getSplits(jobConf, minNumSplits);
+    }
+
     public static int getNumFiles(List<HiveTablePartition> partitions, JobConf jobConf)
             throws IOException {
         int numFiles = 0;
diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableInputFormat.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableInputFormat.java
index 08699662f7f..07354406067 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableInputFormat.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableInputFormat.java
@@ -23,7 +23,7 @@ import org.apache.flink.api.common.io.CheckpointableInputFormat;
 import org.apache.flink.api.common.io.LocatableInputSplitAssigner;
 import org.apache.flink.api.common.io.statistics.BaseStatistics;
 import org.apache.flink.api.java.hadoop.common.HadoopInputFormatCommonBase;
-import org.apache.flink.connectors.hive.FlinkHiveException;
+import org.apache.flink.connectors.hive.HiveSourceFileEnumerator;
 import org.apache.flink.connectors.hive.HiveTablePartition;
 import org.apache.flink.connectors.hive.JobConfWrapper;
 import org.apache.flink.core.io.InputSplitAssigner;
@@ -39,9 +39,8 @@ import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
 import org.apache.hadoop.hive.ql.io.IOConstants;
 import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;
-import org.apache.hadoop.mapred.InputFormat;
+import org.apache.hadoop.mapred.InputSplit;
 import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.util.ReflectionUtils;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -53,7 +52,6 @@ import java.util.stream.Collectors;
 import java.util.stream.IntStream;
 
 import static org.apache.flink.util.Preconditions.checkNotNull;
-import static org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR;
 
 /**
  * The HiveTableInputFormat are inspired by the HCatInputFormat and HadoopInputFormatBase. It's used
@@ -324,32 +322,9 @@ public class HiveTableInputFormat extends HadoopInputFormatCommonBase<RowData, H
         List<HiveTableInputSplit> hiveSplits = new ArrayList<>();
         int splitNum = 0;
         for (HiveTablePartition partition : partitions) {
-            StorageDescriptor sd = partition.getStorageDescriptor();
-            Path inputPath = new Path(sd.getLocation());
-            FileSystem fs = inputPath.getFileSystem(jobConf);
-            // it's possible a partition exists in metastore but the data has been removed
-            if (!fs.exists(inputPath)) {
-                continue;
-            }
-            InputFormat format;
-            try {
-                format =
-                        (InputFormat)
-                                Class.forName(
-                                                sd.getInputFormat(),
-                                                true,
-                                                Thread.currentThread().getContextClassLoader())
-                                        .newInstance();
-            } catch (Exception e) {
-                throw new FlinkHiveException("Unable to instantiate the hadoop input format", e);
-            }
-            ReflectionUtils.setConf(format, jobConf);
-            jobConf.set(INPUT_DIR, sd.getLocation());
-            // TODO: we should consider how to calculate the splits according to minNumSplits in the
-            // future.
-            org.apache.hadoop.mapred.InputSplit[] splitArray =
-                    format.getSplits(jobConf, minNumSplits);
-            for (org.apache.hadoop.mapred.InputSplit inputSplit : splitArray) {
+            for (InputSplit inputSplit :
+                    HiveSourceFileEnumerator.createMRSplits(
+                            minNumSplits, partition.getStorageDescriptor(), jobConf)) {
                 hiveSplits.add(new HiveTableInputSplit(splitNum++, inputSplit, jobConf, partition));
             }
         }
diff --git a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/TableEnvHiveConnectorITCase.java b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/TableEnvHiveConnectorITCase.java
index 860bc42d1db..13e319a0c49 100644
--- a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/TableEnvHiveConnectorITCase.java
+++ b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/TableEnvHiveConnectorITCase.java
@@ -33,6 +33,7 @@ import org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper;
 import org.apache.flink.table.catalog.hive.client.HiveShimLoader;
 import org.apache.flink.types.Row;
 import org.apache.flink.util.CollectionUtil;
+import org.apache.flink.util.IOUtils;
 
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
@@ -40,8 +41,11 @@ import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.api.Table;
 import org.junit.Assume;
 import org.junit.BeforeClass;
+import org.junit.ClassRule;
 import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
 
+import java.io.File;
 import java.sql.Timestamp;
 import java.util.Arrays;
 import java.util.HashMap;
@@ -67,6 +71,8 @@ public class TableEnvHiveConnectorITCase {
                         hiveCatalog.getHiveConf(), HiveShimLoader.getHiveVersion());
     }
 
+    @ClassRule public static TemporaryFolder tempFolder = new TemporaryFolder();
+
     @Test
     public void testDefaultPartitionName() throws Exception {
         TableEnvironment tableEnv = getTableEnvWithHiveCatalog();
@@ -442,6 +448,47 @@ public class TableEnvHiveConnectorITCase {
         }
     }
 
+    @Test
+    public void testLocationWithComma() throws Exception {
+        TableEnvironment tableEnv = getTableEnvWithHiveCatalog();
+        File location = tempFolder.newFolder(",tbl1,location,");
+        try {
+            // test table location
+            tableEnv.executeSql(
+                    String.format(
+                            "create table tbl1 (x int) location '%s'", location.getAbsolutePath()));
+            tableEnv.executeSql("insert into tbl1 values (1),(2)").await();
+            List<Row> results =
+                    CollectionUtil.iteratorToList(
+                            tableEnv.executeSql("select * from tbl1").collect());
+            assertEquals("[+I[1], +I[2]]", results.toString());
+            // test partition location
+            tableEnv.executeSql("create table tbl2 (x int) partitioned by (p string)");
+            location = tempFolder.newFolder(",");
+            tableEnv.executeSql(
+                    String.format(
+                            "alter table tbl2 add partition (p='a') location '%s'",
+                            location.getAbsolutePath()));
+            tableEnv.executeSql("insert into tbl2 partition (p='a') values (1),(2)").await();
+            results =
+                    CollectionUtil.iteratorToList(
+                            tableEnv.executeSql("select * from tbl2").collect());
+            assertEquals("[+I[1, a], +I[2, a]]", results.toString());
+
+            tableEnv.executeSql("insert into tbl2 partition (p) values (3,'b ,')").await();
+            results =
+                    CollectionUtil.iteratorToList(
+                            tableEnv.executeSql("select * from tbl2 where p='b ,'").collect());
+            assertEquals("[+I[3, b ,]]", results.toString());
+        } finally {
+            if (location != null) {
+                IOUtils.deleteFileQuietly(location.toPath());
+            }
+            tableEnv.executeSql("drop table if exists tbl1");
+            tableEnv.executeSql("drop table if exists tbl2");
+        }
+    }
+
     private TableEnvironment getTableEnvWithHiveCatalog() {
         TableEnvironment tableEnv =
                 HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode(SqlDialect.HIVE);
