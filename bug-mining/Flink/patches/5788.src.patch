diff --git a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/FlinkKafkaInternalProducer.java b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/FlinkKafkaInternalProducer.java
index 992bf04c53e..d3b6667fe3d 100644
--- a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/FlinkKafkaInternalProducer.java
+++ b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/FlinkKafkaInternalProducer.java
@@ -33,6 +33,7 @@ import java.lang.reflect.Constructor;
 import java.lang.reflect.Field;
 import java.lang.reflect.InvocationTargetException;
 import java.lang.reflect.Method;
+import java.time.Duration;
 import java.util.Properties;
 
 /**
@@ -86,6 +87,11 @@ class FlinkKafkaInternalProducer<K, V> extends KafkaProducer<K, V> {
         return inTransaction;
     }
 
+    @Override
+    public void close() {
+        super.close(Duration.ZERO);
+    }
+
     public Properties getKafkaProducerConfig() {
         return kafkaProducerConfig;
     }
diff --git a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaCommittable.java b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaCommittable.java
index 0d5bdb2f3eb..e8825fdaf49 100644
--- a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaCommittable.java
+++ b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaCommittable.java
@@ -19,7 +19,10 @@ package org.apache.flink.connector.kafka.sink;
 
 import org.apache.kafka.clients.producer.ProducerConfig;
 
+import javax.annotation.Nullable;
+
 import java.util.Objects;
+import java.util.Optional;
 
 /**
  * This class holds the necessary information to construct a new {@link FlinkKafkaInternalProducer}
@@ -30,11 +33,17 @@ class KafkaCommittable {
     private final long producerId;
     private final short epoch;
     private final String transactionalId;
+    @Nullable private FlinkKafkaInternalProducer<?, ?> producer;
 
-    public KafkaCommittable(long producerId, short epoch, String transactionalId) {
+    public KafkaCommittable(
+            long producerId,
+            short epoch,
+            String transactionalId,
+            @Nullable FlinkKafkaInternalProducer<?, ?> producer) {
         this.producerId = producerId;
         this.epoch = epoch;
         this.transactionalId = transactionalId;
+        this.producer = producer;
     }
 
     public static KafkaCommittable of(FlinkKafkaInternalProducer<byte[], byte[]> producer) {
@@ -42,7 +51,8 @@ class KafkaCommittable {
                 producer.getProducerId(),
                 producer.getEpoch(),
                 producer.getKafkaProducerConfig()
-                        .getProperty(ProducerConfig.TRANSACTIONAL_ID_CONFIG));
+                        .getProperty(ProducerConfig.TRANSACTIONAL_ID_CONFIG),
+                producer);
     }
 
     public long getProducerId() {
@@ -57,6 +67,10 @@ class KafkaCommittable {
         return transactionalId;
     }
 
+    public Optional<FlinkKafkaInternalProducer<?, ?>> getProducer() {
+        return Optional.ofNullable(producer);
+    }
+
     @Override
     public String toString() {
         return "KafkaCommittable{"
diff --git a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaCommittableSerializer.java b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaCommittableSerializer.java
index 742699144f0..78f1472c648 100644
--- a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaCommittableSerializer.java
+++ b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaCommittableSerializer.java
@@ -51,7 +51,7 @@ class KafkaCommittableSerializer implements SimpleVersionedSerializer<KafkaCommi
             final short epoch = in.readShort();
             final long producerId = in.readLong();
             final String transactionalId = in.readUTF();
-            return new KafkaCommittable(producerId, epoch, transactionalId);
+            return new KafkaCommittable(producerId, epoch, transactionalId, null);
         }
     }
 }
diff --git a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaCommitter.java b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaCommitter.java
index 42636561df6..705ae2f5039 100644
--- a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaCommitter.java
+++ b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaCommitter.java
@@ -58,8 +58,7 @@ class KafkaCommitter implements Committer<KafkaCommittable> {
         final String transactionalId = committable.getTransactionalId();
         LOG.debug("Committing Kafka transaction {}", transactionalId);
         try (FlinkKafkaInternalProducer<?, ?> producer =
-                new FlinkKafkaInternalProducer<>(createKafkaProducerConfig(transactionalId))) {
-            producer.resumeTransaction(committable.getProducerId(), committable.getEpoch());
+                committable.getProducer().orElseGet(() -> createProducer(committable))) {
             producer.commitTransaction();
         } catch (InvalidTxnStateException | ProducerFencedException e) {
             // That means we have committed this transaction before.
@@ -71,6 +70,18 @@ class KafkaCommitter implements Committer<KafkaCommittable> {
         }
     }
 
+    /**
+     * Creates a producer that can commit into the same transaction as the upstream producer that
+     * was serialized into {@link KafkaCommittable}.
+     */
+    private FlinkKafkaInternalProducer<?, ?> createProducer(KafkaCommittable committable) {
+        FlinkKafkaInternalProducer<?, ?> producer =
+                new FlinkKafkaInternalProducer<>(
+                        createKafkaProducerConfig(committable.getTransactionalId()));
+        producer.resumeTransaction(committable.getProducerId(), committable.getEpoch());
+        return producer;
+    }
+
     private Properties createKafkaProducerConfig(String transactionalId) {
         final Properties copy = new Properties();
         copy.putAll(kafkaProducerConfig);
diff --git a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaWriter.java b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaWriter.java
index f1aa69ca6d6..e221b6f3228 100644
--- a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaWriter.java
+++ b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaWriter.java
@@ -29,6 +29,7 @@ import org.apache.flink.streaming.connectors.kafka.internals.metrics.KafkaMetric
 import org.apache.flink.util.FlinkRuntimeException;
 
 import org.apache.flink.shaded.guava30.com.google.common.collect.ImmutableList;
+import org.apache.flink.shaded.guava30.com.google.common.io.Closer;
 
 import org.apache.kafka.clients.producer.Callback;
 import org.apache.kafka.clients.producer.Producer;
@@ -80,17 +81,17 @@ class KafkaWriter<IN> implements SinkWriter<IN, KafkaCommittable, KafkaWriterSta
     private final Callback deliveryCallback;
     private final AtomicLong pendingRecords = new AtomicLong();
     private final KafkaRecordSerializationSchema.KafkaSinkContext kafkaSinkContext;
-    private final List<FlinkKafkaInternalProducer<byte[], byte[]>> producers = new ArrayList<>();
     private final Map<String, KafkaMetricMutableWrapper> previouslyCreatedMetrics = new HashMap<>();
     private final SinkWriterMetricGroup metricGroup;
     private final Counter numBytesOutCounter;
     private final Sink.ProcessingTimeService timeService;
     private final boolean disabledMetrics;
 
-    private transient Metric byteOutMetric;
-    private transient FlinkKafkaInternalProducer<byte[], byte[]> currentProducer;
-    private transient KafkaWriterState kafkaWriterState;
-    @Nullable private transient volatile Exception producerAsyncException;
+    private Metric byteOutMetric;
+    private FlinkKafkaInternalProducer<byte[], byte[]> currentProducer;
+    private KafkaWriterState kafkaWriterState;
+    private final Closer closer = Closer.create();
+    @Nullable private volatile Exception producerAsyncException;
 
     private boolean closed = false;
     private long lastSync = System.currentTimeMillis();
@@ -152,8 +153,7 @@ class KafkaWriter<IN> implements SinkWriter<IN, KafkaCommittable, KafkaWriterSta
         }
         this.kafkaWriterState =
                 recoverAndInitializeState(checkNotNull(recoveredStates, "recoveredStates"));
-        this.currentProducer = beginTransaction();
-        producers.add(currentProducer);
+        this.currentProducer = createProducer();
         registerMetricSync();
     }
 
@@ -167,13 +167,10 @@ class KafkaWriter<IN> implements SinkWriter<IN, KafkaCommittable, KafkaWriterSta
     }
 
     @Override
-    public List<KafkaCommittable> prepareCommit(boolean flush) throws IOException {
+    public List<KafkaCommittable> prepareCommit(boolean flush) {
         flushRecords(flush);
-        if (!flush) {
-            currentProducer = beginTransaction();
-        }
-        final List<KafkaCommittable> committables = commit();
-        producers.add(currentProducer);
+        List<KafkaCommittable> committables = precommit();
+        currentProducer = createProducer();
         return committables;
     }
 
@@ -189,7 +186,7 @@ class KafkaWriter<IN> implements SinkWriter<IN, KafkaCommittable, KafkaWriterSta
         }
         currentProducer.flush();
         closed = true;
-        currentProducer.close(Duration.ZERO);
+        closer.close();
     }
 
     private KafkaWriterState recoverAndInitializeState(List<KafkaWriterState> recoveredStates) {
@@ -263,27 +260,26 @@ class KafkaWriter<IN> implements SinkWriter<IN, KafkaCommittable, KafkaWriterSta
         }
     }
 
-    private FlinkKafkaInternalProducer<byte[], byte[]> beginTransaction() {
+    private FlinkKafkaInternalProducer<byte[], byte[]> createProducer() {
         switch (deliveryGuarantee) {
             case EXACTLY_ONCE:
-                if (currentProducer != null) {
-                    currentProducer.close(Duration.ZERO);
-                }
                 final FlinkKafkaInternalProducer<byte[], byte[]> transactionalProducer =
                         createTransactionalProducer();
                 initMetrics(transactionalProducer);
                 transactionalProducer.beginTransaction();
+                closer.register(transactionalProducer);
                 return transactionalProducer;
             case AT_LEAST_ONCE:
             case NONE:
-                if (currentProducer == null) {
-                    final FlinkKafkaInternalProducer<byte[], byte[]> producer =
-                            new FlinkKafkaInternalProducer<>(kafkaProducerConfig);
-                    initMetrics(producer);
-                    return producer;
+                if (currentProducer != null) {
+                    LOG.debug("Reusing existing KafkaProducer");
+                    return currentProducer;
                 }
-                LOG.debug("Reusing existing KafkaProducer");
-                return currentProducer;
+                final FlinkKafkaInternalProducer<byte[], byte[]> producer =
+                        new FlinkKafkaInternalProducer<>(kafkaProducerConfig);
+                initMetrics(producer);
+                closer.register(producer);
+                return producer;
             default:
                 throw new UnsupportedOperationException(
                         "Unsupported Kafka writer semantic " + deliveryGuarantee);
@@ -315,17 +311,15 @@ class KafkaWriter<IN> implements SinkWriter<IN, KafkaCommittable, KafkaWriterSta
         checkErroneous();
     }
 
-    private List<KafkaCommittable> commit() {
+    private List<KafkaCommittable> precommit() {
         final List<KafkaCommittable> committables;
         switch (deliveryGuarantee) {
             case EXACTLY_ONCE:
-                committables =
-                        producers.stream().map(KafkaCommittable::of).collect(Collectors.toList());
-                producers.clear();
+                committables = Collections.singletonList(KafkaCommittable.of(currentProducer));
                 break;
             case AT_LEAST_ONCE:
             case NONE:
-                committables = new ArrayList<>();
+                committables = Collections.emptyList();
                 break;
             default:
                 throw new UnsupportedOperationException(
diff --git a/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/sink/KafkaCommittableSerializerTest.java b/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/sink/KafkaCommittableSerializerTest.java
index 012af075af0..bcf06e91b21 100644
--- a/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/sink/KafkaCommittableSerializerTest.java
+++ b/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/sink/KafkaCommittableSerializerTest.java
@@ -37,7 +37,7 @@ public class KafkaCommittableSerializerTest extends TestLogger {
     public void testCommittableSerDe() throws IOException {
         final String transactionalId = "test-id";
         final short epoch = 5;
-        final KafkaCommittable committable = new KafkaCommittable(1L, epoch, transactionalId);
+        final KafkaCommittable committable = new KafkaCommittable(1L, epoch, transactionalId, null);
         final byte[] serialized = SERIALIZER.serialize(committable);
         assertEquals(committable, SERIALIZER.deserialize(1, serialized));
     }
