diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/functions/hive/HiveGenericUDAF.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/functions/hive/HiveGenericUDAF.java
index 18b1d73e256..84fc7db5735 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/functions/hive/HiveGenericUDAF.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/functions/hive/HiveGenericUDAF.java
@@ -28,8 +28,8 @@ import org.apache.flink.table.functions.FunctionContext;
 import org.apache.flink.table.functions.hive.conversion.HiveInspectors;
 import org.apache.flink.table.functions.hive.conversion.HiveObjectConversion;
 import org.apache.flink.table.functions.hive.conversion.IdentityConversion;
+import org.apache.flink.table.runtime.types.TypeInfoDataTypeConverter;
 import org.apache.flink.table.types.DataType;
-import org.apache.flink.table.types.utils.LegacyTypeInfoDataTypeConverter;
 
 import org.apache.hadoop.hive.ql.exec.UDAF;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
@@ -205,7 +205,7 @@ public class HiveGenericUDAF
 
 	@Override
 	public TypeInformation getResultType() {
-		return LegacyTypeInfoDataTypeConverter.toLegacyTypeInfo(
+		return TypeInfoDataTypeConverter.fromDataTypeToTypeInfo(
 			getHiveResultType(this.constantArguments, this.argTypes));
 	}
 
diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/functions/hive/HiveScalarFunction.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/functions/hive/HiveScalarFunction.java
index 3d92c7d958d..2783bd0f23f 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/functions/hive/HiveScalarFunction.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/functions/hive/HiveScalarFunction.java
@@ -23,8 +23,8 @@ import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.table.functions.FunctionContext;
 import org.apache.flink.table.functions.ScalarFunction;
 import org.apache.flink.table.functions.hive.util.HiveFunctionUtil;
+import org.apache.flink.table.runtime.types.TypeInfoDataTypeConverter;
 import org.apache.flink.table.types.DataType;
-import org.apache.flink.table.types.utils.LegacyTypeInfoDataTypeConverter;
 
 import org.apache.hadoop.hive.ql.exec.UDF;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
@@ -71,7 +71,7 @@ public abstract class HiveScalarFunction<UDFType> extends ScalarFunction impleme
 
 	@Override
 	public TypeInformation getResultType(Class[] signature) {
-		return LegacyTypeInfoDataTypeConverter.toLegacyTypeInfo(
+		return TypeInfoDataTypeConverter.fromDataTypeToTypeInfo(
 			getHiveResultType(this.constantArguments, this.argTypes));
 	}
 
diff --git a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/module/hive/HiveModuleTest.java b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/module/hive/HiveModuleTest.java
index 19d2f661ecd..4ff2c3a91fd 100644
--- a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/module/hive/HiveModuleTest.java
+++ b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/module/hive/HiveModuleTest.java
@@ -118,4 +118,16 @@ public class HiveModuleTest {
 		results = TableUtils.collectToList(tEnv.sqlQuery("select concat('ab',cast(null as int))"));
 		assertEquals("[null]", results.toString());
 	}
+
+	@Test
+	public void testDecimalReturnType() throws Exception {
+		TableEnvironment tEnv = HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode();
+
+		tEnv.unloadModule("core");
+		tEnv.loadModule("hive", new HiveModule(HiveShimLoader.getHiveVersion()));
+
+		List<Row> results = TableUtils.collectToList(tEnv.sqlQuery("select negative(5.1)"));
+
+		assertEquals("[-5.1]", results.toString());
+	}
 }
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/calls/ScalarFunctionCallGen.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/calls/ScalarFunctionCallGen.scala
index 4b4d0c92ae5..6fd62dff286 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/calls/ScalarFunctionCallGen.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/calls/ScalarFunctionCallGen.scala
@@ -28,6 +28,7 @@ import org.apache.flink.table.planner.codegen.calls.ScalarFunctionCallGen.prepar
 import org.apache.flink.table.planner.codegen.{CodeGeneratorContext, GenerateUtils, GeneratedExpression}
 import org.apache.flink.table.planner.functions.utils.UserDefinedFunctionUtils
 import org.apache.flink.table.planner.functions.utils.UserDefinedFunctionUtils._
+import org.apache.flink.table.runtime.types.LogicalTypeDataTypeConverter
 import org.apache.flink.table.runtime.types.LogicalTypeDataTypeConverter.fromLogicalTypeToDataType
 import org.apache.flink.table.types.extraction.utils.ExtractionUtils
 import org.apache.flink.table.types.logical.LogicalType
@@ -79,7 +80,8 @@ class ScalarFunctionCallGen(scalarFunction: ScalarFunction) extends CallGenerato
         val boxedResultClass = ExtractionUtils.boxPrimitive(resultClass).asInstanceOf[Class[_]]
         val javaTypeTerm = boxedResultClass.getCanonicalName
         val resultExternalTypeWithResultClass =
-          if (resultExternalType.getLogicalType.supportsOutputConversion(boxedResultClass)) {
+          if (LogicalTypeDataTypeConverter.fromDataTypeToLogicalType(resultExternalType)
+            .supportsOutputConversion(boxedResultClass)) {
             // resultClass of HiveScalarFunction is Object, which cannot be a valid
             // conversion class
             resultExternalType.bridgedTo(boxedResultClass)
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/CalcITCase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/CalcITCase.scala
index d6aaaf0fbfd..1b633fc7625 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/CalcITCase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/CalcITCase.scala
@@ -305,6 +305,14 @@ class CalcITCase extends BatchTestBase {
       ))
   }
 
+  @Test
+  def testDecimalReturnType(): Unit = {
+    registerFunction("myNegative", MyNegative)
+    checkResult("SELECT myNegative(5.1)",
+      Seq(row(new java.math.BigDecimal("-5.100000000000000000"))
+      ))
+  }
+
   @Test
   def testUDFWithInternalClass(): Unit = {
     registerFunction("func", BinaryStringFunction)
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/utils/UserDefinedFunctionTestUtils.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/utils/UserDefinedFunctionTestUtils.scala
index bd69126013a..231f2e517c1 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/utils/UserDefinedFunctionTestUtils.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/utils/UserDefinedFunctionTestUtils.scala
@@ -370,6 +370,13 @@ object UserDefinedFunctionTestUtils {
     }
   }
 
+  @SerialVersionUID(1L)
+  object MyNegative extends ScalarFunction {
+    def eval(d: java.math.BigDecimal): java.lang.Object = d.negate()
+
+    override def getResultType(signature: Array[Class[_]]): TypeInformation[_] = Types.JAVA_BIG_DEC
+  }
+
   // ------------------------------------------------------------------------------------
   // POJOs
   // ------------------------------------------------------------------------------------
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/dataformat/DataFormatConverters.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/dataformat/DataFormatConverters.java
index 6c9ef4a9f78..5ed8aa4218d 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/dataformat/DataFormatConverters.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/dataformat/DataFormatConverters.java
@@ -164,8 +164,10 @@ public class DataFormatConverters {
 				Tuple2<Integer, Integer> ps = getPrecision(logicalType);
 				if (clazz == BigDecimal.class) {
 					return new BigDecimalConverter(ps.f0, ps.f1);
-				} else {
+				} else if (clazz == Decimal.class) {
 					return new DecimalConverter(ps.f0, ps.f1);
+				} else {
+					throw new RuntimeException("Not support conversion class for DECIMAL: " + clazz);
 				}
 			case TIMESTAMP_WITHOUT_TIME_ZONE:
 				int precisionOfTS = getDateTimePrecision(logicalType);
@@ -173,8 +175,10 @@ public class DataFormatConverters {
 					return new TimestampConverter(precisionOfTS);
 				} else if (clazz == LocalDateTime.class) {
 					return new LocalDateTimeConverter(precisionOfTS);
-				} else {
+				} else if (clazz == SqlTimestamp.class) {
 					return new SqlTimestampConverter(precisionOfTS);
+				} else {
+					throw new RuntimeException("Not support conversion class for TIMESTAMP WITHOUT TIME ZONE: " + clazz);
 				}
 			case TIMESTAMP_WITH_LOCAL_TIME_ZONE:
 				int precisionOfLZTS = getDateTimePrecision(logicalType);
@@ -182,8 +186,10 @@ public class DataFormatConverters {
 					return new InstantConverter(precisionOfLZTS);
 				} else if (clazz == Long.class || clazz == long.class) {
 					return new LongSqlTimestampConverter(precisionOfLZTS);
-				} else {
+				} else if (clazz == SqlTimestamp.class) {
 					return new SqlTimestampConverter(precisionOfLZTS);
+				} else {
+					throw new RuntimeException("Not support conversion class for TIMESTAMP WITH LOCAL TIME ZONE: " + clazz);
 				}
 			case ARRAY:
 				if (clazz == BinaryArray.class) {
