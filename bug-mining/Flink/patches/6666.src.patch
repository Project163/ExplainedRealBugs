diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/hybrid/HsFileDataIndex.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/hybrid/HsFileDataIndex.java
index c5a80618e97..b9bc5c01de4 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/hybrid/HsFileDataIndex.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/hybrid/HsFileDataIndex.java
@@ -39,10 +39,12 @@ public interface HsFileDataIndex {
      *
      * @param subpartitionId that the readable region belongs to
      * @param bufferIndex that the readable region starts with
+     * @param consumingOffset of the downstream
      * @return a {@link ReadableRegion} for the given subpartition that starts with the given buffer
      *     index, if exist; otherwise, {@link Optional#empty()}.
      */
-    Optional<ReadableRegion> getReadableRegion(int subpartitionId, int bufferIndex);
+    Optional<ReadableRegion> getReadableRegion(
+            int subpartitionId, int bufferIndex, int consumingOffset);
 
     /**
      * Add buffers to the index.
@@ -55,12 +57,12 @@ public interface HsFileDataIndex {
     void addBuffers(List<SpilledBuffer> spilledBuffers);
 
     /**
-     * Mark a buffer as READABLE.
+     * Mark a buffer as RELEASED.
      *
      * @param subpartitionId that the buffer belongs to
      * @param bufferIndex of the buffer within the subpartition
      */
-    void markBufferReadable(int subpartitionId, int bufferIndex);
+    void markBufferReleased(int subpartitionId, int bufferIndex);
 
     /**
      * Represents a series of physically continuous buffers in the file, which are readable, from
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/hybrid/HsFileDataIndexImpl.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/hybrid/HsFileDataIndexImpl.java
index 6ff80f5ea31..ad919776e5e 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/hybrid/HsFileDataIndexImpl.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/hybrid/HsFileDataIndexImpl.java
@@ -51,10 +51,13 @@ public class HsFileDataIndexImpl implements HsFileDataIndex {
     }
 
     @Override
-    public Optional<ReadableRegion> getReadableRegion(int subpartitionId, int bufferIndex) {
+    public Optional<ReadableRegion> getReadableRegion(
+            int subpartitionId, int bufferIndex, int consumingOffset) {
         synchronized (lock) {
             return getInternalRegion(subpartitionId, bufferIndex)
-                    .map(internalRegion -> internalRegion.toReadableRegion(bufferIndex))
+                    .map(
+                            internalRegion ->
+                                    internalRegion.toReadableRegion(bufferIndex, consumingOffset))
                     .filter(internalRegion -> internalRegion.numReadable > 0);
         }
     }
@@ -76,10 +79,10 @@ public class HsFileDataIndexImpl implements HsFileDataIndex {
     }
 
     @Override
-    public void markBufferReadable(int subpartitionId, int bufferIndex) {
+    public void markBufferReleased(int subpartitionId, int bufferIndex) {
         synchronized (lock) {
             getInternalRegion(subpartitionId, bufferIndex)
-                    .ifPresent(internalRegion -> internalRegion.markBufferReadable(bufferIndex));
+                    .ifPresent(internalRegion -> internalRegion.markBufferReleased(bufferIndex));
         }
     }
 
@@ -170,25 +173,26 @@ public class HsFileDataIndexImpl implements HsFileDataIndex {
         private final int firstBufferIndex;
         private final long firstBufferOffset;
         private final int numBuffers;
-        private final boolean[] readable;
+        private final boolean[] released;
 
         private InternalRegion(int firstBufferIndex, long firstBufferOffset, int numBuffers) {
             this.firstBufferIndex = firstBufferIndex;
             this.firstBufferOffset = firstBufferOffset;
             this.numBuffers = numBuffers;
-            this.readable = new boolean[numBuffers];
-            Arrays.fill(readable, false);
+            this.released = new boolean[numBuffers];
+            Arrays.fill(released, false);
         }
 
         private boolean containBuffer(int bufferIndex) {
             return bufferIndex >= firstBufferIndex && bufferIndex < firstBufferIndex + numBuffers;
         }
 
-        private HsFileDataIndex.ReadableRegion toReadableRegion(int bufferIndex) {
+        private HsFileDataIndex.ReadableRegion toReadableRegion(
+                int bufferIndex, int consumingOffset) {
             int nSkip = bufferIndex - firstBufferIndex;
             int nReadable = 0;
             while (nSkip + nReadable < numBuffers) {
-                if (!readable[nSkip + nReadable]) {
+                if (!released[nSkip + nReadable] || (bufferIndex + nReadable) <= consumingOffset) {
                     break;
                 }
                 ++nReadable;
@@ -196,8 +200,8 @@ public class HsFileDataIndexImpl implements HsFileDataIndex {
             return new ReadableRegion(nSkip, nReadable, firstBufferOffset);
         }
 
-        private void markBufferReadable(int bufferIndex) {
-            readable[bufferIndex - firstBufferIndex] = true;
+        private void markBufferReleased(int bufferIndex) {
+            released[bufferIndex - firstBufferIndex] = true;
         }
     }
 }
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/hybrid/HsMemoryDataManager.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/hybrid/HsMemoryDataManager.java
index 867ed539d72..22a9408a01f 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/hybrid/HsMemoryDataManager.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/hybrid/HsMemoryDataManager.java
@@ -216,8 +216,8 @@ public class HsMemoryDataManager implements HsSpillingInfoProvider, HsMemoryData
     // ------------------------------------
 
     @Override
-    public void markBufferReadableFromFile(int subpartitionId, int bufferIndex) {
-        fileDataIndex.markBufferReadable(subpartitionId, bufferIndex);
+    public void markBufferReleasedFromFile(int subpartitionId, int bufferIndex) {
+        fileDataIndex.markBufferReleased(subpartitionId, bufferIndex);
     }
 
     @Override
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/hybrid/HsMemoryDataManagerOperation.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/hybrid/HsMemoryDataManagerOperation.java
index ca86a1ae913..37df7709d63 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/hybrid/HsMemoryDataManagerOperation.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/hybrid/HsMemoryDataManagerOperation.java
@@ -33,12 +33,12 @@ public interface HsMemoryDataManagerOperation {
     BufferBuilder requestBufferFromPool() throws InterruptedException;
 
     /**
-     * This method is called when buffer should mark as readable in {@link HsFileDataIndex}.
+     * This method is called when buffer should mark as released in {@link HsFileDataIndex}.
      *
      * @param subpartitionId the subpartition that target buffer belong to.
-     * @param bufferIndex index of buffer to mark as readable.
+     * @param bufferIndex index of buffer to mark as released.
      */
-    void markBufferReadableFromFile(int subpartitionId, int bufferIndex);
+    void markBufferReleasedFromFile(int subpartitionId, int bufferIndex);
 
     /**
      * This method is called when buffer is consumed.
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/hybrid/HsSubpartitionFileReaderImpl.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/hybrid/HsSubpartitionFileReaderImpl.java
index de02a45582b..e6dc7122c5e 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/hybrid/HsSubpartitionFileReaderImpl.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/hybrid/HsSubpartitionFileReaderImpl.java
@@ -188,7 +188,9 @@ public class HsSubpartitionFileReaderImpl implements HsSubpartitionFileReader {
     public void prepareForScheduling() {
         // Access the consuming offset with lock, to prevent loading any buffer released from the
         // memory data manager that is already consumed.
-        bufferIndexManager.updateLastConsumed(operations.getConsumingOffset(true));
+        int consumingOffset = operations.getConsumingOffset(true);
+        bufferIndexManager.updateLastConsumed(consumingOffset);
+        cachedRegionManager.updateConsumingOffset(consumingOffset);
     }
 
     /** Provides priority calculation logic for io scheduler. */
@@ -379,6 +381,8 @@ public class HsSubpartitionFileReaderImpl implements HsSubpartitionFileReader {
         private final int subpartitionId;
         private final HsFileDataIndex dataIndex;
 
+        private int consumingOffset = -1;
+
         private int currentBufferIndex;
         private int numSkip;
         private int numReadable;
@@ -393,6 +397,10 @@ public class HsSubpartitionFileReaderImpl implements HsSubpartitionFileReader {
         //  Called by HsSubpartitionFileReader
         // ------------------------------------------------------------------------
 
+        public void updateConsumingOffset(int consumingOffset) {
+            this.consumingOffset = consumingOffset;
+        }
+
         /** Return Long.MAX_VALUE if region does not exist to giving the lowest priority. */
         private long getFileOffset(int bufferIndex) {
             updateCachedRegionIfNeeded(bufferIndex);
@@ -448,7 +456,7 @@ public class HsSubpartitionFileReaderImpl implements HsSubpartitionFileReader {
             }
 
             Optional<HsFileDataIndex.ReadableRegion> lookupResultOpt =
-                    dataIndex.getReadableRegion(subpartitionId, bufferIndex);
+                    dataIndex.getReadableRegion(subpartitionId, bufferIndex, consumingOffset);
             if (!lookupResultOpt.isPresent()) {
                 currentBufferIndex = -1;
                 numReadable = 0;
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/hybrid/HsSubpartitionMemoryDataManager.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/hybrid/HsSubpartitionMemoryDataManager.java
index d8040da0f56..619bc01d05a 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/hybrid/HsSubpartitionMemoryDataManager.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/hybrid/HsSubpartitionMemoryDataManager.java
@@ -459,8 +459,8 @@ public class HsSubpartitionMemoryDataManager implements HsDataView {
 
     @GuardedBy("subpartitionLock")
     private void checkAndMarkBufferReadable(HsBufferContext bufferContext) {
-        // only spill and not consumed buffer needs to be marked as readable.
-        if (isBufferSatisfyStatus(bufferContext, SpillStatus.SPILL, ConsumeStatus.NOT_CONSUMED)) {
+        // only spill buffer needs to be marked as released.
+        if (isBufferSatisfyStatus(bufferContext, SpillStatus.SPILL, ConsumeStatus.ALL)) {
             bufferContext
                     .getSpilledFuture()
                     .orElseThrow(
@@ -471,7 +471,7 @@ public class HsSubpartitionMemoryDataManager implements HsDataView {
                             () -> {
                                 BufferIndexAndChannel bufferIndexAndChannel =
                                         bufferContext.getBufferIndexAndChannel();
-                                memoryDataManagerOperation.markBufferReadableFromFile(
+                                memoryDataManagerOperation.markBufferReleasedFromFile(
                                         bufferIndexAndChannel.getChannel(),
                                         bufferIndexAndChannel.getBufferIndex());
                             });
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/hybrid/HsFileDataIndexImplTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/hybrid/HsFileDataIndexImplTest.java
index 8af665a4d34..0f05cfdd9a6 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/hybrid/HsFileDataIndexImplTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/hybrid/HsFileDataIndexImplTest.java
@@ -54,23 +54,33 @@ class HsFileDataIndexImplTest {
     @Test
     void testGetReadableRegionBufferNotExist() {
         hsDataIndex.addBuffers(createSpilledBuffers(0, Arrays.asList(0, 2)));
-        hsDataIndex.markBufferReadable(0, 0);
-        hsDataIndex.markBufferReadable(0, 2);
+        hsDataIndex.markBufferReleased(0, 0);
+        hsDataIndex.markBufferReleased(0, 2);
 
         // subpartition 0 does not have buffer with index 1
-        assertThat(hsDataIndex.getReadableRegion(0, 1)).isNotPresent();
+        assertThat(hsDataIndex.getReadableRegion(0, 1, -1)).isNotPresent();
 
         // subpartition 1 has no buffer
-        assertThat(hsDataIndex.getReadableRegion(1, 0)).isNotPresent();
+        assertThat(hsDataIndex.getReadableRegion(1, 0, -1)).isNotPresent();
     }
 
     /** If target buffer is not readable, {@link Optional#empty()} should be eventually returned. */
     @Test
     void testGetReadableRegionNotReadable() {
         hsDataIndex.addBuffers(createSpilledBuffers(0, Collections.singletonList(0)));
+        hsDataIndex.markBufferReleased(0, 0);
 
-        // 0-0 is not readable
-        assertThat(hsDataIndex.getReadableRegion(0, 0)).isNotPresent();
+        // 0-0 is not readable as consuming offset is bigger than 0.
+        assertThat(hsDataIndex.getReadableRegion(0, 0, 1)).isNotPresent();
+    }
+
+    /** If target buffer is not released, {@link Optional#empty()} should be eventually returned. */
+    @Test
+    void testGetReadableRegionNotReleased() {
+        hsDataIndex.addBuffers(createSpilledBuffers(0, Collections.singletonList(0)));
+
+        // 0-0 is not released
+        assertThat(hsDataIndex.getReadableRegion(0, 0, -1)).isNotPresent();
     }
 
     /**
@@ -82,18 +92,18 @@ class HsFileDataIndexImplTest {
         final int subpartitionId = 0;
 
         hsDataIndex.addBuffers(createSpilledBuffers(subpartitionId, Arrays.asList(0, 1, 3, 4, 5)));
-        hsDataIndex.markBufferReadable(subpartitionId, 1);
-        hsDataIndex.markBufferReadable(subpartitionId, 3);
-        hsDataIndex.markBufferReadable(subpartitionId, 4);
+        hsDataIndex.markBufferReleased(subpartitionId, 1);
+        hsDataIndex.markBufferReleased(subpartitionId, 3);
+        hsDataIndex.markBufferReleased(subpartitionId, 4);
 
-        assertThat(hsDataIndex.getReadableRegion(subpartitionId, 1))
+        assertThat(hsDataIndex.getReadableRegion(subpartitionId, 1, 0))
                 .hasValueSatisfying(
                         readableRegion -> {
                             assertRegionStartWithTargetBufferIndex(readableRegion, 1);
                             // Readable region will not include discontinuous buffer.
                             assertThat(readableRegion.numReadable).isEqualTo(1);
                         });
-        assertThat(hsDataIndex.getReadableRegion(subpartitionId, 3))
+        assertThat(hsDataIndex.getReadableRegion(subpartitionId, 3, 0))
                 .hasValueSatisfying(
                         readableRegion -> {
                             assertRegionStartWithTargetBufferIndex(readableRegion, 3);
@@ -101,7 +111,7 @@ class HsFileDataIndexImplTest {
                                     .isGreaterThanOrEqualTo(1)
                                     .isLessThanOrEqualTo(2);
                         });
-        assertThat(hsDataIndex.getReadableRegion(subpartitionId, 4))
+        assertThat(hsDataIndex.getReadableRegion(subpartitionId, 4, 0))
                 .hasValueSatisfying(
                         readableRegion -> {
                             assertRegionStartWithTargetBufferIndex(readableRegion, 4);
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/hybrid/HsSubpartitionFileReaderImplTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/hybrid/HsSubpartitionFileReaderImplTest.java
index fcc51dbfa7d..03a560589d6 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/hybrid/HsSubpartitionFileReaderImplTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/hybrid/HsSubpartitionFileReaderImplTest.java
@@ -113,15 +113,17 @@ class HsSubpartitionFileReaderImplTest {
         writeDataToFile(1, 2, 25, 1);
 
         Queue<MemorySegment> memorySegments = createsMemorySegments(6);
-
+        fileReader1.prepareForScheduling();
         fileReader1.readBuffers(memorySegments, FreeingBufferRecycler.INSTANCE);
         assertThat(memorySegments).hasSize(4);
         checkData(fileReader1, 10, 11);
 
+        fileReader2.prepareForScheduling();
         fileReader2.readBuffers(memorySegments, FreeingBufferRecycler.INSTANCE);
         assertThat(memorySegments).hasSize(2);
         checkData(fileReader2, 20, 21);
 
+        fileReader1.prepareForScheduling();
         fileReader1.readBuffers(memorySegments, FreeingBufferRecycler.INSTANCE);
         assertThat(memorySegments).hasSize(1);
         checkData(fileReader1, 15);
@@ -147,7 +149,7 @@ class HsSubpartitionFileReaderImplTest {
         writeDataToFile(0, 0, 1, 3, bufferCompressor);
 
         Queue<MemorySegment> memorySegments = createsMemorySegments(3);
-
+        fileReader1.prepareForScheduling();
         fileReader1.readBuffers(memorySegments, FreeingBufferRecycler.INSTANCE);
         checkData(fileReader1, bufferDecompressor, 1, 2, 3);
     }
@@ -400,6 +402,7 @@ class HsSubpartitionFileReaderImplTest {
         writeDataToFile(0, 0, 0, 2);
 
         Queue<MemorySegment> memorySegments = createsMemorySegments(2);
+        subpartitionFileReader.prepareForScheduling();
         // trigger reading, add buffer to queue.
         subpartitionFileReader.readBuffers(memorySegments, (ignore) -> {});
 
@@ -455,6 +458,7 @@ class HsSubpartitionFileReaderImplTest {
         writeDataToFile(0, 0, 2);
 
         Queue<MemorySegment> memorySegments = createsMemorySegments(2);
+        subpartitionFileReader.prepareForScheduling();
         // trigger reading, add buffer to queue.
         subpartitionFileReader.readBuffers(memorySegments, (ignore) -> {});
 
@@ -466,6 +470,37 @@ class HsSubpartitionFileReaderImplTest {
                 .isEqualTo(DataType.DATA_BUFFER);
     }
 
+    /**
+     * If subpartitionReader is registered more than once due to failover, the new reader should be
+     * able to read all the released data from disk, even if some data was read from memory before
+     * failover.
+     */
+    @Test
+    void testSubpartitionReaderRegisterMultipleTimes() throws Exception {
+        TestingSubpartitionViewInternalOperation viewNotifier =
+                new TestingSubpartitionViewInternalOperation();
+        HsSubpartitionFileReaderImpl subpartitionFileReader =
+                createSubpartitionFileReader(0, viewNotifier);
+        // mock the scenario that buffer 0 is already read form memory.
+        viewNotifier.advanceConsumptionProgress();
+        writeDataToFile(0, 0, 1, 3);
+        subpartitionFileReader.prepareForScheduling();
+        Queue<MemorySegment> memorySegments = createsMemorySegments(3);
+        subpartitionFileReader.readBuffers(memorySegments, (ignore) -> {});
+        assertThat(memorySegments).hasSize(1);
+        checkData(subpartitionFileReader, 2, 3);
+
+        // after failover, new view and subpartitionFileReader will be created.
+        viewNotifier = new TestingSubpartitionViewInternalOperation();
+        subpartitionFileReader = createSubpartitionFileReader(0, viewNotifier);
+        subpartitionFileReader.prepareForScheduling();
+        memorySegments = createsMemorySegments(3);
+        subpartitionFileReader.readBuffers(memorySegments, (ignore) -> {});
+        assertThat(memorySegments).isEmpty();
+        // buffer 0 can be read from disk correctly.
+        checkData(subpartitionFileReader, 1, 2, 3);
+    }
+
     private static void checkData(
             HsSubpartitionFileReaderImpl fileReader,
             BufferDecompressor bufferDecompressor,
@@ -556,7 +591,7 @@ class HsSubpartitionFileReaderImplTest {
         // mark all buffers status to release.
         spilledBuffers.forEach(
                 spilledBuffer ->
-                        diskIndex.markBufferReadable(subpartitionId, spilledBuffer.bufferIndex));
+                        diskIndex.markBufferReleased(subpartitionId, spilledBuffer.bufferIndex));
     }
 
     private void writeDataToFile(
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/hybrid/TestingFileDataIndex.java b/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/hybrid/TestingFileDataIndex.java
index db5663bb42f..115f1f5c66b 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/hybrid/TestingFileDataIndex.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/hybrid/TestingFileDataIndex.java
@@ -18,22 +18,25 @@
 
 package org.apache.flink.runtime.io.network.partition.hybrid;
 
+import org.apache.flink.util.function.TriFunction;
+
 import java.util.List;
 import java.util.Optional;
 import java.util.function.BiConsumer;
-import java.util.function.BiFunction;
 import java.util.function.Consumer;
 
 /** Mock {@link HsFileDataIndex} for testing. */
 public class TestingFileDataIndex implements HsFileDataIndex {
-    private final BiFunction<Integer, Integer, Optional<ReadableRegion>> getReadableRegionFunction;
+    private final TriFunction<Integer, Integer, Integer, Optional<ReadableRegion>>
+            getReadableRegionFunction;
 
     private final Consumer<List<SpilledBuffer>> addBuffersConsumer;
 
     private final BiConsumer<Integer, Integer> markBufferReadableConsumer;
 
     private TestingFileDataIndex(
-            BiFunction<Integer, Integer, Optional<ReadableRegion>> getReadableRegionFunction,
+            TriFunction<Integer, Integer, Integer, Optional<ReadableRegion>>
+                    getReadableRegionFunction,
             Consumer<List<SpilledBuffer>> addBuffersConsumer,
             BiConsumer<Integer, Integer> markBufferReadableConsumer) {
         this.getReadableRegionFunction = getReadableRegionFunction;
@@ -42,8 +45,9 @@ public class TestingFileDataIndex implements HsFileDataIndex {
     }
 
     @Override
-    public Optional<ReadableRegion> getReadableRegion(int subpartitionId, int bufferIndex) {
-        return getReadableRegionFunction.apply(subpartitionId, bufferIndex);
+    public Optional<ReadableRegion> getReadableRegion(
+            int subpartitionId, int bufferIndex, int consumingOffset) {
+        return getReadableRegionFunction.apply(subpartitionId, bufferIndex, consumingOffset);
     }
 
     @Override
@@ -52,7 +56,7 @@ public class TestingFileDataIndex implements HsFileDataIndex {
     }
 
     @Override
-    public void markBufferReadable(int subpartitionId, int bufferIndex) {
+    public void markBufferReleased(int subpartitionId, int bufferIndex) {
         markBufferReadableConsumer.accept(subpartitionId, bufferIndex);
     }
 
@@ -62,8 +66,8 @@ public class TestingFileDataIndex implements HsFileDataIndex {
 
     /** Builder for {@link TestingFileDataIndex}. */
     public static class Builder {
-        private BiFunction<Integer, Integer, Optional<ReadableRegion>> getReadableRegionFunction =
-                (ignore1, ignore2) -> Optional.empty();
+        private TriFunction<Integer, Integer, Integer, Optional<ReadableRegion>>
+                getReadableRegionFunction = (ignore1, ignore2, ignore3) -> Optional.empty();
 
         private Consumer<List<SpilledBuffer>> addBuffersConsumer = (ignore) -> {};
 
@@ -72,7 +76,8 @@ public class TestingFileDataIndex implements HsFileDataIndex {
         private Builder() {}
 
         public Builder setGetReadableRegionFunction(
-                BiFunction<Integer, Integer, Optional<ReadableRegion>> getReadableRegionFunction) {
+                TriFunction<Integer, Integer, Integer, Optional<ReadableRegion>>
+                        getReadableRegionFunction) {
             this.getReadableRegionFunction = getReadableRegionFunction;
             return this;
         }
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/hybrid/TestingMemoryDataManagerOperation.java b/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/hybrid/TestingMemoryDataManagerOperation.java
index bb441d667af..2b16d5d4d4f 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/hybrid/TestingMemoryDataManagerOperation.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/hybrid/TestingMemoryDataManagerOperation.java
@@ -57,7 +57,7 @@ public class TestingMemoryDataManagerOperation implements HsMemoryDataManagerOpe
     }
 
     @Override
-    public void markBufferReadableFromFile(int subpartitionId, int bufferIndex) {
+    public void markBufferReleasedFromFile(int subpartitionId, int bufferIndex) {
         markBufferReadableConsumer.accept(subpartitionId, bufferIndex);
     }
 
