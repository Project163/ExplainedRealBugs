diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveStatsUtil.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveStatsUtil.java
index f0e6a4e267c..dc4d20c84e3 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveStatsUtil.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveStatsUtil.java
@@ -69,9 +69,10 @@ import java.time.LocalDate;
 import java.time.LocalDateTime;
 import java.util.ArrayList;
 import java.util.HashMap;
+import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
-import java.util.Optional;
+import java.util.Set;
 import java.util.stream.Collectors;
 
 import static org.apache.flink.util.Preconditions.checkNotNull;
@@ -312,18 +313,18 @@ public class HiveStatsUtil {
                                                                     p.getParameters())
                                                             .getRowCount()))
                             .collect(Collectors.toList());
-            Optional<TableStats> optionalTableStats =
-                    catalogTableStatistics.stream().reduce(TableStats::merge);
-            if (!optionalTableStats.isPresent()) {
-                return 0L;
+
+            Set<String> partitionKeys = getFieldNames(hiveTable.getPartitionKeys());
+            TableStats resultTableStats =
+                    catalogTableStatistics.stream()
+                            .reduce((s1, s2) -> s1.merge(s2, partitionKeys))
+                            .orElse(TableStats.UNKNOWN);
+            if (resultTableStats == TableStats.UNKNOWN || resultTableStats.getRowCount() < 0) {
+                return null;
             } else {
-                TableStats tableStats = optionalTableStats.get();
-                if (tableStats == TableStats.UNKNOWN || tableStats.getRowCount() < 0) {
-                    return null;
-                } else {
-                    return tableStats.getRowCount();
-                }
+                return resultTableStats.getRowCount();
             }
+
         } catch (Exception e) {
             LOG.warn(
                     "Can't list partition for table `{}.{}`, partition value {}.",
@@ -334,6 +335,15 @@ public class HiveStatsUtil {
         return null;
     }
 
+    /** Get field names from field schemas. */
+    private static Set<String> getFieldNames(List<FieldSchema> fieldSchemas) {
+        Set<String> names = new HashSet<>();
+        for (FieldSchema fs : fieldSchemas) {
+            names.add(fs.getName());
+        }
+        return names;
+    }
+
     public static CatalogTableStatistics createCatalogTableStatistics(
             Map<String, String> parameters) {
         return new CatalogTableStatistics(
diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/plan/stats/ColumnStats.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/plan/stats/ColumnStats.java
index 00efcf6c9c9..7d0b2134e09 100644
--- a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/plan/stats/ColumnStats.java
+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/plan/stats/ColumnStats.java
@@ -213,12 +213,17 @@ public final class ColumnStats {
      * @param other The other column stats to merge.
      * @return The merged column stats.
      */
-    public ColumnStats merge(ColumnStats other) {
+    public ColumnStats merge(ColumnStats other, boolean isPartitionKey) {
         if (this == UNKNOWN || other == UNKNOWN) {
             return UNKNOWN;
         }
+        Long ndv;
+        if (isPartitionKey) {
+            ndv = combineIfNonNull(Long::sum, this.ndv, other.ndv);
+        } else {
+            ndv = combineIfNonNull(Long::max, this.ndv, other.ndv);
+        }
 
-        Long ndv = combineIfNonNull(Long::sum, this.ndv, other.ndv);
         Long nullCount = combineIfNonNull(Long::sum, this.nullCount, other.nullCount);
         Double avgLen = combineIfNonNull((a1, a2) -> (a1 + a2) / 2, this.avgLen, other.avgLen);
         Integer maxLen = combineIfNonNull(Math::max, this.maxLen, other.maxLen);
diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/plan/stats/TableStats.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/plan/stats/TableStats.java
index 7140ab4c99d..babad747278 100644
--- a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/plan/stats/TableStats.java
+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/plan/stats/TableStats.java
@@ -20,11 +20,12 @@ package org.apache.flink.table.plan.stats;
 
 import org.apache.flink.annotation.PublicEvolving;
 
-import javax.annotation.Nonnull;
+import javax.annotation.Nullable;
 
 import java.util.HashMap;
 import java.util.Map;
 import java.util.Objects;
+import java.util.Set;
 
 /** Table statistics. */
 @PublicEvolving
@@ -79,26 +80,30 @@ public final class TableStats {
      * @param other The other table stats to merge.
      * @return The merged table stats.
      */
-    @Nonnull
-    public TableStats merge(TableStats other) {
-        if (this == UNKNOWN || other == UNKNOWN) {
+    public TableStats merge(TableStats other, @Nullable Set<String> partitionKeys) {
+        if (this.rowCount < 0 || other.rowCount < 0) {
             return TableStats.UNKNOWN;
         }
         long rowCount =
                 this.rowCount >= 0 && other.rowCount >= 0
                         ? this.rowCount + other.rowCount
                         : UNKNOWN.rowCount;
-        return new TableStats(rowCount, mergeColumnStates(other));
+        return new TableStats(rowCount, mergeColumnStates(other, partitionKeys));
     }
 
-    private Map<String, ColumnStats> mergeColumnStates(TableStats other) {
+    private Map<String, ColumnStats> mergeColumnStates(
+            TableStats other, @Nullable Set<String> partitionKeys) {
         Map<String, ColumnStats> colStats = new HashMap<>();
         for (Map.Entry<String, ColumnStats> entry : this.colStats.entrySet()) {
             String col = entry.getKey();
             ColumnStats stats = entry.getValue();
             ColumnStats otherStats = other.colStats.get(col);
             if (otherStats != null) {
-                colStats.put(col, stats.merge(otherStats));
+                if (partitionKeys != null) {
+                    colStats.put(col, stats.merge(otherStats, partitionKeys.contains(col)));
+                } else {
+                    colStats.put(col, stats.merge(otherStats, false));
+                }
             }
         }
         return colStats;
diff --git a/flink-table/flink-table-common/src/test/java/org/apache/flink/table/plan/stats/TableStatsTest.java b/flink-table/flink-table-common/src/test/java/org/apache/flink/table/plan/stats/TableStatsTest.java
index 87ffab4b877..21b492af78c 100644
--- a/flink-table/flink-table-common/src/test/java/org/apache/flink/table/plan/stats/TableStatsTest.java
+++ b/flink-table/flink-table-common/src/test/java/org/apache/flink/table/plan/stats/TableStatsTest.java
@@ -20,7 +20,9 @@ package org.apache.flink.table.plan.stats;
 
 import org.junit.jupiter.api.Test;
 
+import java.util.Collections;
 import java.util.HashMap;
+import java.util.HashSet;
 import java.util.Map;
 
 import static org.assertj.core.api.Assertions.assertThat;
@@ -39,8 +41,43 @@ class TableStatsTest {
         TableStats stats2 = new TableStats(32, colStats2);
 
         Map<String, ColumnStats> colStatsMerge = new HashMap<>();
-        colStatsMerge.put("a", new ColumnStats(7L, 20L, 7D, 23, 35, 2));
-        assertThat(stats1.merge(stats2)).isEqualTo(new TableStats(62, colStatsMerge));
+        colStatsMerge.put("a", new ColumnStats(4L, 20L, 7D, 23, 35, 2));
+        assertThat(stats1.merge(stats2, null)).isEqualTo(new TableStats(62, colStatsMerge));
+
+        Map<String, ColumnStats> colStatsMerge2 = new HashMap<>();
+        colStatsMerge2.put("a", new ColumnStats(4L, 20L, 7D, 23, 35, 2));
+        assertThat(stats1.merge(stats2, new HashSet<>()))
+                .isEqualTo(new TableStats(62, colStatsMerge2));
+
+        // test column stats merge while column 'a' is partition key. Merged Ndv for columns which
+        // are partition keys using sum instead of max.
+        Map<String, ColumnStats> colStatsMerge3 = new HashMap<>();
+        colStatsMerge3.put("a", new ColumnStats(7L, 20L, 7D, 23, 35, 2));
+        assertThat(stats1.merge(stats2, new HashSet<>(Collections.singletonList("a"))))
+                .isEqualTo(new TableStats(62, colStatsMerge3));
+
+        Map<String, ColumnStats> colStats3 = new HashMap<>();
+        colStats3.put("a", new ColumnStats(4L, 5L, 2D, 3, 15, 2));
+        colStats3.put("b", new ColumnStats(4L, 5L, 2D, 3, 15, 2));
+        stats1 = new TableStats(30, colStats3);
+        Map<String, ColumnStats> colStats4 = new HashMap<>();
+        colStats4.put("a", new ColumnStats(3L, 15L, 12D, 23, 35, 6));
+        colStats4.put("b", new ColumnStats(3L, 15L, 12D, 23, 35, 6));
+        stats2 = new TableStats(32, colStats4);
+
+        Map<String, ColumnStats> colStatsMerge4 = new HashMap<>();
+        colStatsMerge4.put("a", new ColumnStats(7L, 20L, 7D, 23, 35, 2));
+        colStatsMerge4.put("b", new ColumnStats(4L, 20L, 7D, 23, 35, 2));
+        assertThat(stats1.merge(stats2, new HashSet<>(Collections.singletonList("a"))))
+                .isEqualTo(new TableStats(62, colStatsMerge4));
+
+        // test merge with one side is TableStats.UNKNOWN.
+        stats2 = TableStats.UNKNOWN;
+        assertThat(stats1.merge(stats2, null)).isEqualTo(TableStats.UNKNOWN);
+
+        // test merge with one side have no column stats.
+        stats2 = new TableStats(32);
+        assertThat(stats1.merge(stats2, null)).isEqualTo(new TableStats(62));
     }
 
     @Test
@@ -55,23 +92,35 @@ class TableStatsTest {
         TableStats stats2 = new TableStats(32, colStats2);
 
         Map<String, ColumnStats> colStatsMerge = new HashMap<>();
-        colStatsMerge.put("a", new ColumnStats(7L, 20L, 7D, 23, 35, 2));
-        assertThat(stats1.merge(stats2)).isEqualTo(new TableStats(62, colStatsMerge));
+        colStatsMerge.put("a", new ColumnStats(4L, 20L, 7D, 23, 35, 2));
+        assertThat(stats1.merge(stats2, null)).isEqualTo(new TableStats(62, colStatsMerge));
+
+        Map<String, ColumnStats> colStatsMerge2 = new HashMap<>();
+        colStatsMerge2.put("a", new ColumnStats(4L, 20L, 7D, 23, 35, 2));
+        assertThat(stats1.merge(stats2, new HashSet<>()))
+                .isEqualTo(new TableStats(62, colStatsMerge2));
+
+        // test column stats merge while column 'a' is partition key. Merged Ndv for columns which
+        // are partition keys using sum instead of max.
+        Map<String, ColumnStats> colStatsMerge3 = new HashMap<>();
+        colStatsMerge3.put("a", new ColumnStats(7L, 20L, 7D, 23, 35, 2));
+        assertThat(stats1.merge(stats2, new HashSet<>(Collections.singletonList("a"))))
+                .isEqualTo(new TableStats(62, colStatsMerge3));
     }
 
     @Test
     void testMergeUnknownRowCount() {
         TableStats stats1 = new TableStats(-1, new HashMap<>());
         TableStats stats2 = new TableStats(32, new HashMap<>());
-        assertThat(stats1.merge(stats2)).isEqualTo(new TableStats(-1, new HashMap<>()));
+        assertThat(stats1.merge(stats2, null)).isEqualTo(TableStats.UNKNOWN);
 
         stats1 = new TableStats(-1, new HashMap<>());
         stats2 = new TableStats(-1, new HashMap<>());
-        assertThat(stats1.merge(stats2)).isEqualTo(new TableStats(-1, new HashMap<>()));
+        assertThat(stats1.merge(stats2, null)).isEqualTo(TableStats.UNKNOWN);
 
         stats1 = new TableStats(-3, new HashMap<>());
         stats2 = new TableStats(-2, new HashMap<>());
-        assertThat(stats1.merge(stats2)).isEqualTo(new TableStats(-1, new HashMap<>()));
+        assertThat(stats1.merge(stats2, null)).isEqualTo(TableStats.UNKNOWN);
     }
 
     @Test
@@ -84,19 +133,25 @@ class TableStatsTest {
         ColumnStats columnStats5 = new ColumnStats(4L, 5L, 2D, 3, 15, null);
         ColumnStats columnStats6 = new ColumnStats(4L, 5L, null, 3, 15, 2);
 
-        assertThat(columnStats0.merge(columnStats1))
+        assertThat(columnStats0.merge(columnStats1, false))
+                .isEqualTo(new ColumnStats(4L, null, 2D, 3, 15, 2));
+        assertThat(columnStats0.merge(columnStats2, false))
+                .isEqualTo(new ColumnStats(4L, 10L, 2D, null, 15, 2));
+        assertThat(columnStats0.merge(columnStats3, false))
+                .isEqualTo(new ColumnStats(null, 10L, 2D, 3, 15, 2));
+        assertThat(columnStats0.merge(columnStats4, false))
+                .isEqualTo(new ColumnStats(4L, 10L, 2D, 3, null, 2));
+        assertThat(columnStats0.merge(columnStats5, false))
+                .isEqualTo(new ColumnStats(4L, 10L, 2D, 3, 15, null));
+        assertThat(columnStats0.merge(columnStats6, false))
+                .isEqualTo(new ColumnStats(4L, 10L, null, 3, 15, 2));
+        assertThat(columnStats6.merge(columnStats6, false))
+                .isEqualTo(new ColumnStats(4L, 10L, null, 3, 15, 2));
+
+        // tet column stats merge while partition key is true.
+        assertThat(columnStats0.merge(columnStats1, true))
                 .isEqualTo(new ColumnStats(8L, null, 2D, 3, 15, 2));
-        assertThat(columnStats0.merge(columnStats2))
-                .isEqualTo(new ColumnStats(8L, 10L, 2D, null, 15, 2));
-        assertThat(columnStats0.merge(columnStats3))
+        assertThat(columnStats0.merge(columnStats3, true))
                 .isEqualTo(new ColumnStats(null, 10L, 2D, 3, 15, 2));
-        assertThat(columnStats0.merge(columnStats4))
-                .isEqualTo(new ColumnStats(8L, 10L, 2D, 3, null, 2));
-        assertThat(columnStats0.merge(columnStats5))
-                .isEqualTo(new ColumnStats(8L, 10L, 2D, 3, 15, null));
-        assertThat(columnStats0.merge(columnStats6))
-                .isEqualTo(new ColumnStats(8L, 10L, null, 3, 15, 2));
-        assertThat(columnStats6.merge(columnStats6))
-                .isEqualTo(new ColumnStats(8L, 10L, null, 3, 15, 2));
     }
 }
diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/utils/CatalogTableStatisticsConverter.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/utils/CatalogTableStatisticsConverter.java
index fda577bb3bb..8041503a1b9 100644
--- a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/utils/CatalogTableStatisticsConverter.java
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/utils/CatalogTableStatisticsConverter.java
@@ -39,6 +39,7 @@ import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
+import java.util.Set;
 
 /**
  * Utility class for converting {@link CatalogTableStatistics} and {@link CatalogColumnStatistics}
@@ -66,7 +67,8 @@ public class CatalogTableStatisticsConverter {
 
     public static TableStats convertToAccumulatedTableStates(
             List<CatalogTableStatistics> tableStatisticsList,
-            List<CatalogColumnStatistics> catalogColumnStatisticsList) {
+            List<CatalogColumnStatistics> catalogColumnStatisticsList,
+            Set<String> partitionKeys) {
         Preconditions.checkState(
                 tableStatisticsList.size() == catalogColumnStatisticsList.size(),
                 String.format(
@@ -80,7 +82,10 @@ public class CatalogTableStatisticsConverter {
                     CatalogTableStatisticsConverter.convertToTableStats(
                             catalogTableStatistics, catalogColumnStatistics));
         }
-        return tableStats.stream().reduce(TableStats::merge).orElse(TableStats.UNKNOWN);
+
+        return tableStats.stream()
+                .reduce((s1, s2) -> s1.merge(s2, partitionKeys))
+                .orElse(TableStats.UNKNOWN);
     }
 
     @VisibleForTesting
diff --git a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/optimize/program/FlinkRecomputeStatisticsProgram.java b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/optimize/program/FlinkRecomputeStatisticsProgram.java
index 94470645dcd..8d6bebc5af4 100644
--- a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/optimize/program/FlinkRecomputeStatisticsProgram.java
+++ b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/optimize/program/FlinkRecomputeStatisticsProgram.java
@@ -47,9 +47,11 @@ import org.apache.calcite.rel.logical.LogicalTableScan;
 import javax.annotation.Nullable;
 
 import java.util.ArrayList;
+import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
 import java.util.Optional;
+import java.util.Set;
 import java.util.stream.Collectors;
 
 import static org.apache.flink.table.api.config.OptimizerConfigOptions.TABLE_OPTIMIZER_SOURCE_REPORT_STATISTICS_ENABLED;
@@ -207,12 +209,22 @@ public class FlinkRecomputeStatisticsProgram implements FlinkOptimizeProgram<Bat
             return Optional.of(
                     convertToAccumulatedTableStates(
                             catalog.bulkGetPartitionStatistics(tablePath, partitionSpecs),
-                            catalog.bulkGetPartitionColumnStatistics(tablePath, partitionSpecs)));
+                            catalog.bulkGetPartitionColumnStatistics(tablePath, partitionSpecs),
+                            getPartitionKeys(partitionSpecs)));
         } catch (PartitionNotExistException e) {
             return Optional.empty();
         }
     }
 
+    private static Set<String> getPartitionKeys(List<CatalogPartitionSpec> catalogPartitionSpecs) {
+        Set<String> partitionKeys = new HashSet<>();
+        for (CatalogPartitionSpec catalogPartitionSpec : catalogPartitionSpecs) {
+            Map<String, String> partitionSpec = catalogPartitionSpec.getPartitionSpec();
+            partitionKeys.addAll(partitionSpec.keySet());
+        }
+        return partitionKeys;
+    }
+
     @SuppressWarnings({"unchecked", "raw"})
     private <T extends SourceAbilitySpec> T getSpec(SourceAbilitySpec[] specs, Class<T> specClass) {
         if (specs == null) {
diff --git a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoLegacyTableSourceScanRule.scala b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoLegacyTableSourceScanRule.scala
index e5e7b9c2974..373b1d7feea 100644
--- a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoLegacyTableSourceScanRule.scala
+++ b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoLegacyTableSourceScanRule.scala
@@ -40,8 +40,8 @@ import org.apache.calcite.rex.{RexInputRef, RexNode, RexShuttle, RexUtil}
 import java.util
 import java.util.TimeZone
 
+import scala.collection.{mutable, JavaConversions}
 import scala.collection.JavaConversions._
-import scala.collection.mutable
 
 /**
  * Planner rule that tries to push partitions evaluated by filter condition into a
@@ -205,7 +205,9 @@ class PushPartitionIntoLegacyTableSourceScanRule
                   if (stats == null) {
                     stats = currStats
                   } else {
-                    stats = stats.merge(currStats)
+                    stats = stats.merge(
+                      currStats,
+                      JavaConversions.setAsJavaSet(partitionFieldNames.toSet))
                   }
                 case None => return null
               }
diff --git a/flink-table/flink-table-planner/src/test/java/org/apache/flink/connector/file/table/FileSystemStatisticsReportTest.java b/flink-table/flink-table-planner/src/test/java/org/apache/flink/connector/file/table/FileSystemStatisticsReportTest.java
index 9bb5c207207..526173a4d26 100644
--- a/flink-table/flink-table-planner/src/test/java/org/apache/flink/connector/file/table/FileSystemStatisticsReportTest.java
+++ b/flink-table/flink-table-planner/src/test/java/org/apache/flink/connector/file/table/FileSystemStatisticsReportTest.java
@@ -22,9 +22,15 @@ import org.apache.flink.table.api.config.OptimizerConfigOptions;
 import org.apache.flink.table.catalog.CatalogPartitionImpl;
 import org.apache.flink.table.catalog.CatalogPartitionSpec;
 import org.apache.flink.table.catalog.ObjectPath;
+import org.apache.flink.table.catalog.stats.CatalogColumnStatistics;
+import org.apache.flink.table.catalog.stats.CatalogColumnStatisticsDataBase;
+import org.apache.flink.table.catalog.stats.CatalogColumnStatisticsDataLong;
+import org.apache.flink.table.catalog.stats.CatalogColumnStatisticsDataString;
 import org.apache.flink.table.catalog.stats.CatalogTableStatistics;
+import org.apache.flink.table.plan.stats.ColumnStats;
 import org.apache.flink.table.plan.stats.TableStats;
 import org.apache.flink.table.planner.plan.stats.FlinkStatistic;
+import org.apache.flink.table.planner.utils.CatalogTableStatisticsConverter;
 import org.apache.flink.table.planner.utils.StatisticsReportTestBase;
 
 import org.junit.jupiter.api.BeforeEach;
@@ -38,6 +44,7 @@ import java.util.Arrays;
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
+import java.util.Map;
 
 import static org.assertj.core.api.Assertions.assertThat;
 
@@ -67,6 +74,7 @@ public class FileSystemStatisticsReportTest extends StatisticsReportTestBase {
         partitionDataPath.mkdirs();
         writeData(new File(partitionDataPath, "b=1"), Arrays.asList("1,1,hi", "2,1,hello"));
         writeData(new File(partitionDataPath, "b=2"), Collections.singletonList("3,2,hello world"));
+        writeData(new File(partitionDataPath, "b=3"), Collections.singletonList("4,3,hello"));
         String ddl2 =
                 String.format(
                         "CREATE TABLE PartTable (\n"
@@ -93,6 +101,13 @@ public class FileSystemStatisticsReportTest extends StatisticsReportTestBase {
                         new CatalogPartitionSpec(Collections.singletonMap("b", "2")),
                         new CatalogPartitionImpl(new HashMap<>(), ""),
                         false);
+        tEnv.getCatalog(tEnv.getCurrentCatalog())
+                .orElseThrow(Exception::new)
+                .createPartition(
+                        new ObjectPath(tEnv.getCurrentDatabase(), "PartTable"),
+                        new CatalogPartitionSpec(Collections.singletonMap("b", "3")),
+                        new CatalogPartitionImpl(new HashMap<>(), ""),
+                        false);
 
         String filePath2 =
                 createFileAndWriteData(
@@ -224,9 +239,40 @@ public class FileSystemStatisticsReportTest extends StatisticsReportTestBase {
                         new CatalogPartitionSpec(Collections.singletonMap("b", "2")),
                         new CatalogTableStatistics(3L, 1, 100L, 100L),
                         false);
+        tEnv.getCatalog(tEnv.getCurrentCatalog())
+                .orElseThrow(Exception::new)
+                .alterPartitionStatistics(
+                        new ObjectPath(tEnv.getCurrentDatabase(), "PartTable"),
+                        new CatalogPartitionSpec(Collections.singletonMap("b", "3")),
+                        new CatalogTableStatistics(3L, 1, 100L, 100L),
+                        false);
 
         FlinkStatistic statistic = getStatisticsFromOptimizedPlan("select * from PartTable");
-        assertThat(statistic.getTableStats()).isEqualTo(new TableStats(9));
+        assertThat(statistic.getTableStats()).isEqualTo(new TableStats(12));
+    }
+
+    @Test
+    public void tesNoPartitionPushDownAndCatalogStatisticsPartialExist() throws Exception {
+        tEnv.getCatalog(tEnv.getCurrentCatalog())
+                .orElseThrow(Exception::new)
+                .alterPartitionStatistics(
+                        new ObjectPath(tEnv.getCurrentDatabase(), "PartTable"),
+                        new CatalogPartitionSpec(Collections.singletonMap("b", "1")),
+                        new CatalogTableStatistics(6L, 1, 100L, 100L),
+                        false);
+        tEnv.getCatalog(tEnv.getCurrentCatalog())
+                .orElseThrow(Exception::new)
+                .alterPartitionStatistics(
+                        new ObjectPath(tEnv.getCurrentDatabase(), "PartTable"),
+                        new CatalogPartitionSpec(Collections.singletonMap("b", "2")),
+                        new CatalogTableStatistics(3L, 1, 100L, 100L),
+                        false);
+        // For partition table 'PartTable', partition 'b=3' have no catalog statistics, so get
+        // partition table stats from catalog will return TableStats.UNKNOWN. So we will recompute
+        // stats from source.
+        FlinkStatistic statistic = getStatisticsFromOptimizedPlan("select * from PartTable");
+        // there are four rows in file system.
+        assertThat(statistic.getTableStats()).isEqualTo(new TableStats(4));
     }
 
     @Test
@@ -261,6 +307,59 @@ public class FileSystemStatisticsReportTest extends StatisticsReportTestBase {
         assertThat(statistic.getTableStats()).isEqualTo(new TableStats(6));
     }
 
+    @Test
+    public void testPartitionPushDownAndCatalogColumnStatisticsExist() throws Exception {
+        // The purpose of this test case is to test the correctness of stats after partition push
+        // down, and recompute partition table and column stats. For partition table, merged Ndv for
+        // columns which are partition keys using sum instead of max (other columns using max).
+        tEnv.getCatalog(tEnv.getCurrentCatalog())
+                .orElseThrow(Exception::new)
+                .alterPartitionStatistics(
+                        new ObjectPath(tEnv.getCurrentDatabase(), "PartTable"),
+                        new CatalogPartitionSpec(Collections.singletonMap("b", "1")),
+                        new CatalogTableStatistics(6L, 1, 100L, 100L),
+                        false);
+        tEnv.getCatalog(tEnv.getCurrentCatalog())
+                .orElseThrow(Exception::new)
+                .alterPartitionStatistics(
+                        new ObjectPath(tEnv.getCurrentDatabase(), "PartTable"),
+                        new CatalogPartitionSpec(Collections.singletonMap("b", "2")),
+                        new CatalogTableStatistics(3L, 1, 100L, 100L),
+                        false);
+        tEnv.getCatalog(tEnv.getCurrentCatalog())
+                .orElseThrow(Exception::new)
+                .alterPartitionStatistics(
+                        new ObjectPath(tEnv.getCurrentDatabase(), "PartTable"),
+                        new CatalogPartitionSpec(Collections.singletonMap("b", "3")),
+                        new CatalogTableStatistics(3L, 1, 100L, 100L),
+                        false);
+        tEnv.getCatalog(tEnv.getCurrentCatalog())
+                .orElseThrow(Exception::new)
+                .alterPartitionColumnStatistics(
+                        new ObjectPath(tEnv.getCurrentDatabase(), "PartTable"),
+                        new CatalogPartitionSpec(Collections.singletonMap("b", "1")),
+                        createSinglePartitionColumnStats(),
+                        false);
+        tEnv.getCatalog(tEnv.getCurrentCatalog())
+                .orElseThrow(Exception::new)
+                .alterPartitionColumnStatistics(
+                        new ObjectPath(tEnv.getCurrentDatabase(), "PartTable"),
+                        new CatalogPartitionSpec(Collections.singletonMap("b", "2")),
+                        createSinglePartitionColumnStats(),
+                        false);
+        tEnv.getCatalog(tEnv.getCurrentCatalog())
+                .orElseThrow(Exception::new)
+                .alterPartitionColumnStatistics(
+                        new ObjectPath(tEnv.getCurrentDatabase(), "PartTable"),
+                        new CatalogPartitionSpec(Collections.singletonMap("b", "3")),
+                        createSinglePartitionColumnStats(),
+                        false);
+        FlinkStatistic statistic =
+                getStatisticsFromOptimizedPlan("select * from PartTable where b < 3");
+        assertThat(statistic.getTableStats())
+                .isEqualTo(new TableStats(9, createMergedPartitionColumnStats()));
+    }
+
     @Test
     public void testFilterPartitionPushDownPushDownAndCatalogStatisticsExist() throws Exception {
         tEnv.getCatalog(tEnv.getCurrentCatalog())
@@ -316,4 +415,31 @@ public class FileSystemStatisticsReportTest extends StatisticsReportTestBase {
                 getStatisticsFromOptimizedPlan("select * from emptyTable limit 1");
         assertThat(statistic.getTableStats()).isEqualTo(new TableStats(1));
     }
+
+    private CatalogColumnStatistics createSinglePartitionColumnStats() {
+        Map<String, CatalogColumnStatisticsDataBase> colStatsMap = new HashMap<>();
+        CatalogColumnStatisticsDataLong longColStats =
+                new CatalogColumnStatisticsDataLong(1L, 10L, 5L, 5L);
+        colStatsMap.put("a", longColStats);
+        colStatsMap.put("b", longColStats);
+        CatalogColumnStatisticsDataString stringColStats =
+                new CatalogColumnStatisticsDataString(10L, 10D, 5L, 5L);
+        colStatsMap.put("c", stringColStats);
+        return new CatalogColumnStatistics(colStatsMap);
+    }
+
+    private Map<String, ColumnStats> createMergedPartitionColumnStats() {
+        Map<String, CatalogColumnStatisticsDataBase> colStatsMap = new HashMap<>();
+        CatalogColumnStatisticsDataLong longColStats =
+                new CatalogColumnStatisticsDataLong(1L, 10L, 5L, 10L);
+        colStatsMap.put("a", longColStats);
+        // Merged Ndv for columns which are partition keys using sum instead of max.
+        CatalogColumnStatisticsDataLong longColStats2 =
+                new CatalogColumnStatisticsDataLong(1L, 10L, 10L, 10L);
+        colStatsMap.put("b", longColStats2);
+        CatalogColumnStatisticsDataString stringColStats =
+                new CatalogColumnStatisticsDataString(10L, 10D, 5L, 10L);
+        colStatsMap.put("c", stringColStats);
+        return CatalogTableStatisticsConverter.convertToColumnStatsMap(colStatsMap);
+    }
 }
diff --git a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/catalog/CatalogStatisticsTest.java b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/catalog/CatalogStatisticsTest.java
index a4b38b137d4..2b5af2b03b3 100644
--- a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/catalog/CatalogStatisticsTest.java
+++ b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/catalog/CatalogStatisticsTest.java
@@ -167,7 +167,7 @@ public class CatalogStatisticsTest {
         assertThat(mq.getAverageColumnSizes(t1)).isEqualTo(Arrays.asList(8.0, 43.5));
 
         // long type
-        assertThat(mq.getDistinctRowCount(t1, ImmutableBitSet.of(0), null)).isEqualTo(46.0);
+        assertThat(mq.getDistinctRowCount(t1, ImmutableBitSet.of(0), null)).isEqualTo(23.0);
         assertThat(mq.getColumnNullCount(t1, 0)).isEqualTo(154.0);
         assertThat(mq.getColumnInterval(t1, 0))
                 .isEqualTo(
@@ -178,7 +178,7 @@ public class CatalogStatisticsTest {
                                 true));
 
         // string type
-        assertThat(mq.getDistinctRowCount(t1, ImmutableBitSet.of(1), null)).isEqualTo(40.0);
+        assertThat(mq.getDistinctRowCount(t1, ImmutableBitSet.of(1), null)).isEqualTo(20.0);
         assertThat(mq.getColumnNullCount(t1, 1)).isEqualTo(0.0);
         assertThat(mq.getColumnInterval(t1, 1)).isNull();
     }
@@ -200,22 +200,16 @@ public class CatalogStatisticsTest {
         FlinkRelMetadataQuery mq =
                 FlinkRelMetadataQuery.reuseOrCreate(t1.getCluster().getMetadataQuery());
         assertThat(mq.getRowCount(t1)).isEqualTo(100_000_000);
-        assertThat(mq.getAverageColumnSizes(t1)).isEqualTo(Arrays.asList(8.0, 43.5));
+        assertThat(mq.getAverageColumnSizes(t1)).isEqualTo(Arrays.asList(4.0, 12.0));
 
         // long type
-        assertThat(mq.getDistinctRowCount(t1, ImmutableBitSet.of(0), null)).isEqualTo(46.0);
-        assertThat(mq.getColumnNullCount(t1, 0)).isEqualTo(154.0);
-        assertThat(mq.getColumnInterval(t1, 0))
-                .isEqualTo(
-                        ValueInterval$.MODULE$.apply(
-                                BigDecimal.valueOf(-123L),
-                                BigDecimal.valueOf(763322L),
-                                true,
-                                true));
+        assertThat(mq.getDistinctRowCount(t1, ImmutableBitSet.of(0), null)).isNull();
+        assertThat(mq.getColumnNullCount(t1, 0)).isNull();
+        assertThat(mq.getColumnInterval(t1, 0)).isNull();
 
         // string type
-        assertThat(mq.getDistinctRowCount(t1, ImmutableBitSet.of(1), null)).isEqualTo(40.0);
-        assertThat(mq.getColumnNullCount(t1, 1)).isEqualTo(0.0);
+        assertThat(mq.getDistinctRowCount(t1, ImmutableBitSet.of(1), null)).isNull();
+        assertThat(mq.getColumnNullCount(t1, 1)).isNull();
         assertThat(mq.getColumnInterval(t1, 1)).isNull();
     }
 
