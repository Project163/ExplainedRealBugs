diff --git a/flink-optimizer/src/main/java/org/apache/flink/optimizer/Optimizer.java b/flink-optimizer/src/main/java/org/apache/flink/optimizer/Optimizer.java
index c4d70f62e4d..a2b78edde1e 100644
--- a/flink-optimizer/src/main/java/org/apache/flink/optimizer/Optimizer.java
+++ b/flink-optimizer/src/main/java/org/apache/flink/optimizer/Optimizer.java
@@ -29,9 +29,6 @@ import org.apache.flink.optimizer.traversals.GraphCreatingVisitor;
 import org.apache.flink.optimizer.traversals.IdAndEstimatesVisitor;
 import org.apache.flink.optimizer.traversals.InterestingPropertyVisitor;
 import org.apache.flink.optimizer.traversals.PlanFinalizer;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
 import org.apache.flink.api.common.ExecutionMode;
 import org.apache.flink.api.common.Plan;
 import org.apache.flink.optimizer.costs.CostEstimator;
@@ -39,7 +36,6 @@ import org.apache.flink.optimizer.costs.DefaultCostEstimator;
 import org.apache.flink.optimizer.dag.DataSinkNode;
 import org.apache.flink.optimizer.dag.OptimizerNode;
 import org.apache.flink.optimizer.dag.SinkJoiner;
-import org.apache.flink.optimizer.deadlockdetect.DeadlockPreventer;
 import org.apache.flink.optimizer.plan.OptimizedPlan;
 import org.apache.flink.optimizer.plan.PlanNode;
 import org.apache.flink.optimizer.plan.SinkJoinerPlanNode;
@@ -48,6 +44,9 @@ import org.apache.flink.optimizer.postpass.OptimizerPostPass;
 import org.apache.flink.configuration.ConfigConstants;
 import org.apache.flink.util.InstantiationUtil;
 
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
 /**
  * The optimizer that takes the user specified program plan and creates an optimized plan that contains
  * exact descriptions about how the physical execution will take place. It first translates the user
@@ -514,9 +513,6 @@ public class Optimizer {
 		} else if (bestPlanRoot instanceof SinkJoinerPlanNode) {
 			((SinkJoinerPlanNode) bestPlanRoot).getDataSinks(bestPlanSinks);
 		}
-		
-		DeadlockPreventer dp = new DeadlockPreventer();
-		dp.resolveDeadlocks(bestPlanSinks);
 
 		// finalize the plan
 		OptimizedPlan plan = new PlanFinalizer().createFinalPlan(bestPlanSinks, program.getJobName(), program);
diff --git a/flink-optimizer/src/main/java/org/apache/flink/optimizer/dag/OptimizerNode.java b/flink-optimizer/src/main/java/org/apache/flink/optimizer/dag/OptimizerNode.java
index 6bf43ea277b..9688bb81dfc 100644
--- a/flink-optimizer/src/main/java/org/apache/flink/optimizer/dag/OptimizerNode.java
+++ b/flink-optimizer/src/main/java/org/apache/flink/optimizer/dag/OptimizerNode.java
@@ -86,7 +86,7 @@ public abstract class OptimizerNode implements Visitable<OptimizerNode>, Estimat
 	protected Set<OptimizerNode> closedBranchingNodes; 	// stack of branching nodes which have already been closed
 	
 	protected List<OptimizerNode> hereJoinedBranches;	// the branching nodes (node with multiple outputs)
-	// 										that are partially joined (through multiple inputs or broadcast vars)
+														// that are partially joined (through multiple inputs or broadcast vars)
 
 	// ---------------------------- Estimates and Annotations -------------------------------------
 	
diff --git a/flink-optimizer/src/main/java/org/apache/flink/optimizer/dag/SingleInputNode.java b/flink-optimizer/src/main/java/org/apache/flink/optimizer/dag/SingleInputNode.java
index 61bee584b3c..5691d194ec1 100644
--- a/flink-optimizer/src/main/java/org/apache/flink/optimizer/dag/SingleInputNode.java
+++ b/flink-optimizer/src/main/java/org/apache/flink/optimizer/dag/SingleInputNode.java
@@ -433,7 +433,7 @@ public abstract class SingleInputNode extends OptimizerNode {
 				}
 				
 				// check if there is a common predecessor and whether there is a dam on the way to all common predecessors
-				if (this.hereJoinedBranches != null) {
+				if (in.isOnDynamicPath() && this.hereJoinedBranches != null) {
 					for (OptimizerNode brancher : this.hereJoinedBranches) {
 						PlanNode candAtBrancher = in.getSource().getCandidateAtBranchPoint(brancher);
 						
diff --git a/flink-optimizer/src/main/java/org/apache/flink/optimizer/dag/TwoInputNode.java b/flink-optimizer/src/main/java/org/apache/flink/optimizer/dag/TwoInputNode.java
index 76b03c1c063..a4199a80fdb 100644
--- a/flink-optimizer/src/main/java/org/apache/flink/optimizer/dag/TwoInputNode.java
+++ b/flink-optimizer/src/main/java/org/apache/flink/optimizer/dag/TwoInputNode.java
@@ -61,14 +61,15 @@ import org.apache.flink.util.Visitor;
 import com.google.common.collect.Sets;
 
 /**
- * A node in the optimizer plan that represents a PACT with a two different inputs, such as MATCH or CROSS.
+ * A node in the optimizer plan that represents an operator with a two different inputs, such as Join,
+ * Cross, CoGroup, or Union.
  * The two inputs are not substitutable in their sides.
  */
 public abstract class TwoInputNode extends OptimizerNode {
 	
-	protected final FieldList keys1; // The set of key fields for the first input
+	protected final FieldList keys1; // The set of key fields for the first input. may be null.
 	
-	protected final FieldList keys2; // The set of key fields for the second input
+	protected final FieldList keys2; // The set of key fields for the second input. may be null.
 	
 	protected DagConnection input1; // The first input edge
 
@@ -79,16 +80,15 @@ public abstract class TwoInputNode extends OptimizerNode {
 	// --------------------------------------------------------------------------------------------
 	
 	/**
-	 * Creates a new node with a single input for the optimizer plan.
+	 * Creates a new two input node for the optimizer plan, representing the given operator.
 	 * 
-	 * @param pactContract
-	 *        The PACT that the node represents.
+	 * @param operator The operator that the optimizer DAG node should represent.
 	 */
-	public TwoInputNode(DualInputOperator<?, ?, ?, ?> pactContract) {
-		super(pactContract);
+	public TwoInputNode(DualInputOperator<?, ?, ?, ?> operator) {
+		super(operator);
 
-		int[] k1 = pactContract.getKeyColumns(0);
-		int[] k2 = pactContract.getKeyColumns(1);
+		int[] k1 = operator.getKeyColumns(0);
+		int[] k2 = operator.getKeyColumns(1);
 		
 		this.keys1 = k1 == null || k1.length == 0 ? null : new FieldList(k1);
 		this.keys2 = k2 == null || k2.length == 0 ? null : new FieldList(k2);
@@ -114,7 +114,7 @@ public abstract class TwoInputNode extends OptimizerNode {
 	}
 
 	/**
-	 * Gets the <tt>PactConnection</tt> through which this node receives its <i>first</i> input.
+	 * Gets the DagConnection through which this node receives its <i>first</i> input.
 	 * 
 	 * @return The first input connection.
 	 */
@@ -123,7 +123,7 @@ public abstract class TwoInputNode extends OptimizerNode {
 	}
 
 	/**
-	 * Gets the <tt>PactConnection</tt> through which this node receives its <i>second</i> input.
+	 * Gets the DagConnection through which this node receives its <i>second</i> input.
 	 * 
 	 * @return The second input connection.
 	 */
@@ -630,7 +630,7 @@ public abstract class TwoInputNode extends OptimizerNode {
 	protected void placePipelineBreakersIfNecessary(DriverStrategy strategy, Channel in1, Channel in2) {
 		// before we instantiate, check for deadlocks by tracing back to the open branches and checking
 		// whether either no input, or all of them have a dam
-		if (this.hereJoinedBranches != null && this.hereJoinedBranches.size() > 0) {
+		if (in1.isOnDynamicPath() && in2.isOnDynamicPath() && this.hereJoinedBranches != null && this.hereJoinedBranches.size() > 0) {
 			boolean someDamOnLeftPaths = false;
 			boolean damOnAllLeftPaths = true;
 			boolean someDamOnRightPaths = false;
diff --git a/flink-optimizer/src/main/java/org/apache/flink/optimizer/deadlockdetect/DeadlockEdge.java b/flink-optimizer/src/main/java/org/apache/flink/optimizer/deadlockdetect/DeadlockEdge.java
deleted file mode 100644
index 707142f2e88..00000000000
--- a/flink-optimizer/src/main/java/org/apache/flink/optimizer/deadlockdetect/DeadlockEdge.java
+++ /dev/null
@@ -1,38 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-package org.apache.flink.optimizer.deadlockdetect;
-
-public class DeadlockEdge {
-	
-	private DeadlockVertex destination;
-	
-	public DeadlockEdge( DeadlockVertex d ){
-		destination = d;
-	}
-	
-	public DeadlockVertex getDestination() {
-		return destination;
-	}
-
-	public void setDestination(DeadlockVertex destination) {
-		this.destination = destination;
-	}
-
-}
diff --git a/flink-optimizer/src/main/java/org/apache/flink/optimizer/deadlockdetect/DeadlockGraph.java b/flink-optimizer/src/main/java/org/apache/flink/optimizer/deadlockdetect/DeadlockGraph.java
deleted file mode 100644
index cae75ad9233..00000000000
--- a/flink-optimizer/src/main/java/org/apache/flink/optimizer/deadlockdetect/DeadlockGraph.java
+++ /dev/null
@@ -1,133 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-package org.apache.flink.optimizer.deadlockdetect;
-
-import java.util.Collection;
-import java.util.HashSet;
-import java.util.LinkedList;
-import java.util.Queue;
-import java.util.Set;
-
-import org.apache.flink.optimizer.plan.PlanNode;
-
-public class DeadlockGraph {
-	
-	public Set<DeadlockVertex> vertices;
-	
-	public DeadlockGraph() {
-		this.vertices = new HashSet<DeadlockVertex>();
-	}
-	
-	public Set<DeadlockVertex> vertices() {
-		return vertices;
-	}
-	
-	public DeadlockVertex addVertex(PlanNode original) {
-		
-		DeadlockVertex v = new DeadlockVertex(original);
-		this.vertices.add(v);
-		return v;
-	}
-	
-	public void addEdge(PlanNode source, PlanNode destination) {
-		
-		DeadlockVertex dest = null;
-		for(DeadlockVertex v : vertices) {
-			if(v.getOriginal().equals(destination)) {
-				dest = v;
-			}
-		}
-		
-		for(DeadlockVertex v : vertices) {
-			if(v.getOriginal().equals(source)) {
-				v.addEdge(dest);
-			}
-		}
-		
-	}
-	
-	public long size() {
-		return vertices.size();
-	}
-	
-	public String toString() {
-		StringBuilder out = new StringBuilder();
-		out.append("------------ GRAPH ------------\n");
-		for (DeadlockVertex n : vertices) {
-			out.append("Node " +n+"_\n");
-			for(DeadlockEdge a: n.getOutEdges()) {
-				out.append("\t->"+a.getDestination()+"\n");
-			}
-			out.append("\n");
-		}
-		
-		return out.toString();
-	}
-
-	public boolean hasCycle() {
-		
-	Collection <DeadlockVertex> vertexCollect = this.vertices();
-	
-	Queue <DeadlockVertex> q; // Queue will store vertices that have in-degree of zero
-	
-	// Calculate the in-degree of all vertices
-	for (DeadlockVertex v: vertexCollect) {
-		v.setInDegree(0);
-	}
-	
-	for (DeadlockVertex v: vertexCollect) {
-		for(DeadlockEdge edge : v.getOutEdges()) {
-			edge.getDestination().setInDegree(edge.getDestination().getInDegree()+1);
-		}
-	}
-	
-	// Find all vertices with in-degree == 0 and put in queue 
-	q = new LinkedList<DeadlockVertex>();
-	for (DeadlockVertex v : vertexCollect) {
-		if (v.getInDegree() == 0) {
-			q.offer(v);
-		}
-	}
-	
-	while (!q.isEmpty()) {
-		
-		DeadlockVertex v = q.poll();
-		this.vertices.remove(v);
-		
-		for (DeadlockEdge e: v.getOutEdges()) {
-			
-			DeadlockVertex w = e.getDestination();
-			w.setInDegree(w.getInDegree() - 1);
-			
-			if(w.getInDegree() == 0) {
-				q.offer(w);
-			}
-		}
-	}
-	
-	if (!vertexCollect.isEmpty() ){
-		return true;  //Cycle found
-	}
-	
-	return false;
-	}
-
-	
-}
diff --git a/flink-optimizer/src/main/java/org/apache/flink/optimizer/deadlockdetect/DeadlockPreventer.java b/flink-optimizer/src/main/java/org/apache/flink/optimizer/deadlockdetect/DeadlockPreventer.java
deleted file mode 100644
index 19ee6c5137f..00000000000
--- a/flink-optimizer/src/main/java/org/apache/flink/optimizer/deadlockdetect/DeadlockPreventer.java
+++ /dev/null
@@ -1,211 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-package org.apache.flink.optimizer.deadlockdetect;
-
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.flink.optimizer.plan.BulkIterationPlanNode;
-import org.apache.flink.optimizer.plan.DualInputPlanNode;
-import org.apache.flink.optimizer.plan.PlanNode;
-import org.apache.flink.optimizer.plan.SingleInputPlanNode;
-import org.apache.flink.optimizer.plan.WorksetIterationPlanNode;
-import org.apache.flink.runtime.operators.DamBehavior;
-import org.apache.flink.runtime.operators.DriverStrategy;
-import org.apache.flink.util.Visitor;
-
-/**
- * 	Certain pipelined flows may lead to deadlocks, in which case we need to make sure the pipelines are broken or made elastic enough to prevent that.
- *
- *  This is only relevat to pipelined data flows where one operator has more than one consumers (successors in the flow).
- *	
- *	Most cases are caught by the general logic that deals with branching/joining flows. The following cases need additional checks:
- *
- *                    <build>
- *  (source1) ------ (join)
- *	          \    /    <probe>
- *	           \  /
- *	            \/
- *	            /\
- *	           /  \
- *	          /    \    <probe>
- *	(source2) ------(join)
- *	                    <build>
- *	
- *	Since both sources pipeline their data into a build and a probe side, they get stalled by the back pressure from the probe side (which waits for the build side to complete) and never finish the build side.
- *	
- *	We can model dependencies of pipelined / materialized connections and do a reguar cyclic dependencies check to detect such situations. Pipelined connections have a dependency from sender to receiver, non-pipelined (fully dammed) connections have a dependency from receiver to sender.
- *
- */
-public class DeadlockPreventer implements Visitor<PlanNode> {
-	
-	private DeadlockGraph g;
-	
-	public DeadlockPreventer() {
-		this.g = new DeadlockGraph();
-	}
-
-	public void resolveDeadlocks(List<? extends PlanNode> sinks) {
-
-		for(PlanNode s : sinks) {
-			s.accept(this);
-		}
-		if(g.hasCycle()) {
-			
-			// in the remaining plan is a cycle
-			for(DeadlockVertex v : g.vertices) {
-
-				// first strategy to fix -> swap build and probe side
-				if(v.getOriginal().getDriverStrategy().equals(DriverStrategy.HYBRIDHASH_BUILD_FIRST)) {
-
-					v.getOriginal().setDriverStrategy(DriverStrategy.HYBRIDHASH_BUILD_SECOND);
-					
-					if(hasDeadlock(sinks)) {
-						// Didn't fix anything -> revert
-						v.getOriginal().setDriverStrategy(DriverStrategy.HYBRIDHASH_BUILD_FIRST);
-					}
-					else {
-						// deadlock resolved
-						break;
-					}
-				}
-				
-				// other direction
-				if(v.getOriginal().getDriverStrategy().equals(DriverStrategy.HYBRIDHASH_BUILD_SECOND)) {
-					
-					v.getOriginal().setDriverStrategy(DriverStrategy.HYBRIDHASH_BUILD_FIRST);
-					
-					if(hasDeadlock(sinks)) {
-						// Didn't fix anything -> revert
-						v.getOriginal().setDriverStrategy(DriverStrategy.HYBRIDHASH_BUILD_SECOND);
-					}
-					else {
-						// deadlock resolved
-						break;
-					}
-				}
-			}
-			
-			// switching build and probe side did not help -> pipeline breaker
-			for(DeadlockVertex v : g.vertices) {
-				if(v.getOriginal() instanceof DualInputPlanNode) {
-					DualInputPlanNode n = (DualInputPlanNode) v.getOriginal();
-					
-					// what is the pipelined side? (other side should be a dam, otherwise operator could not be source of deadlock)
-					if(!(n.getDriverStrategy().firstDam().equals(DamBehavior.FULL_DAM) || n.getInput1().getLocalStrategy().dams() || n.getInput1().getTempMode().breaksPipeline())
-							&& (n.getDriverStrategy().secondDam().equals(DamBehavior.FULL_DAM) || n.getInput2().getLocalStrategy().dams() || n.getInput2().getTempMode().breaksPipeline())) {
-						n.getInput1().setTempMode(n.getInput1().getTempMode().makePipelineBreaker());
-					}
-					else if( !(n.getDriverStrategy().secondDam().equals(DamBehavior.FULL_DAM) || n.getInput2().getLocalStrategy().dams() || n.getInput2().getTempMode().breaksPipeline())
-							&& (n.getDriverStrategy().firstDam().equals(DamBehavior.FULL_DAM) || n.getInput1().getLocalStrategy().dams() || n.getInput1().getTempMode().breaksPipeline())) {
-						n.getInput2().setTempMode(n.getInput2().getTempMode().makePipelineBreaker());
-					}
-					
-					// Deadlock resolved?
-					if(!hasDeadlock(sinks)) {
-						break;
-					}
-				}
-			}
-		}
-	
-	}
-	
-	/**
-	 * Creates new DeadlockGraph from plan and checks for cycles
-	 */
-	public boolean hasDeadlock(List<? extends PlanNode> sinks) {
-		this.g = new DeadlockGraph();
-		
-		for(PlanNode s : sinks) {
-			s.accept(this);
-		}
-		
-		if(g.hasCycle()) {
-			return true;
-		} else {
-			return false;
-		}
-	}
-
-	@Override
-	public boolean preVisit(PlanNode visitable) {
-		
-		g.addVertex(visitable);
-		return true;
-	}
-
-	@Override
-	public void postVisit(PlanNode visitable) {
-		if(visitable instanceof SingleInputPlanNode) {
-			SingleInputPlanNode n = (SingleInputPlanNode) visitable;
-			
-			if(n.getDriverStrategy().firstDam().equals(DamBehavior.FULL_DAM) || n.getInput().getLocalStrategy().dams() || n.getInput().getTempMode().breaksPipeline()) {
-				g.addEdge(n, n.getPredecessor());
-			}
-			else {
-				g.addEdge(n.getPredecessor(), n);
-			}
-		}
-		else if(visitable instanceof DualInputPlanNode) {
-			DualInputPlanNode n = (DualInputPlanNode) visitable;
-			
-			if(n.getDriverStrategy().firstDam().equals(DamBehavior.FULL_DAM) || n.getInput1().getLocalStrategy().dams() || n.getInput1().getTempMode().breaksPipeline()) {
-				g.addEdge(n, n.getInput1().getSource());
-			}
-			else {
-				g.addEdge(n.getInput1().getSource(), n);
-			}
-			
-			if(!n.getDriverStrategy().equals(DriverStrategy.NONE) && (n.getDriverStrategy().secondDam().equals(DamBehavior.FULL_DAM) || n.getInput2().getLocalStrategy().dams() || n.getInput2().getTempMode().breaksPipeline())) {
-				g.addEdge(n, n.getInput2().getSource());
-			}
-			else {
-				g.addEdge(n.getInput2().getSource(), n);
-			}
-		}
-		
-		
-		// recursively fix iterations
-		if (visitable instanceof BulkIterationPlanNode) {
-			
-			DeadlockPreventer dp = new DeadlockPreventer();
-			List<PlanNode> planSinks = new ArrayList<PlanNode>(1);
-			
-			BulkIterationPlanNode pspn = (BulkIterationPlanNode) visitable;
-			planSinks.add(pspn.getRootOfStepFunction());
-			
-			dp.resolveDeadlocks(planSinks);
-			
-		}
-		else if (visitable instanceof WorksetIterationPlanNode) {
-			
-			DeadlockPreventer dp = new DeadlockPreventer();
-			List<PlanNode> planSinks = new ArrayList<PlanNode>(2);
-			
-			WorksetIterationPlanNode pspn = (WorksetIterationPlanNode) visitable;
-			planSinks.add(pspn.getSolutionSetDeltaPlanNode());
-			planSinks.add(pspn.getNextWorkSetPlanNode());
-			
-			dp.resolveDeadlocks(planSinks);
-			
-		}
-	}
-}
diff --git a/flink-optimizer/src/main/java/org/apache/flink/optimizer/deadlockdetect/DeadlockVertex.java b/flink-optimizer/src/main/java/org/apache/flink/optimizer/deadlockdetect/DeadlockVertex.java
deleted file mode 100644
index f26763e474d..00000000000
--- a/flink-optimizer/src/main/java/org/apache/flink/optimizer/deadlockdetect/DeadlockVertex.java
+++ /dev/null
@@ -1,98 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-package org.apache.flink.optimizer.deadlockdetect;
-
-import java.util.LinkedList;
-import java.util.List;
-
-import org.apache.flink.optimizer.plan.PlanNode;
-
-public class DeadlockVertex {
-	
-	private PlanNode original;
-	
-	private List<DeadlockEdge> outEdges;
-	
-	private int inDegree;
-	
-	public DeadlockVertex( PlanNode original ) {
-		this.original = original;
-		outEdges = new LinkedList<DeadlockEdge>();
-		inDegree = 0;
-	}
-	
-	public void addEdge(DeadlockVertex destination) {
-		
-		// no duplicates
-		for(DeadlockEdge e : outEdges) {
-			if(e.getDestination().equals(destination)) {
-				return;
-			}
-		}
-		
-		DeadlockEdge e = new DeadlockEdge(destination);
-		this.outEdges.add(e);
-	}
-	
-	public boolean equals(Object o) {
-		
-		if(!(o instanceof DeadlockVertex)) {
-			return false;
-		}
-		
-		DeadlockVertex v = (DeadlockVertex) o;
-		if(v.getOriginal().equals(this.getOriginal())) {
-			return true;
-		}
-		
-		return false;
-	}
-	
-	public int hashCode() {
-		return this.original.hashCode();
-	}
-	
-	public String toString() {
-		return original.toString();
-	}
-	public PlanNode getOriginal() {
-		return original;
-	}
-
-	public void setOriginal(PlanNode original) {
-		this.original = original;
-	}
-
-	public int getInDegree() {
-		return inDegree;
-	}
-
-	public void setInDegree(int inDegree) {
-		this.inDegree = inDegree;
-	}
-	
-	public List<DeadlockEdge> getOutEdges() {
-		return outEdges;
-	}
-
-	public void setOutEdges(List<DeadlockEdge> outEdges) {
-		this.outEdges = outEdges;
-	}
-}
diff --git a/flink-optimizer/src/test/java/org/apache/flink/optimizer/BroadcastVariablePipelinebreakerTest.java b/flink-optimizer/src/test/java/org/apache/flink/optimizer/BroadcastVariablePipelinebreakerTest.java
index b0ecfe56248..0b15c1e1e26 100644
--- a/flink-optimizer/src/test/java/org/apache/flink/optimizer/BroadcastVariablePipelinebreakerTest.java
+++ b/flink-optimizer/src/test/java/org/apache/flink/optimizer/BroadcastVariablePipelinebreakerTest.java
@@ -24,6 +24,7 @@ import static org.junit.Assert.*;
 import org.apache.flink.api.common.Plan;
 import org.apache.flink.api.java.io.DiscardingOutputFormat;
 import org.apache.flink.optimizer.util.CompilerTestBase;
+import org.apache.flink.runtime.io.network.DataExchangeMode;
 import org.junit.Test;
 import org.apache.flink.api.java.DataSet;
 import org.apache.flink.api.java.ExecutionEnvironment;
@@ -54,6 +55,10 @@ public class BroadcastVariablePipelinebreakerTest extends CompilerTestBase {
 			SingleInputPlanNode mapper = (SingleInputPlanNode) sink.getInput().getSource();
 			
 			assertEquals(TempMode.NONE, mapper.getInput().getTempMode());
+			assertEquals(TempMode.NONE, mapper.getBroadcastInputs().get(0).getTempMode());
+			
+			assertEquals(DataExchangeMode.PIPELINED, mapper.getInput().getDataExchangeMode());
+			assertEquals(DataExchangeMode.PIPELINED, mapper.getBroadcastInputs().get(0).getDataExchangeMode());
 		}
 		catch (Exception e) {
 			e.printStackTrace();
@@ -76,8 +81,15 @@ public class BroadcastVariablePipelinebreakerTest extends CompilerTestBase {
 				
 				SinkPlanNode sink = op.getDataSinks().iterator().next();
 				SingleInputPlanNode mapper = (SingleInputPlanNode) sink.getInput().getSource();
-				
-				assertEquals(TempMode.PIPELINE_BREAKER, mapper.getInput().getTempMode());
+				SingleInputPlanNode beforeMapper = (SingleInputPlanNode) mapper.getInput().getSource();
+
+				assertEquals(TempMode.NONE, mapper.getInput().getTempMode());
+				assertEquals(TempMode.NONE, beforeMapper.getInput().getTempMode());
+				assertEquals(TempMode.NONE, mapper.getBroadcastInputs().get(0).getTempMode());
+
+				assertEquals(DataExchangeMode.PIPELINED, mapper.getInput().getDataExchangeMode());
+				assertEquals(DataExchangeMode.BATCH, beforeMapper.getInput().getDataExchangeMode());
+				assertEquals(DataExchangeMode.BATCH, mapper.getBroadcastInputs().get(0).getDataExchangeMode());
 			}
 			catch (Exception e) {
 				e.printStackTrace();
diff --git a/flink-optimizer/src/test/java/org/apache/flink/optimizer/IterationsCompilerTest.java b/flink-optimizer/src/test/java/org/apache/flink/optimizer/IterationsCompilerTest.java
index 269be6edad6..bd5eb5676cc 100644
--- a/flink-optimizer/src/test/java/org/apache/flink/optimizer/IterationsCompilerTest.java
+++ b/flink-optimizer/src/test/java/org/apache/flink/optimizer/IterationsCompilerTest.java
@@ -20,10 +20,13 @@ package org.apache.flink.optimizer;
 
 import static org.junit.Assert.*;
 
+import org.apache.flink.optimizer.dag.TempMode;
+import org.apache.flink.runtime.io.network.DataExchangeMode;
+import org.junit.Test;
+
 import org.apache.flink.api.common.functions.RichFlatMapFunction;
 import org.apache.flink.api.java.io.DiscardingOutputFormat;
 import org.apache.flink.optimizer.util.CompilerTestBase;
-import org.junit.Test;
 import org.apache.flink.api.common.Plan;
 import org.apache.flink.api.java.DataSet;
 import org.apache.flink.api.java.operators.DeltaIteration;
@@ -50,7 +53,6 @@ import org.apache.flink.optimizer.testfunctions.IdentityMapper;
 import org.apache.flink.runtime.operators.shipping.ShipStrategyType;
 import org.apache.flink.util.Collector;
 
-
 @SuppressWarnings({"serial", "unchecked"})
 public class IterationsCompilerTest extends CompilerTestBase {
 
@@ -117,7 +119,12 @@ public class IterationsCompilerTest extends CompilerTestBase {
 			WorksetIterationPlanNode wipn = (WorksetIterationPlanNode) op.getDataSinks().iterator().next().getInput().getSource();
 			
 			assertEquals(ShipStrategyType.PARTITION_HASH, wipn.getInput1().getShipStrategy());
-			assertTrue(wipn.getInput2().getTempMode().breaksPipeline());
+			
+			assertEquals(TempMode.NONE, wipn.getInput1().getTempMode());
+			assertEquals(TempMode.NONE, wipn.getInput2().getTempMode());
+
+			assertEquals(DataExchangeMode.BATCH, wipn.getInput1().getDataExchangeMode());
+			assertEquals(DataExchangeMode.BATCH, wipn.getInput2().getDataExchangeMode());
 			
 			new JobGraphGenerator().compileJobGraph(op);
 		}
@@ -152,7 +159,12 @@ public class IterationsCompilerTest extends CompilerTestBase {
 			WorksetIterationPlanNode wipn = (WorksetIterationPlanNode) op.getDataSinks().iterator().next().getInput().getSource();
 			
 			assertEquals(ShipStrategyType.PARTITION_HASH, wipn.getInput1().getShipStrategy());
-			assertTrue(wipn.getInput2().getTempMode().breaksPipeline());
+
+			assertEquals(DataExchangeMode.BATCH, wipn.getInput1().getDataExchangeMode());
+			assertEquals(DataExchangeMode.BATCH, wipn.getInput2().getDataExchangeMode());
+			
+			assertEquals(TempMode.NONE, wipn.getInput1().getTempMode());
+			assertEquals(TempMode.NONE, wipn.getInput2().getTempMode());
 			
 			new JobGraphGenerator().compileJobGraph(op);
 		}
@@ -187,7 +199,12 @@ public class IterationsCompilerTest extends CompilerTestBase {
 			WorksetIterationPlanNode wipn = (WorksetIterationPlanNode) op.getDataSinks().iterator().next().getInput().getSource();
 			
 			assertEquals(ShipStrategyType.FORWARD, wipn.getInput1().getShipStrategy());
-			assertTrue(wipn.getInput2().getTempMode().breaksPipeline());
+
+			assertEquals(DataExchangeMode.BATCH, wipn.getInput1().getDataExchangeMode());
+			assertEquals(DataExchangeMode.BATCH, wipn.getInput2().getDataExchangeMode());
+
+			assertEquals(TempMode.NONE, wipn.getInput1().getTempMode());
+			assertEquals(TempMode.NONE, wipn.getInput2().getTempMode());
 			
 			new JobGraphGenerator().compileJobGraph(op);
 		}
diff --git a/flink-optimizer/src/test/java/org/apache/flink/optimizer/PipelineBreakerTest.java b/flink-optimizer/src/test/java/org/apache/flink/optimizer/PipelineBreakerTest.java
index 68e8a4152b0..204014c171b 100644
--- a/flink-optimizer/src/test/java/org/apache/flink/optimizer/PipelineBreakerTest.java
+++ b/flink-optimizer/src/test/java/org/apache/flink/optimizer/PipelineBreakerTest.java
@@ -20,12 +20,14 @@ package org.apache.flink.optimizer;
 
 import static org.junit.Assert.*;
 
+import org.apache.flink.api.common.ExecutionMode;
 import org.apache.flink.api.java.io.DiscardingOutputFormat;
 import org.apache.flink.api.java.tuple.Tuple2;
+import org.apache.flink.optimizer.dag.TempMode;
 import org.apache.flink.optimizer.testfunctions.IdentityMapper;
 import org.apache.flink.optimizer.testfunctions.SelectOneReducer;
 import org.apache.flink.optimizer.util.CompilerTestBase;
-import org.junit.Test;
+import org.apache.flink.runtime.io.network.DataExchangeMode;
 import org.apache.flink.api.common.Plan;
 import org.apache.flink.api.java.DataSet;
 import org.apache.flink.api.java.ExecutionEnvironment;
@@ -37,6 +39,13 @@ import org.apache.flink.optimizer.plan.SingleInputPlanNode;
 import org.apache.flink.optimizer.plan.SinkPlanNode;
 import org.apache.flink.configuration.Configuration;
 
+import org.junit.Test;
+
+/**
+ * Tests in this class validate that the {@link ExecutionMode#PIPELINED} execution mode
+ * properly sets batch data exchanges, to guard against deadlocks, but does not place
+ * pipeline breakers.
+ */
 @SuppressWarnings("serial")
 public class PipelineBreakerTest extends CompilerTestBase {
 
@@ -44,6 +53,7 @@ public class PipelineBreakerTest extends CompilerTestBase {
 	public void testPipelineBreakerWithBroadcastVariable() {
 		try {
 			ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
+			env.getConfig().setExecutionMode(ExecutionMode.PIPELINED);
 			env.setParallelism(64);
 			
 			DataSet<Long> source = env.generateSequence(1, 10).map(new IdentityMapper<Long>());
@@ -54,13 +64,20 @@ public class PipelineBreakerTest extends CompilerTestBase {
 			
 			result.output(new DiscardingOutputFormat<Long>());
 			
+			
 			Plan p = env.createProgramPlan();
 			OptimizedPlan op = compileNoStats(p);
 			
 			SinkPlanNode sink = op.getDataSinks().iterator().next();
 			SingleInputPlanNode mapper = (SingleInputPlanNode) sink.getInput().getSource();
+			SingleInputPlanNode mapperInput = (SingleInputPlanNode) mapper.getInput().getSource();
+			
+			assertEquals(TempMode.NONE, mapper.getInput().getTempMode());
+			assertEquals(TempMode.NONE, mapper.getBroadcastInputs().get(0).getTempMode());
+			
+			assertEquals(DataExchangeMode.BATCH, mapperInput.getInput().getDataExchangeMode());
+			assertEquals(DataExchangeMode.BATCH, mapper.getBroadcastInputs().get(0).getDataExchangeMode());
 			
-			assertTrue(mapper.getInput().getTempMode().breaksPipeline());
 		}
 		catch (Exception e) {
 			e.printStackTrace();
@@ -72,6 +89,7 @@ public class PipelineBreakerTest extends CompilerTestBase {
 	public void testPipelineBreakerBroadcastedAllReduce() {
 		try {
 			ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
+			env.getConfig().setExecutionMode(ExecutionMode.PIPELINED);
 			env.setParallelism(64);
 			
 			DataSet<Long> sourceWithMapper = env.generateSequence(1, 10).map(new IdentityMapper<Long>());
@@ -93,22 +111,42 @@ public class PipelineBreakerTest extends CompilerTestBase {
 			
 			SinkPlanNode sink = op.getDataSinks().iterator().next();
 			SingleInputPlanNode mapper = (SingleInputPlanNode) sink.getInput().getSource();
-			
-			assertTrue(mapper.getInput().getTempMode().breaksPipeline());
+
+			assertEquals(TempMode.NONE, mapper.getInput().getTempMode());
+			assertEquals(DataExchangeMode.BATCH, mapper.getInput().getDataExchangeMode());
 		}
 		catch (Exception e) {
 			e.printStackTrace();
 			fail(e.getMessage());
 		}
 	}
-	
+
+	/**
+	 * 
+	 * 
+	 * 
+	 * <pre>
+	 *                                +----------- ITERATION ---------+
+	 *                                |                               |
+	 *                               +--+                           +----+
+	 *  (source 1) ----------------->|PS| ------------ +        +-->|next|---> (sink)
+	 *                               +--+              | (BC)   |   +----+
+	 *                                |                V        |     |
+	 *  (source 2) --> (map) --+------|-----------> (MAPPER) ---+     |
+	 *                         |      |                ^              |
+	 *                         |      |                | (BC)         |
+	 *                         |      +----------------|--------------+
+	 *                         |                       |
+	 *                         +--(map) --> (reduce) --+
+	 * </pre>
+	 */
 	@Test
 	public void testPipelineBreakerBroadcastedPartialSolution() {
 		try {
 			ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
+			env.getConfig().setExecutionMode(ExecutionMode.PIPELINED);
 			env.setParallelism(64);
 			
-			
 			DataSet<Long> initialSource = env.generateSequence(1, 10);
 			IterativeDataSet<Long> iteration = initialSource.iterate(100);
 			
@@ -134,7 +172,8 @@ public class PipelineBreakerTest extends CompilerTestBase {
 			BulkIterationPlanNode iterationPlanNode = (BulkIterationPlanNode) sink.getInput().getSource();
 			SingleInputPlanNode mapper = (SingleInputPlanNode) iterationPlanNode.getRootOfStepFunction();
 			
-			assertTrue(mapper.getInput().getTempMode().breaksPipeline());
+			assertEquals(TempMode.CACHED, mapper.getInput().getTempMode());
+			assertEquals(DataExchangeMode.BATCH, mapper.getInput().getDataExchangeMode());
 		}
 		catch (Exception e) {
 			e.printStackTrace();
@@ -143,7 +182,7 @@ public class PipelineBreakerTest extends CompilerTestBase {
 	}
 	
 	@Test
-	public void testPilelineBreakerWithCross() {
+	public void testPipelineBreakerWithCross() {
 		try {
 			{
 				ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
@@ -158,13 +197,16 @@ public class PipelineBreakerTest extends CompilerTestBase {
 					.cross(initialSource).withParameters(conf)
 					.output(new DiscardingOutputFormat<Tuple2<Long, Long>>());
 				
-				
 				Plan p = env.createProgramPlan();
 				OptimizedPlan op = compileNoStats(p);
 				SinkPlanNode sink = op.getDataSinks().iterator().next();
-				DualInputPlanNode mapper = (DualInputPlanNode) sink.getInput().getSource();
+				DualInputPlanNode cross = (DualInputPlanNode) sink.getInput().getSource();
+				SingleInputPlanNode mapper = (SingleInputPlanNode) cross.getInput1().getSource();
 				
-				assertTrue(mapper.getInput1().getTempMode().breaksPipeline());
+				assertEquals(TempMode.NONE, mapper.getInput().getTempMode());
+				assertEquals(TempMode.NONE, cross.getInput2().getTempMode());
+				assertEquals(DataExchangeMode.BATCH, mapper.getInput().getDataExchangeMode());
+				assertEquals(DataExchangeMode.BATCH, cross.getInput2().getDataExchangeMode());
 			}
 			
 			{
@@ -185,9 +227,13 @@ public class PipelineBreakerTest extends CompilerTestBase {
 				OptimizedPlan op = compileNoStats(p);
 				
 				SinkPlanNode sink = op.getDataSinks().iterator().next();
-				DualInputPlanNode mapper = (DualInputPlanNode) sink.getInput().getSource();
-				
-				assertTrue(mapper.getInput2().getTempMode().breaksPipeline());
+				DualInputPlanNode cross = (DualInputPlanNode) sink.getInput().getSource();
+				SingleInputPlanNode mapper = (SingleInputPlanNode) cross.getInput1().getSource();
+
+				assertEquals(TempMode.NONE, mapper.getInput().getTempMode());
+				assertEquals(TempMode.NONE, cross.getInput2().getTempMode());
+				assertEquals(DataExchangeMode.BATCH, mapper.getInput().getDataExchangeMode());
+				assertEquals(DataExchangeMode.BATCH, cross.getInput2().getDataExchangeMode());
 			}
 			
 			{
@@ -208,9 +254,13 @@ public class PipelineBreakerTest extends CompilerTestBase {
 				OptimizedPlan op = compileNoStats(p);
 				
 				SinkPlanNode sink = op.getDataSinks().iterator().next();
-				DualInputPlanNode mapper = (DualInputPlanNode) sink.getInput().getSource();
-				
-				assertTrue(mapper.getInput1().getTempMode().breaksPipeline());
+				DualInputPlanNode cross = (DualInputPlanNode) sink.getInput().getSource();
+				SingleInputPlanNode mapper = (SingleInputPlanNode) cross.getInput1().getSource();
+
+				assertEquals(TempMode.NONE, mapper.getInput().getTempMode());
+				assertEquals(TempMode.NONE, cross.getInput2().getTempMode());
+				assertEquals(DataExchangeMode.BATCH, mapper.getInput().getDataExchangeMode());
+				assertEquals(DataExchangeMode.BATCH, cross.getInput2().getDataExchangeMode());
 			}
 			
 			{
@@ -231,9 +281,13 @@ public class PipelineBreakerTest extends CompilerTestBase {
 				OptimizedPlan op = compileNoStats(p);
 				
 				SinkPlanNode sink = op.getDataSinks().iterator().next();
-				DualInputPlanNode mapper = (DualInputPlanNode) sink.getInput().getSource();
-				
-				assertTrue(mapper.getInput2().getTempMode().breaksPipeline());
+				DualInputPlanNode cross = (DualInputPlanNode) sink.getInput().getSource();
+				SingleInputPlanNode mapper = (SingleInputPlanNode) cross.getInput1().getSource();
+
+				assertEquals(TempMode.NONE, mapper.getInput().getTempMode());
+				assertEquals(TempMode.NONE, cross.getInput2().getTempMode());
+				assertEquals(DataExchangeMode.BATCH, mapper.getInput().getDataExchangeMode());
+				assertEquals(DataExchangeMode.BATCH, cross.getInput2().getDataExchangeMode());
 			}
 		}
 		catch (Exception e) {
diff --git a/flink-tests/src/test/java/org/apache/flink/test/compiler/iterations/ConnectedComponentsTest.java b/flink-optimizer/src/test/java/org/apache/flink/optimizer/programs/ConnectedComponentsTest.java
similarity index 65%
rename from flink-tests/src/test/java/org/apache/flink/test/compiler/iterations/ConnectedComponentsTest.java
rename to flink-optimizer/src/test/java/org/apache/flink/optimizer/programs/ConnectedComponentsTest.java
index 05e60bdc5dc..3c350b1ad58 100644
--- a/flink-tests/src/test/java/org/apache/flink/test/compiler/iterations/ConnectedComponentsTest.java
+++ b/flink-optimizer/src/test/java/org/apache/flink/optimizer/programs/ConnectedComponentsTest.java
@@ -16,23 +16,17 @@
  * limitations under the License.
  */
 
-package org.apache.flink.test.compiler.iterations;
+package org.apache.flink.optimizer.programs;
 
-import java.io.Serializable;
-
-import org.apache.flink.api.common.ExecutionConfig;
 import org.apache.flink.api.common.Plan;
+import org.apache.flink.api.common.functions.FlatJoinFunction;
+import org.apache.flink.api.common.functions.MapFunction;
 import org.apache.flink.api.common.operators.util.FieldList;
-import org.apache.flink.api.java.record.functions.JoinFunction;
-import org.apache.flink.api.java.record.functions.FunctionAnnotation.ConstantFieldsSecond;
-import org.apache.flink.api.java.record.io.CsvInputFormat;
-import org.apache.flink.api.java.record.io.CsvOutputFormat;
-import org.apache.flink.api.java.record.operators.DeltaIteration;
-import org.apache.flink.api.java.record.operators.FileDataSink;
-import org.apache.flink.api.java.record.operators.FileDataSource;
-import org.apache.flink.api.java.record.operators.JoinOperator;
-import org.apache.flink.api.java.record.operators.MapOperator;
-import org.apache.flink.api.java.record.operators.ReduceOperator;
+import org.apache.flink.api.java.DataSet;
+import org.apache.flink.api.java.ExecutionEnvironment;
+import org.apache.flink.api.java.io.DiscardingOutputFormat;
+import org.apache.flink.api.java.operators.DeltaIteration;
+import org.apache.flink.api.java.tuple.Tuple2;
 import org.apache.flink.optimizer.dag.TempMode;
 import org.apache.flink.optimizer.plan.DualInputPlanNode;
 import org.apache.flink.optimizer.plan.OptimizedPlan;
@@ -40,23 +34,18 @@ import org.apache.flink.optimizer.plan.SingleInputPlanNode;
 import org.apache.flink.optimizer.plan.SinkPlanNode;
 import org.apache.flink.optimizer.plan.SourcePlanNode;
 import org.apache.flink.optimizer.plan.WorksetIterationPlanNode;
-import org.apache.flink.optimizer.plandump.PlanJSONDumpGenerator;
 import org.apache.flink.optimizer.plantranslate.JobGraphGenerator;
+import org.apache.flink.runtime.io.network.DataExchangeMode;
 import org.apache.flink.runtime.operators.DriverStrategy;
 import org.apache.flink.runtime.operators.shipping.ShipStrategyType;
 import org.apache.flink.runtime.operators.util.LocalStrategy;
 import org.apache.flink.optimizer.util.CompilerTestBase;
-import org.apache.flink.test.recordJobs.graph.WorksetConnectedComponents;
-import org.apache.flink.test.recordJobs.graph.WorksetConnectedComponents.DuplicateLongMap;
-import org.apache.flink.test.recordJobs.graph.WorksetConnectedComponents.MinimumComponentIDReduce;
-import org.apache.flink.test.recordJobs.graph.WorksetConnectedComponents.NeighborWithComponentIDJoin;
-import org.apache.flink.types.LongValue;
-import org.apache.flink.types.Record;
 import org.apache.flink.util.Collector;
+
 import org.junit.Assert;
 import org.junit.Test;
 
-@SuppressWarnings("deprecation")
+@SuppressWarnings("serial")
 public class ConnectedComponentsTest extends CompilerTestBase {
 	
 	private static final String VERTEX_SOURCE = "Vertices";
@@ -70,28 +59,16 @@ public class ConnectedComponentsTest extends CompilerTestBase {
 	
 	private static final String SINK = "Result";
 	
-	private static final boolean PRINT_PLAN = false;
-	
 	private final FieldList set0 = new FieldList(0);
 	
 	
 	@Test
 	public void testWorksetConnectedComponents() {
-		WorksetConnectedComponents cc = new WorksetConnectedComponents();
-
-		Plan plan = cc.getPlan(String.valueOf(DEFAULT_PARALLELISM),
-				IN_FILE, IN_FILE, OUT_FILE, String.valueOf(100));
-		plan.setExecutionConfig(new ExecutionConfig());
+		Plan plan = getConnectedComponentsPlan(DEFAULT_PARALLELISM, 100, false);
 
 		OptimizedPlan optPlan = compileNoStats(plan);
 		OptimizerPlanNodeResolver or = getOptimizerPlanNodeResolver(optPlan);
 		
-		if (PRINT_PLAN) {
-			PlanJSONDumpGenerator dumper = new PlanJSONDumpGenerator();
-			String json = dumper.getOptimizerPlanAsJSON(optPlan);
-			System.out.println(json);
-		}
-		
 		SourcePlanNode vertexSource = or.getNode(VERTEX_SOURCE);
 		SourcePlanNode edgesSource = or.getNode(EDGES_SOURCE);
 		SinkPlanNode sink = or.getNode(SINK);
@@ -151,8 +128,11 @@ public class ConnectedComponentsTest extends CompilerTestBase {
 		Assert.assertEquals(LocalStrategy.NONE, updatingMatch.getInput2().getLocalStrategy()); // solution set
 		
 		// check the dams
-		Assert.assertTrue(TempMode.PIPELINE_BREAKER == iter.getInitialWorksetInput().getTempMode() ||
-							LocalStrategy.SORT == iter.getInitialWorksetInput().getLocalStrategy());
+		Assert.assertEquals(TempMode.NONE, iter.getInitialWorksetInput().getTempMode());
+		Assert.assertEquals(TempMode.NONE, iter.getInitialSolutionSetInput().getTempMode());
+
+		Assert.assertEquals(DataExchangeMode.BATCH, iter.getInitialWorksetInput().getDataExchangeMode());
+		Assert.assertEquals(DataExchangeMode.BATCH, iter.getInitialSolutionSetInput().getDataExchangeMode());
 		
 		JobGraphGenerator jgg = new JobGraphGenerator();
 		jgg.compileJobGraph(optPlan);
@@ -161,19 +141,11 @@ public class ConnectedComponentsTest extends CompilerTestBase {
 	@Test
 	public void testWorksetConnectedComponentsWithSolutionSetAsFirstInput() {
 
-		Plan plan = getPlanForWorksetConnectedComponentsWithSolutionSetAsFirstInput(DEFAULT_PARALLELISM,
-				IN_FILE, IN_FILE, OUT_FILE, 100);
-		plan.setExecutionConfig(new ExecutionConfig());
+		Plan plan = getConnectedComponentsPlan(DEFAULT_PARALLELISM, 100, true);
 
 		OptimizedPlan optPlan = compileNoStats(plan);
 		OptimizerPlanNodeResolver or = getOptimizerPlanNodeResolver(optPlan);
 		
-		if (PRINT_PLAN) {
-			PlanJSONDumpGenerator dumper = new PlanJSONDumpGenerator();
-			String json = dumper.getOptimizerPlanAsJSON(optPlan);
-			System.out.println(json);
-		}
-		
 		SourcePlanNode vertexSource = or.getNode(VERTEX_SOURCE);
 		SourcePlanNode edgesSource = or.getNode(EDGES_SOURCE);
 		SinkPlanNode sink = or.getNode(SINK);
@@ -233,82 +205,81 @@ public class ConnectedComponentsTest extends CompilerTestBase {
 		Assert.assertEquals(LocalStrategy.NONE, updatingMatch.getInput2().getLocalStrategy()); // solution set
 		
 		// check the dams
-		Assert.assertTrue(TempMode.PIPELINE_BREAKER == iter.getInitialWorksetInput().getTempMode() ||
-							LocalStrategy.SORT == iter.getInitialWorksetInput().getLocalStrategy());
+		Assert.assertEquals(TempMode.NONE, iter.getInitialWorksetInput().getTempMode());
+		Assert.assertEquals(TempMode.NONE, iter.getInitialSolutionSetInput().getTempMode());
+
+		Assert.assertEquals(DataExchangeMode.BATCH, iter.getInitialWorksetInput().getDataExchangeMode());
+		Assert.assertEquals(DataExchangeMode.BATCH, iter.getInitialSolutionSetInput().getDataExchangeMode());
 		
 		JobGraphGenerator jgg = new JobGraphGenerator();
 		jgg.compileJobGraph(optPlan);
 	}
 	
 	
-	@ConstantFieldsSecond(0)
-	public static final class UpdateComponentIdMatchMirrored extends JoinFunction implements Serializable {
-		
-		private static final long serialVersionUID = 1L;
+	private static Plan getConnectedComponentsPlan(int parallelism, int iterations, boolean solutionSetFirst) {
 
-		@Override
-		public void join(Record currentVertexWithComponent, Record newVertexWithComponent, Collector<Record> out){
-	
-			long candidateComponentID = newVertexWithComponent.getField(1, LongValue.class).getValue();
-			long currentComponentID = currentVertexWithComponent.getField(1, LongValue.class).getValue();
-	
-			if (candidateComponentID < currentComponentID) {
-				out.collect(newVertexWithComponent);
-			}
-		}
-	}
-	
-	@SuppressWarnings("unchecked")
-	private static Plan getPlanForWorksetConnectedComponentsWithSolutionSetAsFirstInput(
-			int numSubTasks, String verticesInput, String edgeInput, String output, int maxIterations)
-	{
-		// create DataSourceContract for the vertices
-		FileDataSource initialVertices = new FileDataSource(new CsvInputFormat(' ', LongValue.class), verticesInput, "Vertices");
-		
-		MapOperator verticesWithId = MapOperator.builder(DuplicateLongMap.class).input(initialVertices).name("Assign Vertex Ids").build();
-		
-		DeltaIteration iteration = new DeltaIteration(0, "Connected Components Iteration");
-		iteration.setInitialSolutionSet(verticesWithId);
-		iteration.setInitialWorkset(verticesWithId);
-		iteration.setMaximumNumberOfIterations(maxIterations);
-		
-		// create DataSourceContract for the edges
-		FileDataSource edges = new FileDataSource(new CsvInputFormat(' ', LongValue.class, LongValue.class), edgeInput, "Edges");
+		ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
+		env.setParallelism(parallelism);
+		
+		DataSet<Tuple2<Long, Long>> verticesWithId = env.generateSequence(0, 1000).name("Vertices")
+				.map(new MapFunction<Long, Tuple2<Long, Long>>() {
+					@Override
+					public Tuple2<Long, Long> map(Long value) {
+						return new Tuple2<Long, Long>(value, value);
+					}
+				}).name("Assign Vertex Ids");
 
-		// create CrossOperator for distance computation
-		JoinOperator joinWithNeighbors = JoinOperator.builder(new NeighborWithComponentIDJoin(), LongValue.class, 0, 0)
-				.input1(iteration.getWorkset())
-				.input2(edges)
-				.name("Join Candidate Id With Neighbor")
-				.build();
+		DeltaIteration<Tuple2<Long, Long>, Tuple2<Long, Long>> iteration = 
+				verticesWithId.iterateDelta(verticesWithId, iterations, 0).name("Connected Components Iteration");
 
-		// create ReduceOperator for finding the nearest cluster centers
-		ReduceOperator minCandidateId = ReduceOperator.builder(new MinimumComponentIDReduce(), LongValue.class, 0)
-				.input(joinWithNeighbors)
-				.name("Find Minimum Candidate Id")
-				.build();
-		
-		// create CrossOperator for distance computation
-		JoinOperator updateComponentId = JoinOperator.builder(new UpdateComponentIdMatchMirrored(), LongValue.class, 0, 0)
-				.input1(iteration.getSolutionSet())
-				.input2(minCandidateId)
-				.name("Update Component Id")
-				.build();
-		
-		iteration.setNextWorkset(updateComponentId);
-		iteration.setSolutionSetDelta(updateComponentId);
+		@SuppressWarnings("unchecked")
+		DataSet<Tuple2<Long, Long>> edges = env.fromElements(new Tuple2<Long, Long>(0L, 0L)).name("Edges");
 
-		// create DataSinkContract for writing the new cluster positions
-		FileDataSink result = new FileDataSink(new CsvOutputFormat(), output, iteration, "Result");
-		CsvOutputFormat.configureRecordFormat(result)
-			.recordDelimiter('\n')
-			.fieldDelimiter(' ')
-			.field(LongValue.class, 0)
-			.field(LongValue.class, 1);
+		DataSet<Tuple2<Long, Long>> minCandidateId = iteration.getWorkset().join(edges)
+				.where(0).equalTo(0)
+				.projectSecond(1).<Tuple2<Long, Long>>projectFirst(1).name("Join Candidate Id With Neighbor")
+				
+				.groupBy(0)
+				.min(1)
+				.name("Find Minimum Candidate Id");
 
-		// return the plan
-		Plan plan = new Plan(result, "Workset Connected Components");
-		plan.setDefaultParallelism(numSubTasks);
-		return plan;
+		DataSet<Tuple2<Long, Long>> updateComponentId;
+		
+		if (solutionSetFirst) {
+			updateComponentId = iteration.getSolutionSet().join(minCandidateId)
+					.where(0).equalTo(0)
+					.with(new FlatJoinFunction<Tuple2<Long, Long>, Tuple2<Long, Long>, Tuple2<Long, Long>>() {
+						@Override
+						public void join(Tuple2<Long, Long> current, Tuple2<Long, Long> candidate,
+										 Collector<Tuple2<Long, Long>> out) {
+							if (candidate.f1 < current.f1) {
+								out.collect(candidate);
+							}
+						}
+					})
+					.withForwardedFieldsFirst("0")
+					.withForwardedFieldsSecond("0")
+					.name("Update Component Id");
+		} else {
+			updateComponentId = minCandidateId.join(iteration.getSolutionSet())
+					.where(0).equalTo(0)
+					.with(new FlatJoinFunction<Tuple2<Long, Long>, Tuple2<Long, Long>, Tuple2<Long, Long>>() {
+						@Override
+						public void join(Tuple2<Long, Long> candidate, Tuple2<Long, Long> current,
+										 Collector<Tuple2<Long, Long>> out) {
+							if (candidate.f1 < current.f1) {
+								out.collect(candidate);
+							}
+						}
+					})
+					.withForwardedFieldsFirst("0")
+					.withForwardedFieldsSecond("0")
+					.name("Update Component Id");
+		}
+		
+		iteration.closeWith(updateComponentId, updateComponentId)
+				.output(new DiscardingOutputFormat<Tuple2<Long, Long>>()).name("Result");
+		
+		return env.createProgramPlan();
 	}
 }
