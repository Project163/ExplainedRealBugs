diff --git a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/deserializer/KafkaValueOnlyDeserializerWrapper.java b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/deserializer/KafkaValueOnlyDeserializerWrapper.java
index 0321c4bd68a..13c6e9abafe 100644
--- a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/deserializer/KafkaValueOnlyDeserializerWrapper.java
+++ b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/deserializer/KafkaValueOnlyDeserializerWrapper.java
@@ -39,14 +39,14 @@ class KafkaValueOnlyDeserializerWrapper<T> implements KafkaRecordDeserialization
     private static final long serialVersionUID = 5409547407386004054L;
     private static final Logger LOG =
             LoggerFactory.getLogger(KafkaValueOnlyDeserializerWrapper.class);
-    private final String deserializerClass;
+    private final Class<? extends Deserializer<T>> deserializerClass;
     private final Map<String, String> config;
 
     private transient Deserializer<T> deserializer;
 
     KafkaValueOnlyDeserializerWrapper(
             Class<? extends Deserializer<T>> deserializerClass, Map<String, String> config) {
-        this.deserializerClass = deserializerClass.getName();
+        this.deserializerClass = deserializerClass;
         this.config = config;
     }
 
@@ -59,7 +59,7 @@ class KafkaValueOnlyDeserializerWrapper<T> implements KafkaRecordDeserialization
             deserializer =
                     (Deserializer<T>)
                             InstantiationUtil.instantiate(
-                                    deserializerClass,
+                                    deserializerClass.getName(),
                                     Deserializer.class,
                                     getClass().getClassLoader());
             if (deserializer instanceof Configurable) {
@@ -93,7 +93,6 @@ class KafkaValueOnlyDeserializerWrapper<T> implements KafkaRecordDeserialization
 
     @Override
     public TypeInformation<T> getProducedType() {
-        return TypeExtractor.createTypeInfo(
-                Deserializer.class, deserializer.getClass(), 0, null, null);
+        return TypeExtractor.createTypeInfo(Deserializer.class, deserializerClass, 0, null, null);
     }
 }
diff --git a/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/source/KafkaSourceITCase.java b/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/source/KafkaSourceITCase.java
index 40fe4a8027d..25acf752cfb 100644
--- a/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/source/KafkaSourceITCase.java
+++ b/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/source/KafkaSourceITCase.java
@@ -27,6 +27,7 @@ import org.apache.flink.connector.kafka.source.reader.deserializer.KafkaRecordDe
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;
+import org.apache.flink.util.CloseableIterator;
 import org.apache.flink.util.Collector;
 
 import org.apache.kafka.clients.consumer.ConsumerRecord;
@@ -44,6 +45,7 @@ import java.util.Arrays;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
+import java.util.concurrent.atomic.AtomicInteger;
 
 import static org.junit.Assert.assertEquals;
 
@@ -83,6 +85,51 @@ public class KafkaSourceITCase {
         executeAndVerify(env, stream);
     }
 
+    @Test
+    public void testValueOnlyDeserializer() throws Exception {
+        KafkaSource<Integer> source =
+                KafkaSource.<Integer>builder()
+                        .setBootstrapServers(KafkaSourceTestEnv.brokerConnectionStrings)
+                        .setGroupId("testValueOnlyDeserializer")
+                        .setTopics(Arrays.asList(TOPIC1, TOPIC2))
+                        .setDeserializer(
+                                KafkaRecordDeserializationSchema.valueOnly(
+                                        IntegerDeserializer.class))
+                        .setStartingOffsets(OffsetsInitializer.earliest())
+                        .setBounded(OffsetsInitializer.latest())
+                        .build();
+
+        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
+        env.setParallelism(1);
+        final CloseableIterator<Integer> resultIterator =
+                env.fromSource(
+                                source,
+                                WatermarkStrategy.noWatermarks(),
+                                "testValueOnlyDeserializer")
+                        .executeAndCollect();
+
+        AtomicInteger actualSum = new AtomicInteger();
+        resultIterator.forEachRemaining(actualSum::addAndGet);
+
+        // Calculate the actual sum of values
+        // Values in a partition should start from partition ID, and end with
+        // (NUM_RECORDS_PER_PARTITION - 1)
+        // e.g. Values in partition 5 should be {5, 6, 7, 8, 9}
+        int expectedSum = 0;
+        for (int partition = 0; partition < KafkaSourceTestEnv.NUM_PARTITIONS; partition++) {
+            for (int value = partition;
+                    value < KafkaSourceTestEnv.NUM_RECORDS_PER_PARTITION;
+                    value++) {
+                expectedSum += value;
+            }
+        }
+
+        // Since we have two topics, the expected sum value should be doubled
+        expectedSum *= 2;
+
+        assertEquals(expectedSum, actualSum.get());
+    }
+
     // -----------------
 
     private static class PartitionAndValue implements Serializable {
