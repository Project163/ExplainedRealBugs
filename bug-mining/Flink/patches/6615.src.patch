diff --git a/flink-table/flink-sql-gateway-api/src/main/java/org/apache/flink/table/gateway/api/results/ColumnInfo.java b/flink-table/flink-sql-gateway-api/src/main/java/org/apache/flink/table/gateway/api/results/ColumnInfo.java
index 77b636489b0..876a6d67617 100644
--- a/flink-table/flink-sql-gateway-api/src/main/java/org/apache/flink/table/gateway/api/results/ColumnInfo.java
+++ b/flink-table/flink-sql-gateway-api/src/main/java/org/apache/flink/table/gateway/api/results/ColumnInfo.java
@@ -19,15 +19,15 @@
 package org.apache.flink.table.gateway.api.results;
 
 import org.apache.flink.annotation.PublicEvolving;
+import org.apache.flink.table.gateway.api.results.serde.LogicalTypeJsonDeserializer;
+import org.apache.flink.table.gateway.api.results.serde.LogicalTypeJsonSerializer;
 import org.apache.flink.table.types.logical.LogicalType;
-import org.apache.flink.table.types.logical.utils.LogicalTypeParser;
 import org.apache.flink.util.Preconditions;
 
 import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.annotation.JsonCreator;
-import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.annotation.JsonIgnore;
 import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.annotation.JsonProperty;
-
-import javax.annotation.Nullable;
+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.annotation.JsonDeserialize;
+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.annotation.JsonSerialize;
 
 import java.util.Objects;
 
@@ -36,41 +36,33 @@ import java.util.Objects;
 public class ColumnInfo {
 
     private static final String FIELD_NAME_NAME = "name";
-    private static final String FIELD_NAME_TYPE = "type";
+    private static final String FIELD_NAME_TYPE = "logicalType";
 
     @JsonProperty(FIELD_NAME_NAME)
-    private String name;
+    private final String name;
 
     @JsonProperty(FIELD_NAME_TYPE)
-    private String type;
-
-    @JsonIgnore @Nullable private LogicalType logicalType;
+    @JsonSerialize(using = LogicalTypeJsonSerializer.class)
+    @JsonDeserialize(using = LogicalTypeJsonDeserializer.class)
+    private final LogicalType logicalType;
 
     @JsonCreator
     public ColumnInfo(
             @JsonProperty(FIELD_NAME_NAME) String name,
-            @JsonProperty(FIELD_NAME_TYPE) String type) {
+            @JsonProperty(FIELD_NAME_TYPE) LogicalType logicalType) {
         this.name = Preconditions.checkNotNull(name, "name must not be null");
-        this.type = Preconditions.checkNotNull(type, "type must not be null");
+        this.logicalType = Preconditions.checkNotNull(logicalType, "logical type must not be null");
     }
 
     public static ColumnInfo create(String name, LogicalType type) {
-        return new ColumnInfo(name, type.toString());
+        return new ColumnInfo(name, type);
     }
 
     public String getName() {
         return name;
     }
 
-    public String getType() {
-        return type;
-    }
-
-    @JsonIgnore
     public LogicalType getLogicalType() {
-        if (logicalType == null) {
-            logicalType = LogicalTypeParser.parse(type);
-        }
         return logicalType;
     }
 
@@ -83,16 +75,17 @@ public class ColumnInfo {
             return false;
         }
         ColumnInfo that = (ColumnInfo) o;
-        return name.equals(that.name) && type.equals(that.type);
+        return name.equals(that.name) && logicalType.equals(that.logicalType);
     }
 
     @Override
     public int hashCode() {
-        return Objects.hash(name, type);
+        return Objects.hash(name, logicalType);
     }
 
     @Override
     public String toString() {
-        return "ColumnInfo{" + "name='" + name + '\'' + ", type='" + type + '\'' + '}';
+        return String.format(
+                "ColumnInfo{name='%s', logical type='%s'}", name, logicalType.toString());
     }
 }
diff --git a/flink-table/flink-sql-gateway-api/src/main/java/org/apache/flink/table/gateway/api/results/ResultSet.java b/flink-table/flink-sql-gateway-api/src/main/java/org/apache/flink/table/gateway/api/results/ResultSet.java
index b9bb0632a24..b35030426f2 100644
--- a/flink-table/flink-sql-gateway-api/src/main/java/org/apache/flink/table/gateway/api/results/ResultSet.java
+++ b/flink-table/flink-sql-gateway-api/src/main/java/org/apache/flink/table/gateway/api/results/ResultSet.java
@@ -22,6 +22,8 @@ import org.apache.flink.annotation.PublicEvolving;
 import org.apache.flink.table.api.TableResult;
 import org.apache.flink.table.catalog.ResolvedSchema;
 import org.apache.flink.table.data.RowData;
+import org.apache.flink.table.gateway.api.results.serde.JsonResultSetDeserializer;
+import org.apache.flink.table.gateway.api.results.serde.JsonResultSetSerializer;
 
 import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.annotation.JsonDeserialize;
 import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.annotation.JsonSerialize;
diff --git a/flink-table/flink-sql-gateway-api/src/main/java/org/apache/flink/table/gateway/api/results/JsonResultSetDeserializer.java b/flink-table/flink-sql-gateway-api/src/main/java/org/apache/flink/table/gateway/api/results/serde/JsonResultSetDeserializer.java
similarity index 88%
rename from flink-table/flink-sql-gateway-api/src/main/java/org/apache/flink/table/gateway/api/results/JsonResultSetDeserializer.java
rename to flink-table/flink-sql-gateway-api/src/main/java/org/apache/flink/table/gateway/api/results/serde/JsonResultSetDeserializer.java
index 7ae925a7c14..2c6ee7691f8 100644
--- a/flink-table/flink-sql-gateway-api/src/main/java/org/apache/flink/table/gateway/api/results/JsonResultSetDeserializer.java
+++ b/flink-table/flink-sql-gateway-api/src/main/java/org/apache/flink/table/gateway/api/results/serde/JsonResultSetDeserializer.java
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.flink.table.gateway.api.results;
+package org.apache.flink.table.gateway.api.results.serde;
 
 import org.apache.flink.annotation.PublicEvolving;
 import org.apache.flink.formats.common.TimestampFormat;
@@ -26,6 +26,9 @@ import org.apache.flink.table.catalog.ResolvedSchema;
 import org.apache.flink.table.data.GenericRowData;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.data.binary.BinaryStringData;
+import org.apache.flink.table.gateway.api.results.ColumnInfo;
+import org.apache.flink.table.gateway.api.results.ResultSet;
+import org.apache.flink.table.gateway.api.results.RowDataInfo;
 import org.apache.flink.table.types.DataType;
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.table.types.utils.DataTypeUtils;
@@ -33,8 +36,8 @@ import org.apache.flink.types.RowKind;
 
 import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.JsonParser;
 import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.DeserializationContext;
-import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonDeserializer;
 import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;
+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.std.StdDeserializer;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -47,7 +50,13 @@ import static org.apache.flink.table.gateway.api.results.ResultSet.FIELD_NAME_DA
 
 /** Json deserializer for {@link ResultSet}. */
 @PublicEvolving
-public class JsonResultSetDeserializer extends JsonDeserializer<ResultSet> {
+public class JsonResultSetDeserializer extends StdDeserializer<ResultSet> {
+
+    private static final long serialVersionUID = 1L;
+
+    public JsonResultSetDeserializer() {
+        super(ResultSet.class);
+    }
 
     private static final JsonToRowDataConverters TO_ROWDATA_CONVERTERS =
             new JsonToRowDataConverters(false, false, TimestampFormat.ISO_8601);
@@ -60,9 +69,10 @@ public class JsonResultSetDeserializer extends JsonDeserializer<ResultSet> {
         List<RowData> data = new ArrayList<>();
 
         // Deserialize column infos
-        JsonParser columnParser = node.get(FIELD_NAME_COLUMN_INFOS).traverse();
-        columnParser.nextToken();
-        ColumnInfo[] columnInfos = ctx.readValue(columnParser, ColumnInfo[].class);
+        ColumnInfo[] columnInfos =
+                jsonParser
+                        .getCodec()
+                        .treeToValue(node.get(FIELD_NAME_COLUMN_INFOS), ColumnInfo[].class);
         List<Column> columns = new ArrayList<>();
         for (ColumnInfo columnInfo : columnInfos) {
             LogicalType logicalType = columnInfo.getLogicalType();
diff --git a/flink-table/flink-sql-gateway-api/src/main/java/org/apache/flink/table/gateway/api/results/JsonResultSetSerializer.java b/flink-table/flink-sql-gateway-api/src/main/java/org/apache/flink/table/gateway/api/results/serde/JsonResultSetSerializer.java
similarity index 89%
rename from flink-table/flink-sql-gateway-api/src/main/java/org/apache/flink/table/gateway/api/results/JsonResultSetSerializer.java
rename to flink-table/flink-sql-gateway-api/src/main/java/org/apache/flink/table/gateway/api/results/serde/JsonResultSetSerializer.java
index 3fdeaf05dac..5c0dd4f772f 100644
--- a/flink-table/flink-sql-gateway-api/src/main/java/org/apache/flink/table/gateway/api/results/JsonResultSetSerializer.java
+++ b/flink-table/flink-sql-gateway-api/src/main/java/org/apache/flink/table/gateway/api/results/serde/JsonResultSetSerializer.java
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.flink.table.gateway.api.results;
+package org.apache.flink.table.gateway.api.results.serde;
 
 import org.apache.flink.annotation.PublicEvolving;
 import org.apache.flink.formats.common.TimestampFormat;
@@ -24,15 +24,18 @@ import org.apache.flink.formats.json.JsonFormatOptions;
 import org.apache.flink.formats.json.RowDataToJsonConverters;
 import org.apache.flink.table.catalog.Column;
 import org.apache.flink.table.data.RowData;
+import org.apache.flink.table.gateway.api.results.ColumnInfo;
+import org.apache.flink.table.gateway.api.results.ResultSet;
+import org.apache.flink.table.gateway.api.results.RowDataInfo;
 import org.apache.flink.table.types.DataType;
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.types.RowKind;
 
 import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.JsonGenerator;
 import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;
-import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonSerializer;
 import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectMapper;
 import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.SerializerProvider;
+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.std.StdSerializer;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -45,7 +48,13 @@ import static org.apache.flink.table.gateway.api.results.ResultSet.FIELD_NAME_DA
 
 /** Json serializer for {@link ResultSet}. */
 @PublicEvolving
-public class JsonResultSetSerializer extends JsonSerializer<ResultSet> {
+public class JsonResultSetSerializer extends StdSerializer<ResultSet> {
+
+    private static final long serialVersionUID = 1L;
+
+    public JsonResultSetSerializer() {
+        super(ResultSet.class);
+    }
 
     private static final ObjectMapper OBJECT_MAPPER = new ObjectMapper();
     private static final RowDataToJsonConverters TO_JSON_CONVERTERS =
@@ -63,8 +72,7 @@ public class JsonResultSetSerializer extends JsonSerializer<ResultSet> {
         List<ColumnInfo> columnInfos = new ArrayList<>();
         for (Column column : columns) {
             columnInfos.add(
-                    new ColumnInfo(
-                            column.getName(), column.getDataType().getLogicalType().toString()));
+                    new ColumnInfo(column.getName(), column.getDataType().getLogicalType()));
         }
         serializerProvider.defaultSerializeField(
                 FIELD_NAME_COLUMN_INFOS, columnInfos, jsonGenerator);
diff --git a/flink-table/flink-sql-gateway-api/src/main/java/org/apache/flink/table/gateway/api/results/serde/LogicalTypeJsonDeserializer.java b/flink-table/flink-sql-gateway-api/src/main/java/org/apache/flink/table/gateway/api/results/serde/LogicalTypeJsonDeserializer.java
new file mode 100644
index 00000000000..88e710ca53e
--- /dev/null
+++ b/flink-table/flink-sql-gateway-api/src/main/java/org/apache/flink/table/gateway/api/results/serde/LogicalTypeJsonDeserializer.java
@@ -0,0 +1,251 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.gateway.api.results.serde;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.table.api.TableException;
+import org.apache.flink.table.gateway.api.utils.SqlGatewayException;
+import org.apache.flink.table.types.logical.ArrayType;
+import org.apache.flink.table.types.logical.BigIntType;
+import org.apache.flink.table.types.logical.BinaryType;
+import org.apache.flink.table.types.logical.BooleanType;
+import org.apache.flink.table.types.logical.CharType;
+import org.apache.flink.table.types.logical.DateType;
+import org.apache.flink.table.types.logical.DecimalType;
+import org.apache.flink.table.types.logical.DoubleType;
+import org.apache.flink.table.types.logical.FloatType;
+import org.apache.flink.table.types.logical.IntType;
+import org.apache.flink.table.types.logical.LocalZonedTimestampType;
+import org.apache.flink.table.types.logical.LogicalType;
+import org.apache.flink.table.types.logical.LogicalTypeRoot;
+import org.apache.flink.table.types.logical.MapType;
+import org.apache.flink.table.types.logical.MultisetType;
+import org.apache.flink.table.types.logical.NullType;
+import org.apache.flink.table.types.logical.RawType;
+import org.apache.flink.table.types.logical.RowType;
+import org.apache.flink.table.types.logical.RowType.RowField;
+import org.apache.flink.table.types.logical.SmallIntType;
+import org.apache.flink.table.types.logical.TimeType;
+import org.apache.flink.table.types.logical.TimestampType;
+import org.apache.flink.table.types.logical.TinyIntType;
+import org.apache.flink.table.types.logical.VarBinaryType;
+import org.apache.flink.table.types.logical.VarCharType;
+import org.apache.flink.table.types.logical.ZonedTimestampType;
+
+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.JsonParser;
+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.DeserializationContext;
+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;
+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.std.StdDeserializer;
+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.node.ArrayNode;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import static org.apache.flink.table.gateway.api.results.serde.LogicalTypeJsonSerializer.FIELD_NAME_CLASS;
+import static org.apache.flink.table.gateway.api.results.serde.LogicalTypeJsonSerializer.FIELD_NAME_ELEMENT_TYPE;
+import static org.apache.flink.table.gateway.api.results.serde.LogicalTypeJsonSerializer.FIELD_NAME_FIELDS;
+import static org.apache.flink.table.gateway.api.results.serde.LogicalTypeJsonSerializer.FIELD_NAME_FIELD_NAME;
+import static org.apache.flink.table.gateway.api.results.serde.LogicalTypeJsonSerializer.FIELD_NAME_FIELD_TYPE;
+import static org.apache.flink.table.gateway.api.results.serde.LogicalTypeJsonSerializer.FIELD_NAME_FILED_DESCRIPTION;
+import static org.apache.flink.table.gateway.api.results.serde.LogicalTypeJsonSerializer.FIELD_NAME_KEY_TYPE;
+import static org.apache.flink.table.gateway.api.results.serde.LogicalTypeJsonSerializer.FIELD_NAME_LENGTH;
+import static org.apache.flink.table.gateway.api.results.serde.LogicalTypeJsonSerializer.FIELD_NAME_NULLABLE;
+import static org.apache.flink.table.gateway.api.results.serde.LogicalTypeJsonSerializer.FIELD_NAME_PRECISION;
+import static org.apache.flink.table.gateway.api.results.serde.LogicalTypeJsonSerializer.FIELD_NAME_SCALE;
+import static org.apache.flink.table.gateway.api.results.serde.LogicalTypeJsonSerializer.FIELD_NAME_SERIALIZER;
+import static org.apache.flink.table.gateway.api.results.serde.LogicalTypeJsonSerializer.FIELD_NAME_TYPE_NAME;
+import static org.apache.flink.table.gateway.api.results.serde.LogicalTypeJsonSerializer.FIELD_NAME_VALUE_TYPE;
+
+/**
+ * JSON deserializer for {@link LogicalType}.
+ *
+ * @see LogicalTypeJsonSerializer for the reverse operation.
+ */
+@Internal
+public final class LogicalTypeJsonDeserializer extends StdDeserializer<LogicalType> {
+
+    private static final long serialVersionUID = 1L;
+
+    LogicalTypeJsonDeserializer() {
+        super(LogicalType.class);
+    }
+
+    @Override
+    public LogicalType deserialize(JsonParser jsonParser, DeserializationContext ctx)
+            throws IOException {
+        JsonNode logicalTypeNode = jsonParser.readValueAsTree();
+        if (logicalTypeNode.has(FIELD_NAME_TYPE_NAME)) {
+            return deserializeInternal(logicalTypeNode);
+        }
+        throw new UnsupportedOperationException(
+                String.format(
+                        "Cannot parse this Json String:\n%s", logicalTypeNode.toPrettyString()));
+    }
+
+    /**
+     * Deserialize json according to the original type root. It's reverse operation of {@code
+     * SerializerWIP#serializeinternal}.
+     */
+    private LogicalType deserializeInternal(JsonNode logicalTypeNode) {
+        LogicalTypeRoot typeRoot =
+                LogicalTypeRoot.valueOf(logicalTypeNode.get(FIELD_NAME_TYPE_NAME).asText());
+        // the NullType's Json doesn't have other field, so return in advance
+        if (typeRoot.equals(LogicalTypeRoot.NULL)) {
+            return new NullType();
+        }
+        boolean isNullable = logicalTypeNode.get(FIELD_NAME_NULLABLE).asBoolean();
+        switch (typeRoot) {
+            case BOOLEAN:
+                return new BooleanType(isNullable);
+            case TINYINT:
+                return new TinyIntType(isNullable);
+            case SMALLINT:
+                return new SmallIntType(isNullable);
+            case INTEGER:
+                return new IntType(isNullable);
+            case BIGINT:
+                return new BigIntType(isNullable);
+            case FLOAT:
+                return new FloatType(isNullable);
+            case DOUBLE:
+                return new DoubleType(isNullable);
+            case DATE:
+                return new DateType(isNullable);
+            case CHAR:
+            case VARCHAR:
+            case BINARY:
+            case VARBINARY:
+                return deserializeLengthFieldType(typeRoot, logicalTypeNode).copy(isNullable);
+            case DECIMAL:
+                return new DecimalType(
+                        isNullable,
+                        logicalTypeNode.get(FIELD_NAME_PRECISION).asInt(),
+                        logicalTypeNode.get(FIELD_NAME_SCALE).asInt());
+            case TIME_WITHOUT_TIME_ZONE:
+            case TIMESTAMP_WITHOUT_TIME_ZONE:
+            case TIMESTAMP_WITH_TIME_ZONE:
+            case TIMESTAMP_WITH_LOCAL_TIME_ZONE:
+                return deserializeTimestamp(typeRoot, logicalTypeNode).copy(isNullable);
+            case MAP:
+                return deserializeMap(logicalTypeNode).copy(isNullable);
+            case ARRAY:
+            case MULTISET:
+                return deserializeCollection(typeRoot, logicalTypeNode).copy(isNullable);
+            case ROW:
+                return deserializeRow(logicalTypeNode).copy(isNullable);
+            case RAW:
+                return deserializeRaw(logicalTypeNode).copy(isNullable);
+            default:
+                throw new UnsupportedOperationException(
+                        String.format(
+                                "Unable to deserialize a logical type of type root '%s'. Please check the documentation for supported types.",
+                                typeRoot.name()));
+        }
+    }
+
+    // --------------------------------------------------------------------------------------------
+    // Helper methods for some complex types
+    // --------------------------------------------------------------------------------------------
+
+    private LogicalType deserializeLengthFieldType(
+            LogicalTypeRoot typeRoot, JsonNode logicalTypeNode) {
+        int length = logicalTypeNode.get(FIELD_NAME_LENGTH).asInt();
+        switch (typeRoot) {
+            case CHAR:
+                return length == 0 ? CharType.ofEmptyLiteral() : new CharType(length);
+            case VARCHAR:
+                return length == 0 ? VarCharType.ofEmptyLiteral() : new VarCharType(length);
+            case BINARY:
+                return length == 0 ? BinaryType.ofEmptyLiteral() : new BinaryType(length);
+            case VARBINARY:
+                return length == 0 ? VarBinaryType.ofEmptyLiteral() : new VarBinaryType(length);
+            default:
+                throw new SqlGatewayException(
+                        String.format(
+                                "Cannot convert JSON string '%s' to the logical type '%s', '%s', '%s' or '%s'.",
+                                logicalTypeNode.toPrettyString(),
+                                LogicalTypeRoot.CHAR.name(),
+                                LogicalTypeRoot.VARCHAR.name(),
+                                LogicalTypeRoot.BINARY.name(),
+                                LogicalTypeRoot.VARBINARY.name()));
+        }
+    }
+
+    private LogicalType deserializeTimestamp(LogicalTypeRoot typeRoot, JsonNode logicalTypeNode) {
+        int precision = logicalTypeNode.get(FIELD_NAME_PRECISION).asInt();
+        switch (typeRoot) {
+            case TIME_WITHOUT_TIME_ZONE:
+                return new TimeType(precision);
+            case TIMESTAMP_WITHOUT_TIME_ZONE:
+                return new TimestampType(precision);
+            case TIMESTAMP_WITH_TIME_ZONE:
+                return new ZonedTimestampType(precision);
+            case TIMESTAMP_WITH_LOCAL_TIME_ZONE:
+                return new LocalZonedTimestampType(precision);
+            default:
+                throw new TableException("Timestamp type root expected.");
+        }
+    }
+
+    private LogicalType deserializeMap(JsonNode logicalTypeNode) {
+        JsonNode keyNode = logicalTypeNode.get(FIELD_NAME_KEY_TYPE);
+        LogicalType keyType = deserializeInternal(keyNode);
+        JsonNode valueNode = logicalTypeNode.get(FIELD_NAME_VALUE_TYPE);
+        LogicalType valueType = deserializeInternal(valueNode);
+        return new MapType(keyType, valueType);
+    }
+
+    private LogicalType deserializeCollection(LogicalTypeRoot typeRoot, JsonNode logicalTypeNode) {
+        JsonNode elementNode = logicalTypeNode.get(FIELD_NAME_ELEMENT_TYPE);
+        LogicalType elementType = deserializeInternal(elementNode);
+        switch (typeRoot) {
+            case ARRAY:
+                return new ArrayType(elementType);
+            case MULTISET:
+                return new MultisetType(elementType);
+            default:
+                throw new TableException("Collection type root expected.");
+        }
+    }
+
+    private LogicalType deserializeRow(JsonNode logicalTypeNode) {
+        ArrayNode fieldNodes = (ArrayNode) logicalTypeNode.get(FIELD_NAME_FIELDS);
+        List<RowField> fields = new ArrayList<>();
+        fieldNodes.forEach(
+                fieldNode -> {
+                    String fieldName = fieldNode.get(FIELD_NAME_FIELD_NAME).asText();
+                    LogicalType fieldType =
+                            deserializeInternal(fieldNode.get(FIELD_NAME_FIELD_TYPE));
+                    String description = null;
+                    if (fieldNode.has(FIELD_NAME_FILED_DESCRIPTION)) {
+                        description = fieldNode.get(FIELD_NAME_FILED_DESCRIPTION).asText();
+                    }
+                    fields.add(new RowField(fieldName, fieldType, description));
+                });
+        return new RowType(fields);
+    }
+
+    private LogicalType deserializeRaw(JsonNode logicalTypeNode) {
+        String className = logicalTypeNode.get(FIELD_NAME_CLASS).asText();
+        String serializer = logicalTypeNode.get(FIELD_NAME_SERIALIZER).asText();
+        return RawType.restore(
+                LogicalTypeJsonDeserializer.class.getClassLoader(), className, serializer);
+    }
+}
diff --git a/flink-table/flink-sql-gateway-api/src/main/java/org/apache/flink/table/gateway/api/results/serde/LogicalTypeJsonSerializer.java b/flink-table/flink-sql-gateway-api/src/main/java/org/apache/flink/table/gateway/api/results/serde/LogicalTypeJsonSerializer.java
new file mode 100644
index 00000000000..2f39850a302
--- /dev/null
+++ b/flink-table/flink-sql-gateway-api/src/main/java/org/apache/flink/table/gateway/api/results/serde/LogicalTypeJsonSerializer.java
@@ -0,0 +1,243 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.gateway.api.results.serde;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.table.api.TableException;
+import org.apache.flink.table.types.logical.ArrayType;
+import org.apache.flink.table.types.logical.BinaryType;
+import org.apache.flink.table.types.logical.CharType;
+import org.apache.flink.table.types.logical.DecimalType;
+import org.apache.flink.table.types.logical.LocalZonedTimestampType;
+import org.apache.flink.table.types.logical.LogicalType;
+import org.apache.flink.table.types.logical.MapType;
+import org.apache.flink.table.types.logical.MultisetType;
+import org.apache.flink.table.types.logical.NullType;
+import org.apache.flink.table.types.logical.RawType;
+import org.apache.flink.table.types.logical.RowType;
+import org.apache.flink.table.types.logical.TimeType;
+import org.apache.flink.table.types.logical.TimestampType;
+import org.apache.flink.table.types.logical.VarBinaryType;
+import org.apache.flink.table.types.logical.VarCharType;
+import org.apache.flink.table.types.logical.ZonedTimestampType;
+
+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.JsonGenerator;
+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.SerializerProvider;
+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.std.StdSerializer;
+
+import java.io.IOException;
+
+/**
+ * Json serializer for {@link LogicalType}.
+ *
+ * @see LogicalTypeJsonDeserializer for the reverse operation.
+ */
+@Internal
+public final class LogicalTypeJsonSerializer extends StdSerializer<LogicalType> {
+
+    private static final long serialVersionUID = 1L;
+
+    // --------------------------------------------------------------------------------------------
+    // Public string constants for serializer and deserializer
+    // --------------------------------------------------------------------------------------------
+
+    // Common fields
+    public static final String FIELD_NAME_TYPE_NAME = "type";
+    public static final String FIELD_NAME_NULLABLE = "nullable";
+
+    // CHAR, VARCHAR, BINARY, VARBINARY
+    public static final String FIELD_NAME_LENGTH = "length";
+
+    // DECIMAL, TIMESTAMP_WITHOUT_TIME_ZONE, TIMESTAMP_WITH_LOCAL_TIME_ZONE
+    public static final String FIELD_NAME_PRECISION = "precision";
+
+    // DECIMAL
+    public static final String FIELD_NAME_SCALE = "scale";
+
+    // MAP
+    public static final String FIELD_NAME_KEY_TYPE = "keyType";
+    public static final String FIELD_NAME_VALUE_TYPE = "valueType";
+
+    // ARRAY, MULTISET
+    public static final String FIELD_NAME_ELEMENT_TYPE = "elementType";
+
+    // ROW
+    public static final String FIELD_NAME_FIELDS = "fields";
+    public static final String FIELD_NAME_FIELD_NAME = "name";
+    public static final String FIELD_NAME_FIELD_TYPE = "fieldType";
+    public static final String FIELD_NAME_FILED_DESCRIPTION = "description";
+
+    // RAW
+    public static final String FIELD_NAME_CLASS = "class";
+    public static final String FIELD_NAME_SERIALIZER = "serializer";
+
+    public LogicalTypeJsonSerializer() {
+        super(LogicalType.class);
+    }
+
+    @Override
+    public void serialize(
+            LogicalType logicalType,
+            JsonGenerator jsonGenerator,
+            SerializerProvider serializerProvider)
+            throws IOException {
+        serializeInternal(logicalType, jsonGenerator);
+    }
+
+    private void serializeInternal(LogicalType logicalType, JsonGenerator jsonGenerator)
+            throws IOException {
+        jsonGenerator.writeStartObject();
+
+        // write common fields shared by all types
+        jsonGenerator.writeStringField(FIELD_NAME_TYPE_NAME, logicalType.getTypeRoot().name());
+        // handle the special case: NullType doesn't need to have other fields
+        if (logicalType instanceof NullType) {
+            jsonGenerator.writeEndObject();
+            return;
+        }
+        jsonGenerator.writeBooleanField(FIELD_NAME_NULLABLE, logicalType.isNullable());
+        // write special fields according to type root
+        switch (logicalType.getTypeRoot()) {
+            case BOOLEAN:
+            case TINYINT:
+            case SMALLINT:
+            case INTEGER:
+            case BIGINT:
+            case FLOAT:
+            case DOUBLE:
+            case DATE:
+                break;
+            case CHAR:
+                jsonGenerator.writeNumberField(
+                        FIELD_NAME_LENGTH, ((CharType) logicalType).getLength());
+                break;
+            case VARCHAR:
+                jsonGenerator.writeNumberField(
+                        FIELD_NAME_LENGTH, ((VarCharType) logicalType).getLength());
+                break;
+            case BINARY:
+                jsonGenerator.writeNumberField(
+                        FIELD_NAME_LENGTH, ((BinaryType) logicalType).getLength());
+                break;
+            case VARBINARY:
+                jsonGenerator.writeNumberField(
+                        FIELD_NAME_LENGTH, ((VarBinaryType) logicalType).getLength());
+                break;
+            case DECIMAL:
+                jsonGenerator.writeNumberField(
+                        FIELD_NAME_PRECISION, ((DecimalType) logicalType).getPrecision());
+                jsonGenerator.writeNumberField(
+                        FIELD_NAME_SCALE, ((DecimalType) logicalType).getScale());
+                break;
+            case TIME_WITHOUT_TIME_ZONE:
+            case TIMESTAMP_WITHOUT_TIME_ZONE:
+            case TIMESTAMP_WITH_TIME_ZONE:
+            case TIMESTAMP_WITH_LOCAL_TIME_ZONE:
+                serializeTime(logicalType, jsonGenerator);
+                break;
+            case MAP:
+                serializeMap((MapType) logicalType, jsonGenerator);
+                break;
+            case ARRAY:
+                serializeCollection(((ArrayType) logicalType).getElementType(), jsonGenerator);
+                break;
+            case MULTISET:
+                serializeCollection(((MultisetType) logicalType).getElementType(), jsonGenerator);
+                break;
+            case ROW:
+                serializeRow((RowType) logicalType, jsonGenerator);
+                break;
+            case RAW:
+                if (logicalType instanceof RawType) {
+                    serializeRaw((RawType<?>) logicalType, jsonGenerator);
+                    break;
+                }
+                // fall through
+            default:
+                throw new UnsupportedOperationException(
+                        String.format(
+                                "Unable to serialize logical type '%s'. Please check the documentation for supported types.",
+                                logicalType.asSummaryString()));
+        }
+
+        jsonGenerator.writeEndObject();
+    }
+
+    // --------------------------------------------------------------------------------------------
+    // Helper methods for some complex types
+    // --------------------------------------------------------------------------------------------
+
+    private void serializeTime(LogicalType timeType, JsonGenerator jsonGenerator)
+            throws IOException {
+        switch (timeType.getTypeRoot()) {
+            case TIME_WITHOUT_TIME_ZONE:
+                jsonGenerator.writeNumberField(
+                        FIELD_NAME_PRECISION, ((TimeType) timeType).getPrecision());
+                break;
+            case TIMESTAMP_WITHOUT_TIME_ZONE:
+                jsonGenerator.writeNumberField(
+                        FIELD_NAME_PRECISION, ((TimestampType) timeType).getPrecision());
+                break;
+            case TIMESTAMP_WITH_TIME_ZONE:
+                jsonGenerator.writeNumberField(
+                        FIELD_NAME_PRECISION, ((ZonedTimestampType) timeType).getPrecision());
+                break;
+            case TIMESTAMP_WITH_LOCAL_TIME_ZONE:
+                jsonGenerator.writeNumberField(
+                        FIELD_NAME_PRECISION, ((LocalZonedTimestampType) timeType).getPrecision());
+                break;
+            default:
+                throw new TableException("Time or time stamp type root expected.");
+        }
+    }
+
+    private void serializeMap(MapType mapType, JsonGenerator jsonGenerator) throws IOException {
+        jsonGenerator.writeFieldName(FIELD_NAME_KEY_TYPE);
+        serializeInternal(mapType.getKeyType(), jsonGenerator);
+        jsonGenerator.writeFieldName(FIELD_NAME_VALUE_TYPE);
+        serializeInternal(mapType.getValueType(), jsonGenerator);
+    }
+
+    private void serializeCollection(LogicalType elementType, JsonGenerator jsonGenerator)
+            throws IOException {
+        jsonGenerator.writeFieldName(FIELD_NAME_ELEMENT_TYPE);
+        serializeInternal(elementType, jsonGenerator);
+    }
+
+    private void serializeRow(RowType rowType, JsonGenerator jsonGenerator) throws IOException {
+        jsonGenerator.writeArrayFieldStart(FIELD_NAME_FIELDS);
+        for (RowType.RowField rowField : rowType.getFields()) {
+            jsonGenerator.writeStartObject();
+            jsonGenerator.writeStringField(FIELD_NAME_FIELD_NAME, rowField.getName());
+            jsonGenerator.writeFieldName(FIELD_NAME_FIELD_TYPE);
+            serializeInternal(rowField.getType(), jsonGenerator);
+            if (rowField.getDescription().isPresent()) {
+                jsonGenerator.writeStringField(
+                        FIELD_NAME_FILED_DESCRIPTION, rowField.getDescription().get());
+            }
+            jsonGenerator.writeEndObject();
+        }
+        jsonGenerator.writeEndArray();
+    }
+
+    private void serializeRaw(RawType<?> rawType, JsonGenerator jsonGenerator) throws IOException {
+        jsonGenerator.writeStringField(FIELD_NAME_CLASS, rawType.getOriginatingClass().getName());
+        jsonGenerator.writeStringField(FIELD_NAME_SERIALIZER, rawType.getSerializerString());
+    }
+}
diff --git a/flink-table/flink-sql-gateway-api/src/test/java/org/apache/flink/table/gateway/api/results/JsonResultSetSerDeTest.java b/flink-table/flink-sql-gateway-api/src/test/java/org/apache/flink/table/gateway/api/results/serde/JsonResultSetSerDeTest.java
similarity index 91%
rename from flink-table/flink-sql-gateway-api/src/test/java/org/apache/flink/table/gateway/api/results/JsonResultSetSerDeTest.java
rename to flink-table/flink-sql-gateway-api/src/test/java/org/apache/flink/table/gateway/api/results/serde/JsonResultSetSerDeTest.java
index 7a62ab57e7c..27cf091b01d 100644
--- a/flink-table/flink-sql-gateway-api/src/test/java/org/apache/flink/table/gateway/api/results/JsonResultSetSerDeTest.java
+++ b/flink-table/flink-sql-gateway-api/src/test/java/org/apache/flink/table/gateway/api/results/serde/JsonResultSetSerDeTest.java
@@ -16,27 +16,23 @@
  * limitations under the License.
  */
 
-package org.apache.flink.table.gateway.api.results;
+package org.apache.flink.table.gateway.api.results.serde;
 
 import org.apache.flink.table.api.DataTypes;
 import org.apache.flink.table.catalog.ResolvedSchema;
 import org.apache.flink.table.data.GenericRowData;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.data.util.DataFormatConverters;
+import org.apache.flink.table.gateway.api.results.ResultSet;
 import org.apache.flink.table.types.DataType;
 import org.apache.flink.types.Row;
 import org.apache.flink.types.RowKind;
 
-import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.JsonFactory;
-import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.JsonGenerator;
 import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectMapper;
-import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.SerializerProvider;
 
 import org.junit.jupiter.api.Test;
 
 import java.io.IOException;
-import java.io.StringWriter;
-import java.io.Writer;
 import java.math.BigDecimal;
 import java.sql.Timestamp;
 import java.time.Instant;
@@ -102,7 +98,7 @@ class JsonResultSetSerializationTest {
     private static final Map<String, Map<String, Integer>> nestedMap = new HashMap<>();
     private static final Map<String, Integer> innerMap = new HashMap<>();
 
-    {
+    static {
         map.put("element", 123L);
         multiSet.put("element", 2);
         innerMap.put("key", 234);
@@ -146,16 +142,10 @@ class JsonResultSetSerializationTest {
         ResolvedSchema testResolvedSchema = getTestResolvedSchema(fields);
         ResultSet testResultSet =
                 new ResultSet(ResultSet.ResultType.PAYLOAD, 0L, testResolvedSchema, rowDataList);
-        // Test serialization
-        Writer jsonWriter = new StringWriter();
-        JsonGenerator jsonGenerator = new JsonFactory().createGenerator(jsonWriter);
-        SerializerProvider serializerProvider = new ObjectMapper().getSerializerProviderInstance();
-        new JsonResultSetSerializer().serialize(testResultSet, jsonGenerator, serializerProvider);
-        jsonGenerator.flush();
-        String result = jsonWriter.toString();
-        // Test deserialization
-        ObjectMapper mapper = new ObjectMapper();
-        ResultSet resultSet = mapper.readValue(result, ResultSet.class);
+        // Test serialization & deserialization
+        ObjectMapper objectMapper = new ObjectMapper();
+        String result = objectMapper.writeValueAsString(testResultSet);
+        ResultSet resultSet = objectMapper.readValue(result, ResultSet.class);
         List<RowData> deRowDataList = resultSet.getData();
         for (int i = 0; i < deRowDataList.size(); ++i) {
             assertThat(convertToExternal(deRowDataList.get(i), ROW(getFields())))
diff --git a/flink-table/flink-sql-gateway-api/src/test/java/org/apache/flink/table/gateway/api/results/serde/LogicalTypeJsonSerDeTest.java b/flink-table/flink-sql-gateway-api/src/test/java/org/apache/flink/table/gateway/api/results/serde/LogicalTypeJsonSerDeTest.java
new file mode 100644
index 00000000000..6b826107548
--- /dev/null
+++ b/flink-table/flink-sql-gateway-api/src/test/java/org/apache/flink/table/gateway/api/results/serde/LogicalTypeJsonSerDeTest.java
@@ -0,0 +1,233 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.gateway.api.results.serde;
+
+import org.apache.flink.api.common.typeutils.base.LocalDateTimeSerializer;
+import org.apache.flink.core.testutils.FlinkAssertions;
+import org.apache.flink.table.api.DataTypes;
+import org.apache.flink.table.runtime.typeutils.ExternalSerializer;
+import org.apache.flink.table.types.logical.ArrayType;
+import org.apache.flink.table.types.logical.BigIntType;
+import org.apache.flink.table.types.logical.BinaryType;
+import org.apache.flink.table.types.logical.BooleanType;
+import org.apache.flink.table.types.logical.CharType;
+import org.apache.flink.table.types.logical.DateType;
+import org.apache.flink.table.types.logical.DayTimeIntervalType;
+import org.apache.flink.table.types.logical.DecimalType;
+import org.apache.flink.table.types.logical.DoubleType;
+import org.apache.flink.table.types.logical.FloatType;
+import org.apache.flink.table.types.logical.IntType;
+import org.apache.flink.table.types.logical.LocalZonedTimestampType;
+import org.apache.flink.table.types.logical.LogicalType;
+import org.apache.flink.table.types.logical.MapType;
+import org.apache.flink.table.types.logical.MultisetType;
+import org.apache.flink.table.types.logical.NullType;
+import org.apache.flink.table.types.logical.RawType;
+import org.apache.flink.table.types.logical.RowType;
+import org.apache.flink.table.types.logical.SmallIntType;
+import org.apache.flink.table.types.logical.TimeType;
+import org.apache.flink.table.types.logical.TimestampKind;
+import org.apache.flink.table.types.logical.TimestampType;
+import org.apache.flink.table.types.logical.TinyIntType;
+import org.apache.flink.table.types.logical.VarBinaryType;
+import org.apache.flink.table.types.logical.VarCharType;
+import org.apache.flink.table.types.logical.ZonedTimestampType;
+import org.apache.flink.types.Row;
+
+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectMapper;
+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.module.SimpleModule;
+
+import org.junit.jupiter.api.Test;
+import org.junit.jupiter.api.parallel.Execution;
+import org.junit.jupiter.params.ParameterizedTest;
+import org.junit.jupiter.params.provider.MethodSource;
+
+import java.io.IOException;
+import java.time.LocalDateTime;
+import java.util.Arrays;
+import java.util.List;
+import java.util.stream.Collectors;
+import java.util.stream.Stream;
+
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Assertions.assertThatThrownBy;
+import static org.junit.jupiter.api.parallel.ExecutionMode.CONCURRENT;
+
+/** Tests for {@link LogicalType} serialization and deserialization. */
+@Execution(CONCURRENT)
+public class LogicalTypeJsonSerDeTest {
+
+    private final ObjectMapper mapper = buildObjectMapper();
+
+    @ParameterizedTest
+    @MethodSource("generateTestData")
+    public void testLogicalTypeJsonSerDe(LogicalType logicalType) throws IOException {
+        String json = mapper.writeValueAsString(logicalType);
+        LogicalType actualType = mapper.readValue(json, LogicalType.class);
+
+        assertThat(actualType).isEqualTo(logicalType);
+    }
+
+    @Test
+    public void testSerializeUnsupportedType() {
+        LogicalType unsupportedType =
+                new DayTimeIntervalType(DayTimeIntervalType.DayTimeResolution.DAY_TO_HOUR);
+        assertThatThrownBy(() -> mapper.writeValueAsString(unsupportedType))
+                .satisfies(
+                        FlinkAssertions.anyCauseMatches(
+                                UnsupportedOperationException.class,
+                                String.format(
+                                        "Unable to serialize logical type '%s'. Please check the documentation for supported types.",
+                                        unsupportedType.asSummaryString())));
+    }
+
+    @Test
+    public void testDeserializeUnsupportedType() {
+        String unsupportedTypeString = "INTERVAL_DAY_TIME";
+        String json =
+                String.format(
+                        "{\"%s\": \"%s\", \"%s\": %s}",
+                        "type", unsupportedTypeString, "nullable", "true");
+        assertThatThrownBy(() -> mapper.readValue(json, LogicalType.class))
+                .satisfies(
+                        FlinkAssertions.anyCauseMatches(
+                                UnsupportedOperationException.class,
+                                String.format(
+                                        "Unable to deserialize a logical type of type root '%s'. Please check the documentation for supported types.",
+                                        unsupportedTypeString)));
+    }
+
+    @Test
+    public void testDeserializeUnsupportedJson() {
+        String json = String.format("{\"%s\": \"%s\"}", "unknown", "whatever");
+        assertThatThrownBy(() -> mapper.readValue(json, LogicalType.class))
+                .satisfies(
+                        FlinkAssertions.anyCauseMatches(
+                                UnsupportedOperationException.class,
+                                "Cannot parse this Json String"));
+    }
+
+    // --------------------------------------------------------------------------------------------
+    // Test data
+    // --------------------------------------------------------------------------------------------
+
+    private static List<LogicalType> generateTestData() {
+        List<LogicalType> types =
+                Arrays.asList(
+                        new BooleanType(),
+                        new TinyIntType(),
+                        new SmallIntType(),
+                        new IntType(),
+                        new BigIntType(),
+                        new FloatType(),
+                        new DoubleType(),
+                        new DateType(),
+                        CharType.ofEmptyLiteral(),
+                        new CharType(),
+                        new CharType(5),
+                        VarCharType.ofEmptyLiteral(),
+                        new VarCharType(),
+                        new VarCharType(5),
+                        BinaryType.ofEmptyLiteral(),
+                        new BinaryType(),
+                        new BinaryType(100),
+                        VarBinaryType.ofEmptyLiteral(),
+                        new VarBinaryType(),
+                        new VarBinaryType(100),
+                        new DecimalType(10),
+                        new DecimalType(15, 5),
+                        new TimeType(),
+                        new TimeType(3),
+                        new TimestampType(),
+                        new TimestampType(3),
+                        new TimestampType(false, 3),
+                        new ZonedTimestampType(),
+                        new ZonedTimestampType(3),
+                        new LocalZonedTimestampType(),
+                        new LocalZonedTimestampType(3),
+                        new LocalZonedTimestampType(false, 3),
+                        new LocalZonedTimestampType(false, TimestampKind.PROCTIME, 3),
+                        new MapType(new BigIntType(), new IntType(false)),
+                        new MapType(CharType.ofEmptyLiteral(), CharType.ofEmptyLiteral()),
+                        new MapType(VarCharType.ofEmptyLiteral(), VarCharType.ofEmptyLiteral()),
+                        new MapType(BinaryType.ofEmptyLiteral(), BinaryType.ofEmptyLiteral()),
+                        new MapType(VarBinaryType.ofEmptyLiteral(), VarBinaryType.ofEmptyLiteral()),
+                        new MapType(new TimestampType(false, 3), new LocalZonedTimestampType()),
+                        new ArrayType(new IntType(false)),
+                        new ArrayType(new TimestampType()),
+                        new ArrayType(new LocalZonedTimestampType(false, 3)),
+                        new ArrayType(CharType.ofEmptyLiteral()),
+                        new ArrayType(VarCharType.ofEmptyLiteral()),
+                        new ArrayType(BinaryType.ofEmptyLiteral()),
+                        new ArrayType(VarBinaryType.ofEmptyLiteral()),
+                        new MultisetType(new IntType(false)),
+                        new MultisetType(new TimestampType()),
+                        new MultisetType(new TimestampType(true, 3)),
+                        new MultisetType(CharType.ofEmptyLiteral()),
+                        new MultisetType(VarCharType.ofEmptyLiteral()),
+                        new MultisetType(BinaryType.ofEmptyLiteral()),
+                        new MultisetType(VarBinaryType.ofEmptyLiteral()),
+                        RowType.of(new BigIntType(), new IntType(false), new VarCharType(200)),
+                        RowType.of(
+                                new LogicalType[] {
+                                    new BigIntType(), new IntType(false), new VarCharType(200)
+                                },
+                                new String[] {"f1", "f2", "f3"}),
+                        RowType.of(
+                                new TimestampType(false, 3), new LocalZonedTimestampType(false, 3)),
+                        RowType.of(
+                                CharType.ofEmptyLiteral(),
+                                VarCharType.ofEmptyLiteral(),
+                                BinaryType.ofEmptyLiteral(),
+                                VarBinaryType.ofEmptyLiteral()),
+                        // Row with descriptions
+                        new RowType(
+                                Arrays.asList(
+                                        new RowType.RowField("ID", new BigIntType(), "ID desc"),
+                                        new RowType.RowField(
+                                                "Name", new VarCharType(20), "Name desc"))),
+                        // custom RawType
+                        new RawType<>(LocalDateTime.class, LocalDateTimeSerializer.INSTANCE),
+                        // external RawType
+                        new RawType<>(
+                                Row.class,
+                                ExternalSerializer.of(
+                                        DataTypes.ROW(DataTypes.INT(), DataTypes.STRING()))));
+
+        List<LogicalType> testTypes =
+                Stream.concat(
+                                types.stream().map(type -> type.copy(true)),
+                                types.stream().map(type -> type.copy(false)))
+                        .collect(Collectors.toList());
+
+        // ignore nullable for NullType
+        testTypes.add(new NullType());
+
+        return testTypes;
+    }
+
+    private ObjectMapper buildObjectMapper() {
+        ObjectMapper mapper = new ObjectMapper();
+        SimpleModule module = new SimpleModule();
+        module.addSerializer(new LogicalTypeJsonSerializer());
+        module.addDeserializer(LogicalType.class, new LogicalTypeJsonDeserializer());
+        mapper.registerModule(module);
+        return mapper;
+    }
+}
diff --git a/flink-table/flink-sql-gateway/src/main/java/org/apache/flink/table/gateway/cli/SqlGatewayOptionsParser.java b/flink-table/flink-sql-gateway/src/main/java/org/apache/flink/table/gateway/cli/SqlGatewayOptionsParser.java
index 2726954794e..08da5299b27 100644
--- a/flink-table/flink-sql-gateway/src/main/java/org/apache/flink/table/gateway/cli/SqlGatewayOptionsParser.java
+++ b/flink-table/flink-sql-gateway/src/main/java/org/apache/flink/table/gateway/cli/SqlGatewayOptionsParser.java
@@ -58,7 +58,7 @@ public class SqlGatewayOptionsParser {
             CommandLine line = parser.parse(getSqlGatewayOptions(), args, true);
             return new SqlGatewayOptions(
                     line.hasOption(SqlGatewayOptionsParser.OPTION_HELP.getOpt()),
-                    line.getOptionProperties(DYNAMIC_PROPERTY_OPTION));
+                    line.getOptionProperties(DYNAMIC_PROPERTY_OPTION.getOpt()));
         } catch (ParseException e) {
             throw new SqlGatewayException(e.getMessage());
         }
diff --git a/flink-table/flink-sql-gateway/src/test/java/org/apache/flink/table/gateway/rest/StatementCaseITTest.java b/flink-table/flink-sql-gateway/src/test/java/org/apache/flink/table/gateway/rest/StatementCaseITTest.java
index b10073b678b..ebf721e930e 100644
--- a/flink-table/flink-sql-gateway/src/test/java/org/apache/flink/table/gateway/rest/StatementCaseITTest.java
+++ b/flink-table/flink-sql-gateway/src/test/java/org/apache/flink/table/gateway/rest/StatementCaseITTest.java
@@ -24,10 +24,6 @@ import org.apache.flink.configuration.ExecutionOptions;
 import org.apache.flink.core.testutils.CommonTestUtils;
 import org.apache.flink.runtime.rest.RestClient;
 import org.apache.flink.runtime.rest.messages.EmptyRequestBody;
-import org.apache.flink.runtime.rest.messages.MessageHeaders;
-import org.apache.flink.runtime.rest.messages.MessageParameters;
-import org.apache.flink.runtime.rest.messages.RequestBody;
-import org.apache.flink.runtime.rest.messages.ResponseBody;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.gateway.AbstractSqlGatewayStatementITCase;
 import org.apache.flink.table.gateway.api.operation.OperationHandle;
@@ -35,6 +31,7 @@ import org.apache.flink.table.gateway.api.results.ResultSet;
 import org.apache.flink.table.gateway.api.session.SessionEnvironment;
 import org.apache.flink.table.gateway.api.session.SessionHandle;
 import org.apache.flink.table.gateway.api.utils.MockedEndpointVersion;
+import org.apache.flink.table.gateway.api.utils.SqlGatewayException;
 import org.apache.flink.table.gateway.rest.handler.AbstractSqlGatewayRestHandler;
 import org.apache.flink.table.gateway.rest.header.statement.ExecuteStatementHeaders;
 import org.apache.flink.table.gateway.rest.header.statement.FetchResultsHeaders;
@@ -43,17 +40,17 @@ import org.apache.flink.table.gateway.rest.message.statement.ExecuteStatementReq
 import org.apache.flink.table.gateway.rest.message.statement.ExecuteStatementResponseBody;
 import org.apache.flink.table.gateway.rest.message.statement.FetchResultsResponseBody;
 import org.apache.flink.table.gateway.rest.message.statement.FetchResultsTokenParameters;
+import org.apache.flink.table.gateway.rest.util.SqlGatewayRestEndpointExtension;
 import org.apache.flink.table.planner.functions.casting.RowDataToStringConverterImpl;
 import org.apache.flink.table.utils.DateTimeUtils;
+import org.apache.flink.util.ConfigurationException;
 import org.apache.flink.util.concurrent.ExecutorThreadFactory;
 
-import org.jetbrains.annotations.Nullable;
 import org.junit.jupiter.api.BeforeEach;
+import org.junit.jupiter.api.Order;
+import org.junit.jupiter.api.extension.RegisterExtension;
 import org.junit.jupiter.api.io.TempDir;
 
-import java.io.IOException;
-import java.net.InetAddress;
-import java.net.InetSocketAddress;
 import java.nio.file.Path;
 import java.time.Duration;
 import java.util.Arrays;
@@ -64,9 +61,6 @@ import java.util.UUID;
 import java.util.concurrent.CompletableFuture;
 import java.util.concurrent.Executors;
 
-import static org.apache.flink.table.gateway.rest.util.RestConfigUtils.getBaseConfig;
-import static org.apache.flink.table.gateway.rest.util.RestConfigUtils.getFlinkConfig;
-import static org.apache.flink.util.Preconditions.checkNotNull;
 import static org.assertj.core.api.Assertions.assertThat;
 import static org.junit.jupiter.api.Assertions.assertDoesNotThrow;
 import static org.junit.jupiter.api.Assertions.assertNotNull;
@@ -75,12 +69,14 @@ import static org.junit.jupiter.api.Assertions.assertNotNull;
  * Test basic logic of handlers inherited from {@link AbstractSqlGatewayRestHandler} in statement
  * related cases.
  */
-class StatementCaseITTest extends AbstractSqlGatewayStatementITCase {
+class SqlGatewayRestEndpointStatementITCase extends AbstractSqlGatewayStatementITCase {
 
-    @Nullable private static RestClient restClient;
-    private static final String targetAddress;
-    private static final int port;
+    @RegisterExtension
+    @Order(3)
+    private static final SqlGatewayRestEndpointExtension SQL_GATEWAY_REST_ENDPOINT_EXTENSION =
+            new SqlGatewayRestEndpointExtension(SQL_GATEWAY_SERVICE_EXTENSION::getService);
 
+    private static final RestClient restClient = getTestRestClient();
     private static final ExecuteStatementHeaders executeStatementHeaders =
             ExecuteStatementHeaders.getInstance();
     private static SessionMessageParameters sessionMessageParameters;
@@ -91,27 +87,6 @@ class StatementCaseITTest extends AbstractSqlGatewayStatementITCase {
     private static final String PATTERN1 = "Caused by: ";
     private static final String PATTERN2 = "\tat ";
 
-    static {
-        String address = InetAddress.getLoopbackAddress().getHostAddress();
-        Configuration config = getBaseConfig(getFlinkConfig(address, address, "0"));
-        @Nullable SqlGatewayRestEndpoint sqlGatewayRestEndpoint = null;
-        try {
-            sqlGatewayRestEndpoint =
-                    new SqlGatewayRestEndpoint(config, SQL_GATEWAY_SERVICE_EXTENSION.getService());
-            sqlGatewayRestEndpoint.start();
-            restClient =
-                    new RestClient(
-                            new Configuration(),
-                            Executors.newFixedThreadPool(
-                                    1, new ExecutorThreadFactory("rest-client-thread-pool")));
-        } catch (Exception ignored) {
-        }
-        checkNotNull(sqlGatewayRestEndpoint);
-        InetSocketAddress serverAddress = checkNotNull(sqlGatewayRestEndpoint.getServerAddress());
-        targetAddress = serverAddress.getHostName();
-        port = serverAddress.getPort();
-    }
-
     private final SessionEnvironment defaultSessionEnvironment =
             SessionEnvironment.newBuilder()
                     .setSessionEndpointVersion(MockedEndpointVersion.V1)
@@ -132,7 +107,9 @@ class StatementCaseITTest extends AbstractSqlGatewayStatementITCase {
         ExecuteStatementRequestBody executeStatementRequestBody =
                 new ExecuteStatementRequestBody(statement, 0L, new HashMap<>());
         CompletableFuture<ExecuteStatementResponseBody> response =
-                sendRequest(
+                restClient.sendRequest(
+                        SQL_GATEWAY_REST_ENDPOINT_EXTENSION.getTargetAddress(),
+                        SQL_GATEWAY_REST_ENDPOINT_EXTENSION.getTargetPort(),
                         executeStatementHeaders,
                         sessionMessageParameters,
                         executeStatementRequestBody);
@@ -177,7 +154,7 @@ class StatementCaseITTest extends AbstractSqlGatewayStatementITCase {
                 new RowDataToStringConverterImpl(
                         resultSet.getResultSchema().toPhysicalRowDataType(),
                         DateTimeUtils.UTC_ZONE.toZoneId(),
-                        StatementCaseITTest.class.getClassLoader(),
+                        SqlGatewayRestEndpointStatementITCase.class.getClassLoader(),
                         false),
                 new RowDataIterator(sessionHandle, operationHandle));
     }
@@ -188,7 +165,9 @@ class StatementCaseITTest extends AbstractSqlGatewayStatementITCase {
         FetchResultsTokenParameters fetchResultsTokenParameters =
                 new FetchResultsTokenParameters(sessionHandle, operationHandle, token);
         CompletableFuture<FetchResultsResponseBody> response =
-                sendRequest(
+                restClient.sendRequest(
+                        SQL_GATEWAY_REST_ENDPOINT_EXTENSION.getTargetAddress(),
+                        SQL_GATEWAY_REST_ENDPOINT_EXTENSION.getTargetPort(),
                         fetchResultsHeaders,
                         fetchResultsTokenParameters,
                         EmptyRequestBody.getInstance());
@@ -209,16 +188,15 @@ class StatementCaseITTest extends AbstractSqlGatewayStatementITCase {
                 .equals(RuntimeExecutionMode.STREAMING);
     }
 
-    <
-                    M extends MessageHeaders<R, P, U>,
-                    U extends MessageParameters,
-                    R extends RequestBody,
-                    P extends ResponseBody>
-            CompletableFuture<P> sendRequest(M messageHeaders, U messageParameters, R request)
-                    throws IOException {
-        checkNotNull(restClient);
-        return restClient.sendRequest(
-                targetAddress, port, messageHeaders, messageParameters, request);
+    private static RestClient getTestRestClient() {
+        try {
+            return new RestClient(
+                    new Configuration(),
+                    Executors.newFixedThreadPool(
+                            1, new ExecutorThreadFactory("rest-client-thread-pool")));
+        } catch (ConfigurationException e) {
+            throw new SqlGatewayException("Cannot get rest client.", e);
+        }
     }
 
     private class RowDataIterator implements Iterator<RowData> {
@@ -255,7 +233,7 @@ class StatementCaseITTest extends AbstractSqlGatewayStatementITCase {
 
         private void fetch() throws Exception {
             FetchResultsResponseBody fetchResultsResponseBody =
-                    StatementCaseITTest.this.fetchResults(sessionHandle, operationHandle, token);
+                    fetchResults(sessionHandle, operationHandle, token);
             String nextResultUri = fetchResultsResponseBody.getNextResultUri();
             ResultSet resultSet = fetchResultsResponseBody.getResults();
             token = parseTokenFromUri(nextResultUri);
diff --git a/flink-table/flink-sql-gateway/src/test/java/org/apache/flink/table/gateway/rest/util/SqlGatewayRestEndpointExtension.java b/flink-table/flink-sql-gateway/src/test/java/org/apache/flink/table/gateway/rest/util/SqlGatewayRestEndpointExtension.java
new file mode 100644
index 00000000000..4116669e756
--- /dev/null
+++ b/flink-table/flink-sql-gateway/src/test/java/org/apache/flink/table/gateway/rest/util/SqlGatewayRestEndpointExtension.java
@@ -0,0 +1,89 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.gateway.rest.util;
+
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.table.gateway.api.SqlGatewayService;
+import org.apache.flink.table.gateway.api.utils.SqlGatewayException;
+import org.apache.flink.table.gateway.rest.SqlGatewayRestEndpoint;
+
+import org.junit.jupiter.api.extension.AfterAllCallback;
+import org.junit.jupiter.api.extension.BeforeAllCallback;
+import org.junit.jupiter.api.extension.Extension;
+import org.junit.jupiter.api.extension.ExtensionContext;
+
+import java.net.InetAddress;
+import java.net.InetSocketAddress;
+import java.util.function.Supplier;
+
+import static org.apache.flink.table.gateway.rest.util.RestConfigUtils.getBaseConfig;
+import static org.apache.flink.table.gateway.rest.util.RestConfigUtils.getFlinkConfig;
+import static org.apache.flink.util.Preconditions.checkNotNull;
+
+/** A simple {@link Extension} that manages the lifecycle of the {@link SqlGatewayRestEndpoint}. */
+public class SqlGatewayRestEndpointExtension implements BeforeAllCallback, AfterAllCallback {
+
+    private final Supplier<SqlGatewayService> serviceSupplier;
+
+    private SqlGatewayRestEndpoint sqlGatewayRestEndpoint;
+    private String targetAddress;
+    private int targetPort;
+
+    public String getTargetAddress() {
+        return targetAddress;
+    }
+
+    public int getTargetPort() {
+        return targetPort;
+    }
+
+    public SqlGatewayRestEndpointExtension(Supplier<SqlGatewayService> serviceSupplier) {
+        this.serviceSupplier = serviceSupplier;
+    }
+
+    @Override
+    public void beforeAll(ExtensionContext context) {
+        String address = InetAddress.getLoopbackAddress().getHostAddress();
+        Configuration config = getBaseConfig(getFlinkConfig(address, address, "0"));
+
+        try {
+            sqlGatewayRestEndpoint = new SqlGatewayRestEndpoint(config, serviceSupplier.get());
+            sqlGatewayRestEndpoint.start();
+        } catch (Exception e) {
+            throw new SqlGatewayException(
+                    "Unexpected error occurred when trying to start the rest endpoint of sql gateway.",
+                    e);
+        }
+
+        InetSocketAddress serverAddress = checkNotNull(sqlGatewayRestEndpoint.getServerAddress());
+        targetAddress = serverAddress.getHostName();
+        targetPort = serverAddress.getPort();
+    }
+
+    @Override
+    public void afterAll(ExtensionContext context) {
+        try {
+            sqlGatewayRestEndpoint.stop();
+        } catch (Exception e) {
+            throw new SqlGatewayException(
+                    "Unexpected error occurred when trying to stop the rest endpoint of sql gateway.",
+                    e);
+        }
+    }
+}
