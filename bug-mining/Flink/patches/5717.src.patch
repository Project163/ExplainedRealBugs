diff --git a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/KafkaSource.java b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/KafkaSource.java
index 35fd9544825..23acca6e7bf 100644
--- a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/KafkaSource.java
+++ b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/KafkaSource.java
@@ -142,7 +142,7 @@ public class KafkaSource<OUT>
                         new KafkaPartitionSplitReader<>(
                                 props,
                                 deserializationSchema,
-                                readerContext.getIndexOfSubtask(),
+                                readerContext,
                                 kafkaSourceReaderMetrics);
         KafkaRecordEmitter<OUT> recordEmitter = new KafkaRecordEmitter<>();
 
diff --git a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/metrics/KafkaSourceReaderMetrics.java b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/metrics/KafkaSourceReaderMetrics.java
index 356409fa087..73953aa4654 100644
--- a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/metrics/KafkaSourceReaderMetrics.java
+++ b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/metrics/KafkaSourceReaderMetrics.java
@@ -18,9 +18,12 @@
 
 package org.apache.flink.connector.kafka.source.metrics;
 
+import org.apache.flink.connector.kafka.MetricUtil;
 import org.apache.flink.connector.kafka.source.reader.KafkaSourceReader;
 import org.apache.flink.metrics.Counter;
 import org.apache.flink.metrics.MetricGroup;
+import org.apache.flink.metrics.groups.SourceReaderMetricGroup;
+import org.apache.flink.runtime.metrics.MetricNames;
 
 import org.apache.kafka.clients.consumer.KafkaConsumer;
 import org.apache.kafka.common.Metric;
@@ -29,15 +32,20 @@ import org.apache.kafka.common.TopicPartition;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import javax.annotation.Nullable;
+
 import java.util.HashMap;
 import java.util.Map;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ConcurrentMap;
+import java.util.function.Predicate;
 
 /**
  * A collection class for handling metrics in {@link KafkaSourceReader}.
  *
  * <p>All metrics of Kafka source reader are registered under group "KafkaSourceReader", which is a
- * child group of {@link org.apache.flink.runtime.metrics.groups.OperatorMetricGroup}. Metrics
- * related to a specific topic partition will be registered in the group
+ * child group of {@link org.apache.flink.metrics.groups.OperatorMetricGroup}. Metrics related to a
+ * specific topic partition will be registered in the group
  * "KafkaSourceReader.topic.{topic_name}.partition.{partition_id}".
  *
  * <p>For example, current consuming offset of topic "my-topic" and partition 1 will be reported in
@@ -66,9 +74,17 @@ public class KafkaSourceReaderMetrics {
     public static final String COMMITS_FAILED_METRIC_COUNTER = "commitsFailed";
     public static final String KAFKA_CONSUMER_METRIC_GROUP = "KafkaConsumer";
 
+    // Kafka raw metric names and group names
+    public static final String CONSUMER_FETCH_MANAGER_GROUP = "consumer-fetch-manager-metrics";
+    public static final String BYTES_CONSUMED_TOTAL = "bytes-consumed-total";
+    public static final String RECORDS_LAG = "records-lag";
+
     public static final long INITIAL_OFFSET = -1;
 
-    // Metric group for registering Kafka related metrics
+    // Source reader metric group
+    private final SourceReaderMetricGroup sourceReaderMetricGroup;
+
+    // Metric group for registering Kafka specific metrics
     private final MetricGroup kafkaSourceReaderMetricGroup;
 
     // Successful / Failed commits counters
@@ -78,9 +94,16 @@ public class KafkaSourceReaderMetrics {
     // Map for tracking current consuming / committing offsets
     private final Map<TopicPartition, Offset> offsets = new HashMap<>();
 
-    public KafkaSourceReaderMetrics(MetricGroup parentMetricGroup) {
+    // Map for tracking records lag of topic partitions
+    @Nullable private ConcurrentMap<TopicPartition, Metric> recordsLagMetrics;
+
+    // Kafka raw metric for bytes consumed total
+    @Nullable private Metric bytesConsumedTotalMetric;
+
+    public KafkaSourceReaderMetrics(SourceReaderMetricGroup sourceReaderMetricGroup) {
+        this.sourceReaderMetricGroup = sourceReaderMetricGroup;
         this.kafkaSourceReaderMetricGroup =
-                parentMetricGroup.addGroup(KAFKA_SOURCE_READER_METRIC_GROUP);
+                sourceReaderMetricGroup.addGroup(KAFKA_SOURCE_READER_METRIC_GROUP);
         this.commitsSucceeded =
                 this.kafkaSourceReaderMetricGroup.counter(COMMITS_SUCCEEDED_METRIC_COUNTER);
         this.commitsFailed =
@@ -150,6 +173,77 @@ public class KafkaSourceReaderMetrics {
         commitsFailed.inc();
     }
 
+    /**
+     * Register {@link MetricNames#IO_NUM_BYTES_IN}.
+     *
+     * @param consumer Kafka consumer
+     */
+    public void registerNumBytesIn(KafkaConsumer<?, ?> consumer) {
+        try {
+            Predicate<Map.Entry<MetricName, ? extends Metric>> filter =
+                    (entry) ->
+                            entry.getKey().group().equals(CONSUMER_FETCH_MANAGER_GROUP)
+                                    && entry.getKey().name().equals(BYTES_CONSUMED_TOTAL)
+                                    && !entry.getKey().tags().containsKey("topic");
+            this.bytesConsumedTotalMetric = MetricUtil.getKafkaMetric(consumer.metrics(), filter);
+        } catch (IllegalStateException e) {
+            LOG.warn(
+                    String.format(
+                            "Error when getting Kafka consumer metric \"%s\". "
+                                    + "I/O metric \"%s\" will not be reported. ",
+                            BYTES_CONSUMED_TOTAL, MetricNames.IO_NUM_BYTES_IN),
+                    e);
+        }
+    }
+
+    /**
+     * Add a partition's records-lag metric to tracking list if this partition never appears before.
+     *
+     * <p>This method also lazily register {@link
+     * org.apache.flink.runtime.metrics.MetricNames#PENDING_RECORDS} in {@link
+     * SourceReaderMetricGroup}
+     *
+     * @param consumer Kafka consumer
+     * @param tp Topic partition
+     */
+    public void maybeAddRecordsLagMetric(KafkaConsumer<?, ?> consumer, TopicPartition tp) {
+        // Lazily register pendingRecords
+        if (recordsLagMetrics == null) {
+            this.recordsLagMetrics = new ConcurrentHashMap<>();
+            this.sourceReaderMetricGroup.setPendingRecordsGauge(
+                    () -> {
+                        long pendingRecordsTotal = 0;
+                        for (Metric recordsLagMetric : this.recordsLagMetrics.values()) {
+                            pendingRecordsTotal +=
+                                    ((Double) recordsLagMetric.metricValue()).longValue();
+                        }
+                        return pendingRecordsTotal;
+                    });
+        }
+        recordsLagMetrics.computeIfAbsent(
+                tp, (ignored) -> getRecordsLagMetric(consumer.metrics(), tp));
+    }
+
+    /**
+     * Remove a partition's records-lag metric from tracking list.
+     *
+     * @param tp Unassigned topic partition
+     */
+    public void removeRecordsLagMetric(TopicPartition tp) {
+        if (recordsLagMetrics != null) {
+            recordsLagMetrics.remove(tp);
+        }
+    }
+
+    /** Update {@link org.apache.flink.runtime.metrics.MetricNames#IO_NUM_BYTES_IN}. */
+    public void updateNumBytesInCounter() {
+        if (this.bytesConsumedTotalMetric != null) {
+            MetricUtil.sync(
+                    this.bytesConsumedTotalMetric,
+                    this.sourceReaderMetricGroup.getIOMetricGroup().getNumBytesInCounter());
+        }
+    }
+
     // -------- Helper functions --------
     private void registerOffsetMetricsForTopicPartition(TopicPartition tp) {
         final MetricGroup topicPartitionGroup =
@@ -175,6 +269,34 @@ public class KafkaSourceReaderMetrics {
         }
     }
 
+    private @Nullable Metric getRecordsLagMetric(
+            Map<MetricName, ? extends Metric> metrics, TopicPartition tp) {
+        try {
+            Predicate<Map.Entry<MetricName, ? extends Metric>> filter =
+                    entry -> {
+                        final MetricName metricName = entry.getKey();
+                        final Map<String, String> tags = metricName.tags();
+
+                        return metricName.group().equals(CONSUMER_FETCH_MANAGER_GROUP)
+                                && metricName.name().equals(RECORDS_LAG)
+                                && tags.containsKey("topic")
+                                && tags.get("topic").equals(tp.topic())
+                                && tags.containsKey("partition")
+                                && tags.get("partition").equals(String.valueOf(tp.partition()));
+                    };
+            return MetricUtil.getKafkaMetric(metrics, filter);
+        } catch (IllegalStateException e) {
+            LOG.warn(
+                    String.format(
+                            "Error when getting Kafka consumer metric \"%s\" "
+                                    + "for partition \"%s\". "
+                                    + "Metric \"%s\" may not be reported correctly. ",
+                            RECORDS_LAG, tp, MetricNames.PENDING_BYTES),
+                    e);
+            return null;
+        }
+    }
+
     private static class Offset {
         long currentOffset;
         long committedOffset;
diff --git a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/KafkaPartitionSplitReader.java b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/KafkaPartitionSplitReader.java
index e5e142516c8..fbc7eccb7d4 100644
--- a/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/KafkaPartitionSplitReader.java
+++ b/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/KafkaPartitionSplitReader.java
@@ -18,6 +18,7 @@
 
 package org.apache.flink.connector.kafka.source.reader;
 
+import org.apache.flink.api.connector.source.SourceReaderContext;
 import org.apache.flink.api.java.tuple.Tuple3;
 import org.apache.flink.connector.base.source.reader.RecordsWithSplitIds;
 import org.apache.flink.connector.base.source.reader.splitreader.SplitReader;
@@ -75,14 +76,15 @@ public class KafkaPartitionSplitReader<T>
     private final SimpleCollector<T> collector;
     private final String groupId;
     private final int subtaskId;
+
     private final KafkaSourceReaderMetrics kafkaSourceReaderMetrics;
 
     public KafkaPartitionSplitReader(
             Properties props,
             KafkaRecordDeserializationSchema<T> deserializationSchema,
-            int subtaskId,
+            SourceReaderContext context,
             KafkaSourceReaderMetrics kafkaSourceReaderMetrics) {
-        this.subtaskId = subtaskId;
+        this.subtaskId = context.getIndexOfSubtask();
         this.kafkaSourceReaderMetrics = kafkaSourceReaderMetrics;
         Properties consumerProps = new Properties();
         consumerProps.putAll(props);
@@ -92,7 +94,10 @@ public class KafkaPartitionSplitReader<T>
         this.deserializationSchema = deserializationSchema;
         this.collector = new SimpleCollector<>();
         this.groupId = consumerProps.getProperty(ConsumerConfig.GROUP_ID_CONFIG);
+
+        // Metric registration
         maybeRegisterKafkaConsumerMetrics(props, kafkaSourceReaderMetrics, consumer);
+        this.kafkaSourceReaderMetrics.registerNumBytesIn(consumer);
     }
 
     @Override
@@ -173,12 +178,20 @@ public class KafkaPartitionSplitReader<T>
                 kafkaSourceReaderMetrics.recordCurrentOffset(
                         tp, recordsFromPartition.get(recordsFromPartition.size() - 1).offset());
             }
+
+            // Track this partition's record lag if it never appears before
+            kafkaSourceReaderMetrics.maybeAddRecordsLagMetric(consumer, tp);
         }
         // Unassign the partitions that has finished.
         if (!finishedPartitions.isEmpty()) {
+            finishedPartitions.forEach(kafkaSourceReaderMetrics::removeRecordsLagMetric);
             unassignPartitions(finishedPartitions);
         }
         recordsBySplits.prepareForRead();
+
+        // Update numBytesIn
+        kafkaSourceReaderMetrics.updateNumBytesInCounter();
+
         return recordsBySplits;
     }
 
diff --git a/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/source/metrics/KafkaSourceReaderMetricsTest.java b/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/source/metrics/KafkaSourceReaderMetricsTest.java
index e43a3e9a5ce..0a1faa5f3be 100644
--- a/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/source/metrics/KafkaSourceReaderMetricsTest.java
+++ b/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/source/metrics/KafkaSourceReaderMetricsTest.java
@@ -21,6 +21,7 @@ package org.apache.flink.connector.kafka.source.metrics;
 import org.apache.flink.metrics.Counter;
 import org.apache.flink.metrics.Gauge;
 import org.apache.flink.metrics.testutils.MetricListener;
+import org.apache.flink.runtime.metrics.groups.InternalSourceReaderMetricGroup;
 
 import org.apache.kafka.common.TopicPartition;
 import org.junit.Test;
@@ -46,7 +47,8 @@ public class KafkaSourceReaderMetricsTest {
         MetricListener metricListener = new MetricListener();
 
         final KafkaSourceReaderMetrics kafkaSourceReaderMetrics =
-                new KafkaSourceReaderMetrics(metricListener.getMetricGroup());
+                new KafkaSourceReaderMetrics(
+                        InternalSourceReaderMetricGroup.mock(metricListener.getMetricGroup()));
 
         kafkaSourceReaderMetrics.registerTopicPartition(FOO_0);
         kafkaSourceReaderMetrics.registerTopicPartition(FOO_1);
@@ -69,7 +71,8 @@ public class KafkaSourceReaderMetricsTest {
         MetricListener metricListener = new MetricListener();
 
         final KafkaSourceReaderMetrics kafkaSourceReaderMetrics =
-                new KafkaSourceReaderMetrics(metricListener.getMetricGroup());
+                new KafkaSourceReaderMetrics(
+                        InternalSourceReaderMetricGroup.mock(metricListener.getMetricGroup()));
 
         kafkaSourceReaderMetrics.registerTopicPartition(FOO_0);
         kafkaSourceReaderMetrics.registerTopicPartition(FOO_1);
@@ -102,7 +105,8 @@ public class KafkaSourceReaderMetricsTest {
     public void testNonTrackingTopicPartition() {
         MetricListener metricListener = new MetricListener();
         final KafkaSourceReaderMetrics kafkaSourceReaderMetrics =
-                new KafkaSourceReaderMetrics(metricListener.getMetricGroup());
+                new KafkaSourceReaderMetrics(
+                        InternalSourceReaderMetricGroup.mock(metricListener.getMetricGroup()));
         assertThrows(
                 IllegalArgumentException.class,
                 () -> kafkaSourceReaderMetrics.recordCurrentOffset(FOO_0, 15213L));
@@ -115,7 +119,8 @@ public class KafkaSourceReaderMetricsTest {
     public void testFailedCommit() {
         MetricListener metricListener = new MetricListener();
         final KafkaSourceReaderMetrics kafkaSourceReaderMetrics =
-                new KafkaSourceReaderMetrics(metricListener.getMetricGroup());
+                new KafkaSourceReaderMetrics(
+                        InternalSourceReaderMetricGroup.mock(metricListener.getMetricGroup()));
         kafkaSourceReaderMetrics.recordFailedCommit();
         final Optional<Counter> commitsFailedCounter =
                 metricListener.getCounter(
diff --git a/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/source/reader/KafkaPartitionSplitReaderTest.java b/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/source/reader/KafkaPartitionSplitReaderTest.java
index 175c6efdf83..97f29bad0f3 100644
--- a/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/source/reader/KafkaPartitionSplitReaderTest.java
+++ b/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/source/reader/KafkaPartitionSplitReaderTest.java
@@ -19,6 +19,7 @@
 package org.apache.flink.connector.kafka.source.reader;
 
 import org.apache.flink.api.java.tuple.Tuple3;
+import org.apache.flink.configuration.Configuration;
 import org.apache.flink.connector.base.source.reader.RecordsWithSplitIds;
 import org.apache.flink.connector.base.source.reader.splitreader.SplitsAddition;
 import org.apache.flink.connector.base.source.reader.splitreader.SplitsChange;
@@ -27,12 +28,22 @@ import org.apache.flink.connector.kafka.source.reader.deserializer.KafkaRecordDe
 import org.apache.flink.connector.kafka.source.split.KafkaPartitionSplit;
 import org.apache.flink.connector.kafka.source.testutils.KafkaSourceTestEnv;
 import org.apache.flink.connector.testutils.source.deserialization.TestingDeserializationContext;
+import org.apache.flink.connector.testutils.source.reader.TestingReaderContext;
+import org.apache.flink.metrics.Counter;
+import org.apache.flink.metrics.Gauge;
+import org.apache.flink.metrics.groups.OperatorMetricGroup;
+import org.apache.flink.metrics.groups.SourceReaderMetricGroup;
+import org.apache.flink.metrics.groups.UnregisteredMetricsGroup;
+import org.apache.flink.metrics.testutils.MetricListener;
+import org.apache.flink.runtime.metrics.MetricNames;
+import org.apache.flink.runtime.metrics.groups.InternalSourceReaderMetricGroup;
 import org.apache.flink.runtime.metrics.groups.UnregisteredMetricGroups;
 
 import org.apache.kafka.clients.consumer.ConsumerConfig;
 import org.apache.kafka.common.TopicPartition;
 import org.apache.kafka.common.serialization.ByteArrayDeserializer;
 import org.apache.kafka.common.serialization.IntegerDeserializer;
+import org.hamcrest.Matchers;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 import org.junit.Test;
@@ -46,12 +57,18 @@ import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
+import java.util.Optional;
 import java.util.Properties;
 import java.util.Set;
 import java.util.concurrent.atomic.AtomicReference;
 
+import static org.apache.flink.connector.kafka.source.testutils.KafkaSourceTestEnv.NUM_RECORDS_PER_PARTITION;
+import static org.hamcrest.MatcherAssert.assertThat;
 import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertNotNull;
 import static org.junit.Assert.assertNull;
+import static org.junit.Assert.assertTrue;
 
 /** Unit tests for {@link KafkaPartitionSplitReader}. */
 public class KafkaPartitionSplitReaderTest {
@@ -115,6 +132,76 @@ public class KafkaPartitionSplitReaderTest {
         assertNull(error.get());
     }
 
+    @Test
+    public void testNumBytesInCounter() throws Exception {
+        final OperatorMetricGroup operatorMetricGroup =
+                UnregisteredMetricGroups.createUnregisteredOperatorMetricGroup();
+        final Counter numBytesInCounter =
+                operatorMetricGroup.getIOMetricGroup().getNumBytesInCounter();
+        KafkaPartitionSplitReader<Integer> reader =
+                createReader(
+                        new Properties(),
+                        InternalSourceReaderMetricGroup.wrap(operatorMetricGroup));
+        // Add a split
+        reader.handleSplitsChanges(
+                new SplitsAddition<>(
+                        Collections.singletonList(
+                                new KafkaPartitionSplit(new TopicPartition(TOPIC1, 0), 0L))));
+        reader.fetch();
+        final long latestNumBytesIn = numBytesInCounter.getCount();
+        // Since it's hard to know the exact number of bytes consumed, we just check if it is
+        // greater than 0
+        assertThat(latestNumBytesIn, Matchers.greaterThan(0L));
+        // Add another split
+        reader.handleSplitsChanges(
+                new SplitsAddition<>(
+                        Collections.singletonList(
+                                new KafkaPartitionSplit(new TopicPartition(TOPIC2, 0), 0L))));
+        reader.fetch();
+        // We just check if numBytesIn is increasing
+        assertThat(numBytesInCounter.getCount(), Matchers.greaterThan(latestNumBytesIn));
+    }
+
+    @Test
+    public void testPendingRecordsGauge() throws Exception {
+        MetricListener metricListener = new MetricListener();
+        final Properties props = new Properties();
+        props.setProperty(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, "1");
+        KafkaPartitionSplitReader<Integer> reader =
+                createReader(
+                        props,
+                        InternalSourceReaderMetricGroup.mock(metricListener.getMetricGroup()));
+        // Add a split
+        reader.handleSplitsChanges(
+                new SplitsAddition<>(
+                        Collections.singletonList(
+                                new KafkaPartitionSplit(new TopicPartition(TOPIC1, 0), 0L))));
+        // pendingRecords should have not been registered because of lazily registration
+        assertFalse(metricListener.getGauge(MetricNames.PENDING_RECORDS).isPresent());
+        // Trigger first fetch
+        reader.fetch();
+        final Optional<Gauge<Long>> pendingRecords =
+                metricListener.getGauge(MetricNames.PENDING_RECORDS);
+        assertTrue(pendingRecords.isPresent());
+        // Validate pendingRecords
+        assertNotNull(pendingRecords);
+        assertEquals(NUM_RECORDS_PER_PARTITION - 1, (long) pendingRecords.get().getValue());
+        for (int i = 1; i < NUM_RECORDS_PER_PARTITION; i++) {
+            reader.fetch();
+            assertEquals(NUM_RECORDS_PER_PARTITION - i - 1, (long) pendingRecords.get().getValue());
+        }
+        // Add another split
+        reader.handleSplitsChanges(
+                new SplitsAddition<>(
+                        Collections.singletonList(
+                                new KafkaPartitionSplit(new TopicPartition(TOPIC2, 0), 0L))));
+        // Validate pendingRecords
+        for (int i = 0; i < NUM_RECORDS_PER_PARTITION; i++) {
+            reader.fetch();
+            assertEquals(NUM_RECORDS_PER_PARTITION - i - 1, (long) pendingRecords.get().getValue());
+        }
+    }
+
     // ------------------
 
     private void assignSplitsAndFetchUntilFinish(
@@ -160,8 +247,7 @@ public class KafkaPartitionSplitReaderTest {
                 (splitId, recordCount) -> {
                     TopicPartition tp = splits.get(splitId).getTopicPartition();
                     long earliestOffset = earliestOffsets.get(tp);
-                    long expectedRecordCount =
-                            KafkaSourceTestEnv.NUM_RECORDS_PER_PARTITION - earliestOffset;
+                    long expectedRecordCount = NUM_RECORDS_PER_PARTITION - earliestOffset;
                     assertEquals(
                             String.format(
                                     "%s should have %d records.",
@@ -174,17 +260,29 @@ public class KafkaPartitionSplitReaderTest {
     // ------------------
 
     private KafkaPartitionSplitReader<Integer> createReader() throws Exception {
+        return createReader(
+                new Properties(), UnregisteredMetricsGroup.createSourceReaderMetricGroup());
+    }
+
+    private KafkaPartitionSplitReader<Integer> createReader(
+            Properties additionalProperties, SourceReaderMetricGroup sourceReaderMetricGroup)
+            throws Exception {
         Properties props = new Properties();
         props.putAll(KafkaSourceTestEnv.getConsumerProperties(ByteArrayDeserializer.class));
         props.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "none");
+        if (!additionalProperties.isEmpty()) {
+            props.putAll(additionalProperties);
+        }
         KafkaRecordDeserializationSchema<Integer> deserializationSchema =
                 KafkaRecordDeserializationSchema.valueOnly(IntegerDeserializer.class);
         deserializationSchema.open(new TestingDeserializationContext());
         KafkaSourceReaderMetrics kafkaSourceReaderMetrics =
-                new KafkaSourceReaderMetrics(
-                        UnregisteredMetricGroups.createUnregisteredOperatorMetricGroup());
+                new KafkaSourceReaderMetrics(sourceReaderMetricGroup);
         return new KafkaPartitionSplitReader<>(
-                props, deserializationSchema, 0, kafkaSourceReaderMetrics);
+                props,
+                deserializationSchema,
+                new TestingReaderContext(new Configuration(), sourceReaderMetricGroup),
+                kafkaSourceReaderMetrics);
     }
 
     private Map<String, KafkaPartitionSplit> assignSplits(
