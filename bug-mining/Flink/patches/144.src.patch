diff --git a/docs/dataset_transformations.md b/docs/dataset_transformations.md
index f17c000ab72..4460f81abdd 100644
--- a/docs/dataset_transformations.md
+++ b/docs/dataset_transformations.md
@@ -885,6 +885,65 @@ val result1 = input1.joinWithHuge(input2).where(0).equalTo(0)
 </div>
 </div>
 
+#### Join Algorithm Hints
+
+The Flink runtime can execute joins in various ways. Each possible way outperforms the others under
+different circumstances. The system tries to pick a reasonable way automatically, but allows you
+to manually pick a strategy, in case you want to enforce a specific way of executing the join.
+
+<div class="codetabs" markdown="1">
+<div data-lang="java" markdown="1">
+
+~~~java
+DataSet<SomeType> input1 = // [...]
+DataSet<AnotherType> input2 = // [...]
+
+DataSet<Tuple2<SomeType, AnotherType> result = 
+      input1.join(input2, BROADCAST_HASH_FIRST)
+            .where("id").equalTo("key");
+~~~
+
+</div>
+<div data-lang="scala" markdown="1">
+
+~~~scala
+val input1: DataSet[SomeType] = // [...]
+val input2: DataSet[AnotherType] = // [...]
+
+// hint that the second DataSet is very small
+val result1 = input1.join(input2, BROADCAST_HASH_FIRST).where("id").equalTo("key")
+
+~~~
+
+</div>
+</div>
+
+The following hints are available:
+
+* OPTIMIZER_CHOOSES: Equivalent to not giving a hint at all, leaves the choice to the system.
+
+* BROADCAST_HASH_FIRST: Broadcasts the first input and builds a hash table from it, which is
+  probed by the second input. A good strategy if the first input is very small.
+
+* BROADCAST_HASH_SECOND: Broadcasts the second input and builds a hash table from it, which is
+  probed by the first input. A good strategy if the second input is very small.
+
+* REPARTITION_HASH_FIRST: The system partitions (shuffles) each input (unless the input is already
+  partitioned) and builds a hash table from the first input. This strategy is good if the first
+  input is smaller than the second, but both inputs are still large.
+  *Note:* This is the default fallback strategy that the system uses if no size estimates can be made
+  and no pre-existing partitiongs and sort-orders can be re-used.
+
+* REPARTITION_HASH_SECOND: The system partitions (shuffles) each input (unless the input is already
+  partitioned) and builds a hash table from the second input. This strategy is good if the second
+  input is smaller than the first, but both inputs are still large.
+
+* REPARTITION_SORT_MERGE: The system partitions (shuffles) each input (unless the input is already
+  partitioned) and sorts each input (unless it is already sorted). The inputs are joined by
+  a streamed merge of the sorted inputs. This strategy is good if one or both of the inputs are
+  already sorted.
+
+
 ### Cross
 
 The Cross transformation combines two DataSets into one DataSet. It builds all pairwise combinations of the elements of both input DataSets, i.e., it builds a Cartesian product.
diff --git a/docs/programming_guide.md b/docs/programming_guide.md
index dcf1095db11..e7ac064ae29 100644
--- a/docs/programming_guide.md
+++ b/docs/programming_guide.md
@@ -569,6 +569,20 @@ result = input1.join(input2)
                .where(0)       // key of the first input (tuple field 0)
                .equalTo(1);    // key of the second input (tuple field 1)
 {% endhighlight %}
+        You can specify the way that the runtime executes the join via <i>Join Hints</i>. The hints
+        describe whether the join happens through partitioning or broadcasting, and whether it uses
+        a sort-based or a hash-based algorithm. Please refer to the 
+        <a href="dataset_transformations.html#join-algorithm-hints">Transformations Guide</a> for
+        a list of possible hints and an example.</br>
+        If no hint is specified, the system will try to make an estimate of the input sizes and
+        pick a the best strategy according to those estimates. 
+{% highlight java %}
+// This executes a join by broadcasting the first data set
+// using a hash table for the broadcasted data
+result = input1.join(input2, JoinHint.BROADCAST_HASH_FIRST)
+               .where(0).equalTo(1);
+{% endhighlight %}
+
       </td>
     </tr>
 
@@ -789,10 +803,23 @@ val output: DataSet[(Int, String, Doublr)] = input.sum(0).min(2)
         Optionally uses a JoinFunction to turn the pair of elements into a single element, or a
         FlatJoinFunction to turn the pair of elements into arbitararily many (including none)
         elements. See <a href="#specifying-keys">keys</a> on how to define join keys.
-{% highlight java %}
+{% highlight scala %}
 // In this case tuple fields are used as keys. "0" is the join field on the first tuple
 // "1" is the join field on the second tuple.
 val result = input1.join(input2).where(0).equalTo(1)
+{% endhighlight %}
+        You can specify the way that the runtime executes the join via <i>Join Hints</i>. The hints
+        describe whether the join happens through partitioning or broadcasting, and whether it uses
+        a sort-based or a hash-based algorithm. Please refer to the 
+        <a href="dataset_transformations.html#join-algorithm-hints">Transformations Guide</a> for
+        a list of possible hints and an example.</br>
+        If no hint is specified, the system will try to make an estimate of the input sizes and
+        pick a the best strategy according to those estimates.
+{% highlight scala %}
+// This executes a join by broadcasting the first data set
+// using a hash table for the broadcasted data
+val result = input1.join(input2, JoinHint.BROADCAST_HASH_FIRST)
+                   .where(0).equalTo(1)
 {% endhighlight %}
       </td>
     </tr>
@@ -803,7 +830,7 @@ val result = input1.join(input2).where(0).equalTo(1)
         <p>The two-dimensional variant of the reduce operation. Groups each input on one or more
         fields and then joins the groups. The transformation function is called per pair of groups.
         See <a href="#specifying-keys">keys</a> on how to define coGroup keys.</p>
-{% highlight java %}
+{% highlight scala %}
 data1.coGroup(data2).where(0).equalTo(1)
 {% endhighlight %}
       </td>
@@ -815,7 +842,7 @@ data1.coGroup(data2).where(0).equalTo(1)
         <p>Builds the Cartesian product (cross product) of two inputs, creating all pairs of
         elements. Optionally uses a CrossFunction to turn the pair of elements into a single
         element</p>
-{% highlight java %}
+{% highlight scala %}
 val data1: DataSet[Int] = // [...]
 val data2: DataSet[String] = // [...]
 val result: DataSet[(Int, String)] = data1.cross(data2)
diff --git a/flink-core/src/main/java/org/apache/flink/api/common/operators/base/JoinOperatorBase.java b/flink-core/src/main/java/org/apache/flink/api/common/operators/base/JoinOperatorBase.java
index bc0f4a0f524..555175dbd3d 100644
--- a/flink-core/src/main/java/org/apache/flink/api/common/operators/base/JoinOperatorBase.java
+++ b/flink-core/src/main/java/org/apache/flink/api/common/operators/base/JoinOperatorBase.java
@@ -53,8 +53,9 @@ public class JoinOperatorBase<IN1, IN2, OUT, FT extends FlatJoinFunction<IN1, IN
 	 * An enumeration of hints, optionally usable to tell the system how exactly execute the join.
 	 */
 	public static enum JoinHint {
+		
 		/**
-		 * leave the choice how to do the join to the optimizer. If in doubt, the
+		 * Leave the choice how to do the join to the optimizer. If in doubt, the
 		 * optimizer will choose a repartitioning join.
 		 */
 		OPTIMIZER_CHOOSES,
@@ -91,7 +92,7 @@ public class JoinOperatorBase<IN1, IN2, OUT, FT extends FlatJoinFunction<IN1, IN
 		 * Hint that the join should repartitioning both inputs and use sorting and merging
 		 * as the join strategy.
 		 */
-		REPARTITION_SORT_MERGE,
+		REPARTITION_SORT_MERGE
 	};
 	
 	// --------------------------------------------------------------------------------------------
