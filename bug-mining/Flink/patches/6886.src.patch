diff --git a/flink-java/src/main/java/org/apache/flink/api/java/io/CollectionInputFormat.java b/flink-java/src/main/java/org/apache/flink/api/java/io/CollectionInputFormat.java
index b85f70507b1..5fe69e870ff 100644
--- a/flink-java/src/main/java/org/apache/flink/api/java/io/CollectionInputFormat.java
+++ b/flink-java/src/main/java/org/apache/flink/api/java/io/CollectionInputFormat.java
@@ -43,9 +43,8 @@ public class CollectionInputFormat<T> extends GenericInputFormat<T> implements N
 
     private TypeSerializer<T> serializer;
 
-    private transient Collection<T>
-            dataSet; // input data as collection. transient, because it will be serialized in a
-    // custom way
+    // input data as collection. transient, because it will be serialized in a custom way
+    private transient Collection<T> dataSet;
 
     private transient Iterator<T> iterator;
 
diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/typeutils/TimeIndicatorTypeInfo.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/typeutils/TimeIndicatorTypeInfo.java
index 827b8047181..8020f0fcfb7 100644
--- a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/typeutils/TimeIndicatorTypeInfo.java
+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/typeutils/TimeIndicatorTypeInfo.java
@@ -22,7 +22,7 @@ import org.apache.flink.annotation.Internal;
 import org.apache.flink.api.common.ExecutionConfig;
 import org.apache.flink.api.common.typeinfo.SqlTimeTypeInfo;
 import org.apache.flink.api.common.typeutils.TypeSerializer;
-import org.apache.flink.api.common.typeutils.base.LocalDateTimeSerializer;
+import org.apache.flink.api.common.typeutils.base.LongSerializer;
 import org.apache.flink.api.common.typeutils.base.SqlTimestampComparator;
 import org.apache.flink.api.common.typeutils.base.SqlTimestampSerializer;
 import org.apache.flink.table.api.DataTypes;
@@ -30,7 +30,8 @@ import org.apache.flink.table.api.DataTypes;
 import java.sql.Timestamp;
 
 /**
- * Type information for indicating event or processing time.
+ * Type information for indicating event or processing time. However, it behaves like a regular SQL
+ * timestamp but is serialized as Long.
  *
  * @deprecated This class will be removed in future versions as it is used for the old type system.
  *     It is recommended to use {@link DataTypes} instead. Please make sure to use either the old or
@@ -61,10 +62,13 @@ public class TimeIndicatorTypeInfo extends SqlTimeTypeInfo<Timestamp> {
         this.isEventTime = isEventTime;
     }
 
+    // this replaces the effective serializer by a LongSerializer
+    // it is a hacky but efficient solution to keep the object creation overhead low but still
+    // be compatible with the corresponding SqlTimestampTypeInfo
     @Override
     @SuppressWarnings("unchecked")
     public TypeSerializer<Timestamp> createSerializer(ExecutionConfig executionConfig) {
-        return (TypeSerializer) LocalDateTimeSerializer.INSTANCE;
+        return (TypeSerializer) LongSerializer.INSTANCE;
     }
 
     public boolean isEventTime() {
diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/TableSchemaUtils.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/TableSchemaUtils.java
index 0ab5fc5a6f4..c23c68664f3 100644
--- a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/TableSchemaUtils.java
+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/TableSchemaUtils.java
@@ -24,6 +24,7 @@ import org.apache.flink.table.api.TableSchema;
 import org.apache.flink.table.api.ValidationException;
 import org.apache.flink.table.api.WatermarkSpec;
 import org.apache.flink.table.api.constraints.UniqueConstraint;
+import org.apache.flink.table.catalog.ResolvedSchema;
 import org.apache.flink.table.sinks.TableSink;
 import org.apache.flink.table.sources.TableSource;
 import org.apache.flink.table.types.utils.DataTypeUtils;
@@ -31,6 +32,7 @@ import org.apache.flink.util.Preconditions;
 
 import java.util.List;
 import java.util.Optional;
+import java.util.stream.Collectors;
 
 /** Utilities to {@link TableSchema}. */
 @Internal
@@ -98,6 +100,21 @@ public class TableSchemaUtils {
         }
     }
 
+    /** Removes time attributes from the {@link ResolvedSchema} and build a {@link TableSchema}. */
+    public static TableSchema removeTimeAttributeFromResolvedSchema(ResolvedSchema resolvedSchema) {
+        return TableSchema.fromResolvedSchema(
+                new ResolvedSchema(
+                        resolvedSchema.getColumns().stream()
+                                .map(
+                                        col ->
+                                                col.copy(
+                                                        DataTypeUtils.removeTimeAttribute(
+                                                                col.getDataType())))
+                                .collect(Collectors.toList()),
+                        resolvedSchema.getWatermarkSpecs(),
+                        resolvedSchema.getPrimaryKey().orElse(null)));
+    }
+
     /**
      * Creates a builder with given table schema.
      *
diff --git a/flink-table/flink-table-common/src/test/java/org/apache/flink/table/utils/TableSchemaUtilsTest.java b/flink-table/flink-table-common/src/test/java/org/apache/flink/table/utils/TableSchemaUtilsTest.java
index 3fe5c42141b..28b7f96d36c 100644
--- a/flink-table/flink-table-common/src/test/java/org/apache/flink/table/utils/TableSchemaUtilsTest.java
+++ b/flink-table/flink-table-common/src/test/java/org/apache/flink/table/utils/TableSchemaUtilsTest.java
@@ -19,11 +19,24 @@
 package org.apache.flink.table.utils;
 
 import org.apache.flink.table.api.DataTypes;
+import org.apache.flink.table.api.TableColumn;
 import org.apache.flink.table.api.TableSchema;
 import org.apache.flink.table.api.ValidationException;
+import org.apache.flink.table.catalog.Column;
+import org.apache.flink.table.catalog.ResolvedSchema;
+import org.apache.flink.table.catalog.UniqueConstraint;
+import org.apache.flink.table.catalog.WatermarkSpec;
+import org.apache.flink.table.expressions.utils.ResolvedExpressionMock;
+import org.apache.flink.table.types.DataType;
+import org.apache.flink.table.types.logical.TimestampKind;
+import org.apache.flink.table.types.logical.TimestampType;
+import org.apache.flink.table.types.utils.DataTypeUtils;
 
 import org.junit.jupiter.api.Test;
 
+import java.util.Arrays;
+import java.util.Collections;
+
 import static org.assertj.core.api.Assertions.assertThat;
 import static org.assertj.core.api.Assertions.assertThatThrownBy;
 
@@ -72,4 +85,35 @@ class TableSchemaUtilsTest {
                 .isInstanceOf(ValidationException.class)
                 .hasMessage("Constraint ct2 to drop does not exist");
     }
+
+    @Test
+    void testRemoveTimeAttribute() {
+        DataType rowTimeType =
+                DataTypeUtils.replaceLogicalType(
+                        DataTypes.TIMESTAMP(3), new TimestampType(true, TimestampKind.ROWTIME, 3));
+        ResolvedSchema schema =
+                new ResolvedSchema(
+                        Arrays.asList(
+                                Column.physical("id", DataTypes.INT().notNull()),
+                                Column.physical("t", rowTimeType),
+                                Column.computed(
+                                        "date",
+                                        ResolvedExpressionMock.of(DataTypes.DATE(), "TO_DATE(t)")),
+                                Column.metadata("metadata-1", DataTypes.INT(), "metadata", false)),
+                        Collections.singletonList(
+                                WatermarkSpec.of("t", ResolvedExpressionMock.of(rowTimeType, "t"))),
+                        UniqueConstraint.primaryKey("test-pk", Collections.singletonList("id")));
+        assertThat(TableSchemaUtils.removeTimeAttributeFromResolvedSchema(schema))
+                .isEqualTo(
+                        TableSchema.builder()
+                                .field("id", DataTypes.INT().notNull())
+                                .field("t", DataTypes.TIMESTAMP(3))
+                                .field("date", DataTypes.DATE(), "TO_DATE(t)")
+                                .add(
+                                        TableColumn.metadata(
+                                                "metadata-1", DataTypes.INT(), "metadata", false))
+                                .watermark("t", "t", rowTimeType)
+                                .primaryKey("test-pk", new String[] {"id"})
+                                .build());
+    }
 }
diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/catalog/CatalogSchemaTable.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/catalog/CatalogSchemaTable.java
index a70f6bc6cfa..b45265e51c1 100644
--- a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/catalog/CatalogSchemaTable.java
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/catalog/CatalogSchemaTable.java
@@ -24,9 +24,11 @@ import org.apache.flink.table.api.TableSchema;
 import org.apache.flink.table.api.ValidationException;
 import org.apache.flink.table.catalog.CatalogBaseTable;
 import org.apache.flink.table.catalog.CatalogTable;
+import org.apache.flink.table.catalog.CatalogTableImpl;
 import org.apache.flink.table.catalog.Column;
 import org.apache.flink.table.catalog.ConnectorCatalogTable;
 import org.apache.flink.table.catalog.ContextResolvedTable;
+import org.apache.flink.table.catalog.ResolvedCatalogTable;
 import org.apache.flink.table.catalog.ResolvedSchema;
 import org.apache.flink.table.factories.TableFactoryUtil;
 import org.apache.flink.table.factories.TableSourceFactory;
@@ -40,6 +42,7 @@ import org.apache.flink.table.sources.TableSource;
 import org.apache.flink.table.sources.TableSourceValidation;
 import org.apache.flink.table.types.DataType;
 import org.apache.flink.table.types.logical.LogicalType;
+import org.apache.flink.table.utils.TableSchemaUtils;
 
 import org.apache.calcite.rel.type.RelDataType;
 import org.apache.calcite.rel.type.RelDataTypeFactory;
@@ -158,10 +161,23 @@ public class CatalogSchemaTable extends AbstractTable implements TemporalTable {
                 // Use an empty config for TableSourceFactoryContextImpl since we can't fetch the
                 // actual TableConfig here. And currently the empty config do not affect the logic.
                 ReadableConfig config = new Configuration();
+                // The input table is ResolvedCatalogTable that the
+                // rowtime/proctime contains {@link TimestampKind}. However, rowtime
+                // is the concept defined by the WatermarkGenerator and the
+                // WatermarkGenerator is responsible to convert the rowtime column
+                // to Long. For source, it only treats the rowtime column as regular
+                // timestamp. So, we remove the rowtime indicator here. Please take a
+                // look at the usage of the {@link DataTypeUtils#removeTimeAttribute}
+                ResolvedCatalogTable originTable = contextResolvedTable.getResolvedTable();
                 TableSourceFactory.Context context =
                         new TableSourceFactoryContextImpl(
                                 contextResolvedTable.getIdentifier(),
-                                contextResolvedTable.getResolvedTable(),
+                                new CatalogTableImpl(
+                                        TableSchemaUtils.removeTimeAttributeFromResolvedSchema(
+                                                originTable.getResolvedSchema()),
+                                        originTable.getPartitionKeys(),
+                                        originTable.getOptions(),
+                                        originTable.getComment()),
                                 config,
                                 contextResolvedTable.isTemporary());
                 TableSource<?> source = TableFactoryUtil.findAndCreateTableSource(context);
diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/FlinkCalciteCatalogReader.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/FlinkCalciteCatalogReader.java
index 3ac71474500..e19c8af5554 100644
--- a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/FlinkCalciteCatalogReader.java
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/FlinkCalciteCatalogReader.java
@@ -22,11 +22,13 @@ import org.apache.flink.configuration.Configuration;
 import org.apache.flink.table.api.ValidationException;
 import org.apache.flink.table.catalog.CatalogBaseTable;
 import org.apache.flink.table.catalog.CatalogTable;
+import org.apache.flink.table.catalog.CatalogTableImpl;
 import org.apache.flink.table.catalog.CatalogView;
 import org.apache.flink.table.catalog.ConnectorCatalogTable;
 import org.apache.flink.table.catalog.ObjectIdentifier;
 import org.apache.flink.table.catalog.QueryOperationCatalogView;
 import org.apache.flink.table.catalog.ResolvedCatalogBaseTable;
+import org.apache.flink.table.catalog.ResolvedCatalogTable;
 import org.apache.flink.table.descriptors.ConnectorDescriptorValidator;
 import org.apache.flink.table.descriptors.DescriptorProperties;
 import org.apache.flink.table.factories.TableFactoryUtil;
@@ -42,6 +44,7 @@ import org.apache.flink.table.planner.plan.stats.FlinkStatistic;
 import org.apache.flink.table.sources.LookupableTableSource;
 import org.apache.flink.table.sources.StreamTableSource;
 import org.apache.flink.table.sources.TableSource;
+import org.apache.flink.table.utils.TableSchemaUtils;
 
 import org.apache.calcite.config.CalciteConnectionConfig;
 import org.apache.calcite.jdbc.CalciteSchema;
@@ -217,10 +220,25 @@ public class FlinkCalciteCatalogReader extends CalciteCatalogReader {
             // try to create legacy table source using the options,
             // some legacy factories uses the new 'connector' key
             try {
+                // The input table is ResolvedCatalogTable that the
+                // rowtime/proctime contains {@link TimestampKind}. However, rowtime
+                // is the concept defined by the WatermarkGenerator and the
+                // WatermarkGenerator is responsible to convert the rowtime column
+                // to Long. For source, it only treats the rowtime column as regular
+                // timestamp. So, we erase the rowtime indicator here. Please take a
+                // look at the usage of the {@link
+                // DataTypeUtils#removeTimeAttribute}
+                ResolvedCatalogTable originTable =
+                        schemaTable.getContextResolvedTable().getResolvedTable();
                 TableFactoryUtil.findAndCreateTableSource(
                         schemaTable.getContextResolvedTable().getCatalog().orElse(null),
                         schemaTable.getContextResolvedTable().getIdentifier(),
-                        schemaTable.getContextResolvedTable().getResolvedTable(),
+                        new CatalogTableImpl(
+                                TableSchemaUtils.removeTimeAttributeFromResolvedSchema(
+                                        originTable.getResolvedSchema()),
+                                originTable.getPartitionKeys(),
+                                originTable.getOptions(),
+                                originTable.getComment()),
                         new Configuration(),
                         schemaTable.isTemporary());
                 // success, then we will use the legacy factories
diff --git a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/schema/LegacyCatalogSourceTable.scala b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/schema/LegacyCatalogSourceTable.scala
index 2a019ddc1a5..29535ea7a13 100644
--- a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/schema/LegacyCatalogSourceTable.scala
+++ b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/schema/LegacyCatalogSourceTable.scala
@@ -21,7 +21,7 @@ import org.apache.flink.configuration.ReadableConfig
 import org.apache.flink.table.api.{TableException, ValidationException}
 import org.apache.flink.table.api.TableColumn.ComputedColumn
 import org.apache.flink.table.api.config.TableConfigOptions
-import org.apache.flink.table.catalog.CatalogTable
+import org.apache.flink.table.catalog.{CatalogTable, CatalogTableImpl}
 import org.apache.flink.table.factories.TableFactoryUtil
 import org.apache.flink.table.planner.JMap
 import org.apache.flink.table.planner.calcite.{FlinkRelBuilder, FlinkTypeFactory}
@@ -30,6 +30,7 @@ import org.apache.flink.table.planner.hint.FlinkHints
 import org.apache.flink.table.planner.utils.ShortcutUtils.unwrapContext
 import org.apache.flink.table.sources.{StreamTableSource, TableSource, TableSourceValidation}
 import org.apache.flink.table.types.logical.{LocalZonedTimestampType, TimestampKind, TimestampType}
+import org.apache.flink.table.utils.TableSchemaUtils
 
 import org.apache.calcite.plan.{RelOptSchema, RelOptTable}
 import org.apache.calcite.rel.`type`.RelDataType
@@ -176,9 +177,15 @@ class LegacyCatalogSourceTable[T](
     val tableSource = TableFactoryUtil.findAndCreateTableSource(
       schemaTable.getContextResolvedTable.getCatalog.orElse(null),
       identifier,
-      tableToFind,
+      new CatalogTableImpl(
+        TableSchemaUtils.removeTimeAttributeFromResolvedSchema(
+          schemaTable.getContextResolvedTable.getResolvedSchema),
+        tableToFind.getPartitionKeys,
+        tableToFind.getOptions,
+        tableToFind.getComment),
       conf,
-      schemaTable.isTemporary)
+      schemaTable.isTemporary
+    )
 
     // validation
     val tableName = identifier.asSummaryString
