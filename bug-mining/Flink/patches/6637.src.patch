diff --git a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSourceITCase.java b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSourceITCase.java
index 91e98de041a..c982e4c1a6c 100644
--- a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSourceITCase.java
+++ b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSourceITCase.java
@@ -25,7 +25,6 @@ import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 import org.apache.flink.table.api.TableEnvironment;
 import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
 import org.apache.flink.table.connector.source.lookup.cache.LookupCache;
-import org.apache.flink.table.data.DecimalData;
 import org.apache.flink.table.data.GenericRowData;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.data.TimestampData;
@@ -46,12 +45,10 @@ import org.junit.jupiter.api.extension.RegisterExtension;
 import org.junit.jupiter.params.ParameterizedTest;
 import org.junit.jupiter.params.provider.EnumSource;
 
-import java.math.BigDecimal;
 import java.sql.Connection;
 import java.sql.DriverManager;
 import java.sql.SQLException;
 import java.sql.Statement;
-import java.sql.Time;
 import java.time.LocalDateTime;
 import java.util.ArrayList;
 import java.util.Arrays;
@@ -358,18 +355,14 @@ public class JdbcDynamicTableSourceITCase {
     }
 
     private void validateCachedValues(LookupCache cache) {
+        // jdbc does support project push down, the cached row has been projected
         RowData key1 = GenericRowData.of(1L);
         RowData value1 =
                 GenericRowData.of(
                         1L,
                         TimestampData.fromLocalDateTime(
                                 LocalDateTime.parse("2020-01-01T15:35:00.123456")),
-                        TimestampData.fromLocalDateTime(
-                                LocalDateTime.parse("2020-01-01T15:35:00.123456789")),
-                        (int) (Time.valueOf("15:35:00").toLocalTime().toNanoOfDay() / 1_000_000L),
-                        Float.valueOf("1.175E-37"),
-                        Double.valueOf("1.79769E308"),
-                        DecimalData.fromBigDecimal(BigDecimal.valueOf(100.1234), 10, 4));
+                        Double.valueOf("1.79769E308"));
 
         RowData key2 = GenericRowData.of(2L);
         RowData value2 =
@@ -377,12 +370,7 @@ public class JdbcDynamicTableSourceITCase {
                         2L,
                         TimestampData.fromLocalDateTime(
                                 LocalDateTime.parse("2020-01-01T15:36:01.123456")),
-                        TimestampData.fromLocalDateTime(
-                                LocalDateTime.parse("2020-01-01T15:36:01.123456789")),
-                        (int) (Time.valueOf("15:36:01").toLocalTime().toNanoOfDay() / 1_000_000L),
-                        Float.valueOf("-1.175E-37"),
-                        Double.valueOf("-1.79769E308"),
-                        DecimalData.fromBigDecimal(BigDecimal.valueOf(101.1234), 10, 4));
+                        Double.valueOf("-1.79769E308"));
 
         RowData key3 = GenericRowData.of(3L);
 
diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/rules/logical/ProjectSnapshotTransposeRule.java b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/rules/logical/ProjectSnapshotTransposeRule.java
new file mode 100644
index 00000000000..27c9dc7e45c
--- /dev/null
+++ b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/rules/logical/ProjectSnapshotTransposeRule.java
@@ -0,0 +1,77 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.planner.plan.rules.logical;
+
+import org.apache.calcite.plan.RelOptRule;
+import org.apache.calcite.plan.RelOptRuleCall;
+import org.apache.calcite.plan.RelRule;
+import org.apache.calcite.rel.RelNode;
+import org.apache.calcite.rel.logical.LogicalProject;
+import org.apache.calcite.rel.logical.LogicalSnapshot;
+
+/** Transpose {@link LogicalProject} past into {@link LogicalSnapshot}. */
+public class ProjectSnapshotTransposeRule extends RelRule<ProjectSnapshotTransposeRule.Config> {
+
+    public static final RelOptRule INSTANCE =
+            ProjectSnapshotTransposeRule.Config.EMPTY.as(Config.class).withOperator().toRule();
+
+    public ProjectSnapshotTransposeRule(Config config) {
+        super(config);
+    }
+
+    @Override
+    public boolean matches(RelOptRuleCall call) {
+        LogicalProject project = call.rel(0);
+        // Don't push a project which contains over into a snapshot, snapshot on window aggregate is
+        // unsupported for now.
+        return !project.containsOver();
+    }
+
+    @Override
+    public void onMatch(RelOptRuleCall call) {
+        LogicalProject project = call.rel(0);
+        LogicalSnapshot snapshot = call.rel(1);
+        RelNode newProject = project.copy(project.getTraitSet(), snapshot.getInputs());
+        RelNode newSnapshot =
+                snapshot.copy(snapshot.getTraitSet(), newProject, snapshot.getPeriod());
+        call.transformTo(newSnapshot);
+    }
+
+    /** Configuration for {@link ProjectSnapshotTransposeRule}. */
+    public interface Config extends RelRule.Config {
+
+        @Override
+        default RelOptRule toRule() {
+            return new ProjectSnapshotTransposeRule(this);
+        }
+
+        default ProjectSnapshotTransposeRule.Config withOperator() {
+            final RelRule.OperandTransform snapshotTransform =
+                    operandBuilder -> operandBuilder.operand(LogicalSnapshot.class).noInputs();
+
+            final RelRule.OperandTransform projectTransform =
+                    operandBuilder ->
+                            operandBuilder
+                                    .operand(LogicalProject.class)
+                                    .oneInput(snapshotTransform);
+
+            return withOperandSupplier(projectTransform).as(Config.class);
+        }
+    }
+}
diff --git a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/rules/FlinkBatchRuleSets.scala b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/rules/FlinkBatchRuleSets.scala
index 691ca43f339..39fa34cbaf6 100644
--- a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/rules/FlinkBatchRuleSets.scala
+++ b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/rules/FlinkBatchRuleSets.scala
@@ -237,7 +237,8 @@ object FlinkBatchRuleSets {
     PushProjectIntoLegacyTableSourceScanRule.INSTANCE,
     PushFilterIntoTableSourceScanRule.INSTANCE,
     PushFilterIntoLegacyTableSourceScanRule.INSTANCE,
-
+    // transpose project and snapshot for scan optimization
+    ProjectSnapshotTransposeRule.INSTANCE,
     // reorder sort and projection
     CoreRules.SORT_PROJECT_TRANSPOSE,
     // remove unnecessary sort rule
diff --git a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/rules/FlinkStreamRuleSets.scala b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/rules/FlinkStreamRuleSets.scala
index 97312367287..f46ef46a7c0 100644
--- a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/rules/FlinkStreamRuleSets.scala
+++ b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/rules/FlinkStreamRuleSets.scala
@@ -236,6 +236,9 @@ object FlinkStreamRuleSets {
     PushFilterIntoLegacyTableSourceScanRule.INSTANCE,
     PushLimitIntoTableSourceScanRule.INSTANCE,
 
+    // transpose project and snapshot for scan optimization
+    ProjectSnapshotTransposeRule.INSTANCE,
+
     // reorder the project and watermark assigner
     ProjectWatermarkAssignerTransposeRule.INSTANCE,
 
diff --git a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/factories/TestValuesRuntimeFunctions.java b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/factories/TestValuesRuntimeFunctions.java
index 510efcd09ab..42ec001a4d7 100644
--- a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/factories/TestValuesRuntimeFunctions.java
+++ b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/factories/TestValuesRuntimeFunctions.java
@@ -44,6 +44,10 @@ import org.apache.flink.table.data.TimestampData;
 import org.apache.flink.table.functions.AsyncLookupFunction;
 import org.apache.flink.table.functions.FunctionContext;
 import org.apache.flink.table.functions.LookupFunction;
+import org.apache.flink.table.runtime.generated.GeneratedProjection;
+import org.apache.flink.table.runtime.generated.Projection;
+import org.apache.flink.table.runtime.typeutils.InternalSerializers;
+import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.test.util.SuccessException;
 import org.apache.flink.types.Row;
 import org.apache.flink.types.RowKind;
@@ -60,6 +64,7 @@ import java.util.HashMap;
 import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
+import java.util.Optional;
 import java.util.Random;
 import java.util.concurrent.CompletableFuture;
 import java.util.concurrent.ExecutorService;
@@ -558,23 +563,41 @@ final class TestValuesRuntimeFunctions {
         private static final long serialVersionUID = 1L;
         private final List<Row> data;
         private final int[] lookupIndices;
+
+        private final RowType producedRowType;
+
         private final LookupTableSource.DataStructureConverter converter;
+        private final GeneratedProjection generatedProjection;
+        private final boolean projectable;
         private transient Map<RowData, List<RowData>> indexedData;
         private transient boolean isOpenCalled = false;
+        private transient Projection<RowData, GenericRowData> projection;
+        private transient TypeSerializer<RowData> rowSerializer;
 
         protected TestValuesLookupFunction(
                 List<Row> data,
                 int[] lookupIndices,
-                LookupTableSource.DataStructureConverter converter) {
+                RowType producedRowType,
+                LookupTableSource.DataStructureConverter converter,
+                Optional<GeneratedProjection> generatedProjection) {
             this.data = data;
             this.lookupIndices = lookupIndices;
+            this.producedRowType = producedRowType;
             this.converter = converter;
+            this.projectable = generatedProjection.isPresent();
+            this.generatedProjection = generatedProjection.orElse(null);
         }
 
         @Override
         public void open(FunctionContext context) throws Exception {
             RESOURCE_COUNTER.incrementAndGet();
             isOpenCalled = true;
+            if (projectable) {
+                projection =
+                        generatedProjection.newInstance(
+                                Thread.currentThread().getContextClassLoader());
+            }
+            rowSerializer = InternalSerializers.create(producedRowType);
             indexDataByKey();
         }
 
@@ -601,6 +624,9 @@ final class TestValuesRuntimeFunctions {
             data.forEach(
                     record -> {
                         GenericRowData rowData = (GenericRowData) converter.toInternal(record);
+                        if (projectable) {
+                            rowData = projection.apply(rowData);
+                        }
                         checkNotNull(
                                 rowData, "Cannot convert record to internal GenericRowData type");
                         RowData key =
@@ -608,12 +634,13 @@ final class TestValuesRuntimeFunctions {
                                         Arrays.stream(lookupIndices)
                                                 .mapToObj(rowData::getField)
                                                 .toArray());
+                        RowData copiedRow = rowSerializer.copy(rowData);
                         List<RowData> list = indexedData.get(key);
                         if (list != null) {
-                            list.add(rowData);
+                            list.add(copiedRow);
                         } else {
                             list = new ArrayList<>();
-                            list.add(rowData);
+                            list.add(copiedRow);
                             indexedData.put(key, list);
                         }
                     });
@@ -629,25 +656,42 @@ final class TestValuesRuntimeFunctions {
         private static final long serialVersionUID = 1L;
         private final List<Row> data;
         private final int[] lookupIndices;
+        private final RowType producedRowType;
         private final LookupTableSource.DataStructureConverter converter;
+
+        private final GeneratedProjection generatedProjection;
+        private final boolean projectable;
         private final Random random;
         private transient boolean isOpenCalled = false;
         private transient ExecutorService executor;
         private transient Map<RowData, List<RowData>> indexedData;
+        private transient Projection<RowData, GenericRowData> projection;
+        private transient TypeSerializer<RowData> rowSerializer;
 
         protected AsyncTestValueLookupFunction(
                 List<Row> data,
                 int[] lookupIndices,
-                LookupTableSource.DataStructureConverter converter) {
+                RowType producedRowType,
+                LookupTableSource.DataStructureConverter converter,
+                Optional<GeneratedProjection> generatedProjection) {
             this.data = data;
             this.lookupIndices = lookupIndices;
+            this.producedRowType = producedRowType;
             this.converter = converter;
+            this.projectable = generatedProjection.isPresent();
+            this.generatedProjection = generatedProjection.orElse(null);
             this.random = new Random();
         }
 
         @Override
         public void open(FunctionContext context) throws Exception {
             RESOURCE_COUNTER.incrementAndGet();
+            if (projectable) {
+                projection =
+                        generatedProjection.newInstance(
+                                Thread.currentThread().getContextClassLoader());
+            }
+            rowSerializer = InternalSerializers.create(producedRowType);
             isOpenCalled = true;
             // generate unordered result for async lookup
             executor = Executors.newFixedThreadPool(2);
@@ -689,6 +733,9 @@ final class TestValuesRuntimeFunctions {
             data.forEach(
                     record -> {
                         GenericRowData rowData = (GenericRowData) converter.toInternal(record);
+                        if (projectable) {
+                            rowData = projection.apply(rowData);
+                        }
                         checkNotNull(
                                 rowData, "Cannot convert record to internal GenericRowData type");
                         RowData key =
@@ -696,12 +743,13 @@ final class TestValuesRuntimeFunctions {
                                         Arrays.stream(lookupIndices)
                                                 .mapToObj(rowData::getField)
                                                 .toArray());
+                        RowData copiedRow = rowSerializer.copy(rowData);
                         List<RowData> list = indexedData.get(key);
                         if (list != null) {
-                            list.add(rowData);
+                            list.add(copiedRow);
                         } else {
                             list = new ArrayList<>();
-                            list.add(rowData);
+                            list.add(copiedRow);
                             indexedData.put(key, list);
                         }
                     });
@@ -725,9 +773,11 @@ final class TestValuesRuntimeFunctions {
         protected TestNoLookupUntilNthAccessLookupFunction(
                 List<Row> data,
                 int[] lookupIndices,
+                RowType producedRowType,
                 LookupTableSource.DataStructureConverter converter,
+                Optional<GeneratedProjection> generatedProjection,
                 int lookupThreshold) {
-            super(data, lookupIndices, converter);
+            super(data, lookupIndices, producedRowType, converter, generatedProjection);
             this.lookupThreshold = lookupThreshold;
         }
 
@@ -771,9 +821,11 @@ final class TestValuesRuntimeFunctions {
         public TestNoLookupUntilNthAccessAsyncLookupFunction(
                 List<Row> data,
                 int[] lookupIndices,
+                RowType producedRowType,
                 LookupTableSource.DataStructureConverter converter,
+                Optional<GeneratedProjection> generatedProjection,
                 int lookupThreshold) {
-            super(data, lookupIndices, converter);
+            super(data, lookupIndices, producedRowType, converter, generatedProjection);
             this.lookupThreshold = lookupThreshold;
         }
 
diff --git a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java
index 5254750a5f0..ebb4405c38e 100644
--- a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java
+++ b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java
@@ -27,6 +27,7 @@ import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.api.java.io.CollectionInputFormat;
 import org.apache.flink.configuration.ConfigOption;
 import org.apache.flink.configuration.ConfigOptions;
+import org.apache.flink.configuration.Configuration;
 import org.apache.flink.connector.source.DynamicFilteringValuesSource;
 import org.apache.flink.connector.source.ValuesSource;
 import org.apache.flink.streaming.api.datastream.DataStream;
@@ -78,6 +79,7 @@ import org.apache.flink.table.connector.source.lookup.cache.DefaultLookupCache;
 import org.apache.flink.table.connector.source.lookup.cache.LookupCache;
 import org.apache.flink.table.connector.source.lookup.cache.trigger.CacheReloadTrigger;
 import org.apache.flink.table.connector.source.lookup.cache.trigger.PeriodicCacheReloadTrigger;
+import org.apache.flink.table.data.GenericRowData;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.data.util.DataFormatConverters;
 import org.apache.flink.table.expressions.AggregateExpression;
@@ -91,6 +93,8 @@ import org.apache.flink.table.functions.AsyncTableFunction;
 import org.apache.flink.table.functions.FunctionDefinition;
 import org.apache.flink.table.functions.LookupFunction;
 import org.apache.flink.table.functions.TableFunction;
+import org.apache.flink.table.planner.codegen.CodeGeneratorContext;
+import org.apache.flink.table.planner.codegen.ProjectionCodeGenerator;
 import org.apache.flink.table.planner.factories.TestValuesRuntimeFunctions.AppendingOutputFormat;
 import org.apache.flink.table.planner.factories.TestValuesRuntimeFunctions.AppendingSinkFunction;
 import org.apache.flink.table.planner.factories.TestValuesRuntimeFunctions.AsyncTestValueLookupFunction;
@@ -109,9 +113,11 @@ import org.apache.flink.table.planner.runtime.utils.FailingCollectionSource;
 import org.apache.flink.table.planner.utils.FilterUtils;
 import org.apache.flink.table.planner.utils.JavaScalaConversionUtil;
 import org.apache.flink.table.runtime.functions.table.fullcache.FullCacheTestInputFormat;
+import org.apache.flink.table.runtime.generated.GeneratedProjection;
 import org.apache.flink.table.types.DataType;
 import org.apache.flink.table.types.logical.BigIntType;
 import org.apache.flink.table.types.logical.LogicalType;
+import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.table.types.logical.utils.LogicalTypeParser;
 import org.apache.flink.table.types.utils.DataTypeUtils;
 import org.apache.flink.table.types.utils.TypeConversions;
@@ -533,6 +539,7 @@ public final class TestValuesTableFactory
                 }
             } else {
                 return new TestValuesScanLookupTableSource(
+                        context.getCatalogTable().getResolvedSchema().toPhysicalRowDataType(),
                         producedDataType,
                         changelogMode,
                         isBounded,
@@ -1503,7 +1510,10 @@ public final class TestValuesTableFactory
         private final boolean isAsync;
         private final int lookupThreshold;
 
+        private final DataType originType;
+
         private TestValuesScanLookupTableSource(
+                DataType originType,
                 DataType producedDataType,
                 ChangelogMode changelogMode,
                 boolean bounded,
@@ -1542,6 +1552,7 @@ public final class TestValuesTableFactory
                     allPartitions,
                     readableMetadata,
                     projectedMetadataFields);
+            this.originType = originType;
             this.lookupFunctionClass = lookupFunctionClass;
             this.isAsync = isAsync;
             this.cache = cache;
@@ -1590,11 +1601,30 @@ public final class TestValuesTableFactory
                     data = data.subList(numElementToSkip, data.size());
                 }
             }
-            DataStructureConverter converter =
-                    context.createDataStructureConverter(producedDataType);
+            if (nestedProjectionSupported) {
+                throw new UnsupportedOperationException(
+                        "nestedProjectionSupported is unsupported for lookup source currently.");
+            }
+            DataStructureConverter converter = context.createDataStructureConverter(originType);
+            RowType originRowType =
+                    RowType.of(
+                            originType.getLogicalType().getChildren().toArray(new LogicalType[0]));
+            RowType producedRowType =
+                    RowType.of(
+                            producedDataType
+                                    .getLogicalType()
+                                    .getChildren()
+                                    .toArray(new LogicalType[0]));
+            Optional<GeneratedProjection> generatedProjection =
+                    genProjection(originRowType, producedRowType);
             if (isAsync) {
                 AsyncTestValueLookupFunction asyncLookupFunction =
-                        getTestValuesAsyncLookupFunction(data, lookupIndices, converter);
+                        getTestValuesAsyncLookupFunction(
+                                data,
+                                lookupIndices,
+                                producedRowType,
+                                converter,
+                                generatedProjection);
                 if (cache == null) {
                     return AsyncLookupFunctionProvider.of(asyncLookupFunction);
                 } else {
@@ -1603,15 +1633,20 @@ public final class TestValuesTableFactory
                 }
             } else {
                 TestValuesLookupFunction lookupFunction =
-                        getTestValuesLookupFunction(data, lookupIndices, converter);
+                        getTestValuesLookupFunction(
+                                data,
+                                lookupIndices,
+                                producedRowType,
+                                converter,
+                                generatedProjection);
                 if (cache != null) {
                     return PartialCachingLookupProvider.of(lookupFunction, cache);
                 } else if (reloadTrigger != null) {
                     DataFormatConverters.RowConverter rowConverter =
                             new DataFormatConverters.RowConverter(
-                                    producedDataType.getChildren().toArray(new DataType[] {}));
+                                    originType.getChildren().toArray(new DataType[] {}));
                     FullCacheTestInputFormat inputFormat =
-                            new FullCacheTestInputFormat(data, rowConverter);
+                            new FullCacheTestInputFormat(data, generatedProjection, rowConverter);
                     return FullCachingLookupProvider.of(
                             InputFormatProvider.of(inputFormat), reloadTrigger);
                 } else {
@@ -1620,27 +1655,71 @@ public final class TestValuesTableFactory
             }
         }
 
+        /** Does not support nested projection. */
+        private Optional<GeneratedProjection> genProjection(
+                RowType originRowType, RowType producedRowType) {
+            if (null == projectedPhysicalFields) {
+                return Optional.empty();
+            }
+            CodeGeneratorContext context =
+                    new CodeGeneratorContext(
+                            new Configuration(), Thread.currentThread().getContextClassLoader());
+            int[] mapping =
+                    Arrays.stream(projectedPhysicalFields)
+                            .mapToInt(levelOne -> levelOne[0])
+                            .toArray();
+            return Optional.of(
+                    ProjectionCodeGenerator.generateProjection(
+                            context,
+                            "InternalProjection",
+                            originRowType,
+                            producedRowType,
+                            mapping,
+                            GenericRowData.class));
+        }
+
         private AsyncTestValueLookupFunction getTestValuesAsyncLookupFunction(
-                List<Row> data, int[] lookupIndices, DataStructureConverter converter) {
+                List<Row> data,
+                int[] lookupIndices,
+                RowType producedRowType,
+                DataStructureConverter converter,
+                Optional<GeneratedProjection> projection) {
             if (lookupThreshold > 0) {
                 return new TestNoLookupUntilNthAccessAsyncLookupFunction(
-                        data, lookupIndices, converter, lookupThreshold);
+                        data,
+                        lookupIndices,
+                        producedRowType,
+                        converter,
+                        projection,
+                        lookupThreshold);
             }
-            return new AsyncTestValueLookupFunction(data, lookupIndices, converter);
+            return new AsyncTestValueLookupFunction(
+                    data, lookupIndices, producedRowType, converter, projection);
         }
 
         private TestValuesLookupFunction getTestValuesLookupFunction(
-                List<Row> data, int[] lookupIndices, DataStructureConverter converter) {
+                List<Row> data,
+                int[] lookupIndices,
+                RowType producedRowType,
+                DataStructureConverter converter,
+                Optional<GeneratedProjection> generatedProjection) {
             if (lookupThreshold > 0) {
                 return new TestNoLookupUntilNthAccessLookupFunction(
-                        data, lookupIndices, converter, lookupThreshold);
+                        data,
+                        lookupIndices,
+                        producedRowType,
+                        converter,
+                        generatedProjection,
+                        lookupThreshold);
             }
-            return new TestValuesLookupFunction(data, lookupIndices, converter);
+            return new TestValuesLookupFunction(
+                    data, lookupIndices, producedRowType, converter, generatedProjection);
         }
 
         @Override
         public DynamicTableSource copy() {
             return new TestValuesScanLookupTableSource(
+                    originType,
                     producedDataType,
                     changelogMode,
                     bounded,
diff --git a/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/rules/logical/ProjectSnapshotTransposeRuleTest.java b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/rules/logical/ProjectSnapshotTransposeRuleTest.java
new file mode 100644
index 00000000000..7850bda83bd
--- /dev/null
+++ b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/rules/logical/ProjectSnapshotTransposeRuleTest.java
@@ -0,0 +1,136 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.planner.plan.rules.logical;
+
+import org.apache.flink.table.api.TableConfig;
+import org.apache.flink.table.api.TableEnvironment;
+import org.apache.flink.table.planner.plan.optimize.program.FlinkBatchProgram;
+import org.apache.flink.table.planner.plan.optimize.program.FlinkStreamProgram;
+import org.apache.flink.table.planner.utils.BatchTableTestUtil;
+import org.apache.flink.table.planner.utils.StreamTableTestUtil;
+import org.apache.flink.table.planner.utils.TableTestBase;
+import org.apache.flink.table.planner.utils.TableTestUtil;
+
+import org.junit.Before;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
+
+import java.util.Arrays;
+import java.util.Collection;
+
+/** Test rule {@link ProjectSnapshotTransposeRule}. */
+@RunWith(Parameterized.class)
+public class ProjectSnapshotTransposeRuleTest extends TableTestBase {
+
+    private static final String STREAM = "stream";
+    private static final String BATCH = "batch";
+
+    @Parameterized.Parameter public String mode;
+
+    @Parameterized.Parameters(name = "mode = {0}")
+    public static Collection<String> parameters() {
+        return Arrays.asList(STREAM, BATCH);
+    }
+
+    private TableTestUtil util;
+
+    @Before
+    public void setup() {
+        boolean isStreaming = STREAM.equals(mode);
+        if (isStreaming) {
+            util = streamTestUtil(TableConfig.getDefault());
+            ((StreamTableTestUtil) util).buildStreamProgram(FlinkStreamProgram.LOGICAL_REWRITE());
+        } else {
+            util = batchTestUtil(TableConfig.getDefault());
+            ((BatchTableTestUtil) util).buildBatchProgram(FlinkBatchProgram.LOGICAL_REWRITE());
+        }
+
+        TableEnvironment tEnv = util.getTableEnv();
+        String src =
+                String.format(
+                        "CREATE TABLE MyTable (\n"
+                                + "  a int,\n"
+                                + "  b varchar,\n"
+                                + "  c bigint,\n"
+                                + "  proctime as PROCTIME(),\n"
+                                + "  rowtime as TO_TIMESTAMP(FROM_UNIXTIME(c)),\n"
+                                + "  watermark for rowtime as rowtime - INTERVAL '1' second \n"
+                                + ") with (\n"
+                                + "  'connector' = 'values',\n"
+                                + "  'bounded' = '%s')",
+                        !isStreaming);
+        String lookup =
+                String.format(
+                        "CREATE TABLE LookupTable (\n"
+                                + "  id int,\n"
+                                + "  name varchar,\n"
+                                + "  age int \n"
+                                + ") with (\n"
+                                + "  'connector' = 'values',\n"
+                                + "  'bounded' = '%s')",
+                        !isStreaming);
+        tEnv.executeSql(src);
+        tEnv.executeSql(lookup);
+    }
+
+    @Test
+    public void testJoinTemporalTableWithProjectionPushDown() {
+        String sql =
+                "SELECT T.*, D.id\n"
+                        + "FROM MyTable AS T\n"
+                        + "JOIN LookupTable FOR SYSTEM_TIME AS OF T.proctime AS D\n"
+                        + "ON T.a = D.id";
+
+        util.verifyRelPlan(sql);
+    }
+
+    @Test
+    public void testJoinTemporalTableNotProjectable() {
+        String sql =
+                "SELECT T.*, D.*\n"
+                        + "FROM MyTable AS T\n"
+                        + "JOIN LookupTable FOR SYSTEM_TIME AS OF T.proctime AS D\n"
+                        + "ON T.a = D.id";
+
+        util.verifyRelPlan(sql);
+    }
+
+    @Test
+    public void testJoinTemporalTableWithReorderedProject() {
+        String sql =
+                "SELECT T.*, D.age, D.name, D.id\n"
+                        + "FROM MyTable AS T\n"
+                        + "JOIN LookupTable FOR SYSTEM_TIME AS OF T.proctime AS D\n"
+                        + "ON T.a = D.id";
+
+        util.verifyRelPlan(sql);
+    }
+
+    @Test
+    public void testJoinTemporalTableWithProjectAndFilter() {
+        String sql =
+                "SELECT T.*, D.id\n"
+                        + "FROM MyTable AS T\n"
+                        + "JOIN LookupTable FOR SYSTEM_TIME AS OF T.proctime AS D\n"
+                        + "ON T.a = D.id WHERE D.age > 20";
+
+        util.verifyRelPlan(sql);
+    }
+}
diff --git a/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/join/LookupJoinTest.xml b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/join/LookupJoinTest.xml
index d1593ef8f0a..ba8ae14ce5f 100644
--- a/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/join/LookupJoinTest.xml
+++ b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/join/LookupJoinTest.xml
@@ -475,7 +475,7 @@ FlinkLogicalAggregate(group=[{0}], EXPR$1=[COUNT($1)], EXPR$2=[SUM($2)], EXPR$3=
       :     +- FlinkLogicalDataStreamTableScan(table=[[default_catalog, default_database, T1]], fields=[a, b, c, d])
       +- FlinkLogicalSnapshot(period=[$cor0.proctime])
          +- FlinkLogicalCalc(select=[id], where=[>(age, 10)])
-            +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, LookupTable]], fields=[id, name, age])
+            +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, LookupTable, project=[id, age], metadata=[]]], fields=[id, age])
 ]]>
     </Resource>
   </TestCase>
diff --git a/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/nodes/exec/stream/LookupJoinJsonPlanTest_jsonplan/testJoinTemporalTableWithProjectionPushDown.out b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/nodes/exec/stream/LookupJoinJsonPlanTest_jsonplan/testJoinTemporalTableWithProjectionPushDown.out
index 26b74e63d0f..a798ea2aa4f 100644
--- a/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/nodes/exec/stream/LookupJoinJsonPlanTest_jsonplan/testJoinTemporalTableWithProjectionPushDown.out
+++ b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/nodes/exec/stream/LookupJoinJsonPlanTest_jsonplan/testJoinTemporalTableWithProjectionPushDown.out
@@ -246,9 +246,18 @@
               "bounded" : "false"
             }
           }
-        }
+        },
+        "abilities" : [ {
+          "type" : "ProjectPushDown",
+          "projectedFields" : [ [ 0 ] ],
+          "producedType" : "ROW<`id` INT> NOT NULL"
+        }, {
+          "type" : "ReadingMetadata",
+          "metadataKeys" : [ ],
+          "producedType" : "ROW<`id` INT> NOT NULL"
+        } ]
       },
-      "outputType" : "ROW<`id` INT, `name` VARCHAR(2147483647), `age` INT> NOT NULL"
+      "outputType" : "ROW<`id` INT> NOT NULL"
     },
     "lookupKeys" : {
       "0" : {
@@ -256,11 +265,7 @@
         "index" : 0
       }
     },
-    "projectionOnTemporalTable" : [ {
-      "kind" : "INPUT_REF",
-      "inputIndex" : 0,
-      "type" : "INT"
-    } ],
+    "projectionOnTemporalTable" : null,
     "filterOnTemporalTable" : null,
     "lookupKeyContainsPrimaryKey" : false,
     "inputChangelogMode" : [ "INSERT" ],
diff --git a/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/ProjectSnapshotTransposeRuleTest.xml b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/ProjectSnapshotTransposeRuleTest.xml
new file mode 100644
index 00000000000..58d680fb561
--- /dev/null
+++ b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/ProjectSnapshotTransposeRuleTest.xml
@@ -0,0 +1,257 @@
+<?xml version="1.0" ?>
+<!--
+Licensed to the Apache Software Foundation (ASF) under one or more
+contributor license agreements.  See the NOTICE file distributed with
+this work for additional information regarding copyright ownership.
+The ASF licenses this file to you under the Apache License, Version 2.0
+(the "License"); you may not use this file except in compliance with
+the License.  You may obtain a copy of the License at
+
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+-->
+<Root>
+  <TestCase name="testJoinTemporalTableNotProjectable[mode = batch]">
+    <Resource name="sql">
+      <![CDATA[SELECT T.*, D.*
+FROM MyTable AS T
+JOIN LookupTable FOR SYSTEM_TIME AS OF T.proctime AS D
+ON T.a = D.id]]>
+    </Resource>
+    <Resource name="ast">
+      <![CDATA[
+LogicalProject(a=[$0], b=[$1], c=[$2], proctime=[$3], rowtime=[$4], id=[$5], name=[$6], age=[$7])
++- LogicalCorrelate(correlation=[$cor0], joinType=[inner], requiredColumns=[{0, 3}])
+   :- LogicalProject(a=[$0], b=[$1], c=[$2], proctime=[PROCTIME()], rowtime=[TO_TIMESTAMP(FROM_UNIXTIME($2))])
+   :  +- LogicalTableScan(table=[[default_catalog, default_database, MyTable]])
+   +- LogicalFilter(condition=[=($cor0.a, $0)])
+      +- LogicalSnapshot(period=[$cor0.proctime])
+         +- LogicalTableScan(table=[[default_catalog, default_database, LookupTable]])
+]]>
+    </Resource>
+    <Resource name="optimized rel plan">
+      <![CDATA[
+FlinkLogicalJoin(condition=[=($0, $5)], joinType=[inner])
+:- FlinkLogicalCalc(select=[a, b, c, PROCTIME() AS proctime, TO_TIMESTAMP(FROM_UNIXTIME(c)) AS rowtime])
+:  +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c])
++- FlinkLogicalSnapshot(period=[$cor0.proctime])
+   +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, LookupTable]], fields=[id, name, age])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testJoinTemporalTableWithReorderedProject[mode = stream]">
+    <Resource name="sql">
+      <![CDATA[SELECT T.*, D.age, D.name, D.id
+FROM MyTable AS T
+JOIN LookupTable FOR SYSTEM_TIME AS OF T.proctime AS D
+ON T.a = D.id]]>
+    </Resource>
+    <Resource name="ast">
+      <![CDATA[
+LogicalProject(a=[$0], b=[$1], c=[$2], proctime=[$3], rowtime=[$4], age=[$7], name=[$6], id=[$5])
++- LogicalCorrelate(correlation=[$cor0], joinType=[inner], requiredColumns=[{0, 3}])
+   :- LogicalWatermarkAssigner(rowtime=[rowtime], watermark=[-($4, 1000:INTERVAL SECOND)])
+   :  +- LogicalProject(a=[$0], b=[$1], c=[$2], proctime=[PROCTIME()], rowtime=[TO_TIMESTAMP(FROM_UNIXTIME($2))])
+   :     +- LogicalTableScan(table=[[default_catalog, default_database, MyTable]])
+   +- LogicalFilter(condition=[=($cor0.a, $0)])
+      +- LogicalSnapshot(period=[$cor0.proctime])
+         +- LogicalTableScan(table=[[default_catalog, default_database, LookupTable]])
+]]>
+    </Resource>
+    <Resource name="optimized rel plan">
+      <![CDATA[
+FlinkLogicalCalc(select=[a, b, c, proctime, rowtime, age, name, id])
++- FlinkLogicalJoin(condition=[=($0, $5)], joinType=[inner])
+   :- FlinkLogicalWatermarkAssigner(rowtime=[rowtime], watermark=[-($4, 1000:INTERVAL SECOND)])
+   :  +- FlinkLogicalCalc(select=[a, b, c, PROCTIME() AS proctime, TO_TIMESTAMP(FROM_UNIXTIME(c)) AS rowtime])
+   :     +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c])
+   +- FlinkLogicalSnapshot(period=[$cor0.proctime])
+      +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, LookupTable]], fields=[id, name, age])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testJoinTemporalTableNotProjectable[mode = stream]">
+    <Resource name="sql">
+      <![CDATA[SELECT T.*, D.*
+FROM MyTable AS T
+JOIN LookupTable FOR SYSTEM_TIME AS OF T.proctime AS D
+ON T.a = D.id]]>
+    </Resource>
+    <Resource name="ast">
+      <![CDATA[
+LogicalProject(a=[$0], b=[$1], c=[$2], proctime=[$3], rowtime=[$4], id=[$5], name=[$6], age=[$7])
++- LogicalCorrelate(correlation=[$cor0], joinType=[inner], requiredColumns=[{0, 3}])
+   :- LogicalWatermarkAssigner(rowtime=[rowtime], watermark=[-($4, 1000:INTERVAL SECOND)])
+   :  +- LogicalProject(a=[$0], b=[$1], c=[$2], proctime=[PROCTIME()], rowtime=[TO_TIMESTAMP(FROM_UNIXTIME($2))])
+   :     +- LogicalTableScan(table=[[default_catalog, default_database, MyTable]])
+   +- LogicalFilter(condition=[=($cor0.a, $0)])
+      +- LogicalSnapshot(period=[$cor0.proctime])
+         +- LogicalTableScan(table=[[default_catalog, default_database, LookupTable]])
+]]>
+    </Resource>
+    <Resource name="optimized rel plan">
+      <![CDATA[
+FlinkLogicalJoin(condition=[=($0, $5)], joinType=[inner])
+:- FlinkLogicalWatermarkAssigner(rowtime=[rowtime], watermark=[-($4, 1000:INTERVAL SECOND)])
+:  +- FlinkLogicalCalc(select=[a, b, c, PROCTIME() AS proctime, TO_TIMESTAMP(FROM_UNIXTIME(c)) AS rowtime])
+:     +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c])
++- FlinkLogicalSnapshot(period=[$cor0.proctime])
+   +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, LookupTable]], fields=[id, name, age])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testJoinTemporalTableWithProjectAndFilter[mode = batch]">
+    <Resource name="sql">
+      <![CDATA[SELECT T.*, D.id
+FROM MyTable AS T
+JOIN LookupTable FOR SYSTEM_TIME AS OF T.proctime AS D
+ON T.a = D.id WHERE D.age > 20]]>
+    </Resource>
+    <Resource name="ast">
+      <![CDATA[
+LogicalProject(a=[$0], b=[$1], c=[$2], proctime=[$3], rowtime=[$4], id=[$5])
++- LogicalFilter(condition=[>($7, 20)])
+   +- LogicalCorrelate(correlation=[$cor0], joinType=[inner], requiredColumns=[{0, 3}])
+      :- LogicalProject(a=[$0], b=[$1], c=[$2], proctime=[PROCTIME()], rowtime=[TO_TIMESTAMP(FROM_UNIXTIME($2))])
+      :  +- LogicalTableScan(table=[[default_catalog, default_database, MyTable]])
+      +- LogicalFilter(condition=[=($cor0.a, $0)])
+         +- LogicalSnapshot(period=[$cor0.proctime])
+            +- LogicalTableScan(table=[[default_catalog, default_database, LookupTable]])
+]]>
+    </Resource>
+    <Resource name="optimized rel plan">
+      <![CDATA[
+FlinkLogicalJoin(condition=[=($0, $5)], joinType=[inner])
+:- FlinkLogicalCalc(select=[a, b, c, PROCTIME() AS proctime, TO_TIMESTAMP(FROM_UNIXTIME(c)) AS rowtime])
+:  +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c])
++- FlinkLogicalCalc(select=[id], where=[>(age, 20)])
+   +- FlinkLogicalSnapshot(period=[$cor0.proctime])
+      +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, LookupTable, project=[id, age], metadata=[]]], fields=[id, age])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testJoinTemporalTableWithProjectAndFilter[mode = stream]">
+    <Resource name="sql">
+      <![CDATA[SELECT T.*, D.id
+FROM MyTable AS T
+JOIN LookupTable FOR SYSTEM_TIME AS OF T.proctime AS D
+ON T.a = D.id WHERE D.age > 20]]>
+    </Resource>
+    <Resource name="ast">
+      <![CDATA[
+LogicalProject(a=[$0], b=[$1], c=[$2], proctime=[$3], rowtime=[$4], id=[$5])
++- LogicalFilter(condition=[>($7, 20)])
+   +- LogicalCorrelate(correlation=[$cor0], joinType=[inner], requiredColumns=[{0, 3}])
+      :- LogicalWatermarkAssigner(rowtime=[rowtime], watermark=[-($4, 1000:INTERVAL SECOND)])
+      :  +- LogicalProject(a=[$0], b=[$1], c=[$2], proctime=[PROCTIME()], rowtime=[TO_TIMESTAMP(FROM_UNIXTIME($2))])
+      :     +- LogicalTableScan(table=[[default_catalog, default_database, MyTable]])
+      +- LogicalFilter(condition=[=($cor0.a, $0)])
+         +- LogicalSnapshot(period=[$cor0.proctime])
+            +- LogicalTableScan(table=[[default_catalog, default_database, LookupTable]])
+]]>
+    </Resource>
+    <Resource name="optimized rel plan">
+      <![CDATA[
+FlinkLogicalJoin(condition=[=($0, $5)], joinType=[inner])
+:- FlinkLogicalWatermarkAssigner(rowtime=[rowtime], watermark=[-($4, 1000:INTERVAL SECOND)])
+:  +- FlinkLogicalCalc(select=[a, b, c, PROCTIME() AS proctime, TO_TIMESTAMP(FROM_UNIXTIME(c)) AS rowtime])
+:     +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c])
++- FlinkLogicalCalc(select=[id], where=[>(age, 20)])
+   +- FlinkLogicalSnapshot(period=[$cor0.proctime])
+      +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, LookupTable, project=[id, age], metadata=[]]], fields=[id, age])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testJoinTemporalTableWithProjectionPushDown[mode = batch]">
+    <Resource name="sql">
+      <![CDATA[SELECT T.*, D.id
+FROM MyTable AS T
+JOIN LookupTable FOR SYSTEM_TIME AS OF T.proctime AS D
+ON T.a = D.id]]>
+    </Resource>
+    <Resource name="ast">
+      <![CDATA[
+LogicalProject(a=[$0], b=[$1], c=[$2], proctime=[$3], rowtime=[$4], id=[$5])
++- LogicalCorrelate(correlation=[$cor0], joinType=[inner], requiredColumns=[{0, 3}])
+   :- LogicalProject(a=[$0], b=[$1], c=[$2], proctime=[PROCTIME()], rowtime=[TO_TIMESTAMP(FROM_UNIXTIME($2))])
+   :  +- LogicalTableScan(table=[[default_catalog, default_database, MyTable]])
+   +- LogicalFilter(condition=[=($cor0.a, $0)])
+      +- LogicalSnapshot(period=[$cor0.proctime])
+         +- LogicalTableScan(table=[[default_catalog, default_database, LookupTable]])
+]]>
+    </Resource>
+    <Resource name="optimized rel plan">
+      <![CDATA[
+FlinkLogicalJoin(condition=[=($0, $5)], joinType=[inner])
+:- FlinkLogicalCalc(select=[a, b, c, PROCTIME() AS proctime, TO_TIMESTAMP(FROM_UNIXTIME(c)) AS rowtime])
+:  +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c])
++- FlinkLogicalSnapshot(period=[$cor0.proctime])
+   +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, LookupTable, project=[id], metadata=[]]], fields=[id])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testJoinTemporalTableWithProjectionPushDown[mode = stream]">
+    <Resource name="sql">
+      <![CDATA[SELECT T.*, D.id
+FROM MyTable AS T
+JOIN LookupTable FOR SYSTEM_TIME AS OF T.proctime AS D
+ON T.a = D.id]]>
+    </Resource>
+    <Resource name="ast">
+      <![CDATA[
+LogicalProject(a=[$0], b=[$1], c=[$2], proctime=[$3], rowtime=[$4], id=[$5])
++- LogicalCorrelate(correlation=[$cor0], joinType=[inner], requiredColumns=[{0, 3}])
+   :- LogicalWatermarkAssigner(rowtime=[rowtime], watermark=[-($4, 1000:INTERVAL SECOND)])
+   :  +- LogicalProject(a=[$0], b=[$1], c=[$2], proctime=[PROCTIME()], rowtime=[TO_TIMESTAMP(FROM_UNIXTIME($2))])
+   :     +- LogicalTableScan(table=[[default_catalog, default_database, MyTable]])
+   +- LogicalFilter(condition=[=($cor0.a, $0)])
+      +- LogicalSnapshot(period=[$cor0.proctime])
+         +- LogicalTableScan(table=[[default_catalog, default_database, LookupTable]])
+]]>
+    </Resource>
+    <Resource name="optimized rel plan">
+      <![CDATA[
+FlinkLogicalJoin(condition=[=($0, $5)], joinType=[inner])
+:- FlinkLogicalWatermarkAssigner(rowtime=[rowtime], watermark=[-($4, 1000:INTERVAL SECOND)])
+:  +- FlinkLogicalCalc(select=[a, b, c, PROCTIME() AS proctime, TO_TIMESTAMP(FROM_UNIXTIME(c)) AS rowtime])
+:     +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c])
++- FlinkLogicalSnapshot(period=[$cor0.proctime])
+   +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, LookupTable, project=[id], metadata=[]]], fields=[id])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testJoinTemporalTableWithReorderedProject[mode = batch]">
+    <Resource name="sql">
+      <![CDATA[SELECT T.*, D.age, D.name, D.id
+FROM MyTable AS T
+JOIN LookupTable FOR SYSTEM_TIME AS OF T.proctime AS D
+ON T.a = D.id]]>
+    </Resource>
+    <Resource name="ast">
+      <![CDATA[
+LogicalProject(a=[$0], b=[$1], c=[$2], proctime=[$3], rowtime=[$4], age=[$7], name=[$6], id=[$5])
++- LogicalCorrelate(correlation=[$cor0], joinType=[inner], requiredColumns=[{0, 3}])
+   :- LogicalProject(a=[$0], b=[$1], c=[$2], proctime=[PROCTIME()], rowtime=[TO_TIMESTAMP(FROM_UNIXTIME($2))])
+   :  +- LogicalTableScan(table=[[default_catalog, default_database, MyTable]])
+   +- LogicalFilter(condition=[=($cor0.a, $0)])
+      +- LogicalSnapshot(period=[$cor0.proctime])
+         +- LogicalTableScan(table=[[default_catalog, default_database, LookupTable]])
+]]>
+    </Resource>
+    <Resource name="optimized rel plan">
+      <![CDATA[
+FlinkLogicalCalc(select=[a, b, c, proctime, rowtime, age, name, id])
++- FlinkLogicalJoin(condition=[=($0, $5)], joinType=[inner])
+   :- FlinkLogicalCalc(select=[a, b, c, PROCTIME() AS proctime, TO_TIMESTAMP(FROM_UNIXTIME(c)) AS rowtime])
+   :  +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c])
+   +- FlinkLogicalSnapshot(period=[$cor0.proctime])
+      +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, LookupTable]], fields=[id, name, age])
+]]>
+    </Resource>
+  </TestCase>
+</Root>
diff --git a/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/NonDeterministicDagTest.xml b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/NonDeterministicDagTest.xml
index be561131927..44d388bfdb5 100644
--- a/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/NonDeterministicDagTest.xml
+++ b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/NonDeterministicDagTest.xml
@@ -565,7 +565,7 @@ LogicalSink(table=[default_catalog.default_database.sink_without_pk], fields=[a,
       <![CDATA[
 Sink(table=[default_catalog.default_database.sink_without_pk], fields=[a, b, c])
 +- Calc(select=[a, b, c])
-   +- LookupJoin(table=[default_catalog.default_database.dim_without_pk], joinType=[InnerJoin], lookup=[a=a], select=[a, b, a, c])
+   +- LookupJoin(table=[default_catalog.default_database.dim_without_pk], joinType=[InnerJoin], lookup=[a=a], select=[a, b, a0, c])
       +- TableSourceScan(table=[[default_catalog, default_database, cdc, project=[a, b], metadata=[]]], fields=[a, b])
 ]]>
     </Resource>
@@ -597,7 +597,7 @@ LogicalSink(table=[default_catalog.default_database.sink_without_pk], fields=[a,
       <![CDATA[
 Sink(table=[default_catalog.default_database.sink_without_pk], fields=[a, b, c])
 +- Calc(select=[a, b, c])
-   +- LookupJoin(table=[default_catalog.default_database.dim_without_pk], joinType=[InnerJoin], lookup=[a=a], select=[a, b, a, c], upsertMaterialize=[true])
+   +- LookupJoin(table=[default_catalog.default_database.dim_without_pk], joinType=[InnerJoin], lookup=[a=a], select=[a, b, a0, c], upsertMaterialize=[true])
       +- TableSourceScan(table=[[default_catalog, default_database, cdc, project=[a, b], metadata=[]]], fields=[a, b])
 ]]>
     </Resource>
@@ -629,7 +629,7 @@ LogicalSink(table=[default_catalog.default_database.sink_with_pk], fields=[a, b,
       <![CDATA[
 Sink(table=[default_catalog.default_database.sink_with_pk], fields=[a, b, c], upsertMaterialize=[true])
 +- Calc(select=[a, b, c])
-   +- LookupJoin(table=[default_catalog.default_database.dim_without_pk], joinType=[InnerJoin], lookup=[a=a], select=[a, b, a, c])
+   +- LookupJoin(table=[default_catalog.default_database.dim_without_pk], joinType=[InnerJoin], lookup=[a=a], select=[a, b, a0, c])
       +- TableSourceScan(table=[[default_catalog, default_database, cdc, project=[a, b], metadata=[]]], fields=[a, b])
 ]]>
     </Resource>
@@ -661,7 +661,7 @@ LogicalSink(table=[default_catalog.default_database.sink_with_pk], fields=[a, b,
       <![CDATA[
 Sink(table=[default_catalog.default_database.sink_with_pk], fields=[a, b, c], upsertMaterialize=[true])
 +- Calc(select=[a, b, c])
-   +- LookupJoin(table=[default_catalog.default_database.dim_without_pk], joinType=[InnerJoin], lookup=[a=a], select=[a, b, a, c], upsertMaterialize=[true])
+   +- LookupJoin(table=[default_catalog.default_database.dim_without_pk], joinType=[InnerJoin], lookup=[a=a], select=[a, b, a0, c], upsertMaterialize=[true])
       +- TableSourceScan(table=[[default_catalog, default_database, cdc, project=[a, b], metadata=[]]], fields=[a, b])
 ]]>
     </Resource>
@@ -693,7 +693,7 @@ LogicalSink(table=[default_catalog.default_database.sink_without_pk], fields=[a,
       <![CDATA[
 Sink(table=[default_catalog.default_database.sink_without_pk], fields=[a, b, c])
 +- Calc(select=[ndFunc(a0) AS a, b, c])
-   +- LookupJoin(table=[default_catalog.default_database.dim_with_pk], joinType=[InnerJoin], lookup=[a=a], select=[a, b, c, a])
+   +- LookupJoin(table=[default_catalog.default_database.dim_with_pk], joinType=[InnerJoin], lookup=[a=a], select=[a, b, c, a0])
       +- TableSourceScan(table=[[default_catalog, default_database, cdc, project=[a, b, c], metadata=[]]], fields=[a, b, c])
 ]]>
     </Resource>
@@ -791,7 +791,7 @@ LogicalSink(table=[default_catalog.default_database.sink_without_pk], fields=[a,
       <![CDATA[
 Sink(table=[default_catalog.default_database.sink_without_pk], fields=[a, b, c])
 +- Calc(select=[a, b, c])
-   +- LookupJoin(table=[default_catalog.default_database.dim_with_pk], joinType=[InnerJoin], lookup=[a=a], select=[a, b, c, a])
+   +- LookupJoin(table=[default_catalog.default_database.dim_with_pk], joinType=[InnerJoin], lookup=[a=a], select=[a, b, c, a0])
       +- TableSourceScan(table=[[default_catalog, default_database, cdc, project=[a, b, c], metadata=[]]], fields=[a, b, c])
 ]]>
     </Resource>
@@ -823,7 +823,7 @@ LogicalSink(table=[default_catalog.default_database.sink_without_pk], fields=[a,
       <![CDATA[
 Sink(table=[default_catalog.default_database.sink_without_pk], fields=[a, b, c])
 +- Calc(select=[a, b, c])
-   +- LookupJoin(table=[default_catalog.default_database.dim_with_pk], joinType=[InnerJoin], lookup=[a=a], select=[a, b, c, a])
+   +- LookupJoin(table=[default_catalog.default_database.dim_with_pk], joinType=[InnerJoin], lookup=[a=a], select=[a, b, c, a0])
       +- TableSourceScan(table=[[default_catalog, default_database, cdc, project=[a, b, c], metadata=[]]], fields=[a, b, c])
 ]]>
     </Resource>
@@ -855,7 +855,7 @@ LogicalSink(table=[default_catalog.default_database.sink_without_pk], fields=[a,
       <![CDATA[
 Sink(table=[default_catalog.default_database.sink_without_pk], fields=[a, b, c])
 +- Calc(select=[a, b, c])
-   +- LookupJoin(table=[default_catalog.default_database.dim_with_pk], joinType=[InnerJoin], lookup=[a=a], select=[a, c, a, b])
+   +- LookupJoin(table=[default_catalog.default_database.dim_with_pk], joinType=[InnerJoin], lookup=[a=a], select=[a, c, a0, b])
       +- TableSourceScan(table=[[default_catalog, default_database, cdc, project=[a, c], metadata=[]]], fields=[a, c])
 ]]>
     </Resource>
@@ -887,7 +887,7 @@ LogicalSink(table=[default_catalog.default_database.sink_without_pk], fields=[a,
       <![CDATA[
 Sink(table=[default_catalog.default_database.sink_without_pk], fields=[a, b, c])
 +- Calc(select=[a, b, c])
-   +- LookupJoin(table=[default_catalog.default_database.dim_with_pk], joinType=[InnerJoin], lookup=[a=a], select=[a, c, a, b], upsertMaterialize=[true])
+   +- LookupJoin(table=[default_catalog.default_database.dim_with_pk], joinType=[InnerJoin], lookup=[a=a], select=[a, c, a0, b], upsertMaterialize=[true])
       +- TableSourceScan(table=[[default_catalog, default_database, cdc, project=[a, c], metadata=[]]], fields=[a, c])
 ]]>
     </Resource>
@@ -919,7 +919,7 @@ LogicalSink(table=[default_catalog.default_database.sink_without_pk], fields=[a,
       <![CDATA[
 Sink(table=[default_catalog.default_database.sink_without_pk], fields=[a, b, c])
 +- Calc(select=[a, b, c])
-   +- LookupJoin(table=[default_catalog.default_database.dim_with_pk], joinType=[InnerJoin], lookup=[a=a], select=[a, b, a, c])
+   +- LookupJoin(table=[default_catalog.default_database.dim_with_pk], joinType=[InnerJoin], lookup=[a=a], select=[a, b, a0, c])
       +- TableSourceScan(table=[[default_catalog, default_database, cdc, project=[a, b], metadata=[]]], fields=[a, b])
 ]]>
     </Resource>
@@ -951,7 +951,7 @@ LogicalSink(table=[default_catalog.default_database.sink_without_pk], fields=[a,
       <![CDATA[
 Sink(table=[default_catalog.default_database.sink_without_pk], fields=[a, b, c])
 +- Calc(select=[a, b, c])
-   +- LookupJoin(table=[default_catalog.default_database.dim_without_pk], joinType=[LeftOuterJoin], lookup=[a=a], select=[a, b, a, c], upsertMaterialize=[true])
+   +- LookupJoin(table=[default_catalog.default_database.dim_without_pk], joinType=[LeftOuterJoin], lookup=[a=a], select=[a, b, a0, c], upsertMaterialize=[true])
       +- TableSourceScan(table=[[default_catalog, default_database, cdc, project=[a, b], metadata=[]]], fields=[a, b])
 ]]>
     </Resource>
@@ -983,7 +983,7 @@ LogicalSink(table=[default_catalog.default_database.sink_without_pk], fields=[a,
       <![CDATA[
 Sink(table=[default_catalog.default_database.sink_without_pk], fields=[a, b, c])
 +- Calc(select=[a, b, c])
-   +- LookupJoin(table=[default_catalog.default_database.dim_with_pk], joinType=[InnerJoin], lookup=[a=a], select=[a, b, a, c], upsertMaterialize=[true])
+   +- LookupJoin(table=[default_catalog.default_database.dim_with_pk], joinType=[InnerJoin], lookup=[a=a], select=[a, b, a0, c], upsertMaterialize=[true])
       +- TableSourceScan(table=[[default_catalog, default_database, cdc, project=[a, b], metadata=[]]], fields=[a, b])
 ]]>
     </Resource>
@@ -1015,7 +1015,7 @@ LogicalSink(table=[default_catalog.default_database.sink_with_pk], fields=[a, b,
       <![CDATA[
 Sink(table=[default_catalog.default_database.sink_with_pk], fields=[a, b, c])
 +- Calc(select=[a, b, c])
-   +- LookupJoin(table=[default_catalog.default_database.dim_with_pk], joinType=[InnerJoin], lookup=[a=a], select=[a, b, a, c])
+   +- LookupJoin(table=[default_catalog.default_database.dim_with_pk], joinType=[InnerJoin], lookup=[a=a], select=[a, b, a0, c])
       +- DropUpdateBefore
          +- TableSourceScan(table=[[default_catalog, default_database, cdc, project=[a, b], metadata=[]]], fields=[a, b])
 ]]>
@@ -1048,7 +1048,7 @@ LogicalSink(table=[default_catalog.default_database.sink_with_pk], fields=[a, b,
       <![CDATA[
 Sink(table=[default_catalog.default_database.sink_with_pk], fields=[a, b, c])
 +- Calc(select=[a, b, c])
-   +- LookupJoin(table=[default_catalog.default_database.dim_with_pk], joinType=[InnerJoin], lookup=[a=a], select=[a, b, a, c])
+   +- LookupJoin(table=[default_catalog.default_database.dim_with_pk], joinType=[InnerJoin], lookup=[a=a], select=[a, b, a0, c])
       +- DropUpdateBefore
          +- TableSourceScan(table=[[default_catalog, default_database, cdc, project=[a, b], metadata=[]]], fields=[a, b])
 ]]>
@@ -1153,7 +1153,7 @@ LogicalSink(table=[default_catalog.default_database.sink_without_pk], fields=[a,
       <![CDATA[
 Sink(table=[default_catalog.default_database.sink_without_pk], fields=[a, b, c])
 +- Calc(select=[a, b, c])
-   +- LookupJoin(table=[default_catalog.default_database.dim_without_pk], joinType=[LeftOuterJoin], lookup=[a=a], select=[a, b, a, c])
+   +- LookupJoin(table=[default_catalog.default_database.dim_without_pk], joinType=[LeftOuterJoin], lookup=[a=a], select=[a, b, a0, c])
       +- TableSourceScan(table=[[default_catalog, default_database, cdc, project=[a, b], metadata=[]]], fields=[a, b])
 ]]>
     </Resource>
@@ -1185,7 +1185,7 @@ LogicalSink(table=[default_catalog.default_database.sink_with_pk], fields=[a, b,
       <![CDATA[
 Sink(table=[default_catalog.default_database.sink_with_pk], fields=[a, b, c])
 +- Calc(select=[a, b, c])
-   +- LookupJoin(table=[default_catalog.default_database.dim_with_pk], joinType=[LeftOuterJoin], lookup=[a=a], select=[a, b, a, c])
+   +- LookupJoin(table=[default_catalog.default_database.dim_with_pk], joinType=[LeftOuterJoin], lookup=[a=a], select=[a, b, a0, c])
       +- DropUpdateBefore
          +- TableSourceScan(table=[[default_catalog, default_database, cdc, project=[a, b], metadata=[]]], fields=[a, b])
 ]]>
@@ -1218,7 +1218,7 @@ LogicalSink(table=[default_catalog.default_database.sink_with_pk], fields=[a, b,
       <![CDATA[
 Sink(table=[default_catalog.default_database.sink_with_pk], fields=[a, b, c])
 +- Calc(select=[a, b, c])
-   +- LookupJoin(table=[default_catalog.default_database.dim_with_pk], joinType=[LeftOuterJoin], lookup=[a=a], select=[a, b, a, c])
+   +- LookupJoin(table=[default_catalog.default_database.dim_with_pk], joinType=[LeftOuterJoin], lookup=[a=a], select=[a, b, a0, c])
       +- DropUpdateBefore
          +- TableSourceScan(table=[[default_catalog, default_database, cdc, project=[a, b], metadata=[]]], fields=[a, b])
 ]]>
diff --git a/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/join/TemporalJoinTest.xml b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/join/TemporalJoinTest.xml
index d5ea6ea41d3..37ceb28bcd7 100644
--- a/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/join/TemporalJoinTest.xml
+++ b/flink-table/flink-table-planner/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/join/TemporalJoinTest.xml
@@ -216,7 +216,7 @@ Calc(select=[currency, currency0, rate1])
    +- Exchange(distribution=[hash[currency]])
       +- Calc(select=[currency, rate1, rowtime], where=[(rate < 100)])
          +- WatermarkAssigner(rowtime=[rowtime], watermark=[rowtime])
-            +- Calc(select=[currency, rate, (rate + 1) AS rate1, PROCTIME() AS proctime, rowtime])
+            +- Calc(select=[currency, rate, (rate + 1) AS rate1, rowtime])
                +- TableSourceScan(table=[[default_catalog, default_database, RatesBinlogWithComputedColumn]], fields=[currency, rate, rowtime])
 ]]>
     </Resource>
@@ -501,7 +501,7 @@ Calc(select=[currency, currency0, rate1])
    :           +- TableSourceScan(table=[[default_catalog, default_database, Orders]], fields=[amount, currency, rowtime])
    +- Exchange(distribution=[hash[currency]])
       +- Calc(select=[currency, (rate + 1) AS rate1], where=[(rate < 100)])
-         +- TableSourceScan(table=[[default_catalog, default_database, RatesBinlogWithoutWatermark, filter=[]]], fields=[currency, rate, rowtime])
+         +- TableSourceScan(table=[[default_catalog, default_database, RatesBinlogWithoutWatermark, project=[currency, rate], metadata=[], filter=[]]], fields=[currency, rate])
 ]]>
     </Resource>
   </TestCase>
@@ -535,7 +535,7 @@ Calc(select=[currency, currency0, rate1])
    +- Exchange(distribution=[hash[currency]])
       +- Calc(select=[currency, rate1], where=[(rate < 100)])
          +- WatermarkAssigner(rowtime=[rowtime], watermark=[rowtime])
-            +- Calc(select=[currency, rate, (rate + 1) AS rate1, PROCTIME() AS proctime, rowtime])
+            +- Calc(select=[currency, rate, (rate + 1) AS rate1, rowtime])
                +- TableSourceScan(table=[[default_catalog, default_database, RatesBinlogWithComputedColumn]], fields=[currency, rate, rowtime])
 ]]>
     </Resource>
diff --git a/flink-table/flink-table-runtime/src/test/java/org/apache/flink/table/runtime/functions/table/fullcache/FullCacheTestInputFormat.java b/flink-table/flink-table-runtime/src/test/java/org/apache/flink/table/runtime/functions/table/fullcache/FullCacheTestInputFormat.java
index 8084a279276..e3fadef05d7 100644
--- a/flink-table/flink-table-runtime/src/test/java/org/apache/flink/table/runtime/functions/table/fullcache/FullCacheTestInputFormat.java
+++ b/flink-table/flink-table-runtime/src/test/java/org/apache/flink/table/runtime/functions/table/fullcache/FullCacheTestInputFormat.java
@@ -25,12 +25,16 @@ import org.apache.flink.api.common.io.statistics.BaseStatistics;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.core.io.InputSplit;
 import org.apache.flink.core.io.InputSplitAssigner;
+import org.apache.flink.table.data.GenericRowData;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.data.util.DataFormatConverters;
+import org.apache.flink.table.runtime.generated.GeneratedProjection;
+import org.apache.flink.table.runtime.generated.Projection;
 import org.apache.flink.types.Row;
 
 import java.io.IOException;
 import java.util.Collection;
+import java.util.Optional;
 import java.util.concurrent.ConcurrentLinkedQueue;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.function.Consumer;
@@ -48,10 +52,14 @@ public class FullCacheTestInputFormat
     // RowData is not serializable, so we store Rows
     private final Collection<Row> dataRows;
     private final DataFormatConverters.RowConverter rowConverter;
+    private final GeneratedProjection generatedProjection;
+    private final boolean projectable;
     private final int deltaNumSplits;
     private final transient Consumer<Collection<RowData>> secondLoadDataChange;
 
     private transient ConcurrentLinkedQueue<RowData> queue;
+    private transient Projection<RowData, GenericRowData> projection;
+
     private int loadCounter;
     private int maxReadRecords;
     private int readRecords;
@@ -60,20 +68,27 @@ public class FullCacheTestInputFormat
 
     public FullCacheTestInputFormat(
             Collection<Row> dataRows,
+            Optional<GeneratedProjection> generatedProjection,
             DataFormatConverters.RowConverter rowConverter,
             int deltaNumSplits,
             Consumer<Collection<RowData>> secondLoadDataChange) {
         // for unit tests
         this.dataRows = dataRows;
+        this.projectable = generatedProjection.isPresent();
+        this.generatedProjection = generatedProjection.orElse(null);
         this.rowConverter = rowConverter;
         this.deltaNumSplits = deltaNumSplits;
         this.secondLoadDataChange = secondLoadDataChange;
     }
 
     public FullCacheTestInputFormat(
-            Collection<Row> dataRows, DataFormatConverters.RowConverter rowConverter) {
+            Collection<Row> dataRows,
+            Optional<GeneratedProjection> generatedProjection,
+            DataFormatConverters.RowConverter rowConverter) {
         // for integration tests
         this.dataRows = dataRows;
+        this.projectable = generatedProjection.isPresent();
+        this.generatedProjection = generatedProjection.orElse(null);
         this.rowConverter = rowConverter;
         this.deltaNumSplits = 0;
         this.secondLoadDataChange = null;
@@ -105,6 +120,10 @@ public class FullCacheTestInputFormat
     @Override
     public void open(QueueInputSplit split) throws IOException {
         this.queue = split.getQueue();
+        if (projectable) {
+            projection =
+                    generatedProjection.newInstance(Thread.currentThread().getContextClassLoader());
+        }
         this.readRecords = 0;
         numOpens++;
         OPEN_CLOSED_COUNTER.incrementAndGet();
@@ -122,7 +141,12 @@ public class FullCacheTestInputFormat
             return null;
         }
         readRecords++;
-        return queue.poll();
+        RowData rowData = queue.poll();
+        if (rowData != null && projectable) {
+            // InputSplitCacheLoadTask will do copy work
+            return projection.apply(rowData);
+        }
+        return rowData;
     }
 
     @Override
