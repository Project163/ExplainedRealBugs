diff --git a/docs/dev/table/hive/hive_catalog.md b/docs/dev/table/hive/hive_catalog.md
index a9c4cb46afc..e5285a13a2c 100644
--- a/docs/dev/table/hive/hive_catalog.md
+++ b/docs/dev/table/hive/hive_catalog.md
@@ -190,7 +190,7 @@ root
 
 {% endhighlight %}
 
-Verify the table from Hive side via Hive Cli:
+Verify the table is also visible to Hive via Hive Cli:
 
 {% highlight bash %}
 hive> show tables;
@@ -198,14 +198,11 @@ OK
 mykafka
 Time taken: 0.038 seconds, Fetched: 1 row(s)
 
-hive> describe mykafka;
-OK
-name                	string
-age                 	int
-Time taken: 0.056 seconds, Fetched: 2 row(s)
-
 {% endhighlight %}
 
+Please note since this is a generic table, Hive doesn't understand such a table and using this table in
+Hive leads to undefined behavior.
+
 
 #### step 5: run Flink SQL to query the Kakfa table
 
diff --git a/docs/dev/table/hive/hive_catalog.zh.md b/docs/dev/table/hive/hive_catalog.zh.md
index a9c4cb46afc..e5285a13a2c 100644
--- a/docs/dev/table/hive/hive_catalog.zh.md
+++ b/docs/dev/table/hive/hive_catalog.zh.md
@@ -190,7 +190,7 @@ root
 
 {% endhighlight %}
 
-Verify the table from Hive side via Hive Cli:
+Verify the table is also visible to Hive via Hive Cli:
 
 {% highlight bash %}
 hive> show tables;
@@ -198,14 +198,11 @@ OK
 mykafka
 Time taken: 0.038 seconds, Fetched: 1 row(s)
 
-hive> describe mykafka;
-OK
-name                	string
-age                 	int
-Time taken: 0.056 seconds, Fetched: 2 row(s)
-
 {% endhighlight %}
 
+Please note since this is a generic table, Hive doesn't understand such a table and using this table in
+Hive leads to undefined behavior.
+
 
 #### step 5: run Flink SQL to query the Kakfa table
 
diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java
index 33747cf7aa7..37bb276e004 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java
@@ -64,6 +64,7 @@ import org.apache.flink.table.catalog.hive.util.HiveStatsUtil;
 import org.apache.flink.table.catalog.hive.util.HiveTableUtil;
 import org.apache.flink.table.catalog.stats.CatalogColumnStatistics;
 import org.apache.flink.table.catalog.stats.CatalogTableStatistics;
+import org.apache.flink.table.descriptors.DescriptorProperties;
 import org.apache.flink.table.expressions.Expression;
 import org.apache.flink.table.factories.FunctionDefinitionFactory;
 import org.apache.flink.table.factories.TableFactory;
@@ -518,36 +519,41 @@ public class HiveCatalog extends AbstractCatalog {
 		// When retrieving a table, a generic table needs explicitly have a key is_generic = true
 		// otherwise, this is a Hive table if 1) the key is missing 2) is_generic = false
 		// this is opposite to creating a table. See instantiateHiveTable()
+		boolean isGeneric = Boolean.parseBoolean(hiveTable.getParameters().getOrDefault(CatalogConfig.IS_GENERIC, "false"));
 
-		if (!properties.containsKey(CatalogConfig.IS_GENERIC)) {
-			// must be a hive table
-			properties.put(CatalogConfig.IS_GENERIC, String.valueOf(false));
-		} else {
-			boolean isGeneric = Boolean.valueOf(properties.get(CatalogConfig.IS_GENERIC));
+		TableSchema tableSchema;
+		// Partition keys
+		List<String> partitionKeys = new ArrayList<>();
 
-			if (isGeneric) {
-				properties = retrieveFlinkProperties(properties);
+		if (isGeneric) {
+			properties = retrieveFlinkProperties(properties);
+			DescriptorProperties tableSchemaProps = new DescriptorProperties(true);
+			tableSchemaProps.putProperties(properties);
+			ObjectPath tablePath = new ObjectPath(hiveTable.getDbName(), hiveTable.getTableName());
+			tableSchema = tableSchemaProps.getOptionalTableSchema(HiveCatalogConfig.GENERIC_TABLE_SCHEMA_PREFIX)
+					.orElseThrow(() -> new CatalogException("Failed to get table schema from properties for generic table " + tablePath));
+			// remove the schema from properties
+			properties = properties.entrySet().stream()
+					.filter(e -> !e.getKey().startsWith(HiveCatalogConfig.GENERIC_TABLE_SCHEMA_PREFIX))
+					.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));
+		} else {
+			properties.put(CatalogConfig.IS_GENERIC, String.valueOf(false));
+			// Table schema
+			List<FieldSchema> fields = getNonPartitionFields(hiveConf, hiveTable);
+			Set<String> notNullColumns = client.getNotNullColumns(hiveConf, hiveTable.getDbName(), hiveTable.getTableName());
+			Optional<UniqueConstraint> primaryKey = isView ? Optional.empty() :
+					client.getPrimaryKey(hiveTable.getDbName(), hiveTable.getTableName(), HiveTableUtil.relyConstraint((byte) 0));
+			// PK columns cannot be null
+			primaryKey.ifPresent(pk -> notNullColumns.addAll(pk.getColumns()));
+			tableSchema = HiveTableUtil.createTableSchema(fields, hiveTable.getPartitionKeys(), notNullColumns, primaryKey.orElse(null));
+
+			if (!hiveTable.getPartitionKeys().isEmpty()) {
+				partitionKeys = getFieldNames(hiveTable.getPartitionKeys());
 			}
 		}
 
 		String comment = properties.remove(HiveCatalogConfig.COMMENT);
 
-		// Table schema
-		List<FieldSchema> fields = getNonPartitionFields(hiveConf, hiveTable);
-		Set<String> notNullColumns = client.getNotNullColumns(hiveConf, hiveTable.getDbName(), hiveTable.getTableName());
-		Optional<UniqueConstraint> primaryKey = isView ? Optional.empty() :
-				client.getPrimaryKey(hiveTable.getDbName(), hiveTable.getTableName(), HiveTableUtil.relyConstraint((byte) 0));
-		// PK columns cannot be null
-		primaryKey.ifPresent(pk -> notNullColumns.addAll(pk.getColumns()));
-		TableSchema tableSchema =
-				HiveTableUtil.createTableSchema(fields, hiveTable.getPartitionKeys(), notNullColumns, primaryKey.orElse(null));
-
-		// Partition keys
-		List<String> partitionKeys = new ArrayList<>();
-		if (!hiveTable.getPartitionKeys().isEmpty()) {
-			partitionKeys = getFieldNames(hiveTable.getPartitionKeys());
-		}
-
 		if (isView) {
 			return new CatalogViewImpl(
 					hiveTable.getViewOriginalText(),
@@ -573,6 +579,10 @@ public class HiveCatalog extends AbstractCatalog {
 
 	@VisibleForTesting
 	protected static Table instantiateHiveTable(ObjectPath tablePath, CatalogBaseTable table) {
+		if (!(table instanceof CatalogTableImpl) && !(table instanceof CatalogViewImpl)) {
+			throw new CatalogException(
+					"HiveCatalog only supports CatalogTableImpl and CatalogViewImpl");
+		}
 		// let Hive set default parameters for us, e.g. serialization.format
 		Table hiveTable = org.apache.hadoop.hive.ql.metadata.Table.getEmptyTable(tablePath.getDatabaseName(),
 			tablePath.getObjectName());
@@ -587,57 +597,67 @@ public class HiveCatalog extends AbstractCatalog {
 		// When creating a table, A hive table needs explicitly have a key is_generic = false
 		// otherwise, this is a generic table if 1) the key is missing 2) is_generic = true
 		// this is opposite to reading a table and instantiating a CatalogTable. See instantiateCatalogTable()
+		boolean isGeneric;
 		if (!properties.containsKey(CatalogConfig.IS_GENERIC)) {
-			// must be a generic catalog
+			// must be a generic table
+			isGeneric = true;
 			properties.put(CatalogConfig.IS_GENERIC, String.valueOf(true));
-			properties = maskFlinkProperties(properties);
 		} else {
-			boolean isGeneric = Boolean.valueOf(properties.get(CatalogConfig.IS_GENERIC));
-
-			if (isGeneric) {
-				properties = maskFlinkProperties(properties);
-			}
+			isGeneric = Boolean.parseBoolean(properties.get(CatalogConfig.IS_GENERIC));
 		}
 
-		// Table properties
-		hiveTable.setParameters(properties);
-
 		// Hive table's StorageDescriptor
 		StorageDescriptor sd = hiveTable.getSd();
 		setStorageFormat(sd, properties);
 
-		List<FieldSchema> allColumns = HiveTableUtil.createHiveColumns(table.getSchema());
-
-		// Table columns and partition keys
-		if (table instanceof CatalogTableImpl) {
-			CatalogTable catalogTable = (CatalogTableImpl) table;
-
-			if (catalogTable.isPartitioned()) {
-				int partitionKeySize = catalogTable.getPartitionKeys().size();
-				List<FieldSchema> regularColumns = allColumns.subList(0, allColumns.size() - partitionKeySize);
-				List<FieldSchema> partitionColumns = allColumns.subList(allColumns.size() - partitionKeySize, allColumns.size());
+		if (isGeneric) {
+			DescriptorProperties tableSchemaProps = new DescriptorProperties(true);
+			tableSchemaProps.putTableSchema(HiveCatalogConfig.GENERIC_TABLE_SCHEMA_PREFIX, table.getSchema());
+			properties.putAll(tableSchemaProps.asMap());
+
+			if (table instanceof CatalogTableImpl) {
+				List<String> partColNames = ((CatalogTableImpl) table).getPartitionKeys();
+				if (!partColNames.isEmpty()) {
+					throw new CatalogException("Partitioned generic table is not supported yet by HiveCatalog");
+				}
+			}
 
-				sd.setCols(regularColumns);
-				hiveTable.setPartitionKeys(partitionColumns);
+			properties = maskFlinkProperties(properties);
+		} else {
+			List<FieldSchema> allColumns = HiveTableUtil.createHiveColumns(table.getSchema());
+			// Table columns and partition keys
+			if (table instanceof CatalogTableImpl) {
+				CatalogTable catalogTable = (CatalogTableImpl) table;
+
+				if (catalogTable.isPartitioned()) {
+					int partitionKeySize = catalogTable.getPartitionKeys().size();
+					List<FieldSchema> regularColumns = allColumns.subList(0, allColumns.size() - partitionKeySize);
+					List<FieldSchema> partitionColumns = allColumns.subList(allColumns.size() - partitionKeySize, allColumns.size());
+
+					sd.setCols(regularColumns);
+					hiveTable.setPartitionKeys(partitionColumns);
+				} else {
+					sd.setCols(allColumns);
+					hiveTable.setPartitionKeys(new ArrayList<>());
+				}
 			} else {
 				sd.setCols(allColumns);
-				hiveTable.setPartitionKeys(new ArrayList<>());
 			}
-		} else if (table instanceof CatalogViewImpl) {
-			CatalogView view = (CatalogViewImpl) table;
+		}
 
+		if (table instanceof CatalogViewImpl) {
 			// TODO: [FLINK-12398] Support partitioned view in catalog API
-			sd.setCols(allColumns);
 			hiveTable.setPartitionKeys(new ArrayList<>());
 
+			CatalogView view = (CatalogView) table;
 			hiveTable.setViewOriginalText(view.getOriginalQuery());
 			hiveTable.setViewExpandedText(view.getExpandedQuery());
 			hiveTable.setTableType(TableType.VIRTUAL_VIEW.name());
-		} else {
-			throw new CatalogException(
-				"HiveCatalog only supports CatalogTableImpl and CatalogViewImpl");
 		}
 
+		// Table properties
+		hiveTable.setParameters(properties);
+
 		return hiveTable;
 	}
 
diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalogConfig.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalogConfig.java
index 2ece5512253..0859690ffee 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalogConfig.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalogConfig.java
@@ -32,4 +32,7 @@ public class HiveCatalogConfig {
 
 	// Partition related configs
 	public static final String PARTITION_LOCATION = "partition.location";
+
+	// config prefix for the table schema of a generic table
+	public static final String GENERIC_TABLE_SCHEMA_PREFIX = "generic.table.schema";
 }
diff --git a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/catalog/hive/HiveCatalogGenericMetadataTest.java b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/catalog/hive/HiveCatalogGenericMetadataTest.java
index 4ede98221ba..6dbfdc7bde2 100644
--- a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/catalog/hive/HiveCatalogGenericMetadataTest.java
+++ b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/catalog/hive/HiveCatalogGenericMetadataTest.java
@@ -18,11 +18,18 @@
 
 package org.apache.flink.table.catalog.hive;
 
+import org.apache.flink.table.api.DataTypes;
+import org.apache.flink.table.api.TableSchema;
 import org.apache.flink.table.catalog.CatalogPartition;
+import org.apache.flink.table.catalog.CatalogTableImpl;
+import org.apache.flink.table.catalog.ObjectPath;
+import org.apache.flink.table.types.DataType;
 
 import org.junit.BeforeClass;
 import org.junit.Test;
 
+import static org.junit.Assert.assertEquals;
+
 /**
  * Test for HiveCatalog on generic metadata.
  */
@@ -34,6 +41,30 @@ public class HiveCatalogGenericMetadataTest extends HiveCatalogMetadataTestBase
 		catalog.open();
 	}
 
+	// ------ tables ------
+
+	@Test
+	public void testGenericTableSchema() throws Exception {
+		catalog.createDatabase(db1, createDb(), false);
+
+		TableSchema tableSchema = TableSchema.builder()
+				.fields(new String[]{"col1", "col2", "col3"},
+						new DataType[]{DataTypes.TIMESTAMP(3), DataTypes.TIMESTAMP(6), DataTypes.TIMESTAMP(9)})
+				.watermark("col3", "col3", DataTypes.TIMESTAMP(9))
+				.build();
+
+		ObjectPath tablePath = new ObjectPath(db1, "generic_table");
+		try {
+			catalog.createTable(tablePath,
+					new CatalogTableImpl(tableSchema, getBatchTableProperties(), TEST_COMMENT),
+					false);
+
+			assertEquals(tableSchema, catalog.getTable(tablePath).getSchema());
+		} finally {
+			catalog.dropTable(tablePath, true);
+		}
+	}
+
 	// ------ partitions ------
 
 	@Test
@@ -144,6 +175,18 @@ public class HiveCatalogGenericMetadataTest extends HiveCatalogMetadataTestBase
 	public void testAlterPartitionTableStats() throws Exception {
 	}
 
+	@Override
+	public void testAlterPartitionedTable() throws Exception {
+	}
+
+	@Override
+	public void testAlterTableStats_partitionedTable() throws Exception {
+	}
+
+	@Override
+	public void testCreatePartitionedTable_Batch() throws Exception {
+	}
+
 	// ------ test utils ------
 
 	@Override
diff --git a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/catalog/hive/HiveCatalogITCase.java b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/catalog/hive/HiveCatalogITCase.java
index 9e2eb0d3be8..7530ed45362 100644
--- a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/catalog/hive/HiveCatalogITCase.java
+++ b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/catalog/hive/HiveCatalogITCase.java
@@ -28,6 +28,7 @@ import org.apache.flink.table.api.TableEnvironment;
 import org.apache.flink.table.api.TableSchema;
 import org.apache.flink.table.api.TableUtils;
 import org.apache.flink.table.api.Types;
+import org.apache.flink.table.api.config.ExecutionConfigOptions;
 import org.apache.flink.table.catalog.CatalogTable;
 import org.apache.flink.table.catalog.CatalogTableBuilder;
 import org.apache.flink.table.catalog.ObjectPath;
@@ -35,6 +36,7 @@ import org.apache.flink.table.descriptors.FileSystem;
 import org.apache.flink.table.descriptors.FormatDescriptor;
 import org.apache.flink.table.descriptors.OldCsv;
 import org.apache.flink.types.Row;
+import org.apache.flink.util.FileUtils;
 
 import com.klarna.hiverunner.HiveShell;
 import com.klarna.hiverunner.annotations.HiveSQL;
@@ -49,6 +51,7 @@ import org.junit.runner.RunWith;
 import java.io.BufferedReader;
 import java.io.File;
 import java.io.FileReader;
+import java.net.URI;
 import java.nio.file.Path;
 import java.nio.file.Paths;
 import java.util.Arrays;
@@ -225,4 +228,37 @@ public class HiveCatalogITCase {
 		tableEnv.sqlUpdate(String.format("DROP TABLE %s", sourceTableName));
 		tableEnv.sqlUpdate(String.format("DROP TABLE %s", sinkTableName));
 	}
+
+	@Test
+	public void testReadWriteCsv() throws Exception {
+		// similar to CatalogTableITCase::testReadWriteCsvUsingDDL but uses HiveCatalog
+		EnvironmentSettings settings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();
+		TableEnvironment tableEnv = TableEnvironment.create(settings);
+		tableEnv.getConfig().getConfiguration().setInteger(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM, 1);
+
+		tableEnv.registerCatalog("myhive", hiveCatalog);
+		tableEnv.useCatalog("myhive");
+
+		String srcPath = this.getClass().getResource("/csv/test3.csv").getPath();
+
+		tableEnv.sqlUpdate("CREATE TABLE src (" +
+				"price DECIMAL(10, 2),currency STRING,ts6 TIMESTAMP(6),ts AS CAST(ts6 AS TIMESTAMP(3)),WATERMARK FOR ts AS ts) " +
+				String.format("WITH ('connector.type' = 'filesystem','connector.path' = 'file://%s','format.type' = 'csv')", srcPath));
+
+		String sinkPath = new File(tempFolder.newFolder(), "csv-order-sink").toURI().toString();
+
+		tableEnv.sqlUpdate("CREATE TABLE sink (" +
+				"window_end TIMESTAMP(3),max_ts TIMESTAMP(6),counter BIGINT,total_price DECIMAL(10, 2)) " +
+				String.format("WITH ('connector.type' = 'filesystem','connector.path' = '%s','format.type' = 'csv')", sinkPath));
+
+		tableEnv.sqlUpdate("INSERT INTO sink " +
+				"SELECT TUMBLE_END(ts, INTERVAL '5' SECOND),MAX(ts6),COUNT(*),MAX(price) FROM src " +
+				"GROUP BY TUMBLE(ts, INTERVAL '5' SECOND)");
+
+		tableEnv.execute("testJob");
+
+		String expected = "2019-12-12 00:00:05.0,2019-12-12 00:00:04.004001,3,50.00\n" +
+				"2019-12-12 00:00:10.0,2019-12-12 00:00:06.006001,2,5.33\n";
+		assertEquals(expected, FileUtils.readFileUtf8(new File(new URI(sinkPath))));
+	}
 }
diff --git a/flink-connectors/flink-connector-hive/src/test/resources/csv/test3.csv b/flink-connectors/flink-connector-hive/src/test/resources/csv/test3.csv
new file mode 100644
index 00000000000..fdacf62fafd
--- /dev/null
+++ b/flink-connectors/flink-connector-hive/src/test/resources/csv/test3.csv
@@ -0,0 +1,5 @@
+2.02,Euro,2019-12-12 00:00:01.001001
+1.11,US Dollar,2019-12-12 00:00:02.002001
+50,Yen,2019-12-12 00:00:04.004001
+3.1,Euro,2019-12-12 00:00:05.005001
+5.33,US Dollar,2019-12-12 00:00:06.006001
\ No newline at end of file
diff --git a/flink-table/flink-table-common/src/test/java/org/apache/flink/table/catalog/CatalogTest.java b/flink-table/flink-table-common/src/test/java/org/apache/flink/table/catalog/CatalogTest.java
index 64cb2cecc3a..3ef2f945205 100644
--- a/flink-table/flink-table-common/src/test/java/org/apache/flink/table/catalog/CatalogTest.java
+++ b/flink-table/flink-table-common/src/test/java/org/apache/flink/table/catalog/CatalogTest.java
@@ -259,14 +259,19 @@ public abstract class CatalogTest {
 		assertEquals(path1.getObjectName(), tables.get(0));
 
 		catalog.dropTable(path1, false);
+	}
+
+	@Test
+	public void testCreatePartitionedTable_Batch() throws Exception {
+		catalog.createDatabase(db1, createDb(), false);
 
 		// Partitioned table
-		table = createPartitionedTable();
+		CatalogTable table = createPartitionedTable();
 		catalog.createTable(path1, table, false);
 
 		CatalogTestUtil.checkEquals(table, (CatalogTable) catalog.getTable(path1));
 
-		tables = catalog.listTables(db1);
+		List<String> tables = catalog.listTables(db1);
 
 		assertEquals(1, tables.size());
 		assertEquals(path1.getObjectName(), tables.get(0));
@@ -364,17 +369,6 @@ public abstract class CatalogTest {
 
 		catalog.dropTable(path1, false);
 
-		// Partitioned table
-		table = createPartitionedTable();
-		catalog.createTable(path1, table, false);
-
-		CatalogTestUtil.checkEquals(table, (CatalogTable) catalog.getTable(path1));
-
-		newTable = createAnotherPartitionedTable();
-		catalog.alterTable(path1, newTable, false);
-
-		CatalogTestUtil.checkEquals(newTable, (CatalogTable) catalog.getTable(path1));
-
 		// View
 		CatalogView view = createView();
 		catalog.createTable(path3, view, false);
@@ -388,6 +382,22 @@ public abstract class CatalogTest {
 		CatalogTestUtil.checkEquals(newView, (CatalogView) catalog.getTable(path3));
 	}
 
+	@Test
+	public void testAlterPartitionedTable() throws Exception {
+		catalog.createDatabase(db1, createDb(), false);
+
+		// Partitioned table
+		CatalogTable table = createPartitionedTable();
+		catalog.createTable(path1, table, false);
+
+		CatalogTestUtil.checkEquals(table, (CatalogTable) catalog.getTable(path1));
+
+		CatalogTable newTable = createAnotherPartitionedTable();
+		catalog.alterTable(path1, newTable, false);
+
+		CatalogTestUtil.checkEquals(newTable, (CatalogTable) catalog.getTable(path1));
+	}
+
 	@Test
 	public void testAlterTable_differentTypedTable() throws Exception {
 		catalog.createDatabase(db1, createDb(), false);
