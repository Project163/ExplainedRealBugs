<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Sat Nov 08 21:16:03 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[POOL-86] GenericKeyedObjectPool retaining too many idle objects</title>
                <link>https://issues.apache.org/jira/browse/POOL-86</link>
                <project id="12310488" key="POOL">Commons Pool</project>
                    <description>&lt;p&gt;There are two somewhat related problems in GenericKeyedObjectPool that cause&lt;br/&gt;
many more idle objects to be retained than should be, for much longer than they&lt;br/&gt;
should be.&lt;/p&gt;

&lt;p&gt;Firstly, borrowObject() is returning the LRU object rather than the MRU object.&lt;br/&gt;
That minimizes rather than maximizes object reuse and tends to refresh all the&lt;br/&gt;
idle objects, preventing them from becoming evictable.&lt;/p&gt;

&lt;p&gt;The idle LinkedList is being maintained with:&lt;/p&gt;

&lt;p&gt;    borrowObject:   list.removeFirst()&lt;br/&gt;
    returnObject:   list.addLast()&lt;/p&gt;

&lt;p&gt;These should either both be ...First() or both ...Last() so the list maintains&lt;br/&gt;
a newer-to-older, or vice-versa, ordering.  The code in evict() works from the&lt;br/&gt;
end of the list which indicates newer-to-older might have been originally&lt;br/&gt;
intended.&lt;/p&gt;

&lt;p&gt;Secondly, evict() itself has a couple of problems, both of which only show up&lt;br/&gt;
when many keys are in play:&lt;/p&gt;

&lt;p&gt;1.  Once it processes a key it doesn&apos;t advance to the next key.&lt;/p&gt;

&lt;p&gt;2.  _evictLastIndex is not working as documented (&quot;Position in the _pool where&lt;br/&gt;
    the _evictor last stopped&quot;).  Instead it&apos;s the position where the last scan&lt;br/&gt;
    started, and becomes the position at which it attempts to start scanning&lt;br/&gt;
    &lt;b&gt;in the next pool&lt;/b&gt;.  That just causes objects eligible for eviction to&lt;br/&gt;
    sometimes be skipped entirely.&lt;/p&gt;

&lt;p&gt;Here&apos;s a patch fixing both problems:&lt;/p&gt;

&lt;p&gt;GenericKeyedObjectPool.java&lt;br/&gt;
990c990&lt;br/&gt;
&amp;lt;             pool.addLast(new ObjectTimestampPair(obj));&lt;br/&gt;
&amp;#8212;&lt;br/&gt;
&amp;gt;             pool.addFirst(new ObjectTimestampPair(obj));&lt;br/&gt;
1094,1102c1094,1095&lt;br/&gt;
&amp;lt;                 }&lt;br/&gt;
&amp;lt;&lt;br/&gt;
&amp;lt;                 // if we don&apos;t have a keyed object pool iterator&lt;br/&gt;
&amp;lt;                 if (objIter == null) {&lt;br/&gt;
&amp;lt;                     final LinkedList list = (LinkedList)_poolMap.get(key);&lt;br/&gt;
&amp;lt;                     if (_evictLastIndex &amp;lt; 0 || _evictLastIndex &amp;gt; list.size()) &lt;/p&gt;
{
&amp;lt;                         _evictLastIndex = list.size();
&amp;lt;                     }
&lt;p&gt;&amp;lt;                     objIter = list.listIterator(_evictLastIndex);&lt;br/&gt;
&amp;#8212;&lt;br/&gt;
&amp;gt;                     LinkedList list = (LinkedList)_poolMap.get(key);&lt;br/&gt;
&amp;gt;                     objIter = list.listIterator(list.size());&lt;br/&gt;
1154,1155c1147&lt;br/&gt;
&amp;lt;                     _evictLastIndex = -1;&lt;br/&gt;
&amp;lt;                     objIter = null;&lt;br/&gt;
&amp;#8212;&lt;br/&gt;
&amp;gt;                     key = null;&lt;br/&gt;
1547,1551d1538&lt;br/&gt;
&amp;lt;&lt;br/&gt;
&amp;lt;     /**&lt;br/&gt;
&amp;lt;      * Position in the _pool where the _evictor last stopped.&lt;br/&gt;
&amp;lt;      */&lt;br/&gt;
&amp;lt;     private int _evictLastIndex = -1;&lt;/p&gt;

&lt;p&gt;I have a local unit test for this but it depends on some other code I can&apos;t&lt;br/&gt;
donate.  It works like this:&lt;/p&gt;

&lt;p&gt;1.  Fill the pool with _maxTotal objects using many different keys.&lt;br/&gt;
2.  Select X as a small number, e.g. 2.&lt;br/&gt;
3.  Compute:&lt;br/&gt;
        maxEvictionRunsNeeded = (maxTotal - X) / numTestsPerEvictionRun + 2;&lt;br/&gt;
        maxEvictionTime = minEvictableIdleTimeMillis + maxEvictionRunsNeeded * timeBetweenEvictionRunsMillis;&lt;br/&gt;
4.  Enter a loop:&lt;br/&gt;
    a.  Borrow X objects.&lt;br/&gt;
    b.  Exit if _totalIdle = 0&lt;br/&gt;
    c.  Return the X objects.&lt;/p&gt;

&lt;p&gt;Fail if loop doesn&apos;t exit within maxEvictionTime.&lt;/p&gt;

&lt;p&gt;Mike&lt;/p&gt;</description>
                <environment></environment>
        <key id="12352917">POOL-86</key>
            <summary>GenericKeyedObjectPool retaining too many idle objects</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="jackknifebarber">Mike Martin</reporter>
                        <labels>
                    </labels>
                <created>Tue, 10 Oct 2006 23:01:53 +0000</created>
                <updated>Sat, 5 May 2012 22:43:12 +0000</updated>
                            <resolved>Sat, 5 May 2012 22:43:12 +0000</resolved>
                                    <version>1.3</version>
                                    <fixVersion>1.4</fixVersion>
                                        <due></due>
                            <votes>1</votes>
                                    <watches>1</watches>
                                                                                                                <comments>
                            <comment id="12445490" author="sandymac" created="Mon, 30 Oct 2006 04:18:29 +0000"  >&lt;p&gt;&amp;gt; Firstly, borrowObject() is returning the LRU object rather than the MRU object. That minimizes rather than maximizes object reuse and tends to refresh all the idle objects, preventing them from becoming evictable. &lt;/p&gt;

&lt;p&gt;I think you have minimize and maximize backwards and reusing idle objects is why you&apos;d want to use a pool. There are a couple of reasons for GKOP preferring the LRU idle object:&lt;/p&gt;

&lt;p&gt;1. The reason to use a pool is that you have reusable objects that are expensive to create or dispose of. Using a FIFO ordering (LRU) means you use all the pooled objects over time instead of just a handful. Idle object will be kept fresh and ready to reuse or they will fail validateObject and be discarded. LIFO ordering (MRU) will tend to reuse the same objects and others will rarely be used under peak load, unless you have an idle object evictor to check their reusability ...&lt;/p&gt;

&lt;p&gt;2. Using an idle object evictor can cause deadlocks. If the PoolableObjectFactory uses synchronization and the pool is accessed in a synchronized context it can lead to locks being acquired in the opposite order which creates a deadlock possible race condition and thread-safty cannot be guaranteed. Basically, avoid idle object eviction if you can.&lt;/p&gt;

&lt;p&gt;3. A FIFO pool is implicitly optimized to be prepared to handle the next load spike. And that is the point of the pool. Optimizing the pool for low load levels reduces the important advantage of using a pool, being prepared for lots of work.&lt;/p&gt;

&lt;p&gt;I can think of situations where the above doesn&apos;t apply but &quot;Generic&quot; pools are named &quot;Generic&quot; because they aren&apos;t optimized for any specific situation.&lt;/p&gt;


&lt;p&gt;&amp;gt; Secondly, evict() itself has a couple of problems, both of which only show up when many keys are in play: &lt;/p&gt;

&lt;p&gt;&amp;gt; Here&apos;s a patch fixing both problems: &lt;/p&gt;

&lt;p&gt;If you could work the patch to only address the evict issue and attach it as a file instead of a copy/paste in it&apos;s own issue I&apos;d appreciate it.&lt;/p&gt;</comment>
                            <comment id="12445665" author="jackknifebarber" created="Mon, 30 Oct 2006 17:52:33 +0000"  >&lt;p&gt;Sandy wrote:&lt;br/&gt;
&amp;gt; I think you have minimize and maximize backwards and reusing idle objects is&lt;br/&gt;
&amp;gt; why you&apos;d want to use a pool. There are a couple of reasons for GKOP&lt;br/&gt;
&amp;gt; preferring the LRU idle object: &lt;br/&gt;
&amp;gt; &lt;br/&gt;
&amp;gt; 1. The reason to use a pool is that you have reusable objects that are&lt;br/&gt;
&amp;gt; expensive to create or dispose of. Using a FIFO ordering (LRU) means you use&lt;br/&gt;
&amp;gt; all the pooled objects over time instead of just a handful.  Idle object will&lt;br/&gt;
&amp;gt; be kept fresh and ready to reuse or they will fail validateObject and be&lt;br/&gt;
&amp;gt; discarded.  LIFO ordering (MRU) will tend to reuse the same objects and others&lt;br/&gt;
&amp;gt; will rarely be used under peak load, unless you have an idle object evictor to&lt;br/&gt;
&amp;gt; check their reusability ... &lt;/p&gt;

&lt;p&gt;I think you&apos;re omitting a key point:  the reason to use a pool is not just to&lt;br/&gt;
avoid object creation expense; it&apos;s also to minimize the number of objects&lt;br/&gt;
needed to satisfy demand.  At the least, that is &lt;b&gt;very&lt;/b&gt; true of the primary use&lt;br/&gt;
case I know of for pooling, DB connection pooling.&lt;/p&gt;

&lt;p&gt;DB connections are a very expensive resource to lie around unused (or underused)&lt;br/&gt;
indefinitely.  They are usually associated with an &lt;b&gt;entire process&lt;/b&gt; on a DB&lt;br/&gt;
server.  My company began reworking its pooling code using commons-pool&lt;br/&gt;
specifically to gain control over an exploding # of idle connections as the # of&lt;br/&gt;
DB users and # of servers grew (I&apos;m talking hundreds of users and thousands of&lt;br/&gt;
connections).&lt;/p&gt;

&lt;p&gt;So we can&apos;t simply say &quot;avoid idle object eviction&quot;; for many folks it&apos;s&lt;br/&gt;
absolutely necessary.  And if the feature is there, it needs to work.&lt;/p&gt;

&lt;p&gt;Idle eviction can only work properly using a LIFO list.  Consider again the unit&lt;br/&gt;
test I described.  It&apos;s not an exaggerated case.  If I experience a momentary&lt;br/&gt;
load spike that opens, say, 100 concurrent connections, and it&apos;s followed by a&lt;br/&gt;
persistent &quot;normal&quot; load that needs only 2, I absolutely &lt;b&gt;cannot&lt;/b&gt; continually&lt;br/&gt;
consume 100 connections thereafter when 2 would suffice.  The FIFO list only&lt;br/&gt;
allows idleness to be detected if demand drops all the way to zero.  Otherwise&lt;br/&gt;
it becomes impossible for the pool to realize that only 2 connections are now&lt;br/&gt;
sufficient; the 2 concurrent users keep cycling around all 100 connections, and&lt;br/&gt;
each of the 100 sees very little work.&lt;/p&gt;

&lt;p&gt;In addition, consider that a LIFO list still works correctly if the user is&lt;br/&gt;
&lt;b&gt;not&lt;/b&gt; using idle eviction.  But the reverse is not true; FIFO ruins things when&lt;br/&gt;
idle eviction is needed.&lt;/p&gt;

&lt;p&gt;There is no such thing as keeping idle objects &quot;fresh&quot;.  By definition, every&lt;br/&gt;
object in a pool is equivalent and interchangeable.  You&apos;re right that LIFO&lt;br/&gt;
&lt;b&gt;will&lt;/b&gt; tend to reuse the same objects, but that&apos;s the whole point!  That is how&lt;br/&gt;
a pool tracks the demand being placed on it.&lt;/p&gt;

&lt;p&gt;I&apos;ve done quite a bit of analysis of GKOP&apos;s behavior in both cases, before and&lt;br/&gt;
after this fix, in a real-world production scenario.  Idle eviction simply *does&lt;br/&gt;
not work* with a FIFO list unless demand drops to zero.  Otherwise the pool&lt;br/&gt;
rises to its high-water mark and stays there.&lt;/p&gt;

&lt;p&gt;So I hope you&apos;ll reconsider fixing this, and I&apos;ll attach the patch as a file for&lt;br/&gt;
you.&lt;/p&gt;

&lt;p&gt;Thanks,&lt;/p&gt;

&lt;p&gt;Mike&lt;/p&gt;</comment>
                            <comment id="12445703" author="jackknifebarber" created="Mon, 30 Oct 2006 20:15:28 +0000"  >&lt;p&gt;Here&apos;s a copy of the patch as applied against the current trunk sources.&lt;/p&gt;</comment>
                            <comment id="12445717" author="jackknifebarber" created="Mon, 30 Oct 2006 21:07:35 +0000"  >&lt;p&gt;I&apos;ve attached another patch which includes the necessary changes to the unit test.&lt;/p&gt;</comment>
                            <comment id="12445798" author="sandymac" created="Tue, 31 Oct 2006 06:36:38 +0000"  >&lt;p&gt;I think the changes you suggest to the evict method will fail to make forward progress if getNumTests() is less than the number of idle objects for the current key. This is why the _evictLastIndex was needed. Could you rework the patch taking in to account  small values of _numTestsPerEvictionRun and attach it to a seperate issue so that each issue only tracks one bug at a time.&lt;/p&gt;</comment>
                            <comment id="12445803" author="sandymac" created="Tue, 31 Oct 2006 07:00:42 +0000"  >&lt;p&gt;Back to the first part of your original submission: What I understand you really want is a way to prune the pool size down after a peak load spike.&lt;/p&gt;

&lt;p&gt;Would a decorator that either occasionally discards returned objects or uses some heuristics to determine when to discard returned objects meet your needs? This could be implemented so that it doesn&apos;t need an eviction thread which means we can guarantee thread-safety of the pool implementation but it would require some pool activity to do it&apos;s work.&lt;/p&gt;</comment>
                            <comment id="12445951" author="jackknifebarber" created="Tue, 31 Oct 2006 16:44:14 +0000"  >&lt;p&gt;Sandy wrote:&lt;br/&gt;
&amp;gt; I think the changes you suggest to the evict method will fail to make forward&lt;br/&gt;
&amp;gt; progress if getNumTests() is less than the number of idle objects for the&lt;br/&gt;
&amp;gt; current key. This is why the _evictLastIndex was needed.&lt;/p&gt;

&lt;p&gt;_recentlyEvictedKeys can serve the same purpose.  This change would do that:&lt;/p&gt;

&lt;p&gt;&amp;#8212; GenericKeyedObjectPool.java.prev    Tue Oct 31 10:24:44 2006&lt;br/&gt;
+++ GenericKeyedObjectPool.java Tue Oct 31 10:25:06 2006&lt;br/&gt;
@@ -1207,6 +1207,7 @@&lt;br/&gt;
                         return;&lt;br/&gt;
                     }&lt;br/&gt;
                     key = keyIter.next();&lt;br/&gt;
+                    _recentlyEvictedKeys.add(key);&lt;br/&gt;
                     LinkedList list = (LinkedList)_poolMap.get(key);&lt;br/&gt;
                     objIter = list.listIterator(list.size());&lt;br/&gt;
                 }&lt;br/&gt;
@@ -1259,7 +1260,6 @@&lt;br/&gt;
                     }&lt;br/&gt;
                 } else &lt;/p&gt;
{
                     // else done evicting keyed pool
-                    _recentlyEvictedKeys.add(key);
                     key = null;
                 }
&lt;p&gt;             }&lt;/p&gt;

&lt;p&gt;&amp;gt; Back to the first part of your original submission: What I understand you&lt;br/&gt;
&amp;gt; really want is a way to prune the pool size down after a peak load spike.&lt;br/&gt;
&amp;gt;&lt;br/&gt;
&amp;gt; Would a decorator that either occasionally discards returned objects or uses&lt;br/&gt;
&amp;gt; some heuristics to determine when to discard returned objects meet your needs?&lt;br/&gt;
&amp;gt; This could be implemented so that it doesn&apos;t need an eviction thread which&lt;br/&gt;
&amp;gt; means we can guarantee thread-safety of the pool implementation but it would&lt;br/&gt;
&amp;gt; require some pool activity to do it&apos;s work.&lt;/p&gt;

&lt;p&gt;I think what I want, and what most DB connection pools need, is the existing&lt;br/&gt;
idle eviction facility as advertised.&lt;/p&gt;

&lt;p&gt;I&apos;ve never encountered the thread-safety problems you mention.  Is there an&lt;br/&gt;
outstanding bug regarding that?  If so, it&apos;s not occurring in my environment&lt;br/&gt;
which as I said involves hundreds of keys (DB users), thousands of connections,&lt;br/&gt;
and in fact is used by hundreds of threads.&lt;/p&gt;</comment>
                            <comment id="12446123" author="sandymac" created="Tue, 31 Oct 2006 23:51:46 +0000"  >&lt;p&gt;&amp;gt; I think what I want, and what most DB connection pools need, is the existing&lt;br/&gt;
&amp;gt; idle eviction facility as advertised.&lt;/p&gt;

&lt;p&gt;I can appreciate that you probably think I&apos;m just being bone headed. But idle object eviction as implemented by pool runs the risk of deadlocks and it cannot be fixed without breaking backwards compatibility. When considering only one usage of pool it is possible to use the evictor in a thread-safe manner but it would be irresponsible of me to encourage use of a risky pool feature when the desired results can be implemented in a manner that is thread-safe in every use case.&lt;/p&gt;

&lt;p&gt;I&apos;m trying to provide a solution that will meet your needs and be deadlock proof for everyone else at the same time. If you don&apos;t want to work with me on this then you are free to maintain your own version of a pool under the terms of the ASL-2.0.&lt;/p&gt;


&lt;p&gt;&amp;gt; I&apos;ve never encountered the thread-safety problems you mention. Is there an&lt;br/&gt;
&amp;gt; outstanding bug regarding that?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/DBCP-65&quot; title=&quot;[dbcp] Deadlock when evicting dbcp objects (testWhileIdle=true)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;DBCP-65&quot;&gt;&lt;del&gt;DBCP-65&lt;/del&gt;&lt;/a&gt; is one example: &lt;a href=&quot;https://issues.apache.org/jira/browse/DBCP-65&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/DBCP-65&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The problem is locks are acquired in reverse order. If there are two locks, ClientLock and PoolLock and the ClientLock is aquired by code using pool and by the PoolableObjectFactory provided to the pool you run the risk of deadlock when using an evitor.&lt;/p&gt;

&lt;p&gt;Normal usage will acquire ClientLock, call a pool method which will acquire the internal PoolLock, and then the pool will call the PoolableObjectFactory which also acquires the ClientLock again. Effectively the lock acquistion order is ClientLock then PoolLock.&lt;/p&gt;

&lt;p&gt;A pool with an evictor thread won&apos;t know about the ClientLock. After it wakes up to do idle object eviction it will acquire the PoolLock and the call the PoolableObjectFactory which will acquire the ClientLock. In the case the lock acquisition is PoolLock then ClientLock.&lt;/p&gt;

&lt;p&gt;Acquiring locks in reverse order is the most basic example of how to cause a deadlock in any text on concurrency.&lt;/p&gt;


&lt;p&gt;&amp;gt; If so, it&apos;s not occurring in my environment&lt;br/&gt;
&amp;gt; which as I said involves hundreds of keys (DB users), thousands of connections,&lt;br/&gt;
&amp;gt; and in fact is used by hundreds of threads.&lt;/p&gt;

&lt;p&gt;I&apos;m not privy to your code. I don&apos;t know if your code is thread-safe or just lucky. Just because pool works in a particular setup doesn&apos;t change the fact that idle object eviction is risky and should be avoided.&lt;/p&gt;


&lt;p&gt;&amp;gt; _recentlyEvictedKeys can serve the same purpose. This change would do that: &lt;/p&gt;

&lt;p&gt;Almost, this has the same problem of possibly skipping objects eligible for eviction which is the same problem we have now mentioned in item #2 of your original post.&lt;/p&gt;</comment>
                            <comment id="12446150" author="jackknifebarber" created="Wed, 1 Nov 2006 03:36:32 +0000"  >&lt;p&gt;If you think through it, you&apos;ll realize that the lock ordering vulnerability&lt;br/&gt;
you&apos;re describing is not a function of there being an idle evictor thread.  It&lt;br/&gt;
comes from the fact that there&apos;s a user callback (the factory) in play in the&lt;br/&gt;
first place.&lt;/p&gt;

&lt;p&gt;If the locking policy of the pool is clearly documented (factory is called&lt;br/&gt;
while pool is locked), then the deadlock scenario you described is not&lt;br/&gt;
something the pool itself can avoid; that client&apos;s approach to the use of his&lt;br/&gt;
ClientLock is provably wrong to begin with.&lt;/p&gt;

&lt;p&gt;Sounds like we&apos;ll need to maintain our own pool class.  Thanks anyway.&lt;/p&gt;

&lt;p&gt;Mike&lt;/p&gt;</comment>
                            <comment id="12449219" author="sandymac" created="Mon, 13 Nov 2006 01:41:43 +0000"  >&lt;p&gt;Committed some Poolutils.erodingPool decorators which addresses the issue of need a means to shrink the size of a FIFO ordered pool during low workloads. See: &lt;a href=&quot;http://svn.apache.org/viewvc?view=rev&amp;amp;revision=474109&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewvc?view=rev&amp;amp;revision=474109&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="12486735" author="reamon" created="Wed, 4 Apr 2007 18:33:08 +0000"  >&lt;p&gt;With respect to all the constraints on what can be done to the code, the resolution to this issue feels like a hack to me.&lt;/p&gt;

&lt;p&gt;From Sandy&apos;s earlier post on Oct 29:&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;quote&amp;#93;&lt;/span&gt;&lt;br/&gt;
1. The reason to use a pool is that you have reusable objects that are expensive to create or dispose of. Using a FIFO ordering (LRU) means you use all the pooled objects over time instead of just a handful. Idle object will be kept fresh and ready to reuse or they will fail validateObject and be discarded. LIFO ordering (MRU) will tend to reuse the same objects and others will rarely be used under peak load, unless you have an idle object evictor to check their reusability ...&lt;/p&gt;

&lt;p&gt;2. Using an idle object evictor can cause deadlocks. If the PoolableObjectFactory uses synchronization and the pool is accessed in a synchronized context it can lead to locks being acquired in the opposite order which creates a deadlock possible race condition and thread-safty cannot be guaranteed. Basically, avoid idle object eviction if you can. &lt;/p&gt;

&lt;p&gt;3. A FIFO pool is implicitly optimized to be prepared to handle the next load spike. And that is the point of the pool. Optimizing the pool for low load levels reduces the important advantage of using a pool, being prepared for lots of work.&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;/quote&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;1. We all agree on the purpose of the pool--to have idle objects ready to go when they are needed. Often it is also necessary to limit the number of objects in the pool. Where there is disagreement is on how the pool should behave.&lt;/p&gt;

&lt;p&gt;If initial performance under peak load is paramount (want to have enough idle objects on hand), then the existing configuration parameters permit doing that. One sets the minIdle value and the testWhileIdle appropriately and one can assure having enough pooled objects to handle periodic usage bursts.&lt;/p&gt;

&lt;p&gt;However, if one wants the pool to grow and shrink according to usage and wants the number of idle objects to quiesce when usage subsides (e.g. no sense consuming a client license when it isn&apos;t being used), then there is little that can be done if the pool is FIFO. As Mike points out, one or two threads could keep all pool objects &quot;fresh&quot; even when they aren&apos;t needed any longer.&lt;/p&gt;

&lt;p&gt;2. Use of an evictor thread can definitely cause deadlocks--if the factory uses client locks. If it doesn, then it doesn&apos;t matter. And as Mike points out, if the programmer knows the behavior of the pool, then they can presumably design around that to avoid deadlock.&lt;/p&gt;

&lt;p&gt;Are you saying that the evictor thread should never have been implemented? Is it going to be deprecated? What would you do as an alternative, if you had a clean slate?&lt;/p&gt;

&lt;p&gt;3. Having the pool large enough to handle peak load is but one of many constraints that the pool concept is designed to address. As mentioned above,  if one needs a minimum pool size at the ready for peak loads this can be managed using configuration. But if one doesn&apos;t need that and insteads needs the pool to grow and shrink using the max/min sizes and idle timeouts then the current implementation inhibits shrinking of the pool.&lt;/p&gt;

&lt;p&gt;Wrapping the pool classes with the eroding class seems like a stop-gap added to address something that should be part of the core pool implementation. While throwing away the object being returned vs throwing away an idle and expired object from the pool is a wash, it seems useful for the pool implementation to have some sort of mechanism to purge expired objects from the pool based simply on idle time. How that is implemented, I don&apos;t have much preference, but the current implementation clearly needs a tweak.&lt;/p&gt;

&lt;p&gt;I respectfully request that this solution be reconsidered.&lt;/p&gt;</comment>
                            <comment id="12507060" author="psteitz" created="Thu, 21 Jun 2007 22:44:29 +0000"  >&lt;p&gt;I agree with Rob that a better solution should be provided - either make GenericObjectPool&apos;s LRU/MRU behavior configurable or provide an alternative MRU-based impl.  Robust alternatives are available in the compositepool package, so one resolution is to close this on release of pool 2.0, but I would like to consider addressing this in the 1.x branch as well.&lt;/p&gt;</comment>
                            <comment id="12532934" author="psteitz" created="Sun, 7 Oct 2007 00:09:23 +0000"  >&lt;p&gt;Fixed for GenericObjectPool, GenericKeyedObjectPool in r582538.&lt;/p&gt;</comment>
                            <comment id="12549673" author="markt" created="Sat, 8 Dec 2007 08:52:23 +0000"  >&lt;p&gt;I&apos;ve looked through the code and the patch looks good to me.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="12379238">POOL-105</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12343956" name="pool-86.patch" size="2464" author="jackknifebarber" created="Mon, 30 Oct 2006 20:15:28 +0000"/>
                            <attachment id="12343958" name="pool-86.withtest.patch" size="3858" author="jackknifebarber" created="Mon, 30 Oct 2006 21:07:35 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>166679</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            17 years, 51 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0zeiv:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>204655</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            </customfields>
    </item>
</channel>
</rss>