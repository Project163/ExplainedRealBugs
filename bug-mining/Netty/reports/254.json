{"url":"https://api.github.com/repos/netty/netty/issues/12749","repository_url":"https://api.github.com/repos/netty/netty","labels_url":"https://api.github.com/repos/netty/netty/issues/12749/labels{/name}","comments_url":"https://api.github.com/repos/netty/netty/issues/12749/comments","events_url":"https://api.github.com/repos/netty/netty/issues/12749/events","html_url":"https://github.com/netty/netty/issues/12749","id":1353402572,"node_id":"I_kwDOABA-c85Qq0jM","number":12749,"title":"Usage of PooledByteBufAllocator from resizable thread pools may lead to memory leaks","user":{"login":"hpple","id":5455032,"node_id":"MDQ6VXNlcjU0NTUwMzI=","avatar_url":"https://avatars.githubusercontent.com/u/5455032?v=4","gravatar_id":"","url":"https://api.github.com/users/hpple","html_url":"https://github.com/hpple","followers_url":"https://api.github.com/users/hpple/followers","following_url":"https://api.github.com/users/hpple/following{/other_user}","gists_url":"https://api.github.com/users/hpple/gists{/gist_id}","starred_url":"https://api.github.com/users/hpple/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hpple/subscriptions","organizations_url":"https://api.github.com/users/hpple/orgs","repos_url":"https://api.github.com/users/hpple/repos","events_url":"https://api.github.com/users/hpple/events{/privacy}","received_events_url":"https://api.github.com/users/hpple/received_events","type":"User","user_view_type":"public","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2022-08-28T16:23:51Z","updated_at":"2022-10-18T14:42:50Z","closed_at":"2022-10-18T14:42:50Z","author_association":"CONTRIBUTOR","type":null,"active_lock_reason":null,"sub_issues_summary":{"total":0,"completed":0,"percent_completed":0},"issue_dependencies_summary":{"blocked_by":0,"total_blocked_by":0,"blocking":0,"total_blocking":0},"body":"### Expected behavior\r\nNo memory leaks\r\n\r\n### Actual behavior\r\nMemory is leaking in the presence of resizable thread pools\r\n\r\n### Steps to reproduce\r\n### Minimal yet complete reproducer code (or URL to code)\r\nsee below\r\n(all reproducers were tested with `-XX:MaxDirectMemorySize=1G`)\r\n\r\n### Netty version\r\nI'm using 4.1.63.Final in prod, but it is still reproducible for 4.1.80.Final too\r\n\r\n### JVM version (e.g. `java -version`)\r\nopenjdk version \"15.0.7\" 2022-04-19\r\nOpenJDK Runtime Environment Zulu15.40+19-CA (build 15.0.7+4-MTS)\r\nOpenJDK 64-Bit Server VM Zulu15.40+19-CA (build 15.0.7+4-MTS, mixed mode, sharing)\r\n\r\n### OS version (e.g. `uname -a`)\r\nLinux hpple-l 5.11.0-41-generic #45~20.04.1-Ubuntu SMP Wed Nov 10 10:20:10 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n----\r\nHi!\r\n\r\nI'm using Netty not only as a networking lib but as a general-purpose allocator as well. \r\nIn my case, buf allocations could be performed from any kind of thread pools: ELGs, ThreadPoolExecutors, FJPs.\r\nAlso, I'm using `FastThreadLocalThread` (via `DefaultThreadFactory`) for my own `ThreadPoolExecutor` pools to leverage the power of `FastThreadLocals`.\r\nUnfortunately, it's not possible to do the same thing for FJP since it requires `ForkJoinWorkerThreads` to operate.\r\n\r\nSo, I've noticed that if the pool resizes over the time (like FJP that has a KeepAlive mechanism to shrink the pool if it hasn't been used for some time) then it could lead to unexpected memory leaks invisible to the current leak detector.\r\n\r\nMy first thoughts were like:\r\n- What if some buffers outlive the thread which created them?\r\n- On buffer release, underlying memory is always returned to the thread-local cache of the thread which did the allocation, no matter which thread deallocates\r\n- `PooledByteBuf` holds a strong reference to `PoolThreadCache`\r\n- So, no finalizer will be called until the last buffer created by this thread is collected by GC\r\n- When the allocating thread is not a `FastThreadLocalThread` (e.g. FJP worker) no `onRemoval` will be called either \r\n- No further allocations since the thread is terminated -> no internal trim (`freeSweepAllocationThreshold`)\r\n- It's enough to have only one long living buffer to prevent the whole thread-local cache from returning memory to the related arena \r\n\r\nThere is the simple reproducer (important properties are set explicitly because different Netty versions may have different defaults):\r\n```java\r\npublic class NettyLeak0 {\r\n\r\n    public static void main(String[] args) {\r\n        System.setProperty(\"io.netty.allocator.useCacheForAllThreads\", \"true\");\r\n        System.setProperty(\"io.netty.allocator.maxOrder\", \"9\");\r\n        Thread.setDefaultUncaughtExceptionHandler((t, e) -> {\r\n            e.printStackTrace();\r\n            Runtime.getRuntime().halt(42);\r\n        });\r\n\r\n        var bufSize = 32 * 1024;\r\n\r\n        var longLiving = new AtomicInteger();\r\n        var longLivingSink = Collections.synchronizedList(new ArrayList<ByteBuf>());\r\n        for (int i = 0; true; i++) {\r\n            new Thread(() -> {\r\n                for (int x = 0; x < 256; x++) {\r\n                    if (x != 0) {\r\n                        // allocate different size classes to fill more thread caches\r\n                        for (int j = 0; j <= 10; j++) {\r\n                            ByteBuf buf = PooledByteBufAllocator.DEFAULT.directBuffer(bufSize >> j);\r\n                            buf.release();\r\n                        }\r\n                    } else {\r\n                        // only one buf is alive after thread termination\r\n                        var buf = PooledByteBufAllocator.DEFAULT.directBuffer(1);\r\n                        longLiving.incrementAndGet();\r\n                        longLivingSink.add(buf);\r\n                    }\r\n                }\r\n            }).start();\r\n\r\n            if (i % 1000 == 0) {\r\n                System.gc(); // to get a chance for io.netty.buffer.PoolThreadCache.finalize\r\n                System.out.println(PooledByteBufAllocator.DEFAULT.metric());\r\n                System.out.println(\r\n                    \"chunks count: \" +\r\n                        PooledByteBufAllocator.DEFAULT.metric().directArenas().stream()\r\n                            .flatMap(m -> m.chunkLists().stream())\r\n                            .mapToInt(m -> {\r\n                                int count = 0;\r\n                                for (var __ : m) {\r\n                                    count += 1;\r\n                                }\r\n                                return count;\r\n                            })\r\n                            .sum()\r\n                );\r\n                var ll = longLiving.get();\r\n                System.out.println(\"long living: \" + ll);\r\n                // real size of a buf will be > 1b & < 1kb, but just estimate it as 1kb\r\n                System.out.println(\"long living size (MB): \" + ll / 1024);\r\n                System.out.println();\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nAs far as I can see, there were several attempts to fix this problem in the past:\r\n- a method to call directly: https://github.com/netty/netty/blame/4.1/buffer/src/main/java/io/netty/buffer/PooledByteBufAllocator.java#L499\r\n- a global executor to perform a cleanup: https://github.com/netty/netty/commit/ceffa82d0d341c08804554441601fdf253227fa9\r\n- another kind of watcher: https://github.com/netty/netty/commit/085a61a310187052e32b4a0e7ae9700dbe926848#diff-2fbf82a7217a5ef66fbe8ff7ea653e1d802b14af03ce8d72863f8d66d057cf44\r\n- object cleaner: https://github.com/netty/netty/commit/e329ca1cf3cecd41c9cf1a5a7be369a65f002ab5\r\n- finalizer: https://github.com/netty/netty/pull/8064\r\n\r\nMoreover, caching for user threads has already been disabled in latest releases: https://github.com/netty/netty/pull/12109\r\nBut, as I've already said, I'm using Netty as a general-purpose allocator, so `io.netty.allocator.useCacheForAllThreads=false` just does not fit. \r\n\r\nSo, initially, I've tried to use this deprecated `freeThreadLocalCache` method from `PoolThreadCache` on thread termination. But, it looks like it only makes things worse:\r\n- It frees the cache and returns memory blocks acquired from already released buffers to the related arena \r\n- What if we have some long living buffers that outlive the thread but actually being released during the application lifetime, just a bit later?\r\n- Reclaimed memory of such long living buffers will be added to the cache after it has been freed\r\n- Eventually, finalizer will come, but it will do nothing since `PoolThreadCache#freed` will be set to `true`\r\n- So, those memory blocks will never be returned to any arena\r\n\r\nHere is the reproducer:\r\n```java\r\npublic class NettyLeak1 {\r\n\r\n    public static void main(String[] args) {\r\n        System.setProperty(\"io.netty.allocator.useCacheForAllThreads\", \"true\");\r\n        System.setProperty(\"io.netty.allocator.maxOrder\", \"9\");\r\n        Thread.setDefaultUncaughtExceptionHandler((t, e) -> {\r\n            e.printStackTrace();\r\n            Runtime.getRuntime().halt(42);\r\n        });\r\n\r\n        var bufSize = 16 * 1024;\r\n\r\n        var longLiving = new AtomicInteger();\r\n        for (int i = 0; true; i++) {\r\n            new Thread(() -> {\r\n                for (int x = 0; x < 10; x++) {\r\n                    ByteBuf buf = PooledByteBufAllocator.DEFAULT.directBuffer(bufSize);\r\n                    if (x != 0) {\r\n                        buf.release();\r\n                    } else {\r\n                        // release one buffer with delay, keeping some in-flight\r\n                        longLiving.incrementAndGet();\r\n                        CompletableFuture.runAsync(() -> {\r\n                            buf.release();\r\n                            longLiving.decrementAndGet();\r\n                        }, delayedExecutor(5, TimeUnit.SECONDS));\r\n                    }\r\n                }\r\n                // explicitly free before thread termination\r\n                PooledByteBufAllocator.DEFAULT.freeThreadLocalCache();\r\n            }).start();\r\n\r\n            if (i % 1000 == 0) {\r\n                System.gc(); // to ensure io.netty.buffer.PoolThreadCache.finalize\r\n                System.out.println(PooledByteBufAllocator.DEFAULT.metric());\r\n                System.out.println(\r\n                    \"chunks count: \" +\r\n                        PooledByteBufAllocator.DEFAULT.metric().directArenas().stream()\r\n                            .flatMap(m -> m.chunkLists().stream())\r\n                            .mapToInt(m -> {\r\n                                int count = 0;\r\n                                for (var __ : m) {\r\n                                    count += 1;\r\n                                }\r\n                                return count;\r\n                            })\r\n                            .sum()\r\n                );\r\n                var ll = longLiving.get();\r\n                System.out.println(\"long living: \" + ll);\r\n                System.out.println(\"long living size (MB): \" + ll * bufSize / 1024 / 1024);\r\n                System.out.println();\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nWhich means that this method is not only obsolete, but also is unsafe to use in current versions. Perhaps, it's worth mentioning this dangerous side-effect  in javadoc.\r\n\r\nSo, my final workaround is a hacky double call to the trim method on thread termination (it works because a second call trims everything since there are no allocation stats between two consequent calls and trim does not interfere with finalizer):\r\n```java\r\npublic class NettyNoLeak1 {\r\n\r\n    public static void main(String[] args) {\r\n        System.setProperty(\"io.netty.allocator.useCacheForAllThreads\", \"true\");\r\n        System.setProperty(\"io.netty.allocator.maxOrder\", \"9\");\r\n        Thread.setDefaultUncaughtExceptionHandler((t, e) -> {\r\n            e.printStackTrace();\r\n            Runtime.getRuntime().halt(42);\r\n        });\r\n\r\n        var bufSize = 16 * 1024;\r\n\r\n        var longLiving = new AtomicInteger();\r\n        for (int i = 0; true; i++) {\r\n            new Thread(() -> {\r\n                for (int x = 0; x < 10; x++) {\r\n                    ByteBuf buf = PooledByteBufAllocator.DEFAULT.directBuffer(bufSize);\r\n                    if (x != 0) {\r\n                        buf.release();\r\n                    } else {\r\n                        // release one buffer with delay, keeping some in-flight\r\n                        longLiving.incrementAndGet();\r\n                        CompletableFuture.runAsync(() -> {\r\n                            buf.release();\r\n                            longLiving.decrementAndGet();\r\n                        }, delayedExecutor(5, TimeUnit.SECONDS));\r\n                    }\r\n                }\r\n                // explicitly trim before thread termination\r\n                if (PooledByteBufAllocator.DEFAULT.trimCurrentThreadCache()) {\r\n                    PooledByteBufAllocator.DEFAULT.trimCurrentThreadCache();\r\n                };\r\n            }).start();\r\n\r\n            if (i % 1000 == 0) {\r\n                System.gc(); // to ensure io.netty.buffer.PoolThreadCache.finalize\r\n                System.out.println(PooledByteBufAllocator.DEFAULT.metric());\r\n                System.out.println(\r\n                    \"chunks count: \" +\r\n                        PooledByteBufAllocator.DEFAULT.metric().directArenas().stream()\r\n                            .flatMap(m -> m.chunkLists().stream())\r\n                            .mapToInt(m -> {\r\n                                int count = 0;\r\n                                for (var __ : m) {\r\n                                    count += 1;\r\n                                }\r\n                                return count;\r\n                            })\r\n                            .sum()\r\n                );\r\n                var ll = longLiving.get();\r\n                System.out.println(\"long living: \" + ll);\r\n                System.out.println(\"long living size (MB): \" + ll * bufSize / 1024 / 1024);\r\n                System.out.println();\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nAlso, I suppose that if multiple ELGs are created and destroyed during the lifetime of application the same situation is possible too. In this case `onRemoval` will interfere with finalizer and prevent it from returning the memory of long living buffers.\r\n\r\nWhat do you think about it after all?\r\nHypothetically, it could be fixed if `PooledByteBuf` does more checks over the current state of `PoolThreadCache` during the release.\r\nE.g. it could return the memory directly to the arena if the cache is freed/freeing.\r\n","closed_by":{"login":"normanmaurer","id":439362,"node_id":"MDQ6VXNlcjQzOTM2Mg==","avatar_url":"https://avatars.githubusercontent.com/u/439362?v=4","gravatar_id":"","url":"https://api.github.com/users/normanmaurer","html_url":"https://github.com/normanmaurer","followers_url":"https://api.github.com/users/normanmaurer/followers","following_url":"https://api.github.com/users/normanmaurer/following{/other_user}","gists_url":"https://api.github.com/users/normanmaurer/gists{/gist_id}","starred_url":"https://api.github.com/users/normanmaurer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/normanmaurer/subscriptions","organizations_url":"https://api.github.com/users/normanmaurer/orgs","repos_url":"https://api.github.com/users/normanmaurer/repos","events_url":"https://api.github.com/users/normanmaurer/events{/privacy}","received_events_url":"https://api.github.com/users/normanmaurer/received_events","type":"User","user_view_type":"public","site_admin":false},"reactions":{"url":"https://api.github.com/repos/netty/netty/issues/12749/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/netty/netty/issues/12749/timeline","performed_via_github_app":null,"state_reason":"completed"}