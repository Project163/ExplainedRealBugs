{"url":"https://api.github.com/repos/akka/akka-core/issues/27259","repository_url":"https://api.github.com/repos/akka/akka-core","labels_url":"https://api.github.com/repos/akka/akka-core/issues/27259/labels{/name}","comments_url":"https://api.github.com/repos/akka/akka-core/issues/27259/comments","events_url":"https://api.github.com/repos/akka/akka-core/issues/27259/events","html_url":"https://github.com/akka/akka-core/issues/27259","id":464043981,"node_id":"MDU6SXNzdWU0NjQwNDM5ODE=","number":27259,"title":"Shard reallocation problems","user":{"login":"jroper","id":105833,"node_id":"MDQ6VXNlcjEwNTgzMw==","avatar_url":"https://avatars.githubusercontent.com/u/105833?v=4","gravatar_id":"","url":"https://api.github.com/users/jroper","html_url":"https://github.com/jroper","followers_url":"https://api.github.com/users/jroper/followers","following_url":"https://api.github.com/users/jroper/following{/other_user}","gists_url":"https://api.github.com/users/jroper/gists{/gist_id}","starred_url":"https://api.github.com/users/jroper/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jroper/subscriptions","organizations_url":"https://api.github.com/users/jroper/orgs","repos_url":"https://api.github.com/users/jroper/repos","events_url":"https://api.github.com/users/jroper/events{/privacy}","received_events_url":"https://api.github.com/users/jroper/received_events","type":"User","user_view_type":"public","site_admin":false},"labels":[{"id":92356828,"node_id":"MDU6TGFiZWw5MjM1NjgyOA==","url":"https://api.github.com/repos/akka/akka-core/labels/1%20-%20triaged","name":"1 - triaged","color":"5319e7","default":false,"description":"Tickets that are safe to pick up for contributing in terms of likeliness of being accepted"},{"id":1433205678,"node_id":"MDU6TGFiZWwxNDMzMjA1Njc4","url":"https://api.github.com/repos/akka/akka-core/labels/t:cluster-sharding","name":"t:cluster-sharding","color":"cccccc","default":false,"description":""}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":{"url":"https://api.github.com/repos/akka/akka-core/milestones/146","html_url":"https://github.com/akka/akka-core/milestone/146","labels_url":"https://api.github.com/repos/akka/akka-core/milestones/146/labels","id":4523881,"node_id":"MDk6TWlsZXN0b25lNDUyMzg4MQ==","number":146,"title":"2.6.0-M6","description":"","creator":{"login":"raboof","id":131856,"node_id":"MDQ6VXNlcjEzMTg1Ng==","avatar_url":"https://avatars.githubusercontent.com/u/131856?v=4","gravatar_id":"","url":"https://api.github.com/users/raboof","html_url":"https://github.com/raboof","followers_url":"https://api.github.com/users/raboof/followers","following_url":"https://api.github.com/users/raboof/following{/other_user}","gists_url":"https://api.github.com/users/raboof/gists{/gist_id}","starred_url":"https://api.github.com/users/raboof/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/raboof/subscriptions","organizations_url":"https://api.github.com/users/raboof/orgs","repos_url":"https://api.github.com/users/raboof/repos","events_url":"https://api.github.com/users/raboof/events{/privacy}","received_events_url":"https://api.github.com/users/raboof/received_events","type":"User","user_view_type":"public","site_admin":false},"open_issues":0,"closed_issues":20,"state":"closed","created_at":"2019-07-26T08:48:19Z","updated_at":"2019-08-16T12:15:08Z","due_on":null,"closed_at":"2019-08-16T12:15:08Z"},"comments":13,"created_at":"2019-07-04T03:35:56Z","updated_at":"2019-09-17T15:44:16Z","closed_at":"2019-08-15T14:36:03Z","author_association":"CONTRIBUTOR","type":null,"active_lock_reason":null,"sub_issues_summary":{"total":0,"completed":0,"percent_completed":0},"issue_dependencies_summary":{"blocked_by":0,"total_blocked_by":0,"blocking":0,"total_blocking":0},"body":"I see this issue a lot when scaling a cluster down. I think the problem happens when more than one node leaves the cluster at a time. I'll describe the symptom first, here's the logs:\r\n\r\n```\r\n02:32:41.620 DEBUG c.l.s.autoscaler.Autoscaler - Scaling down to 2 because desired nodes for user function 1 and desired nodes for request handling 1 is below cluster members 4\r\n02:32:44.901 INFO  a.c.Cluster(akka://statefulserverless-backend) - Cluster Node [akka.tcp://statefulserverless-backend@10.52.14.20:2552] - Exiting confirmed [akka.tcp://statefulserverless-backend@10.52.13.17:2552]\r\n02:32:45.010 INFO  akka.remote.EndpointWriter - AssociationError [akka.tcp://statefulserverless-backend@10.52.14.20:2552] <- [akka.tcp://statefulserverless-backend@10.52.13.17:2552]: Error [Shut down address: akka.tcp://statefulserverless-backend@10.52.13.17:2552] [\r\nakka.remote.ShutDownAssociation: Shut down address: akka.tcp://statefulserverless-backend@10.52.13.17:2552\r\nCaused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.\r\n]\r\n02:32:46.254 INFO  a.c.s.ClusterSingletonManager - Member removed [akka.tcp://statefulserverless-backend@10.52.13.17:2552]\r\n02:32:46.264 INFO  a.c.s.ClusterSingletonManager - Member removed [akka.tcp://statefulserverless-backend@10.52.13.17:2552]\r\n02:32:52.942 WARN  akka.cluster.sharding.ShardRegion - shopping-cart: Retry request for shard [94] homes from coordinator at [Actor[akka://statefulserverless-backend/system/sharding/shopping-cartCoordinator/singleton/coordinator#1273361376]]. [17] buffered messages.\r\n[snip - above message is repeated 18 times per second, differing only in the shard id and buffered messages]\r\n02:32:53.137 INFO  a.c.Cluster(akka://statefulserverless-backend) - Cluster Node [akka.tcp://statefulserverless-backend@10.52.14.20:2552] - Exiting confirmed [akka.tcp://statefulserverless-backend@10.52.12.18:2552]\r\n02:32:53.225 INFO  akka.remote.EndpointWriter - AssociationError [akka.tcp://statefulserverless-backend@10.52.14.20:2552] <- [akka.tcp://statefulserverless-backend@10.52.12.18:2552]: Error [Shut down address: akka.tcp://statefulserverless-backend@10.52.12.18:2552] [\r\nakka.remote.ShutDownAssociation: Shut down address: akka.tcp://statefulserverless-backend@10.52.12.18:2552\r\nCaused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.\r\n]\r\n02:32:53.243 INFO  a.c.Cluster(akka://statefulserverless-backend) - Cluster Node [akka.tcp://statefulserverless-backend@10.52.14.20:2552] - is the new leader among reachable nodes (more leaders may exist)\r\n02:32:54.242 INFO  a.c.Cluster(akka://statefulserverless-backend) - Cluster Node [akka.tcp://statefulserverless-backend@10.52.14.20:2552] - Leader is removing confirmed Exiting node [akka.tcp://statefulserverless-backend@10.52.12.18:2552]\r\n02:32:54.250 INFO  a.c.s.ClusterSingletonManager - Member removed [akka.tcp://statefulserverless-backend@10.52.12.18:2552]\r\n02:32:54.255 INFO  a.c.s.ClusterSingletonManager - Member removed [akka.tcp://statefulserverless-backend@10.52.12.18:2552]\r\n02:32:54.942 WARN  akka.cluster.sharding.ShardRegion - shopping-cart: Retry request for shard [94] homes from coordinator at [Actor[akka://statefulserverless-backend/system/sharding/shopping-cartCoordinator/singleton/coordinator#1273361376]]. [17] buffered messages.\r\n[snip - above message is repeated 18 times per second, differing only in the shard id and buffered messages]\r\n```\r\n\r\nThe 18 retry request warnings are then repeated every second for about 50 seconds, until the last one, 50 seconds after the first message was output:\r\n\r\n```\r\n02:33:42.932 WARN  akka.cluster.sharding.ShardRegion - shopping-cart: Retry request for shard [84] homes from coordinator at [Actor[akka://statefulserverless-backend/system/sharding/shopping-cartCoordinator/singleton/coordinator#1273361376]]. [35] buffered messages.\r\n```\r\n\r\nAnd then, everything goes back to normal, the shards start working again, and then 16 seconds after that:\r\n\r\n```\r\n02:33:58.432 WARN  a.r.transport.netty.NettyTransport - Remote connection to [null] failed with org.jboss.netty.channel.ConnectTimeoutException: connection timed out: /10.52.12.18:2552\r\n02:33:58.439 WARN  a.remote.ReliableDeliverySupervisor - Association with remote system [akka.tcp://statefulserverless-backend@10.52.12.18:2552] has failed, address is now gated for [5000] ms. Reason: [Association failed with [akka.tcp://statefulserverless-backend@10.52.12.18:2552]] Caused by: [org.jboss.netty.channel.ConnectTimeoutException: connection timed out: /10.52.12.18:2552]\r\n```\r\n\r\nSo, in the above, `10.52.13.17` shut down first, followed by `10.52.12.18`, but it appears that the node still attempts to communicate with `10.52.12.18` after its left the cluster, hence the last error message?\r\n\r\nI suspect what is happening is that during shard allocation after the first node leaves the cluster, shards can be allocated to the second node that is also concurrently leaving, and there is a race condition that means that if that node shuts down while the shards are being allocated to it, the shard coordinator does not attempt to fix that and reallocate for a minute.","closed_by":{"login":"patriknw","id":336161,"node_id":"MDQ6VXNlcjMzNjE2MQ==","avatar_url":"https://avatars.githubusercontent.com/u/336161?v=4","gravatar_id":"","url":"https://api.github.com/users/patriknw","html_url":"https://github.com/patriknw","followers_url":"https://api.github.com/users/patriknw/followers","following_url":"https://api.github.com/users/patriknw/following{/other_user}","gists_url":"https://api.github.com/users/patriknw/gists{/gist_id}","starred_url":"https://api.github.com/users/patriknw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/patriknw/subscriptions","organizations_url":"https://api.github.com/users/patriknw/orgs","repos_url":"https://api.github.com/users/patriknw/repos","events_url":"https://api.github.com/users/patriknw/events{/privacy}","received_events_url":"https://api.github.com/users/patriknw/received_events","type":"User","user_view_type":"public","site_admin":false},"reactions":{"url":"https://api.github.com/repos/akka/akka-core/issues/27259/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/akka/akka-core/issues/27259/timeline","performed_via_github_app":null,"state_reason":"completed"}