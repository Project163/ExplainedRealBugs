<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 09:03:04 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF Jira</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[ZOOKEEPER-3756] Members failing to rejoin quorum</title>
                <link>https://issues.apache.org/jira/browse/ZOOKEEPER-3756</link>
                <project id="12310801" key="ZOOKEEPER">ZooKeeper</project>
                    <description>&lt;p&gt;Not sure if this is the place to ask, please close if it&apos;s not.&lt;/p&gt;

&lt;p&gt;I am seeing some behavior that I can&apos;t explain since upgrading to 3.5:&lt;/p&gt;

&lt;p&gt;In a 5 member quorum, when server 3 is the leader and each server has this in their configuration:&#160;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
server.1=100.71.255.254:2888:3888:participant;2181
server.2=100.71.255.253:2888:3888:participant;2181
server.3=100.71.255.252:2888:3888:participant;2181
server.4=100.71.255.251:2888:3888:participant;2181
server.5=100.71.255.250:2888:3888:participant;2181&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;If servers 1 or 2 are restarted, they fail to rejoin the quorum with this in the logs:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2020-03-11 20:23:35,720 [myid:2] - INFO  [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):QuorumPeer@1175] - LOOKING
2020-03-11 20:23:35,721 [myid:2] - INFO  [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):FastLeaderElection@885] - New election. My id =  2, proposed zxid=0x1b8005f4bba
2020-03-11 20:23:35,733 [myid:2] - INFO  [WorkerSender[myid=2]:QuorumCnxManager@438] - Have smaller server identifier, so dropping the connection: (3, 2)
2020-03-11 20:23:35,734 [myid:2] - INFO  [0.0.0.0/0.0.0.0:3888:QuorumCnxManager$Listener@924] - Received connection request 100.126.116.201:36140
2020-03-11 20:23:35,735 [myid:2] - INFO  [WorkerSender[myid=2]:QuorumCnxManager@438] - Have smaller server identifier, so dropping the connection: (4, 2)
2020-03-11 20:23:35,740 [myid:2] - INFO  [WorkerSender[myid=2]:QuorumCnxManager@438] - Have smaller server identifier, so dropping the connection: (5, 2)
2020-03-11 20:23:35,740 [myid:2] - INFO  [0.0.0.0/0.0.0.0:3888:QuorumCnxManager$Listener@924] - Received connection request 100.126.116.201:36142
2020-03-11 20:23:35,740 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection@679] - Notification: 2 (message format version), 2 (n.leader), 0x1b8005f4bba (n.zxid), 0x1 (n.round), LOOKING (n.state), 2 (n.sid), 0x1b8 (n.peerEPoch), LOOKING (my state)0 (n.config version)
2020-03-11 20:23:35,742 [myid:2] - WARN  [SendWorker:3:QuorumCnxManager$SendWorker@1143] - Interrupted &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; waiting &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; message on queue
java.lang.InterruptedException
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2088)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.pollSendQueue(QuorumCnxManager.java:1294)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.access$700(QuorumCnxManager.java:82)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.run(QuorumCnxManager.java:1131)
2020-03-11 20:23:35,744 [myid:2] - WARN  [SendWorker:3:QuorumCnxManager$SendWorker@1153] - Send worker leaving thread  id 3 my id = 2
2020-03-11 20:23:35,745 [myid:2] - WARN  [RecvWorker:3:QuorumCnxManager$RecvWorker@1230] - Interrupting SendWorker&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The only way I can seem to get them to rejoin the quorum is to restart the leader.&lt;/p&gt;

&lt;p&gt;However, if I remove server 4 and 5 from the configuration of server 1 or 2 (so only servers 1, 2, and 3 remain in the configuration file), then they can rejoin the quorum fine. Is this expected and am I doing something wrong? Any help or explanation would be greatly appreciated. Thank you.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13291171">ZOOKEEPER-3756</key>
            <summary>Members failing to rejoin quorum</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="symat">Mate Szalay-Beko</assignee>
                                    <reporter username="dshi">Dai Shi</reporter>
                        <labels>
                            <label>pull-request-available</label>
                    </labels>
                <created>Wed, 11 Mar 2020 20:43:33 +0000</created>
                <updated>Wed, 20 May 2020 07:06:58 +0000</updated>
                            <resolved>Mon, 23 Mar 2020 15:20:55 +0000</resolved>
                                    <version>3.5.6</version>
                    <version>3.5.7</version>
                                    <fixVersion>3.6.1</fixVersion>
                    <fixVersion>3.5.8</fixVersion>
                                    <component>leaderElection</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                    <progress percentage="100">
                                    <originalProgress>
                                                    <row percentage="0" backgroundColor="#89afd7"/>
                                                    <row percentage="100" backgroundColor="transparent"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="100" backgroundColor="#51a825"/>
                                                    <row percentage="0" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </progress>
                                    <aggregateprogress percentage="100">
                                    <originalProgress>
                                                    <row percentage="0" backgroundColor="#89afd7"/>
                                                    <row percentage="100" backgroundColor="transparent"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="100" backgroundColor="#51a825"/>
                                                    <row percentage="0" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </aggregateprogress>
                                            <timeestimate seconds="0">0h</timeestimate>
                            <timespent seconds="12600">3.5h</timespent>
                                <comments>
                            <comment id="17058642" author="symat" created="Fri, 13 Mar 2020 11:17:16 +0000"  >&lt;p&gt;Hello!&lt;/p&gt;

&lt;p&gt;I was working on these parts recently and happy to take a look on your case.&lt;/p&gt;

&lt;p&gt;The log file you sent is more-or-less OK. In ZooKeeper, the servers are communicating with each other using the 3888 port (in your config) for election protocol. When a server starts, it tries to connect to all other server&apos;s election port and asks for the IDs from each server. But only those channels remain open, which were initiated by the servers with higher ID. We expect that the other servers will initiate connections later to the newly joined servers and the channels initiated by them will remain open. This is why you see the following message, which is completely normal:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2020-03-11 20:23:35,733 [myid:2] - INFO  [WorkerSender[myid=2]:QuorumCnxManager@438] - Have smaller server identifier, so dropping the connection: (3, 2)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;And also the SendWorker thread gets interrupted towards server 3 because of the same reason. That is also OK.&lt;/p&gt;


&lt;p&gt;The only strange thing I noticed in your logs are in these lines: &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2020-03-11 20:23:35,734 [myid:2] - INFO  [0.0.0.0/0.0.0.0:3888:QuorumCnxManager$Listener@924] - Received connection request 100.126.116.201:36140

2020-03-11 20:23:35,740 [myid:2] - INFO  [0.0.0.0/0.0.0.0:3888:QuorumCnxManager$Listener@924] - Received connection request 100.126.116.201:36142
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It looks like you get two connection requests from this IP: 100.126.116.201. Unfortunately the log doesn&apos;t tell the ID of this server. This IP is not in your config.&lt;/p&gt;

&lt;p&gt;Are you sure you are using the same config you sent in all ZK nodes? Can you check where this 100.126.116.201 comes from?&lt;/p&gt;

&lt;p&gt;There is a known bug which could explain your situation, but that would happen only if you use 0.0.0.0 in your configs.&lt;/p&gt;

&lt;p&gt;Are you using some dockerized environment maybe?&lt;/p&gt;

&lt;p&gt;Can you share the ZooKeeper configs and server logs from all the 5 nodes? &lt;/p&gt;</comment>
                            <comment id="17058718" author="dshi" created="Fri, 13 Mar 2020 13:16:41 +0000"  >&lt;p&gt;Hi Mate,&lt;/p&gt;

&lt;p&gt;Thanks for taking a look at this! I suspected it may have something to do with the IPs, so hoping you may be able to explain more clearly what might be happening &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;We are running zookeeper inside kubernetes as a stateful set. The IP&#160;100.126.116.201 is the actual IP of the pod that the leader, member 3, is running in. Unfortunately as far as I know you cannot set static IPs for pods in kubernetes, so that is why the IPs in the configs do not match the IP that zookeeper binds to since we cannot know in advance what the pod IP will be. We have&#160;quorumListenOnAllIPs=true in the config. The IPs in the config point to kubernetes services, which remain static, each of which select the single corresponding pod as the backend.&lt;/p&gt;

&lt;p&gt;Just for reference, prior to upgrading to 3.5 we were using 0.0.0.0 in our configs, but only in a 3 member cluster. As far as I can tell, there are no issues with member rejoining the quorum with either the 3.4 config or 3.5 config in a 3 member cluster. However, there is one major issue in both that if the leader pod is restarted, the cluster becomes hard down until the pod restarts. It seems that the leader election does not take place until the pod comes back and the zookeeper process starts, so even though the logs say leader election took 1ms, the cluster was down for maybe 30 to 60 seconds.&lt;/p&gt;

&lt;p&gt;This is what led to me trying a 5 member cluster to see if that would trigger the leader election quicker, but it led to me discovering this behavior instead. I still don&apos;t understand why if I remove servers 4 and 5 from the config of servers 1 and 2 they can then successfully rejoin.&lt;/p&gt;

&lt;p&gt;I&apos;ve since shifted the cluster back down to 3 members, but if you think it would be helpful to get a log from all 5 members I can repeat the experiment again.&lt;/p&gt;

&lt;p&gt;Thanks again for your help!&lt;/p&gt;</comment>
                            <comment id="17058800" author="symat" created="Fri, 13 Mar 2020 14:44:46 +0000"  >&lt;p&gt;It is strange indeed...&lt;/p&gt;

&lt;p&gt;&amp;gt; We have quorumListenOnAllIPs=true in the config.&lt;br/&gt;
I think it is the correct config for kubernetes using 3.5+, I would expect this to work. But it would be nice to see your full config.&lt;/p&gt;

&lt;p&gt;The 3.4 config (where you used 0.0.0.0) is expected to not work in 3.5, see &lt;a href=&quot;https://issues.apache.org/jira/browse/ZOOKEEPER-2164&quot; title=&quot;fast leader election keeps failing&quot; class=&quot;issue-link&quot; data-issue-key=&quot;ZOOKEEPER-2164&quot;&gt;&lt;del&gt;ZOOKEEPER-2164&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/ZOOKEEPER-3725&quot; title=&quot;Zookeeper fails to establish quorum with 2 servers using 3.5.6&quot; class=&quot;issue-link&quot; data-issue-key=&quot;ZOOKEEPER-3725&quot;&gt;&lt;del&gt;ZOOKEEPER-3725&lt;/del&gt;&lt;/a&gt; for details. We already have a fix for this, but it will be shipped only in the next releases in 3.5.8 and 3.6.1.&lt;/p&gt;

&lt;p&gt;If you use &lt;tt&gt;quorumListenOnAllIPs=true&lt;/tt&gt; and you don&apos;t use &lt;tt&gt;0.0.0.0&lt;/tt&gt; in the configs, then your problem is something else...&lt;/p&gt;

&lt;p&gt;Unfortunately I have not much experience with running ZooKeeper in Kubernetes. But I would be happy to reproduce your situation and take a deeper look inside the ZooKeeper leader election. If it is possible for you to share your pod descriptors / Dockerfiles then I would try them out with both 3 and 5 ZK servers. Also I would try the fix we committed for &lt;a href=&quot;https://issues.apache.org/jira/browse/ZOOKEEPER-2164&quot; title=&quot;fast leader election keeps failing&quot; class=&quot;issue-link&quot; data-issue-key=&quot;ZOOKEEPER-2164&quot;&gt;&lt;del&gt;ZOOKEEPER-2164&lt;/del&gt;&lt;/a&gt;, maybe that helps in this case too. (although this seem to me a different case)&lt;/p&gt;

&lt;p&gt;Do you think it is something that can be reproduced on a single host machine with minikube, or it is something you experienced only with a real kubernetes cluster?&lt;/p&gt;

&lt;p&gt;Alternatively you can share your experience on user@zookeeper.apache.org - I know a few guys on that mailing list who are operating ZooKeeper with Kubernetes, maybe they can share their zookeeper config / pod / docker files.&lt;/p&gt;</comment>
                            <comment id="17058962" author="dshi" created="Fri, 13 Mar 2020 18:00:00 +0000"  >&lt;p&gt;Here are the kubernetes files:&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12996689/12996689_zookeeper.yaml&quot; title=&quot;zookeeper.yaml attached to ZOOKEEPER-3756&quot;&gt;zookeeper.yaml&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12996690/12996690_configmap.yaml&quot; title=&quot;configmap.yaml attached to ZOOKEEPER-3756&quot;&gt;configmap.yaml&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The zookeeper yaml is slightly redacted to remove some more sensitive information, but it should only be cosmetic.&lt;/p&gt;

&lt;p&gt;Here are the docker image build files:&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12996684/12996684_Dockerfile&quot; title=&quot;Dockerfile attached to ZOOKEEPER-3756&quot;&gt;Dockerfile&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12996683/12996683_docker-entrypoint.sh&quot; title=&quot;docker-entrypoint.sh attached to ZOOKEEPER-3756&quot;&gt;docker-entrypoint.sh&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12996685/12996685_jmx.yaml&quot; title=&quot;jmx.yaml attached to ZOOKEEPER-3756&quot;&gt;jmx.yaml&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Note that while docker-entrypoint.sh tries to build the config file if it doesn&apos;t exist, I have since updated our kubernetes config to supply a configmap with the config file, so the entrypoint doesn&apos;t need to generate the config file anymore.&lt;/p&gt;

&lt;p&gt;My gut feeling says this should be able to be reproduced in minikube on a single machine, but I have not tried (you probably have to remove the pod anti-affinity). Please let me know if you have any questions regarding these files or our setup.&lt;/p&gt;</comment>
                            <comment id="17058986" author="dshi" created="Fri, 13 Mar 2020 18:34:09 +0000"  >&lt;p&gt;Ah I forgot one very important file:&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12996692/12996692_zoo-service.yaml&quot; title=&quot;zoo-service.yaml attached to ZOOKEEPER-3756&quot;&gt;zoo-service.yaml&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;And actually looking through this file I found out why the original issue was happening :facelpalm:&lt;/p&gt;

&lt;p&gt;I had a copy/paste error and forgot to update the selectors for&#160;zoo-internal-3 and&#160;zoo-internal-4 (the peer and leader election ports) when adding the 2 new members, so they were both still pointing to zoo-2. After fixing this error things now behave correctly as they do with a 3 member cluster. Sorry so much to waste your time!&lt;/p&gt;

&lt;p&gt;However, there is still the outstanding issue where restarting the leader pod causes the cluster to be down for around 30 seconds while it restarts. Is this expected? I was going to spin up a zookeeper cluster outside of kubernetes today just to confirm if it still experiences the same behavior. This is a big issue for running in kubernetes because pods and nodes need to be restarted relatively frequently due to kubernetes upgrades, and our services cannot tolerate a 30 second downtime to zookeeper. Since this is not the issue mentioned at the beginning of this issue, I&apos;m happy to close this and open a new issue if needed.&lt;/p&gt;</comment>
                            <comment id="17058988" author="symat" created="Fri, 13 Mar 2020 18:36:01 +0000"  >&lt;p&gt;Thanks for all the details! I will try to reproduce the issue on Monday and come back with questions / comments soon.&lt;/p&gt;</comment>
                            <comment id="17059014" author="symat" created="Fri, 13 Mar 2020 18:55:44 +0000"  >&lt;p&gt;&amp;gt; restarting the leader pod causes the cluster to be down for around 30 seconds while it restarts&lt;/p&gt;

&lt;p&gt;No, usually a full cluster recovery shouldn&apos;t take more than a few seconds. At least this is what I saw with normal clusters without Kubetnetes.&lt;/p&gt;

&lt;p&gt;It would be interesting to see where ZooKeeper spend the 30 seconds. Is the detection of the loosing of the old leader is slow? Or the new election is this slow for some reason?&lt;/p&gt;

&lt;p&gt;&amp;gt;  I&apos;m happy to close this and open a new issue if needed.&lt;/p&gt;

&lt;p&gt;I would prefer to keep this one open. Partly to minimize the administration &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; and also there are some discussion / attachment here that is valuable for the new issue too.&lt;/p&gt;</comment>
                            <comment id="17059026" author="dshi" created="Fri, 13 Mar 2020 19:32:09 +0000"  >&lt;p&gt;I repeated the experiment again though with just 3 members. Here are logs from each:&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12996698/12996698_zoo-0.log&quot; title=&quot;zoo-0.log attached to ZOOKEEPER-3756&quot;&gt;zoo-0.log&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;&lt;br/&gt;
 &lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12996699/12996699_zoo-1.log&quot; title=&quot;zoo-1.log attached to ZOOKEEPER-3756&quot;&gt;zoo-1.log&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;&lt;br/&gt;
 &lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12996700/12996700_zoo-2.log&quot; title=&quot;zoo-2.log attached to ZOOKEEPER-3756&quot;&gt;zoo-2.log&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Here zoo-2 (ID 3) was the leader the whole time. I did restart zoo-1 just to get a clean log, so you&apos;ll see in zoo-0&apos;s logs that zoo-1 was down for a little bit. Then I restarted zoo-2, which was the leader. I ran this on zoo-0 to measure the downtime from the client&apos;s perspective:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; :; &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; date; &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; echo srvr | nc localhost 2181 | grep -q Mode; then &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;; &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; echo DOWN; fi; sleep 0.05; done
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The cluster was from&#160;19:15:34 to&#160;19:16:06. This roughly matches the timestamps in the logs for these messages:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2020-03-13 19:15:34,945 [myid:2] - WARN  [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):Follower@96] - Exception when following the leader
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2020-03-13 19:16:05,247 [myid:2] - INFO [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):Follower@69] - FOLLOWING - LEADER ELECTION TOOK - 1 MS
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="17060051" author="symat" created="Mon, 16 Mar 2020 09:16:04 +0000"  >&lt;p&gt;Thanks, it&apos;s great that you were able to do this test and sent all the logs. I need a bit more time to dig into it, I hope I can analyze it deeper and come back with some answers (possibly questions? &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; ) today / tomorrow. &lt;/p&gt;</comment>
                            <comment id="17060200" author="symat" created="Mon, 16 Mar 2020 13:10:00 +0000"  >&lt;p&gt;OK, I have a theory... Maybe this is what happens:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;After shutting down the leader, the whole leader election restarts&lt;/li&gt;
	&lt;li&gt;ZooKeeper tries to open socket connection to the other ZooKeeper servers by using synchronized methods, so only one can run a time (see  on the master branch: &lt;a href=&quot;https://github.com/apache/zookeeper/blob/a5a4743733b8939464af82c1ee68a593fadbe362/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/QuorumCnxManager.java#L688&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/zookeeper/blob/a5a4743733b8939464af82c1ee68a593fadbe362/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/QuorumCnxManager.java#L688&lt;/a&gt; and &lt;a href=&quot;https://github.com/apache/zookeeper/blob/a5a4743733b8939464af82c1ee68a593fadbe362/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/QuorumCnxManager.java#L759&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/zookeeper/blob/a5a4743733b8939464af82c1ee68a593fadbe362/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/QuorumCnxManager.java#L759&lt;/a&gt;)&lt;/li&gt;
	&lt;li&gt;the default timeout is 5 secs (this is why there is nothing leader election related log message in your log files for 5 sec, until we hit the timeout of socket open to server 3)&lt;/li&gt;
	&lt;li&gt;by the time when the 5 sec timeout elapsed, the leader election protocol was also timeouted (but AFAIK it is increasing its internal timeout always? I will need to verify this)&lt;/li&gt;
	&lt;li&gt;after this happens a few time, either the leader election protocol timeout is increased enough to be able tolerate the 5 sec delay (and/or the fact that the server-3 restarted and the socket can be opened now) will cause that this block gets removed and everything goes smoothly after this. But it took 30 seconds, what is way too long...&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The question is, why the socket needs to timeout (wait for 5 sec) and why the connection doesn&apos;t get closed immediately with some &apos;host unreachable&apos; exception, what we would expect in case if the server goes down and no IP connection can be established. Usually we don&apos;t see this problem in production, so I guess it has to do something with Kubernetes networking.&lt;/p&gt;

&lt;p&gt;Still, this part needs to be refactored in ZooKeeper, we have to make the &lt;tt&gt;connectOne&lt;/tt&gt; asynchronous, what is not an easy task. Actually this is also something which was suggested in &lt;a href=&quot;https://issues.apache.org/jira/browse/ZOOKEEPER-2164&quot; title=&quot;fast leader election keeps failing&quot; class=&quot;issue-link&quot; data-issue-key=&quot;ZOOKEEPER-2164&quot;&gt;&lt;del&gt;ZOOKEEPER-2164&lt;/del&gt;&lt;/a&gt; (but in that ticket there were other errors fixed in the end). &lt;/p&gt;

&lt;p&gt;In the meanwhile there might be some workarounds:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;you can decrease the connection timeout to e.g. 500ms or 1000ms using the &lt;tt&gt;-Dzookeeper.cnxTimeout=500&apos;&lt;/tt&gt; system property. I am not sure if it will help, but I would be glad if you could test it&lt;/li&gt;
	&lt;li&gt;an other independent workaround would be using the multiAddress feature of ZooKeeper 3.6.0, enabling it by &lt;tt&gt;-Dzookeeper.multiAddress.enabled=true&lt;/tt&gt;. Then ZooKeeper should periodically check the availability of the currently used election addresses and kill the socket if the host is unavailable. This way we might kill the dead socket before the timeout happen. However, it might run ICMP traffic (ping) in the background, which I am not sure if will be reliable in kubernetes.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;No matter if the workarounds would fix the problem for you or not, I would suggest to keep this ticket open, and I will try to implement an asynchronous connection establishment somehow.&lt;/p&gt;</comment>
                            <comment id="17060440" author="dshi" created="Mon, 16 Mar 2020 19:17:51 +0000"  >&lt;p&gt;I think you are right that kubernetes networking is one of the main issues here. Because the server IPs in the zookeeper configs are pointing to kubernetes services, opening a TCP connection to those IPs when there are no backend endpoints (which is the case when a pod is deleted) will just hang.&lt;/p&gt;

&lt;p&gt;I tried running with &lt;tt&gt;-Dzookeeper.cnxTimeout=500&lt;/tt&gt; and now the cluster stays down for around 3 to 5 seconds when restarting the leader instead of more than 30 seconds. We may be able to tolerate this duration of downtime as a bandaid.&lt;/p&gt;

&lt;p&gt;I can try and build a 3.6.0 docker image and test the multiAddress feature as well. Is there anything I should pay attention to while upgrading to 3.6.0? Also is it possible to downgrade back to 3.5.7 afterwards?&lt;/p&gt;</comment>
                            <comment id="17060978" author="symat" created="Tue, 17 Mar 2020 15:00:48 +0000"  >&lt;blockquote&gt;&lt;p&gt;the server IPs in the zookeeper configs are pointing to kubernetes services, opening a TCP connection to those IPs when there are no backend endpoints (which is the case when a pod is deleted) will just hang.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;this explains the problem. And also explains why we don&apos;t necessary see this when using non-kubernetes deployments. I usually test with docker-compose (on a single host, defining virtual networks), but I haven&apos;t seen this issue there either.&lt;/p&gt;

&lt;p&gt;Anyway, I think this is actually a bug in the election protocol implementation, I should be able to fix this soon in the ZooKeeper code (hopefully in a few days, when I have a couple of hours to spare).&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Is there anything I should pay attention to while upgrading to 3.6.0?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Not much. 3.6.0 should be fully compatible with 3.5. AFAIK even the 3.5 clients can connect to 3.6.0 server without any problem. The 3.5 server configs should work with 3.6.0.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Also is it possible to downgrade back to 3.5.7 afterwards?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes, until you start to use any 3.6 specific features in the server or in the clients, you should be able to downgrade anytime. (the snapshot / transaction log formats are the same) But I actually never tested this, and also I don&apos;t think this would be supported officially.&lt;/p&gt;

&lt;p&gt;Although some large companies are already using 3.6 ZooKeeper for some time (they did use the master branch even before it was released officially), still using 3.6.0 in production I would consider a bit risky as this is the very first 3.6 release. But it depends on your use-case / how much you rely on ZooKeeper / how large traffic to put into it, etc.&lt;/p&gt;

&lt;p&gt;But rolling-upgrade or rolling-downgrade with &lt;tt&gt;multiAddress.enabled=true&lt;/tt&gt; would not work, as the multiAddress feature will cause backward-incompatible change in the ZooKeeper internal protocol. If you need continuous availability, then you have to first upgrade to 3.6.0 with a rolling upgrade, then enable multiAddress using a rolling restart. And similarly, for downgrade: first disable multiAddress with a rolling restart, then perform a rolling-downgrade.&lt;/p&gt;

&lt;p&gt;Anyway, if &lt;tt&gt;-Dzookeeper.cnxTimeout=500&lt;/tt&gt; is enough for you, then maybe it doesn&apos;t worth for you to put much more effort into testing the 3.6.0 workaround.&lt;/p&gt;

&lt;p&gt;If I do the fix, then it will be part of the next 3.5 and 3.6 releases.&lt;/p&gt;</comment>
                            <comment id="17061072" author="dshi" created="Tue, 17 Mar 2020 17:29:20 +0000"  >&lt;p&gt;OK, I think I may just stick with using the lower connection timeout on 3.5.7 for now. I&apos;ve been testing in our dev environment. It is too risky to try 3.6.0 in production for us anyway, as we only just moved to 3.5.X recently due to stability concerns on the earlier 3.5 releases.&lt;/p&gt;

&lt;p&gt;I&apos;m looking forward to testing out your fixes to the election protocols though, so will keep an eye out on your progress. Thanks so much for all your help!&lt;/p&gt;</comment>
                            <comment id="17061180" author="symat" created="Tue, 17 Mar 2020 20:48:09 +0000"  >&lt;p&gt;I created a fix, but I am not 100% sure it would solve the problem. I will try to come up with some automated test to catch this scenario (injecting timeout somehow into the connection creation), but in the meanwhile I share with you a patched version: &lt;a href=&quot;https://drive.google.com/open?id=1qcO-anHIPNn6ipD8nmNAiHmuQD7G4Nvf&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://drive.google.com/open?id=1qcO-anHIPNn6ipD8nmNAiHmuQD7G4Nvf&lt;/a&gt;&#160; (&lt;a href=&quot;https://github.com/symat/zookeeper/commit/d4159aca0d2aea0a544810d3d9b352c10ea55bdf&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/symat/zookeeper/commit/d4159aca0d2aea0a544810d3d9b352c10ea55bdf&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;This is based on the branch 3.5, I compiled it with OpenJDK 8u242 (I did it on Mac, but it shouldn&apos;t be a problem).&lt;/p&gt;

&lt;p&gt;How easy would be for you to test this version without using the &lt;tt&gt;-Dzookeeper.cnxTimeout=500&lt;/tt&gt; workaround?&lt;/p&gt;

&lt;p&gt;If it takes much time, then please wait until I verify the patch on my machine (either manually or with automated tests)...&lt;/p&gt;</comment>
                            <comment id="17061215" author="dshi" created="Tue, 17 Mar 2020 22:02:17 +0000"  >&lt;p&gt;I can try and test it in a little bit, just have to rejigger our Dockerfile to use this instead.&lt;/p&gt;</comment>
                            <comment id="17061248" author="dshi" created="Tue, 17 Mar 2020 23:14:53 +0000"  >&lt;p&gt;I&apos;m actually getting this error when trying to start the server:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Error: Could not find or load main &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.zookeeper.server.quorum.QuorumPeerMain&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Not sure if I need to change something with our deployment configs or not. All I changed in our Docker image build is to use the new tar.gz file I downloaded instead of downloading from the apache website.&lt;/p&gt;</comment>
                            <comment id="17061481" author="symat" created="Wed, 18 Mar 2020 08:02:23 +0000"  >&lt;p&gt;I am not familiar with Kubernetes enough to understand all the yaml files you sent &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;so I started with the plain docker part. I was able to build your dockerfile (modified a bit, removed the GPG stuff and adding the tar.gz file), then I was able to start a standalone ZooKeeper Servr&#160; container by:&#160;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
docker build -t zookeeper-3756 .
docker run --rm zookeeper-3756:latest
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;the modified Dockerfile:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
FROM ubuntu:16.04

# install jre
RUN apt-get update -y &amp;amp;&amp;amp; \
    apt-get upgrade -y &amp;amp;&amp;amp; \
    apt-get install -y &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;-jre gosu netcat-openbsd wget

ARG DISTRO_NAME=zookeeper-3.5.8-SNAPSHOT
ARG ARCHIVE_NAME=apache-$DISTRO_NAME-bin

ENV ZOO_USER=zookeeper \
    ZOO_CONF_DIR=/conf \
    ZOO_DATA_DIR=/data \
    ZOO_DATA_LOG_DIR=/datalog \
    ZOO_PORT=2181 \
    ZOO_TICK_TIME=2000 \
    ZOO_INIT_LIMIT=5 \
    ZOO_SYNC_LIMIT=2 \
    ZOO_AUTOPURGE_RETAIN_COUNT=50 \
    ZOO_AUTOPURGE_INTERVAL=6 \
    ZOO_LOG_DIR=/logs \
    JMX_CONF_DIR=/etc/jmx

COPY  apache-zookeeper-3.5.8-SNAPSHOT-bin.tar.gz /

# Add a user and make dirs
RUN set -x \
    &amp;amp;&amp;amp; useradd &lt;span class=&quot;code-quote&quot;&gt;&quot;$ZOO_USER&quot;&lt;/span&gt; \
    &amp;amp;&amp;amp; mkdir -p &lt;span class=&quot;code-quote&quot;&gt;&quot;$ZOO_DATA_LOG_DIR&quot;&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&quot;$ZOO_DATA_DIR&quot;&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&quot;$ZOO_CONF_DIR&quot;&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&quot;$ZOO_LOG_DIR&quot;&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&quot;$JMX_CONF_DIR&quot;&lt;/span&gt; \
    &amp;amp;&amp;amp; chown &lt;span class=&quot;code-quote&quot;&gt;&quot;$ZOO_USER:$ZOO_USER&quot;&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&quot;$ZOO_DATA_LOG_DIR&quot;&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&quot;$ZOO_DATA_DIR&quot;&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&quot;$ZOO_CONF_DIR&quot;&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&quot;$ZOO_LOG_DIR&quot;&lt;/span&gt;

# Download Apache Zookeeper, verify its PGP signature, untar and clean up
RUN set -x &amp;amp;&amp;amp; \
    cd / &amp;amp;&amp;amp; \
    tar -xzf &lt;span class=&quot;code-quote&quot;&gt;&quot;$ARCHIVE_NAME.tar.gz&quot;&lt;/span&gt; &amp;amp;&amp;amp; \
    mv &lt;span class=&quot;code-quote&quot;&gt;&quot;$ARCHIVE_NAME/conf/&quot;&lt;/span&gt;* &lt;span class=&quot;code-quote&quot;&gt;&quot;$ZOO_CONF_DIR&quot;&lt;/span&gt; &amp;amp;&amp;amp; \
    rm &lt;span class=&quot;code-quote&quot;&gt;&quot;$ARCHIVE_NAME.tar.gz&quot;&lt;/span&gt; &amp;amp;&amp;amp; \
    cd /$ARCHIVE_NAME &amp;amp;&amp;amp; \
    wget -q &lt;span class=&quot;code-quote&quot;&gt;&quot;https:&lt;span class=&quot;code-comment&quot;&gt;//repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.11.0/jmx_prometheus_javaagent-0.11.0.jar&quot;&lt;/span&gt;
&lt;/span&gt;

WORKDIR $ARCHIVE_NAME
VOLUME [&lt;span class=&quot;code-quote&quot;&gt;&quot;$ZOO_DATA_DIR&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;$ZOO_DATA_LOG_DIR&quot;&lt;/span&gt;]

EXPOSE $ZOO_PORT 2888 3888

ENV PATH=$PATH:/$ARCHIVE_NAME/bin \
    ZOOCFGDIR=$ZOO_CONF_DIR

COPY docker-entrypoint.sh /
COPY jmx.yaml /etc/jmx/config.yaml
ENTRYPOINT [&lt;span class=&quot;code-quote&quot;&gt;&quot;/docker-entrypoint.sh&quot;&lt;/span&gt;]
CMD [&lt;span class=&quot;code-quote&quot;&gt;&quot;zkServer.sh&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;start-foreground&quot;&lt;/span&gt;]

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;The fact that the standalone Zookeeper server started is a good sign &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
But I am not sure why you saw the&#160;&lt;tt&gt;&quot;Could not find or load main class&quot;&lt;/tt&gt;&#160; error.&lt;/p&gt;

&lt;p&gt;I will try to make a minimal Kubernetes setup where I can&#160;reproduce the problem with the connection timeout, using the original 3.5.7 version.&lt;/p&gt;</comment>
                            <comment id="17062083" author="dshi" created="Wed, 18 Mar 2020 21:41:04 +0000"  >&lt;p&gt;OK thanks, let me know if you need any clarifications on the config files I sent.&lt;/p&gt;</comment>
                            <comment id="17062621" author="symat" created="Thu, 19 Mar 2020 14:17:25 +0000"  >&lt;p&gt;I managed to reproduce the same behaviour / log messages you saw with some unit tests, so I consider this to be enough for reproduction / testing. I submitted a PR for the master &amp;amp; 3.6 branches. Once it got reviewed, I will also submit the fix on the branch 3.5.&lt;/p&gt;</comment>
                            <comment id="17062900" author="dshi" created="Thu, 19 Mar 2020 19:57:29 +0000"  >&lt;p&gt;OK, thank you!&lt;/p&gt;</comment>
                            <comment id="17064876" author="nkalmar" created="Mon, 23 Mar 2020 15:20:55 +0000"  >&lt;p&gt;Issue resolved by pull request 1289&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/zookeeper/pull/1289&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/zookeeper/pull/1289&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17065017" author="lasaro" created="Mon, 23 Mar 2020 18:55:24 +0000"  >&lt;p&gt;Dear all,&lt;/p&gt;

&lt;p&gt;currently, I am consistently facing the following scenario while running 3.5.5 and 3.5.7, which I believe is related to this bug:&lt;/p&gt;

&lt;p&gt;3 nodes up.&lt;br/&gt;
 Node 3 stop -&amp;gt;&#160;node 2 is elected; node 1 follows.&lt;br/&gt;
 Node 3 start -&amp;gt; node 3 elected the leader; node 2 follows; node 1 is unable to elect.&lt;/p&gt;

&lt;p&gt;Node 1 stop and start -&amp;gt; node 1 rejoins the quorum.&lt;br/&gt;
 Node 2 stop and start -&amp;gt; node 2 is unable to elect.&lt;br/&gt;
 Node 1 stop and start -&amp;gt; node 2 joins the quorum; node 1 joins the quorum&lt;br/&gt;
 Node 2 stop and start -&amp;gt; node 2 unable to join the quorum&lt;br/&gt;
 Node 3 stop and start -&amp;gt; node 3 elected the leader; node 2 follows; node 1 is unable to elect.&lt;/p&gt;

&lt;p&gt;Reducing the cnxTimeout value didn&apos;t change the behavior.&lt;/p&gt;

&lt;p&gt;I tested with this fix and now it is now worse; after a round of restarts, there doesn&apos;t seem to have anything I can to make node 1 join finish the election.&lt;/p&gt;

&lt;p&gt;This is such a nasty problem that I am wondering if there is something else to it. Maybe my configuration. Could you point me what would be useful in terms of information in order to debug this better? Full logs?&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17065026" author="nkalmar" created="Mon, 23 Mar 2020 19:14:37 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lasaro&quot; class=&quot;user-hover&quot; rel=&quot;lasaro&quot;&gt;lasaro&lt;/a&gt; are you sure it&apos;s the same problem? Why does node3 get elected as Leader after a restart, when node2 already taken over? That seems strange to me. A log would be helpful.&lt;br/&gt;
I would suggest you write to user@zookeeper.apache.org with more details and logs. &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=symat&quot; class=&quot;user-hover&quot; rel=&quot;symat&quot;&gt;symat&lt;/a&gt; did the fix, perhaps he can also chip in, as he has more insight on this one.&lt;/p&gt;</comment>
                            <comment id="17065412" author="symat" created="Tue, 24 Mar 2020 08:06:24 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lasaro&quot; class=&quot;user-hover&quot; rel=&quot;lasaro&quot;&gt;lasaro&lt;/a&gt; this is a strange issue indeed. It seems to be a bit different from this particular Jira. You can try the &lt;tt&gt;-Dzookeeper.cnxTimeout=500&lt;/tt&gt; workaround (it helped in the case we found with this Jira).&lt;/p&gt;

&lt;p&gt;If it doesn&apos;t help, then I recommend to open an other Jira which I will be happy to assign to myself. Full logs and config files would help a lot for sure. Also the ZooKeeper and the Java version is important. And we had several docker / kubernetes specific issues lately, so it is also important to know if you are facing with this problem in Docker / Kubernetes.&lt;/p&gt;

&lt;p&gt;This thread can be also interesting for you, especially if you use 0.0.0.0 in your configs: &lt;a href=&quot;https://mail-archives.apache.org/mod_mbox/zookeeper-user/202003.mbox/%3CCAAMoRKLDVeL0jfEJndyP3pnVVxztthqZ35d_UM%2Bhurx7%3DqO_PQ%40mail.gmail.com%3E&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://mail-archives.apache.org/mod_mbox/zookeeper-user/202003.mbox/%3CCAAMoRKLDVeL0jfEJndyP3pnVVxztthqZ35d_UM%2Bhurx7%3DqO_PQ%40mail.gmail.com%3E&lt;/a&gt;&lt;/p&gt;
</comment>
                            <comment id="17066127" author="lasaro" created="Tue, 24 Mar 2020 19:16:56 +0000"  >&lt;p&gt;Thanks for the feedback. I&apos;ve opened&#160;&lt;a href=&quot;https://issues.apache.org/jira/browse/ZOOKEEPER-3769&quot; title=&quot;fast leader election does not end if leader is taken down&quot; class=&quot;issue-link&quot; data-issue-key=&quot;ZOOKEEPER-3769&quot;&gt;&lt;del&gt;ZOOKEEPER-3769&lt;/del&gt;&lt;/a&gt;&#160;with a slightly different scenario but problematic in the same sense. To give the complete answer, I am not using 0.0.0.0 addresses (not explicitly, at least) and not using containers.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=symat&quot; class=&quot;user-hover&quot; rel=&quot;symat&quot;&gt;symat&lt;/a&gt;, I appreciate your willingness to look into it. It&apos;s been troubling me for some time.&lt;/p&gt;</comment>
                            <comment id="17091945" author="dshi" created="Fri, 24 Apr 2020 22:30:07 +0000"  >&lt;p&gt;Hi Mate,&lt;/p&gt;

&lt;p&gt;Is there an estimated release time for 3.5.8? I&apos;m hoping to try out the fix soon, &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="17093144" author="symat" created="Mon, 27 Apr 2020 07:56:58 +0000"  >&lt;p&gt;I think I will drive the 3.5.8 release. I hope I will have time to make the first release candidate on this week, or early next week. I will send out the official announcement on the dev mailing list. I would appreciate if you can test it. &lt;br/&gt;
Are you on the dev zookeeper list? Or should I forward the RC to you?&lt;br/&gt;
&lt;a href=&quot;https://zookeeper.apache.org/lists.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://zookeeper.apache.org/lists.html&lt;/a&gt; &lt;/p&gt;</comment>
                            <comment id="17093880" author="dshi" created="Mon, 27 Apr 2020 19:48:15 +0000"  >&lt;p&gt;I will definitely test it once it&apos;s available. I will join the dev zookeeper list to keep an eye on it.&lt;/p&gt;</comment>
                            <comment id="17104583" author="symat" created="Mon, 11 May 2020 15:41:16 +0000"  >&lt;p&gt;closed after releasing 3.5.8&lt;/p&gt;</comment>
                            <comment id="17111589" author="dshi" created="Tue, 19 May 2020 22:27:26 +0000"  >&lt;p&gt;Hi Mate,&lt;/p&gt;

&lt;p&gt;I just wanted to report back after testing 3.5.8. I am happy to say that it seems to work well after a brief bit of testing. I am no longer setting &lt;tt&gt;-Dzookeeper.cnxTimeout=500&lt;/tt&gt;, and now when I roll the leader the cluster downtime is only 2-3 seconds instead of 30+ seconds.&lt;/p&gt;

&lt;p&gt;Thanks again for helping me debug and creating this fix!&lt;/p&gt;</comment>
                            <comment id="17111842" author="symat" created="Wed, 20 May 2020 07:06:58 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dshi&quot; class=&quot;user-hover&quot; rel=&quot;dshi&quot;&gt;dshi&lt;/a&gt; for the tests and the feedback, it is great that the fix solved your problem (and not an other problem &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/tongue.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; )!&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="13305944">ZOOKEEPER-3838</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12820740">ZOOKEEPER-2164</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12996684" name="Dockerfile" size="1933" author="dshi" created="Fri, 13 Mar 2020 17:54:33 +0000"/>
                            <attachment id="12996690" name="configmap.yaml" size="1628" author="dshi" created="Fri, 13 Mar 2020 18:15:12 +0000"/>
                            <attachment id="12996683" name="docker-entrypoint.sh" size="1082" author="dshi" created="Fri, 13 Mar 2020 17:54:33 +0000"/>
                            <attachment id="12996685" name="jmx.yaml" size="760" author="dshi" created="Fri, 13 Mar 2020 17:54:33 +0000"/>
                            <attachment id="12996698" name="zoo-0.log" size="215553" author="dshi" created="Fri, 13 Mar 2020 19:23:19 +0000"/>
                            <attachment id="12996699" name="zoo-1.log" size="124258" author="dshi" created="Fri, 13 Mar 2020 19:23:19 +0000"/>
                            <attachment id="12996700" name="zoo-2.log" size="39875" author="dshi" created="Fri, 13 Mar 2020 19:23:18 +0000"/>
                            <attachment id="12996692" name="zoo-service.yaml" size="2990" author="dshi" created="Fri, 13 Mar 2020 18:27:30 +0000"/>
                            <attachment id="12996689" name="zookeeper.yaml" size="2125" author="dshi" created="Fri, 13 Mar 2020 18:15:12 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            5 years, 26 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z0cfrc:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>