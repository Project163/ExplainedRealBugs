<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Sat Nov 08 18:13:53 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[HBASE-27947] RegionServer OOM under load when TLS is enabled</title>
                <link>https://issues.apache.org/jira/browse/HBASE-27947</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;We are rolling out the server side TLS settings to all of our QA clusters. This has mostly gone fine, except on 1 cluster.&#160;Most clusters, including this one have a sampled &lt;tt&gt;nettyDirectMemory&lt;/tt&gt; usage of about 30-100mb. This cluster tends to get bursts of traffic, in which case it would typically jump to 400-500mb. Again this is sampled, so it could have been higher than that. When we enabled SSL on this cluster, we started seeing bursts up to at least 4gb. This exceeded our &lt;tt&gt;-XX:MaxDirectMemorySize&lt;/tt&gt;, which caused OOM&apos;s and general chaos on the cluster.&lt;br/&gt;
&#160;&lt;br/&gt;
We&apos;ve gotten it under control a little bit by setting &lt;tt&gt;-Dorg.apache.hbase.thirdparty.io.netty.maxDirectMemory&lt;/tt&gt; and &lt;tt&gt;-Dorg.apache.hbase.thirdparty.io.netty.tryReflectionSetAccessible&lt;/tt&gt;. We&apos;ve set netty&apos;s maxDirectMemory to be approx equal to (&lt;tt&gt;-XX:MaxDirectMemorySize - BucketCacheSize - ReservoirSize&lt;/tt&gt;). Now we are seeing netty&apos;s own OutOfDirectMemoryError, which is still causing pain for clients but at least insulates the other components of the regionserver.&lt;br/&gt;
&#160;&lt;br/&gt;
We&apos;re still digging into exactly why this is happening. The cluster clearly has a bad access pattern, but it doesn&apos;t seem like SSL should increase the memory footprint by 5-10x like we&apos;re seeing.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13541080">HBASE-27947</key>
            <summary>RegionServer OOM under load when TLS is enabled</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="bbeaudreault">Bryan Beaudreault</assignee>
                                    <reporter username="bbeaudreault">Bryan Beaudreault</reporter>
                        <labels>
                    </labels>
                <created>Thu, 22 Jun 2023 18:45:29 +0000</created>
                <updated>Sat, 19 Aug 2023 19:53:04 +0000</updated>
                            <resolved>Fri, 18 Aug 2023 14:21:58 +0000</resolved>
                                    <version>2.6.0</version>
                                    <fixVersion>2.6.0</fixVersion>
                    <fixVersion>3.0.0-beta-1</fixVersion>
                                    <component>rpc</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>8</watches>
                                                                                                                <comments>
                            <comment id="17736251" author="bbeaudreault" created="Thu, 22 Jun 2023 18:56:15 +0000"  >&lt;p&gt;The cluster in question has a bunch of large (1mb+) rows, and there seems to be a periodic job which causes large batches of multigets. The client in that job has rpc timeout of 60s and operation timeout of 120s. We have our server-side max result size set to 50mb, and have 80 handlers. During these spikes, we see server side latencies and queue times elevated into the seconds. We also see a bunch of concurrent requests which end up reaching our max result size.&lt;/p&gt;

&lt;p&gt;A worst case of 50mb * 80 is greater than our netty maxDirectMemory, so could theoretically be the problem. I tried lowering the max result size to 25mb and still saw OutOfDirectMemoryErrors. It&apos;s also telling that we previously never breached 1gb of direct memory for netty, and now going over 4gb. There must be some new SSL-related allocations inflating the pool, and haven&apos;t figured out yet how to tune.&lt;/p&gt;</comment>
                            <comment id="17736340" author="apache9" created="Fri, 23 Jun 2023 02:46:41 +0000"  >&lt;p&gt;Netty has a way to guess the input message size so it could allocate a ByteBuf which is larger enough.&lt;/p&gt;

&lt;p&gt;You can see the code around RecvByteBufAllocator.&lt;/p&gt;

&lt;p&gt;IIRC in spark they used to observer a problem that, when receiving large messages, and if the network is not so good, netty will waste a lot spaces, as the ByteBuf is large but it can only read a small amount of data, and if you use COMPOSITE_CUMULATOR instead of MERGE_CUMULATOR. things will get worse as you cache all the ByteBufs...&lt;/p&gt;

&lt;p&gt;SslHandler just extends ByteToMessageDecoder, by default it uses MERGE_CUMULATOR, but for SslHandler, different SSLEngine will lead to different cumulator implementation. You can see the code around SslEngineType for more details.&lt;/p&gt;

&lt;p&gt;Receiving large messages is always challenging, especially if you want to control the memory usage...&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;</comment>
                            <comment id="17736413" author="apache9" created="Fri, 23 Jun 2023 08:54:17 +0000"  >&lt;p&gt;And for max result size, IIRC we have our own memory pool implementation, so the returned cells are not allocated by netty, they are managed by ourselves. So lowering the max result size does not affect netty&apos;s direct memory usage.&lt;/p&gt;</comment>
                            <comment id="17736419" author="apache9" created="Fri, 23 Jun 2023 09:05:48 +0000"  >&lt;p&gt;So what is ssl engine you guys use? The jdk one or the openssl one? For the jdk one, it just uses MERGE_CUMULATOR which should have less problem on memory fragmentation.&lt;/p&gt;</comment>
                            <comment id="17736435" author="apache9" created="Fri, 23 Jun 2023 09:51:45 +0000"  >&lt;p&gt;Oh, seems I misunderstood the description. I thought there are large writes (1mb+ rows) periodically to the cluster...&lt;/p&gt;

&lt;p&gt;If you can see that the memory spike is always at the same time with the multiget request spike, then probably the problem is not about receiving and cumulating.&lt;/p&gt;

&lt;p&gt;When writing data back, the SslHandler needs to split the data to 16KB chunks, so for a single connection, it is not likely to have a out buffer larger than 16KB, of course if this slows down the processing, we may buffer a lot of cells in memory, but these cells are in our own memory pool, not netty&apos;s, so if you can see netty&apos;s OOME,  there must be other problems.&lt;/p&gt;

&lt;p&gt;And the max memory is not 50MB * 80, the rpc handler will not block on sending data back, as we use non blocking socket here. The handler will begin to process other requests right after adding the ByteBuf to a connection&apos;s output buffer. So the max memory is more likely related to the connections, not rpc handlers.&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;</comment>
                            <comment id="17736456" author="bbeaudreault" created="Fri, 23 Jun 2023 10:54:40 +0000"  >&lt;p&gt;Thanks for all of those useful insights! I will try to dig into those today. To clarify a few things:&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Yes, we are mostly seeing the issue with reads. I haven&#8217;t tested writes in isolation, but will try that later.&#160;&lt;/li&gt;
	&lt;li&gt;We have tried both tcnative (OpenSSL) and jdk default (jdk17), both exhibited the issue. I think tcnative helped but did not eliminate the issue.&#160;&lt;/li&gt;
	&lt;li&gt;For the last day I&#8217;ve been working on reproducing this with a synthetic load test. I&#8217;ve been able to do it, even with max result size set to 1mb. So this confirms what you said. However, my synthetic test only uses about &amp;lt; 20 connections per server. So there must be something that is using bigger buffers or something beyond connections.&lt;/li&gt;
	&lt;li&gt;The OOM is always when netty is trying to allocate a new 4mb buffer in the PoolArena. Not sure if that&#8217;s obvious or tells us something. I will try to attach a stacktrace in a couple hours.&#160;&lt;/li&gt;
&lt;/ol&gt;
</comment>
                            <comment id="17736481" author="bbeaudreault" created="Fri, 23 Jun 2023 12:30:25 +0000"  >&lt;p&gt;Here&apos;s an example stacktrace:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
org.apache.hbase.thirdparty.io.netty.util.internal.OutOfDirectMemoryError: failed to allocate 4194304 &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt;(s) of direct memory (used: 3070248230, max: 3073741824)
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent.incrementMemoryCounter(PlatformDependent.java:845) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent.allocateDirectNoCleaner(PlatformDependent.java:774) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.buffer.PoolArena$DirectArena.allocateDirect(PoolArena.java:701) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.buffer.PoolArena$DirectArena.newChunk(PoolArena.java:676) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.buffer.PoolArena.allocateNormal(PoolArena.java:215) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.buffer.PoolArena.tcacheAllocateNormal(PoolArena.java:197) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.buffer.PoolArena.allocate(PoolArena.java:139) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.buffer.PoolArena.allocate(PoolArena.java:129) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.buffer.PooledByteBufAllocator.newDirectBuffer(PooledByteBufAllocator.java:396) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:188) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:179) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.channel.unix.PreferredDirectByteBufAllocator.ioBuffer(PreferredDirectByteBufAllocator.java:53) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.channel.DefaultMaxMessagesRecvByteBufAllocator$MaxMessageHandle.allocate(DefaultMaxMessagesRecvByteBufAllocator.java:120) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.channel.epoll.EpollRecvByteAllocatorHandle.allocate(EpollRecvByteAllocatorHandle.java:75) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:785) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:499) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:397) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:833) ~[?:?] &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="17736535" author="apache9" created="Fri, 23 Jun 2023 15:47:33 +0000"  >&lt;p&gt;Oh, this is for receiving message...&lt;/p&gt;</comment>
                            <comment id="17736542" author="bbeaudreault" created="Fri, 23 Jun 2023 16:05:47 +0000"  >&lt;p&gt;Yea, to be honest I&apos;m having a hard time tracing where the memory comes and goes. I also noticed that in the stacktrace, but it didn&apos;t make sense to me. All of the OOM&apos;s seem to originate there, but the predominant access pattern here is &lt;em&gt;sending&lt;/em&gt; data back to client. For example, the bytesSent vs bytesReceived on the RegionServer is 150MB/s sent vs 100KB/s received.&#160;&lt;/p&gt;

&lt;p&gt;All I can say for certain is that in my synthetic tests which reproduces this, the workload I am sending is 100% reads.&#160;&lt;/p&gt;</comment>
                            <comment id="17736785" author="bbeaudreault" created="Sat, 24 Jun 2023 20:01:06 +0000"  >&lt;p&gt;I&apos;m wondering if the reason the OOM&apos;s appear to come from receiving is just because we get to a point where we&apos;re saturating the pool with writes. As you said, it&apos;s async so we could theoretically keep receiving requests but we can&apos;t receive anymore because there is no room left in the pool for receiving so we oom.&lt;/p&gt;

&lt;p&gt;I took a heap dump when nettyDirectMemory was well over 2gb. In this case there are 150k PooledUnsafeDirectByteBufs, and they are almost all related to ChannelOutboundBuffer.&lt;/p&gt;</comment>
                            <comment id="17736796" author="bbeaudreault" created="Sat, 24 Jun 2023 21:31:14 +0000"  >&lt;p&gt;Can we use WRITE_BUFFER_WATER_MARK to trigger channel writability, and then use &lt;tt&gt;channelWritabilityChanged to trigger disabling autoRead on channel? this way we can apply backpressure to clients instead of failing.&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;the question still remains why the buffer is filling up so much more when ssl enabled. Some performance hit is expected, but this much?&lt;/p&gt;</comment>
                            <comment id="17736889" author="apache9" created="Sun, 25 Jun 2023 15:47:58 +0000"  >&lt;p&gt;First, tt is a bit strange that why we need to allocate 4MB receive buffer, if as you said, you just do not send large requests to the server, the large messages are for returning to client.&lt;/p&gt;

&lt;p&gt;But anyway, if you have a strong evidence that most DirectByteBufs are almost related to ChannelOutboundBuffer, then I think we should try to see if we can address this problem first.&lt;/p&gt;

&lt;p&gt;So what are the sizes for these output ByteBufs? Are they all with a size round 16KB? Or we have some very large ByteBufs, like several MB?&lt;/p&gt;
</comment>
                            <comment id="17736890" author="apache9" created="Sun, 25 Jun 2023 15:52:43 +0000"  >&lt;p&gt;OK, 2GB / 150k, average message size is about 13-14KB.&lt;/p&gt;

&lt;p&gt;So the problem is that, we buffered too many SSL messages waiting to write back to client... Should have some bottleneck here? And it can also explain why using openssl could help here, because openssl have better performance than jdk&apos;s ssl implementation, so we could write back faster and reduce the possibility of OOM, but still can not fully eliminate.&lt;/p&gt;

&lt;p&gt;What is the CPU usage of your region server?&lt;/p&gt;</comment>
                            <comment id="17736922" author="bbeaudreault" created="Sun, 25 Jun 2023 20:28:06 +0000"  >&lt;p&gt;Yes at this point I feel pretty confident that most of the memory is held up in the ChannelOutboundBuffer. For example, a few of the buffers each have totalPendingBytes &amp;gt; 600MB. It seems to add up to what I&apos;m seeing in nettyDirectMemory.&lt;/p&gt;

&lt;p&gt;During the OOMs, CPU is near 100%. So yes, I realize this is an extreme example. But the key thing is how HBase handles it. An OOM is not great, backpressure would be better.&lt;/p&gt;

&lt;p&gt;In my test rig I can flip a switch to run the same workload through a regionserver-local haproxy for ssl termination or through hbase&apos;s netty TLS. The haproxy is able to get more req/s with fewer failures, almost 2x the throughput. I suppose we can expect haproxy to perform better, but that seems extreme. I think part of the reason might be due to the OOM&apos;s.&lt;/p&gt;</comment>
                            <comment id="17737036" author="apache9" created="Mon, 26 Jun 2023 08:16:57 +0000"  >&lt;p&gt;If a local haproxy can perform better, I think there are still rooms to increase the performance of our TLS implementation, maybe we need to tune some parameters of netty. But anyway, it does cost more CPUs so we should still try to implement some types of flow controls to prevent OOM.&lt;/p&gt;

&lt;p&gt;Agree that we could try to implement back pressure here. Will try to provide a PR soon.&lt;/p&gt;</comment>
                            <comment id="17737348" author="bbeaudreault" created="Mon, 26 Jun 2023 21:17:25 +0000"  >&lt;p&gt;FYI I swapped to using netty-tcnative with BoringSSL, and still seeing lots of OOMs but the stacktrace has changed. It now appears to mostly be originating in the writeAndFlush callpath. Example:&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
org.apache.hbase.thirdparty.io.netty.util.internal.OutOfDirectMemoryError: failed to allocate 4194304 &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt;(s) of direct memory (used: 3070234510, max: 3073741824)
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent.incrementMemoryCounter(PlatformDependent.java:845) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent.allocateDirectNoCleaner(PlatformDependent.java:774) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.buffer.PoolArena$DirectArena.allocateDirect(PoolArena.java:701) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.buffer.PoolArena$DirectArena.newChunk(PoolArena.java:676) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.buffer.PoolArena.allocateNormal(PoolArena.java:215) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.buffer.PoolArena.tcacheAllocateSmall(PoolArena.java:180) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.buffer.PoolArena.allocate(PoolArena.java:137) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.buffer.PoolArena.allocate(PoolArena.java:129) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.buffer.PooledByteBufAllocator.newDirectBuffer(PooledByteBufAllocator.java:396) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:188) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:179) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.handler.ssl.SslHandler$SslEngineType$1.allocateWrapBuffer(SslHandler.java:232) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.handler.ssl.SslHandler.allocateOutNetBuf(SslHandler.java:2266) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.handler.ssl.SslHandler.wrap(SslHandler.java:825) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.handler.ssl.SslHandler.wrapAndFlush(SslHandler.java:799) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.handler.ssl.SslHandler.flush(SslHandler.java:780) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeFlush0(AbstractChannelHandlerContext.java:925) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:907) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:893) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.channel.ChannelOutboundHandlerAdapter.flush(ChannelOutboundHandlerAdapter.java:125) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeFlush0(AbstractChannelHandlerContext.java:925) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:941) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1247) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:403) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at org.apache.hbase.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[hbase-shaded-netty-4.1.4.jar:?]
&#160; &#160; &#160; &#160; at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:833) ~[?:?] &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="17737717" author="bbeaudreault" created="Tue, 27 Jun 2023 15:15:02 +0000"  >&lt;p&gt;You probably already expected this, but I tried the simplistic &quot;turn off autoRead when !isWritable&quot; and it did nothing. We instead (or also) need to apply backpressure to the handler threads which are writing responses into the channel.&lt;/p&gt;</comment>
                            <comment id="17738150" author="bbeaudreault" created="Wed, 28 Jun 2023 14:31:39 +0000"  >&lt;p&gt;I implemented a pretty rudimentary POC for applying backpressure. Basically added a channelWritabilityChanged handler in NettyRpcFrameDecoder. When encountered, it sets an AtomicBoolean on the NettyServerRpcConnection and calls &lt;tt&gt;notifyAll()&lt;/tt&gt; if writable is true. Then in NettyServerCall.sendResponseIfReady, we check if the channel is writable and if not, we &lt;tt&gt;wait()&lt;/tt&gt; and poll the AtomicBoolean until it is true. sendResponseIfReady is called from a number of places, including within the event loop. We don&apos;t want to block the event loop, so I added a &lt;tt&gt;boolean canBlock&lt;/tt&gt; argument to the method, which is only true when called from CallRunner.run().&lt;/p&gt;

&lt;p&gt;This has resolved the OOMs for my test case, and the total throughput achievable by the test is now much more similar to when run through haproxy. Currently seeing about 5% throughput reduction compared to haproxy, but I&apos;m also not yet sure if that could be due to variance/noise since the test case is so extreme. This is with tcnative/boringSSL.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17738165" author="apache9" created="Wed, 28 Jun 2023 15:01:59 +0000"  >&lt;p&gt;Good. Please open a PR and I will help polishing the patch. I think in general the approach is good. The only concern is about blocking the rpc handler, in fact, even if you block the handler, the output buffer is still there...&lt;/p&gt;

&lt;p&gt;I was on a mobile device and tried many times this morning but could not post the comment here... This is what I want to post this morning&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;You can not stop handler from writing responses back, the only way is to close the connection, which means we will just drop the response.&lt;/p&gt;

&lt;p&gt;If it does not work, please try lowering the buffer size or the water mark threshold?&lt;/p&gt;&lt;/blockquote&gt;</comment>
                            <comment id="17738186" author="bbeaudreault" created="Wed, 28 Jun 2023 15:53:29 +0000"  >&lt;p&gt;Thanks, will do. Can you elaborate on this part?&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;even if you block the handler, the output buffer is still there...&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The blocking occurs prior to calling &lt;tt&gt;channel.writeAndFlush(this)&lt;/tt&gt;. I realize that the handler is still holding resources for the response in our own hbase ByteBuffAllocator. This shouldn&apos;t be a huge problem since the handler is not able to do more work while holding it, since it&apos;s blocked. The big issue is that the handler is currently able to produce a large buffer, and then immediately start doing work which might create other large buffers. So blocking stops that at the expense of response times.&lt;/p&gt;

&lt;p&gt;The ChannelOutputBuffer is still there and still has contents which netty is draining into the socket. Blocking prior to calling writeAndFlush gives netty time to drain the ChannelOutputBuffer before accepting more.&lt;/p&gt;

&lt;p&gt;To me this is sort of similar to the natural handler backpressure that occurs when, for example, the disk is slow. When disk is fast, a handler might be active for a few millis or less per-request. When disk is saturated, it might be active for a lot longer per request. The handler is blocked during that time, but is currently never blocked when the output socket is saturated.&lt;/p&gt;</comment>
                            <comment id="17738235" author="bbeaudreault" created="Wed, 28 Jun 2023 18:20:27 +0000"  >&lt;p&gt;I suppose one problem with blocking the RPC handler is a single slow client (whose channel gets very backed up) could affect all clients, if a majority of handlers end up getting stuck in this state. I will have to think about how to work around this, but also open to suggestions. One obvious possiblity would be to limit the time it&apos;s allowed to block for, after which it closes the channel as you said.&#160;&lt;/p&gt;

&lt;p&gt;I think we need to find a balance &#8211; simply closing the connection whenever there&apos;s a backlog will be very detrimental to throughput, since closing the connection will cause the client to have to re-establish and retry everything. On the other hand, we can&apos;t allow a noisy neighbor to hold all the handlers.&lt;/p&gt;</comment>
                            <comment id="17738303" author="bbeaudreault" created="Wed, 28 Jun 2023 21:23:36 +0000"  >&lt;blockquote&gt;&lt;p&gt;If it does not work, please try lowering the buffer size or the water mark threshold?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The default value is 64k. I will try lowering it, but I don&apos;t think this will matter for the case of a sudden burst of large responses. Writing a single response to the channel will already exceed the default threshold and trigger unwritable.&lt;/p&gt;</comment>
                            <comment id="17738710" author="bbeaudreault" created="Thu, 29 Jun 2023 17:39:43 +0000"  >&lt;p&gt;I have been trying to think about how to solve this without blocking RPC handlers.&lt;/p&gt;

&lt;p&gt;The problem with just relying on setAutoRead(false) is it only pauses acceptance of new requests into the call queue. There will already be requests in progress by RPC handlers, and there could be even more requests queued in our call queue. Allowing them to publish to the channel can still result in OOM.&lt;/p&gt;

&lt;p&gt;In terms of solving this without blocking RPC handlers, we might need to either clear or temporarily invalidate calls in the call queue originating from that channel. We could possibly achieve this by having the ServerCall retain a reference to the originating ServerRpcConnection. When a handler pulls a call from the queue, it checks if that call&apos;s connection.channel is writable. If not it could re-enqueue it, drop it, or maybe close the connection? Not sure yet. I&apos;m open to any thoughts. Then the other question is what to do with calls which have already been in progress when the channel is made unwritable. Do we need a size-limited per-channel responseQueue?&lt;/p&gt;</comment>
                            <comment id="17738847" author="aoxiang" created="Fri, 30 Jun 2023 02:02:15 +0000"  >&lt;p&gt;We encounter a similar problem, there is too much response accumulated in the responseQueue and cause the regionserver OOm, we fix tit by just close this channel and release the heap reference by response.&lt;/p&gt;</comment>
                            <comment id="17738849" author="bbeaudreault" created="Fri, 30 Jun 2023 02:04:17 +0000"  >&lt;p&gt;Sorry for all the updates, but I&#8217;ve had some success today with the non-blocking idea in my last comment. I did the simple thing and had handlers drop calls off the channel is not writable when they pull from the queue. I set the netty high watermark to 5mb and low watermark to 512kb. This still resolved the OOMs but handler usage is better. I&#8217;m going to do more testing tomorrow before I package it up for a PR&lt;/p&gt;</comment>
                            <comment id="17739130" author="bbeaudreault" created="Fri, 30 Jun 2023 15:14:11 +0000"  >&lt;p&gt;Thanks for the input &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=binlijin&quot; class=&quot;user-hover&quot; rel=&quot;binlijin&quot;&gt;binlijin&lt;/a&gt;! I&apos;m concerned about closing the connection because it can be highly disruptive to the caller. Also, with SSL it&apos;s complicated because you can&apos;t just close an SSL connection &#8211; the underlying SSL handler tries to flush and close_notify first, which is async and can take a bit. I did a quick test with closing connection and still saw OOM for my case. For those reasons, I&apos;m trying to find a middle ground where we drop requests first.&lt;/p&gt;

&lt;p&gt;Currently I am iterating on the following:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;channelWritabilityChanged manages three states:
	&lt;ul&gt;
		&lt;li&gt;Channel autoread&lt;/li&gt;
		&lt;li&gt;An AtomicBoolean denoting the writable state&lt;/li&gt;
		&lt;li&gt;A TimerTask which, unless cancelled by going back into writable state, closes the connection after a configurable time period of unwritability.&#160;&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;When an RPC handler executes CallRunner, we check if the channel is writable. If not, drop the request.&lt;/li&gt;
	&lt;li&gt;When we close the channel, we also set another AtomicBoolean. When attempting to write a response to the pipeline, if the boolean is false, we throw a ConnectionClosingException. We can&apos;t rely on channel.isOpen() because SSL close_notify might delay this returning false.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I&apos;m working on a comprehensive list of configurations for the high and low watermark and the closeAfterUnwritable millis, to find a good happy medium for my extreme test case. Once I have that, I&apos;ll try it on a more normal load test case. Ideally anything I do here will not affect normal load performance at all.&lt;/p&gt;

&lt;p&gt;I will note that doing setAutoRead(false) drastically reduces overall throughput for the bad caller. When we manage setAutoRead, the test runs about 50% fewer requests in the allotted time. This is because the caller spends a lot more time in a situation where it can&apos;t even enqueue requests to the server. This may be ok or even preferable, but I want to see how it plays out in different scenarios.&lt;/p&gt;</comment>
                            <comment id="17748889" author="apache9" created="Sun, 30 Jul 2023 06:57:05 +0000"  >&lt;p&gt;Any updates here? I think this is the last blocker issue for our 2.6.0 release?&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;</comment>
                            <comment id="17748925" author="bbeaudreault" created="Sun, 30 Jul 2023 13:10:31 +0000"  >&lt;p&gt;Sorry for the delay. I am still doing some performance testing, but have had limited time recently. Hoping to get back to it this week.&#160;&lt;/p&gt;

&lt;p&gt;My one concern with this is it solves pretty well when there are just a few problematic clients. But if you had 100 problematic clients even with these limits they would probably cause OOM in aggregate.&#160;&lt;/p&gt;

&lt;p&gt;I&#8217;ve been thinking of adopting an approach from Cassandra where they disable netty&#8217;s size estimator and instead keep their own local and global limits.&#160;&lt;/p&gt;</comment>
                            <comment id="17748928" author="apache9" created="Sun, 30 Jul 2023 13:25:22 +0000"  >&lt;p&gt;If we have 100 bad clients, I guess we will be OOM even if TLS is turned off? So it should not be a blocker for the 2.6.0 release.&lt;/p&gt;

&lt;p&gt;We can land your current solution first, release 2.6.0, and then keep improving.&lt;/p&gt;

&lt;p&gt;WDYT?&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;</comment>
                            <comment id="17748929" author="apache9" created="Sun, 30 Jul 2023 13:27:08 +0000"  >&lt;blockquote&gt;
&lt;p&gt;I&#8217;ve been thinking of adopting an approach from Cassandra where they disable netty&#8217;s size estimator and instead keep their own local and global limits. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Do you have any references? So this means the netty&apos;s buffer usage is not very accurate? We will get OOM even if netty think it is still OK?&lt;/p&gt;</comment>
                            <comment id="17748937" author="bbeaudreault" created="Sun, 30 Jul 2023 14:07:29 +0000"  >&lt;blockquote&gt;&lt;p&gt;WDYT?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes, that&apos;s true. I can try to wrap up the simpler solution for now.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Do you have any references?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I think I was noticing their &lt;a href=&quot;https://github.com/apache/cassandra/blob/5815f0d5eb43ce890dc3ea71f45a7488e5c6163a/src/java/org/apache/cassandra/net/NoSizeEstimator.java&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;NoSizeEstimator&lt;/a&gt;, which now I&apos;m realizing is for outbound connections. In their PipelineConfigurator, which I believe is the server configuration for client requests, they just set a relatively low watermark of 8k - 32k. I don&apos;t see any channelWritabilityChanged or isWritable checks, so not sure why they are bothering with a watermark. Searching git history, they used to but lost them in a major rewrite a few years ago. I don&apos;t really know much about cassandra code or history, so trying not to draw too many conclusions.&lt;/p&gt;</comment>
                            <comment id="17749230" author="apache9" created="Mon, 31 Jul 2023 15:11:29 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Yes, that&apos;s true. I can try to wrap up the simpler solution for now.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Good. Thanks.&lt;/p&gt;</comment>
                            <comment id="17753580" author="bbeaudreault" created="Sat, 12 Aug 2023 20:13:17 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zhangduo&quot; class=&quot;user-hover&quot; rel=&quot;zhangduo&quot;&gt;zhangduo&lt;/a&gt;&#160; sorry for the very long delay. I have finally submitted a patch.&lt;/p&gt;

&lt;p&gt;I also have some idea about why this was such a big problem. This &lt;a href=&quot;https://github.com/netty/netty/pull/7468&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;patch in netty&lt;/a&gt; causes huge amounts of allocations when processing large messages with SSL. The messages need to be broken down into 16kb chunks, so the messages passes through this copyAndCompose over and over as they&apos;re broken down. The pressure on the PoolArena is enough to slow down outbound buffer flushing, which also needs to release buffers as it does.&lt;/p&gt;

&lt;p&gt;I created a fork and reverted that change, and the OOMs disappeared even without the protections in my patch. Of course even with that reverted, it&apos;s still possible to back up the outbound socket with enough load, so I think this protections are still worth it.&lt;/p&gt;</comment>
                            <comment id="17753671" author="apache9" created="Sun, 13 Aug 2023 09:09:36 +0000"  >&lt;p&gt;Just come back from vacation.&lt;/p&gt;

&lt;p&gt;Let me take a look on the netty problem first.&lt;/p&gt;

&lt;p&gt;Since we shade netty, it is possible to apply some patches to the netty code base, just like what we have done on protobuf.&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;</comment>
                            <comment id="17753680" author="bbeaudreault" created="Sun, 13 Aug 2023 10:29:21 +0000"  >&lt;p&gt;Thanks and welcome back!&lt;/p&gt;

&lt;p&gt;i was planning on creating another Jira for the netty issue. I have some other details to share. Reverting that patch removes the OOM at a certain load, but there&#8217;s still more room for improvement compared to haproxy. I&#8217;ll try to write up the extra details this week, I don&#8217;t have the notes handy now.&#160;&lt;/p&gt;</comment>
                            <comment id="17753784" author="bbeaudreault" created="Sun, 13 Aug 2023 14:20:00 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zhangduo&quot; class=&quot;user-hover&quot; rel=&quot;zhangduo&quot;&gt;zhangduo&lt;/a&gt; Sorry, I need to do a little more verification around the composeIntoComposite patch before you spend too much time on it. I started writing some notes for you and realized that the composeIntoComposite issue might have been more related to when I was trying to tune the SslHandler.setWrapDataSize. I am going to dig in more on that during the workday my time (EST) tomorrow.&lt;/p&gt;

&lt;p&gt;Here&apos;s the most up-to-date summary on the problem, and my next steps:&lt;/p&gt;

&lt;p&gt;The key issue we are seeing is that ChannelOutboundBuffer is unable to flush fast enough, and eventually builds up to an OOM. We can verify this with metrics and heap dumps. You&apos;d think if SSL slowness were the issue, we&apos;d see the buildup in SslHandler&apos;s pendingUnencryptedBytes, but we don&apos;t. So why is ChannelOutboundBuffer flushing slower?&lt;/p&gt;

&lt;p&gt;Here&apos;s what I can say with some certainty, on why SslHandler causes a buildup in ChannelOutboundBuffer: Looking at AsyncProfiles with SSL enabled and disabled, when SSL is enabled the AbstractEpollStreamChannel.doWrite spends &lt;em&gt;way more&lt;/em&gt; time doing PoolArena.release. The majority (about 80%) of the doWrite time is in PoolArena, rather than socket writing. Without SSL enabled, 0% of time is spent in PoolArena.&lt;/p&gt;

&lt;p&gt;Here&apos;s my theory on why:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;For a 5mb response, our NettyRpcServerResponseEncoder writes a CompositeByteBuf wrapping 80 64kb ByteBuffs from our own ByteBuffAllocator.&lt;/li&gt;
	&lt;li&gt;Without SSL:
	&lt;ul&gt;
		&lt;li&gt;The CompositeByteBuf is added right to the ChannelOutboundBuffer.&lt;/li&gt;
		&lt;li&gt;The IO worker pulls from buffer and sends to the socket via FileDescriptor.writevAddresses.&#160;&lt;/li&gt;
		&lt;li&gt;&lt;b&gt;Since the backing ByteBuffs are from our own ByteBuffAllocator, this process does not involve netty PoolArena&lt;/b&gt;&lt;/li&gt;
		&lt;li&gt;Once finished, our own callbacks release the buffers to our reservoir.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;With SSL:
	&lt;ul&gt;
		&lt;li&gt;SslHandler tries to break the 5mb CompositeByteBuf into 16kb chunks, each allocated from PoolArena (allocateOutNetBuf)&lt;/li&gt;
		&lt;li&gt;So the 80 64kb ByteBuffs will be broken down into at least 300 netty ByteBuffs from the PoolArena&lt;/li&gt;
		&lt;li&gt;Those smaller chunks are written onto ChannelOutboundBuffer individually&lt;/li&gt;
		&lt;li&gt;When the IO worker pulls from the ChannelOutboundBuffer, instead of pulling 1 CompositeByteBuff it needs to pull 300+ &lt;b&gt;PooledUnsafeByteBuff from the PoolArena. This requires releasing those ByteBuff from the PoolArena.&lt;/b&gt;&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;b&gt;Summary&lt;/b&gt;: Netty&apos;s PoolArena might be a highly optimized jemalloc implementation, but we are going from 0 interactions without SSL to 600+ interactions (allocating and then releasing). Releasing buffers from the PoolArena takes the majority of the time when flushing the ChannelOutboundBuffer.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Next step:&lt;/b&gt; &lt;/p&gt;

&lt;p&gt;The majority of wasted time is in PoolArena.release for AbstractEpollStreamChannel.doWrite, and PoolArena.allocate for SslHandler.wrap.&lt;/p&gt;

&lt;p&gt;On the allocation front, it&apos;s largely allocateOutNetBuf. It seems like it would be beneficial to allocate fewer larger buffers. I tried tuning SslHandler.setWrapDataSize, and this caused copyAndCompose to be called a ton (hence changing to composeIntoComposite). It also doesn&apos;t actually seem to work, because SSL will only wrap around 16kb at a time, so most of the extra WrapDataSize ends up getting added back onto the bufferQueue and re-polled next time.&lt;/p&gt;

&lt;p&gt;I&apos;m wondering if we could improve SslHandler.wrap &#8211; pull the full response buffer, and allocate one large outNetBuf, then pass to SSL with offsets/lengths so we step through the larger buffers without extra allocations. We can estimate how big of an outNetBuf we need, and possibly allocate one extra if there&apos;s overflow. I think this would be a lot more efficient since it&apos;s fewer operations in PoolArena and the memory is contiguous.&lt;/p&gt;

&lt;p&gt;-&#160;&lt;/p&gt;</comment>
                            <comment id="17753786" author="apache9" created="Sun, 13 Aug 2023 14:37:07 +0000"  >&lt;p&gt;What does the flame graph look like?&lt;/p&gt;</comment>
                            <comment id="17753787" author="bbeaudreault" created="Sun, 13 Aug 2023 14:43:47 +0000"  >&lt;p&gt;I just uploaded the flamegraphs &#8211; one with ssl enabled, one with it disabled. Take a look at&#160;AbstractEpollStreamChannel.doWrite in both, and allocateOutNetBuf in SSL enabled.&lt;/p&gt;

&lt;p&gt;Note: there&apos;s some slight differences in call path (i.e. wrapWithCatch), because i&apos;ve added some custom code so i could better trace the call stacks with logs. But that does not affect performance, since i disabled logging for these.&lt;/p&gt;</comment>
                            <comment id="17753817" author="bbeaudreault" created="Sun, 13 Aug 2023 19:56:02 +0000"  >&lt;p&gt;I had some time this afternoon and got a POC going of my idea above. It results in a massive improvement in throughput, virtually eliminating the contention in PoolArena. With this change, our netty-based SSL throughput is about onpar with haproxy (for this test at least). I just attached another flamegraph, running with the optimization.&lt;/p&gt;

&lt;p&gt;I haven&apos;t tried contributing to the netty project before, but this improvement is good enough that I&apos;m going to give it a try. I will post a link to the git issue here once it&apos;s ready (next 1-2 days).&lt;/p&gt;</comment>
                            <comment id="17754721" author="bbeaudreault" created="Tue, 15 Aug 2023 17:12:09 +0000"  >&lt;p&gt;I have not started on an official patch yet, but I created a git issue for this &lt;a href=&quot;https://github.com/netty/netty/issues/13549&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/netty/netty/issues/13549&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17755988" author="bbeaudreault" created="Fri, 18 Aug 2023 14:21:58 +0000"  >&lt;p&gt;Thank you everyone for the input, and especially &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zhangduo&quot; class=&quot;user-hover&quot; rel=&quot;zhangduo&quot;&gt;zhangduo&lt;/a&gt; for detailed help and review. Thanks to &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=norman&quot; class=&quot;user-hover&quot; rel=&quot;norman&quot;&gt;norman&lt;/a&gt; for his extra feedback and review on the upstream netty PR (to be integrated in follow-up task &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-28029&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HBASE-28029&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I pushed this to master, branch-2, and branch-3. Since it&apos;s not SSL specific, it could go to older branches but the diff is too complicated.&lt;/p&gt;</comment>
                            <comment id="17756236" author="hudson" created="Sat, 19 Aug 2023 05:44:00 +0000"  >&lt;p&gt;Results for branch branch-2&lt;br/&gt;
	&lt;a href=&quot;https://ci-hbase.apache.org/job/HBase%20Nightly/job/branch-2/866/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;build #866 on builds.a.o&lt;/a&gt;: &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;b&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;&lt;/b&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;details (if available):&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/check.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;font color=&quot;green&quot;&gt;+1 general checks&lt;/font&gt;&lt;br/&gt;
&amp;#8211; For more information &lt;a href=&quot;https://ci-hbase.apache.org/job/HBase%20Nightly/job/branch-2/866/General_20Nightly_20Build_20Report/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;see general report&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/check.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;font color=&quot;green&quot;&gt;+1 jdk8 hadoop2 checks&lt;/font&gt;&lt;br/&gt;
&amp;#8211; For more information &lt;a href=&quot;https://ci-hbase.apache.org/job/HBase%20Nightly/job/branch-2/866/JDK8_20Nightly_20Build_20Report_20_28Hadoop2_29/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;see jdk8 (hadoop2) report&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;font color=&quot;red&quot;&gt;-1 jdk8 hadoop3 checks&lt;/font&gt;&lt;br/&gt;
&amp;#8211; For more information &lt;a href=&quot;https://ci-hbase.apache.org/job/HBase%20Nightly/job/branch-2/866/JDK8_20Nightly_20Build_20Report_20_28Hadoop3_29/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;see jdk8 (hadoop3) report&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/check.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;font color=&quot;green&quot;&gt;+1 jdk11 hadoop3 checks&lt;/font&gt;&lt;br/&gt;
&amp;#8211; For more information &lt;a href=&quot;https://ci-hbase.apache.org/job/HBase%20Nightly/job/branch-2/866/JDK11_20Nightly_20Build_20Report_20_28Hadoop3_29/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;see jdk11 report&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/check.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;font color=&quot;green&quot;&gt;+1 source release artifact&lt;/font&gt;&lt;br/&gt;
&amp;#8211; See build output for details.&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/check.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;font color=&quot;green&quot;&gt;+1 client integration test&lt;/font&gt;&lt;/p&gt;</comment>
                            <comment id="17756359" author="hudson" created="Sat, 19 Aug 2023 19:51:21 +0000"  >&lt;p&gt;Results for branch master&lt;br/&gt;
	&lt;a href=&quot;https://ci-hbase.apache.org/job/HBase%20Nightly/job/master/891/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;build #891 on builds.a.o&lt;/a&gt;: &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/check.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;b&gt;&lt;font color=&quot;green&quot;&gt;+1 overall&lt;/font&gt;&lt;/b&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;details (if available):&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/check.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;font color=&quot;green&quot;&gt;+1 general checks&lt;/font&gt;&lt;br/&gt;
&amp;#8211; For more information &lt;a href=&quot;https://ci-hbase.apache.org/job/HBase%20Nightly/job/master/891/General_20Nightly_20Build_20Report/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;see general report&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/check.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;font color=&quot;green&quot;&gt;+1 jdk8 hadoop3 checks&lt;/font&gt;&lt;br/&gt;
&amp;#8211; For more information &lt;a href=&quot;https://ci-hbase.apache.org/job/HBase%20Nightly/job/master/891/JDK8_20Nightly_20Build_20Report_20_28Hadoop3_29/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;see jdk8 (hadoop3) report&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/check.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;font color=&quot;green&quot;&gt;+1 jdk11 hadoop3 checks&lt;/font&gt;&lt;br/&gt;
&amp;#8211; For more information &lt;a href=&quot;https://ci-hbase.apache.org/job/HBase%20Nightly/job/master/891/JDK11_20Nightly_20Build_20Report_20_28Hadoop3_29/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;see jdk11 report&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/check.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;font color=&quot;green&quot;&gt;+1 source release artifact&lt;/font&gt;&lt;br/&gt;
&amp;#8211; See build output for details.&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/check.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;font color=&quot;green&quot;&gt;+1 client integration test&lt;/font&gt;&lt;/p&gt;</comment>
                            <comment id="17756361" author="hudson" created="Sat, 19 Aug 2023 19:53:04 +0000"  >&lt;p&gt;Results for branch branch-3&lt;br/&gt;
	&lt;a href=&quot;https://ci-hbase.apache.org/job/HBase%20Nightly/job/branch-3/33/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;build #33 on builds.a.o&lt;/a&gt;: &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/check.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;b&gt;&lt;font color=&quot;green&quot;&gt;+1 overall&lt;/font&gt;&lt;/b&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;details (if available):&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/check.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;font color=&quot;green&quot;&gt;+1 general checks&lt;/font&gt;&lt;br/&gt;
&amp;#8211; For more information &lt;a href=&quot;https://ci-hbase.apache.org/job/HBase%20Nightly/job/branch-3/33/General_20Nightly_20Build_20Report/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;see general report&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/check.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;font color=&quot;green&quot;&gt;+1 jdk8 hadoop3 checks&lt;/font&gt;&lt;br/&gt;
&amp;#8211; For more information &lt;a href=&quot;https://ci-hbase.apache.org/job/HBase%20Nightly/job/branch-3/33/JDK8_20Nightly_20Build_20Report_20_28Hadoop3_29/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;see jdk8 (hadoop3) report&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/check.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;font color=&quot;green&quot;&gt;+1 jdk11 hadoop3 checks&lt;/font&gt;&lt;br/&gt;
&amp;#8211; For more information &lt;a href=&quot;https://ci-hbase.apache.org/job/HBase%20Nightly/job/branch-3/33/JDK11_20Nightly_20Build_20Report_20_28Hadoop3_29/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;see jdk11 report&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/check.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;font color=&quot;green&quot;&gt;+1 source release artifact&lt;/font&gt;&lt;br/&gt;
&amp;#8211; See build output for details.&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/check.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;font color=&quot;green&quot;&gt;+1 client integration test&lt;/font&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                            <outwardlinks description="blocks">
                                        <issuelink>
            <issuekey id="13546184">HBASE-28009</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="12310760">
                    <name>Testing</name>
                                            <outwardlinks description="Testing discovered">
                                        <issuelink>
            <issuekey id="13546183">HBASE-28008</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="13062161" name="ssl-disabled-flamegraph.html" size="131418" author="bbeaudreault" created="Sun, 13 Aug 2023 14:40:11 +0000"/>
                            <attachment id="13062162" name="ssl-enabled-flamegraph.html" size="143090" author="bbeaudreault" created="Sun, 13 Aug 2023 14:40:11 +0000"/>
                            <attachment id="13062165" name="ssl-enabled-optimized.html" size="133305" author="bbeaudreault" created="Sun, 13 Aug 2023 19:55:34 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            2 years, 11 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z1iq6g:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12310192" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Release Note</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>When a slow client is not able to read responses from the server fast enough, the server side channel outbound buffer will grow. This can eventually lead to an OOM under extreme cases, and here we add new configurations to protect against that:&lt;br/&gt;
- hbase.server.netty.writable.watermark.low&lt;br/&gt;
- hbase.server.netty.writable.watermark.high&lt;br/&gt;
- hbase.server.netty.writable.watermark.fatal&lt;br/&gt;
&lt;br/&gt;
When high watermark is exceeded, server will stop accepting new requests from the client. When outbound bytes drops below the low watermark, it will start again. This does not stop the server from processing already enqueued requests, so if those requests continue to grow the outbound bytes beyond the fatal threshold, the connection will be forcibly closed.&lt;br/&gt;
&lt;br/&gt;
Also added new metrics for monitoring this situation in bean &amp;quot;Hadoop:service=HBase,name=RegionServer,sub=IPC&amp;quot;:&lt;br/&gt;
&amp;nbsp;- UnwritableTime_* - histogram of time periods between when the high watermark was exceeded and when it eventually drops below low watermark. &lt;br/&gt;
- nettyTotalPendingOutboundBytes - as the name suggests, for all channels the total amount of bytes waiting to be written to sockets&lt;br/&gt;
- nettyMaxPendingOutboundBytes - the number of bytes waiting on the most backed up channel across all channels</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            </customfields>
    </item>
</channel>
</rss>