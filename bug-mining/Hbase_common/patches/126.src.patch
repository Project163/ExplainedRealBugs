diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java
index 6d26510cd1..77fb8e9f6d 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java
@@ -376,4 +376,42 @@ public final class CellUtil {
       // Serialization is probably preceded by a length (it is in the KeyValueCodec at least).
       Bytes.SIZEOF_INT;
   }
+  
+  
+  /********************* tags *************************************/
+  /**
+   * Util method to iterate through the tags. Used in testcase
+   * 
+   * @param tags
+   * @param offset
+   * @param length
+   * @return
+   */
+  public static Iterator<Tag> tagsIterator(final byte[] tags, final int offset, final short length) {
+    return new Iterator<Tag>() {
+      private int pos = offset;
+      private int endOffset = offset + length - 1;
+
+      @Override
+      public boolean hasNext() {
+        return this.pos < endOffset;
+      }
+
+      @Override
+      public Tag next() {
+        if (hasNext()) {
+          short curTagLen = Bytes.toShort(tags, this.pos);
+          Tag tag = new Tag(tags, pos, (short) (curTagLen + Bytes.SIZEOF_SHORT));
+          this.pos += Bytes.SIZEOF_SHORT + curTagLen;
+          return tag;
+        }
+        return null;
+      }
+
+      @Override
+      public void remove() {
+        throw new UnsupportedOperationException();
+      }
+    };
+  }
 }
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java
index b15021aafc..f2d777c60c 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java
@@ -31,7 +31,6 @@ import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Comparator;
 import java.util.HashMap;
-import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 
@@ -80,6 +79,8 @@ import com.google.common.primitives.Longs;
  */
 @InterfaceAudience.Private
 public class KeyValue implements Cell, HeapSize, Cloneable {
+  private static final ArrayList<Tag> EMPTY_ARRAY_LIST = new ArrayList<Tag>();
+
   static final Log LOG = LogFactory.getLog(KeyValue.class);
 
   /**
@@ -254,38 +255,6 @@ public class KeyValue implements Cell, HeapSize, Cloneable {
     }
   }
 
-  /**
-   * @return an iterator over the tags in this KeyValue.
-   */
-  public Iterator<Tag> tagsIterator() {
-    // Subtract -1 to point to the end of the complete tag byte[]
-    final int endOffset = this.offset + this.length - 1;
-    return new Iterator<Tag>() {
-      private int pos = getTagsOffset();
-
-      @Override
-      public boolean hasNext() {
-        return this.pos < endOffset;
-      }
-
-      @Override
-      public Tag next() {
-        if (hasNext()) {
-          short curTagLen = Bytes.toShort(bytes, this.pos);
-          Tag tag = new Tag(bytes, pos, (short) (curTagLen + Bytes.SIZEOF_SHORT));
-          this.pos += Bytes.SIZEOF_SHORT + curTagLen;
-          return tag;
-        }
-        return null;
-      }
-
-      @Override
-      public void remove() {
-        throw new UnsupportedOperationException();
-      }
-    };
-  }
-
   /**
    * Lowest possible key.
    * Makes a Key with highest possible Timestamp, empty row and column.  No
@@ -1618,18 +1587,15 @@ public class KeyValue implements Cell, HeapSize, Cloneable {
   }
 
   /**
-   * This method may not be right.  But we cannot use the CellUtil.getTagIterator because we don't know
-   * getKeyOffset and getKeyLength
-   * Cannnot use the getKeyOffset and getKeyLength in CellUtil as they are not part of the Cell interface.
-   * Returns any tags embedded in the KeyValue.
+   * Returns any tags embedded in the KeyValue.  Used in testcases.
    * @return The tags
    */
   public List<Tag> getTags() {
     short tagsLength = getTagsLength();
     if (tagsLength == 0) {
-      return new ArrayList<Tag>();
+      return EMPTY_ARRAY_LIST;
     }
-    return Tag.createTags(getBuffer(), getTagsOffset(), tagsLength);
+    return Tag.asList(getBuffer(), getTagsOffset(), tagsLength);
   }
 
   /**
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/Tag.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/Tag.java
index 380baa8f68..c5acbd24d4 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/Tag.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/Tag.java
@@ -25,11 +25,9 @@ import java.util.List;
 import org.apache.hadoop.classification.InterfaceAudience;
 import org.apache.hadoop.classification.InterfaceStability;
 import org.apache.hadoop.hbase.util.Bytes;
-
 /**
- * <code>&lt;taglength>&lt;tagtype>&lt;tagbytes></code>. <code>tagtype</code> is
- * one byte and <code>taglength</code> maximum is <code>Short.MAX_SIZE</code>.
- * It includes 1 byte type length and actual tag bytes length.
+ * Tags are part of cells and helps to add metadata about the KVs.
+ * Metadata could be ACLs per cells, visibility labels, etc.
  */
 @InterfaceAudience.Private
 @InterfaceStability.Evolving
@@ -38,8 +36,8 @@ public class Tag {
   public final static int TAG_LENGTH_SIZE = Bytes.SIZEOF_SHORT;
   public final static int INFRASTRUCTURE_SIZE = TYPE_LENGTH_SIZE + TAG_LENGTH_SIZE;
 
-  private byte type;
-  private byte[] bytes;
+  private final byte type;
+  private final byte[] bytes;
   private int offset = 0;
   private short length = 0;
 
@@ -56,7 +54,9 @@ public class Tag {
    * @param tag
    */
   public Tag(byte tagType, byte[] tag) {
-    // <length of tag - 2 bytes><type code - 1 byte><tag>
+    /** <length of tag - 2 bytes><type code - 1 byte><tag>
+     * taglength maximum is Short.MAX_SIZE.  It includes 1 byte type length and actual tag bytes length.
+     */
     short tagLength = (short) ((tag.length & 0x0000ffff) + TYPE_LENGTH_SIZE);
     length = (short) (TAG_LENGTH_SIZE + tagLength);
     bytes = new byte[length];
@@ -119,14 +119,14 @@ public class Tag {
   /**
    * @return Length of actual tag bytes within the backed buffer
    */
-  public int getTagLength() {
+  int getTagLength() {
     return this.length - INFRASTRUCTURE_SIZE;
   }
 
   /**
    * @return Offset of actual tag bytes within the backed buffer
    */
-  public int getTagOffset() {
+  int getTagOffset() {
     return this.offset + INFRASTRUCTURE_SIZE;
   }
 
@@ -145,7 +145,7 @@ public class Tag {
    * @param length
    * @return List of tags
    */
-  public static List<Tag> createTags(byte[] b, int offset, short length) {
+  public static List<Tag> asList(byte[] b, int offset, short length) {
     List<Tag> tags = new ArrayList<Tag>();
     int pos = offset;
     while (pos < offset + length) {
@@ -169,4 +169,4 @@ public class Tag {
   int getOffset() {
     return this.offset;
   }
-}
\ No newline at end of file
+}
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java
index 5578e0132f..8ccab7fd87 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java
@@ -52,7 +52,7 @@ abstract class BufferedDataBlockEncoder implements DataBlockEncoder {
 
     HFileBlockDefaultDecodingContext decodingCtx =
         (HFileBlockDefaultDecodingContext) blkDecodingCtx;
-    if (decodingCtx.getHFileContext().shouldCompressTags()) {
+    if (decodingCtx.getHFileContext().isCompressTags()) {
       try {
         TagCompressionContext tagCompressionContext = new TagCompressionContext(LRUDictionary.class);
         decodingCtx.setTagCompressionContext(tagCompressionContext);
@@ -162,7 +162,7 @@ abstract class BufferedDataBlockEncoder implements DataBlockEncoder {
         this.samePrefixComparator = null;
       }
       this.decodingCtx = decodingCtx;
-      if (decodingCtx.getHFileContext().shouldCompressTags()) {
+      if (decodingCtx.getHFileContext().isCompressTags()) {
         try {
           tagCompressionContext = new TagCompressionContext(LRUDictionary.class);
         } catch (Exception e) {
@@ -172,11 +172,11 @@ abstract class BufferedDataBlockEncoder implements DataBlockEncoder {
     }
     
     protected boolean includesMvcc() {
-      return this.decodingCtx.getHFileContext().shouldIncludeMvcc();
+      return this.decodingCtx.getHFileContext().isIncludesMvcc();
     }
 
     protected boolean includesTags() {
-      return this.decodingCtx.getHFileContext().shouldIncludeTags();
+      return this.decodingCtx.getHFileContext().isIncludesTags();
     }
 
     @Override
@@ -264,7 +264,7 @@ abstract class BufferedDataBlockEncoder implements DataBlockEncoder {
       return true;
     }
 
-    public void decodeTags() {
+    protected void decodeTags() {
       current.tagsLength = ByteBufferUtils.readCompressedInt(currentBuffer);
       if (tagCompressionContext != null) {
         // Tag compression is been used. uncompress it into tagsBuffer
@@ -373,7 +373,7 @@ abstract class BufferedDataBlockEncoder implements DataBlockEncoder {
 
   protected final void afterEncodingKeyValue(ByteBuffer in,
       DataOutputStream out, HFileBlockDefaultEncodingContext encodingCtx) throws IOException {
-    if (encodingCtx.getHFileContext().shouldIncludeTags()) {
+    if (encodingCtx.getHFileContext().isIncludesTags()) {
       short tagsLength = in.getShort();
       ByteBufferUtils.putCompressedInt(out, tagsLength);
       // There are some tags to be written
@@ -388,7 +388,7 @@ abstract class BufferedDataBlockEncoder implements DataBlockEncoder {
         }
       }
     }
-    if (encodingCtx.getHFileContext().shouldIncludeMvcc()) {
+    if (encodingCtx.getHFileContext().isIncludesMvcc()) {
       // Copy memstore timestamp from the byte buffer to the output stream.
       long memstoreTS = -1;
       try {
@@ -403,7 +403,7 @@ abstract class BufferedDataBlockEncoder implements DataBlockEncoder {
 
   protected final void afterDecodingKeyValue(DataInputStream source,
       ByteBuffer dest, HFileBlockDefaultDecodingContext decodingCtx) throws IOException {
-    if (decodingCtx.getHFileContext().shouldIncludeTags()) {
+    if (decodingCtx.getHFileContext().isIncludesTags()) {
       short tagsLength = (short) ByteBufferUtils.readCompressedInt(source);
       dest.putShort(tagsLength);
       if (tagsLength > 0) {
@@ -417,7 +417,7 @@ abstract class BufferedDataBlockEncoder implements DataBlockEncoder {
         }
       }
     }
-    if (decodingCtx.getHFileContext().shouldIncludeMvcc()) {
+    if (decodingCtx.getHFileContext().isIncludesMvcc()) {
       long memstoreTS = -1;
       try {
         // Copy memstore timestamp from the data input stream to the byte
@@ -452,7 +452,7 @@ abstract class BufferedDataBlockEncoder implements DataBlockEncoder {
   public abstract void internalEncodeKeyValues(DataOutputStream out,
       ByteBuffer in, HFileBlockDefaultEncodingContext encodingCtx) throws IOException;
 
-  public abstract ByteBuffer internalDecodeKeyValues(DataInputStream source,
+  protected abstract ByteBuffer internalDecodeKeyValues(DataInputStream source,
       int allocateHeaderLength, int skipLastBytes, HFileBlockDefaultDecodingContext decodingCtx)
       throws IOException;
 
@@ -471,7 +471,7 @@ abstract class BufferedDataBlockEncoder implements DataBlockEncoder {
     DataOutputStream dataOut =
         ((HFileBlockDefaultEncodingContext) encodingCtx)
         .getOutputStreamForEncoder();
-    if (encodingCtx.getHFileContext().shouldCompressTags()) {
+    if (encodingCtx.getHFileContext().isCompressTags()) {
       try {
         TagCompressionContext tagCompressionContext = new TagCompressionContext(LRUDictionary.class);
         encodingCtx.setTagCompressionContext(tagCompressionContext);
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/CopyKeyDataBlockEncoder.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/CopyKeyDataBlockEncoder.java
index b660f423b0..1dc8413181 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/CopyKeyDataBlockEncoder.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/CopyKeyDataBlockEncoder.java
@@ -88,7 +88,7 @@ public class CopyKeyDataBlockEncoder extends BufferedDataBlockEncoder {
   }
 
   @Override
-  public ByteBuffer internalDecodeKeyValues(DataInputStream source, int allocateHeaderLength,
+  protected ByteBuffer internalDecodeKeyValues(DataInputStream source, int allocateHeaderLength,
       int skipLastBytes, HFileBlockDefaultDecodingContext decodingCtx) throws IOException {
     int decompressedSize = source.readInt();
     ByteBuffer buffer = ByteBuffer.allocate(decompressedSize +
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/DiffKeyDeltaEncoder.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/DiffKeyDeltaEncoder.java
index 144501140c..f72878bcb6 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/DiffKeyDeltaEncoder.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/DiffKeyDeltaEncoder.java
@@ -26,7 +26,6 @@ import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.KeyValue.KVComparator;
 import org.apache.hadoop.hbase.util.ByteBufferUtils;
 import org.apache.hadoop.hbase.util.Bytes;
-import org.apache.hadoop.io.RawComparator;
 
 /**
  * Compress using:
@@ -534,7 +533,7 @@ public class DiffKeyDeltaEncoder extends BufferedDataBlockEncoder {
   }
 
   @Override
-  public ByteBuffer internalDecodeKeyValues(DataInputStream source, int allocateHeaderLength,
+  protected ByteBuffer internalDecodeKeyValues(DataInputStream source, int allocateHeaderLength,
       int skipLastBytes, HFileBlockDefaultDecodingContext decodingCtx) throws IOException {
     int decompressedSize = source.readInt();
     ByteBuffer buffer = ByteBuffer.allocate(decompressedSize +
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodedDataBlock.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodedDataBlock.java
index e75b32efc9..9e832368b5 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodedDataBlock.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodedDataBlock.java
@@ -113,13 +113,13 @@ public class EncodedDataBlock {
         short tagsLen = 0;
         ByteBufferUtils.skip(decompressedData, klen + vlen);
         // Read the tag length in case when steam contain tags
-        if (meta.shouldIncludeTags()) {
+        if (meta.isIncludesTags()) {
           tagsLen = decompressedData.getShort();
           ByteBufferUtils.skip(decompressedData, tagsLen);
         }
         KeyValue kv = new KeyValue(decompressedData.array(), offset,
             (int) KeyValue.getKeyValueDataStructureSize(klen, vlen, tagsLen));
-        if (meta.shouldIncludeMvcc()) {
+        if (meta.isIncludesMvcc()) {
           long mvccVersion = ByteBufferUtils.readVLong(decompressedData);
           kv.setMvccVersion(mvccVersion);
         }
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/FastDiffDeltaEncoder.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/FastDiffDeltaEncoder.java
index 559db7c0d9..0346b201bd 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/FastDiffDeltaEncoder.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/FastDiffDeltaEncoder.java
@@ -27,7 +27,6 @@ import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.KeyValue.KVComparator;
 import org.apache.hadoop.hbase.util.ByteBufferUtils;
 import org.apache.hadoop.hbase.util.Bytes;
-import org.apache.hadoop.io.RawComparator;
 
 /**
  * Encoder similar to {@link DiffKeyDeltaEncoder} but supposedly faster.
@@ -362,7 +361,7 @@ public class FastDiffDeltaEncoder extends BufferedDataBlockEncoder {
   }
 
   @Override
-  public ByteBuffer internalDecodeKeyValues(DataInputStream source, int allocateHeaderLength,
+  protected ByteBuffer internalDecodeKeyValues(DataInputStream source, int allocateHeaderLength,
       int skipLastBytes, HFileBlockDefaultDecodingContext decodingCtx) throws IOException {
     int decompressedSize = source.readInt();
     ByteBuffer buffer = ByteBuffer.allocate(decompressedSize +
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/PrefixKeyDeltaEncoder.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/PrefixKeyDeltaEncoder.java
index e8a6c4957d..f57ff4fc7e 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/PrefixKeyDeltaEncoder.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/PrefixKeyDeltaEncoder.java
@@ -26,7 +26,6 @@ import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.KeyValue.KVComparator;
 import org.apache.hadoop.hbase.util.ByteBufferUtils;
 import org.apache.hadoop.hbase.util.Bytes;
-import org.apache.hadoop.io.RawComparator;
 
 /**
  * Compress key by storing size of common prefix with previous KeyValue
@@ -92,7 +91,7 @@ public class PrefixKeyDeltaEncoder extends BufferedDataBlockEncoder {
   }
 
   @Override
-  public ByteBuffer internalDecodeKeyValues(DataInputStream source, int allocateHeaderLength,
+  protected ByteBuffer internalDecodeKeyValues(DataInputStream source, int allocateHeaderLength,
       int skipLastBytes, HFileBlockDefaultDecodingContext decodingCtx) throws IOException {
     int decompressedSize = source.readInt();
     ByteBuffer buffer = ByteBuffer.allocate(decompressedSize +
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileContext.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileContext.java
index 7de89f86cf..add5fe7643 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileContext.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileContext.java
@@ -27,9 +27,8 @@ import org.apache.hadoop.hbase.util.ClassSize;
 
 /**
  * This carries the information on some of the meta data about the HFile. This
- * meta data would be used across the HFileWriter/Readers and the HFileBlocks.
- * This would help to add new information to the HFile.
- * This class is not meant to be immutable.
+ * meta data is used across the HFileWriter/Readers and the HFileBlocks.
+ * This helps to add new information to the HFile.
  */
 @InterfaceAudience.Private
 public class HFileContext implements HeapSize, Cloneable {
@@ -96,29 +95,27 @@ public class HFileContext implements HeapSize, Cloneable {
     return compressAlgo;
   }
 
-  public boolean shouldUseHBaseChecksum() {
+  public boolean isUseHBaseChecksum() {
     return usesHBaseChecksum;
   }
 
-  public boolean shouldIncludeMvcc() {
+  public boolean isIncludesMvcc() {
     return includesMvcc;
   }
 
-  // TODO : This setter should be removed
   public void setIncludesMvcc(boolean includesMvcc) {
     this.includesMvcc = includesMvcc;
   }
 
-  public boolean shouldIncludeTags() {
+  public boolean isIncludesTags() {
     return includesTags;
   }
 
-  // TODO : This setter should be removed?
   public void setIncludesTags(boolean includesTags) {
     this.includesTags = includesTags;
   }
 
-  public boolean shouldCompressTags() {
+  public boolean isCompressTags() {
     return compressTags;
   }
 
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileContextBuilder.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileContextBuilder.java
index dcd3e85a7e..3a95080cbb 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileContextBuilder.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileContextBuilder.java
@@ -38,7 +38,7 @@ public class HFileContextBuilder {
   /** Whether tags are to be included in the Read/Write **/
   private boolean includesTags;
   /** Compression algorithm used **/
-  private Algorithm compressAlgo = Algorithm.NONE;
+  private Algorithm compression = Algorithm.NONE;
   /** Whether tags to be compressed or not **/
   private boolean compressTags;
   /** the checksum type **/
@@ -65,8 +65,8 @@ public class HFileContextBuilder {
     return this;
   }
 
-  public HFileContextBuilder withCompressionAlgo(Algorithm compressionAlgo) {
-    this.compressAlgo = compressionAlgo;
+  public HFileContextBuilder withCompression(Algorithm compression) {
+    this.compression = compression;
     return this;
   }
 
@@ -101,7 +101,7 @@ public class HFileContextBuilder {
   }
 
   public HFileContext build() {
-    return new HFileContext(usesHBaseChecksum, includesMvcc, includesTags, compressAlgo,
+    return new HFileContext(usesHBaseChecksum, includesMvcc, includesTags, compression,
         compressTags, checksumType, bytesPerChecksum, blocksize, encodingOnDisk, encodingInCache);
   }
 }
diff --git a/hbase-common/src/test/java/org/apache/hadoop/hbase/TestKeyValue.java b/hbase-common/src/test/java/org/apache/hadoop/hbase/TestKeyValue.java
index f0df4720e5..f565cd3b7e 100644
--- a/hbase-common/src/test/java/org/apache/hadoop/hbase/TestKeyValue.java
+++ b/hbase-common/src/test/java/org/apache/hadoop/hbase/TestKeyValue.java
@@ -560,8 +560,9 @@ public class TestKeyValue extends TestCase {
     }
     assertTrue(meta1Ok);
     assertTrue(meta2Ok);
-
-    Iterator<Tag> tagItr = kv.tagsIterator();
+    Iterator<Tag> tagItr = CellUtil.tagsIterator(kv.getTagsArray(), kv.getTagsOffset(),
+        kv.getTagsLength());
+    //Iterator<Tag> tagItr = kv.tagsIterator();
     assertTrue(tagItr.hasNext());
     Tag next = tagItr.next();
     assertEquals(10, next.getTagLength());
@@ -574,7 +575,7 @@ public class TestKeyValue extends TestCase {
     Bytes.equals(next.getValue(), metaValue2);
     assertFalse(tagItr.hasNext());
 
-    tagItr = kv.tagsIterator();
+    tagItr = CellUtil.tagsIterator(kv.getTagsArray(), kv.getTagsOffset(), kv.getTagsLength());
     assertTrue(tagItr.hasNext());
     next = tagItr.next();
     assertEquals(10, next.getTagLength());
