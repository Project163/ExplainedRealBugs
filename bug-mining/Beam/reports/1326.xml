<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 11:01:24 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[BEAM-5164] ParquetIOIT fails on Spark and Flink</title>
                <link>https://issues.apache.org/jira/browse/BEAM-5164</link>
                <project id="12319527" key="BEAM">Beam</project>
                    <description>&lt;p&gt;When run on Spark or Flink remote cluster, ParquetIOIT fails with the following stacktrace:&#160;&lt;/p&gt;


&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
org.apache.beam.sdk.io.parquet.ParquetIOIT &amp;gt; writeThenReadAll FAILED
org.apache.beam.sdk.Pipeline$PipelineExecutionException: java.lang.NoSuchMethodError: org.apache.parquet.hadoop.ParquetWriter$Builder.&amp;lt;init&amp;gt;(Lorg/apache/parquet/io/OutputFile;)V
at org.apache.beam.runners.spark.SparkPipelineResult.beamExceptionFrom(SparkPipelineResult.java:66)
at org.apache.beam.runners.spark.SparkPipelineResult.waitUntilFinish(SparkPipelineResult.java:99)
at org.apache.beam.runners.spark.SparkPipelineResult.waitUntilFinish(SparkPipelineResult.java:87)
at org.apache.beam.runners.spark.TestSparkRunner.run(TestSparkRunner.java:116)
at org.apache.beam.runners.spark.TestSparkRunner.run(TestSparkRunner.java:61)
at org.apache.beam.sdk.Pipeline.run(Pipeline.java:313)
at org.apache.beam.sdk.testing.TestPipeline.run(TestPipeline.java:350)
at org.apache.beam.sdk.testing.TestPipeline.run(TestPipeline.java:331)
at org.apache.beam.sdk.io.parquet.ParquetIOIT.writeThenReadAll(ParquetIOIT.java:133)

Caused by:
java.lang.NoSuchMethodError: org.apache.parquet.hadoop.ParquetWriter$Builder.&amp;lt;init&amp;gt;(Lorg/apache/parquet/io/OutputFile;)V&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</description>
                <environment></environment>
        <key id="13179617">BEAM-5164</key>
            <summary>ParquetIOIT fails on Spark and Flink</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10103" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">P3</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="&#321;ukaszG">Lukasz Gajowy</reporter>
                        <labels>
                    </labels>
                <created>Fri, 17 Aug 2018 16:00:18 +0000</created>
                <updated>Fri, 3 Jun 2022 22:13:23 +0000</updated>
                                                                            <component>testing</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                    <progress percentage="100">
                                    <originalProgress>
                                                    <row percentage="0" backgroundColor="#89afd7"/>
                                                    <row percentage="100" backgroundColor="transparent"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="100" backgroundColor="#51a825"/>
                                                    <row percentage="0" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </progress>
                                    <aggregateprogress percentage="100">
                                    <originalProgress>
                                                    <row percentage="0" backgroundColor="#89afd7"/>
                                                    <row percentage="100" backgroundColor="transparent"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="100" backgroundColor="#51a825"/>
                                                    <row percentage="0" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </aggregateprogress>
                                            <timeestimate seconds="0">0h</timeestimate>
                            <timespent seconds="4800">1h 20m</timespent>
                                <comments>
                            <comment id="16903656" author="ryanskraba" created="Fri, 9 Aug 2019 07:35:34 +0000"  >&lt;p&gt;See question and possible workarounds on &lt;a href=&quot;https://stackoverflow.com/questions/57395634/apache-beam-java-sdk-sparkrunner-write-to-parquet-error/57399273&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;stack overflow&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Parquet relies on some new methods in Parquet &lt;b&gt;1.10.x&lt;/b&gt;, while Spark 2.2 and Spark 2.3 embed &lt;b&gt;Parquet 1.8.2&lt;/b&gt;.   In this case, should the parquet jars be shaded/vendored to work with all supported versions of spark?&lt;/p&gt;</comment>
                            <comment id="16905107" author="&#322;ukaszg" created="Mon, 12 Aug 2019 11:47:44 +0000"  >&lt;p&gt;I think so&#160;but&#160;I&apos;m not very confident when it comes to these decisions.&#160;&lt;a href=&quot;https://lists.apache.org/thread.html/00202ff6a1e6eb7c2ce8c54cafd2afe7eb5524c63f29ed3bd695860e@%3Cdev.beam.apache.org%3E&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;After removing shading by default&lt;/a&gt;&#160;and switching to vendoring some dependencies in general I&apos;m not sure if we should just use shadow when declaring the dependency or vendor the jars in that case same as we vendor e.g. guava. Adding &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lcwik&quot; class=&quot;user-hover&quot; rel=&quot;lcwik&quot;&gt;lcwik&lt;/a&gt; so that he could share his opinion.&#160;&lt;/p&gt;</comment>
                            <comment id="16905907" author="ryanskraba" created="Tue, 13 Aug 2019 07:47:12 +0000"  >&lt;p&gt;Thanks for the link for the context!  Is it possible that de-shading of parquet was a mistake?&lt;/p&gt;

&lt;p&gt;From the discussion, it sounds like (1) we should shade to prevent transitive dependency collisions in runners when necessary, but (2) don&apos;t shade systematically by default &quot;just in case&quot;, and (3) once a dependency has reached a certain threshold, like the extremely common guava and grpc jars, vendor them for reuse.&lt;/p&gt;

&lt;p&gt;Is that about right?&lt;/p&gt;

&lt;p&gt;Specifically for Spark, it looks like this is reported at least since 2.12.0 for versions of Spark &amp;lt; 2.4 &amp;#8211; it looks like ParquetIOIT should be OK as-is with 2.4.3.  I couldn&apos;t find any references to older versions of spark in the code.  &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=%C5%81ukaszG&quot; class=&quot;user-hover&quot; rel=&quot;&#321;ukaszG&quot;&gt;&#321;ukaszG&lt;/a&gt; Were you running with your own spark installation?&lt;/p&gt;


</comment>
                            <comment id="16905999" author="&#322;ukaszg" created="Tue, 13 Aug 2019 09:19:13 +0000"  >&lt;p&gt;&amp;gt; Were you running with your own spark installation?&lt;/p&gt;

&lt;p&gt;At the time of reporting this issue i used Spark 2.3.2 prebuilt from Hadoop 2.7 as this was the latest supported version in beam back then. Downloaded from here:&#160;&lt;a href=&quot;https://spark.apache.org/downloads.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://spark.apache.org/downloads.html&lt;/a&gt;.&#160;It is possible that this issue doesn&apos;t concern future versions.&lt;/p&gt;</comment>
                            <comment id="16906020" author="ryanskraba" created="Tue, 13 Aug 2019 09:50:26 +0000"  >&lt;p&gt;I checked with a spark local run on Spark 2.4.3, there is no issue (expected, since it includes the &quot;right&quot; parquet jars in the spark-supplied classpath).&lt;/p&gt;

&lt;p&gt;The workaround that I proposed in the stack overflow question works for Spark 2.2, 2.3 and 2.4, but is only a workaround... I added the following relocations to the &quot;fat jar&quot; build proposed in the Spark runner instructions:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
            &amp;lt;relocation&amp;gt;
              &amp;lt;pattern&amp;gt;org.apache.parquet&amp;lt;/pattern&amp;gt;
              &amp;lt;shadedPattern&amp;gt;shaded.org.apache.parquet&amp;lt;/shadedPattern&amp;gt;
            &amp;lt;/relocation&amp;gt;
            &amp;lt;!-- Some packages are shaded already, and on the original spark classpath. Shade them more. --&amp;gt;
            &amp;lt;relocation&amp;gt;
              &amp;lt;pattern&amp;gt;shaded.parquet&amp;lt;/pattern&amp;gt;
              &amp;lt;shadedPattern&amp;gt;reshaded.parquet&amp;lt;/shadedPattern&amp;gt;
            &amp;lt;/relocation&amp;gt;
            &amp;lt;relocation&amp;gt;
              &amp;lt;pattern&amp;gt;org.apache.avro&amp;lt;/pattern&amp;gt;
              &amp;lt;shadedPattern&amp;gt;shaded.org.apache.avro&amp;lt;/shadedPattern&amp;gt;
            &amp;lt;/relocation&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="16906058" author="&#322;ukaszg" created="Tue, 13 Aug 2019 10:30:15 +0000"  >&lt;p&gt;Do you know what is the situation on Flink runner? Should we still shadow/vendor this dependency in your opinion?&lt;/p&gt;</comment>
                            <comment id="16906154" author="ryanskraba" created="Tue, 13 Aug 2019 12:42:20 +0000"  >&lt;p&gt;I am not confident on the overall strategy with respect to shading/relocating or vendoring, so waiting for advice from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lcwik&quot; class=&quot;user-hover&quot; rel=&quot;lcwik&quot;&gt;lcwik&lt;/a&gt; sounds good.  It seems like the right thing to do for older, supported versions of Spark.&lt;/p&gt;

&lt;p&gt;I checked with the flink distributions on docker from 1.5 to 1.8 and there aren&apos;t existing parquet artifacts in the image, or inside &lt;tt&gt;flink-dist_2.11_1.X.X.jar&lt;/tt&gt;, so I&apos;m also unsure why the IT test failed on your remote cluster!&lt;/p&gt;</comment>
                            <comment id="16906445" author="lcwik" created="Tue, 13 Aug 2019 17:38:53 +0000"  >&lt;p&gt;In this specific case I think we could shade the parquet library.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Your reasoning listed above is correct:&lt;/p&gt;

&lt;p&gt; &lt;em&gt;(1) we should shade to prevent transitive dependency collisions in runners when necessary, but (2) don&apos;t shade systematically by default &quot;just in case&quot;, and (3) once a dependency has reached a certain threshold, like the extremely common guava and grpc jars, vendor them for reuse.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;The downside to shading/vendoring is that it makes it more difficult for users to force a dependency version change without having the Apache Beam folks perform a release and also getting the shading/vendoring done correctly is quite annoying and very error prone. Vendoring requires two releases (the vendored artifact, and then core Beam projects that are updated to consume it) while shading only needs one but vendoring is much easier to reason about/builds faster/...&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;The best option is typically to try and get all&#160;parts aligned to use the same version but this is not possible always (such as in the case where you are trying to use multiple versions of Spark and Spark itself is incompatible with the newer version of a library) then your forced to shade/vendor.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16907232" author="ryanskraba" created="Wed, 14 Aug 2019 12:40:35 +0000"  >&lt;p&gt;OK &amp;#8211; one huge complication :/  Relocating the parquet library &lt;b&gt;also&lt;/b&gt; requires relocating avro...  It uses an Avro method that is not API-compatible with avro-1.7.7 delivered with Spark 2.2 and 2.3 (specifically logical types in the Avro write support).&lt;/p&gt;

&lt;p&gt;At the same time, pretty much anything using Avro with Beam SQL is also very likely to fail on avro-1.7.7 (therefore Spark 2.2 and 2.3) due to logical type conversions.&lt;/p&gt;

&lt;p&gt;Amusingly enough, &lt;b&gt;AvroIO&lt;/b&gt; looks like it&apos;s fine on Spark 2.2, 2.3.&lt;/p&gt;

&lt;p&gt;How does this sound: instead of trying to correctly satisfy everyone with Parquet shading, I&apos;ll create the equivalent doc for the workaround (similar to what &lt;a href=&quot;https://beam.apache.org/documentation/io/built-in/hcatalog/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;hcatalog already does&lt;/a&gt;) and link it to the ParquetIO javadoc and built-in sources documentation.&lt;/p&gt;

&lt;p&gt;In any case, this documentation should exist for users of Spark 2.2, 2.3 with Beam 2.12.0-2.15.0..&lt;/p&gt;

&lt;p&gt;At the same time I can start investigating what it would take to vendor/shade Avro overall and find/create a JIRA for that.&lt;/p&gt;

&lt;p&gt;What do you think?&lt;/p&gt;
</comment>
                            <comment id="16907334" author="&#322;ukaszg" created="Wed, 14 Aug 2019 15:04:21 +0000"  >&lt;p&gt;I&apos;m fine with adding the docs as you suggested. I left comments in your PR.&lt;/p&gt;</comment>
                            <comment id="17474197" author="kenn" created="Wed, 12 Jan 2022 03:50:54 +0000"  >&lt;p&gt;This Jira ticket has a pull request attached to it, but is still open. Did the pull request resolve the issue? If so, could you please mark it resolved? This will help the project have a clear view of its open issues.&lt;/p&gt;</comment>
                            <comment id="17547326" author="kenn" created="Fri, 3 Jun 2022 22:13:23 +0000"  >&lt;p&gt;This issue has been migrated to &lt;a href=&quot;https://github.com/apache/beam/issues/19094&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/beam/issues/19094&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="13250818">BEAM-7979</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[no_permission]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            3 years, 23 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3x5tb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            </customfields>
    </item>
</channel>
</rss>