<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 10:54:41 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[BEAM-5514] BigQueryIO doesn&apos;t handle quotaExceeded errors properly</title>
                <link>https://issues.apache.org/jira/browse/BEAM-5514</link>
                <project id="12319527" key="BEAM">Beam</project>
                    <description>&lt;p&gt;When exceeding a streaming quota for BigQuery insertAll requests, BigQuery returns a 403 with reason &quot;quotaExceeded&quot;.&lt;/p&gt;

&lt;p&gt;The current implementation of BigQueryIO does not consider this to be a rate limited exception, and therefore does not perform exponential backoff properly, leading to repeated calls to BQ.&lt;/p&gt;

&lt;p&gt;The actual error is in the &lt;a href=&quot;https://github.com/apache/beam/blob/master/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryServicesImpl.java#L739&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;ApiErrorExtractor&lt;/a&gt; class, which is called from &lt;a href=&quot;https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/util/src/main/java/com/google/cloud/hadoop/util/ApiErrorExtractor.java#L263&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;BigQueryServicesImpl&lt;/a&gt; to determine how to retry the failure.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13187713">BEAM-5514</key>
            <summary>BigQueryIO doesn&apos;t handle quotaExceeded errors properly</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10102" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">P2</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="heejong">Heejong Lee</assignee>
                                    <reporter username="kvncp">Kevin Peterson</reporter>
                        <labels>
                    </labels>
                <created>Wed, 26 Sep 2018 19:51:33 +0000</created>
                <updated>Sat, 16 May 2020 14:02:58 +0000</updated>
                            <resolved>Tue, 11 Dec 2018 22:00:08 +0000</resolved>
                                                    <fixVersion>2.10.0</fixVersion>
                                    <component>io-java-gcp</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>7</watches>
                                                    <progress percentage="100">
                                    <originalProgress>
                                                    <row percentage="0" backgroundColor="#89afd7"/>
                                                    <row percentage="100" backgroundColor="transparent"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="100" backgroundColor="#51a825"/>
                                                    <row percentage="0" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </progress>
                                    <aggregateprogress percentage="100">
                                    <originalProgress>
                                                    <row percentage="0" backgroundColor="#89afd7"/>
                                                    <row percentage="100" backgroundColor="transparent"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="100" backgroundColor="#51a825"/>
                                                    <row percentage="0" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </aggregateprogress>
                                            <timeestimate seconds="0">0h</timeestimate>
                            <timespent seconds="14400">4h</timespent>
                                <comments>
                            <comment id="16643834" author="chamikara" created="Tue, 9 Oct 2018 18:04:50 +0000"  >&lt;p&gt;I&apos;m trying to determine the priority at which this should be addressed.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=reuvenlax&quot; class=&quot;user-hover&quot; rel=&quot;reuvenlax&quot;&gt;reuvenlax&lt;/a&gt; any reason why&#160;we rely on workitems retries instead of retrying BQ streaming write requests with exponential backoff ?&lt;/p&gt;</comment>
                            <comment id="16643919" author="reuvenlax" created="Tue, 9 Oct 2018 18:31:09 +0000"  >&lt;p&gt;Is the problem simply that ApiErrorExtractor doesn&apos;t see quotaExceeded as a rate limit error? Appears that it currently looks for either rateLimitExceeded or userRateLimitExceeded.&lt;/p&gt;</comment>
                            <comment id="16644101" author="chamikara" created="Tue, 9 Oct 2018 21:13:57 +0000"  >&lt;p&gt;Thanks.&lt;/p&gt;

&lt;p&gt;I believe HTTP 403 issues in general are considered non-retriable. So it makes sense for Dataflow to not to retry requests at the client. In-fact BigQuery support page provides following instructions regarding HTTP 403 quotaExceeded errors.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://cloud.google.com/bigquery/troubleshooting-errors&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://cloud.google.com/bigquery/troubleshooting-errors&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&quot;View the&#160;&lt;tt&gt;message&lt;/tt&gt;&#160;property of the error object for more information about which quota was exceeded. To reset or raise a BigQuery quota,&#160;&lt;a href=&quot;https://cloud.google.com/support&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;contact support&lt;/a&gt;. To modify a custom quota, submit a request from the&#160;&lt;a href=&quot;https://console.cloud.google.com/iam-admin/quotas&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Google Cloud Platform Console&lt;/a&gt;&#160;page.&quot;&lt;/p&gt;

&lt;p&gt;So&#160;basically this is asking to fix the issue (request a quota increase) before retrying.&lt;/p&gt;

&lt;p&gt;The issues is, due to the architecture if Dataflow streaming jobs, even though we do not retry at the client, we do in fact retry all work items indefinitely.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;So we end up sending a large number of requests to BigQuery whenever a user hit quota errors.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16649288" author="reuvenlax" created="Sun, 14 Oct 2018 07:54:04 +0000"  >&lt;p&gt;streaming insert quota is a rate limit, not a quota. If a job temporarily exceeds this, I think it should retry the request with a backoff. If BQ fails those requests with a quota error, I think that might be incorrect behavior on BQ&apos;s part (as this type of error should be retryable).&lt;/p&gt;</comment>
                            <comment id="16649844" author="chamikara" created="Mon, 15 Oct 2018 07:30:33 +0000"  >&lt;p&gt;I agree that we should retry with backoff for rate limit errors (and we already do ?) Note that this is HTTP 503 not 403.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Seems like this bug was filed for HTTP 403 quota errors which is&#160;a&#160;more permanent error. Not exactly sure which quota was exceeded here though. BigQuery lists a number of streaming related quotas that can be increased through a request here:&#160;&lt;a href=&quot;https://cloud.google.com/bigquery/quotas&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://cloud.google.com/bigquery/quotas&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16650805" author="kvncp" created="Mon, 15 Oct 2018 21:16:50 +0000"  >&lt;p&gt;From the docs on that page, under the streaming inserts section, it specifies the failure as &quot;quotaExceeded&quot; for rate limit failures.&lt;/p&gt;

&lt;p&gt;In this particular case, I see both &quot;rateLimitExceeded&quot; errors and &quot;quotaExceeded&quot; errors in the logs. I&apos;m not 100% clear on when BQ sends which one, but my impression is that &quot;rateLimitExceeded&quot; is more short term, while &quot;quotaExceeded&quot; is longer term.&lt;/p&gt;

&lt;p&gt;The rate limit ones are retried within the client using backoff, while the others are retried via the worker retry mechanism (streaming pipeline, so forever), and the issue is that the worker mechanism doesn&apos;t do exponential backoff, so the retries happen much too quickly. In my observations, once you start getting quotaExceeded errors, you pretty much continue to get them instead of the rateLimitErrors - so the client never goes back into the backoff part of the loop and just fails/retries right away.&lt;/p&gt;

&lt;p&gt;I&apos;d say the current implementation is probably correct from an HTTP error codes standpoint, but it has the side effect of retrying errors much too quickly, which is not great for the BQ frontend. There are 3 solutions I can think of:&lt;/p&gt;

&lt;p&gt;1. Handle quotaExceeded within the client with a backoff retry.&lt;br/&gt;
2. Retry worker failure in dataflow with a backoff (maybe only for sinks?).&lt;br/&gt;
3. Do nothing, and rely on users to notice the errors and stop the pipeline until quotas are increased.&lt;/p&gt;

&lt;p&gt;I&apos;d advocate for #1.&lt;/p&gt;</comment>
                            <comment id="16665747" author="rangadi" created="Fri, 26 Oct 2018 23:13:52 +0000"  >&lt;p&gt;&amp;gt; 1. Handle quotaExceeded within the client with a backoff retry.&lt;br/&gt;
Agreed. Quota Excceeded should be treated same as &apos;Rate Limited&apos;. I think they are logically the same thing w.r.t BigQueryIO.&lt;br/&gt;
A note about worker retries : It does not do exponential backoff, but it does wait 10 seconds before re-running a failed bundle (&apos;work item&apos; in Dataflow terminology), which is actually quite high.&lt;/p&gt;

&lt;p&gt;The main issue is with the backoff mechanism itself. &apos;insertAll&apos; in BigQueryIO uses an unlimited thread pool execute each insert from a separate thread. There could be thousands of inserts in a bundle. The backoff is calculated for each insert independently.. so we could 1000 threads each backing of a bit.. which does not really help cut down the load.&lt;/p&gt;

&lt;p&gt;Over all we should control the over all rate (by reducing both the parallism and the frequency of retries within each thread). As such I think we could use a smaller pool to insert, but I am not sure what the right size is. A simple policy could be to multiply retry time by number active inserts : next_retry = backoff(num_retries) * num_active_inserts.&lt;/p&gt;



</comment>
                            <comment id="16665779" author="reuvenlax" created="Sat, 27 Oct 2018 00:04:02 +0000"  >&lt;p&gt;II would start off by simply handling all errors with the backoff instead of trying to special case the rate-limit error.&lt;/p&gt;</comment>
                            <comment id="16705305" author="heejong" created="Fri, 30 Nov 2018 21:34:38 +0000"  >&lt;p&gt;I think there are two different issues in this single ticket. The first issue is that quotaExceeded error is not properly handled and the second one is that the parallelism is not properly controlled so that it generates lots of backoff messages as well as unnecessary loads to the BigQuery backend.&lt;/p&gt;

&lt;p&gt;The first issue can be easily fixed by handling quotaExceeded error in the same way as rateLimitExceeded error. For the second issue, I would suggest that we can simply use SingleThreadExecutor instead of UnboundedThreadExecutor. It won&apos;t hurt the overall performance much since BigQuery.write already set up 50 shards before writing which means there will be up to 50 concurrent StreamingWriteFns if&#160;possible.&lt;/p&gt;</comment>
                            <comment id="16718073" author="heejong" created="Tue, 11 Dec 2018 22:00:08 +0000"  >&lt;p&gt;Fixed by handling all IOExceptions with backoff.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="13209756">BEAM-6443</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[no_permission]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 48 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3yjdz:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            </customfields>
    </item>
</channel>
</rss>