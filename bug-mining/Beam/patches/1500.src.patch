diff --git a/sdks/python/apache_beam/runners/dataflow/dataflow_runner.py b/sdks/python/apache_beam/runners/dataflow/dataflow_runner.py
index 718ab61bdbe..dcd76ac03d8 100644
--- a/sdks/python/apache_beam/runners/dataflow/dataflow_runner.py
+++ b/sdks/python/apache_beam/runners/dataflow/dataflow_runner.py
@@ -103,13 +103,18 @@ class DataflowRunner(PipelineRunner):
   from apache_beam.runners.dataflow.ptransform_overrides import ReadPTransformOverride
 
   _PTRANSFORM_OVERRIDES = [
-      CreatePTransformOverride(),
   ]
 
   _SDF_PTRANSFORM_OVERRIDES = [
       ReadPTransformOverride(),
   ]
 
+  # These overrides should be applied after the proto representation of the
+  # graph is created.
+  _NON_PORTABLE_PTRANSFORM_OVERRIDES = [
+      CreatePTransformOverride(),
+  ]
+
   def __init__(self, cache=None):
     # Cache of CloudWorkflowStep protos generated while the runner
     # "executes" a pipeline.
@@ -424,6 +429,11 @@ class DataflowRunner(PipelineRunner):
       self.proto_pipeline, self.proto_context = pipeline.to_runner_api(
           return_context=True, default_environment=default_environment)
 
+    else:
+      # Performing configured PTransform overrides which should not be reflected
+      # in the proto representation of the graph.
+      pipeline.replace_all(DataflowRunner._NON_PORTABLE_PTRANSFORM_OVERRIDES)
+
     # Add setup_options for all the BeamPlugin imports
     setup_options = options.view_as(SetupOptions)
     plugins = BeamPlugin.get_all_plugin_paths()
diff --git a/sdks/python/apache_beam/runners/dataflow/native_io/streaming_create.py b/sdks/python/apache_beam/runners/dataflow/native_io/streaming_create.py
deleted file mode 100644
index 481209ef90b..00000000000
--- a/sdks/python/apache_beam/runners/dataflow/native_io/streaming_create.py
+++ /dev/null
@@ -1,76 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#    http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-
-"""Create transform for streaming."""
-
-from __future__ import absolute_import
-
-from builtins import map
-
-from apache_beam import DoFn
-from apache_beam import ParDo
-from apache_beam import PTransform
-from apache_beam import Windowing
-from apache_beam import pvalue
-from apache_beam.transforms.window import GlobalWindows
-
-
-class StreamingCreate(PTransform):
-  """A specialized implementation for ``Create`` transform in streaming mode.
-
-  Note: There is no unbounded source API in python to wrap the Create source,
-  so we map this to composite of Impulse primitive and an SDF.
-  """
-
-  def __init__(self, values, coder):
-    self.coder = coder
-    self.encoded_values = list(map(coder.encode, values))
-
-  class DecodeAndEmitDoFn(DoFn):
-    """A DoFn which stores encoded versions of elements.
-
-    It also stores a Coder to decode and emit those elements.
-    TODO: BEAM-2422 - Make this a SplittableDoFn.
-    """
-
-    def __init__(self, encoded_values, coder):
-      self.encoded_values = encoded_values
-      self.coder = coder
-
-    def process(self, unused_element):
-      for encoded_value in self.encoded_values:
-        yield self.coder.decode(encoded_value)
-
-  class Impulse(PTransform):
-    """The Dataflow specific override for the impulse primitive."""
-
-    def expand(self, pbegin):
-      assert isinstance(pbegin, pvalue.PBegin), (
-          'Input to Impulse transform must be a PBegin but found %s' % pbegin)
-      return pvalue.PCollection(pbegin.pipeline, is_bounded=False)
-
-    def get_windowing(self, inputs):
-      return Windowing(GlobalWindows())
-
-    def infer_output_type(self, unused_input_type):
-      return bytes
-
-  def expand(self, pbegin):
-    return (pbegin
-            | 'Impulse' >> self.Impulse()
-            | 'Decode Values' >> ParDo(
-                self.DecodeAndEmitDoFn(self.encoded_values, self.coder)))
diff --git a/sdks/python/apache_beam/runners/dataflow/ptransform_overrides.py b/sdks/python/apache_beam/runners/dataflow/ptransform_overrides.py
index 6e84c15cbc8..be98a9c247a 100644
--- a/sdks/python/apache_beam/runners/dataflow/ptransform_overrides.py
+++ b/sdks/python/apache_beam/runners/dataflow/ptransform_overrides.py
@@ -19,7 +19,6 @@
 
 from __future__ import absolute_import
 
-from apache_beam.coders import typecoders
 from apache_beam.pipeline import PTransformOverride
 
 
@@ -30,24 +29,24 @@ class CreatePTransformOverride(PTransformOverride):
     # Imported here to avoid circular dependencies.
     # pylint: disable=wrong-import-order, wrong-import-position
     from apache_beam import Create
-    from apache_beam.options.pipeline_options import StandardOptions
+    from apache_beam.runners.dataflow.internal import apiclient
 
     if isinstance(applied_ptransform.transform, Create):
-      standard_options = (applied_ptransform
-                          .outputs[None]
-                          .pipeline._options
-                          .view_as(StandardOptions))
-      return standard_options.streaming
+      return not apiclient._use_fnapi(
+          applied_ptransform.outputs[None].pipeline._options)
     else:
       return False
 
   def get_replacement_transform(self, ptransform):
     # Imported here to avoid circular dependencies.
     # pylint: disable=wrong-import-order, wrong-import-position
-    from apache_beam.runners.dataflow.native_io.streaming_create import \
-      StreamingCreate
-    coder = typecoders.registry.get_coder(ptransform.get_output_type())
-    return StreamingCreate(ptransform.values, coder)
+    from apache_beam import PTransform
+    # Return a wrapper rather than ptransform.as_read() directly to
+    # ensure backwards compatibility of the pipeline structure.
+    class LegacyCreate(PTransform):
+      def expand(self, pbegin):
+        return pbegin | ptransform.as_read()
+    return LegacyCreate().with_output_types(ptransform.get_output_type())
 
 
 class ReadPTransformOverride(PTransformOverride):
diff --git a/sdks/python/apache_beam/transforms/core.py b/sdks/python/apache_beam/transforms/core.py
index 6c48b23bd79..e21e0d91bcb 100644
--- a/sdks/python/apache_beam/transforms/core.py
+++ b/sdks/python/apache_beam/transforms/core.py
@@ -38,7 +38,6 @@ from apache_beam import typehints
 from apache_beam.coders import typecoders
 from apache_beam.internal import pickler
 from apache_beam.internal import util
-from apache_beam.options.pipeline_options import DebugOptions
 from apache_beam.options.pipeline_options import TypeOptions
 from apache_beam.portability import common_urns
 from apache_beam.portability import python_urns
@@ -2507,38 +2506,32 @@ class Create(PTransform):
 
   def expand(self, pbegin):
     assert isinstance(pbegin, pvalue.PBegin)
-    # Must guard against this as some legacy runners don't implement impulse.
-    debug_options = pbegin.pipeline._options.view_as(DebugOptions)
-    fn_api = (debug_options.experiments
-              and 'beam_fn_api' in debug_options.experiments)
-    if fn_api:
-      coder = typecoders.registry.get_coder(self.get_output_type())
-      serialized_values = [coder.encode(v) for v in self.values]
-      reshuffle = self.reshuffle
-      # Avoid the "redistributing" reshuffle for 0 and 1 element Creates.
-      # These special cases are often used in building up more complex
-      # transforms (e.g. Write).
-
-      class MaybeReshuffle(PTransform):
-        def expand(self, pcoll):
-          if len(serialized_values) > 1 and reshuffle:
-            from apache_beam.transforms.util import Reshuffle
-            return pcoll | Reshuffle()
-          else:
-            return pcoll
-      return (
-          pbegin
-          | Impulse()
-          | FlatMap(lambda _: serialized_values)
-          | MaybeReshuffle()
-          | Map(coder.decode).with_output_types(self.get_output_type()))
-    else:
-      self.pipeline = pbegin.pipeline
-      from apache_beam.io import iobase
-      coder = typecoders.registry.get_coder(self.get_output_type())
-      source = self._create_source_from_iterable(self.values, coder)
-      return (pbegin.pipeline
-              | iobase.Read(source).with_output_types(self.get_output_type()))
+    coder = typecoders.registry.get_coder(self.get_output_type())
+    serialized_values = [coder.encode(v) for v in self.values]
+    reshuffle = self.reshuffle
+    # Avoid the "redistributing" reshuffle for 0 and 1 element Creates.
+    # These special cases are often used in building up more complex
+    # transforms (e.g. Write).
+
+    class MaybeReshuffle(PTransform):
+      def expand(self, pcoll):
+        if len(serialized_values) > 1 and reshuffle:
+          from apache_beam.transforms.util import Reshuffle
+          return pcoll | Reshuffle()
+        else:
+          return pcoll
+    return (
+        pbegin
+        | Impulse()
+        | FlatMap(lambda _: serialized_values)
+        | MaybeReshuffle()
+        | Map(coder.decode).with_output_types(self.get_output_type()))
+
+  def as_read(self):
+    from apache_beam.io import iobase
+    coder = typecoders.registry.get_coder(self.get_output_type())
+    source = self._create_source_from_iterable(self.values, coder)
+    return iobase.Read(source).with_output_types(self.get_output_type())
 
   def get_windowing(self, unused_inputs):
     return Windowing(GlobalWindows())
