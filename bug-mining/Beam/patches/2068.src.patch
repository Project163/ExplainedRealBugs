diff --git a/examples/java/src/main/java/org/apache/beam/examples/subprocess/ExampleEchoPipeline.java b/examples/java/src/main/java/org/apache/beam/examples/subprocess/ExampleEchoPipeline.java
index b2108ec5d5f..2ece169f829 100644
--- a/examples/java/src/main/java/org/apache/beam/examples/subprocess/ExampleEchoPipeline.java
+++ b/examples/java/src/main/java/org/apache/beam/examples/subprocess/ExampleEchoPipeline.java
@@ -44,7 +44,7 @@ import org.slf4j.LoggerFactory;
  * SubProcessPipelineOptions}
  */
 public class ExampleEchoPipeline {
-  static final Logger LOG = LoggerFactory.getLogger(ExampleEchoPipeline.class);
+  private static final Logger LOG = LoggerFactory.getLogger(ExampleEchoPipeline.class);
 
   public static void main(String[] args) throws Exception {
 
@@ -76,7 +76,7 @@ public class ExampleEchoPipeline {
   @SuppressWarnings("serial")
   public static class EchoInputDoFn extends DoFn<KV<String, String>, KV<String, String>> {
 
-    static final Logger LOG = LoggerFactory.getLogger(EchoInputDoFn.class);
+    private static final Logger LOG = LoggerFactory.getLogger(EchoInputDoFn.class);
 
     private SubProcessConfiguration configuration;
     private String binaryName;
diff --git a/examples/java/src/main/java/org/apache/beam/examples/subprocess/utils/CallingSubProcessUtils.java b/examples/java/src/main/java/org/apache/beam/examples/subprocess/utils/CallingSubProcessUtils.java
index a6b1fb4b12b..3acd1eed336 100644
--- a/examples/java/src/main/java/org/apache/beam/examples/subprocess/utils/CallingSubProcessUtils.java
+++ b/examples/java/src/main/java/org/apache/beam/examples/subprocess/utils/CallingSubProcessUtils.java
@@ -32,7 +32,7 @@ public class CallingSubProcessUtils {
   // Prevent Instantiation
   private CallingSubProcessUtils() {}
 
-  static final Logger LOG = LoggerFactory.getLogger(CallingSubProcessUtils.class);
+  private static final Logger LOG = LoggerFactory.getLogger(CallingSubProcessUtils.class);
 
   static boolean initCompleted = false;
 
diff --git a/examples/java/src/main/java/org/apache/beam/examples/subprocess/utils/ExecutableFile.java b/examples/java/src/main/java/org/apache/beam/examples/subprocess/utils/ExecutableFile.java
index e42af3e5e68..675d81a42bc 100644
--- a/examples/java/src/main/java/org/apache/beam/examples/subprocess/utils/ExecutableFile.java
+++ b/examples/java/src/main/java/org/apache/beam/examples/subprocess/utils/ExecutableFile.java
@@ -32,7 +32,7 @@ public class ExecutableFile {
   private String sourceGCSLocation;
   private String destinationLocation;
 
-  static final Logger LOG = LoggerFactory.getLogger(ExecutableFile.class);
+  private static final Logger LOG = LoggerFactory.getLogger(ExecutableFile.class);
 
   public String getSourceGCSLocation() {
     return sourceGCSLocation;
diff --git a/examples/java/src/test/java/org/apache/beam/examples/subprocess/ExampleEchoPipelineTest.java b/examples/java/src/test/java/org/apache/beam/examples/subprocess/ExampleEchoPipelineTest.java
index d7fd06ca5a6..5e00f49d588 100644
--- a/examples/java/src/test/java/org/apache/beam/examples/subprocess/ExampleEchoPipelineTest.java
+++ b/examples/java/src/test/java/org/apache/beam/examples/subprocess/ExampleEchoPipelineTest.java
@@ -63,7 +63,7 @@ import org.slf4j.LoggerFactory;
 @RunWith(JUnit4.class)
 public class ExampleEchoPipelineTest {
 
-  static final Logger LOG = LoggerFactory.getLogger(ExampleEchoPipelineTest.class);
+  private static final Logger LOG = LoggerFactory.getLogger(ExampleEchoPipelineTest.class);
 
   @Rule public TestPipeline p = TestPipeline.create().enableAbandonedNodeEnforcement(false);
 
@@ -128,7 +128,7 @@ public class ExampleEchoPipelineTest {
   @SuppressWarnings("serial")
   private static class EchoInputDoFn extends DoFn<KV<String, String>, KV<String, String>> {
 
-    static final Logger LOG = LoggerFactory.getLogger(EchoInputDoFn.class);
+    private static final Logger LOG = LoggerFactory.getLogger(EchoInputDoFn.class);
 
     private SubProcessConfiguration configuration;
     private String binaryName;
diff --git a/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/graph/GreedyPCollectionFusers.java b/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/graph/GreedyPCollectionFusers.java
index 23c9738a9cc..d004de63707 100644
--- a/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/graph/GreedyPCollectionFusers.java
+++ b/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/graph/GreedyPCollectionFusers.java
@@ -362,8 +362,7 @@ class GreedyPCollectionFusers {
         "Unknown {} {} will not fuse into an existing {}",
         PTransform.class.getSimpleName(),
         transform.getTransform(),
-        ExecutableStage.class.getSimpleName(),
-        PTransform.class.getSimpleName());
+        ExecutableStage.class.getSimpleName());
     return false;
   }
 
diff --git a/runners/flink/src/main/java/org/apache/beam/runners/flink/PipelineTranslationModeOptimizer.java b/runners/flink/src/main/java/org/apache/beam/runners/flink/PipelineTranslationModeOptimizer.java
index 4d90bc85dbd..bc62c144f42 100644
--- a/runners/flink/src/main/java/org/apache/beam/runners/flink/PipelineTranslationModeOptimizer.java
+++ b/runners/flink/src/main/java/org/apache/beam/runners/flink/PipelineTranslationModeOptimizer.java
@@ -63,7 +63,7 @@ class PipelineTranslationModeOptimizer extends FlinkPipelineTranslator {
     AppliedPTransform<?, ?, ?> appliedPTransform = node.toAppliedPTransform(getPipeline());
     if (hasUnboundedOutput(appliedPTransform)) {
       Class<? extends PTransform> transformClass = node.getTransform().getClass();
-      LOG.debug("Found unbounded PCollection for transform %s", transformClass);
+      LOG.debug("Found unbounded PCollection for transform {}", transformClass);
       hasUnboundedCollections = true;
     }
   }
diff --git a/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/SourceInputFormat.java b/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/SourceInputFormat.java
index b22fd8814d1..e64c6d322c9 100644
--- a/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/SourceInputFormat.java
+++ b/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/SourceInputFormat.java
@@ -98,7 +98,7 @@ public class SourceInputFormat<T> extends RichInputFormat<WindowedValue<T>, Sour
         }
       };
     } catch (Exception e) {
-      LOG.warn("Could not read Source statistics: {}", e);
+      LOG.warn("Could not read Source statistics.", e);
     }
 
     return null;
diff --git a/runners/flink/src/test/java/org/apache/beam/runners/flink/ReadSourcePortableTest.java b/runners/flink/src/test/java/org/apache/beam/runners/flink/ReadSourcePortableTest.java
index fb50ce3a2c0..2a240725e17 100644
--- a/runners/flink/src/test/java/org/apache/beam/runners/flink/ReadSourcePortableTest.java
+++ b/runners/flink/src/test/java/org/apache/beam/runners/flink/ReadSourcePortableTest.java
@@ -51,7 +51,7 @@ import org.slf4j.LoggerFactory;
 @RunWith(Parameterized.class)
 public class ReadSourcePortableTest implements Serializable {
 
-  private static final Logger LOG = LoggerFactory.getLogger(PortableExecutionTest.class);
+  private static final Logger LOG = LoggerFactory.getLogger(ReadSourcePortableTest.class);
 
   @Parameters(name = "streaming: {0}")
   public static Object[] data() {
diff --git a/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowPipelineJob.java b/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowPipelineJob.java
index f102b785815..72a80180a40 100644
--- a/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowPipelineJob.java
+++ b/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowPipelineJob.java
@@ -296,7 +296,7 @@ public class DataflowPipelineJob implements PipelineResult {
       } catch (IOException e) {
         exception = e;
         LOG.warn("Failed to get job state: {}", e.getMessage());
-        LOG.debug("Failed to get job state: {}", e);
+        LOG.debug("Failed to get job state.", e);
         continue;
       }
 
@@ -323,7 +323,7 @@ public class DataflowPipelineJob implements PipelineResult {
     if (exception == null) {
       LOG.warn("No terminal state was returned within allotted timeout. State value {}", state);
     } else {
-      LOG.error("Failed to fetch job metadata with error: {}", exception);
+      LOG.error("Failed to fetch job metadata.", exception);
     }
 
     return null;
@@ -392,7 +392,7 @@ public class DataflowPipelineJob implements PipelineResult {
         }
       } catch (GoogleJsonResponseException | SocketTimeoutException e) {
         LOG.warn("Failed to get job messages: {}", e.getMessage());
-        LOG.debug("Failed to get job messages: {}", e);
+        LOG.debug("Failed to get job messages.", e);
         return e;
       }
     }
diff --git a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/BatchDataflowWorker.java b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/BatchDataflowWorker.java
index 9031ce089d6..2780c503687 100644
--- a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/BatchDataflowWorker.java
+++ b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/BatchDataflowWorker.java
@@ -443,7 +443,7 @@ public class BatchDataflowWorker implements Closeable {
     try {
       this.memoryMonitorThread.join(timeoutMilliSec);
     } catch (InterruptedException ex) {
-      LOG.warn("Failed to wait for monitor thread to exit. Ex: {}", ex);
+      LOG.warn("Failed to wait for monitor thread to exit.", ex);
     }
     if (this.memoryMonitorThread.isAlive()) {
       LOG.warn("memoryMonitorThread didn't exit. Please, check for potential memory leaks.");
diff --git a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/DataflowWorkerHarnessHelper.java b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/DataflowWorkerHarnessHelper.java
index f66869a558c..bda38b3757d 100644
--- a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/DataflowWorkerHarnessHelper.java
+++ b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/DataflowWorkerHarnessHelper.java
@@ -77,6 +77,7 @@ public final class DataflowWorkerHarnessHelper {
     return pipelineOptions;
   }
 
+  @SuppressWarnings("Slf4jIllegalPassedClass")
   public static void initializeLogging(Class<?> workerHarnessClass) {
     /* Set up exception handling tied to the workerHarnessClass. */
     Thread.setDefaultUncaughtExceptionHandler(
diff --git a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/HotKeyLogger.java b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/HotKeyLogger.java
index 212f231756d..473c89b0d5f 100644
--- a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/HotKeyLogger.java
+++ b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/HotKeyLogger.java
@@ -25,7 +25,7 @@ import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 public class HotKeyLogger {
-  Logger LOG = LoggerFactory.getLogger(HotKeyLogger.class);
+  private final Logger LOG = LoggerFactory.getLogger(HotKeyLogger.class);
 
   /** Clock used to either provide real system time or mocked to virtualize time for testing. */
   private Clock clock = Clock.SYSTEM;
diff --git a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorker.java b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorker.java
index 49adda0bc32..ce60eb3e146 100644
--- a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorker.java
+++ b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorker.java
@@ -861,7 +861,7 @@ public class StreamingDataflowWorker {
                   writer = new PrintWriter(outputFile, UTF_8.name());
                   page.captureData(writer);
                 } catch (IOException e) {
-                  LOG.warn("Error dumping status page., {}", e);
+                  LOG.warn("Error dumping status page.", e);
                 } finally {
                   if (writer != null) {
                     writer.close();
diff --git a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/fn/control/ElementCountMonitoringInfoToCounterUpdateTransformer.java b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/fn/control/ElementCountMonitoringInfoToCounterUpdateTransformer.java
index d5947348180..c10cac521b5 100644
--- a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/fn/control/ElementCountMonitoringInfoToCounterUpdateTransformer.java
+++ b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/fn/control/ElementCountMonitoringInfoToCounterUpdateTransformer.java
@@ -39,7 +39,8 @@ import org.slf4j.LoggerFactory;
 public class ElementCountMonitoringInfoToCounterUpdateTransformer
     implements MonitoringInfoToCounterUpdateTransformer {
 
-  private static final Logger LOG = LoggerFactory.getLogger(BeamFnMapTaskExecutor.class);
+  private static final Logger LOG =
+      LoggerFactory.getLogger(ElementCountMonitoringInfoToCounterUpdateTransformer.class);
 
   private final SpecMonitoringInfoValidator specValidator;
   private final Map<String, NameContext> pcollectionIdToNameContext;
diff --git a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/fn/control/ExecutionTimeMonitoringInfoToCounterUpdateTransformer.java b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/fn/control/ExecutionTimeMonitoringInfoToCounterUpdateTransformer.java
index 6015940a842..05cb9f7ee62 100644
--- a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/fn/control/ExecutionTimeMonitoringInfoToCounterUpdateTransformer.java
+++ b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/fn/control/ExecutionTimeMonitoringInfoToCounterUpdateTransformer.java
@@ -46,7 +46,8 @@ import org.slf4j.LoggerFactory;
 public class ExecutionTimeMonitoringInfoToCounterUpdateTransformer
     implements MonitoringInfoToCounterUpdateTransformer {
 
-  private static final Logger LOG = LoggerFactory.getLogger(BeamFnMapTaskExecutor.class);
+  private static final Logger LOG =
+      LoggerFactory.getLogger(ExecutionTimeMonitoringInfoToCounterUpdateTransformer.class);
 
   private SpecMonitoringInfoValidator specValidator;
   private Map<String, DataflowStepContext> transformIdMapping;
diff --git a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/fn/control/MeanByteCountMonitoringInfoToCounterUpdateTransformer.java b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/fn/control/MeanByteCountMonitoringInfoToCounterUpdateTransformer.java
index 796aab128f0..ecd2482b9b7 100644
--- a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/fn/control/MeanByteCountMonitoringInfoToCounterUpdateTransformer.java
+++ b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/fn/control/MeanByteCountMonitoringInfoToCounterUpdateTransformer.java
@@ -41,7 +41,8 @@ import org.slf4j.LoggerFactory;
 public class MeanByteCountMonitoringInfoToCounterUpdateTransformer
     implements MonitoringInfoToCounterUpdateTransformer {
 
-  private static final Logger LOG = LoggerFactory.getLogger(BeamFnMapTaskExecutor.class);
+  private static final Logger LOG =
+      LoggerFactory.getLogger(MeanByteCountMonitoringInfoToCounterUpdateTransformer.class);
 
   private final SpecMonitoringInfoValidator specValidator;
   private final Map<String, NameContext> pcollectionIdToNameContext;
diff --git a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/fn/control/UserDistributionMonitoringInfoToCounterUpdateTransformer.java b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/fn/control/UserDistributionMonitoringInfoToCounterUpdateTransformer.java
index bc0cbf462c4..652a7712811 100644
--- a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/fn/control/UserDistributionMonitoringInfoToCounterUpdateTransformer.java
+++ b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/fn/control/UserDistributionMonitoringInfoToCounterUpdateTransformer.java
@@ -47,7 +47,8 @@ import org.slf4j.LoggerFactory;
 class UserDistributionMonitoringInfoToCounterUpdateTransformer
     implements MonitoringInfoToCounterUpdateTransformer {
 
-  private static final Logger LOG = LoggerFactory.getLogger(BeamFnMapTaskExecutor.class);
+  private static final Logger LOG =
+      LoggerFactory.getLogger(UserDistributionMonitoringInfoToCounterUpdateTransformer.class);
 
   private final Map<String, DataflowStepContext> transformIdMapping;
 
diff --git a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/fn/control/UserMonitoringInfoToCounterUpdateTransformer.java b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/fn/control/UserMonitoringInfoToCounterUpdateTransformer.java
index 8fce58b1da1..a02a1dfc9c9 100644
--- a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/fn/control/UserMonitoringInfoToCounterUpdateTransformer.java
+++ b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/fn/control/UserMonitoringInfoToCounterUpdateTransformer.java
@@ -45,7 +45,8 @@ import org.slf4j.LoggerFactory;
 class UserMonitoringInfoToCounterUpdateTransformer
     implements MonitoringInfoToCounterUpdateTransformer {
 
-  private static final Logger LOG = LoggerFactory.getLogger(BeamFnMapTaskExecutor.class);
+  private static final Logger LOG =
+      LoggerFactory.getLogger(UserMonitoringInfoToCounterUpdateTransformer.class);
 
   private final Map<String, DataflowStepContext> transformIdMapping;
 
diff --git a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/fn/logging/BeamFnLoggingService.java b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/fn/logging/BeamFnLoggingService.java
index feb05c24e44..e705dec7e66 100644
--- a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/fn/logging/BeamFnLoggingService.java
+++ b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/fn/logging/BeamFnLoggingService.java
@@ -133,7 +133,7 @@ public class BeamFnLoggingService extends BeamFnLoggingGrpc.BeamFnLoggingImplBas
 
     @Override
     public void onError(Throwable t) {
-      LOG.warn("Logging client failed unexpectedly. ClientId: {}", t, sdkWorkerId);
+      LOG.warn("Logging client failed unexpectedly. ClientId: {}", sdkWorkerId, t);
       // We remove these from the connected clients map to prevent a race between
       // the close method and this InboundObserver calling a terminal method on the
       // StreamObserver. If we removed it, then we are responsible for the terminal call.
diff --git a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/windmill/GrpcWindmillServer.java b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/windmill/GrpcWindmillServer.java
index f71aa3903cb..c0423127a42 100644
--- a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/windmill/GrpcWindmillServer.java
+++ b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/windmill/GrpcWindmillServer.java
@@ -701,7 +701,7 @@ public class GrpcWindmillServer extends WindmillServerStub {
         try {
           sendHealthCheck();
         } catch (RuntimeException e) {
-          LOG.debug("Received exception sending health check {}", e);
+          LOG.debug("Received exception sending health check.", e);
         }
       }
     }
@@ -1226,7 +1226,7 @@ public class GrpcWindmillServer extends WindmillServerStub {
           send(batchedRequest);
         } catch (IllegalStateException e) {
           // The stream broke before this call went through; onNewStream will retry the fetch.
-          LOG.warn("GetData stream broke before call started {}", e);
+          LOG.warn("GetData stream broke before call started.", e);
         }
       }
     }
@@ -1361,7 +1361,7 @@ public class GrpcWindmillServer extends WindmillServerStub {
           } catch (RuntimeException e) {
             // Catch possible exceptions to ensure that an exception for one commit does not prevent
             // other commits from being processed.
-            LOG.warn("Exception while processing commit response {} ", e);
+            LOG.warn("Exception while processing commit response.", e);
             finalException = e;
           }
         }
diff --git a/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/HotKeyLoggerTest.java b/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/HotKeyLoggerTest.java
index 2728e266184..0f205847888 100644
--- a/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/HotKeyLoggerTest.java
+++ b/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/HotKeyLoggerTest.java
@@ -85,6 +85,7 @@ public class HotKeyLoggerTest {
   }
 
   @Test
+  @SuppressWarnings("Slf4jIllegalPassedClass")
   public void logsHotKeyMessage() {
     mockStatic(LoggerFactory.class);
     Logger logger = mock(Logger.class);
diff --git a/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/windmill/GrpcWindmillServerTest.java b/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/windmill/GrpcWindmillServerTest.java
index 53aa470d090..f0d9f822a9d 100644
--- a/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/windmill/GrpcWindmillServerTest.java
+++ b/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/windmill/GrpcWindmillServerTest.java
@@ -81,7 +81,7 @@ import org.slf4j.LoggerFactory;
 /** Unit tests for {@link GrpcWindmillServer}. */
 @RunWith(JUnit4.class)
 public class GrpcWindmillServerTest {
-  private static final Logger LOG = LoggerFactory.getLogger(GrpcWindmillServer.class);
+  private static final Logger LOG = LoggerFactory.getLogger(GrpcWindmillServerTest.class);
 
   private final MutableHandlerRegistry serviceRegistry = new MutableHandlerRegistry();
   @Rule public ErrorCollector errorCollector = new ErrorCollector();
diff --git a/runners/java-fn-execution/src/main/java/org/apache/beam/runners/fnexecution/control/FnApiControlClient.java b/runners/java-fn-execution/src/main/java/org/apache/beam/runners/fnexecution/control/FnApiControlClient.java
index 11ea3ab77b4..1694cb303bb 100644
--- a/runners/java-fn-execution/src/main/java/org/apache/beam/runners/fnexecution/control/FnApiControlClient.java
+++ b/runners/java-fn-execution/src/main/java/org/apache/beam/runners/fnexecution/control/FnApiControlClient.java
@@ -189,7 +189,7 @@ public class FnApiControlClient implements Closeable, InstructionRequestHandler
 
     @Override
     public void onError(Throwable cause) {
-      LOG.error("{} received error {}", FnApiControlClient.class.getSimpleName(), cause);
+      LOG.error("{} received an error.", FnApiControlClient.class.getSimpleName(), cause);
       closeAndTerminateOutstandingRequests(cause);
     }
   }
diff --git a/runners/java-fn-execution/src/main/java/org/apache/beam/runners/fnexecution/status/WorkerStatusClient.java b/runners/java-fn-execution/src/main/java/org/apache/beam/runners/fnexecution/status/WorkerStatusClient.java
index eacb3fc5d5a..0d892e518eb 100644
--- a/runners/java-fn-execution/src/main/java/org/apache/beam/runners/fnexecution/status/WorkerStatusClient.java
+++ b/runners/java-fn-execution/src/main/java/org/apache/beam/runners/fnexecution/status/WorkerStatusClient.java
@@ -39,7 +39,7 @@ import org.slf4j.LoggerFactory;
  */
 class WorkerStatusClient implements Closeable {
 
-  public static final Logger LOG = LoggerFactory.getLogger(WorkerStatusClient.class);
+  private static final Logger LOG = LoggerFactory.getLogger(WorkerStatusClient.class);
   private final IdGenerator idGenerator = IdGenerators.incrementingLongs();
   private final StreamObserver<WorkerStatusRequest> requestReceiver;
   private final Map<String, CompletableFuture<WorkerStatusResponse>> pendingResponses =
@@ -149,7 +149,7 @@ class WorkerStatusClient implements Closeable {
 
     @Override
     public void onError(Throwable throwable) {
-      LOG.error("{} received error {}", WorkerStatusClient.class.getSimpleName(), throwable);
+      LOG.error("{} received error.", WorkerStatusClient.class.getSimpleName(), throwable);
       onCompleted();
     }
 
diff --git a/runners/java-job-service/src/main/java/org/apache/beam/runners/jobsubmission/InMemoryJobService.java b/runners/java-job-service/src/main/java/org/apache/beam/runners/jobsubmission/InMemoryJobService.java
index cc1dc1d2ae4..e27d56d3bef 100644
--- a/runners/java-job-service/src/main/java/org/apache/beam/runners/jobsubmission/InMemoryJobService.java
+++ b/runners/java-job-service/src/main/java/org/apache/beam/runners/jobsubmission/InMemoryJobService.java
@@ -259,9 +259,7 @@ public class InMemoryJobService extends JobServiceGrpc.JobServiceImplBase implem
               }
             } catch (Exception e) {
               LOG.warn(
-                  "Failed to remove job staging directory for token {}: {}",
-                  stagingSessionToken,
-                  e);
+                  "Failed to remove job staging directory for token {}.", stagingSessionToken, e);
             } finally {
               onFinishedInvocationCleanup(invocationId);
             }
diff --git a/runners/jet/src/main/java/org/apache/beam/runners/jet/JetPipelineResult.java b/runners/jet/src/main/java/org/apache/beam/runners/jet/JetPipelineResult.java
index 2fa514200c0..09fd3a35e51 100644
--- a/runners/jet/src/main/java/org/apache/beam/runners/jet/JetPipelineResult.java
+++ b/runners/jet/src/main/java/org/apache/beam/runners/jet/JetPipelineResult.java
@@ -40,7 +40,7 @@ import org.slf4j.LoggerFactory;
 /** Jet specific implementation of {@link PipelineResult}. */
 public class JetPipelineResult implements PipelineResult {
 
-  private static final Logger LOG = LoggerFactory.getLogger(JetRunner.class);
+  private static final Logger LOG = LoggerFactory.getLogger(JetPipelineResult.class);
 
   private final Job job;
   private final JetMetricResults metricResults;
diff --git a/runners/portability/java/src/main/java/org/apache/beam/runners/portability/ExternalWorkerService.java b/runners/portability/java/src/main/java/org/apache/beam/runners/portability/ExternalWorkerService.java
index 363b01310e0..c4311a385e5 100644
--- a/runners/portability/java/src/main/java/org/apache/beam/runners/portability/ExternalWorkerService.java
+++ b/runners/portability/java/src/main/java/org/apache/beam/runners/portability/ExternalWorkerService.java
@@ -34,7 +34,7 @@ import org.slf4j.LoggerFactory;
  */
 public class ExternalWorkerService extends BeamFnExternalWorkerPoolImplBase implements FnService {
 
-  private static final Logger LOG = LoggerFactory.getLogger(BeamFnExternalWorkerPoolImplBase.class);
+  private static final Logger LOG = LoggerFactory.getLogger(ExternalWorkerService.class);
 
   private final PipelineOptions options;
   private final ServerFactory serverFactory = ServerFactory.createDefault();
diff --git a/runners/samza/src/main/java/org/apache/beam/runners/samza/container/BeamContainerRunner.java b/runners/samza/src/main/java/org/apache/beam/runners/samza/container/BeamContainerRunner.java
index be7aa4ffaf5..d49c23ffea7 100644
--- a/runners/samza/src/main/java/org/apache/beam/runners/samza/container/BeamContainerRunner.java
+++ b/runners/samza/src/main/java/org/apache/beam/runners/samza/container/BeamContainerRunner.java
@@ -34,7 +34,7 @@ import org.slf4j.LoggerFactory;
 
 /** Runs the beam Yarn container, using the static global job model. */
 public class BeamContainerRunner implements ApplicationRunner {
-  private static final Logger LOG = LoggerFactory.getLogger(ContainerCfgFactory.class);
+  private static final Logger LOG = LoggerFactory.getLogger(BeamContainerRunner.class);
 
   private final ApplicationDescriptorImpl<? extends ApplicationDescriptor> appDesc;
 
diff --git a/runners/samza/src/main/java/org/apache/beam/runners/samza/translation/TranslationContext.java b/runners/samza/src/main/java/org/apache/beam/runners/samza/translation/TranslationContext.java
index a6333829d96..fc0d4a70548 100644
--- a/runners/samza/src/main/java/org/apache/beam/runners/samza/translation/TranslationContext.java
+++ b/runners/samza/src/main/java/org/apache/beam/runners/samza/translation/TranslationContext.java
@@ -66,7 +66,7 @@ import org.slf4j.LoggerFactory;
  * PTransform}.
  */
 public class TranslationContext {
-  public static final Logger LOG = LoggerFactory.getLogger(TranslationContext.class);
+  private static final Logger LOG = LoggerFactory.getLogger(TranslationContext.class);
   private final StreamApplicationDescriptor appDescriptor;
   private final Map<PValue, MessageStream<?>> messsageStreams = new HashMap<>();
   private final Map<PCollectionView<?>, MessageStream<?>> viewStreams = new HashMap<>();
diff --git a/sdks/java/core/src/main/java/org/apache/beam/sdk/io/FileBasedSink.java b/sdks/java/core/src/main/java/org/apache/beam/sdk/io/FileBasedSink.java
index 4db00ed9254..37d83c78798 100644
--- a/sdks/java/core/src/main/java/org/apache/beam/sdk/io/FileBasedSink.java
+++ b/sdks/java/core/src/main/java/org/apache/beam/sdk/io/FileBasedSink.java
@@ -943,7 +943,6 @@ public abstract class FileBasedSink<UserT, DestinationT, OutputT>
 
       // The caller shouldn't have to close() this Writer if it fails to open(), so close
       // the channel if prepareWrite() or writeHeader() fails.
-      String step = "";
       try {
         LOG.debug("Preparing write to {}.", outputFile);
         prepareWrite(channel);
@@ -951,7 +950,7 @@ public abstract class FileBasedSink<UserT, DestinationT, OutputT>
         LOG.debug("Writing header to {}.", outputFile);
         writeHeader();
       } catch (Exception e) {
-        LOG.error("Beginning write to {} failed, closing channel.", step, outputFile, e);
+        LOG.error("Beginning write to {} failed, closing channel.", outputFile, e);
         closeChannelAndThrow(channel, outputFile, e);
       }
 
diff --git a/sdks/java/core/src/main/java/org/apache/beam/sdk/io/LocalFileSystem.java b/sdks/java/core/src/main/java/org/apache/beam/sdk/io/LocalFileSystem.java
index 25b7c47e5de..5a201e89ab4 100644
--- a/sdks/java/core/src/main/java/org/apache/beam/sdk/io/LocalFileSystem.java
+++ b/sdks/java/core/src/main/java/org/apache/beam/sdk/io/LocalFileSystem.java
@@ -195,7 +195,7 @@ class LocalFileSystem extends FileSystem<LocalResourceId> {
         Files.delete(resourceId.getPath());
       } catch (NoSuchFileException e) {
         LOG.info(
-            "Ignoring failed deletion of file {} which already does not exist: {}", resourceId, e);
+            "Ignoring failed deletion of file {} which already does not exist.", resourceId, e);
       }
     }
   }
diff --git a/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/join/CoGbkResult.java b/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/join/CoGbkResult.java
index 9b7a3264808..d559d401bce 100644
--- a/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/join/CoGbkResult.java
+++ b/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/join/CoGbkResult.java
@@ -104,10 +104,8 @@ public class CoGbkResult {
       // keep in memory, so we copy the re-iterable of remaining items
       // and append filtered views to each of the sorted lists computed earlier.
       LOG.info(
-          "CoGbkResult has more than "
-              + inMemoryElementCount
-              + " elements,"
-              + " reiteration (which may be slow) is required.");
+          "CoGbkResult has more than {} elements, reiteration (which may be slow) is required.",
+          inMemoryElementCount);
       final Reiterator<RawUnionValue> tail = (Reiterator<RawUnionValue>) taggedIter;
       // This is a trinary-state array recording whether a given tag is present in the tail. The
       // initial value is null (unknown) for all tags, and the first iteration through the entire
diff --git a/sdks/java/extensions/google-cloud-platform-core/src/main/java/org/apache/beam/sdk/extensions/gcp/options/GcpOptions.java b/sdks/java/extensions/google-cloud-platform-core/src/main/java/org/apache/beam/sdk/extensions/gcp/options/GcpOptions.java
index 722ab2c7a2a..cf358e4c901 100644
--- a/sdks/java/extensions/google-cloud-platform-core/src/main/java/org/apache/beam/sdk/extensions/gcp/options/GcpOptions.java
+++ b/sdks/java/extensions/google-cloud-platform-core/src/main/java/org/apache/beam/sdk/extensions/gcp/options/GcpOptions.java
@@ -295,7 +295,7 @@ public interface GcpOptions extends GoogleApiDebugOptions, PipelineOptions {
     private static final FluentBackoff BACKOFF_FACTORY =
         FluentBackoff.DEFAULT.withMaxRetries(3).withInitialBackoff(Duration.millis(200));
     static final String DEFAULT_REGION = "us-central1";
-    static final Logger LOG = LoggerFactory.getLogger(GcpTempLocationFactory.class);
+    private static final Logger LOG = LoggerFactory.getLogger(GcpTempLocationFactory.class);
 
     @Override
     public @Nullable String create(PipelineOptions options) {
diff --git a/sdks/java/extensions/ml/src/main/java/org/apache/beam/sdk/extensions/ml/BatchRequestForDLP.java b/sdks/java/extensions/ml/src/main/java/org/apache/beam/sdk/extensions/ml/BatchRequestForDLP.java
index aabac4e0fcd..a42b9c21092 100644
--- a/sdks/java/extensions/ml/src/main/java/org/apache/beam/sdk/extensions/ml/BatchRequestForDLP.java
+++ b/sdks/java/extensions/ml/src/main/java/org/apache/beam/sdk/extensions/ml/BatchRequestForDLP.java
@@ -40,7 +40,7 @@ import org.slf4j.LoggerFactory;
 /** Batches input rows to reduce number of requests sent to Cloud DLP service. */
 @Experimental
 class BatchRequestForDLP extends DoFn<KV<String, Table.Row>, KV<String, Iterable<Table.Row>>> {
-  public static final Logger LOG = LoggerFactory.getLogger(BatchRequestForDLP.class);
+  private static final Logger LOG = LoggerFactory.getLogger(BatchRequestForDLP.class);
 
   private final Counter numberOfRowsBagged =
       Metrics.counter(BatchRequestForDLP.class, "numberOfRowsBagged");
diff --git a/sdks/java/harness/src/main/java/org/apache/beam/fn/harness/state/BeamFnStateGrpcClientCache.java b/sdks/java/harness/src/main/java/org/apache/beam/fn/harness/state/BeamFnStateGrpcClientCache.java
index bb4a661c6bc..313cf625fee 100644
--- a/sdks/java/harness/src/main/java/org/apache/beam/fn/harness/state/BeamFnStateGrpcClientCache.java
+++ b/sdks/java/harness/src/main/java/org/apache/beam/fn/harness/state/BeamFnStateGrpcClientCache.java
@@ -23,7 +23,6 @@ import java.util.concurrent.CompletableFuture;
 import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.ConcurrentMap;
 import java.util.function.Function;
-import org.apache.beam.fn.harness.data.BeamFnDataGrpcClient;
 import org.apache.beam.model.fnexecution.v1.BeamFnApi.StateRequest;
 import org.apache.beam.model.fnexecution.v1.BeamFnApi.StateResponse;
 import org.apache.beam.model.fnexecution.v1.BeamFnStateGrpc;
@@ -42,7 +41,7 @@ import org.slf4j.LoggerFactory;
  * <p>TODO: Add the ability to close which cancels any pending and stops any future requests.
  */
 public class BeamFnStateGrpcClientCache {
-  private static final Logger LOG = LoggerFactory.getLogger(BeamFnDataGrpcClient.class);
+  private static final Logger LOG = LoggerFactory.getLogger(BeamFnStateGrpcClientCache.class);
 
   private final ConcurrentMap<ApiServiceDescriptor, BeamFnStateClient> cache;
   private final Function<ApiServiceDescriptor, ManagedChannel> channelFactory;
diff --git a/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIO.java b/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIO.java
index e253f8e1e1c..f25d6ba61b5 100644
--- a/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIO.java
+++ b/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIO.java
@@ -495,7 +495,7 @@ public final class DynamoDBIO {
                   || !spec.getRetryConfiguration().getRetryPredicate().test(ex)) {
                 DYNAMO_DB_WRITE_FAILURES.inc();
                 LOG.info(
-                    "Unable to write batch items {} due to {} ",
+                    "Unable to write batch items {}.",
                     batchRequest.getRequestItems().entrySet(),
                     ex);
                 throw new IOException("Error writing to DynamoDB (no attempt made to retry)", ex);
diff --git a/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/sns/SnsIO.java b/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/sns/SnsIO.java
index 59bc8f727bb..a7dfecaab77 100644
--- a/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/sns/SnsIO.java
+++ b/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/sns/SnsIO.java
@@ -353,7 +353,7 @@ public final class SnsIO {
             if (spec.getRetryConfiguration() == null
                 || !spec.getRetryConfiguration().getRetryPredicate().test(ex)) {
               SNS_WRITE_FAILURES.inc();
-              LOG.info("Unable to publish message {} due to {} ", request.getMessage(), ex);
+              LOG.info("Unable to publish message {}.", request.getMessage(), ex);
               throw new IOException("Error writing to SNS (no attempt made to retry)", ex);
             }
 
diff --git a/sdks/java/io/amazon-web-services2/src/main/java/org/apache/beam/sdk/io/aws2/dynamodb/DynamoDBIO.java b/sdks/java/io/amazon-web-services2/src/main/java/org/apache/beam/sdk/io/aws2/dynamodb/DynamoDBIO.java
index d8c795cfec3..857b1646e7f 100644
--- a/sdks/java/io/amazon-web-services2/src/main/java/org/apache/beam/sdk/io/aws2/dynamodb/DynamoDBIO.java
+++ b/sdks/java/io/amazon-web-services2/src/main/java/org/apache/beam/sdk/io/aws2/dynamodb/DynamoDBIO.java
@@ -510,9 +510,7 @@ public final class DynamoDBIO {
                   || !spec.getRetryConfiguration().getRetryPredicate().test(ex)) {
                 DYNAMO_DB_WRITE_FAILURES.inc();
                 LOG.info(
-                    "Unable to write batch items {} due to {} ",
-                    batchRequest.requestItems().entrySet(),
-                    ex);
+                    "Unable to write batch items {}.", batchRequest.requestItems().entrySet(), ex);
                 throw new IOException("Error writing to DynamoDB (no attempt made to retry)", ex);
               }
 
diff --git a/sdks/java/io/amazon-web-services2/src/main/java/org/apache/beam/sdk/io/aws2/sns/SnsIO.java b/sdks/java/io/amazon-web-services2/src/main/java/org/apache/beam/sdk/io/aws2/sns/SnsIO.java
index 7ce109328f0..0ff02f5b8b5 100644
--- a/sdks/java/io/amazon-web-services2/src/main/java/org/apache/beam/sdk/io/aws2/sns/SnsIO.java
+++ b/sdks/java/io/amazon-web-services2/src/main/java/org/apache/beam/sdk/io/aws2/sns/SnsIO.java
@@ -379,7 +379,7 @@ public final class SnsIO {
             if (spec.getRetryConfiguration() == null
                 || !spec.getRetryConfiguration().getRetryPredicate().test(ex)) {
               SNS_WRITE_FAILURES.inc();
-              LOG.info("Unable to publish message {} due to {} ", request.message(), ex);
+              LOG.info("Unable to publish message {}.", request.message(), ex);
               throw new IOException("Error writing to SNS (no attempt made to retry)", ex);
             }
 
diff --git a/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BatchLoads.java b/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BatchLoads.java
index 96773df864b..b39399e5dd2 100644
--- a/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BatchLoads.java
+++ b/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BatchLoads.java
@@ -80,7 +80,7 @@ import org.slf4j.LoggerFactory;
 /** PTransform that uses BigQuery batch-load jobs to write a PCollection to BigQuery. */
 class BatchLoads<DestinationT, ElementT>
     extends PTransform<PCollection<KV<DestinationT, ElementT>>, WriteResult> {
-  static final Logger LOG = LoggerFactory.getLogger(BatchLoads.class);
+  private static final Logger LOG = LoggerFactory.getLogger(BatchLoads.class);
 
   // The maximum number of file writers to keep open in a single bundle at a time, since file
   // writers default to 64mb buffers. This comes into play when writing dynamic table destinations.
diff --git a/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryHelpers.java b/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryHelpers.java
index 5ba95406f78..1f00723799a 100644
--- a/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryHelpers.java
+++ b/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryHelpers.java
@@ -205,7 +205,7 @@ public class BigQueryHelpers {
         this.started = false;
         executeJob.apply(currentJobId);
       } catch (RuntimeException e) {
-        LOG.warn("Job {} failed with {}", currentJobId.getJobId(), e);
+        LOG.warn("Job {} failed.", currentJobId.getJobId(), e);
         // It's possible that the job actually made it to BQ even though we got a failure here.
         // For example, the response from BQ may have timed out returning. getRetryJobId will
         // return the correct job id to use on retry, or a job id to continue polling (if it turns
diff --git a/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/WriteRename.java b/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/WriteRename.java
index 9b80f5fa751..5ceeceff293 100644
--- a/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/WriteRename.java
+++ b/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/WriteRename.java
@@ -210,7 +210,7 @@ class WriteRename extends DoFn<Iterable<KV<TableDestination, String>>, Void> {
               try {
                 jobService.startCopyJob(jobRef, copyConfig);
               } catch (IOException | InterruptedException e) {
-                LOG.warn("Copy job {} failed with {}", jobRef, e);
+                LOG.warn("Copy job {} failed.", jobRef, e);
                 throw new RuntimeException(e);
               }
               return null;
diff --git a/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigtable/BigtableServiceImpl.java b/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigtable/BigtableServiceImpl.java
index 9cd47aa039d..478de53cc63 100644
--- a/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigtable/BigtableServiceImpl.java
+++ b/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigtable/BigtableServiceImpl.java
@@ -58,7 +58,7 @@ import org.slf4j.LoggerFactory;
  * service.
  */
 class BigtableServiceImpl implements BigtableService {
-  private static final Logger LOG = LoggerFactory.getLogger(BigtableService.class);
+  private static final Logger LOG = LoggerFactory.getLogger(BigtableServiceImpl.class);
 
   public BigtableServiceImpl(BigtableOptions options) {
     this.options = options;
diff --git a/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/datastore/EntityToRow.java b/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/datastore/EntityToRow.java
index b86323c1c56..11e320e51e9 100644
--- a/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/datastore/EntityToRow.java
+++ b/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/datastore/EntityToRow.java
@@ -39,7 +39,7 @@ import org.slf4j.LoggerFactory;
 public class EntityToRow extends PTransform<PCollection<Entity>, PCollection<Row>> {
   private final Schema schema;
   private final String keyField;
-  private static final Logger LOG = LoggerFactory.getLogger(DataStoreV1SchemaIOProvider.class);
+  private static final Logger LOG = LoggerFactory.getLogger(EntityToRow.class);
 
   private EntityToRow(Schema schema, String keyField) {
     this.schema = schema;
diff --git a/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/datastore/RowToEntity.java b/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/datastore/RowToEntity.java
index 02ca67ce43c..39dc27f16ce 100644
--- a/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/datastore/RowToEntity.java
+++ b/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/datastore/RowToEntity.java
@@ -45,7 +45,7 @@ public class RowToEntity extends PTransform<PCollection<Row>, PCollection<Entity
   private final Supplier<String> keySupplier;
   private final String kind;
   private final String keyField;
-  private static final Logger LOG = LoggerFactory.getLogger(DataStoreV1SchemaIOProvider.class);
+  private static final Logger LOG = LoggerFactory.getLogger(RowToEntity.class);
 
   private RowToEntity(Supplier<String> keySupplier, String kind, String keyField) {
     this.keySupplier = keySupplier;
diff --git a/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/pubsub/PubsubUnboundedSource.java b/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/pubsub/PubsubUnboundedSource.java
index 8fc8c7f1650..eeb9216e9ce 100644
--- a/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/pubsub/PubsubUnboundedSource.java
+++ b/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/pubsub/PubsubUnboundedSource.java
@@ -1068,7 +1068,7 @@ public class PubsubUnboundedSource extends PTransform<PBegin, PCollection<Pubsub
           checkpoint.nackAll(reader);
         } catch (IOException e) {
           LOG.error(
-              "Pubsub {} cannot have {} lost messages NACKed, ignoring: {}",
+              "Pubsub {} cannot have {} lost messages NACKed, ignoring exception.",
               subscriptionPath,
               checkpoint.notYetReadIds.size(),
               e);
diff --git a/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaUnboundedReader.java b/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaUnboundedReader.java
index 6d9a8035b11..72329b90026 100644
--- a/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaUnboundedReader.java
+++ b/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaUnboundedReader.java
@@ -284,7 +284,7 @@ class KafkaUnboundedReader<K, V> extends UnboundedReader<KafkaRecord<K, V>> {
 
   ////////////////////////////////////////////////////////////////////////////////////////////////
 
-  private static final Logger LOG = LoggerFactory.getLogger(KafkaUnboundedSource.class);
+  private static final Logger LOG = LoggerFactory.getLogger(KafkaUnboundedReader.class);
 
   @VisibleForTesting static final String METRIC_NAMESPACE = "KafkaIOReader";
 
diff --git a/sdks/java/io/kudu/src/test/java/org/apache/beam/sdk/io/kudu/KuduIOTest.java b/sdks/java/io/kudu/src/test/java/org/apache/beam/sdk/io/kudu/KuduIOTest.java
index 13db17b51f1..86d60e0768a 100644
--- a/sdks/java/io/kudu/src/test/java/org/apache/beam/sdk/io/kudu/KuduIOTest.java
+++ b/sdks/java/io/kudu/src/test/java/org/apache/beam/sdk/io/kudu/KuduIOTest.java
@@ -202,7 +202,7 @@ public class KuduIOTest {
     @Override
     public void close() {
       // called on teardown which give no guarantees
-      LOG.debug("FakeWriter[{}] close {}", id);
+      LOG.debug("FakeWriter[{}] closed.", id);
     }
 
     /** Sets the unique id on deserialzation using the shared counter. */
diff --git a/sdks/java/testing/nexmark/src/main/java/org/apache/beam/sdk/nexmark/NexmarkUtils.java b/sdks/java/testing/nexmark/src/main/java/org/apache/beam/sdk/nexmark/NexmarkUtils.java
index a12c1101c76..f6b56fdb4a5 100644
--- a/sdks/java/testing/nexmark/src/main/java/org/apache/beam/sdk/nexmark/NexmarkUtils.java
+++ b/sdks/java/testing/nexmark/src/main/java/org/apache/beam/sdk/nexmark/NexmarkUtils.java
@@ -461,7 +461,7 @@ public class NexmarkUtils {
         new DoFn<T, T>() {
           @ProcessElement
           public void processElement(ProcessContext c) {
-            LOG.info("%s: %s", name, c.element());
+            LOG.info("{}: {}", name, c.element());
             c.output(c.element());
           }
         });
diff --git a/sdks/java/testing/nexmark/src/main/java/org/apache/beam/sdk/nexmark/queries/Query10.java b/sdks/java/testing/nexmark/src/main/java/org/apache/beam/sdk/nexmark/queries/Query10.java
index 67d72c9262d..c48e95d55e4 100644
--- a/sdks/java/testing/nexmark/src/main/java/org/apache/beam/sdk/nexmark/queries/Query10.java
+++ b/sdks/java/testing/nexmark/src/main/java/org/apache/beam/sdk/nexmark/queries/Query10.java
@@ -185,7 +185,7 @@ public class Query10 extends NexmarkQueryTransform<Done> {
                   public void processElement(ProcessContext c) {
                     if (c.element().hasAnnotation("LATE")) {
                       lateCounter.inc();
-                      LOG.info("Observed late: %s", c.element());
+                      LOG.info("Observed late: {}", c.element());
                     } else {
                       onTimeCounter.inc();
                     }
@@ -249,20 +249,21 @@ public class Query10 extends NexmarkQueryTransform<Done> {
                             window.maxTimestamp()));
                     if (c.pane().getTiming() == PaneInfo.Timing.LATE) {
                       if (numLate == 0) {
-                        LOG.error("ERROR! No late events in late pane for %s", shard);
+                        LOG.error("ERROR! No late events in late pane for {}", shard);
                         unexpectedLatePaneCounter.inc();
                       }
                       if (numOnTime > 0) {
                         LOG.error(
-                            "ERROR! Have %d on-time events in late pane for %s", numOnTime, shard);
+                            "ERROR! Have {} on-time events in late pane for {}", numOnTime, shard);
                         unexpectedOnTimeElementCounter.inc();
                       }
                       lateCounter.inc();
                     } else if (c.pane().getTiming() == PaneInfo.Timing.EARLY) {
                       if (numOnTime + numLate < configuration.maxLogEvents) {
                         LOG.error(
-                            "ERROR! Only have %d events in early pane for %s",
-                            numOnTime + numLate, shard);
+                            "ERROR! Only have {} events in early pane for {}",
+                            numOnTime + numLate,
+                            shard);
                       }
                       earlyCounter.inc();
                     } else {
@@ -290,7 +291,7 @@ public class Query10 extends NexmarkQueryTransform<Done> {
                             "Writing %s with record timestamp %s, window timestamp %s, pane %s",
                             shard, c.timestamp(), window.maxTimestamp(), c.pane()));
                     if (outputFile.filename != null) {
-                      LOG.info("Beginning write to '%s'", outputFile.filename);
+                      LOG.info("Beginning write to '{}'", outputFile.filename);
                       int n = 0;
                       try (OutputStream output =
                           Channels.newOutputStream(
@@ -299,11 +300,11 @@ public class Query10 extends NexmarkQueryTransform<Done> {
                           Event.CODER.encode(event, output, Coder.Context.OUTER);
                           writtenRecordsCounter.inc();
                           if (++n % 10000 == 0) {
-                            LOG.info("So far written %d records to '%s'", n, outputFile.filename);
+                            LOG.info("So far written {} records to '{}'", n, outputFile.filename);
                           }
                         }
                       }
-                      LOG.info("Written all %d records to '%s'", n, outputFile.filename);
+                      LOG.info("Written all {} records to '{}'", n, outputFile.filename);
                     }
                     savedFileCounter.inc();
                     c.output(KV.of(null, outputFile));
@@ -337,22 +338,24 @@ public class Query10 extends NexmarkQueryTransform<Done> {
                       throws IOException {
                     if (c.pane().getTiming() == Timing.LATE) {
                       unexpectedLateCounter.inc();
-                      LOG.error("ERROR! Unexpected LATE pane: %s", c.pane());
+                      LOG.error("ERROR! Unexpected LATE pane: {}", c.pane());
                     } else if (c.pane().getTiming() == Timing.EARLY) {
                       unexpectedEarlyCounter.inc();
-                      LOG.error("ERROR! Unexpected EARLY pane: %s", c.pane());
+                      LOG.error("ERROR! Unexpected EARLY pane: {}", c.pane());
                     } else if (c.pane().getTiming() == Timing.ON_TIME && c.pane().getIndex() != 0) {
                       unexpectedIndexCounter.inc();
-                      LOG.error("ERROR! Unexpected ON_TIME pane index: %s", c.pane());
+                      LOG.error("ERROR! Unexpected ON_TIME pane index: {}", c.pane());
                     } else {
                       GcsOptions options = c.getPipelineOptions().as(GcsOptions.class);
                       LOG.info(
-                          "Index with record timestamp %s, window timestamp %s, pane %s",
-                          c.timestamp(), window.maxTimestamp(), c.pane());
+                          "Index with record timestamp {}, window timestamp {}, pane {}",
+                          c.timestamp(),
+                          window.maxTimestamp(),
+                          c.pane());
 
                       @Nullable String filename = indexPathFor(window);
                       if (filename != null) {
-                        LOG.info("Beginning write to '%s'", filename);
+                        LOG.info("Beginning write to '{}'", filename);
                         int n = 0;
                         try (OutputStream output =
                             Channels.newOutputStream(openWritableGcsFile(options, filename))) {
@@ -361,7 +364,7 @@ public class Query10 extends NexmarkQueryTransform<Done> {
                             n++;
                           }
                         }
-                        LOG.info("Written all %d lines to '%s'", n, filename);
+                        LOG.info("Written all {} lines to '{}'", n, filename);
                       }
                       c.output(new Done("written for timestamp " + window.maxTimestamp()));
                       finalizedCounter.inc();
