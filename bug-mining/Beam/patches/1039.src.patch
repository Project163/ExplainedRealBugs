diff --git a/runners/flink/1.6/build.gradle b/runners/flink/1.6/build.gradle
index 1792389e7db..1fdbca9838c 100644
--- a/runners/flink/1.6/build.gradle
+++ b/runners/flink/1.6/build.gradle
@@ -23,7 +23,7 @@ project.ext {
   // Set the version of all Flink-related dependencies here.
   flink_version = '1.6.2'
   // Look for the source code in the parent module
-  main_source_dirs = ["$basePath/src/main/java"]
+  main_source_dirs = ["$basePath/src/main/java", './src/main/java']
   test_source_dirs = ["$basePath/src/test/java"]
   main_resources_dirs = ["$basePath/src/main/resources"]
   test_resources_dirs = ["$basePath/src/test/resources"]
diff --git a/runners/flink/1.6/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/state/custom/AllKeyStateFunction.java b/runners/flink/1.6/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/state/custom/AllKeyStateFunction.java
new file mode 100644
index 00000000000..870573911c3
--- /dev/null
+++ b/runners/flink/1.6/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/state/custom/AllKeyStateFunction.java
@@ -0,0 +1,42 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.beam.runners.flink.translation.wrappers.streaming.state.custom;
+
+import org.apache.flink.api.common.state.ListState;
+import org.apache.flink.runtime.state.KeyedStateFunction;
+
+/** Flink 1.6 specific KeyedStateFunction which extracts all BagState values for all keys. */
+public class AllKeyStateFunction<K, StateT extends ListState>
+    implements KeyedStateFunction<K, StateT> {
+
+  private final StateConsumer<StateT> stateConsumer;
+
+  public AllKeyStateFunction(StateConsumer<StateT> stateConsumer) {
+    this.stateConsumer = stateConsumer;
+  }
+
+  @Override
+  public void process(K key, StateT state) throws Exception {
+    stateConsumer.consume(state);
+  }
+
+  /** A consumer of state which may throw an exception. */
+  public interface StateConsumer<StateT> {
+    void consume(StateT state) throws Exception;
+  }
+}
diff --git a/runners/flink/1.6/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/state/custom/package-info.java b/runners/flink/1.6/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/state/custom/package-info.java
new file mode 100644
index 00000000000..326e66c06f4
--- /dev/null
+++ b/runners/flink/1.6/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/state/custom/package-info.java
@@ -0,0 +1,20 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** Flink version-specific state implementations. */
+package org.apache.beam.runners.flink.translation.wrappers.streaming.state.custom;
diff --git a/runners/flink/1.7/build.gradle b/runners/flink/1.7/build.gradle
index 5982c1b2f84..c7680626962 100644
--- a/runners/flink/1.7/build.gradle
+++ b/runners/flink/1.7/build.gradle
@@ -23,7 +23,7 @@ project.ext {
   // Set the version of all Flink-related dependencies here.
   flink_version = '1.7.1'
   // Look for the source code in the parent module
-  main_source_dirs = ["$basePath/src/main/java"]
+  main_source_dirs = ["$basePath/src/main/java", './src/main/java']
   test_source_dirs = ["$basePath/src/test/java"]
   main_resources_dirs = ["$basePath/src/main/resources"]
   test_resources_dirs = ["$basePath/src/test/resources"]
diff --git a/runners/flink/1.7/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/state/custom/AllKeyStateFunction.java b/runners/flink/1.7/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/state/custom/AllKeyStateFunction.java
new file mode 100644
index 00000000000..d3c7dd93d74
--- /dev/null
+++ b/runners/flink/1.7/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/state/custom/AllKeyStateFunction.java
@@ -0,0 +1,42 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.beam.runners.flink.translation.wrappers.streaming.state.custom;
+
+import org.apache.flink.api.common.state.ListState;
+import org.apache.flink.runtime.state.KeyedStateFunction;
+
+/** Flink 1.7 specific KeyedStateFunction. */
+public class AllKeyStateFunction<K, StateT extends ListState>
+    implements KeyedStateFunction<K, StateT> {
+
+  private final StateConsumer<StateT> stateConsumer;
+
+  public AllKeyStateFunction(StateConsumer<StateT> stateConsumer) {
+    this.stateConsumer = stateConsumer;
+  }
+
+  @Override
+  public void process(K key, StateT state) throws Exception {
+    stateConsumer.consume(state);
+  }
+
+  /** A consumer of state which may throw an exception. */
+  public interface StateConsumer<StateT> {
+    void consume(StateT state) throws Exception;
+  }
+}
diff --git a/runners/flink/1.7/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/state/custom/package-info.java b/runners/flink/1.7/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/state/custom/package-info.java
new file mode 100644
index 00000000000..ea7757a2700
--- /dev/null
+++ b/runners/flink/1.7/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/state/custom/package-info.java
@@ -0,0 +1,21 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** Internal state implementation of the Beam runner for Apache Flink. */
+/** Flink version-specific state implementations. */
+package org.apache.beam.runners.flink.translation.wrappers.streaming.state.custom;
diff --git a/runners/flink/build.gradle b/runners/flink/build.gradle
index 39addc39840..d290d201c04 100644
--- a/runners/flink/build.gradle
+++ b/runners/flink/build.gradle
@@ -21,7 +21,7 @@ project.ext {
   // Set the version of all Flink-related dependencies here.
   flink_version = '1.5.6'
   // Look for the source code in the current module
-  main_source_dirs = ['./src/main/java']
+  main_source_dirs = ['./src/main/java', './src/1.5/main/java']
   test_source_dirs = ['./src/test/java']
   main_resources_dirs = ['./src/main/resources']
   test_resources_dirs = ['./src/test/resources']
diff --git a/runners/flink/src/1.5/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/state/custom/AllKeyStateFunction.java b/runners/flink/src/1.5/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/state/custom/AllKeyStateFunction.java
new file mode 100644
index 00000000000..8d58580b633
--- /dev/null
+++ b/runners/flink/src/1.5/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/state/custom/AllKeyStateFunction.java
@@ -0,0 +1,42 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.beam.runners.flink.translation.wrappers.streaming.state.custom;
+
+import org.apache.flink.api.common.state.ListState;
+import org.apache.flink.runtime.state.KeyedStateFunction;
+
+/** Flink 1.5 specific KeyedStateFunction. */
+public class AllKeyStateFunction<K, StateT extends ListState>
+    extends KeyedStateFunction<K, StateT> {
+
+  private final StateConsumer<StateT> stateConsumer;
+
+  public AllKeyStateFunction(StateConsumer<StateT> stateConsumer) {
+    this.stateConsumer = stateConsumer;
+  }
+
+  @Override
+  public void process(K key, StateT state) throws Exception {
+    stateConsumer.consume(state);
+  }
+
+  /** A consumer of state which may throw an exception. */
+  public interface StateConsumer<StateT> {
+    void consume(StateT state) throws Exception;
+  }
+}
diff --git a/runners/flink/src/1.5/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/state/custom/package-info.java b/runners/flink/src/1.5/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/state/custom/package-info.java
new file mode 100644
index 00000000000..ea7757a2700
--- /dev/null
+++ b/runners/flink/src/1.5/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/state/custom/package-info.java
@@ -0,0 +1,21 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** Internal state implementation of the Beam runner for Apache Flink. */
+/** Flink version-specific state implementations. */
+package org.apache.beam.runners.flink.translation.wrappers.streaming.state.custom;
diff --git a/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/DoFnOperator.java b/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/DoFnOperator.java
index a52b5094b89..9e5d06cdf97 100644
--- a/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/DoFnOperator.java
+++ b/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/DoFnOperator.java
@@ -173,8 +173,6 @@ public class DoFnOperator<InputT, OutputT> extends AbstractStreamOperator<Window
 
   protected transient FlinkTimerInternals timerInternals;
 
-  private transient StateInternals nonKeyedStateInternals;
-
   private transient long pushedBackWatermark;
 
   private transient PushedBackElementsHandler<WindowedValue<InputT>> pushedBackElementsHandler;
@@ -300,16 +298,6 @@ public class DoFnOperator<InputT, OutputT> extends AbstractStreamOperator<Window
 
     sideInputReader = NullSideInputReader.of(sideInputs);
 
-    // maybe init by initializeState
-    if (nonKeyedStateInternals == null) {
-      if (keyCoder != null) {
-        nonKeyedStateInternals =
-            new FlinkKeyGroupStateInternals<>(keyCoder, getKeyedStateBackend());
-      } else {
-        nonKeyedStateInternals = new FlinkSplitStateInternals<>(getOperatorStateBackend());
-      }
-    }
-
     if (!sideInputs.isEmpty()) {
 
       FlinkBroadcastStateInternals sideInputStateInternals =
@@ -327,7 +315,14 @@ public class DoFnOperator<InputT, OutputT> extends AbstractStreamOperator<Window
       setPushedBackWatermark(Long.MAX_VALUE);
     }
 
-    outputManager = outputManagerFactory.create(output, nonKeyedStateInternals);
+    final StateInternals outputManagerStateInternals;
+    if (keyCoder != null) {
+      outputManagerStateInternals =
+          new FlinkKeyGroupStateInternals<>(keyCoder, getKeyedStateBackend());
+    } else {
+      outputManagerStateInternals = new FlinkSplitStateInternals<>(getOperatorStateBackend());
+    }
+    outputManager = outputManagerFactory.create(output, outputManagerStateInternals);
 
     // StatefulPardo or WindowDoFn
     if (keyCoder != null) {
@@ -441,7 +436,7 @@ public class DoFnOperator<InputT, OutputT> extends AbstractStreamOperator<Window
     }
 
     // sanity check: these should have been flushed out by +Inf watermarks
-    if (!sideInputs.isEmpty() && nonKeyedStateInternals != null) {
+    if (!sideInputs.isEmpty()) {
 
       List<WindowedValue<InputT>> pushedBackElements =
           pushedBackElementsHandler.getElements().collect(Collectors.toList());
@@ -689,13 +684,18 @@ public class DoFnOperator<InputT, OutputT> extends AbstractStreamOperator<Window
 
   @Override
   public void snapshotState(StateSnapshotContext context) throws Exception {
-
     // Forced finish a bundle in checkpoint barrier otherwise may lose data.
     // Careful, it use OperatorState or KeyGroupState to store outputs, So it
     // must be called before their snapshot.
-    outputManager.openBuffer();
-    invokeFinishBundle();
-    outputManager.closeBuffer();
+    // If keyed state is used, must only be done if a key has already been set
+    // by a previous element. If there are no previous elements the active key
+    // is null and we can't buffer elements in finalizeBundle.
+    // TODO Move this to prepareSnapshotPreBarrier when we drop Flink 1.5 support
+    if (getKeyedStateBackend() == null || getKeyedStateBackend().getCurrentKey() != null) {
+      outputManager.openBuffer();
+      invokeFinishBundle();
+      outputManager.closeBuffer();
+    }
 
     super.snapshotState(context);
   }
diff --git a/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/state/FlinkKeyGroupStateInternals.java b/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/state/FlinkKeyGroupStateInternals.java
index 01026db5b56..f73d4e336a7 100644
--- a/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/state/FlinkKeyGroupStateInternals.java
+++ b/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/state/FlinkKeyGroupStateInternals.java
@@ -17,77 +17,54 @@
  */
 package org.apache.beam.runners.flink.translation.wrappers.streaming.state;
 
-import static org.apache.flink.util.Preconditions.checkArgument;
-
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
 import java.nio.ByteBuffer;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
+import java.util.ArrayDeque;
+import java.util.Deque;
+import java.util.Iterator;
+import javax.annotation.Nonnull;
 import org.apache.beam.runners.core.StateInternals;
 import org.apache.beam.runners.core.StateNamespace;
 import org.apache.beam.runners.core.StateTag;
+import org.apache.beam.runners.flink.translation.types.CoderTypeSerializer;
+import org.apache.beam.runners.flink.translation.wrappers.streaming.state.custom.AllKeyStateFunction;
+import org.apache.beam.runners.flink.translation.wrappers.streaming.state.custom.AllKeyStateFunction.StateConsumer;
 import org.apache.beam.sdk.coders.Coder;
 import org.apache.beam.sdk.coders.CoderException;
-import org.apache.beam.sdk.coders.ListCoder;
-import org.apache.beam.sdk.coders.StringUtf8Coder;
 import org.apache.beam.sdk.state.BagState;
 import org.apache.beam.sdk.state.CombiningState;
 import org.apache.beam.sdk.state.MapState;
 import org.apache.beam.sdk.state.ReadableState;
 import org.apache.beam.sdk.state.SetState;
 import org.apache.beam.sdk.state.State;
+import org.apache.beam.sdk.state.StateBinder;
 import org.apache.beam.sdk.state.StateContext;
+import org.apache.beam.sdk.state.StateSpec;
 import org.apache.beam.sdk.state.ValueState;
 import org.apache.beam.sdk.state.WatermarkHoldState;
 import org.apache.beam.sdk.transforms.Combine;
 import org.apache.beam.sdk.transforms.CombineWithContext;
 import org.apache.beam.sdk.transforms.windowing.TimestampCombiner;
 import org.apache.beam.sdk.util.CoderUtils;
-import org.apache.flink.api.java.tuple.Tuple2;
-import org.apache.flink.runtime.state.KeyGroupsList;
+import org.apache.flink.api.common.state.ListState;
+import org.apache.flink.api.common.state.ListStateDescriptor;
+import org.apache.flink.api.common.typeutils.base.StringSerializer;
 import org.apache.flink.runtime.state.KeyedStateBackend;
-import org.apache.flink.util.InstantiationUtil;
 import org.apache.flink.util.Preconditions;
 
 /**
- * {@link StateInternals} that uses {@link KeyGroupCheckpointedOperator} to checkpoint state.
- *
- * <p>Note: Ignore index of key. Just implement BagState.
- *
- * <p>Reference from Flink's HeapInternalTimerService to the local key-group range.
+ * {@link StateInternals} which uses Flink's KeyedStateBackend to store BagState which retrieves and
+ * clears state for _all_ of its keys when using the read method.
  */
 public class FlinkKeyGroupStateInternals<K> implements StateInternals {
 
   private final Coder<K> keyCoder;
-  private final KeyGroupsList localKeyGroupRange;
   private KeyedStateBackend keyedStateBackend;
-  private final int localKeyGroupRangeStartIdx;
-
-  // stateName -> namespace -> (valueCoder, value)
-  private final Map<String, Tuple2<Coder<?>, Map<String, ?>>>[] stateTables;
 
   public FlinkKeyGroupStateInternals(Coder<K> keyCoder, KeyedStateBackend keyedStateBackend) {
     this.keyCoder = Preconditions.checkNotNull(keyCoder, "Coder for key must be provided.");
     this.keyedStateBackend =
         Preconditions.checkNotNull(
             keyedStateBackend, "KeyedStateBackend must not be null. Missing keyBy call?");
-    this.localKeyGroupRange = keyedStateBackend.getKeyGroupRange();
-    // find the starting index of the local key-group range
-    int startIdx = Integer.MAX_VALUE;
-    for (Integer keyGroupIdx : localKeyGroupRange) {
-      startIdx = Math.min(keyGroupIdx, startIdx);
-    }
-    this.localKeyGroupRangeStartIdx = startIdx;
-    stateTables =
-        (Map<String, Tuple2<Coder<?>, Map<String, ?>>>[])
-            new Map[localKeyGroupRange.getNumberOfKeyGroups()];
-    for (int i = 0; i < stateTables.length; i++) {
-      stateTables[i] = new HashMap<>();
-    }
   }
 
   @Override
@@ -106,217 +83,103 @@ public class FlinkKeyGroupStateInternals<K> implements StateInternals {
   @Override
   public <T extends State> T state(
       final StateNamespace namespace, StateTag<T> address, final StateContext<?> context) {
-
-    return address.bind(
-        new StateTag.StateBinder() {
-
-          @Override
-          public <T2> ValueState<T2> bindValue(StateTag<ValueState<T2>> address, Coder<T2> coder) {
-            throw new UnsupportedOperationException(
-                String.format("%s is not supported", ValueState.class.getSimpleName()));
-          }
-
-          @Override
-          public <T2> BagState<T2> bindBag(StateTag<BagState<T2>> address, Coder<T2> elemCoder) {
-
-            return new FlinkKeyGroupBagState<>(address, namespace, elemCoder);
-          }
-
-          @Override
-          public <T2> SetState<T2> bindSet(StateTag<SetState<T2>> address, Coder<T2> elemCoder) {
-            throw new UnsupportedOperationException(
-                String.format("%s is not supported", SetState.class.getSimpleName()));
-          }
-
-          @Override
-          public <KeyT, ValueT> MapState<KeyT, ValueT> bindMap(
-              StateTag<MapState<KeyT, ValueT>> spec,
-              Coder<KeyT> mapKeyCoder,
-              Coder<ValueT> mapValueCoder) {
-            throw new UnsupportedOperationException(
-                String.format("%s is not supported", MapState.class.getSimpleName()));
-          }
-
-          @Override
-          public <InputT, AccumT, OutputT>
-              CombiningState<InputT, AccumT, OutputT> bindCombiningValue(
-                  StateTag<CombiningState<InputT, AccumT, OutputT>> address,
-                  Coder<AccumT> accumCoder,
-                  Combine.CombineFn<InputT, AccumT, OutputT> combineFn) {
-            throw new UnsupportedOperationException("bindCombiningValue is not supported.");
-          }
-
-          @Override
-          public <InputT, AccumT, OutputT>
-              CombiningState<InputT, AccumT, OutputT> bindCombiningValueWithContext(
-                  StateTag<CombiningState<InputT, AccumT, OutputT>> address,
-                  Coder<AccumT> accumCoder,
-                  CombineWithContext.CombineFnWithContext<InputT, AccumT, OutputT> combineFn) {
-            throw new UnsupportedOperationException(
-                "bindCombiningValueWithContext is not supported.");
-          }
-
-          @Override
-          public WatermarkHoldState bindWatermark(
-              StateTag<WatermarkHoldState> address, TimestampCombiner timestampCombiner) {
-            throw new UnsupportedOperationException(
-                String.format("%s is not supported", CombiningState.class.getSimpleName()));
-          }
-        });
-  }
-
-  /**
-   * Reference from {@link Combine.CombineFn}.
-   *
-   * <p>Accumulators are stored in each KeyGroup, call addInput() when a element comes, call
-   * extractOutput() to produce the desired value when need to read data.
-   */
-  interface KeyGroupCombiner<InputT, AccumT, OutputT> {
-
-    /**
-     * Returns a new, mutable accumulator value, representing the accumulation of zero input values.
-     */
-    AccumT createAccumulator();
-
-    /** Adds the given input value to the given accumulator, returning the new accumulator value. */
-    AccumT addInput(AccumT accumulator, InputT input);
-
-    /**
-     * Returns the output value that is the result of all accumulators from KeyGroups that are
-     * assigned to this operator.
-     */
-    OutputT extractOutput(Iterable<AccumT> accumulators);
-  }
-
-  private abstract class AbstractKeyGroupState<InputT, AccumT, OutputT> {
-
-    private String stateName;
-    private String namespace;
-    private Coder<AccumT> coder;
-    private KeyGroupCombiner<InputT, AccumT, OutputT> keyGroupCombiner;
-
-    AbstractKeyGroupState(
-        String stateName,
-        String namespace,
-        Coder<AccumT> coder,
-        KeyGroupCombiner<InputT, AccumT, OutputT> keyGroupCombiner) {
-      this.stateName = stateName;
-      this.namespace = namespace;
-      this.coder = coder;
-      this.keyGroupCombiner = keyGroupCombiner;
-    }
-
-    /** Choose keyGroup of input and addInput to accumulator. */
-    void addInput(InputT input) {
-      int keyGroupIdx = keyedStateBackend.getCurrentKeyGroupIndex();
-      int localIdx = getIndexForKeyGroup(keyGroupIdx);
-      Map<String, Tuple2<Coder<?>, Map<String, ?>>> stateTable = stateTables[localIdx];
-      Tuple2<Coder<?>, Map<String, ?>> tuple2 = stateTable.get(stateName);
-      if (tuple2 == null) {
-        tuple2 = new Tuple2<>();
-        tuple2.f0 = coder;
-        tuple2.f1 = new HashMap<>();
-        stateTable.put(stateName, tuple2);
-      }
-      Map<String, AccumT> map = (Map<String, AccumT>) tuple2.f1;
-      AccumT accumulator = map.get(namespace);
-      if (accumulator == null) {
-        accumulator = keyGroupCombiner.createAccumulator();
-      }
-      accumulator = keyGroupCombiner.addInput(accumulator, input);
-      map.put(namespace, accumulator);
-    }
-
-    /** Get all accumulators and invoke extractOutput(). */
-    OutputT extractOutput() {
-      List<AccumT> accumulators = new ArrayList<>(stateTables.length);
-      for (Map<String, Tuple2<Coder<?>, Map<String, ?>>> stateTable : stateTables) {
-        Tuple2<Coder<?>, Map<String, ?>> tuple2 = stateTable.get(stateName);
-        if (tuple2 != null) {
-          AccumT accumulator = (AccumT) tuple2.f1.get(namespace);
-          if (accumulator != null) {
-            accumulators.add(accumulator);
-          }
-        }
-      }
-      return keyGroupCombiner.extractOutput(accumulators);
-    }
-
-    /** Find the first accumulator and return immediately. */
-    boolean isEmptyInternal() {
-      for (Map<String, Tuple2<Coder<?>, Map<String, ?>>> stateTable : stateTables) {
-        Tuple2<Coder<?>, Map<String, ?>> tuple2 = stateTable.get(stateName);
-        if (tuple2 != null) {
-          AccumT accumulator = (AccumT) tuple2.f1.get(namespace);
-          if (accumulator != null) {
-            return false;
-          }
-        }
-      }
-      return true;
-    }
-
-    /** Clear accumulators and clean empty map. */
-    void clearInternal() {
-      for (Map<String, Tuple2<Coder<?>, Map<String, ?>>> stateTable : stateTables) {
-        Tuple2<Coder<?>, Map<String, ?>> tuple2 = stateTable.get(stateName);
-        if (tuple2 != null) {
-          tuple2.f1.remove(namespace);
-          if (tuple2.f1.isEmpty()) {
-            stateTable.remove(stateName);
-          }
-        }
-      }
-    }
-  }
-
-  private int getIndexForKeyGroup(int keyGroupIdx) {
-    checkArgument(
-        localKeyGroupRange.contains(keyGroupIdx),
-        "Key Group " + keyGroupIdx + " does not belong to the local range.");
-    return keyGroupIdx - this.localKeyGroupRangeStartIdx;
+    return address
+        .getSpec()
+        .bind(
+            address.getId(),
+            new StateBinder() {
+              @Override
+              public <T2> ValueState<T2> bindValue(
+                  String id, StateSpec<ValueState<T2>> spec, Coder<T2> coder) {
+                throw new UnsupportedOperationException(
+                    String.format("%s is not supported", ValueState.class.getSimpleName()));
+              }
+
+              @Override
+              public <T2> BagState<T2> bindBag(
+                  String id, StateSpec<BagState<T2>> spec, Coder<T2> elemCoder) {
+                // Only implement bag state
+                return new FlinkKeyGroupBagState<>(keyedStateBackend, id, namespace, elemCoder);
+              }
+
+              @Override
+              public <T2> SetState<T2> bindSet(
+                  String id, StateSpec<SetState<T2>> spec, Coder<T2> elemCoder) {
+                throw new UnsupportedOperationException(
+                    String.format("%s is not supported", SetState.class.getSimpleName()));
+              }
+
+              @Override
+              public <KeyT, ValueT> MapState<KeyT, ValueT> bindMap(
+                  String id,
+                  StateSpec<MapState<KeyT, ValueT>> spec,
+                  Coder<KeyT> mapKeyCoder,
+                  Coder<ValueT> mapValueCoder) {
+                throw new UnsupportedOperationException(
+                    String.format("%s is not supported", MapState.class.getSimpleName()));
+              }
+
+              @Override
+              public <InputT, AccumT, OutputT>
+                  CombiningState<InputT, AccumT, OutputT> bindCombining(
+                      String id,
+                      StateSpec<CombiningState<InputT, AccumT, OutputT>> spec,
+                      Coder<AccumT> accumCoder,
+                      Combine.CombineFn<InputT, AccumT, OutputT> combineFn) {
+                throw new UnsupportedOperationException("bindCombiningValue is not supported.");
+              }
+
+              @Override
+              public <InputT, AccumT, OutputT>
+                  CombiningState<InputT, AccumT, OutputT> bindCombiningWithContext(
+                      String id,
+                      StateSpec<CombiningState<InputT, AccumT, OutputT>> spec,
+                      Coder<AccumT> accumCoder,
+                      CombineWithContext.CombineFnWithContext<InputT, AccumT, OutputT> combineFn) {
+                throw new UnsupportedOperationException(
+                    "bindCombiningValueWithContext is not supported.");
+              }
+
+              @Override
+              public WatermarkHoldState bindWatermark(
+                  String id,
+                  StateSpec<WatermarkHoldState> spec,
+                  TimestampCombiner timestampCombiner) {
+                throw new UnsupportedOperationException(
+                    String.format("%s is not supported", CombiningState.class.getSimpleName()));
+              }
+            });
   }
 
-  private static class KeyGroupBagCombiner<T> implements KeyGroupCombiner<T, List<T>, Iterable<T>> {
-
-    @Override
-    public List<T> createAccumulator() {
-      return new ArrayList<>();
-    }
-
-    @Override
-    public List<T> addInput(List<T> accumulator, T input) {
-      accumulator.add(input);
-      return accumulator;
-    }
-
-    @Override
-    public Iterable<T> extractOutput(Iterable<List<T>> accumulators) {
-      List<T> result = new ArrayList<>();
-      // maybe can return an unmodifiable view.
-      for (List<T> list : accumulators) {
-        result.addAll(list);
-      }
-      return result;
-    }
-  }
-
-  private class FlinkKeyGroupBagState<T> extends AbstractKeyGroupState<T, List<T>, Iterable<T>>
-      implements BagState<T> {
+  private static class FlinkKeyGroupBagState<K, T> implements BagState<T> {
 
     private final StateNamespace namespace;
-    private final StateTag<BagState<T>> address;
+    private final CoderTypeSerializer namespaceSerializer;
+    private final ListStateDescriptor<T> flinkStateDescriptor;
+    private final KeyedStateBackend<ByteBuffer> flinkStateBackend;
+
+    FlinkKeyGroupBagState(
+        KeyedStateBackend<ByteBuffer> flinkStateBackend,
+        String stateId,
+        StateNamespace namespace,
+        Coder<T> coder) {
 
-    FlinkKeyGroupBagState(StateTag<BagState<T>> address, StateNamespace namespace, Coder<T> coder) {
-      super(
-          address.getId(), namespace.stringKey(), ListCoder.of(coder), new KeyGroupBagCombiner<>());
       this.namespace = namespace;
-      this.address = address;
+      this.flinkStateBackend = flinkStateBackend;
+      this.namespaceSerializer = new CoderTypeSerializer<>(coder);
+      this.flinkStateDescriptor = new ListStateDescriptor<>(stateId, namespaceSerializer);
     }
 
     @Override
     public void add(T input) {
-      addInput(input);
+      try {
+        ListState<T> partitionedState =
+            flinkStateBackend.getPartitionedState(
+                namespace.stringKey(), StringSerializer.INSTANCE, flinkStateDescriptor);
+        partitionedState.add(input);
+      } catch (Exception e) {
+        throw new RuntimeException("Error adding to bag state.", e);
+      }
     }
 
     @Override
@@ -324,118 +187,65 @@ public class FlinkKeyGroupStateInternals<K> implements StateInternals {
       return this;
     }
 
+    /** Reads the state of _all_ keys and combines it. */
     @Override
+    @Nonnull
     public Iterable<T> read() {
-      Iterable<T> result = extractOutput();
-      return result != null ? result : Collections.emptyList();
+      final Deque<T> state = new ArrayDeque<>();
+      // Load specific implementation for Flink 1.5/1.6/1.7
+      final AllKeyStateFunction allKeyStateFunction =
+          new AllKeyStateFunction(
+              (StateConsumer<ListState>)
+                  listState -> {
+                    for (T item : (Iterable<T>) listState.get()) {
+                      state.add(item);
+                    }
+                  });
+      try {
+        flinkStateBackend.applyToAllKeys(
+            namespace.stringKey(),
+            StringSerializer.INSTANCE,
+            flinkStateDescriptor,
+            allKeyStateFunction);
+      } catch (Exception e) {
+        throw new RuntimeException(
+            String.format(
+                "Failed to iterate over all key state for giving namespace %s", namespace));
+      }
+      return () ->
+          new Iterator<T>() {
+            @Override
+            public boolean hasNext() {
+              return state.size() > 0;
+            }
+
+            @Override
+            public T next() {
+              return state.pop();
+            }
+          };
     }
 
     @Override
     public ReadableState<Boolean> isEmpty() {
-      return new ReadableState<Boolean>() {
-        @Override
-        public Boolean read() {
-          try {
-            return isEmptyInternal();
-          } catch (Exception e) {
-            throw new RuntimeException("Error reading state.", e);
-          }
-        }
-
-        @Override
-        public ReadableState<Boolean> readLater() {
-          return this;
-        }
-      };
+      throw new UnsupportedOperationException("isEmpty is not implemented and should not be used.");
     }
 
     @Override
     public void clear() {
-      clearInternal();
-    }
-
-    @Override
-    public boolean equals(Object o) {
-      if (this == o) {
-        return true;
-      }
-      if (o == null || getClass() != o.getClass()) {
-        return false;
-      }
-
-      FlinkKeyGroupBagState<?> that = (FlinkKeyGroupBagState<?>) o;
-
-      return namespace.equals(that.namespace) && address.equals(that.address);
-    }
-
-    @Override
-    public int hashCode() {
-      int result = namespace.hashCode();
-      result = 31 * result + address.hashCode();
-      return result;
-    }
-  }
-
-  /**
-   * Snapshots the state {@code (stateName -> (valueCoder && (namespace -> value)))} for a given
-   * {@code keyGroupIdx}.
-   *
-   * @param keyGroupIdx the id of the key-group to be put in the snapshot.
-   * @param out the stream to write to.
-   */
-  public void snapshotKeyGroupState(int keyGroupIdx, DataOutputStream out) throws Exception {
-    int localIdx = getIndexForKeyGroup(keyGroupIdx);
-    Map<String, Tuple2<Coder<?>, Map<String, ?>>> stateTable = stateTables[localIdx];
-    Preconditions.checkState(
-        stateTable.size() <= Short.MAX_VALUE,
-        "Too many States: "
-            + stateTable.size()
-            + ". Currently at most "
-            + Short.MAX_VALUE
-            + " states are supported");
-    out.writeShort(stateTable.size());
-    for (Map.Entry<String, Tuple2<Coder<?>, Map<String, ?>>> entry : stateTable.entrySet()) {
-      out.writeUTF(entry.getKey());
-      Coder coder = entry.getValue().f0;
-      InstantiationUtil.serializeObject(out, coder);
-      Map<String, ?> map = entry.getValue().f1;
-      out.writeInt(map.size());
-      for (Map.Entry<String, ?> entry1 : map.entrySet()) {
-        StringUtf8Coder.of().encode(entry1.getKey(), out);
-        coder.encode(entry1.getValue(), out);
-      }
-    }
-  }
-
-  /**
-   * Restore the state {@code (stateName -> (valueCoder && (namespace -> value)))} for a given
-   * {@code keyGroupIdx}.
-   *
-   * @param keyGroupIdx the id of the key-group to be put in the snapshot.
-   * @param in the stream to read from.
-   * @param userCodeClassLoader the class loader that will be used to deserialize the valueCoder.
-   */
-  public void restoreKeyGroupState(
-      int keyGroupIdx, DataInputStream in, ClassLoader userCodeClassLoader) throws Exception {
-    int localIdx = getIndexForKeyGroup(keyGroupIdx);
-    Map<String, Tuple2<Coder<?>, Map<String, ?>>> stateTable = stateTables[localIdx];
-    int numStates = in.readShort();
-    for (int i = 0; i < numStates; ++i) {
-      String stateName = in.readUTF();
-      Coder coder = InstantiationUtil.deserializeObject(in, userCodeClassLoader);
-      Tuple2<Coder<?>, Map<String, ?>> tuple2 = stateTable.get(stateName);
-      if (tuple2 == null) {
-        tuple2 = new Tuple2<>();
-        tuple2.f0 = coder;
-        tuple2.f1 = new HashMap<>();
-        stateTable.put(stateName, tuple2);
-      }
-      Map<String, Object> map = (Map<String, Object>) tuple2.f1;
-      int mapSize = in.readInt();
-      for (int j = 0; j < mapSize; j++) {
-        String namespace = StringUtf8Coder.of().decode(in);
-        Object value = coder.decode(in);
-        map.put(namespace, value);
+      // Load specific implementation for Flink 1.5/1.6/1.7
+      final AllKeyStateFunction allKeyStateFunction =
+          new AllKeyStateFunction((StateConsumer<ListState>) listState -> listState.clear());
+      try {
+        flinkStateBackend.applyToAllKeys(
+            namespace.stringKey(),
+            StringSerializer.INSTANCE,
+            flinkStateDescriptor,
+            allKeyStateFunction);
+      } catch (Exception e) {
+        throw new RuntimeException(
+            String.format(
+                "Failed to iterate over all key state for giving namespace %s", namespace));
       }
     }
   }
diff --git a/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/state/FlinkSplitStateInternals.java b/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/state/FlinkSplitStateInternals.java
index 146128071ff..72da34df498 100644
--- a/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/state/FlinkSplitStateInternals.java
+++ b/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/state/FlinkSplitStateInternals.java
@@ -169,7 +169,7 @@ public class FlinkSplitStateInternals<K> implements StateInternals {
         Iterable<T> result = flinkStateBackend.getListState(descriptor).get();
         return result != null ? result : Collections.emptyList();
       } catch (Exception e) {
-        throw new RuntimeException("Error updating state.", e);
+        throw new RuntimeException("Error reading state.", e);
       }
     }
 
@@ -200,7 +200,7 @@ public class FlinkSplitStateInternals<K> implements StateInternals {
       try {
         flinkStateBackend.getListState(descriptor).clear();
       } catch (Exception e) {
-        throw new RuntimeException("Error reading state.", e);
+        throw new RuntimeException("Error clearing state.", e);
       }
     }
 
diff --git a/runners/flink/src/test/java/org/apache/beam/runners/flink/streaming/FlinkKeyGroupStateInternalsTest.java b/runners/flink/src/test/java/org/apache/beam/runners/flink/streaming/FlinkKeyGroupStateInternalsTest.java
index d6eb10b8aea..d646f98ef20 100644
--- a/runners/flink/src/test/java/org/apache/beam/runners/flink/streaming/FlinkKeyGroupStateInternalsTest.java
+++ b/runners/flink/src/test/java/org/apache/beam/runners/flink/streaming/FlinkKeyGroupStateInternalsTest.java
@@ -17,20 +17,18 @@
  */
 package org.apache.beam.runners.flink.streaming;
 
-import static org.junit.Assert.assertThat;
+import static org.hamcrest.MatcherAssert.assertThat;
+import static org.hamcrest.Matchers.containsInAnyOrder;
 
-import java.io.ByteArrayInputStream;
-import java.io.ByteArrayOutputStream;
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
 import java.nio.ByteBuffer;
+import java.util.UUID;
 import org.apache.beam.runners.core.StateInternals;
-import org.apache.beam.runners.core.StateInternalsTest;
 import org.apache.beam.runners.core.StateNamespace;
 import org.apache.beam.runners.core.StateNamespaceForTest;
 import org.apache.beam.runners.core.StateTag;
 import org.apache.beam.runners.core.StateTags;
 import org.apache.beam.runners.flink.translation.wrappers.streaming.state.FlinkKeyGroupStateInternals;
+import org.apache.beam.sdk.coders.CoderException;
 import org.apache.beam.sdk.coders.StringUtf8Coder;
 import org.apache.beam.sdk.state.BagState;
 import org.apache.beam.sdk.util.CoderUtils;
@@ -45,181 +43,79 @@ import org.apache.flink.runtime.state.KeyGroupRange;
 import org.apache.flink.runtime.state.KeyedStateBackend;
 import org.apache.flink.runtime.state.memory.MemoryStateBackend;
 import org.hamcrest.Matchers;
-import org.junit.Ignore;
+import org.junit.Before;
 import org.junit.Test;
-import org.junit.runner.RunWith;
-import org.junit.runners.JUnit4;
 
-/**
- * Tests for {@link FlinkKeyGroupStateInternals}. This is based on the tests for {@code
- * StateInternalsTest}.
- */
+/** Tests for {@link FlinkKeyGroupStateInternals}. */
 public class FlinkKeyGroupStateInternalsTest {
 
-  /** A standard StateInternals test. Just test BagState. */
-  @RunWith(JUnit4.class)
-  public static class StandardStateInternalsTests extends StateInternalsTest {
-    @Override
-    protected StateInternals createStateInternals() {
-      KeyedStateBackend keyedStateBackend = getKeyedStateBackend(2, new KeyGroupRange(0, 1));
-      return new FlinkKeyGroupStateInternals<>(StringUtf8Coder.of(), keyedStateBackend);
-    }
-
-    @Override
-    @Ignore
-    public void testValue() {}
-
-    @Override
-    @Ignore
-    public void testSet() {}
-
-    @Override
-    @Ignore
-    public void testSetIsEmpty() {}
-
-    @Override
-    @Ignore
-    public void testMergeSetIntoSource() {}
+  private static final StateNamespace NAMESPACE_1 = new StateNamespaceForTest("ns1");
+  private static final StateNamespace NAMESPACE_2 = new StateNamespaceForTest("ns2");
 
-    @Override
-    @Ignore
-    public void testMergeSetIntoNewNamespace() {}
+  private static final StateTag<BagState<String>> STRING_BAG_ADDR =
+      StateTags.bag("stringBag", StringUtf8Coder.of());
 
-    @Override
-    @Ignore
-    public void testMap() {}
+  private AbstractKeyedStateBackend<ByteBuffer> keyedStateBackend;
+  private StateInternals stateInternals;
 
-    @Override
-    @Ignore
-    public void testCombiningValue() {}
-
-    @Override
-    @Ignore
-    public void testCombiningIsEmpty() {}
-
-    @Override
-    @Ignore
-    public void testMergeCombiningValueIntoSource() {}
+  @Before
+  public void createStateInternals() {
+    initStateBackend(2, new KeyGroupRange(0, 1));
+    stateInternals = new FlinkKeyGroupStateInternals<>(StringUtf8Coder.of(), keyedStateBackend);
+  }
 
-    @Override
-    @Ignore
-    public void testMergeCombiningValueIntoNewNamespace() {}
+  @Test
+  public void testBag() throws Exception {
+    BagState<String> bagState1 = stateInternals.state(NAMESPACE_1, STRING_BAG_ADDR);
+    BagState<String> bagState2 = stateInternals.state(NAMESPACE_2, STRING_BAG_ADDR);
 
-    @Override
-    @Ignore
-    public void testWatermarkEarliestState() {}
+    assertThat(bagState1.read(), Matchers.emptyIterable());
+    assertThat(bagState2.read(), Matchers.emptyIterable());
 
-    @Override
-    @Ignore
-    public void testWatermarkLatestState() {}
+    bagState1.add("hello");
+    bagState1.add("world");
+    bagState2.add("hallo");
+    bagState2.add("welt");
 
-    @Override
-    @Ignore
-    public void testWatermarkEndOfWindowState() {}
+    assertThat(bagState1.read(), containsInAnyOrder("hello", "world"));
+    assertThat(bagState2.read(), containsInAnyOrder("hallo", "welt"));
 
-    @Override
-    @Ignore
-    public void testWatermarkStateIsEmpty() {}
+    changeStateBackendKey();
+    bagState1.add("hey");
+    bagState1.add("joe");
+    bagState2.add("hey");
+    bagState2.add("jane");
 
-    @Override
-    @Ignore
-    public void testSetReadable() {}
+    assertThat(bagState1.read(), containsInAnyOrder("hello", "world", "hey", "joe"));
+    assertThat(bagState2.read(), containsInAnyOrder("hallo", "welt", "hey", "jane"));
 
-    @Override
-    @Ignore
-    public void testMapReadable() {}
+    bagState1.clear();
+    bagState2.clear();
+    assertThat(bagState1.read(), Matchers.emptyIterable());
+    assertThat(bagState2.read(), Matchers.emptyIterable());
   }
 
-  /** A specific test of FlinkKeyGroupStateInternalsTest. */
-  @RunWith(JUnit4.class)
-  public static class OtherTests {
-
-    private static final StateNamespace NAMESPACE_1 = new StateNamespaceForTest("ns1");
-    private static final StateNamespace NAMESPACE_2 = new StateNamespaceForTest("ns2");
-    private static final StateTag<BagState<String>> STRING_BAG_ADDR =
-        StateTags.bag("stringBag", StringUtf8Coder.of());
-
-    @Test
-    public void testKeyGroupAndCheckpoint() throws Exception {
-      // assign to keyGroup 0
-      ByteBuffer key0 =
-          ByteBuffer.wrap(CoderUtils.encodeToByteArray(StringUtf8Coder.of(), "11111111"));
-      // assign to keyGroup 1
-      ByteBuffer key1 =
-          ByteBuffer.wrap(CoderUtils.encodeToByteArray(StringUtf8Coder.of(), "22222222"));
-      FlinkKeyGroupStateInternals<String> allState;
-      {
-        KeyedStateBackend<ByteBuffer> keyedStateBackend =
-            getKeyedStateBackend(2, new KeyGroupRange(0, 1));
-        allState = new FlinkKeyGroupStateInternals<>(StringUtf8Coder.of(), keyedStateBackend);
-        BagState<String> valueForNamespace0 = allState.state(NAMESPACE_1, STRING_BAG_ADDR);
-        BagState<String> valueForNamespace1 = allState.state(NAMESPACE_2, STRING_BAG_ADDR);
-        keyedStateBackend.setCurrentKey(key0);
-        valueForNamespace0.add("0");
-        valueForNamespace1.add("2");
-        keyedStateBackend.setCurrentKey(key1);
-        valueForNamespace0.add("1");
-        valueForNamespace1.add("3");
-        assertThat(valueForNamespace0.read(), Matchers.containsInAnyOrder("0", "1"));
-        assertThat(valueForNamespace1.read(), Matchers.containsInAnyOrder("2", "3"));
-      }
-
-      ClassLoader classLoader = FlinkKeyGroupStateInternalsTest.class.getClassLoader();
-
-      // 1. scale up
-      ByteArrayOutputStream out0 = new ByteArrayOutputStream();
-      allState.snapshotKeyGroupState(0, new DataOutputStream(out0));
-      DataInputStream in0 = new DataInputStream(new ByteArrayInputStream(out0.toByteArray()));
-      {
-        KeyedStateBackend<ByteBuffer> keyedStateBackend =
-            getKeyedStateBackend(2, new KeyGroupRange(0, 0));
-        FlinkKeyGroupStateInternals<String> state0 =
-            new FlinkKeyGroupStateInternals<>(StringUtf8Coder.of(), keyedStateBackend);
-        state0.restoreKeyGroupState(0, in0, classLoader);
-        BagState<String> valueForNamespace0 = state0.state(NAMESPACE_1, STRING_BAG_ADDR);
-        BagState<String> valueForNamespace1 = state0.state(NAMESPACE_2, STRING_BAG_ADDR);
-        assertThat(valueForNamespace0.read(), Matchers.containsInAnyOrder("0"));
-        assertThat(valueForNamespace1.read(), Matchers.containsInAnyOrder("2"));
-      }
-
-      ByteArrayOutputStream out1 = new ByteArrayOutputStream();
-      allState.snapshotKeyGroupState(1, new DataOutputStream(out1));
-      DataInputStream in1 = new DataInputStream(new ByteArrayInputStream(out1.toByteArray()));
-      {
-        KeyedStateBackend<ByteBuffer> keyedStateBackend =
-            getKeyedStateBackend(2, new KeyGroupRange(1, 1));
-        FlinkKeyGroupStateInternals<String> state1 =
-            new FlinkKeyGroupStateInternals<>(StringUtf8Coder.of(), keyedStateBackend);
-        state1.restoreKeyGroupState(1, in1, classLoader);
-        BagState<String> valueForNamespace0 = state1.state(NAMESPACE_1, STRING_BAG_ADDR);
-        BagState<String> valueForNamespace1 = state1.state(NAMESPACE_2, STRING_BAG_ADDR);
-        assertThat(valueForNamespace0.read(), Matchers.containsInAnyOrder("1"));
-        assertThat(valueForNamespace1.read(), Matchers.containsInAnyOrder("3"));
-      }
-
-      // 2. scale down
-      {
-        KeyedStateBackend<ByteBuffer> keyedStateBackend =
-            getKeyedStateBackend(2, new KeyGroupRange(0, 1));
-        FlinkKeyGroupStateInternals<String> newAllState =
-            new FlinkKeyGroupStateInternals<>(StringUtf8Coder.of(), keyedStateBackend);
-        in0.reset();
-        in1.reset();
-        newAllState.restoreKeyGroupState(0, in0, classLoader);
-        newAllState.restoreKeyGroupState(1, in1, classLoader);
-        BagState<String> valueForNamespace0 = newAllState.state(NAMESPACE_1, STRING_BAG_ADDR);
-        BagState<String> valueForNamespace1 = newAllState.state(NAMESPACE_2, STRING_BAG_ADDR);
-        assertThat(valueForNamespace0.read(), Matchers.containsInAnyOrder("0", "1"));
-        assertThat(valueForNamespace1.read(), Matchers.containsInAnyOrder("2", "3"));
-      }
+  @Test
+  public void testBagIsEmpty() throws Exception {
+    BagState<String> value = stateInternals.state(NAMESPACE_1, STRING_BAG_ADDR);
+    try {
+      value.isEmpty();
+    } catch (UnsupportedOperationException e) {
+      // this is what we want
     }
   }
 
-  private static KeyedStateBackend<ByteBuffer> getKeyedStateBackend(
+  private void changeStateBackendKey() throws CoderException {
+    keyedStateBackend.setCurrentKey(
+        ByteBuffer.wrap(
+            CoderUtils.encodeToByteArray(StringUtf8Coder.of(), UUID.randomUUID().toString())));
+  }
+
+  private KeyedStateBackend<ByteBuffer> initStateBackend(
       int numberOfKeyGroups, KeyGroupRange keyGroupRange) {
     MemoryStateBackend backend = new MemoryStateBackend();
     try {
-      AbstractKeyedStateBackend<ByteBuffer> keyedStateBackend =
+      keyedStateBackend =
           backend.createKeyedStateBackend(
               new DummyEnvironment("test", 1, 0),
               new JobID(),
@@ -228,8 +124,7 @@ public class FlinkKeyGroupStateInternalsTest {
               numberOfKeyGroups,
               keyGroupRange,
               new KvStateRegistry().createTaskRegistry(new JobID(), new JobVertexID()));
-      keyedStateBackend.setCurrentKey(
-          ByteBuffer.wrap(CoderUtils.encodeToByteArray(StringUtf8Coder.of(), "1")));
+      changeStateBackendKey();
       return keyedStateBackend;
     } catch (Exception e) {
       throw new RuntimeException(e);
