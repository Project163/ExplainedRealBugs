diff --git a/sdks/python/apache_beam/runners/portability/fn_api_runner/translations.py b/sdks/python/apache_beam/runners/portability/fn_api_runner/translations.py
index ee44f236e82..2d6dd29aaeb 100644
--- a/sdks/python/apache_beam/runners/portability/fn_api_runner/translations.py
+++ b/sdks/python/apache_beam/runners/portability/fn_api_runner/translations.py
@@ -98,7 +98,7 @@ class Stage(object):
       transforms,  # type: List[beam_runner_api_pb2.PTransform]
       downstream_side_inputs=None,  # type: Optional[FrozenSet[str]]
       must_follow=frozenset(),  # type: FrozenSet[Stage]
-      parent=None,  # type: Optional[Stage]
+      parent=None,  # type: Optional[str]
       environment=None,  # type: Optional[str]
       forced_root=False):
     self.name = name
@@ -165,8 +165,8 @@ class Stage(object):
         self.is_all_sdk_urns(context) and consumer.is_all_sdk_urns(context) and
         no_overlap(self.downstream_side_inputs, consumer.side_inputs()))
 
-  def fuse(self, other):
-    # type: (Stage) -> Stage
+  def fuse(self, other, context):
+    # type: (Stage, TransformContext) -> Stage
     return Stage(
         "(%s)+(%s)" % (self.name, other.name),
         self.transforms + other.transforms,
@@ -174,7 +174,7 @@ class Stage(object):
         union(self.must_follow, other.must_follow),
         environment=self._merge_environments(
             self.environment, other.environment),
-        parent=self.parent if self.parent == other.parent else None,
+        parent=_lowest_common_ancestor(self.name, other.name, context),
         forced_root=self.forced_root or other.forced_root)
 
   def is_runner_urn(self, context):
@@ -484,9 +484,20 @@ class TransformContext(object):
         self.maybe_length_prefixed_coder(
             self.components.pcollections[pcoll_id].coder_id))
 
+  @memoize_on_instance
+  def parents_map(self):
+    return {
+        child: parent
+        for (parent, transform) in self.components.transforms.items()
+        for child in transform.subtransforms
+    }
 
 def leaf_transform_stages(
-    root_ids, components, parent=None, known_composites=KNOWN_COMPOSITES):
+    root_ids,  # type: Iterable[str]
+    components,  # type: beam_runner_api_pb2.Components
+    parent=None,  # type: Optional[str]
+    known_composites=KNOWN_COMPOSITES  # type: FrozenSet[str]
+):
   # type: (...) -> Iterator[Stage]
   for root_id in root_ids:
     root = components.transforms[root_id]
@@ -539,8 +550,6 @@ def pipeline_from_stages(
     if parent is None:
       roots.add(child)
     else:
-      if isinstance(parent, Stage):
-        parent = parent.name
       if (parent not in components.transforms and
           parent in pipeline_proto.components.transforms):
         components.transforms[parent].CopyFrom(
@@ -783,9 +792,14 @@ def eliminate_common_key_with_none(stages, context):
         only_element(stage.transforms[0].outputs.values())
         for stage in sibling_stages
     ]
+    parent = functools.reduce(
+        lambda a,
+        b: _lowest_common_ancestor(a, b, context),
+        [s.name for s in sibling_stages])
     for to_delete_pcoll_id in output_pcoll_ids[1:]:
       pcoll_id_remap[to_delete_pcoll_id] = output_pcoll_ids[0]
       del context.components.pcollections[to_delete_pcoll_id]
+    sibling_stages[0].parent = parent
     remaining_stages.append(sibling_stages[0])
 
   # Remap all transforms in components.
@@ -849,7 +863,7 @@ def pack_combiners(stages, context):
 
   def _try_fuse_stages(a, b):
     if a.can_fuse(b, context):
-      return a.fuse(b)
+      return a.fuse(b, context)
     else:
       raise ValueError
 
@@ -965,7 +979,7 @@ def pack_combiners(stages, context):
         pack_combine_name + '/Pack', [pack_transform],
         downstream_side_inputs=fused_stage.downstream_side_inputs,
         must_follow=fused_stage.must_follow,
-        parent=fused_stage,
+        parent=fused_stage.parent,
         environment=fused_stage.environment)
     yield pack_stage
 
@@ -987,7 +1001,7 @@ def pack_combiners(stages, context):
         pack_combine_name + '/Unpack', [unpack_transform],
         downstream_side_inputs=fused_stage.downstream_side_inputs,
         must_follow=fused_stage.must_follow,
-        parent=fused_stage,
+        parent=fused_stage.parent,
         environment=fused_stage.environment)
     yield unpack_stage
 
@@ -1049,7 +1063,7 @@ def lift_combiners(stages, context):
         transform.unique_name, [transform],
         downstream_side_inputs=base_stage.downstream_side_inputs,
         must_follow=base_stage.must_follow,
-        parent=base_stage,
+        parent=base_stage.name,
         environment=base_stage.environment)
 
   def lifted_stages(stage):
@@ -1173,6 +1187,31 @@ def lift_combiners(stages, context):
       yield stage
 
 
+def _lowest_common_ancestor(a, b, context):
+  # type: (str, str, TransformContext) -> Optional[str]
+
+  '''Returns the name of the lowest common ancestor of the two named stages.
+
+  The provided context is used to compute ancestors of stages. Note that stages
+  are considered to be ancestors of themselves.
+  '''
+  assert a != b
+
+  parents = context.parents_map()
+
+  def get_ancestors(name):
+    ancestor = name
+    while ancestor is not None:
+      yield ancestor
+      ancestor = parents.get(ancestor)
+
+  a_ancestors = set(get_ancestors(a))
+  for b_ancestor in get_ancestors(b):
+    if b_ancestor in a_ancestors:
+      return b_ancestor
+  return None
+
+
 def expand_sdf(stages, context):
   # type: (Iterable[Stage], TransformContext) -> Iterator[Stage]
 
@@ -1222,7 +1261,7 @@ def expand_sdf(stages, context):
               transform.unique_name, [transform],
               base_stage.downstream_side_inputs,
               union(base_stage.must_follow, frozenset(extra_must_follow)),
-              parent=base_stage,
+              parent=base_stage.name,
               environment=base_stage.environment)
 
         main_input_tag = only_element(
@@ -1525,7 +1564,7 @@ def greedily_fuse(stages, pipeline_context):
     return s
 
   def fuse(producer, consumer):
-    fused = producer.fuse(consumer)
+    fused = producer.fuse(consumer, pipeline_context)
     replacements[producer] = fused
     replacements[consumer] = fused
 
diff --git a/sdks/python/apache_beam/runners/portability/fn_api_runner/translations_test.py b/sdks/python/apache_beam/runners/portability/fn_api_runner/translations_test.py
index 10bf8c05b04..4627c05c506 100644
--- a/sdks/python/apache_beam/runners/portability/fn_api_runner/translations_test.py
+++ b/sdks/python/apache_beam/runners/portability/fn_api_runner/translations_test.py
@@ -34,27 +34,34 @@ from apache_beam.transforms.core import Create
 
 class TranslationsTest(unittest.TestCase):
   def test_eliminate_common_key_with_void(self):
-    pipeline = beam.Pipeline()
-    pcoll = pipeline | 'Start' >> beam.Create([1, 2, 3])
-    _ = pcoll | 'TestKeyWithNoneA' >> beam.ParDo(core._KeyWithNone())
-    _ = pcoll | 'TestKeyWithNoneB' >> beam.ParDo(core._KeyWithNone())
+    class MultipleKeyWithNone(beam.PTransform):
+      def expand(self, pcoll):
+        _ = pcoll | 'key-with-none-a' >> beam.ParDo(core._KeyWithNone())
+        _ = pcoll | 'key-with-none-b' >> beam.ParDo(core._KeyWithNone())
 
+    pipeline = beam.Pipeline()
+    _ = pipeline | beam.Create(
+        [1, 2, 3]) | 'multiple-key-with-none' >> MultipleKeyWithNone()
     pipeline_proto = pipeline.to_runner_api()
     _, stages = translations.create_and_optimize_stages(
         pipeline_proto, [translations.eliminate_common_key_with_none],
         known_runner_urns=frozenset())
     key_with_none_stages = [
-        stage for stage in stages if 'TestKeyWithNone' in stage.name
+        stage for stage in stages if 'key-with-none' in stage.name
     ]
     self.assertEqual(len(key_with_none_stages), 1)
+    self.assertIn('multiple-key-with-none', key_with_none_stages[0].parent)
 
   def test_pack_combiners(self):
+    class MultipleCombines(beam.PTransform):
+      def expand(self, pcoll):
+        _ = pcoll | 'mean-perkey' >> combiners.Mean.PerKey()
+        _ = pcoll | 'count-perkey' >> combiners.Count.PerKey()
+
     pipeline = beam.Pipeline()
     vals = [6, 3, 1, 1, 9, 1, 5, 2, 0, 6]
-    pcoll = pipeline | 'start-perkey' >> Create([('a', x) for x in vals])
-    _ = pcoll | 'mean-perkey' >> combiners.Mean.PerKey()
-    _ = pcoll | 'count-perkey' >> combiners.Count.PerKey()
-
+    _ = pipeline | Create([('a', x) for x in vals
+                           ]) | 'multiple-combines' >> MultipleCombines()
     environment = environments.DockerEnvironment.from_options(
         pipeline_options.PortableOptions(sdk_location='container'))
     pipeline_proto = pipeline.to_runner_api(default_environment=environment)
@@ -68,14 +75,18 @@ class TranslationsTest(unittest.TestCase):
           combine_per_key_stages.append(stage)
     self.assertEqual(len(combine_per_key_stages), 1)
     self.assertIn('/Pack', combine_per_key_stages[0].name)
+    self.assertIn('multiple-combines', combine_per_key_stages[0].parent)
+    self.assertNotIn('-perkey', combine_per_key_stages[0].parent)
 
   def test_pack_combiners_with_missing_environment_capability(self):
+    class MultipleCombines(beam.PTransform):
+      def expand(self, pcoll):
+        _ = pcoll | 'mean-perkey' >> combiners.Mean.PerKey()
+        _ = pcoll | 'count-perkey' >> combiners.Count.PerKey()
+
     pipeline = beam.Pipeline()
     vals = [6, 3, 1, 1, 9, 1, 5, 2, 0, 6]
-    pcoll = pipeline | 'start-perkey' >> Create([('a', x) for x in vals])
-    _ = pcoll | 'mean-perkey' >> combiners.Mean.PerKey()
-    _ = pcoll | 'count-perkey' >> combiners.Count.PerKey()
-
+    _ = pipeline | Create([('a', x) for x in vals]) | MultipleCombines()
     environment = environments.DockerEnvironment(capabilities=())
     pipeline_proto = pipeline.to_runner_api(default_environment=environment)
     _, stages = translations.create_and_optimize_stages(
@@ -89,15 +100,18 @@ class TranslationsTest(unittest.TestCase):
     # Combiner packing should be skipped because the environment is missing
     # the beam:combinefn:packed_python:v1 capability.
     self.assertEqual(len(combine_per_key_stages), 2)
-    self.assertNotIn('/Pack', combine_per_key_stages[0].name)
+    for combine_per_key_stage in combine_per_key_stages:
+      self.assertNotIn('/Pack', combine_per_key_stage.name)
 
   def test_pack_global_combiners(self):
+    class MultipleCombines(beam.PTransform):
+      def expand(self, pcoll):
+        _ = pcoll | 'mean-globally' >> combiners.Mean.Globally()
+        _ = pcoll | 'count-globally' >> combiners.Count.Globally()
+
     pipeline = beam.Pipeline()
     vals = [6, 3, 1, 1, 9, 1, 5, 2, 0, 6]
-    pcoll = pipeline | 'start' >> Create(vals)
-    _ = pcoll | 'mean' >> combiners.Mean.Globally()
-    _ = pcoll | 'count' >> combiners.Count.Globally()
-
+    _ = pipeline | Create(vals) | 'multiple-combines' >> MultipleCombines()
     environment = environments.DockerEnvironment.from_options(
         pipeline_options.PortableOptions(sdk_location='container'))
     pipeline_proto = pipeline.to_runner_api(default_environment=environment)
@@ -111,6 +125,8 @@ class TranslationsTest(unittest.TestCase):
         stage for stage in stages if 'KeyWithVoid' in stage.name
     ]
     self.assertEqual(len(key_with_void_stages), 1)
+    self.assertIn('multiple-combines', key_with_void_stages[0].parent)
+    self.assertNotIn('-globally', key_with_void_stages[0].parent)
 
     combine_per_key_stages = []
     for stage in stages:
@@ -119,6 +135,8 @@ class TranslationsTest(unittest.TestCase):
           combine_per_key_stages.append(stage)
     self.assertEqual(len(combine_per_key_stages), 1)
     self.assertIn('/Pack', combine_per_key_stages[0].name)
+    self.assertIn('multiple-combines', combine_per_key_stages[0].parent)
+    self.assertNotIn('-globally', combine_per_key_stages[0].parent)
 
   def test_optimize_empty_pipeline(self):
     pipeline = beam.Pipeline()
