diff --git a/sdks/python/apache_beam/io/gcp/bigquery_io_read_it_test.py b/sdks/python/apache_beam/io/gcp/bigquery_io_read_it_test.py
new file mode 100644
index 00000000000..b9b3b41c324
--- /dev/null
+++ b/sdks/python/apache_beam/io/gcp/bigquery_io_read_it_test.py
@@ -0,0 +1,60 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+
+"""A Dataflow job that counts the number of rows in a BQ table.
+
+   Can be configured to simulate slow reading for a given number of rows.
+"""
+
+from __future__ import absolute_import
+
+import logging
+import unittest
+
+from hamcrest.core.core.allof import all_of
+from nose.plugins.attrib import attr
+
+from apache_beam.io.gcp import bigquery_io_read_pipeline
+from apache_beam.testing.pipeline_verifiers import PipelineStateMatcher
+from apache_beam.testing.test_pipeline import TestPipeline
+
+
+class BigqueryIOReadIT(unittest.TestCase):
+
+  DEFAULT_DATASET = "big_query_import_export"
+  DEFAULT_TABLE_PREFIX = "export_"
+  NUM_RECORDS = {"1K": 1000,}
+
+  def run_bigquery_io_read_pipeline(self, input_size):
+    test_pipeline = TestPipeline(is_integration_test=True)
+    pipeline_verifiers = [PipelineStateMatcher(),]
+    extra_opts = {'input_table': self.DEFAULT_DATASET + "." +
+                                 self.DEFAULT_TABLE_PREFIX + input_size,
+                  'num_records': self.NUM_RECORDS[input_size],
+                  'on_success_matcher': all_of(*pipeline_verifiers)}
+    bigquery_io_read_pipeline.run(test_pipeline.get_full_options_as_args(
+        **extra_opts))
+
+  @attr('IT')
+  def test_1K_table(self):
+    self.run_bigquery_io_read_pipeline('1K')
+
+
+if __name__ == '__main__':
+  logging.getLogger().setLevel(logging.INFO)
+  unittest.main()
diff --git a/sdks/python/apache_beam/io/gcp/bigquery_io_read_pipeline.py b/sdks/python/apache_beam/io/gcp/bigquery_io_read_pipeline.py
new file mode 100644
index 00000000000..15bedf84748
--- /dev/null
+++ b/sdks/python/apache_beam/io/gcp/bigquery_io_read_pipeline.py
@@ -0,0 +1,93 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+
+# Copyright 2016 Google Inc. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+"""A Dataflow job that counts the number of rows in a BQ table.
+
+   Can be configured to simulate slow reading for a given number of rows.
+"""
+
+from __future__ import absolute_import
+
+import argparse
+import logging
+import random
+import time
+
+import apache_beam as beam
+from apache_beam.options.pipeline_options import PipelineOptions
+from apache_beam.testing.test_pipeline import TestPipeline
+from apache_beam.testing.util import assert_that
+from apache_beam.testing.util import equal_to
+
+
+class RowToStringWithSlowDown(beam.DoFn):
+
+  def process(self, element, num_slow=0, *args, **kwargs):
+
+    if num_slow == 0:
+      yield ['row']
+    else:
+      rand = random.random() * 100
+      if rand < num_slow:
+        time.sleep(0.01)
+        yield ['slow_row']
+      else:
+        yield ['row']
+
+
+def run(argv=None):
+  parser = argparse.ArgumentParser()
+  parser.add_argument('--input_table', required=True,
+                      help='Input table to process.')
+  parser.add_argument('--num_records', required=True,
+                      help='The expected number of records', type=int)
+  parser.add_argument('--num_slow', default=0,
+                      help=('Percentage of rows that will be slow. '
+                            'Must be in the range [0, 100)'))
+  known_args, pipeline_args = parser.parse_known_args(argv)
+
+  p = TestPipeline(options=PipelineOptions(pipeline_args))
+
+  # pylint: disable=expression-not-assigned
+  count = (p | 'read' >> beam.io.Read(beam.io.BigQuerySource(
+      known_args.input_table))
+           | 'row to string' >> beam.ParDo(RowToStringWithSlowDown(),
+                                           num_slow=known_args.num_slow)
+           | 'count' >> beam.combiners.Count.Globally())
+
+  assert_that(count, equal_to([known_args.num_records]))
+
+  p.run()
+
+
+if __name__ == '__main__':
+  logging.getLogger().setLevel(logging.INFO)
+  run()
