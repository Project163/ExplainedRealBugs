diff --git a/sdks/python/apache_beam/io/gcp/gcsio.py b/sdks/python/apache_beam/io/gcp/gcsio.py
index 2aa2f6f8f75..2e5b0599c49 100644
--- a/sdks/python/apache_beam/io/gcp/gcsio.py
+++ b/sdks/python/apache_beam/io/gcp/gcsio.py
@@ -110,6 +110,36 @@ def parse_gcs_path(gcs_path, object_optional=False):
   return match.group(1), match.group(2)
 
 
+def default_gcs_bucket_name(project, region):
+  from hashlib import md5
+  return 'dataflow-staging-%s-%s' % (
+      region, md5(project.encode('utf8')).hexdigest())
+
+
+def get_or_create_default_gcs_bucket(options):
+  """Create a default GCS bucket for this project."""
+  if getattr(options, 'dataflow_kms_key', None):
+    _LOGGER.warning(
+        'Cannot create a default bucket when --dataflow_kms_key is set.')
+    return None
+
+  project = getattr(options, 'project', None)
+  region = getattr(options, 'region', None)
+  if not project or not region:
+    return None
+
+  bucket_name = default_gcs_bucket_name(project, region)
+  bucket = GcsIO().get_bucket(bucket_name)
+  if bucket:
+    return bucket
+  else:
+    _LOGGER.warning(
+        'Creating default GCS bucket for project %s: gs://%s',
+        project,
+        bucket_name)
+    return GcsIO().create_bucket(bucket_name, project, location=region)
+
+
 class GcsIOError(IOError, retry.PermanentException):
   """GCS IO error that should not be retried."""
   pass
@@ -135,6 +165,30 @@ class GcsIO(object):
     """
     self._rewrite_cb = callback
 
+  def get_bucket(self, bucket_name):
+    """Returns an object bucket from its name, or None if it does not exist."""
+    try:
+      request = storage.StorageBucketsGetRequest(bucket=bucket_name)
+      return self.client.buckets.Get(request)
+    except HttpError:
+      return None
+
+  def create_bucket(self, bucket_name, project, kms_key=None, location=None):
+    """Create and return a GCS bucket in a specific project."""
+    encryption = None
+    if kms_key:
+      encryption = storage.Bucket.EncryptionValue(kms_key)
+
+    request = storage.StorageBucketsInsertRequest(
+        bucket=storage.Bucket(
+            name=bucket_name, location=location, encryption=encryption),
+        project=project,
+    )
+    try:
+      return self.client.buckets.Insert(request)
+    except HttpError:
+      return None
+
   def open(
       self,
       filename,
diff --git a/sdks/python/apache_beam/io/gcp/gcsio_test.py b/sdks/python/apache_beam/io/gcp/gcsio_test.py
index b01150c3edf..29c7e7be60e 100644
--- a/sdks/python/apache_beam/io/gcp/gcsio_test.py
+++ b/sdks/python/apache_beam/io/gcp/gcsio_test.py
@@ -49,6 +49,8 @@ except ImportError:
   HttpError = None
 # pylint: enable=wrong-import-order, wrong-import-position
 
+DEFAULT_GCP_PROJECT = 'apache-beam-testing'
+
 
 class FakeGcsClient(object):
   # Fake storage client.  Usage in gcsio.py is client.objects.Get(...) and
@@ -280,6 +282,13 @@ class TestGCSPathParser(unittest.TestCase):
       self.assertRaises(ValueError, gcsio.parse_gcs_path, path, True)
 
 
+class SampleOptions(object):
+  def __init__(self, project, region, kms_key=None):
+    self.project = DEFAULT_GCP_PROJECT
+    self.region = region
+    self.dataflow_kms_key = kms_key
+
+
 @unittest.skipIf(HttpError is None, 'GCP dependencies are not installed')
 @mock.patch.multiple(
     'time', time=mock.MagicMock(side_effect=range(100)), sleep=mock.MagicMock())
@@ -301,6 +310,18 @@ class TestGCSIO(unittest.TestCase):
     self.client = FakeGcsClient()
     self.gcs = gcsio.GcsIO(self.client)
 
+  def test_default_bucket_name(self):
+    self.assertEqual(
+        gcsio.default_gcs_bucket_name(DEFAULT_GCP_PROJECT, "us-central1"),
+        'dataflow-staging-us-central1-77b801c0838aee13391c0d1885860494')
+
+  def test_default_bucket_name_failure(self):
+    self.assertEqual(
+        gcsio.get_or_create_default_gcs_bucket(
+            SampleOptions(
+                DEFAULT_GCP_PROJECT, "us-central1", kms_key="kmskey!")),
+        None)
+
   def test_num_retries(self):
     # BEAM-7424: update num_retries accordingly if storage_client is
     # regenerated.
diff --git a/sdks/python/apache_beam/options/pipeline_options.py b/sdks/python/apache_beam/options/pipeline_options.py
index 0ff0d6475f5..32629bef57e 100644
--- a/sdks/python/apache_beam/options/pipeline_options.py
+++ b/sdks/python/apache_beam/options/pipeline_options.py
@@ -600,11 +600,32 @@ class GoogleCloudOptions(PipelineOptions):
         choices=['COST_OPTIMIZED', 'SPEED_OPTIMIZED'],
         help='Set the Flexible Resource Scheduling mode')
 
+  def _create_default_gcs_bucket(self):
+    try:
+      from apache_beam.io.gcp import gcsio
+    except ImportError:
+      _LOGGER.warning('Unable to create default GCS bucket.')
+      return None
+    bucket = gcsio.get_or_create_default_gcs_bucket(self)
+    if bucket:
+      return 'gs://%s' % bucket.id
+    else:
+      return None
+
   def validate(self, validator):
     errors = []
     if validator.is_service_runner():
       errors.extend(validator.validate_cloud_options(self))
-      errors.extend(validator.validate_gcs_path(self, 'temp_location'))
+
+      # Validating temp_location, or adding a default if there are issues
+      temp_location_errors = validator.validate_gcs_path(self, 'temp_location')
+      if temp_location_errors:
+        default_bucket = self._create_default_gcs_bucket()
+        if default_bucket is None:
+          errors.extend(temp_location_errors)
+        else:
+          setattr(self, 'temp_location', default_bucket)
+
       if getattr(self, 'staging_location',
                  None) or getattr(self, 'temp_location', None) is None:
         errors.extend(validator.validate_gcs_path(self, 'staging_location'))
