diff --git a/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ConfluentSchemaRegistryDeserializerProvider.java b/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ConfluentSchemaRegistryDeserializerProvider.java
index b1cfffae0f6..ed1d99de1d5 100644
--- a/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ConfluentSchemaRegistryDeserializerProvider.java
+++ b/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ConfluentSchemaRegistryDeserializerProvider.java
@@ -91,15 +91,19 @@ public class ConfluentSchemaRegistryDeserializerProvider<T> implements Deseriali
             .put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, schemaRegistryUrl)
             .build();
     Deserializer<T> deserializer =
-        (Deserializer<T>) new KafkaAvroDeserializer(getSchemaRegistryClient());
+        (Deserializer<T>)
+            new ConfluentSchemaRegistryDeserializer(getSchemaRegistryClient(), getAvroSchema());
     deserializer.configure(csrConfig, isKey);
     return deserializer;
   }
 
   @Override
   public Coder<T> getCoder(CoderRegistry coderRegistry) {
-    final Schema avroSchema = new Schema.Parser().parse(getSchemaMetadata().getSchema());
-    return (Coder<T>) AvroCoder.of(avroSchema);
+    return (Coder<T>) AvroCoder.of(getAvroSchema());
+  }
+
+  private Schema getAvroSchema() {
+    return new Schema.Parser().parse(getSchemaMetadata().getSchema());
   }
 
   private SchemaMetadata getSchemaMetadata() {
@@ -116,3 +120,17 @@ public class ConfluentSchemaRegistryDeserializerProvider<T> implements Deseriali
     return this.schemaRegistryClientProviderFn.apply(null);
   }
 }
+
+class ConfluentSchemaRegistryDeserializer extends KafkaAvroDeserializer {
+  Schema readerSchema;
+
+  ConfluentSchemaRegistryDeserializer(SchemaRegistryClient client, Schema readerSchema) {
+    super(client);
+    this.readerSchema = readerSchema;
+  }
+
+  @Override
+  public Object deserialize(String s, byte[] bytes) {
+    return this.deserialize(bytes, readerSchema);
+  }
+}
diff --git a/sdks/java/io/kafka/src/test/java/org/apache/beam/sdk/io/kafka/ConfluentSchemaRegistryDeserializerProviderTest.java b/sdks/java/io/kafka/src/test/java/org/apache/beam/sdk/io/kafka/ConfluentSchemaRegistryDeserializerProviderTest.java
index ff2d281fbfd..dcbf2b34ca7 100644
--- a/sdks/java/io/kafka/src/test/java/org/apache/beam/sdk/io/kafka/ConfluentSchemaRegistryDeserializerProviderTest.java
+++ b/sdks/java/io/kafka/src/test/java/org/apache/beam/sdk/io/kafka/ConfluentSchemaRegistryDeserializerProviderTest.java
@@ -22,10 +22,17 @@ import static org.junit.Assert.assertEquals;
 import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;
 import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;
 import io.confluent.kafka.schemaregistry.testutil.MockSchemaRegistry;
+import io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig;
+import io.confluent.kafka.serializers.KafkaAvroSerializer;
 import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+import org.apache.avro.generic.GenericRecord;
+import org.apache.avro.generic.GenericRecordBuilder;
 import org.apache.beam.sdk.coders.AvroCoder;
 import org.apache.beam.sdk.coders.CoderRegistry;
 import org.apache.beam.sdk.transforms.SerializableFunction;
+import org.apache.kafka.common.serialization.Serializer;
 import org.junit.Test;
 import org.junit.runner.RunWith;
 import org.junit.runners.JUnit4;
@@ -45,7 +52,7 @@ public class ConfluentSchemaRegistryDeserializerProviderTest {
     assertEquals(AVRO_SCHEMA, coderV0.getSchema());
 
     try {
-      Integer version = mockRegistryClient.register(subject, AVRO_SCHEMA_V1);
+      Integer version = mockRegistryClient.getVersion(subject, AVRO_SCHEMA_V1);
       AvroCoder coderV1 =
           (AvroCoder)
               mockDeserializerProvider(schemaRegistryUrl, subject, version).getCoder(coderRegistry);
@@ -55,6 +62,45 @@ public class ConfluentSchemaRegistryDeserializerProviderTest {
     }
   }
 
+  @Test
+  public void testDeserialize() {
+    // Test deserializing evolved schema.
+    // Verify that records from older schemas are deserialized to the latest schema
+    String schemaRegistryUrl = "mock://my-scope-name";
+    String subject = "mytopic";
+    SchemaRegistryClient mockRegistryClient = mockSchemaRegistryClient(schemaRegistryUrl, subject);
+
+    Map<String, Object> map = new HashMap<>();
+    map.put(AbstractKafkaAvroSerDeConfig.AUTO_REGISTER_SCHEMAS, true);
+    map.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, schemaRegistryUrl);
+    Serializer<GenericRecord> serializer = (Serializer) new KafkaAvroSerializer(mockRegistryClient);
+    serializer.configure(map, true);
+
+    byte[] bytes =
+        serializer.serialize(
+            subject,
+            new GenericRecordBuilder(AVRO_SCHEMA_V1)
+                .set("name", "KeyName")
+                .set("age", 1)
+                .set("favorite_number", 2)
+                .set("favorite_color", "color3")
+                .build());
+
+    Object deserialized =
+        mockDeserializerProvider(schemaRegistryUrl, subject, null)
+            .getDeserializer(new HashMap<>(), true)
+            .deserialize(subject, bytes);
+
+    GenericRecord expected =
+        new GenericRecordBuilder(AVRO_SCHEMA)
+            .set("name", "KeyName")
+            .set("favorite_number", 2)
+            .set("favorite_color", "color3")
+            .build();
+
+    assertEquals(expected, deserialized);
+  }
+
   static <T> DeserializerProvider<T> mockDeserializerProvider(
       String schemaRegistryUrl, String subject, Integer version) {
     return new ConfluentSchemaRegistryDeserializerProvider<>(
@@ -70,6 +116,7 @@ public class ConfluentSchemaRegistryDeserializerProviderTest {
     SchemaRegistryClient mockRegistryClient =
         MockSchemaRegistry.getClientForScope(schemaRegistryUrl);
     try {
+      mockRegistryClient.register(subject, AVRO_SCHEMA_V1);
       mockRegistryClient.register(subject, AVRO_SCHEMA);
     } catch (IOException | RestClientException e) {
       throw new RuntimeException("Unable to register schema for subject: " + subject, e);
