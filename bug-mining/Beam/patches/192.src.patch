diff --git a/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/util/PackageUtil.java b/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/util/PackageUtil.java
index 482ddd93556..0d52c5db67c 100644
--- a/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/util/PackageUtil.java
+++ b/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/util/PackageUtil.java
@@ -90,14 +90,12 @@ class PackageUtil {
    */
   static PackageAttributes createPackageAttributes(File source,
       String stagingPath, @Nullable String overridePackageName) {
-    try {
-      boolean directory = source.isDirectory();
-
-      // Compute size and hash in one pass over file or directory.
-      Hasher hasher = Hashing.md5().newHasher();
-      OutputStream hashStream = Funnels.asOutputStream(hasher);
-      CountingOutputStream countingOutputStream = new CountingOutputStream(hashStream);
+    boolean directory = source.isDirectory();
 
+    // Compute size and hash in one pass over file or directory.
+    Hasher hasher = Hashing.md5().newHasher();
+    OutputStream hashStream = Funnels.asOutputStream(hasher);
+    try (CountingOutputStream countingOutputStream = new CountingOutputStream(hashStream)) {
       if (!directory) {
         // Files are staged as-is.
         Files.asByteSource(source).copyTo(countingOutputStream);
@@ -105,6 +103,7 @@ class PackageUtil {
         // Directories are recursively zipped.
         ZipFiles.zipDirectory(source, countingOutputStream);
       }
+      countingOutputStream.flush();
 
       long size = countingOutputStream.getCount();
       String hash = Base64Variants.MODIFIED_FOR_URL.encode(hasher.hash().asBytes());
