diff --git a/sdks/java/io/hadoop-file-system/src/main/java/org/apache/beam/sdk/io/hdfs/HadoopFileSystem.java b/sdks/java/io/hadoop-file-system/src/main/java/org/apache/beam/sdk/io/hdfs/HadoopFileSystem.java
index 8230c699836..c12f6af7a86 100644
--- a/sdks/java/io/hadoop-file-system/src/main/java/org/apache/beam/sdk/io/hdfs/HadoopFileSystem.java
+++ b/sdks/java/io/hadoop-file-system/src/main/java/org/apache/beam/sdk/io/hdfs/HadoopFileSystem.java
@@ -41,6 +41,8 @@ import org.apache.hadoop.fs.FSInputStream;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileUtil;
 import org.apache.hadoop.fs.Path;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 /**
  * Adapts {@link org.apache.hadoop.fs.FileSystem} connectors to be used as Apache Beam {@link
@@ -68,6 +70,9 @@ import org.apache.hadoop.fs.Path;
  * </ul>
  */
 class HadoopFileSystem extends FileSystem<HadoopResourceId> {
+  private static final Logger LOG = LoggerFactory.getLogger(HadoopFileSystem.class);
+
+  @VisibleForTesting static final String LOG_CREATE_DIRECTORY = "Creating directory %s";
   @VisibleForTesting final org.apache.hadoop.fs.FileSystem fileSystem;
 
   HadoopFileSystem(Configuration configuration) throws IOException {
@@ -129,14 +134,22 @@ class HadoopFileSystem extends FileSystem<HadoopResourceId> {
       // implementing it. The DFSFileSystem implemented concat by deleting the srcs after which
       // is not what we want. Also, all the other FileSystem implementations I saw threw
       // UnsupportedOperationException within concat.
-      FileUtil.copy(
-          fileSystem,
-          srcResourceIds.get(i).toPath(),
-          fileSystem,
-          destResourceIds.get(i).toPath(),
-          false,
-          true,
-          fileSystem.getConf());
+      boolean success =
+          FileUtil.copy(
+              fileSystem,
+              srcResourceIds.get(i).toPath(),
+              fileSystem,
+              destResourceIds.get(i).toPath(),
+              false,
+              true,
+              fileSystem.getConf());
+      if (!success) {
+        // Defensive coding as this this should not happen in practice
+        throw new IOException(
+            String.format(
+                "Unable to copy resource %s to %s. No further information provided by underlying filesystem.",
+                srcResourceIds.get(i).toPath(), destResourceIds.get(i).toPath()));
+      }
     }
   }
 
@@ -145,13 +158,37 @@ class HadoopFileSystem extends FileSystem<HadoopResourceId> {
       List<HadoopResourceId> srcResourceIds, List<HadoopResourceId> destResourceIds)
       throws IOException {
     for (int i = 0; i < srcResourceIds.size(); ++i) {
-      fileSystem.rename(srcResourceIds.get(i).toPath(), destResourceIds.get(i).toPath());
+
+      // rename in HDFS requires the target directory to exist or silently fails (BEAM-4861)
+      Path targetDirectory = destResourceIds.get(i).toPath().getParent();
+      if (!fileSystem.exists(targetDirectory)) {
+        LOG.debug(
+            String.format(
+                LOG_CREATE_DIRECTORY, Path.getPathWithoutSchemeAndAuthority(targetDirectory)));
+        boolean success = fileSystem.mkdirs(targetDirectory);
+        if (!success) {
+          throw new IOException(
+              String.format(
+                  "Unable to create target directory %s. No further information provided by underlying filesystem.",
+                  targetDirectory));
+        }
+      }
+
+      boolean success =
+          fileSystem.rename(srcResourceIds.get(i).toPath(), destResourceIds.get(i).toPath());
+      if (!success) {
+        throw new IOException(
+            String.format(
+                "Unable to rename resource %s to %s. No further information provided by underlying filesystem.",
+                srcResourceIds.get(i).toPath(), destResourceIds.get(i).toPath()));
+      }
     }
   }
 
   @Override
   protected void delete(Collection<HadoopResourceId> resourceIds) throws IOException {
     for (HadoopResourceId resourceId : resourceIds) {
+      // ignore response as issues are surfaced with exception
       fileSystem.delete(resourceId.toPath(), false);
     }
   }
diff --git a/sdks/java/io/hadoop-file-system/src/test/java/org/apache/beam/sdk/io/hdfs/HadoopFileSystemTest.java b/sdks/java/io/hadoop-file-system/src/test/java/org/apache/beam/sdk/io/hdfs/HadoopFileSystemTest.java
index 5b962ee533b..b8c3dcb95c6 100644
--- a/sdks/java/io/hadoop-file-system/src/test/java/org/apache/beam/sdk/io/hdfs/HadoopFileSystemTest.java
+++ b/sdks/java/io/hadoop-file-system/src/test/java/org/apache/beam/sdk/io/hdfs/HadoopFileSystemTest.java
@@ -43,6 +43,7 @@ import org.apache.beam.sdk.io.fs.CreateOptions.StandardCreateOptions;
 import org.apache.beam.sdk.io.fs.MatchResult;
 import org.apache.beam.sdk.io.fs.MatchResult.Metadata;
 import org.apache.beam.sdk.io.fs.MatchResult.Status;
+import org.apache.beam.sdk.testing.ExpectedLogs;
 import org.apache.beam.sdk.testing.PAssert;
 import org.apache.beam.sdk.testing.TestPipeline;
 import org.apache.beam.sdk.util.MimeTypes;
@@ -66,6 +67,7 @@ public class HadoopFileSystemTest {
   @Rule public TestPipeline p = TestPipeline.create();
   @Rule public TemporaryFolder tmpFolder = new TemporaryFolder();
   @Rule public ExpectedException thrown = ExpectedException.none();
+  @Rule public final ExpectedLogs expectedLogs = ExpectedLogs.none(HadoopFileSystem.class);
   private MiniDFSCluster hdfsCluster;
   private URI hdfsClusterBaseUri;
   private HadoopFileSystem fileSystem;
@@ -152,6 +154,12 @@ public class HadoopFileSystemTest {
                         .build()))));
   }
 
+  /** Verifies that an attempt to delete a non existing file is silently ignored. */
+  @Test
+  public void testDeleteNonExisting() throws Exception {
+    fileSystem.delete(ImmutableList.of(testPath("MissingFile")));
+  }
+
   @Test
   public void testMatch() throws Exception {
     create("testFileAA", "testDataAA".getBytes(StandardCharsets.UTF_8));
@@ -255,6 +263,30 @@ public class HadoopFileSystemTest {
     assertArrayEquals("testDataB".getBytes(StandardCharsets.UTF_8), read("renameFileB", 0));
   }
 
+  /** Ensure that missing parent directories are created when required. */
+  @Test
+  public void testRenameMissingTargetDir() throws Exception {
+    create("pathA/testFileA", "testDataA".getBytes(StandardCharsets.UTF_8));
+    create("pathA/testFileB", "testDataB".getBytes(StandardCharsets.UTF_8));
+
+    // ensure files exist
+    assertArrayEquals("testDataA".getBytes(StandardCharsets.UTF_8), read("pathA/testFileA", 0));
+    assertArrayEquals("testDataB".getBytes(StandardCharsets.UTF_8), read("pathA/testFileB", 0));
+
+    // move to a directory that does not exist
+    fileSystem.rename(
+        ImmutableList.of(testPath("pathA/testFileA"), testPath("pathA/testFileB")),
+        ImmutableList.of(testPath("pathB/testFileA"), testPath("pathB/pathC/pathD/testFileB")));
+
+    // ensure the directories were created and the files can be read
+    expectedLogs.verifyDebug(String.format(HadoopFileSystem.LOG_CREATE_DIRECTORY, "/pathB"));
+    expectedLogs.verifyDebug(
+        String.format(HadoopFileSystem.LOG_CREATE_DIRECTORY, "/pathB/pathC/pathD"));
+    assertArrayEquals("testDataA".getBytes(StandardCharsets.UTF_8), read("pathB/testFileA", 0));
+    assertArrayEquals(
+        "testDataB".getBytes(StandardCharsets.UTF_8), read("pathB/pathC/pathD/testFileB", 0));
+  }
+
   @Test
   public void testMatchNewResource() {
     // match file spec
