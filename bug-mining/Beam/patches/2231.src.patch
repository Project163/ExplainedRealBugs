diff --git a/CHANGES.md b/CHANGES.md
index 0a4dc68afb4..200fbc5815c 100644
--- a/CHANGES.md
+++ b/CHANGES.md
@@ -72,6 +72,7 @@
   This new Guava version may introduce dependency conflicts if your project or dependencies rely
   on removed APIs. If affected, ensure to use an appropriate Guava version via `dependencyManagement` in Maven and
   `force` in Gradle.
+* Deterministic coding enforced for GroupByKey and Stateful DoFns.  Previously non-deterministic coding was allowed, resulting in keys not properly being grouped in some cases. ([BEAM-11719](https://issues.apache.org/jira/browse/BEAM-11719))
 * X behavior was changed ([BEAM-X](https://issues.apache.org/jira/browse/BEAM-X)).
 
 ## Deprecations
diff --git a/sdks/python/apache_beam/coders/coder_impl.py b/sdks/python/apache_beam/coders/coder_impl.py
index 5cd3685afb6..9ef22c53a8c 100644
--- a/sdks/python/apache_beam/coders/coder_impl.py
+++ b/sdks/python/apache_beam/coders/coder_impl.py
@@ -300,7 +300,7 @@ class DeterministicFastPrimitivesCoderImpl(CoderImpl):
         self._check_safe(x)
     else:
       raise TypeError(
-          "Unable to deterministically code '%s' of type '%s', "
+          "Unable to deterministically encode '%s' of type '%s', "
           "please provide a type hint for the input of '%s'" %
           (value, type(value), self._step_label))
 
@@ -908,7 +908,7 @@ class AvroCoderImpl(SimpleCoderImpl):
 class TupleCoderImpl(AbstractComponentCoderImpl):
   """A coder for tuple objects."""
   def _extract_components(self, value):
-    return value
+    return tuple(value)
 
   def _construct_from_components(self, components):
     return tuple(components)
diff --git a/sdks/python/apache_beam/coders/coders.py b/sdks/python/apache_beam/coders/coders.py
index 1bf0fa8c5ef..2dc88cc10c5 100644
--- a/sdks/python/apache_beam/coders/coders.py
+++ b/sdks/python/apache_beam/coders/coders.py
@@ -165,7 +165,9 @@ class Coder(object):
     if self.is_deterministic():
       return self
     else:
-      raise ValueError(error_message or "'%s' cannot be made deterministic.")
+      raise ValueError(
+          error_message or
+          "%s cannot be made deterministic for '%s'." % (self, step_label))
 
   def estimate_size(self, value):
     """Estimates the encoded size of the given value, in bytes.
diff --git a/sdks/python/apache_beam/pipeline.py b/sdks/python/apache_beam/pipeline.py
index 7f3162fc000..2d6dcf5acbe 100644
--- a/sdks/python/apache_beam/pipeline.py
+++ b/sdks/python/apache_beam/pipeline.py
@@ -841,6 +841,7 @@ class Pipeline(object):
     # general shapes, potential conflicts will have to be resolved.
     # We also only handle single-input, and (for fixing the output) single
     # output, which is sufficient.
+    # Also marks such values as requiring deterministic key coders.
     class ForceKvInputTypes(PipelineVisitor):
       def enter_composite_transform(self, transform_node):
         # type: (AppliedPTransform) -> None
@@ -854,18 +855,26 @@ class Pipeline(object):
           pcoll = transform_node.inputs[0]
           pcoll.element_type = typehints.coerce_to_kv_type(
               pcoll.element_type, transform_node.full_label)
+          pcoll.requires_deterministic_key_coder = transform_node.full_label
           if len(transform_node.outputs) == 1:
             # The runner often has expectations about the output types as well.
             output, = transform_node.outputs.values()
             if not output.element_type:
               output.element_type = transform_node.transform.infer_output_type(
                   pcoll.element_type)
+            if (isinstance(output.element_type,
+                           typehints.TupleHint.TupleConstraint) and
+                len(output.element_type.tuple_types) == 2):
+              output.requires_deterministic_key_coder = (
+                  transform_node.full_label)
         for side_input in transform_node.transform.side_inputs:
           if side_input.requires_keyed_input():
             side_input.pvalue.element_type = typehints.coerce_to_kv_type(
                 side_input.pvalue.element_type,
                 transform_node.full_label,
                 side_input_producer=side_input.pvalue.producer.full_label)
+            side_input.pvalue.requires_deterministic_key_coder = (
+                transform_node.full_label)
 
     self.visit(ForceKvInputTypes())
 
diff --git a/sdks/python/apache_beam/pvalue.py b/sdks/python/apache_beam/pvalue.py
index 5196525a643..0744e981fad 100644
--- a/sdks/python/apache_beam/pvalue.py
+++ b/sdks/python/apache_beam/pvalue.py
@@ -109,6 +109,7 @@ class PValue(object):
     self.is_bounded = is_bounded
     if windowing:
       self._windowing = windowing
+    self.requires_deterministic_key_coder = None
 
   def __str__(self):
     return self._str_internal()
@@ -187,7 +188,8 @@ class PCollection(PValue, Generic[T]):
     # type: (PipelineContext) -> beam_runner_api_pb2.PCollection
     return beam_runner_api_pb2.PCollection(
         unique_name=self._unique_name(),
-        coder_id=context.coder_id_from_element_type(self.element_type),
+        coder_id=context.coder_id_from_element_type(
+            self.element_type, self.requires_deterministic_key_coder),
         is_bounded=beam_runner_api_pb2.IsBounded.BOUNDED
         if self.is_bounded else beam_runner_api_pb2.IsBounded.UNBOUNDED,
         windowing_strategy_id=context.windowing_strategies.get_id(
diff --git a/sdks/python/apache_beam/runners/dataflow/dataflow_runner.py b/sdks/python/apache_beam/runners/dataflow/dataflow_runner.py
index e837d833f8f..9302a287998 100644
--- a/sdks/python/apache_beam/runners/dataflow/dataflow_runner.py
+++ b/sdks/python/apache_beam/runners/dataflow/dataflow_runner.py
@@ -292,11 +292,14 @@ class DataflowRunner(PipelineRunner):
           pcoll = transform_node.inputs[0]
           pcoll.element_type = typehints.coerce_to_kv_type(
               pcoll.element_type, transform_node.full_label)
+          pcoll.requires_deterministic_key_coder = transform_node.full_label
           key_type, value_type = pcoll.element_type.tuple_types
           if transform_node.outputs:
             key = DataflowRunner._only_element(transform_node.outputs.keys())
             transform_node.outputs[key].element_type = typehints.KV[
                 key_type, typehints.Iterable[value_type]]
+            transform_node.outputs[key].requires_deterministic_key_coder = (
+                transform_node.full_label)
 
     return GroupByKeyInputVisitor()
 
@@ -352,6 +355,8 @@ class DataflowRunner(PipelineRunner):
               # access pattern to appease Dataflow.
               side_input.pvalue.element_type = typehints.coerce_to_kv_type(
                   side_input.pvalue.element_type, transform_node.full_label)
+              side_input.pvalue.requires_deterministic_key_coder = (
+                  transform_node.full_label)
               new_side_input = _DataflowMultimapSideInput(side_input)
             else:
               raise ValueError(
diff --git a/sdks/python/apache_beam/runners/pipeline_context.py b/sdks/python/apache_beam/runners/pipeline_context.py
index aafa257b3dd..09950f5fbe0 100644
--- a/sdks/python/apache_beam/runners/pipeline_context.py
+++ b/sdks/python/apache_beam/runners/pipeline_context.py
@@ -233,12 +233,20 @@ class PipelineContext(object):
   # rather than an actual coder. The element type is required for some runners,
   # as well as performing a round-trip through protos.
   # TODO(BEAM-2717): Remove once this is no longer needed.
-  def coder_id_from_element_type(self, element_type):
-    # type: (Any) -> str
+  def coder_id_from_element_type(
+      self, element_type, requires_deterministic_key_coder=None):
+    # type: (Any, Optional[str]) -> str
     if self.use_fake_coders:
       return pickler.dumps(element_type).decode('ascii')
     else:
-      return self.coders.get_id(coders.registry.get_coder(element_type))
+      coder = coders.registry.get_coder(element_type)
+      if requires_deterministic_key_coder:
+        coder = coders.TupleCoder([
+            coder.key_coder().as_deterministic_coder(
+                requires_deterministic_key_coder),
+            coder.value_coder()
+        ])
+      return self.coders.get_id(coder)
 
   def element_type_from_coder_id(self, coder_id):
     # type: (str) -> Any
diff --git a/sdks/python/apache_beam/transforms/ptransform_test.py b/sdks/python/apache_beam/transforms/ptransform_test.py
index d4f8424c32a..abc93a2c0b9 100644
--- a/sdks/python/apache_beam/transforms/ptransform_test.py
+++ b/sdks/python/apache_beam/transforms/ptransform_test.py
@@ -25,6 +25,8 @@ from __future__ import print_function
 
 import collections
 import operator
+import pickle
+import random
 import re
 import sys
 import typing
@@ -500,6 +502,57 @@ class PTransformTest(unittest.TestCase):
           | 'Reiteration-Sum' >> beam.ParDo(MyDoFn()))
       assert_that(result, equal_to([(1, 170)]))
 
+  def test_group_by_key_determanistic_coder(self):
+    # pylint: disable=global-variable-not-assigned
+    global MyObject  # for pickling of the class instance
+
+    class MyObject:
+      def __init__(self, value):
+        self.value = value
+
+      def __eq__(self, other):
+        return self.value == other.value
+
+      def __hash__(self):
+        return hash(self.value)
+
+    class MyObjectCoder(beam.coders.Coder):
+      def encode(self, o):
+        return pickle.dumps((o.value, random.random()))
+
+      def decode(self, encoded):
+        return MyObject(pickle.loads(encoded)[0])
+
+      def as_deterministic_coder(self, *args):
+        return MyDetermanisticObjectCoder()
+
+      def to_type_hint(self):
+        return MyObject
+
+    class MyDetermanisticObjectCoder(beam.coders.Coder):
+      def encode(self, o):
+        return pickle.dumps(o.value)
+
+      def decode(self, encoded):
+        return MyObject(pickle.loads(encoded))
+
+      def is_deterministic(self):
+        return True
+
+    beam.coders.registry.register_coder(MyObject, MyObjectCoder)
+
+    with TestPipeline() as pipeline:
+      pcoll = pipeline | beam.Create([(MyObject(k % 2), k) for k in range(10)])
+      grouped = pcoll | beam.GroupByKey() | beam.MapTuple(
+          lambda k, vs: (k.value, sorted(vs)))
+      combined = pcoll | beam.CombinePerKey(sum) | beam.MapTuple(
+          lambda k, v: (k.value, v))
+      assert_that(
+          grouped,
+          equal_to([(0, [0, 2, 4, 6, 8]), (1, [1, 3, 5, 7, 9])]),
+          'CheckGrouped')
+      assert_that(combined, equal_to([(0, 20), (1, 25)]), 'CheckCombined')
+
   def test_partition_with_partition_fn(self):
     class SomePartitionFn(beam.PartitionFn):
       def partition_for(self, element, num_partitions, offset):
@@ -1368,7 +1421,6 @@ class PTransformTypeCheckTestCase(TypeHintTestCase):
   def test_filter_type_checks_using_type_hints_decorator(self):
     @with_input_types(b=int)
     def half(b):
-      import random
       return bool(random.choice([0, 1]))
 
     # Filter should deduce that it returns the same type that it takes.
