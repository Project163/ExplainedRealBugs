diff --git a/CHANGES.md b/CHANGES.md
index 4406bf594f5..244682dc15e 100644
--- a/CHANGES.md
+++ b/CHANGES.md
@@ -54,7 +54,7 @@
 * New highly anticipated feature Y added to Java SDK ([BEAM-Y](https://issues.apache.org/jira/browse/BEAM-Y)).
 
 ## I/Os
-
+* ReadFromMongoDB can now be used with MongoDB Atlas (Python) ([BEAM-11266](https://issues.apache.org/jira/browse/BEAM-11266).)
 * Support for X source added (Java/Python) ([BEAM-X](https://issues.apache.org/jira/browse/BEAM-X)).
 
 ## New Features / Improvements
diff --git a/sdks/python/apache_beam/io/mongodbio.py b/sdks/python/apache_beam/io/mongodbio.py
index aed0ff2bd31..392c824b376 100644
--- a/sdks/python/apache_beam/io/mongodbio.py
+++ b/sdks/python/apache_beam/io/mongodbio.py
@@ -32,6 +32,18 @@ Example usage::
                              db='testdb',
                              coll='input')
 
+To read from MongoDB Atlas, use ``bucket_auto`` option to enable
+``@bucketAuto`` MongoDB aggregation instead of ``splitVector``
+command which is a high-privilege function that cannot be assigned
+to any user in Atlas.
+
+Example usage::
+
+  pipeline | ReadFromMongoDB(uri='mongodb+srv://user:pwd@cluster0.mongodb.net',
+                             db='testdb',
+                             coll='input',
+                             bucket_auto=True)
+
 
 Write to MongoDB:
 -----------------
@@ -57,8 +69,10 @@ No backward compatibility guarantees. Everything in this module is experimental.
 from __future__ import absolute_import
 from __future__ import division
 
+import itertools
 import json
 import logging
+import math
 import struct
 
 import apache_beam as beam
@@ -101,22 +115,27 @@ class ReadFromMongoDB(PTransform):
       coll=None,
       filter=None,
       projection=None,
-      extra_client_params=None):
+      extra_client_params=None,
+      bucket_auto=False):
     """Initialize a :class:`ReadFromMongoDB`
 
     Args:
-      uri (str): The MongoDB connection string following the URI format
-      db (str): The MongoDB database name
-      coll (str): The MongoDB collection name
+      uri (str): The MongoDB connection string following the URI format.
+      db (str): The MongoDB database name.
+      coll (str): The MongoDB collection name.
       filter: A `bson.SON
         <https://api.mongodb.com/python/current/api/bson/son.html>`_ object
         specifying elements which must be present for a document to be included
-        in the result set
+        in the result set.
       projection: A list of field names that should be returned in the result
-        set or a dict specifying the fields to include or exclude
+        set or a dict specifying the fields to include or exclude.
       extra_client_params(dict): Optional `MongoClient
         <https://api.mongodb.com/python/current/api/pymongo/mongo_client.html>`_
-        parameters
+        parameters.
+      bucket_auto (bool): If :data:`True`, use MongoDB `$bucketAuto` aggregation
+        to split collection into bundles instead of `splitVector` command,
+        which does not work with MongoDB Atlas.
+        If :data:`False` (the default), use `splitVector` command for bundling.
 
     Returns:
       :class:`~apache_beam.transforms.ptransform.PTransform`
@@ -136,7 +155,8 @@ class ReadFromMongoDB(PTransform):
         coll=coll,
         filter=filter,
         projection=projection,
-        extra_client_params=extra_client_params)
+        extra_client_params=extra_client_params,
+        bucket_auto=bucket_auto)
 
   def expand(self, pcoll):
     return pcoll | iobase.Read(self._mongo_source)
@@ -150,7 +170,8 @@ class _BoundedMongoSource(iobase.BoundedSource):
       coll=None,
       filter=None,
       projection=None,
-      extra_client_params=None):
+      extra_client_params=None,
+      bucket_auto=False):
     if extra_client_params is None:
       extra_client_params = {}
     if filter is None:
@@ -161,40 +182,61 @@ class _BoundedMongoSource(iobase.BoundedSource):
     self.filter = filter
     self.projection = projection
     self.spec = extra_client_params
+    self.bucket_auto = bucket_auto
 
   def estimate_size(self):
     with MongoClient(self.uri, **self.spec) as client:
       return client[self.db].command('collstats', self.coll).get('size')
 
-  def split(self, desired_bundle_size, start_position=None, stop_position=None):
-    start_position, stop_position = self._replace_none_positions(
-        start_position, stop_position)
+  def _estimate_average_document_size(self):
+    with MongoClient(self.uri, **self.spec) as client:
+      return client[self.db].command('collstats', self.coll).get('avgObjSize')
 
+  def split(self, desired_bundle_size, start_position=None, stop_position=None):
     desired_bundle_size_in_mb = desired_bundle_size // 1024 // 1024
 
     # for desired bundle size, if desired chunk size smaller than 1mb, use
-    # mongodb default split size of 1mb.
+    # MongoDB default split size of 1mb.
     if desired_bundle_size_in_mb < 1:
       desired_bundle_size_in_mb = 1
 
-    split_keys = self._get_split_keys(
-        desired_bundle_size_in_mb, start_position, stop_position)
+    is_initial_split = start_position is None and stop_position is None
+    start_position, stop_position = self._replace_none_positions(
+      start_position, stop_position)
+
+    if self.bucket_auto:
+      # Use $bucketAuto for bundling
+      split_keys = []
+      weights = []
+      for bucket in self._get_auto_buckets(desired_bundle_size_in_mb,
+                                           start_position,
+                                           stop_position,
+                                           is_initial_split):
+        split_keys.append({'_id': bucket['_id']['max']})
+        weights.append(bucket['count'])
+    else:
+      # Use splitVector for bundling
+      split_keys = self._get_split_keys(
+          desired_bundle_size_in_mb, start_position, stop_position)
+      weights = itertools.cycle((desired_bundle_size_in_mb, ))
 
     bundle_start = start_position
-    for split_key_id in split_keys:
+    for split_key_id, weight in zip(split_keys, weights):
       if bundle_start >= stop_position:
         break
       bundle_end = min(stop_position, split_key_id['_id'])
       yield iobase.SourceBundle(
-          weight=desired_bundle_size_in_mb,
+          weight=weight,
           source=self,
           start_position=bundle_start,
           stop_position=bundle_end)
       bundle_start = bundle_end
     # add range of last split_key to stop_position
     if bundle_start < stop_position:
+      # bucket_auto mode can come here if not split due to single document
+      weight = 1 if self.bucket_auto else desired_bundle_size_in_mb
       yield iobase.SourceBundle(
-          weight=desired_bundle_size_in_mb,
+          weight=weight,
           source=self,
           start_position=bundle_start,
           stop_position=stop_position)
@@ -206,7 +248,8 @@ class _BoundedMongoSource(iobase.BoundedSource):
 
   def read(self, range_tracker):
     with MongoClient(self.uri, **self.spec) as client:
-      all_filters = self._merge_id_filter(range_tracker)
+      all_filters = self._merge_id_filter(
+          range_tracker.start_position(), range_tracker.stop_position())
       docs_cursor = client[self.db][self.coll].find(
           filter=all_filters,
           projection=self.projection).sort([('_id', ASCENDING)])
@@ -223,11 +266,12 @@ class _BoundedMongoSource(iobase.BoundedSource):
     res['filter'] = json.dumps(self.filter)
     res['projection'] = str(self.projection)
     res['mongo_client_spec'] = json.dumps(self.spec)
+    res['bucket_auto'] = self.bucket_auto
     return res
 
   def _get_split_keys(self, desired_chunk_size_in_mb, start_pos, end_pos):
     # calls mongodb splitVector command to get document ids at split position
-    if start_pos >= end_pos:
+    if start_pos >= _ObjectIdHelper.increment_id(end_pos, -1):
       # single document not splittable
       return []
     with MongoClient(self.uri, **self.spec) as client:
@@ -241,25 +285,62 @@ class _BoundedMongoSource(iobase.BoundedSource):
               max={'_id': end_pos},
               maxChunkSize=desired_chunk_size_in_mb)['splitKeys'])
 
-  def _merge_id_filter(self, range_tracker):
-    # Merge the default filter with refined _id field range of range_tracker.
-    # see more at https://docs.mongodb.com/manual/reference/operator/query/and/
-    all_filters = {
-        '$and': [
-            self.filter.copy(),
-            # add additional range filter to query. $gte specifies start
-            # position(inclusive) and $lt specifies the end position(exclusive),
-            # see more at
-            # https://docs.mongodb.com/manual/reference/operator/query/gte/ and
-            # https://docs.mongodb.com/manual/reference/operator/query/lt/
-            {
-                '_id': {
-                    '$gte': range_tracker.start_position(),
-                    '$lt': range_tracker.stop_position()
-                }
-            },
-        ]
-    }
+  def _get_auto_buckets(
+      self, desired_chunk_size_in_mb, start_pos, end_pos, is_initial_split):
+
+    if start_pos >= _ObjectIdHelper.increment_id(end_pos, -1):
+      # single document not splittable
+      return []
+
+    if is_initial_split and not self.filter:
+      # total collection size
+      size_in_mb = self.estimate_size() / float(1 << 20)
+    else:
+      # size of documents within start/end id range and possibly filtered
+      documents_count = self._count_id_range(start_pos, end_pos)
+      avg_document_size = self._estimate_average_document_size()
+      size_in_mb = documents_count * avg_document_size / float(1 << 20)
+
+    if size_in_mb == 0:
+      # no documents not splittable (maybe a result of filtering)
+      return []
+
+    bucket_count = math.ceil(size_in_mb / desired_chunk_size_in_mb)
+    with beam.io.mongodbio.MongoClient(self.uri, **self.spec) as client:
+      pipeline = [
+          {
+              # filter by positions and by the custom filter if any
+              '$match': self._merge_id_filter(start_pos, end_pos)
+          },
+          {
+              '$bucketAuto': {
+                  'groupBy': '$_id', 'buckets': bucket_count
+              }
+          }
+      ]
+      buckets = list(client[self.db][self.coll].aggregate(pipeline))
+      if buckets:
+        buckets[-1]['_id']['max'] = end_pos
+
+      return buckets
+
+  def _merge_id_filter(self, start_position, stop_position):
+    # Merge the default filter (if any) with refined _id field range
+    # of range_tracker.
+    # $gte specifies start position (inclusive)
+    # and $lt specifies the end position (exclusive),
+    # see more at
+    # https://docs.mongodb.com/manual/reference/operator/query/gte/ and
+    # https://docs.mongodb.com/manual/reference/operator/query/lt/
+    id_filter = {'_id': {'$gte': start_position, '$lt': stop_position}}
+    if self.filter:
+      all_filters = {
+          # see more at
+          # https://docs.mongodb.com/manual/reference/operator/query/and/
+          '$and': [self.filter.copy(), id_filter]
+      }
+    else:
+      all_filters = id_filter
 
     return all_filters
 
@@ -282,6 +363,13 @@ class _BoundedMongoSource(iobase.BoundedSource):
       stop_position = _ObjectIdHelper.increment_id(last_doc_id, 1)
     return start_position, stop_position
 
+  def _count_id_range(self, start_position, stop_position):
+    # Number of documents between start_position (inclusive)
+    # and stop_position (exclusive), respecting the custom filter if any.
+    with MongoClient(self.uri, **self.spec) as client:
+      return client[self.db][self.coll].count_documents(
+          filter=self._merge_id_filter(start_position, stop_position))
+
 
 class _ObjectIdHelper(object):
   """A Utility class to manipulate bson object ids."""
diff --git a/sdks/python/apache_beam/io/mongodbio_it_test.py b/sdks/python/apache_beam/io/mongodbio_it_test.py
index 74416339edb..9619c7af64b 100644
--- a/sdks/python/apache_beam/io/mongodbio_it_test.py
+++ b/sdks/python/apache_beam/io/mongodbio_it_test.py
@@ -70,7 +70,7 @@ def run(argv=None):
   # Test Write to MongoDB
   with TestPipeline(options=PipelineOptions(pipeline_args)) as p:
     start_time = time.time()
-    _LOGGER.info('Writing %d documents to mongodb' % known_args.num_documents)
+    _LOGGER.info('Writing %d documents to mongodb', known_args.num_documents)
 
     _ = (
         p | beam.Create([known_args.num_documents])
@@ -86,25 +86,83 @@ def run(argv=None):
       (known_args.num_documents, elapsed))
 
   # Test Read from MongoDB
-  with TestPipeline(options=PipelineOptions(pipeline_args)) as p:
-    start_time = time.time()
+  total_sum = sum(range(known_args.num_documents))
+  mod_3_sum = sum(
+      num for num in range(known_args.num_documents) if num % 3 == 0)
+  mod_3_count = sum(
+      1 for num in range(known_args.num_documents) if num % 3 == 0)
+  # yapf: disable
+  read_cases = [
+      # (reader_params, expected)
+      (
+          {
+              'projection': ['number']
+          },
+          {
+              'number_sum': total_sum,
+              'docs_count': known_args.num_documents
+          }
+      ),
+      (
+          {
+              'filter': {'number_mod_3': 0},
+              'projection': ['number']
+          },
+          {
+              'number_sum': mod_3_sum,
+              'docs_count': mod_3_count
+          }
+      ),
+      (
+          {
+              'projection': ['number'],
+              'bucket_auto': True
+          },
+          {
+              'number_sum': total_sum,
+              'docs_count': known_args.num_documents
+          }
+      ),
+      (
+          {
+              'filter': {'number_mod_3': 0},
+              'projection': ['number'],
+              'bucket_auto': True
+          },
+          {
+              'number_sum': mod_3_sum,
+              'docs_count': mod_3_count
+          }
+      ),
+  ]
+  # yapf: enable
+  for reader_params, expected in read_cases:
+    with TestPipeline(options=PipelineOptions(pipeline_args)) as p:
+      start_time = time.time()
+      _LOGGER.info('=' * 80)
+      _LOGGER.info(
+          'Reading from mongodb %s:%s',
+          known_args.mongo_db,
+          known_args.mongo_coll)
+      _LOGGER.info('reader params   : %s', reader_params)
+      _LOGGER.info('expected results: %s', expected)
+      docs = (
+          p | 'ReadFromMongoDB' >> beam.io.ReadFromMongoDB(
+              known_args.mongo_uri,
+              known_args.mongo_db,
+              known_args.mongo_coll,
+              **reader_params)
+          | 'Map' >> beam.Map(lambda doc: doc['number']))
+      number_sum = (docs | 'Combine' >> beam.CombineGlobally(sum))
+      docs_count = (docs | 'Count' >> beam.combiners.Count.Globally())
+      r = ([number_sum, docs_count] | 'Flatten' >> beam.Flatten())
+      assert_that(r, equal_to([expected['number_sum'], expected['docs_count']]))
+
+    elapsed = time.time() - start_time
     _LOGGER.info(
-        'Reading from mongodb %s:%s' %
-        (known_args.mongo_db, known_args.mongo_coll))
-    r = (
-        p | 'ReadFromMongoDB' >> beam.io.ReadFromMongoDB(
-            known_args.mongo_uri,
-            known_args.mongo_db,
-            known_args.mongo_coll,
-            projection=['number'])
-        | 'Map' >> beam.Map(lambda doc: doc['number'])
-        | 'Combine' >> beam.CombineGlobally(sum))
-    assert_that(r, equal_to([sum(range(known_args.num_documents))]))
+        'Reading documents from mongodb finished in %.3f seconds', elapsed)
 
-  elapsed = time.time() - start_time
-  _LOGGER.info(
-      'Read %d documents from mongodb finished in %.3f seconds' %
-      (known_args.num_documents, elapsed))
+  # Clean-up
   with MongoClient(host=known_args.mongo_uri) as client:
     client.drop_database(known_args.mongo_db)
 
diff --git a/sdks/python/apache_beam/io/mongodbio_test.py b/sdks/python/apache_beam/io/mongodbio_test.py
index e327763b90d..dbcd7001c9d 100644
--- a/sdks/python/apache_beam/io/mongodbio_test.py
+++ b/sdks/python/apache_beam/io/mongodbio_test.py
@@ -28,6 +28,7 @@ from unittest import TestCase
 
 import mock
 from bson import objectid
+from parameterized import parameterized_class
 from pymongo import ASCENDING
 from pymongo import ReplaceOne
 
@@ -51,29 +52,65 @@ class _MockMongoColl(object):
   def __init__(self, docs):
     self.docs = docs
 
-  def _filter(self, filter, projection):
-    projection = [] if projection is None else projection
+  def __getitem__(self, index):
+    return self.docs[index]
+
+  def __len__(self):
+    return len(self.docs)
+
+  @staticmethod
+  def _make_filter(conditions):
+    assert isinstance(conditions, dict)
+    checks = []
+    for field, value in conditions.items():
+      if isinstance(value, dict):
+        for op, val in value.items():
+          if op == '$gte':
+            op = '__ge__'
+          elif op == '$lt':
+            op = '__lt__'
+          else:
+            raise Exception('Operator "{0}" not supported.'.format(op))
+          checks.append((field, op, val))
+      else:
+        checks.append((field, '__eq__', value))
+
+    def func(doc):
+      for field, op, value in checks:
+        if not getattr(doc[field], op)(value):
+          return False
+      return True
+
+    return func
+
+  def _filter(self, filter):
     match = []
     if not filter:
       return self
-    if '$and' not in filter or not filter['$and']:
-      return self
-    start = filter['$and'][1]['_id'].get('$gte')
-    end = filter['$and'][1]['_id'].get('$lt')
-    assert start is not None
-    assert end is not None
+    all_filters = []
+    if '$and' in filter:
+      for item in filter['$and']:
+        all_filters.append(self._make_filter(item))
+    else:
+      all_filters.append(self._make_filter(filter))
+
     for doc in self.docs:
-      if start and doc['_id'] < start:
-        continue
-      if end and doc['_id'] >= end:
+      if not all(check(doc) for check in all_filters):
         continue
-      if len(projection) > 0:
-        doc = {k: v for k, v in doc.items() if k in projection or k == '_id'}
       match.append(doc)
+
     return match
 
+  @staticmethod
+  def _projection(docs, projection=None):
+    if projection:
+      return [{k: v
+               for k, v in doc.items() if k in projection or k == '_id'}
+              for doc in docs]
+    return docs
+
   def find(self, filter=None, projection=None, **kwargs):
-    return _MockMongoColl(self._filter(filter, projection))
+    return _MockMongoColl(self._projection(self._filter(filter), projection))
 
   def sort(self, sort_items):
     key, order = sort_items[0]
@@ -87,8 +124,48 @@ class _MockMongoColl(object):
   def count_documents(self, filter):
     return len(self._filter(filter))
 
-  def __getitem__(self, index):
-    return self.docs[index]
+  def aggregate(self, pipeline):
+    # Simulate $bucketAuto aggregate pipeline.
+    # Example splits doc count for the total of 5 docs:
+    #   - 1 bucket:  [5]
+    #   - 2 buckets: [3, 2]
+    #   - 3 buckets: [2, 2, 1]
+    #   - 4 buckets: [2, 1, 1, 1]
+    #   - 5 buckets: [1, 1, 1, 1, 1]
+    match_step = next((step for step in pipeline if '$match' in step), None)
+    bucket_auto_step = next(step for step in pipeline if '$bucketAuto' in step)
+    if match_step is None:
+      docs = self.docs
+    else:
+      docs = self.find(filter=match_step['$match'])
+    doc_count = len(docs)
+    bucket_count = min(bucket_auto_step['$bucketAuto']['buckets'], doc_count)
+    # bucket_count â‰  0
+    bucket_len, remainder = divmod(doc_count, bucket_count)
+    bucket_sizes = (
+        remainder * [bucket_len + 1] +
+        (bucket_count - remainder) * [bucket_len])
+    buckets = []
+    start = 0
+    for bucket_size in bucket_sizes:
+      stop = start + bucket_size
+      if stop >= doc_count:
+        # MongoDB: the last bucket's 'max' is inclusive
+        stop = doc_count - 1
+        count = stop - start + 1
+      else:
+        # non-last bucket's 'max' is exclusive and == next bucket's 'min'
+        count = stop - start
+      buckets.append({
+          '_id': {
+              'min': docs[start]['_id'],
+              'max': docs[stop]['_id'],
+          },
+          'count': count
+      })
+      start = stop
+
+    return buckets
 
 
 class _MockMongoDb(object):
@@ -101,7 +178,7 @@ class _MockMongoDb(object):
 
   def command(self, command, *args, **kwargs):
     if command == 'collstats':
-      return {'size': 5, 'avgSize': 1}
+      return {'size': 5 * 1024 * 1024, 'avgObjSize': 1 * 1024 * 1024}
     elif command == 'splitVector':
       return self.get_split_keys(command, *args, **kwargs)
 
@@ -146,6 +223,7 @@ class _MockMongoClient(object):
     pass
 
 
+@parameterized_class(('bucket_auto', ), [(None, ), (True, )])
 class MongoSourceTest(unittest.TestCase):
   @mock.patch('apache_beam.io.mongodbio.MongoClient')
   def setUp(self, mock_client):
@@ -157,23 +235,39 @@ class MongoSourceTest(unittest.TestCase):
     self._docs = [{'_id': self._ids[i], 'x': i} for i in range(len(self._ids))]
     mock_client.return_value = _MockMongoClient(self._docs)
 
-    self.mongo_source = _BoundedMongoSource(
-        'mongodb://test', 'testdb', 'testcoll')
+    self.mongo_source = self._create_source(bucket_auto=self.bucket_auto)
+
+  @staticmethod
+  def _create_source(filter=None, bucket_auto=None):
+    kwargs = {}
+    if filter is not None:
+      kwargs['filter'] = filter
+    if bucket_auto is not None:
+      kwargs['bucket_auto'] = bucket_auto
+    return _BoundedMongoSource('mongodb://test', 'testdb', 'testcoll', **kwargs)
 
   @mock.patch('apache_beam.io.mongodbio.MongoClient')
   def test_estimate_size(self, mock_client):
     mock_client.return_value = _MockMongoClient(self._docs)
-    self.assertEqual(self.mongo_source.estimate_size(), 5)
+    self.assertEqual(self.mongo_source.estimate_size(), 5 * 1024 * 1024)
+
+  @mock.patch('apache_beam.io.mongodbio.MongoClient')
+  def test_estimate_average_document_size(self, mock_client):
+    mock_client.return_value = _MockMongoClient(self._docs)
+    self.assertEqual(
+        self.mongo_source._estimate_average_document_size(), 1 * 1024 * 1024)
 
   @mock.patch('apache_beam.io.mongodbio.MongoClient')
   def test_split(self, mock_client):
     mock_client.return_value = _MockMongoClient(self._docs)
-    for size in [i * 1024 * 1024 for i in (1, 2, 10)]:
+    for size_mb, expected_split_count in [(0.5, 5), (1, 5), (2, 3), (10, 1)]:
+      size = size_mb * 1024 * 1024
       splits = list(
           self.mongo_source.split(
               start_position=None, stop_position=None,
               desired_bundle_size=size))
 
+      self.assertEqual(len(splits), expected_split_count)
       reference_info = (self.mongo_source, None, None)
       sources_info = ([
           (split.source, split.start_position, split.stop_position)
@@ -182,6 +276,101 @@ class MongoSourceTest(unittest.TestCase):
       source_test_utils.assert_sources_equal_reference_source(
           reference_info, sources_info)
 
+  @mock.patch('apache_beam.io.mongodbio.MongoClient')
+  def test_split_single_document(self, mock_client):
+    mock_client.return_value = _MockMongoClient(self._docs[0:1])
+    for size_mb in [1, 5]:
+      size = size_mb * 1024 * 1024
+      splits = list(
+          self.mongo_source.split(
+              start_position=None, stop_position=None,
+              desired_bundle_size=size))
+      self.assertEqual(len(splits), 1)
+      self.assertEqual(splits[0].start_position, self._docs[0]['_id'])
+      self.assertEqual(
+          splits[0].stop_position,
+          _ObjectIdHelper.increment_id(self._docs[0]['_id'], 1))
+
+  @mock.patch('apache_beam.io.mongodbio.MongoClient')
+  def test_split_no_documents(self, mock_client):
+    mock_client.return_value = _MockMongoClient([])
+    with self.assertRaises(ValueError) as cm:
+      list(
+          self.mongo_source.split(
+              start_position=None,
+              stop_position=None,
+              desired_bundle_size=1024 * 1024))
+    self.assertEqual(str(cm.exception), 'Empty Mongodb collection')
+
+  @mock.patch('apache_beam.io.mongodbio.MongoClient')
+  def test_split_filtered(self, mock_client):
+    # filtering 2 documents: 2 <= 'x' < 4
+    filtered_mongo_source = self._create_source(
+        filter={'x': {
+            '$gte': 2, '$lt': 4
+        }}, bucket_auto=self.bucket_auto)
+
+    mock_client.return_value = _MockMongoClient(self._docs)
+    for size_mb, (bucket_auto_count, split_vector_count) in [(1, (2, 5)),
+                                                             (2, (1, 3)),
+                                                             (10, (1, 1))]:
+      size = size_mb * 1024 * 1024
+      splits = list(
+          filtered_mongo_source.split(
+              start_position=None, stop_position=None,
+              desired_bundle_size=size))
+
+      if self.bucket_auto:
+        self.assertEqual(len(splits), bucket_auto_count)
+      else:
+        # Note: splitVector mode does not respect filter
+        self.assertEqual(len(splits), split_vector_count)
+      reference_info = (
+          filtered_mongo_source, self._docs[2]['_id'], self._docs[4]['_id'])
+      sources_info = ([
+          (split.source, split.start_position, split.stop_position)
+          for split in splits
+      ])
+      source_test_utils.assert_sources_equal_reference_source(
+          reference_info, sources_info)
+
+  @mock.patch('apache_beam.io.mongodbio.MongoClient')
+  def test_split_filtered_empty(self, mock_client):
+    # filtering doesn't match any documents
+    filtered_mongo_source = self._create_source(
+        filter={'x': {
+            '$lt': 0
+        }}, bucket_auto=self.bucket_auto)
+
+    mock_client.return_value = _MockMongoClient(self._docs)
+    for size_mb, (bucket_auto_count, split_vector_count) in [(1, (1, 5)),
+                                                             (2, (1, 3)),
+                                                             (10, (1, 1))]:
+      size = size_mb * 1024 * 1024
+      splits = list(
+          filtered_mongo_source.split(
+              start_position=None, stop_position=None,
+              desired_bundle_size=size))
+
+      if self.bucket_auto:
+        # Note: if filter matches no docs - one split covers entire range
+        self.assertEqual(len(splits), bucket_auto_count)
+      else:
+        # Note: splitVector mode does not respect filter
+        self.assertEqual(len(splits), split_vector_count)
+      reference_info = (
+          filtered_mongo_source,
+          # range to match no documents:
+          _ObjectIdHelper.increment_id(self._docs[-1]['_id'], 1),
+          _ObjectIdHelper.increment_id(self._docs[-1]['_id'], 2),
+      )
+      sources_info = ([
+          (split.source, split.start_position, split.stop_position)
+          for split in splits
+      ])
+      source_test_utils.assert_sources_equal_reference_source(
+          reference_info, sources_info)
+
   @mock.patch('apache_beam.io.mongodbio.MongoClient')
   def test_dynamic_work_rebalancing(self, mock_client):
     mock_client.return_value = _MockMongoClient(self._docs)
@@ -246,6 +435,7 @@ class MongoSourceTest(unittest.TestCase):
     self.assertTrue('collection' in data)
 
 
+@parameterized_class(('bucket_auto', ), [(False, ), (True, )])
 class ReadFromMongoDBTest(unittest.TestCase):
   @mock.patch('apache_beam.io.mongodbio.MongoClient')
   def test_read_from_mongodb(self, mock_client):
@@ -265,7 +455,8 @@ class ReadFromMongoDBTest(unittest.TestCase):
           uri='mongodb://test',
           db='db',
           coll='collection',
-          projection=projection)
+          projection=projection,
+          bucket_auto=self.bucket_auto)
       assert_that(docs, equal_to(projected_documents))
 
 
