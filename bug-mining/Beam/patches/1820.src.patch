diff --git a/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/ArtifactResolver.java b/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/ArtifactResolver.java
index 6cad644a2b4..08d024552f2 100644
--- a/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/ArtifactResolver.java
+++ b/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/ArtifactResolver.java
@@ -40,6 +40,10 @@ public interface ArtifactResolver {
   /** Updating pipeline proto by applying registered {@link ResolutionFn}s. */
   RunnerApi.Pipeline resolveArtifacts(RunnerApi.Pipeline pipeline);
 
+  /** Resolves a list of artifacts by applying registered {@link ResolutionFn}s. */
+  List<RunnerApi.ArtifactInformation> resolveArtifacts(
+      List<RunnerApi.ArtifactInformation> artifacts);
+
   /**
    * A lazy transformer for resolving {@link RunnerApi.ArtifactInformation}. Note that {@link
    * ResolutionFn} postpones any side-effect until {@link
diff --git a/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/DefaultArtifactResolver.java b/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/DefaultArtifactResolver.java
index f3447a99e04..3c75579c41a 100644
--- a/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/DefaultArtifactResolver.java
+++ b/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/DefaultArtifactResolver.java
@@ -17,6 +17,7 @@
  */
 package org.apache.beam.runners.core.construction;
 
+import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
 import java.util.Optional;
@@ -62,6 +63,24 @@ public class DefaultArtifactResolver implements ArtifactResolver {
     fns.add(fn);
   }
 
+  @Override
+  public List<RunnerApi.ArtifactInformation> resolveArtifacts(
+      List<RunnerApi.ArtifactInformation> artifacts) {
+    for (ResolutionFn fn : Lists.reverse(fns)) {
+      List<RunnerApi.ArtifactInformation> moreResolved = new ArrayList<>();
+      for (RunnerApi.ArtifactInformation artifact : artifacts) {
+        Optional<List<RunnerApi.ArtifactInformation>> resolved = fn.resolve(artifact);
+        if (resolved.isPresent()) {
+          moreResolved.addAll(resolved.get());
+        } else {
+          moreResolved.add(artifact);
+        }
+      }
+      artifacts = moreResolved;
+    }
+    return artifacts;
+  }
+
   @Override
   public RunnerApi.Pipeline resolveArtifacts(RunnerApi.Pipeline pipeline) {
     ImmutableMap.Builder<String, RunnerApi.Environment> environmentMapBuilder =
diff --git a/runners/java-fn-execution/src/main/java/org/apache/beam/runners/fnexecution/artifact/ArtifactRetrievalService.java b/runners/java-fn-execution/src/main/java/org/apache/beam/runners/fnexecution/artifact/ArtifactRetrievalService.java
index 3c2f1f68e53..059eee77895 100644
--- a/runners/java-fn-execution/src/main/java/org/apache/beam/runners/fnexecution/artifact/ArtifactRetrievalService.java
+++ b/runners/java-fn-execution/src/main/java/org/apache/beam/runners/fnexecution/artifact/ArtifactRetrievalService.java
@@ -25,7 +25,9 @@ import java.nio.channels.Channels;
 import org.apache.beam.model.jobmanagement.v1.ArtifactApi;
 import org.apache.beam.model.jobmanagement.v1.ArtifactRetrievalServiceGrpc;
 import org.apache.beam.model.pipeline.v1.RunnerApi;
+import org.apache.beam.runners.core.construction.ArtifactResolver;
 import org.apache.beam.runners.core.construction.BeamUrns;
+import org.apache.beam.runners.core.construction.DefaultArtifactResolver;
 import org.apache.beam.runners.fnexecution.FnService;
 import org.apache.beam.sdk.io.FileSystems;
 import org.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString;
@@ -51,13 +53,24 @@ public class ArtifactRetrievalService
             BeamUrns.getUrn(RunnerApi.StandardArtifacts.Roles.STAGING_TO)));
   }
 
+  private final ArtifactResolver resolver;
+
   private final int bufferSize;
 
   public ArtifactRetrievalService() {
     this(DEFAULT_BUFFER_SIZE);
   }
 
+  public ArtifactRetrievalService(ArtifactResolver resolver) {
+    this(resolver, DEFAULT_BUFFER_SIZE);
+  }
+
   public ArtifactRetrievalService(int bufferSize) {
+    this(DefaultArtifactResolver.INSTANCE, bufferSize);
+  }
+
+  public ArtifactRetrievalService(ArtifactResolver resolver, int bufferSize) {
+    this.resolver = resolver;
     this.bufferSize = bufferSize;
   }
 
@@ -67,7 +80,7 @@ public class ArtifactRetrievalService
       StreamObserver<ArtifactApi.ResolveArtifactsResponse> responseObserver) {
     responseObserver.onNext(
         ArtifactApi.ResolveArtifactsResponse.newBuilder()
-            .addAllReplacements(request.getArtifactsList())
+            .addAllReplacements(resolver.resolveArtifacts(request.getArtifactsList()))
             .build());
     responseObserver.onCompleted();
   }
diff --git a/runners/spark/build.gradle b/runners/spark/build.gradle
index 2325e2b238d..f1561089aa1 100644
--- a/runners/spark/build.gradle
+++ b/runners/spark/build.gradle
@@ -61,6 +61,7 @@ dependencies {
   compile project(":runners:core-construction-java")
   compile project(":runners:core-java")
   compile project(":runners:java-fn-execution")
+  compile project(":runners:java-job-service")
   compile project(":sdks:java:extensions:google-cloud-platform-core")
   compile library.java.jackson_annotations
   compile library.java.slf4j_api
@@ -238,7 +239,7 @@ task validatesRunner {
   description "Validates Spark runner"
   dependsOn validatesRunnerBatch
   dependsOn validatesRunnerStreaming
-  // It should be uncommented once all "validatesStructuredStreamingRunnerBatch" tests will pass. 
+  // It should be uncommented once all "validatesStructuredStreamingRunnerBatch" tests will pass.
   // Otherwise, it breaks Spark runner ValidatesRunner tests.
   //dependsOn validatesStructuredStreamingRunnerBatch
 }
diff --git a/sdks/java/expansion-service/build.gradle b/sdks/java/expansion-service/build.gradle
index def9e809141..d981fc3b0db 100644
--- a/sdks/java/expansion-service/build.gradle
+++ b/sdks/java/expansion-service/build.gradle
@@ -36,6 +36,7 @@ dependencies {
   compile project(path: ":model:fn-execution", configuration: "shadow")
   compile project(path: ":sdks:java:core", configuration: "shadow")
   compile project(path: ":runners:core-construction-java")
+  compile project(path: ":runners:java-fn-execution")
   compile library.java.vendored_grpc_1_26_0
   compile library.java.vendored_guava_26_0_jre
   compile library.java.slf4j_api
diff --git a/sdks/java/expansion-service/src/main/java/org/apache/beam/sdk/expansion/service/ExpansionService.java b/sdks/java/expansion-service/src/main/java/org/apache/beam/sdk/expansion/service/ExpansionService.java
index e19754dd4ac..578e8758343 100644
--- a/sdks/java/expansion-service/src/main/java/org/apache/beam/sdk/expansion/service/ExpansionService.java
+++ b/sdks/java/expansion-service/src/main/java/org/apache/beam/sdk/expansion/service/ExpansionService.java
@@ -17,6 +17,8 @@
  */
 package org.apache.beam.sdk.expansion.service;
 
+import static org.apache.beam.runners.core.construction.resources.PipelineResources.detectClassPathResourcesToStage;
+
 import com.google.auto.service.AutoService;
 import java.io.IOException;
 import java.lang.reflect.Constructor;
@@ -41,10 +43,12 @@ import org.apache.beam.runners.core.construction.Environments;
 import org.apache.beam.runners.core.construction.PipelineTranslation;
 import org.apache.beam.runners.core.construction.RehydratedComponents;
 import org.apache.beam.runners.core.construction.SdkComponents;
+import org.apache.beam.runners.fnexecution.artifact.ArtifactRetrievalService;
 import org.apache.beam.sdk.Pipeline;
 import org.apache.beam.sdk.coders.Coder;
 import org.apache.beam.sdk.expansion.ExternalTransformRegistrar;
 import org.apache.beam.sdk.options.ExperimentalOptions;
+import org.apache.beam.sdk.options.PortablePipelineOptions;
 import org.apache.beam.sdk.transforms.ExternalTransformBuilder;
 import org.apache.beam.sdk.transforms.PTransform;
 import org.apache.beam.sdk.values.PCollection;
@@ -317,6 +321,14 @@ public class ExpansionService extends ExpansionServiceGrpc.ExpansionServiceImplB
     Pipeline pipeline = Pipeline.create();
     ExperimentalOptions.addExperiment(
         pipeline.getOptions().as(ExperimentalOptions.class), "beam_fn_api");
+    List<String> classpathResources =
+        detectClassPathResourcesToStage(Environments.class.getClassLoader(), pipeline.getOptions());
+    if (classpathResources.isEmpty()) {
+      throw new IllegalArgumentException("No classpath elements found.");
+    }
+    LOG.debug("Staging to files from the classpath: {}", classpathResources.size());
+    pipeline.getOptions().as(PortablePipelineOptions.class).setFilesToStage(classpathResources);
+
     RehydratedComponents rehydratedComponents =
         RehydratedComponents.forComponents(request.getComponents()).withPipeline(pipeline);
 
@@ -348,7 +360,9 @@ public class ExpansionService extends ExpansionServiceGrpc.ExpansionServiceImplB
     // Needed to find which transform was new...
     SdkComponents sdkComponents =
         rehydratedComponents.getSdkComponents(null).withNewIdPrefix(request.getNamespace());
-    sdkComponents.registerEnvironment(Environments.JAVA_SDK_HARNESS_ENVIRONMENT);
+    sdkComponents.registerEnvironment(
+        Environments.createOrGetDefaultEnvironment(
+            pipeline.getOptions().as(PortablePipelineOptions.class)));
     Map<String, String> outputMap =
         outputs.entrySet().stream()
             .collect(
@@ -413,7 +427,12 @@ public class ExpansionService extends ExpansionServiceGrpc.ExpansionServiceImplB
     for (Map.Entry<String, TransformProvider> entry : service.registeredTransforms.entrySet()) {
       System.out.println("\t" + entry.getKey() + ": " + entry.getValue());
     }
-    Server server = ServerBuilder.forPort(port).addService(service).build();
+
+    Server server =
+        ServerBuilder.forPort(port)
+            .addService(service)
+            .addService(new ArtifactRetrievalService())
+            .build();
     server.start();
     server.awaitTermination();
   }
diff --git a/sdks/python/apache_beam/examples/wordcount_xlang_sql.py b/sdks/python/apache_beam/examples/wordcount_xlang_sql.py
index 4d2054a1ab8..410c8a65cb1 100644
--- a/sdks/python/apache_beam/examples/wordcount_xlang_sql.py
+++ b/sdks/python/apache_beam/examples/wordcount_xlang_sql.py
@@ -32,7 +32,6 @@ from apache_beam.io import WriteToText
 from apache_beam.options.pipeline_options import PipelineOptions
 from apache_beam.options.pipeline_options import SetupOptions
 from apache_beam.transforms.sql import SqlTransform
-from apache_beam.utils import subprocess_server
 
 MyRow = typing.NamedTuple('MyRow', [('word', unicode)])
 coders.registry.register_coder(MyRow, coders.RowCoder)
@@ -96,11 +95,6 @@ def main():
 
   known_args, pipeline_args = parser.parse_known_args()
 
-  path_to_jar = subprocess_server.JavaJarServer.path_to_beam_jar(
-      ":sdks:java:extensions:sql:expansion-service:shadowJar")
-  # TODO(BEAM-9238): Remove this when it's no longer needed for artifact
-  # staging.
-  pipeline_args.extend(['--experiment', 'jar_packages=%s' % path_to_jar])
   pipeline_options = PipelineOptions(pipeline_args)
 
   # We use the save_main_session option because one or more DoFn's in this
diff --git a/sdks/python/apache_beam/pipeline.py b/sdks/python/apache_beam/pipeline.py
index e14045e9018..c4d0bbc8ff4 100644
--- a/sdks/python/apache_beam/pipeline.py
+++ b/sdks/python/apache_beam/pipeline.py
@@ -213,6 +213,8 @@ class Pipeline(object):
         experiments.append('beam_fn_api')
         self._options.view_as(DebugOptions).experiments = experiments
 
+    self.local_tempdir = tempfile.mkdtemp(prefix='beam-pipeline-temp')
+
     # Default runner to be used.
     self.runner = runner
     # Stack of transforms generated by nested apply() calls. The stack will
@@ -497,32 +499,36 @@ class Pipeline(object):
 
     """Runs the pipeline. Returns whatever our runner returns after running."""
 
-    if test_runner_api == 'AUTO':
-      # Don't pay the cost of a round-trip if we're going to be going through
-      # the FnApi anyway...
-      test_runner_api = (
-          not self.runner.is_fnapi_compatible() and (
-              self.runner.__class__.__name__ != 'SwitchingDirectRunner' or
-              self._options.view_as(StandardOptions).streaming))
-
-    # When possible, invoke a round trip through the runner API.
-    if test_runner_api and self._verify_runner_api_compatible():
-      return Pipeline.from_runner_api(
-          self.to_runner_api(use_fake_coders=True), self.runner,
-          self._options).run(False)
-
-    if self._options.view_as(TypeOptions).runtime_type_check:
-      from apache_beam.typehints import typecheck
-      self.visit(typecheck.TypeCheckVisitor())
-
-    if self._options.view_as(SetupOptions).save_main_session:
-      # If this option is chosen, verify we can pickle the main session early.
-      tmpdir = tempfile.mkdtemp()
-      try:
-        pickler.dump_session(os.path.join(tmpdir, 'main_session.pickle'))
-      finally:
-        shutil.rmtree(tmpdir)
-    return self.runner.run_pipeline(self, self._options)
+    try:
+      if test_runner_api == 'AUTO':
+        # Don't pay the cost of a round-trip if we're going to be going through
+        # the FnApi anyway...
+        test_runner_api = (
+            not self.runner.is_fnapi_compatible() and (
+                self.runner.__class__.__name__ != 'SwitchingDirectRunner' or
+                self._options.view_as(StandardOptions).streaming))
+
+      # When possible, invoke a round trip through the runner API.
+      if test_runner_api and self._verify_runner_api_compatible():
+        return Pipeline.from_runner_api(
+            self.to_runner_api(use_fake_coders=True),
+            self.runner,
+            self._options).run(False)
+
+      if self._options.view_as(TypeOptions).runtime_type_check:
+        from apache_beam.typehints import typecheck
+        self.visit(typecheck.TypeCheckVisitor())
+
+      if self._options.view_as(SetupOptions).save_main_session:
+        # If this option is chosen, verify we can pickle the main session early.
+        tmpdir = tempfile.mkdtemp()
+        try:
+          pickler.dump_session(os.path.join(tmpdir, 'main_session.pickle'))
+        finally:
+          shutil.rmtree(tmpdir)
+      return self.runner.run_pipeline(self, self._options)
+    finally:
+      shutil.rmtree(self.local_tempdir, ignore_errors=True)
 
   def __enter__(self):
     # type: () -> Pipeline
@@ -531,11 +537,12 @@ class Pipeline(object):
     self._extra_context.__enter__()
     return self
 
-  def __exit__(self,
-               exc_type,  # type: Optional[Type[BaseException]]
-               exc_val,  # type: Optional[BaseException]
-               exc_tb  # type: Optional[TracebackType]
-              ):
+  def __exit__(
+      self,
+      exc_type,  # type: Optional[Type[BaseException]]
+      exc_val,  # type: Optional[BaseException]
+      exc_tb  # type: Optional[TracebackType]
+  ):
     # type: (...) -> None
 
     try:
@@ -572,7 +579,7 @@ class Pipeline(object):
       transform,  # type: ptransform.PTransform
       pvalueish=None,  # type: Optional[pvalue.PValue]
       label=None  # type: Optional[str]
-    ):
+  ):
     # type: (...) -> pvalue.PValue
 
     """Applies a custom transform using the pvalueish specified.
@@ -792,12 +799,13 @@ class Pipeline(object):
     self.visit(Visitor())
     return Visitor.ok
 
-  def to_runner_api(self,
-                    return_context=False,  # type: bool
-                    context=None,  # type: Optional[PipelineContext]
-                    use_fake_coders=False,  # type: bool
-                    default_environment=None  # type: Optional[environments.Environment]
-                   ):
+  def to_runner_api(
+      self,
+      return_context=False,  # type: bool
+      context=None,  # type: Optional[PipelineContext]
+      use_fake_coders=False,  # type: bool
+      default_environment=None  # type: Optional[environments.Environment]
+  ):
     # type: (...) -> beam_runner_api_pb2.Pipeline
 
     """For internal use only; no backwards-compatibility guarantees."""
@@ -860,12 +868,13 @@ class Pipeline(object):
       return proto
 
   @staticmethod
-  def from_runner_api(proto,  # type: beam_runner_api_pb2.Pipeline
-                      runner,  # type: PipelineRunner
-                      options,  # type: PipelineOptions
-                      return_context=False,  # type: bool
-                      allow_proto_holders=False  # type: bool
-                     ):
+  def from_runner_api(
+      proto,  # type: beam_runner_api_pb2.Pipeline
+      runner,  # type: PipelineRunner
+      options,  # type: PipelineOptions
+      return_context=False,  # type: bool
+      allow_proto_holders=False  # type: bool
+  ):
     # type: (...) -> Pipeline
 
     """For internal use only; no backwards-compatibility guarantees."""
@@ -946,15 +955,15 @@ class AppliedPTransform(object):
   A transform node representing an instance of applying a PTransform
   (used internally by Pipeline for bookeeping purposes).
   """
-
-  def __init__(self,
-               parent,  # type:  Optional[AppliedPTransform]
-               transform,  # type: Optional[ptransform.PTransform]
-               full_label,  # type: str
-               inputs,  # type: Optional[Sequence[Union[pvalue.PBegin, pvalue.PCollection]]]
-               environment_id=None,  # type: Optional[str]
-               input_tags_to_preserve=None,  # type: Dict[pvalue.PCollection, str]
-              ):
+  def __init__(
+      self,
+      parent,  # type:  Optional[AppliedPTransform]
+      transform,  # type: Optional[ptransform.PTransform]
+      full_label,  # type: str
+      inputs,  # type: Optional[Sequence[Union[pvalue.PBegin, pvalue.PCollection]]]
+      environment_id=None,  # type: Optional[str]
+      input_tags_to_preserve=None,  # type: Dict[pvalue.PCollection, str]
+  ):
     # type: (...) -> None
     self.parent = parent
     self.transform = transform
@@ -977,10 +986,11 @@ class AppliedPTransform(object):
     return "%s(%s, %s)" % (
         self.__class__.__name__, self.full_label, type(self.transform).__name__)
 
-  def replace_output(self,
-                     output,  # type: Union[pvalue.PValue, pvalue.DoOutputsTuple]
-                     tag=None  # type: Union[str, int, None]
-                    ):
+  def replace_output(
+      self,
+      output,  # type: Union[pvalue.PValue, pvalue.DoOutputsTuple]
+      tag=None  # type: Union[str, int, None]
+  ):
     # type: (...) -> None
 
     """Replaces the output defined by the given tag with the given output.
@@ -999,10 +1009,11 @@ class AppliedPTransform(object):
     else:
       raise TypeError("Unexpected output type: %s" % output)
 
-  def add_output(self,
-                 output,  # type: Union[pvalue.DoOutputsTuple, pvalue.PValue]
-                 tag  # type: Union[str, int, None]
-                ):
+  def add_output(
+      self,
+      output,  # type: Union[pvalue.DoOutputsTuple, pvalue.PValue]
+      tag  # type: Union[str, int, None]
+  ):
     # type: (...) -> None
     if isinstance(output, pvalue.DoOutputsTuple):
       self.add_output(output[tag], tag)
@@ -1029,11 +1040,12 @@ class AppliedPTransform(object):
     return bool(self.parts) or all(
         pval.producer is not self for pval in self.outputs.values())
 
-  def visit(self,
-            visitor,  # type: PipelineVisitor
-            pipeline,  # type: Pipeline
-            visited  # type: Set[pvalue.PValue]
-           ):
+  def visit(
+      self,
+      visitor,  # type: PipelineVisitor
+      pipeline,  # type: Pipeline
+      visited  # type: Set[pvalue.PValue]
+  ):
     # type: (...) -> None
 
     """Visits all nodes reachable from the current node."""
@@ -1115,9 +1127,10 @@ class AppliedPTransform(object):
 
     from apache_beam.portability.api import beam_runner_api_pb2
 
-    def transform_to_runner_api(transform,  # type: Optional[ptransform.PTransform]
-                                context  # type: PipelineContext
-                               ):
+    def transform_to_runner_api(
+        transform,  # type: Optional[ptransform.PTransform]
+        context  # type: PipelineContext
+    ):
       # type: (...) -> Optional[beam_runner_api_pb2.FunctionSpec]
       if transform is None:
         return None
@@ -1168,9 +1181,10 @@ class AppliedPTransform(object):
         display_data=None)
 
   @staticmethod
-  def from_runner_api(proto,  # type: beam_runner_api_pb2.PTransform
-                      context  # type: PipelineContext
-                     ):
+  def from_runner_api(
+      proto,  # type: beam_runner_api_pb2.PTransform
+      context  # type: PipelineContext
+  ):
     # type: (...) -> AppliedPTransform
 
     if common_urns.primitives.PAR_DO.urn == proto.spec.urn:
diff --git a/sdks/python/apache_beam/runners/portability/artifact_service.py b/sdks/python/apache_beam/runners/portability/artifact_service.py
index 55a26684d93..4879ecd6ebe 100644
--- a/sdks/python/apache_beam/runners/portability/artifact_service.py
+++ b/sdks/python/apache_beam/runners/portability/artifact_service.py
@@ -31,6 +31,7 @@ import hashlib
 import os
 import queue
 import sys
+import tempfile
 import threading
 import typing
 import zipfile
@@ -536,3 +537,55 @@ class BeamFilesystemHandler(object):
   def file_writer(self, name=None):
     full_path = filesystems.FileSystems.join(self._root, name)
     return filesystems.FileSystems.create(full_path), full_path
+
+
+def resolve_artifacts(artifacts, service, dest_dir):
+  if not artifacts:
+    return artifacts
+  else:
+    return [
+        maybe_store_artifact(artifact, service,
+                             dest_dir) for artifact in service.ResolveArtifacts(
+                                 beam_artifact_api_pb2.ResolveArtifactsRequest(
+                                     artifacts=artifacts)).replacements
+    ]
+
+
+def maybe_store_artifact(artifact, service, dest_dir):
+  if artifact.type_urn in (common_urns.artifact_types.URL.urn,
+                           common_urns.artifact_types.EMBEDDED.urn):
+    return artifact
+  elif artifact.type_urn == common_urns.artifact_types.FILE.urn:
+    payload = beam_runner_api_pb2.ArtifactFilePayload.FromString(
+        artifact.type_payload)
+    if os.path.exists(
+        payload.path) and payload.sha256 and payload.sha256 == sha256(
+            payload.path) and False:
+      return artifact
+    else:
+      return store_artifact(artifact, service, dest_dir)
+  else:
+    return store_artifact(artifact, service, dest_dir)
+
+
+def store_artifact(artifact, service, dest_dir):
+  hasher = hashlib.sha256()
+  with tempfile.NamedTemporaryFile(dir=dest_dir, delete=False) as fout:
+    for block in service.GetArtifact(
+        beam_artifact_api_pb2.GetArtifactRequest(artifact=artifact)):
+      hasher.update(block.data)
+      fout.write(block.data)
+  return beam_runner_api_pb2.ArtifactInformation(
+      type_urn=common_urns.artifact_types.FILE.urn,
+      type_payload=beam_runner_api_pb2.ArtifactFilePayload(
+          path=fout.name, sha256=hasher.hexdigest()).SerializeToString(),
+      role_urn=artifact.role_urn,
+      role_payload=artifact.role_payload)
+
+
+def sha256(path):
+  hasher = hashlib.sha256()
+  with open(path, 'rb') as fin:
+    for block in iter(lambda: fin.read(4 << 20), b''):
+      hasher.update(block)
+  return hasher.hexdigest()
diff --git a/sdks/python/apache_beam/runners/portability/expansion_service.py b/sdks/python/apache_beam/runners/portability/expansion_service.py
index 31810612a25..2536ef3fb5b 100644
--- a/sdks/python/apache_beam/runners/portability/expansion_service.py
+++ b/sdks/python/apache_beam/runners/portability/expansion_service.py
@@ -40,7 +40,7 @@ class ExpansionServiceServicer(
     self._options = options or beam_pipeline.PipelineOptions(
         environment_type=python_urns.EMBEDDED_PYTHON, sdk_location='container')
 
-  def Expand(self, request, context):
+  def Expand(self, request, context=None):
     try:
       pipeline = beam_pipeline.Pipeline(options=self._options)
 
diff --git a/sdks/python/apache_beam/transforms/external.py b/sdks/python/apache_beam/transforms/external.py
index 4f31cbb04dc..3c853195653 100644
--- a/sdks/python/apache_beam/transforms/external.py
+++ b/sdks/python/apache_beam/transforms/external.py
@@ -47,6 +47,8 @@ from apache_beam.typehints.typehints import UnionConstraint
 # pylint: disable=wrong-import-order, wrong-import-position, ungrouped-imports
 try:
   import grpc
+  from apache_beam.runners.portability import artifact_service
+  from apache_beam.portability.api import beam_artifact_api_pb2_grpc
   from apache_beam.portability.api import beam_expansion_api_pb2_grpc
   from apache_beam.utils import subprocess_server
 except ImportError:
@@ -314,21 +316,18 @@ class ExternalTransform(ptransform.PTransform):
         namespace=self._namespace,  # type: ignore  # mypy thinks self._namespace is threading.local
         transform=transform_proto)
 
-    if isinstance(self._expansion_service, str):
-      # Some environments may not support unsecure channels. Hence using a
-      # secure channel with local credentials here.
-      # TODO: update this to support secure non-local channels.
-      channel_creds = grpc.local_channel_credentials()
-      with grpc.secure_channel(self._expansion_service,
-                               channel_creds) as channel:
-        response = beam_expansion_api_pb2_grpc.ExpansionServiceStub(
-            channel).Expand(request)
-    else:
-      response = self._expansion_service.Expand(request, None)
+    with self._service() as service:
+      response = service.Expand(request)
+      if response.error:
+        raise RuntimeError(response.error)
+      self._expanded_components = response.components
+      if any(env.dependencies
+             for env in self._expanded_components.environments.values()):
+        self._expanded_components = self._resolve_artifacts(
+            self._expanded_components,
+            service.artifact_service(),
+            pipeline.local_tempdir)
 
-    if response.error:
-      raise RuntimeError(response.error)
-    self._expanded_components = response.components
     self._expanded_transform = response.transform
     self._expanded_requirements = response.requirements
     result_context = pipeline_context.PipelineContext(response.components)
@@ -346,6 +345,34 @@ class ExternalTransform(ptransform.PTransform):
 
     return self._output_to_pvalueish(self._outputs)
 
+  @contextlib.contextmanager
+  def _service(self):
+    if isinstance(self._expansion_service, str):
+      # Some environments may not support unsecure channels. Hence using a
+      # secure channel with local credentials here.
+      # TODO: update this to support secure non-local channels.
+      channel_creds = grpc.local_channel_credentials()
+      channel_options = [("grpc.max_receive_message_length", -1),
+                         ("grpc.max_send_message_length", -1)]
+      with grpc.secure_channel(self._expansion_service,
+                               channel_creds,
+                               options=channel_options) as channel:
+        yield ExpansionAndArtifactRetrievalStub(channel)
+    elif hasattr(self._expansion_service, 'Expand'):
+      yield self._expansion_service
+    else:
+      with self._expansion_service as stub:
+        yield stub
+
+  def _resolve_artifacts(self, components, service, dest):
+    for env in components.environments.values():
+      if env.dependencies:
+        resolved = list(
+            artifact_service.resolve_artifacts(env.dependencies, service, dest))
+        del env.dependencies[:]
+        env.dependencies.extend(resolved)
+    return components
+
   def _output_to_pvalueish(self, output_dict):
     if len(output_dict) == 1:
       return next(iter(output_dict.values()))
@@ -443,6 +470,18 @@ class ExternalTransform(ptransform.PTransform):
         environment_id=self._expanded_transform.environment_id)
 
 
+class ExpansionAndArtifactRetrievalStub(
+    beam_expansion_api_pb2_grpc.ExpansionServiceStub):
+  def __init__(self, channel, **kwargs):
+    self._channel = channel
+    self._kwargs = kwargs
+    super(ExpansionAndArtifactRetrievalStub, self).__init__(channel, **kwargs)
+
+  def artifact_service(self):
+    return beam_artifact_api_pb2_grpc.ArtifactRetrievalServiceStub(
+        self._channel, **self._kwargs)
+
+
 class JavaJarExpansionService(object):
   """An expansion service based on an Java Jar file.
 
@@ -455,16 +494,25 @@ class JavaJarExpansionService(object):
       extra_args = ['{{PORT}}']
     self._path_to_jar = path_to_jar
     self._extra_args = extra_args
-
-  def Expand(self, request, context):
-    self._path_to_jar = subprocess_server.JavaJarServer.local_jar(
-        self._path_to_jar)
-    # Consider memoizing these servers (with some timeout).
-    with subprocess_server.JavaJarServer(
-        beam_expansion_api_pb2_grpc.ExpansionServiceStub,
-        self._path_to_jar,
-        self._extra_args) as service:
-      return service.Expand(request, context)
+    self._service_count = 0
+
+  def __enter__(self):
+    if self._service_count == 0:
+      self._path_to_jar = subprocess_server.JavaJarServer.local_jar(
+          self._path_to_jar)
+      # Consider memoizing these servers (with some timeout).
+      self._service_provider = subprocess_server.JavaJarServer(
+          ExpansionAndArtifactRetrievalStub,
+          self._path_to_jar,
+          self._extra_args)
+      self._service = self._service_provider.__enter__()
+    self._service_count += 1
+    return self._service
+
+  def __exit__(self, *args):
+    self._service_count -= 1
+    if self._service_count == 0:
+      self._service_provider.__exit__(*args)
 
 
 class BeamJarExpansionService(JavaJarExpansionService):
diff --git a/sdks/python/apache_beam/transforms/sql_test.py b/sdks/python/apache_beam/transforms/sql_test.py
index d4527bb4d86..72acb36bcd1 100644
--- a/sdks/python/apache_beam/transforms/sql_test.py
+++ b/sdks/python/apache_beam/transforms/sql_test.py
@@ -30,13 +30,11 @@ from past.builtins import unicode
 
 import apache_beam as beam
 from apache_beam import coders
-from apache_beam.options.pipeline_options import DebugOptions
 from apache_beam.options.pipeline_options import StandardOptions
 from apache_beam.testing.test_pipeline import TestPipeline
 from apache_beam.testing.util import assert_that
 from apache_beam.testing.util import equal_to
 from apache_beam.transforms.sql import SqlTransform
-from apache_beam.utils import subprocess_server
 
 SimpleRow = typing.NamedTuple(
     "SimpleRow", [("int", int), ("str", unicode), ("flt", float)])
@@ -66,20 +64,8 @@ class SqlTransformTest(unittest.TestCase):
         --tests apache_beam.transforms.sql_test \\
         --test-pipeline-options="--runner=FlinkRunner"
   """
-  @staticmethod
-  def make_test_pipeline():
-    path_to_jar = subprocess_server.JavaJarServer.path_to_beam_jar(
-        ":sdks:java:extensions:sql:expansion-service:shadowJar")
-    test_pipeline = TestPipeline()
-    # TODO(BEAM-9238): Remove this when it's no longer needed for artifact
-    # staging.
-    test_pipeline.get_pipeline_options().view_as(DebugOptions).experiments = [
-        'jar_packages=' + path_to_jar
-    ]
-    return test_pipeline
-
   def test_generate_data(self):
-    with self.make_test_pipeline() as p:
+    with TestPipeline() as p:
       out = p | SqlTransform(
           """SELECT
             CAST(1 AS INT) AS `int`,
@@ -88,14 +74,14 @@ class SqlTransformTest(unittest.TestCase):
       assert_that(out, equal_to([(1, "foo", 3.14)]))
 
   def test_project(self):
-    with self.make_test_pipeline() as p:
+    with TestPipeline() as p:
       out = (
           p | beam.Create([SimpleRow(1, "foo", 3.14)])
           | SqlTransform("SELECT `int`, `flt` FROM PCOLLECTION"))
       assert_that(out, equal_to([(1, 3.14)]))
 
   def test_filter(self):
-    with self.make_test_pipeline() as p:
+    with TestPipeline() as p:
       out = (
           p
           | beam.Create([SimpleRow(1, "foo", 3.14), SimpleRow(2, "bar", 1.414)])
@@ -103,7 +89,7 @@ class SqlTransformTest(unittest.TestCase):
       assert_that(out, equal_to([(2, "bar", 1.414)]))
 
   def test_agg(self):
-    with self.make_test_pipeline() as p:
+    with TestPipeline() as p:
       out = (
           p
           | beam.Create([
diff --git a/sdks/python/apache_beam/transforms/validate_runner_xlang_test.py b/sdks/python/apache_beam/transforms/validate_runner_xlang_test.py
index 9955dd61da9..4d6b56b7314 100644
--- a/sdks/python/apache_beam/transforms/validate_runner_xlang_test.py
+++ b/sdks/python/apache_beam/transforms/validate_runner_xlang_test.py
@@ -25,7 +25,6 @@ from nose.plugins.attrib import attr
 from past.builtins import unicode
 
 import apache_beam as beam
-from apache_beam.options.pipeline_options import DebugOptions
 from apache_beam.testing.test_pipeline import TestPipeline
 from apache_beam.testing.util import assert_that
 from apache_beam.testing.util import equal_to
@@ -135,87 +134,46 @@ class CrossLanguageTestPipelines(object):
 
 
 @attr('UsesCrossLanguageTransforms')
-@unittest.skipUnless(
-    os.environ.get('EXPANSION_JAR'),
-    "EXPANSION_JAR environment variable is not set.")
 @unittest.skipUnless(
     os.environ.get('EXPANSION_PORT'),
     "EXPANSION_PORT environment var is not provided.")
 class ValidateRunnerXlangTest(unittest.TestCase):
-  expansion_jar = os.environ.get('EXPANSION_JAR')
+  def create_pipeline(self):
+    test_pipeline = TestPipeline()
+    test_pipeline.not_use_test_runner_api = True
+    return test_pipeline
 
   def test_prefix(self, test_pipeline=None):
-    if not test_pipeline:
-      test_pipeline = TestPipeline()
-      test_pipeline.get_pipeline_options().view_as(
-          DebugOptions).experiments.append(
-              'jar_packages=' + ValidateRunnerXlangTest.expansion_jar)
-      test_pipeline.not_use_test_runner_api = True
-    CrossLanguageTestPipelines().run_prefix(test_pipeline)
+    CrossLanguageTestPipelines().run_prefix(
+        test_pipeline or self.create_pipeline())
 
   def test_multi_input_output_with_sideinput(self, test_pipeline=None):
-    if not test_pipeline:
-      test_pipeline = TestPipeline()
-      test_pipeline.get_pipeline_options().view_as(
-          DebugOptions).experiments.append(
-              'jar_packages=' + ValidateRunnerXlangTest.expansion_jar)
-      test_pipeline.not_use_test_runner_api = True
     CrossLanguageTestPipelines().run_multi_input_output_with_sideinput(
-        test_pipeline)
+        test_pipeline or self.create_pipeline())
 
   def test_group_by_key(self, test_pipeline=None):
-    if not test_pipeline:
-      test_pipeline = TestPipeline()
-      test_pipeline.get_pipeline_options().view_as(
-          DebugOptions).experiments.append(
-              'jar_packages=' + ValidateRunnerXlangTest.expansion_jar)
-      test_pipeline.not_use_test_runner_api = True
-    CrossLanguageTestPipelines().run_group_by_key(test_pipeline)
+    CrossLanguageTestPipelines().run_group_by_key(
+        test_pipeline or self.create_pipeline())
 
   def test_cogroup_by_key(self, test_pipeline=None):
-    if not test_pipeline:
-      test_pipeline = TestPipeline()
-      test_pipeline.get_pipeline_options().view_as(
-          DebugOptions).experiments.append(
-              'jar_packages=' + ValidateRunnerXlangTest.expansion_jar)
-      test_pipeline.not_use_test_runner_api = True
-    CrossLanguageTestPipelines().run_cogroup_by_key(test_pipeline)
+    CrossLanguageTestPipelines().run_cogroup_by_key(
+        test_pipeline or self.create_pipeline())
 
   def test_combine_globally(self, test_pipeline=None):
-    if not test_pipeline:
-      test_pipeline = TestPipeline()
-      test_pipeline.get_pipeline_options().view_as(
-          DebugOptions).experiments.append(
-              'jar_packages=' + ValidateRunnerXlangTest.expansion_jar)
-      test_pipeline.not_use_test_runner_api = True
-    CrossLanguageTestPipelines().run_combine_globally(test_pipeline)
+    CrossLanguageTestPipelines().run_combine_globally(
+        test_pipeline or self.create_pipeline())
 
   def test_combine_per_key(self, test_pipeline=None):
-    if not test_pipeline:
-      test_pipeline = TestPipeline()
-      test_pipeline.get_pipeline_options().view_as(
-          DebugOptions).experiments.append(
-              'jar_packages=' + ValidateRunnerXlangTest.expansion_jar)
-      test_pipeline.not_use_test_runner_api = True
-    CrossLanguageTestPipelines().run_combine_per_key(test_pipeline)
+    CrossLanguageTestPipelines().run_combine_per_key(
+        test_pipeline or self.create_pipeline())
 
   def test_flatten(self, test_pipeline=None):
-    if not test_pipeline:
-      test_pipeline = TestPipeline()
-      test_pipeline.get_pipeline_options().view_as(
-          DebugOptions).experiments.append(
-              'jar_packages=' + ValidateRunnerXlangTest.expansion_jar)
-      test_pipeline.not_use_test_runner_api = True
-    CrossLanguageTestPipelines().run_flatten(test_pipeline)
+    CrossLanguageTestPipelines().run_flatten(
+        test_pipeline or self.create_pipeline())
 
   def test_partition(self, test_pipeline=None):
-    if not test_pipeline:
-      test_pipeline = TestPipeline()
-      test_pipeline.get_pipeline_options().view_as(
-          DebugOptions).experiments.append(
-              'jar_packages=' + ValidateRunnerXlangTest.expansion_jar)
-      test_pipeline.not_use_test_runner_api = True
-    CrossLanguageTestPipelines().run_partition(test_pipeline)
+    CrossLanguageTestPipelines().run_partition(
+        test_pipeline or self.create_pipeline())
 
 
 if __name__ == '__main__':
diff --git a/sdks/python/apache_beam/utils/subprocess_server.py b/sdks/python/apache_beam/utils/subprocess_server.py
index 92b60b53a08..c30f53e9030 100644
--- a/sdks/python/apache_beam/utils/subprocess_server.py
+++ b/sdks/python/apache_beam/utils/subprocess_server.py
@@ -84,7 +84,9 @@ class SubprocessServer(object):
     try:
       endpoint = self.start_process()
       wait_secs = .1
-      channel = grpc.insecure_channel(endpoint)
+      channel_options = [("grpc.max_receive_message_length", -1),
+                         ("grpc.max_send_message_length", -1)]
+      channel = grpc.insecure_channel(endpoint, options=channel_options)
       channel_ready = grpc.channel_ready_future(channel)
       while True:
         if self._process is not None and self._process.poll() is not None:
