diff --git a/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIO.java b/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIO.java
index b11eca7f9f4..021cacc6cb3 100644
--- a/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIO.java
+++ b/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIO.java
@@ -17,6 +17,9 @@
  */
 package org.apache.beam.sdk.io.aws.dynamodb;
 
+import static java.util.stream.Collectors.groupingBy;
+import static java.util.stream.Collectors.mapping;
+import static java.util.stream.Collectors.toList;
 import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;
 
 import com.amazonaws.regions.Regions;
@@ -24,6 +27,7 @@ import com.amazonaws.services.dynamodbv2.AmazonDynamoDB;
 import com.amazonaws.services.dynamodbv2.model.AmazonDynamoDBException;
 import com.amazonaws.services.dynamodbv2.model.AttributeValue;
 import com.amazonaws.services.dynamodbv2.model.BatchWriteItemRequest;
+import com.amazonaws.services.dynamodbv2.model.BatchWriteItemResult;
 import com.amazonaws.services.dynamodbv2.model.ScanRequest;
 import com.amazonaws.services.dynamodbv2.model.ScanResult;
 import com.amazonaws.services.dynamodbv2.model.WriteRequest;
@@ -454,9 +458,21 @@ public final class DynamoDBIO {
 
     static class WriteFn<T> extends DoFn<T, Void> {
       @VisibleForTesting
-      static final String RETRY_ATTEMPT_LOG = "Error writing to DynamoDB. Retry attempt[%d]";
+      static final String RETRY_ERROR_LOG = "Error writing items to DynamoDB [attempts:{}]: {}";
+
+      private static final String RESUME_ERROR_LOG =
+          "Error writing remaining unprocessed items to DynamoDB: {}";
+
+      private static final String ERROR_NO_RETRY =
+          "Error writing to DynamoDB. No attempt made to retry";
+      private static final String ERROR_RETRIES_EXCEEDED =
+          "Error writing to DynamoDB after %d attempt(s). No more attempts allowed";
+      private static final String ERROR_UNPROCESSED_ITEMS =
+          "Error writing to DynamoDB. Unprocessed items remaining";
+
+      private transient FluentBackoff resumeBackoff; // resume from partial failures (unlimited)
+      private transient FluentBackoff retryBackoff; // retry erroneous calls (default: none)
 
-      private transient FluentBackoff retryBackoff; // defaults to no retries
       private static final Logger LOG = LoggerFactory.getLogger(WriteFn.class);
       private static final Counter DYNAMO_DB_WRITE_FAILURES =
           Metrics.counter(WriteFn.class, "DynamoDB_Write_Failures");
@@ -473,13 +489,17 @@ public final class DynamoDBIO {
       @Setup
       public void setup() {
         client = spec.getAwsClientsProvider().createDynamoDB();
-        retryBackoff = FluentBackoff.DEFAULT.withMaxRetries(0); // default to no retrying
-        if (spec.getRetryConfiguration() != null) {
+        resumeBackoff = FluentBackoff.DEFAULT; // resume from partial failures (unlimited)
+        retryBackoff = FluentBackoff.DEFAULT.withMaxRetries(0); // retry on errors (default: none)
+
+        RetryConfiguration retryConfig = spec.getRetryConfiguration();
+        if (retryConfig != null) {
+          resumeBackoff = resumeBackoff.withInitialBackoff(retryConfig.getInitialDuration());
           retryBackoff =
               retryBackoff
-                  .withMaxRetries(spec.getRetryConfiguration().getMaxAttempts() - 1)
-                  .withInitialBackoff(spec.getRetryConfiguration().getInitialDuration())
-                  .withMaxCumulativeBackoff(spec.getRetryConfiguration().getMaxDuration());
+                  .withMaxRetries(retryConfig.getMaxAttempts() - 1)
+                  .withInitialBackoff(retryConfig.getInitialDuration())
+                  .withMaxCumulativeBackoff(retryConfig.getMaxDuration());
         }
       }
 
@@ -528,59 +548,63 @@ public final class DynamoDBIO {
         if (batch.isEmpty()) {
           return;
         }
-
         try {
-          // Since each element is a KV<tableName, writeRequest> in the batch, we need to group them
-          // by tableName
-          Map<String, List<WriteRequest>> mapTableRequest =
+          // Group values KV<tableName, writeRequest> by tableName
+          // Note: The original order of arrival is lost reading the map entries.
+          Map<String, List<WriteRequest>> writesPerTable =
               batch.values().stream()
-                  .collect(
-                      Collectors.groupingBy(
-                          KV::getKey, Collectors.mapping(KV::getValue, Collectors.toList())));
-
-          BatchWriteItemRequest batchRequest = new BatchWriteItemRequest();
-          mapTableRequest
-              .entrySet()
-              .forEach(
-                  entry -> batchRequest.addRequestItemsEntry(entry.getKey(), entry.getValue()));
-
-          Sleeper sleeper = Sleeper.DEFAULT;
-          BackOff backoff = retryBackoff.backoff();
-          int attempt = 0;
-          while (true) {
-            attempt++;
-            try {
-              client.batchWriteItem(batchRequest);
-              break;
-            } catch (Exception ex) {
-              // Fail right away if there is no retry configuration
-              if (spec.getRetryConfiguration() == null
-                  || !spec.getRetryConfiguration().getRetryPredicate().test(ex)) {
-                DYNAMO_DB_WRITE_FAILURES.inc();
-                LOG.info(
-                    "Unable to write batch items {}.",
-                    batchRequest.getRequestItems().entrySet(),
-                    ex);
-                throw new IOException("Error writing to DynamoDB (no attempt made to retry)", ex);
-              }
-
-              if (!BackOffUtils.next(sleeper, backoff)) {
-                throw new IOException(
-                    String.format(
-                        "Error writing to DynamoDB after %d attempt(s). No more attempts allowed",
-                        attempt),
-                    ex);
-              } else {
-                // Note: this used in test cases to verify behavior
-                LOG.warn(String.format(RETRY_ATTEMPT_LOG, attempt), ex);
-              }
-            }
+                  .collect(groupingBy(KV::getKey, mapping(KV::getValue, toList())));
+
+          // Backoff used to resume from partial failures
+          BackOff resume = resumeBackoff.backoff();
+          do {
+            BatchWriteItemRequest batchRequest = new BatchWriteItemRequest(writesPerTable);
+            // If unprocessed items remain, we have to resume the operation (with backoff)
+            writesPerTable = writeWithRetries(batchRequest).getUnprocessedItems();
+          } while (!writesPerTable.isEmpty() && BackOffUtils.next(Sleeper.DEFAULT, resume));
+
+          if (!writesPerTable.isEmpty()) {
+            DYNAMO_DB_WRITE_FAILURES.inc();
+            LOG.error(RESUME_ERROR_LOG, writesPerTable);
+            throw new IOException(ERROR_UNPROCESSED_ITEMS);
           }
         } finally {
           batch.clear();
         }
       }
 
+      /**
+       * Write batch of items to DynamoDB and potentially retry in case of exceptions. Though, in
+       * case of a partial failure, unprocessed items remain but the request succeeds. This has to
+       * be handled by the caller.
+       */
+      private BatchWriteItemResult writeWithRetries(BatchWriteItemRequest request)
+          throws IOException, InterruptedException {
+        BackOff backoff = retryBackoff.backoff();
+        Exception lastThrown;
+
+        int attempt = 0;
+        do {
+          attempt++;
+          try {
+            return client.batchWriteItem(request);
+          } catch (Exception ex) {
+            lastThrown = ex;
+          }
+        } while (canRetry(lastThrown) && BackOffUtils.next(Sleeper.DEFAULT, backoff));
+
+        DYNAMO_DB_WRITE_FAILURES.inc();
+        LOG.warn(RETRY_ERROR_LOG, attempt, request.getRequestItems());
+        throw new IOException(
+            canRetry(lastThrown) ? String.format(ERROR_RETRIES_EXCEEDED, attempt) : ERROR_NO_RETRY,
+            lastThrown);
+      }
+
+      private boolean canRetry(Exception ex) {
+        return spec.getRetryConfiguration() != null
+            && spec.getRetryConfiguration().getRetryPredicate().test(ex);
+      }
+
       @Teardown
       public void tearDown() {
         if (client != null) {
diff --git a/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIOTest.java b/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIOTest.java
index d3f64f9b700..5eb9170c7d1 100644
--- a/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIOTest.java
+++ b/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIOTest.java
@@ -17,13 +17,17 @@
  */
 package org.apache.beam.sdk.io.aws.dynamodb;
 
+import static org.apache.beam.sdk.io.aws.dynamodb.DynamoDBIO.Write.WriteFn.RETRY_ERROR_LOG;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.fail;
+import static org.mockito.ArgumentMatchers.any;
+import static org.mockito.Mockito.when;
 
 import com.amazonaws.services.dynamodbv2.AmazonDynamoDB;
 import com.amazonaws.services.dynamodbv2.model.AmazonDynamoDBException;
 import com.amazonaws.services.dynamodbv2.model.AttributeValue;
 import com.amazonaws.services.dynamodbv2.model.BatchWriteItemRequest;
+import com.amazonaws.services.dynamodbv2.model.BatchWriteItemResult;
 import com.amazonaws.services.dynamodbv2.model.ScanRequest;
 import com.amazonaws.services.dynamodbv2.model.WriteRequest;
 import java.io.IOException;
@@ -51,6 +55,8 @@ import org.junit.Test;
 import org.junit.rules.ExpectedException;
 import org.mockito.ArgumentCaptor;
 import org.mockito.Mockito;
+import org.slf4j.helpers.MessageFormatter;
+import org.testcontainers.shaded.com.google.common.collect.ImmutableMap;
 
 /** Test Coverage for the IO. */
 public class DynamoDBIOTest implements Serializable {
@@ -204,7 +210,7 @@ public class DynamoDBIOTest implements Serializable {
     final List<WriteRequest> writeRequests = DynamoDBIOTestHelper.generateWriteRequests(numOfItems);
 
     AmazonDynamoDB amazonDynamoDBMock = Mockito.mock(AmazonDynamoDB.class);
-    Mockito.when(amazonDynamoDBMock.batchWriteItem(Mockito.any(BatchWriteItemRequest.class)))
+    when(amazonDynamoDBMock.batchWriteItem(any(BatchWriteItemRequest.class)))
         .thenThrow(new AmazonDynamoDBException("Service unavailable"));
 
     pipeline
@@ -220,10 +226,8 @@ public class DynamoDBIOTest implements Serializable {
     try {
       pipeline.run().waitUntilFinish();
     } catch (final Pipeline.PipelineExecutionException e) {
-      // check 3 retries were initiated by inspecting the log before passing on the exception
-      writeFnLogs.verifyWarn(String.format(DynamoDBIO.Write.WriteFn.RETRY_ATTEMPT_LOG, 1));
-      writeFnLogs.verifyWarn(String.format(DynamoDBIO.Write.WriteFn.RETRY_ATTEMPT_LOG, 2));
-      writeFnLogs.verifyWarn(String.format(DynamoDBIO.Write.WriteFn.RETRY_ATTEMPT_LOG, 3));
+      // check 4 retries were initiated by inspecting the log before passing on the exception
+      writeFnLogs.verifyWarn(MessageFormatter.format(RETRY_ERROR_LOG, 4, "").getMessage());
       throw e.getCause();
     }
   }
@@ -250,6 +254,8 @@ public class DynamoDBIOTest implements Serializable {
         Arrays.asList(DynamoDBIOTestHelper.ATTR_NAME_1, DynamoDBIOTestHelper.ATTR_NAME_2);
 
     AmazonDynamoDB amazonDynamoDBMock = Mockito.mock(AmazonDynamoDB.class);
+    when(amazonDynamoDBMock.batchWriteItem(any(BatchWriteItemRequest.class)))
+        .thenReturn(new BatchWriteItemResult().withUnprocessedItems(ImmutableMap.of()));
 
     pipeline
         .apply(Create.of(duplications))
diff --git a/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIOWriteTest.java b/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIOWriteTest.java
index 0ec36c18882..cb98295bc80 100644
--- a/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIOWriteTest.java
+++ b/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIOWriteTest.java
@@ -20,9 +20,12 @@ package org.apache.beam.sdk.io.aws.dynamodb;
 import static java.util.stream.Collectors.toList;
 import static java.util.stream.IntStream.range;
 import static java.util.stream.IntStream.rangeClosed;
+import static org.apache.beam.sdk.io.aws.dynamodb.DynamoDBIO.Write.WriteFn.RETRY_ERROR_LOG;
 import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Maps.transformValues;
 import static org.assertj.core.api.Assertions.assertThat;
 import static org.mockito.ArgumentMatchers.any;
+import static org.mockito.ArgumentMatchers.argThat;
+import static org.mockito.Mockito.inOrder;
 import static org.mockito.Mockito.times;
 import static org.mockito.Mockito.verify;
 import static org.mockito.Mockito.when;
@@ -43,6 +46,7 @@ import java.util.Map;
 import java.util.Objects;
 import java.util.function.Function;
 import java.util.function.Supplier;
+import java.util.stream.IntStream;
 import org.apache.beam.sdk.Pipeline;
 import org.apache.beam.sdk.PipelineResult;
 import org.apache.beam.sdk.coders.AvroCoder;
@@ -67,8 +71,11 @@ import org.junit.Test;
 import org.junit.rules.ExpectedException;
 import org.junit.runner.RunWith;
 import org.mockito.ArgumentCaptor;
+import org.mockito.ArgumentMatcher;
+import org.mockito.InOrder;
 import org.mockito.Mock;
 import org.mockito.junit.MockitoJUnitRunner;
+import org.slf4j.helpers.MessageFormatter;
 
 @RunWith(MockitoJUnitRunner.class)
 public class DynamoDBIOWriteTest {
@@ -82,7 +89,7 @@ public class DynamoDBIOWriteTest {
 
   @Test
   public void testWritePutItems() {
-    List<Item> items = range(0, 100).mapToObj(Item::of).collect(toList());
+    List<Item> items = Item.range(0, 100);
 
     Supplier<List<Item>> capturePuts =
         captureBatchWrites(client, req -> req.getPutRequest().getItem());
@@ -103,7 +110,7 @@ public class DynamoDBIOWriteTest {
 
   @Test
   public void testWritePutItemsWithDuplicates() {
-    List<Item> items = range(0, 100).mapToObj(Item::of).collect(toList());
+    List<Item> items = Item.range(0, 100);
 
     Supplier<List<Item>> capturePuts =
         captureBatchWrites(client, req -> req.getPutRequest().getItem());
@@ -125,7 +132,7 @@ public class DynamoDBIOWriteTest {
 
   @Test
   public void testWritePutItemsWithDuplicatesByKey() {
-    List<Item> items = range(0, 100).mapToObj(Item::of).collect(toList());
+    List<Item> items = Item.range(0, 100);
 
     Supplier<List<Item>> capturePuts =
         captureBatchWrites(client, req -> req.getPutRequest().getItem());
@@ -148,7 +155,7 @@ public class DynamoDBIOWriteTest {
 
   @Test
   public void testWriteDeleteItems() {
-    List<Item> items = range(0, 100).mapToObj(Item::of).collect(toList());
+    List<Item> items = Item.range(0, 100);
 
     Supplier<List<Item>> captureDeletes =
         captureBatchWrites(client, req -> req.getDeleteRequest().getKey());
@@ -170,7 +177,7 @@ public class DynamoDBIOWriteTest {
 
   @Test
   public void testWriteDeleteItemsWithDuplicates() {
-    List<Item> items = range(0, 100).mapToObj(Item::of).collect(toList());
+    List<Item> items = Item.range(0, 100);
 
     Supplier<List<Item>> captureDeletes =
         captureBatchWrites(client, req -> req.getDeleteRequest().getKey());
@@ -197,7 +204,7 @@ public class DynamoDBIOWriteTest {
             AmazonDynamoDBException.class,
             AmazonDynamoDBException.class,
             AmazonDynamoDBException.class)
-        .thenReturn(new BatchWriteItemResult());
+        .thenReturn(new BatchWriteItemResult().withUnprocessedItems(ImmutableMap.of()));
 
     pipeline
         .apply(Create.of(Item.of(1)))
@@ -212,7 +219,36 @@ public class DynamoDBIOWriteTest {
     result.waitUntilFinish();
 
     verify(client, times(4)).batchWriteItem(any(BatchWriteItemRequest.class));
-    range(1, 4).forEach(i -> writeFnLogs.verifyWarn(String.format(WriteFn.RETRY_ATTEMPT_LOG, i)));
+  }
+
+  @Test
+  public void testWritePutItemsWithPartialSuccess() {
+    List<WriteRequest> writes = putRequests(Item.range(0, 10));
+
+    when(client.batchWriteItem(any(BatchWriteItemRequest.class)))
+        .thenReturn(partialWriteSuccess(writes.subList(4, 10)))
+        .thenReturn(partialWriteSuccess(writes.subList(8, 10)))
+        .thenReturn(new BatchWriteItemResult().withUnprocessedItems(ImmutableMap.of()));
+
+    pipeline
+        .apply(Create.of(10)) // number if items to produce
+        .apply(ParDo.of(new GenerateItems())) // 10 items in one bundle
+        .apply(
+            "write",
+            DynamoDBIO.<Item>write()
+                .withWriteRequestMapperFn(putRequestMapper)
+                .withAwsClientsProvider(StaticAwsClientsProvider.of(client))
+                .withRetryConfiguration(try4Times));
+
+    PipelineResult result = pipeline.run();
+    result.waitUntilFinish();
+
+    verify(client, times(3)).batchWriteItem(any(BatchWriteItemRequest.class));
+
+    InOrder ordered = inOrder(client);
+    ordered.verify(client).batchWriteItem(argThat(matchWritesUnordered(writes)));
+    ordered.verify(client).batchWriteItem(argThat(matchWritesUnordered(writes.subList(4, 10))));
+    ordered.verify(client).batchWriteItem(argThat(matchWritesUnordered(writes.subList(8, 10))));
   }
 
   @Test
@@ -236,7 +272,7 @@ public class DynamoDBIOWriteTest {
       pipeline.run().waitUntilFinish();
     } catch (final Pipeline.PipelineExecutionException e) {
       verify(client, times(4)).batchWriteItem(any(BatchWriteItemRequest.class));
-      range(1, 4).forEach(i -> writeFnLogs.verifyWarn(String.format(WriteFn.RETRY_ATTEMPT_LOG, i)));
+      writeFnLogs.verifyWarn(MessageFormatter.format(RETRY_ERROR_LOG, 4, "").getMessage());
       throw e.getCause();
     }
   }
@@ -259,6 +295,10 @@ public class DynamoDBIOWriteTest {
       return new Item(ImmutableMap.copyOf(transformValues(attributes, a -> a.getS())));
     }
 
+    static List<Item> range(int startInclusive, int endExclusive) {
+      return IntStream.range(startInclusive, endExclusive).mapToObj(Item::of).collect(toList());
+    }
+
     Item withEntry(String key, String value) {
       return new Item(
           ImmutableMap.<String, String>builder().putAll(entries).put(key, value).build());
@@ -294,7 +334,8 @@ public class DynamoDBIOWriteTest {
       AmazonDynamoDB mock, Function<WriteRequest, Map<String, AttributeValue>> extractor) {
     ArgumentCaptor<BatchWriteItemRequest> reqCaptor =
         ArgumentCaptor.forClass(BatchWriteItemRequest.class);
-    when(mock.batchWriteItem(reqCaptor.capture())).thenReturn(new BatchWriteItemResult());
+    when(mock.batchWriteItem(reqCaptor.capture()))
+        .thenReturn(new BatchWriteItemResult().withUnprocessedItems(ImmutableMap.of()));
 
     return () ->
         reqCaptor.getAllValues().stream()
@@ -305,21 +346,44 @@ public class DynamoDBIOWriteTest {
             .collect(toList());
   }
 
+  private static ArgumentMatcher<BatchWriteItemRequest> matchWritesUnordered(
+      List<WriteRequest> writes) {
+    return (BatchWriteItemRequest req) ->
+        req != null
+            && req.getRequestItems().get(tableName).size() == writes.size()
+            && req.getRequestItems().get(tableName).containsAll(writes);
+  }
+
+  private static BatchWriteItemResult partialWriteSuccess(List<WriteRequest> unprocessed) {
+    return new BatchWriteItemResult().withUnprocessedItems(ImmutableMap.of(tableName, unprocessed));
+  }
+
+  private static List<WriteRequest> putRequests(List<Item> items) {
+    return items.stream().map(putRequest).collect(toList());
+  }
+
+  private static Function<Item, WriteRequest> putRequest =
+      item -> new WriteRequest().withPutRequest(new PutRequest().withItem(item.attributeMap()));
+
+  private static Function<Item, WriteRequest> deleteRequest =
+      key -> new WriteRequest().withDeleteRequest(new DeleteRequest().withKey(key.attributeMap()));
+
   private static SerializableFunction<Item, KV<String, WriteRequest>> putRequestMapper =
-      item -> {
-        PutRequest req = new PutRequest().withItem(item.attributeMap());
-        return KV.of(tableName, new WriteRequest().withPutRequest(req));
-      };
+      item -> KV.of(tableName, putRequest.apply(item));
 
   private static SerializableFunction<Item, KV<String, WriteRequest>> deleteRequestMapper =
-      key -> {
-        DeleteRequest req = new DeleteRequest().withKey(key.attributeMap());
-        return KV.of(tableName, new WriteRequest().withDeleteRequest(req));
-      };
+      key -> KV.of(tableName, deleteRequest.apply(key));
 
   private static RetryConfiguration try4Times =
       RetryConfiguration.create(4, Duration.standardSeconds(1), Duration.millis(1));
 
+  private static class GenerateItems extends DoFn<Integer, Item> {
+    @ProcessElement
+    public void processElement(ProcessContext ctx) {
+      range(0, ctx.element()).forEach(i -> ctx.output(Item.of(i)));
+    }
+  }
+
   /**
    * A DoFn that adds N duplicates to a bundle. The original is emitted last and is the only item
    * kept if deduplicating appropriately.
