diff --git a/website/src/_includes/section-menu/documentation.html b/website/src/_includes/section-menu/documentation.html
index b7ff50165a1..0365f8b6f35 100644
--- a/website/src/_includes/section-menu/documentation.html
+++ b/website/src/_includes/section-menu/documentation.html
@@ -13,7 +13,7 @@
 <li><span class="section-nav-list-main-title">Documentation</span></li>
 <li><a href="{{ site.baseurl }}/documentation">Using the Documentation</a></li>
 <li><a href="{{ site.baseurl }}/documentation/execution-model">Beam Execution Model</a></li>
-<li class="section-nav-item--collapsible">
+<li>
   <span class="section-nav-list-title">Pipeline development lifecycle</span>
 
   <ul class="section-nav-list">
@@ -22,7 +22,7 @@
     <li><a href="{{ site.baseurl }}/documentation/pipelines/test-your-pipeline/">Test Your Pipeline</a></li>
   </ul>
 </li>
-<li class="section-nav-item--collapsible">
+<li>
   <span class="section-nav-list-title">Beam programming guide</span>
 
   <ul class="section-nav-list">
@@ -65,8 +65,18 @@
 
       <ul class="section-nav-list">
         <li><a href="{{ site.baseurl }}/documentation/programming-guide/#pipeline-io">Using I/O transforms</a></li>
-        <li><a href="{{ site.baseurl }}/documentation/io/built-in/">Built-in I/O transforms</a></li>
-        <li><a href="{{ site.baseurl }}/documentation/io/authoring-overview/">Authoring new I/O transforms</a></li>
+        <li><a href="{{ site.baseurl }}/documentation/io/built-in/">Built-in I/O connectors</a></li>
+
+        <li class="section-nav-item--collapsible">
+           <span class="section-nav-list-title">Developing new I/O connectors</span>
+
+          <ul class="section-nav-list">
+           <li><a href="{{ site.baseurl }}/documentation/io/developing-io-overview/">Overview: Developing connectors</a></li>
+           <li><a href="{{ site.baseurl }}/documentation/io/developing-io-java/">Developing connectors (Java)</a></li>
+           <li><a href="{{ site.baseurl }}/documentation/io/developing-io-python/">Developing connectors (Python)</a></li>
+          </ul>
+        </li>
+
         <li><a href="{{ site.baseurl }}/documentation/io/testing/">Testing I/O transforms</a></li>
       </ul>
     </li>
diff --git a/website/src/_includes/section-menu/sdks.html b/website/src/_includes/section-menu/sdks.html
index 5e3ce044de5..f15f0ac1889 100644
--- a/website/src/_includes/section-menu/sdks.html
+++ b/website/src/_includes/section-menu/sdks.html
@@ -39,7 +39,6 @@
     <li><a href="{{ site.baseurl }}/documentation/sdks/python-streaming/">Python streaming pipelines</a></li>
     <li><a href="{{ site.baseurl }}/documentation/sdks/python-type-safety/">Ensuring Python type safety</a></li>
     <li><a href="{{ site.baseurl }}/documentation/sdks/python-pipeline-dependencies/">Managing pipeline dependencies</a></li>
-    <li><a href="{{ site.baseurl }}/documentation/sdks/python-custom-io/">Creating new sources and sinks</a></li>
   </ul>
 </li>
 
diff --git a/website/src/_posts/2018-08-20-review-input-streaming-connectors.md b/website/src/_posts/2018-08-20-review-input-streaming-connectors.md
index 09da93d92f1..5f53c5a2b08 100644
--- a/website/src/_posts/2018-08-20-review-input-streaming-connectors.md
+++ b/website/src/_posts/2018-08-20-review-input-streaming-connectors.md
@@ -115,7 +115,7 @@ and <a href="https://spark.apache.org/docs/latest/api/java/org/apache/spark/stre
    </td>
    <td>Custom receivers
    </td>
-   <td><a href="{{ site.baseurl }}/documentation/io/authoring-overview/#read-transforms">Read Transforms</a>
+   <td><a href="{{ site.baseurl }}/documentation/io/developing-io-overview/">Read Transforms</a>
    </td>
    <td><a href="https://spark.apache.org/docs/latest/streaming-custom-receivers.html">receiverStream</a>
    </td>
diff --git a/website/src/documentation/io/authoring-java.md b/website/src/documentation/io/authoring-java.md
deleted file mode 100644
index c2b4c8f4a6f..00000000000
--- a/website/src/documentation/io/authoring-java.md
+++ /dev/null
@@ -1,37 +0,0 @@
----
-layout: section
-title: "Authoring I/O Transforms - Java"
-section_menu: section-menu/documentation.html
-permalink: /documentation/io/authoring-java/
----
-<!--
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
--->
-
-[Pipeline I/O Table of Contents]({{site.baseurl}}/documentation/io/io-toc/)
-
-# Authoring I/O Transforms - Java
-
-> Note: This guide is still in progress. There is an open issue to finish the guide: [BEAM-1025](https://issues.apache.org/jira/browse/BEAM-1025).
-
-## Example I/O Transforms
-Currently, Apache Beam's I/O transforms use a variety of different
-styles. These transforms are good examples to follow:
-* [`DatastoreIO`](https://github.com/apache/beam/blob/master/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/datastore/DatastoreIO.java) - `ParDo` based database read and write that conforms to the PTransform style guide
-* [`BigtableIO`](https://github.com/apache/beam/blob/master/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigtable/BigtableIO.java) - Good test examples, and demonstrates Dynamic Work Rebalancing
-* [`JdbcIO`](https://github.com/apache/beam/blob/master/sdks/java/io/jdbc/src/main/java/org/apache/beam/sdk/io/jdbc/JdbcIO.java) - Demonstrates reading using single `ParDo`+`GroupByKey` when data stores cannot be read in parallel
-
-
-# Next steps
-
-[Testing I/O Transforms]({{site.baseurl }}/documentation/io/testing/)
diff --git a/website/src/documentation/io/authoring-overview.md b/website/src/documentation/io/authoring-overview.md
deleted file mode 100644
index 2089435bbd0..00000000000
--- a/website/src/documentation/io/authoring-overview.md
+++ /dev/null
@@ -1,103 +0,0 @@
----
-layout: section
-title: "Authoring I/O Transforms - Overview"
-section_menu: section-menu/documentation.html
-permalink: /documentation/io/authoring-overview/
----
-<!--
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
--->
-
-[Pipeline I/O Table of Contents]({{site.baseurl}}/documentation/io/io-toc/)
-
-# Authoring I/O Transforms - Overview
-
-_A guide for users who need to connect to a data store that isn't supported by the [Built-in I/O Transforms]({{site.baseurl }}/documentation/io/built-in/)_
-
-
-* TOC
-{:toc}
-
-## Introduction
-This guide covers how to implement I/O transforms in the Beam model. Beam pipelines use these read and write transforms to import data for processing, and write data to a store.
-
-Reading and writing data in Beam is a parallel task, and using `ParDo`s, `GroupByKey`s, etc... is usually sufficient. Rarely, you will need the more specialized `Source` and `Sink` classes for specific features. There are changes coming soon (`SplittableDoFn`, [BEAM-65](https://issues.apache.org/jira/browse/BEAM-65)) that will make `Source` unnecessary.
-
-As you work on your I/O Transform, be aware that the Beam community is excited to help those building new I/O Transforms and that there are many examples and helper classes.
-
-
-## Suggested steps for implementers
-1. Check out this guide and come up with your design. If you'd like, you can email the [Beam dev mailing list]({{ site.baseurl }}/get-started/support) with any questions you might have. It's good to check there to see if anyone else is working on the same I/O Transform.
-2. If you are planning to contribute your I/O transform to the Beam community, you'll be going through the normal Beam contribution life cycle - see the [Apache Beam Contribution Guide]({{ site.baseurl }}/contribute/contribution-guide/) for more details.
-3. As you're working on your IO transform, see the [PTransform Style Guide]({{ site.baseurl }}/contribute/ptransform-style-guide/) for specific information about writing I/O Transforms.
-
-
-## Read transforms
-Read transforms take data from outside of the Beam pipeline and produce `PCollection`s of data.
-
-For data stores or file types where the data can be read in parallel, you can think of the process as a mini-pipeline. This often consists of two steps:
-1. Splitting the data into parts to be read in parallel
-2. Reading from each of those parts
-
-Each of those steps will be a `ParDo`, with a `GroupByKey` in between. The `GroupByKey` is an implementation detail, but for most runners it allows the runner to use different numbers of workers for:
-* Determining how to split up the data to be read into chunks - this will likely occur on very few workers
-* Reading - will likely benefit from more workers
-
-The `GroupByKey` will also allow Dynamic Work Rebalancing to occur (on supported runners).
-
-Here are some examples of read transform implementations that use the "reading as a mini-pipeline" model when data can be read in parallel:
-* **Reading from a file glob** - For example reading all files in "~/data/**"
-  * Get File Paths `ParDo`: As input, take in a file glob. Produce a `PCollection` of strings, each of which is a file path.
-  * Reading `ParDo`: Given the `PCollection` of file paths, read each one, producing a `PCollection` of records.
-* **Reading from a NoSQL Database** (eg Apache HBase) - these databases often allow reading from ranges in parallel.
-  * Determine Key Ranges `ParDo`: As input, receive connection information for the database and the key range to read from. Produce a `PCollection` of key ranges that can be read in parallel efficiently.
-  * Read Key Range `ParDo`: Given the `PCollection` of key ranges, read the key range, producing a `PCollection` of records.
-
-For data stores or files where reading cannot occur in parallel, reading is a simple task that can be accomplished with a single `ParDo`+`GroupByKey`. For example:
-* **Reading from a database query** - traditional SQL database queries often can only be read in sequence. The `ParDo` in this case would establish a connection to the database and read batches of records, producing a `PCollection` of those records.
-* **Reading from a gzip file** - a gzip file has to be read in order, so it cannot be parallelized. The `ParDo` in this case would open the file and read in sequence, producing a `PCollection` of records from the file.
-
-
-### When to implement using the `Source` API
-The above discussion is in terms of `ParDo`s - this is because `Source`s have proven to be tricky to implement. At this point in time, the recommendation is to **use  `Source` only if `ParDo` doesn't meet your needs**. A class derived from `FileBasedSource` is often the best option when reading from files.
-
- If you're trying to decide on whether or not to use `Source`, feel free to email the [Beam dev mailing list]({{ site.baseurl }}/get-started/support) and we can discuss the specific pros and cons of your case.
-
-In some cases implementing a `Source` may be necessary or result in better performance.
-* `ParDo`s will not work for reading from unbounded sources - they do not support checkpointing and don't support mechanisms like de-duping that have proven useful for streaming data sources.
-* `ParDo`s cannot provide hints to runners about their progress or the size of data they are reading -  without size estimation of the data or progress on your read, the runner doesn't have any way to guess how large your read will be, and thus if it attempts to dynamically allocate workers, it does not have any clues as to how many workers you may need for your pipeline.
-* `ParDo`s do not support Dynamic Work Rebalancing - these are features used by some readers to improve the processing speed of jobs (but may not be possible with your data source).
-* `ParDo`s do not receive 'desired_bundle_size' as a hint from runners when performing initial splitting.
-`SplittableDoFn` ([BEAM-65](https://issues.apache.org/jira/browse/BEAM-65)) will mitigate many of these concerns.
-
-
-## Write transforms
-Write transforms are responsible for taking the contents of a `PCollection` and transferring that data outside of the Beam pipeline.
-
-Write transforms can usually be implemented using a single `ParDo` that writes the records received to the data store.
-
-TODO: this section needs further explanation.
-
-### When to implement using the `Sink` API
-You are strongly discouraged from using the `Sink` class unless you are creating a `FileBasedSink`. Most of the time, a simple `ParDo` is all that's necessary. If you think you have a case that is only possible using a `Sink`, please email the [Beam dev mailing list]({{ site.baseurl }}/get-started/support).
-
-# Next steps
-
-This guide is still in progress. There is an open issue to finish the guide: [BEAM-1025](https://issues.apache.org/jira/browse/BEAM-1025).
-
-<!-- TODO: commented out until this content is ready.
-For more details on actual implementation, continue with one of the the language specific guides:
-
-* [Authoring I/O Transforms - Python]({{site.baseurl }}/documentation/io/authoring-python/)
-* [Authoring I/O Transforms - Java]({{site.baseurl }}/documentation/io/authoring-java/)
--->
diff --git a/website/src/documentation/io/authoring-python.md b/website/src/documentation/io/authoring-python.md
deleted file mode 100644
index b8392b02b14..00000000000
--- a/website/src/documentation/io/authoring-python.md
+++ /dev/null
@@ -1,32 +0,0 @@
----
-layout: section
-title: "Authoring I/O Transforms - Python"
-section_menu: section-menu/documentation.html
-permalink: /documentation/io/authoring-python/
----
-<!--
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
--->
-
-[Pipeline I/O Table of Contents]({{site.baseurl}}/documentation/io/io-toc/)
-
-# Authoring I/O Transforms - Python
-
-> Note: This guide is still in progress. There is an open issue to finish the guide: [BEAM-1025](https://issues.apache.org/jira/browse/BEAM-1025).
-
-TODO - move in the [current python SDK content]({{site.baseurl}}/documentation/sdks/python-custom-io/)
-
-
-# Next steps
-
-[Testing I/O Transforms]({{site.baseurl}}/documentation/io/testing/)
diff --git a/website/src/documentation/io/built-in-hadoop.md b/website/src/documentation/io/built-in-hadoop.md
index 349b6e6b196..f136e59b4da 100644
--- a/website/src/documentation/io/built-in-hadoop.md
+++ b/website/src/documentation/io/built-in-hadoop.md
@@ -18,8 +18,6 @@ See the License for the specific language governing permissions and
 limitations under the License.
 -->
 
-[Pipeline I/O Table of Contents]({{site.baseurl}}/documentation/io/io-toc/)
-
 # Hadoop InputFormat IO
 
 A `HadoopInputFormatIO` is a transform for reading data from any source that implements Hadoop's `InputFormat`. For example, Cassandra, Elasticsearch, HBase, Redis, Postgres, etc.
diff --git a/website/src/documentation/io/built-in-hcatalog.md b/website/src/documentation/io/built-in-hcatalog.md
index 46f39a1471d..e28890cf47f 100644
--- a/website/src/documentation/io/built-in-hcatalog.md
+++ b/website/src/documentation/io/built-in-hcatalog.md
@@ -18,8 +18,6 @@ See the License for the specific language governing permissions and
 limitations under the License.
 -->
 
-[Pipeline I/O Table of Contents]({{site.baseurl}}/documentation/io/io-toc/)
-
 # HCatalog IO
 
 An `HCatalogIO` is a transform for reading and writing data to an HCatalog managed source.
diff --git a/website/src/documentation/io/built-in.md b/website/src/documentation/io/built-in.md
index 9ed4c1be99d..b2b98044e04 100644
--- a/website/src/documentation/io/built-in.md
+++ b/website/src/documentation/io/built-in.md
@@ -18,8 +18,6 @@ See the License for the specific language governing permissions and
 limitations under the License.
 -->
 
-[Pipeline I/O Table of Contents]({{site.baseurl}}/documentation/io/io-toc/)
-
 # Built-in I/O Transforms
 
 This table contains the currently available I/O transforms.
diff --git a/website/src/documentation/io/contributing.md b/website/src/documentation/io/contributing.md
deleted file mode 100644
index b5bdccd0b03..00000000000
--- a/website/src/documentation/io/contributing.md
+++ /dev/null
@@ -1,29 +0,0 @@
----
-layout: section
-title: "Contributing I/O Transforms"
-section_menu: section-menu/documentation.html
-permalink: /documentation/io/contributing/
----
-<!--
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
--->
-
-[Pipeline I/O Table of Contents]({{site.baseurl}}/documentation/io/io-toc/)
-
-# Contributing I/O Transforms
-
-* If you are planning to contribute your I/O transform to the Apache Beam community, you'll be going through the normal Beam contribution life cycle - see the [Apache Beam Contribution Guide]({{ site.baseurl }}/contribute/contribution-guide/) for more details.
-* Talk to the community!
-* Make sure you've implemented the appropriate tests as discussed in the [Testing I/O Transforms]({{site.baseurl }}/documentation/io/testing/) section.
-
-> Note: This guide is still in progress. There is an open issue to finish the guide: [BEAM-1025](https://issues.apache.org/jira/browse/BEAM-1025).
diff --git a/website/src/documentation/io/developing-io-java.md b/website/src/documentation/io/developing-io-java.md
new file mode 100644
index 00000000000..388229c8b8a
--- /dev/null
+++ b/website/src/documentation/io/developing-io-java.md
@@ -0,0 +1,368 @@
+---
+layout: section
+title: "Apache Beam: Developing I/O connectors for Java"
+section_menu: section-menu/documentation.html
+permalink: /documentation/io/developing-io-java/
+redirect_from: /documentation/io/authoring-java/
+---
+<!--
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+-->
+# Developing I/O connectors for Java
+
+To connect to a data store that isn’t supported by Beam’s existing I/O
+connectors, you must create a custom I/O connector that usually consist of a
+source and a sink. All Beam sources and sinks are composite transforms; however,
+the implementation of your custom I/O depends on your use case. Before you
+start, read the
+[new I/O connector overview]({{ site.baseurl }}/documentation/io/developing-io-overview/)
+for an overview of developing a new I/O connector, the available implementation
+options, and how to choose the right option for your use case.
+
+This guide covers using the `Source` and `FileBasedSink` interfaces using Java.
+The Python SDK offers the same functionality, but uses a slightly different API.
+See [Developing I/O connectors for Python]({{ site.baseurl }}/documentation/io/developing-io-python/)
+for information specific to the Python SDK.
+
+## Basic code requirements {#basic-code-reqs}
+
+Beam runners use the classes you provide to read and/or write data using
+multiple worker instances in parallel. As such, the code you provide for
+`Source` and `FileBasedSink` subclasses must meet some basic requirements:
+
+  1. **Serializability:** Your `Source` or `FileBasedSink` subclass, whether
+     bounded or unbounded, must be Serializable. A runner might create multiple
+     instances of your `Source` or `FileBasedSink` subclass to be sent to
+     multiple remote workers to facilitate reading or writing in parallel.  
+
+  1. **Immutability:**
+     Your `Source` or `FileBasedSink` subclass must be effectively immutable.
+     All private fields must be declared final, and all private variables of
+     collection type must be effectively immutable. If your class has setter
+     methods, those methods must return an independent copy of the object with
+     the relevant field modified.  
+
+     You should only use mutable state in your `Source` or `FileBasedSink`
+     subclass if you are using lazy evaluation of expensive computations that
+     you need to implement the source or sink; in that case, you must declare
+     all mutable instance variables transient.  
+
+  1. **Thread-Safety:** Your code must be thread-safe. If you build your source
+     to work with dynamic work rebalancing, it is critical that you make your
+     code thread-safe. The Beam SDK provides a helper class to make this easier.
+     See [Using Your BoundedSource with dynamic work rebalancing](#bounded-dynamic)
+     for more details.  
+
+  1. **Testability:** It is critical to exhaustively unit test all of your
+     `Source` and `FileBasedSink` subclasses, especially if you build your
+     classes to work with advanced features such as dynamic work rebalancing. A
+     minor implementation error can lead to data corruption or data loss (such
+     as skipping or duplicating records) that can be hard to detect.  
+
+     To assist in testing `BoundedSource` implementations, you can use the
+     SourceTestUtils class. `SourceTestUtils` contains utilities for automatically
+     verifying some of the properties of your `BoundedSource` implementation. You
+     can use `SourceTestUtils` to increase your implementation's test coverage
+     using a wide range of inputs with relatively few lines of code. For
+     examples that use `SourceTestUtils`, see the
+     [AvroSourceTest](https://github.com/apache/beam/blob/master/sdks/java/core/src/test/java/org/apache/beam/sdk/io/AvroSourceTest.java) and
+     [TextIOReadTest](https://github.com/apache/beam/blob/master/sdks/java/core/src/test/java/org/apache/beam/sdk/io/TextIOReadTest.java)
+     source code.
+
+In addition, see the [PTransform style guide]({{ site.baseurl }}/contribute/ptransform-style-guide/)
+for Beam's transform style guidance.
+
+## Implementing the Source interface
+
+To create a data source for your pipeline, you must provide the format-specific
+logic that tells a runner how to read data from your input source, and how to
+split your data source into multiple parts so that multiple worker instances can
+read your data in parallel. If you're creating a data source that reads
+unbounded data, you must provide additional logic for managing your source's
+watermark and optional checkpointing.
+
+Supply the logic for your source by creating the following classes:
+
+  * A subclass of `BoundedSource` if you want to read a finite (batch) data set,
+    or a subclass of `UnboundedSource` if you want to read an infinite (streaming)
+    data set. These subclasses describe the data you want to read, including the
+    data's location and parameters (such as how much data to read).  
+
+  * A subclass of `Source.Reader`. Each Source must have an associated Reader that
+    captures all the state involved in reading from that `Source`. This can
+    include things like file handles, RPC connections, and other parameters that
+    depend on the specific requirements of the data format you want to read.  
+
+  * The `Reader` class hierarchy mirrors the Source hierarchy. If you're extending
+    `BoundedSource`, you'll need to provide an associated `BoundedReader`. if you're
+    extending `UnboundedSource`, you'll need to provide an associated
+    `UnboundedReader`.
+
+  * One or more user-facing wrapper composite transforms (`PTransform`) that
+    wrap read operations. [PTransform wrappers](#ptransform-wrappers) discusses
+    why you should avoid exposing your sources.
+
+
+### Implementing the Source subclass
+
+You must create a subclass of either `BoundedSource` or `UnboundedSource`,
+depending on whether your data is a finite batch or an infinite stream. In
+either case, your `Source` subclass must override the abstract methods in the
+superclass. A runner might call these methods when using your data source. For
+example, when reading from a bounded source, a runner uses these methods to
+estimate the size of your data set and to split it up for parallel reading.
+
+Your `Source` subclass should also manage basic information about your data
+source, such as the location. For example, the example `Source` implementation
+in Beam’s [DatastoreIO](https://beam.apache.org/releases/javadoc/current/index.html?org/apache/beam/sdk/io/gcp/datastore/DatastoreIO.html)
+class takes host, datasetID, and query as arguments. The connector uses these
+values to obtain data from Cloud Datastore.
+
+#### BoundedSource
+
+`BoundedSource` represents a finite data set from which a Beam runner may read,
+possibly in parallel. `BoundedSource` contains a set of abstract methods that
+the runner uses to split the data set for reading by multiple workers.
+
+To implement a `BoundedSource`, your subclass must override the following
+abstract methods:
+
+  * `split`: The runner uses this method to split your finite data
+    into bundles of a given size.  
+
+  * `getEstimatedSizeBytes`: The runner uses this method to estimate the total
+    size of your data, in bytes.  
+
+  * `createReader`: Creates the associated `BoundedReader` for this
+    `BoundedSource`.
+
+You can see a model of how to implement `BoundedSource` and the required
+abstract methods in Beam’s implementations for Cloud BigTable
+([BigtableIO.java](https://github.com/apache/beam/blob/master/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigtable/BigtableIO.java))
+and BigQuery ([BigQuerySourceBase.java](https://github.com/apache/beam/blob/master/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQuerySourceBase.java)).
+
+#### UnboundedSource
+
+`UnboundedSource` represents an infinite data stream from which the runner may
+read, possibly in parallel. `UnboundedSource` contains a set of abstract methods
+that the runner uses to support streaming reads in parallel; these include
+*checkpointing* for failure recovery, *record IDs* to prevent data duplication,
+and *watermarking* for estimating data completeness in downstream parts of your
+pipeline.
+
+To implement an `UnboundedSource`, your subclass must override the following
+abstract methods:
+
+  * `split`: The runner uses this method to generate a list of
+    `UnboundedSource` objects which represent the number of sub-stream instances
+    from which the service should read in parallel.
+
+  * `getCheckpointMarkCoder`: The runner uses this method to obtain the Coder for
+    the checkpoints for your source (if any).
+
+  * `requiresDeduping`: The runner uses this method to determine whether the data
+    requires explicit removal of duplicate records. If this method returns true,
+    the runner will automatically insert a step to remove duplicates from your
+    source's output.  This should return true if and only if your source
+    provides record IDs for each record. See `UnboundedReader.getCurrentRecordId`
+    for when this should be done.
+
+  * `createReader`: Creates the associated `UnboundedReader` for this
+    `UnboundedSource`.
+
+### Implementing the Reader subclass
+
+You must create a subclass of either `BoundedReader` or `UnboundedReader` to be
+returned by your source subclass's `createReader` method. The runner uses the
+methods in your `Reader` (whether bounded or unbounded) to do the actual reading
+of your dataset.
+
+`BoundedReader` and `UnboundedReader` have similar basic interfaces, which
+you'll need to define. In addition, there are some additional methods unique to
+`UnboundedReader` that you'll need to implement for working with unbounded data,
+and an optional method you can implement if you want your `BoundedReader` to
+take advantage of dynamic work rebalancing. There are also minor differences in
+the semantics for the `start()` and `advance()` methods when using
+`UnboundedReader`.
+
+#### Reader methods common to both BoundedReader and UnboundedReader
+
+A runner uses the following methods to read data using `BoundedReader` or
+`UnboundedReader`:
+
+  * `start`: Initializes the `Reader` and advances to the first record to be read.
+    This method is called exactly once when the runner begins reading your data,
+    and is a good place to put expensive operations needed for initialization.  
+
+  * `advance`: Advances the reader to the next valid record. This method must
+    return false if there is no more input available. `BoundedReader` should stop
+    reading once advance returns false, but `UnboundedReader` can return true in
+    future calls once more data is available from your stream.  
+
+  * `getCurrent`: Returns the data record at the current position, last read by
+    start or advance.  
+
+  * `getCurrentTimestamp`: Returns the timestamp for the current data record. You
+    only need to override `getCurrentTimestamp` if your source reads data that has
+    intrinsic timestamps. The runner uses this value to set the intrinsic
+    timestamp for each element in the resulting output `PCollection`.
+
+#### Reader methods unique to UnboundedReader
+
+In addition to the basic `Reader` interface, `UnboundedReader` has some
+additional methods for managing reads from an unbounded data source:
+
+  * `getCurrentRecordId`: Returns a unique identifier for the current record.
+    The runner uses these record IDs to filter out duplicate records. If your
+    data has logical IDs present in each record, you can have this method return
+    them; otherwise, you can return a hash of the record contents, using at
+    least a 128-bit hash. It is incorrect to use Java's `Object.hashCode()`, as
+    a 32-bit hash is generally insufficient for preventing collisions, and
+    `hasCode()` is not guaranteed to be stable across processes.  
+
+    Implementing `getCurrentRecordId` is optional if your source uses a
+    checkpointing scheme that uniquely identifies each record. For example, if
+    your splits are files and the checkpoints are file positions up to which all
+    data has been read, you do not need record IDs. However, record IDs can
+    still be useful if upstream systems writing data to your source occasionally
+    produce duplicate records that your source might then read.  
+
+  * `getWatermark`: Returns a watermark that your `Reader` provides. The watermark
+    is the approximate lower bound on timestamps of future elements to be read
+    by your `Reader`. The runner uses the watermark as an estimate of data
+    completeness. Watermarks are used in windowing and triggers.  
+
+  * `getCheckpointMark`: The runner uses this method to create a checkpoint in
+    your data stream. The checkpoint represents the progress of the
+    `UnboundedReader`, which can be used for failure recovery. Different data
+    streams may use different checkpointing methods; some sources might require
+    received records to be acknowledged, while others might use positional
+    checkpointing. You'll need to tailor this method to the most appropriate
+    checkpointing scheme. For example, you might have this method return the
+    most recently acked record(s).  
+
+  * `getCheckpointMark` is optional; you don't need to implement it if your data
+    does not have meaningful checkpoints. However, if you choose not to
+    implement checkpointing in your source, you may encounter duplicate data or
+    data loss in your pipeline, depending on whether your data source tries to
+    re-send records in case of errors.  
+
+You can read a bounded `PCollection` from an `UnboundedSource` by specifying
+either `.withMaxNumRecords` or `.withMaxReadTime` when you read from your
+source.  `.withMaxNumRecords` reads a fixed maximum number of records from your
+unbounded source, while `.withMaxReadTime` reads from your unbounded source for
+a fixed maximum time duration.
+
+#### Using your BoundedSource with dynamic work rebalancing {#bounded-dynamic}
+
+If your source provides bounded data, you can have your `BoundedReader` work
+with dynamic work rebalancing by implementing the method `splitAtFraction`. The
+runner may call `splitAtFraction` concurrently with start or advance on a given
+reader so that the remaining data in your `Source` can be split and
+redistributed to other workers.
+
+When you implement `splitAtFraction`, your code must produce a
+mutually-exclusive set of splits where the union of those splits matches the
+total data set.
+
+If you implement `splitAtFraction`, you must implement both `splitAtFraction`
+and `getFractionConsumed` in a thread-safe manner, or data loss is possible. You
+should also unit-test your implementation exhaustively to avoid data duplication
+or data loss.
+
+To ensure that your code is thread-safe, use the `RangeTracker` thread-safe
+helper object to manage positions in your data source when implementing
+`splitAtFraction` and `getFractionConsumed`.
+
+We highly recommended that you unit test your implementations of
+`splitAtFraction` using the `SourceTestUtils` class. `SourceTestUtils` contains
+a number of methods for testing your implementation of `splitAtFraction`,
+including exhaustive automatic testing.
+
+### Convenience Source and Reader base classes
+
+The Beam SDK contains some convenient abstract base classes to help you create
+`Source` and `Reader` classes that work with common data storage formats, like
+files.
+
+#### FileBasedSource
+
+If your data source uses files, you can derive your `Source` and `Reader`
+classes from the `FileBasedSource` and `FileBasedReader` abstract base classes.
+`FileBasedSource` is a bounded source subclass that implements code common to
+Beam sources that interact with files, including:
+
+  * File pattern expansion
+  * Sequential record reading
+  * Split points
+
+
+## Using the FileBasedSink abstraction {#using-filebasedsink}
+
+If your data source uses files, you can implement the `FileBasedSink`
+abstraction to create a file-based sink. For other sinks, use `ParDo`,
+`GroupByKey`, and other transforms offered by the Beam SDK for Java. See the
+[developing I/O connectors overview]({{ site.baseurl }}/documentation/io/developing-io-overview/)
+for more details.
+
+When using the `FileBasedSink` interface, you must provide the format-specific
+logic that tells the runner how to write bounded data from your pipeline's
+`PCollection`s to an output sink. The runner writes bundles of data in parallel
+using multiple workers.
+
+Supply the logic for your file-based sink by implementing the following classes:
+
+  * A subclass of the abstract base class `FileBasedSink`. `FileBasedSink`
+    describes a location or resource that your pipeline can write to in
+    parallel. To avoid exposing your sink to end-users, your `FileBasedSink`
+    subclass should be protected or private.
+
+  * A user-facing wrapper `PTransform` that, as part of the logic, calls
+    [WriteFiles](https://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/io/WriteFiles.java)
+    and passes your `FileBasedSink` as a parameter. A user should not need to
+    call `WriteFiles` directly.
+
+The `FileBasedSink` abstract base class implements code that is common to Beam
+sinks that interact with files, including:
+
+  * Setting file headers and footers
+  * Sequential record writing
+  * Setting the output MIME type
+
+`FileBasedSink` and its subclasses support writing files to any Beam-supported
+`FileSystem` implementations. See the following Beam-provided `FileBasedSink`
+implementations for examples:
+
+  * [TextSink](https://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/io/TextSink.java) and
+  * [AvroSink](https://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/io/AvroSink.java).
+
+
+## PTransform wrappers {#ptransform-wrappers}
+
+When you create a source or sink that end-users will use, avoid exposing your
+source or sink code. To avoid exposing your sources and sinks to end-users, your
+new classes should be protected or private. Then, implement a user-facing
+wrapper `PTransform`. By exposing your source or sink as a transform, your
+implementation is hidden and can be arbitrarily complex or simple. The greatest
+benefit of not exposing implementation details is that later on, you can add
+additional functionality without breaking the existing implementation for users.
+
+For example, if your users’ pipelines read from your source using
+`read` and you want to insert a reshard into the pipeline, all
+users would need to add the reshard themselves (using the `GroupByKey`
+transform). To solve this, we recommended that you expose the source as a
+composite `PTransform` that performs both the read operation and the reshard.
+
+See Beam’s [PTransform style guide]({{ site.baseurl }}/contribute/ptransform-style-guide/#exposing-a-ptransform-vs-something-else)
+for additional information about wrapping with a `PTransform`.
+
diff --git a/website/src/documentation/io/developing-io-overview.md b/website/src/documentation/io/developing-io-overview.md
new file mode 100644
index 00000000000..eabb8d9919e
--- /dev/null
+++ b/website/src/documentation/io/developing-io-overview.md
@@ -0,0 +1,170 @@
+---
+layout: section
+title: "Overview: Developing a new I/O connector"
+section_menu: section-menu/documentation.html
+permalink: /documentation/io/developing-io-overview/
+redirect_from:
+  - /documentation/io/authoring-overview/
+  - /documentation/io/io-toc/
+---
+<!--
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+-->
+
+# Overview: Developing a new I/O connector
+
+_A guide for users who need to connect to a data store that isn't supported by
+the [Built-in I/O connectors]({{site.baseurl }}/documentation/io/built-in/)_
+
+To connect to a data store that isn’t supported by Beam’s existing I/O
+connectors, you must create a custom I/O connector. A connector usually consists
+of a source and a sink. All Beam sources and sinks are composite transforms;
+however, the implementation of your custom I/O depends on your use case. Here
+are the recommended steps to get started:
+
+1. Read this overview and choose your implementation. You can email the
+   [Beam dev mailing list]({{ site.baseurl }}/get-started/support) with any
+   questions you might have. In addition, you can check if anyone else is
+   working on the same I/O connector.  
+
+1. If you plan to contribute your I/O connector to the Beam community, see the
+   [Apache Beam contribution guide]({{ site.baseurl }}/contribute/contribution-guide/).  
+
+1. Read the [PTransform style guide]({{ site.baseurl }}/contribute/ptransform-style-guide/)
+   for additional style guide recommendations.
+
+
+## Sources
+
+For **bounded (batch) sources**, there are currently two options for creating a
+Beam source:
+
+1. Use `ParDo` and `GroupByKey`.  
+
+1. Use the `Source` interface and extend the `BoundedSource` abstract subclass.
+
+`ParDo` is the recommended option, as implementing a `Source` can be tricky. See
+[When to use the Source interface](#when-to-use-source) for a list of some use
+cases where you might want to use a `Source` (such as
+[dynamic work rebalancing]({{ site.baseurl }}/blog/2016/05/18/splitAtFraction-method.html)).
+
+(Java only) For **unbounded (streaming) sources**, you must use the `Source`
+interface and extend the `UnboundedSource` abstract subclass. `UnboundedSource`
+supports features that are useful for streaming pipelines, such as
+checkpointing.
+
+Splittable DoFn is a new sources framework that is under development and will
+replace the other options for developing bounded and unbounded sources. For more
+information, see the
+[roadmap for multi-SDK connector efforts]({{ site.baseurl }}/roadmap/connectors-multi-sdk/).
+
+### When to use the Source interface {#when-to-use-source}
+
+If you are not sure whether to use `Source`, feel free to email the [Beam dev
+mailing list]({{ site.baseurl }}/get-started/support) and we can discuss the
+specific pros and cons of your case.
+
+In some cases, implementing a `Source` might be necessary or result in better
+performance:
+
+* **Unbounded sources:** `ParDo` does not work for reading from unbounded
+  sources.  `ParDo` does not support checkpointing or mechanisms like de-duping
+  that are useful for streaming data sources.  
+
+* **Progress and size estimation:** `ParDo` can't provide hints to runners about
+  progress or the size of data they are reading. Without size estimation of the
+  data or progress on your read, the runner doesn't have any way to guess how
+  large your read will be. Therefore, if the runner attempts to dynamically
+  allocate workers, it does not have any clues as to how many workers you might
+  need for your pipeline.  
+
+* **Dynamic work rebalancing:** `ParDo` does not support dynamic work
+  rebalancing, which is used by some readers to improve the processing speed of
+  jobs. Depending on your data source, dynamic work rebalancing might not be
+  possible.  
+
+* **Splitting into parts of particular size recommended by the runner:** `ParDo`
+  does not receive `desired_bundle_size` as a hint from runners when performing
+  initial splitting.
+
+For example, if you'd like to read from a new file format that contains many
+records per file, or if you'd like to read from a key-value store that supports
+read operations in sorted key order.
+
+### Using ParDo and GroupByKey
+
+For data stores or file types where the data can be read in parallel, you can
+think of the process as a mini-pipeline. This often consists of two steps:
+
+1. Splitting the data into parts to be read in parallel  
+
+2. Reading from each of those parts
+
+Each of those steps will be a `ParDo`, with a `GroupByKey` in between. The
+`GroupByKey` is an implementation detail, but for most runners `GroupByKey`
+allows the runner to use different numbers of workers in some situations:
+
+* Determining how to split up the data to be read into chunks  
+
+* Reading data, which often benefits from more workers
+
+In addition, `GroupByKey` also allows dynamic work rebalancing to happen on
+runners that support the feature.
+
+Here are some examples of read transform implementations that use the "reading
+as a mini-pipeline" model when data can be read in parallel:
+
+* **Reading from a file glob**: For example, reading all files in "~/data/**".
+  * Get File Paths `ParDo`: As input, take in a file glob. Produce a
+    `PCollection` of strings, each of which is a file path.
+  * Reading `ParDo`: Given the `PCollection` of file paths, read each one,
+    producing a `PCollection` of records.
+
+* **Reading from a NoSQL database** (such as Apache HBase): These databases
+  often allow reading from ranges in parallel.
+  * Determine Key Ranges `ParDo`: As input, receive connection information for
+    the database and the key range to read from. Produce a `PCollection` of key
+    ranges that can be read in parallel efficiently.
+  * Read Key Range `ParDo`: Given the `PCollection` of key ranges, read the key
+    range, producing a `PCollection` of records.
+
+For data stores or files where reading cannot occur in parallel, reading is a
+simple task that can be accomplished with a single `ParDo`+`GroupByKey`. For
+example:
+
+  * **Reading from a database query**: Traditional SQL database queries often
+    can only be read in sequence. In this case, the `ParDo` would establish a
+    connection to the database and read batches of records, producing a
+    `PCollection` of those records.
+
+  * **Reading from a gzip file**: A gzip file must be read in order, so the read
+    cannot be parallelized. In this case, the `ParDo` would open the file and
+    read in sequence, producing a `PCollection` of records from the file.
+
+
+## Sinks
+
+To create a Beam sink, we recommend that you use a `ParDo` that writes the
+received records to the data store. To develop more complex sinks (for example,
+to support data de-duplication when failures are retried by a runner), use
+`ParDo`, `GroupByKey`, and other available Beam transforms.
+
+For **file-based sinks**, you can use the `FileBasedSink` abstraction that is
+provided by both the Java and Python SDKs. See our language specific
+implementation guides for more details:
+
+* [Developing I/O connectors for Java]({{ site.baseurl }}/documentation/io/developing-io-java/)
+* [Developing I/O connectors for Python]({{ site.baseurl }}/documentation/io/developing-io-python/)
+
+
+
diff --git a/website/src/documentation/io/developing-io-python.md b/website/src/documentation/io/developing-io-python.md
new file mode 100644
index 00000000000..fdb2a766905
--- /dev/null
+++ b/website/src/documentation/io/developing-io-python.md
@@ -0,0 +1,281 @@
+---
+layout: section
+title: "Apache Beam: Developing I/O connectors for Python"
+section_menu: section-menu/documentation.html
+permalink: /documentation/io/developing-io-python/
+redirect_from:
+  - /documentation/io/authoring-python/
+  - /documentation/sdks/python-custom-io/
+---
+<!--
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+-->
+# Developing I/O connectors for Python
+
+To connect to a data store that isn’t supported by Beam’s existing I/O
+connectors, you must create a custom I/O connector that usually consist of a
+source and a sink. All Beam sources and sinks are composite transforms; however,
+the implementation of your custom I/O depends on your use case. Before you
+start, read the [new I/O connector overview]({{ site.baseurl }}/documentation/io/developing-io-overview/)
+for an overview of developing a new I/O connector, the available implementation
+options, and how to choose the right option for your use case.
+
+This guide covers using the [Source and FileBasedSink interfaces](https://beam.apache.org/releases/pydoc/{{ site.release_latest }}/apache_beam.io.iobase.html)
+for Python. The Java SDK offers the same functionality, but uses a slightly
+different API. See [Developing I/O connectors for Java]({{ site.baseurl }}/documentation/io/developing-io-java/)
+for information specific to the Java SDK.
+
+## Basic code requirements {#basic-code-reqs}
+
+Beam runners use the classes you provide to read and/or write data using
+multiple worker instances in parallel. As such, the code you provide for
+`Source` and `FileBasedSink` subclasses must meet some basic requirements:
+
+  1. **Serializability:** Your `Source` or `FileBasedSink` subclass must be
+     serializable.  The service may create multiple instances of your `Source`
+     or `FileBasedSink` subclass to be sent to multiple remote workers to
+     facilitate reading or writing in parallel. The *way* the source and sink
+     objects are serialized is runner specific.  
+
+  1. **Immutability:** Your `Source` or `FileBasedSink` subclass must be
+     effectively immutable. You should only use mutable state in your `Source`
+     or `FileBasedSink` subclass if you are using lazy evaluation of expensive
+     computations that you need to implement the source.  
+
+  1. **Thread-Safety:** Your code must be thread-safe. The Beam SDK for Python
+     provides the `RangeTracker` class to make this easier.  
+
+  1. **Testability:** It is critical to exhaustively unit-test all of your
+     `Source` and `FileBasedSink` subclasses. A minor implementation error can
+     lead to data corruption or data loss (such as skipping or duplicating
+     records) that can be hard to detect. You can use test harnesses and utility
+     methods available in the [source_test_utils module](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/io/source_test_utils.py)
+     to develop tests for your source.
+
+In addition, see the [PTransform style guide]({{ site.baseurl }}/contribute/ptransform-style-guide/)
+for Beam's transform style guidance.
+
+## Implementing the Source interface
+
+To create a new data source for your pipeline, you'll need to provide the format-specific logic that tells the service how to read data from your input source, and how to split your data source into multiple parts so that multiple worker instances can read your data in parallel.
+
+Supply the logic for your new source by creating the following classes:
+
+  * A subclass of `BoundedSource`. `BoundedSource` is a source that reads a
+    finite amount of input records. The class describes the data you want to
+    read, including the data's location and parameters (such as how much data to
+    read).
+  * A subclass of `RangeTracker`. `RangeTracker` is a thread-safe object used to
+    manage a range for a given position type.
+  * One or more user-facing wrapper composite transforms (`PTransform`) that
+    wrap read operations. [PTransform wrappers](#ptransform-wrappers) discusses
+    why you should avoid exposing your sources, and walks through how to create
+    a wrapper.
+
+You can find these classes in the
+[apache_beam.io.iobase module](https://beam.apache.org/releases/pydoc/{{ site.release_latest }}/apache_beam.io.iobase.html).
+
+### Implementing the BoundedSource subclass
+
+`BoundedSource` represents a finite data set from which the service reads, possibly in parallel. `BoundedSource` contains a set of methods that the service uses to split the data set for reading by multiple remote workers.
+
+To implement a `BoundedSource`, your subclass must override the following methods:
+
+* `estimate_size`: Services use this method to estimate the *total size* of your data, in bytes. This estimate is in terms of external storage size, before performing decompression or other processing.
+
+* `split`: Service use this method to split your finite data into bundles of a given size.
+
+* `get_range_tracker`: Services use this method to get the `RangeTracker` for a given position range, and use the information to report progress and perform dynamic splitting of sources.
+
+* `read`: This method returns an iterator that reads data from the source, with respect to the boundaries defined by the given `RangeTracker` object.
+
+### Implementing the RangeTracker subclass
+
+A `RangeTracker` is a thread-safe object used to manage the current range and current position of the reader of a `BoundedSource` and protect concurrent access to them.
+
+To implement a `RangeTracker`, you should first familiarize yourself with the following definitions:
+
+* **Position-based sources** - A position-based source can be described by a range of positions of an ordered type, and the records read by the source can be described by positions of that type. For example, for a record within a file, the position can be the starting byte offset of the record. The position type for the record in this case is `long`.
+
+    The main requirement for position-based sources is **associativity**: Reading records in position range '[A, B)' and records in position range '[B, C)' should give the same records as reading records in position range '[A, C)', where 'A' &lt;= 'B' &lt;= 'C'.  This property ensures that no matter how many arbitrary sub-ranges a range of positions is split into, the total set of records they describe stays the same.
+
+    The other important property is how the source's range relates to positions of records in the source. In many sources each record can be identified by a unique starting position. In this case:
+
+     * All records returned by a source '[A, B)' must have starting positions in this range.
+     * All but the last record should end within this range. The last record may or may not extend past the end of the range.
+     * Records must not overlap.
+
+    Such sources should define "read '[A, B)'" as "read from the first record starting at or after 'A', up to but not including the first record starting at or after 'B'".
+
+    Some examples of such sources include reading lines or CSV from a text file, reading keys and values from a database, etc.
+
+    The concept of *split points* allows to extend the definitions for dealing with sources where some records cannot be identified by a unique starting position.
+
+* **Split points** - A split point describes a record that is the first one returned when reading the range from and including position **A** up to infinity (i.e. [A, infinity)).
+
+    Some sources may have records that are not directly addressable. For example, imagine a file format consisting of a sequence of compressed blocks. Each block can be assigned an offset, but records within the block cannot be directly addressed without decompressing the block. Let us refer to this hypothetical format as *CBF (Compressed Blocks Format)*.
+
+    Many such formats can still satisfy the associativity property. For example, in CBF, reading [A, B) can mean "read all the records in all blocks whose starting offset is in [A, B)".
+
+    To support such complex formats, Beam introduces the notion of *split points*. A record is a split point if there exists a position **A** such that the record is the first one to be returned when reading the range [A, infinity). In CBF, the only split points would be the first records in each block.
+
+    Split points allow us to define the meaning of a record's position and a source's range in the following cases:
+
+     * For a record that is at a split point, its position is defined to be the largest **A** such that reading a source with the range [A, infinity) returns this record.
+     * Positions of other records are only required to be non-decreasing.
+     * Reading the source [A, B) must return records starting from the first split point at or after **A**, up to but not including the first split point at or after **B**. In particular, this means that the first record returned by a source MUST always be a split point.
+     * Positions of split points must be unique.
+
+    As a result, for any decomposition of the full range of the source into position ranges, the total set of records will be the full set of records in the source, and each record will be read exactly once.
+
+* **Consumed positions** - Consumed positions refer to records that have been read.
+
+    As the source is being read, and records read from it are being passed to the downstream transforms in the pipeline, we say that positions in the source are being *consumed*. When a reader has read a record (or promised to a caller that a record will be returned), positions up to and including the record's start position are considered *consumed*.
+
+    Dynamic splitting can happen only at *unconsumed* positions. If the reader just returned a record at offset 42 in a file, dynamic splitting can happen only at offset 43 or beyond. Otherwise, that record could be read twice (by the current reader and the reader of the new task).
+
+#### RangeTracker methods
+
+To implement a `RangeTracker`, your subclass must override the following methods:
+
+* `start_position`: Returns the starting position of the current range, inclusive.
+
+* `stop_position`: Returns the ending position of the current range, exclusive.
+
+* `try_claim`: This method is used to determine if a record at a split point is within the range. This method should modify the internal state of the `RangeTracker` by updating the last-consumed position to the given starting `position` of the record being read by the source. The method returns true if the given position falls within the current range.
+
+* `set_current_position`: This method updates the last-consumed position to the given starting position of a record being read by a source. You can invoke this method for records that do not start at split points, and this should modify the internal state of the `RangeTracker`. If the record starts at a split point, you must invoke `try_claim` instead of this method.
+
+* `position_at_fraction`: Given a fraction within the range [0.0, 1.0), this method will return the position at the given fraction compared to the position range [`self.start_position`, `self.stop_position`).
+
+* `try_split`: This method attempts to split the current range into two parts around a suggested position. It is allowed to split at a different position, but in most cases it will split at the suggested position.
+
+This method splits the current range [`self.start_position`, `self.stop_position`) into a "primary" part [`self.start_position`, `split_position`), and a "residual" part [`split_position`, `self.stop_position`), assuming that `split_position` has not been consumed yet.
+
+If `split_position` has already been consumed, the method returns `None`.  Otherwise, it updates the current range to be the primary and returns a tuple (`split_position`, `split_fraction`).  `split_fraction` should be the fraction of size of range [`self.start_position`, `split_position`) compared to the original (before split) range [`self.start_position`, `self.stop_position`).
+
+* `fraction_consumed`: Returns the approximate fraction of consumed positions in the source.
+
+**Note:** Methods of class `iobase.RangeTracker` may be invoked by multiple threads, hence this class must be made thread-safe, for example, by using a single lock object.
+
+### Convenience Source base classes
+
+The Beam SDK for Python contains some convenient abstract base classes to help you easily create new sources.
+
+#### FileBasedSource
+
+`FileBasedSource` is a framework for developing sources for new file types. You can derive your `BoundedSource` class from the [FileBasedSource](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/io/filebasedsource.py) class.
+
+To create a source for a new file type, you need to create a sub-class of `FileBasedSource`.  Sub-classes of `FileBasedSource` must implement the method `FileBasedSource.read_records()`.
+
+See [AvroSource](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/io/avroio.py) for an example implementation of `FileBasedSource`.
+
+
+### Reading from a new Source
+
+The following example, `CountingSource`, demonstrates an implementation of `BoundedSource` and uses the SDK-provided `RangeTracker` called `OffsetRangeTracker`.
+
+```
+{% github_sample /apache/beam/blob/master/sdks/python/apache_beam/examples/snippets/snippets.py tag:model_custom_source_new_source %}```
+
+To read data from the source in your pipeline, use the `Read` transform:
+
+```
+{% github_sample /apache/beam/blob/master/sdks/python/apache_beam/examples/snippets/snippets.py tag:model_custom_source_use_new_source %}```
+
+**Note:** When you create a source that end-users are going to use, we
+recommended that you do not expose the code for the source itself as
+demonstrated in the example above. Use a wrapping `PTransform` instead.
+[PTransform wrappers](#ptransform-wrappers) discusses why you should avoid
+exposing your sources, and walks through how to create a wrapper.
+
+
+## Using the FileBasedSink abstraction
+
+If your data source uses files, you can implement the [FileBasedSink](https://beam.apache.org/releases/pydoc/{{ site.release_latest }}/apache_beam.io.filebasedsink.html)
+abstraction to create a file-based sink. For other sinks, use `ParDo`,
+`GroupByKey`, and other transforms offered by the Beam SDK for Python. See the
+[developing I/O connectors overview]({{ site.baseurl }}/documentation/io/developing-io-overview/)
+for more details.
+
+When using the `FileBasedSink` interface, you must provide the format-specific
+logic that tells the runner how to write bounded data from your pipeline's
+`PCollection`s to an output sink. The runner writes bundles of data in parallel
+using multiple workers.
+
+Supply the logic for your file-based sink by implementing the following classes:
+
+  * A subclass of the abstract base class `FileBasedSink`. `FileBasedSink`
+    describes a location or resource that your pipeline can write to in
+    parallel. To avoid exposing your sink to end-users, use the `_` prefix when
+    creating your `FileBasedSink` subclass.
+
+  * A user-facing wrapper `PTransform` that, as part of the logic, calls
+    `Write` and passes your `FileBasedSink` as a parameter. A user should not
+    need to call `Write` directly.
+
+The `FileBasedSink` abstract base class implements code that is common to Beam
+sinks that interact with files, including:
+
+  * Setting file headers and footers
+  * Sequential record writing
+  * Setting the output MIME type
+
+`FileBasedSink` and its subclasses support writing files to any Beam-supported
+`FileSystem` implementations. See the following Beam-provided `FileBasedSink`
+implementation for an example:
+
+  * [TextSink](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/io/textio.py)
+
+
+## PTransform wrappers {#ptransform-wrappers}
+
+When you create a source or sink that end-users will use, avoid exposing your
+source or sink code. To avoid exposing your sources and sinks to end-users, your
+new classes should use the `_` prefix. Then, implement a user-facing
+wrapper `PTransform`.`By exposing your source or sink as a transform, your
+implementation is hidden and can be arbitrarily complex or simple. The greatest
+benefit of not exposing implementation details is that later on, you can add
+additional functionality without breaking the existing implementation for users.
+
+For example, if your users’ pipelines read from your source using
+`beam.io.Read` and you want to insert a reshard into the pipeline, all
+users would need to add the reshard themselves (using the `GroupByKey`
+transform). To solve this, we recommended that you expose the source as a
+composite `PTransform` that performs both the read operation and the reshard.
+
+See Beam’s [PTransform style guide]({{ site.baseurl }}/contribute/ptransform-style-guide/#exposing-a-ptransform-vs-something-else)
+for additional information about wrapping with a `PTransform`.
+
+The following examples change the source and sink from the above sections so
+that they are not exposed to end-users. For the source, rename `CountingSource`
+to `_CountingSource`. Then, create the wrapper `PTransform`, called
+`ReadFromCountingSource`:
+
+```
+{% github_sample /apache/beam/blob/master/sdks/python/apache_beam/examples/snippets/snippets.py tag:model_custom_source_new_ptransform %}```
+
+Finally, read from the source:
+
+```
+{% github_sample /apache/beam/blob/master/sdks/python/apache_beam/examples/snippets/snippets.py tag:model_custom_source_use_ptransform %}```
+
+For the sink, rename `SimpleKVSink` to `_SimpleKVSink`. Then, create the wrapper `PTransform`, called `WriteToKVSink`:
+
+```
+{% github_sample /apache/beam/blob/master/sdks/python/apache_beam/examples/snippets/snippets.py tag:model_custom_sink_new_ptransform %}```
+
+Finally, write to the sink:
+
+```
+{% github_sample /apache/beam/blob/master/sdks/python/apache_beam/examples/snippets/snippets.py tag:model_custom_sink_use_ptransform %}```
diff --git a/website/src/documentation/io/io-toc.md b/website/src/documentation/io/io-toc.md
deleted file mode 100644
index 1002fc94850..00000000000
--- a/website/src/documentation/io/io-toc.md
+++ /dev/null
@@ -1,40 +0,0 @@
----
-layout: section
-title: "Pipeline I/O"
-section_menu: section-menu/documentation.html
-permalink: /documentation/io/io-toc/
----
-<!--
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
--->
-
-# Pipeline I/O
-
-## Using Pipeline I/O
-* [Programming Guide: Using I/O Transforms]({{site.baseurl }}/documentation/programming-guide#pipeline-io)
-* [Built-in I/O Transforms]({{site.baseurl }}/documentation/io/built-in/)
-
-
-## Authoring Read &amp; Write I/O Transforms
-
-> Note: This guide is still in progress. There is an open issue to finish the guide: [BEAM-1025](https://issues.apache.org/jira/browse/BEAM-1025).
-
-* [Authoring I/O Transforms - Overview]({{site.baseurl }}/documentation/io/authoring-overview/)
-* [Testing I/O Transforms]({{site.baseurl }}/documentation/io/testing/)
-
-<!-- TODO: commented out until this content is ready.
-* [Authoring I/O Transforms - Python]({{site.baseurl }}/documentation/io/authoring-python/)
-* [Authoring I/O Transforms - Java]({{site.baseurl }}/documentation/io/authoring-java/)
-
-* [Contributing I/O Transforms]({{site.baseurl }}/documentation/io/contributing/)
--->
diff --git a/website/src/documentation/io/testing.md b/website/src/documentation/io/testing.md
index abfa3a6fb55..05ae6b0ea74 100644
--- a/website/src/documentation/io/testing.md
+++ b/website/src/documentation/io/testing.md
@@ -18,9 +18,6 @@ See the License for the specific language governing permissions and
 limitations under the License.
 -->
 
-[Pipeline I/O Table of Contents]({{site.baseurl}}/documentation/io/io-toc/)
-
-
 ## Testing I/O Transforms in Apache Beam
 
 *Examples and design patterns for testing Apache Beam I/O transforms*
diff --git a/website/src/documentation/programming-guide.md b/website/src/documentation/programming-guide.md
index 48f4ead3c3d..915a4fd8de4 100644
--- a/website/src/documentation/programming-guide.md
+++ b/website/src/documentation/programming-guide.md
@@ -1716,7 +1716,7 @@ Beam provides read and write transforms for a [number of common data storage
 types]({{ site.baseurl }}/documentation/io/built-in/). If you want your pipeline
 to read from or write to a data storage format that isn't supported by the
 built-in transforms, you can [implement your own read and write
-transforms]({{site.baseurl }}/documentation/io/io-toc/).
+transforms]({{site.baseurl }}/documentation/io/developing-io-overview/).
 
 ### 5.1. Reading input data {#pipeline-io-reading-data}
 
diff --git a/website/src/documentation/sdks/python-custom-io.md b/website/src/documentation/sdks/python-custom-io.md
deleted file mode 100644
index 9d96c18dfe5..00000000000
--- a/website/src/documentation/sdks/python-custom-io.md
+++ /dev/null
@@ -1,307 +0,0 @@
----
-layout: section
-title: "Apache Beam: Creating New Sources and Sinks with the Python SDK"
-section_menu: section-menu/sdks.html
-permalink: /documentation/sdks/python-custom-io/
----
-<!--
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
--->
-# Creating New Sources and Sinks with the Python SDK
-
-The Apache Beam SDK for Python provides an extensible API that you can use to create new data sources and sinks. This tutorial shows how to create new sources and sinks using [Beam's Source and Sink API](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/io/iobase.py).
-
-* Create a new source by extending the `BoundedSource` and `RangeTracker` interfaces.
-* Create a new sink by implementing the `Sink` and `Writer` classes.
-
-
-## Why Create a New Source or Sink
-
-You'll need to create a new source or sink if you want your pipeline to read data from (or write data to) a storage system for which the Beam SDK for Python does not provide [native support]({{ site.baseurl }}/documentation/programming-guide/#pipeline-io).
-
-In simple cases, you may not need to create a new source or sink. For example, if you need to read data from an SQL database using an arbitrary query, none of the advanced Source API features would benefit you. Likewise, if you'd like to write data to a third-party API via a protocol that lacks deduplication support, the Sink API wouldn't benefit you. In such cases it makes more sense to use a `ParDo`.
-
-However, if you'd like to use advanced features such as dynamic splitting and size estimation, you should use Beam's APIs and create a new source or sink.
-
-
-## Basic Code Requirements for New Sources and Sinks {#basic-code-reqs}
-
-Services use the classes you provide to read and/or write data using multiple worker instances in parallel. As such, the code you provide for `Source` and `Sink` subclasses must meet some basic requirements:
-
-### Serializability
-
-Your `Source` or `Sink` subclass must be serializable. The service may create multiple instances of your `Source` or `Sink` subclass to be sent to multiple remote workers to facilitate reading or writing in parallel. Note that the *way* the source and sink objects are serialized is runner specific.
-
-### Immutability
-
-Your `Source` or `Sink` subclass must be effectively immutable. You should only use mutable state in your `Source` or `Sink` subclass if you are using lazy evaluation of expensive computations that you need to implement the source.
-
-### Thread-Safety
-
-Your code must be thread-safe. The Beam SDK for Python provides the `RangeTracker` class to make this easier.
-
-### Testability
-
-It is critical to exhaustively unit-test all of your `Source` and `Sink` subclasses. A minor implementation error can lead to data corruption or data loss (such as skipping or duplicating records) that can be hard to detect.
-
-You can use test harnesses and utility methods available in the [source_test_utils module](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/io/source_test_utils.py) to develop tests for your source.
-
-
-## Creating a New Source
-
-You should create a new source if you'd like to use the advanced features that the Source API provides:
-
-* Dynamic splitting
-* Progress estimation
-* Size estimation
-* Splitting into parts of particular size recommended by the service
-
-For example, if you'd like to read from a new file format that contains many records per file, or if you'd like to read from a key-value store that supports read operations in sorted key order.
-
-To create a new data source for your pipeline, you'll need to provide the format-specific logic that tells the service how to read data from your input source, and how to split your data source into multiple parts so that multiple worker instances can read your data in parallel.
-
-You supply the logic for your new source by creating the following classes:
-
-* A subclass of `BoundedSource`, which you can find in the [iobase.py](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/io/iobase.py) module. `BoundedSource` is a source that reads a finite amount of input records. The class describes the data you want to read, including the data's location and parameters (such as how much data to read).
-* A subclass of `RangeTracker`, which you can find in the [iobase.py](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/io/iobase.py) module. `RangeTracker` is a thread-safe object used to manage a range for a given position type.
-
-### Implementing the BoundedSource Subclass
-
-`BoundedSource` represents a finite data set from which the service reads, possibly in parallel. `BoundedSource` contains a set of methods that the service uses to split the data set for reading by multiple remote workers.
-
-To implement a `BoundedSource`, your subclass must override the following methods:
-
-* `estimate_size`: Services use this method to estimate the *total size* of your data, in bytes. This estimate is in terms of external storage size, before performing decompression or other processing.
-
-* `split`: Service use this method to split your finite data into bundles of a given size.
-
-* `get_range_tracker`: Services use this method to get the `RangeTracker` for a given position range, and use the information to report progress and perform dynamic splitting of sources.
-
-* `read`: This method returns an iterator that reads data from the source, with respect to the boundaries defined by the given `RangeTracker` object.
-
-### Implementing the RangeTracker Subclass
-
-A `RangeTracker` is a thread-safe object used to manage the current range and current position of the reader of a `BoundedSource` and protect concurrent access to them.
-
-To implement a `RangeTracker`, you should first familiarize yourself with the following definitions:
-
-* **Position-based sources** - A position-based source can be described by a range of positions of an ordered type, and the records read by the source can be described by positions of that type. For example, for a record within a file, the position can be the starting byte offset of the record. The position type for the record in this case is `long`.
-
-    The main requirement for position-based sources is **associativity**: Reading records in position range '[A, B)' and records in position range '[B, C)' should give the same records as reading records in position range '[A, C)', where 'A' &lt;= 'B' &lt;= 'C'.  This property ensures that no matter how many arbitrary sub-ranges a range of positions is split into, the total set of records they describe stays the same.
-
-    The other important property is how the source's range relates to positions of records in the source. In many sources each record can be identified by a unique starting position. In this case:
-
-     * All records returned by a source '[A, B)' must have starting positions in this range.
-     * All but the last record should end within this range. The last record may or may not extend past the end of the range.
-     * Records must not overlap.
-
-    Such sources should define "read '[A, B)'" as "read from the first record starting at or after 'A', up to but not including the first record starting at or after 'B'".
-
-    Some examples of such sources include reading lines or CSV from a text file, reading keys and values from a database, etc.
-
-    The concept of *split points* allows to extend the definitions for dealing with sources where some records cannot be identified by a unique starting position.
-
-* **Split points** - A split point describes a record that is the first one returned when reading the range from and including position **A** up to infinity (i.e. [A, infinity)).
-
-    Some sources may have records that are not directly addressable. For example, imagine a file format consisting of a sequence of compressed blocks. Each block can be assigned an offset, but records within the block cannot be directly addressed without decompressing the block. Let us refer to this hypothetical format as *CBF (Compressed Blocks Format)*.
-
-    Many such formats can still satisfy the associativity property. For example, in CBF, reading [A, B) can mean "read all the records in all blocks whose starting offset is in [A, B)".
-
-    To support such complex formats, Beam introduces the notion of *split points*. A record is a split point if there exists a position **A** such that the record is the first one to be returned when reading the range [A, infinity). In CBF, the only split points would be the first records in each block.
-
-    Split points allow us to define the meaning of a record's position and a source's range in the following cases:
-
-     * For a record that is at a split point, its position is defined to be the largest **A** such that reading a source with the range [A, infinity) returns this record.
-     * Positions of other records are only required to be non-decreasing.
-     * Reading the source [A, B) must return records starting from the first split point at or after **A**, up to but not including the first split point at or after **B**. In particular, this means that the first record returned by a source MUST always be a split point.
-     * Positions of split points must be unique.
-
-    As a result, for any decomposition of the full range of the source into position ranges, the total set of records will be the full set of records in the source, and each record will be read exactly once.
-
-* **Consumed positions** - Consumed positions refer to records that have been read.
-
-    As the source is being read, and records read from it are being passed to the downstream transforms in the pipeline, we say that positions in the source are being *consumed*. When a reader has read a record (or promised to a caller that a record will be returned), positions up to and including the record's start position are considered *consumed*.
-
-    Dynamic splitting can happen only at *unconsumed* positions. If the reader just returned a record at offset 42 in a file, dynamic splitting can happen only at offset 43 or beyond. Otherwise, that record could be read twice (by the current reader and the reader of the new task).
-
-#### RangeTracker Methods
-
-To implement a `RangeTracker`, your subclass must override the following methods:
-
-* `start_position`: Returns the starting position of the current range, inclusive.
-
-* `stop_position`: Returns the ending position of the current range, exclusive.
-
-* `try_claim`: This method is used to determine if a record at a split point is within the range. This method should modify the internal state of the `RangeTracker` by updating the last-consumed position to the given starting `position` of the record being read by the source. The method returns true if the given position falls within the current range.
-
-* `set_current_position`: This method updates the last-consumed position to the given starting position of a record being read by a source. You can invoke this method for records that do not start at split points, and this should modify the internal state of the `RangeTracker`. If the record starts at a split point, you must invoke `try_claim` instead of this method.
-
-* `position_at_fraction`: Given a fraction within the range [0.0, 1.0), this method will return the position at the given fraction compared to the position range [`self.start_position`, `self.stop_position`).
-
-* `try_split`: This method attempts to split the current range into two parts around a suggested position. It is allowed to split at a different position, but in most cases it will split at the suggested position.
-
-This method splits the current range [`self.start_position`, `self.stop_position`) into a "primary" part [`self.start_position`, `split_position`), and a "residual" part [`split_position`, `self.stop_position`), assuming that `split_position` has not been consumed yet.
-
-If `split_position` has already been consumed, the method returns `None`.  Otherwise, it updates the current range to be the primary and returns a tuple (`split_position`, `split_fraction`).  `split_fraction` should be the fraction of size of range [`self.start_position`, `split_position`) compared to the original (before split) range [`self.start_position`, `self.stop_position`).
-
-* `fraction_consumed`: Returns the approximate fraction of consumed positions in the source.
-
-**Note:** Methods of class `iobase.RangeTracker` may be invoked by multiple threads, hence this class must be made thread-safe, for example, by using a single lock object.
-
-### Convenience Source Base Classes
-
-The Beam SDK for Python contains some convenient abstract base classes to help you easily create new sources.
-
-#### FileBasedSource
-
-`FileBasedSource` is a framework for developing sources for new file types. You can derive your `BoundedSource` class from the [FileBasedSource](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/io/filebasedsource.py) class.
-
-To create a source for a new file type, you need to create a sub-class of `FileBasedSource`.  Sub-classes of `FileBasedSource` must implement the method `FileBasedSource.read_records()`.
-
-See [AvroSource](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/io/avroio.py) for an example implementation of `FileBasedSource`.
-
-
-## Reading from a New Source
-
-The following example, `CountingSource`, demonstrates an implementation of `BoundedSource` and uses the SDK-provided `RangeTracker` called `OffsetRangeTracker`.
-
-```
-{% github_sample /apache/beam/blob/master/sdks/python/apache_beam/examples/snippets/snippets.py tag:model_custom_source_new_source %}```
-
-To read data from the source in your pipeline, use the `Read` transform:
-
-```
-{% github_sample /apache/beam/blob/master/sdks/python/apache_beam/examples/snippets/snippets.py tag:model_custom_source_use_new_source %}```
-
-**Note:** When you create a source that end-users are going to use, it's recommended that you do not expose the code for the source itself as demonstrated in the example above, but rather use a wrapping `PTransform` instead. See [PTransform wrappers](#ptransform-wrappers) to see how and why to avoid exposing your sources.
-
-
-## Creating a New Sink
-
-You should create a new sink if you'd like to use the advanced features that the Sink API provides, such as global initialization and finalization that allow the write operation to appear "atomic" (i.e. either all data is written or none is).
-
-A sink represents a resource that can be written to using the `Write` transform. A parallel write to a sink consists of three phases:
-
-1. A sequential initialization phase. For example, creating a temporary output directory.
-2. A parallel write phase where workers write bundles of records.
-3. A sequential finalization phase. For example, merging output files.
-
-For example, if you'd like to write to a new table in a database, you should use the Sink API. In this case, the initializer will create a temporary table, the writer will write rows to it, and the finalizer will rename the table to a final location.
-
-To create a new data sink for your pipeline, you'll need to provide the format-specific logic that tells the sink how to write bounded data from your pipeline's `PCollection`s to an output sink. The sink writes bundles of data in parallel using multiple workers.
-
-You supply the writing logic by creating the following classes:
-
-* A subclass of `Sink`, which you can find in the [iobase.py](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/io/iobase.py) module.  `Sink` describes the location or resource to write to. Depending on the type of sink, your `Sink` subclass may contain fields such as the path to an output directory on a filesystem or a database table name. `Sink` provides three methods for performing a write operation to the sink it describes. Your subclass of `Sink` must implement these three methods: `initialize_write()`, `open_writer()`, and `finalize_write()`.
-
-* A subclass of `Writer`, which you can find in the [iobase.py](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/io/iobase.py) module. `Writer` writes a bundle of elements from an input `PCollection` to your designated data sink. `Writer` defines two methods: `write()`, which writes a single record from the bundle, and `close()`, which is called once at the end of writing a bundle.
-
-### Implementing the Sink Subclass
-
-Your `Sink` subclass describes the location or resource to which your pipeline writes its output. This might include a file system location, the name of a database table or dataset, etc.
-
-To implement a `Sink`, your subclass must override the following methods:
-
-* `initialize_write`: This method performs any necessary initialization before writing to the output location. Services call this method before writing begins. For example, you can use `initialize_write` to create a temporary output directory.
-
-* `open_writer`: This method enables writing a bundle of elements to the sink.
-
-* `finalize_write`:This method finalizes the sink after all data is written to it. Given the result of initialization and an iterable of results from bundle writes, `finalize_write` performs finalization after writing and closes the sink. This method is called after all bundle write operations are complete.
-
-**Caution:** `initialize_write` and `finalize_write` are conceptually called once: at the beginning and end of a `Write` transform.  However, when you implement these methods, you must ensure that they are **idempotent**, as they may be called multiple times on different machines in the case of failure, retry, or for redundancy.
-
-### Implementing the Writer Subclass
-
-Your `Writer` subclass implements the logic for writing a bundle of elements from a `PCollection` to output location defined in your `Sink`. Services may instantiate multiple instances of your `Writer` in different threads on the same worker, so access to any static members or methods must be thread-safe.
-
-To implement a `Writer`, your subclass must override the following abstract methods:
-
-* `write`: This method writes a value to your `Sink` using the current writer.
-
-* `close`: This method closes the current writer.
-
-#### Handling Bundle IDs
-
-When the service calls `Sink.open_writer`, it will pass a unique bundle ID for the records to be written. Your `Writer` must use this bundle ID to ensure that its output does not interfere with that of other `Writer` instances that might have been created in parallel. This is particularly important as the service may retry write operations multiple times in case of failure.
-
-For example, if your `Sink`'s output is file-based, your `Writer` class might use the bundle ID as a filename suffix to ensure that your `Writer` writes its records to a unique output file not used by other `Writer`s. You can then have your `Writer`'s `close` method return that file location as part of the write result.
-
-### Convenience Sink and Writer Base Classes
-
-The Beam SDK for Python contains some convenient abstract base classes to help you create `Source` and `Reader` classes that work with common data storage formats, like files.
-
-#### FileSink
-
-If your data source uses files, you can derive your `Sink` and `Writer` classes from the `FileBasedSink` and `FileBasedSinkWriter` classes, which can be found in the [filebasedsink.py](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/io/filebasedsink.py) module. These classes implement code common sinks that interact with files, including:
-
-* Setting file headers and footers
-* Sequential record writing
-* Setting the output MIME type
-
-
-## Writing to a New Sink
-
-Consider a simple key-value storage that writes a given set of key-value pairs to a set of tables. The following is the key-value storage's API:
-
-* `connect(url)` Connects to the storage and returns an access token which can be used to perform further operations.
-* `open_table(access_token, table_name)` Creates a table named 'table_name'. Returns a table object.
-* `write_to_table(access_token, table, key, value)` Writes a key, value pair to the given table.
-* `rename_table(access_token, old_name, new_name)` Renames the table named 'old_name' to 'new_name'.
-
-The following code demonstrates how to implement the `Sink` class for this key-value storage.
-
-```
-{% github_sample /apache/beam/blob/master/sdks/python/apache_beam/examples/snippets/snippets.py tag:model_custom_sink_new_sink %}```
-
-The following code demonstrates how to implement the `Writer` class for this key-value storage.
-
-```
-{% github_sample /apache/beam/blob/master/sdks/python/apache_beam/examples/snippets/snippets.py tag:model_custom_sink_new_writer %}```
-
-The following code demonstrates how to write to the sink using the `Write` transform.
-
-```
-{% github_sample /apache/beam/blob/master/sdks/python/apache_beam/examples/snippets/snippets.py tag:model_custom_sink_use_new_sink %}```
-
-**Note:** When you create a sink that end-users are going to use, it's recommended that you do not expose the code for the sink itself as demonstrated in the example above, but rather use a wrapping `PTransform` instead. See [PTransform wrappers](#ptransform-wrappers) to see how and why to avoid exposing your sinks.
-
-
-## PTransform Wrappers
-
-If you create a new source or sink for your own use, such as for learning purposes, you should create them as explained in the sections above and use them as demonstrated in the examples.
-
-However, when you create a source or sink that end-users are going to use, instead of exposing the source or sink itself, you should create a wrapper `PTransform`. Ideally, a source or sink should be exposed to users simply as "something that can be applied in a pipeline", which is actually a `PTransform`. That way, its implementation can be hidden and arbitrarily complex or simple.
-
-The greatest benefit of not exposing the implementation details is that later on you will be able to add additional functionality without breaking the existing implementation for users.  For example, if your users' pipelines read from your source using `beam.io.Read(...)` and you want to insert a reshard into the pipeline, all of your users would need to add the reshard themselves (using the `GroupByKey` transform). To solve this, it's recommended that you expose your source as a composite `PTransform` that performs both the read operation and the reshard.
-
-To avoid exposing your sources and sinks to end-users, it's recommended that you use the `_` prefix when creating your new source and sink classes. Then, create a wrapper `PTransform`.
-
-The following examples change the source and sink from the above sections so that they are not exposed to end-users. For the source, rename `CountingSource` to `_CountingSource`. Then, create the wrapper `PTransform`, called `ReadFromCountingSource`:
-
-```
-{% github_sample /apache/beam/blob/master/sdks/python/apache_beam/examples/snippets/snippets.py tag:model_custom_source_new_ptransform %}```
-
-Finally, read from the source:
-
-```
-{% github_sample /apache/beam/blob/master/sdks/python/apache_beam/examples/snippets/snippets.py tag:model_custom_source_use_ptransform %}```
-
-For the sink, rename `SimpleKVSink` to `_SimpleKVSink`. Then, create the wrapper `PTransform`, called `WriteToKVSink`:
-
-```
-{% github_sample /apache/beam/blob/master/sdks/python/apache_beam/examples/snippets/snippets.py tag:model_custom_sink_new_ptransform %}```
-
-Finally, write to the sink:
-
-```
-{% github_sample /apache/beam/blob/master/sdks/python/apache_beam/examples/snippets/snippets.py tag:model_custom_sink_use_ptransform %}```
diff --git a/website/src/documentation/sdks/python.md b/website/src/documentation/sdks/python.md
index ae19ee69930..8b80c7d8058 100644
--- a/website/src/documentation/sdks/python.md
+++ b/website/src/documentation/sdks/python.md
@@ -41,6 +41,10 @@ Python is a dynamically-typed language with no static type checking. The Beam SD
 
 When you run your pipeline locally, the packages that your pipeline depends on are available because they are installed on your local machine. However, when you want to run your pipeline remotely, you must make sure these dependencies are available on the remote machines. [Managing Python Pipeline Dependencies]({{ site.baseurl }}/documentation/sdks/python-pipeline-dependencies) shows you how to make your dependencies available to the remote workers.
 
-## Creating new sources and Sinks
+## Developing new I/O connectors for Python
+
+The Beam SDK for Python provides an extensible API that you can use to create
+new I/O connectors. See the [Developing I/O connectors overview]({{ site.baseurl }}/documentation/io/developing-io-overview)
+for information about developing new I/O connectors and links to
+language-specific implementation guidance.
 
-The Beam SDK for Python provides an extensible API that you can use to create new data sources and sinks. [Creating New Sources and Sinks with the Python SDK]({{ site.baseurl }}/documentation/sdks/python-custom-io) shows how to create new sources and sinks using [Beam's Source and Sink API](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/io/iobase.py).
