diff --git a/sdks/python/apache_beam/examples/snippets/snippets.py b/sdks/python/apache_beam/examples/snippets/snippets.py
index 936d06ebd23..f60330c65de 100644
--- a/sdks/python/apache_beam/examples/snippets/snippets.py
+++ b/sdks/python/apache_beam/examples/snippets/snippets.py
@@ -230,6 +230,9 @@ def pipeline_options_remote():
         job_name='unique-job-name',
         temp_location='gs://my-bucket/temp',
         region='us-central1')
+    # Note: Repeatable options like dataflow_service_options or experiments must
+    # be specified as a list of string(s).
+    # e.g. dataflow_service_options=['enable_prime']
 
     # Create the Pipeline with the specified options.
     with beam.Pipeline(options=beam_options) as pipeline:
diff --git a/sdks/python/apache_beam/options/pipeline_options.py b/sdks/python/apache_beam/options/pipeline_options.py
index d15e8754555..0d55a160ee6 100644
--- a/sdks/python/apache_beam/options/pipeline_options.py
+++ b/sdks/python/apache_beam/options/pipeline_options.py
@@ -707,7 +707,9 @@ class GoogleCloudOptions(PipelineOptions):
         help=(
             'Options to configure the Dataflow service. These '
             'options decouple service side feature availbility '
-            'from the Apache Beam release cycle.'))
+            'from the Apache Beam release cycle.'
+            'Note: If set programmatically, must be set as a '
+            'list of strings'))
     parser.add_argument(
         '--enable_hot_key_logging',
         default=False,
@@ -752,6 +754,12 @@ class GoogleCloudOptions(PipelineOptions):
             '--dataflow_job_file and --template_location '
             'are mutually exclusive.')
 
+    # Validate that dataflow_service_options is a list
+    if self.dataflow_service_options:
+      errors.extend(
+          validator.validate_repeatable_argument_passed_as_list(
+              self, 'dataflow_service_options'))
+
     return errors
 
 
@@ -984,6 +992,14 @@ class DebugOptions(PipelineOptions):
         return experiment.split('=', 1)[1]
     return default
 
+  def validate(self, validator):
+    errors = []
+    if self.experiments:
+      errors.extend(
+          validator.validate_repeatable_argument_passed_as_list(
+              self, 'experiments'))
+    return errors
+
 
 class ProfilingOptions(PipelineOptions):
   @classmethod
diff --git a/sdks/python/apache_beam/options/pipeline_options_validator.py b/sdks/python/apache_beam/options/pipeline_options_validator.py
index 15f1f65dc93..97fc0527f98 100644
--- a/sdks/python/apache_beam/options/pipeline_options_validator.py
+++ b/sdks/python/apache_beam/options/pipeline_options_validator.py
@@ -112,6 +112,9 @@ class PipelineOptionsValidator(object):
       'Option %s is required for environment type %s.')
   ERR_NUM_WORKERS_TOO_HIGH = (
       'num_workers (%s) cannot exceed max_num_workers (%s)')
+  ERR_REPEATABLE_OPTIONS_NOT_SET_AS_LIST = (
+      '(%s) is a string. Programmatically set PipelineOptions like (%s) '
+      'options need to be specified as a list.')
 
   # GCS path specific patterns.
   GCS_URI = '(?P<SCHEME>[^:]+)://(?P<BUCKET>[^/]+)(/(?P<OBJECT>.*))?'
@@ -365,3 +368,16 @@ class PipelineOptionsValidator(object):
           self._validate_error(
               self.ERR_INVALID_ENVIRONMENT, 'environment_config', 'LOOPBACK'))
     return errors
+
+  def validate_repeatable_argument_passed_as_list(self, view, arg_name):
+    """Validates that repeatable PipelineOptions like dataflow_service_options
+    or experiments are specified as a list when set programmatically. This
+    way, users do not inadvertently specify it as a string, mirroring the way
+    they are set via the command lineRepeatable options, which are as passed a
+    list.
+    """
+    arg = getattr(view, arg_name, None)
+    if not isinstance(arg, list):
+      return self._validate_error(
+          self.ERR_REPEATABLE_OPTIONS_NOT_SET_AS_LIST, arg, arg_name)
+    return []
diff --git a/sdks/python/apache_beam/options/pipeline_options_validator_test.py b/sdks/python/apache_beam/options/pipeline_options_validator_test.py
index 15071d8c7b7..ea5fe32aff5 100644
--- a/sdks/python/apache_beam/options/pipeline_options_validator_test.py
+++ b/sdks/python/apache_beam/options/pipeline_options_validator_test.py
@@ -28,6 +28,8 @@ from hamcrest import only_contains
 from hamcrest.core.base_matcher import BaseMatcher
 
 from apache_beam.internal import pickler
+from apache_beam.options.pipeline_options import DebugOptions
+from apache_beam.options.pipeline_options import GoogleCloudOptions
 from apache_beam.options.pipeline_options import PipelineOptions
 from apache_beam.options.pipeline_options import WorkerOptions
 from apache_beam.options.pipeline_options_validator import PipelineOptionsValidator
@@ -493,6 +495,37 @@ class SetupTest(unittest.TestCase):
     self.assertIn('worker_region', errors[0])
     self.assertIn('worker_zone', errors[0])
 
+  def test_programmatically_set_experiment_passed_as_string(self):
+    runner = MockRunners.DataflowRunner()
+    options = PipelineOptions(
+        project='example.com:example',
+        temp_location='gs://foo/bar/',
+        experiments='enable_prime',
+        dataflow_service_options='use_runner_v2',
+    )
+    validator = PipelineOptionsValidator(options, runner)
+    errors = validator.validate()
+    self.assertEqual(len(errors), 2)
+    self.assertIn('experiments', errors[0])
+    self.assertIn('dataflow_service_options', errors[1])
+
+  def test_programmatically_set_experiment_passed_as_list(self):
+    runner = MockRunners.DataflowRunner()
+    options = PipelineOptions(
+        project='example.com:example',
+        temp_location='gs://foo/bar/',
+        experiments=['enable_prime'],
+        dataflow_service_options=['use_runner_v2'],
+    )
+    validator = PipelineOptionsValidator(options, runner)
+    errors = validator.validate()
+    self.assertEqual(len(errors), 0)
+    self.assertEqual(
+        options.view_as(DebugOptions).experiments, ['enable_prime'])
+    self.assertEqual(
+        options.view_as(GoogleCloudOptions).dataflow_service_options,
+        ['use_runner_v2'])
+
   def test_worker_region_and_worker_zone_mutually_exclusive(self):
     runner = MockRunners.DataflowRunner()
     options = PipelineOptions([
