diff --git a/sdks/java/extensions/sql/build.gradle b/sdks/java/extensions/sql/build.gradle
index 5db33229708..e17f94365bc 100644
--- a/sdks/java/extensions/sql/build.gradle
+++ b/sdks/java/extensions/sql/build.gradle
@@ -177,16 +177,6 @@ task runPojoExample(type: JavaExec) {
   args = ["--runner=DirectRunner"]
 }
 
-// These tests become flaky when run in parallel at more than 3 threads so run them in a separate task
-task runKafkaTableProviderIT(type: Test) {
-  outputs.upToDateWhen { false }
-  include '**/KafkaTableProvider*IT.class'
-  maxParallelForks 2
-  classpath = project(":sdks:java:extensions:sql").sourceSets.test.runtimeClasspath
-  testClassesDirs = files(project(":sdks:java:extensions:sql").sourceSets.test.output.classesDirs)
-  useJUnit { }
-}
-
 task integrationTest(type: Test) {
   def gcpProject = project.findProperty('gcpProject') ?: 'apache-beam-testing'
   def gcsTempRoot = project.findProperty('gcsTempRoot') ?: 'gs://temp-storage-for-end-to-end-tests/'
@@ -202,7 +192,6 @@ task integrationTest(type: Test) {
   systemProperty "beamTestPipelineOptions", JsonOutput.toJson(pipelineOptions)
 
   include '**/*IT.class'
-  exclude '**/KafkaTableProvider*IT.class'
 
   maxParallelForks 4
   classpath = project(":sdks:java:extensions:sql")
@@ -216,7 +205,6 @@ task integrationTest(type: Test) {
 task postCommit {
   group = "Verification"
   description = "Various integration tests"
-  dependsOn runKafkaTableProviderIT
   dependsOn integrationTest
 }
 
diff --git a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/KafkaTableProviderAvroIT.java b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/KafkaTableProviderAvroIT.java
deleted file mode 100644
index 22c834ec0ce..00000000000
--- a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/KafkaTableProviderAvroIT.java
+++ /dev/null
@@ -1,39 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.beam.sdk.extensions.sql.meta.provider.kafka;
-
-import org.apache.beam.sdk.schemas.utils.AvroUtils;
-import org.apache.beam.sdk.transforms.SimpleFunction;
-import org.apache.beam.sdk.values.Row;
-import org.apache.kafka.clients.producer.ProducerRecord;
-
-public class KafkaTableProviderAvroIT extends KafkaTableProviderIT {
-  private final SimpleFunction<Row, byte[]> toBytesFn =
-      AvroUtils.getRowToAvroBytesFunction(TEST_TABLE_SCHEMA);
-
-  @Override
-  protected ProducerRecord<String, byte[]> generateProducerRecord(int i) {
-    return new ProducerRecord<>(
-        kafkaOptions.getKafkaTopic(), "k" + i, toBytesFn.apply(generateRow(i)));
-  }
-
-  @Override
-  protected String getPayloadFormat() {
-    return "avro";
-  }
-}
diff --git a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/KafkaTableProviderCSVIT.java b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/KafkaTableProviderCSVIT.java
deleted file mode 100644
index 935ac833442..00000000000
--- a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/KafkaTableProviderCSVIT.java
+++ /dev/null
@@ -1,42 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.beam.sdk.extensions.sql.meta.provider.kafka;
-
-import static java.nio.charset.StandardCharsets.UTF_8;
-import static org.apache.beam.sdk.extensions.sql.impl.schema.BeamTableUtils.beamRow2CsvLine;
-
-import org.apache.commons.csv.CSVFormat;
-import org.apache.kafka.clients.producer.ProducerRecord;
-
-@SuppressWarnings({
-  "nullness" // TODO(https://issues.apache.org/jira/browse/BEAM-10402)
-})
-public class KafkaTableProviderCSVIT extends KafkaTableProviderIT {
-  @Override
-  protected ProducerRecord<String, byte[]> generateProducerRecord(int i) {
-    return new ProducerRecord<>(
-        kafkaOptions.getKafkaTopic(),
-        "k" + i,
-        beamRow2CsvLine(generateRow(i), CSVFormat.DEFAULT).getBytes(UTF_8));
-  }
-
-  @Override
-  protected String getPayloadFormat() {
-    return null;
-  }
-}
diff --git a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/KafkaTableProviderIT.java b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/KafkaTableProviderIT.java
index 1a38911e517..9fab3a868af 100644
--- a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/KafkaTableProviderIT.java
+++ b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/KafkaTableProviderIT.java
@@ -17,7 +17,13 @@
  */
 package org.apache.beam.sdk.extensions.sql.meta.provider.kafka;
 
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.apache.beam.sdk.extensions.sql.impl.schema.BeamTableUtils.beamRow2CsvLine;
+
 import com.alibaba.fastjson.JSON;
+import java.io.Serializable;
+import java.util.Arrays;
+import java.util.Collection;
 import java.util.Map;
 import java.util.Properties;
 import java.util.Set;
@@ -30,6 +36,7 @@ import org.apache.beam.runners.direct.DirectOptions;
 import org.apache.beam.sdk.coders.KvCoder;
 import org.apache.beam.sdk.coders.RowCoder;
 import org.apache.beam.sdk.coders.StringUtf8Coder;
+import org.apache.beam.sdk.extensions.protobuf.ProtoMessageSchema;
 import org.apache.beam.sdk.extensions.sql.impl.BeamSqlEnv;
 import org.apache.beam.sdk.extensions.sql.impl.rel.BeamSqlRelUtils;
 import org.apache.beam.sdk.extensions.sql.meta.Table;
@@ -39,6 +46,7 @@ import org.apache.beam.sdk.options.Description;
 import org.apache.beam.sdk.options.PipelineOptions;
 import org.apache.beam.sdk.options.Validation;
 import org.apache.beam.sdk.schemas.Schema;
+import org.apache.beam.sdk.schemas.utils.AvroUtils;
 import org.apache.beam.sdk.state.BagState;
 import org.apache.beam.sdk.state.StateSpec;
 import org.apache.beam.sdk.state.StateSpecs;
@@ -53,49 +61,68 @@ import org.apache.beam.sdk.values.PCollection;
 import org.apache.beam.sdk.values.Row;
 import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.MoreObjects;
 import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableSet;
+import org.apache.commons.csv.CSVFormat;
 import org.apache.kafka.clients.producer.KafkaProducer;
 import org.apache.kafka.clients.producer.Producer;
 import org.apache.kafka.clients.producer.ProducerRecord;
 import org.checkerframework.checker.nullness.qual.Nullable;
 import org.junit.Assert;
 import org.junit.Before;
+import org.junit.ClassRule;
 import org.junit.Rule;
 import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
+import org.junit.runners.Parameterized.Parameter;
+import org.junit.runners.Parameterized.Parameters;
 import org.testcontainers.containers.KafkaContainer;
 import org.testcontainers.utility.DockerImageName;
 
 /** Integration Test utility for KafkaTableProvider implementations. */
+@RunWith(Parameterized.class)
 @SuppressWarnings({
   "nullness" // TODO(https://issues.apache.org/jira/browse/BEAM-10402)
 })
-public abstract class KafkaTableProviderIT {
+public class KafkaTableProviderIT {
   private static final String KAFKA_CONTAINER_VERSION = "5.5.2";
 
   @Rule public transient TestPipeline pipeline = TestPipeline.create();
 
-  @Rule
-  public transient KafkaContainer kafka =
+  @ClassRule
+  public static final KafkaContainer KAFKA_CONTAINER =
       new KafkaContainer(
           DockerImageName.parse("confluentinc/cp-kafka").withTag(KAFKA_CONTAINER_VERSION));
 
-  protected KafkaOptions kafkaOptions;
+  private static KafkaOptions kafkaOptions;
 
-  protected static final Schema TEST_TABLE_SCHEMA =
+  private static final Schema TEST_TABLE_SCHEMA =
       Schema.builder()
           .addInt64Field("f_long")
           .addInt32Field("f_int")
           .addStringField("f_string")
           .build();
 
-  protected abstract ProducerRecord<String, byte[]> generateProducerRecord(int i);
+  @Parameters
+  public static Collection<Object[]> data() {
+    return Arrays.asList(
+        new Object[][] {
+          {new KafkaJsonObjectProvider(), "json_topic"},
+          {new KafkaAvroObjectProvider(), "avro_topic"},
+          {new KafkaProtoObjectProvider(), "proto_topic"},
+          {new KafkaCsvObjectProvider(), "csv_topic"}
+        });
+  }
 
-  protected abstract String getPayloadFormat();
+  @Parameter public KafkaObjectProvider objectsProvider;
+
+  @Parameter(1)
+  public String topic;
 
   @Before
   public void setUp() {
     kafkaOptions = pipeline.getOptions().as(KafkaOptions.class);
-    kafkaOptions.setKafkaTopic("topic");
-    kafkaOptions.setKafkaBootstrapServerAddress(kafka.getBootstrapServers());
+    kafkaOptions.setKafkaTopic(topic);
+    kafkaOptions.setKafkaBootstrapServerAddress(KAFKA_CONTAINER.getBootstrapServers());
   }
 
   @Test
@@ -104,11 +131,11 @@ public abstract class KafkaTableProviderIT {
     Table table =
         Table.builder()
             .name("kafka_table")
-            .comment("kafka" + " table")
+            .comment("kafka table")
             .location("")
             .schema(TEST_TABLE_SCHEMA)
             .type("kafka")
-            .properties(JSON.parseObject(getKafkaPropertiesString()))
+            .properties(JSON.parseObject(objectsProvider.getKafkaPropertiesString()))
             .build();
     BeamKafkaTable kafkaTable = (BeamKafkaTable) new KafkaTableProvider().buildBeamSqlTable(table);
     produceSomeRecordsWithDelay(100, 20);
@@ -118,16 +145,6 @@ public abstract class KafkaTableProviderIT {
     Assert.assertTrue(rate2 > rate1);
   }
 
-  protected String getKafkaPropertiesString() {
-    return "{ "
-        + (getPayloadFormat() == null ? "" : "\"format\" : \"" + getPayloadFormat() + "\",")
-        + "\"bootstrap.servers\" : \""
-        + kafkaOptions.getKafkaBootstrapServerAddress()
-        + "\",\"topics\":[\""
-        + kafkaOptions.getKafkaTopic()
-        + "\"] }";
-  }
-
   static final transient Map<Long, Boolean> FLAG = new ConcurrentHashMap<>();
 
   @Test
@@ -143,7 +160,7 @@ public abstract class KafkaTableProviderIT {
                 + "TYPE 'kafka' \n"
                 + "LOCATION ''\n"
                 + "TBLPROPERTIES '%s'",
-            getKafkaPropertiesString());
+            objectsProvider.getKafkaPropertiesString());
     TableProvider tb = new KafkaTableProvider();
     BeamSqlEnv env = BeamSqlEnv.inMemory(tb);
 
@@ -162,14 +179,14 @@ public abstract class KafkaTableProviderIT {
                     ImmutableSet.of(generateRow(0), generateRow(1), generateRow(2)))));
     queryOutput.apply(logRecords(""));
     pipeline.run();
-    TimeUnit.MILLISECONDS.sleep(3000);
+    TimeUnit.SECONDS.sleep(4);
     produceSomeRecords(3);
 
     for (int i = 0; i < 200; i++) {
       if (FLAG.getOrDefault(pipeline.getOptions().getOptionsId(), false)) {
         return;
       }
-      TimeUnit.MILLISECONDS.sleep(60);
+      TimeUnit.MILLISECONDS.sleep(90);
     }
     Assert.fail();
   }
@@ -231,7 +248,7 @@ public abstract class KafkaTableProviderIT {
     }
   }
 
-  protected Row generateRow(int i) {
+  private static Row generateRow(int i) {
     return Row.withSchema(TEST_TABLE_SCHEMA).addValues((long) i, i % 3 + 1, "value" + i).build();
   }
 
@@ -242,7 +259,7 @@ public abstract class KafkaTableProviderIT {
         .limit(num)
         .forEach(
             i -> {
-              ProducerRecord<String, byte[]> record = generateProducerRecord(i);
+              ProducerRecord<String, byte[]> record = objectsProvider.generateProducerRecord(i);
               producer.send(record);
             });
     producer.flush();
@@ -256,7 +273,7 @@ public abstract class KafkaTableProviderIT {
         .limit(num)
         .forEach(
             i -> {
-              ProducerRecord<String, byte[]> record = generateProducerRecord(i);
+              ProducerRecord<String, byte[]> record = objectsProvider.generateProducerRecord(i);
               producer.send(record);
               try {
                 TimeUnit.MILLISECONDS.sleep(delayMilis);
@@ -281,6 +298,104 @@ public abstract class KafkaTableProviderIT {
     return props;
   }
 
+  private abstract static class KafkaObjectProvider implements Serializable {
+
+    protected abstract ProducerRecord<String, byte[]> generateProducerRecord(int i);
+
+    protected abstract String getPayloadFormat();
+
+    protected String getKafkaPropertiesString() {
+      return "{ "
+          + (getPayloadFormat() == null ? "" : "\"format\" : \"" + getPayloadFormat() + "\",")
+          + "\"bootstrap.servers\" : \""
+          + kafkaOptions.getKafkaBootstrapServerAddress()
+          + "\",\"topics\":[\""
+          + kafkaOptions.getKafkaTopic()
+          + "\"] }";
+    }
+  }
+
+  private static class KafkaJsonObjectProvider extends KafkaObjectProvider {
+    @Override
+    protected ProducerRecord<String, byte[]> generateProducerRecord(int i) {
+      return new ProducerRecord<>(
+          kafkaOptions.getKafkaTopic(), "k" + i, createJson(i).getBytes(UTF_8));
+    }
+
+    @Override
+    protected String getPayloadFormat() {
+      return "json";
+    }
+
+    private String createJson(int i) {
+      return String.format(
+          "{\"f_long\": %s, \"f_int\": %s, \"f_string\": \"%s\"}", i, i % 3 + 1, "value" + i);
+    }
+  }
+
+  private static class KafkaProtoObjectProvider extends KafkaObjectProvider {
+    private final SimpleFunction<Row, byte[]> toBytesFn =
+        ProtoMessageSchema.getRowToProtoBytesFn(KafkaMessages.ItMessage.class);
+
+    @Override
+    protected ProducerRecord<String, byte[]> generateProducerRecord(int i) {
+      return new ProducerRecord<>(
+          kafkaOptions.getKafkaTopic(), "k" + i, toBytesFn.apply(generateRow(i)));
+    }
+
+    @Override
+    protected String getPayloadFormat() {
+      return "proto";
+    }
+
+    @Override
+    protected String getKafkaPropertiesString() {
+      return "{ "
+          + "\"format\" : \"proto\","
+          + "\"bootstrap.servers\" : \""
+          + kafkaOptions.getKafkaBootstrapServerAddress()
+          + "\",\"topics\":[\""
+          + kafkaOptions.getKafkaTopic()
+          + "\"],"
+          + "\"protoClass\": \""
+          + KafkaMessages.ItMessage.class.getName()
+          + "\"}";
+    }
+  }
+
+  private static class KafkaCsvObjectProvider extends KafkaObjectProvider {
+
+    @Override
+    protected ProducerRecord<String, byte[]> generateProducerRecord(int i) {
+      return new ProducerRecord<>(
+          kafkaOptions.getKafkaTopic(),
+          "k" + i,
+          beamRow2CsvLine(generateRow(i), CSVFormat.DEFAULT).getBytes(UTF_8));
+    }
+
+    @Override
+    protected String getPayloadFormat() {
+      return null;
+    }
+  }
+
+  private static class KafkaAvroObjectProvider extends KafkaObjectProvider {
+
+    private final SimpleFunction<Row, byte[]> toBytesFn =
+        AvroUtils.getRowToAvroBytesFunction(TEST_TABLE_SCHEMA);
+
+    @Override
+    protected ProducerRecord<String, byte[]> generateProducerRecord(int i) {
+      return new ProducerRecord<>(
+          kafkaOptions.getKafkaTopic(), "k" + i, toBytesFn.apply(generateRow(i)));
+    }
+
+    @Override
+    protected String getPayloadFormat() {
+      return "avro";
+    }
+  }
+
   /** Pipeline options specific for this test. */
   public interface KafkaOptions extends PipelineOptions {
 
diff --git a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/KafkaTableProviderJsonIT.java b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/KafkaTableProviderJsonIT.java
deleted file mode 100644
index 735fabf80fc..00000000000
--- a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/KafkaTableProviderJsonIT.java
+++ /dev/null
@@ -1,40 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.beam.sdk.extensions.sql.meta.provider.kafka;
-
-import static java.nio.charset.StandardCharsets.UTF_8;
-
-import org.apache.kafka.clients.producer.ProducerRecord;
-
-public class KafkaTableProviderJsonIT extends KafkaTableProviderIT {
-  @Override
-  protected ProducerRecord<String, byte[]> generateProducerRecord(int i) {
-    return new ProducerRecord<>(
-        kafkaOptions.getKafkaTopic(), "k" + i, createJson(i).getBytes(UTF_8));
-  }
-
-  @Override
-  protected String getPayloadFormat() {
-    return "json";
-  }
-
-  private String createJson(int i) {
-    return String.format(
-        "{\"f_long\": %s, \"f_int\": %s, \"f_string\": \"%s\"}", i, i % 3 + 1, "value" + i);
-  }
-}
diff --git a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/KafkaTableProviderProtoIT.java b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/KafkaTableProviderProtoIT.java
deleted file mode 100644
index a6622e5923d..00000000000
--- a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/KafkaTableProviderProtoIT.java
+++ /dev/null
@@ -1,53 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.beam.sdk.extensions.sql.meta.provider.kafka;
-
-import org.apache.beam.sdk.extensions.protobuf.ProtoMessageSchema;
-import org.apache.beam.sdk.transforms.SimpleFunction;
-import org.apache.beam.sdk.values.Row;
-import org.apache.kafka.clients.producer.ProducerRecord;
-
-public class KafkaTableProviderProtoIT extends KafkaTableProviderIT {
-  private final SimpleFunction<Row, byte[]> toBytesFn =
-      ProtoMessageSchema.getRowToProtoBytesFn(KafkaMessages.ItMessage.class);
-
-  @Override
-  protected ProducerRecord<String, byte[]> generateProducerRecord(int i) {
-    return new ProducerRecord<>(
-        kafkaOptions.getKafkaTopic(), "k" + i, toBytesFn.apply(generateRow(i)));
-  }
-
-  @Override
-  protected String getPayloadFormat() {
-    return "proto";
-  }
-
-  @Override
-  protected String getKafkaPropertiesString() {
-    return "{ "
-        + "\"format\" : \"proto\","
-        + "\"bootstrap.servers\" : \""
-        + kafkaOptions.getKafkaBootstrapServerAddress()
-        + "\",\"topics\":[\""
-        + kafkaOptions.getKafkaTopic()
-        + "\"],"
-        + "\"protoClass\": \""
-        + KafkaMessages.ItMessage.class.getName()
-        + "\"}";
-  }
-}
diff --git a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/pubsub/PubsubAvroIT.java b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/pubsub/PubsubAvroIT.java
deleted file mode 100644
index 9b9ed68c612..00000000000
--- a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/pubsub/PubsubAvroIT.java
+++ /dev/null
@@ -1,102 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.beam.sdk.extensions.sql.meta.provider.pubsub;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.hasProperty;
-
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.util.List;
-import org.apache.avro.generic.GenericRecord;
-import org.apache.avro.generic.GenericRecordBuilder;
-import org.apache.beam.sdk.coders.AvroCoder;
-import org.apache.beam.sdk.io.gcp.pubsub.PubsubMessage;
-import org.apache.beam.sdk.schemas.Schema;
-import org.apache.beam.sdk.schemas.utils.AvroUtils;
-import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;
-import org.hamcrest.Matcher;
-import org.joda.time.Instant;
-import org.junit.runner.RunWith;
-import org.junit.runners.JUnit4;
-
-/** Integration tests for querying Pubsub AVRO messages with SQL. */
-@RunWith(JUnit4.class)
-public class PubsubAvroIT extends PubsubTableProviderIT {
-  private static final Schema NAME_HEIGHT_KNOWS_JS_SCHEMA =
-      Schema.builder()
-          .addNullableField("name", Schema.FieldType.STRING)
-          .addNullableField("height", Schema.FieldType.INT32)
-          .addNullableField("knowsJavascript", Schema.FieldType.BOOLEAN)
-          .build();
-
-  private static final Schema NAME_HEIGHT_SCHEMA =
-      Schema.builder()
-          .addNullableField("name", Schema.FieldType.STRING)
-          .addNullableField("height", Schema.FieldType.INT32)
-          .build();
-
-  @Override
-  protected String getPayloadFormat() {
-    return "avro";
-  }
-
-  @Override
-  protected PubsubMessage messageIdName(Instant timestamp, int id, String name) throws IOException {
-    byte[] encodedRecord = createEncodedGenericRecord(PAYLOAD_SCHEMA, ImmutableList.of(id, name));
-    return message(timestamp, encodedRecord);
-  }
-
-  @Override
-  protected Matcher<PubsubMessage> matcherNames(String name) throws IOException {
-    Schema schema = Schema.builder().addStringField("name").build();
-    byte[] encodedRecord = createEncodedGenericRecord(schema, ImmutableList.of(name));
-    return hasProperty("payload", equalTo(encodedRecord));
-  }
-
-  @Override
-  protected Matcher<PubsubMessage> matcherNameHeight(String name, int height) throws IOException {
-    byte[] encodedRecord =
-        createEncodedGenericRecord(NAME_HEIGHT_SCHEMA, ImmutableList.of(name, height));
-    return hasProperty("payload", equalTo(encodedRecord));
-  }
-
-  @Override
-  protected Matcher<PubsubMessage> matcherNameHeightKnowsJS(
-      String name, int height, boolean knowsJS) throws IOException {
-    byte[] encodedRecord =
-        createEncodedGenericRecord(
-            NAME_HEIGHT_KNOWS_JS_SCHEMA, ImmutableList.of(name, height, knowsJS));
-    return hasProperty("payload", equalTo(encodedRecord));
-  }
-
-  private byte[] createEncodedGenericRecord(Schema beamSchema, List<Object> values)
-      throws IOException {
-    org.apache.avro.Schema avroSchema = AvroUtils.toAvroSchema(beamSchema);
-    GenericRecordBuilder builder = new GenericRecordBuilder(avroSchema);
-    List<org.apache.avro.Schema.Field> fields = avroSchema.getFields();
-    for (int i = 0; i < fields.size(); ++i) {
-      builder.set(fields.get(i), values.get(i));
-    }
-    AvroCoder<GenericRecord> coder = AvroCoder.of(avroSchema);
-    ByteArrayOutputStream out = new ByteArrayOutputStream();
-
-    coder.encode(builder.build(), out);
-    return out.toByteArray();
-  }
-}
diff --git a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/pubsub/PubsubJsonIT.java b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/pubsub/PubsubJsonIT.java
deleted file mode 100644
index 8db903642ce..00000000000
--- a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/pubsub/PubsubJsonIT.java
+++ /dev/null
@@ -1,76 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.beam.sdk.extensions.sql.meta.provider.pubsub;
-
-import static java.nio.charset.StandardCharsets.UTF_8;
-import static org.apache.beam.sdk.testing.JsonMatcher.jsonBytesLike;
-import static org.hamcrest.Matchers.hasProperty;
-
-import java.io.IOException;
-import org.apache.beam.sdk.io.gcp.pubsub.PubsubMessage;
-import org.hamcrest.Matcher;
-import org.joda.time.Instant;
-import org.junit.runner.RunWith;
-import org.junit.runners.JUnit4;
-
-/** Integration tests for querying Pubsub JSON messages with SQL. */
-@RunWith(JUnit4.class)
-@SuppressWarnings({"nullness", "keyfor"}) // TODO(https://issues.apache.org/jira/browse/BEAM-10402)
-public class PubsubJsonIT extends PubsubTableProviderIT {
-
-  // Pubsub table provider should default to json
-  @Override
-  protected String getPayloadFormat() {
-    return null;
-  }
-
-  @Override
-  protected PubsubMessage messageIdName(Instant timestamp, int id, String name) {
-    String jsonString = "{ \"id\" : " + id + ", \"name\" : \"" + name + "\" }";
-    return message(timestamp, jsonString);
-  }
-
-  @Override
-  protected Matcher<PubsubMessage> matcherNames(String name) throws IOException {
-    return hasProperty("payload", toJsonByteLike(String.format("{\"name\":\"%s\"}", name)));
-  }
-
-  @Override
-  protected Matcher<PubsubMessage> matcherNameHeightKnowsJS(
-      String name, int height, boolean knowsJS) throws IOException {
-    String jsonString =
-        String.format(
-            "{\"name\":\"%s\", \"height\": %s, \"knowsJavascript\": %s}", name, height, knowsJS);
-
-    return hasProperty("payload", toJsonByteLike(jsonString));
-  }
-
-  @Override
-  protected Matcher<PubsubMessage> matcherNameHeight(String name, int height) throws IOException {
-    String jsonString = String.format("{\"name\":\"%s\", \"height\": %s}", name, height);
-    return hasProperty("payload", toJsonByteLike(jsonString));
-  }
-
-  private PubsubMessage message(Instant timestamp, String jsonPayload) {
-    return message(timestamp, jsonPayload.getBytes(UTF_8));
-  }
-
-  private Matcher<byte[]> toJsonByteLike(String jsonString) throws IOException {
-    return jsonBytesLike(jsonString);
-  }
-}
diff --git a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/pubsub/PubsubTableProviderIT.java b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/pubsub/PubsubTableProviderIT.java
index ab788098042..4148b040543 100644
--- a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/pubsub/PubsubTableProviderIT.java
+++ b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/pubsub/PubsubTableProviderIT.java
@@ -17,6 +17,8 @@
  */
 package org.apache.beam.sdk.extensions.sql.meta.provider.pubsub;
 
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.apache.beam.sdk.testing.JsonMatcher.jsonBytesLike;
 import static org.hamcrest.MatcherAssert.assertThat;
 import static org.hamcrest.Matchers.allOf;
 import static org.hamcrest.Matchers.equalTo;
@@ -25,11 +27,14 @@ import static org.hamcrest.Matchers.hasProperty;
 
 import com.fasterxml.jackson.core.JsonProcessingException;
 import com.fasterxml.jackson.databind.ObjectMapper;
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
 import java.io.Serializable;
 import java.nio.charset.StandardCharsets;
 import java.sql.ResultSet;
-import java.sql.SQLException;
 import java.sql.Statement;
+import java.util.Arrays;
+import java.util.Collection;
 import java.util.List;
 import java.util.Map;
 import java.util.concurrent.Callable;
@@ -38,6 +43,9 @@ import java.util.concurrent.Executors;
 import java.util.concurrent.Future;
 import java.util.concurrent.TimeUnit;
 import java.util.stream.Collectors;
+import org.apache.avro.generic.GenericRecord;
+import org.apache.avro.generic.GenericRecordBuilder;
+import org.apache.beam.sdk.coders.AvroCoder;
 import org.apache.beam.sdk.extensions.gcp.options.GcpOptions;
 import org.apache.beam.sdk.extensions.sql.impl.BeamSqlEnv;
 import org.apache.beam.sdk.extensions.sql.impl.JdbcConnection;
@@ -52,6 +60,7 @@ import org.apache.beam.sdk.io.gcp.pubsub.TestPubsubSignal;
 import org.apache.beam.sdk.options.PipelineOptions;
 import org.apache.beam.sdk.schemas.Schema;
 import org.apache.beam.sdk.schemas.SchemaCoder;
+import org.apache.beam.sdk.schemas.utils.AvroUtils;
 import org.apache.beam.sdk.testing.TestPipeline;
 import org.apache.beam.sdk.util.common.ReflectHelpers;
 import org.apache.beam.sdk.values.PCollection;
@@ -66,16 +75,18 @@ import org.joda.time.Instant;
 import org.junit.Rule;
 import org.junit.Test;
 import org.junit.runner.RunWith;
-import org.junit.runners.JUnit4;
+import org.junit.runners.Parameterized;
+import org.junit.runners.Parameterized.Parameter;
+import org.junit.runners.Parameterized.Parameters;
 
-@RunWith(JUnit4.class)
+@RunWith(Parameterized.class)
 @SuppressWarnings({
   "keyfor",
   "nullness" // TODO(https://issues.apache.org/jira/browse/BEAM-10402)
 })
-public abstract class PubsubTableProviderIT implements Serializable {
+public class PubsubTableProviderIT implements Serializable {
 
-  protected static final Schema PAYLOAD_SCHEMA =
+  private static final Schema PAYLOAD_SCHEMA =
       Schema.builder()
           .addNullableField("id", Schema.FieldType.INT32)
           .addNullableField("name", Schema.FieldType.STRING)
@@ -88,14 +99,20 @@ public abstract class PubsubTableProviderIT implements Serializable {
   @Rule public transient TestPipeline pipeline = TestPipeline.create();
   @Rule public transient TestPipeline filterPipeline = TestPipeline.create();
   private final SchemaIOTableProviderWrapper tableProvider = new PubsubTableProvider();
-  private final String payloadFormatParam =
-      getPayloadFormat() == null ? "" : String.format("\"format\" : \"%s\", ", getPayloadFormat());
+
+  @Parameters
+  public static Collection<Object[]> data() {
+    return Arrays.asList(
+        new Object[][] {{new PubsubJsonObjectProvider()}, {new PubsubAvroObjectProvider()}});
+  }
+
+  @Parameter public PubsubObjectProvider objectsProvider;
 
   /**
    * HACK: we need an objectmapper to turn pipelineoptions back into a map. We need to use
    * ReflectHelpers to get the extra PipelineOptions.
    */
-  protected static final ObjectMapper MAPPER =
+  private static final ObjectMapper MAPPER =
       new ObjectMapper()
           .registerModules(ObjectMapper.findModules(ReflectHelpers.findClassLoader()));
 
@@ -116,16 +133,16 @@ public abstract class PubsubTableProviderIT implements Serializable {
                 + "TBLPROPERTIES '{ "
                 + "%s"
                 + "\"timestampAttributeKey\" : \"ts\" }'",
-            tableProvider.getTableType(), eventsTopic.topicPath(), payloadFormatParam);
+            tableProvider.getTableType(), eventsTopic.topicPath(), payloadFormatParam());
 
     String queryString = "SELECT message.payload.id, message.payload.name from message";
 
     // Prepare messages to send later
     List<PubsubMessage> messages =
         ImmutableList.of(
-            messageIdName(ts(1), 3, "foo"),
-            messageIdName(ts(2), 5, "bar"),
-            messageIdName(ts(3), 7, "baz"));
+            objectsProvider.messageIdName(ts(1), 3, "foo"),
+            objectsProvider.messageIdName(ts(2), 5, "bar"),
+            objectsProvider.messageIdName(ts(3), 7, "baz"));
 
     // Initialize SQL environment and create the pubsub table
     BeamSqlEnv sqlEnv = BeamSqlEnv.inMemory(new PubsubTableProvider());
@@ -161,6 +178,7 @@ public abstract class PubsubTableProviderIT implements Serializable {
   }
 
   @Test
+  @SuppressWarnings("unchecked")
   public void testUsesDlq() throws Exception {
     String createTableString =
         String.format(
@@ -182,7 +200,7 @@ public abstract class PubsubTableProviderIT implements Serializable {
                 + "     }'",
             tableProvider.getTableType(),
             eventsTopic.topicPath(),
-            payloadFormatParam,
+            payloadFormatParam(),
             dlqTopic.topicPath());
 
     String queryString = "SELECT message.payload.id, message.payload.name from message";
@@ -190,9 +208,9 @@ public abstract class PubsubTableProviderIT implements Serializable {
     // Prepare messages to send later
     List<PubsubMessage> messages =
         ImmutableList.of(
-            messageIdName(ts(1), 3, "foo"),
-            messageIdName(ts(2), 5, "bar"),
-            messageIdName(ts(3), 7, "baz"),
+            objectsProvider.messageIdName(ts(1), 3, "foo"),
+            objectsProvider.messageIdName(ts(2), 5, "bar"),
+            objectsProvider.messageIdName(ts(3), 7, "baz"),
             messagePayload(ts(4), "{ - }"), // invalid message, will go to DLQ
             messagePayload(ts(5), "{ + }")); // invalid message, will go to DLQ
 
@@ -230,10 +248,11 @@ public abstract class PubsubTableProviderIT implements Serializable {
     dlqTopic
         .assertThatTopicEventuallyReceives(
             matcherPayload(ts(4), "{ - }"), matcherPayload(ts(5), "{ + }"))
-        .waitForUpTo(Duration.standardSeconds(20));
+        .waitForUpTo(Duration.standardSeconds(40));
   }
 
   @Test
+  @SuppressWarnings({"unchecked", "rawtypes"})
   public void testSQLLimit() throws Exception {
     String createTableString =
         String.format(
@@ -255,18 +274,18 @@ public abstract class PubsubTableProviderIT implements Serializable {
                 + "     }'",
             tableProvider.getTableType(),
             eventsTopic.topicPath(),
-            payloadFormatParam,
+            payloadFormatParam(),
             dlqTopic.topicPath());
 
     List<PubsubMessage> messages =
         ImmutableList.of(
-            messageIdName(ts(1), 3, "foo"),
-            messageIdName(ts(2), 5, "bar"),
-            messageIdName(ts(3), 7, "baz"),
-            messageIdName(ts(4), 9, "ba2"),
-            messageIdName(ts(5), 10, "ba3"),
-            messageIdName(ts(6), 13, "ba4"),
-            messageIdName(ts(7), 15, "ba5"));
+            objectsProvider.messageIdName(ts(1), 3, "foo"),
+            objectsProvider.messageIdName(ts(2), 5, "bar"),
+            objectsProvider.messageIdName(ts(3), 7, "baz"),
+            objectsProvider.messageIdName(ts(4), 9, "ba2"),
+            objectsProvider.messageIdName(ts(5), 10, "ba3"),
+            objectsProvider.messageIdName(ts(6), 13, "ba4"),
+            objectsProvider.messageIdName(ts(7), 15, "ba5"));
 
     // We need the default options on the schema to include the project passed in for the
     // integration test
@@ -316,16 +335,16 @@ public abstract class PubsubTableProviderIT implements Serializable {
                 + "       %s"
                 + "       \"timestampAttributeKey\" : \"ts\" "
                 + "     }'",
-            tableProvider.getTableType(), eventsTopic.topicPath(), payloadFormatParam);
+            tableProvider.getTableType(), eventsTopic.topicPath(), payloadFormatParam());
 
     String queryString = "SELECT message.id, message.name from message";
 
     // Prepare messages to send later
     List<PubsubMessage> messages =
         ImmutableList.of(
-            messageIdName(ts(1), 3, "foo"),
-            messageIdName(ts(2), 5, "bar"),
-            messageIdName(ts(3), 7, "baz"));
+            objectsProvider.messageIdName(ts(1), 3, "foo"),
+            objectsProvider.messageIdName(ts(2), 5, "bar"),
+            objectsProvider.messageIdName(ts(3), 7, "baz"));
 
     // Initialize SQL environment and create the pubsub table
     BeamSqlEnv sqlEnv = BeamSqlEnv.inMemory(new PubsubTableProvider());
@@ -361,6 +380,7 @@ public abstract class PubsubTableProviderIT implements Serializable {
   }
 
   @Test
+  @SuppressWarnings("unchecked")
   public void testSQLInsertRowsToPubsubFlat() throws Exception {
     String createTableString =
         String.format(
@@ -379,7 +399,7 @@ public abstract class PubsubTableProviderIT implements Serializable {
                 + "     }'",
             tableProvider.getTableType(),
             eventsTopic.topicPath(),
-            payloadFormatParam,
+            payloadFormatParam(),
             dlqTopic.topicPath());
 
     // Initialize SQL environment and create the pubsub table
@@ -395,18 +415,19 @@ public abstract class PubsubTableProviderIT implements Serializable {
             + "('person2', 70, FALSE)";
 
     // Apply the PTransform to insert the rows
-    PCollection<Row> queryOutput = query(sqlEnv, pipeline, queryString);
+    query(sqlEnv, pipeline, queryString);
 
     pipeline.run().waitUntilFinish(Duration.standardMinutes(5));
 
     eventsTopic
         .assertThatTopicEventuallyReceives(
-            matcherNameHeightKnowsJS("person1", 80, true),
-            matcherNameHeightKnowsJS("person2", 70, false))
-        .waitForUpTo(Duration.standardSeconds(20));
+            objectsProvider.matcherNameHeightKnowsJS("person1", 80, true),
+            objectsProvider.matcherNameHeightKnowsJS("person2", 70, false))
+        .waitForUpTo(Duration.standardSeconds(40));
   }
 
   @Test
+  @SuppressWarnings("unchecked")
   public void testSQLInsertRowsToPubsubWithTimestampAttributeFlat() throws Exception {
     String createTableString =
         String.format(
@@ -426,7 +447,7 @@ public abstract class PubsubTableProviderIT implements Serializable {
                 + "   }'",
             tableProvider.getTableType(),
             eventsTopic.topicPath(),
-            payloadFormatParam,
+            payloadFormatParam(),
             dlqTopic.topicPath());
 
     // Initialize SQL environment and create the pubsub table
@@ -438,7 +459,7 @@ public abstract class PubsubTableProviderIT implements Serializable {
             + "VALUES "
             + "(TIMESTAMP '1970-01-01 00:00:00.001', 'person1', 80, TRUE), "
             + "(TIMESTAMP '1970-01-01 00:00:00.002', 'person2', 70, FALSE)";
-    PCollection<Row> queryOutput = query(sqlEnv, pipeline, queryString);
+    query(sqlEnv, pipeline, queryString);
 
     pipeline.run().waitUntilFinish(Duration.standardMinutes(5));
 
@@ -446,10 +467,11 @@ public abstract class PubsubTableProviderIT implements Serializable {
         .assertThatTopicEventuallyReceives(
             matcherTsNameHeightKnowsJS(ts(1), "person1", 80, true),
             matcherTsNameHeightKnowsJS(ts(2), "person2", 70, false))
-        .waitForUpTo(Duration.standardSeconds(20));
+        .waitForUpTo(Duration.standardSeconds(40));
   }
 
   @Test
+  @SuppressWarnings("unchecked")
   public void testSQLReadAndWriteWithSameFlatTableDefinition() throws Exception {
     // This test verifies that the same pubsub table definition can be used for both reading and
     // writing
@@ -458,9 +480,10 @@ public abstract class PubsubTableProviderIT implements Serializable {
     // `javascript_people`
 
     String tblProperties =
-        getPayloadFormat() == null
+        objectsProvider.getPayloadFormat() == null
             ? ""
-            : String.format("TBLPROPERTIES '{\"format\": \"%s\"}'", getPayloadFormat());
+            : String.format(
+                "TBLPROPERTIES '{\"format\": \"%s\"}'", objectsProvider.getPayloadFormat());
     String createTableString =
         String.format(
             "CREATE EXTERNAL TABLE people (\n"
@@ -529,14 +552,14 @@ public abstract class PubsubTableProviderIT implements Serializable {
 
     filteredEventsTopic
         .assertThatTopicEventuallyReceives(
-            matcherNameHeight("person1", 80),
-            matcherNameHeight("person3", 60),
-            matcherNameHeight("person5", 40))
+            objectsProvider.matcherNameHeight("person1", 80),
+            objectsProvider.matcherNameHeight("person3", 60),
+            objectsProvider.matcherNameHeight("person5", 40))
         .waitForUpTo(Duration.standardMinutes(5));
   }
 
-  private CalciteConnection connect(PipelineOptions options, TableProvider... tableProviders)
-      throws SQLException {
+  @SuppressWarnings("unchecked")
+  private CalciteConnection connect(PipelineOptions options, TableProvider... tableProviders) {
     // HACK: PipelineOptions should expose a prominent method to do this reliably
     // The actual options are in the "options" field of the converted map
     Map<String, String> argsMap =
@@ -569,50 +592,180 @@ public abstract class PubsubTableProviderIT implements Serializable {
     }
   }
 
+  private String payloadFormatParam() {
+    return objectsProvider.getPayloadFormat() == null
+        ? ""
+        : String.format("\"format\" : \"%s\", ", objectsProvider.getPayloadFormat());
+  }
+
   private PCollection<Row> query(BeamSqlEnv sqlEnv, TestPipeline pipeline, String queryString) {
 
     return BeamSqlRelUtils.toPCollection(pipeline, sqlEnv.parseQuery(queryString));
   }
 
-  protected Row row(Schema schema, Object... values) {
+  private Row row(Schema schema, Object... values) {
     return Row.withSchema(schema).addValues(values).build();
   }
 
-  protected PubsubMessage message(Instant timestamp, byte[] payload) {
+  private static PubsubMessage message(Instant timestamp, byte[] payload) {
     return new PubsubMessage(payload, ImmutableMap.of("ts", String.valueOf(timestamp.getMillis())));
   }
 
-  protected Matcher<PubsubMessage> matcherTsNameHeightKnowsJS(
+  private Matcher<PubsubMessage> matcherTsNameHeightKnowsJS(
       Instant ts, String name, int height, boolean knowsJS) throws Exception {
     return allOf(
-        matcherNameHeightKnowsJS(name, height, knowsJS),
+        objectsProvider.matcherNameHeightKnowsJS(name, height, knowsJS),
         hasProperty("attributeMap", hasEntry("ts", String.valueOf(ts.getMillis()))));
   }
 
-  protected Matcher<PubsubMessage> matcherPayload(Instant timestamp, String payload) {
+  private Matcher<PubsubMessage> matcherPayload(Instant timestamp, String payload) {
     return allOf(
         hasProperty("payload", equalTo(payload.getBytes(StandardCharsets.US_ASCII))),
         hasProperty("attributeMap", hasEntry("ts", String.valueOf(timestamp.getMillis()))));
   }
 
-  protected Instant ts(long millis) {
+  private Instant ts(long millis) {
     return Instant.ofEpochMilli(millis);
   }
 
-  protected abstract String getPayloadFormat();
+  private PubsubMessage messagePayload(Instant timestamp, String payload) {
+    return message(timestamp, payload.getBytes(StandardCharsets.US_ASCII));
+  }
 
-  protected abstract PubsubMessage messageIdName(Instant timestamp, int id, String name)
-      throws Exception;
+  private abstract static class PubsubObjectProvider implements Serializable {
+    protected abstract String getPayloadFormat();
 
-  protected abstract Matcher<PubsubMessage> matcherNames(String name) throws Exception;
+    protected abstract PubsubMessage messageIdName(Instant timestamp, int id, String name)
+        throws Exception;
 
-  protected abstract Matcher<PubsubMessage> matcherNameHeightKnowsJS(
-      String name, int height, boolean knowsJS) throws Exception;
+    protected abstract Matcher<PubsubMessage> matcherNames(String name) throws Exception;
 
-  protected abstract Matcher<PubsubMessage> matcherNameHeight(String name, int height)
-      throws Exception;
+    protected abstract Matcher<PubsubMessage> matcherNameHeightKnowsJS(
+        String name, int height, boolean knowsJS) throws Exception;
 
-  private PubsubMessage messagePayload(Instant timestamp, String payload) {
-    return message(timestamp, payload.getBytes(StandardCharsets.US_ASCII));
+    protected abstract Matcher<PubsubMessage> matcherNameHeight(String name, int height)
+        throws Exception;
+  }
+
+  private static class PubsubJsonObjectProvider extends PubsubObjectProvider {
+
+    // Pubsub table provider should default to json
+    @Override
+    protected String getPayloadFormat() {
+      return null;
+    }
+
+    @Override
+    protected PubsubMessage messageIdName(Instant timestamp, int id, String name) {
+      String jsonString = "{ \"id\" : " + id + ", \"name\" : \"" + name + "\" }";
+      return message(timestamp, jsonString);
+    }
+
+    @Override
+    protected Matcher<PubsubMessage> matcherNames(String name) throws IOException {
+      return hasProperty("payload", toJsonByteLike(String.format("{\"name\":\"%s\"}", name)));
+    }
+
+    @Override
+    protected Matcher<PubsubMessage> matcherNameHeightKnowsJS(
+        String name, int height, boolean knowsJS) throws IOException {
+      String jsonString =
+          String.format(
+              "{\"name\":\"%s\", \"height\": %s, \"knowsJavascript\": %s}", name, height, knowsJS);
+
+      return hasProperty("payload", toJsonByteLike(jsonString));
+    }
+
+    @Override
+    protected Matcher<PubsubMessage> matcherNameHeight(String name, int height) throws IOException {
+      String jsonString = String.format("{\"name\":\"%s\", \"height\": %s}", name, height);
+      return hasProperty("payload", toJsonByteLike(jsonString));
+    }
+
+    private PubsubMessage message(Instant timestamp, String jsonPayload) {
+      return PubsubTableProviderIT.message(timestamp, jsonPayload.getBytes(UTF_8));
+    }
+
+    private Matcher<byte[]> toJsonByteLike(String jsonString) throws IOException {
+      return jsonBytesLike(jsonString);
+    }
+  }
+
+  private static class PubsubAvroObjectProvider extends PubsubObjectProvider {
+    private static final Schema NAME_HEIGHT_KNOWS_JS_SCHEMA =
+        Schema.builder()
+            .addNullableField("name", Schema.FieldType.STRING)
+            .addNullableField("height", Schema.FieldType.INT32)
+            .addNullableField("knowsJavascript", Schema.FieldType.BOOLEAN)
+            .build();
+
+    private static final Schema NAME_HEIGHT_SCHEMA =
+        Schema.builder()
+            .addNullableField("name", Schema.FieldType.STRING)
+            .addNullableField("height", Schema.FieldType.INT32)
+            .build();
+
+    @Override
+    protected String getPayloadFormat() {
+      return "avro";
+    }
+
+    @Override
+    protected PubsubMessage messageIdName(Instant timestamp, int id, String name)
+        throws IOException {
+      byte[] encodedRecord =
+          createEncodedGenericRecord(
+              PAYLOAD_SCHEMA,
+              org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList.of(
+                  id, name));
+      return message(timestamp, encodedRecord);
+    }
+
+    @Override
+    protected Matcher<PubsubMessage> matcherNames(String name) throws IOException {
+      Schema schema = Schema.builder().addStringField("name").build();
+      byte[] encodedRecord =
+          createEncodedGenericRecord(
+              schema,
+              org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList.of(
+                  name));
+      return hasProperty("payload", equalTo(encodedRecord));
+    }
+
+    @Override
+    protected Matcher<PubsubMessage> matcherNameHeight(String name, int height) throws IOException {
+      byte[] encodedRecord =
+          createEncodedGenericRecord(
+              NAME_HEIGHT_SCHEMA,
+              org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList.of(
+                  name, height));
+      return hasProperty("payload", equalTo(encodedRecord));
+    }
+
+    @Override
+    protected Matcher<PubsubMessage> matcherNameHeightKnowsJS(
+        String name, int height, boolean knowsJS) throws IOException {
+      byte[] encodedRecord =
+          createEncodedGenericRecord(
+              NAME_HEIGHT_KNOWS_JS_SCHEMA,
+              org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList.of(
+                  name, height, knowsJS));
+      return hasProperty("payload", equalTo(encodedRecord));
+    }
+
+    private byte[] createEncodedGenericRecord(Schema beamSchema, List<Object> values)
+        throws IOException {
+      org.apache.avro.Schema avroSchema = AvroUtils.toAvroSchema(beamSchema);
+      GenericRecordBuilder builder = new GenericRecordBuilder(avroSchema);
+      List<org.apache.avro.Schema.Field> fields = avroSchema.getFields();
+      for (int i = 0; i < fields.size(); ++i) {
+        builder.set(fields.get(i), values.get(i));
+      }
+      AvroCoder<GenericRecord> coder = AvroCoder.of(avroSchema);
+      ByteArrayOutputStream out = new ByteArrayOutputStream();
+
+      coder.encode(builder.build(), out);
+      return out.toByteArray();
+    }
   }
 }
