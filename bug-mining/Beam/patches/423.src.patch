diff --git a/sdks/java/io/elasticsearch-tests/elasticsearch-tests-common/src/test/java/org/apache/beam/sdk/io/elasticsearch/ElasticsearchIOTestCommon.java b/sdks/java/io/elasticsearch-tests/elasticsearch-tests-common/src/test/java/org/apache/beam/sdk/io/elasticsearch/ElasticsearchIOTestCommon.java
index 3fb08bb39c2..03eaf005da9 100644
--- a/sdks/java/io/elasticsearch-tests/elasticsearch-tests-common/src/test/java/org/apache/beam/sdk/io/elasticsearch/ElasticsearchIOTestCommon.java
+++ b/sdks/java/io/elasticsearch-tests/elasticsearch-tests-common/src/test/java/org/apache/beam/sdk/io/elasticsearch/ElasticsearchIOTestCommon.java
@@ -190,10 +190,6 @@ class ElasticsearchIOTestCommon implements Serializable {
         ElasticsearchIO.write()
             .withConnectionConfiguration(connectionConfiguration)
             .withMaxBatchSize(BATCH_SIZE);
-    // write bundles size is the runner decision, we cannot force a bundle size,
-    // so we test the Writer as a DoFn outside of a runner.
-    DoFnTester<String, Void> fnTester = DoFnTester.of(new Write.WriteFn(write));
-
     List<String> input =
         ElasticSearchIOTestUtils.createDocuments(
             numDocs, ElasticSearchIOTestUtils.InjectionMode.INJECT_SOME_INVALID_DOCS);
@@ -216,8 +212,12 @@ class ElasticsearchIOTestCommon implements Serializable {
                     + "Document id .+: failed to parse \\(.+\\).*Caused by: .+ \\(.+\\).*");
           }
         });
-    // inserts into Elasticsearch
-    fnTester.processBundle(input);
+    // write bundles size is the runner decision, we cannot force a bundle size,
+    // so we test the Writer as a DoFn outside of a runner.
+    try (DoFnTester<String, Void> fnTester = DoFnTester.of(new Write.WriteFn(write))) {
+      // inserts into Elasticsearch
+      fnTester.processBundle(input);
+    }
   }
 
   void testWriteWithMaxBatchSize() throws Exception {
@@ -227,34 +227,35 @@ class ElasticsearchIOTestCommon implements Serializable {
             .withMaxBatchSize(BATCH_SIZE);
     // write bundles size is the runner decision, we cannot force a bundle size,
     // so we test the Writer as a DoFn outside of a runner.
-    DoFnTester<String, Void> fnTester = DoFnTester.of(new Write.WriteFn(write));
-    List<String> input =
-        ElasticSearchIOTestUtils.createDocuments(
-            numDocs, ElasticSearchIOTestUtils.InjectionMode.DO_NOT_INJECT_INVALID_DOCS);
-    long numDocsProcessed = 0;
-    long numDocsInserted = 0;
-    for (String document : input) {
-      fnTester.processElement(document);
-      numDocsProcessed++;
-      // test every 100 docs to avoid overloading ES
-      if ((numDocsProcessed % 100) == 0) {
-        // force the index to upgrade after inserting for the inserted docs
-        // to be searchable immediately
-        long currentNumDocs = ElasticSearchIOTestUtils
-            .refreshIndexAndGetCurrentNumDocs(connectionConfiguration, restClient);
-        if ((numDocsProcessed % BATCH_SIZE) == 0) {
+    try (DoFnTester<String, Void> fnTester = DoFnTester.of(new Write.WriteFn(write))) {
+      List<String> input =
+          ElasticSearchIOTestUtils.createDocuments(
+              numDocs, ElasticSearchIOTestUtils.InjectionMode.DO_NOT_INJECT_INVALID_DOCS);
+      long numDocsProcessed = 0;
+      long numDocsInserted = 0;
+      for (String document : input) {
+        fnTester.processElement(document);
+        numDocsProcessed++;
+        // test every 100 docs to avoid overloading ES
+        if ((numDocsProcessed % 100) == 0) {
+          // force the index to upgrade after inserting for the inserted docs
+          // to be searchable immediately
+          long currentNumDocs = ElasticSearchIOTestUtils
+              .refreshIndexAndGetCurrentNumDocs(connectionConfiguration, restClient);
+          if ((numDocsProcessed % BATCH_SIZE) == 0) {
           /* bundle end */
-          assertEquals(
-              "we are at the end of a bundle, we should have inserted all processed documents",
-              numDocsProcessed,
-              currentNumDocs);
-          numDocsInserted = currentNumDocs;
-        } else {
+            assertEquals(
+                "we are at the end of a bundle, we should have inserted all processed documents",
+                numDocsProcessed,
+                currentNumDocs);
+            numDocsInserted = currentNumDocs;
+          } else {
           /* not bundle end */
-          assertEquals(
-              "we are not at the end of a bundle, we should have inserted no more documents",
-              numDocsInserted,
-              currentNumDocs);
+            assertEquals(
+                "we are not at the end of a bundle, we should have inserted no more documents",
+                numDocsInserted,
+                currentNumDocs);
+          }
         }
       }
     }
@@ -267,38 +268,39 @@ class ElasticsearchIOTestCommon implements Serializable {
             .withMaxBatchSizeBytes(BATCH_SIZE_BYTES);
     // write bundles size is the runner decision, we cannot force a bundle size,
     // so we test the Writer as a DoFn outside of a runner.
-    DoFnTester<String, Void> fnTester = DoFnTester.of(new Write.WriteFn(write));
-    List<String> input =
-        ElasticSearchIOTestUtils.createDocuments(
-            numDocs, ElasticSearchIOTestUtils.InjectionMode.DO_NOT_INJECT_INVALID_DOCS);
-    long numDocsProcessed = 0;
-    long sizeProcessed = 0;
-    long numDocsInserted = 0;
-    long batchInserted = 0;
-    for (String document : input) {
-      fnTester.processElement(document);
-      numDocsProcessed++;
-      sizeProcessed += document.getBytes().length;
-      // test every 40 docs to avoid overloading ES
-      if ((numDocsProcessed % 40) == 0) {
-        // force the index to upgrade after inserting for the inserted docs
-        // to be searchable immediately
-        long currentNumDocs = ElasticSearchIOTestUtils
-            .refreshIndexAndGetCurrentNumDocs(connectionConfiguration, restClient);
-        if (sizeProcessed / BATCH_SIZE_BYTES > batchInserted) {
+    try (DoFnTester<String, Void> fnTester = DoFnTester.of(new Write.WriteFn(write))) {
+      List<String> input =
+          ElasticSearchIOTestUtils.createDocuments(
+              numDocs, ElasticSearchIOTestUtils.InjectionMode.DO_NOT_INJECT_INVALID_DOCS);
+      long numDocsProcessed = 0;
+      long sizeProcessed = 0;
+      long numDocsInserted = 0;
+      long batchInserted = 0;
+      for (String document : input) {
+        fnTester.processElement(document);
+        numDocsProcessed++;
+        sizeProcessed += document.getBytes().length;
+        // test every 40 docs to avoid overloading ES
+        if ((numDocsProcessed % 40) == 0) {
+          // force the index to upgrade after inserting for the inserted docs
+          // to be searchable immediately
+          long currentNumDocs = ElasticSearchIOTestUtils
+              .refreshIndexAndGetCurrentNumDocs(connectionConfiguration, restClient);
+          if (sizeProcessed / BATCH_SIZE_BYTES > batchInserted) {
           /* bundle end */
-          assertThat(
-              "we have passed a bundle size, we should have inserted some documents",
-              currentNumDocs,
-              greaterThan(numDocsInserted));
-          numDocsInserted = currentNumDocs;
-          batchInserted = (sizeProcessed / BATCH_SIZE_BYTES);
-        } else {
+            assertThat(
+                "we have passed a bundle size, we should have inserted some documents",
+                currentNumDocs,
+                greaterThan(numDocsInserted));
+            numDocsInserted = currentNumDocs;
+            batchInserted = (sizeProcessed / BATCH_SIZE_BYTES);
+          } else {
           /* not bundle end */
-          assertEquals(
-              "we are not at the end of a bundle, we should have inserted no more documents",
-              numDocsInserted,
-              currentNumDocs);
+            assertEquals(
+                "we are not at the end of a bundle, we should have inserted no more documents",
+                numDocsInserted,
+                currentNumDocs);
+          }
         }
       }
     }
