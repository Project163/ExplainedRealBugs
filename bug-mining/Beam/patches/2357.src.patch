diff --git a/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java b/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java
index 50d58e434fa..81f597869dc 100644
--- a/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java
+++ b/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java
@@ -1154,6 +1154,8 @@ public class ParquetIO {
 
     abstract int getRowGroupSize();
 
+    abstract @Nullable Class<? extends GenericData> getAvroDataModelClass();
+
     abstract Builder toBuilder();
 
     @AutoValue.Builder
@@ -1166,6 +1168,8 @@ public class ParquetIO {
 
       abstract Builder setRowGroupSize(int rowGroupSize);
 
+      abstract Builder setAvroDataModelClass(Class<? extends GenericData> modelClass);
+
       abstract Sink build();
     }
 
@@ -1192,6 +1196,13 @@ public class ParquetIO {
       return toBuilder().setRowGroupSize(rowGroupSize).build();
     }
 
+    /**
+     * Define the Avro data model; see {@link AvroParquetWriter.Builder#withDataModel(GenericData)}.
+     */
+    public Sink withAvroDataModel(GenericData model) {
+      return toBuilder().setAvroDataModelClass(model.getClass()).build();
+    }
+
     private transient @Nullable ParquetWriter<GenericRecord> writer;
 
     @Override
@@ -1199,6 +1210,7 @@ public class ParquetIO {
       checkNotNull(getJsonSchema(), "Schema cannot be null");
 
       Schema schema = new Schema.Parser().parse(getJsonSchema());
+      Class<? extends GenericData> modelClass = getAvroDataModelClass();
 
       BeamParquetOutputFile beamParquetOutputFile =
           new BeamParquetOutputFile(Channels.newOutputStream(channel));
@@ -1210,6 +1222,14 @@ public class ParquetIO {
               .withWriteMode(OVERWRITE)
               .withConf(SerializableConfiguration.newConfiguration(getConfiguration()))
               .withRowGroupSize(getRowGroupSize());
+      if (modelClass != null) {
+        try {
+          builder.withDataModel(buildModelObject(modelClass));
+        } catch (ReflectiveOperationException e) {
+          throw new IOException(
+              "Couldn't set the specified Avro data model " + modelClass.getName(), e);
+        }
+      }
       this.writer = builder.build();
     }
 
diff --git a/sdks/java/io/parquet/src/test/java/org/apache/beam/sdk/io/parquet/ParquetIOTest.java b/sdks/java/io/parquet/src/test/java/org/apache/beam/sdk/io/parquet/ParquetIOTest.java
index 9b9d4ae6bd9..d2609b4fcd8 100644
--- a/sdks/java/io/parquet/src/test/java/org/apache/beam/sdk/io/parquet/ParquetIOTest.java
+++ b/sdks/java/io/parquet/src/test/java/org/apache/beam/sdk/io/parquet/ParquetIOTest.java
@@ -506,6 +506,53 @@ public class ParquetIOTest implements Serializable {
     readPipeline.run().waitUntilFinish();
   }
 
+  @Test
+  public void testWriteAndReadUsingGenericDataSchemaWithDataModel() {
+    Schema schema = new Schema.Parser().parse(SCHEMA_STRING);
+
+    List<GenericRecord> records = generateGenericRecords(1000);
+    mainPipeline
+        .apply(Create.of(records).withCoder(AvroCoder.of(schema)))
+        .apply(
+            FileIO.<GenericRecord>write()
+                .via(ParquetIO.sink(schema).withAvroDataModel(GenericData.get()))
+                .to(temporaryFolder.getRoot().getAbsolutePath()));
+    mainPipeline.run().waitUntilFinish();
+
+    PCollection<GenericRecord> readBack =
+        readPipeline.apply(
+            ParquetIO.read(schema)
+                .withAvroDataModel(GenericData.get())
+                .from(temporaryFolder.getRoot().getAbsolutePath() + "/*"));
+
+    PAssert.that(readBack).containsInAnyOrder(records);
+    readPipeline.run().waitUntilFinish();
+  }
+
+  @Test
+  public void testWriteAndReadwithSplitUsingGenericDataSchemaWithDataModel() {
+    Schema schema = new Schema.Parser().parse(SCHEMA_STRING);
+
+    List<GenericRecord> records = generateGenericRecords(1000);
+    mainPipeline
+        .apply(Create.of(records).withCoder(AvroCoder.of(schema)))
+        .apply(
+            FileIO.<GenericRecord>write()
+                .via(ParquetIO.sink(schema).withAvroDataModel(GenericData.get()))
+                .to(temporaryFolder.getRoot().getAbsolutePath()));
+    mainPipeline.run().waitUntilFinish();
+
+    PCollection<GenericRecord> readBack =
+        readPipeline.apply(
+            ParquetIO.read(schema)
+                .withSplit()
+                .withAvroDataModel(GenericData.get())
+                .from(temporaryFolder.getRoot().getAbsolutePath() + "/*"));
+
+    PAssert.that(readBack).containsInAnyOrder(records);
+    readPipeline.run().waitUntilFinish();
+  }
+
   @Test
   public void testWriteAndReadWithConfiguration() {
     List<GenericRecord> records = generateGenericRecords(10);
