diff --git a/sdks/python/apache_beam/runners/dataflow/dataflow_runner.py b/sdks/python/apache_beam/runners/dataflow/dataflow_runner.py
index 3a8ae47b30d..b8b66927a8e 100644
--- a/sdks/python/apache_beam/runners/dataflow/dataflow_runner.py
+++ b/sdks/python/apache_beam/runners/dataflow/dataflow_runner.py
@@ -382,12 +382,11 @@ class DataflowRunner(PipelineRunner):
 
     use_fnapi = apiclient._use_fnapi(options)
     from apache_beam.portability.api import beam_runner_api_pb2
-    default_container_image = (
-        apiclient.get_default_container_image_for_current_sdk(use_fnapi))
     default_environment = beam_runner_api_pb2.Environment(
         urn=common_urns.environments.DOCKER.urn,
         payload=beam_runner_api_pb2.DockerPayload(
-            container_image=default_container_image).SerializeToString())
+            container_image=apiclient.get_container_image_from_options(options)
+            ).SerializeToString())
 
     # Snapshot the pipeline in a portable proto.
     self.proto_pipeline, self.proto_context = pipeline.to_runner_api(
diff --git a/sdks/python/apache_beam/runners/dataflow/dataflow_runner_test.py b/sdks/python/apache_beam/runners/dataflow/dataflow_runner_test.py
index bbb283305f3..f9c80c509d3 100644
--- a/sdks/python/apache_beam/runners/dataflow/dataflow_runner_test.py
+++ b/sdks/python/apache_beam/runners/dataflow/dataflow_runner_test.py
@@ -35,6 +35,7 @@ from apache_beam.options.pipeline_options import PipelineOptions
 from apache_beam.pipeline import AppliedPTransform
 from apache_beam.pipeline import Pipeline
 from apache_beam.portability import common_urns
+from apache_beam.portability.api import beam_runner_api_pb2
 from apache_beam.pvalue import PCollection
 from apache_beam.runners import DataflowRunner
 from apache_beam.runners import TestDataflowRunner
@@ -176,6 +177,23 @@ class DataflowRunnerTest(unittest.TestCase):
         isinstance(create_runner('TestDataflowRunner'),
                    TestDataflowRunner))
 
+  def test_environment_override_translation(self):
+    self.default_properties.append('--experiments=beam_fn_api')
+    self.default_properties.append('--worker_harness_container_image=FOO')
+    remote_runner = DataflowRunner()
+    p = Pipeline(remote_runner,
+                 options=PipelineOptions(self.default_properties))
+    (p | ptransform.Create([1, 2, 3])  # pylint: disable=expression-not-assigned
+     | 'Do' >> ptransform.FlatMap(lambda x: [(x, x)])
+     | ptransform.GroupByKey())
+    p.run()
+    self.assertEqual(
+        list(remote_runner.proto_pipeline.components.environments.values()),
+        [beam_runner_api_pb2.Environment(
+            urn=common_urns.environments.DOCKER.urn,
+            payload=beam_runner_api_pb2.DockerPayload(
+                container_image='FOO').SerializeToString())])
+
   def test_remote_runner_translation(self):
     remote_runner = DataflowRunner()
     p = Pipeline(remote_runner,
diff --git a/sdks/python/apache_beam/runners/dataflow/internal/apiclient.py b/sdks/python/apache_beam/runners/dataflow/internal/apiclient.py
index 29cd37e6dd6..fb91d97f4e3 100644
--- a/sdks/python/apache_beam/runners/dataflow/internal/apiclient.py
+++ b/sdks/python/apache_beam/runners/dataflow/internal/apiclient.py
@@ -250,12 +250,8 @@ class Environment(object):
       pool.network = self.worker_options.network
     if self.worker_options.subnetwork:
       pool.subnetwork = self.worker_options.subnetwork
-    if self.worker_options.worker_harness_container_image:
-      pool.workerHarnessContainerImage = (
-          self.worker_options.worker_harness_container_image)
-    else:
-      pool.workerHarnessContainerImage = (
-          get_default_container_image_for_current_sdk(_use_fnapi(options)))
+    pool.workerHarnessContainerImage = (
+        get_container_image_from_options(options))
     if self.worker_options.use_public_ips is not None:
       if self.worker_options.use_public_ips:
         pool.ipConfiguration = (
@@ -908,15 +904,19 @@ def _get_container_image_tag():
   return base_version
 
 
-def get_default_container_image_for_current_sdk(use_fnapi):
+def get_container_image_from_options(pipeline_options):
   """For internal use only; no backwards-compatibility guarantees.
 
     Args:
-      use_fnapi (bool): True, if pipeline is using FnAPI, False otherwise.
+      pipeline_options (PipelineOptions): A container for pipeline options.
 
     Returns:
-      str: Google Cloud Dataflow container image for remote execution.
+      str: Container image for remote execution.
     """
+  worker_options = pipeline_options.view_as(WorkerOptions)
+  if worker_options.worker_harness_container_image:
+    return worker_options.worker_harness_container_image
+
   if sys.version_info[0] == 2:
     version_suffix = ''
   elif sys.version_info[0:2] == (3, 5):
@@ -929,6 +929,7 @@ def get_default_container_image_for_current_sdk(use_fnapi):
     raise Exception('Dataflow only supports Python versions 2 and 3.5+, got: %s'
                     % str(sys.version_info[0:2]))
 
+  use_fnapi = _use_fnapi(pipeline_options)
   # TODO(tvalentyn): Use enumerated type instead of strings for job types.
   if use_fnapi:
     fnapi_suffix = '-fnapi'
