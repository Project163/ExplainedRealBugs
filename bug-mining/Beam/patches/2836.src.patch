diff --git a/sdks/java/io/hadoop-format/src/main/java/org/apache/beam/sdk/io/hadoop/format/HadoopFormatIO.java b/sdks/java/io/hadoop-format/src/main/java/org/apache/beam/sdk/io/hadoop/format/HadoopFormatIO.java
index ad8aeb83a2c..3977a0f972c 100644
--- a/sdks/java/io/hadoop-format/src/main/java/org/apache/beam/sdk/io/hadoop/format/HadoopFormatIO.java
+++ b/sdks/java/io/hadoop-format/src/main/java/org/apache/beam/sdk/io/hadoop/format/HadoopFormatIO.java
@@ -969,7 +969,7 @@ public class HadoopFormatIO {
         } else {
           output = (T3) input;
         }
-        return skipClone ? output : cloneIfPossiblyMutable(output, coder);
+        return skipClone || output == null ? output : cloneIfPossiblyMutable(output, coder);
       }
 
       /**
diff --git a/sdks/java/io/hadoop-format/src/test/java/org/apache/beam/sdk/io/hadoop/format/HadoopFormatIOReadTest.java b/sdks/java/io/hadoop-format/src/test/java/org/apache/beam/sdk/io/hadoop/format/HadoopFormatIOReadTest.java
index bfad990caae..9d9dbcec90a 100644
--- a/sdks/java/io/hadoop-format/src/test/java/org/apache/beam/sdk/io/hadoop/format/HadoopFormatIOReadTest.java
+++ b/sdks/java/io/hadoop-format/src/test/java/org/apache/beam/sdk/io/hadoop/format/HadoopFormatIOReadTest.java
@@ -32,10 +32,7 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.stream.Collectors;
-import org.apache.beam.sdk.coders.AvroCoder;
-import org.apache.beam.sdk.coders.Coder;
-import org.apache.beam.sdk.coders.RowCoder;
-import org.apache.beam.sdk.coders.StringUtf8Coder;
+import org.apache.beam.sdk.coders.*;
 import org.apache.beam.sdk.io.BoundedSource;
 import org.apache.beam.sdk.io.BoundedSource.BoundedReader;
 import org.apache.beam.sdk.io.hadoop.SerializableConfiguration;
@@ -78,6 +75,8 @@ public class HadoopFormatIOReadTest {
   private static SimpleFunction<Text, String> myKeyTranslate;
   private static SimpleFunction<Employee, String> myValueTranslate;
   private static SimpleFunction<Employee, Row> myValueToRowTranslate;
+  private static SimpleFunction<Employee, Row> myValueToNullRowTranslate;
+  private static SimpleFunction<Text, Row> myKeyToNullRowTranslate;
   private static Schema myValueToRowSchema;
 
   @Rule public final transient TestPipeline p = TestPipeline.create();
@@ -114,6 +113,20 @@ public class HadoopFormatIOReadTest {
                 .build();
           }
         };
+    myValueToNullRowTranslate =
+        new SimpleFunction<Employee, Row>() {
+          @Override
+          public Row apply(Employee input) {
+            return null;
+          }
+        };
+    myKeyToNullRowTranslate =
+        new SimpleFunction<Text, Row>() {
+          @Override
+          public Row apply(Text input) {
+            return null;
+          }
+        };
   }
 
   @Test
@@ -496,6 +509,38 @@ public class HadoopFormatIOReadTest {
     p.run();
   }
 
+  @Test
+  public void testReadingDataWithNullKeys() {
+    HadoopFormatIO.Read<Row, Employee> read =
+        HadoopFormatIO.<Row, Employee>read()
+            .withConfiguration(serConf.get())
+            .withKeyTranslation(
+                myKeyToNullRowTranslate, NullableCoder.of(RowCoder.of(Schema.of())));
+    List<KV<Row, Employee>> expected =
+        TestEmployeeDataSet.getEmployeeData().stream()
+            .map(data -> KV.of((Row) null, data.getValue()))
+            .collect(Collectors.toList());
+    PCollection<KV<Row, Employee>> actual = p.apply("ReadTestWithNullKeys", read);
+    PAssert.that(actual).containsInAnyOrder(expected);
+    p.run();
+  }
+
+  @Test
+  public void testReadingDataWithNullValues() {
+    HadoopFormatIO.Read<Text, Row> read =
+        HadoopFormatIO.<Text, Row>read()
+            .withConfiguration(serConf.get())
+            .withValueTranslation(
+                myValueToNullRowTranslate, NullableCoder.of(RowCoder.of(Schema.of())));
+    List<KV<Text, Row>> expected =
+        TestEmployeeDataSet.getEmployeeData().stream()
+            .map(data -> KV.of(data.getKey(), (Row) null))
+            .collect(Collectors.toList());
+    PCollection<KV<Text, Row>> actual = p.apply("ReadTestWithNullValues", read);
+    PAssert.that(actual).containsInAnyOrder(expected);
+    p.run();
+  }
+
   @Test
   public void testReadingDataWithCoder() {
     HadoopFormatIO.Read<Text, Row> read =
