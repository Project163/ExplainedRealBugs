diff --git a/sdks/java/io/jdbc/src/main/java/org/apache/beam/sdk/io/jdbc/JdbcIO.java b/sdks/java/io/jdbc/src/main/java/org/apache/beam/sdk/io/jdbc/JdbcIO.java
index 95abea332c3..d009f47cfd3 100644
--- a/sdks/java/io/jdbc/src/main/java/org/apache/beam/sdk/io/jdbc/JdbcIO.java
+++ b/sdks/java/io/jdbc/src/main/java/org/apache/beam/sdk/io/jdbc/JdbcIO.java
@@ -2000,7 +2000,9 @@ public class JdbcIO {
       // allow insert only if missing fields are nullable
       checkState(
           !checkNullabilityForFields(missingFields),
-          "Non nullable fields are not allowed without schema.");
+          "Non nullable fields are not allowed without a matching schema. "
+              + "Fields %s were in the destination table but not in the input schema.",
+          missingFields);
 
       List<SchemaUtil.FieldWithIndex> tableFilteredFields =
           tableSchema.getFields().stream()
@@ -2388,6 +2390,8 @@ public class JdbcIO {
             MS_PER_BATCH.update(TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - startTimeNs));
             break;
           } catch (SQLException exception) {
+            LOG.trace(
+                "SQL exception thrown while writing to JDBC database: {}", exception.getMessage());
             if (!spec.getRetryStrategy().apply(exception)) {
               throw exception;
             }
diff --git a/sdks/java/io/jdbc/src/test/java/org/apache/beam/sdk/io/jdbc/JdbcIOExceptionHandlingParameterizedTest.java b/sdks/java/io/jdbc/src/test/java/org/apache/beam/sdk/io/jdbc/JdbcIOExceptionHandlingParameterizedTest.java
new file mode 100644
index 00000000000..6fd245d5ba6
--- /dev/null
+++ b/sdks/java/io/jdbc/src/test/java/org/apache/beam/sdk/io/jdbc/JdbcIOExceptionHandlingParameterizedTest.java
@@ -0,0 +1,146 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.beam.sdk.io.jdbc;
+
+import static org.hamcrest.MatcherAssert.assertThat;
+import static org.hamcrest.Matchers.containsString;
+import static org.junit.Assert.assertThrows;
+
+import java.sql.Connection;
+import java.sql.PreparedStatement;
+import java.sql.SQLException;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.logging.LogRecord;
+import javax.sql.DataSource;
+import org.apache.beam.sdk.Pipeline;
+import org.apache.beam.sdk.io.common.DatabaseTestHelper;
+import org.apache.beam.sdk.testing.ExpectedLogs;
+import org.apache.beam.sdk.transforms.Create;
+import org.apache.beam.sdk.transforms.SerializableFunction;
+import org.apache.beam.sdk.values.KV;
+import org.hamcrest.Description;
+import org.hamcrest.TypeSafeMatcher;
+import org.joda.time.Duration;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
+import org.mockito.Mockito;
+
+@RunWith(Parameterized.class)
+public class JdbcIOExceptionHandlingParameterizedTest {
+  @Rule public final transient ExpectedLogs expectedLogs = ExpectedLogs.none(JdbcIO.class);
+
+  private static final JdbcIO.DataSourceConfiguration DATA_SOURCE_CONFIGURATION =
+      JdbcIO.DataSourceConfiguration.create(
+          "org.apache.derby.jdbc.EmbeddedDriver", "jdbc:derby:memory:testDB;create=true");
+  private static final DataSource DATA_SOURCE = DATA_SOURCE_CONFIGURATION.buildDatasource();
+
+  @Parameterized.Parameters
+  public static Collection<Object[]> data() {
+    return Arrays.asList(
+        new Object[][] {
+          {new SQLException("SQL deadlock", "40001"), null, 4},
+          {new SQLException("PostgreSQL deadlock", "40P01"), null, 4},
+          {new SQLException("NOT A deadlock", "40234"), null, 1},
+          {new SQLException("PostgreSQL NOT A deadlock", "40P12"), null, 1},
+          {null, new SQLException("SQL deadlock", "40001"), 4},
+          {null, new SQLException("PostgreSQL deadlock", "40P01"), 4},
+          {null, new SQLException("NOT A deadlock", "40234"), 1},
+          {null, new SQLException("PostgreSQL NOT A deadlock", "40P12"), 1},
+        });
+  }
+
+  private final Exception executeBatchException;
+  private final Exception commitException;
+  private final Integer retries;
+
+  public JdbcIOExceptionHandlingParameterizedTest(
+      Exception executeBatchException, Exception commitException, Integer retries) {
+    this.executeBatchException = executeBatchException;
+    this.commitException = commitException;
+    this.retries = retries;
+  }
+
+  @Test
+  public void testExceptionsAndRetries() throws Exception {
+    String tableName = DatabaseTestHelper.getTestTableName("UT_EXCEPTION_HANDLING");
+    DatabaseTestHelper.createTable(DATA_SOURCE, tableName);
+
+    DataSource mockedDs = Mockito.mock(DataSource.class, Mockito.withSettings().serializable());
+    Connection mockedConn = Mockito.mock(Connection.class, Mockito.withSettings().serializable());
+    PreparedStatement mockedStatement =
+        Mockito.mock(PreparedStatement.class, Mockito.withSettings().serializable());
+    Mockito.when(mockedDs.getConnection()).thenReturn(mockedConn);
+    Mockito.when(mockedConn.prepareStatement(Mockito.anyString())).thenReturn(mockedStatement);
+    SerializableFunction<Void, DataSource> dsprovider = (vd) -> mockedDs;
+
+    final String excMessage;
+    if (executeBatchException != null) {
+      Mockito.when(mockedStatement.executeBatch()).thenThrow(executeBatchException);
+      excMessage = executeBatchException.getMessage();
+    } else if (commitException != null) {
+      Mockito.doThrow(commitException).when(mockedConn).commit();
+      excMessage = commitException.getMessage();
+    } else {
+      excMessage = "";
+    }
+
+    Pipeline pipeline = Pipeline.create();
+    pipeline
+        .apply(Create.of(Collections.singletonList(KV.of(1, "TEST"))))
+        .apply(
+            JdbcIO.<KV<Integer, String>>write()
+                .withDataSourceProviderFn(dsprovider)
+                .withRetryConfiguration(
+                    JdbcIO.RetryConfiguration.create(3, Duration.millis(1000), Duration.millis(1)))
+                .withStatement(String.format("insert into %s values(?, ?)", tableName))
+                .withPreparedStatementSetter(
+                    (element, statement) -> {
+                      statement.setInt(1, element.getKey());
+                      statement.setString(2, element.getValue());
+                    }));
+    Exception exception =
+        assertThrows(
+            Exception.class,
+            () -> {
+              pipeline.run().waitUntilFinish();
+            });
+    assertThat(exception.getMessage(), containsString(excMessage));
+    exception.printStackTrace();
+
+    expectedLogs.verifyLogRecords(
+        new TypeSafeMatcher<Iterable<LogRecord>>() {
+          @Override
+          public void describeTo(Description description) {}
+
+          @Override
+          protected boolean matchesSafely(Iterable<LogRecord> logRecords) {
+            int count = 0;
+            for (LogRecord logRecord : logRecords) {
+              if (logRecord.getMessage().contains(excMessage)) {
+                count += 1;
+              }
+            }
+            return count == retries;
+          }
+        });
+  }
+}
diff --git a/sdks/java/io/jdbc/src/test/java/org/apache/beam/sdk/io/jdbc/JdbcIOTest.java b/sdks/java/io/jdbc/src/test/java/org/apache/beam/sdk/io/jdbc/JdbcIOTest.java
index 29a3bd1bf6f..6b3fb87d0cd 100644
--- a/sdks/java/io/jdbc/src/test/java/org/apache/beam/sdk/io/jdbc/JdbcIOTest.java
+++ b/sdks/java/io/jdbc/src/test/java/org/apache/beam/sdk/io/jdbc/JdbcIOTest.java
@@ -60,7 +60,9 @@ import java.util.TimeZone;
 import java.util.UUID;
 import java.util.logging.LogRecord;
 import javax.sql.DataSource;
+import org.apache.beam.sdk.Pipeline;
 import org.apache.beam.sdk.Pipeline.PipelineExecutionException;
+import org.apache.beam.sdk.PipelineResult;
 import org.apache.beam.sdk.coders.KvCoder;
 import org.apache.beam.sdk.coders.SerializableCoder;
 import org.apache.beam.sdk.coders.StringUtf8Coder;
@@ -337,6 +339,29 @@ public class JdbcIOTest implements Serializable {
     pipeline.run();
   }
 
+  @Test
+  @SuppressWarnings({"UnusedVariable", "AssertThrowsMultipleStatements"})
+  public void testReadRowsFailedToGetSchema() {
+    Exception exc =
+        assertThrows(
+            BeamSchemaInferenceException.class,
+            () -> {
+              // Using a new pipeline object to avoid the various checks made by TestPipeline in
+              // this pipeline which is
+              // expected to throw an exception.
+              Pipeline pipeline = Pipeline.create();
+              pipeline.apply(
+                  JdbcIO.readRows()
+                      .withDataSourceConfiguration(DATA_SOURCE_CONFIGURATION)
+                      .withQuery(
+                          String.format(
+                              "SELECT CAST(1 AS NUMERIC(1, 0)) AS T1 FROM %s", "unknown_table")));
+              pipeline.run();
+            });
+
+    assertThat(exc.getMessage(), containsString("Failed to infer Beam schema"));
+  }
+
   @Test
   public void testReadRowsWithNumericFieldsWithExcessPrecision() {
     PCollection<Row> rows =
@@ -633,7 +658,7 @@ public class JdbcIOTest implements Serializable {
     DatabaseTestHelper.createTable(DATA_SOURCE, tableName);
 
     // lock table
-    Connection connection = DATA_SOURCE.getConnection();
+    final Connection connection = DATA_SOURCE.getConnection();
     Statement lockStatement = connection.createStatement();
     lockStatement.execute("ALTER TABLE " + tableName + " LOCKSIZE TABLE");
     lockStatement.execute("LOCK TABLE " + tableName + " IN EXCLUSIVE MODE");
@@ -666,19 +691,29 @@ public class JdbcIOTest implements Serializable {
                     }));
 
     // starting a thread to perform the commit later, while the pipeline is running into the backoff
-    Thread commitThread =
+    final Thread commitThread =
         new Thread(
             () -> {
+              while (true) {
+                try {
+                  Thread.sleep(500);
+                  expectedLogs.verifyWarn("Deadlock detected, retrying");
+                  break;
+                } catch (AssertionError | java.lang.InterruptedException e) {
+                  // nothing to do
+                }
+              }
               try {
-                Thread.sleep(10000);
                 connection.commit();
               } catch (Exception e) {
-                // nothing to do
+                // nothing to do.
               }
             });
+
     commitThread.start();
-    pipeline.run();
+    PipelineResult result = pipeline.run();
     commitThread.join();
+    result.waitUntilFinish();
 
     // we verify that the backoff has been called thanks to the log message
     expectedLogs.verifyWarn("Deadlock detected, retrying");
@@ -810,6 +845,7 @@ public class JdbcIOTest implements Serializable {
     } finally {
       DatabaseTestHelper.deleteTable(DATA_SOURCE, tableName);
       thrown.expect(RuntimeException.class);
+      thrown.expectMessage("Non nullable fields are not allowed without a matching schema.");
     }
   }
 
