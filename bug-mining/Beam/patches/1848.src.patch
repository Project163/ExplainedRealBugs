diff --git a/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/spanner/SpannerIO.java b/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/spanner/SpannerIO.java
index 07f227ad250..94ad154ff7b 100644
--- a/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/spanner/SpannerIO.java
+++ b/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/spanner/SpannerIO.java
@@ -59,6 +59,7 @@ import org.apache.beam.sdk.options.ValueProvider;
 import org.apache.beam.sdk.transforms.Create;
 import org.apache.beam.sdk.transforms.DoFn;
 import org.apache.beam.sdk.transforms.Flatten;
+import org.apache.beam.sdk.transforms.MapElements;
 import org.apache.beam.sdk.transforms.PTransform;
 import org.apache.beam.sdk.transforms.ParDo;
 import org.apache.beam.sdk.transforms.Reshuffle;
@@ -81,6 +82,7 @@ import org.apache.beam.sdk.values.PCollectionTuple;
 import org.apache.beam.sdk.values.PCollectionView;
 import org.apache.beam.sdk.values.TupleTag;
 import org.apache.beam.sdk.values.TupleTagList;
+import org.apache.beam.sdk.values.TypeDescriptor;
 import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;
 import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Stopwatch;
 import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;
@@ -1022,84 +1024,98 @@ public class SpannerIO {
 
     @Override
     public SpannerWriteResult expand(PCollection<MutationGroup> input) {
+      PCollection<Iterable<MutationGroup>> batches;
+
+      if (spec.getBatchSizeBytes() <= 1
+          || spec.getMaxNumMutations() <= 1
+          || spec.getMaxNumRows() <= 1) {
+        LOG.info("Batching of mutationGroups is disabled");
+        TypeDescriptor<Iterable<MutationGroup>> descriptor =
+            new TypeDescriptor<Iterable<MutationGroup>>() {};
+        batches =
+            input.apply(MapElements.into(descriptor).via(element -> ImmutableList.of(element)));
+      } else {
 
-      // First, read the Cloud Spanner schema.
-      PCollection<Void> schemaSeed =
-          input.getPipeline().apply("Create Seed", Create.of((Void) null));
-      if (spec.getSchemaReadySignal() != null) {
-        // Wait for external signal before reading schema.
-        schemaSeed = schemaSeed.apply("Wait for schema", Wait.on(spec.getSchemaReadySignal()));
+        // First, read the Cloud Spanner schema.
+        PCollection<Void> schemaSeed =
+            input.getPipeline().apply("Create Seed", Create.of((Void) null));
+        if (spec.getSchemaReadySignal() != null) {
+          // Wait for external signal before reading schema.
+          schemaSeed = schemaSeed.apply("Wait for schema", Wait.on(spec.getSchemaReadySignal()));
+        }
+        final PCollectionView<SpannerSchema> schemaView =
+            schemaSeed
+                .apply(
+                    "Read information schema",
+                    ParDo.of(new ReadSpannerSchema(spec.getSpannerConfig())))
+                .apply("Schema View", View.asSingleton());
+
+        // Split the mutations into batchable and unbatchable mutations.
+        // Filter out mutation groups too big to be batched.
+        PCollectionTuple filteredMutations =
+            input
+                .apply(
+                    "RewindowIntoGlobal",
+                    Window.<MutationGroup>into(new GlobalWindows())
+                        .triggering(DefaultTrigger.of())
+                        .discardingFiredPanes())
+                .apply(
+                    "Filter Unbatchable Mutations",
+                    ParDo.of(
+                            new BatchableMutationFilterFn(
+                                schemaView,
+                                UNBATCHABLE_MUTATIONS_TAG,
+                                spec.getBatchSizeBytes(),
+                                spec.getMaxNumMutations(),
+                                spec.getMaxNumRows()))
+                        .withSideInputs(schemaView)
+                        .withOutputTags(
+                            BATCHABLE_MUTATIONS_TAG, TupleTagList.of(UNBATCHABLE_MUTATIONS_TAG)));
+
+        // Build a set of Mutation groups from the current bundle,
+        // sort them by table/key then split into batches.
+        PCollection<Iterable<MutationGroup>> batchedMutations =
+            filteredMutations
+                .get(BATCHABLE_MUTATIONS_TAG)
+                .apply(
+                    "Gather And Sort",
+                    ParDo.of(
+                            new GatherBundleAndSortFn(
+                                spec.getBatchSizeBytes(),
+                                spec.getMaxNumMutations(),
+                                spec.getMaxNumRows(),
+                                // Do not group on streaming unless explicitly set.
+                                spec.getGroupingFactor()
+                                    .orElse(
+                                        input.isBounded() == IsBounded.BOUNDED
+                                            ? DEFAULT_GROUPING_FACTOR
+                                            : 1),
+                                schemaView))
+                        .withSideInputs(schemaView))
+                .apply(
+                    "Create Batches",
+                    ParDo.of(
+                            new BatchFn(
+                                spec.getBatchSizeBytes(),
+                                spec.getMaxNumMutations(),
+                                spec.getMaxNumRows(),
+                                schemaView))
+                        .withSideInputs(schemaView));
+
+        // Merge the batched and unbatchable mutation PCollections and write to Spanner.
+        batches =
+            PCollectionList.of(filteredMutations.get(UNBATCHABLE_MUTATIONS_TAG))
+                .and(batchedMutations)
+                .apply("Merge", Flatten.pCollections());
       }
-      final PCollectionView<SpannerSchema> schemaView =
-          schemaSeed
-              .apply(
-                  "Read information schema",
-                  ParDo.of(new ReadSpannerSchema(spec.getSpannerConfig())))
-              .apply("Schema View", View.asSingleton());
-
-      // Split the mutations into batchable and unbatchable mutations.
-      // Filter out mutation groups too big to be batched.
-      PCollectionTuple filteredMutations =
-          input
-              .apply(
-                  "RewindowIntoGlobal",
-                  Window.<MutationGroup>into(new GlobalWindows())
-                      .triggering(DefaultTrigger.of())
-                      .discardingFiredPanes())
-              .apply(
-                  "Filter Unbatchable Mutations",
-                  ParDo.of(
-                          new BatchableMutationFilterFn(
-                              schemaView,
-                              UNBATCHABLE_MUTATIONS_TAG,
-                              spec.getBatchSizeBytes(),
-                              spec.getMaxNumMutations(),
-                              spec.getMaxNumRows()))
-                      .withSideInputs(schemaView)
-                      .withOutputTags(
-                          BATCHABLE_MUTATIONS_TAG, TupleTagList.of(UNBATCHABLE_MUTATIONS_TAG)));
-
-      // Build a set of Mutation groups from the current bundle,
-      // sort them by table/key then split into batches.
-      PCollection<Iterable<MutationGroup>> batchedMutations =
-          filteredMutations
-              .get(BATCHABLE_MUTATIONS_TAG)
-              .apply(
-                  "Gather And Sort",
-                  ParDo.of(
-                          new GatherBundleAndSortFn(
-                              spec.getBatchSizeBytes(),
-                              spec.getMaxNumMutations(),
-                              spec.getMaxNumRows(),
-                              // Do not group on streaming unless explicitly set.
-                              spec.getGroupingFactor()
-                                  .orElse(
-                                      input.isBounded() == IsBounded.BOUNDED
-                                          ? DEFAULT_GROUPING_FACTOR
-                                          : 1),
-                              schemaView))
-                      .withSideInputs(schemaView))
-              .apply(
-                  "Create Batches",
-                  ParDo.of(
-                          new BatchFn(
-                              spec.getBatchSizeBytes(),
-                              spec.getMaxNumMutations(),
-                              spec.getMaxNumRows(),
-                              schemaView))
-                      .withSideInputs(schemaView));
-
-      // Merge the batchable and unbatchable mutation PCollections and write to Spanner.
+
       PCollectionTuple result =
-          PCollectionList.of(filteredMutations.get(UNBATCHABLE_MUTATIONS_TAG))
-              .and(batchedMutations)
-              .apply("Merge", Flatten.pCollections())
-              .apply(
-                  "Write mutations to Spanner",
-                  ParDo.of(
-                          new WriteToSpannerFn(
-                              spec.getSpannerConfig(), spec.getFailureMode(), FAILED_MUTATIONS_TAG))
-                      .withOutputTags(MAIN_OUT_TAG, TupleTagList.of(FAILED_MUTATIONS_TAG)));
+          batches.apply(
+              "Write batches to Spanner",
+              ParDo.of(
+                      new WriteToSpannerFn(
+                          spec.getSpannerConfig(), spec.getFailureMode(), FAILED_MUTATIONS_TAG))
+                  .withOutputTags(MAIN_OUT_TAG, TupleTagList.of(FAILED_MUTATIONS_TAG)));
 
       return new SpannerWriteResult(
           input.getPipeline(),
diff --git a/sdks/java/io/google-cloud-platform/src/test/java/org/apache/beam/sdk/io/gcp/spanner/SpannerIOWriteTest.java b/sdks/java/io/google-cloud-platform/src/test/java/org/apache/beam/sdk/io/gcp/spanner/SpannerIOWriteTest.java
index ffcbaefff9a..816556cf1bc 100644
--- a/sdks/java/io/google-cloud-platform/src/test/java/org/apache/beam/sdk/io/gcp/spanner/SpannerIOWriteTest.java
+++ b/sdks/java/io/google-cloud-platform/src/test/java/org/apache/beam/sdk/io/gcp/spanner/SpannerIOWriteTest.java
@@ -263,18 +263,34 @@ public class SpannerIOWriteTest implements Serializable {
 
   @Test
   public void noBatching() throws Exception {
+
+    // This test uses a different mock/fake because it explicitly does not want to populate the
+    // Spanner schema.
+    FakeServiceFactory fakeServiceFactory = new FakeServiceFactory();
+    ReadOnlyTransaction tx = mock(ReadOnlyTransaction.class);
+    when(fakeServiceFactory.mockDatabaseClient().readOnlyTransaction()).thenReturn(tx);
+
+    // Capture batches sent to writeAtLeastOnce.
+    when(fakeServiceFactory.mockDatabaseClient().writeAtLeastOnce(mutationBatchesCaptor.capture()))
+        .thenReturn(null);
+
     PCollection<MutationGroup> mutations = pipeline.apply(Create.of(g(m(1L)), g(m(2L))));
     mutations.apply(
         SpannerIO.write()
             .withProjectId("test-project")
             .withInstanceId("test-instance")
             .withDatabaseId("test-database")
-            .withServiceFactory(serviceFactory)
+            .withServiceFactory(fakeServiceFactory)
             .withBatchSizeBytes(1)
             .grouped());
     pipeline.run();
 
-    verifyBatches(batch(m(1L)), batch(m(2L)));
+    verify(fakeServiceFactory.mockDatabaseClient(), times(1))
+        .writeAtLeastOnce(mutationsInNoOrder(batch(m(1L))));
+    verify(fakeServiceFactory.mockDatabaseClient(), times(1))
+        .writeAtLeastOnce(mutationsInNoOrder(batch(m(2L))));
+    // If no batching then the DB schema is never read.
+    verify(tx, never()).executeQuery(any());
   }
 
   @Test
