diff --git a/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIO.java b/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIO.java
index 319f2bea5b4..dcbf0a170c2 100644
--- a/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIO.java
+++ b/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIO.java
@@ -32,6 +32,7 @@ import java.io.IOException;
 import java.io.Serializable;
 import java.util.ArrayList;
 import java.util.Collections;
+import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.function.Predicate;
@@ -97,6 +98,11 @@ import org.slf4j.LoggerFactory;
  *       writeRequest>
  * </ul>
  *
+ * If primary keys could repeat in your stream (i.e. an upsert stream), you could encounter a
+ * ValidationError, as AWS does not allow writing duplicate keys within a single batch operation.
+ * For such use cases, you can explicitly set the key names corresponding to the primary key to be
+ * deduplicated using the withDeduplicateKeys method
+ *
  * <h3>Reading from DynamoDB</h3>
  *
  * <p>Example usage:
@@ -131,7 +137,7 @@ public final class DynamoDBIO {
   }
 
   public static <T> Write<T> write() {
-    return new AutoValue_DynamoDBIO_Write.Builder().build();
+    return new AutoValue_DynamoDBIO_Write.Builder().setDeduplicateKeys(new ArrayList<>()).build();
   }
 
   /** Read data from DynamoDB and return ScanResult. */
@@ -346,6 +352,8 @@ public final class DynamoDBIO {
 
     abstract @Nullable SerializableFunction<T, KV<String, WriteRequest>> getWriteItemMapperFn();
 
+    abstract List<String> getDeduplicateKeys();
+
     abstract Builder<T> builder();
 
     @AutoValue.Builder
@@ -358,6 +366,8 @@ public final class DynamoDBIO {
       abstract Builder<T> setWriteItemMapperFn(
           SerializableFunction<T, KV<String, WriteRequest>> writeItemMapperFn);
 
+      abstract Builder<T> setDeduplicateKeys(List<String> deduplicateKeys);
+
       abstract Write<T> build();
     }
 
@@ -406,6 +416,10 @@ public final class DynamoDBIO {
       return builder().setWriteItemMapperFn(writeItemMapperFn).build();
     }
 
+    public Write<T> withDeduplicateKeys(List<String> deduplicateKeys) {
+      return builder().setDeduplicateKeys(deduplicateKeys).build();
+    }
+
     @Override
     public PCollection<Void> expand(PCollection<T> input) {
       return input.apply(ParDo.of(new WriteFn<>(this)));
@@ -424,7 +438,7 @@ public final class DynamoDBIO {
       private static final int BATCH_SIZE = 25;
       private transient AmazonDynamoDB client;
       private final DynamoDBIO.Write spec;
-      private List<KV<String, WriteRequest>> batch;
+      private Map<KV<String, Map<String, AttributeValue>>, KV<String, WriteRequest>> batch;
 
       WriteFn(DynamoDBIO.Write spec) {
         this.spec = spec;
@@ -447,19 +461,35 @@ public final class DynamoDBIO {
 
       @StartBundle
       public void startBundle(StartBundleContext context) {
-        batch = new ArrayList<>();
+        batch = new HashMap<>();
       }
 
       @ProcessElement
       public void processElement(ProcessContext context) throws Exception {
         final KV<String, WriteRequest> writeRequest =
             (KV<String, WriteRequest>) spec.getWriteItemMapperFn().apply(context.element());
-        batch.add(writeRequest);
+        batch.put(
+            KV.of(writeRequest.getKey(), extractDeduplicateKeyValues(writeRequest.getValue())),
+            writeRequest);
         if (batch.size() >= BATCH_SIZE) {
           flushBatch();
         }
       }
 
+      private Map<String, AttributeValue> extractDeduplicateKeyValues(WriteRequest request) {
+        if (request.getPutRequest() != null) {
+          return request.getPutRequest().getItem().entrySet().stream()
+              .filter(entry -> spec.getDeduplicateKeys().contains(entry.getKey()))
+              .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));
+        } else if (request.getDeleteRequest() != null) {
+          return request.getDeleteRequest().getKey().entrySet().stream()
+              .filter(entry -> spec.getDeduplicateKeys().contains(entry.getKey()))
+              .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));
+        } else {
+          return Collections.emptyMap();
+        }
+      }
+
       @FinishBundle
       public void finishBundle(FinishBundleContext context) throws Exception {
         flushBatch();
@@ -474,7 +504,7 @@ public final class DynamoDBIO {
           // Since each element is a KV<tableName, writeRequest> in the batch, we need to group them
           // by tableName
           Map<String, List<WriteRequest>> mapTableRequest =
-              batch.stream()
+              batch.values().stream()
                   .collect(
                       Collectors.groupingBy(
                           KV::getKey, Collectors.mapping(KV::getValue, Collectors.toList())));
diff --git a/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIOTest.java b/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIOTest.java
index a867d3afe4f..9578d57a65e 100644
--- a/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIOTest.java
+++ b/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIOTest.java
@@ -27,14 +27,19 @@ import com.amazonaws.services.dynamodbv2.model.BatchWriteItemRequest;
 import com.amazonaws.services.dynamodbv2.model.ScanRequest;
 import com.amazonaws.services.dynamodbv2.model.WriteRequest;
 import java.io.Serializable;
+import java.util.Arrays;
+import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
+import java.util.stream.Collectors;
 import org.apache.beam.sdk.Pipeline;
 import org.apache.beam.sdk.testing.ExpectedLogs;
 import org.apache.beam.sdk.testing.PAssert;
 import org.apache.beam.sdk.testing.TestPipeline;
 import org.apache.beam.sdk.transforms.Count;
 import org.apache.beam.sdk.transforms.Create;
+import org.apache.beam.sdk.transforms.DoFn;
+import org.apache.beam.sdk.transforms.ParDo;
 import org.apache.beam.sdk.transforms.SerializableFunction;
 import org.apache.beam.sdk.values.KV;
 import org.apache.beam.sdk.values.PCollection;
@@ -44,6 +49,7 @@ import org.junit.BeforeClass;
 import org.junit.Rule;
 import org.junit.Test;
 import org.junit.rules.ExpectedException;
+import org.mockito.ArgumentCaptor;
 import org.mockito.Mockito;
 
 /** Test Coverage for the IO. */
@@ -213,4 +219,64 @@ public class DynamoDBIOTest implements Serializable {
     }
     fail("Pipeline is expected to fail because we were unable to write to DynamoDB.");
   }
+
+  /**
+   * A DoFn used to generate outputs duplicated N times, where N is the input. Used to generate
+   * bundles with duplicate elements.
+   */
+  private static class WriteDuplicateGeneratorDoFn extends DoFn<Integer, WriteRequest> {
+    @ProcessElement
+    public void processElement(ProcessContext ctx) {
+      for (int i = 0; i < ctx.element(); i++) {
+        DynamoDBIOTestHelper.generateWriteRequests(numOfItems).forEach(ctx::output);
+      }
+    }
+  }
+
+  @Test
+  public void testWriteDeduplication() {
+    // designate duplication factor for each bundle
+    final List<Integer> duplications = Arrays.asList(1, 2, 3);
+
+    final List<String> deduplicateKeys =
+        Arrays.asList(DynamoDBIOTestHelper.ATTR_NAME_1, DynamoDBIOTestHelper.ATTR_NAME_2);
+
+    AmazonDynamoDB amazonDynamoDBMock = Mockito.mock(AmazonDynamoDB.class);
+
+    pipeline
+        .apply(Create.of(duplications))
+        .apply("duplicate", ParDo.of(new WriteDuplicateGeneratorDoFn()))
+        .apply(
+            DynamoDBIO.<WriteRequest>write()
+                .withWriteRequestMapperFn(
+                    (SerializableFunction<WriteRequest, KV<String, WriteRequest>>)
+                        writeRequest -> KV.of(tableName, writeRequest))
+                .withRetryConfiguration(
+                    DynamoDBIO.RetryConfiguration.create(5, Duration.standardMinutes(1)))
+                .withAwsClientsProvider(AwsClientsProviderMock.of(amazonDynamoDBMock))
+                .withDeduplicateKeys(deduplicateKeys));
+
+    pipeline.run().waitUntilFinish();
+
+    ArgumentCaptor<BatchWriteItemRequest> argumentCaptor =
+        ArgumentCaptor.forClass(BatchWriteItemRequest.class);
+    Mockito.verify(amazonDynamoDBMock, Mockito.times(3)).batchWriteItem(argumentCaptor.capture());
+    List<BatchWriteItemRequest> batchRequests = argumentCaptor.getAllValues();
+    batchRequests.forEach(
+        batchRequest -> {
+          List<WriteRequest> requests = batchRequest.getRequestItems().get(tableName);
+          // assert that each bundle contains expected number of items
+          assertEquals(numOfItems, requests.size());
+          List<Map<String, AttributeValue>> requestKeys =
+              requests.stream()
+                  .map(
+                      request ->
+                          request.getPutRequest() != null
+                              ? request.getPutRequest().getItem()
+                              : request.getDeleteRequest().getKey())
+                  .collect(Collectors.toList());
+          // assert no duplicate keys in each bundle
+          assertEquals(new HashSet<>(requestKeys).size(), requestKeys.size());
+        });
+  }
 }
diff --git a/sdks/java/io/amazon-web-services2/src/main/java/org/apache/beam/sdk/io/aws2/dynamodb/DynamoDBIO.java b/sdks/java/io/amazon-web-services2/src/main/java/org/apache/beam/sdk/io/aws2/dynamodb/DynamoDBIO.java
index 16f5dae47d8..e247b6a721b 100644
--- a/sdks/java/io/amazon-web-services2/src/main/java/org/apache/beam/sdk/io/aws2/dynamodb/DynamoDBIO.java
+++ b/sdks/java/io/amazon-web-services2/src/main/java/org/apache/beam/sdk/io/aws2/dynamodb/DynamoDBIO.java
@@ -25,6 +25,7 @@ import java.io.Serializable;
 import java.net.URI;
 import java.util.ArrayList;
 import java.util.Collections;
+import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.function.Predicate;
@@ -126,6 +127,11 @@ import software.amazon.awssdk.services.dynamodb.model.WriteRequest;
  *   <li>Mapper function with a table name to map or transform your object into KV<tableName,
  *       writeRequest>
  * </ul>
+ *
+ * If primary keys could repeat in your stream (i.e. an upsert stream), you could encounter a
+ * ValidationError, as AWS does not allow writing duplicate keys within a single batch operation.
+ * For such use cases, you can explicitly set the key names corresponding to the primary key to be
+ * deduplicated using the withDeduplicateKeys method
  */
 @Experimental(Kind.SOURCE_SINK)
 @SuppressWarnings({
@@ -138,7 +144,7 @@ public final class DynamoDBIO {
   }
 
   public static <T> Write<T> write() {
-    return new AutoValue_DynamoDBIO_Write.Builder().build();
+    return new AutoValue_DynamoDBIO_Write.Builder().setDeduplicateKeys(new ArrayList<>()).build();
   }
 
   /** Read data from DynamoDB and return ScanResult. */
@@ -360,6 +366,8 @@ public final class DynamoDBIO {
 
     abstract @Nullable SerializableFunction<T, KV<String, WriteRequest>> getWriteItemMapperFn();
 
+    abstract List<String> getDeduplicateKeys();
+
     abstract Builder<T> toBuilder();
 
     @AutoValue.Builder
@@ -372,6 +380,8 @@ public final class DynamoDBIO {
       abstract Builder<T> setWriteItemMapperFn(
           SerializableFunction<T, KV<String, WriteRequest>> writeItemMapperFn);
 
+      abstract Builder<T> setDeduplicateKeys(List<String> deduplicateKeys);
+
       abstract Write<T> build();
     }
 
@@ -424,6 +434,10 @@ public final class DynamoDBIO {
       return toBuilder().setWriteItemMapperFn(writeItemMapperFn).build();
     }
 
+    public Write<T> withDeduplicateKeys(List<String> deduplicateKeys) {
+      return toBuilder().setDeduplicateKeys(deduplicateKeys).build();
+    }
+
     @Override
     public PCollection<Void> expand(PCollection<T> input) {
       return input.apply(ParDo.of(new WriteFn<>(this)));
@@ -442,7 +456,7 @@ public final class DynamoDBIO {
       private static final int BATCH_SIZE = 25;
       private transient DynamoDbClient client;
       private final Write spec;
-      private List<KV<String, WriteRequest>> batch;
+      private Map<KV<String, Map<String, AttributeValue>>, KV<String, WriteRequest>> batch;
 
       WriteFn(Write spec) {
         this.spec = spec;
@@ -465,19 +479,35 @@ public final class DynamoDBIO {
 
       @StartBundle
       public void startBundle(StartBundleContext context) {
-        batch = new ArrayList<>();
+        batch = new HashMap<>();
       }
 
       @ProcessElement
       public void processElement(ProcessContext context) throws Exception {
         final KV<String, WriteRequest> writeRequest =
             (KV<String, WriteRequest>) spec.getWriteItemMapperFn().apply(context.element());
-        batch.add(writeRequest);
+        batch.put(
+            KV.of(writeRequest.getKey(), extractDeduplicateKeyValues(writeRequest.getValue())),
+            writeRequest);
         if (batch.size() >= BATCH_SIZE) {
           flushBatch();
         }
       }
 
+      private Map<String, AttributeValue> extractDeduplicateKeyValues(WriteRequest request) {
+        if (request.putRequest() != null) {
+          return request.putRequest().item().entrySet().stream()
+              .filter(entry -> spec.getDeduplicateKeys().contains(entry.getKey()))
+              .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));
+        } else if (request.deleteRequest() != null) {
+          return request.deleteRequest().key().entrySet().stream()
+              .filter(entry -> spec.getDeduplicateKeys().contains(entry.getKey()))
+              .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));
+        } else {
+          return Collections.emptyMap();
+        }
+      }
+
       @FinishBundle
       public void finishBundle(FinishBundleContext context) throws Exception {
         flushBatch();
@@ -492,7 +522,7 @@ public final class DynamoDBIO {
           // Since each element is a KV<tableName, writeRequest> in the batch, we need to group them
           // by tableName
           Map<String, List<WriteRequest>> mapTableRequest =
-              batch.stream()
+              batch.values().stream()
                   .collect(
                       Collectors.groupingBy(
                           KV::getKey, Collectors.mapping(KV::getValue, Collectors.toList())));
diff --git a/sdks/java/io/amazon-web-services2/src/test/java/org/apache/beam/sdk/io/aws2/dynamodb/DynamoDBIOTest.java b/sdks/java/io/amazon-web-services2/src/test/java/org/apache/beam/sdk/io/aws2/dynamodb/DynamoDBIOTest.java
index 607ee960439..a204cec2a55 100644
--- a/sdks/java/io/amazon-web-services2/src/test/java/org/apache/beam/sdk/io/aws2/dynamodb/DynamoDBIOTest.java
+++ b/sdks/java/io/amazon-web-services2/src/test/java/org/apache/beam/sdk/io/aws2/dynamodb/DynamoDBIOTest.java
@@ -22,8 +22,11 @@ import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.fail;
 
 import java.io.Serializable;
+import java.util.Arrays;
+import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
+import java.util.stream.Collectors;
 import org.apache.beam.sdk.Pipeline;
 import org.apache.beam.sdk.testing.ExpectedLogs;
 import org.apache.beam.sdk.testing.PAssert;
@@ -48,6 +51,7 @@ import org.junit.BeforeClass;
 import org.junit.Rule;
 import org.junit.Test;
 import org.junit.rules.ExpectedException;
+import org.mockito.ArgumentCaptor;
 import org.mockito.Mockito;
 import software.amazon.awssdk.services.dynamodb.DynamoDbClient;
 import software.amazon.awssdk.services.dynamodb.model.AttributeValue;
@@ -311,4 +315,83 @@ public class DynamoDBIOTest implements Serializable {
     }
     fail("Pipeline is expected to fail because we were unable to write to DynamoDb.");
   }
+
+  /**
+   * A DoFn used to generate outputs duplicated N times, where N is the input. Used to generate
+   * bundles with duplicate elements.
+   */
+  private static class WriteDuplicateGeneratorDoFn extends DoFn<Integer, KV<String, Integer>> {
+    @ProcessElement
+    public void processElement(ProcessContext ctx) {
+      for (int i = 0; i < ctx.element(); i++) {
+        for (int j = 1; j <= numOfItems; j++) {
+          KV<String, Integer> item = KV.of("test" + j, 1000 + j);
+          ctx.output(item);
+        }
+      }
+    }
+  }
+
+  @Test
+  public void testWriteDeduplication() {
+    // designate duplication factor for each bundle
+    final List<Integer> duplications = Arrays.asList(1, 2, 3);
+
+    final List<String> deduplicateKeys = Arrays.asList("hashKey1", "rangeKey2");
+
+    DynamoDbClient amazonDynamoDBMock = Mockito.mock(DynamoDbClient.class);
+
+    pipeline
+        .apply(Create.of(duplications))
+        .apply("duplicate", ParDo.of(new WriteDuplicateGeneratorDoFn()))
+        .apply(
+            DynamoDBIO.<KV<String, Integer>>write()
+                .withWriteRequestMapperFn(
+                    (SerializableFunction<KV<String, Integer>, KV<String, WriteRequest>>)
+                        entry -> {
+                          Map<String, AttributeValue> putRequest =
+                              ImmutableMap.of(
+                                  "hashKey1",
+                                  AttributeValue.builder().s(entry.getKey()).build(),
+                                  "rangeKey2",
+                                  AttributeValue.builder().n(entry.getValue().toString()).build());
+
+                          WriteRequest writeRequest =
+                              WriteRequest.builder()
+                                  .putRequest(PutRequest.builder().item(putRequest).build())
+                                  .build();
+                          return KV.of(tableName, writeRequest);
+                        })
+                .withRetryConfiguration(
+                    DynamoDBIO.RetryConfiguration.builder()
+                        .setMaxAttempts(5)
+                        .setMaxDuration(Duration.standardMinutes(1))
+                        .setRetryPredicate(DEFAULT_RETRY_PREDICATE)
+                        .build())
+                .withDynamoDbClientProvider(DynamoDbClientProviderMock.of(amazonDynamoDBMock))
+                .withDeduplicateKeys(deduplicateKeys));
+
+    pipeline.run().waitUntilFinish();
+
+    ArgumentCaptor<BatchWriteItemRequest> argumentCaptor =
+        ArgumentCaptor.forClass(BatchWriteItemRequest.class);
+    Mockito.verify(amazonDynamoDBMock, Mockito.times(3)).batchWriteItem(argumentCaptor.capture());
+    List<BatchWriteItemRequest> batchRequests = argumentCaptor.getAllValues();
+    batchRequests.forEach(
+        batchRequest -> {
+          List<WriteRequest> requests = batchRequest.requestItems().get(tableName);
+          // assert that each bundle contains expected number of items
+          assertEquals(numOfItems, requests.size());
+          List<Map<String, AttributeValue>> requestKeys =
+              requests.stream()
+                  .map(
+                      request ->
+                          request.putRequest() != null
+                              ? request.putRequest().item()
+                              : request.deleteRequest().key())
+                  .collect(Collectors.toList());
+          // assert no duplicate keys in each bundle
+          assertEquals(new HashSet<>(requestKeys).size(), requestKeys.size());
+        });
+  }
 }
