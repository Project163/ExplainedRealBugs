diff --git a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorker.java b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorker.java
index 6c127c84f84..f24bf281a8b 100644
--- a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorker.java
+++ b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorker.java
@@ -1126,7 +1126,8 @@ public class StreamingDataflowWorker {
 
     @Override
     public String toString() {
-      return String.format("%s-%d", TextFormat.escapeBytes(key()), shardingKey());
+      return String.format(
+          "%016x-%s", shardingKey(), TextFormat.escapeBytes(key().substring(0, 100)));
     }
   }
 
@@ -2257,20 +2258,28 @@ public class StreamingDataflowWorker {
       Work nextWork;
       synchronized (activeWork) {
         Queue<Work> queue = activeWork.get(shardedKey);
-        Preconditions.checkNotNull(queue);
+        if (queue == null) {
+          // Work may have been completed due to clearing of stuck commits.
+          LOG.warn(
+              "Unable to complete inactive work for key {} and token {}.", shardedKey, workToken);
+          return;
+        }
         Work completedWork = queue.peek();
-        // avoid Preconditions.checkNotNull and checkState here to prevent eagerly evaluating the
+        // avoid Preconditions.checkState here to prevent eagerly evaluating the
         // format string parameters for the error message.
         if (completedWork == null) {
-          throw new NullPointerException(
+          throw new IllegalStateException(
               String.format(
-                  "No active state for key %s, expected token %s", shardedKey, workToken));
+                  "Active key %s without work, expected token %d", shardedKey, workToken));
         }
         if (completedWork.getWorkItem().getWorkToken() != workToken) {
-          throw new IllegalStateException(
-              String.format(
-                  "Token mismatch for key %s: %s and %s",
-                  shardedKey, completedWork.getWorkItem().getWorkToken(), workToken));
+          // Work may have been completed due to clearing of stuck commits.
+          LOG.warn(
+              "Unable to complete due to token mismatch for key {} and token {}, actual token was {}.",
+              shardedKey,
+              workToken,
+              completedWork.getWorkItem().getWorkToken());
+          return;
         }
         queue.remove(); // We consumed the matching work item.
         nextWork = queue.peek();
@@ -2294,8 +2303,9 @@ public class StreamingDataflowWorker {
           if (work.getState() == State.COMMITTING
               && work.getStateStartTime().isBefore(stuckCommitDeadline)) {
             LOG.error(
-                "Detected key with sharding key {} stuck in COMMITTING state, completing it with error.",
-                work.workItem.getShardingKey());
+                "Detected key with sharding key 0x{} stuck in COMMITTING state since {}, completing it with error.",
+                shardedKey,
+                work.getStateStartTime());
             stuckCommits.put(shardedKey, work.getWorkItem().getWorkToken());
           }
         }
diff --git a/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/FakeWindmillServer.java b/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/FakeWindmillServer.java
index 1a004e2b046..aec0793b59d 100644
--- a/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/FakeWindmillServer.java
+++ b/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/FakeWindmillServer.java
@@ -76,7 +76,7 @@ class FakeWindmillServer extends WindmillServerStub {
   private final ErrorCollector errorCollector;
   private boolean isReady = true;
   private boolean dropStreamingCommits = false;
-  private final AtomicInteger droppedStreamingCommits;
+  private final ConcurrentHashMap<Long, Consumer<Windmill.CommitStatus>> droppedStreamingCommits;
 
   public FakeWindmillServer(ErrorCollector errorCollector) {
     workToOffer = new ConcurrentLinkedQueue<>();
@@ -86,7 +86,7 @@ class FakeWindmillServer extends WindmillServerStub {
     expectedExceptionCount = new AtomicInteger();
     this.errorCollector = errorCollector;
     statsReceived = new ArrayList<>();
-    droppedStreamingCommits = new AtomicInteger();
+    droppedStreamingCommits = new ConcurrentHashMap<>();
   }
 
   public void setDropStreamingCommits(boolean dropStreamingCommits) {
@@ -315,7 +315,7 @@ class FakeWindmillServer extends WindmillServerStub {
             request.getShardingKey(), allOf(greaterThan(0L), lessThan(Long.MAX_VALUE)));
         errorCollector.checkThat(request.getCacheToken(), not(equalTo(0L)));
         if (dropStreamingCommits) {
-          droppedStreamingCommits.incrementAndGet();
+          droppedStreamingCommits.put(request.getWorkToken(), onDone);
         } else {
           commitsReceived.put(request.getWorkToken(), request);
           onDone.accept(Windmill.CommitStatus.OK);
@@ -377,13 +377,15 @@ class FakeWindmillServer extends WindmillServerStub {
     commitsReceived.clear();
   }
 
-  public void waitForDroppedCommits(int droppedCommits) {
+  public ConcurrentHashMap<Long, Consumer<Windmill.CommitStatus>> waitForDroppedCommits(
+      int droppedCommits) {
     LOG.debug("waitForDroppedCommits: {}", droppedCommits);
     int maxTries = 10;
-    while (maxTries-- > 0 && droppedStreamingCommits.get() < droppedCommits) {
+    while (maxTries-- > 0 && droppedStreamingCommits.size() < droppedCommits) {
       Uninterruptibles.sleepUninterruptibly(1000, TimeUnit.MILLISECONDS);
     }
-    assertEquals(droppedCommits, droppedStreamingCommits.get());
+    assertEquals(droppedCommits, droppedStreamingCommits.size());
+    return droppedStreamingCommits;
   }
 
   public void setExpectedExceptionCount(int i) {
diff --git a/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorkerTest.java b/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorkerTest.java
index 0231eb0825a..4d94198e085 100644
--- a/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorkerTest.java
+++ b/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorkerTest.java
@@ -63,11 +63,13 @@ import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
+import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.Semaphore;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.atomic.AtomicLong;
+import java.util.function.Consumer;
 import java.util.function.Function;
 import java.util.function.Supplier;
 import org.apache.beam.runners.core.construction.Environments;
@@ -86,6 +88,7 @@ import org.apache.beam.runners.dataflow.worker.testing.TestCountingSource;
 import org.apache.beam.runners.dataflow.worker.util.BoundedQueueExecutor;
 import org.apache.beam.runners.dataflow.worker.util.WorkerPropertyNames;
 import org.apache.beam.runners.dataflow.worker.windmill.Windmill;
+import org.apache.beam.runners.dataflow.worker.windmill.Windmill.CommitStatus;
 import org.apache.beam.runners.dataflow.worker.windmill.Windmill.ComputationGetDataRequest;
 import org.apache.beam.runners.dataflow.worker.windmill.Windmill.ComputationGetDataResponse;
 import org.apache.beam.runners.dataflow.worker.windmill.Windmill.GetDataRequest;
@@ -2960,12 +2963,15 @@ public class StreamingDataflowWorkerTest {
     // Add some work for key 1.
     server.addWorkToOffer(makeInput(10, TimeUnit.MILLISECONDS.toMicros(2), DEFAULT_KEY_STRING, 1));
     server.addWorkToOffer(makeInput(15, TimeUnit.MILLISECONDS.toMicros(3), DEFAULT_KEY_STRING, 5));
-    server.waitForDroppedCommits(2);
+    ConcurrentHashMap<Long, Consumer<CommitStatus>> droppedCommits =
+        server.waitForDroppedCommits(2);
     server.setDropStreamingCommits(false);
     // Enqueue another work item for key 1.
     server.addWorkToOffer(makeInput(1, TimeUnit.MILLISECONDS.toMicros(1), DEFAULT_KEY_STRING, 1));
     // Ensure that the this work item processes.
     Map<Long, Windmill.WorkItemCommitRequest> result = server.waitForAndGetCommits(1);
+    // Now ensure that nothing happens if a dropped commit actually completes.
+    droppedCommits.values().iterator().next().accept(CommitStatus.OK);
     worker.stop();
 
     assertTrue(result.containsKey(1L));
