diff --git a/sdks/python/apache_beam/io/avroio.py b/sdks/python/apache_beam/io/avroio.py
index 8de3e5cdbb7..e861c434e0a 100644
--- a/sdks/python/apache_beam/io/avroio.py
+++ b/sdks/python/apache_beam/io/avroio.py
@@ -36,7 +36,7 @@ For example, if schema of the Avro file is the following.
 
 Then records generated by read transforms will be dictionaries of the
 following form.
-{u'name': u'Alyssa', u'favorite_number': 256, u'favorite_color': None}).
+{'name': 'Alyssa', 'favorite_number': 256, 'favorite_color': None}).
 
 Additionally, this module provides a write ``PTransform`` ``WriteToAvro``
 that can be used to write a given ``PCollection`` of Python objects to an
@@ -48,8 +48,6 @@ from __future__ import absolute_import
 
 import io
 import os
-import sys
-import warnings
 import zlib
 from builtins import object
 from functools import partial
@@ -57,6 +55,7 @@ from functools import partial
 import avro
 from avro import io as avroio
 from avro import datafile
+from avro.schema import Parse
 from fastavro.read import block_reader
 from fastavro.write import Writer
 
@@ -68,20 +67,9 @@ from apache_beam.io.filesystem import CompressionTypes
 from apache_beam.io.iobase import Read
 from apache_beam.transforms import PTransform
 
-# pylint: disable=wrong-import-order, wrong-import-position, ungrouped-imports
-try:
-  from avro.schema import Parse  # avro-python3 library for python3
-except ImportError:
-  from avro.schema import parse as Parse  # avro library for python2
-# pylint: enable=wrong-import-order, wrong-import-position, ungrouped-imports
-
 __all__ = ['ReadFromAvro', 'ReadAllFromAvro', 'WriteToAvro']
 
 
-def _use_fastavro():
-  return sys.version_info[0] >= 3
-
-
 class ReadFromAvro(PTransform):
   """A :class:`~apache_beam.transforms.ptransform.PTransform` for reading avro
   files."""
@@ -90,7 +78,7 @@ class ReadFromAvro(PTransform):
       file_pattern=None,
       min_bundle_size=0,
       validate=True,
-      use_fastavro=_use_fastavro()):
+      use_fastavro=True):
     """Initializes :class:`ReadFromAvro`.
 
     Uses source :class:`~apache_beam.io._AvroSource` to read a set of Avro
@@ -147,7 +135,7 @@ class ReadFromAvro(PTransform):
     Then records generated by :class:`~apache_beam.io._AvroSource` will be
     dictionaries of the following form. ::
 
-      {u'name': u'Alyssa', u'favorite_number': 256, u'favorite_color': None}).
+      {'name': 'Alyssa', 'favorite_number': 256, 'favorite_color': None}).
 
     Args:
       file_pattern (str): the file glob to read
@@ -156,13 +144,8 @@ class ReadFromAvro(PTransform):
       validate (bool): flag to verify that the files exist during the pipeline
         creation time.
       use_fastavro (bool); when set, use the `fastavro` library for IO, which
-        is significantly faster, and will likely become the default
+        is significantly faster, and is now the default.
     """
-    if sys.version_info[0] >= 3 and not use_fastavro:
-      warnings.warn(
-          "Due to a known issue in avro-python3 package, it is "
-          "recommended to use fastavro with Beam Avro IO on "
-          "Python 3 until BEAM-6522 is addressed.")
     super(ReadFromAvro, self).__init__()
     self._source = _create_avro_source(
         file_pattern,
@@ -190,7 +173,7 @@ class ReadAllFromAvro(PTransform):
       self,
       min_bundle_size=0,
       desired_bundle_size=DEFAULT_DESIRED_BUNDLE_SIZE,
-      use_fastavro=_use_fastavro(),
+      use_fastavro=True,
       label='ReadAllFiles'):
     """Initializes ``ReadAllFromAvro``.
 
@@ -199,12 +182,9 @@ class ReadAllFromAvro(PTransform):
                        splitting the input into bundles.
       desired_bundle_size: the desired size in bytes, to be considered when
                        splitting the input into bundles.
+      use_fastavro (bool); when set, use the `fastavro` library for IO, which
+        is significantly faster, and is now the default.
     """
-    if sys.version_info[0] >= 3 and not use_fastavro:
-      warnings.warn(
-          "Due to a known issue in avro-python3 package, it is "
-          "recommended to use fastavro with Beam Avro IO on "
-          "Python 3 until BEAM-6522 is addressed.")
     source_from_file = partial(
         _create_avro_source,
         min_bundle_size=min_bundle_size,
@@ -315,10 +295,7 @@ class _AvroUtils(object):
 
 
 def _create_avro_source(
-    file_pattern=None,
-    min_bundle_size=0,
-    validate=False,
-    use_fastavro=_use_fastavro()):
+    file_pattern=None, min_bundle_size=0, validate=False, use_fastavro=True):
   return \
       _FastAvroSource(
           file_pattern=file_pattern,
@@ -502,7 +479,7 @@ class WriteToAvro(beam.transforms.PTransform):
       num_shards=0,
       shard_name_template=None,
       mime_type='application/x-avro',
-      use_fastavro=_use_fastavro()):
+      use_fastavro=True):
     """Initialize a WriteToAvro transform.
 
     Args:
@@ -511,7 +488,9 @@ class WriteToAvro(beam.transforms.PTransform):
         end in a common extension, if given by file_name_suffix. In most cases,
         only this argument is specified and num_shards, shard_name_template, and
         file_name_suffix use default values.
-      schema: The schema to use, as returned by avro.schema.Parse
+      schema: The schema to use (dict).
+        If using with avro-python3 via use_fastavro=False, provide parsed schema
+        as returned by avro.schema.Parse().
       codec: The codec to use for block-level compression. Any string supported
         by the Avro specification is accepted (for example 'null').
       file_name_suffix: Suffix for the files written.
@@ -529,22 +508,21 @@ class WriteToAvro(beam.transforms.PTransform):
         is '-SSSSS-of-NNNNN' if None is passed as the shard_name_template.
       mime_type: The MIME type to use for the produced files, if the filesystem
         supports specifying MIME types.
-      use_fastavro: when set, use the `fastavro` library for IO
+      use_fastavro (bool); when set, use the `fastavro` library for IO, which
+        is significantly faster, and is now the default.
 
     Returns:
       A WriteToAvro transform usable for writing.
     """
-    self._sink = \
-      _create_avro_sink(
-          file_path_prefix,
-          schema,
-          codec,
-          file_name_suffix,
-          num_shards,
-          shard_name_template,
-          mime_type,
-          use_fastavro
-      )
+    self._sink = _create_avro_sink(
+        file_path_prefix,
+        schema,
+        codec,
+        file_name_suffix,
+        num_shards,
+        shard_name_template,
+        mime_type,
+        use_fastavro)
 
   def expand(self, pcoll):
     return pcoll | beam.io.iobase.Write(self._sink)
@@ -562,27 +540,29 @@ def _create_avro_sink(
     shard_name_template,
     mime_type,
     use_fastavro):
-  return \
-      _FastAvroSink(
-          file_path_prefix,
-          schema,
-          codec,
-          file_name_suffix,
-          num_shards,
-          shard_name_template,
-          mime_type
-      ) \
-      if use_fastavro \
-      else \
-      _AvroSink(
-          file_path_prefix,
-          schema,
-          codec,
-          file_name_suffix,
-          num_shards,
-          shard_name_template,
-          mime_type
-      )
+  if use_fastavro:
+    if "class \'avro.schema" in str(type(schema)):
+      raise ValueError(
+          'You are using Avro IO with fastavro (default with Beam on '
+          'Python 3), but supplying a schema parsed by avro-python3. '
+          'Please change the schema to a dict.')
+    return _FastAvroSink(
+        file_path_prefix,
+        schema,
+        codec,
+        file_name_suffix,
+        num_shards,
+        shard_name_template,
+        mime_type)
+  else:
+    return _AvroSink(
+        file_path_prefix,
+        schema,
+        codec,
+        file_name_suffix,
+        num_shards,
+        shard_name_template,
+        mime_type)
 
 
 class _BaseAvroSink(filebasedsink.FileBasedSink):
