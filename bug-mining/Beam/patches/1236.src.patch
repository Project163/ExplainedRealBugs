diff --git a/sdks/python/apache_beam/transforms/stats_test.py b/sdks/python/apache_beam/transforms/stats_test.py
index d8760a86060..2cdf976b44b 100644
--- a/sdks/python/apache_beam/transforms/stats_test.py
+++ b/sdks/python/apache_beam/transforms/stats_test.py
@@ -25,7 +25,8 @@ import sys
 import unittest
 from collections import defaultdict
 
-import numpy as np
+from tenacity import retry
+from tenacity import stop_after_attempt
 
 import apache_beam as beam
 from apache_beam.testing.test_pipeline import TestPipeline
@@ -34,7 +35,10 @@ from apache_beam.testing.util import equal_to
 
 
 class ApproximateUniqueTest(unittest.TestCase):
-  """Unit tests for ApproximateUnique.Globally and ApproximateUnique.PerKey."""
+  """Unit tests for ApproximateUnique.Globally and ApproximateUnique.PerKey.
+  Hash() with Python3 is nondeterministic, so Approximation algorithm generates
+  different result each time and sometimes error rate is out of range, so add
+  retries for all tests who actually running approximation algorithm."""
 
   def test_approximate_unique_global_by_invalid_size(self):
     # test if the transformation throws an error as expected with an invalid
@@ -154,13 +158,15 @@ class ApproximateUniqueTest(unittest.TestCase):
 
   @unittest.skipIf(sys.version_info < (3, 0, 0),
                    'Skip with py27 because hash function is not good enough.')
+  @retry(reraise=True, stop=stop_after_attempt(3))
   def test_approximate_unique_global_by_sample_size(self):
     # test if estimation error with a given sample size is not greater than
     # expected max error (sample size = 50% of population).
-    sample_size = 50
+    sample_size = 20
     max_err = 2 / math.sqrt(sample_size)
-    random.seed(1)
-    test_input = [random.randint(0, 1000) for _ in range(100)]
+    test_input = [4, 34, 30, 46, 80, 66, 51, 81, 31, 9, 26, 36, 12, 41, 90, 35,
+                  33, 19, 88, 86, 28, 93, 38, 76, 15, 87, 12, 39, 84, 13, 32,
+                  49, 65, 88, 16, 27, 31, 30, 96, 54]
     actual_count = len(set(test_input))
 
     pipeline = TestPipeline()
@@ -176,6 +182,7 @@ class ApproximateUniqueTest(unittest.TestCase):
                 label='assert:global_by_size')
     pipeline.run()
 
+  @retry(reraise=True, stop=stop_after_attempt(3))
   def test_approximate_unique_global_by_sample_size_with_duplicates(self):
     # test if estimation error with a given sample size is not greater than
     # expected max error with duplicated input.
@@ -197,11 +204,14 @@ class ApproximateUniqueTest(unittest.TestCase):
                 label='assert:global_by_size_with_duplicates')
     pipeline.run()
 
+  @retry(reraise=True, stop=stop_after_attempt(3))
   def test_approximate_unique_global_by_sample_size_with_small_population(self):
     # test if estimation is exactly same to actual value when sample size is
     # not smaller than population size (sample size > 100% of population).
     sample_size = 31
-    test_input = [random.randint(0, 1000) for _ in range(30)]
+    test_input = [144, 160, 229, 923, 390, 756, 674, 769, 145, 888,
+                  809, 159, 222, 101, 943, 901, 876, 194, 232, 631,
+                  221, 829, 965, 729, 35, 33, 115, 894, 827, 364]
     actual_count = len(set(test_input))
 
     pipeline = TestPipeline()
@@ -216,11 +226,13 @@ class ApproximateUniqueTest(unittest.TestCase):
 
   @unittest.skipIf(sys.version_info < (3, 0, 0),
                    'Skip with py27 because hash function is not good enough.')
+  @retry(reraise=True, stop=stop_after_attempt(3))
   def test_approximate_unique_global_by_error(self):
     # test if estimation error from input error is not greater than input error.
     est_err = 0.3
-    random.seed(1)
-    test_input = [random.randint(0, 1000) for _ in range(100)]
+    test_input = [291, 371, 271, 126, 762, 391, 222, 565, 428, 786,
+                  801, 867, 337, 690, 261, 436, 311, 568, 946, 722,
+                  973, 386, 506, 546, 991, 450, 226, 889, 514, 693]
     actual_count = len(set(test_input))
 
     pipeline = TestPipeline()
@@ -235,13 +247,15 @@ class ApproximateUniqueTest(unittest.TestCase):
     assert_that(result, equal_to([True]), label='assert:global_by_error')
     pipeline.run()
 
+  @retry(reraise=True, stop=stop_after_attempt(3))
   def test_approximate_unique_global_by_error_with_small_population(self):
     # test if estimation error from input error of a small dataset is not
     # greater than input error. Sample size is always not smaller than 16, so
     # when population size is smaller than 16, estimation should be exactly
     # same to actual value.
     est_err = 0.01
-    test_input = [random.randint(0, 1000) for _ in range(15)]
+    test_input = [585, 104, 613, 503, 658, 640, 118, 492, 189, 798,
+                  756, 755, 839, 79, 393]
     actual_count = len(set(test_input))
 
     pipeline = TestPipeline()
@@ -254,13 +268,18 @@ class ApproximateUniqueTest(unittest.TestCase):
                 label='assert:global_by_error_with_small_population')
     pipeline.run()
 
+  @retry(reraise=True, stop=stop_after_attempt(3))
   def test_approximate_unique_perkey_by_size(self):
     # test if est error per key from sample size is in a expected range.
-    sample_size = 50
+    sample_size = 20
     max_err = 2 / math.sqrt(sample_size)
-    number_of_keys = 10
-    test_input = [(random.randint(1, number_of_keys), random.randint(0, 1000))
-                  for _ in range(100)]
+    test_input = [(8, 73), (6, 724), (7, 70), (1, 576), (10, 120), (2, 662),
+                  (7, 115), (3, 731), (6, 340), (6, 623), (1, 74), (9, 280),
+                  (8, 298), (6, 440), (10, 243), (1, 125), (9, 754), (8, 833),
+                  (9, 751), (4, 818), (6, 176), (9, 253), (2, 721), (8, 936),
+                  (3, 691), (10, 685), (1, 69), (3, 155), (8, 86), (5, 693),
+                  (2, 809), (4, 723), (8, 102), (9, 707), (8, 558), (4, 537),
+                  (5, 371), (7, 432), (2, 51), (10, 397)]
     actual_count_dict = defaultdict(set)
     for (x, y) in test_input:
       actual_count_dict[x].add(y)
@@ -276,16 +295,17 @@ class ApproximateUniqueTest(unittest.TestCase):
                                          * 1.0 / len(actual_count_dict[x[0]])
                                          <= max_err]))
 
-    assert_that(result, equal_to([True] * number_of_keys),
+    assert_that(result, equal_to([True] * len(actual_count_dict)),
                 label='assert:perkey_by_size')
     pipeline.run()
 
+  @retry(reraise=True, stop=stop_after_attempt(3))
   def test_approximate_unique_perkey_by_error(self):
     # test if estimation error per key from input err is in the expected range.
     est_err = 0.01
-    number_of_keys = 10
-    test_input = [(random.randint(1, number_of_keys), random.randint(0, 1000))
-                  for _ in range(100)]
+    test_input = [(9, 6), (5, 5), (6, 9), (2, 4), (8, 3), (9, 0), (6, 10),
+                  (8, 8), (9, 7), (2, 0), (9, 2), (1, 3), (4, 0), (7, 6),
+                  (10, 6), (4, 7), (5, 8), (7, 2), (7, 10), (5, 10)]
     actual_count_dict = defaultdict(set)
     for (x, y) in test_input:
       actual_count_dict[x].add(y)
@@ -301,26 +321,16 @@ class ApproximateUniqueTest(unittest.TestCase):
                                          * 1.0 / len(actual_count_dict[x[0]])
                                          <= est_err]))
 
-    assert_that(result, equal_to([True] * number_of_keys),
+    assert_that(result, equal_to([True] * len(actual_count_dict)),
                 label='assert:perkey_by_error')
     pipeline.run()
 
+  @retry(reraise=True, stop=stop_after_attempt(3))
   def test_approximate_unique_globally_by_error_with_skewed_data(self):
     # test if estimation error is within the expected range with skewed data.
     est_err = 0.01
-
-    # generate skewed dataset
-    values = [i for i in range(200)]
-    probs = [1.0 / 200] * 200
-
-    for idx, _ in enumerate(probs):
-      if idx > 3 and idx < 20:
-        probs[idx] = probs[idx] * (1 + math.log(idx + 1))
-      if idx > 20 and idx < 40:
-        probs[idx] = probs[idx] * (1 + math.log((40 - idx) + 1))
-
-    probs = [p / sum(probs) for p in probs]
-    test_input = np.random.choice(values, 1000, p=probs)
+    test_input = [19, 21, 32, 29, 5, 31, 52, 50, 59, 80, 7, 3, 34, 19, 13,
+                  6, 55, 1, 13, 90, 4, 18, 52, 33, 0, 77, 21, 26, 5, 18]
     actual_count = len(set(test_input))
 
     pipeline = TestPipeline()
