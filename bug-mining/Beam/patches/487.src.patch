diff --git a/sdks/python/apache_beam/io/filesystemio.py b/sdks/python/apache_beam/io/filesystemio.py
index 05be5587a65..35e141bb756 100644
--- a/sdks/python/apache_beam/io/filesystemio.py
+++ b/sdks/python/apache_beam/io/filesystemio.py
@@ -158,7 +158,7 @@ class DownloaderStream(io.RawIOBase):
 
 
 class UploaderStream(io.RawIOBase):
-  """Provides a stream interface for Downloader objects."""
+  """Provides a stream interface for Uploader objects."""
 
   def __init__(self, uploader, mode='w'):
     """Initializes the stream.
@@ -171,6 +171,9 @@ class UploaderStream(io.RawIOBase):
     self.mode = mode
     self._position = 0
 
+  def tell(self):
+    return self._position
+
   def write(self, b):
     """Write bytes from b.
 
diff --git a/sdks/python/apache_beam/io/filesystems.py b/sdks/python/apache_beam/io/filesystems.py
index dad4e5f9f27..6150907631d 100644
--- a/sdks/python/apache_beam/io/filesystems.py
+++ b/sdks/python/apache_beam/io/filesystems.py
@@ -51,7 +51,7 @@ class FileSystems(object):
     Args:
       pipeline_options: Instance of ``PipelineOptions``.
     """
-    cls._options = pipeline_options
+    cls._pipeline_options = pipeline_options
 
   @staticmethod
   def get_scheme(path):
diff --git a/sdks/python/apache_beam/io/gcp/gcsio.py b/sdks/python/apache_beam/io/gcp/gcsio.py
index e38f9ff30be..3bdf2e64ca2 100644
--- a/sdks/python/apache_beam/io/gcp/gcsio.py
+++ b/sdks/python/apache_beam/io/gcp/gcsio.py
@@ -516,7 +516,6 @@ class GcsUploader(Uploader):
     self._path = path
     self._bucket, self._name = parse_gcs_path(path)
     self._mime_type = mime_type
-    self._last_error = None
 
     # Set up communication with child thread.
     parent_conn, child_conn = multiprocessing.Pipe()
diff --git a/sdks/python/apache_beam/io/hadoopfilesystem.py b/sdks/python/apache_beam/io/hadoopfilesystem.py
index a761068f413..054d56df643 100644
--- a/sdks/python/apache_beam/io/hadoopfilesystem.py
+++ b/sdks/python/apache_beam/io/hadoopfilesystem.py
@@ -20,28 +20,72 @@ Hadoop Distributed File System files."""
 
 from __future__ import absolute_import
 
+import io
 import logging
 import posixpath
 import re
 
-from hdfs3 import HDFileSystem
+import hdfs
 
+from apache_beam.io import filesystemio
 from apache_beam.io.filesystem import BeamIOError
 from apache_beam.io.filesystem import CompressedFile
 from apache_beam.io.filesystem import CompressionTypes
 from apache_beam.io.filesystem import FileMetadata
 from apache_beam.io.filesystem import FileSystem
 from apache_beam.io.filesystem import MatchResult
+from apache_beam.options.pipeline_options import HadoopFileSystemOptions
 
 __all__ = ['HadoopFileSystem']
 
 _HDFS_PREFIX = 'hdfs:/'
 _URL_RE = re.compile(r'^' + _HDFS_PREFIX + r'(/.*)')
 _COPY_BUFFER_SIZE = 2 ** 16
+_DEFAULT_BUFFER_SIZE = 20 * 1024 * 1024
 
+# WebHDFS FileStatus property constants.
+_FILE_STATUS_NAME = 'name'
+_FILE_STATUS_PATH_SUFFIX = 'pathSuffix'
+_FILE_STATUS_TYPE = 'type'
+_FILE_STATUS_TYPE_DIRECTORY = 'DIRECTORY'
+_FILE_STATUS_TYPE_FILE = 'FILE'
+_FILE_STATUS_SIZE = 'size'
 
-# TODO(udim): Add @retry.with_exponential_backoff to some functions, like in
-# gcsio.py.
+
+class HdfsDownloader(filesystemio.Downloader):
+
+  def __init__(self, hdfs_client, path):
+    self._hdfs_client = hdfs_client
+    self._path = path
+    self._size = self._hdfs_client.status(path)[_FILE_STATUS_SIZE]
+
+  @property
+  def size(self):
+    return self._size
+
+  def get_range(self, start, end):
+    with self._hdfs_client.read(
+        self._path, offset=start, length=end - start + 1) as reader:
+      return reader.read()
+
+
+class HdfsUploader(filesystemio.Uploader):
+
+  def __init__(self, hdfs_client, path):
+    self._hdfs_client = hdfs_client
+    if self._hdfs_client.status(path, strict=False) is not None:
+      raise BeamIOError('Path already exists: %s' % path)
+
+    self._handle_context = self._hdfs_client.write(path)
+    self._handle = self._handle_context.__enter__()
+
+  def put(self, data):
+    self._handle.write(data)
+
+  def finish(self):
+    self._handle.__exit__(None, None, None)
+    self._handle = None
+    self._handle_context = None
 
 
 class HadoopFileSystem(FileSystem):
@@ -49,16 +93,31 @@ class HadoopFileSystem(FileSystem):
 
   URL arguments to methods expect strings starting with ``hdfs://``.
 
-  Uses client library :class:`hdfs3.core.HDFileSystem`.
+  Experimental; TODO(BEAM-3600): Writes are experimental until file rename
+    retries are better handled.
   """
 
   def __init__(self, pipeline_options):
     """Initializes a connection to HDFS.
 
-    Connection configuration is done using :doc:`hdfs`.
+    Connection configuration is done by passing pipeline options.
+    See :class:`~apache_beam.options.pipeline_options.HadoopFileSystemOptions`.
     """
     super(HadoopFileSystem, self).__init__(pipeline_options)
-    self._hdfs_client = HDFileSystem()
+
+    if pipeline_options is None:
+      raise ValueError('pipeline_options is not set')
+    hdfs_options = pipeline_options.view_as(HadoopFileSystemOptions)
+    if hdfs_options.hdfs_host is None:
+      raise ValueError('hdfs_host is not set')
+    if hdfs_options.hdfs_port is None:
+      raise ValueError('hdfs_port is not set')
+    if hdfs_options.hdfs_user is None:
+      raise ValueError('hdfs_user is not set')
+    self._hdfs_client = hdfs.InsecureClient(
+        'http://%s:%s' % (
+            hdfs_options.hdfs_host, str(hdfs_options.hdfs_port)),
+        user=hdfs_options.hdfs_user)
 
   @classmethod
   def scheme(cls):
@@ -108,7 +167,7 @@ class HadoopFileSystem(FileSystem):
   def mkdirs(self, url):
     path = self._parse_url(url)
     if self._exists(path):
-      raise IOError('Path already exists: %s' % path)
+      raise BeamIOError('Path already exists: %s' % path)
     return self._mkdirs(path)
 
   def _mkdirs(self, path):
@@ -123,12 +182,17 @@ class HadoopFileSystem(FileSystem):
           'Patterns and limits should be equal in length: %d != %d' % (
               len(url_patterns), len(limits)))
 
-    # TODO(udim): Update client to allow batched results.
     def _match(path_pattern, limit):
       """Find all matching paths to the pattern provided."""
-      file_infos = self._hdfs_client.ls(path_pattern, detail=True)[:limit]
-      metadata_list = [FileMetadata(file_info['name'], file_info['size'])
-                       for file_info in file_infos]
+      fs = self._hdfs_client.status(path_pattern, strict=False)
+      if fs and fs[_FILE_STATUS_TYPE] == _FILE_STATUS_TYPE_FILE:
+        file_statuses = [(fs[_FILE_STATUS_PATH_SUFFIX], fs)][:limit]
+      else:
+        file_statuses = self._hdfs_client.list(path_pattern,
+                                               status=True)[:limit]
+      metadata_list = [FileMetadata(file_status[1][_FILE_STATUS_NAME],
+                                    file_status[1][_FILE_STATUS_SIZE])
+                       for file_status in file_statuses]
       return MatchResult(path_pattern, metadata_list)
 
     exceptions = {}
@@ -144,46 +208,55 @@ class HadoopFileSystem(FileSystem):
       raise BeamIOError('Match operation failed', exceptions)
     return result
 
-  def _open_hdfs(self, path, mode, mime_type, compression_type):
+  @staticmethod
+  def _add_compression(stream, path, mime_type, compression_type):
     if mime_type != 'application/octet-stream':
       logging.warning('Mime types are not supported. Got non-default mime_type:'
                       ' %s', mime_type)
     if compression_type == CompressionTypes.AUTO:
       compression_type = CompressionTypes.detect_compression_type(path)
-    res = self._hdfs_client.open(path, mode)
     if compression_type != CompressionTypes.UNCOMPRESSED:
-      res = CompressedFile(res)
-    return res
+      return CompressedFile(stream)
+
+    return stream
 
   def create(self, url, mime_type='application/octet-stream',
              compression_type=CompressionTypes.AUTO):
     """
     Returns:
-      *hdfs3.core.HDFile*: An Python File-like object.
+      A Python File-like object.
     """
     path = self._parse_url(url)
     return self._create(path, mime_type, compression_type)
 
   def _create(self, path, mime_type='application/octet-stream',
               compression_type=CompressionTypes.AUTO):
-    return self._open_hdfs(path, 'wb', mime_type, compression_type)
+    stream = io.BufferedWriter(
+        filesystemio.UploaderStream(
+            HdfsUploader(self._hdfs_client, path)),
+        buffer_size=_DEFAULT_BUFFER_SIZE)
+    return self._add_compression(stream, path, mime_type, compression_type)
 
   def open(self, url, mime_type='application/octet-stream',
            compression_type=CompressionTypes.AUTO):
     """
     Returns:
-      *hdfs3.core.HDFile*: An Python File-like object.
+      A Python File-like object.
     """
     path = self._parse_url(url)
     return self._open(path, mime_type, compression_type)
 
   def _open(self, path, mime_type='application/octet-stream',
             compression_type=CompressionTypes.AUTO):
-    return self._open_hdfs(path, 'rb', mime_type, compression_type)
+    stream = io.BufferedReader(
+        filesystemio.DownloaderStream(
+            HdfsDownloader(self._hdfs_client, path)),
+        buffer_size=_DEFAULT_BUFFER_SIZE)
+    return self._add_compression(stream, path, mime_type, compression_type)
 
   def copy(self, source_file_names, destination_file_names):
     """
-    Will overwrite files and directories in destination_file_names.
+    It is an error if any file to copy already exists at the destination.
 
     Raises ``BeamIOError`` if any error occurred.
 
@@ -208,7 +281,8 @@ class HadoopFileSystem(FileSystem):
 
     def _copy_path(source, destination):
       """Recursively copy the file tree from the source to the destination."""
-      if not self._hdfs_client.isdir(source):
+      if self._hdfs_client.status(
+          source)[_FILE_STATUS_TYPE] != _FILE_STATUS_TYPE_DIRECTORY:
         _copy_file(source, destination)
         return
 
@@ -243,9 +317,11 @@ class HadoopFileSystem(FileSystem):
       try:
         rel_source = self._parse_url(source)
         rel_destination = self._parse_url(destination)
-        if not self._hdfs_client.mv(rel_source, rel_destination):
+        try:
+          self._hdfs_client.rename(rel_source, rel_destination)
+        except hdfs.HdfsError as e:
           raise BeamIOError(
-              'libhdfs error in renaming %s to %s' % (source, destination))
+              'libhdfs error in renaming %s to %s' % (source, destination), e)
       except Exception as e:  # pylint: disable=broad-except
         exceptions[(source, destination)] = e
 
@@ -270,14 +346,14 @@ class HadoopFileSystem(FileSystem):
     Args:
       path: String in the form /...
     """
-    return self._hdfs_client.exists(path)
+    return self._hdfs_client.status(path, strict=False) is not None
 
   def delete(self, urls):
     exceptions = {}
     for url in urls:
       try:
         path = self._parse_url(url)
-        self._hdfs_client.rm(path, recursive=True)
+        self._hdfs_client.delete(path, recursive=True)
       except Exception as e:  # pylint: disable=broad-except
         exceptions[url] = e
 
diff --git a/sdks/python/apache_beam/io/hadoopfilesystem_test.py b/sdks/python/apache_beam/io/hadoopfilesystem_test.py
index 8a1c0f1b3d2..2ba1da26468 100644
--- a/sdks/python/apache_beam/io/hadoopfilesystem_test.py
+++ b/sdks/python/apache_beam/io/hadoopfilesystem_test.py
@@ -19,23 +19,26 @@
 
 from __future__ import absolute_import
 
+import io
 import posixpath
-import StringIO
 import unittest
 
-from apache_beam.io import hadoopfilesystem
+from apache_beam.io import hadoopfilesystem as hdfs
 from apache_beam.io.filesystem import BeamIOError
+from apache_beam.options.pipeline_options import HadoopFileSystemOptions
 from apache_beam.options.pipeline_options import PipelineOptions
 
 
-class FakeFile(StringIO.StringIO):
+class FakeFile(io.BytesIO):
   """File object for FakeHdfs"""
 
-  def __init__(self, path, mode):
-    StringIO.StringIO.__init__(self)
+  def __init__(self, path, mode='', type='FILE'):
+    io.BytesIO.__init__(self)
+
     self.stat = {
         'path': path,
         'mode': mode,
+        'type': type,
     }
     self.saved_data = None
 
@@ -44,7 +47,7 @@ class FakeFile(StringIO.StringIO):
 
   def close(self):
     self.saved_data = self.getvalue()
-    StringIO.StringIO.close(self)
+    io.BytesIO.close(self)
 
   def __enter__(self):
     return self
@@ -60,74 +63,87 @@ class FakeFile(StringIO.StringIO):
       return len(self.saved_data)
     return len(self.getvalue())
 
+  def get_file_status(self):
+    """Returns a partial WebHDFS FileStatus object."""
+    return {
+        hdfs._FILE_STATUS_NAME: self.stat['path'],
+        hdfs._FILE_STATUS_PATH_SUFFIX: posixpath.basename(self.stat['path']),
+        hdfs._FILE_STATUS_SIZE: self.size,
+        hdfs._FILE_STATUS_TYPE: self.stat['type'],
+    }
+
 
 class FakeHdfsError(Exception):
   """Generic error for FakeHdfs methods."""
 
 
 class FakeHdfs(object):
-  """Fake implementation of hdfs3.HadoopFileSystem."""
+  """Fake implementation of ``hdfs.Client``."""
 
   def __init__(self):
     self.files = {}
 
-  def open(self, path, mode='rb'):
-    if mode == 'rb' and not self.exists(path):
+  def write(self, path):
+    if self.status(path, strict=False) is not None:
+      raise FakeHdfsError('Path already exists: %s' % path)
+
+    new_file = FakeFile(path, 'wb')
+    self.files[path] = new_file
+    return new_file
+
+  def read(self, path, offset=0, length=None):
+    old_file = self.files.get(path, None)
+    if old_file is None:
       raise FakeHdfsError('Path not found: %s' % path)
+    if old_file.stat['type'] == 'DIRECTORY':
+      raise FakeHdfsError('Cannot open a directory: %s' % path)
+    if not old_file.closed:
+      raise FakeHdfsError('File already opened: %s' % path)
+
+    # old_file is closed and can't be operated upon. Return a copy instead.
+    new_file = FakeFile(path, 'rb')
+    if old_file.saved_data:
+      new_file.write(old_file.saved_data)
+      new_file.seek(0)
+    return new_file
+
+  def list(self, path, status=False):
+    if not status:
+      raise ValueError('status must be True')
+    fs = self.status(path, strict=False)
+    if (fs is not None and
+        fs[hdfs._FILE_STATUS_TYPE] == hdfs._FILE_STATUS_TYPE_FILE):
+      raise ValueError('list must be called on a directory, got file: %s', path)
 
-    if mode in ['rb', 'wb']:
-      new_file = FakeFile(path, mode)
-      # Required to support read and write operations with CompressedFile.
-      new_file.mode = 'rw'
-
-      if mode == 'rb':
-        old_file = self.files.get(path, None)
-        if old_file is not None:
-          if old_file.stat['mode'] == 'dir':
-            raise FakeHdfsError('Cannot open a directory: %s' % path)
-          if old_file.saved_data:
-            old_file = self.files[path]
-            new_file.write(old_file.saved_data)
-            new_file.seek(0)
-
-      self.files[path] = new_file
-      return new_file
-    else:
-      raise FakeHdfsError('Unknown mode: %s' % mode)
-
-  def ls(self, path, detail=False):
     result = []
     for file in self.files.itervalues():
       if file.stat['path'].startswith(path):
-        result.append({
-            'name': file.stat['path'],
-            'size': file.size,
-        })
+        fs = file.get_file_status()
+        result.append((fs[hdfs._FILE_STATUS_PATH_SUFFIX], fs))
     return result
 
   def makedirs(self, path):
-    self.files[path] = FakeFile(path, 'dir')
-
-  def exists(self, path):
-    return path in self.files
-
-  def rm(self, path, recursive=True):
+    self.files[path] = FakeFile(path, type='DIRECTORY')
+
+  def status(self, path, strict=True):
+    f = self.files.get(path)
+    if f is None:
+      if strict:
+        raise FakeHdfsError('Path not found: %s' % path)
+      else:
+        return f
+    return f.get_file_status()
+
+  def delete(self, path, recursive=True):
     if not recursive:
       raise FakeHdfsError('Non-recursive mode not implemented')
 
-    if not self.exists(path):
-      raise FakeHdfsError('Path not found: %s' % path)
+    _ = self.status(path)
 
     for filepath in self.files.keys():  # pylint: disable=consider-iterating-dictionary
       if filepath.startswith(path):
         del self.files[filepath]
 
-  def isdir(self, path):
-    if not self.exists(path):
-      raise FakeHdfsError('Path not found: %s' % path)
-
-    return self.files[path].stat['mode'] == 'dir'
-
   def walk(self, path):
     paths = [path]
     while paths:
@@ -139,7 +155,7 @@ class FakeHdfs(object):
           continue
         short_path = posixpath.relpath(full_path, path)
         if '/' not in short_path:
-          if self.isdir(full_path):
+          if self.status(full_path)[hdfs._FILE_STATUS_TYPE] == 'DIRECTORY':
             if short_path != '.':
               dirs.append(short_path)
           else:
@@ -148,8 +164,8 @@ class FakeHdfs(object):
       yield path, dirs, files
       paths = [posixpath.join(path, dir) for dir in dirs]
 
-  def mv(self, path1, path2):
-    if not self.exists(path1):
+  def rename(self, path1, path2):
+    if self.status(path1, strict=False) is None:
       raise FakeHdfsError('Path1 not found: %s' % path1)
 
     for fullpath in self.files.keys():  # pylint: disable=consider-iterating-dictionary
@@ -159,16 +175,20 @@ class FakeHdfs(object):
         f.stat['path'] = newpath
         self.files[newpath] = f
 
-    return True
-
 
 class HadoopFileSystemTest(unittest.TestCase):
 
   def setUp(self):
     self._fake_hdfs = FakeHdfs()
-    hadoopfilesystem.HDFileSystem = lambda *args, **kwargs: self._fake_hdfs
+    hdfs.hdfs.InsecureClient = (
+        lambda *args, **kwargs: self._fake_hdfs)
     pipeline_options = PipelineOptions()
-    self.fs = hadoopfilesystem.HadoopFileSystem(pipeline_options)
+    hdfs_options = pipeline_options.view_as(HadoopFileSystemOptions)
+    hdfs_options.hdfs_host = ''
+    hdfs_options.hdfs_port = 0
+    hdfs_options.hdfs_user = ''
+
+    self.fs = hdfs.HadoopFileSystem(pipeline_options)
     self.tmpdir = 'hdfs://test_dir'
 
     for filename in ['old_file1', 'old_file2']:
@@ -177,7 +197,7 @@ class HadoopFileSystemTest(unittest.TestCase):
 
   def test_scheme(self):
     self.assertEqual(self.fs.scheme(), 'hdfs')
-    self.assertEqual(hadoopfilesystem.HadoopFileSystem.scheme(), 'hdfs')
+    self.assertEqual(hdfs.HadoopFileSystem.scheme(), 'hdfs')
 
   def test_url_join(self):
     self.assertEqual('hdfs://tmp/path/to/file',
@@ -303,7 +323,9 @@ class HadoopFileSystemTest(unittest.TestCase):
   def test_open(self):
     url = self.fs.join(self.tmpdir, 'old_file1')
     handle = self.fs.open(url)
-    self.assertEqual(handle, self._fake_hdfs.files[self.fs._parse_url(url)])
+    expected_data = ''
+    data = handle.read()
+    self.assertEqual(data, expected_data)
 
   def test_open_bad_path(self):
     with self.assertRaises(FakeHdfsError):
@@ -326,15 +348,16 @@ class HadoopFileSystemTest(unittest.TestCase):
     self.assertTrue(self._cmpfiles(url1, url2))
     self.assertTrue(self._cmpfiles(url1, url3))
 
-  def test_copy_file_overwrite(self):
+  def test_copy_file_overwrite_error(self):
     url1 = self.fs.join(self.tmpdir, 'new_file1')
     url2 = self.fs.join(self.tmpdir, 'new_file2')
     with self.fs.create(url1) as f1:
       f1.write('Hello')
     with self.fs.create(url2) as f2:
       f2.write('nope')
-    self.fs.copy([url1], [url2])
-    self.assertTrue(self._cmpfiles(url1, url2))
+    with self.assertRaisesRegexp(
+        BeamIOError, r'already exists.*%s' % posixpath.basename(url2)):
+      self.fs.copy([url1], [url2])
 
   def test_copy_file_error(self):
     url1 = self.fs.join(self.tmpdir, 'new_file1')
@@ -366,7 +389,7 @@ class HadoopFileSystemTest(unittest.TestCase):
     self.fs.copy([url_t1], [url_t2])
     self.assertTrue(self._cmpfiles(url1, url2))
 
-  def test_copy_directory_overwrite(self):
+  def test_copy_directory_overwrite_error(self):
     url_t1 = self.fs.join(self.tmpdir, 't1')
     url_t1_inner = self.fs.join(self.tmpdir, 't1/inner')
     url_t2 = self.fs.join(self.tmpdir, 't2')
@@ -379,7 +402,7 @@ class HadoopFileSystemTest(unittest.TestCase):
     url1 = self.fs.join(url_t1, 'f1')
     url1_inner = self.fs.join(url_t1_inner, 'f2')
     url2 = self.fs.join(url_t2, 'f1')
-    url2_inner = self.fs.join(url_t2_inner, 'f2')
+    unused_url2_inner = self.fs.join(url_t2_inner, 'f2')
     url3_inner = self.fs.join(url_t2_inner, 'f3')
     for url in [url1, url1_inner, url3_inner]:
       with self.fs.create(url) as f:
@@ -387,10 +410,8 @@ class HadoopFileSystemTest(unittest.TestCase):
     with self.fs.create(url2) as f:
       f.write('nope')
 
-    self.fs.copy([url_t1], [url_t2])
-    self.assertTrue(self._cmpfiles(url1, url2))
-    self.assertTrue(self._cmpfiles(url1_inner, url2_inner))
-    self.assertTrue(self.fs.exists(url3_inner))
+    with self.assertRaisesRegexp(BeamIOError, r'already exists'):
+      self.fs.copy([url_t1], [url_t2])
 
   def test_rename_file(self):
     url1 = self.fs.join(self.tmpdir, 'f1')
diff --git a/sdks/python/apache_beam/options/pipeline_options.py b/sdks/python/apache_beam/options/pipeline_options.py
index aaac9a4fd99..7a2cd4bf1e4 100644
--- a/sdks/python/apache_beam/options/pipeline_options.py
+++ b/sdks/python/apache_beam/options/pipeline_options.py
@@ -30,6 +30,7 @@ __all__ = [
     'TypeOptions',
     'DirectOptions',
     'GoogleCloudOptions',
+    'HadoopFileSystemOptions',
     'WorkerOptions',
     'DebugOptions',
     'ProfilingOptions',
@@ -392,6 +393,33 @@ class GoogleCloudOptions(PipelineOptions):
     return errors
 
 
+class HadoopFileSystemOptions(PipelineOptions):
+  """``HadoopFileSystem`` connection options."""
+
+  @classmethod
+  def _add_argparse_args(cls, parser):
+    parser.add_argument(
+        '--hdfs_host',
+        default=None,
+        help=
+        ('Hostname or address of the HDFS namenode.'))
+    parser.add_argument(
+        '--hdfs_port',
+        default=None,
+        help=
+        ('Port of the HDFS namenode.'))
+    parser.add_argument(
+        '--hdfs_user',
+        default=None,
+        help=
+        ('HDFS username to use.'))
+
+  def validate(self, validator):
+    errors = []
+    errors.extend(validator.validate_optional_argument_positive(self, 'port'))
+    return errors
+
+
 # Command line options controlling the worker pool configuration.
 # TODO(silviuc): Update description when autoscaling options are in.
 class WorkerOptions(PipelineOptions):
diff --git a/sdks/python/generate_pydoc.sh b/sdks/python/generate_pydoc.sh
index b4ec96990c9..1a5f4d3ba1a 100755
--- a/sdks/python/generate_pydoc.sh
+++ b/sdks/python/generate_pydoc.sh
@@ -100,7 +100,6 @@ import apache_beam as beam
 intersphinx_mapping = {
   'python': ('https://docs.python.org/2', None),
   'hamcrest': ('https://pyhamcrest.readthedocs.io/en/latest/', None),
-  'hdfs3': ('https://hdfs3.readthedocs.io/en/latest/', None),
 }
 
 # Since private classes are skipped by sphinx, if there is any cross reference
diff --git a/sdks/python/setup.py b/sdks/python/setup.py
index 722dd2c4234..a069237e22b 100644
--- a/sdks/python/setup.py
+++ b/sdks/python/setup.py
@@ -100,6 +100,7 @@ REQUIRED_PACKAGES = [
     'crcmod>=1.7,<2.0',
     'dill==0.2.6',
     'grpcio>=1.0,<2',
+    'hdfs>=2.1.0,<3.0.0',
     'httplib2>=0.8,<0.10',
     'mock>=1.0.1,<3.0.0',
     'oauth2client>=2.0.1,<5',
@@ -110,7 +111,6 @@ REQUIRED_PACKAGES = [
     'six>=1.9,<1.12',
     'typing>=3.6.0,<3.7.0',
     'futures>=3.1.1,<4.0.0',
-    'hdfs3>=0.3.0,<0.4.0',
     ]
 
 REQUIRED_SETUP_PACKAGES = [
