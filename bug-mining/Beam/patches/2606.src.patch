diff --git a/CHANGES.md b/CHANGES.md
index e39709a9f36..3ba827e2f6e 100644
--- a/CHANGES.md
+++ b/CHANGES.md
@@ -90,6 +90,8 @@
 * Fixed X (Java/Python) ([BEAM-X](https://issues.apache.org/jira/browse/BEAM-X)).
 * The Go SDK now properly maps main input windows to side input windows by
   default ([BEAM-11087](https://issues.apache.org/jira/browse/BEAM-11087)).
+* Fixed data loss when writing to DynamoDB without setting deduplication key names (Java)
+  ([BEAM-13009](https://issues.apache.org/jira/browse/BEAM-13009)).
 
 ## Known Issues
 
diff --git a/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIO.java b/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIO.java
index 9f78a87057f..b11eca7f9f4 100644
--- a/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIO.java
+++ b/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIO.java
@@ -98,10 +98,12 @@ import org.slf4j.LoggerFactory;
  *       writeRequest>
  * </ul>
  *
- * If primary keys could repeat in your stream (i.e. an upsert stream), you could encounter a
- * ValidationError, as AWS does not allow writing duplicate keys within a single batch operation.
- * For such use cases, you can explicitly set the key names corresponding to the primary key to be
- * deduplicated using the withDeduplicateKeys method
+ * <b>Note:</b> AWS does not allow writing duplicate keys within a single batch operation. If
+ * primary keys possibly repeat in your stream (i.e. an upsert stream), you may encounter a
+ * `ValidationError`. To address this you have to provide the key names corresponding to your
+ * primary key using {@link Write#withDeduplicateKeys(List)}. Based on these keys only the last
+ * observed element is kept. Nevertheless, if no deduplication keys are provided, identical elements
+ * are still deduplicated.
  *
  * <h3>Reading from DynamoDB</h3>
  *
@@ -294,6 +296,8 @@ public final class DynamoDBIO {
    */
   @AutoValue
   public abstract static class RetryConfiguration implements Serializable {
+    private static final Duration DEFAULT_INITIAL_DURATION = Duration.standardSeconds(5);
+
     @VisibleForTesting
     static final RetryPredicate DEFAULT_RETRY_PREDICATE = new DefaultRetryPredicate();
 
@@ -301,18 +305,30 @@ public final class DynamoDBIO {
 
     abstract Duration getMaxDuration();
 
+    abstract Duration getInitialDuration();
+
     abstract DynamoDBIO.RetryConfiguration.RetryPredicate getRetryPredicate();
 
     abstract DynamoDBIO.RetryConfiguration.Builder builder();
 
     public static DynamoDBIO.RetryConfiguration create(int maxAttempts, Duration maxDuration) {
+      return create(maxAttempts, maxDuration, DEFAULT_INITIAL_DURATION);
+    }
+
+    static DynamoDBIO.RetryConfiguration create(
+        int maxAttempts, Duration maxDuration, Duration initialDuration) {
       checkArgument(maxAttempts > 0, "maxAttempts should be greater than 0");
       checkArgument(
           maxDuration != null && maxDuration.isLongerThan(Duration.ZERO),
           "maxDuration should be greater than 0");
+      checkArgument(
+          initialDuration != null && initialDuration.isLongerThan(Duration.ZERO),
+          "initialDuration should be greater than 0");
+
       return new AutoValue_DynamoDBIO_RetryConfiguration.Builder()
           .setMaxAttempts(maxAttempts)
           .setMaxDuration(maxDuration)
+          .setInitialDuration(initialDuration)
           .setRetryPredicate(DEFAULT_RETRY_PREDICATE)
           .build();
     }
@@ -323,6 +339,8 @@ public final class DynamoDBIO {
 
       abstract DynamoDBIO.RetryConfiguration.Builder setMaxDuration(Duration maxDuration);
 
+      abstract DynamoDBIO.RetryConfiguration.Builder setInitialDuration(Duration initialDuration);
+
       abstract DynamoDBIO.RetryConfiguration.Builder setRetryPredicate(
           RetryPredicate retryPredicate);
 
@@ -438,7 +456,6 @@ public final class DynamoDBIO {
       @VisibleForTesting
       static final String RETRY_ATTEMPT_LOG = "Error writing to DynamoDB. Retry attempt[%d]";
 
-      private static final Duration RETRY_INITIAL_BACKOFF = Duration.standardSeconds(5);
       private transient FluentBackoff retryBackoff; // defaults to no retries
       private static final Logger LOG = LoggerFactory.getLogger(WriteFn.class);
       private static final Counter DYNAMO_DB_WRITE_FAILURES =
@@ -456,14 +473,12 @@ public final class DynamoDBIO {
       @Setup
       public void setup() {
         client = spec.getAwsClientsProvider().createDynamoDB();
-        retryBackoff =
-            FluentBackoff.DEFAULT
-                .withMaxRetries(0) // default to no retrying
-                .withInitialBackoff(RETRY_INITIAL_BACKOFF);
+        retryBackoff = FluentBackoff.DEFAULT.withMaxRetries(0); // default to no retrying
         if (spec.getRetryConfiguration() != null) {
           retryBackoff =
               retryBackoff
                   .withMaxRetries(spec.getRetryConfiguration().getMaxAttempts() - 1)
+                  .withInitialBackoff(spec.getRetryConfiguration().getInitialDuration())
                   .withMaxCumulativeBackoff(spec.getRetryConfiguration().getMaxDuration());
         }
       }
@@ -486,17 +501,22 @@ public final class DynamoDBIO {
       }
 
       private Map<String, AttributeValue> extractDeduplicateKeyValues(WriteRequest request) {
+        List<String> deduplicationKeys = spec.getDeduplicateKeys();
+        Map<String, AttributeValue> attributes = Collections.emptyMap();
+
         if (request.getPutRequest() != null) {
-          return request.getPutRequest().getItem().entrySet().stream()
-              .filter(entry -> spec.getDeduplicateKeys().contains(entry.getKey()))
-              .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));
+          attributes = request.getPutRequest().getItem();
         } else if (request.getDeleteRequest() != null) {
-          return request.getDeleteRequest().getKey().entrySet().stream()
-              .filter(entry -> spec.getDeduplicateKeys().contains(entry.getKey()))
-              .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));
-        } else {
-          return Collections.emptyMap();
+          attributes = request.getDeleteRequest().getKey();
         }
+
+        if (attributes.isEmpty() || deduplicationKeys.isEmpty()) {
+          return attributes;
+        }
+
+        return attributes.entrySet().stream()
+            .filter(entry -> deduplicationKeys.contains(entry.getKey()))
+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));
       }
 
       @FinishBundle
diff --git a/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIOReadTest.java b/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIOReadTest.java
new file mode 100644
index 00000000000..220796fff00
--- /dev/null
+++ b/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIOReadTest.java
@@ -0,0 +1,201 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.beam.sdk.io.aws.dynamodb;
+
+import static java.lang.Math.min;
+import static java.util.stream.Collectors.toList;
+import static java.util.stream.IntStream.range;
+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Iterables.getLast;
+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Lists.newArrayList;
+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Lists.transform;
+import static org.mockito.ArgumentMatchers.argThat;
+import static org.mockito.Mockito.when;
+
+import com.amazonaws.services.dynamodbv2.AmazonDynamoDB;
+import com.amazonaws.services.dynamodbv2.model.AttributeValue;
+import com.amazonaws.services.dynamodbv2.model.ScanRequest;
+import com.amazonaws.services.dynamodbv2.model.ScanResult;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Map;
+import java.util.Objects;
+import java.util.stream.IntStream;
+import org.apache.beam.sdk.testing.PAssert;
+import org.apache.beam.sdk.testing.TestPipeline;
+import org.apache.beam.sdk.transforms.Count;
+import org.apache.beam.sdk.transforms.Flatten;
+import org.apache.beam.sdk.values.PCollection;
+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.ExpectedException;
+import org.junit.runner.RunWith;
+import org.mockito.ArgumentMatcher;
+import org.mockito.Mock;
+import org.mockito.junit.MockitoJUnitRunner;
+
+@RunWith(MockitoJUnitRunner.class)
+public class DynamoDBIOReadTest {
+  private static final String tableName = "Test";
+
+  @Rule public final TestPipeline pipeline = TestPipeline.create();
+  @Rule public final ExpectedException thrown = ExpectedException.none();
+  @Mock public AmazonDynamoDB client;
+
+  @Test
+  public void testReadOneSegment() {
+    MockData mockData = new MockData(range(0, 10));
+    mockData.mockScan(10, client); // 1 scan iteration
+
+    PCollection<List<Map<String, AttributeValue>>> actual =
+        pipeline.apply(
+            DynamoDBIO.<List<Map<String, AttributeValue>>>read()
+                .withAwsClientsProvider(StaticAwsClientsProvider.of(client))
+                .withScanRequestFn(
+                    in -> new ScanRequest().withTableName(tableName).withTotalSegments(1))
+                .items());
+
+    PAssert.that(actual.apply(Count.globally())).containsInAnyOrder(1L);
+    PAssert.that(actual).containsInAnyOrder(mockData.getAllItems());
+
+    pipeline.run().waitUntilFinish();
+  }
+
+  @Test
+  public void testReadThreeSegments() {
+    MockData mockData = new MockData(range(0, 10), range(10, 20), range(20, 30));
+    mockData.mockScan(10, client); // 1 scan iteration per segment
+
+    PCollection<List<Map<String, AttributeValue>>> actual =
+        pipeline.apply(
+            DynamoDBIO.<List<Map<String, AttributeValue>>>read()
+                .withAwsClientsProvider(StaticAwsClientsProvider.of(client))
+                .withScanRequestFn(
+                    in -> new ScanRequest().withTableName(tableName).withTotalSegments(3))
+                .items());
+
+    PAssert.that(actual.apply(Count.globally())).containsInAnyOrder(3L);
+    PAssert.that(actual.apply(Flatten.iterables())).containsInAnyOrder(mockData.getAllItems());
+
+    pipeline.run().waitUntilFinish();
+  }
+
+  @Test
+  public void testReadWithStartKey() {
+    MockData mockData = new MockData(range(0, 10), range(20, 32));
+    mockData.mockScan(5, client); // 2 + 3 scan iterations
+
+    PCollection<List<Map<String, AttributeValue>>> actual =
+        pipeline.apply(
+            DynamoDBIO.<List<Map<String, AttributeValue>>>read()
+                .withAwsClientsProvider(StaticAwsClientsProvider.of(client))
+                .withScanRequestFn(
+                    in -> new ScanRequest().withTableName(tableName).withTotalSegments(2))
+                .items());
+
+    PAssert.that(actual.apply(Count.globally())).containsInAnyOrder(5L);
+    PAssert.that(actual.apply(Flatten.iterables())).containsInAnyOrder(mockData.getAllItems());
+
+    pipeline.run().waitUntilFinish();
+  }
+
+  @Test
+  public void testReadMissingScanRequestFn() {
+    pipeline.enableAbandonedNodeEnforcement(false);
+    thrown.expect(IllegalArgumentException.class);
+    thrown.expectMessage("withScanRequestFn() is required");
+
+    pipeline.apply(DynamoDBIO.read().withAwsClientsProvider(StaticAwsClientsProvider.of(client)));
+  }
+
+  @Test
+  public void testReadMissingAwsClientsProvider() {
+    pipeline.enableAbandonedNodeEnforcement(false);
+    thrown.expect(IllegalArgumentException.class);
+    thrown.expectMessage("withAwsClientsProvider() is required");
+
+    pipeline.apply(DynamoDBIO.read().withScanRequestFn(in -> new ScanRequest()));
+  }
+
+  @Test
+  public void testReadMissingTotalSegments() {
+    pipeline.enableAbandonedNodeEnforcement(false);
+    thrown.expect(IllegalArgumentException.class);
+    thrown.expectMessage("TotalSegments is required with withScanRequestFn() and greater zero");
+
+    pipeline.apply(
+        DynamoDBIO.read()
+            .withAwsClientsProvider(StaticAwsClientsProvider.of(client))
+            .withScanRequestFn(in -> new ScanRequest()));
+  }
+
+  @Test
+  public void testReadInvalidTotalSegments() {
+    pipeline.enableAbandonedNodeEnforcement(false);
+    thrown.expect(IllegalArgumentException.class);
+    thrown.expectMessage("TotalSegments is required with withScanRequestFn() and greater zero");
+
+    pipeline.apply(
+        DynamoDBIO.read()
+            .withAwsClientsProvider(StaticAwsClientsProvider.of(client))
+            .withScanRequestFn(in -> new ScanRequest().withTotalSegments(0)));
+  }
+
+  private static class MockData {
+    private final List<List<Integer>> data;
+
+    MockData(IntStream... segments) {
+      data = Arrays.stream(segments).map(ids -> newArrayList(ids.iterator())).collect(toList());
+    }
+
+    List<Map<String, AttributeValue>> getAllItems() {
+      return data.stream().flatMap(ids -> ids.stream()).map(id -> item(id)).collect(toList());
+    }
+
+    void mockScan(int sizeLimit, AmazonDynamoDB mock) {
+      for (int segment = 0; segment < data.size(); segment++) {
+        List<Integer> ids = data.get(segment);
+
+        List<Map<String, AttributeValue>> items = null;
+        Map<String, AttributeValue> startKey, lastKey;
+        for (int start = 0; start < ids.size(); start += sizeLimit) {
+          startKey = items != null ? getLast(items) : null;
+          items = transform(ids.subList(start, min(ids.size(), start + sizeLimit)), id -> item(id));
+          lastKey = start + sizeLimit < ids.size() ? getLast(items) : null;
+
+          when(mock.scan(argThat(matchesScanRequest(segment, startKey))))
+              .thenReturn(new ScanResult().withItems(items).withLastEvaluatedKey(lastKey));
+        }
+      }
+    }
+
+    ArgumentMatcher<ScanRequest> matchesScanRequest(
+        Integer segment, Map<String, AttributeValue> startKey) {
+      return req ->
+          req != null
+              && segment.equals(req.getSegment())
+              && Objects.equals(startKey, req.getExclusiveStartKey());
+    }
+  }
+
+  private static Map<String, AttributeValue> item(int id) {
+    return ImmutableMap.of(
+        "rangeKey", new AttributeValue().withN(String.valueOf(id)),
+        "hashKey", new AttributeValue().withS(String.valueOf(id)));
+  }
+}
diff --git a/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIOTest.java b/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIOTest.java
index 85f1d01e52f..d3f64f9b700 100644
--- a/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIOTest.java
+++ b/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIOTest.java
@@ -26,6 +26,7 @@ import com.amazonaws.services.dynamodbv2.model.AttributeValue;
 import com.amazonaws.services.dynamodbv2.model.BatchWriteItemRequest;
 import com.amazonaws.services.dynamodbv2.model.ScanRequest;
 import com.amazonaws.services.dynamodbv2.model.WriteRequest;
+import java.io.IOException;
 import java.io.Serializable;
 import java.util.Arrays;
 import java.util.HashSet;
@@ -40,7 +41,6 @@ import org.apache.beam.sdk.transforms.Count;
 import org.apache.beam.sdk.transforms.Create;
 import org.apache.beam.sdk.transforms.DoFn;
 import org.apache.beam.sdk.transforms.ParDo;
-import org.apache.beam.sdk.transforms.SerializableFunction;
 import org.apache.beam.sdk.values.KV;
 import org.apache.beam.sdk.values.PCollection;
 import org.joda.time.Duration;
@@ -55,7 +55,10 @@ import org.mockito.Mockito;
 /** Test Coverage for the IO. */
 public class DynamoDBIOTest implements Serializable {
   @Rule public final transient TestPipeline pipeline = TestPipeline.create();
-  @Rule public final transient ExpectedLogs expectedLogs = ExpectedLogs.none(DynamoDBIO.class);
+
+  @Rule
+  public final transient ExpectedLogs writeFnLogs =
+      ExpectedLogs.none(DynamoDBIO.Write.WriteFn.class);
 
   private static final String tableName = "TaskA";
   private static final int numOfItems = 10;
@@ -81,10 +84,8 @@ public class DynamoDBIOTest implements Serializable {
         pipeline.apply(
             DynamoDBIO.<List<Map<String, AttributeValue>>>read()
                 .withAwsClientsProvider(
-                    AwsClientsProviderMock.of(DynamoDBIOTestHelper.getDynamoDBClient()))
-                .withScanRequestFn(
-                    (SerializableFunction<Void, ScanRequest>)
-                        input -> new ScanRequest(tableName).withTotalSegments(1))
+                    StaticAwsClientsProvider.of(DynamoDBIOTestHelper.getDynamoDBClient()))
+                .withScanRequestFn(input -> new ScanRequest(tableName).withTotalSegments(1))
                 .items());
     PAssert.that(actual).containsInAnyOrder(expected);
     pipeline.run().waitUntilFinish();
@@ -100,11 +101,9 @@ public class DynamoDBIOTest implements Serializable {
             .apply(
                 DynamoDBIO.<List<Map<String, AttributeValue>>>read()
                     .withAwsClientsProvider(
-                        AwsClientsProviderMock.of(DynamoDBIOTestHelper.getDynamoDBClient()))
+                        StaticAwsClientsProvider.of(DynamoDBIOTestHelper.getDynamoDBClient()))
                     .withScanRequestFn(
-                        (SerializableFunction<Void, ScanRequest>)
-                            input ->
-                                new ScanRequest(tableName).withTotalSegments(1).withLimit(limit))
+                        input -> new ScanRequest(tableName).withTotalSegments(1).withLimit(limit))
                     .items())
             .apply(ParDo.of(new IterateListDoFn()));
     PAssert.that(actual).containsInAnyOrder(expected);
@@ -118,7 +117,7 @@ public class DynamoDBIOTest implements Serializable {
     pipeline.apply(
         DynamoDBIO.read()
             .withAwsClientsProvider(
-                AwsClientsProviderMock.of(DynamoDBIOTestHelper.getDynamoDBClient())));
+                StaticAwsClientsProvider.of(DynamoDBIOTestHelper.getDynamoDBClient())));
     try {
       pipeline.run().waitUntilFinish();
       fail("withScanRequestFn() is required");
@@ -132,9 +131,7 @@ public class DynamoDBIOTest implements Serializable {
     thrown.expectMessage("withAwsClientsProvider() is required");
     pipeline.apply(
         DynamoDBIO.read()
-            .withScanRequestFn(
-                (SerializableFunction<Void, ScanRequest>)
-                    input -> new ScanRequest(tableName).withTotalSegments(3)));
+            .withScanRequestFn(input -> new ScanRequest(tableName).withTotalSegments(3)));
     try {
       pipeline.run().waitUntilFinish();
       fail("withAwsClientsProvider() is required");
@@ -148,10 +145,9 @@ public class DynamoDBIOTest implements Serializable {
     thrown.expectMessage("TotalSegments is required with withScanRequestFn()");
     pipeline.apply(
         DynamoDBIO.read()
-            .withScanRequestFn(
-                (SerializableFunction<Void, ScanRequest>) input -> new ScanRequest(tableName))
+            .withScanRequestFn(input -> new ScanRequest(tableName))
             .withAwsClientsProvider(
-                AwsClientsProviderMock.of(DynamoDBIOTestHelper.getDynamoDBClient())));
+                StaticAwsClientsProvider.of(DynamoDBIOTestHelper.getDynamoDBClient())));
     try {
       pipeline.run().waitUntilFinish();
       fail("TotalSegments is required with withScanRequestFn()");
@@ -165,11 +161,9 @@ public class DynamoDBIOTest implements Serializable {
     thrown.expectMessage("TotalSegments is required with withScanRequestFn() and greater zero");
     pipeline.apply(
         DynamoDBIO.read()
-            .withScanRequestFn(
-                (SerializableFunction<Void, ScanRequest>)
-                    input -> new ScanRequest(tableName).withTotalSegments(-1))
+            .withScanRequestFn(input -> new ScanRequest(tableName).withTotalSegments(-1))
             .withAwsClientsProvider(
-                AwsClientsProviderMock.of(DynamoDBIOTestHelper.getDynamoDBClient())));
+                StaticAwsClientsProvider.of(DynamoDBIOTestHelper.getDynamoDBClient())));
     try {
       pipeline.run().waitUntilFinish();
       fail("withTotalSegments() is expected and greater than zero");
@@ -189,13 +183,9 @@ public class DynamoDBIOTest implements Serializable {
             .apply(Create.of(writeRequests))
             .apply(
                 DynamoDBIO.<WriteRequest>write()
-                    .withWriteRequestMapperFn(
-                        (SerializableFunction<WriteRequest, KV<String, WriteRequest>>)
-                            writeRequest -> KV.of(tableName, writeRequest))
-                    .withRetryConfiguration(
-                        DynamoDBIO.RetryConfiguration.create(5, Duration.standardMinutes(1)))
+                    .withWriteRequestMapperFn(writeRequest -> KV.of(tableName, writeRequest))
                     .withAwsClientsProvider(
-                        AwsClientsProviderMock.of(DynamoDBIOTestHelper.getDynamoDBClient())));
+                        StaticAwsClientsProvider.of(DynamoDBIOTestHelper.getDynamoDBClient())));
 
     final PCollection<Long> publishedResultsSize = output.apply(Count.globally());
     PAssert.that(publishedResultsSize).containsInAnyOrder(0L);
@@ -207,7 +197,9 @@ public class DynamoDBIOTest implements Serializable {
 
   @Test
   public void testRetries() throws Throwable {
+    thrown.expect(IOException.class);
     thrown.expectMessage("Error writing to DynamoDB");
+    thrown.expectMessage("No more attempts allowed");
 
     final List<WriteRequest> writeRequests = DynamoDBIOTestHelper.generateWriteRequests(numOfItems);
 
@@ -219,23 +211,21 @@ public class DynamoDBIOTest implements Serializable {
         .apply(Create.of(writeRequests))
         .apply(
             DynamoDBIO.<WriteRequest>write()
-                .withWriteRequestMapperFn(
-                    (SerializableFunction<WriteRequest, KV<String, WriteRequest>>)
-                        writeRequest -> KV.of(tableName, writeRequest))
+                .withWriteRequestMapperFn(writeRequest -> KV.of(tableName, writeRequest))
                 .withRetryConfiguration(
-                    DynamoDBIO.RetryConfiguration.create(4, Duration.standardSeconds(10)))
-                .withAwsClientsProvider(AwsClientsProviderMock.of(amazonDynamoDBMock)));
+                    DynamoDBIO.RetryConfiguration.create(
+                        4, Duration.standardSeconds(10), Duration.millis(1)))
+                .withAwsClientsProvider(StaticAwsClientsProvider.of(amazonDynamoDBMock)));
 
     try {
       pipeline.run().waitUntilFinish();
     } catch (final Pipeline.PipelineExecutionException e) {
       // check 3 retries were initiated by inspecting the log before passing on the exception
-      expectedLogs.verifyWarn(String.format(DynamoDBIO.Write.WriteFn.RETRY_ATTEMPT_LOG, 1));
-      expectedLogs.verifyWarn(String.format(DynamoDBIO.Write.WriteFn.RETRY_ATTEMPT_LOG, 2));
-      expectedLogs.verifyWarn(String.format(DynamoDBIO.Write.WriteFn.RETRY_ATTEMPT_LOG, 3));
+      writeFnLogs.verifyWarn(String.format(DynamoDBIO.Write.WriteFn.RETRY_ATTEMPT_LOG, 1));
+      writeFnLogs.verifyWarn(String.format(DynamoDBIO.Write.WriteFn.RETRY_ATTEMPT_LOG, 2));
+      writeFnLogs.verifyWarn(String.format(DynamoDBIO.Write.WriteFn.RETRY_ATTEMPT_LOG, 3));
       throw e.getCause();
     }
-    fail("Pipeline is expected to fail because we were unable to write to DynamoDB.");
   }
 
   /**
@@ -266,12 +256,8 @@ public class DynamoDBIOTest implements Serializable {
         .apply("duplicate", ParDo.of(new WriteDuplicateGeneratorDoFn()))
         .apply(
             DynamoDBIO.<WriteRequest>write()
-                .withWriteRequestMapperFn(
-                    (SerializableFunction<WriteRequest, KV<String, WriteRequest>>)
-                        writeRequest -> KV.of(tableName, writeRequest))
-                .withRetryConfiguration(
-                    DynamoDBIO.RetryConfiguration.create(5, Duration.standardMinutes(1)))
-                .withAwsClientsProvider(AwsClientsProviderMock.of(amazonDynamoDBMock))
+                .withWriteRequestMapperFn(writeRequest -> KV.of(tableName, writeRequest))
+                .withAwsClientsProvider(StaticAwsClientsProvider.of(amazonDynamoDBMock))
                 .withDeduplicateKeys(deduplicateKeys));
 
     pipeline.run().waitUntilFinish();
diff --git a/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIOWriteTest.java b/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIOWriteTest.java
new file mode 100644
index 00000000000..0ec36c18882
--- /dev/null
+++ b/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/dynamodb/DynamoDBIOWriteTest.java
@@ -0,0 +1,344 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.beam.sdk.io.aws.dynamodb;
+
+import static java.util.stream.Collectors.toList;
+import static java.util.stream.IntStream.range;
+import static java.util.stream.IntStream.rangeClosed;
+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Maps.transformValues;
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.mockito.ArgumentMatchers.any;
+import static org.mockito.Mockito.times;
+import static org.mockito.Mockito.verify;
+import static org.mockito.Mockito.when;
+
+import com.amazonaws.services.dynamodbv2.AmazonDynamoDB;
+import com.amazonaws.services.dynamodbv2.model.AmazonDynamoDBException;
+import com.amazonaws.services.dynamodbv2.model.AttributeValue;
+import com.amazonaws.services.dynamodbv2.model.BatchWriteItemRequest;
+import com.amazonaws.services.dynamodbv2.model.BatchWriteItemResult;
+import com.amazonaws.services.dynamodbv2.model.DeleteRequest;
+import com.amazonaws.services.dynamodbv2.model.PutRequest;
+import com.amazonaws.services.dynamodbv2.model.WriteRequest;
+import java.io.IOException;
+import java.io.Serializable;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Objects;
+import java.util.function.Function;
+import java.util.function.Supplier;
+import org.apache.beam.sdk.Pipeline;
+import org.apache.beam.sdk.PipelineResult;
+import org.apache.beam.sdk.coders.AvroCoder;
+import org.apache.beam.sdk.coders.DefaultCoder;
+import org.apache.beam.sdk.io.aws.dynamodb.DynamoDBIO.RetryConfiguration;
+import org.apache.beam.sdk.io.aws.dynamodb.DynamoDBIO.Write.WriteFn;
+import org.apache.beam.sdk.testing.ExpectedLogs;
+import org.apache.beam.sdk.testing.PAssert;
+import org.apache.beam.sdk.testing.TestPipeline;
+import org.apache.beam.sdk.transforms.Create;
+import org.apache.beam.sdk.transforms.DoFn;
+import org.apache.beam.sdk.transforms.ParDo;
+import org.apache.beam.sdk.transforms.SerializableBiFunction;
+import org.apache.beam.sdk.transforms.SerializableFunction;
+import org.apache.beam.sdk.values.KV;
+import org.apache.beam.sdk.values.PCollection;
+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;
+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;
+import org.joda.time.Duration;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.ExpectedException;
+import org.junit.runner.RunWith;
+import org.mockito.ArgumentCaptor;
+import org.mockito.Mock;
+import org.mockito.junit.MockitoJUnitRunner;
+
+@RunWith(MockitoJUnitRunner.class)
+public class DynamoDBIOWriteTest {
+  private static final String tableName = "Test";
+
+  @Rule public final TestPipeline pipeline = TestPipeline.create();
+  @Rule public final ExpectedLogs writeFnLogs = ExpectedLogs.none(WriteFn.class);
+  @Rule public final ExpectedException thrown = ExpectedException.none();
+
+  @Mock public AmazonDynamoDB client;
+
+  @Test
+  public void testWritePutItems() {
+    List<Item> items = range(0, 100).mapToObj(Item::of).collect(toList());
+
+    Supplier<List<Item>> capturePuts =
+        captureBatchWrites(client, req -> req.getPutRequest().getItem());
+
+    PCollection<Void> output =
+        pipeline
+            .apply(Create.of(items))
+            .apply(
+                DynamoDBIO.<Item>write()
+                    .withWriteRequestMapperFn(putRequestMapper)
+                    .withAwsClientsProvider(StaticAwsClientsProvider.of(client)));
+
+    PAssert.that(output).empty();
+    pipeline.run().waitUntilFinish();
+
+    assertThat(capturePuts.get()).containsExactlyInAnyOrderElementsOf(items);
+  }
+
+  @Test
+  public void testWritePutItemsWithDuplicates() {
+    List<Item> items = range(0, 100).mapToObj(Item::of).collect(toList());
+
+    Supplier<List<Item>> capturePuts =
+        captureBatchWrites(client, req -> req.getPutRequest().getItem());
+
+    pipeline
+        .apply(Create.of(items))
+        // generate identical duplicates
+        .apply(ParDo.of(new AddDuplicatesDoFn(3, false)))
+        .apply(
+            DynamoDBIO.<Item>write()
+                .withWriteRequestMapperFn(putRequestMapper)
+                .withAwsClientsProvider(StaticAwsClientsProvider.of(client)));
+
+    pipeline.run().waitUntilFinish();
+
+    assertThat(capturePuts.get()).hasSize(100);
+    assertThat(capturePuts.get()).containsExactlyInAnyOrderElementsOf(items);
+  }
+
+  @Test
+  public void testWritePutItemsWithDuplicatesByKey() {
+    List<Item> items = range(0, 100).mapToObj(Item::of).collect(toList());
+
+    Supplier<List<Item>> capturePuts =
+        captureBatchWrites(client, req -> req.getPutRequest().getItem());
+
+    pipeline
+        .apply(Create.of(items))
+        // decorate duplicates so they are different
+        .apply(ParDo.of(new AddDuplicatesDoFn(3, true)))
+        .apply(
+            DynamoDBIO.<Item>write()
+                .withWriteRequestMapperFn(putRequestMapper)
+                .withAwsClientsProvider(StaticAwsClientsProvider.of(client))
+                .withDeduplicateKeys(ImmutableList.of("id")));
+
+    pipeline.run().waitUntilFinish();
+
+    assertThat(capturePuts.get()).hasSize(100);
+    assertThat(capturePuts.get()).containsExactlyInAnyOrderElementsOf(items);
+  }
+
+  @Test
+  public void testWriteDeleteItems() {
+    List<Item> items = range(0, 100).mapToObj(Item::of).collect(toList());
+
+    Supplier<List<Item>> captureDeletes =
+        captureBatchWrites(client, req -> req.getDeleteRequest().getKey());
+
+    PCollection<Void> output =
+        pipeline
+            .apply(Create.of(items))
+            .apply(
+                DynamoDBIO.<Item>write()
+                    .withWriteRequestMapperFn(deleteRequestMapper)
+                    .withAwsClientsProvider(StaticAwsClientsProvider.of(client)));
+
+    PAssert.that(output).empty();
+    pipeline.run().waitUntilFinish();
+
+    assertThat(captureDeletes.get()).hasSize(100);
+    assertThat(captureDeletes.get()).containsExactlyInAnyOrderElementsOf(items);
+  }
+
+  @Test
+  public void testWriteDeleteItemsWithDuplicates() {
+    List<Item> items = range(0, 100).mapToObj(Item::of).collect(toList());
+
+    Supplier<List<Item>> captureDeletes =
+        captureBatchWrites(client, req -> req.getDeleteRequest().getKey());
+
+    pipeline
+        .apply(Create.of(items))
+        // generate identical duplicates
+        .apply(ParDo.of(new AddDuplicatesDoFn(3, false)))
+        .apply(
+            DynamoDBIO.<Item>write()
+                .withWriteRequestMapperFn(deleteRequestMapper)
+                .withAwsClientsProvider(StaticAwsClientsProvider.of(client)));
+
+    pipeline.run().waitUntilFinish();
+
+    assertThat(captureDeletes.get()).hasSize(100);
+    assertThat(captureDeletes.get()).containsExactlyInAnyOrderElementsOf(items);
+  }
+
+  @Test
+  public void testWritePutItemsWithRetrySuccess() {
+    when(client.batchWriteItem(any(BatchWriteItemRequest.class)))
+        .thenThrow(
+            AmazonDynamoDBException.class,
+            AmazonDynamoDBException.class,
+            AmazonDynamoDBException.class)
+        .thenReturn(new BatchWriteItemResult());
+
+    pipeline
+        .apply(Create.of(Item.of(1)))
+        .apply(
+            "write",
+            DynamoDBIO.<Item>write()
+                .withWriteRequestMapperFn(putRequestMapper)
+                .withAwsClientsProvider(StaticAwsClientsProvider.of(client))
+                .withRetryConfiguration(try4Times));
+
+    PipelineResult result = pipeline.run();
+    result.waitUntilFinish();
+
+    verify(client, times(4)).batchWriteItem(any(BatchWriteItemRequest.class));
+    range(1, 4).forEach(i -> writeFnLogs.verifyWarn(String.format(WriteFn.RETRY_ATTEMPT_LOG, i)));
+  }
+
+  @Test
+  public void testWritePutItemsWithRetryFailure() throws Throwable {
+    thrown.expect(IOException.class);
+    thrown.expectMessage("Error writing to DynamoDB");
+    thrown.expectMessage("No more attempts allowed");
+
+    when(client.batchWriteItem(any(BatchWriteItemRequest.class)))
+        .thenThrow(AmazonDynamoDBException.class);
+
+    pipeline
+        .apply(Create.of(Item.of(1)))
+        .apply(
+            DynamoDBIO.<Item>write()
+                .withWriteRequestMapperFn(putRequestMapper)
+                .withAwsClientsProvider(StaticAwsClientsProvider.of(client))
+                .withRetryConfiguration(try4Times));
+
+    try {
+      pipeline.run().waitUntilFinish();
+    } catch (final Pipeline.PipelineExecutionException e) {
+      verify(client, times(4)).batchWriteItem(any(BatchWriteItemRequest.class));
+      range(1, 4).forEach(i -> writeFnLogs.verifyWarn(String.format(WriteFn.RETRY_ATTEMPT_LOG, i)));
+      throw e.getCause();
+    }
+  }
+
+  @DefaultCoder(AvroCoder.class)
+  static class Item implements Serializable {
+    Map<String, String> entries;
+
+    private Item() {}
+
+    private Item(Map<String, String> entries) {
+      this.entries = entries;
+    }
+
+    static Item of(int id) {
+      return new Item(ImmutableMap.of("id", String.valueOf(id)));
+    }
+
+    static Item of(Map<String, AttributeValue> attributes) {
+      return new Item(ImmutableMap.copyOf(transformValues(attributes, a -> a.getS())));
+    }
+
+    Item withEntry(String key, String value) {
+      return new Item(
+          ImmutableMap.<String, String>builder().putAll(entries).put(key, value).build());
+    }
+
+    Map<String, AttributeValue> attributeMap() {
+      return new HashMap<>(transformValues(entries, v -> new AttributeValue().withS(v)));
+    }
+
+    @Override
+    public boolean equals(Object o) {
+      if (this == o) {
+        return true;
+      }
+      if (o == null || getClass() != o.getClass()) {
+        return false;
+      }
+      return Objects.equals(entries, ((Item) o).entries);
+    }
+
+    @Override
+    public int hashCode() {
+      return Objects.hash(entries);
+    }
+
+    @Override
+    public String toString() {
+      return "Item" + entries;
+    }
+  }
+
+  private Supplier<List<Item>> captureBatchWrites(
+      AmazonDynamoDB mock, Function<WriteRequest, Map<String, AttributeValue>> extractor) {
+    ArgumentCaptor<BatchWriteItemRequest> reqCaptor =
+        ArgumentCaptor.forClass(BatchWriteItemRequest.class);
+    when(mock.batchWriteItem(reqCaptor.capture())).thenReturn(new BatchWriteItemResult());
+
+    return () ->
+        reqCaptor.getAllValues().stream()
+            .flatMap(req -> req.getRequestItems().values().stream())
+            .flatMap(writes -> writes.stream())
+            .map(extractor)
+            .map(Item::of)
+            .collect(toList());
+  }
+
+  private static SerializableFunction<Item, KV<String, WriteRequest>> putRequestMapper =
+      item -> {
+        PutRequest req = new PutRequest().withItem(item.attributeMap());
+        return KV.of(tableName, new WriteRequest().withPutRequest(req));
+      };
+
+  private static SerializableFunction<Item, KV<String, WriteRequest>> deleteRequestMapper =
+      key -> {
+        DeleteRequest req = new DeleteRequest().withKey(key.attributeMap());
+        return KV.of(tableName, new WriteRequest().withDeleteRequest(req));
+      };
+
+  private static RetryConfiguration try4Times =
+      RetryConfiguration.create(4, Duration.standardSeconds(1), Duration.millis(1));
+
+  /**
+   * A DoFn that adds N duplicates to a bundle. The original is emitted last and is the only item
+   * kept if deduplicating appropriately.
+   */
+  private static class AddDuplicatesDoFn extends DoFn<Item, Item> {
+    private final int duplicates;
+    private final SerializableBiFunction<Item, Integer, Item> decorator;
+
+    AddDuplicatesDoFn(int duplicates, boolean decorate) {
+      this.duplicates = duplicates;
+      this.decorator =
+          decorate ? (item, i) -> item.withEntry("duplicate", i.toString()) : (item, i) -> item;
+    }
+
+    @ProcessElement
+    public void processElement(ProcessContext ctx) {
+      Item original = ctx.element();
+      rangeClosed(1, duplicates).forEach(i -> ctx.output(decorator.apply(original, i)));
+      ctx.output(original);
+    }
+  }
+}
diff --git a/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/dynamodb/AwsClientsProviderMock.java b/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/dynamodb/StaticAwsClientsProvider.java
similarity index 54%
rename from sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/dynamodb/AwsClientsProviderMock.java
rename to sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/dynamodb/StaticAwsClientsProvider.java
index dfcf302e006..ed2c080253c 100644
--- a/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/dynamodb/AwsClientsProviderMock.java
+++ b/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/dynamodb/StaticAwsClientsProvider.java
@@ -17,30 +17,45 @@
  */
 package org.apache.beam.sdk.io.aws.dynamodb;
 
+import static java.util.Collections.synchronizedMap;
+
 import com.amazonaws.services.cloudwatch.AmazonCloudWatch;
 import com.amazonaws.services.dynamodbv2.AmazonDynamoDB;
-import org.mockito.Mockito;
+import java.util.HashMap;
+import java.util.Map;
 
-/** Mocking AwsClientProvider. */
-public class AwsClientsProviderMock implements AwsClientsProvider {
+/** Client provider supporting unserializable clients such as mock instances for unit tests. */
+class StaticAwsClientsProvider implements AwsClientsProvider {
+  private static final Map<Integer, AmazonDynamoDB> clients = synchronizedMap(new HashMap<>());
 
-  private static AwsClientsProviderMock instance = new AwsClientsProviderMock();
-  private static AmazonDynamoDB db;
+  private final int id;
+  private final transient boolean cleanup;
 
-  private AwsClientsProviderMock() {}
+  private StaticAwsClientsProvider(AmazonDynamoDB client) {
+    this.id = System.identityHashCode(client);
+    this.cleanup = true;
+  }
 
-  public static AwsClientsProviderMock of(AmazonDynamoDB dynamoDB) {
-    db = dynamoDB;
-    return instance;
+  static AwsClientsProvider of(AmazonDynamoDB client) {
+    StaticAwsClientsProvider provider = new StaticAwsClientsProvider(client);
+    clients.put(provider.id, client);
+    return provider;
   }
 
   @Override
   public AmazonCloudWatch getCloudWatchClient() {
-    return Mockito.mock(AmazonCloudWatch.class);
+    return null; // never used
   }
 
   @Override
   public AmazonDynamoDB createDynamoDB() {
-    return db;
+    return clients.get(id);
+  }
+
+  @Override
+  protected void finalize() {
+    if (cleanup) {
+      clients.remove(id);
+    }
   }
 }
