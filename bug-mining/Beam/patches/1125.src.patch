diff --git a/sdks/python/apache_beam/runners/dataflow/dataflow_runner.py b/sdks/python/apache_beam/runners/dataflow/dataflow_runner.py
index 3f210115d56..3b6574aa366 100644
--- a/sdks/python/apache_beam/runners/dataflow/dataflow_runner.py
+++ b/sdks/python/apache_beam/runners/dataflow/dataflow_runner.py
@@ -375,6 +375,15 @@ class DataflowRunner(PipelineRunner):
         and/or enable_streaming_engine experiments are present.
         It is recommended you only set the enable_streaming_engine flag.""")
 
+    dataflow_worker_jar = getattr(worker_options, 'dataflow_worker_jar', None)
+    if dataflow_worker_jar is not None:
+      if not apiclient._use_fnapi(options):
+        logging.warn(
+            'Typical end users should not use this worker jar feature. '
+            'It can only be used when FnAPI is enabled.')
+      else:
+        debug_options.add_experiment('use_staged_dataflow_worker_jar')
+
     self.job = apiclient.Job(options, self.proto_pipeline)
 
     # Dataflow runner requires a KV type for GBK inputs, hence we enforce that
@@ -396,18 +405,6 @@ class DataflowRunner(PipelineRunner):
     # Get a Dataflow API client and set its options
     self.dataflow_client = apiclient.DataflowApplicationClient(options)
 
-    dataflow_worker_jar = getattr(worker_options, 'dataflow_worker_jar', None)
-    if dataflow_worker_jar is not None:
-      if not apiclient._use_fnapi(options):
-        logging.fatal(
-            'Typical end users should not use this worker jar feature. '
-            'It can only be used when fnapi is enabled.')
-
-      experiments = ["use_staged_dataflow_worker_jar"]
-      if debug_options.experiments is not None:
-        experiments = list(set(experiments + debug_options.experiments))
-      debug_options.experiments = experiments
-
     # Create the job description and send a request to the service. The result
     # can be None if there is no need to send a request to the service (e.g.
     # template creation). If a request was sent and failed then the call will
diff --git a/sdks/python/apache_beam/runners/dataflow/dataflow_runner_test.py b/sdks/python/apache_beam/runners/dataflow/dataflow_runner_test.py
index 8c6ad058d15..c774b610a36 100644
--- a/sdks/python/apache_beam/runners/dataflow/dataflow_runner_test.py
+++ b/sdks/python/apache_beam/runners/dataflow/dataflow_runner_test.py
@@ -60,14 +60,15 @@ except ImportError:
 
 @unittest.skipIf(apiclient is None, 'GCP dependencies are not installed')
 class DataflowRunnerTest(unittest.TestCase):
-  default_properties = [
-      '--dataflow_endpoint=ignored',
-      '--job_name=test-job',
-      '--project=test-project',
-      '--staging_location=ignored',
-      '--temp_location=/dev/null',
-      '--no_auth=True',
-      '--dry_run=True']
+  def setUp(self):
+    self.default_properties = [
+        '--dataflow_endpoint=ignored',
+        '--job_name=test-job',
+        '--project=test-project',
+        '--staging_location=ignored',
+        '--temp_location=/dev/null',
+        '--no_auth=True',
+        '--dry_run=True']
 
   @mock.patch('time.sleep', return_value=None)
   def test_wait_until_finish(self, patched_time_sleep):
@@ -415,6 +416,34 @@ class DataflowRunnerTest(unittest.TestCase):
     self.assertIn('enable_windmill_service', experiments_for_job)
     self.assertIn('some_other_experiment', experiments_for_job)
 
+  def test_dataflow_worker_jar_flag_non_fnapi_noop(self):
+    remote_runner = DataflowRunner()
+    self.default_properties.append('--experiment=some_other_experiment')
+    self.default_properties.append('--dataflow_worker_jar=test.jar')
+
+    p = Pipeline(remote_runner, PipelineOptions(self.default_properties))
+    p | ptransform.Create([1])  # pylint: disable=expression-not-assigned
+    p.run()
+
+    experiments_for_job = (
+        remote_runner.job.options.view_as(DebugOptions).experiments)
+    self.assertIn('some_other_experiment', experiments_for_job)
+    self.assertNotIn('use_staged_dataflow_worker_jar', experiments_for_job)
+
+  def test_dataflow_worker_jar_flag_adds_use_staged_worker_jar_experiment(self):
+    remote_runner = DataflowRunner()
+    self.default_properties.append('--experiment=beam_fn_api')
+    self.default_properties.append('--dataflow_worker_jar=test.jar')
+
+    p = Pipeline(remote_runner, PipelineOptions(self.default_properties))
+    p | ptransform.Create([1])  # pylint: disable=expression-not-assigned
+    p.run()
+
+    experiments_for_job = (
+        remote_runner.job.options.view_as(DebugOptions).experiments)
+    self.assertIn('beam_fn_api', experiments_for_job)
+    self.assertIn('use_staged_dataflow_worker_jar', experiments_for_job)
+
 
 if __name__ == '__main__':
   unittest.main()
diff --git a/sdks/python/build.gradle b/sdks/python/build.gradle
index 8294b8db270..17188a5227c 100644
--- a/sdks/python/build.gradle
+++ b/sdks/python/build.gradle
@@ -229,9 +229,14 @@ task integrationTest(dependsOn: ['installGcpTest', 'sdist']) {
 
 // Run PostCommit integration tests on default runner (TestDataflowRunner)
 task postCommitIT(dependsOn: ['installGcpTest', 'sdist']) {
+  dependsOn ":beam-runners-google-cloud-dataflow-java-fn-api-worker:shadowJar"
+
+  def dataflowWorkerJar = project(":beam-runners-google-cloud-dataflow-java-fn-api-worker").shadowJar.archivePath
+
   doLast {
     def testOpts = basicTestOpts + ["--attr=IT"]
-    def cmdArgs = project.mapToArgString(["test_opts": testOpts])
+    def cmdArgs = project.mapToArgString(["test_opts": testOpts,
+                                          "worker_jar": dataflowWorkerJar])
     exec {
       executable 'sh'
       args '-c', ". ${project.ext.envdir}/bin/activate && ./scripts/run_integration_test.sh $cmdArgs"
@@ -240,9 +245,14 @@ task postCommitIT(dependsOn: ['installGcpTest', 'sdist']) {
 }
 
 task validatesRunnerBatchTests(dependsOn: ['installGcpTest', 'sdist']) {
+  dependsOn ":beam-runners-google-cloud-dataflow-java-fn-api-worker:shadowJar"
+
+  def dataflowWorkerJar = project(":beam-runners-google-cloud-dataflow-java-fn-api-worker").shadowJar.archivePath
+
   doLast {
     def testOpts = basicTestOpts + ["--attr=ValidatesRunner"]
-    def cmdArgs = project.mapToArgString(["test_opts": testOpts])
+    def cmdArgs = project.mapToArgString(["test_opts": testOpts,
+                                          "worker_jar": dataflowWorkerJar])
     exec {
       executable 'sh'
       args '-c', ". ${project.ext.envdir}/bin/activate && ./scripts/run_integration_test.sh $cmdArgs"
