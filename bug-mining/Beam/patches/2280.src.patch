diff --git a/sdks/python/apache_beam/runners/dataflow/internal/apiclient.py b/sdks/python/apache_beam/runners/dataflow/internal/apiclient.py
index 1b7b4af65c9..6362b8cedbc 100644
--- a/sdks/python/apache_beam/runners/dataflow/internal/apiclient.py
+++ b/sdks/python/apache_beam/runners/dataflow/internal/apiclient.py
@@ -64,6 +64,7 @@ from apache_beam.transforms import DataflowDistributionCounter
 from apache_beam.transforms import cy_combiners
 from apache_beam.transforms.display import DisplayData
 from apache_beam.transforms.environments import is_apache_beam_container
+from apache_beam.transforms.environments import is_beam_java_container_name
 from apache_beam.utils import retry
 from apache_beam.utils import proto_utils
 
@@ -282,18 +283,11 @@ class Environment(object):
     # Dataflow workers.
     environments_to_use = self._get_environments_from_tranforms()
     if _use_unified_worker(options):
-      # Adding a SDK container image for the pipeline SDKs
-      container_image = dataflow.SdkHarnessContainerImage()
-      pipeline_sdk_container_image = get_container_image_from_options(options)
-      container_image.containerImage = pipeline_sdk_container_image
-      container_image.useSingleCorePerContainer = True  # True for Python SDK.
-      pool.sdkHarnessContainerImages.append(container_image)
-
-      already_added_containers = [pipeline_sdk_container_image]
+      python_sdk_container_image = get_container_image_from_options(options)
 
       # Adding container images for other SDKs that may be needed for
       # cross-language pipelines.
-      for environment in environments_to_use:
+      for id, environment in environments_to_use:
         if environment.urn != common_urns.environments.DOCKER.urn:
           raise Exception(
               'Dataflow can only execute pipeline steps in Docker environments.'
@@ -301,26 +295,14 @@ class Environment(object):
         environment_payload = proto_utils.parse_Bytes(
             environment.payload, beam_runner_api_pb2.DockerPayload)
         container_image_url = environment_payload.container_image
-        if container_image_url in already_added_containers:
-          # Do not add the pipeline environment again.
-
-          # Currently, Dataflow uses Docker container images to uniquely
-          # identify execution environments. Hence Dataflow executes all
-          # transforms that specify the the same Docker container image in a
-          # single container instance. Dependencies of all environments that
-          # specify a given container image will be staged in the container
-          # instance for that particular container image.
-          # TODO(BEAM-9455): loosen this restriction to support multiple
-          # environments with the same container image when Dataflow supports
-          # environment specific artifact provisioning.
-          continue
-        already_added_containers.append(container_image_url)
 
         container_image = dataflow.SdkHarnessContainerImage()
         container_image.containerImage = container_image_url
         # Currently we only set following to True for Python SDK.
         # TODO: set this correctly for remote environments that might be Python.
-        container_image.useSingleCorePerContainer = False
+        container_image.useSingleCorePerContainer = (
+            container_image_url == python_sdk_container_image)
+        container_image.environmentId = id
         pool.sdkHarnessContainerImages.append(container_image)
 
     if self.debug_options.number_of_worker_harness_threads:
@@ -378,10 +360,8 @@ class Environment(object):
         for transform in self._proto_pipeline.components.transforms.values()
         if transform.environment_id)
 
-    return [
-        self._proto_pipeline.components.environments[id]
-        for id in environment_ids
-    ]
+    return [(id, self._proto_pipeline.components.environments[id])
+            for id in environment_ids]
 
   def _get_python_sdk_name(self):
     python_version = '%d.%d' % (sys.version_info[0], sys.version_info[1])
@@ -740,6 +720,28 @@ class DataflowApplicationClient(object):
       new_payload.container_image = new_container_image
       environment.payload = new_payload.SerializeToString()
 
+    # De-dup environments that use Java SDK by Docker container image since
+    # currently running multiple Java SDK Harnesses with Dataflow could result
+    # in dependency conflicts.
+    # TODDO(BEAM-9455): remove following restriction when Dataflow supports
+    # environment specific dependency provisioning.
+    container_url_to_env_map = dict()
+    container_url_to_env_id_map = dict()
+    for transform in proto_pipeline.components.transforms.values():
+      environment_id = transform.environment_id
+      if not environment_id:
+        continue
+      environment = proto_pipeline.components.environments[environment_id]
+      docker_payload = proto_utils.parse_Bytes(
+          environment.payload, beam_runner_api_pb2.DockerPayload)
+      image = docker_payload.container_image
+      if is_beam_java_container_name(image):
+        if image in container_url_to_env_map:
+          transform.environment_id = container_url_to_env_id_map[image]
+        else:
+          container_url_to_env_map[image] = environment
+          container_url_to_env_id_map[image] = environment_id
+
   def create_job_description(self, job):
     """Creates a job described by the workflow proto."""
     DataflowApplicationClient._apply_sdk_environment_overrides(
diff --git a/sdks/python/apache_beam/runners/dataflow/internal/apiclient_test.py b/sdks/python/apache_beam/runners/dataflow/internal/apiclient_test.py
index cf37c844e18..933e9d37bfd 100644
--- a/sdks/python/apache_beam/runners/dataflow/internal/apiclient_test.py
+++ b/sdks/python/apache_beam/runners/dataflow/internal/apiclient_test.py
@@ -171,7 +171,7 @@ class UtilTest(unittest.TestCase):
             dataflow.Environment.FlexResourceSchedulingGoalValueValuesEnum.
             FLEXRS_SPEED_OPTIMIZED))
 
-  def test_sdk_harness_container_images_get_set(self):
+  def test_default_environment_get_set(self):
 
     pipeline_options = PipelineOptions([
         '--experiments=beam_fn_api',
@@ -210,16 +210,13 @@ class UtilTest(unittest.TestCase):
         })
     worker_pool = env.proto.workerPools[0]
 
-    # For the test, a third environment get added since actual default
-    # container image for Dataflow is different from 'test_default_image'
-    # we've provided above.
-    self.assertEqual(3, len(worker_pool.sdkHarnessContainerImages))
+    self.assertEqual(2, len(worker_pool.sdkHarnessContainerImages))
 
-    # Container image should be overridden by a Dataflow specific URL.
-    self.assertTrue(
-        str.startswith(
-            (worker_pool.sdkHarnessContainerImages[0]).containerImage,
-            'gcr.io/cloud-dataflow/v1beta3/python'))
+    images_from_proto = [
+        sdk_info.containerImage
+        for sdk_info in worker_pool.sdkHarnessContainerImages
+    ]
+    self.assertTrue('test_default_image' in images_from_proto)
 
   def test_sdk_harness_container_image_overrides(self):
     test_environment = DockerEnvironment(
@@ -293,6 +290,64 @@ class UtilTest(unittest.TestCase):
 
     self.assertTrue(found_override)
 
+  # TODDO(BEAM-9455): remove following test when Dataflow supports environment
+  # specific dependency provisioning.
+  def test_java_sdk_harness_dedup(self):
+    pipeline_options = PipelineOptions([
+        '--experiments=beam_fn_api',
+        '--experiments=use_unified_worker',
+        '--temp_location',
+        'gs://any-location/temp'
+    ])
+
+    pipeline = Pipeline(options=pipeline_options)
+    pipeline | Create([1, 2, 3]) | ParDo(DoFn())  # pylint:disable=expression-not-assigned
+
+    proto_pipeline, _ = pipeline.to_runner_api(return_context=True)
+
+    dummy_env_1 = beam_runner_api_pb2.Environment(
+        urn=common_urns.environments.DOCKER.urn,
+        payload=(
+            beam_runner_api_pb2.DockerPayload(
+                container_image='apache/beam_java:dummy_tag')
+        ).SerializeToString())
+    proto_pipeline.components.environments['dummy_env_id_1'].CopyFrom(
+        dummy_env_1)
+
+    dummy_transform_1 = beam_runner_api_pb2.PTransform(
+        environment_id='dummy_env_id_1')
+    proto_pipeline.components.transforms['dummy_transform_id_1'].CopyFrom(
+        dummy_transform_1)
+
+    dummy_env_2 = beam_runner_api_pb2.Environment(
+        urn=common_urns.environments.DOCKER.urn,
+        payload=(
+            beam_runner_api_pb2.DockerPayload(
+                container_image='apache/beam_java:dummy_tag')
+        ).SerializeToString())
+    proto_pipeline.components.environments['dummy_env_id_2'].CopyFrom(
+        dummy_env_2)
+
+    dummy_transform_2 = beam_runner_api_pb2.PTransform(
+        environment_id='dummy_env_id_2')
+    proto_pipeline.components.transforms['dummy_transform_id_2'].CopyFrom(
+        dummy_transform_2)
+
+    # Accessing non-public method for testing.
+    apiclient.DataflowApplicationClient._apply_sdk_environment_overrides(
+        proto_pipeline, dict(), pipeline_options)
+
+    # Only one of 'dummy_env_id_1' or 'dummy_env_id_2' should be in the set of
+    # environment IDs used by the proto after Java environment de-duping.
+    env_ids_from_transforms = [
+        proto_pipeline.components.transforms[transform_id].environment_id
+        for transform_id in proto_pipeline.components.transforms
+    ]
+    if 'dummy_env_id_1' in env_ids_from_transforms:
+      self.assertTrue('dummy_env_id_2' not in env_ids_from_transforms)
+    else:
+      self.assertTrue('dummy_env_id_2' in env_ids_from_transforms)
+
   def test_non_apache_container_not_overridden(self):
     pipeline_options = PipelineOptions([
         '--experiments=beam_fn_api',
diff --git a/sdks/python/apache_beam/transforms/environments.py b/sdks/python/apache_beam/transforms/environments.py
index 282ef9c81f6..8769418fbd3 100644
--- a/sdks/python/apache_beam/transforms/environments.py
+++ b/sdks/python/apache_beam/transforms/environments.py
@@ -88,12 +88,20 @@ def looks_like_json(s):
 
 APACHE_BEAM_DOCKER_IMAGE_PREFIX = 'apache/beam'
 
+APACHE_BEAM_JAVA_CONTAINER_NAME_PREFIX = 'beam_java'
+
 
 def is_apache_beam_container(container_image):
   return container_image and container_image.startswith(
       APACHE_BEAM_DOCKER_IMAGE_PREFIX)
 
 
+def is_beam_java_container_name(container_image):
+  return (
+      '/' in container_image and (container_image.rsplit(
+          '/', 1)[1]).startswith(APACHE_BEAM_JAVA_CONTAINER_NAME_PREFIX))
+
+
 class Environment(object):
   """Abstract base class for environments.
 
