diff --git a/CHANGES.md b/CHANGES.md
index eaa11f3a420..6f613aad24d 100644
--- a/CHANGES.md
+++ b/CHANGES.md
@@ -72,8 +72,10 @@
   This new Guava version may introduce dependency conflicts if your project or dependencies rely
   on removed APIs. If affected, ensure to use an appropriate Guava version via `dependencyManagement` in Maven and
   `force` in Gradle.
-* Deterministic coding enforced for GroupByKey and Stateful DoFns.  Previously non-deterministic coding was allowed, resulting in keys not properly being grouped in some cases. ([BEAM-11719](https://issues.apache.org/jira/browse/BEAM-11719)) To restore the old behavior, one can register `FakeDeterministicFastPrimitivesCoder` with
-  `beam.coders.registry.register_fallback_coder(beam.coders.coders.FakeDeterministicFastPrimitivesCoder())`.
+* Deterministic coding enforced for GroupByKey and Stateful DoFns.  Previously non-deterministic coding was allowed, resulting in keys not properly being grouped in some cases. ([BEAM-11719](https://issues.apache.org/jira/browse/BEAM-11719))
+  To restore the old behavior, one can register `FakeDeterministicFastPrimitivesCoder` with
+  `beam.coders.registry.register_fallback_coder(beam.coders.coders.FakeDeterministicFastPrimitivesCoder())`
+  or use the `allow_non_deterministic_key_coders` pipeline option.
 * X behavior was changed ([BEAM-X](https://issues.apache.org/jira/browse/BEAM-X)).
 
 ## Deprecations
diff --git a/sdks/python/apache_beam/options/pipeline_options.py b/sdks/python/apache_beam/options/pipeline_options.py
index bda9df188d6..31832e4951c 100644
--- a/sdks/python/apache_beam/options/pipeline_options.py
+++ b/sdks/python/apache_beam/options/pipeline_options.py
@@ -516,6 +516,14 @@ class TypeOptions(PipelineOptions):
         help='Enable faster type checking via sampling at pipeline execution '
         'time. NOTE: only supported with portable runners '
         '(including the DirectRunner)')
+    parser.add_argument(
+        '--allow_non_deterministic_key_coders',
+        default=False,
+        action='store_true',
+        help='Use non-deterministic coders (such as pickling) for key-grouping '
+        'operations such as GropuByKey.  This is unsafe, as runners may group '
+        'keys based on their encoded bytes, but is available for backwards '
+        'compatibility. See BEAM-11719.')
 
   def validate(self, unused_validator):
     errors = []
diff --git a/sdks/python/apache_beam/pipeline.py b/sdks/python/apache_beam/pipeline.py
index 2d6dcf5acbe..372bde86f35 100644
--- a/sdks/python/apache_beam/pipeline.py
+++ b/sdks/python/apache_beam/pipeline.py
@@ -842,6 +842,9 @@ class Pipeline(object):
     # We also only handle single-input, and (for fixing the output) single
     # output, which is sufficient.
     # Also marks such values as requiring deterministic key coders.
+    deterministic_key_coders = not self._options.view_as(
+        TypeOptions).allow_non_deterministic_key_coders
+
     class ForceKvInputTypes(PipelineVisitor):
       def enter_composite_transform(self, transform_node):
         # type: (AppliedPTransform) -> None
@@ -855,7 +858,8 @@ class Pipeline(object):
           pcoll = transform_node.inputs[0]
           pcoll.element_type = typehints.coerce_to_kv_type(
               pcoll.element_type, transform_node.full_label)
-          pcoll.requires_deterministic_key_coder = transform_node.full_label
+          pcoll.requires_deterministic_key_coder = (
+              deterministic_key_coders and transform_node.full_label)
           if len(transform_node.outputs) == 1:
             # The runner often has expectations about the output types as well.
             output, = transform_node.outputs.values()
@@ -866,7 +870,7 @@ class Pipeline(object):
                            typehints.TupleHint.TupleConstraint) and
                 len(output.element_type.tuple_types) == 2):
               output.requires_deterministic_key_coder = (
-                  transform_node.full_label)
+                  deterministic_key_coders and transform_node.full_label)
         for side_input in transform_node.transform.side_inputs:
           if side_input.requires_keyed_input():
             side_input.pvalue.element_type = typehints.coerce_to_kv_type(
@@ -874,7 +878,7 @@ class Pipeline(object):
                 transform_node.full_label,
                 side_input_producer=side_input.pvalue.producer.full_label)
             side_input.pvalue.requires_deterministic_key_coder = (
-                transform_node.full_label)
+                deterministic_key_coders and transform_node.full_label)
 
     self.visit(ForceKvInputTypes())
 
diff --git a/sdks/python/apache_beam/runners/dataflow/dataflow_runner.py b/sdks/python/apache_beam/runners/dataflow/dataflow_runner.py
index 9302a287998..0303a43d49b 100644
--- a/sdks/python/apache_beam/runners/dataflow/dataflow_runner.py
+++ b/sdks/python/apache_beam/runners/dataflow/dataflow_runner.py
@@ -270,7 +270,7 @@ class DataflowRunner(PipelineRunner):
     return element
 
   @staticmethod
-  def group_by_key_input_visitor():
+  def group_by_key_input_visitor(deterministic_key_coders=True):
     # Imported here to avoid circular dependencies.
     from apache_beam.pipeline import PipelineVisitor
 
@@ -292,19 +292,23 @@ class DataflowRunner(PipelineRunner):
           pcoll = transform_node.inputs[0]
           pcoll.element_type = typehints.coerce_to_kv_type(
               pcoll.element_type, transform_node.full_label)
-          pcoll.requires_deterministic_key_coder = transform_node.full_label
+          pcoll.requires_deterministic_key_coder = (
+              deterministic_key_coders and transform_node.full_label)
           key_type, value_type = pcoll.element_type.tuple_types
           if transform_node.outputs:
             key = DataflowRunner._only_element(transform_node.outputs.keys())
             transform_node.outputs[key].element_type = typehints.KV[
                 key_type, typehints.Iterable[value_type]]
             transform_node.outputs[key].requires_deterministic_key_coder = (
-                transform_node.full_label)
+                deterministic_key_coders and transform_node.full_label)
 
     return GroupByKeyInputVisitor()
 
   @staticmethod
-  def side_input_visitor(use_unified_worker=False, use_fn_api=False):
+  def side_input_visitor(
+      use_unified_worker=False,
+      use_fn_api=False,
+      deterministic_key_coders=True):
     # Imported here to avoid circular dependencies.
     # pylint: disable=wrong-import-order, wrong-import-position
     from apache_beam.pipeline import PipelineVisitor
@@ -356,7 +360,7 @@ class DataflowRunner(PipelineRunner):
               side_input.pvalue.element_type = typehints.coerce_to_kv_type(
                   side_input.pvalue.element_type, transform_node.full_label)
               side_input.pvalue.requires_deterministic_key_coder = (
-                  transform_node.full_label)
+                  deterministic_key_coders and transform_node.full_label)
               new_side_input = _DataflowMultimapSideInput(side_input)
             else:
               raise ValueError(
@@ -439,7 +443,10 @@ class DataflowRunner(PipelineRunner):
   def _adjust_pipeline_for_dataflow_v2(self, pipeline):
     # Dataflow runner requires a KV type for GBK inputs, hence we enforce that
     # here.
-    pipeline.visit(self.group_by_key_input_visitor())
+    pipeline.visit(
+        self.group_by_key_input_visitor(
+            not pipeline._options.view_as(
+                TypeOptions).allow_non_deterministic_key_coders))
 
   def _check_for_unsupported_features_on_non_portable_worker(self, pipeline):
     pipeline.visit(self.combinefn_visitor())
@@ -476,7 +483,9 @@ class DataflowRunner(PipelineRunner):
     pipeline.visit(
         self.side_input_visitor(
             apiclient._use_unified_worker(options),
-            apiclient._use_fnapi(options)))
+            apiclient._use_fnapi(options),
+            deterministic_key_coders=not options.view_as(
+                TypeOptions).allow_non_deterministic_key_coders))
 
     # Performing configured PTransform overrides.  Note that this is currently
     # done before Runner API serialization, since the new proto needs to contain
diff --git a/sdks/python/apache_beam/runners/portability/fn_api_runner/fn_runner.py b/sdks/python/apache_beam/runners/portability/fn_api_runner/fn_runner.py
index 0c88299266a..4b47b5e76ef 100644
--- a/sdks/python/apache_beam/runners/portability/fn_api_runner/fn_runner.py
+++ b/sdks/python/apache_beam/runners/portability/fn_api_runner/fn_runner.py
@@ -155,7 +155,10 @@ class FnApiRunner(runner.PipelineRunner):
     # are known to be KVs.
     from apache_beam.runners.dataflow.dataflow_runner import DataflowRunner
     # TODO: Move group_by_key_input_visitor() to a non-dataflow specific file.
-    pipeline.visit(DataflowRunner.group_by_key_input_visitor())
+    pipeline.visit(
+        DataflowRunner.group_by_key_input_visitor(
+            not options.view_as(pipeline_options.TypeOptions).
+            allow_non_deterministic_key_coders))
     self._bundle_repeat = self._bundle_repeat or options.view_as(
         pipeline_options.DirectOptions).direct_runner_bundle_repeat
     pipeline_direct_num_workers = options.view_as(
diff --git a/sdks/python/apache_beam/transforms/ptransform_test.py b/sdks/python/apache_beam/transforms/ptransform_test.py
index 14115b6420d..73eabe2b11d 100644
--- a/sdks/python/apache_beam/transforms/ptransform_test.py
+++ b/sdks/python/apache_beam/transforms/ptransform_test.py
@@ -554,6 +554,27 @@ class PTransformTest(unittest.TestCase):
           'CheckGrouped')
       assert_that(combined, equal_to([(0, 20), (1, 25)]), 'CheckCombined')
 
+  def test_group_by_key_non_determanistic_coder(self):
+    with self.assertRaises(Exception) as e:
+      with TestPipeline() as pipeline:
+        grouped = (
+            pipeline
+            | beam.Create([(PickledObject(10), None)])
+            | beam.GroupByKey()
+            | beam.MapTuple(lambda k, v: list(v)))
+
+  def test_group_by_key_allow_non_determanistic_coder(self):
+    with TestPipeline() as pipeline:
+      # The GroupByKey below would fail without this option.
+      pipeline._options.view_as(
+          TypeOptions).allow_non_deterministic_key_coders = True
+      grouped = (
+          pipeline
+          | beam.Create([(PickledObject(10), None)])
+          | beam.GroupByKey()
+          | beam.MapTuple(lambda k, v: list(v)))
+    assert_that(grouped, equal_to([[None]]))
+
   def test_group_by_key_fake_determanistic_coder(self):
     fresh_registry = beam.coders.typecoders.CoderRegistry()
     with patch.object(
