diff --git a/sdks/python/apache_beam/runners/common.py b/sdks/python/apache_beam/runners/common.py
index d427a53ca81..44408e2586a 100644
--- a/sdks/python/apache_beam/runners/common.py
+++ b/sdks/python/apache_beam/runners/common.py
@@ -98,41 +98,6 @@ class NameContext(object):
     return self.step_name
 
 
-# TODO(BEAM-4028): Move DataflowNameContext to Dataflow internal code.
-class DataflowNameContext(NameContext):
-  """Holds the name information for a step in Dataflow.
-
-  This includes a step_name (e.g. s2), a user_name (e.g. Foo/Bar/ParDo(Fab)),
-  and a system_name (e.g. s2-shuffle-read34)."""
-  def __init__(self, step_name, user_name, system_name):
-    """Creates a new step NameContext.
-
-    Args:
-      step_name: The internal name of the step (e.g. s2).
-      user_name: The full user-given name of the step (e.g. Foo/Bar/ParDo(Far)).
-      system_name: The step name in the optimized graph (e.g. s2-1).
-    """
-    super().__init__(step_name)
-    self.user_name = user_name
-    self.system_name = system_name
-
-  def __eq__(self, other):
-    return (
-        self.step_name == other.step_name and
-        self.user_name == other.user_name and
-        self.system_name == other.system_name)
-
-  def __hash__(self):
-    return hash((self.step_name, self.user_name, self.system_name))
-
-  def __repr__(self):
-    return 'DataflowNameContext(%s)' % self.__dict__
-
-  def logging_name(self):
-    """Stackdriver logging relies on user-given step names (e.g. Foo/Bar)."""
-    return self.user_name
-
-
 class Receiver(object):
   """For internal use only; no backwards-compatibility guarantees.
 
diff --git a/sdks/python/apache_beam/runners/worker/bundle_processor.py b/sdks/python/apache_beam/runners/worker/bundle_processor.py
index 532b7dd6960..3ce8fe58da5 100644
--- a/sdks/python/apache_beam/runners/worker/bundle_processor.py
+++ b/sdks/python/apache_beam/runners/worker/bundle_processor.py
@@ -119,7 +119,7 @@ class RunnerIOOperation(operations.Operation):
   """Common baseclass for runner harness IO operations."""
 
   def __init__(self,
-               name_context,  # type: Union[str, common.NameContext]
+               name_context,  # type: common.NameContext
                step_name,  # type: Any
                consumers,  # type: Mapping[Any, Iterable[operations.Operation]]
                counter_factory,  # type: counters.CounterFactory
@@ -164,7 +164,7 @@ class DataInputOperation(RunnerIOOperation):
   """A source-like operation that gathers input from the runner."""
 
   def __init__(self,
-               operation_name,  # type: Union[str, common.NameContext]
+               operation_name,  # type: common.NameContext
                step_name,
                consumers,  # type: Mapping[Any, List[operations.Operation]]
                counter_factory,  # type: counters.CounterFactory
diff --git a/sdks/python/apache_beam/runners/worker/operation_specs.py b/sdks/python/apache_beam/runners/worker/operation_specs.py
index 75fccd864d4..b603d6c9c70 100644
--- a/sdks/python/apache_beam/runners/worker/operation_specs.py
+++ b/sdks/python/apache_beam/runners/worker/operation_specs.py
@@ -26,7 +26,6 @@ source, write to a sink, parallel do, etc.
 import collections
 
 from apache_beam import coders
-from apache_beam.runners import common
 
 # This module is experimental. No backwards-compatibility guarantees.
 
@@ -371,73 +370,3 @@ def get_coder_from_spec(coder_spec):
   # We pass coders in the form "<coder_name>$<pickled_data>" to make the job
   # description JSON more readable.
   return coders.coders.deserialize_coder(coder_spec['@type'].encode('ascii'))
-
-
-class MapTask(object):
-  """A map task decoded into operations and ready to be executed.
-
-  Attributes:
-    operations: A list of Worker* object created by parsing the instructions
-      within the map task.
-    stage_name: The name of this map task execution stage.
-    system_names: The system names of the step corresponding to each map task
-      operation in the execution graph.
-    step_names: The user-given names of the step corresponding to each map task
-      operation (e.g. Foo/Bar/ParDo).
-    original_names: The internal name of a step in the original workflow graph.
-    name_contexts: A common.NameContext object containing name information
-      about a step.
-  """
-  def __init__(
-      self,
-      operations,
-      stage_name,
-      system_names=None,
-      step_names=None,
-      original_names=None,
-      name_contexts=None):
-    # TODO(BEAM-4028): Remove arguments other than name_contexts.
-    self.operations = operations
-    self.stage_name = stage_name
-    self.name_contexts = name_contexts or self._make_name_contexts(
-        original_names, step_names, system_names)
-
-  @staticmethod
-  def _make_name_contexts(original_names, user_names, system_names):
-    # TODO(BEAM-4028): Remove method once map task relies on name contexts.
-    return [
-        common.DataflowNameContext(step_name, user_name, system_name)
-        for step_name,
-        user_name,
-        system_name in zip(original_names, user_names, system_names)
-    ]
-
-  @property
-  def system_names(self):
-    """Returns a list containing the system names of steps.
-
-    A System name is the name of a step in the optimized Dataflow graph.
-    """
-    return [nc.system_name for nc in self.name_contexts]
-
-  @property
-  def original_names(self):
-    """Returns a list containing the original names of steps.
-
-    An original name is the internal name of a step in the Dataflow graph
-    (e.g. 's2').
-    """
-    return [nc.step_name for nc in self.name_contexts]
-
-  @property
-  def step_names(self):
-    """Returns a list containing the user names of steps.
-
-    In this context, a step name is the user-given name of a step in the
-    Dataflow graph (e.g. 's2').
-    """
-    return [nc.user_name for nc in self.name_contexts]
-
-  def __str__(self):
-    return '<%s %s steps=%s>' % (
-        self.__class__.__name__, self.stage_name, '+'.join(self.step_names))
diff --git a/sdks/python/apache_beam/runners/worker/operations.py b/sdks/python/apache_beam/runners/worker/operations.py
index 1de42f9908d..1d8173eb2b0 100644
--- a/sdks/python/apache_beam/runners/worker/operations.py
+++ b/sdks/python/apache_beam/runners/worker/operations.py
@@ -232,7 +232,7 @@ class Operation(object):
   """
 
   def __init__(self,
-               name_context,  # type: Union[str, common.NameContext]
+               name_context,  # type: common.NameContext
                spec,
                counter_factory,
                state_sampler  # type: StateSampler
@@ -240,19 +240,14 @@ class Operation(object):
     """Initializes a worker operation instance.
 
     Args:
-      name_context: A NameContext instance or string(deprecated), with the
-        name information for this operation.
+      name_context: A NameContext instance, with the name information for this
+        operation.
       spec: A operation_specs.Worker* instance.
       counter_factory: The CounterFactory to use for our counters.
       state_sampler: The StateSampler for the current operation.
     """
-    if isinstance(name_context, common.NameContext):
-      # TODO(BEAM-4028): Clean this up once it's completely migrated.
-      # We use the specific operation name that is used for metrics and state
-      # sampling.
-      self.name_context = name_context
-    else:
-      self.name_context = common.NameContext(name_context)
+    assert isinstance(name_context, common.NameContext)
+    self.name_context = name_context
 
     self.spec = spec
     self.counter_factory = counter_factory
@@ -501,7 +496,7 @@ class ReadOperation(Operation):
 class ImpulseReadOperation(Operation):
   def __init__(
       self,
-      name_context,  # type: Union[str, common.NameContext]
+      name_context,  # type: common.NameContext
       counter_factory,
       state_sampler,  # type: StateSampler
       consumers,  # type: Mapping[Any, List[Operation]]
