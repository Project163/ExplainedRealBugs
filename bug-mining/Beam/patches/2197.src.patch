diff --git a/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ConsumerSpEL.java b/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ConsumerSpEL.java
index 03dd9288694..3d38f90fdd6 100644
--- a/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ConsumerSpEL.java
+++ b/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ConsumerSpEL.java
@@ -43,32 +43,40 @@ import org.springframework.expression.spel.support.StandardEvaluationContext;
  * It auto detects the input type List/Collection/Varargs, to eliminate the method definition
  * differences.
  */
-@SuppressWarnings({
-  "rawtypes" // TODO(https://issues.apache.org/jira/browse/BEAM-10556)
-})
 class ConsumerSpEL {
 
   private static final Logger LOG = LoggerFactory.getLogger(ConsumerSpEL.class);
 
-  private SpelParserConfiguration config = new SpelParserConfiguration(true, true);
-  private ExpressionParser parser = new SpelExpressionParser(config);
+  private static final SpelParserConfiguration config = new SpelParserConfiguration(true, true);
+  private static final ExpressionParser parser = new SpelExpressionParser(config);
 
-  private Expression seek2endExpression = parser.parseExpression("#consumer.seekToEnd(#tp)");
+  // This method changed from accepting varargs to accepting a Collection.
+  private static final Expression seek2endExpression =
+      parser.parseExpression("#consumer.seekToEnd(#tp)");
+  // This method changed from accepting a List to accepting a Collection.
+  private static final Expression assignExpression =
+      parser.parseExpression("#consumer.assign(#tp)");
 
-  private Expression assignExpression = parser.parseExpression("#consumer.assign(#tp)");
+  private static boolean hasRecordTimestamp;
+  private static boolean hasHeaders;
+  private static boolean hasOffsetsForTimes;
+  private static boolean deserializerSupportsHeaders;
 
-  private Expression deserializeWithHeadersExpression =
-      parser.parseExpression("#deserializer.deserialize(#topic, #headers, #data)");
-
-  private boolean hasRecordTimestamp = false;
-  private boolean hasOffsetsForTimes = false;
-  private boolean deserializerSupportsHeaders = false;
+  static {
+    try {
+      // It is supported by Kafka Client 0.10.0.0 onwards.
+      hasRecordTimestamp =
+          ConsumerRecord.class
+              .getMethod("timestamp", (Class<?>[]) null)
+              .getReturnType()
+              .equals(Long.TYPE);
+    } catch (NoSuchMethodException | SecurityException e) {
+      LOG.debug("Timestamp for Kafka message is not available.");
+    }
 
-  static boolean hasHeaders() {
-    boolean clientHasHeaders = false;
     try {
       // It is supported by Kafka Client 0.11.0.0 onwards.
-      clientHasHeaders =
+      hasHeaders =
           "org.apache.kafka.common.header.Headers"
               .equals(
                   ConsumerRecord.class
@@ -78,20 +86,6 @@ class ConsumerSpEL {
     } catch (NoSuchMethodException | SecurityException e) {
       LOG.debug("Headers is not available");
     }
-    return clientHasHeaders;
-  }
-
-  public ConsumerSpEL() {
-    try {
-      // It is supported by Kafka Client 0.10.0.0 onwards.
-      hasRecordTimestamp =
-          ConsumerRecord.class
-              .getMethod("timestamp", (Class<?>[]) null)
-              .getReturnType()
-              .equals(Long.TYPE);
-    } catch (NoSuchMethodException | SecurityException e) {
-      LOG.debug("Timestamp for Kafka message is not available.");
-    }
 
     try {
       // It is supported by Kafka Client 0.10.1.0 onwards.
@@ -115,38 +109,30 @@ class ConsumerSpEL {
     }
   }
 
-  public void evaluateSeek2End(Consumer consumer, TopicPartition topicPartition) {
+  public static void evaluateSeek2End(Consumer<?, ?> consumer, TopicPartition topicPartition) {
     StandardEvaluationContext mapContext = new StandardEvaluationContext();
     mapContext.setVariable("consumer", consumer);
     mapContext.setVariable("tp", topicPartition);
     seek2endExpression.getValue(mapContext);
   }
 
-  public void evaluateAssign(Consumer consumer, Collection<TopicPartition> topicPartitions) {
+  public static void evaluateAssign(
+      Consumer<?, ?> consumer, Collection<TopicPartition> topicPartitions) {
     StandardEvaluationContext mapContext = new StandardEvaluationContext();
     mapContext.setVariable("consumer", consumer);
     mapContext.setVariable("tp", topicPartitions);
     assignExpression.getValue(mapContext);
   }
 
-  public Object evaluateDeserializeWithHeaders(
-      Deserializer deserializer, ConsumerRecord<byte[], byte[]> rawRecord, Boolean isKey) {
-    StandardEvaluationContext mapContext = new StandardEvaluationContext();
-    mapContext.setVariable("deserializer", deserializer);
-    mapContext.setVariable("topic", rawRecord.topic());
-    mapContext.setVariable("headers", rawRecord.headers());
-    mapContext.setVariable("data", isKey ? rawRecord.key() : rawRecord.value());
-    return deserializeWithHeadersExpression.getValue(mapContext);
-  }
-
-  public long getRecordTimestamp(ConsumerRecord<byte[], byte[]> rawRecord) {
+  public static long getRecordTimestamp(ConsumerRecord<byte[], byte[]> rawRecord) {
     if (hasRecordTimestamp) {
       return rawRecord.timestamp();
     }
     return -1L; // This is the timestamp used in Kafka for older messages without timestamps.
   }
 
-  public KafkaTimestampType getRecordTimestampType(ConsumerRecord<byte[], byte[]> rawRecord) {
+  public static KafkaTimestampType getRecordTimestampType(
+      ConsumerRecord<byte[], byte[]> rawRecord) {
     if (hasRecordTimestamp) {
       return KafkaTimestampType.forOrdinal(rawRecord.timestampType().ordinal());
     } else {
@@ -154,29 +140,33 @@ class ConsumerSpEL {
     }
   }
 
-  public boolean hasOffsetsForTimes() {
+  public static boolean hasOffsetsForTimes() {
     return hasOffsetsForTimes;
   }
 
-  public boolean deserializerSupportsHeaders() {
+  public static boolean hasHeaders() {
+    return hasHeaders;
+  }
+
+  public static boolean deserializerSupportsHeaders() {
     return deserializerSupportsHeaders;
   }
 
-  public Object deserializeKey(
-      Deserializer deserializer, ConsumerRecord<byte[], byte[]> rawRecord) {
+  public static <T> T deserializeKey(
+      Deserializer<T> deserializer, ConsumerRecord<byte[], byte[]> rawRecord) {
     if (deserializerSupportsHeaders) {
       // Kafka API 2.1.0 onwards
-      return evaluateDeserializeWithHeaders(deserializer, rawRecord, true);
+      return deserializer.deserialize(rawRecord.topic(), rawRecord.headers(), rawRecord.key());
     } else {
       return deserializer.deserialize(rawRecord.topic(), rawRecord.key());
     }
   }
 
-  public Object deserializeValue(
-      Deserializer deserializer, ConsumerRecord<byte[], byte[]> rawRecord) {
+  public static <T> T deserializeValue(
+      Deserializer<T> deserializer, ConsumerRecord<byte[], byte[]> rawRecord) {
     if (deserializerSupportsHeaders) {
       // Kafka API 2.1.0 onwards
-      return evaluateDeserializeWithHeaders(deserializer, rawRecord, false);
+      return deserializer.deserialize(rawRecord.topic(), rawRecord.headers(), rawRecord.value());
     } else {
       return deserializer.deserialize(rawRecord.topic(), rawRecord.value());
     }
@@ -187,7 +177,8 @@ class ConsumerSpEL {
    * no messages later than timestamp or if this partition does not support timestamp based offset.
    */
   @SuppressWarnings("unchecked")
-  public long offsetForTime(Consumer<?, ?> consumer, TopicPartition topicPartition, Instant time) {
+  public static long offsetForTime(
+      Consumer<?, ?> consumer, TopicPartition topicPartition, Instant time) {
 
     checkArgument(hasOffsetsForTimes, "This Kafka Client must support Consumer.OffsetsForTimes().");
 
diff --git a/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaCommitOffset.java b/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaCommitOffset.java
index 54f561094a2..78100d97b24 100644
--- a/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaCommitOffset.java
+++ b/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaCommitOffset.java
@@ -65,8 +65,6 @@ public class KafkaCommitOffset<K, V>
     private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>
         consumerFactoryFn;
 
-    private transient ConsumerSpEL consumerSpEL = null;
-
     CommitOffsetDoFn(KafkaIO.ReadSourceDescriptors readSourceDescriptors) {
       offsetConsumerConfig = readSourceDescriptors.getOffsetConsumerConfig();
       consumerConfig = readSourceDescriptors.getConsumerConfig();
diff --git a/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java b/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java
index 04b48fa8797..3f24412949d 100644
--- a/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java
+++ b/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java
@@ -1058,8 +1058,7 @@ public class KafkaIO {
       checkArgument(getKeyDeserializerProvider() != null, "withKeyDeserializer() is required");
       checkArgument(getValueDeserializerProvider() != null, "withValueDeserializer() is required");
 
-      ConsumerSpEL consumerSpEL = new ConsumerSpEL();
-      if (!consumerSpEL.hasOffsetsForTimes()) {
+      if (!ConsumerSpEL.hasOffsetsForTimes()) {
         LOG.warn(
             "Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and "
                 + "may not be supported in next release of Apache Beam. "
@@ -1068,7 +1067,7 @@ public class KafkaIO {
       }
       if (getStartReadTime() != null) {
         checkArgument(
-            consumerSpEL.hasOffsetsForTimes(),
+            ConsumerSpEL.hasOffsetsForTimes(),
             "Consumer.offsetsForTimes is only supported by Kafka Client 0.10.1.0 onwards, "
                 + "current version of Kafka Client is "
                 + AppInfoParser.getVersion()
@@ -1670,8 +1669,7 @@ public class KafkaIO {
       checkArgument(getKeyDeserializerProvider() != null, "withKeyDeserializer() is required");
       checkArgument(getValueDeserializerProvider() != null, "withValueDeserializer() is required");
 
-      ConsumerSpEL consumerSpEL = new ConsumerSpEL();
-      if (!consumerSpEL.hasOffsetsForTimes()) {
+      if (!ConsumerSpEL.hasOffsetsForTimes()) {
         LOG.warn(
             "Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and "
                 + "may not be supported in next release of Apache Beam. "
diff --git a/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaUnboundedReader.java b/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaUnboundedReader.java
index be32c47fbd9..aaeb1b40c65 100644
--- a/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaUnboundedReader.java
+++ b/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaUnboundedReader.java
@@ -82,7 +82,7 @@ class KafkaUnboundedReader<K, V> extends UnboundedReader<KafkaRecord<K, V>> {
   public boolean start() throws IOException {
     Read<K, V> spec = source.getSpec();
     consumer = spec.getConsumerFactoryFn().apply(spec.getConsumerConfig());
-    consumerSpEL.evaluateAssign(consumer, spec.getTopicPartitions());
+    ConsumerSpEL.evaluateAssign(consumer, spec.getTopicPartitions());
 
     keyDeserializerInstance =
         spec.getKeyDeserializerProvider().getDeserializer(spec.getConsumerConfig(), true);
@@ -130,7 +130,7 @@ class KafkaUnboundedReader<K, V> extends UnboundedReader<KafkaRecord<K, V>> {
             name, spec.getOffsetConsumerConfig(), spec.getConsumerConfig());
 
     offsetConsumer = spec.getConsumerFactoryFn().apply(offsetConsumerConfig);
-    consumerSpEL.evaluateAssign(offsetConsumer, spec.getTopicPartitions());
+    ConsumerSpEL.evaluateAssign(offsetConsumer, spec.getTopicPartitions());
 
     // Fetch offsets once before running periodically.
     updateLatestOffsets();
@@ -192,11 +192,11 @@ class KafkaUnboundedReader<K, V> extends UnboundedReader<KafkaRecord<K, V>> {
                 rawRecord.topic(),
                 rawRecord.partition(),
                 rawRecord.offset(),
-                consumerSpEL.getRecordTimestamp(rawRecord),
-                consumerSpEL.getRecordTimestampType(rawRecord),
+                ConsumerSpEL.getRecordTimestamp(rawRecord),
+                ConsumerSpEL.getRecordTimestampType(rawRecord),
                 ConsumerSpEL.hasHeaders() ? rawRecord.headers() : null,
-                (K) consumerSpEL.deserializeKey(keyDeserializerInstance, rawRecord),
-                (V) consumerSpEL.deserializeValue(valueDeserializerInstance, rawRecord));
+                ConsumerSpEL.deserializeKey(keyDeserializerInstance, rawRecord),
+                ConsumerSpEL.deserializeValue(valueDeserializerInstance, rawRecord));
 
         curTimestamp =
             pState.timestampPolicy.getTimestampForRecord(pState.mkTimestampPolicyContext(), record);
@@ -357,9 +357,6 @@ class KafkaUnboundedReader<K, V> extends UnboundedReader<KafkaRecord<K, V>> {
 
   private static final long UNINITIALIZED_OFFSET = -1;
 
-  // Add SpEL instance to cover the interface difference of Kafka client
-  private transient ConsumerSpEL consumerSpEL;
-
   /** watermark before any records have been read. */
   private static Instant initialWatermark = BoundedWindow.TIMESTAMP_MIN_VALUE;
 
@@ -464,7 +461,6 @@ class KafkaUnboundedReader<K, V> extends UnboundedReader<KafkaRecord<K, V>> {
 
   KafkaUnboundedReader(
       KafkaUnboundedSource<K, V> source, @Nullable KafkaCheckpointMark checkpointMark) {
-    this.consumerSpEL = new ConsumerSpEL();
     this.source = source;
     this.name = "Reader-" + source.getId();
 
@@ -616,7 +612,7 @@ class KafkaUnboundedReader<K, V> extends UnboundedReader<KafkaRecord<K, V>> {
       Instant startReadTime = spec.getStartReadTime();
       if (startReadTime != null) {
         pState.nextOffset =
-            consumerSpEL.offsetForTime(consumer, pState.topicPartition, spec.getStartReadTime());
+            ConsumerSpEL.offsetForTime(consumer, pState.topicPartition, spec.getStartReadTime());
         consumer.seek(pState.topicPartition, pState.nextOffset);
       } else {
         pState.nextOffset = consumer.position(pState.topicPartition);
@@ -630,7 +626,7 @@ class KafkaUnboundedReader<K, V> extends UnboundedReader<KafkaRecord<K, V>> {
     for (PartitionState p : partitionStates) {
       try {
         Instant fetchTime = Instant.now();
-        consumerSpEL.evaluateSeek2End(offsetConsumer, p.topicPartition);
+        ConsumerSpEL.evaluateSeek2End(offsetConsumer, p.topicPartition);
         long offset = offsetConsumer.position(p.topicPartition);
         p.setLatestOffset(offset, fetchTime);
       } catch (Exception e) {
diff --git a/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaUnboundedSource.java b/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaUnboundedSource.java
index d0ed7d44d7e..a516a6738af 100644
--- a/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaUnboundedSource.java
+++ b/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaUnboundedSource.java
@@ -83,6 +83,10 @@ class KafkaUnboundedSource<K, V> extends UnboundedSource<KafkaRecord<K, V>, Kafk
         "Could not find any partitions. Please check Kafka configuration and topic names");
 
     int numSplits = Math.min(desiredNumSplits, partitions.size());
+    // XXX make all splits have the same # of partitions
+    while (partitions.size() % numSplits > 0) {
+      ++numSplits;
+    }
     List<List<TopicPartition>> assignments = new ArrayList<>(numSplits);
 
     for (int i = 0; i < numSplits; i++) {
diff --git a/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaDoFn.java b/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaDoFn.java
index d12332af98b..c828ef26dbb 100644
--- a/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaDoFn.java
+++ b/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaDoFn.java
@@ -172,7 +172,6 @@ class ReadFromKafkaDoFn<K, V>
   private final TimestampPolicyFactory<K, V> timestampPolicyFactory;
 
   // Valid between bundle start and bundle finish.
-  private transient ConsumerSpEL consumerSpEL = null;
   private transient Deserializer<K> keyDeserializerInstance = null;
   private transient Deserializer<V> valueDeserializerInstance = null;
 
@@ -193,19 +192,17 @@ class ReadFromKafkaDoFn<K, V>
 
     private final Consumer<byte[], byte[]> offsetConsumer;
     private final TopicPartition topicPartition;
-    private final ConsumerSpEL consumerSpEL;
     private final Supplier<Long> memoizedBacklog;
 
     KafkaLatestOffsetEstimator(
         Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {
       this.offsetConsumer = offsetConsumer;
       this.topicPartition = topicPartition;
-      this.consumerSpEL = new ConsumerSpEL();
-      this.consumerSpEL.evaluateAssign(this.offsetConsumer, ImmutableList.of(this.topicPartition));
+      ConsumerSpEL.evaluateAssign(this.offsetConsumer, ImmutableList.of(this.topicPartition));
       memoizedBacklog =
           Suppliers.memoizeWithExpiration(
               () -> {
-                consumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);
+                ConsumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);
                 return offsetConsumer.position(topicPartition);
               },
               5,
@@ -235,14 +232,14 @@ class ReadFromKafkaDoFn<K, V>
         consumerFactoryFn.apply(
             KafkaIOUtils.getOffsetConsumerConfig(
                 "initialOffset", offsetConsumerConfig, updatedConsumerConfig))) {
-      consumerSpEL.evaluateAssign(
+      ConsumerSpEL.evaluateAssign(
           offsetConsumer, ImmutableList.of(kafkaSourceDescriptor.getTopicPartition()));
       long startOffset;
       if (kafkaSourceDescriptor.getStartReadOffset() != null) {
         startOffset = kafkaSourceDescriptor.getStartReadOffset();
       } else if (kafkaSourceDescriptor.getStartReadTime() != null) {
         startOffset =
-            consumerSpEL.offsetForTime(
+            ConsumerSpEL.offsetForTime(
                 offsetConsumer,
                 kafkaSourceDescriptor.getTopicPartition(),
                 kafkaSourceDescriptor.getStartReadTime());
@@ -329,7 +326,7 @@ class ReadFromKafkaDoFn<K, V>
         return ProcessContinuation.stop();
       }
 
-      consumerSpEL.evaluateAssign(
+      ConsumerSpEL.evaluateAssign(
           consumer, ImmutableList.of(kafkaSourceDescriptor.getTopicPartition()));
       long startOffset = tracker.currentRestriction().getFrom();
       long expectedOffset = startOffset;
@@ -352,11 +349,11 @@ class ReadFromKafkaDoFn<K, V>
                   rawRecord.topic(),
                   rawRecord.partition(),
                   rawRecord.offset(),
-                  consumerSpEL.getRecordTimestamp(rawRecord),
-                  consumerSpEL.getRecordTimestampType(rawRecord),
+                  ConsumerSpEL.getRecordTimestamp(rawRecord),
+                  ConsumerSpEL.getRecordTimestampType(rawRecord),
                   ConsumerSpEL.hasHeaders() ? rawRecord.headers() : null,
-                  (K) consumerSpEL.deserializeKey(keyDeserializerInstance, rawRecord),
-                  (V) consumerSpEL.deserializeValue(valueDeserializerInstance, rawRecord));
+                  ConsumerSpEL.deserializeKey(keyDeserializerInstance, rawRecord),
+                  ConsumerSpEL.deserializeValue(valueDeserializerInstance, rawRecord));
           int recordSize =
               (rawRecord.key() == null ? 0 : rawRecord.key().length)
                   + (rawRecord.value() == null ? 0 : rawRecord.value().length);
@@ -402,7 +399,6 @@ class ReadFromKafkaDoFn<K, V>
                     return new AverageRecordSize();
                   }
                 });
-    consumerSpEL = new ConsumerSpEL();
     keyDeserializerInstance = keyDeserializerProvider.getDeserializer(consumerConfig, true);
     valueDeserializerInstance = valueDeserializerProvider.getDeserializer(consumerConfig, false);
   }
diff --git a/sdks/java/io/kafka/src/test/java/org/apache/beam/sdk/io/kafka/KafkaIOTest.java b/sdks/java/io/kafka/src/test/java/org/apache/beam/sdk/io/kafka/KafkaIOTest.java
index 91b610bb74c..d17755c5a7b 100644
--- a/sdks/java/io/kafka/src/test/java/org/apache/beam/sdk/io/kafka/KafkaIOTest.java
+++ b/sdks/java/io/kafka/src/test/java/org/apache/beam/sdk/io/kafka/KafkaIOTest.java
@@ -26,7 +26,6 @@ import static org.hamcrest.Matchers.greaterThan;
 import static org.hamcrest.Matchers.hasItem;
 import static org.hamcrest.Matchers.isA;
 import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertNotEquals;
 import static org.junit.Assert.assertNotNull;
 import static org.junit.Assert.assertNull;
 import static org.junit.Assert.assertTrue;
@@ -508,50 +507,34 @@ public class KafkaIOTest {
 
   public static class IntegerDeserializerWithHeadersAssertor extends IntegerDeserializer
       implements Deserializer<Integer> {
-    ConsumerSpEL consumerSpEL = null;
 
     @Override
     public Integer deserialize(String topic, byte[] data) {
-      StackTraceElement[] stackTraceElements = Thread.currentThread().getStackTrace();
-      if (consumerSpEL == null) {
-        consumerSpEL = new ConsumerSpEL();
-      }
-      if (consumerSpEL.deserializerSupportsHeaders()) {
-        // Assert we have the default deserializer with headers API in the stack trace for Kafka API
-        // 2.1.0 onwards
-        try {
-          assertEquals(Deserializer.class, Class.forName(stackTraceElements[3].getClassName()));
-          assertEquals("deserialize", stackTraceElements[3].getMethodName());
-        } catch (ClassNotFoundException e) {
-        }
-      } else {
-        assertNotEquals("deserialize", stackTraceElements[3].getMethodName());
-      }
+      assertEquals(false, ConsumerSpEL.deserializerSupportsHeaders());
+      return super.deserialize(topic, data);
+    }
+
+    @Override
+    public Integer deserialize(String topic, Headers headers, byte[] data) {
+      // Overriding the default should trigger header evaluation and this to be called.
+      assertEquals(true, ConsumerSpEL.deserializerSupportsHeaders());
       return super.deserialize(topic, data);
     }
   }
 
   public static class LongDeserializerWithHeadersAssertor extends LongDeserializer
       implements Deserializer<Long> {
-    ConsumerSpEL consumerSpEL = null;
 
     @Override
     public Long deserialize(String topic, byte[] data) {
-      if (consumerSpEL == null) {
-        consumerSpEL = new ConsumerSpEL();
-      }
-      StackTraceElement[] stackTraceElements = Thread.currentThread().getStackTrace();
-      if (consumerSpEL.deserializerSupportsHeaders()) {
-        // Assert we have the default deserializer with headers API in the stack trace for Kafka API
-        // 2.1.0 onwards
-        try {
-          assertEquals(Deserializer.class, Class.forName(stackTraceElements[3].getClassName()));
-          assertEquals("deserialize", stackTraceElements[3].getMethodName());
-        } catch (ClassNotFoundException e) {
-        }
-      } else {
-        assertNotEquals("deserialize", stackTraceElements[3].getMethodName());
-      }
+      assertEquals(false, ConsumerSpEL.deserializerSupportsHeaders());
+      return super.deserialize(topic, data);
+    }
+
+    @Override
+    public Long deserialize(String topic, Headers headers, byte[] data) {
+      // Overriding the default should trigger header evaluation and this to be called.
+      assertEquals(true, ConsumerSpEL.deserializerSupportsHeaders());
       return super.deserialize(topic, data);
     }
   }
@@ -1597,7 +1580,7 @@ public class KafkaIOTest {
   @Test
   public void testUnboundedSourceStartReadTime() {
 
-    assumeTrue(new ConsumerSpEL().hasOffsetsForTimes());
+    assumeTrue(ConsumerSpEL.hasOffsetsForTimes());
 
     int numElements = 1000;
     // In this MockConsumer, we let the elements of the time and offset equal and there are 20
@@ -1621,7 +1604,7 @@ public class KafkaIOTest {
   @Test
   public void testUnboundedSourceStartReadTimeException() {
 
-    assumeTrue(new ConsumerSpEL().hasOffsetsForTimes());
+    assumeTrue(ConsumerSpEL.hasOffsetsForTimes());
 
     noMessagesException.expect(RuntimeException.class);
 
