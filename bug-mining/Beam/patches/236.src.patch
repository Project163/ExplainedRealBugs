diff --git a/sdks/python/apache_beam/io/fileio.py b/sdks/python/apache_beam/io/fileio.py
index f33942a846f..b128dc5a1c7 100644
--- a/sdks/python/apache_beam/io/fileio.py
+++ b/sdks/python/apache_beam/io/fileio.py
@@ -31,7 +31,6 @@ from apache_beam.io.filesystem import CompressionTypes
 from apache_beam.io.filesystems_util import get_filesystem
 from apache_beam.transforms.display import DisplayDataItem
 
-MAX_BATCH_OPERATION_SIZE = 100
 DEFAULT_SHARD_NAME_TEMPLATE = '-SSSSS-of-NNNNN'
 
 
@@ -244,6 +243,7 @@ class FileSink(iobase.Sink):
 
     source_files = []
     destination_files = []
+    chunk_size = self._file_system.CHUNK_SIZE
     for shard_num, shard in enumerate(writer_results):
       final_name = ''.join([
           self.file_path_prefix, self.shard_name_format % dict(
@@ -252,12 +252,12 @@ class FileSink(iobase.Sink):
       source_files.append(shard)
       destination_files.append(final_name)
 
-    source_file_batch = [source_files[i:i + MAX_BATCH_OPERATION_SIZE]
+    source_file_batch = [source_files[i:i + chunk_size]
                          for i in xrange(0, len(source_files),
-                                         MAX_BATCH_OPERATION_SIZE)]
-    destination_file_batch = [destination_files[i:i + MAX_BATCH_OPERATION_SIZE]
+                                         chunk_size)]
+    destination_file_batch = [destination_files[i:i + chunk_size]
                               for i in xrange(0, len(destination_files),
-                                              MAX_BATCH_OPERATION_SIZE)]
+                                              chunk_size)]
 
     logging.info(
         'Starting finalize_write threads with num_shards: %d, '
diff --git a/sdks/python/apache_beam/io/filesystem.py b/sdks/python/apache_beam/io/filesystem.py
index 85c7f066e6f..3a71ac15fc4 100644
--- a/sdks/python/apache_beam/io/filesystem.py
+++ b/sdks/python/apache_beam/io/filesystem.py
@@ -414,6 +414,7 @@ class FileSystem(object):
   the correct file system based on the provided file pattern scheme.
   """
   __metaclass__ = abc.ABCMeta
+  CHUNK_SIZE = 1  # Chuck size in the batch operations
 
   @staticmethod
   def _get_compression_type(path, compression_type):
diff --git a/sdks/python/apache_beam/io/gcp/gcsfilesystem.py b/sdks/python/apache_beam/io/gcp/gcsfilesystem.py
index d79630f7b11..b2bc8092cd8 100644
--- a/sdks/python/apache_beam/io/gcp/gcsfilesystem.py
+++ b/sdks/python/apache_beam/io/gcp/gcsfilesystem.py
@@ -31,6 +31,8 @@ class GCSFileSystem(FileSystem):
   """A GCS ``FileSystem`` implementation for accessing files on GCS.
   """
 
+  CHUNK_SIZE = gcsio.MAX_BATCH_OPERATION_SIZE  # Chuck size in batch operations
+
   def mkdirs(self, path):
     """Recursively create directories for the provided path.
 
@@ -174,7 +176,7 @@ class GCSFileSystem(FileSystem):
     gcs_current_batch = []
     for src, dest in zip(source_file_names, destination_file_names):
       gcs_current_batch.append((src, dest))
-      if len(gcs_current_batch) == gcsio.MAX_BATCH_OPERATION_SIZE:
+      if len(gcs_current_batch) == self.CHUNK_SIZE:
         gcs_batches.append(gcs_current_batch)
         gcs_current_batch = []
     if gcs_current_batch:
