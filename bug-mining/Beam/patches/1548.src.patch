diff --git a/sdks/python/apache_beam/io/gcp/bigquery.py b/sdks/python/apache_beam/io/gcp/bigquery.py
index 68a82b876ff..e685c8b1495 100644
--- a/sdks/python/apache_beam/io/gcp/bigquery.py
+++ b/sdks/python/apache_beam/io/gcp/bigquery.py
@@ -254,6 +254,7 @@ from apache_beam.io.filesystems import FileSystems
 from apache_beam.io.gcp import bigquery_tools
 from apache_beam.io.gcp.internal.clients import bigquery
 from apache_beam.io.iobase import BoundedSource
+from apache_beam.io.iobase import RangeTracker
 from apache_beam.io.iobase import SourceBundle
 from apache_beam.io.textio import _TextSource as TextSource
 from apache_beam.options import value_provider as vp
@@ -554,7 +555,7 @@ class _JsonToDictCoder(coders.Coder):
             for x in table_field_schemas]
 
   def decode(self, value):
-    value = json.loads(value)
+    value = json.loads(value.decode('utf-8'))
     return self._decode_with_schema(value, self.fields)
 
   def _decode_with_schema(self, value, schema_fields):
@@ -649,7 +650,16 @@ class _CustomBigQuerySource(BoundedSource):
       yield SourceBundle(0, source, None, None)
 
   def get_range_tracker(self, start_position, stop_position):
-    raise NotImplementedError('BigQuery source must be split before being read')
+    class CustomBigQuerySourceRangeTracker(RangeTracker):
+      """A RangeTracker that always returns positions as None."""
+
+      def start_position(self):
+        return None
+
+      def stop_position(self):
+        return None
+
+    return CustomBigQuerySourceRangeTracker()
 
   def read(self, range_tracker):
     raise NotImplementedError('BigQuery source must be split before being read')
diff --git a/sdks/python/apache_beam/io/gcp/bigquery_read_it_test.py b/sdks/python/apache_beam/io/gcp/bigquery_read_it_test.py
index b1a108ef2ea..31ca577fac4 100644
--- a/sdks/python/apache_beam/io/gcp/bigquery_read_it_test.py
+++ b/sdks/python/apache_beam/io/gcp/bigquery_read_it_test.py
@@ -146,8 +146,6 @@ class ReadTests(BigQueryReadIntegrationTests):
           query=self.query, use_standard_sql=True)))
       assert_that(result, equal_to(self.TABLE_DATA))
 
-  @skip(['DirectRunner', 'TestDirectRunner', 'SwitchingDirectRunner',
-         'BundleBasedDirectRunner'])
   @attr('IT')
   def test_iobase_source(self):
     with beam.Pipeline(argv=self.args) as p:
@@ -252,8 +250,6 @@ class ReadNewTypesTests(BigQueryReadIntegrationTests):
           query=self.query, use_standard_sql=True)))
       assert_that(result, equal_to(self.get_expected_data()))
 
-  @skip(['DirectRunner', 'TestDirectRunner', 'SwitchingDirectRunner',
-         'BundleBasedDirectRunner'])
   @attr('IT')
   def test_iobase_source(self):
     with beam.Pipeline(argv=self.args) as p:
diff --git a/sdks/python/apache_beam/io/gcp/bigquery_test.py b/sdks/python/apache_beam/io/gcp/bigquery_test.py
index 3ddeed020a3..21252603f32 100644
--- a/sdks/python/apache_beam/io/gcp/bigquery_test.py
+++ b/sdks/python/apache_beam/io/gcp/bigquery_test.py
@@ -279,7 +279,7 @@ class TestJsonToDictCoder(unittest.TestCase):
       self.fail('{} is not pickable'.format(coder.__class__.__name__))
 
   def test_values_are_converted(self):
-    input_row = '{"float": "10.5", "string": "abc"}'
+    input_row = b'{"float": "10.5", "string": "abc"}'
     expected_row = {'float': 10.5, 'string': 'abc'}
     schema = self._make_schema([
         ('float', 'FLOAT', []),
@@ -291,7 +291,7 @@ class TestJsonToDictCoder(unittest.TestCase):
     self.assertEqual(expected_row, actual)
 
   def test_null_fields_are_preserved(self):
-    input_row = '{"float": "10.5"}'
+    input_row = b'{"float": "10.5"}'
     expected_row = {'float': 10.5, 'string': None}
     schema = self._make_schema([
         ('float', 'FLOAT', []),
@@ -303,7 +303,7 @@ class TestJsonToDictCoder(unittest.TestCase):
     self.assertEqual(expected_row, actual)
 
   def test_record_field_is_properly_converted(self):
-    input_row = '{"record": {"float": "55.5"}, "integer": 10}'
+    input_row = b'{"record": {"float": "55.5"}, "integer": 10}'
     expected_row = {'record': {'float': 55.5}, 'integer': 10}
     schema = self._make_schema([
         ('record', 'RECORD', [
diff --git a/sdks/python/test-suites/portable/py2/build.gradle b/sdks/python/test-suites/portable/py2/build.gradle
index ad213f8c5b8..f9fb5f41d8c 100644
--- a/sdks/python/test-suites/portable/py2/build.gradle
+++ b/sdks/python/test-suites/portable/py2/build.gradle
@@ -40,6 +40,7 @@ task postCommitPy2() {
   dependsOn ':runners:flink:1.9:job-server:shadowJar'
   dependsOn portableWordCountFlinkRunnerBatch
   dependsOn portableWordCountFlinkRunnerStreaming
+  dependsOn 'postCommitPy2IT'
   dependsOn ':runners:spark:job-server:shadowJar'
   dependsOn portableWordCountSparkRunnerBatch
 }
@@ -147,10 +148,10 @@ task crossLanguagePortableWordCount {
   }
 }
 
-task postCommitIT {
+task postCommitPy2IT {
   dependsOn 'installGcpTest'
   dependsOn 'setupVirtualenv'
-  dependsOn ':runners:flink:1.8:job-server:shadowJar'
+  dependsOn ':runners:flink:1.9:job-server:shadowJar'
 
   doLast {
     def tests = [
