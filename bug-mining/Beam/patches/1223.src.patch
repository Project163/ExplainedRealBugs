diff --git a/runners/flink/1.5/src/main/java/org/apache/beam/runners/flink/translation/types/CoderTypeSerializer.java b/runners/flink/1.5/src/main/java/org/apache/beam/runners/flink/translation/types/CoderTypeSerializer.java
index fbe88c0491e..7e2120784a9 100644
--- a/runners/flink/1.5/src/main/java/org/apache/beam/runners/flink/translation/types/CoderTypeSerializer.java
+++ b/runners/flink/1.5/src/main/java/org/apache/beam/runners/flink/translation/types/CoderTypeSerializer.java
@@ -38,7 +38,7 @@ import org.apache.flink.core.memory.DataOutputView;
  */
 public class CoderTypeSerializer<T> extends TypeSerializer<T> {
 
-  private Coder<T> coder;
+  private final Coder<T> coder;
 
   public CoderTypeSerializer(Coder<T> coder) {
     Preconditions.checkNotNull(coder);
diff --git a/runners/flink/1.8/src/main/java/org/apache/beam/runners/flink/translation/types/CoderTypeSerializer.java b/runners/flink/1.8/src/main/java/org/apache/beam/runners/flink/translation/types/CoderTypeSerializer.java
index 2b0650fd426..73140bf2ec2 100644
--- a/runners/flink/1.8/src/main/java/org/apache/beam/runners/flink/translation/types/CoderTypeSerializer.java
+++ b/runners/flink/1.8/src/main/java/org/apache/beam/runners/flink/translation/types/CoderTypeSerializer.java
@@ -39,7 +39,7 @@ import org.apache.flink.core.memory.DataOutputView;
  */
 public class CoderTypeSerializer<T> extends TypeSerializer<T> {
 
-  private Coder<T> coder;
+  private final Coder<T> coder;
 
   public CoderTypeSerializer(Coder<T> coder) {
     Preconditions.checkNotNull(coder);
diff --git a/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/DoFnOperator.java b/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/DoFnOperator.java
index 0ff04861dde..d32f966a54c 100644
--- a/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/DoFnOperator.java
+++ b/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/DoFnOperator.java
@@ -67,12 +67,14 @@ import org.apache.beam.sdk.coders.StructuredCoder;
 import org.apache.beam.sdk.coders.VarIntCoder;
 import org.apache.beam.sdk.io.FileSystems;
 import org.apache.beam.sdk.options.PipelineOptions;
+import org.apache.beam.sdk.state.StateSpec;
 import org.apache.beam.sdk.state.TimeDomain;
 import org.apache.beam.sdk.transforms.DoFn;
 import org.apache.beam.sdk.transforms.DoFnSchemaInformation;
 import org.apache.beam.sdk.transforms.join.RawUnionValue;
 import org.apache.beam.sdk.transforms.reflect.DoFnInvoker;
 import org.apache.beam.sdk.transforms.reflect.DoFnInvokers;
+import org.apache.beam.sdk.transforms.reflect.DoFnSignature;
 import org.apache.beam.sdk.transforms.reflect.DoFnSignatures;
 import org.apache.beam.sdk.transforms.windowing.BoundedWindow;
 import org.apache.beam.sdk.util.WindowedValue;
@@ -357,6 +359,17 @@ public class DoFnOperator<InputT, OutputT> extends AbstractStreamOperator<Window
       keyedStateInternals =
           new FlinkStateInternals<>((KeyedStateBackend) getKeyedStateBackend(), keyCoder);
 
+      if (doFn != null) {
+        DoFnSignature signature = DoFnSignatures.getSignature(doFn.getClass());
+        FlinkStateInternals.EarlyBinder earlyBinder =
+            new FlinkStateInternals.EarlyBinder(getKeyedStateBackend());
+        for (DoFnSignature.StateDeclaration value : signature.stateDeclarations().values()) {
+          StateSpec<?> spec =
+              (StateSpec<?>) signature.stateDeclarations().get(value.id()).field().get(doFn);
+          spec.bind(value.id(), earlyBinder);
+        }
+      }
+
       if (timerService == null) {
         timerService =
             getInternalTimerService("beam-timer", new CoderTypeSerializer<>(timerCoder), this);
diff --git a/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/ExecutableStageDoFnOperator.java b/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/ExecutableStageDoFnOperator.java
index f60d4178b3f..735e9616d6f 100644
--- a/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/ExecutableStageDoFnOperator.java
+++ b/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/ExecutableStageDoFnOperator.java
@@ -38,6 +38,7 @@ import java.util.function.BiConsumer;
 import java.util.function.Consumer;
 import java.util.function.Supplier;
 import java.util.stream.Collectors;
+import javax.annotation.Nullable;
 import org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleProgressResponse;
 import org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleResponse;
 import org.apache.beam.model.fnexecution.v1.BeamFnApi.StateKey.TypeCase;
@@ -57,6 +58,7 @@ import org.apache.beam.runners.core.construction.graph.UserStateReference;
 import org.apache.beam.runners.flink.metrics.FlinkMetricContainer;
 import org.apache.beam.runners.flink.translation.functions.FlinkExecutableStageContext;
 import org.apache.beam.runners.flink.translation.functions.FlinkStreamingSideInputHandlerFactory;
+import org.apache.beam.runners.flink.translation.types.CoderTypeSerializer;
 import org.apache.beam.runners.fnexecution.control.BundleProgressHandler;
 import org.apache.beam.runners.fnexecution.control.OutputReceiverFactory;
 import org.apache.beam.runners.fnexecution.control.ProcessBundleDescriptors;
@@ -83,6 +85,9 @@ import org.apache.beam.sdk.values.PCollectionView;
 import org.apache.beam.sdk.values.TupleTag;
 import org.apache.beam.sdk.values.WindowingStrategy;
 import org.apache.beam.vendor.guava.v20_0.com.google.common.base.Preconditions;
+import org.apache.beam.vendor.sdk.v2.sdk.extensions.protobuf.ByteStringCoder;
+import org.apache.flink.api.common.state.ListStateDescriptor;
+import org.apache.flink.api.common.typeutils.base.StringSerializer;
 import org.apache.flink.api.java.functions.KeySelector;
 import org.apache.flink.runtime.state.KeyedStateBackend;
 import org.apache.flink.streaming.api.operators.InternalTimer;
@@ -173,6 +178,7 @@ public class ExecutableStageDoFnOperator<InputT, OutputT> extends DoFnOperator<I
   @Override
   public void open() throws Exception {
     executableStage = ExecutableStage.fromPayload(payload);
+    initializeUserState(executableStage, getKeyedStateBackend());
     // TODO: Wire this into the distributed cache and make it pluggable.
     // TODO: Do we really want this layer of indirection when accessing the stage bundle factory?
     // It's a little strange because this operator is responsible for the lifetime of the stage
@@ -834,6 +840,26 @@ public class ExecutableStageDoFnOperator<InputT, OutputT> extends DoFnOperator<I
     }
   }
 
+  /**
+   * Eagerly create the user state to work around https://jira.apache.org/jira/browse/FLINK-12653.
+   */
+  private static void initializeUserState(
+      ExecutableStage executableStage, @Nullable KeyedStateBackend keyedStateBackend) {
+    executableStage
+        .getUserStates()
+        .forEach(
+            ref -> {
+              try {
+                keyedStateBackend.getOrCreateKeyedState(
+                    StringSerializer.INSTANCE,
+                    new ListStateDescriptor<>(
+                        ref.localName(), new CoderTypeSerializer<>(ByteStringCoder.of())));
+              } catch (Exception e) {
+                throw new RuntimeException("Couldn't initialize user states.", e);
+              }
+            });
+  }
+
   private static class NoOpDoFn<InputT, OutputT> extends DoFn<InputT, OutputT> {
     @ProcessElement
     public void doNothing(ProcessContext context) {}
diff --git a/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/KeyedPushedBackElementsHandler.java b/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/KeyedPushedBackElementsHandler.java
index b5230b762ce..82229d96925 100644
--- a/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/KeyedPushedBackElementsHandler.java
+++ b/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/KeyedPushedBackElementsHandler.java
@@ -38,37 +38,38 @@ class KeyedPushedBackElementsHandler<K, T> implements PushedBackElementsHandler<
   static <K, T> KeyedPushedBackElementsHandler<K, T> create(
       KeySelector<T, K> keySelector,
       KeyedStateBackend<K> backend,
-      ListStateDescriptor<T> stateDescriptor) {
+      ListStateDescriptor<T> stateDescriptor)
+      throws Exception {
     return new KeyedPushedBackElementsHandler<>(keySelector, backend, stateDescriptor);
   }
 
   private final KeySelector<T, K> keySelector;
   private final KeyedStateBackend<K> backend;
-  private final ListStateDescriptor<T> stateDescriptor;
+  private final String stateName;
+  private final ListState<T> state;
 
   private KeyedPushedBackElementsHandler(
       KeySelector<T, K> keySelector,
       KeyedStateBackend<K> backend,
-      ListStateDescriptor<T> stateDescriptor) {
+      ListStateDescriptor<T> stateDescriptor)
+      throws Exception {
     this.keySelector = keySelector;
     this.backend = backend;
-    this.stateDescriptor = stateDescriptor;
+    this.stateName = stateDescriptor.getName();
+    // Eagerly retrieve the state to work around https://jira.apache.org/jira/browse/FLINK-12653
+    this.state =
+        backend.getPartitionedState(
+            VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, stateDescriptor);
   }
 
   @Override
   public Stream<T> getElements() {
-
     return backend
-        .getKeys(stateDescriptor.getName(), VoidNamespace.INSTANCE)
+        .getKeys(stateName, VoidNamespace.INSTANCE)
         .flatMap(
             key -> {
               try {
                 backend.setCurrentKey(key);
-
-                ListState<T> state =
-                    backend.getPartitionedState(
-                        VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, stateDescriptor);
-
                 return StreamSupport.stream(state.get().spliterator(), false);
               } catch (Exception e) {
                 throw new RuntimeException("Error reading keyed state.", e);
@@ -80,29 +81,16 @@ class KeyedPushedBackElementsHandler<K, T> implements PushedBackElementsHandler<
   public void clear() throws Exception {
     // TODO we have to collect all keys because otherwise we get ConcurrentModificationExceptions
     // from flink. We can change this once it's fixed in Flink
-
-    List<K> keys =
-        backend
-            .getKeys(stateDescriptor.getName(), VoidNamespace.INSTANCE)
-            .collect(Collectors.toList());
+    List<K> keys = backend.getKeys(stateName, VoidNamespace.INSTANCE).collect(Collectors.toList());
 
     for (K key : keys) {
       backend.setCurrentKey(key);
-
-      ListState<T> state =
-          backend.getPartitionedState(
-              VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, stateDescriptor);
-
       state.clear();
     }
   }
 
   @Override
   public void pushBack(T element) throws Exception {
-    ListState<T> state =
-        backend.getPartitionedState(
-            VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, stateDescriptor);
-
     backend.setCurrentKey(keySelector.getKey(element));
     state.add(element);
   }
diff --git a/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/stableinput/KeyedBufferingElementsHandler.java b/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/stableinput/KeyedBufferingElementsHandler.java
index 5aba6a9dd63..5dd010d17dd 100644
--- a/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/stableinput/KeyedBufferingElementsHandler.java
+++ b/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/stableinput/KeyedBufferingElementsHandler.java
@@ -31,27 +31,29 @@ import org.apache.flink.runtime.state.VoidNamespaceSerializer;
 public class KeyedBufferingElementsHandler implements BufferingElementsHandler {
 
   static KeyedBufferingElementsHandler create(
-      KeyedStateBackend backend, ListStateDescriptor<BufferedElement> stateDescriptor) {
+      KeyedStateBackend backend, ListStateDescriptor<BufferedElement> stateDescriptor)
+      throws Exception {
     return new KeyedBufferingElementsHandler(backend, stateDescriptor);
   }
 
-  private final KeyedStateBackend backend;
-  private final ListStateDescriptor<BufferedElement> stateDescriptor;
+  private final KeyedStateBackend<Object> backend;
+  private final String stateName;
+  private final ListState<BufferedElement> state;
 
   private KeyedBufferingElementsHandler(
-      KeyedStateBackend backend, ListStateDescriptor<BufferedElement> stateDescriptor) {
+      KeyedStateBackend<Object> backend, ListStateDescriptor<BufferedElement> stateDescriptor)
+      throws Exception {
     this.backend = backend;
-    this.stateDescriptor = stateDescriptor;
+    this.stateName = stateDescriptor.getName();
+    // Eagerly retrieve the state to work around https://jira.apache.org/jira/browse/FLINK-12653
+    this.state =
+        backend.getPartitionedState(
+            VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, stateDescriptor);
   }
 
   @Override
   public void buffer(BufferedElement element) {
     try {
-      ListState<BufferedElement> state =
-          (ListState<BufferedElement>)
-              backend.getPartitionedState(
-                  VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, stateDescriptor);
-
       // assumes state backend is already keyed
       state.add(element);
     } catch (Exception e) {
@@ -62,19 +64,11 @@ public class KeyedBufferingElementsHandler implements BufferingElementsHandler {
   @Override
   public Stream<BufferedElement> getElements() {
     return backend
-        .getKeys(stateDescriptor.getName(), VoidNamespace.INSTANCE)
+        .getKeys(stateName, VoidNamespace.INSTANCE)
         .flatMap(
             key -> {
               try {
                 backend.setCurrentKey(key);
-
-                ListState<BufferedElement> state =
-                    (ListState<BufferedElement>)
-                        backend.getPartitionedState(
-                            VoidNamespace.INSTANCE,
-                            VoidNamespaceSerializer.INSTANCE,
-                            stateDescriptor);
-
                 return StreamSupport.stream(state.get().spliterator(), false);
               } catch (Exception e) {
                 throw new RuntimeException(
@@ -85,21 +79,10 @@ public class KeyedBufferingElementsHandler implements BufferingElementsHandler {
 
   @Override
   public void clear() {
-    List keys =
-        (List)
-            backend
-                .getKeys(stateDescriptor.getName(), VoidNamespace.INSTANCE)
-                .collect(Collectors.toList());
-
+    List keys = backend.getKeys(stateName, VoidNamespace.INSTANCE).collect(Collectors.toList());
     try {
       for (Object key : keys) {
         backend.setCurrentKey(key);
-
-        ListState<BufferedElement> state =
-            (ListState<BufferedElement>)
-                backend.getPartitionedState(
-                    VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, stateDescriptor);
-
         state.clear();
       }
     } catch (Exception e) {
diff --git a/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/state/FlinkStateInternals.java b/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/state/FlinkStateInternals.java
index 4407d33a9cd..684ada3307d 100644
--- a/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/state/FlinkStateInternals.java
+++ b/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/state/FlinkStateInternals.java
@@ -60,6 +60,7 @@ import org.apache.flink.api.common.state.MapStateDescriptor;
 import org.apache.flink.api.common.state.ValueStateDescriptor;
 import org.apache.flink.api.common.typeutils.base.BooleanSerializer;
 import org.apache.flink.api.common.typeutils.base.StringSerializer;
+import org.apache.flink.api.common.typeutils.base.VoidSerializer;
 import org.apache.flink.runtime.state.KeyedStateBackend;
 import org.apache.flink.runtime.state.VoidNamespace;
 import org.apache.flink.runtime.state.VoidNamespaceSerializer;
@@ -1233,4 +1234,121 @@ public class FlinkStateInternals<K> implements StateInternals {
       }
     }
   }
+
+  /** Eagerly create user state to work around https://jira.apache.org/jira/browse/FLINK-12653. */
+  public static class EarlyBinder implements StateBinder {
+
+    private final KeyedStateBackend keyedStateBackend;
+
+    public EarlyBinder(KeyedStateBackend keyedStateBackend) {
+      this.keyedStateBackend = keyedStateBackend;
+    }
+
+    @Override
+    public <T> ValueState<T> bindValue(String id, StateSpec<ValueState<T>> spec, Coder<T> coder) {
+      try {
+        keyedStateBackend.getOrCreateKeyedState(
+            StringSerializer.INSTANCE,
+            new ValueStateDescriptor<>(id, new CoderTypeSerializer<>(coder)));
+      } catch (Exception e) {
+        throw new RuntimeException(e);
+      }
+
+      return null;
+    }
+
+    @Override
+    public <T> BagState<T> bindBag(String id, StateSpec<BagState<T>> spec, Coder<T> elemCoder) {
+      try {
+        keyedStateBackend.getOrCreateKeyedState(
+            StringSerializer.INSTANCE,
+            new ListStateDescriptor<>(id, new CoderTypeSerializer<>(elemCoder)));
+      } catch (Exception e) {
+        throw new RuntimeException(e);
+      }
+
+      return null;
+    }
+
+    @Override
+    public <T> SetState<T> bindSet(String id, StateSpec<SetState<T>> spec, Coder<T> elemCoder) {
+      try {
+        keyedStateBackend.getOrCreateKeyedState(
+            StringSerializer.INSTANCE,
+            new MapStateDescriptor<>(
+                id, new CoderTypeSerializer<>(elemCoder), VoidSerializer.INSTANCE));
+      } catch (Exception e) {
+        throw new RuntimeException(e);
+      }
+      return null;
+    }
+
+    @Override
+    public <KeyT, ValueT> org.apache.beam.sdk.state.MapState<KeyT, ValueT> bindMap(
+        String id,
+        StateSpec<org.apache.beam.sdk.state.MapState<KeyT, ValueT>> spec,
+        Coder<KeyT> mapKeyCoder,
+        Coder<ValueT> mapValueCoder) {
+      try {
+        keyedStateBackend.getOrCreateKeyedState(
+            StringSerializer.INSTANCE,
+            new MapStateDescriptor<>(
+                id,
+                new CoderTypeSerializer<>(mapKeyCoder),
+                new CoderTypeSerializer<>(mapValueCoder)));
+      } catch (Exception e) {
+        throw new RuntimeException(e);
+      }
+      return null;
+    }
+
+    @Override
+    public <InputT, AccumT, OutputT> CombiningState<InputT, AccumT, OutputT> bindCombining(
+        String id,
+        StateSpec<CombiningState<InputT, AccumT, OutputT>> spec,
+        Coder<AccumT> accumCoder,
+        Combine.CombineFn<InputT, AccumT, OutputT> combineFn) {
+      try {
+        keyedStateBackend.getOrCreateKeyedState(
+            StringSerializer.INSTANCE,
+            new ValueStateDescriptor<>(id, new CoderTypeSerializer<>(accumCoder)));
+      } catch (Exception e) {
+        throw new RuntimeException(e);
+      }
+      return null;
+    }
+
+    @Override
+    public <InputT, AccumT, OutputT>
+        CombiningState<InputT, AccumT, OutputT> bindCombiningWithContext(
+            String id,
+            StateSpec<CombiningState<InputT, AccumT, OutputT>> spec,
+            Coder<AccumT> accumCoder,
+            CombineWithContext.CombineFnWithContext<InputT, AccumT, OutputT> combineFn) {
+      try {
+        keyedStateBackend.getOrCreateKeyedState(
+            StringSerializer.INSTANCE,
+            new ValueStateDescriptor<>(id, new CoderTypeSerializer<>(accumCoder)));
+      } catch (Exception e) {
+        throw new RuntimeException(e);
+      }
+      return null;
+    }
+
+    @Override
+    public WatermarkHoldState bindWatermark(
+        String id, StateSpec<WatermarkHoldState> spec, TimestampCombiner timestampCombiner) {
+      try {
+        keyedStateBackend.getOrCreateKeyedState(
+            VoidNamespaceSerializer.INSTANCE,
+            new MapStateDescriptor<>(
+                "watermark-holds",
+                StringSerializer.INSTANCE,
+                new CoderTypeSerializer<>(InstantCoder.of())));
+      } catch (Exception e) {
+        throw new RuntimeException(e);
+      }
+      return null;
+    }
+  }
 }
diff --git a/runners/flink/src/test/java/org/apache/beam/runners/flink/FlinkSavepointTest.java b/runners/flink/src/test/java/org/apache/beam/runners/flink/FlinkSavepointTest.java
index f8c3a2eb092..4e01b12e0e4 100644
--- a/runners/flink/src/test/java/org/apache/beam/runners/flink/FlinkSavepointTest.java
+++ b/runners/flink/src/test/java/org/apache/beam/runners/flink/FlinkSavepointTest.java
@@ -17,16 +17,18 @@
  */
 package org.apache.beam.runners.flink;
 
-import static org.junit.Assert.assertNotNull;
+import static org.hamcrest.MatcherAssert.assertThat;
 
 import java.io.Serializable;
 import java.lang.reflect.Method;
 import java.net.URI;
 import java.util.Collections;
+import java.util.Objects;
 import java.util.concurrent.CompletableFuture;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.ExecutionException;
 import java.util.concurrent.Executors;
+import java.util.concurrent.TimeUnit;
 import org.apache.beam.model.pipeline.v1.RunnerApi;
 import org.apache.beam.runners.core.construction.Environments;
 import org.apache.beam.runners.core.construction.PipelineTranslation;
@@ -63,46 +65,55 @@ import org.apache.flink.runtime.jobgraph.JobStatus;
 import org.apache.flink.runtime.jobgraph.SavepointRestoreSettings;
 import org.apache.flink.runtime.minicluster.MiniCluster;
 import org.apache.flink.runtime.minicluster.MiniClusterConfiguration;
-import org.apache.flink.streaming.util.TestStreamEnvironment;
+import org.hamcrest.Matchers;
+import org.hamcrest.core.IsIterableContaining;
 import org.joda.time.Duration;
 import org.junit.After;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 import org.junit.ClassRule;
+import org.junit.Rule;
 import org.junit.Test;
 import org.junit.rules.TemporaryFolder;
+import org.junit.rules.Timeout;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-/** Tests that Flink's Savepoints work with the Flink Runner. */
+/**
+ * Tests that Flink's Savepoints work with the Flink Runner. This includes taking a savepoint of a
+ * running pipeline, shutting down the pipeline, and restarting the pipeline from the savepoint with
+ * a different parallelism.
+ */
 public class FlinkSavepointTest implements Serializable {
 
   private static final Logger LOG = LoggerFactory.getLogger(FlinkSavepointTest.class);
 
+  /** Flink cluster that runs over the lifespan of the tests. */
+  private static transient MiniCluster flinkCluster;
+
   /** Static for synchronization between the pipeline state and the test. */
-  private static CountDownLatch oneShotLatch;
+  private static volatile CountDownLatch oneShotLatch;
 
+  /** Temporary folder for savepoints. */
   @ClassRule public static transient TemporaryFolder tempFolder = new TemporaryFolder();
 
-  private static transient MiniCluster flinkCluster;
+  /** Each test has a timeout of 60 seconds (for safety). */
+  @Rule public Timeout timeout = new Timeout(60, TimeUnit.SECONDS);
 
   @BeforeClass
   public static void beforeClass() throws Exception {
-    final int parallelism = 4;
-
     Configuration config = new Configuration();
     // Avoid port collision in parallel tests
     config.setInteger(RestOptions.PORT, 0);
     config.setString(CheckpointingOptions.STATE_BACKEND, "filesystem");
+
+    String savepointPath = "file://" + tempFolder.getRoot().getAbsolutePath();
+    LOG.info("Savepoints will be written to {}", savepointPath);
     // It is necessary to configure the checkpoint directory for the state backend,
     // even though we only create savepoints in this test.
-    config.setString(
-        CheckpointingOptions.CHECKPOINTS_DIRECTORY,
-        "file://" + tempFolder.getRoot().getAbsolutePath());
+    config.setString(CheckpointingOptions.CHECKPOINTS_DIRECTORY, savepointPath);
     // Checkpoints will go into a subdirectory of this directory
-    config.setString(
-        CheckpointingOptions.SAVEPOINT_DIRECTORY,
-        "file://" + tempFolder.getRoot().getAbsolutePath());
+    config.setString(CheckpointingOptions.SAVEPOINT_DIRECTORY, savepointPath);
 
     MiniClusterConfiguration clusterConfig =
         new MiniClusterConfiguration.Builder()
@@ -113,14 +124,12 @@ public class FlinkSavepointTest implements Serializable {
 
     flinkCluster = new MiniCluster(clusterConfig);
     flinkCluster.start();
-
-    TestStreamEnvironment.setAsContext(flinkCluster, parallelism);
   }
 
   @AfterClass
   public static void afterClass() throws Exception {
-    TestStreamEnvironment.unsetAsContext();
     flinkCluster.close();
+    flinkCluster = null;
   }
 
   @After
@@ -130,14 +139,18 @@ public class FlinkSavepointTest implements Serializable {
         flinkCluster.cancelJob(jobStatusMessage.getJobId()).get();
       }
     }
+    while (!flinkCluster.listJobs().get().stream()
+        .allMatch(job -> job.getJobState().isTerminalState())) {
+      Thread.sleep(50);
+    }
   }
 
-  @Test(timeout = 60_000)
+  @Test
   public void testSavepointRestoreLegacy() throws Exception {
     runSavepointAndRestore(false);
   }
 
-  @Test(timeout = 60_000)
+  @Test
   public void testSavepointRestorePortable() throws Exception {
     runSavepointAndRestore(true);
   }
@@ -145,8 +158,8 @@ public class FlinkSavepointTest implements Serializable {
   private void runSavepointAndRestore(boolean isPortablePipeline) throws Exception {
     FlinkPipelineOptions options = PipelineOptionsFactory.as(FlinkPipelineOptions.class);
     options.setStreaming(true);
-    // savepoint assumes local file system
-    options.setParallelism(1);
+    // Initial parallelism
+    options.setParallelism(2);
     options.setRunner(FlinkRunner.class);
 
     oneShotLatch = new CountDownLatch(1);
@@ -163,6 +176,8 @@ public class FlinkSavepointTest implements Serializable {
     String savepointDir = takeSavepointAndCancelJob(jobID);
 
     oneShotLatch = new CountDownLatch(1);
+    // Increase parallelism
+    options.setParallelism(4);
     pipeline = Pipeline.create(options);
     createStreamingJob(pipeline, true, isPortablePipeline);
 
@@ -282,13 +297,16 @@ public class FlinkSavepointTest implements Serializable {
                       new InferableFunction<byte[], KV<String, Void>>() {
                         @Override
                         public KV<String, Void> apply(byte[] input) throws Exception {
+                          // This only writes data to one of the two initial partitions.
+                          // We want to test this due to
+                          // https://jira.apache.org/jira/browse/BEAM-7144
                           return KV.of("key", null);
                         }
                       }))
               .apply(
                   ParDo.of(
                       new DoFn<KV<String, Void>, KV<String, Long>>() {
-                        @StateId("valueState")
+                        @StateId("nextInteger")
                         private final StateSpec<ValueState<Long>> valueStateSpec =
                             StateSpecs.value();
 
@@ -306,15 +324,15 @@ public class FlinkSavepointTest implements Serializable {
                         @OnTimer("timer")
                         public void onTimer(
                             OnTimerContext context,
-                            @StateId("valueState") ValueState<Long> intValueState,
+                            @StateId("nextInteger") ValueState<Long> nextInteger,
                             @TimerId("timer") Timer timer) {
-                          Long current = intValueState.read();
+                          Long current = nextInteger.read();
                           if (current == null) {
                             current = -1L;
                           }
                           long next = current + 1;
+                          nextInteger.write(next);
                           context.output(KV.of("key", next));
-                          intValueState.write(next);
                           timer.offset(Duration.millis(100)).setRelative();
                         }
                       }));
@@ -347,12 +365,9 @@ public class FlinkSavepointTest implements Serializable {
                     ProcessContext context,
                     @StateId("valueState") ValueState<Integer> intValueState,
                     @StateId("bagState") BagState<Integer> intBagState) {
-                  Integer read = intValueState.read();
-                  assertNotNull(read);
-                  if (read == 42) {
-                    intValueState.write(0);
-                    oneShotLatch.countDown();
-                  }
+                  assertThat(intValueState.read(), Matchers.is(42));
+                  assertThat(intBagState.read(), IsIterableContaining.hasItems(40, 1, 1));
+                  oneShotLatch.countDown();
                 }
               }));
     } else {
@@ -371,8 +386,7 @@ public class FlinkSavepointTest implements Serializable {
                     ProcessContext context,
                     @StateId("valueState") ValueState<Integer> intValueState,
                     @StateId("bagState") BagState<Integer> intBagState) {
-                  Long value = context.element().getValue();
-                  assertNotNull(value);
+                  Long value = Objects.requireNonNull(context.element().getValue());
                   if (value == 0L) {
                     intValueState.write(42);
                     intBagState.add(40);
