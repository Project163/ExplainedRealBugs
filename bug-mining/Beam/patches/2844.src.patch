diff --git a/sdks/java/core/src/main/java/org/apache/beam/sdk/schemas/transforms/SchemaTransform.java b/sdks/java/core/src/main/java/org/apache/beam/sdk/schemas/transforms/SchemaTransform.java
new file mode 100644
index 00000000000..e91bd1164ca
--- /dev/null
+++ b/sdks/java/core/src/main/java/org/apache/beam/sdk/schemas/transforms/SchemaTransform.java
@@ -0,0 +1,41 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.beam.sdk.schemas.transforms;
+
+import org.apache.beam.sdk.annotations.Experimental;
+import org.apache.beam.sdk.annotations.Experimental.Kind;
+import org.apache.beam.sdk.annotations.Internal;
+import org.apache.beam.sdk.transforms.PTransform;
+import org.apache.beam.sdk.values.PCollectionRowTuple;
+
+/**
+ * An abstraction to create schema capable and aware transforms. The interface is intended to be
+ * used in conjunction with the interface {@link SchemaTransformProvider}.
+ *
+ * <p>The interfaces can be implemented to make transforms available in other SDKs in addition to
+ * Beam SQL.
+ *
+ * <p><b>Internal only:</b> This interface is actively being worked on and it will likely change as
+ * we provide implementations for more standard Beam transforms. We provide no backwards
+ * compatibility guarantees and it should not be implemented outside of the Beam repository.
+ */
+@Internal
+@Experimental(Kind.SCHEMAS)
+public interface SchemaTransform {
+  PTransform<PCollectionRowTuple, PCollectionRowTuple> buildTransform();
+}
diff --git a/sdks/java/core/src/main/java/org/apache/beam/sdk/schemas/transforms/SchemaTransformProvider.java b/sdks/java/core/src/main/java/org/apache/beam/sdk/schemas/transforms/SchemaTransformProvider.java
new file mode 100644
index 00000000000..3e21be255aa
--- /dev/null
+++ b/sdks/java/core/src/main/java/org/apache/beam/sdk/schemas/transforms/SchemaTransformProvider.java
@@ -0,0 +1,67 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.beam.sdk.schemas.transforms;
+
+import java.util.List;
+import java.util.Optional;
+import org.apache.beam.sdk.annotations.Experimental;
+import org.apache.beam.sdk.annotations.Experimental.Kind;
+import org.apache.beam.sdk.annotations.Internal;
+import org.apache.beam.sdk.options.PipelineOptions;
+import org.apache.beam.sdk.schemas.Schema;
+import org.apache.beam.sdk.values.Row;
+
+/**
+ * Provider to create {@link SchemaTransform} instances for use in Beam SQL and other SDKs.
+ *
+ * <p><b>Internal only:</b> This interface is actively being worked on and it will likely change as
+ * we provide implementations for more standard Beam transforms. We provide no backwards
+ * compatibility guarantees and it should not be implemented outside of the Beam repository.
+ */
+@Internal
+@Experimental(Kind.SCHEMAS)
+public interface SchemaTransformProvider {
+  /** Returns an id that uniquely represents this transform. */
+  String identifier();
+
+  /**
+   * Returns the expected schema of the configuration object. Note this is distinct from the schema
+   * of the transform itself.
+   */
+  Schema configurationSchema();
+
+  /**
+   * Produce a SchemaTransform some transform-specific configuration object. Can throw a {@link
+   * InvalidConfigurationException} or a {@link InvalidSchemaException}.
+   */
+  SchemaTransform from(Row configuration);
+
+  /** Returns the input collection names of this transform. */
+  List<String> inputCollectionNames();
+
+  /** Returns the output collection names of this transform. */
+  List<String> outputCollectionNames();
+
+  /**
+   * List the dependencies needed for this transform. Jars from classpath are used by default when
+   * Optional.empty() is returned.
+   */
+  default Optional<List<String>> dependencies(Row configuration, PipelineOptions options) {
+    return Optional.empty();
+  }
+}
diff --git a/sdks/java/core/src/main/java/org/apache/beam/sdk/schemas/transforms/TypedSchemaTransformProvider.java b/sdks/java/core/src/main/java/org/apache/beam/sdk/schemas/transforms/TypedSchemaTransformProvider.java
new file mode 100644
index 00000000000..c237d03e052
--- /dev/null
+++ b/sdks/java/core/src/main/java/org/apache/beam/sdk/schemas/transforms/TypedSchemaTransformProvider.java
@@ -0,0 +1,93 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.beam.sdk.schemas.transforms;
+
+import java.util.List;
+import java.util.Optional;
+import org.apache.beam.sdk.annotations.Experimental;
+import org.apache.beam.sdk.annotations.Experimental.Kind;
+import org.apache.beam.sdk.annotations.Internal;
+import org.apache.beam.sdk.options.PipelineOptions;
+import org.apache.beam.sdk.schemas.NoSuchSchemaException;
+import org.apache.beam.sdk.schemas.Schema;
+import org.apache.beam.sdk.schemas.SchemaRegistry;
+import org.apache.beam.sdk.values.Row;
+
+/**
+ * Like {@link SchemaTransformProvider} except uses a configuration object instead of Schema and
+ * Row.
+ *
+ * <p>ConfigT should be available in the SchemaRegistry.
+ *
+ * <p><b>Internal only:</b> This interface is actively being worked on and it will likely change as
+ * we provide implementations for more standard Beam transforms. We provide no backwards
+ * compatibility guarantees and it should not be implemented outside of the Beam repository.
+ */
+@Internal
+@Experimental(Kind.SCHEMAS)
+public abstract class TypedSchemaTransformProvider<ConfigT> implements SchemaTransformProvider {
+
+  abstract Class<ConfigT> configurationClass();
+
+  /**
+   * Produce a SchemaTransform from ConfigT. Can throw a {@link InvalidConfigurationException} or a
+   * {@link InvalidSchemaException}.
+   */
+  abstract SchemaTransform from(ConfigT configuration);
+
+  /**
+   * List the dependencies needed for this transform. Jars from classpath are used by default when
+   * Optional.empty() is returned.
+   */
+  Optional<List<String>> dependencies(ConfigT configuration, PipelineOptions options) {
+    return Optional.empty();
+  }
+
+  @Override
+  public final Schema configurationSchema() {
+    try {
+      return SchemaRegistry.createDefault().getSchema(configurationClass());
+    } catch (NoSuchSchemaException e) {
+      throw new RuntimeException(
+          "Unable to find schema for "
+              + identifier()
+              + " SchemaTransformProvider's configuration.");
+    }
+  }
+
+  @Override
+  public final SchemaTransform from(Row configuration) {
+    return from(configFromRow(configuration));
+  }
+
+  @Override
+  public final Optional<List<String>> dependencies(Row configuration, PipelineOptions options) {
+    return dependencies(configFromRow(configuration), options);
+  }
+
+  private ConfigT configFromRow(Row configuration) {
+    try {
+      return SchemaRegistry.createDefault()
+          .getFromRowFunction(configurationClass())
+          .apply(configuration);
+    } catch (NoSuchSchemaException e) {
+      throw new RuntimeException(
+          "Unable to find schema for " + identifier() + "SchemaTransformProvider's config");
+    }
+  }
+}
diff --git a/sdks/java/core/src/main/java/org/apache/beam/sdk/values/PCollectionRowTuple.java b/sdks/java/core/src/main/java/org/apache/beam/sdk/values/PCollectionRowTuple.java
new file mode 100644
index 00000000000..80935710dff
--- /dev/null
+++ b/sdks/java/core/src/main/java/org/apache/beam/sdk/values/PCollectionRowTuple.java
@@ -0,0 +1,271 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.beam.sdk.values;
+
+import java.util.Collections;
+import java.util.LinkedHashMap;
+import java.util.Map;
+import java.util.Objects;
+import org.apache.beam.sdk.Pipeline;
+import org.apache.beam.sdk.transforms.PTransform;
+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;
+import org.checkerframework.checker.nullness.qual.Nullable;
+
+/**
+ * A {@link PCollectionRowTuple} is an immutable tuple of {@link PCollection PCollection<Row>s},
+ * "keyed" by a string tag. A {@link PCollectionRowTuple} can be used as the input or output of a
+ * {@link PTransform} taking or producing multiple {@code PCollection<Row>} inputs or outputs.
+ *
+ * <p>A {@link PCollectionRowTuple} can be created and accessed like follows:
+ *
+ * <pre>{@code
+ * PCollection<Row> pc1 = ...;
+ * PCollection<Row> pc2 = ...;
+ *
+ * // Create tags for each of the PCollections to put in the PCollectionRowTuple:
+ * String tag1 = "pc1";
+ * String tag2 = "pc2";
+ * String tag3 = "pc3";
+ *
+ * // Create a PCollectionRowTuple with three PCollections:
+ * PCollectionRowTuple pcs = PCollectionRowTuple.of(tag1, pc1).and(tag2, pc2).and(tag3, pc3);
+ *
+ * // Create an empty PCollectionRowTuple:
+ * Pipeline p = ...;
+ * PCollectionRowTuple pcs2 = PCollectionRowTuple.empty(p);
+ *
+ * // Get PCollections out of a PCollectionRowTuple, using the same tags that were used to put them in:
+ * PCollection<Row> pcX = pcs.get(tag2);
+ * PCollection<Row> pcY = pcs.get(tag1);
+ *
+ * // Get a map of all PCollections in a PCollectionRowTuple:
+ * Map<String, PCollection<Row>> allPcs = pcs.getAll();
+ * }</pre>
+ */
+@SuppressWarnings({
+  "rawtypes" // TODO(https://issues.apache.org/jira/browse/BEAM-10556)
+})
+public class PCollectionRowTuple implements PInput, POutput {
+  /**
+   * Returns an empty {@link PCollectionRowTuple} that is part of the given {@link Pipeline}.
+   *
+   * <p>A {@link PCollectionRowTuple} containing additional elements can be created by calling
+   * {@link #and} on the result.
+   */
+  public static PCollectionRowTuple empty(Pipeline pipeline) {
+    return new PCollectionRowTuple(pipeline);
+  }
+
+  /**
+   * Returns a singleton {@link PCollectionRowTuple} containing the given {@link PCollection} keyed
+   * by the given tag.
+   *
+   * <p>A {@link PCollectionRowTuple} containing additional elements can be created by calling
+   * {@link #and} on the result.
+   */
+  public static PCollectionRowTuple of(String tag, PCollection<Row> pc) {
+    return empty(pc.getPipeline()).and(tag, pc);
+  }
+
+  /**
+   * A version of {@link #of(String, PCollection)} that takes in two PCollections of the same type.
+   */
+  public static PCollectionRowTuple of(
+      String tag1, PCollection<Row> pc1, String tag2, PCollection<Row> pc2) {
+    return of(tag1, pc1).and(tag2, pc2);
+  }
+
+  /**
+   * A version of {@link #of(String, PCollection)} that takes in three PCollections of the same
+   * type.
+   */
+  public static PCollectionRowTuple of(
+      String tag1,
+      PCollection<Row> pc1,
+      String tag2,
+      PCollection<Row> pc2,
+      String tag3,
+      PCollection<Row> pc3) {
+    return of(tag1, pc1, tag2, pc2).and(tag3, pc3);
+  }
+
+  /**
+   * A version of {@link #of(String, PCollection)} that takes in four PCollections of the same type.
+   */
+  public static PCollectionRowTuple of(
+      String tag1,
+      PCollection<Row> pc1,
+      String tag2,
+      PCollection<Row> pc2,
+      String tag3,
+      PCollection<Row> pc3,
+      String tag4,
+      PCollection<Row> pc4) {
+    return of(tag1, pc1, tag2, pc2, tag3, pc3).and(tag4, pc4);
+  }
+
+  /**
+   * A version of {@link #of(String, PCollection)} that takes in five PCollections of the same type.
+   */
+  public static PCollectionRowTuple of(
+      String tag1,
+      PCollection<Row> pc1,
+      String tag2,
+      PCollection<Row> pc2,
+      String tag3,
+      PCollection<Row> pc3,
+      String tag4,
+      PCollection<Row> pc4,
+      String tag5,
+      PCollection<Row> pc5) {
+    return of(tag1, pc1, tag2, pc2, tag3, pc3, tag4, pc4).and(tag5, pc5);
+  }
+
+  // To create a PCollectionRowTuple with more than five inputs, use the and() builder method.
+
+  /**
+   * Returns a new {@link PCollectionRowTuple} that has each {@link PCollection} and tag of this
+   * {@link PCollectionRowTuple} plus the given {@link PCollection} associated with the given tag.
+   *
+   * <p>The given tag should not already be mapped to a {@link PCollection} in this {@link
+   * PCollectionRowTuple}.
+   *
+   * <p>Each {@link PCollection} in the resulting {@link PCollectionRowTuple} must be part of the
+   * same {@link Pipeline}.
+   */
+  public PCollectionRowTuple and(String tag, PCollection<Row> pc) {
+    if (pc.getPipeline() != pipeline) {
+      throw new IllegalArgumentException("PCollections come from different Pipelines");
+    }
+
+    return new PCollectionRowTuple(
+        pipeline,
+        new ImmutableMap.Builder<String, PCollection<Row>>()
+            .putAll(pcollectionMap)
+            .put(tag, pc)
+            .build());
+  }
+
+  /**
+   * Returns whether this {@link PCollectionRowTuple} contains a {@link PCollection} with the given
+   * tag.
+   */
+  public boolean has(String tag) {
+    return pcollectionMap.containsKey(tag);
+  }
+
+  /**
+   * Returns the {@link PCollection} associated with the given {@link String} in this {@link
+   * PCollectionRowTuple}. Throws {@link IllegalArgumentException} if there is no such {@link
+   * PCollection}, i.e., {@code !has(tag)}.
+   */
+  public PCollection<Row> get(String tag) {
+    @SuppressWarnings("unchecked")
+    PCollection<Row> pcollection = pcollectionMap.get(tag);
+    if (pcollection == null) {
+      throw new IllegalArgumentException("Tag not found in this PCollectionRowTuple tuple");
+    }
+    return pcollection;
+  }
+
+  /**
+   * Returns an immutable Map from tag to corresponding {@link PCollection}, for all the members of
+   * this {@link PCollectionRowTuple}.
+   */
+  public Map<String, PCollection<Row>> getAll() {
+    return pcollectionMap;
+  }
+
+  /**
+   * Like {@link #apply(String, PTransform)} but defaulting to the name of the {@link PTransform}.
+   *
+   * @return the output of the applied {@link PTransform}
+   */
+  public <OutputT extends POutput> OutputT apply(
+      PTransform<? super PCollectionRowTuple, OutputT> t) {
+    return Pipeline.applyTransform(this, t);
+  }
+
+  /**
+   * Applies the given {@link PTransform} to this input {@link PCollectionRowTuple}, using {@code
+   * name} to identify this specific application of the transform. This name is used in various
+   * places, including the monitoring UI, logging, and to stably identify this application node in
+   * the job graph.
+   *
+   * @return the output of the applied {@link PTransform}
+   */
+  public <OutputT extends POutput> OutputT apply(
+      String name, PTransform<? super PCollectionRowTuple, OutputT> t) {
+    return Pipeline.applyTransform(name, this, t);
+  }
+
+  /////////////////////////////////////////////////////////////////////////////
+  // Internal details below here.
+
+  final Pipeline pipeline;
+  final Map<String, PCollection<Row>> pcollectionMap;
+
+  PCollectionRowTuple(Pipeline pipeline) {
+    this(pipeline, new LinkedHashMap<>());
+  }
+
+  PCollectionRowTuple(Pipeline pipeline, Map<String, PCollection<Row>> pcollectionMap) {
+    this.pipeline = pipeline;
+    this.pcollectionMap = Collections.unmodifiableMap(pcollectionMap);
+  }
+
+  @Override
+  public Pipeline getPipeline() {
+    return pipeline;
+  }
+
+  @Override
+  public Map<TupleTag<?>, PValue> expand() {
+    ImmutableMap.Builder<TupleTag<?>, PValue> builder = ImmutableMap.builder();
+    pcollectionMap.forEach((tag, value) -> builder.put(new TupleTag<Row>(tag), value));
+    return builder.build();
+  }
+
+  @Override
+  public void finishSpecifyingOutput(
+      String transformName, PInput input, PTransform<?, ?> transform) {
+    // All component PCollections will already have been finished. Update their names if
+    // appropriate.
+    for (Map.Entry<String, PCollection<Row>> entry : pcollectionMap.entrySet()) {
+      String tag = entry.getKey();
+      PCollection<Row> pc = entry.getValue();
+      if (pc.getName().equals(PValueBase.defaultName(transformName))) {
+        pc.setName(String.format("%s.%s", transformName, tag));
+      }
+    }
+  }
+
+  @Override
+  public boolean equals(@Nullable Object other) {
+    if (!(other instanceof PCollectionRowTuple)) {
+      return false;
+    }
+    PCollectionRowTuple that = (PCollectionRowTuple) other;
+    return this.pipeline.equals(that.pipeline) && this.pcollectionMap.equals(that.pcollectionMap);
+  }
+
+  @Override
+  public int hashCode() {
+    return Objects.hash(this.pipeline, this.pcollectionMap);
+  }
+}
diff --git a/sdks/java/core/src/test/java/org/apache/beam/sdk/schemas/transforms/TypedSchemaTransformProviderTest.java b/sdks/java/core/src/test/java/org/apache/beam/sdk/schemas/transforms/TypedSchemaTransformProviderTest.java
new file mode 100644
index 00000000000..2df44069dca
--- /dev/null
+++ b/sdks/java/core/src/test/java/org/apache/beam/sdk/schemas/transforms/TypedSchemaTransformProviderTest.java
@@ -0,0 +1,126 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.beam.sdk.schemas.transforms;
+
+import static org.junit.Assert.assertEquals;
+
+import com.google.auto.value.AutoValue;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Optional;
+import org.apache.beam.sdk.options.PipelineOptions;
+import org.apache.beam.sdk.schemas.AutoValueSchema;
+import org.apache.beam.sdk.schemas.annotations.DefaultSchema;
+import org.apache.beam.sdk.testing.UsesSchema;
+import org.apache.beam.sdk.transforms.PTransform;
+import org.apache.beam.sdk.values.PCollectionRowTuple;
+import org.apache.beam.sdk.values.Row;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.runner.RunWith;
+import org.junit.runners.JUnit4;
+
+/** Test for {@link Select}. */
+@RunWith(JUnit4.class)
+@Category(UsesSchema.class)
+public class TypedSchemaTransformProviderTest {
+
+  /** flat schema to select from. */
+  @DefaultSchema(AutoValueSchema.class)
+  @AutoValue
+  abstract static class Configuration {
+    abstract String getField1();
+
+    abstract Integer getField2();
+
+    static Configuration create(String field1, int field2) {
+      return new AutoValue_TypedSchemaTransformProviderTest_Configuration(field1, field2);
+    }
+  };
+
+  private static class FakeTypedSchemaIOProvider
+      extends TypedSchemaTransformProvider<Configuration> {
+    private FakeTypedSchemaIOProvider() {}
+
+    @Override
+    public String identifier() {
+      return "fake:v1";
+    }
+
+    @Override
+    Class<Configuration> configurationClass() {
+      return Configuration.class;
+    }
+
+    @Override
+    public SchemaTransform from(Configuration config) {
+      return new FakeSchemaTransform(config);
+    }
+
+    @Override
+    public List<String> inputCollectionNames() {
+      return null;
+    }
+
+    @Override
+    public List<String> outputCollectionNames() {
+      return null;
+    }
+
+    @Override
+    public Optional<List<String>> dependencies(
+        Configuration configuration, PipelineOptions options) {
+      return Optional.of(
+          Arrays.asList(configuration.getField1(), configuration.getField2().toString()));
+    }
+  }
+
+  public static class FakeSchemaTransform implements SchemaTransform {
+
+    public Configuration config;
+
+    public FakeSchemaTransform(Configuration config) {
+      this.config = config;
+    }
+
+    @Override
+    public PTransform<PCollectionRowTuple, PCollectionRowTuple> buildTransform() {
+      return null;
+    }
+  }
+
+  @Test
+  public void testFrom() {
+    SchemaTransformProvider provider = new FakeTypedSchemaIOProvider();
+    Row inputConfig =
+        Row.withSchema(provider.configurationSchema()).addValues("field1", 13).build();
+
+    Configuration outputConfig = ((FakeSchemaTransform) provider.from(inputConfig)).config;
+    assertEquals("field1", outputConfig.getField1());
+    assertEquals(13, outputConfig.getField2().intValue());
+  }
+
+  @Test
+  public void testDependencies() {
+    SchemaTransformProvider provider = new FakeTypedSchemaIOProvider();
+    Row inputConfig =
+        Row.withSchema(provider.configurationSchema()).addValues("field1", 13).build();
+
+    assertEquals(Arrays.asList("field1", "13"), provider.dependencies(inputConfig, null).get());
+  }
+}
diff --git a/sdks/java/core/src/test/java/org/apache/beam/sdk/values/PCollectionRowTupleTest.java b/sdks/java/core/src/test/java/org/apache/beam/sdk/values/PCollectionRowTupleTest.java
new file mode 100644
index 00000000000..1563ee7d870
--- /dev/null
+++ b/sdks/java/core/src/test/java/org/apache/beam/sdk/values/PCollectionRowTupleTest.java
@@ -0,0 +1,177 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.beam.sdk.values;
+
+import static org.hamcrest.MatcherAssert.assertThat;
+import static org.hamcrest.Matchers.equalTo;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+
+import com.google.common.testing.EqualsTester;
+import java.io.Serializable;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.stream.Collectors;
+import org.apache.beam.sdk.Pipeline;
+import org.apache.beam.sdk.coders.RowCoder;
+import org.apache.beam.sdk.schemas.Schema;
+import org.apache.beam.sdk.schemas.Schema.Field;
+import org.apache.beam.sdk.schemas.Schema.FieldType;
+import org.apache.beam.sdk.testing.PAssert;
+import org.apache.beam.sdk.testing.TestPipeline;
+import org.apache.beam.sdk.testing.ValidatesRunner;
+import org.apache.beam.sdk.transforms.Create;
+import org.apache.beam.sdk.transforms.MapElements;
+import org.apache.beam.sdk.transforms.SimpleFunction;
+import org.apache.beam.sdk.values.PCollection.IsBounded;
+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.runner.RunWith;
+import org.junit.runners.JUnit4;
+
+/** Unit tests for {@link PCollectionRowTuple}. */
+@RunWith(JUnit4.class)
+public final class PCollectionRowTupleTest implements Serializable {
+
+  public static final Schema INT_SCHEMA = Schema.of(Field.of("int", FieldType.INT32));
+  public static final Schema STRING_SCHEMA = Schema.of(Field.of("str", FieldType.STRING));
+  public static final Schema BOOL_SCHEMA = Schema.of(Field.of("str", FieldType.BOOLEAN));
+
+  @Rule
+  public final transient TestPipeline pipeline =
+      TestPipeline.create().enableAbandonedNodeEnforcement(false);
+
+  List<Row> toRows(List<Object> ints, Schema schema) {
+    return ints.stream()
+        .map((value) -> Row.withSchema(schema).addValue(value).build())
+        .collect(Collectors.toList());
+  }
+
+  @Test
+  public void testOfThenHas() {
+    PCollection<Row> pCollection =
+        PCollection.createPrimitiveOutputInternal(
+            pipeline,
+            WindowingStrategy.globalDefault(),
+            IsBounded.BOUNDED,
+            RowCoder.of(INT_SCHEMA));
+    String tag = "collection1";
+    assertTrue(PCollectionRowTuple.of(tag, pCollection).has(tag));
+  }
+
+  @Test
+  public void testEmpty() {
+    String tag = "collection1";
+    assertFalse(PCollectionRowTuple.empty(pipeline).has(tag));
+  }
+
+  @Test
+  @Category(ValidatesRunner.class)
+  public void testComposePCollectionRowTuple() {
+    pipeline.enableAbandonedNodeEnforcement(true);
+
+    List<Row> inputs = toRows(Arrays.asList(3, -42, 77), INT_SCHEMA);
+
+    PCollection<Row> mainInput = pipeline.apply("main", Create.of(inputs));
+    PCollection<Row> secondInput = pipeline.apply("second", Create.of(inputs));
+
+    PCollectionRowTuple tuple = PCollectionRowTuple.empty(pipeline);
+    tuple = tuple.and("main", mainInput);
+    tuple = tuple.and("second", secondInput);
+
+    PAssert.that(tuple.get("main")).containsInAnyOrder(inputs);
+    PAssert.that(tuple.get("second")).containsInAnyOrder(inputs);
+
+    pipeline.run();
+  }
+
+  @Test
+  public void testEquals() {
+    TestPipeline p = TestPipeline.create();
+    String intTag = "int";
+    String strTag = "strs";
+
+    PCollection<Row> ints =
+        p.apply("ints", Create.of(toRows(Arrays.asList(3, -42, 77), INT_SCHEMA)));
+    PCollection<Row> strs =
+        p.apply("strs", Create.of(toRows(Arrays.asList("ab", "cd", "ef"), STRING_SCHEMA)));
+
+    EqualsTester tester = new EqualsTester();
+    // Empty tuples in the same pipeline are equal
+    tester.addEqualityGroup(PCollectionRowTuple.empty(p), PCollectionRowTuple.empty(p));
+
+    tester.addEqualityGroup(
+        PCollectionRowTuple.of(intTag, ints).and(strTag, strs),
+        PCollectionRowTuple.of(intTag, ints).and(strTag, strs));
+
+    tester.addEqualityGroup(PCollectionRowTuple.of(intTag, ints));
+    tester.addEqualityGroup(PCollectionRowTuple.of(strTag, strs));
+
+    TestPipeline otherPipeline = TestPipeline.create();
+    // Empty tuples in different pipelines are not equal
+    tester.addEqualityGroup(PCollectionRowTuple.empty(otherPipeline));
+    tester.testEquals();
+  }
+
+  @Test
+  public void testExpandHasMatchingTags() {
+    String intTag = "ints";
+    String strTag = "strs";
+    String boolTag = "bools";
+
+    Pipeline p = TestPipeline.create();
+    PCollection<Row> ints =
+        p.apply("ints", Create.of(toRows(Arrays.asList(3, -42, 77), INT_SCHEMA)));
+    PCollection<Row> strs =
+        p.apply("strs", Create.of(toRows(Arrays.asList("ab", "cd", "ef"), STRING_SCHEMA)));
+    PCollection<Row> bools =
+        ints.apply(
+            MapElements.via(
+                new SimpleFunction<Row, Row>() {
+                  @Override
+                  public Row apply(Row input) {
+                    Boolean result = input.getInt32(0) % 2 == 0;
+                    return Row.withSchema(BOOL_SCHEMA).addValue(result).build();
+                  }
+                }));
+
+    Map<String, PCollection<Row>> pcsByTag =
+        ImmutableMap.<String, PCollection<Row>>builder()
+            .put(strTag, strs)
+            .put(intTag, ints)
+            .put(boolTag, bools)
+            .build();
+    PCollectionRowTuple tuple =
+        PCollectionRowTuple.of(intTag, ints).and(boolTag, bools).and(strTag, strs);
+    assertThat(tuple.getAll(), equalTo(pcsByTag));
+    PCollectionRowTuple reconstructed = PCollectionRowTuple.empty(p);
+    for (Entry<TupleTag<?>, PValue> taggedValue : tuple.expand().entrySet()) {
+      TupleTag<?> tag = taggedValue.getKey();
+      PValue value = taggedValue.getValue();
+      assertThat("The tag should map back to the value", tuple.get(tag.getId()), equalTo(value));
+      assertThat(value, equalTo(pcsByTag.get(tag.getId())));
+      reconstructed = reconstructed.and(tag.getId(), (PCollection) value);
+    }
+
+    assertThat(reconstructed, equalTo(tuple));
+  }
+}
