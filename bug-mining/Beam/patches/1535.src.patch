diff --git a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorker.java b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorker.java
index 4def3503276..d6d017ab7db 100644
--- a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorker.java
+++ b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorker.java
@@ -37,6 +37,7 @@ import java.io.PrintWriter;
 import java.util.ArrayDeque;
 import java.util.ArrayList;
 import java.util.Collections;
+import java.util.Deque;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.Iterator;
@@ -183,6 +184,7 @@ public class StreamingDataflowWorker {
   static final long TARGET_COMMIT_BUNDLE_BYTES = 32 << 20;
   static final int MAX_COMMIT_QUEUE_BYTES = 500 << 20; // 500MB
   static final int NUM_COMMIT_STREAMS = 1;
+  static final int GET_WORK_STREAM_TIMEOUT_MINUTES = 3;
   static final Duration COMMIT_STREAM_TIMEOUT = Duration.standardMinutes(1);
 
   private static final int DEFAULT_STATUS_PORT = 8081;
@@ -424,7 +426,7 @@ public class StreamingDataflowWorker {
   private final Counter<Long, Long> javaHarnessMaxMemory;
   private final Counter<Integer, Integer> windmillMaxObservedWorkItemCommitBytes;
   private final Counter<Integer, Integer> memoryThrashing;
-  private Timer refreshActiveWorkTimer;
+  private Timer refreshWorkTimer;
   private Timer statusPageTimer;
 
   private final boolean publishCounters;
@@ -785,9 +787,9 @@ public class StreamingDataflowWorker {
         0,
         options.getWindmillHarnessUpdateReportingPeriod().getMillis());
 
+    refreshWorkTimer = new Timer("RefreshWork");
     if (options.getActiveWorkRefreshPeriodMillis() > 0) {
-      refreshActiveWorkTimer = new Timer("RefreshActiveWork");
-      refreshActiveWorkTimer.schedule(
+      refreshWorkTimer.schedule(
           new TimerTask() {
             @Override
             public void run() {
@@ -797,6 +799,18 @@ public class StreamingDataflowWorker {
           options.getActiveWorkRefreshPeriodMillis(),
           options.getActiveWorkRefreshPeriodMillis());
     }
+    if (windmillServiceEnabled && options.getStuckCommitDurationMillis() > 0) {
+      int periodMillis = Math.max(options.getStuckCommitDurationMillis() / 10, 100);
+      refreshWorkTimer.schedule(
+          new TimerTask() {
+            @Override
+            public void run() {
+              invalidateStuckCommits();
+            }
+          },
+          periodMillis,
+          periodMillis);
+    }
 
     if (options.getPeriodicStatusPageOutputDirectory() != null) {
       statusPageTimer = new Timer("DumpStatusPages");
@@ -863,8 +877,8 @@ public class StreamingDataflowWorker {
         globalConfigRefreshTimer.cancel();
       }
       globalWorkerUpdatesTimer.cancel();
-      if (refreshActiveWorkTimer != null) {
-        refreshActiveWorkTimer.cancel();
+      if (refreshWorkTimer != null) {
+        refreshWorkTimer.cancel();
       }
       if (statusPageTimer != null) {
         statusPageTimer.cancel();
@@ -925,7 +939,11 @@ public class StreamingDataflowWorker {
       computationMap.put(
           computationId,
           new ComputationState(
-              computationId, mapTask, workUnitExecutor, transformUserNameToStateFamily));
+              computationId,
+              mapTask,
+              workUnitExecutor,
+              transformUserNameToStateFamily,
+              stateCache.forComputation(computationId)));
     }
   }
 
@@ -1014,7 +1032,9 @@ public class StreamingDataflowWorker {
         // Reconnect every now and again to enable better load balancing.
         // If at any point the server closes the stream, we will reconnect immediately; otherwise
         // we half-close the stream after some time and create a new one.
-        stream.closeAfterDefaultTimeout();
+        if (!stream.awaitTermination(GET_WORK_STREAM_TIMEOUT_MINUTES, TimeUnit.MINUTES)) {
+          stream.close();
+        }
       } catch (InterruptedException e) {
         // Continue processing until !running.get()
       }
@@ -1080,11 +1100,12 @@ public class StreamingDataflowWorker {
 
     private final Windmill.WorkItem workItem;
     private final Instant startTime;
+    private Instant stateStartTime;
     private State state;
 
     public Work(Windmill.WorkItem workItem) {
       this.workItem = workItem;
-      this.startTime = Instant.now();
+      this.startTime = this.stateStartTime = Instant.now();
       this.state = State.QUEUED;
     }
 
@@ -1102,6 +1123,11 @@ public class StreamingDataflowWorker {
 
     public void setState(State state) {
       this.state = state;
+      this.stateStartTime = Instant.now();
+    }
+
+    public Instant getStateStartTime() {
+      return stateStartTime;
     }
   }
 
@@ -1423,8 +1449,8 @@ public class StreamingDataflowWorker {
         sleep(retryLocallyDelayMs);
         workUnitExecutor.forceExecute(work);
       } else {
-        // Consider the item invalid. It will eventually be retried by Windmill if it still needs
-        // to be processed.
+        // Consider the item invalid. It will eventually be retried by Windmill if it still needs to
+        // be processed.
         computationState.completeWork(key, workItem.getWorkToken());
       }
     } finally {
@@ -1494,7 +1520,7 @@ public class StreamingDataflowWorker {
       Windmill.CommitWorkRequest commitRequest = commitRequestBuilder.build();
       LOG.trace("Commit: {}", commitRequest);
       activeCommitBytes.set(commitBytes);
-      commitWork(commitRequest);
+      windmillServer.commitWork(commitRequest);
       activeCommitBytes.set(0);
       for (Map.Entry<ComputationState, Windmill.ComputationCommitWorkRequest.Builder> entry :
           computationRequestMap.entrySet()) {
@@ -1506,71 +1532,82 @@ public class StreamingDataflowWorker {
     }
   }
 
+  // Adds the commit to the commitStream if it fits, returning true iff it is consumed.
+  private boolean addCommitToStream(Commit commit, CommitWorkStream commitStream) {
+    Preconditions.checkNotNull(commit);
+    final ComputationState state = commit.getComputationState();
+    final Windmill.WorkItemCommitRequest request = commit.getRequest();
+    final int size = commit.getSize();
+    commit.getWork().setState(State.COMMITTING);
+    activeCommitBytes.addAndGet(size);
+    if (commitStream.commitWorkItem(
+        state.computationId,
+        request,
+        (Windmill.CommitStatus status) -> {
+          if (status != Windmill.CommitStatus.OK) {
+            stateCache.forComputation(state.computationId).invalidate(request.getKey());
+          }
+          activeCommitBytes.addAndGet(-size);
+          // This may throw an exception if the commit was not active, which is possible if it
+          // was deemed stuck.
+          state.completeWork(request.getKey(), request.getWorkToken());
+        })) {
+      return true;
+    } else {
+      // Back out the stats changes since the commit wasn't consumed.
+      commit.getWork().setState(State.COMMIT_QUEUED);
+      activeCommitBytes.addAndGet(-size);
+      return false;
+    }
+  }
+
+  // Helper to batch additional commits into the commit stream as long as they fit.
+  // Returns a commit that was removed from the queue but not consumed or null.
+  private Commit batchCommitsToStream(CommitWorkStream commitStream) {
+    int commits = 1;
+    while (running.get()) {
+      Commit commit;
+      try {
+        if (commits < 5) {
+          commit = commitQueue.poll(10 - 2 * commits, TimeUnit.MILLISECONDS);
+        } else {
+          commit = commitQueue.poll();
+        }
+      } catch (InterruptedException e) {
+        // Continue processing until !running.get()
+        continue;
+      }
+      if (commit == null || !addCommitToStream(commit, commitStream)) {
+        return commit;
+      }
+      commits++;
+    }
+    return null;
+  }
+
   private void streamingCommitLoop() {
     StreamPool<CommitWorkStream> streamPool =
         new StreamPool<>(
             NUM_COMMIT_STREAMS, COMMIT_STREAM_TIMEOUT, windmillServer::commitWorkStream);
-    Commit commit = null;
+    Commit initialCommit = null;
     while (running.get()) {
-      // Batch commits as long as there are more and we can fit them in the current request.
-      // We lazily initialize the commit stream to make sure that we only create one after
-      // we have a commit.
-      CommitWorkStream commitStream = null;
-      int commits = 0;
-      while (running.get()) {
-        // There may be a commit left over from the previous iteration but if not, pull one.
-        if (commit == null) {
-          try {
-            if (commits == 0) {
-              commit = commitQueue.take();
-            } else if (commits < 10) {
-              commit = commitQueue.poll(20 - 2 * commits, TimeUnit.MILLISECONDS);
-            } else {
-              commit = commitQueue.poll();
-            }
-          } catch (InterruptedException e) {
-            // Continue processing until !running.get()
-            continue;
-          }
-          if (commit == null) {
-            // No longer batching, break loop to trigger flush.
-            break;
-          }
-        }
-
-        commits++;
-        final ComputationState state = commit.getComputationState();
-        final Windmill.WorkItemCommitRequest request = commit.getRequest();
-        final int size = commit.getSize();
-        commit.getWork().setState(State.COMMITTING);
-        if (commitStream == null) {
-          commitStream = streamPool.getStream();
-        }
-        if (commitStream.commitWorkItem(
-            state.computationId,
-            request,
-            (Windmill.CommitStatus status) -> {
-              if (status != Windmill.CommitStatus.OK) {
-                stateCache.forComputation(state.computationId).invalidate(request.getKey());
-              }
-              state.completeWork(request.getKey(), request.getWorkToken());
-              activeCommitBytes.addAndGet(-size);
-            })) {
-          // The commit was consumed.
-          commit = null;
-          // It's possible we could decrement from the callback above before adding here but since
-          // it's just for the status page we don't care.
-          activeCommitBytes.addAndGet(size);
-        } else {
-          // The commit was not consumed, leave it set and it will be added to the subsequent stream
-          // on the next iteration after this stream is flushed.
-          break;
+      if (initialCommit == null) {
+        try {
+          initialCommit = commitQueue.take();
+        } catch (InterruptedException e) {
+          continue;
         }
       }
-      if (commitStream != null) {
-        commitStream.flush();
-        streamPool.releaseStream(commitStream);
+      // We initialize the commit stream only after we have a commit to make sure it is fresh.
+      CommitWorkStream commitStream = streamPool.getStream();
+      if (!addCommitToStream(initialCommit, commitStream)) {
+        throw new AssertionError("Initial commit on flushed stream should always be accepted.");
       }
+      // Batch additional commits to the stream and possibly make an un-batched commit the next
+      // initial commit.
+      initialCommit = batchCommitsToStream(commitStream);
+      commitStream.flush();
+      streamPool.releaseStream(commitStream);
     }
   }
 
@@ -1583,10 +1620,6 @@ public class StreamingDataflowWorker {
             .build());
   }
 
-  private void commitWork(Windmill.CommitWorkRequest request) {
-    windmillServer.commitWork(request);
-  }
-
   private void getConfigFromWindmill(String computation) {
     Windmill.GetConfigRequest request =
         Windmill.GetConfigRequest.newBuilder().addComputations(computation).build();
@@ -2056,6 +2089,14 @@ public class StreamingDataflowWorker {
     metricTrackingWindmillServer.refreshActiveWork(active);
   }
 
+  private void invalidateStuckCommits() {
+    Instant stuckCommitDeadline =
+        Instant.now().minus(Duration.millis(options.getStuckCommitDurationMillis()));
+    for (Map.Entry<String, ComputationState> entry : computationMap.entrySet()) {
+      entry.getValue().invalidateStuckCommits(stuckCommitDeadline);
+    }
+  }
+
   /**
    * Class representing the state of a computation.
    *
@@ -2068,16 +2109,18 @@ public class StreamingDataflowWorker {
     private final ImmutableMap<String, String> transformUserNameToStateFamily;
     // Map from key to work for the key.  The first item in the queue is
     // actively processing.  Synchronized by itself.
-    private final Map<ByteString, Queue<Work>> activeWork = new HashMap<>();
+    private final Map<ByteString, Deque<Work>> activeWork = new HashMap<>();
     private final BoundedQueueExecutor executor;
     private final ConcurrentMap<SdkWorkerHarness, ConcurrentLinkedQueue<ExecutionState>>
         executionStateQueues = new ConcurrentHashMap<>();
+    private final WindmillStateCache.ForComputation computationStateCache;
 
     public ComputationState(
         String computationId,
         MapTask mapTask,
         BoundedQueueExecutor executor,
-        Map<String, String> transformUserNameToStateFamily) {
+        Map<String, String> transformUserNameToStateFamily,
+        WindmillStateCache.ForComputation computationStateCache) {
       this.computationId = computationId;
       this.mapTask = mapTask;
       this.executor = executor;
@@ -2085,6 +2128,7 @@ public class StreamingDataflowWorker {
           transformUserNameToStateFamily != null
               ? ImmutableMap.copyOf(transformUserNameToStateFamily)
               : ImmutableMap.of();
+      this.computationStateCache = computationStateCache;
       Preconditions.checkNotNull(mapTask.getStageName());
       Preconditions.checkNotNull(mapTask.getSystemName());
     }
@@ -2110,20 +2154,23 @@ public class StreamingDataflowWorker {
     /** Mark the given key and work as active. */
     public boolean activateWork(ByteString key, Work work) {
       synchronized (activeWork) {
-        Queue<Work> queue = activeWork.get(key);
-        if (queue == null) {
+        Deque<Work> queue = activeWork.get(key);
+        if (queue != null) {
+          Preconditions.checkState(!queue.isEmpty());
+          // Ensure we don't already have this work token queueud.
+          for (Work queuedWork : queue) {
+            if (queuedWork.getWorkItem().getWorkToken() == work.getWorkItem().getWorkToken()) {
+              return false;
+            }
+          }
+          // Queue the work for later processing.
+          queue.addLast(work);
+          return true;
+        } else {
           queue = new ArrayDeque<>();
+          queue.addLast(work);
           activeWork.put(key, queue);
-          queue.add(work);
           // Fall through to execute without the lock held.
-        } else {
-          if (queue.peek().getWorkItem().getWorkToken() != work.getWorkItem().getWorkToken()) {
-            // Queue the work for later processing.
-            queue.add(work);
-            return true;
-          }
-          // Skip the work if duplicate
-          return false;
         }
       }
       executor.execute(work);
@@ -2132,11 +2179,11 @@ public class StreamingDataflowWorker {
 
     /** Marks the work for a the given key as complete. Schedules queued work for the key if any. */
     public void completeWork(ByteString key, long workToken) {
-      Work work = null;
+      Work nextWork;
       synchronized (activeWork) {
         Queue<Work> queue = activeWork.get(key);
         Preconditions.checkNotNull(queue);
-        Work completedWork = queue.poll();
+        Work completedWork = queue.peek();
         // avoid Preconditions.checkNotNull and checkState here to prevent eagerly evaluating the
         // format string parameters for the error message.
         if (completedWork == null) {
@@ -2153,20 +2200,45 @@ public class StreamingDataflowWorker {
                   completedWork.getWorkItem().getWorkToken(),
                   workToken));
         }
-        if (queue.peek() == null) {
-          activeWork.remove(key);
-          return;
+        queue.remove(); // We consumed the matching work item.
+        nextWork = queue.peek();
+        if (nextWork == null) {
+          Preconditions.checkState(queue == activeWork.remove(key));
+        }
+      }
+      if (nextWork != null) {
+        executor.forceExecute(nextWork);
+      }
+    }
+
+    public void invalidateStuckCommits(Instant stuckCommitDeadline) {
+      synchronized (activeWork) {
+        // Determine the stuck commit keys but complete them outside of iterating over
+        // activeWork as completeWork may delete the entry from activeWork.
+        Map<ByteString, Long> stuckCommits = new HashMap<>();
+        for (Map.Entry<ByteString, Deque<Work>> entry : activeWork.entrySet()) {
+          ByteString key = entry.getKey();
+          Work work = entry.getValue().peek();
+          if (work.getState() == State.COMMITTING
+              && work.getStateStartTime().isBefore(stuckCommitDeadline)) {
+            LOG.error(
+                "Detected key with sharding key {} stuck in COMMITTING state, completing it with error. Key ",
+                work.workItem.getShardingKey());
+            stuckCommits.put(key, work.getWorkItem().getWorkToken());
+          }
+        }
+        for (Map.Entry<ByteString, Long> stuckCommit : stuckCommits.entrySet()) {
+          computationStateCache.invalidate(stuckCommit.getKey());
+          completeWork(stuckCommit.getKey(), stuckCommit.getValue());
         }
-        work = queue.peek();
       }
-      executor.forceExecute(work);
     }
 
     /** Adds any work started before the refreshDeadline to the GetDataRequest builder. */
     public List<Windmill.KeyedGetDataRequest> getKeysToRefresh(Instant refreshDeadline) {
       List<Windmill.KeyedGetDataRequest> result = new ArrayList<>();
       synchronized (activeWork) {
-        for (Map.Entry<ByteString, Queue<Work>> entry : activeWork.entrySet()) {
+        for (Map.Entry<ByteString, Deque<Work>> entry : activeWork.entrySet()) {
           ByteString key = entry.getKey();
           for (Work work : entry.getValue()) {
             if (work.getStartTime().isBefore(refreshDeadline)) {
@@ -2183,6 +2255,12 @@ public class StreamingDataflowWorker {
       return result;
     }
 
+    private String elapsedString(Instant start, Instant end) {
+      Duration activeFor = new Duration(start, end);
+      // Duration's toString always starts with "PT"; remove that here.
+      return activeFor.toString().substring(2);
+    }
+
     public void printActiveWork(PrintWriter writer) {
       final Instant now = Instant.now();
       // The max number of keys in COMMITTING or COMMIT_QUEUED status to be shown.
@@ -2192,17 +2270,16 @@ public class StreamingDataflowWorker {
           "<table border=\"1\" "
               + "style=\"border-collapse:collapse;padding:5px;border-spacing:5px;border:1px\">");
       writer.println(
-          "<tr><th>Key</th><th>Token</th><th>Queued</th><th>Active For</th><th>State</th></tr>");
+          "<tr><th>Key</th><th>Token</th><th>Queued</th><th>Active For</th><th>State</th><th>State Active For</th></tr>");
       // We use a StringBuilder in the synchronized section to buffer writes since the provided
       // PrintWriter may block when flushing.
       StringBuilder builder = new StringBuilder();
       synchronized (activeWork) {
-        for (Map.Entry<ByteString, Queue<Work>> entry : activeWork.entrySet()) {
+        for (Map.Entry<ByteString, Deque<Work>> entry : activeWork.entrySet()) {
           Queue<Work> queue = entry.getValue();
+          Preconditions.checkNotNull(queue);
           Work work = queue.peek();
-          if (work == null) {
-            continue;
-          }
+          Preconditions.checkNotNull(work);
           Windmill.WorkItem workItem = work.getWorkItem();
           State state = work.getState();
           if (state == State.COMMITTING || state == State.COMMIT_QUEUED) {
@@ -2218,12 +2295,12 @@ public class StreamingDataflowWorker {
           builder.append("</td><td>");
           builder.append(queue.size() - 1);
           builder.append("</td><td>");
-          Duration activeFor = new Duration(work.getStartTime(), now);
-          // Duration's toString always starts with "PT"; remove that here.
-          builder.append(activeFor.toString().substring(2));
+          builder.append(elapsedString(work.getStartTime(), now));
           builder.append("</td><td>");
           builder.append(state);
           builder.append("</td></tr>\n");
+          builder.append(elapsedString(work.getStateStartTime(), now));
+          builder.append("</td></tr>\n");
         }
       }
       writer.print(builder.toString());
@@ -2231,7 +2308,7 @@ public class StreamingDataflowWorker {
       if (commitPendingCount >= maxCommitPending) {
         writer.println("<br>");
         writer.print("Skipped keys in COMMITTING/COMMIT_QUEUED: ");
-        writer.println(maxCommitPending - commitPendingCount);
+        writer.println(commitPendingCount - maxCommitPending);
         writer.println("<br>");
       }
     }
@@ -2326,20 +2403,13 @@ public class StreamingDataflowWorker {
               + MAX_WORK_UNITS_QUEUED
               + "<br>");
       writer.print("Commit Queue: ");
-      writer.print(commitQueue.weight() >> 20);
-      writer.print("MB, ");
+      appendHumanizedBytes(commitQueue.weight(), writer);
+      writer.print(", ");
       writer.print(commitQueue.size());
       writer.println(" elements<br>");
 
       writer.print("Active commit: ");
-      long commitBytes = activeCommitBytes.get();
-      if (commitBytes == 0) {
-        writer.print("none");
-      } else {
-        writer.print("~");
-        writer.print((commitBytes >> 20) + 1);
-        writer.print("MB");
-      }
+      appendHumanizedBytes(activeCommitBytes.get(), writer);
       writer.println("<br>");
 
       metricTrackingWindmillServer.printHtml(writer);
@@ -2354,5 +2424,20 @@ public class StreamingDataflowWorker {
         writer.println("<br>");
       }
     }
+
+    private void appendHumanizedBytes(long bytes, PrintWriter writer) {
+      if (bytes < (4 << 10)) {
+        writer.print(bytes);
+        writer.print("B");
+      } else if (bytes < (4 << 20)) {
+        writer.print("~");
+        writer.print(bytes >> 10);
+        writer.print("KB");
+      } else {
+        writer.print("~");
+        writer.print(bytes >> 20);
+        writer.print("MB");
+      }
+    }
   }
 }
diff --git a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/WindmillStateCache.java b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/WindmillStateCache.java
index d74b0db7c94..b419a38e3ce 100644
--- a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/WindmillStateCache.java
+++ b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/WindmillStateCache.java
@@ -106,10 +106,9 @@ public class WindmillStateCache implements StatusDataProvider {
     public void invalidate(ByteString processingKey) {
       synchronized (this) {
         ComputationKey key = new ComputationKey(computation, processingKey);
-        for (StateId id : keyIndex.get(key)) {
+        for (StateId id : keyIndex.removeAll(key)) {
           stateCache.invalidate(id);
         }
-        keyIndex.removeAll(key);
       }
     }
 
diff --git a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/options/StreamingDataflowWorkerOptions.java b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/options/StreamingDataflowWorkerOptions.java
index dbad9807edb..b5d32246b3a 100644
--- a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/options/StreamingDataflowWorkerOptions.java
+++ b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/options/StreamingDataflowWorkerOptions.java
@@ -69,6 +69,12 @@ public interface StreamingDataflowWorkerOptions extends DataflowWorkerHarnessOpt
 
   void setActiveWorkRefreshPeriodMillis(int value);
 
+  @Description("Necessary duration for a commit to be considered stuck and invalidated.")
+  @Default.Integer(10 * 60 * 1000)
+  int getStuckCommitDurationMillis();
+
+  void setStuckCommitDurationMillis(int value);
+
   @Description(
       "Period for sending 'global get config' requests to the service. The duration is "
           + "specified as seconds in 'PTx.yS' format, e.g. 'PT5.125S'."
diff --git a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/windmill/GrpcWindmillServer.java b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/windmill/GrpcWindmillServer.java
index 98f74c73985..c64803d40ed 100644
--- a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/windmill/GrpcWindmillServer.java
+++ b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/windmill/GrpcWindmillServer.java
@@ -117,8 +117,6 @@ public class GrpcWindmillServer extends WindmillServerStub {
   // high.
   private static final long DEFAULT_UNARY_RPC_DEADLINE_SECONDS = 300;
   private static final long DEFAULT_STREAM_RPC_DEADLINE_SECONDS = 300;
-  // Stream clean close seconds must be set lower than the stream deadline seconds.
-  private static final long DEFAULT_STREAM_CLEAN_CLOSE_SECONDS = 180;
 
   private static final Duration MIN_BACKOFF = Duration.millis(1);
   private static final Duration MAX_BACKOFF = Duration.standardSeconds(30);
@@ -137,13 +135,14 @@ public class GrpcWindmillServer extends WindmillServerStub {
       syncStubList = new ArrayList<>();
   private WindmillApplianceGrpc.WindmillApplianceBlockingStub syncApplianceStub = null;
   private long unaryDeadlineSeconds = DEFAULT_UNARY_RPC_DEADLINE_SECONDS;
+  private long streamDeadlineSeconds = DEFAULT_STREAM_RPC_DEADLINE_SECONDS;
   private ImmutableSet<HostAndPort> endpoints;
   private int logEveryNStreamFailures = 20;
   private Duration maxBackoff = MAX_BACKOFF;
   private final ThrottleTimer getWorkThrottleTimer = new ThrottleTimer();
   private final ThrottleTimer getDataThrottleTimer = new ThrottleTimer();
   private final ThrottleTimer commitWorkThrottleTimer = new ThrottleTimer();
-  Random rand = new Random();
+  private final Random rand = new Random();
 
   private final Set<AbstractWindmillStream<?, ?>> streamRegistry =
       Collections.newSetFromMap(new ConcurrentHashMap<AbstractWindmillStream<?, ?>, Boolean>());
@@ -213,7 +212,7 @@ public class GrpcWindmillServer extends WindmillServerStub {
   private synchronized void initializeLocalHost(int port) throws IOException {
     this.logEveryNStreamFailures = 1;
     this.maxBackoff = Duration.millis(500);
-    this.unaryDeadlineSeconds = 10; // For local testing use a short deadline.
+    this.unaryDeadlineSeconds = 10; // For local testing use short deadlines.
     Channel channel = localhostChannel(port);
     if (streamingEngineEnabled()) {
       this.stubList.add(CloudWindmillServiceV1Alpha1Grpc.newStub(channel));
@@ -599,7 +598,7 @@ public class GrpcWindmillServer extends WindmillServerStub {
       this.clientFactory = clientFactory;
     }
 
-    /** Called on each response from the server */
+    /** Called on each response from the server. */
     protected abstract void onResponse(ResponseT response);
     /** Called when a new underlying stream to the server has been opened. */
     protected abstract void onNewStream();
@@ -607,7 +606,7 @@ public class GrpcWindmillServer extends WindmillServerStub {
     protected abstract boolean hasPendingRequests();
     /**
      * Called when the stream is throttled due to resource exhausted errors. Will be called for each
-     * resource exhausted error not just the first. onResponse() must stop throttling on reciept of
+     * resource exhausted error not just the first. onResponse() must stop throttling on receipt of
      * the first good message.
      */
     protected abstract void startThrottleTimer();
@@ -745,15 +744,6 @@ public class GrpcWindmillServer extends WindmillServerStub {
       return finishLatch.await(time, unit);
     }
 
-    @Override
-    public final void closeAfterDefaultTimeout() throws InterruptedException {
-      if (!finishLatch.await(DEFAULT_STREAM_CLEAN_CLOSE_SECONDS, TimeUnit.SECONDS)) {
-        // If the stream did not close due to error in the specified amount of time, half-close
-        // the stream cleanly.
-        close();
-      }
-    }
-
     @Override
     public final Instant startTime() {
       return new Instant(startTimeMs.get());
@@ -773,7 +763,7 @@ public class GrpcWindmillServer extends WindmillServerStub {
       super(
           responseObserver ->
               stub()
-                  .withDeadlineAfter(DEFAULT_STREAM_RPC_DEADLINE_SECONDS, TimeUnit.SECONDS)
+                  .withDeadlineAfter(streamDeadlineSeconds, TimeUnit.SECONDS)
                   .getWorkStream(responseObserver));
       this.request = request;
       this.receiver = receiver;
@@ -946,7 +936,7 @@ public class GrpcWindmillServer extends WindmillServerStub {
       super(
           responseObserver ->
               stub()
-                  .withDeadlineAfter(DEFAULT_STREAM_RPC_DEADLINE_SECONDS, TimeUnit.SECONDS)
+                  .withDeadlineAfter(streamDeadlineSeconds, TimeUnit.SECONDS)
                   .getDataStream(responseObserver));
       startStream();
     }
@@ -1161,7 +1151,7 @@ public class GrpcWindmillServer extends WindmillServerStub {
 
     private class Batcher {
       long queuedBytes = 0;
-      Map<Long, PendingRequest> queue = new HashMap<>();
+      final Map<Long, PendingRequest> queue = new HashMap<>();
 
       boolean canAccept(PendingRequest request) {
         return queue.isEmpty()
@@ -1178,6 +1168,7 @@ public class GrpcWindmillServer extends WindmillServerStub {
       void flush() {
         flushInternal(queue);
         queuedBytes = 0;
+        queue.clear();
       }
     }
 
@@ -1187,7 +1178,7 @@ public class GrpcWindmillServer extends WindmillServerStub {
       super(
           responseObserver ->
               stub()
-                  .withDeadlineAfter(DEFAULT_STREAM_RPC_DEADLINE_SECONDS, TimeUnit.SECONDS)
+                  .withDeadlineAfter(streamDeadlineSeconds, TimeUnit.SECONDS)
                   .commitWorkStream(responseObserver));
       startStream();
     }
@@ -1219,16 +1210,27 @@ public class GrpcWindmillServer extends WindmillServerStub {
     protected void onResponse(StreamingCommitResponse response) {
       commitWorkThrottleTimer.stop();
 
+      RuntimeException finalException = null;
       for (int i = 0; i < response.getRequestIdCount(); ++i) {
         long requestId = response.getRequestId(i);
         PendingRequest done = pending.remove(requestId);
         if (done == null) {
           LOG.error("Got unknown commit request ID: {}", requestId);
         } else {
-          done.onDone.accept(
-              (i < response.getStatusCount()) ? response.getStatus(i) : CommitStatus.OK);
+          try {
+            done.onDone.accept(
+                (i < response.getStatusCount()) ? response.getStatus(i) : CommitStatus.OK);
+          } catch (RuntimeException e) {
+            // Catch possible exceptions to ensure that an exception for one commit does not prevent
+            // other commits from being processed.
+            LOG.warn("Exception while processing commit response {} ", e);
+            finalException = e;
+          }
         }
       }
+      if (finalException != null) {
+        throw finalException;
+      }
     }
 
     @Override
@@ -1252,7 +1254,7 @@ public class GrpcWindmillServer extends WindmillServerStub {
       batcher.flush();
     }
 
-    private final void flushInternal(Map<Long, PendingRequest> requests) {
+    private void flushInternal(Map<Long, PendingRequest> requests) {
       if (requests.isEmpty()) {
         return;
       }
@@ -1266,7 +1268,6 @@ public class GrpcWindmillServer extends WindmillServerStub {
       } else {
         issueBatchedRequest(requests);
       }
-      requests.clear();
     }
 
     private void issueSingleRequest(final long id, PendingRequest pendingRequest) {
@@ -1278,13 +1279,13 @@ public class GrpcWindmillServer extends WindmillServerStub {
           .setShardingKey(pendingRequest.request.getShardingKey())
           .setSerializedWorkItemCommit(pendingRequest.request.toByteString());
       StreamingCommitWorkRequest chunk = requestBuilder.build();
-      try {
-        synchronized (this) {
-          pending.put(id, pendingRequest);
+      synchronized (this) {
+        pending.put(id, pendingRequest);
+        try {
           send(chunk);
+        } catch (IllegalStateException e) {
+          // Stream was broken, request will be retried when stream is reopened.
         }
-      } catch (IllegalStateException e) {
-        // Stream was broken, request will be retried when stream is reopened.
       }
     }
 
@@ -1303,13 +1304,13 @@ public class GrpcWindmillServer extends WindmillServerStub {
         chunkBuilder.setSerializedWorkItemCommit(request.request.toByteString());
       }
       StreamingCommitWorkRequest request = requestBuilder.build();
-      try {
-        synchronized (this) {
-          pending.putAll(requests);
+      synchronized (this) {
+        pending.putAll(requests);
+        try {
           send(request);
+        } catch (IllegalStateException e) {
+          // Stream was broken, request will be retried when stream is reopened.
         }
-      } catch (IllegalStateException e) {
-        // Stream was broken, request will be retried when stream is reopened.
       }
     }
 
@@ -1492,7 +1493,7 @@ public class GrpcWindmillServer extends WindmillServerStub {
       }
     }
 
-    /** Returns if the specified type is currently being throttled */
+    /** Returns if the specified type is currently being throttled. */
     public synchronized boolean throttled() {
       return startTime != -1;
     }
diff --git a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/windmill/WindmillServerStub.java b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/windmill/WindmillServerStub.java
index 2b5453b51b8..31c51142c79 100644
--- a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/windmill/WindmillServerStub.java
+++ b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/windmill/WindmillServerStub.java
@@ -106,12 +106,6 @@ public abstract class WindmillServerStub implements StatusDataProvider {
     /** Waits for the server to close its end of the connection, with timeout. */
     boolean awaitTermination(int time, TimeUnit unit) throws InterruptedException;
 
-    /**
-     * Cleanly closes the stream after implementation-speficied timeout, unless the stream is
-     * aborted before the timeout is reached.
-     */
-    void closeAfterDefaultTimeout() throws InterruptedException;
-
     /** Returns when the stream was opened. */
     Instant startTime();
   }
diff --git a/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/FakeWindmillServer.java b/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/FakeWindmillServer.java
index 2c27606bb19..e788578f6ad 100644
--- a/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/FakeWindmillServer.java
+++ b/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/FakeWindmillServer.java
@@ -64,6 +64,8 @@ class FakeWindmillServer extends WindmillServerStub {
   private final AtomicInteger expectedExceptionCount;
   private final ErrorCollector errorCollector;
   private boolean isReady = true;
+  private boolean dropStreamingCommits = false;
+  private final AtomicInteger droppedStreamingCommits;
 
   public FakeWindmillServer(ErrorCollector errorCollector) {
     workToOffer = new ConcurrentLinkedQueue<>();
@@ -73,6 +75,11 @@ class FakeWindmillServer extends WindmillServerStub {
     expectedExceptionCount = new AtomicInteger();
     this.errorCollector = errorCollector;
     statsReceived = new ArrayList<>();
+    droppedStreamingCommits = new AtomicInteger();
+  }
+
+  public void setDropStreamingCommits(boolean dropStreamingCommits) {
+    this.dropStreamingCommits = dropStreamingCommits;
   }
 
   public void addWorkToOffer(Windmill.GetWorkResponse work) {
@@ -188,7 +195,12 @@ class FakeWindmillServer extends WindmillServerStub {
     final CountDownLatch done = new CountDownLatch(1);
     return new GetWorkStream() {
       @Override
-      public void closeAfterDefaultTimeout() {
+      public void close() {
+        done.countDown();
+      }
+
+      @Override
+      public boolean awaitTermination(int time, TimeUnit unit) throws InterruptedException {
         while (done.getCount() > 0) {
           Windmill.GetWorkResponse response = workToOffer.poll();
           if (response == null) {
@@ -210,15 +222,6 @@ class FakeWindmillServer extends WindmillServerStub {
             }
           }
         }
-      }
-
-      @Override
-      public void close() {
-        done.countDown();
-      }
-
-      @Override
-      public boolean awaitTermination(int time, TimeUnit unit) throws InterruptedException {
         return done.await(time, unit);
       }
 
@@ -279,9 +282,6 @@ class FakeWindmillServer extends WindmillServerStub {
         return true;
       }
 
-      @Override
-      public void closeAfterDefaultTimeout() {}
-
       @Override
       public Instant startTime() {
         return startTime;
@@ -303,9 +303,15 @@ class FakeWindmillServer extends WindmillServerStub {
         errorCollector.checkThat(
             request.getShardingKey(), allOf(greaterThan(0L), lessThan(Long.MAX_VALUE)));
         errorCollector.checkThat(request.getCacheToken(), not(equalTo(0L)));
-        commitsReceived.put(request.getWorkToken(), request);
-        onDone.accept(Windmill.CommitStatus.OK);
-        return true; // The request was accepted.
+        if (dropStreamingCommits) {
+          droppedStreamingCommits.incrementAndGet();
+        } else {
+          commitsReceived.put(request.getWorkToken(), request);
+          onDone.accept(Windmill.CommitStatus.OK);
+        }
+        // Return true to indicate the request was accepted even if we are dropping the commit
+        // to simulate a dropped commit.
+        return true;
       }
 
       @Override
@@ -319,9 +325,6 @@ class FakeWindmillServer extends WindmillServerStub {
         return true;
       }
 
-      @Override
-      public void closeAfterDefaultTimeout() {}
-
       @Override
       public Instant startTime() {
         return startTime;
@@ -358,6 +361,15 @@ class FakeWindmillServer extends WindmillServerStub {
     return commitsReceived;
   }
 
+  public void waitForDroppedCommits(int droppedCommits) {
+    LOG.debug("waitForDroppedCommits: {}", droppedCommits);
+    int maxTries = 10;
+    while (maxTries-- > 0 && droppedStreamingCommits.get() < droppedCommits) {
+      Uninterruptibles.sleepUninterruptibly(1000, TimeUnit.MILLISECONDS);
+    }
+    assertEquals(droppedCommits, droppedStreamingCommits.get());
+  }
+
   public void setExpectedExceptionCount(int i) {
     expectedExceptionCount.getAndAdd(i);
   }
diff --git a/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorkerTest.java b/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorkerTest.java
index 2cb5ada069f..c954bdbc9b6 100644
--- a/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorkerTest.java
+++ b/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorkerTest.java
@@ -2089,7 +2089,8 @@ public class StreamingDataflowWorkerTest {
             "computation",
             defaultMapTask(Arrays.asList(makeSourceInstruction(StringUtf8Coder.of()))),
             mockExecutor,
-            ImmutableMap.of());
+            ImmutableMap.of(),
+            null);
 
     ByteString key1 = ByteString.copyFromUtf8("key1");
     ByteString key2 = ByteString.copyFromUtf8("key2");
@@ -2600,4 +2601,39 @@ public class StreamingDataflowWorkerTest {
 
     assertThat(commit.getSerializedSize(), isWithinBundleSizeLimits);
   }
+
+  @Test
+  public void testStuckCommit() throws Exception {
+    if (!streamingEngine) {
+      // Stuck commits have only been observed with streaming engine and thus recovery from them is
+      // not implemented for non-streaming engine.
+      return;
+    }
+
+    List<ParallelInstruction> instructions =
+        Arrays.asList(
+            makeSourceInstruction(StringUtf8Coder.of()),
+            makeSinkInstruction(StringUtf8Coder.of(), 0));
+
+    FakeWindmillServer server = new FakeWindmillServer(errorCollector);
+    StreamingDataflowWorkerOptions options = createTestingPipelineOptions(server);
+    options.setStuckCommitDurationMillis(2000);
+    StreamingDataflowWorker worker = makeWorker(instructions, options, true /* publishCounters */);
+    worker.start();
+    // Prevent commit callbacks from being called to simulate a stuck commit.
+    server.setDropStreamingCommits(true);
+
+    // Add some work for key 1.
+    server.addWorkToOffer(makeInput(10, TimeUnit.MILLISECONDS.toMicros(2), keyStringForIndex(1)));
+    server.waitForDroppedCommits(1);
+    server.setDropStreamingCommits(false);
+    // Enqueue another work item for key 1.
+    server.addWorkToOffer(makeInput(1, TimeUnit.MILLISECONDS.toMicros(1)));
+    // Ensure that the second work item processes.
+    Map<Long, Windmill.WorkItemCommitRequest> result = server.waitForAndGetCommits(1);
+    worker.stop();
+
+    assertTrue(result.containsKey(1L));
+    assertEquals(makeExpectedOutput(1, TimeUnit.MILLISECONDS.toMicros(1)).build(), result.get(1L));
+  }
 }
