diff --git a/CHANGES.md b/CHANGES.md
index e14ed80e4b4..7f742595c1a 100644
--- a/CHANGES.md
+++ b/CHANGES.md
@@ -52,6 +52,14 @@
 ## Highlights
 
 ## I/Os
+* Python: Deprecated module `apache_beam.io.gcp.datastore.v1` has been removed
+as the client it uses is out of date and does not support Python 3
+([BEAM-9529](https://issues.apache.org/jira/browse/BEAM-9529)).
+Please migrate your code to use
+[apache_beam.io.gcp.datastore.**v1new**](https://beam.apache.org/releases/pydoc/current/apache_beam.io.gcp.datastore.v1new.datastoreio.html).
+See the updated
+[datastore_wordcount](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/cookbook/datastore_wordcount.py)
+for example usage.
 
 ## New Features / Improvements
 * Python SDK will now use Python 3 type annotations as pipeline type hints.
diff --git a/ownership/PYTHON_DEPENDENCY_OWNERS.yaml b/ownership/PYTHON_DEPENDENCY_OWNERS.yaml
index 865afe477d2..52292949797 100644
--- a/ownership/PYTHON_DEPENDENCY_OWNERS.yaml
+++ b/ownership/PYTHON_DEPENDENCY_OWNERS.yaml
@@ -54,9 +54,6 @@ deps:
   google-cloud-pubsub:
     owners: markflyhigh
 
-  googledatastore:
-    owners:
-
   grpcio:
     owners: robertwb
 
@@ -75,9 +72,6 @@ deps:
   oauth2client:
     owners:
 
-  proto-google-cloud-datastore-v1:
-    owners:
-
   proto-google-cloud-pubsub-v1:
     owners:
 
diff --git a/sdks/python/apache_beam/examples/cookbook/datastore_wordcount.py b/sdks/python/apache_beam/examples/cookbook/datastore_wordcount.py
index a8dfe79cde3..39c76ca80ee 100644
--- a/sdks/python/apache_beam/examples/cookbook/datastore_wordcount.py
+++ b/sdks/python/apache_beam/examples/cookbook/datastore_wordcount.py
@@ -32,67 +32,57 @@ counts them and write the output to a set of files.
 
 The following options must be provided to run this pipeline in read-only mode:
 ``
---dataset YOUR_DATASET
+--project GCP_PROJECT
 --kind YOUR_DATASTORE_KIND
 --output [YOUR_LOCAL_FILE *or* gs://YOUR_OUTPUT_PATH]
 --read_only
 ``
 
-Dataset maps to Project ID for v1 version of datastore.
-
 Read-write Mode: In this mode, this example reads words from an input file,
-converts them to Cloud Datastore ``Entity`` objects and writes them to
-Cloud Datastore using the ``datastoreio.Write`` transform. The second pipeline
+converts them to Beam ``Entity`` objects and writes them to Cloud Datastore
+using the ``datastoreio.WriteToDatastore`` transform. The second pipeline
 will then read these Cloud Datastore entities using the
 ``datastoreio.ReadFromDatastore`` transform, extract the words, count them and
 write the output to a set of files.
 
 The following options must be provided to run this pipeline in read-write mode:
 ``
---dataset YOUR_DATASET
+--project GCP_PROJECT
 --kind YOUR_DATASTORE_KIND
 --output [YOUR_LOCAL_FILE *or* gs://YOUR_OUTPUT_PATH]
 ``
-
-Note: We are using the Cloud Datastore protobuf objects directly because
-that is the interface that the ``datastoreio`` exposes.
-See the following links on more information about these protobuf messages.
-https://cloud.google.com/datastore/docs/reference/rpc/google.datastore.v1 and
-https://github.com/googleapis/googleapis/tree/master/google/datastore/v1
 """
 
 # pytype: skip-file
 
 from __future__ import absolute_import
+from __future__ import print_function
 
 import argparse
 import logging
 import re
+import sys
+from typing import Iterable
+from typing import Optional
+from typing import Text
 import uuid
 from builtins import object
 
 import apache_beam as beam
 from apache_beam.io import ReadFromText
-from apache_beam.io.gcp.datastore.v1.datastoreio import ReadFromDatastore
-from apache_beam.io.gcp.datastore.v1.datastoreio import WriteToDatastore
+from apache_beam.io.gcp.datastore.v1new.datastoreio import ReadFromDatastore
+from apache_beam.io.gcp.datastore.v1new.datastoreio import WriteToDatastore
+from apache_beam.io.gcp.datastore.v1new.types import Entity
+from apache_beam.io.gcp.datastore.v1new.types import Key
+from apache_beam.io.gcp.datastore.v1new.types import Query
 from apache_beam.metrics import Metrics
 from apache_beam.metrics.metric import MetricsFilter
+from apache_beam.options.pipeline_options import GoogleCloudOptions
 from apache_beam.options.pipeline_options import PipelineOptions
-from apache_beam.options.pipeline_options import SetupOptions
-
-# Protect against environments where datastore library is not available.
-# pylint: disable=wrong-import-order, wrong-import-position
-try:
-  from google.cloud.proto.datastore.v1 import entity_pb2
-  from google.cloud.proto.datastore.v1 import query_pb2
-  from googledatastore import helper as datastore_helper
-  from googledatastore import PropertyFilter
-  from past.builtins import unicode
-except ImportError:
-  pass
-# pylint: enable=wrong-import-order, wrong-import-position
 
 
+@beam.typehints.with_input_types(Entity)
+@beam.typehints.with_output_types(Text)
 class WordExtractingDoFn(beam.DoFn):
   """Parse each line of input text into words."""
   def __init__(self):
@@ -102,20 +92,21 @@ class WordExtractingDoFn(beam.DoFn):
     self.word_lengths_dist = Metrics.distribution('main', 'word_len_dist')
 
   def process(self, element):
-    """Returns an iterator over words in contents of Cloud Datastore entity.
+    # type: (Entity) -> Optional[Iterable[Text]]
+
+    """Extract words from the 'content' property of Cloud Datastore entities.
+
     The element is a line of text.  If the line is blank, note that, too.
     Args:
-      element: the input element to be processed
+      element: the input entity to be processed
     Returns:
-      The processed element.
+      A list of words found.
     """
-    content_value = element.properties.get('content', None)
-    text_line = ''
-    if content_value:
-      text_line = content_value.string_value
-
+    text_line = element.properties.get('content', '')
     if not text_line:
       self.empty_line_counter.inc()
+      return None
+
     words = re.findall(r'[A-Za-z\']+', text_line)
     for w in words:
       self.word_length_counter.inc(len(w))
@@ -126,80 +117,66 @@ class WordExtractingDoFn(beam.DoFn):
 
 class EntityWrapper(object):
   """Create a Cloud Datastore entity from the given string."""
-  def __init__(self, namespace, kind, ancestor):
+  def __init__(self, project, namespace, kind, ancestor):
+    self._project = project
     self._namespace = namespace
     self._kind = kind
     self._ancestor = ancestor
 
   def make_entity(self, content):
-    entity = entity_pb2.Entity()
-    if self._namespace is not None:
-      entity.key.partition_id.namespace_id = self._namespace
-
-    # All entities created will have the same ancestor
-    datastore_helper.add_key_path(
-        entity.key, self._kind, self._ancestor, self._kind, str(uuid.uuid4()))
-
-    datastore_helper.add_properties(entity, {"content": unicode(content)})
+    ancestor_key = Key([self._kind, self._ancestor],
+                       self._namespace,
+                       self._project)
+    # Namespace and project are inherited from parent key.
+    key = Key([self._kind, str(uuid.uuid4())], parent=ancestor_key)
+    entity = Entity(key)
+    entity.set_properties({'content': content})
     return entity
 
 
-def write_to_datastore(user_options, pipeline_options):
+def write_to_datastore(project, user_options, pipeline_options):
   """Creates a pipeline that writes entities to Cloud Datastore."""
   with beam.Pipeline(options=pipeline_options) as p:
-
-    # pylint: disable=expression-not-assigned
-    (
+    _ = (
         p
         | 'read' >> ReadFromText(user_options.input)
         | 'create entity' >> beam.Map(
             EntityWrapper(
+                project,
                 user_options.namespace,
                 user_options.kind,
                 user_options.ancestor).make_entity)
-        | 'write to datastore' >> WriteToDatastore(user_options.dataset))
+        | 'write to datastore' >> WriteToDatastore(project))
 
 
-def make_ancestor_query(kind, namespace, ancestor):
+def make_ancestor_query(project, kind, namespace, ancestor):
   """Creates a Cloud Datastore ancestor query.
 
   The returned query will fetch all the entities that have the parent key name
   set to the given `ancestor`.
   """
-  ancestor_key = entity_pb2.Key()
-  datastore_helper.add_key_path(ancestor_key, kind, ancestor)
-  if namespace is not None:
-    ancestor_key.partition_id.namespace_id = namespace
-
-  query = query_pb2.Query()
-  query.kind.add().name = kind
+  ancestor_key = Key([kind, ancestor], project=project, namespace=namespace)
+  return Query(kind, project, namespace, ancestor_key)
 
-  datastore_helper.set_property_filter(
-      query.filter, '__key__', PropertyFilter.HAS_ANCESTOR, ancestor_key)
 
-  return query
-
-
-def read_from_datastore(user_options, pipeline_options):
+def read_from_datastore(project, user_options, pipeline_options):
   """Creates a pipeline that reads entities from Cloud Datastore."""
   p = beam.Pipeline(options=pipeline_options)
   # Create a query to read entities from datastore.
   query = make_ancestor_query(
-      user_options.kind, user_options.namespace, user_options.ancestor)
+      project, user_options.kind, user_options.namespace, user_options.ancestor)
 
   # Read entities from Cloud Datastore into a PCollection.
-  lines = p | 'read from datastore' >> ReadFromDatastore(
-      user_options.dataset, query, user_options.namespace)
+  lines = p | 'read from datastore' >> ReadFromDatastore(query)
 
   # Count the occurrences of each word.
   def count_ones(word_ones):
     (word, ones) = word_ones
-    return (word, sum(ones))
+    return word, sum(ones)
 
   counts = (
       lines
-      | 'split' >>
-      (beam.ParDo(WordExtractingDoFn()).with_output_types(unicode))
+      | 'split' >> beam.ParDo(WordExtractingDoFn())
       | 'pair_with_one' >> beam.Map(lambda x: (x, 1))
       | 'group' >> beam.GroupByKey()
       | 'count' >> beam.Map(count_ones))
@@ -231,10 +208,6 @@ def run(argv=None):
       dest='input',
       default='gs://dataflow-samples/shakespeare/kinglear.txt',
       help='Input file to process.')
-  parser.add_argument(
-      '--dataset',
-      dest='dataset',
-      help='Dataset ID to read from Cloud Datastore.')
   parser.add_argument(
       '--kind', dest='kind', required=True, help='Datastore Kind')
   parser.add_argument(
@@ -265,14 +238,18 @@ def run(argv=None):
   # We use the save_main_session option because one or more DoFn's in this
   # workflow rely on global context (e.g., a module imported at module level).
   pipeline_options = PipelineOptions(pipeline_args)
-  pipeline_options.view_as(SetupOptions).save_main_session = True
+  project = pipeline_options.view_as(GoogleCloudOptions).project
+  if project is None:
+    parser.print_usage()
+    print(sys.argv[0] + ': error: argument --project is required')
+    sys.exit(1)
 
   # Write to Datastore if `read_only` options is not specified.
   if not known_args.read_only:
-    write_to_datastore(known_args, pipeline_options)
+    write_to_datastore(project, known_args, pipeline_options)
 
   # Read entities from Datastore.
-  result = read_from_datastore(known_args, pipeline_options)
+  result = read_from_datastore(project, known_args, pipeline_options)
 
   empty_lines_filter = MetricsFilter().with_name('empty_lines')
   query_result = result.metrics().query(empty_lines_filter)
diff --git a/sdks/python/apache_beam/examples/cookbook/datastore_wordcount_it_test.py b/sdks/python/apache_beam/examples/cookbook/datastore_wordcount_it_test.py
index dbd132775ac..0cff4c40d48 100644
--- a/sdks/python/apache_beam/examples/cookbook/datastore_wordcount_it_test.py
+++ b/sdks/python/apache_beam/examples/cookbook/datastore_wordcount_it_test.py
@@ -22,24 +22,23 @@
 from __future__ import absolute_import
 
 import logging
-import os
-import sys
 import time
 import unittest
 
 from hamcrest.core.core.allof import all_of
 from nose.plugins.attrib import attr
 
-from apache_beam.examples.cookbook import datastore_wordcount
 from apache_beam.testing.pipeline_verifiers import FileChecksumMatcher
 from apache_beam.testing.pipeline_verifiers import PipelineStateMatcher
 from apache_beam.testing.test_pipeline import TestPipeline
 
+# Don't fail during pytest collection scan.
+try:
+  from apache_beam.examples.cookbook import datastore_wordcount
+except ImportError:
+  datastore_wordcount = None
+
 
-@unittest.skipIf(
-    sys.version_info[0] == 3 and os.environ.get('RUN_SKIPPED_PY3_TESTS') != '1',
-    'This test still needs to be fixed on Python 3'
-    'TODO: BEAM-4543')
 class DatastoreWordCountIT(unittest.TestCase):
 
   DATASTORE_WORDCOUNT_KIND = "DatastoreWordCount"
@@ -48,7 +47,6 @@ class DatastoreWordCountIT(unittest.TestCase):
   @attr('IT')
   def test_datastore_wordcount_it(self):
     test_pipeline = TestPipeline(is_integration_test=True)
-    dataset = test_pipeline.get_option("project")
     kind = self.DATASTORE_WORDCOUNT_KIND
     output = '/'.join([
         test_pipeline.get_option('output'),
@@ -64,9 +62,10 @@ class DatastoreWordCountIT(unittest.TestCase):
             output + '*-of-*', self.EXPECTED_CHECKSUM, sleep_secs)
     ]
     extra_opts = {
-        'dataset': dataset,
         'kind': kind,
         'output': output,
+        # Comment this out to regenerate input data on Datastore (delete
+        # existing data first using the bulk delete Dataflow template).
         'read_only': True,
         'on_success_matcher': all_of(*pipeline_verifiers)
     }
@@ -76,5 +75,5 @@ class DatastoreWordCountIT(unittest.TestCase):
 
 
 if __name__ == '__main__':
-  logging.getLogger().setLevel(logging.DEBUG)
+  logging.getLogger().setLevel(logging.INFO)
   unittest.main()
diff --git a/sdks/python/apache_beam/examples/snippets/snippets.py b/sdks/python/apache_beam/examples/snippets/snippets.py
index 979f30ad63d..fcf183d784b 100644
--- a/sdks/python/apache_beam/examples/snippets/snippets.py
+++ b/sdks/python/apache_beam/examples/snippets/snippets.py
@@ -40,8 +40,6 @@ from builtins import object
 from builtins import range
 from decimal import Decimal
 
-from past.builtins import unicode
-
 import apache_beam as beam
 from apache_beam.io import iobase
 from apache_beam.io.range_trackers import OffsetRangeTracker
@@ -1038,22 +1036,21 @@ def model_datastoreio():
   """Using a Read and Write transform to read/write to Cloud Datastore."""
 
   import uuid
-  from google.cloud.proto.datastore.v1 import entity_pb2
-  from google.cloud.proto.datastore.v1 import query_pb2
-  import googledatastore
   import apache_beam as beam
   from apache_beam.options.pipeline_options import PipelineOptions
-  from apache_beam.io.gcp.datastore.v1.datastoreio import ReadFromDatastore
-  from apache_beam.io.gcp.datastore.v1.datastoreio import WriteToDatastore
+  from apache_beam.io.gcp.datastore.v1new.datastoreio import ReadFromDatastore
+  from apache_beam.io.gcp.datastore.v1new.datastoreio import WriteToDatastore
+  from apache_beam.io.gcp.datastore.v1new.types import Entity
+  from apache_beam.io.gcp.datastore.v1new.types import Key
+  from apache_beam.io.gcp.datastore.v1new.types import Query
 
   project = 'my_project'
   kind = 'my_kind'
-  query = query_pb2.Query()
-  query.kind.add().name = kind
+  query = Query(kind, project)
 
   # [START model_datastoreio_read]
   p = beam.Pipeline(options=PipelineOptions())
-  entities = p | 'Read From Datastore' >> ReadFromDatastore(project, query)
+  entities = p | 'Read From Datastore' >> ReadFromDatastore(query)
   # [END model_datastoreio_read]
 
   # [START model_datastoreio_write]
@@ -1062,9 +1059,9 @@ def model_datastoreio():
       ['Mozart', 'Chopin', 'Beethoven', 'Vivaldi'])
 
   def to_entity(content):
-    entity = entity_pb2.Entity()
-    googledatastore.helper.add_key_path(entity.key, kind, str(uuid.uuid4()))
-    googledatastore.helper.add_properties(entity, {'content': unicode(content)})
+    key = Key([kind, str(uuid.uuid4())])
+    entity = Entity(key)
+    entity.set_properties({'content': content})
     return entity
 
   entities = musicians | 'To Entity' >> beam.Map(to_entity)
diff --git a/sdks/python/apache_beam/examples/snippets/snippets_test.py b/sdks/python/apache_beam/examples/snippets/snippets_test.py
index 4efe6589d5d..8daea5d0b07 100644
--- a/sdks/python/apache_beam/examples/snippets/snippets_test.py
+++ b/sdks/python/apache_beam/examples/snippets/snippets_test.py
@@ -75,9 +75,9 @@ except ImportError:
 # Protect against environments where datastore library is not available.
 # pylint: disable=wrong-import-order, wrong-import-position
 try:
-  from google.cloud.proto.datastore.v1 import datastore_pb2
+  from google.cloud.datastore import client as datastore_client
 except ImportError:
-  datastore_pb2 = None
+  datastore_client = None
 # pylint: enable=wrong-import-order, wrong-import-position
 
 # Protect against environments where the PubSub library is not available.
@@ -645,11 +645,7 @@ class SnippetsTest(unittest.TestCase):
                                      ['aa', 'bb', 'cc'])
 
   @unittest.skipIf(
-      sys.version_info[0] == 3 and
-      os.environ.get('RUN_SKIPPED_PY3_TESTS') != '1',
-      'This test still needs to be fixed on Python 3'
-      'TODO: BEAM-4543')
-  @unittest.skipIf(datastore_pb2 is None, 'GCP dependencies are not installed')
+      datastore_client is None, 'GCP dependencies are not installed')
   def test_model_datastoreio(self):
     # We cannot test DatastoreIO functionality in unit tests, therefore we limit
     # ourselves to making sure the pipeline containing Datastore read and write
diff --git a/sdks/python/apache_beam/io/gcp/datastore/v1/__init__.py b/sdks/python/apache_beam/io/gcp/datastore/v1/__init__.py
deleted file mode 100644
index f4f43cbb123..00000000000
--- a/sdks/python/apache_beam/io/gcp/datastore/v1/__init__.py
+++ /dev/null
@@ -1,17 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#    http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-from __future__ import absolute_import
diff --git a/sdks/python/apache_beam/io/gcp/datastore/v1/datastoreio.py b/sdks/python/apache_beam/io/gcp/datastore/v1/datastoreio.py
deleted file mode 100644
index a2c983e3fb6..00000000000
--- a/sdks/python/apache_beam/io/gcp/datastore/v1/datastoreio.py
+++ /dev/null
@@ -1,516 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#    http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-
-"""
-A connector for reading from and writing to Google Cloud Datastore.
-
-Please avoid using this module for new pipelines since
-``apache_beam.io.gcp.datastore.v1new.datastoreio`` will replace it in the
-next Beam major release.
-"""
-# pytype: skip-file
-
-from __future__ import absolute_import
-from __future__ import division
-
-import logging
-import sys
-import time
-import warnings
-from builtins import round
-
-from apache_beam.io.gcp.datastore.v1 import helper
-from apache_beam.io.gcp.datastore.v1 import query_splitter
-from apache_beam.io.gcp.datastore.v1 import util
-from apache_beam.io.gcp.datastore.v1.adaptive_throttler import AdaptiveThrottler
-from apache_beam.metrics.metric import Metrics
-from apache_beam.transforms import Create
-from apache_beam.transforms import DoFn
-from apache_beam.transforms import FlatMap
-from apache_beam.transforms import GroupByKey
-from apache_beam.transforms import Map
-from apache_beam.transforms import ParDo
-from apache_beam.transforms import PTransform
-from apache_beam.transforms.util import Values
-
-_LOGGER = logging.getLogger(__name__)
-
-# Protect against environments where datastore library is not available.
-# pylint: disable=wrong-import-order, wrong-import-position
-try:
-  from google.cloud.proto.datastore.v1 import datastore_pb2
-  from googledatastore import helper as datastore_helper
-  _LOGGER.warning(
-      'Using deprecated Datastore client.\n'
-      'This client will be removed in Beam 3.0 (next Beam major release).\n'
-      'Please migrate to apache_beam.io.gcp.datastore.v1new.datastoreio.')
-except ImportError:
-  if sys.version_info[0] == 3:
-    warnings.warn(
-        'Datastore IO will support Python 3 after replacing '
-        'googledatastore by google-cloud-datastore, '
-        'see: BEAM-4543.')
-# pylint: enable=wrong-import-order, wrong-import-position
-
-__all__ = ['ReadFromDatastore', 'WriteToDatastore', 'DeleteFromDatastore']
-
-
-class ReadFromDatastore(PTransform):
-  """A ``PTransform`` for reading from Google Cloud Datastore.
-
-  To read a ``PCollection[Entity]`` from a Cloud Datastore ``Query``, use
-  ``ReadFromDatastore`` transform by providing a `project` id and a `query` to
-  read from. You can optionally provide a `namespace` and/or specify how many
-  splits you want for the query through `num_splits` option.
-
-  Note: Normally, a runner will read from Cloud Datastore in parallel across
-  many workers. However, when the `query` is configured with a `limit` or if the
-  query contains inequality filters like `GREATER_THAN, LESS_THAN` etc., then
-  all the returned results will be read by a single worker in order to ensure
-  correct data. Since data is read from a single worker, this could have
-  significant impact on the performance of the job.
-
-  The semantics for the query splitting is defined below:
-    1. If `num_splits` is equal to 0, then the number of splits will be chosen
-    dynamically at runtime based on the query data size.
-
-    2. Any value of `num_splits` greater than
-    `ReadFromDatastore._NUM_QUERY_SPLITS_MAX` will be capped at that value.
-
-    3. If the `query` has a user limit set, or contains inequality filters, then
-    `num_splits` will be ignored and no split will be performed.
-
-    4. Under certain cases Cloud Datastore is unable to split query to the
-    requested number of splits. In such cases we just use whatever the Cloud
-    Datastore returns.
-
-  See https://developers.google.com/datastore/ for more details on Google Cloud
-  Datastore.
-  """
-
-  # An upper bound on the number of splits for a query.
-  _NUM_QUERY_SPLITS_MAX = 50000
-  # A lower bound on the number of splits for a query. This is to ensure that
-  # we parellelize the query even when Datastore statistics are not available.
-  _NUM_QUERY_SPLITS_MIN = 12
-  # Default bundle size of 64MB.
-  _DEFAULT_BUNDLE_SIZE_BYTES = 64 * 1024 * 1024
-
-  def __init__(self, project, query, namespace=None, num_splits=0):
-    """Initialize the `ReadFromDatastore` transform.
-
-    Args:
-      project: The ID of the project to read from.
-      query: Cloud Datastore query to be read from.
-      namespace: An optional namespace.
-      num_splits: Number of splits for the query.
-    """
-
-    # Import here to avoid adding the dependency for local running scenarios.
-    try:
-      # pylint: disable=wrong-import-order, wrong-import-position
-      from apitools.base import py  # pylint: disable=unused-import
-    except ImportError:
-      raise ImportError(
-          'Google Cloud IO not available, '
-          'please install apache_beam[gcp]')
-
-    _LOGGER.warning('datastoreio read transform is experimental.')
-    super(ReadFromDatastore, self).__init__()
-
-    if not project:
-      raise ValueError("Project cannot be empty")
-    if not query:
-      raise ValueError("Query cannot be empty")
-    if num_splits < 0:
-      raise ValueError("num_splits must be greater than or equal 0")
-
-    self._project = project
-    # using _namespace conflicts with DisplayData._namespace
-    self._datastore_namespace = namespace
-    self._query = query
-    self._num_splits = num_splits
-
-  def expand(self, pcoll):
-    # This is a composite transform involves the following:
-    #   1. Create a singleton of the user provided `query` and apply a ``ParDo``
-    #   that splits the query into `num_splits` and assign each split query a
-    #   unique `int` as the key. The resulting output is of the type
-    #   ``PCollection[(int, Query)]``.
-    #
-    #   If the value of `num_splits` is less than or equal to 0, then the
-    #   number of splits will be computed dynamically based on the size of the
-    #   data for the `query`.
-    #
-    #   2. The resulting ``PCollection`` is sharded using a ``GroupByKey``
-    #   operation. The queries are extracted from the (int, Iterable[Query]) and
-    #   flattened to output a ``PCollection[Query]``.
-    #
-    #   3. In the third step, a ``ParDo`` reads entities for each query and
-    #   outputs a ``PCollection[Entity]``.
-
-    queries = (
-        pcoll.pipeline
-        | 'UserQuery' >> Create([self._query])
-        | 'SplitQuery' >> ParDo(
-            ReadFromDatastore.SplitQueryFn(
-                self._project,
-                self._query,
-                self._datastore_namespace,
-                self._num_splits)))
-
-    sharded_queries = (
-        queries
-        | GroupByKey()
-        | Values()
-        | 'Flatten' >> FlatMap(lambda x: x))
-
-    entities = sharded_queries | 'Read' >> ParDo(
-        ReadFromDatastore.ReadFn(self._project, self._datastore_namespace))
-    return entities
-
-  def display_data(self):
-    disp_data = {
-        'project': self._project,
-        'query': str(self._query),
-        'num_splits': self._num_splits
-    }
-
-    if self._datastore_namespace is not None:
-      disp_data['namespace'] = self._datastore_namespace
-
-    return disp_data
-
-  class SplitQueryFn(DoFn):
-    """A `DoFn` that splits a given query into multiple sub-queries."""
-    def __init__(self, project, query, namespace, num_splits):
-      super(ReadFromDatastore.SplitQueryFn, self).__init__()
-      self._datastore = None
-      self._project = project
-      self._datastore_namespace = namespace
-      self._query = query
-      self._num_splits = num_splits
-
-    def start_bundle(self):
-      self._datastore = helper.get_datastore(self._project)
-
-    def process(self, query, *args, **kwargs):
-      # distinct key to be used to group query splits.
-      key = 1
-
-      # If query has a user set limit, then the query cannot be split.
-      if query.HasField('limit'):
-        return [(key, query)]
-
-      # Compute the estimated numSplits if not specified by the user.
-      if self._num_splits == 0:
-        estimated_num_splits = ReadFromDatastore.get_estimated_num_splits(
-            self._project,
-            self._datastore_namespace,
-            self._query,
-            self._datastore)
-      else:
-        estimated_num_splits = self._num_splits
-
-      _LOGGER.info("Splitting the query into %d splits", estimated_num_splits)
-      try:
-        query_splits = query_splitter.get_splits(
-            self._datastore,
-            query,
-            estimated_num_splits,
-            helper.make_partition(self._project, self._datastore_namespace))
-      except Exception:
-        _LOGGER.warning(
-            "Unable to parallelize the given query: %s", query, exc_info=True)
-        query_splits = [query]
-
-      sharded_query_splits = []
-      for split_query in query_splits:
-        sharded_query_splits.append((key, split_query))
-        key += 1
-
-      return sharded_query_splits
-
-    def display_data(self):
-      disp_data = {
-          'project': self._project,
-          'query': str(self._query),
-          'num_splits': self._num_splits
-      }
-
-      if self._datastore_namespace is not None:
-        disp_data['namespace'] = self._datastore_namespace
-
-      return disp_data
-
-  class ReadFn(DoFn):
-    """A DoFn that reads entities from Cloud Datastore, for a given query."""
-    def __init__(self, project, namespace=None):
-      super(ReadFromDatastore.ReadFn, self).__init__()
-      self._project = project
-      self._datastore_namespace = namespace
-      self._datastore = None
-
-    def start_bundle(self):
-      self._datastore = helper.get_datastore(self._project)
-
-    def process(self, query, *args, **kwargs):
-      # Returns an iterator of entities that reads in batches.
-      entities = helper.fetch_entities(
-          self._project, self._datastore_namespace, query, self._datastore)
-      return entities
-
-    def display_data(self):
-      disp_data = {'project': self._project}
-
-      if self._datastore_namespace is not None:
-        disp_data['namespace'] = self._datastore_namespace
-
-      return disp_data
-
-  @staticmethod
-  def query_latest_statistics_timestamp(project, namespace, datastore):
-    """Fetches the latest timestamp of statistics from Cloud Datastore.
-
-    Cloud Datastore system tables with statistics are periodically updated.
-    This method fethes the latest timestamp (in microseconds) of statistics
-    update using the `__Stat_Total__` table.
-    """
-    query = helper.make_latest_timestamp_query(namespace)
-    req = helper.make_request(project, namespace, query)
-    resp = datastore.run_query(req)
-    if not resp.batch.entity_results:
-      raise RuntimeError("Datastore total statistics unavailable.")
-
-    entity = resp.batch.entity_results[0].entity
-    return datastore_helper.micros_from_timestamp(
-        entity.properties['timestamp'].timestamp_value)
-
-  @staticmethod
-  def get_estimated_size_bytes(project, namespace, query, datastore):
-    """Get the estimated size of the data returned by the given query.
-
-    Cloud Datastore provides no way to get a good estimate of how large the
-    result of a query is going to be. Hence we use the __Stat_Kind__ system
-    table to get size of the entire kind as an approximate estimate, assuming
-    exactly 1 kind is specified in the query.
-    See https://cloud.google.com/datastore/docs/concepts/stats.
-    """
-    kind = query.kind[0].name
-    latest_timestamp = ReadFromDatastore.query_latest_statistics_timestamp(
-        project, namespace, datastore)
-    _LOGGER.info(
-        'Latest stats timestamp for kind %s is %s', kind, latest_timestamp)
-
-    kind_stats_query = (
-        helper.make_kind_stats_query(namespace, kind, latest_timestamp))
-
-    req = helper.make_request(project, namespace, kind_stats_query)
-    resp = datastore.run_query(req)
-    if not resp.batch.entity_results:
-      raise RuntimeError("Datastore statistics for kind %s unavailable" % kind)
-
-    entity = resp.batch.entity_results[0].entity
-    return datastore_helper.get_value(entity.properties['entity_bytes'])
-
-  @staticmethod
-  def get_estimated_num_splits(project, namespace, query, datastore):
-    """Computes the number of splits to be performed on the given query."""
-    try:
-      estimated_size_bytes = ReadFromDatastore.get_estimated_size_bytes(
-          project, namespace, query, datastore)
-      _LOGGER.info('Estimated size bytes for query: %s', estimated_size_bytes)
-      num_splits = int(
-          min(
-              ReadFromDatastore._NUM_QUERY_SPLITS_MAX,
-              round((
-                  float(estimated_size_bytes) /
-                  ReadFromDatastore._DEFAULT_BUNDLE_SIZE_BYTES))))
-
-    except Exception as e:
-      _LOGGER.warning('Failed to fetch estimated size bytes: %s', e)
-      # Fallback in case estimated size is unavailable.
-      num_splits = ReadFromDatastore._NUM_QUERY_SPLITS_MIN
-
-    return max(num_splits, ReadFromDatastore._NUM_QUERY_SPLITS_MIN)
-
-
-class _Mutate(PTransform):
-  """A ``PTransform`` that writes mutations to Cloud Datastore.
-
-  Only idempotent Datastore mutation operations (upsert and delete) are
-  supported, as the commits are retried when failures occur.
-  """
-  def __init__(self, project, mutation_fn):
-    """Initializes a Mutate transform.
-
-     Args:
-       project: The Project ID
-       mutation_fn: A function that converts `entities` or `keys` to
-         `mutations`.
-     """
-    self._project = project
-    self._mutation_fn = mutation_fn
-    _LOGGER.warning('datastoreio write transform is experimental.')
-
-  def expand(self, pcoll):
-    return (
-        pcoll
-        | 'Convert to Mutation' >> Map(self._mutation_fn)
-        | 'Write Mutation to Datastore' >> ParDo(
-            _Mutate.DatastoreWriteFn(self._project)))
-
-  def display_data(self):
-    return {
-        'project': self._project,
-        'mutation_fn': self._mutation_fn.__class__.__name__
-    }
-
-  class DatastoreWriteFn(DoFn):
-    """A ``DoFn`` that write mutations to Datastore.
-
-    Mutations are written in batches, where the maximum batch size is
-    `util.WRITE_BATCH_SIZE`.
-
-    Commits are non-transactional. If a commit fails because of a conflict over
-    an entity group, the commit will be retried. This means that the mutation
-    should be idempotent (`upsert` and `delete` mutations) to prevent duplicate
-    data or errors.
-    """
-    def __init__(self, project, fixed_batch_size=None):
-      """
-      Args:
-        project: str, the cloud project id.
-        fixed_batch_size: int, for testing only, this forces all batches of
-           writes to be a fixed size, for easier unittesting.
-      """
-      self._project = project
-      self._datastore = None
-      self._fixed_batch_size = fixed_batch_size
-      self._rpc_successes = Metrics.counter(
-          _Mutate.DatastoreWriteFn, "datastoreRpcSuccesses")
-      self._rpc_errors = Metrics.counter(
-          _Mutate.DatastoreWriteFn, "datastoreRpcErrors")
-      self._throttled_secs = Metrics.counter(
-          _Mutate.DatastoreWriteFn, "cumulativeThrottlingSeconds")
-      self._throttler = AdaptiveThrottler(
-          window_ms=120000, bucket_ms=1000, overload_ratio=1.25)
-
-    def _update_rpc_stats(self, successes=0, errors=0, throttled_secs=0):
-      self._rpc_successes.inc(successes)
-      self._rpc_errors.inc(errors)
-      self._throttled_secs.inc(throttled_secs)
-
-    def start_bundle(self):
-      self._mutations = []
-      self._mutations_size = 0
-      self._datastore = helper.get_datastore(self._project)
-      if self._fixed_batch_size:
-        self._target_batch_size = self._fixed_batch_size
-      else:
-        self._batch_sizer = util.DynamicBatchSizer()
-        self._target_batch_size = self._batch_sizer.get_batch_size(
-            time.time() * 1000)
-
-    def process(self, element):
-      size = element.ByteSize()
-      if (self._mutations and
-          size + self._mutations_size > util.WRITE_BATCH_MAX_BYTES_SIZE):
-        self._flush_batch()
-      self._mutations.append(element)
-      self._mutations_size += size
-      if len(self._mutations) >= self._target_batch_size:
-        self._flush_batch()
-
-    def finish_bundle(self):
-      if self._mutations:
-        self._flush_batch()
-
-    def _flush_batch(self):
-      # Flush the current batch of mutations to Cloud Datastore.
-      _, latency_ms = helper.write_mutations(
-          self._datastore, self._project, self._mutations,
-          self._throttler, self._update_rpc_stats,
-          throttle_delay=util.WRITE_BATCH_TARGET_LATENCY_MS//1000)
-      _LOGGER.debug(
-          "Successfully wrote %d mutations in %dms.",
-          len(self._mutations),
-          latency_ms)
-
-      if not self._fixed_batch_size:
-        now = time.time() * 1000
-        self._batch_sizer.report_latency(now, latency_ms, len(self._mutations))
-        self._target_batch_size = self._batch_sizer.get_batch_size(now)
-
-      self._mutations = []
-      self._mutations_size = 0
-
-
-class WriteToDatastore(_Mutate):
-  """A ``PTransform`` to write a ``PCollection[Entity]`` to Cloud Datastore."""
-  def __init__(self, project):
-    """Initialize the `WriteToDatastore` transform.
-
-    Args:
-      project: The ID of the project to write to.
-    """
-
-    # Import here to avoid adding the dependency for local running scenarios.
-    try:
-      # pylint: disable=wrong-import-order, wrong-import-position
-      from apitools.base import py  # pylint: disable=unused-import
-    except ImportError:
-      raise ImportError(
-          'Google Cloud IO not available, '
-          'please install apache_beam[gcp]')
-
-    super(WriteToDatastore,
-          self).__init__(project, WriteToDatastore.to_upsert_mutation)
-
-  @staticmethod
-  def to_upsert_mutation(entity):
-    if not helper.is_key_valid(entity.key):
-      raise ValueError(
-          'Entities to be written to the Cloud Datastore must '
-          'have complete keys:\n%s' % entity)
-    mutation = datastore_pb2.Mutation()
-    mutation.upsert.CopyFrom(entity)
-    return mutation
-
-
-class DeleteFromDatastore(_Mutate):
-  """A ``PTransform`` to delete a ``PCollection[Key]`` from Cloud Datastore."""
-  def __init__(self, project):
-    """Initialize the `DeleteFromDatastore` transform.
-
-    Args:
-      project: The ID of the project from which the entities will be deleted.
-    """
-
-    super(DeleteFromDatastore,
-          self).__init__(project, DeleteFromDatastore.to_delete_mutation)
-
-  @staticmethod
-  def to_delete_mutation(key):
-    if not helper.is_key_valid(key):
-      raise ValueError(
-          'Keys to be deleted from the Cloud Datastore must be '
-          'complete:\n%s',
-          key)
-    mutation = datastore_pb2.Mutation()
-    mutation.delete.CopyFrom(key)
-    return mutation
diff --git a/sdks/python/apache_beam/io/gcp/datastore/v1/datastoreio_test.py b/sdks/python/apache_beam/io/gcp/datastore/v1/datastoreio_test.py
deleted file mode 100644
index 839893d1970..00000000000
--- a/sdks/python/apache_beam/io/gcp/datastore/v1/datastoreio_test.py
+++ /dev/null
@@ -1,313 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#    http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-
-# pytype: skip-file
-
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-
-import sys
-import unittest
-from builtins import map
-from builtins import range
-from builtins import zip
-
-from mock import MagicMock
-from mock import call
-from mock import patch
-
-# Protect against environments where datastore library is not available.
-# pylint: disable=wrong-import-order, wrong-import-position, ungrouped-imports
-try:
-  from google.cloud.proto.datastore.v1 import datastore_pb2
-  from apache_beam.io.gcp.datastore.v1 import fake_datastore
-  from apache_beam.io.gcp.datastore.v1 import helper
-  from google.cloud.proto.datastore.v1 import query_pb2
-  from apache_beam.io.gcp.datastore.v1 import query_splitter
-  from apache_beam.io.gcp.datastore.v1 import util
-  from apache_beam.io.gcp.datastore.v1.datastoreio import ReadFromDatastore
-  from apache_beam.io.gcp.datastore.v1.datastoreio import WriteToDatastore
-  from apache_beam.io.gcp.datastore.v1.datastoreio import _Mutate
-  from google.protobuf import timestamp_pb2
-  from googledatastore import helper as datastore_helper
-except (ImportError, TypeError):
-  datastore_pb2 = None
-# pylint: enable=wrong-import-order, wrong-import-position, ungrouped-imports
-
-
-class DatastoreioTest(unittest.TestCase):
-  _PROJECT = 'project'
-  _KIND = 'kind'
-  _NAMESPACE = 'namespace'
-
-  @unittest.skipIf(
-      sys.version_info[0] == 3,
-      'v1/datastoreio does not support Python 3 TODO: BEAM-4543')
-  @unittest.skipIf(datastore_pb2 is None, 'GCP dependencies are not installed')
-  def setUp(self):
-    self._mock_datastore = MagicMock()
-    self._query = query_pb2.Query()
-    self._query.kind.add().name = self._KIND
-    self._WRITE_BATCH_INITIAL_SIZE = util.WRITE_BATCH_INITIAL_SIZE
-
-  def get_timestamp(self):
-    return timestamp_pb2.Timestamp(seconds=1234)
-
-  def test_get_estimated_size_bytes_without_namespace(self):
-    entity_bytes = 100
-    timestamp = self.get_timestamp()
-    self.check_estimated_size_bytes(entity_bytes, timestamp)
-
-  def test_get_estimated_size_bytes_with_namespace(self):
-    entity_bytes = 100
-    timestamp = self.get_timestamp()
-    self.check_estimated_size_bytes(entity_bytes, timestamp, self._NAMESPACE)
-
-  def test_SplitQueryFn_with_num_splits(self):
-    with patch.object(helper,
-                      'get_datastore',
-                      return_value=self._mock_datastore):
-      num_splits = 23
-
-      def fake_get_splits(datastore, query, num_splits, partition=None):
-        return self.split_query(query, num_splits)
-
-      with patch.object(query_splitter,
-                        'get_splits',
-                        side_effect=fake_get_splits):
-
-        split_query_fn = ReadFromDatastore.SplitQueryFn(
-            self._PROJECT, self._query, None, num_splits)
-        split_query_fn.start_bundle()
-        returned_split_queries = []
-        for split_query in split_query_fn.process(self._query):
-          returned_split_queries.append(split_query)
-
-        self.assertEqual(len(returned_split_queries), num_splits)
-        self.assertEqual(0, len(self._mock_datastore.run_query.call_args_list))
-        self.verify_unique_keys(returned_split_queries)
-
-  def test_SplitQueryFn_without_num_splits(self):
-    with patch.object(helper,
-                      'get_datastore',
-                      return_value=self._mock_datastore):
-      # Force SplitQueryFn to compute the number of query splits
-      num_splits = 0
-      expected_num_splits = 23
-      entity_bytes = (
-          expected_num_splits * ReadFromDatastore._DEFAULT_BUNDLE_SIZE_BYTES)
-      with patch.object(ReadFromDatastore,
-                        'get_estimated_size_bytes',
-                        return_value=entity_bytes):
-
-        def fake_get_splits(datastore, query, num_splits, partition=None):
-          return self.split_query(query, num_splits)
-
-        with patch.object(query_splitter,
-                          'get_splits',
-                          side_effect=fake_get_splits):
-          split_query_fn = ReadFromDatastore.SplitQueryFn(
-              self._PROJECT, self._query, None, num_splits)
-          split_query_fn.start_bundle()
-          returned_split_queries = []
-          for split_query in split_query_fn.process(self._query):
-            returned_split_queries.append(split_query)
-
-          self.assertEqual(len(returned_split_queries), expected_num_splits)
-          self.assertEqual(
-              0, len(self._mock_datastore.run_query.call_args_list))
-          self.verify_unique_keys(returned_split_queries)
-
-  def test_SplitQueryFn_with_query_limit(self):
-    """A test that verifies no split is performed when the query has a limit."""
-    with patch.object(helper,
-                      'get_datastore',
-                      return_value=self._mock_datastore):
-      self._query.limit.value = 3
-      split_query_fn = ReadFromDatastore.SplitQueryFn(
-          self._PROJECT, self._query, None, 4)
-      split_query_fn.start_bundle()
-      returned_split_queries = []
-      for split_query in split_query_fn.process(self._query):
-        returned_split_queries.append(split_query)
-
-      self.assertEqual(1, len(returned_split_queries))
-      self.assertEqual(0, len(self._mock_datastore.method_calls))
-
-  def test_SplitQueryFn_with_exception(self):
-    """A test that verifies that no split is performed when failures occur."""
-    with patch.object(helper,
-                      'get_datastore',
-                      return_value=self._mock_datastore):
-      # Force SplitQueryFn to compute the number of query splits
-      num_splits = 0
-      expected_num_splits = 1
-      entity_bytes = (
-          expected_num_splits * ReadFromDatastore._DEFAULT_BUNDLE_SIZE_BYTES)
-      with patch.object(ReadFromDatastore,
-                        'get_estimated_size_bytes',
-                        return_value=entity_bytes):
-
-        with patch.object(query_splitter,
-                          'get_splits',
-                          side_effect=ValueError("Testing query split error")):
-          split_query_fn = ReadFromDatastore.SplitQueryFn(
-              self._PROJECT, self._query, None, num_splits)
-          split_query_fn.start_bundle()
-          returned_split_queries = []
-          for split_query in split_query_fn.process(self._query):
-            returned_split_queries.append(split_query)
-
-          self.assertEqual(len(returned_split_queries), expected_num_splits)
-          self.assertEqual(returned_split_queries[0][1], self._query)
-          self.assertEqual(
-              0, len(self._mock_datastore.run_query.call_args_list))
-          self.verify_unique_keys(returned_split_queries)
-
-  def test_DatastoreWriteFn_with_empty_batch(self):
-    self.check_DatastoreWriteFn(0)
-
-  def test_DatastoreWriteFn_with_one_batch(self):
-    num_entities_to_write = self._WRITE_BATCH_INITIAL_SIZE * 1 - 50
-    self.check_DatastoreWriteFn(num_entities_to_write)
-
-  def test_DatastoreWriteFn_with_multiple_batches(self):
-    num_entities_to_write = self._WRITE_BATCH_INITIAL_SIZE * 3 + 50
-    self.check_DatastoreWriteFn(num_entities_to_write)
-
-  def test_DatastoreWriteFn_with_batch_size_exact_multiple(self):
-    num_entities_to_write = self._WRITE_BATCH_INITIAL_SIZE * 2
-    self.check_DatastoreWriteFn(num_entities_to_write)
-
-  def test_DatastoreWriteFn_with_dynamic_batch_sizes(self):
-    num_entities_to_write = self._WRITE_BATCH_INITIAL_SIZE * 3 + 50
-    self.check_DatastoreWriteFn(
-        num_entities_to_write, use_fixed_batch_size=False)
-
-  def check_DatastoreWriteFn(self, num_entities, use_fixed_batch_size=True):
-    """A helper function to test DatastoreWriteFn."""
-
-    with patch.object(helper,
-                      'get_datastore',
-                      return_value=self._mock_datastore):
-      entities = [
-          e.entity for e in fake_datastore.create_entities(num_entities)
-      ]
-
-      expected_mutations = list(
-          map(WriteToDatastore.to_upsert_mutation, entities))
-      actual_mutations = []
-
-      self._mock_datastore.commit.side_effect = (
-          fake_datastore.create_commit(actual_mutations))
-
-      fixed_batch_size = None
-      if use_fixed_batch_size:
-        fixed_batch_size = self._WRITE_BATCH_INITIAL_SIZE
-      datastore_write_fn = _Mutate.DatastoreWriteFn(
-          self._PROJECT, fixed_batch_size=fixed_batch_size)
-
-      datastore_write_fn.start_bundle()
-      for mutation in expected_mutations:
-        datastore_write_fn.process(mutation)
-      datastore_write_fn.finish_bundle()
-
-      self.assertEqual(actual_mutations, expected_mutations)
-      if use_fixed_batch_size:
-        self.assertEqual(
-            (num_entities - 1) // self._WRITE_BATCH_INITIAL_SIZE + 1,
-            self._mock_datastore.commit.call_count)
-      else:
-        self._mock_datastore.commit.assert_called()
-
-  def test_DatastoreWriteLargeEntities(self):
-    """100*100kB entities gets split over two Commit RPCs."""
-    with patch.object(helper,
-                      'get_datastore',
-                      return_value=self._mock_datastore):
-      entities = [e.entity for e in fake_datastore.create_entities(100)]
-
-      datastore_write_fn = _Mutate.DatastoreWriteFn(
-          self._PROJECT, fixed_batch_size=self._WRITE_BATCH_INITIAL_SIZE)
-      datastore_write_fn.start_bundle()
-      for entity in entities:
-        datastore_helper.add_properties(
-            entity, {'large': u'A' * 100000}, exclude_from_indexes=True)
-        datastore_write_fn.process(WriteToDatastore.to_upsert_mutation(entity))
-      datastore_write_fn.finish_bundle()
-
-      self.assertEqual(2, self._mock_datastore.commit.call_count)
-
-  def verify_unique_keys(self, queries):
-    """A helper function that verifies if all the queries have unique keys."""
-    keys, _ = zip(*queries)
-    keys = set(keys)
-    self.assertEqual(len(keys), len(queries))
-
-  def check_estimated_size_bytes(self, entity_bytes, timestamp, namespace=None):
-    """A helper method to test get_estimated_size_bytes"""
-
-    timestamp_req = helper.make_request(
-        self._PROJECT, namespace, helper.make_latest_timestamp_query(namespace))
-    timestamp_resp = self.make_stats_response(
-        {'timestamp': datastore_helper.from_timestamp(timestamp)})
-    kind_stat_req = helper.make_request(
-        self._PROJECT,
-        namespace,
-        helper.make_kind_stats_query(
-            namespace,
-            self._query.kind[0].name,
-            datastore_helper.micros_from_timestamp(timestamp)))
-    kind_stat_resp = self.make_stats_response({'entity_bytes': entity_bytes})
-
-    def fake_run_query(req):
-      if req == timestamp_req:
-        return timestamp_resp
-      elif req == kind_stat_req:
-        return kind_stat_resp
-      else:
-        print(kind_stat_req)
-        raise ValueError("Unknown req: %s" % req)
-
-    self._mock_datastore.run_query.side_effect = fake_run_query
-    self.assertEqual(
-        entity_bytes,
-        ReadFromDatastore.get_estimated_size_bytes(
-            self._PROJECT, namespace, self._query, self._mock_datastore))
-    self.assertEqual(
-        self._mock_datastore.run_query.call_args_list,
-        [call(timestamp_req), call(kind_stat_req)])
-
-  def make_stats_response(self, property_map):
-    resp = datastore_pb2.RunQueryResponse()
-    entity_result = resp.batch.entity_results.add()
-    datastore_helper.add_properties(entity_result.entity, property_map)
-    return resp
-
-  def split_query(self, query, num_splits):
-    """Generate dummy query splits."""
-    split_queries = []
-    for _ in range(0, num_splits):
-      q = query_pb2.Query()
-      q.CopyFrom(query)
-      split_queries.append(q)
-    return split_queries
-
-
-if __name__ == '__main__':
-  unittest.main()
diff --git a/sdks/python/apache_beam/io/gcp/datastore/v1/fake_datastore.py b/sdks/python/apache_beam/io/gcp/datastore/v1/fake_datastore.py
deleted file mode 100644
index 4dd915da0b0..00000000000
--- a/sdks/python/apache_beam/io/gcp/datastore/v1/fake_datastore.py
+++ /dev/null
@@ -1,111 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#    http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-
-"""Fake datastore used for unit testing.
-
-For internal use only; no backwards-compatibility guarantees.
-"""
-
-# pytype: skip-file
-
-from __future__ import absolute_import
-
-import uuid
-from builtins import range
-
-# Protect against environments where datastore library is not available.
-# pylint: disable=wrong-import-order, wrong-import-position
-try:
-  from google.cloud.proto.datastore.v1 import datastore_pb2
-  from google.cloud.proto.datastore.v1 import query_pb2
-except ImportError:
-  pass
-# pylint: enable=wrong-import-order, wrong-import-position
-
-
-def create_run_query(entities, batch_size):
-  """A fake datastore run_query method that returns entities in batches.
-
-  Note: the outer method is needed to make the `entities` and `batch_size`
-  available in the scope of fake_run_query method.
-
-  Args:
-    entities: list of entities supposed to be contained in the datastore.
-    batch_size: the number of entities that run_query method returns in one
-                request.
-  """
-  def run_query(req):
-    start = int(req.query.start_cursor) if req.query.start_cursor else 0
-    # if query limit is less than batch_size, then only return that much.
-    count = min(batch_size, req.query.limit.value)
-    # cannot go more than the number of entities contained in datastore.
-    end = min(len(entities), start + count)
-    finish = False
-    # Finish reading when there are no more entities to return,
-    # or request query limit has been satisfied.
-    if end == len(entities) or count == req.query.limit.value:
-      finish = True
-    return create_response(entities[start:end], str(end), finish)
-
-  return run_query
-
-
-def create_commit(mutations):
-  """A fake Datastore commit method that writes the mutations to a list.
-
-  Args:
-    mutations: A list to write mutations to.
-
-  Returns:
-    A fake Datastore commit method
-  """
-  def commit(req):
-    for mutation in req.mutations:
-      mutations.append(mutation)
-
-  return commit
-
-
-def create_response(entities, end_cursor, finish):
-  """Creates a query response for a given batch of scatter entities."""
-  resp = datastore_pb2.RunQueryResponse()
-  if finish:
-    resp.batch.more_results = query_pb2.QueryResultBatch.NO_MORE_RESULTS
-  else:
-    resp.batch.more_results = query_pb2.QueryResultBatch.NOT_FINISHED
-
-  resp.batch.end_cursor = end_cursor
-  for entity_result in entities:
-    resp.batch.entity_results.add().CopyFrom(entity_result)
-
-  return resp
-
-
-def create_entities(count, id_or_name=False):
-  """Creates a list of entities with random keys."""
-  entities = []
-
-  for _ in range(count):
-    entity_result = query_pb2.EntityResult()
-    if id_or_name:
-      entity_result.entity.key.path.add().id = (
-          uuid.uuid4().int & ((1 << 63) - 1))
-    else:
-      entity_result.entity.key.path.add().name = str(uuid.uuid4())
-    entities.append(entity_result)
-
-  return entities
diff --git a/sdks/python/apache_beam/io/gcp/datastore/v1/helper.py b/sdks/python/apache_beam/io/gcp/datastore/v1/helper.py
deleted file mode 100644
index 163cc7614b3..00000000000
--- a/sdks/python/apache_beam/io/gcp/datastore/v1/helper.py
+++ /dev/null
@@ -1,335 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#    http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-
-"""Cloud Datastore helper functions.
-
-For internal use only; no backwards-compatibility guarantees.
-"""
-
-# pytype: skip-file
-
-from __future__ import absolute_import
-
-import errno
-import logging
-import sys
-import time
-from builtins import object
-from socket import error as SocketError
-
-from future.builtins import next
-from past.builtins import unicode
-
-# pylint: disable=ungrouped-imports
-from apache_beam.internal.gcp import auth
-from apache_beam.utils import retry
-
-# Protect against environments where datastore library is not available.
-# pylint: disable=wrong-import-order, wrong-import-position
-try:
-  from google.cloud.proto.datastore.v1 import datastore_pb2
-  from google.cloud.proto.datastore.v1 import entity_pb2
-  from google.cloud.proto.datastore.v1 import query_pb2
-  from google.rpc import code_pb2
-  from googledatastore import PropertyFilter, CompositeFilter
-  from googledatastore import helper as datastore_helper
-  from googledatastore.connection import Datastore
-  from googledatastore.connection import RPCError
-except ImportError:
-  pass
-# pylint: enable=wrong-import-order, wrong-import-position
-
-# pylint: enable=ungrouped-imports
-
-_LOGGER = logging.getLogger(__name__)
-
-
-def key_comparator(k1, k2):
-  """A comparator for Datastore keys.
-
-  Comparison is only valid for keys in the same partition. The comparison here
-  is between the list of paths for each key.
-  """
-
-  if k1.partition_id != k2.partition_id:
-    raise ValueError('Cannot compare keys with different partition ids.')
-
-  k2_iter = iter(k2.path)
-
-  for k1_path in k1.path:
-    k2_path = next(k2_iter, None)
-    if not k2_path:
-      return 1
-
-    result = compare_path(k1_path, k2_path)
-
-    if result != 0:
-      return result
-
-  k2_path = next(k2_iter, None)
-  if k2_path:
-    return -1
-  return 0
-
-
-def compare_path(p1, p2):
-  """A comparator for key path.
-
-  A path has either an `id` or a `name` field defined. The
-  comparison works with the following rules:
-
-  1. If one path has `id` defined while the other doesn't, then the
-  one with `id` defined is considered smaller.
-  2. If both paths have `id` defined, then their ids are compared.
-  3. If no `id` is defined for both paths, then their `names` are compared.
-  """
-
-  result = (p1.kind > p2.kind) - (p1.kind < p2.kind)
-  if result != 0:
-    return result
-
-  if p1.HasField('id'):
-    if not p2.HasField('id'):
-      return -1
-
-    return (p1.id > p2.id) - (p1.id < p2.id)
-
-  if p2.HasField('id'):
-    return 1
-
-  return (p1.name > p2.name) - (p1.name < p2.name)
-
-
-def get_datastore(project):
-  """Returns a Cloud Datastore client."""
-  credentials = auth.get_service_credentials()
-  return Datastore(project, credentials, host='batch-datastore.googleapis.com')
-
-
-def make_request(project, namespace, query):
-  """Make a Cloud Datastore request for the given query."""
-  req = datastore_pb2.RunQueryRequest()
-  req.partition_id.CopyFrom(make_partition(project, namespace))
-
-  req.query.CopyFrom(query)
-  return req
-
-
-def make_partition(project, namespace):
-  """Make a PartitionId for the given project and namespace."""
-  partition = entity_pb2.PartitionId()
-  partition.project_id = project
-  if namespace is not None:
-    partition.namespace_id = namespace
-
-  return partition
-
-
-def retry_on_rpc_error(exception):
-  """A retry filter for Cloud Datastore RPCErrors."""
-  if isinstance(exception, RPCError):
-    err_code = exception.code
-    # TODO(BEAM-2156): put these codes in a global list and use that instead.
-    # https://cloud.google.com/datastore/docs/concepts/errors#error_codes
-    return err_code in [
-        code_pb2.ABORTED,
-        code_pb2.DEADLINE_EXCEEDED,
-        code_pb2.INTERNAL,
-        code_pb2.UNAVAILABLE,
-        code_pb2.UNKNOWN,
-    ]
-
-  if isinstance(exception, SocketError):
-    return (
-        exception.errno == errno.ECONNRESET or
-        exception.errno == errno.ETIMEDOUT or exception.errno == errno.EPIPE)
-
-  return False
-
-
-def fetch_entities(project, namespace, query, datastore):
-  """A helper method to fetch entities from Cloud Datastore.
-
-  Args:
-    project: Project ID
-    namespace: Cloud Datastore namespace
-    query: Query to be read from
-    datastore: Cloud Datastore Client
-
-  Returns:
-    An iterator of entities.
-  """
-  return QueryIterator(project, namespace, query, datastore)
-
-
-def is_key_valid(key):
-  """Returns True if a Cloud Datastore key is complete.
-
-  A key is complete if its last element has either an id or a name.
-  """
-  if not key.path:
-    return False
-  return key.path[-1].HasField('id') or key.path[-1].HasField('name')
-
-
-def write_mutations(
-    datastore,
-    project,
-    mutations,
-    throttler,
-    rpc_stats_callback=None,
-    throttle_delay=1):
-  """A helper function to write a batch of mutations to Cloud Datastore.
-
-  If a commit fails, it will be retried upto 5 times. All mutations in the
-  batch will be committed again, even if the commit was partially successful.
-  If the retry limit is exceeded, the last exception from Cloud Datastore will
-  be raised.
-
-  Args:
-    datastore: googledatastore.connection.Datastore
-    project: str, project id
-    mutations: list of google.cloud.proto.datastore.v1.datastore_pb2.Mutation
-    rpc_stats_callback: a function to call with arguments `successes` and
-        `failures` and `throttled_secs`; this is called to record successful
-        and failed RPCs to Datastore and time spent waiting for throttling.
-    throttler: AdaptiveThrottler, to use to select requests to be throttled.
-    throttle_delay: float, time in seconds to sleep when throttled.
-
-  Returns a tuple of:
-    CommitResponse, the response from Datastore;
-    int, the latency of the successful RPC in milliseconds.
-  """
-  commit_request = datastore_pb2.CommitRequest()
-  commit_request.mode = datastore_pb2.CommitRequest.NON_TRANSACTIONAL
-  commit_request.project_id = project
-  for mutation in mutations:
-    commit_request.mutations.add().CopyFrom(mutation)
-
-  @retry.with_exponential_backoff(
-      num_retries=5, retry_filter=retry_on_rpc_error)
-  def commit(request):
-    # Client-side throttling.
-    while throttler.throttle_request(time.time() * 1000):
-      _LOGGER.info(
-          "Delaying request for %ds due to previous failures", throttle_delay)
-      time.sleep(throttle_delay)
-      rpc_stats_callback(throttled_secs=throttle_delay)
-
-    try:
-      start_time = time.time()
-      response = datastore.commit(request)
-      end_time = time.time()
-
-      rpc_stats_callback(successes=1)
-      throttler.successful_request(start_time * 1000)
-      commit_time_ms = int((end_time - start_time) * 1000)
-      return response, commit_time_ms
-    except (RPCError, SocketError):
-      if rpc_stats_callback:
-        rpc_stats_callback(errors=1)
-      raise
-
-  response, commit_time_ms = commit(commit_request)
-  return response, commit_time_ms
-
-
-def make_latest_timestamp_query(namespace):
-  """Make a Query to fetch the latest timestamp statistics."""
-  query = query_pb2.Query()
-  if namespace is None:
-    query.kind.add().name = '__Stat_Total__'
-  else:
-    query.kind.add().name = '__Stat_Ns_Total__'
-
-  # Descending order of `timestamp`
-  datastore_helper.add_property_orders(query, "-timestamp")
-  # Only get the latest entity
-  query.limit.value = 1
-  return query
-
-
-def make_kind_stats_query(namespace, kind, latest_timestamp):
-  """Make a Query to fetch the latest kind statistics."""
-  kind_stat_query = query_pb2.Query()
-  if namespace is None:
-    kind_stat_query.kind.add().name = '__Stat_Kind__'
-  else:
-    kind_stat_query.kind.add().name = '__Stat_Ns_Kind__'
-
-  kind_filter = datastore_helper.set_property_filter(
-      query_pb2.Filter(), 'kind_name', PropertyFilter.EQUAL, unicode(kind))
-  timestamp_filter = datastore_helper.set_property_filter(
-      query_pb2.Filter(), 'timestamp', PropertyFilter.EQUAL, latest_timestamp)
-
-  datastore_helper.set_composite_filter(
-      kind_stat_query.filter,
-      CompositeFilter.AND,
-      kind_filter,
-      timestamp_filter)
-  return kind_stat_query
-
-
-class QueryIterator(object):
-  """A iterator class for entities of a given query.
-
-  Entities are read in batches. Retries on failures.
-  """
-  # Maximum number of results to request per query.
-  _BATCH_SIZE = 500
-
-  def __init__(self, project, namespace, query, datastore):
-    self._query = query
-    self._datastore = datastore
-    self._project = project
-    self._namespace = namespace
-    self._start_cursor = None
-    self._limit = self._query.limit.value or sys.maxsize
-    self._req = make_request(project, namespace, query)
-
-  @retry.with_exponential_backoff(
-      num_retries=5, retry_filter=retry_on_rpc_error)
-  def _next_batch(self):
-    """Fetches the next batch of entities."""
-    if self._start_cursor is not None:
-      self._req.query.start_cursor = self._start_cursor
-
-    # set batch size
-    self._req.query.limit.value = min(self._BATCH_SIZE, self._limit)
-    resp = self._datastore.run_query(self._req)
-    return resp
-
-  def __iter__(self):
-    more_results = True
-    while more_results:
-      resp = self._next_batch()
-      for entity_result in resp.batch.entity_results:
-        yield entity_result.entity
-
-      self._start_cursor = resp.batch.end_cursor
-      num_results = len(resp.batch.entity_results)
-      self._limit -= num_results
-
-      # Check if we need to read more entities.
-      # True when query limit hasn't been satisfied and there are more entities
-      # to be read. The latter is true if the response has a status
-      # `NOT_FINISHED` or if the number of results read in the previous batch
-      # is equal to `_BATCH_SIZE` (all indications that there is more data be
-      # read).
-      more_results = ((self._limit > 0) and (
-          (num_results == self._BATCH_SIZE) or
-          (resp.batch.more_results == query_pb2.QueryResultBatch.NOT_FINISHED)))
diff --git a/sdks/python/apache_beam/io/gcp/datastore/v1/helper_test.py b/sdks/python/apache_beam/io/gcp/datastore/v1/helper_test.py
deleted file mode 100644
index 7f67eb7ed5a..00000000000
--- a/sdks/python/apache_beam/io/gcp/datastore/v1/helper_test.py
+++ /dev/null
@@ -1,296 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#    http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-
-"""Tests for datastore helper."""
-# pytype: skip-file
-
-from __future__ import absolute_import
-
-import errno
-import random
-import sys
-import unittest
-from builtins import map
-from socket import error as SocketError
-
-from mock import MagicMock
-
-# Protect against environments where apitools library is not available.
-# pylint: disable=wrong-import-order, wrong-import-position
-try:
-  from apache_beam.testing.test_utils import patch_retry
-  from apache_beam.io.gcp.datastore.v1 import fake_datastore
-  from apache_beam.io.gcp.datastore.v1 import helper
-  from google.cloud.proto.datastore.v1 import datastore_pb2
-  from google.cloud.proto.datastore.v1 import entity_pb2
-  from google.cloud.proto.datastore.v1 import query_pb2
-  from google.cloud.proto.datastore.v1.entity_pb2 import Key
-  from google.rpc import code_pb2
-  from googledatastore.connection import RPCError
-  from googledatastore import helper as datastore_helper
-except (ImportError, TypeError):
-  datastore_helper = None
-# pylint: enable=wrong-import-order, wrong-import-position
-
-
-@unittest.skipIf(
-    sys.version_info[0] == 3,
-    'v1/helper does not support Python 3 TODO: BEAM-4543')
-@unittest.skipIf(datastore_helper is None, 'GCP dependencies are not installed')
-class HelperTest(unittest.TestCase):
-  def setUp(self):
-    self._mock_datastore = MagicMock()
-    self._query = query_pb2.Query()
-    self._query.kind.add().name = 'dummy_kind'
-    patch_retry(self, helper)
-    self._retriable_errors = [
-        RPCError("dummy", code_pb2.INTERNAL, "failed"),
-        SocketError(errno.ECONNRESET, "Connection Reset"),
-        SocketError(errno.ETIMEDOUT, "Timed out")
-    ]
-
-    self._non_retriable_errors = [
-        RPCError("dummy", code_pb2.UNAUTHENTICATED, "failed"),
-        SocketError(errno.EADDRNOTAVAIL, "Address not available")
-    ]
-
-  def permanent_retriable_datastore_failure(self, req):
-    raise RPCError("dummy", code_pb2.UNAVAILABLE, "failed")
-
-  def transient_retriable_datastore_failure(self, req):
-    if self._transient_fail_count:
-      self._transient_fail_count -= 1
-      raise random.choice(self._retriable_errors)
-    else:
-      return datastore_pb2.RunQueryResponse()
-
-  def non_retriable_datastore_failure(self, req):
-    raise random.choice(self._non_retriable_errors)
-
-  def test_query_iterator(self):
-    self._mock_datastore.run_query.side_effect = (
-        self.permanent_retriable_datastore_failure)
-    query_iterator = helper.QueryIterator(
-        "project", None, self._query, self._mock_datastore)
-    self.assertRaises(RPCError, iter(query_iterator).next)
-    self.assertEqual(6, len(self._mock_datastore.run_query.call_args_list))
-
-  def test_query_iterator_with_transient_failures(self):
-    self._mock_datastore.run_query.side_effect = (
-        self.transient_retriable_datastore_failure)
-    query_iterator = helper.QueryIterator(
-        "project", None, self._query, self._mock_datastore)
-    fail_count = 5
-    self._transient_fail_count = fail_count
-    for _ in query_iterator:
-      pass
-
-    self.assertEqual(
-        fail_count + 1, len(self._mock_datastore.run_query.call_args_list))
-
-  def test_query_iterator_with_non_retriable_failures(self):
-    self._mock_datastore.run_query.side_effect = (
-        self.non_retriable_datastore_failure)
-    query_iterator = helper.QueryIterator(
-        "project", None, self._query, self._mock_datastore)
-    self.assertRaises(
-        tuple(map(type, self._non_retriable_errors)), iter(query_iterator).next)
-    self.assertEqual(1, len(self._mock_datastore.run_query.call_args_list))
-
-  def test_query_iterator_with_single_batch(self):
-    num_entities = 100
-    batch_size = 500
-    self.check_query_iterator(num_entities, batch_size, self._query)
-
-  def test_query_iterator_with_multiple_batches(self):
-    num_entities = 1098
-    batch_size = 500
-    self.check_query_iterator(num_entities, batch_size, self._query)
-
-  def test_query_iterator_with_exact_batch_multiple(self):
-    num_entities = 1000
-    batch_size = 500
-    self.check_query_iterator(num_entities, batch_size, self._query)
-
-  def test_query_iterator_with_query_limit(self):
-    num_entities = 1098
-    batch_size = 500
-    self._query.limit.value = 1004
-    self.check_query_iterator(num_entities, batch_size, self._query)
-
-  def test_query_iterator_with_large_query_limit(self):
-    num_entities = 1098
-    batch_size = 500
-    self._query.limit.value = 10000
-    self.check_query_iterator(num_entities, batch_size, self._query)
-
-  def check_query_iterator(self, num_entities, batch_size, query):
-    """A helper method to test the QueryIterator.
-
-    Args:
-      num_entities: number of entities contained in the fake datastore.
-      batch_size: the number of entities returned by fake datastore in one req.
-      query: the query to be executed
-
-    """
-    entities = fake_datastore.create_entities(num_entities)
-    self._mock_datastore.run_query.side_effect = \
-        fake_datastore.create_run_query(entities, batch_size)
-    query_iterator = helper.QueryIterator(
-        "project", None, self._query, self._mock_datastore)
-
-    i = 0
-    for entity in query_iterator:
-      self.assertEqual(entity, entities[i].entity)
-      i += 1
-
-    limit = query.limit.value if query.HasField('limit') else sys.maxsize
-    self.assertEqual(i, min(num_entities, limit))
-
-  def test_is_key_valid(self):
-    key = entity_pb2.Key()
-    # Complete with name, no ancestor
-    datastore_helper.add_key_path(key, 'kind', 'name')
-    self.assertTrue(helper.is_key_valid(key))
-
-    key = entity_pb2.Key()
-    # Complete with id, no ancestor
-    datastore_helper.add_key_path(key, 'kind', 12)
-    self.assertTrue(helper.is_key_valid(key))
-
-    key = entity_pb2.Key()
-    # Incomplete, no ancestor
-    datastore_helper.add_key_path(key, 'kind')
-    self.assertFalse(helper.is_key_valid(key))
-
-    key = entity_pb2.Key()
-    # Complete with name and ancestor
-    datastore_helper.add_key_path(key, 'kind', 'name', 'kind2', 'name2')
-    self.assertTrue(helper.is_key_valid(key))
-
-    key = entity_pb2.Key()
-    # Complete with id and ancestor
-    datastore_helper.add_key_path(key, 'kind', 'name', 'kind2', 123)
-    self.assertTrue(helper.is_key_valid(key))
-
-    key = entity_pb2.Key()
-    # Incomplete with ancestor
-    datastore_helper.add_key_path(key, 'kind', 'name', 'kind2')
-    self.assertFalse(helper.is_key_valid(key))
-
-    key = entity_pb2.Key()
-    self.assertFalse(helper.is_key_valid(key))
-
-  def test_compare_path_with_different_kind(self):
-    p1 = Key.PathElement()
-    p1.kind = 'dummy1'
-
-    p2 = Key.PathElement()
-    p2.kind = 'dummy2'
-
-    self.assertLess(helper.compare_path(p1, p2), 0)
-
-  def test_compare_path_with_different_id(self):
-    p1 = Key.PathElement()
-    p1.kind = 'dummy'
-    p1.id = 10
-
-    p2 = Key.PathElement()
-    p2.kind = 'dummy'
-    p2.id = 15
-
-    self.assertLess(helper.compare_path(p1, p2), 0)
-
-  def test_compare_path_with_different_name(self):
-    p1 = Key.PathElement()
-    p1.kind = 'dummy'
-    p1.name = "dummy1"
-
-    p2 = Key.PathElement()
-    p2.kind = 'dummy'
-    p2.name = 'dummy2'
-
-    self.assertLess(helper.compare_path(p1, p2), 0)
-
-  def test_compare_path_of_different_type(self):
-    p1 = Key.PathElement()
-    p1.kind = 'dummy'
-    p1.id = 10
-
-    p2 = Key.PathElement()
-    p2.kind = 'dummy'
-    p2.name = 'dummy'
-
-    self.assertLess(helper.compare_path(p1, p2), 0)
-
-  def test_key_comparator_with_different_partition(self):
-    k1 = Key()
-    k1.partition_id.namespace_id = 'dummy1'
-    k2 = Key()
-    k2.partition_id.namespace_id = 'dummy2'
-    self.assertRaises(ValueError, helper.key_comparator, k1, k2)
-
-  def test_key_comparator_with_single_path(self):
-    k1 = Key()
-    k2 = Key()
-    p1 = k1.path.add()
-    p2 = k2.path.add()
-    p1.kind = p2.kind = 'dummy'
-    self.assertEqual(helper.key_comparator(k1, k2), 0)
-
-  def test_key_comparator_with_multiple_paths_1(self):
-    k1 = Key()
-    k2 = Key()
-    p11 = k1.path.add()
-    p12 = k1.path.add()
-    p21 = k2.path.add()
-    p11.kind = p12.kind = p21.kind = 'dummy'
-    self.assertGreater(helper.key_comparator(k1, k2), 0)
-
-  def test_key_comparator_with_multiple_paths_2(self):
-    k1 = Key()
-    k2 = Key()
-    p11 = k1.path.add()
-    p21 = k2.path.add()
-    p22 = k2.path.add()
-    p11.kind = p21.kind = p22.kind = 'dummy'
-    self.assertLess(helper.key_comparator(k1, k2), 0)
-
-  def test_key_comparator_with_multiple_paths_3(self):
-    k1 = Key()
-    k2 = Key()
-    p11 = k1.path.add()
-    p12 = k1.path.add()
-    p21 = k2.path.add()
-    p22 = k2.path.add()
-    p11.kind = p12.kind = p21.kind = p22.kind = 'dummy'
-    self.assertEqual(helper.key_comparator(k1, k2), 0)
-
-  def test_key_comparator_with_multiple_paths_4(self):
-    k1 = Key()
-    k2 = Key()
-    p11 = k1.path.add()
-    p12 = k2.path.add()
-    p21 = k2.path.add()
-    p11.kind = p12.kind = 'dummy'
-    # make path2 greater than path1
-    p21.kind = 'dummy1'
-    self.assertLess(helper.key_comparator(k1, k2), 0)
-
-
-if __name__ == '__main__':
-  unittest.main()
diff --git a/sdks/python/apache_beam/io/gcp/datastore/v1/query_splitter.py b/sdks/python/apache_beam/io/gcp/datastore/v1/query_splitter.py
deleted file mode 100644
index c6707cff766..00000000000
--- a/sdks/python/apache_beam/io/gcp/datastore/v1/query_splitter.py
+++ /dev/null
@@ -1,283 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#    http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-
-"""Implements a Cloud Datastore query splitter."""
-# pytype: skip-file
-
-from __future__ import absolute_import
-from __future__ import division
-
-from builtins import range
-from builtins import round
-
-from apache_beam.io.gcp.datastore.v1 import helper
-
-# Protect against environments where datastore library is not available.
-# pylint: disable=wrong-import-order, wrong-import-position
-try:
-  from google.cloud.proto.datastore.v1 import datastore_pb2
-  from google.cloud.proto.datastore.v1 import query_pb2
-  from google.cloud.proto.datastore.v1.query_pb2 import PropertyFilter
-  from google.cloud.proto.datastore.v1.query_pb2 import CompositeFilter
-  from googledatastore import helper as datastore_helper
-  UNSUPPORTED_OPERATORS = [
-      PropertyFilter.LESS_THAN,
-      PropertyFilter.LESS_THAN_OR_EQUAL,
-      PropertyFilter.GREATER_THAN,
-      PropertyFilter.GREATER_THAN_OR_EQUAL
-  ]
-except ImportError:
-  UNSUPPORTED_OPERATORS = None  # type: ignore
-# pylint: enable=wrong-import-order, wrong-import-position
-
-__all__ = [
-    'get_splits',
-]
-
-SCATTER_PROPERTY_NAME = '__scatter__'
-KEY_PROPERTY_NAME = '__key__'
-# The number of keys to sample for each split.
-KEYS_PER_SPLIT = 32
-
-
-def get_splits(datastore, query, num_splits, partition=None):
-  """Returns a list of sharded queries for the given Cloud Datastore query.
-
-  This will create up to the desired number of splits, however it may return
-  less splits if the desired number of splits is unavailable. This will happen
-  if the number of split points provided by the underlying Datastore is less
-  than the desired number, which will occur if the number of results for the
-  query is too small.
-
-  This implementation of the QuerySplitter uses the __scatter__ property to
-  gather random split points for a query.
-
-  Note: This implementation is derived from the java query splitter in
-  https://github.com/GoogleCloudPlatform/google-cloud-datastore/blob/master/java/datastore/src/main/java/com/google/datastore/v1/client/QuerySplitterImpl.java
-
-  Args:
-    datastore: the datastore client.
-    query: the query to split.
-    num_splits: the desired number of splits.
-    partition: the partition the query is running in.
-
-  Returns:
-    A list of split queries, of a max length of `num_splits`
-  """
-
-  # Validate that the number of splits is not out of bounds.
-  if num_splits < 1:
-    raise ValueError('The number of splits must be greater than 0.')
-
-  if num_splits == 1:
-    return [query]
-
-  _validate_query(query)
-
-  splits = []
-  scatter_keys = _get_scatter_keys(datastore, query, num_splits, partition)
-  last_key = None
-  for next_key in _get_split_key(scatter_keys, num_splits):
-    splits.append(_create_split(last_key, next_key, query))
-    last_key = next_key
-
-  splits.append(_create_split(last_key, None, query))
-  return splits
-
-
-def _validate_query(query):
-  """ Verifies that the given query can be properly scattered."""
-
-  if len(query.kind) != 1:
-    raise ValueError('Query must have exactly one kind.')
-
-  if query.order:
-    raise ValueError('Query cannot have any sort orders.')
-
-  if query.HasField('limit'):
-    raise ValueError('Query cannot have a limit set.')
-
-  if query.offset > 0:
-    raise ValueError('Query cannot have an offset set.')
-
-  _validate_filter(query.filter)
-
-
-def _validate_filter(filter):
-  """Validates that we only have allowable filters.
-
-  Note that equality and ancestor filters are allowed, however they may result
-  in inefficient sharding.
-  """
-
-  if filter.HasField('composite_filter'):
-    for sub_filter in filter.composite_filter.filters:
-      _validate_filter(sub_filter)
-  elif filter.HasField('property_filter'):
-    if filter.property_filter.op in UNSUPPORTED_OPERATORS:
-      raise ValueError('Query cannot have any inequality filters.')
-  else:
-    pass
-
-
-def _create_scatter_query(query, num_splits):
-  """Creates a scatter query from the given user query."""
-
-  scatter_query = query_pb2.Query()
-  for kind in query.kind:
-    scatter_kind = scatter_query.kind.add()
-    scatter_kind.CopyFrom(kind)
-
-  # ascending order
-  datastore_helper.add_property_orders(scatter_query, SCATTER_PROPERTY_NAME)
-
-  # There is a split containing entities before and after each scatter entity:
-  # ||---*------*------*------*------*------*------*---||  * = scatter entity
-  # If we represent each split as a region before a scatter entity, there is an
-  # extra region following the last scatter point. Thus, we do not need the
-  # scatter entity for the last region.
-  scatter_query.limit.value = (num_splits - 1) * KEYS_PER_SPLIT
-  datastore_helper.add_projection(scatter_query, KEY_PROPERTY_NAME)
-
-  return scatter_query
-
-
-def _get_scatter_keys(datastore, query, num_splits, partition):
-  """Gets a list of split keys given a desired number of splits.
-
-  This list will contain multiple split keys for each split. Only a single split
-  key will be chosen as the split point, however providing multiple keys allows
-  for more uniform sharding.
-
-  Args:
-    numSplits: the number of desired splits.
-    query: the user query.
-    partition: the partition to run the query in.
-    datastore: the client to datastore containing the data.
-
-  Returns:
-    A list of scatter keys returned by Datastore.
-  """
-  scatter_point_query = _create_scatter_query(query, num_splits)
-
-  key_splits = []
-  while True:
-    req = datastore_pb2.RunQueryRequest()
-    if partition:
-      req.partition_id.CopyFrom(partition)
-
-    req.query.CopyFrom(scatter_point_query)
-
-    resp = datastore.run_query(req)
-    for entity_result in resp.batch.entity_results:
-      key_splits.append(entity_result.entity.key)
-
-    if resp.batch.more_results != query_pb2.QueryResultBatch.NOT_FINISHED:
-      break
-
-    scatter_point_query.start_cursor = resp.batch.end_cursor
-    scatter_point_query.limit.value -= len(resp.batch.entity_results)
-
-  key_splits.sort(helper.key_comparator)
-  return key_splits
-
-
-def _get_split_key(keys, num_splits):
-  """Given a list of keys and a number of splits find the keys to split on.
-
-  Args:
-    keys: the list of keys.
-    num_splits: the number of splits.
-
-  Returns:
-    A list of keys to split on.
-
-  """
-
-  # If the number of keys is less than the number of splits, we are limited
-  # in the number of splits we can make.
-  if not keys or (len(keys) < (num_splits - 1)):
-    return keys
-
-  # Calculate the number of keys per split. This should be KEYS_PER_SPLIT,
-  # but may be less if there are not KEYS_PER_SPLIT * (numSplits - 1) scatter
-  # entities.
-  #
-  # Consider the following dataset, where - represents an entity and
-  # * represents an entity that is returned as a scatter entity:
-  # ||---*-----*----*-----*-----*------*----*----||
-  # If we want 4 splits in this data, the optimal split would look like:
-  # ||---*-----*----*-----*-----*------*----*----||
-  #            |          |            |
-  # The scatter keys in the last region are not useful to us, so we never
-  # request them:
-  # ||---*-----*----*-----*-----*------*---------||
-  #            |          |            |
-  # With 6 scatter keys we want to set scatter points at indexes: 1, 3, 5.
-  #
-  # We keep this as a float so that any "fractional" keys per split get
-  # distributed throughout the splits and don't make the last split
-  # significantly larger than the rest.
-
-  num_keys_per_split = max(1.0, float(len(keys)) / (num_splits - 1))
-
-  split_keys = []
-
-  # Grab the last sample for each split, otherwise the first split will be too
-  # small.
-  for i in range(1, num_splits):
-    split_index = int(round(i * num_keys_per_split) - 1)
-    split_keys.append(keys[split_index])
-
-  return split_keys
-
-
-def _create_split(last_key, next_key, query):
-  """Create a new {@link Query} given the query and range..
-
-  Args:
-    last_key: the previous key. If null then assumed to be the beginning.
-    next_key: the next key. If null then assumed to be the end.
-    query: the desired query.
-
-  Returns:
-    A split query with fetches entities in the range [last_key, next_key)
-  """
-  if not (last_key or next_key):
-    return query
-
-  split_query = query_pb2.Query()
-  split_query.CopyFrom(query)
-  composite_filter = split_query.filter.composite_filter
-  composite_filter.op = CompositeFilter.AND
-
-  if query.HasField('filter'):
-    composite_filter.filters.add().CopyFrom(query.filter)
-
-  if last_key:
-    lower_bound = composite_filter.filters.add()
-    lower_bound.property_filter.property.name = KEY_PROPERTY_NAME
-    lower_bound.property_filter.op = PropertyFilter.GREATER_THAN_OR_EQUAL
-    lower_bound.property_filter.value.key_value.CopyFrom(last_key)
-
-  if next_key:
-    upper_bound = composite_filter.filters.add()
-    upper_bound.property_filter.property.name = KEY_PROPERTY_NAME
-    upper_bound.property_filter.op = PropertyFilter.LESS_THAN
-    upper_bound.property_filter.value.key_value.CopyFrom(next_key)
-
-  return split_query
diff --git a/sdks/python/apache_beam/io/gcp/datastore/v1/query_splitter_test.py b/sdks/python/apache_beam/io/gcp/datastore/v1/query_splitter_test.py
deleted file mode 100644
index e68d5dbff2d..00000000000
--- a/sdks/python/apache_beam/io/gcp/datastore/v1/query_splitter_test.py
+++ /dev/null
@@ -1,244 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#    http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-
-"""Cloud Datastore query splitter test."""
-
-# pytype: skip-file
-
-from __future__ import absolute_import
-
-import sys
-import unittest
-from typing import Type
-
-# patches unittest.TestCase to be python3 compatible
-import future.tests.base  # pylint: disable=unused-import
-from mock import MagicMock
-from mock import call
-
-# Protect against environments where datastore library is not available.
-# pylint: disable=wrong-import-order, wrong-import-position
-try:
-  from apache_beam.io.gcp.datastore.v1 import fake_datastore
-  from apache_beam.io.gcp.datastore.v1 import query_splitter
-  from google.cloud.proto.datastore.v1 import datastore_pb2
-  from google.cloud.proto.datastore.v1 import query_pb2
-  from google.cloud.proto.datastore.v1.query_pb2 import PropertyFilter
-except (ImportError, TypeError):
-  datastore_pb2 = None
-  query_splitter = None  # type: ignore
-# pylint: enable=wrong-import-order, wrong-import-position
-
-
-class QuerySplitterTest(unittest.TestCase):
-  @unittest.skipIf(
-      sys.version_info[0] == 3,
-      'v1/query_splitter does not support Python 3 TODO: BEAM-4543')
-  @unittest.skipIf(datastore_pb2 is None, 'GCP dependencies are not installed')
-  def setUp(self):
-    pass
-
-  def create_query(
-      self,
-      kinds=(),
-      order=False,
-      limit=None,
-      offset=None,
-      inequality_filter=False):
-    query = query_pb2.Query()
-    for kind in kinds:
-      query.kind.add().name = kind
-    if order:
-      query.order.add()
-    if limit is not None:
-      query.limit.value = limit
-    if offset is not None:
-      query.offset = offset
-    if inequality_filter:
-      test_filter = query.filter.composite_filter.filters.add()
-      test_filter.property_filter.op = PropertyFilter.GREATER_THAN
-    return query
-
-  split_error = ValueError  # type: Type[Exception]
-  query_splitter = query_splitter
-
-  def test_get_splits_query_with_multiple_kinds(self):
-    query = self.create_query(kinds=['a', 'b'])
-    with self.assertRaisesRegex(self.split_error, r'one kind'):
-      self.query_splitter.get_splits(None, query, 4)
-
-  def test_get_splits_query_with_order(self):
-    query = self.create_query(kinds=['a'], order=True)
-    with self.assertRaisesRegex(self.split_error, r'sort orders'):
-      self.query_splitter.get_splits(None, query, 3)
-
-  def test_get_splits_query_with_unsupported_filter(self):
-    query = self.create_query(kinds=['a'], inequality_filter=True)
-    with self.assertRaisesRegex(self.split_error, r'inequality filters'):
-      self.query_splitter.get_splits(None, query, 2)
-
-  def test_get_splits_query_with_limit(self):
-    query = self.create_query(kinds=['a'], limit=10)
-    with self.assertRaisesRegex(self.split_error, r'limit set'):
-      self.query_splitter.get_splits(None, query, 2)
-
-  def test_get_splits_query_with_offset(self):
-    query = self.create_query(kinds=['a'], offset=10)
-    with self.assertRaisesRegex(self.split_error, r'offset set'):
-      self.query_splitter.get_splits(None, query, 2)
-
-  def test_create_scatter_query(self):
-    query = self.create_query(kinds=['shakespeare-demo'])
-    num_splits = 10
-    scatter_query = self.query_splitter._create_scatter_query(query, num_splits)
-    self.assertEqual(scatter_query.kind[0], query.kind[0])
-    self.assertEqual(
-        scatter_query.limit.value,
-        (num_splits - 1) * self.query_splitter.KEYS_PER_SPLIT)
-    self.assertEqual(
-        scatter_query.order[0].direction, query_pb2.PropertyOrder.ASCENDING)
-    self.assertEqual(
-        scatter_query.projection[0].property.name,
-        self.query_splitter.KEY_PROPERTY_NAME)
-
-  def test_get_splits_with_two_splits(self):
-    query = self.create_query(kinds=['shakespeare-demo'])
-    num_splits = 2
-    num_entities = 97
-    batch_size = 9
-
-    self.check_get_splits(query, num_splits, num_entities, batch_size)
-
-  def test_get_splits_with_multiple_splits(self):
-    query = self.create_query(kinds=['shakespeare-demo'])
-    num_splits = 4
-    num_entities = 369
-    batch_size = 12
-
-    self.check_get_splits(query, num_splits, num_entities, batch_size)
-
-  def test_get_splits_with_large_num_splits(self):
-    query = self.create_query(kinds=['shakespeare-demo'])
-    num_splits = 10
-    num_entities = 4
-    batch_size = 10
-
-    self.check_get_splits(query, num_splits, num_entities, batch_size)
-
-  def test_get_splits_with_small_num_entities(self):
-    query = self.create_query(kinds=['shakespeare-demo'])
-    num_splits = 4
-    num_entities = 50
-    batch_size = 10
-
-    self.check_get_splits(query, num_splits, num_entities, batch_size)
-
-  def test_get_splits_with_batch_size_exact_multiple(self):
-    """Test get_splits when num scatter keys is a multiple of batch size."""
-    query = self.create_query(kinds=['shakespeare-demo'])
-    num_splits = 4
-    num_entities = 400
-    batch_size = 32
-
-    self.check_get_splits(query, num_splits, num_entities, batch_size)
-
-  def test_get_splits_with_large_batch_size(self):
-    """Test get_splits when all scatter keys are retured in a single req."""
-    query = self.create_query(kinds=['shakespeare-demo'])
-    num_splits = 4
-    num_entities = 400
-    batch_size = 500
-
-    self.check_get_splits(query, num_splits, num_entities, batch_size)
-
-  def test_get_splits_with_num_splits_gt_entities(self):
-    query = self.create_query(kinds=['shakespeare-demo'])
-    num_splits = 10
-    num_entities = 4
-    batch_size = 10
-
-    self.check_get_splits(query, num_splits, num_entities, batch_size)
-
-  def check_get_splits(self, query, num_splits, num_entities, batch_size):
-    """A helper method to test the query_splitter get_splits method.
-
-    Args:
-      query: the query to be split
-      num_splits: number of splits
-      num_entities: number of scatter entities contained in the fake datastore.
-      batch_size: the number of entities returned by fake datastore in one req.
-    """
-
-    # Test for random long ids, string ids, and a mix of both.
-    id_or_name = [True, False, None]
-
-    for id_type in id_or_name:
-      if id_type is None:
-        entities = fake_datastore.create_entities(num_entities, False)
-        entities.extend(fake_datastore.create_entities(num_entities, True))
-        num_entities *= 2
-      else:
-        entities = fake_datastore.create_entities(num_entities, id_type)
-      mock_datastore = MagicMock()
-      # Assign a fake run_query method as a side_effect to the mock.
-      mock_datastore.run_query.side_effect = \
-          fake_datastore.create_run_query(entities, batch_size)
-
-      split_queries = self.query_splitter.get_splits(
-          mock_datastore, query, num_splits)
-
-      # if request num_splits is greater than num_entities, the best it can
-      # do is one entity per split.
-      expected_num_splits = min(num_splits, num_entities + 1)
-      self.assertEqual(len(split_queries), expected_num_splits)
-
-      expected_requests = self.create_scatter_requests(
-          query, num_splits, batch_size, num_entities)
-
-      expected_calls = []
-      for req in expected_requests:
-        expected_calls.append(call(req))
-
-      self.assertEqual(expected_calls, mock_datastore.run_query.call_args_list)
-
-  def create_scatter_requests(
-      self, query, num_splits, batch_size, num_entities):
-    """Creates a list of expected scatter requests from the query splitter.
-
-    This list of requests returned is used to verify that the query splitter
-    made the same number of requests in the same order to datastore.
-    """
-
-    requests = []
-    count = (num_splits - 1) * self.query_splitter.KEYS_PER_SPLIT
-    start_cursor = ''
-    i = 0
-    scatter_query = self.query_splitter._create_scatter_query(query, count)
-    while i < count and i < num_entities:
-      request = datastore_pb2.RunQueryRequest()
-      request.query.CopyFrom(scatter_query)
-      request.query.start_cursor = start_cursor
-      request.query.limit.value = count - i
-      requests.append(request)
-      i += batch_size
-      start_cursor = str(i)
-
-    return requests
-
-
-if __name__ == '__main__':
-  unittest.main()
diff --git a/sdks/python/apache_beam/io/gcp/datastore/v1/adaptive_throttler.py b/sdks/python/apache_beam/io/gcp/datastore/v1new/adaptive_throttler.py
similarity index 98%
rename from sdks/python/apache_beam/io/gcp/datastore/v1/adaptive_throttler.py
rename to sdks/python/apache_beam/io/gcp/datastore/v1new/adaptive_throttler.py
index c940a9d0e70..9ae046ebb62 100644
--- a/sdks/python/apache_beam/io/gcp/datastore/v1/adaptive_throttler.py
+++ b/sdks/python/apache_beam/io/gcp/datastore/v1new/adaptive_throttler.py
@@ -27,7 +27,7 @@ from __future__ import division
 import random
 from builtins import object
 
-from apache_beam.io.gcp.datastore.v1 import util
+from apache_beam.io.gcp.datastore.v1new import util
 
 
 class AdaptiveThrottler(object):
diff --git a/sdks/python/apache_beam/io/gcp/datastore/v1/adaptive_throttler_test.py b/sdks/python/apache_beam/io/gcp/datastore/v1new/adaptive_throttler_test.py
similarity index 97%
rename from sdks/python/apache_beam/io/gcp/datastore/v1/adaptive_throttler_test.py
rename to sdks/python/apache_beam/io/gcp/datastore/v1new/adaptive_throttler_test.py
index 1deb997c491..cd45b5c6ec7 100644
--- a/sdks/python/apache_beam/io/gcp/datastore/v1/adaptive_throttler_test.py
+++ b/sdks/python/apache_beam/io/gcp/datastore/v1new/adaptive_throttler_test.py
@@ -25,7 +25,7 @@ from builtins import range
 
 from mock import patch
 
-from apache_beam.io.gcp.datastore.v1.adaptive_throttler import AdaptiveThrottler
+from apache_beam.io.gcp.datastore.v1new.adaptive_throttler import AdaptiveThrottler
 
 
 class AdaptiveThrottlerTest(unittest.TestCase):
diff --git a/sdks/python/apache_beam/io/gcp/datastore/v1new/datastore_write_it_test.py b/sdks/python/apache_beam/io/gcp/datastore/v1new/datastore_write_it_test.py
index bc838253fc7..7b3c01b2b72 100644
--- a/sdks/python/apache_beam/io/gcp/datastore/v1new/datastore_write_it_test.py
+++ b/sdks/python/apache_beam/io/gcp/datastore/v1new/datastore_write_it_test.py
@@ -42,8 +42,7 @@ from apache_beam.testing.test_pipeline import TestPipeline
 
 try:
   from apache_beam.io.gcp.datastore.v1new import datastore_write_it_pipeline
-# TODO(BEAM-4543): Remove TypeError once googledatastore dependency is removed.
-except (ImportError, TypeError):
+except ImportError:
   datastore_write_it_pipeline = None  # type: ignore
 
 
diff --git a/sdks/python/apache_beam/io/gcp/datastore/v1new/datastoreio.py b/sdks/python/apache_beam/io/gcp/datastore/v1new/datastoreio.py
index f2139e24347..065e5d8830f 100644
--- a/sdks/python/apache_beam/io/gcp/datastore/v1new/datastoreio.py
+++ b/sdks/python/apache_beam/io/gcp/datastore/v1new/datastoreio.py
@@ -18,14 +18,8 @@
 """
 A connector for reading from and writing to Google Cloud Datastore.
 
-Please use this module for Datastore I/O since
-``apache_beam.io.gcp.datastore.v1.datastoreio`` will be deprecated in the
-next Beam major release.
-
-This module uses the newer google-cloud-datastore package. Its API was different
-enough to require extensive changes to this and associated modules.
-
-This module is experimental, no backwards compatibility guarantees.
+This module uses the newer google-cloud-datastore client package. Its API was
+different enough to require extensive changes to this and associated modules.
 """
 # pytype: skip-file
 
@@ -37,11 +31,11 @@ import time
 from builtins import round
 
 from apache_beam import typehints
-from apache_beam.io.gcp.datastore.v1 import util
-from apache_beam.io.gcp.datastore.v1.adaptive_throttler import AdaptiveThrottler
 from apache_beam.io.gcp.datastore.v1new import helper
 from apache_beam.io.gcp.datastore.v1new import query_splitter
 from apache_beam.io.gcp.datastore.v1new import types
+from apache_beam.io.gcp.datastore.v1new import util
+from apache_beam.io.gcp.datastore.v1new.adaptive_throttler import AdaptiveThrottler
 from apache_beam.metrics.metric import Metrics
 from apache_beam.transforms import Create
 from apache_beam.transforms import DoFn
@@ -363,7 +357,7 @@ class _Mutate(PTransform):
         rpc_stats_callback: a function to call with arguments `successes` and
             `failures` and `throttled_secs`; this is called to record successful
             and failed RPCs to Datastore and time spent waiting for throttling.
-        throttler: (``apache_beam.io.gcp.datastore.v1.adaptive_throttler.
+        throttler: (``apache_beam.io.gcp.datastore.v1new.adaptive_throttler.
           AdaptiveThrottler``)
           Throttler instance used to select requests to be throttled.
         throttle_delay: (:class:`float`) time in seconds to sleep when
diff --git a/sdks/python/apache_beam/io/gcp/datastore/v1new/datastoreio_test.py b/sdks/python/apache_beam/io/gcp/datastore/v1new/datastoreio_test.py
index 269d913509f..10922e28788 100644
--- a/sdks/python/apache_beam/io/gcp/datastore/v1new/datastoreio_test.py
+++ b/sdks/python/apache_beam/io/gcp/datastore/v1new/datastoreio_test.py
@@ -34,8 +34,7 @@ from mock import ANY
 
 # Protect against environments where datastore library is not available.
 try:
-  from apache_beam.io.gcp.datastore.v1 import util
-  from apache_beam.io.gcp.datastore.v1new import helper
+  from apache_beam.io.gcp.datastore.v1new import helper, util
   from apache_beam.io.gcp.datastore.v1new import query_splitter
   from apache_beam.io.gcp.datastore.v1new import datastoreio
   from apache_beam.io.gcp.datastore.v1new.datastoreio import DeleteFromDatastore
@@ -47,13 +46,8 @@ try:
   from google.cloud.datastore import helpers
   from google.cloud.datastore import key
   from google.api_core import exceptions
-  # Keep this import last so it doesn't import conflicting pb2 modules.
-  from apache_beam.io.gcp.datastore.v1 import datastoreio_test  # pylint: disable=ungrouped-imports
-  DatastoreioTestBase = datastoreio_test.DatastoreioTest
-# TODO(BEAM-4543): Remove TypeError once googledatastore dependency is removed.
-except (ImportError, TypeError):
+except ImportError:
   client = None
-  DatastoreioTestBase = unittest.TestCase  # type: ignore
 
 
 class FakeMutation(object):
@@ -189,11 +183,11 @@ class MutateTest(unittest.TestCase):
 
 
 @unittest.skipIf(client is None, 'Datastore dependencies are not installed')
-class DatastoreioTest(DatastoreioTestBase):
-  """
-  NOTE: This test inherits test cases from DatastoreioTestBase.
-    Please prefer to add new test cases to v1/datastoreio_test if possible.
-  """
+class DatastoreioTest(unittest.TestCase):
+  _PROJECT = 'project'
+  _KIND = 'kind'
+  _NAMESPACE = 'namespace'
+
   def setUp(self):
     self._WRITE_BATCH_INITIAL_SIZE = util.WRITE_BATCH_INITIAL_SIZE
     self._mock_client = MagicMock()
@@ -209,9 +203,6 @@ class DatastoreioTest(DatastoreioTestBase):
         # Don't do any network requests.
         _http=MagicMock())
 
-  def get_timestamp(self):
-    return datetime.datetime(2019, 3, 14, 15, 9, 26, 535897)
-
   def test_SplitQueryFn_with_num_splits(self):
     with patch.object(helper, 'get_client', return_value=self._mock_client):
       num_splits = 23
@@ -283,7 +274,7 @@ class DatastoreioTest(DatastoreioTestBase):
           self.assertEqual(expected_num_splits, len(split_queries))
           self.assertEqual(self._mock_query, split_queries[0])
 
-  def check_DatastoreWriteFn(self, num_entities, use_fixed_batch_size=False):
+  def check_DatastoreWriteFn(self, num_entities):
     """A helper function to test _DatastoreWriteFn."""
     with patch.object(helper, 'get_client', return_value=self._mock_client):
       entities = helper.create_entities(num_entities)
@@ -315,6 +306,25 @@ class DatastoreioTest(DatastoreioTestBase):
       batch_count = math.ceil(num_entities / util.WRITE_BATCH_MAX_SIZE)
       self.assertLessEqual(batch_count, commit_count[0])
 
+  def test_DatastoreWriteFn_with_empty_batch(self):
+    self.check_DatastoreWriteFn(0)
+
+  def test_DatastoreWriteFn_with_one_batch(self):
+    num_entities_to_write = self._WRITE_BATCH_INITIAL_SIZE * 1 - 50
+    self.check_DatastoreWriteFn(num_entities_to_write)
+
+  def test_DatastoreWriteFn_with_multiple_batches(self):
+    num_entities_to_write = self._WRITE_BATCH_INITIAL_SIZE * 3 + 50
+    self.check_DatastoreWriteFn(num_entities_to_write)
+
+  def test_DatastoreWriteFn_with_batch_size_exact_multiple(self):
+    num_entities_to_write = self._WRITE_BATCH_INITIAL_SIZE * 2
+    self.check_DatastoreWriteFn(num_entities_to_write)
+
+  def test_DatastoreWriteFn_with_dynamic_batch_sizes(self):
+    num_entities_to_write = self._WRITE_BATCH_INITIAL_SIZE * 3 + 50
+    self.check_DatastoreWriteFn(num_entities_to_write)
+
   def test_DatastoreWriteLargeEntities(self):
     """100*100kB entities gets split over two Commit RPCs."""
     with patch.object(helper, 'get_client', return_value=self._mock_client):
@@ -367,6 +377,19 @@ class DatastoreioTest(DatastoreioTestBase):
         call().fetch(limit=1),
     ])
 
+  def get_timestamp(self):
+    return datetime.datetime(2019, 3, 14, 15, 9, 26, 535897)
+
+  def test_get_estimated_size_bytes_without_namespace(self):
+    entity_bytes = 100
+    timestamp = self.get_timestamp()
+    self.check_estimated_size_bytes(entity_bytes, timestamp)
+
+  def test_get_estimated_size_bytes_with_namespace(self):
+    entity_bytes = 100
+    timestamp = self.get_timestamp()
+    self.check_estimated_size_bytes(entity_bytes, timestamp, self._NAMESPACE)
+
   def test_DatastoreDeleteFn(self):
     with patch.object(helper, 'get_client', return_value=self._mock_client):
       keys = [entity.key for entity in helper.create_entities(10)]
@@ -394,8 +417,5 @@ class DatastoreioTest(DatastoreioTestBase):
       self.assertListEqual(all_batch_keys, expected_keys)
 
 
-# Hide base class from collection by nose.
-del DatastoreioTestBase
-
 if __name__ == '__main__':
   unittest.main()
diff --git a/sdks/python/apache_beam/io/gcp/datastore/v1new/query_splitter_test.py b/sdks/python/apache_beam/io/gcp/datastore/v1new/query_splitter_test.py
index babbaa37d91..a9b2b65c2ff 100644
--- a/sdks/python/apache_beam/io/gcp/datastore/v1new/query_splitter_test.py
+++ b/sdks/python/apache_beam/io/gcp/datastore/v1new/query_splitter_test.py
@@ -34,33 +34,15 @@ try:
   from apache_beam.io.gcp.datastore.v1new import types
   from apache_beam.io.gcp.datastore.v1new.query_splitter import SplitNotPossibleError
   from google.cloud.datastore import key
-  # Keep this import last so it doesn't import conflicting pb2 modules.
-  from apache_beam.io.gcp.datastore.v1 import query_splitter_test  # pylint: disable=ungrouped-imports
-  QuerySplitterTestBase = query_splitter_test.QuerySplitterTest
-
-# TODO(BEAM-4543): Remove TypeError once googledatastore dependency is removed.
-except (ImportError, TypeError):
+except ImportError:
   query_splitter = None  # type: ignore
-  SplitNotPossibleError = None  # type: ignore
-  QuerySplitterTestBase = unittest.TestCase  # type: ignore
 
 
 @unittest.skipIf(query_splitter is None, 'GCP dependencies are not installed')
-class QuerySplitterTest(QuerySplitterTestBase):
-  """v1new adaptation of QuerySplitterTest.
-
-  NOTE: This test inherits test cases from QuerySplitterTestBase.
-  Please prefer to add new test cases to v1/query_splitter_test if possible.
-  """
+class QuerySplitterTest(unittest.TestCase):
   _PROJECT = 'project'
   _NAMESPACE = 'namespace'
 
-  split_error = SplitNotPossibleError
-  query_splitter = query_splitter
-
-  def setUp(self):
-    """Overrides base class version with skipIf() decorators."""
-
   def create_query(
       self,
       kinds=(),
@@ -68,10 +50,6 @@ class QuerySplitterTest(QuerySplitterTestBase):
       limit=None,
       offset=None,
       inequality_filter=False):
-    if len(kinds) > 1:
-      self.skipTest('v1new queries do not support more than one kind.')
-    if offset is not None:
-      self.skipTest('v1new queries do not support offsets.')
 
     kind = None
     filters = []
@@ -84,9 +62,24 @@ class QuerySplitterTest(QuerySplitterTestBase):
 
     return types.Query(kind=kind, filters=filters, order=order, limit=limit)
 
+  def test_get_splits_query_with_order(self):
+    query = self.create_query(kinds=['a'], order=True)
+    with self.assertRaisesRegex(SplitNotPossibleError, r'sort orders'):
+      query_splitter.get_splits(None, query, 3)
+
+  def test_get_splits_query_with_unsupported_filter(self):
+    query = self.create_query(kinds=['a'], inequality_filter=True)
+    with self.assertRaisesRegex(SplitNotPossibleError, r'inequality filters'):
+      query_splitter.get_splits(None, query, 2)
+
+  def test_get_splits_query_with_limit(self):
+    query = self.create_query(kinds=['a'], limit=10)
+    with self.assertRaisesRegex(SplitNotPossibleError, r'limit set'):
+      query_splitter.get_splits(None, query, 2)
+
   def test_get_splits_query_with_num_splits_of_one(self):
     query = self.create_query()
-    with self.assertRaisesRegex(self.split_error, r'num_splits'):
+    with self.assertRaisesRegex(SplitNotPossibleError, r'num_splits'):
       query_splitter.get_splits(None, query, 1)
 
   def test_create_scatter_query(self):
@@ -101,16 +94,13 @@ class QuerySplitterTest(QuerySplitterTestBase):
     self.assertEqual(
         scatter_query.projection, [query_splitter.KEY_PROPERTY_NAME])
 
-  def check_get_splits(
-      self, query, num_splits, num_entities, unused_batch_size):
+  def check_get_splits(self, query, num_splits, num_entities):
     """A helper method to test the query_splitter get_splits method.
 
     Args:
       query: the query to be split
       num_splits: number of splits
       num_entities: number of scatter entities returned to the splitter.
-      unused_batch_size: ignored in v1new since query results are entirely
-        handled by the Datastore client.
     """
     # Test for random long ids, string ids, and a mix of both.
     for id_or_name in [True, False, None]:
@@ -171,6 +161,57 @@ class QuerySplitterTest(QuerySplitterTestBase):
           if lt_key is None:
             last_query_seen = True
 
+  def test_get_splits_with_two_splits(self):
+    query = self.create_query(kinds=['shakespeare-demo'])
+    num_splits = 2
+    num_entities = 97
+
+    self.check_get_splits(query, num_splits, num_entities)
+
+  def test_get_splits_with_multiple_splits(self):
+    query = self.create_query(kinds=['shakespeare-demo'])
+    num_splits = 4
+    num_entities = 369
+
+    self.check_get_splits(query, num_splits, num_entities)
+
+  def test_get_splits_with_large_num_splits(self):
+    query = self.create_query(kinds=['shakespeare-demo'])
+    num_splits = 10
+    num_entities = 4
+
+    self.check_get_splits(query, num_splits, num_entities)
+
+  def test_get_splits_with_small_num_entities(self):
+    query = self.create_query(kinds=['shakespeare-demo'])
+    num_splits = 4
+    num_entities = 50
+
+    self.check_get_splits(query, num_splits, num_entities)
+
+  def test_get_splits_with_batch_size_exact_multiple(self):
+    """Test get_splits when num scatter keys is a multiple of batch size."""
+    query = self.create_query(kinds=['shakespeare-demo'])
+    num_splits = 4
+    num_entities = 400
+
+    self.check_get_splits(query, num_splits, num_entities)
+
+  def test_get_splits_with_large_batch_size(self):
+    """Test get_splits when all scatter keys are retured in a single req."""
+    query = self.create_query(kinds=['shakespeare-demo'])
+    num_splits = 4
+    num_entities = 400
+
+    self.check_get_splits(query, num_splits, num_entities)
+
+  def test_get_splits_with_num_splits_gt_entities(self):
+    query = self.create_query(kinds=['shakespeare-demo'])
+    num_splits = 10
+    num_entities = 4
+
+    self.check_get_splits(query, num_splits, num_entities)
+
   def test_id_or_name(self):
     id_ = query_splitter.IdOrName(1)
     self.assertEqual(1, id_.id)
@@ -221,8 +262,5 @@ class QuerySplitterTest(QuerySplitterTestBase):
     self.assertEqual(expected_sort, keys)
 
 
-# Hide base class from collection by nose.
-del QuerySplitterTestBase
-
 if __name__ == '__main__':
   unittest.main()
diff --git a/sdks/python/apache_beam/io/gcp/datastore/v1new/types.py b/sdks/python/apache_beam/io/gcp/datastore/v1new/types.py
index 29fccac4229..ab9a18dd637 100644
--- a/sdks/python/apache_beam/io/gcp/datastore/v1new/types.py
+++ b/sdks/python/apache_beam/io/gcp/datastore/v1new/types.py
@@ -17,8 +17,6 @@
 
 """
 Beam Datastore types.
-
-This module is experimental, no backwards compatibility guarantees.
 """
 
 # pytype: skip-file
@@ -26,6 +24,11 @@ This module is experimental, no backwards compatibility guarantees.
 from __future__ import absolute_import
 
 import copy
+from typing import Iterable
+from typing import List
+from typing import Optional
+from typing import Text
+from typing import Union
 
 from google.cloud.datastore import entity
 from google.cloud.datastore import key
@@ -152,7 +155,12 @@ class Query(object):
 
 
 class Key(object):
-  def __init__(self, path_elements, parent=None, project=None, namespace=None):
+  def __init__(self,
+               path_elements,  # type: List[Union[Text, int]]
+               parent=None,  # type: Optional[Key]
+               project=None,  # type: Optional[Text]
+               namespace=None  # type: Optional[Text]
+               ):
     """
     Represents a Datastore key.
 
@@ -223,7 +231,11 @@ class Key(object):
 
 
 class Entity(object):
-  def __init__(self, key, exclude_from_indexes=()):
+  def __init__(
+      self,
+      key,  # type: Key
+      exclude_from_indexes=()  # type: Iterable[str]
+  ):
     """
     Represents a Datastore entity.
 
diff --git a/sdks/python/apache_beam/io/gcp/datastore/v1new/types_test.py b/sdks/python/apache_beam/io/gcp/datastore/v1new/types_test.py
index ea5f7d27b65..c92da4200bc 100644
--- a/sdks/python/apache_beam/io/gcp/datastore/v1new/types_test.py
+++ b/sdks/python/apache_beam/io/gcp/datastore/v1new/types_test.py
@@ -37,8 +37,7 @@ try:
   from apache_beam.io.gcp.datastore.v1new.types import Key
   from apache_beam.io.gcp.datastore.v1new.types import Query
   from apache_beam.options.value_provider import StaticValueProvider
-# TODO(BEAM-4543): Remove TypeError once googledatastore dependency is removed.
-except (ImportError, TypeError):
+except ImportError:
   client = None
 
 _LOGGER = logging.getLogger(__name__)
diff --git a/sdks/python/apache_beam/io/gcp/datastore/v1/util.py b/sdks/python/apache_beam/io/gcp/datastore/v1new/util.py
similarity index 100%
rename from sdks/python/apache_beam/io/gcp/datastore/v1/util.py
rename to sdks/python/apache_beam/io/gcp/datastore/v1new/util.py
diff --git a/sdks/python/apache_beam/io/gcp/datastore/v1/util_test.py b/sdks/python/apache_beam/io/gcp/datastore/v1new/util_test.py
similarity index 98%
rename from sdks/python/apache_beam/io/gcp/datastore/v1/util_test.py
rename to sdks/python/apache_beam/io/gcp/datastore/v1new/util_test.py
index 31f7024a1f2..84bc9fabd1b 100644
--- a/sdks/python/apache_beam/io/gcp/datastore/v1/util_test.py
+++ b/sdks/python/apache_beam/io/gcp/datastore/v1new/util_test.py
@@ -22,7 +22,7 @@ from __future__ import absolute_import
 
 import unittest
 
-from apache_beam.io.gcp.datastore.v1 import util
+from apache_beam.io.gcp.datastore.v1new import util
 
 
 class MovingSumTest(unittest.TestCase):
diff --git a/sdks/python/apache_beam/io/gcp/datastore_write_it_pipeline.py b/sdks/python/apache_beam/io/gcp/datastore_write_it_pipeline.py
deleted file mode 100644
index f1136697c13..00000000000
--- a/sdks/python/apache_beam/io/gcp/datastore_write_it_pipeline.py
+++ /dev/null
@@ -1,213 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#    http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-
-"""A job that write Entries into Datastore.
-
-The pipelines behave in the steps below.
-
-  1. Create and write Entities to Datastore
-  2. (Optional) If read limit was provided,
-     read it and confirm that the expected Entities were read.
-  3. Query the written Entities and verify result.
-  4. Delete Entries.
-  5. Query the written Entities, verify no results.
-"""
-
-# pytype: skip-file
-
-from __future__ import absolute_import
-
-import argparse
-import hashlib
-import logging
-import uuid
-
-import apache_beam as beam
-from apache_beam.io.gcp.datastore.v1.datastoreio import DeleteFromDatastore
-from apache_beam.io.gcp.datastore.v1.datastoreio import ReadFromDatastore
-from apache_beam.io.gcp.datastore.v1.datastoreio import WriteToDatastore
-from apache_beam.options.pipeline_options import GoogleCloudOptions
-from apache_beam.options.pipeline_options import PipelineOptions
-from apache_beam.testing.test_pipeline import TestPipeline
-from apache_beam.testing.util import assert_that
-from apache_beam.testing.util import equal_to
-
-# Protect against environments where Datastore library is not available.
-# pylint: disable=wrong-import-order, wrong-import-position
-try:
-  from google.cloud.proto.datastore.v1 import entity_pb2
-  from google.cloud.proto.datastore.v1 import query_pb2
-  from googledatastore import helper as datastore_helper
-  from googledatastore import PropertyFilter
-except ImportError:
-  pass
-# pylint: enable=wrong-import-order, wrong-import-position
-# pylint: enable=ungrouped-imports
-
-_LOGGER = logging.getLogger(__name__)
-
-
-def new_pipeline_with_job_name(pipeline_options, job_name, suffix):
-  """Create a pipeline with the given job_name and a suffix."""
-  gcp_options = pipeline_options.view_as(GoogleCloudOptions)
-  # DirectRunner doesn't have a job name.
-  if job_name:
-    gcp_options.job_name = job_name + suffix
-
-  return TestPipeline(options=pipeline_options)
-
-
-class EntityWrapper(object):
-  """Create a Cloud Datastore entity from the given string."""
-  def __init__(self, kind, namespace, ancestor):
-    self._kind = kind
-    self._namespace = namespace
-    self._ancestor = ancestor
-
-  def make_entity(self, content):
-    """Create entity from given string."""
-    entity = entity_pb2.Entity()
-    if self._namespace is not None:
-      entity.key.partition_id.namespace_id = self._namespace
-
-    # All entities created will have the same ancestor
-    datastore_helper.add_key_path(
-        entity.key,
-        self._kind,
-        self._ancestor,
-        self._kind,
-        hashlib.sha1(content).hexdigest())
-
-    datastore_helper.add_properties(entity, {'content': str(content)})
-    return entity
-
-
-def make_ancestor_query(kind, namespace, ancestor):
-  """Creates a Cloud Datastore ancestor query."""
-  ancestor_key = entity_pb2.Key()
-  datastore_helper.add_key_path(ancestor_key, kind, ancestor)
-  if namespace is not None:
-    ancestor_key.partition_id.namespace_id = namespace
-
-  query = query_pb2.Query()
-  query.kind.add().name = kind
-
-  datastore_helper.set_property_filter(
-      query.filter, '__key__', PropertyFilter.HAS_ANCESTOR, ancestor_key)
-
-  return query
-
-
-def run(argv=None):
-  """Main entry point."""
-
-  parser = argparse.ArgumentParser()
-
-  parser.add_argument(
-      '--kind', dest='kind', default='writereadtest', help='Datastore Kind')
-  parser.add_argument(
-      '--num_entities',
-      dest='num_entities',
-      type=int,
-      required=True,
-      help='Number of entities to write')
-  parser.add_argument(
-      '--limit',
-      dest='limit',
-      type=int,
-      help='Limit of number of entities to write')
-
-  known_args, pipeline_args = parser.parse_known_args(argv)
-  pipeline_options = PipelineOptions(pipeline_args)
-  gcloud_options = pipeline_options.view_as(GoogleCloudOptions)
-  job_name = gcloud_options.job_name
-  kind = known_args.kind
-  num_entities = known_args.num_entities
-  project = gcloud_options.project
-  # a random ancesor key
-  ancestor = str(uuid.uuid4())
-  query = make_ancestor_query(kind, None, ancestor)
-
-  # Pipeline 1: Create and write the specified number of Entities to the
-  # Cloud Datastore.
-  _LOGGER.info('Writing %s entities to %s', num_entities, project)
-  p = new_pipeline_with_job_name(pipeline_options, job_name, '-write')
-
-  # pylint: disable=expression-not-assigned
-  (
-      p
-      | 'Input' >> beam.Create(list(range(known_args.num_entities)))
-      | 'To String' >> beam.Map(str)
-      |
-      'To Entity' >> beam.Map(EntityWrapper(kind, None, ancestor).make_entity)
-      | 'Write to Datastore' >> WriteToDatastore(project))
-
-  p.run()
-
-  # Optional Pipeline 2: If a read limit was provided, read it and confirm
-  # that the expected entities were read.
-  if known_args.limit is not None:
-    _LOGGER.info(
-        'Querying a limited set of %s entities and verifying count.',
-        known_args.limit)
-    p = new_pipeline_with_job_name(pipeline_options, job_name, '-verify-limit')
-    query_with_limit = query_pb2.Query()
-    query_with_limit.CopyFrom(query)
-    query_with_limit.limit.value = known_args.limit
-    entities = p | 'read from datastore' >> ReadFromDatastore(
-        project, query_with_limit)
-    assert_that(
-        entities | beam.combiners.Count.Globally(),
-        equal_to([known_args.limit]))
-
-    p.run()
-
-  # Pipeline 3: Query the written Entities and verify result.
-  _LOGGER.info('Querying entities, asserting they match.')
-  p = new_pipeline_with_job_name(pipeline_options, job_name, '-verify')
-  entities = p | 'read from datastore' >> ReadFromDatastore(project, query)
-
-  assert_that(
-      entities | beam.combiners.Count.Globally(), equal_to([num_entities]))
-
-  p.run()
-
-  # Pipeline 4: Delete Entities.
-  _LOGGER.info('Deleting entities.')
-  p = new_pipeline_with_job_name(pipeline_options, job_name, '-delete')
-  entities = p | 'read from datastore' >> ReadFromDatastore(project, query)
-  # pylint: disable=expression-not-assigned
-  (
-      entities
-      | 'To Keys' >> beam.Map(lambda entity: entity.key)
-      | 'Delete keys' >> DeleteFromDatastore(project))
-
-  p.run()
-
-  # Pipeline 5: Query the written Entities, verify no results.
-  _LOGGER.info('Querying for the entities to make sure there are none present.')
-  p = new_pipeline_with_job_name(pipeline_options, job_name, '-verify-deleted')
-  entities = p | 'read from datastore' >> ReadFromDatastore(project, query)
-
-  assert_that(entities | beam.combiners.Count.Globally(), equal_to([0]))
-
-  p.run()
-
-
-if __name__ == '__main__':
-  logging.getLogger().setLevel(logging.INFO)
-  run()
diff --git a/sdks/python/apache_beam/io/gcp/datastore_write_it_test.py b/sdks/python/apache_beam/io/gcp/datastore_write_it_test.py
deleted file mode 100644
index 4961da01c14..00000000000
--- a/sdks/python/apache_beam/io/gcp/datastore_write_it_test.py
+++ /dev/null
@@ -1,86 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#    http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-
-"""An integration test for datastore_write_it_pipeline
-
-This test creates entities and writes them to Cloud Datastore. Subsequently,
-these entities are read from Cloud Datastore, compared to the expected value
-for the entity, and deleted.
-
-There is no output; instead, we use `assert_that` transform to verify the
-results in the pipeline.
-"""
-
-# pytype: skip-file
-
-from __future__ import absolute_import
-
-import logging
-import os
-import random
-import sys
-import unittest
-from datetime import datetime
-
-from hamcrest.core.core.allof import all_of
-from nose.plugins.attrib import attr
-
-from apache_beam.testing.pipeline_verifiers import PipelineStateMatcher
-from apache_beam.testing.test_pipeline import TestPipeline
-
-try:
-  from apache_beam.io.gcp import datastore_write_it_pipeline
-except TypeError:
-  datastore_write_it_pipeline = None  # type: ignore
-
-
-@unittest.skipIf(
-    sys.version_info[0] == 3 and os.environ.get('RUN_SKIPPED_PY3_TESTS') != '1',
-    'This test still needs to be fixed on Python 3'
-    'TODO: BEAM-4543')
-class DatastoreWriteIT(unittest.TestCase):
-
-  NUM_ENTITIES = 1001
-  LIMIT = 500
-
-  def run_datastore_write(self, limit=None):
-    test_pipeline = TestPipeline(is_integration_test=True)
-    current_time = datetime.now().strftime("%m%d%H%M%S")
-    seed = random.randint(0, 100000)
-    kind = 'testkind%s%d' % (current_time, seed)
-    pipeline_verifiers = [PipelineStateMatcher()]
-    extra_opts = {
-        'kind': kind,
-        'num_entities': self.NUM_ENTITIES,
-        'on_success_matcher': all_of(*pipeline_verifiers)
-    }
-    if limit is not None:
-      extra_opts['limit'] = limit
-
-    datastore_write_it_pipeline.run(
-        test_pipeline.get_full_options_as_args(**extra_opts))
-
-  @attr('IT')
-  @unittest.skipIf(
-      datastore_write_it_pipeline is None, 'GCP dependencies are not installed')
-  def test_datastore_write_limit(self):
-    self.run_datastore_write(limit=self.LIMIT)
-
-
-if __name__ == '__main__':
-  logging.getLogger().setLevel(logging.INFO)
-  unittest.main()
diff --git a/sdks/python/container/base_image_requirements.txt b/sdks/python/container/base_image_requirements.txt
index 8c63566c32f..49375dc31ec 100644
--- a/sdks/python/container/base_image_requirements.txt
+++ b/sdks/python/container/base_image_requirements.txt
@@ -46,10 +46,8 @@ typing-extensions==3.7.4.1
 
 # GCP extra features
 google-apitools==0.5.28
-googledatastore==7.0.2;python_version<"3.0"
 google-cloud-pubsub==1.0.2
 google-cloud-bigquery==1.24.0
-proto-google-cloud-datastore-v1==0.90.4;python_version<"3.0"
 google-cloud-bigtable==1.0.0
 google-cloud-core==1.1.0
 google-cloud-datastore==1.7.4
diff --git a/sdks/python/scripts/generate_pydoc.sh b/sdks/python/scripts/generate_pydoc.sh
index 6d6df820e48..c048e993d80 100755
--- a/sdks/python/scripts/generate_pydoc.sh
+++ b/sdks/python/scripts/generate_pydoc.sh
@@ -58,9 +58,6 @@ excluded_patterns=(
     apache_beam/examples/
     apache_beam/internal/clients/
     apache_beam/io/gcp/internal/
-    # TODO(BEAM-4543): Remove datastore paths once they no longer exist.
-    apache_beam/io/gcp/datastore/v1/
-    apache_beam/io/gcp/datastore_write_it_*
     apache_beam/io/gcp/tests/
     apache_beam/metrics/execution.*
     apache_beam/runners/common.*
@@ -154,7 +151,6 @@ ignore_identifiers = [
   'apache_beam.coders.coders.FastCoder',
   'apache_beam.io._AvroSource',
   'apache_beam.io.gcp.bigquery.RowAsDictJsonCoder',
-  'apache_beam.io.gcp.datastore.v1.datastoreio._Mutate',
   'apache_beam.io.gcp.datastore.v1new.datastoreio._Mutate',
   'apache_beam.io.gcp.datastore.v1new.datastoreio.DatastoreMutateFn',
   'apache_beam.io.gcp.internal.clients.bigquery.'
diff --git a/sdks/python/setup.py b/sdks/python/setup.py
index 2ece426f13d..0dd0aaa6188 100644
--- a/sdks/python/setup.py
+++ b/sdks/python/setup.py
@@ -198,16 +198,12 @@ REQUIRED_TEST_PACKAGES = [
 GCP_REQUIREMENTS = [
     'cachetools>=3.1.0,<4',
     'google-apitools>=0.5.28,<0.5.29',
-    # [BEAM-4543] googledatastore is not supported in Python 3.
-    'googledatastore>=7.0.1,<7.1; python_version < "3.0"',
     'google-cloud-datastore>=1.7.1,<1.8.0',
     'google-cloud-pubsub>=0.39.0,<1.1.0',
     # GCP packages required by tests
     'google-cloud-bigquery>=1.6.0,<=1.24.0',
     'google-cloud-core>=0.28.1,<2',
     'google-cloud-bigtable>=0.31.1,<1.1.0',
-    # [BEAM-4543] googledatastore is not supported in Python 3.
-    'proto-google-cloud-datastore-v1>=0.90.0,<=0.90.4; python_version < "3.0"',
     'google-cloud-spanner>=1.13.0,<1.14.0',
     'grpcio-gcp>=0.2.2,<1',
     # GCP Packages required by ML functionality
diff --git a/sdks/python/tox.ini b/sdks/python/tox.ini
index 241851c55e5..da689a7a66e 100644
--- a/sdks/python/tox.ini
+++ b/sdks/python/tox.ini
@@ -133,12 +133,6 @@ extras = test,gcp,aws
 commands =
   python apache_beam/examples/complete/autocomplete_test.py
   {toxinidir}/scripts/run_pytest.sh {envname} "{posargs}"
-  # Old and new Datastore client unit tests cannot be run in the same process
-  # due to conflicting protobuf modules.
-  # TODO(BEAM-4543): Remove these separate pytest invocations once the
-  # googledatastore dependency is removed.
-  pytest -o junit_suite_name={envname}_v1 --junitxml=pytest_{envname}_v1.xml apache_beam/io/gcp/datastore/v1
-  pytest -o junit_suite_name={envname}_v1new --junitxml=pytest_{envname}_v1new.xml apache_beam/io/gcp/datastore/v1new
 
 [testenv:py35-cloud]
 extras = test,gcp,aws
