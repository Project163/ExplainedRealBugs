diff --git a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/ReaderCache.java b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/ReaderCache.java
index 33c77ec18de..bf65cfac1d4 100644
--- a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/ReaderCache.java
+++ b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/ReaderCache.java
@@ -52,11 +52,13 @@ class ReaderCache {
   private static class CacheEntry {
 
     final UnboundedSource.UnboundedReader<?> reader;
-    final long token;
+    final long cacheToken;
+    final long workToken;
 
-    CacheEntry(UnboundedSource.UnboundedReader<?> reader, long token) {
+    CacheEntry(UnboundedSource.UnboundedReader<?> reader, long cacheToken, long workToken) {
       this.reader = reader;
-      this.token = token;
+      this.cacheToken = cacheToken;
+      this.workToken = workToken;
     }
   }
 
@@ -99,15 +101,17 @@ class ReaderCache {
    * Return null in case of a cache miss.
    */
   UnboundedSource.UnboundedReader<?> acquireReader(
-      WindmillComputationKey computationKey, long cacheToken) {
+      WindmillComputationKey computationKey, long cacheToken, long workToken) {
     CacheEntry entry = cache.asMap().remove(computationKey);
 
     cache.cleanUp();
 
     if (entry != null) {
-      if (entry.token == cacheToken) {
+      if (entry.cacheToken == cacheToken && workToken > entry.workToken) {
         return entry.reader;
-      } else { // new cacheToken invalidates old one. close the reader.
+      } else {
+        // new cacheToken invalidates old one or this is a retried or stale request,
+        // close the reader.
         closeReader(computationKey, entry);
       }
     }
@@ -118,9 +122,10 @@ class ReaderCache {
   void cacheReader(
       WindmillComputationKey computationKey,
       long cacheToken,
+      long workToken,
       UnboundedSource.UnboundedReader<?> reader) {
     CacheEntry existing =
-        cache.asMap().putIfAbsent(computationKey, new CacheEntry(reader, cacheToken));
+        cache.asMap().putIfAbsent(computationKey, new CacheEntry(reader, cacheToken, workToken));
     Preconditions.checkState(existing == null, "Overwriting existing readers is not allowed");
     cache.cleanUp();
   }
@@ -128,6 +133,6 @@ class ReaderCache {
   /** If a reader is cached for this key, remove and close it. */
   void invalidateReader(WindmillComputationKey computationKey) {
     // use an invalid cache token that will trigger close.
-    acquireReader(computationKey, -1L);
+    acquireReader(computationKey, -1L, -1);
   }
 }
diff --git a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorker.java b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorker.java
index 6b7c91687f5..6c127c84f84 100644
--- a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorker.java
+++ b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorker.java
@@ -1609,6 +1609,9 @@ public class StreamingDataflowWorker {
         request,
         (Windmill.CommitStatus status) -> {
           if (status != Windmill.CommitStatus.OK) {
+            readerCache.invalidateReader(
+                WindmillComputationKey.create(
+                    state.computationId, request.getKey(), request.getShardingKey()));
             stateCache
                 .forComputation(state.computationId)
                 .invalidate(request.getKey(), request.getShardingKey());
diff --git a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/StreamingModeExecutionContext.java b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/StreamingModeExecutionContext.java
index b221571bd06..48b78a3a758 100644
--- a/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/StreamingModeExecutionContext.java
+++ b/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/StreamingModeExecutionContext.java
@@ -344,7 +344,8 @@ public class StreamingModeExecutionContext extends DataflowExecutionContext<Step
    * The caller is responsible for the reader and should appropriately close it as required.
    */
   public UnboundedSource.UnboundedReader<?> getCachedReader() {
-    return readerCache.acquireReader(getComputationKey(), getWork().getCacheToken());
+    return readerCache.acquireReader(
+        getComputationKey(), getWork().getCacheToken(), getWork().getWorkToken());
   }
 
   public void setActiveReader(UnboundedSource.UnboundedReader<?> reader) {
@@ -429,7 +430,8 @@ public class StreamingModeExecutionContext extends DataflowExecutionContext<Step
       }
       outputBuilder.setSourceBacklogBytes(backlogBytes);
 
-      readerCache.cacheReader(getComputationKey(), getWork().getCacheToken(), activeReader);
+      readerCache.cacheReader(
+          getComputationKey(), getWork().getCacheToken(), getWork().getWorkToken(), activeReader);
       activeReader = null;
     }
     return callbacks;
diff --git a/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/FakeWindmillServer.java b/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/FakeWindmillServer.java
index 0f92665d5be..1a004e2b046 100644
--- a/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/FakeWindmillServer.java
+++ b/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/FakeWindmillServer.java
@@ -372,6 +372,11 @@ class FakeWindmillServer extends WindmillServerStub {
     return commitsReceived;
   }
 
+  public void clearCommitsReceived() {
+    commitsRequested = 0;
+    commitsReceived.clear();
+  }
+
   public void waitForDroppedCommits(int droppedCommits) {
     LOG.debug("waitForDroppedCommits: {}", droppedCommits);
     int maxTries = 10;
diff --git a/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/ReaderCacheTest.java b/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/ReaderCacheTest.java
index ce8f58c806e..f9ab12c41b0 100644
--- a/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/ReaderCacheTest.java
+++ b/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/ReaderCacheTest.java
@@ -69,20 +69,25 @@ public class ReaderCacheTest {
   public void testReaderCache() throws IOException {
     // Test basic caching expectations
 
-    readerCache.cacheReader(WindmillComputationKey.create(C_ID, KEY_1, SHARDING_KEY), 1, reader1);
-    readerCache.cacheReader(WindmillComputationKey.create(C_ID, KEY_2, SHARDING_KEY), 2, reader2);
+    readerCache.cacheReader(
+        WindmillComputationKey.create(C_ID, KEY_1, SHARDING_KEY), 1, 0, reader1);
+    readerCache.cacheReader(
+        WindmillComputationKey.create(C_ID, KEY_2, SHARDING_KEY), 2, 0, reader2);
 
     assertEquals(
         reader1,
-        readerCache.acquireReader(WindmillComputationKey.create(C_ID, KEY_1, SHARDING_KEY), 1));
+        readerCache.acquireReader(WindmillComputationKey.create(C_ID, KEY_1, SHARDING_KEY), 1, 1));
     assertNull(
-        readerCache.acquireReader(WindmillComputationKey.create(C_ID_1, KEY_1, SHARDING_KEY), 1));
+        readerCache.acquireReader(
+            WindmillComputationKey.create(C_ID_1, KEY_1, SHARDING_KEY), 1, 1));
     assertNull(
-        readerCache.acquireReader(WindmillComputationKey.create(C_ID, KEY_1, SHARDING_KEY_1), 1));
+        readerCache.acquireReader(
+            WindmillComputationKey.create(C_ID, KEY_1, SHARDING_KEY_1), 1, 1));
 
     // Trying to override existing reader should throw
     try {
-      readerCache.cacheReader(WindmillComputationKey.create(C_ID, KEY_2, SHARDING_KEY), 2, reader1);
+      readerCache.cacheReader(
+          WindmillComputationKey.create(C_ID, KEY_2, SHARDING_KEY), 2, 2, reader1);
       fail("Exception should have been thrown");
     } catch (RuntimeException expected) {
       // expected
@@ -91,31 +96,38 @@ public class ReaderCacheTest {
     // And it should not have overwritten the old value
     assertEquals(
         reader2,
-        readerCache.acquireReader(WindmillComputationKey.create(C_ID, KEY_2, SHARDING_KEY), 2));
+        readerCache.acquireReader(WindmillComputationKey.create(C_ID, KEY_2, SHARDING_KEY), 2, 3));
 
     assertNull(
         "acquireReader(WindmillComputationKey.create() should remove matching entry",
-        readerCache.acquireReader(WindmillComputationKey.create(C_ID, KEY_2, SHARDING_KEY), 2));
+        readerCache.acquireReader(WindmillComputationKey.create(C_ID, KEY_2, SHARDING_KEY), 2, 4));
 
     // Make sure computationId is part of the cache key
-    readerCache.cacheReader(WindmillComputationKey.create(C_ID_1, KEY_1, SHARDING_KEY), 1, reader2);
+    readerCache.cacheReader(
+        WindmillComputationKey.create(C_ID_1, KEY_1, SHARDING_KEY), 1, 5, reader2);
     assertEquals(
         reader2,
-        readerCache.acquireReader(WindmillComputationKey.create(C_ID_1, KEY_1, SHARDING_KEY), 1));
+        readerCache.acquireReader(
+            WindmillComputationKey.create(C_ID_1, KEY_1, SHARDING_KEY), 1, 6));
 
     // Make sure sharding key is part of the cache key
-    readerCache.cacheReader(WindmillComputationKey.create(C_ID, KEY_1, SHARDING_KEY_1), 1, reader3);
+    readerCache.cacheReader(
+        WindmillComputationKey.create(C_ID, KEY_1, SHARDING_KEY_1), 1, 0, reader3);
     assertEquals(
         reader3,
-        readerCache.acquireReader(WindmillComputationKey.create(C_ID, KEY_1, SHARDING_KEY_1), 1));
+        readerCache.acquireReader(
+            WindmillComputationKey.create(C_ID, KEY_1, SHARDING_KEY_1), 1, 1));
   }
 
   @Test
   public void testInvalidation() throws IOException {
 
-    readerCache.cacheReader(WindmillComputationKey.create(C_ID, KEY_1, SHARDING_KEY), 1, reader1);
-    readerCache.cacheReader(WindmillComputationKey.create(C_ID, KEY_2, SHARDING_KEY), 2, reader2);
-    readerCache.cacheReader(WindmillComputationKey.create(C_ID, KEY_2, SHARDING_KEY_1), 2, reader3);
+    readerCache.cacheReader(
+        WindmillComputationKey.create(C_ID, KEY_1, SHARDING_KEY), 1, 0, reader1);
+    readerCache.cacheReader(
+        WindmillComputationKey.create(C_ID, KEY_2, SHARDING_KEY), 2, 0, reader2);
+    readerCache.cacheReader(
+        WindmillComputationKey.create(C_ID, KEY_2, SHARDING_KEY_1), 2, 0, reader3);
 
     readerCache.invalidateReader(WindmillComputationKey.create(C_ID, KEY_1, SHARDING_KEY));
     verify(reader1).close();
@@ -142,8 +154,10 @@ public class ReaderCacheTest {
     // Create a cache with short expiry period.
     ReaderCache readerCache = new ReaderCache(cacheDuration);
 
-    readerCache.cacheReader(WindmillComputationKey.create(C_ID, KEY_1, SHARDING_KEY), 1, reader1);
-    readerCache.cacheReader(WindmillComputationKey.create(C_ID, KEY_2, SHARDING_KEY), 2, reader2);
+    readerCache.cacheReader(
+        WindmillComputationKey.create(C_ID, KEY_1, SHARDING_KEY), 1, 0, reader1);
+    readerCache.cacheReader(
+        WindmillComputationKey.create(C_ID, KEY_2, SHARDING_KEY), 2, 0, reader2);
 
     Stopwatch stopwatch = Stopwatch.createStarted();
     while (stopwatch.elapsed(TimeUnit.MILLISECONDS) < 2 * cacheDuration.getMillis()) {
@@ -151,8 +165,33 @@ public class ReaderCacheTest {
     }
 
     // Trigger clean up with a lookup, non-existing key is ok.
-    readerCache.acquireReader(WindmillComputationKey.create(C_ID, KEY_3, SHARDING_KEY), 3);
+    readerCache.acquireReader(WindmillComputationKey.create(C_ID, KEY_3, SHARDING_KEY), 3, 0);
 
     verify(reader1).close();
   }
+
+  @Test
+  public void testReaderCacheRetries() throws IOException, InterruptedException {
+    ReaderCache readerCache = new ReaderCache();
+
+    readerCache.cacheReader(
+        WindmillComputationKey.create(C_ID, KEY_1, SHARDING_KEY), 1, 1, reader1);
+    // Same cache token should not recover the reader from the cache.
+    assertNull(
+        readerCache.acquireReader(WindmillComputationKey.create(C_ID, KEY_1, SHARDING_KEY), 1, 1));
+
+    readerCache.cacheReader(
+        WindmillComputationKey.create(C_ID, KEY_1, SHARDING_KEY), 1, 1, reader2);
+    assertEquals(
+        reader2,
+        readerCache.acquireReader(WindmillComputationKey.create(C_ID, KEY_1, SHARDING_KEY), 1, 20));
+    readerCache.cacheReader(
+        WindmillComputationKey.create(C_ID, KEY_1, SHARDING_KEY), 1, 20, reader2);
+    // Stale cache token should not recover the reader from the cache.
+    assertNull(
+        readerCache.acquireReader(WindmillComputationKey.create(C_ID, KEY_1, SHARDING_KEY), 1, 3));
+
+    verify(reader1).close();
+    verify(reader2).close();
+  }
 }
diff --git a/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorkerTest.java b/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorkerTest.java
index 5bff75a7987..0231eb0825a 100644
--- a/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorkerTest.java
+++ b/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorkerTest.java
@@ -2228,6 +2228,136 @@ public class StreamingDataflowWorkerTest {
     assertThat(finalizeTracker, contains(0));
   }
 
+  // Regression test to ensure that a reader is not used from the cache
+  // on work item retry.
+  @Test
+  public void testUnboundedSourceWorkRetry() throws Exception {
+    List<Integer> finalizeTracker = Lists.newArrayList();
+    TestCountingSource.setFinalizeTracker(finalizeTracker);
+
+    FakeWindmillServer server = new FakeWindmillServer(errorCollector);
+    StreamingDataflowWorkerOptions options = createTestingPipelineOptions(server);
+    options.setWorkerCacheMb(0); // Disable state cache so it doesn't detect retry.
+    StreamingDataflowWorker worker =
+        makeWorker(makeUnboundedSourcePipeline(), options, false /* publishCounters */);
+    worker.start();
+
+    // Test new key.
+    Windmill.GetWorkResponse work =
+        buildInput(
+            "work {"
+                + "  computation_id: \"computation\""
+                + "  input_data_watermark: 0"
+                + "  work {"
+                + "    key: \"0000000000000001\""
+                + "    sharding_key: 1"
+                + "    work_token: 1"
+                + "    cache_token: 1"
+                + "  }"
+                + "}",
+            null);
+    server.addWorkToOffer(work);
+
+    Map<Long, Windmill.WorkItemCommitRequest> result = server.waitForAndGetCommits(1);
+    Iterable<CounterUpdate> counters = worker.buildCounters();
+    Windmill.WorkItemCommitRequest commit = result.get(1L);
+    UnsignedLong finalizeId =
+        UnsignedLong.fromLongBits(commit.getSourceStateUpdates().getFinalizeIds(0));
+
+    Windmill.WorkItemCommitRequest expectedCommit =
+        setMessagesMetadata(
+                PaneInfo.NO_FIRING,
+                CoderUtils.encodeToByteArray(
+                    CollectionCoder.of(GlobalWindow.Coder.INSTANCE),
+                    Arrays.asList(GlobalWindow.INSTANCE)),
+                parseCommitRequest(
+                    "key: \"0000000000000001\" "
+                        + "sharding_key: 1 "
+                        + "work_token: 1 "
+                        + "cache_token: 1 "
+                        + "source_backlog_bytes: 7 "
+                        + "output_messages {"
+                        + "  destination_stream_id: \"out\""
+                        + "  bundles {"
+                        + "    key: \"0000000000000001\""
+                        + "    messages {"
+                        + "      timestamp: 0"
+                        + "      data: \"0:0\""
+                        + "    }"
+                        + "    messages_ids: \"\""
+                        + "  }"
+                        + "} "
+                        + "source_state_updates {"
+                        + "  state: \"\000\""
+                        + "  finalize_ids: "
+                        + finalizeId
+                        + "} "
+                        + "source_watermark: 1000"))
+            .build();
+
+    assertThat(commit, equalTo(expectedCommit));
+    assertEquals(
+        18L, splitIntToLong(getCounter(counters, "dataflow_input_size-computation").getInteger()));
+
+    // Test retry of work item, it should return the same result and not start the reader from the
+    // position it was left at.
+    server.clearCommitsReceived();
+    server.addWorkToOffer(work);
+    result = server.waitForAndGetCommits(1);
+    commit = result.get(1L);
+    finalizeId = UnsignedLong.fromLongBits(commit.getSourceStateUpdates().getFinalizeIds(0));
+    Windmill.WorkItemCommitRequest.Builder commitBuilder = expectedCommit.toBuilder();
+    commitBuilder
+        .getSourceStateUpdatesBuilder()
+        .setFinalizeIds(0, commit.getSourceStateUpdates().getFinalizeIds(0));
+    expectedCommit = commitBuilder.build();
+    assertThat(commit, equalTo(expectedCommit));
+
+    // Continue with processing.
+    server.addWorkToOffer(
+        buildInput(
+            "work {"
+                + "  computation_id: \"computation\""
+                + "  input_data_watermark: 0"
+                + "  work {"
+                + "    key: \"0000000000000001\""
+                + "    sharding_key: 1"
+                + "    work_token: 2"
+                + "    cache_token: 1"
+                + "    source_state {"
+                + "      state: \"\001\""
+                + "      finalize_ids: "
+                + finalizeId
+                + "    } "
+                + "  }"
+                + "}",
+            null));
+
+    result = server.waitForAndGetCommits(1);
+
+    commit = result.get(2L);
+    finalizeId = UnsignedLong.fromLongBits(commit.getSourceStateUpdates().getFinalizeIds(0));
+
+    assertThat(
+        commit,
+        equalTo(
+            parseCommitRequest(
+                    "key: \"0000000000000001\" "
+                        + "sharding_key: 1 "
+                        + "work_token: 2 "
+                        + "cache_token: 1 "
+                        + "source_backlog_bytes: 7 "
+                        + "source_state_updates {"
+                        + "  state: \"\000\""
+                        + "  finalize_ids: "
+                        + finalizeId
+                        + "} "
+                        + "source_watermark: 1000")
+                .build()));
+
+    assertThat(finalizeTracker, contains(0));
+  }
+
   private static class MockWork extends StreamingDataflowWorker.Work {
 
     public MockWork(long workToken) {
diff --git a/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/WorkerCustomSourcesTest.java b/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/WorkerCustomSourcesTest.java
index 806db0939f8..cf12da5230b 100644
--- a/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/WorkerCustomSourcesTest.java
+++ b/runners/google-cloud-dataflow-java/worker/src/test/java/org/apache/beam/runners/dataflow/worker/WorkerCustomSourcesTest.java
@@ -502,11 +502,12 @@ public class WorkerCustomSourcesTest {
     CounterSet counterSet = new CounterSet();
     StreamingModeExecutionStateRegistry executionStateRegistry =
         new StreamingModeExecutionStateRegistry(null);
+    ReaderCache readerCache = new ReaderCache();
     StreamingModeExecutionContext context =
         new StreamingModeExecutionContext(
             counterSet,
             "computationId",
-            new ReaderCache(),
+            readerCache,
             /*stateNameMap=*/ ImmutableMap.of(),
             /*stateCache=*/ null,
             StreamingStepMetricsContainer.createRegistry(),
@@ -530,7 +531,8 @@ public class WorkerCustomSourcesTest {
           "key",
           Windmill.WorkItem.newBuilder()
               .setKey(ByteString.copyFromUtf8("0000000000000001")) // key is zero-padded index.
-              .setWorkToken(0) // Required proto field, unused.
+              .setWorkToken(i) // Must be increasing across activations for cache to be used.
+              .setCacheToken(1)
               .setSourceState(
                   Windmill.SourceState.newBuilder().setState(state).build()) // Source state.
               .build(),
@@ -583,7 +585,11 @@ public class WorkerCustomSourcesTest {
       assertEquals(
           1, context.getOutputBuilder().getSourceStateUpdates().getFinalizeIdsList().size());
 
-      assertNotNull(context.getCachedReader());
+      assertNotNull(
+          readerCache.acquireReader(
+              context.getComputationKey(),
+              context.getWork().getCacheToken(),
+              context.getWorkToken() + 1));
       assertEquals(7L, context.getBacklogBytes());
     }
   }
