diff --git a/runners/spark/src/main/java/org/apache/beam/runners/spark/io/SparkUnboundedSource.java b/runners/spark/src/main/java/org/apache/beam/runners/spark/io/SparkUnboundedSource.java
index 162bca47fee..6b34590be1e 100644
--- a/runners/spark/src/main/java/org/apache/beam/runners/spark/io/SparkUnboundedSource.java
+++ b/runners/spark/src/main/java/org/apache/beam/runners/spark/io/SparkUnboundedSource.java
@@ -18,6 +18,8 @@
 
 package org.apache.beam.runners.spark.io;
 
+import java.io.Closeable;
+import java.io.IOException;
 import java.io.Serializable;
 import java.util.Collections;
 import org.apache.beam.runners.spark.SparkPipelineOptions;
@@ -32,6 +34,10 @@ import org.apache.beam.runners.spark.util.GlobalWatermarkHolder.SparkWatermarks;
 import org.apache.beam.sdk.io.Source;
 import org.apache.beam.sdk.io.UnboundedSource;
 import org.apache.beam.sdk.io.UnboundedSource.CheckpointMark;
+import org.apache.beam.sdk.metrics.Gauge;
+import org.apache.beam.sdk.metrics.Metrics;
+import org.apache.beam.sdk.metrics.MetricsContainer;
+import org.apache.beam.sdk.metrics.MetricsEnvironment;
 import org.apache.beam.sdk.transforms.windowing.BoundedWindow;
 import org.apache.beam.sdk.transforms.windowing.GlobalWindow;
 import org.apache.beam.sdk.util.WindowedValue;
@@ -105,7 +111,8 @@ public class SparkUnboundedSource {
     JavaDStream<Metadata> metadataDStream = mapWithStateDStream.map(new Tuple2MetadataFunction());
 
     // register ReadReportDStream to report information related to this read.
-    new ReadReportDStream(metadataDStream.dstream(), id, getSourceName(source, id)).register();
+    new ReadReportDStream(metadataDStream.dstream(), id, getSourceName(source, id), stepName)
+        .register();
 
     // output the actual (deserialized) stream.
     WindowedValue.FullWindowedValueCoder<T> coder =
@@ -148,18 +155,25 @@ public class SparkUnboundedSource {
    * <p>Updates {@link MetricsAccumulator} with metrics reported in the read.</p>
    */
   private static class ReadReportDStream extends DStream<BoxedUnit> {
+
+    private static final String READ_DURATION_MILLIS = "readDurationMillis";
+    private static final String NAMESPACE = "spark-runner.io";
+
     private final DStream<Metadata> parent;
     private final int inputDStreamId;
     private final String sourceName;
+    private final String stepName;
 
     ReadReportDStream(
         DStream<Metadata> parent,
         int inputDStreamId,
-        String sourceName) {
+        String sourceName,
+        String stepName) {
       super(parent.ssc(), JavaSparkContext$.MODULE$.<BoxedUnit>fakeClassTag());
       this.parent = parent;
       this.inputDStreamId = inputDStreamId;
       this.sourceName = sourceName;
+      this.stepName = stepName;
     }
 
     @Override
@@ -182,6 +196,7 @@ public class SparkUnboundedSource {
       SparkWatermarks sparkWatermark = null;
       Instant globalLowWatermarkForBatch = BoundedWindow.TIMESTAMP_MIN_VALUE;
       Instant globalHighWatermarkForBatch = BoundedWindow.TIMESTAMP_MIN_VALUE;
+      long maxReadDuration = 0;
       if (parentRDDOpt.isDefined()) {
         JavaRDD<Metadata> parentRDD = parentRDDOpt.get().toJavaRDD();
         for (Metadata metadata: parentRDD.collect()) {
@@ -196,6 +211,16 @@ public class SparkUnboundedSource {
               globalHighWatermarkForBatch.isBefore(partitionHighWatermark)
                   ? partitionHighWatermark : globalHighWatermarkForBatch;
           // Update metrics reported in the read
+          final Gauge gauge = Metrics.gauge(NAMESPACE, READ_DURATION_MILLIS);
+          final MetricsContainer container = metadata.getMetricsContainer().getContainer(stepName);
+          try (Closeable ignored = MetricsEnvironment.scopedMetricsContainer(container)) {
+            final long readDurationMillis = metadata.getReadDurationMillis();
+            if (readDurationMillis > maxReadDuration) {
+              gauge.set(readDurationMillis);
+            }
+          } catch (IOException e) {
+            throw new RuntimeException(e);
+          }
           metricsAccum.value().update(metadata.getMetricsContainer());
         }
 
@@ -235,14 +260,17 @@ public class SparkUnboundedSource {
     private final long numRecords;
     private final Instant lowWatermark;
     private final Instant highWatermark;
+    private final long readDurationMillis;
     private final SparkMetricsContainer metricsContainer;
 
     public Metadata(
         long numRecords,
         Instant lowWatermark,
         Instant highWatermark,
+        final long readDurationMillis,
         SparkMetricsContainer metricsContainer) {
       this.numRecords = numRecords;
+      this.readDurationMillis = readDurationMillis;
       this.metricsContainer = metricsContainer;
       this.lowWatermark = lowWatermark;
       this.highWatermark = highWatermark;
@@ -261,6 +289,10 @@ public class SparkUnboundedSource {
       return highWatermark;
     }
 
+    public long getReadDurationMillis() {
+      return readDurationMillis;
+    }
+
     SparkMetricsContainer getMetricsContainer() {
       return metricsContainer;
     }
diff --git a/runners/spark/src/main/java/org/apache/beam/runners/spark/stateful/StateSpecFunctions.java b/runners/spark/src/main/java/org/apache/beam/runners/spark/stateful/StateSpecFunctions.java
index 803fe458d1e..6d1b7c08313 100644
--- a/runners/spark/src/main/java/org/apache/beam/runners/spark/stateful/StateSpecFunctions.java
+++ b/runners/spark/src/main/java/org/apache/beam/runners/spark/stateful/StateSpecFunctions.java
@@ -114,6 +114,7 @@ public class StateSpecFunctions {
 
         SparkMetricsContainer sparkMetricsContainer = new SparkMetricsContainer();
         MetricsContainer metricsContainer = sparkMetricsContainer.getContainer(stepName);
+
         // Add metrics container to the scope of org.apache.beam.sdk.io.Source.Reader methods
         // since they may report metrics.
         try (Closeable ignored = MetricsEnvironment.scopedMetricsContainer(metricsContainer)) {
@@ -146,6 +147,9 @@ public class StateSpecFunctions {
 
         // create reader.
         BoundedSource.BoundedReader<T> reader;
+        Stopwatch stopwatch = Stopwatch.createStarted();
+        long readDurationMillis = 0;
+
         try {
           reader =
               microbatchSource.createReader(runtimeContext.getPipelineOptions(), checkpointMark);
@@ -155,14 +159,12 @@ public class StateSpecFunctions {
 
         // read microbatch as a serialized collection.
         final List<byte[]> readValues = new ArrayList<>();
-        final Instant watermark;
         WindowedValue.FullWindowedValueCoder<T> coder =
             WindowedValue.FullWindowedValueCoder.of(
                 source.getDefaultOutputCoder(),
                 GlobalWindow.Coder.INSTANCE);
         try {
           // measure how long a read takes per-partition.
-          Stopwatch stopwatch = Stopwatch.createStarted();
           boolean finished = !reader.start();
           while (!finished) {
             WindowedValue<T> wv = WindowedValue.of(reader.getCurrent(),
@@ -177,8 +179,12 @@ public class StateSpecFunctions {
 
           // close and checkpoint reader.
           reader.close();
-          LOG.info("Source id {} spent {} msec on reading.", microbatchSource.getId(),
-              stopwatch.stop().elapsed(TimeUnit.MILLISECONDS));
+          readDurationMillis = stopwatch.stop().elapsed(TimeUnit.MILLISECONDS);
+
+          LOG.info(
+              "Source id {} spent {} millis on reading.",
+              microbatchSource.getId(),
+              readDurationMillis);
 
           // if the Source does not supply a CheckpointMark skip updating the state.
           @SuppressWarnings("unchecked")
@@ -202,7 +208,12 @@ public class StateSpecFunctions {
 
         return new Tuple2<>(
             (Iterable<byte[]>) payload,
-            new Metadata(readValues.size(), lowWatermark, highWatermark, sparkMetricsContainer));
+            new Metadata(
+                readValues.size(),
+                lowWatermark,
+                highWatermark,
+                readDurationMillis,
+                sparkMetricsContainer));
 
         } catch (IOException e) {
           throw new RuntimeException(e);
