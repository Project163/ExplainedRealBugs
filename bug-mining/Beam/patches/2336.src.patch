diff --git a/sdks/python/apache_beam/runners/dataflow/internal/apiclient_test.py b/sdks/python/apache_beam/runners/dataflow/internal/apiclient_test.py
index 2824ba61cd0..64920647d67 100644
--- a/sdks/python/apache_beam/runners/dataflow/internal/apiclient_test.py
+++ b/sdks/python/apache_beam/runners/dataflow/internal/apiclient_test.py
@@ -21,6 +21,7 @@
 
 from __future__ import absolute_import
 
+import json
 import logging
 import sys
 import unittest
@@ -28,6 +29,7 @@ import unittest
 import mock
 
 from apache_beam.metrics.cells import DistributionData
+from apache_beam.options.pipeline_options import GoogleCloudOptions
 from apache_beam.options.pipeline_options import PipelineOptions
 from apache_beam.pipeline import Pipeline
 from apache_beam.portability import common_urns
@@ -1008,7 +1010,6 @@ class UtilTest(unittest.TestCase):
 
     assert encoding == 'utf8'
 
-  @unittest.skip("Enable once BEAM-1080 is fixed.")
   def test_graph_is_uploaded(self):
     pipeline_options = PipelineOptions([
         '--project',
@@ -1023,6 +1024,7 @@ class UtilTest(unittest.TestCase):
         'upload_graph'
     ])
     job = apiclient.Job(pipeline_options, FAKE_PIPELINE_URL)
+    pipeline_options.view_as(GoogleCloudOptions).no_auth = True
     client = apiclient.DataflowApplicationClient(pipeline_options)
     with mock.patch.object(client, 'stage_file', side_effect=None):
       with mock.patch.object(client, 'create_job_description',
@@ -1035,6 +1037,49 @@ class UtilTest(unittest.TestCase):
               mock.ANY, "dataflow_graph.json", mock.ANY)
           client.create_job_description.assert_called_once()
 
+  def test_template_file_generation_with_upload_graph(self):
+    pipeline_options = PipelineOptions([
+        '--project',
+        'test_project',
+        '--job_name',
+        'test_job_name',
+        '--temp_location',
+        'gs://test-location/temp',
+        '--experiments',
+        'upload_graph',
+        '--template_location',
+        'gs://test-location/template'
+    ])
+    job = apiclient.Job(pipeline_options, FAKE_PIPELINE_URL)
+    job.proto.steps.append(dataflow.Step(name='test_step_name'))
+
+    pipeline_options.view_as(GoogleCloudOptions).no_auth = True
+    client = apiclient.DataflowApplicationClient(pipeline_options)
+    with mock.patch.object(client, 'stage_file', side_effect=None):
+      with mock.patch.object(client, 'create_job_description',
+                             side_effect=None):
+        with mock.patch.object(client,
+                               'submit_job_description',
+                               side_effect=None):
+          client.create_job(job)
+
+          client.stage_file.assert_has_calls([
+              mock.call(mock.ANY, 'dataflow_graph.json', mock.ANY),
+              mock.call(mock.ANY, 'template', mock.ANY)
+          ])
+          client.create_job_description.assert_called_once()
+          # template is generated, but job should not be submitted to the
+          # service.
+          client.submit_job_description.assert_not_called()
+
+          template_filename = client.stage_file.call_args_list[-1][0][1]
+          self.assertTrue('template' in template_filename)
+          template_content = client.stage_file.call_args_list[-1][0][2].read(
+          ).decode('utf-8')
+          template_obj = json.loads(template_content)
+          self.assertFalse(template_obj.get('steps'))
+          self.assertTrue(template_obj['stepsLocation'])
+
   def test_stage_resources(self):
     pipeline_options = PipelineOptions([
         '--temp_location',
