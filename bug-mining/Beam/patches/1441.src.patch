diff --git a/sdks/python/apache_beam/io/gcp/datastore/v1new/datastoreio.py b/sdks/python/apache_beam/io/gcp/datastore/v1new/datastoreio.py
index 7ecd1fcf648..f71a801eee2 100644
--- a/sdks/python/apache_beam/io/gcp/datastore/v1new/datastoreio.py
+++ b/sdks/python/apache_beam/io/gcp/datastore/v1new/datastoreio.py
@@ -46,6 +46,7 @@ from apache_beam.transforms import DoFn
 from apache_beam.transforms import ParDo
 from apache_beam.transforms import PTransform
 from apache_beam.transforms import Reshuffle
+from apache_beam.utils import retry
 
 __all__ = ['ReadFromDatastore', 'WriteToDatastore', 'DeleteFromDatastore']
 
@@ -322,11 +323,73 @@ class _Mutate(PTransform):
       self._target_batch_size = self._batch_sizer.get_batch_size(
           time.time() * 1000)
 
-    def add_element_to_batch(self, element):
+    def element_to_client_batch_item(self, element):
       raise NotImplementedError
 
+    def add_to_batch(self, client_batch_item):
+      raise NotImplementedError
+
+    @retry.with_exponential_backoff(num_retries=5,
+                                    retry_filter=helper.retry_on_rpc_error)
+    def write_mutations(self, throttler, rpc_stats_callback, throttle_delay=1):
+      """Writes a batch of mutations to Cloud Datastore.
+
+      If a commit fails, it will be retried up to 5 times. All mutations in the
+      batch will be committed again, even if the commit was partially
+      successful. If the retry limit is exceeded, the last exception from
+      Cloud Datastore will be raised.
+
+      Assumes that the Datastore client library does not perform any retries on
+      commits. It has not been determined how such retries would interact with
+      the retries and throttler used here.
+      See ``google.cloud.datastore_v1.gapic.datastore_client_config`` for
+      retry config.
+
+      Args:
+        rpc_stats_callback: a function to call with arguments `successes` and
+            `failures` and `throttled_secs`; this is called to record successful
+            and failed RPCs to Datastore and time spent waiting for throttling.
+        throttler: (``apache_beam.io.gcp.datastore.v1.adaptive_throttler.
+          AdaptiveThrottler``)
+          Throttler instance used to select requests to be throttled.
+        throttle_delay: (:class:`float`) time in seconds to sleep when
+            throttled.
+
+      Returns:
+        (int) The latency of the successful RPC in milliseconds.
+      """
+      # Client-side throttling.
+      while throttler.throttle_request(time.time() * 1000):
+        logging.info("Delaying request for %ds due to previous failures",
+                     throttle_delay)
+        time.sleep(throttle_delay)
+        rpc_stats_callback(throttled_secs=throttle_delay)
+
+      if self._batch is None:
+        # this will only happen when we re-try previously failed batch
+        self._batch = self._client.batch()
+        self._batch.begin()
+        for element in self._batch_elements:
+          self.add_to_batch(element)
+
+      try:
+        start_time = time.time()
+        self._batch.commit()
+        end_time = time.time()
+
+        rpc_stats_callback(successes=1)
+        throttler.successful_request(start_time * 1000)
+        commit_time_ms = int((end_time-start_time) * 1000)
+        return commit_time_ms
+      except Exception:
+        self._batch = None
+        rpc_stats_callback(errors=1)
+        raise
+
     def process(self, element):
-      self.add_element_to_batch(element)
+      client_element = self.element_to_client_batch_item(element)
+      self._batch_elements.append(client_element)
+      self.add_to_batch(client_element)
       self._batch_bytes_size += self._batch.mutations[-1].ByteSize()
 
       if (len(self._batch.mutations) >= self._target_batch_size or
@@ -334,18 +397,20 @@ class _Mutate(PTransform):
         self._flush_batch()
 
     def finish_bundle(self):
-      if self._batch.mutations:
+      if self._batch_elements:
         self._flush_batch()
 
     def _init_batch(self):
       self._batch_bytes_size = 0
       self._batch = self._client.batch()
       self._batch.begin()
+      self._batch_elements = []
 
     def _flush_batch(self):
       # Flush the current batch of mutations to Cloud Datastore.
-      latency_ms = helper.write_mutations(
-          self._batch, self._throttler, self._update_rpc_stats,
+      latency_ms = self.write_mutations(
+          self._throttler,
+          rpc_stats_callback=self._update_rpc_stats,
           throttle_delay=util.WRITE_BATCH_TARGET_LATENCY_MS // 1000)
       logging.debug("Successfully wrote %d mutations in %dms.",
                     len(self._batch.mutations), latency_ms)
@@ -380,7 +445,7 @@ class WriteToDatastore(_Mutate):
     super(WriteToDatastore, self).__init__(mutate_fn)
 
   class _DatastoreWriteFn(_Mutate.DatastoreMutateFn):
-    def add_element_to_batch(self, element):
+    def element_to_client_batch_item(self, element):
       if not isinstance(element, types.Entity):
         raise ValueError('apache_beam.io.gcp.datastore.v1new.datastoreio.Entity'
                          ' expected, got: %s' % type(element))
@@ -390,6 +455,9 @@ class WriteToDatastore(_Mutate):
       if client_entity.key.is_partial:
         raise ValueError('Entities to be written to Cloud Datastore must '
                          'have complete keys:\n%s' % client_entity)
+      return client_entity
+
+    def add_to_batch(self, client_entity):
       self._batch.put(client_entity)
 
     def display_data(self):
@@ -421,7 +489,7 @@ class DeleteFromDatastore(_Mutate):
     super(DeleteFromDatastore, self).__init__(mutate_fn)
 
   class _DatastoreDeleteFn(_Mutate.DatastoreMutateFn):
-    def add_element_to_batch(self, element):
+    def element_to_client_batch_item(self, element):
       if not isinstance(element, types.Key):
         raise ValueError('apache_beam.io.gcp.datastore.v1new.datastoreio.Key'
                          ' expected, got: %s' % type(element))
@@ -431,6 +499,9 @@ class DeleteFromDatastore(_Mutate):
       if client_key.is_partial:
         raise ValueError('Keys to be deleted from Cloud Datastore must be '
                          'complete:\n%s' % client_key)
+      return client_key
+
+    def add_to_batch(self, client_key):
       self._batch.delete(client_key)
 
     def display_data(self):
diff --git a/sdks/python/apache_beam/io/gcp/datastore/v1new/datastoreio_test.py b/sdks/python/apache_beam/io/gcp/datastore/v1new/datastoreio_test.py
index 79d43fec485..5da204d9eb8 100644
--- a/sdks/python/apache_beam/io/gcp/datastore/v1new/datastoreio_test.py
+++ b/sdks/python/apache_beam/io/gcp/datastore/v1new/datastoreio_test.py
@@ -28,20 +28,24 @@ import unittest
 from mock import MagicMock
 from mock import call
 from mock import patch
+from mock import ANY
 
 # Protect against environments where datastore library is not available.
 try:
   from apache_beam.io.gcp.datastore.v1 import util
   from apache_beam.io.gcp.datastore.v1new import helper
   from apache_beam.io.gcp.datastore.v1new import query_splitter
+  from apache_beam.io.gcp.datastore.v1new import datastoreio
   from apache_beam.io.gcp.datastore.v1new.datastoreio import DeleteFromDatastore
   from apache_beam.io.gcp.datastore.v1new.datastoreio import ReadFromDatastore
   from apache_beam.io.gcp.datastore.v1new.datastoreio import WriteToDatastore
   from apache_beam.io.gcp.datastore.v1new.types import Key
+  from apache_beam.testing.test_utils import patch_retry
   from google.cloud.datastore import client
   from google.cloud.datastore import entity
   from google.cloud.datastore import helpers
   from google.cloud.datastore import key
+  from google.api_core import exceptions
   # Keep this import last so it doesn't import conflicting pb2 modules.
   from apache_beam.io.gcp.datastore.v1 import datastoreio_test  # pylint: disable=ungrouped-imports
   DatastoreioTestBase = datastoreio_test.DatastoreioTest
@@ -107,6 +111,81 @@ class FakeBatch(object):
       self._commit_count[0] += 1
 
 
+@unittest.skipIf(client is None, 'Datastore dependencies are not installed')
+class MutateTest(unittest.TestCase):
+
+  def setUp(self):
+    patch_retry(self, datastoreio)
+
+  def test_write_mutations_no_errors(self):
+    mock_batch = MagicMock()
+    mock_throttler = MagicMock()
+    rpc_stats_callback = MagicMock()
+    mock_throttler.throttle_request.return_value = []
+    mutate = datastoreio._Mutate.DatastoreMutateFn(lambda: None)
+    mutate._batch = mock_batch
+    mutate.write_mutations(mock_throttler, rpc_stats_callback)
+    rpc_stats_callback.assert_has_calls([
+        call(successes=1),
+    ])
+
+  def test_write_mutations_reconstruct_on_error(self):
+    mock_batch = MagicMock()
+    mock_batch.begin.side_effect = [None, ValueError]
+    mock_batch.commit.side_effect = [exceptions.DeadlineExceeded('retryable'),
+                                     None]
+    mock_throttler = MagicMock()
+    rpc_stats_callback = MagicMock()
+    mock_throttler.throttle_request.return_value = []
+    mutate = datastoreio._Mutate.DatastoreMutateFn(lambda: None)
+    mutate._batch = mock_batch
+    mutate._client = MagicMock()
+    mutate._batch_elements = [None]
+    mock_add_to_batch = MagicMock()
+    mutate.add_to_batch = mock_add_to_batch
+    mutate.write_mutations(mock_throttler, rpc_stats_callback)
+    rpc_stats_callback.assert_has_calls([
+        call(successes=1),
+    ])
+    self.assertEqual(1, mock_add_to_batch.call_count)
+
+  def test_write_mutations_throttle_delay_retryable_error(self):
+    mock_batch = MagicMock()
+    mock_batch.commit.side_effect = [exceptions.DeadlineExceeded('retryable'),
+                                     None]
+    mock_throttler = MagicMock()
+    rpc_stats_callback = MagicMock()
+    # First try: throttle once [True, False]
+    # Second try: no throttle [False]
+    mock_throttler.throttle_request.side_effect = [True, False, False]
+    mutate = datastoreio._Mutate.DatastoreMutateFn(lambda: None)
+    mutate._batch = mock_batch
+    mutate._batch_elements = []
+    mutate._client = MagicMock()
+    mutate.write_mutations(mock_throttler, rpc_stats_callback, throttle_delay=0)
+    rpc_stats_callback.assert_has_calls([
+        call(successes=1),
+        call(throttled_secs=ANY),
+        call(errors=1),
+    ], any_order=True)
+    self.assertEqual(3, rpc_stats_callback.call_count)
+
+  def test_write_mutations_non_retryable_error(self):
+    mock_batch = MagicMock()
+    mock_batch.commit.side_effect = [
+        exceptions.InvalidArgument('non-retryable'),
+    ]
+    mock_throttler = MagicMock()
+    rpc_stats_callback = MagicMock()
+    mock_throttler.throttle_request.return_value = False
+    mutate = datastoreio._Mutate.DatastoreMutateFn(lambda: None)
+    mutate._batch = mock_batch
+    with self.assertRaises(exceptions.InvalidArgument):
+      mutate.write_mutations(mock_throttler, rpc_stats_callback,
+                             throttle_delay=0)
+    rpc_stats_callback.assert_called_once_with(errors=1)
+
+
 @unittest.skipIf(client is None, 'Datastore dependencies are not installed')
 class DatastoreioTest(DatastoreioTestBase):
   """
diff --git a/sdks/python/apache_beam/io/gcp/datastore/v1new/helper.py b/sdks/python/apache_beam/io/gcp/datastore/v1new/helper.py
index a5e9ce3f870..2bce903b775 100644
--- a/sdks/python/apache_beam/io/gcp/datastore/v1new/helper.py
+++ b/sdks/python/apache_beam/io/gcp/datastore/v1new/helper.py
@@ -23,9 +23,7 @@ For internal use only; no backwards-compatibility guarantees.
 
 from __future__ import absolute_import
 
-import logging
 import os
-import time
 import uuid
 from builtins import range
 
@@ -34,7 +32,6 @@ from google.cloud import environment_vars
 from google.cloud.datastore import client
 
 from apache_beam.io.gcp.datastore.v1new import types
-from apache_beam.utils import retry
 from cachetools.func import ttl_cache
 
 # https://cloud.google.com/datastore/docs/concepts/errors#error_codes
@@ -61,57 +58,6 @@ def retry_on_rpc_error(exception):
   return isinstance(exception, _RETRYABLE_DATASTORE_ERRORS)
 
 
-@retry.with_exponential_backoff(num_retries=5,
-                                retry_filter=retry_on_rpc_error)
-def write_mutations(batch, throttler, rpc_stats_callback, throttle_delay=1):
-  """A helper function to write a batch of mutations to Cloud Datastore.
-
-  If a commit fails, it will be retried up to 5 times. All mutations in the
-  batch will be committed again, even if the commit was partially successful.
-  If the retry limit is exceeded, the last exception from Cloud Datastore will
-  be raised.
-
-  Assumes that the Datastore client library does not perform any retries on
-  commits. It has not been determined how such retries would interact with the
-  retries and throttler used here.
-  See ``google.cloud.datastore_v1.gapic.datastore_client_config`` for
-  retry config.
-
-  Args:
-    batch: (:class:`~google.cloud.datastore.batch.Batch`) An instance of an
-      in-progress batch.
-    rpc_stats_callback: a function to call with arguments `successes` and
-        `failures` and `throttled_secs`; this is called to record successful
-        and failed RPCs to Datastore and time spent waiting for throttling.
-    throttler: (``apache_beam.io.gcp.datastore.v1.adaptive_throttler.
-      AdaptiveThrottler``)
-      Throttler instance used to select requests to be throttled.
-    throttle_delay: (:class:`float`) time in seconds to sleep when throttled.
-
-  Returns:
-    (int) The latency of the successful RPC in milliseconds.
-  """
-  # Client-side throttling.
-  while throttler.throttle_request(time.time() * 1000):
-    logging.info("Delaying request for %ds due to previous failures",
-                 throttle_delay)
-    time.sleep(throttle_delay)
-    rpc_stats_callback(throttled_secs=throttle_delay)
-
-  try:
-    start_time = time.time()
-    batch.commit()
-    end_time = time.time()
-
-    rpc_stats_callback(successes=1)
-    throttler.successful_request(start_time * 1000)
-    commit_time_ms = int((end_time-start_time) * 1000)
-    return commit_time_ms
-  except Exception:
-    rpc_stats_callback(errors=1)
-    raise
-
-
 def create_entities(count, id_or_name=False):
   """Creates a list of entities with random keys."""
   if id_or_name:
diff --git a/sdks/python/apache_beam/io/gcp/datastore/v1new/helper_test.py b/sdks/python/apache_beam/io/gcp/datastore/v1new/helper_test.py
deleted file mode 100644
index 6c2d1a629fc..00000000000
--- a/sdks/python/apache_beam/io/gcp/datastore/v1new/helper_test.py
+++ /dev/null
@@ -1,85 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#    http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-
-"""Tests for datastore helper."""
-from __future__ import absolute_import
-
-import unittest
-
-import mock
-
-# Protect against environments where apitools library is not available.
-try:
-  from apache_beam.io.gcp.datastore.v1new import helper
-  from apache_beam.testing.test_utils import patch_retry
-  from google.api_core import exceptions
-# TODO(BEAM-4543): Remove TypeError once googledatastore dependency is removed.
-except (ImportError, TypeError):
-  helper = None
-
-
-@unittest.skipIf(helper is None, 'GCP dependencies are not installed')
-class HelperTest(unittest.TestCase):
-
-  def setUp(self):
-    self._mock_datastore = mock.MagicMock()
-    patch_retry(self, helper)
-
-  def test_write_mutations_no_errors(self):
-    mock_batch = mock.MagicMock()
-    mock_throttler = mock.MagicMock()
-    rpc_stats_callback = mock.MagicMock()
-    mock_throttler.throttle_request.return_value = []
-    helper.write_mutations(mock_batch, mock_throttler, rpc_stats_callback)
-    rpc_stats_callback.assert_has_calls([
-        mock.call(successes=1),
-    ])
-
-  def test_write_mutations_throttle_delay_retryable_error(self):
-    mock_batch = mock.MagicMock()
-    mock_batch.commit.side_effect = [exceptions.DeadlineExceeded('retryable'),
-                                     None]
-    mock_throttler = mock.MagicMock()
-    rpc_stats_callback = mock.MagicMock()
-    # First try: throttle once [True, False]
-    # Second try: no throttle [False]
-    mock_throttler.throttle_request.side_effect = [True, False, False]
-    helper.write_mutations(mock_batch, mock_throttler, rpc_stats_callback,
-                           throttle_delay=0)
-    rpc_stats_callback.assert_has_calls([
-        mock.call(successes=1),
-        mock.call(throttled_secs=mock.ANY),
-        mock.call(errors=1),
-    ], any_order=True)
-    self.assertEqual(3, rpc_stats_callback.call_count)
-
-  def test_write_mutations_non_retryable_error(self):
-    mock_batch = mock.MagicMock()
-    mock_batch.commit.side_effect = [
-        exceptions.InvalidArgument('non-retryable'),
-    ]
-    mock_throttler = mock.MagicMock()
-    rpc_stats_callback = mock.MagicMock()
-    mock_throttler.throttle_request.return_value = False
-    with self.assertRaises(exceptions.InvalidArgument):
-      helper.write_mutations(mock_batch, mock_throttler, rpc_stats_callback,
-                             throttle_delay=0)
-    rpc_stats_callback.assert_called_once_with(errors=1)
-
-
-if __name__ == '__main__':
-  unittest.main()
