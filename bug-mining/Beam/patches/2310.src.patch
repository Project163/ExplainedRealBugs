diff --git a/sdks/python/apache_beam/runners/interactive/augmented_pipeline.py b/sdks/python/apache_beam/runners/interactive/augmented_pipeline.py
new file mode 100644
index 00000000000..35595e4c2a9
--- /dev/null
+++ b/sdks/python/apache_beam/runners/interactive/augmented_pipeline.py
@@ -0,0 +1,136 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+"""Module to augment interactive flavor into the given pipeline.
+
+For internal use only; no backward-compatibility guarantees.
+"""
+# pytype: skip-file
+
+from __future__ import absolute_import
+
+import copy
+from typing import Dict
+from typing import Set
+
+import apache_beam as beam
+from apache_beam.pipeline import PipelineVisitor
+from apache_beam.portability.api import beam_runner_api_pb2
+from apache_beam.runners.interactive import interactive_environment as ie
+from apache_beam.runners.interactive import background_caching_job
+from apache_beam.runners.interactive.caching.cacheable import Cacheable
+from apache_beam.runners.interactive.caching.read_cache import ReadCache
+from apache_beam.runners.interactive.caching.write_cache import WriteCache
+
+
+class AugmentedPipeline:
+  """A pipeline with augmented interactive flavor that caches intermediate
+  PCollections defined by the user, reads computed PCollections as source and
+  prunes unnecessary pipeline parts for fast computation.
+  """
+  def __init__(
+      self,
+      user_pipeline: beam.Pipeline,
+      pcolls: Set[beam.pvalue.PCollection] = set()):
+    """
+    Initializes a pipelilne for augmenting interactive flavor.
+
+    Args:
+      user_pipeline: a beam.Pipeline instance defined by the user.
+      pcolls: cacheable pcolls to be computed/retrieved. If the set is
+        empty, all intermediate pcolls assigned to variables are applicable.
+    """
+    assert not pcolls or all([pcoll.pipeline is user_pipeline for pcoll in
+      pcolls]), 'All %s need to belong to %s' % (pcolls, user_pipeline)
+    self._user_pipeline = user_pipeline
+    self._pcolls = pcolls
+    self._cache_manager = ie.current_env().get_cache_manager(
+        self._user_pipeline, create_if_absent=True)
+    if background_caching_job.has_source_to_cache(self._user_pipeline):
+      self._cache_manager = ie.current_env().get_cache_manager(
+          self._user_pipeline)
+    self._pipeline, self._context = self._user_pipeline.to_runner_api(return_context=True)
+    self._context.component_id_map = copy.copy(
+        self._user_pipeline.component_id_map)
+    self._cacheables = self.cacheables()
+    self._augmented = False
+
+  @property
+  def augmented_pipeline(self) -> beam_runner_api_pb2.Pipeline:
+    assert self._augmented, 'Call augment() before retrieving the value.'
+    return self._pipeline
+
+  # TODO(BEAM-10708): Support generating a background recording job that
+  # contains unbound source recording transforms only.
+  @property
+  def background_recording_pipeline(self) -> beam_runner_api_pb2.Pipeline:
+    raise NotImplementedError
+
+  def cacheables(self) -> Dict[beam.pvalue.PCollection, Cacheable]:
+    """Finds all the cacheable intermediate PCollections in the pipeline with
+    their metadata.
+    """
+    c = {}
+    for watching in ie.current_env().watching():
+      for key, val in watching:
+        if (isinstance(val, beam.pvalue.PCollection) and
+            val.pipeline is self._user_pipeline and
+            (not self._pcolls or val in self._pcolls)):
+          pcoll_id = self._context.pcollections.get_id(val)
+          c[val] = Cacheable(
+              pcoll_id=pcoll_id,
+              var=key,
+              pcoll=val,
+              version=str(id(val)),
+              producer_version=str(id(val.producer)))
+    return c
+
+  def augment(self) -> 'AugmentedPipeline':
+    """Augments the pipeline with cache. Idempotent and returns self.
+
+    For a cacheable PCollection, if cache exists, read cache; else, write cache.
+    """
+    if self._augmented:
+      return self
+    self._augmented = True
+
+    # Find pcolls eligible for reading or writing cache.
+    readcache_pcolls = set()
+    for pcoll, cacheable in self._cacheables.items():
+      key = repr(cacheable.to_key())
+      if (self._cache_manager.exists('full', key) and
+          pcoll in ie.current_env().computed_pcollections):
+        readcache_pcolls.add(pcoll)
+    writecache_pcolls = set(
+        self._cacheables.keys()).difference(readcache_pcolls)
+
+    # Wire in additional transforms to read cache and write cache.
+    for readcache_pcoll in readcache_pcolls:
+      ReadCache(
+          self._pipeline,
+          self._context,
+          self._cache_manager,
+          self._cacheables[readcache_pcoll]).read_cache()
+    for writecache_pcoll in writecache_pcolls:
+      WriteCache(
+          self._pipeline,
+          self._context,
+          self._cache_manager,
+          self._cacheables[writecache_pcoll]).write_cache()
+    # TODO(BEAM-10708): Support streaming, add pruning logic, and integrate
+    # pipeline fragment logic.
+    return self
diff --git a/sdks/python/apache_beam/runners/interactive/augmented_pipeline_test.py b/sdks/python/apache_beam/runners/interactive/augmented_pipeline_test.py
new file mode 100644
index 00000000000..8260554618b
--- /dev/null
+++ b/sdks/python/apache_beam/runners/interactive/augmented_pipeline_test.py
@@ -0,0 +1,93 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+"""Tests for augmented_pipeline module."""
+
+# pytest: skip-file
+
+from __future__ import absolute_import
+
+import unittest
+
+import apache_beam as beam
+from apache_beam.runners.interactive import augmented_pipeline as ap
+from apache_beam.runners.interactive import interactive_beam as ib
+from apache_beam.runners.interactive import interactive_environment as ie
+
+
+class CacheableTest(unittest.TestCase):
+  def setUp(self):
+    ie.new_env()
+
+  def test_find_all_cacheables(self):
+    p = beam.Pipeline()
+    cacheable_pcoll_1 = p | beam.Create([1, 2, 3])
+    cacheable_pcoll_2 = cacheable_pcoll_1 | beam.Map(lambda x: x * x)
+    ib.watch(locals())
+
+    aug_p = ap.AugmentedPipeline(p)
+    cacheables = aug_p.cacheables()
+    self.assertIn(cacheable_pcoll_1, cacheables)
+    self.assertIn(cacheable_pcoll_2, cacheables)
+
+  def test_ignore_cacheables(self):
+    p = beam.Pipeline()
+    cacheable_pcoll_1 = p | 'cacheable_pcoll_1' >> beam.Create([1, 2, 3])
+    cacheable_pcoll_2 = p | 'cacheable_pcoll_2' >> beam.Create([4, 5, 6])
+    ib.watch(locals())
+
+    aug_p = ap.AugmentedPipeline(p, (cacheable_pcoll_1, ))
+    cacheables = aug_p.cacheables()
+    self.assertIn(cacheable_pcoll_1, cacheables)
+    self.assertNotIn(cacheable_pcoll_2, cacheables)
+
+  def test_ignore_pcoll_from_other_pipeline(self):
+    p = beam.Pipeline()
+    p2 = beam.Pipeline()
+    cacheable_from_p2 = p2 | beam.Create([1, 2, 3])
+    ib.watch(locals())
+
+    aug_p = ap.AugmentedPipeline(p)
+    cacheables = aug_p.cacheables()
+    self.assertNotIn(cacheable_from_p2, cacheables)
+
+
+class AugmentTest(unittest.TestCase):
+  def setUp(self):
+    ie.new_env()
+
+  def test_error_when_pcolls_from_mixed_pipelines(self):
+    p = beam.Pipeline()
+    cacheable_from_p = p | beam.Create([1, 2, 3])
+    p2 = beam.Pipeline()
+    cacheable_from_p2 = p2 | beam.Create([1, 2, 3])
+    ib.watch(locals())
+
+    self.assertRaises(
+        AssertionError,
+        lambda: ap.AugmentedPipeline(p, (cacheable_from_p, cacheable_from_p2)))
+
+  def test_error_retrieve_b4_augment(self):
+    p = beam.Pipeline()
+    ib.watch(locals())
+
+    aug_p = ap.AugmentedPipeline(p)
+    self.assertRaises(AssertionError, lambda: aug_p.augmented_pipeline)
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/sdks/python/apache_beam/runners/interactive/caching/read_cache.py b/sdks/python/apache_beam/runners/interactive/caching/read_cache.py
new file mode 100644
index 00000000000..5f8fc0793ca
--- /dev/null
+++ b/sdks/python/apache_beam/runners/interactive/caching/read_cache.py
@@ -0,0 +1,130 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+"""Module to read cache of computed PCollections.
+
+For internal use only; no backward-compatibility guarantees.
+"""
+# pytype: skip-file
+
+from __future__ import absolute_import
+
+from typing import Tuple
+
+import apache_beam as beam
+from apache_beam.portability.api import beam_runner_api_pb2
+from apache_beam.runners.interactive import cache_manager as cache
+from apache_beam.runners.interactive.caching.cacheable import Cacheable
+from apache_beam.runners.pipeline_context import PipelineContext
+from apache_beam.transforms.ptransform import PTransform
+
+
+class ReadCache:
+  """Class that facilitates reading cache of computed PCollections.
+  """
+  def __init__(
+      self,
+      pipeline: beam_runner_api_pb2.Pipeline,
+      context: PipelineContext,
+      cache_manager: cache.CacheManager,
+      cacheable: Cacheable):
+    self._pipeline = pipeline
+    self._context = context
+    self._cache_manager = cache_manager
+    self._cacheable = cacheable
+    self._key = repr(cacheable.to_key())
+    self._label = '{}{}'.format('_cache_', self._key)
+
+  def read_cache(self) -> Tuple[str, str]:
+    """Reads cache of the cacheable PCollection and wires the cache into the
+    pipeline proto. Returns the pipeline-scoped ids of the cacheable PCollection
+    and the cache reading output PCollection that replaces it.
+    """
+    template, read_output = self._build_runner_api_template()
+    output_id = self._context.pcollections.get_id(read_output)
+    source_id = self._context.pcollections.get_id(self._cacheable.pcoll)
+    # Copy cache reading subgraph from the template to the pipeline proto.
+    for pcoll_id in template.components.pcollections:
+      if pcoll_id in self._pipeline.components.pcollections:
+        continue
+      self._pipeline.components.pcollections[pcoll_id].CopyFrom(
+          template.components.pcollections[pcoll_id])
+    for coder_id in template.components.coders:
+      if coder_id in self._pipeline.components.coders:
+        continue
+      self._pipeline.components.coders[coder_id].CopyFrom(
+          template.components.coders[coder_id])
+    for windowing_strategy_id in template.components.windowing_strategies:
+      if windowing_strategy_id in self._pipeline.components.windowing_strategies:
+        continue
+      self._pipeline.components.windowing_strategies[
+          windowing_strategy_id].CopyFrom(
+              template.components.windowing_strategies[windowing_strategy_id])
+    template_root_transform_id = template.root_transform_ids[0]
+    root_transform_id = self._pipeline.root_transform_ids[0]
+    for transform_id in template.components.transforms:
+      if (transform_id == template_root_transform_id or
+          transform_id in self._pipeline.components.transforms):
+        continue
+      self._pipeline.components.transforms[transform_id].CopyFrom(
+          template.components.transforms[transform_id])
+    self._pipeline.components.transforms[
+        root_transform_id].subtransforms.extend(
+            template.components.transforms[template_root_transform_id].
+            subtransforms)
+
+    # Replace all the input pcoll of source_id with output pcoll of output_id
+    # from cache reading.
+    for transform in self._pipeline.components.transforms.values():
+      inputs = transform.inputs
+      if source_id in inputs.values():
+        keys_need_replacement = set()
+        for key in inputs:
+          if inputs[key] == source_id:
+            keys_need_replacement.add(key)
+        for key in keys_need_replacement:
+          inputs[key] = output_id
+
+    return source_id, output_id
+
+  def _build_runner_api_template(
+      self) -> Tuple[beam_runner_api_pb2.Pipeline, beam.pvalue.PCollection]:
+    transform = _ReadCacheTransform(self._cache_manager, self._key, self._label)
+    tmp_pipeline = beam.Pipeline()
+    tmp_pipeline.component_id_map = self._context.component_id_map
+    read_output = tmp_pipeline | 'source' + self._label >> transform
+    return tmp_pipeline.to_runner_api(), read_output
+
+
+class _ReadCacheTransform(PTransform):
+  """A composite transform encapsulates reading cache of PCollections.
+  """
+  def __init__(self, cache_manager: cache.CacheManager, key: str, label: str):
+    self._cache_manager = cache_manager
+    self._key = key
+    self._label = label
+
+  def expand(self, pcoll: beam.pvalue.PCollection) -> beam.pvalue.PCollection:
+    class Unreify(beam.DoFn):
+      def process(self, e):
+        yield e.windowed_value
+
+    return (
+        pcoll.pipeline
+        |
+        'read' + self._label >> cache.ReadCache(self._cache_manager, self._key)
+        | 'unreify' + self._label >> beam.ParDo(Unreify()))
diff --git a/sdks/python/apache_beam/runners/interactive/caching/read_cache_test.py b/sdks/python/apache_beam/runners/interactive/caching/read_cache_test.py
new file mode 100644
index 00000000000..a6afc4bebd2
--- /dev/null
+++ b/sdks/python/apache_beam/runners/interactive/caching/read_cache_test.py
@@ -0,0 +1,85 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+"""Tests for read_cache."""
+# pytype: skip-file
+
+from __future__ import absolute_import
+
+import unittest
+
+import apache_beam as beam
+from apache_beam.runners.interactive import augmented_pipeline as ap
+from apache_beam.runners.interactive import interactive_beam as ib
+from apache_beam.runners.interactive import interactive_environment as ie
+from apache_beam.runners.interactive.caching import read_cache
+from apache_beam.runners.interactive.testing.pipeline_assertion import assert_pipeline_proto_equal
+from apache_beam.runners.interactive.testing.test_cache_manager import InMemoryCache
+
+
+class ReadCacheTest(unittest.TestCase):
+  def setUp(self):
+    ie.new_env()
+
+  def test_read_cache(self):
+    p = beam.Pipeline()
+    pcoll = p | beam.Create([1, 2, 3])
+    consumer_transform = beam.Map(lambda x: x * x)
+    pcoll_consumer = pcoll | consumer_transform
+    ib.watch(locals())
+
+    # Create the cache in memory.
+    cache_manager = InMemoryCache()
+    ie.current_env().set_cache_manager(cache_manager, p)
+    aug_p = ap.AugmentedPipeline(p)
+    key = repr(aug_p._cacheables[pcoll].to_key())
+    cache_manager.write('test', 'full', key)
+
+    # Capture the applied transform of the consumer_transform.
+    pcoll_id = aug_p._context.pcollections.get_id(pcoll)
+    consumer_transform_id = None
+    for transform_id, transform in aug_p._pipeline.components.transforms.items():
+      if pcoll_id in transform.inputs.values():
+        consumer_transform_id = transform_id
+        break
+    self.assertIsNotNone(consumer_transform_id)
+
+    # Read cache on the pipeline proto.
+    _, cache_id = read_cache.ReadCache(
+        aug_p._pipeline, aug_p._context, aug_p._cache_manager,
+        aug_p._cacheables[pcoll]).read_cache()
+    actual_pipeline = aug_p._pipeline
+
+    # Read cache directly on the pipeline instance.
+    label = '{}{}'.format('_cache_', key)
+    transform = read_cache._ReadCacheTransform(aug_p._cache_manager, key, label)
+    p | 'source' + label >> transform
+    expected_pipeline = p.to_runner_api()
+
+    # This rougly checks the equivalence between two protos, not detailed
+    # wiring in sub transforms under top level transforms.
+    assert_pipeline_proto_equal(self, expected_pipeline, actual_pipeline)
+
+    # Check if the actual_pipeline uses cache as input of the
+    # consumer_transform instead of the original pcoll from source.
+    inputs = actual_pipeline.components.transforms[consumer_transform_id].inputs
+    self.assertIn(cache_id, inputs.values())
+    self.assertNotIn(pcoll_id, inputs.values())
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/sdks/python/apache_beam/runners/interactive/caching/write_cache.py b/sdks/python/apache_beam/runners/interactive/caching/write_cache.py
new file mode 100644
index 00000000000..bb4ae605513
--- /dev/null
+++ b/sdks/python/apache_beam/runners/interactive/caching/write_cache.py
@@ -0,0 +1,170 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+"""Module to write cache for PCollections being computed.
+
+For internal use only; no backward-compatibility guarantees.
+"""
+# pytype: skip-file
+
+from __future__ import absolute_import
+
+from typing import Tuple
+
+import apache_beam as beam
+from apache_beam.portability.api import beam_runner_api_pb2
+from apache_beam.runners.interactive import cache_manager as cache
+from apache_beam.runners.interactive.caching.cacheable import Cacheable
+from apache_beam.runners.pipeline_context import PipelineContext
+from apache_beam.testing import test_stream
+from apache_beam.transforms.ptransform import PTransform
+from apache_beam.transforms.window import WindowedValue
+
+
+class WriteCache:
+  """Class that facilitates writing cache for PCollections being computed.
+  """
+  def __init__(
+      self,
+      pipeline: beam_runner_api_pb2.Pipeline,
+      context: PipelineContext,
+      cache_manager: cache.CacheManager,
+      cacheable: Cacheable):
+    self._pipeline = pipeline
+    self._context = context
+    self._cache_manager = cache_manager
+    self._cacheable = cacheable
+    self._key = repr(cacheable.to_key())
+    self._label = '{}{}'.format('_cache_', self._key)
+
+  def write_cache(self) -> None:
+    """Writes cache for the cacheable PCollection that is being computed.
+    """
+    template, write_input_placeholder = self._build_runner_api_template()
+    input_placeholder_id = self._context.pcollections.get_id(
+        write_input_placeholder.placeholder_pcoll)
+    input_id = self._context.pcollections.get_id(self._cacheable.pcoll)
+
+    # Copy cache writing subgraph from the template to the pipeline proto.
+    for pcoll_id in template.components.pcollections:
+      if (pcoll_id in self._pipeline.components.pcollections or
+          pcoll_id in write_input_placeholder.ignorable_components.pcollections
+          ):
+        continue
+      self._pipeline.components.pcollections[pcoll_id].CopyFrom(
+          template.components.pcollections[pcoll_id])
+    for coder_id in template.components.coders:
+      if (coder_id in self._pipeline.components.coders or
+          coder_id in write_input_placeholder.ignorable_components.coders):
+        continue
+      self._pipeline.components.coders[coder_id].CopyFrom(
+          template.components.coders[coder_id])
+    for windowing_strategy_id in template.components.windowing_strategies:
+      if (windowing_strategy_id in
+          self._pipeline.components.windowing_strategies or
+          windowing_strategy_id in
+          write_input_placeholder.ignorable_components.windowing_strategies):
+        continue
+      self._pipeline.components.windowing_strategies[
+          windowing_strategy_id].CopyFrom(
+              template.components.windowing_strategies[windowing_strategy_id])
+    template_root_transform_id = template.root_transform_ids[0]
+    root_transform_id = self._pipeline.root_transform_ids[0]
+    for transform_id in template.components.transforms:
+      if (transform_id in self._pipeline.components.transforms or transform_id
+          in write_input_placeholder.ignorable_components.transforms):
+        continue
+      self._pipeline.components.transforms[transform_id].CopyFrom(
+          template.components.transforms[transform_id])
+    for top_level_transform in template.components.transforms[
+        template_root_transform_id].subtransforms:
+      if (top_level_transform in
+          write_input_placeholder.ignorable_components.transforms):
+        continue
+      self._pipeline.components.transforms[
+          root_transform_id].subtransforms.append(top_level_transform)
+
+    # Replace all the input pcoll of input_placeholder_id from cache writing
+    # with cacheable pcoll of input_id.
+    for transform in self._pipeline.components.transforms.values():
+      inputs = transform.inputs
+      if input_placeholder_id in inputs.values():
+        keys_need_replacement = set()
+        for key in inputs:
+          if inputs[key] == input_placeholder_id:
+            keys_need_replacement.add(key)
+        for key in keys_need_replacement:
+          inputs[key] = input_id
+
+  def _build_runner_api_template(
+      self) -> Tuple[beam_runner_api_pb2.Pipeline, '_PCollectionPlaceHolder']:
+    pph = _PCollectionPlaceHolder(self._cacheable.pcoll, self._context)
+    transform = _WriteCacheTransform(
+        self._cache_manager, self._key, self._label)
+    _ = pph.placeholder_pcoll | 'sink' + self._label >> transform
+    return pph.placeholder_pcoll.pipeline.to_runner_api(), pph
+
+
+class _WriteCacheTransform(PTransform):
+  """A composite transform encapsulates writing cache for PCollections.
+  """
+  def __init__(self, cache_manager: cache.CacheManager, key: str, label: str):
+    self._cache_manager = cache_manager
+    self._key = key
+    self._label = label
+
+  def expand(self, pcoll: beam.pvalue.PCollection) -> beam.pvalue.PCollection:
+    class Reify(beam.DoFn):
+      def process(
+          self,
+          e,
+          w=beam.DoFn.WindowParam,
+          p=beam.DoFn.PaneInfoParam,
+          t=beam.DoFn.TimestampParam):
+        yield test_stream.WindowedValueHolder(WindowedValue(e, t, [w], p))
+
+    return (
+        pcoll
+        | 'reify' + self._label >> beam.ParDo(Reify())
+        | 'write' + self._label >> cache.WriteCache(
+            self._cache_manager, self._key, is_capture=False))
+
+
+class _PCollectionPlaceHolder:
+  """A placeholder as an input to the cache writing transform.
+  """
+  def __init__(self, pcoll: beam.pvalue.PCollection, context: PipelineContext):
+    tmp_pipeline = beam.Pipeline()
+    tmp_pipeline.component_id_map = context.component_id_map
+    self._input_placeholder = tmp_pipeline | 'CreatePInput' >> beam.Create(
+        [], reshuffle=False)
+    self._input_placeholder.tag = pcoll.tag
+    self._input_placeholder.element_type = pcoll.element_type
+    self._input_placeholder.is_bounded = pcoll.is_bounded
+    self._input_placeholder._windowing = pcoll.windowing
+    self._ignorable_components = tmp_pipeline.to_runner_api().components
+
+  @property
+  def placeholder_pcoll(self) -> beam.pvalue.PCollection:
+    return self._input_placeholder
+
+  @property
+  def ignorable_components(self) -> beam_runner_api_pb2.Components:
+    """Subgraph generated by the placeholder that can be ignored in the final
+    pipeline proto.
+    """
+    return self._ignorable_components
diff --git a/sdks/python/apache_beam/runners/interactive/caching/write_cache_test.py b/sdks/python/apache_beam/runners/interactive/caching/write_cache_test.py
new file mode 100644
index 00000000000..8a3250e2ca7
--- /dev/null
+++ b/sdks/python/apache_beam/runners/interactive/caching/write_cache_test.py
@@ -0,0 +1,79 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+"""Tests for write_cache."""
+# pytype: skip-file
+
+from __future__ import absolute_import
+
+import unittest
+
+import apache_beam as beam
+from apache_beam.runners.interactive import augmented_pipeline as ap
+from apache_beam.runners.interactive import interactive_beam as ib
+from apache_beam.runners.interactive import interactive_environment as ie
+from apache_beam.runners.interactive.caching import write_cache
+from apache_beam.runners.interactive.testing.pipeline_assertion import assert_pipeline_proto_equal
+from apache_beam.runners.interactive.testing.test_cache_manager import InMemoryCache
+
+
+class WriteCacheTest(unittest.TestCase):
+  def setUp(self):
+    ie.new_env()
+
+  def test_write_cache(self):
+    p = beam.Pipeline()
+    pcoll = p | beam.Create([1, 2, 3])
+    ib.watch(locals())
+
+    cache_manager = InMemoryCache()
+    ie.current_env().set_cache_manager(cache_manager, p)
+    aug_p = ap.AugmentedPipeline(p)
+    key = repr(aug_p._cacheables[pcoll].to_key())
+
+    # Write cache on the pipeline proto.
+    write_cache.WriteCache(
+        aug_p._pipeline,
+        aug_p._context,
+        aug_p._cache_manager,
+        aug_p._cacheables[pcoll]).write_cache()
+    actual_pipeline = aug_p._pipeline
+
+    # Write cache directly on the piepline instance.
+    label = '{}{}'.format('_cache_', key)
+    transform = write_cache._WriteCacheTransform(
+        aug_p._cache_manager, key, label)
+    _ = pcoll | 'sink' + label >> transform
+    expected_pipeline = p.to_runner_api()
+
+    assert_pipeline_proto_equal(self, expected_pipeline, actual_pipeline)
+
+    # Check if the actual_pipeline uses pcoll as an input of a write transform.
+    pcoll_id = aug_p._context.pcollections.get_id(pcoll)
+    write_transform_id = None
+    for transform_id, transform in actual_pipeline.components.transforms.items():
+      if pcoll_id in transform.inputs.values():
+        write_transform_id = transform_id
+        break
+    self.assertIsNotNone(write_transform_id)
+    self.assertIn(
+        'sink',
+        actual_pipeline.components.transforms[write_transform_id].unique_name)
+
+
+if __name__ == '__main__':
+  unittest.main()
