diff --git a/sdks/go/pkg/beam/core/metrics/store.go b/sdks/go/pkg/beam/core/metrics/store.go
index e90cfeb1e67..c0355b544bc 100644
--- a/sdks/go/pkg/beam/core/metrics/store.go
+++ b/sdks/go/pkg/beam/core/metrics/store.go
@@ -52,12 +52,34 @@ func PCollectionLabels(pcollection string) Labels {
 	return Labels{pcollection: pcollection}
 }
 
+// PCollection returns the PCollection id for this metric.
+func (l Labels) PCollection() string { return l.pcollection }
+
 // PTransformLabels builds a Labels for transform metrics.
 // Intended for framework use.
 func PTransformLabels(transform string) Labels {
 	return Labels{transform: transform}
 }
 
+// Map produces a map of present labels to their values.
+//
+// Returns nil map if invalid.
+func (l Labels) Map() map[string]string {
+	if l.transform != "" {
+		return map[string]string{
+			"PTRANSFORM": l.transform,
+			"NAMESPACE":  l.namespace,
+			"NAME":       l.name,
+		}
+	}
+	if l.pcollection != "" {
+		return map[string]string{
+			"PCOLLECTION": l.pcollection,
+		}
+	}
+	return nil
+}
+
 // Extractor allows users to access metrics programatically after
 // pipeline completion. Users assign functions to fields that
 // interest them, and that function is called for each metric
diff --git a/sdks/go/pkg/beam/core/runtime/exec/datasink.go b/sdks/go/pkg/beam/core/runtime/exec/datasink.go
index e11c47a9440..ddb7ef812b2 100644
--- a/sdks/go/pkg/beam/core/runtime/exec/datasink.go
+++ b/sdks/go/pkg/beam/core/runtime/exec/datasink.go
@@ -20,12 +20,9 @@ import (
 	"context"
 	"fmt"
 	"io"
-	"sync/atomic"
-	"time"
 
 	"github.com/apache/beam/sdks/go/pkg/beam/core/graph/coder"
 	"github.com/apache/beam/sdks/go/pkg/beam/internal/errors"
-	"github.com/apache/beam/sdks/go/pkg/beam/log"
 )
 
 // DataSink is a Node.
@@ -34,11 +31,9 @@ type DataSink struct {
 	SID   StreamID
 	Coder *coder.Coder
 
-	enc   ElementEncoder
-	wEnc  WindowEncoder
-	w     io.WriteCloser
-	count int64
-	start time.Time
+	enc  ElementEncoder
+	wEnc WindowEncoder
+	w    io.WriteCloser
 }
 
 func (n *DataSink) ID() UnitID {
@@ -57,8 +52,6 @@ func (n *DataSink) StartBundle(ctx context.Context, id string, data DataContext)
 		return err
 	}
 	n.w = w
-	atomic.StoreInt64(&n.count, 0)
-	n.start = time.Now()
 	return nil
 }
 
@@ -67,7 +60,6 @@ func (n *DataSink) ProcessElement(ctx context.Context, value *FullValue, values
 	// unit.
 	var b bytes.Buffer
 
-	atomic.AddInt64(&n.count, 1)
 	if err := EncodeWindowedValueHeader(n.wEnc, value.Windows, value.Timestamp, &b); err != nil {
 		return err
 	}
@@ -81,7 +73,6 @@ func (n *DataSink) ProcessElement(ctx context.Context, value *FullValue, values
 }
 
 func (n *DataSink) FinishBundle(ctx context.Context) error {
-	log.Infof(ctx, "DataSink: %d elements in %d ns", atomic.LoadInt64(&n.count), time.Now().Sub(n.start))
 	return n.w.Close()
 }
 
diff --git a/sdks/go/pkg/beam/core/runtime/exec/datasource.go b/sdks/go/pkg/beam/core/runtime/exec/datasource.go
index fe9d54033e8..f5455070930 100644
--- a/sdks/go/pkg/beam/core/runtime/exec/datasource.go
+++ b/sdks/go/pkg/beam/core/runtime/exec/datasource.go
@@ -28,7 +28,6 @@ import (
 	"github.com/apache/beam/sdks/go/pkg/beam/core/graph/coder"
 	"github.com/apache/beam/sdks/go/pkg/beam/core/util/ioutilx"
 	"github.com/apache/beam/sdks/go/pkg/beam/internal/errors"
-	"github.com/apache/beam/sdks/go/pkg/beam/log"
 )
 
 // DataSource is a Root execution unit.
@@ -38,14 +37,14 @@ type DataSource struct {
 	Name  string
 	Coder *coder.Coder
 	Out   Node
+	PCol  PCollection // Handles size metrics. Value instead of pointer so it's initialized by default in tests.
 
 	source DataManager
 	state  StateReader
-	// TODO(lostluck) 2020/02/06: refactor to support more general PCollection metrics on nodes.
-	outputPID string // The index is the output count for the PCollection.
-	index     int64
-	splitIdx  int64
-	start     time.Time
+
+	index    int64
+	splitIdx int64
+	start    time.Time
 
 	// su is non-nil if this DataSource feeds directly to a splittable unit,
 	// and receives that splittable unit when it is available for splitting.
@@ -90,6 +89,29 @@ func (n *DataSource) StartBundle(ctx context.Context, id string, data DataContex
 	return n.Out.StartBundle(ctx, id, data)
 }
 
+// ByteCountReader is a passthrough reader that counts all the bytes read through it.
+// It trusts the nested reader to return accurate byte information.
+type byteCountReader struct {
+	count  *int
+	reader io.ReadCloser
+}
+
+func (r *byteCountReader) Read(p []byte) (int, error) {
+	n, err := r.reader.Read(p)
+	*r.count += n
+	return n, err
+}
+
+func (r *byteCountReader) Close() error {
+	return r.reader.Close()
+}
+
+func (r *byteCountReader) reset() int {
+	c := *r.count
+	*r.count = 0
+	return c
+}
+
 // Process opens the data source, reads and decodes data, kicking off element processing.
 func (n *DataSource) Process(ctx context.Context) error {
 	r, err := n.source.OpenRead(ctx, n.SID)
@@ -97,6 +119,9 @@ func (n *DataSource) Process(ctx context.Context) error {
 		return err
 	}
 	defer r.Close()
+	n.PCol.resetSize() // initialize the size distribution for this bundle.
+	var byteCount int
+	bcr := byteCountReader{reader: r, count: &byteCount}
 
 	c := coder.SkipW(n.Coder)
 	wc := MakeWindowDecoder(n.Coder.Window)
@@ -119,6 +144,7 @@ func (n *DataSource) Process(ctx context.Context) error {
 		if n.incrementIndexAndCheckSplit() {
 			return nil
 		}
+		// TODO(lostluck) 2020/02/22: Should we include window headers or just count the element sizes?
 		ws, t, err := DecodeWindowedValueHeader(wc, r)
 		if err != nil {
 			if err == io.EOF {
@@ -128,7 +154,7 @@ func (n *DataSource) Process(ctx context.Context) error {
 		}
 
 		// Decode key or parallel element.
-		pe, err := cp.Decode(r)
+		pe, err := cp.Decode(&bcr)
 		if err != nil {
 			return errors.Wrap(err, "source decode failed")
 		}
@@ -137,7 +163,7 @@ func (n *DataSource) Process(ctx context.Context) error {
 
 		var valReStreams []ReStream
 		for _, cv := range cvs {
-			values, err := n.makeReStream(ctx, pe, cv, r)
+			values, err := n.makeReStream(ctx, pe, cv, &bcr)
 			if err != nil {
 				return err
 			}
@@ -147,11 +173,15 @@ func (n *DataSource) Process(ctx context.Context) error {
 		if err := n.Out.ProcessElement(ctx, pe, valReStreams...); err != nil {
 			return err
 		}
+		// Collect the actual size of the element, and reset the bytecounter reader.
+		n.PCol.addSize(int64(bcr.reset()))
+		bcr.reader = r
 	}
 }
 
-func (n *DataSource) makeReStream(ctx context.Context, key *FullValue, cv ElementDecoder, r io.ReadCloser) (ReStream, error) {
-	size, err := coder.DecodeInt32(r)
+func (n *DataSource) makeReStream(ctx context.Context, key *FullValue, cv ElementDecoder, bcr *byteCountReader) (ReStream, error) {
+	// TODO(lostluck) 2020/02/22: Do we include the chunk size, or just the element sizes?
+	size, err := coder.DecodeInt32(bcr.reader)
 	if err != nil {
 		return nil, errors.Wrap(err, "stream size decoding failed")
 	}
@@ -160,16 +190,16 @@ func (n *DataSource) makeReStream(ctx context.Context, key *FullValue, cv Elemen
 	case size >= 0:
 		// Single chunk streams are fully read in and buffered in memory.
 		buf := make([]FullValue, 0, size)
-		buf, err = readStreamToBuffer(cv, r, int64(size), buf)
+		buf, err = readStreamToBuffer(cv, bcr, int64(size), buf)
 		if err != nil {
 			return nil, err
 		}
 		return &FixedReStream{Buf: buf}, nil
-	case size == -1: // Shouldn't this be 0?
+	case size == -1:
 		// Multi-chunked stream.
 		var buf []FullValue
 		for {
-			chunk, err := coder.DecodeVarInt(r)
+			chunk, err := coder.DecodeVarInt(bcr.reader)
 			if err != nil {
 				return nil, errors.Wrap(err, "stream chunk size decoding failed")
 			}
@@ -179,17 +209,17 @@ func (n *DataSource) makeReStream(ctx context.Context, key *FullValue, cv Elemen
 				return &FixedReStream{Buf: buf}, nil
 			case chunk > 0: // Non-zero chunk, read that many elements from the stream, and buffer them.
 				chunkBuf := make([]FullValue, 0, chunk)
-				chunkBuf, err = readStreamToBuffer(cv, r, chunk, chunkBuf)
+				chunkBuf, err = readStreamToBuffer(cv, bcr, chunk, chunkBuf)
 				if err != nil {
 					return nil, err
 				}
 				buf = append(buf, chunkBuf...)
 			case chunk == -1: // State backed iterable!
-				chunk, err := coder.DecodeVarInt(r)
+				chunk, err := coder.DecodeVarInt(bcr.reader)
 				if err != nil {
 					return nil, err
 				}
-				token, err := ioutilx.ReadN(r, (int)(chunk))
+				token, err := ioutilx.ReadN(bcr.reader, (int)(chunk))
 				if err != nil {
 					return nil, err
 				}
@@ -201,6 +231,9 @@ func (n *DataSource) makeReStream(ctx context.Context, key *FullValue, cv Elemen
 							if err != nil {
 								return nil, err
 							}
+							// We can't re-use the original bcr, since we may get new iterables,
+							// or multiple of them at the same time, but we can re-use the count itself.
+							r = &byteCountReader{reader: r, count: bcr.count}
 							return &elementStream{r: r, ec: cv}, nil
 						},
 					},
@@ -229,7 +262,6 @@ func readStreamToBuffer(cv ElementDecoder, r io.ReadCloser, size int64, buf []Fu
 func (n *DataSource) FinishBundle(ctx context.Context) error {
 	n.mu.Lock()
 	defer n.mu.Unlock()
-	log.Infof(ctx, "DataSource: %d elements in %d ns", n.index, time.Now().Sub(n.start))
 	n.source = nil
 	n.splitIdx = 0 // Ensure errors are returned for split requests if this plan is re-used.
 	return n.Out.FinishBundle(ctx)
@@ -261,12 +293,11 @@ func (n *DataSource) incrementIndexAndCheckSplit() bool {
 }
 
 // ProgressReportSnapshot captures the progress reading an input source.
-//
-// TODO(lostluck) 2020/02/06: Add a visitor pattern for collecting progress
-// metrics from downstream Nodes.
 type ProgressReportSnapshot struct {
-	ID, Name, PID string
-	Count         int64
+	ID, Name string
+	Count    int64
+
+	pcol PCollectionSnapshot
 }
 
 // Progress returns a snapshot of the source's progress.
@@ -275,6 +306,7 @@ func (n *DataSource) Progress() ProgressReportSnapshot {
 		return ProgressReportSnapshot{}
 	}
 	n.mu.Lock()
+	pcol := n.PCol.snapshot()
 	// The count is the number of "completely processed elements"
 	// which matches the index of the currently processing element.
 	c := n.index
@@ -283,7 +315,8 @@ func (n *DataSource) Progress() ProgressReportSnapshot {
 	if c < 0 {
 		c = 0
 	}
-	return ProgressReportSnapshot{PID: n.outputPID, ID: n.SID.PtransformID, Name: n.Name, Count: c}
+	pcol.ElementCount = c
+	return ProgressReportSnapshot{ID: n.SID.PtransformID, Name: n.Name, Count: c, pcol: pcol}
 }
 
 // Split takes a sorted set of potential split indices and a fraction of the
diff --git a/sdks/go/pkg/beam/core/runtime/exec/datasource_test.go b/sdks/go/pkg/beam/core/runtime/exec/datasource_test.go
index 15094ff8409..2a294249f45 100644
--- a/sdks/go/pkg/beam/core/runtime/exec/datasource_test.go
+++ b/sdks/go/pkg/beam/core/runtime/exec/datasource_test.go
@@ -203,6 +203,16 @@ func TestDataSource_Iterators(t *testing.T) {
 			if got, want := iVals, expectedKeys; !equalList(got, want) {
 				t.Errorf("DataSource => %#v, want %#v", extractValues(got...), extractValues(want...))
 			}
+
+			// We're using integers that encode to 1 byte, so do some quick math to validate.
+			sizeOfSmallInt := 1
+			snap := quickTestSnapshot(source, int64(len(test.keys)))
+			snap.pcol.SizeSum = int64(len(test.keys) * (1 + len(test.vals)) * sizeOfSmallInt)
+			snap.pcol.SizeMin = int64((1 + len(test.vals)) * sizeOfSmallInt)
+			snap.pcol.SizeMax = int64((1 + len(test.vals)) * sizeOfSmallInt)
+			if got, want := source.Progress(), snap; got != want {
+				t.Errorf("progress didn't match: got %v, want %v", got, want)
+			}
 		})
 	}
 }
@@ -371,15 +381,6 @@ func TestDataSource_Split(t *testing.T) {
 				})
 
 				validateSource(t, out, source, makeValues(test.expected...))
-
-				// Adjust expectations to maximum number of elements.
-				adjustedExpectation := test.splitIdx
-				if adjustedExpectation > int64(len(elements)) {
-					adjustedExpectation = int64(len(elements))
-				}
-				if got, want := source.Progress().Count, adjustedExpectation; got != want {
-					t.Fatalf("progress didn't match split: got %v, want %v", got, want)
-				}
 			})
 		}
 	})
@@ -871,13 +872,29 @@ func constructAndExecutePlanWithContext(t *testing.T, us []Unit, dc DataContext)
 	}
 }
 
+func quickTestSnapshot(source *DataSource, count int64) ProgressReportSnapshot {
+	return ProgressReportSnapshot{
+		Name:  source.Name,
+		ID:    source.SID.PtransformID,
+		Count: count,
+		pcol: PCollectionSnapshot{
+			ElementCount: count,
+			SizeCount:    count,
+			SizeSum:      count,
+			// We're only encoding small ints here, so size will only be 1.
+			SizeMin: 1,
+			SizeMax: 1,
+		},
+	}
+}
+
 func validateSource(t *testing.T, out *CaptureNode, source *DataSource, expected []FullValue) {
 	t.Helper()
 	if got, want := len(out.Elements), len(expected); got != want {
 		t.Fatalf("lengths don't match: got %v, want %v", got, want)
 	}
-	if got, want := source.Progress().Count, int64(len(expected)); got != want {
-		t.Fatalf("progress count didn't match: got %v, want %v", got, want)
+	if got, want := source.Progress(), quickTestSnapshot(source, int64(len(expected))); got != want {
+		t.Fatalf("progress snapshot didn't match: got %v, want %v", got, want)
 	}
 	if !equalList(out.Elements, expected) {
 		t.Errorf("DataSource => %#v, want %#v", extractValues(out.Elements...), extractValues(expected...))
diff --git a/sdks/go/pkg/beam/core/runtime/exec/pcollection.go b/sdks/go/pkg/beam/core/runtime/exec/pcollection.go
new file mode 100644
index 00000000000..3c74f189455
--- /dev/null
+++ b/sdks/go/pkg/beam/core/runtime/exec/pcollection.go
@@ -0,0 +1,158 @@
+// Licensed to the Apache Software Foundation (ASF) under one or more
+// contributor license agreements.  See the NOTICE file distributed with
+// this work for additional information regarding copyright ownership.
+// The ASF licenses this file to You under the Apache License, Version 2.0
+// (the "License"); you may not use this file except in compliance with
+// the License.  You may obtain a copy of the License at
+//
+//    http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package exec
+
+import (
+	"context"
+	"fmt"
+	"math"
+	"math/rand"
+	"sync"
+	"sync/atomic"
+
+	"github.com/apache/beam/sdks/go/pkg/beam/core/graph/coder"
+)
+
+// PCollection is a passthrough node to collect PCollection metrics, and
+// must be placed as the Out node of any producer of a PCollection.
+//
+// In particular, must not be placed after a Multiplex, and must be placed
+// after a Flatten.
+type PCollection struct {
+	UID    UnitID
+	PColID string
+	Out    Node // Out is the consumer of this PCollection.
+	Coder  *coder.Coder
+	Seed   int64
+
+	r             *rand.Rand
+	nextSampleIdx int64 // The index of the next value to sample.
+	elementCoder  ElementEncoder
+
+	elementCount                         int64 // must use atomic operations.
+	sizeMu                               sync.Mutex
+	sizeCount, sizeSum, sizeMin, sizeMax int64
+}
+
+// ID returns the debug id for this unit.
+func (p *PCollection) ID() UnitID {
+	return p.UID
+}
+
+// Up initializes the random sampling source and element encoder.
+func (p *PCollection) Up(ctx context.Context) error {
+	// dedicated rand source
+	p.r = rand.New(rand.NewSource(p.Seed))
+	p.elementCoder = MakeElementEncoder(p.Coder)
+	return nil
+}
+
+// StartBundle resets collected metrics for this PCollection, and propagates bundle start.
+func (p *PCollection) StartBundle(ctx context.Context, id string, data DataContext) error {
+	atomic.StoreInt64(&p.elementCount, 0)
+	p.nextSampleIdx = 1
+	p.resetSize()
+	return MultiStartBundle(ctx, id, data, p.Out)
+}
+
+type byteCounter struct {
+	count int
+}
+
+func (w *byteCounter) Write(p []byte) (n int, err error) {
+	w.count += len(p)
+	return len(p), nil
+}
+
+// ProcessElement increments the element count and sometimes takes size samples of the elements.
+func (p *PCollection) ProcessElement(ctx context.Context, elm *FullValue, values ...ReStream) error {
+	cur := atomic.AddInt64(&p.elementCount, 1)
+	if cur == p.nextSampleIdx {
+		// Always encode the first 3 elements. Otherwise...
+		// We pick the next sampling index based on how large this pcollection already is.
+		// We don't want to necessarily wait until the pcollection has doubled, so we reduce the range.
+		// We don't want to always encode the first consecutive elements, so we add 2 to give some variance.
+		// Finally we add 1 no matter what, so that it can trigger again.
+		// Otherwise, there's the potential for the random int to be 0, which means we don't change the
+		// nextSampleIdx at all.
+		if p.nextSampleIdx < 4 {
+			p.nextSampleIdx++
+		} else {
+			p.nextSampleIdx = cur + p.r.Int63n(cur/10+2) + 1
+		}
+		var w byteCounter
+		p.elementCoder.Encode(elm, &w)
+		p.addSize(int64(w.count))
+	}
+	return p.Out.ProcessElement(ctx, elm, values...)
+}
+
+func (p *PCollection) addSize(size int64) {
+	p.sizeMu.Lock()
+	defer p.sizeMu.Unlock()
+	p.sizeCount++
+	p.sizeSum += size
+	if size > p.sizeMax {
+		p.sizeMax = size
+	}
+	if size < p.sizeMin {
+		p.sizeMin = size
+	}
+}
+
+func (p *PCollection) resetSize() {
+	p.sizeMu.Lock()
+	defer p.sizeMu.Unlock()
+	p.sizeCount = 0
+	p.sizeSum = 0
+	p.sizeMax = math.MinInt64
+	p.sizeMin = math.MaxInt64
+}
+
+// FinishBundle propagates bundle termination.
+func (p *PCollection) FinishBundle(ctx context.Context) error {
+	return MultiFinishBundle(ctx, p.Out)
+}
+
+// Down is a no-op.
+func (p *PCollection) Down(ctx context.Context) error {
+	return nil
+}
+
+func (p *PCollection) String() string {
+	return fmt.Sprintf("PCollection[%v] Out:%v", p.PColID, IDs(p.Out))
+}
+
+// PCollectionSnapshot contains the PCollectionID
+type PCollectionSnapshot struct {
+	ID           string
+	ElementCount int64
+	// If SizeCount is zero, then no size metrics should be exported.
+	SizeCount, SizeSum, SizeMin, SizeMax int64
+}
+
+func (p *PCollection) snapshot() PCollectionSnapshot {
+	p.sizeMu.Lock()
+	defer p.sizeMu.Unlock()
+	return PCollectionSnapshot{
+		ID:           p.PColID,
+		ElementCount: atomic.LoadInt64(&p.elementCount),
+		SizeCount:    p.sizeCount,
+		SizeSum:      p.sizeSum,
+		SizeMin:      p.sizeMin,
+		SizeMax:      p.sizeMax,
+	}
+}
diff --git a/sdks/go/pkg/beam/core/runtime/exec/pcollection_test.go b/sdks/go/pkg/beam/core/runtime/exec/pcollection_test.go
new file mode 100644
index 00000000000..a79baa52f49
--- /dev/null
+++ b/sdks/go/pkg/beam/core/runtime/exec/pcollection_test.go
@@ -0,0 +1,154 @@
+// Licensed to the Apache Software Foundation (ASF) under one or more
+// contributor license agreements.  See the NOTICE file distributed with
+// this work for additional information regarding copyright ownership.
+// The ASF licenses this file to You under the Apache License, Version 2.0
+// (the "License"); you may not use this file except in compliance with
+// the License.  You may obtain a copy of the License at
+//
+//    http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package exec
+
+import (
+	"context"
+	"math"
+	"testing"
+
+	"github.com/apache/beam/sdks/go/pkg/beam/core/graph/coder"
+	"github.com/apache/beam/sdks/go/pkg/beam/core/graph/mtime"
+	"github.com/apache/beam/sdks/go/pkg/beam/core/graph/window"
+)
+
+// TestPCollection verifies that the PCollection node works correctly.
+// Seed is by default set to 0, so we have a "deterministic" set of
+// randomness for the samples.
+func TestPCollection(t *testing.T) {
+	a := &CaptureNode{UID: 1}
+	pcol := &PCollection{UID: 2, Out: a, Coder: coder.NewVarInt()}
+	// The "large" 2nd value is to ensure the values are encoded properly,
+	// and that Min & Max are behaving.
+	inputs := []interface{}{int64(1), int64(2000000000), int64(3)}
+	in := &FixedRoot{UID: 3, Elements: makeInput(inputs...), Out: pcol}
+
+	p, err := NewPlan("a", []Unit{a, pcol, in})
+	if err != nil {
+		t.Fatalf("failed to construct plan: %v", err)
+	}
+
+	if err := p.Execute(context.Background(), "1", DataContext{}); err != nil {
+		t.Fatalf("execute failed: %v", err)
+	}
+	if err := p.Down(context.Background()); err != nil {
+		t.Fatalf("down failed: %v", err)
+	}
+
+	expected := makeValues(inputs...)
+	if !equalList(a.Elements, expected) {
+		t.Errorf("multiplex returned %v for a, want %v", extractValues(a.Elements...), extractValues(expected...))
+	}
+	snap := pcol.snapshot()
+	if want, got := int64(len(expected)), snap.ElementCount; got != want {
+		t.Errorf("snapshot miscounted: got %v, want %v", got, want)
+	}
+	checkPCollectionSizeSample(t, snap, 3, 7, 1, 5)
+}
+
+func TestPCollection_sizeReset(t *testing.T) {
+	// Check the initial values after resetting.
+	var pcol PCollection
+	pcol.resetSize()
+	snap := pcol.snapshot()
+	checkPCollectionSizeSample(t, snap, 0, 0, math.MaxInt64, math.MinInt64)
+}
+
+func checkPCollectionSizeSample(t *testing.T, snap PCollectionSnapshot, count, sum, min, max int64) {
+	t.Helper()
+	if want, got := int64(count), snap.SizeCount; got != want {
+		t.Errorf("sample count incorrect: got %v, want %v", got, want)
+	}
+	if want, got := int64(sum), snap.SizeSum; got != want {
+		t.Errorf("sample sum incorrect: got %v, want %v", got, want)
+	}
+	if want, got := int64(min), snap.SizeMin; got != want {
+		t.Errorf("sample min incorrect: got %v, want %v", got, want)
+	}
+	if want, got := int64(max), snap.SizeMax; got != want {
+		t.Errorf("sample max incorrect: got %v, want %v", got, want)
+	}
+}
+
+// BenchmarkPCollection measures the overhead of invoking a ParDo in a plan.
+//
+// On @lostluck's desktop (2020/02/20):
+// BenchmarkPCollection-12                 44699806                24.8 ns/op             0 B/op          0 allocs/op
+func BenchmarkPCollection(b *testing.B) {
+	// Pre allocate the capture buffer and process buffer to avoid
+	// unnecessary overhead.
+	out := &CaptureNode{UID: 1, Elements: make([]FullValue, 0, b.N)}
+	process := make([]MainInput, 0, b.N)
+	for i := 0; i < b.N; i++ {
+		process = append(process, MainInput{Key: FullValue{
+			Windows:   window.SingleGlobalWindow,
+			Timestamp: mtime.ZeroTimestamp,
+			Elm:       int64(1),
+		}})
+	}
+	pcol := &PCollection{UID: 2, Out: out, Coder: coder.NewVarInt()}
+	n := &FixedRoot{UID: 3, Elements: process, Out: pcol}
+	p, err := NewPlan("a", []Unit{n, pcol, out})
+	if err != nil {
+		b.Fatalf("failed to construct plan: %v", err)
+	}
+	b.ResetTimer()
+	if err := p.Execute(context.Background(), "1", DataContext{}); err != nil {
+		b.Fatalf("execute failed: %v", err)
+	}
+	if err := p.Down(context.Background()); err != nil {
+		b.Fatalf("down failed: %v", err)
+	}
+	if got, want := pcol.snapshot().ElementCount, int64(b.N); got != want {
+		b.Errorf("did not process all elements: got %v, want %v", got, want)
+	}
+	if got, want := len(out.Elements), b.N; got != want {
+		b.Errorf("did not process all elements: got %v, want %v", got, want)
+	}
+}
+
+// BenchmarkPCollection_Baseline measures the baseline of the node benchmarking scaffold.
+//
+// On @lostluck's desktop (2020/02/20):
+// BenchmarkPCollection_Baseline-12        62186372                18.8 ns/op             0 B/op          0 allocs/op
+func BenchmarkPCollection_Baseline(b *testing.B) {
+	// Pre allocate the capture buffer and process buffer to avoid
+	// unnecessary overhead.
+	out := &CaptureNode{UID: 1, Elements: make([]FullValue, 0, b.N)}
+	process := make([]MainInput, 0, b.N)
+	for i := 0; i < b.N; i++ {
+		process = append(process, MainInput{Key: FullValue{
+			Windows:   window.SingleGlobalWindow,
+			Timestamp: mtime.ZeroTimestamp,
+			Elm:       1,
+		}})
+	}
+	n := &FixedRoot{UID: 3, Elements: process, Out: out}
+	p, err := NewPlan("a", []Unit{n, out})
+	if err != nil {
+		b.Fatalf("failed to construct plan: %v", err)
+	}
+	b.ResetTimer()
+	if err := p.Execute(context.Background(), "1", DataContext{}); err != nil {
+		b.Fatalf("execute failed: %v", err)
+	}
+	if err := p.Down(context.Background()); err != nil {
+		b.Fatalf("down failed: %v", err)
+	}
+	if got, want := len(out.Elements), b.N; got != want {
+		b.Errorf("did not process all elements: got %v, want %v", got, want)
+	}
+}
diff --git a/sdks/go/pkg/beam/core/runtime/exec/plan.go b/sdks/go/pkg/beam/core/runtime/exec/plan.go
index 13566d5fa72..8aec766af22 100644
--- a/sdks/go/pkg/beam/core/runtime/exec/plan.go
+++ b/sdks/go/pkg/beam/core/runtime/exec/plan.go
@@ -21,9 +21,7 @@ import (
 	"context"
 	"fmt"
 	"strings"
-	"sync"
 
-	"github.com/apache/beam/sdks/go/pkg/beam/core/metrics"
 	"github.com/apache/beam/sdks/go/pkg/beam/internal/errors"
 )
 
@@ -31,34 +29,22 @@ import (
 // from a part of a pipeline. A plan can be used to process multiple bundles
 // serially.
 type Plan struct {
-	id       string
-	roots    []Root
-	units    []Unit
-	parDoIDs []string
+	id    string // id of the bundle descriptor for this plan
+	roots []Root
+	units []Unit
+	pcols []*PCollection
 
 	status Status
 
-	// While the store is threadsafe, the reference to it
-	// is not, so we need to protect the store field to be
-	// able to asynchronously provide tentative metrics.
-	storeMu sync.Mutex
-	store   *metrics.Store
-
 	// TODO: there can be more than 1 DataSource in a bundle.
 	source *DataSource
 }
 
-// hasPID provides a common interface for extracting PTransformIDs
-// from Units.
-type hasPID interface {
-	GetPID() string
-}
-
 // NewPlan returns a new bundle execution plan from the given units.
 func NewPlan(id string, units []Unit) (*Plan, error) {
 	var roots []Root
+	var pcols []*PCollection
 	var source *DataSource
-	var pardoIDs []string
 
 	for _, u := range units {
 		if u == nil {
@@ -70,8 +56,8 @@ func NewPlan(id string, units []Unit) (*Plan, error) {
 		if s, ok := u.(*DataSource); ok {
 			source = s
 		}
-		if p, ok := u.(hasPID); ok {
-			pardoIDs = append(pardoIDs, p.GetPID())
+		if p, ok := u.(*PCollection); ok {
+			pcols = append(pcols, p)
 		}
 	}
 	if len(roots) == 0 {
@@ -79,12 +65,12 @@ func NewPlan(id string, units []Unit) (*Plan, error) {
 	}
 
 	return &Plan{
-		id:       id,
-		status:   Initializing,
-		roots:    roots,
-		units:    units,
-		parDoIDs: pardoIDs,
-		source:   source,
+		id:     id,
+		status: Initializing,
+		roots:  roots,
+		units:  units,
+		pcols:  pcols,
+		source: source,
 	}, nil
 }
 
@@ -102,10 +88,6 @@ func (p *Plan) SourcePTransformID() string {
 // are brought up on the first execution. If a bundle fails, the plan cannot
 // be reused for further bundles. Does not panic. Blocking.
 func (p *Plan) Execute(ctx context.Context, id string, manager DataContext) error {
-	ctx = metrics.SetBundleID(ctx, p.id)
-	p.storeMu.Lock()
-	p.store = metrics.GetStore(ctx)
-	p.storeMu.Unlock()
 	if p.status == Initializing {
 		for _, u := range p.units {
 			if err := callNoPanic(ctx, u.Up); err != nil {
@@ -181,19 +163,28 @@ func (p *Plan) String() string {
 	return fmt.Sprintf("Plan[%v]:\n%v", p.ID(), strings.Join(units, "\n"))
 }
 
-// Progress returns a snapshot of input progress of the plan, and associated metrics.
-func (p *Plan) Progress() (ProgressReportSnapshot, bool) {
-	if p.source != nil {
-		return p.source.Progress(), true
-	}
-	return ProgressReportSnapshot{}, false
+// PlanSnapshot contains system metrics for the current run of the plan.
+type PlanSnapshot struct {
+	Source ProgressReportSnapshot
+	PCols  []PCollectionSnapshot
 }
 
-// Store returns the metric store for the last use of this plan.
-func (p *Plan) Store() *metrics.Store {
-	p.storeMu.Lock()
-	defer p.storeMu.Unlock()
-	return p.store
+// Progress returns a snapshot of progress of the plan, and associated metrics. 
+// The retuend boolean indicates whether the plan includes a DataSource, which is
+// important for handling legacy metrics. This boolean will be removed once
+// we no longer return legacy metrics.
+func (p *Plan) Progress() (PlanSnapshot, bool) {
+	pcolSnaps := make([]PCollectionSnapshot, 0, len(p.pcols)+1) // include space for the datasource pcollection.
+	for _, pcol := range p.pcols {
+		pcolSnaps = append(pcolSnaps, pcol.snapshot())
+	}
+	snap := PlanSnapshot{PCols: pcolSnaps}
+	if p.source != nil {
+		snap.Source = p.source.Progress()
+		snap.PCols = append(pcolSnaps, snap.Source.pcol)
+		return snap, true
+	}
+	return snap, false
 }
 
 // SplitPoints captures the split requested by the Runner.
diff --git a/sdks/go/pkg/beam/core/runtime/exec/translate.go b/sdks/go/pkg/beam/core/runtime/exec/translate.go
index 415b8adbb2a..e5836d91d6d 100644
--- a/sdks/go/pkg/beam/core/runtime/exec/translate.go
+++ b/sdks/go/pkg/beam/core/runtime/exec/translate.go
@@ -82,12 +82,16 @@ func UnmarshalPlan(desc *fnpb.ProcessBundleDescriptor) (*Plan, error) {
 		for key, pid := range transform.GetOutputs() {
 			u.SID = StreamID{PtransformID: id, Port: port}
 			u.Name = key
-			u.outputPID = pid
 
 			u.Out, err = b.makePCollection(pid)
 			if err != nil {
 				return nil, err
 			}
+			// Elide the PCollection Node for DataSources
+			// DataSources can get byte samples directly, and can handle CoGBKs.
+			u.PCol = *u.Out.(*PCollection)
+			u.Out = u.PCol.Out
+			b.units = b.units[:len(b.units)-1]
 		}
 
 		b.units = append(b.units, u)
@@ -103,8 +107,8 @@ type builder struct {
 	succ map[string][]linkID // PCollectionID -> []linkID
 
 	windowing map[string]*window.WindowingStrategy
-	nodes     map[string]Node // PCollectionID -> Node (cache)
-	links     map[linkID]Node // linkID -> Node (cache)
+	nodes     map[string]*PCollection // PCollectionID -> Node (cache)
+	links     map[linkID]Node         // linkID -> Node (cache)
 
 	units []Unit // result
 	idgen *GenID
@@ -146,7 +150,7 @@ func newBuilder(desc *fnpb.ProcessBundleDescriptor) (*builder, error) {
 		succ: succ,
 
 		windowing: make(map[string]*window.WindowingStrategy),
-		nodes:     make(map[string]Node),
+		nodes:     make(map[string]*PCollection),
 		links:     make(map[linkID]Node),
 
 		idgen: &GenID{},
@@ -263,7 +267,7 @@ func (b *builder) makeCoderForPCollection(id string) (*coder.Coder, *coder.Windo
 	return c, wc, nil
 }
 
-func (b *builder) makePCollection(id string) (Node, error) {
+func (b *builder) makePCollection(id string) (*PCollection, error) {
 	if n, exists := b.nodes[id]; exists {
 		return n, nil
 	}
@@ -278,8 +282,11 @@ func (b *builder) makePCollection(id string) (Node, error) {
 		u = &Discard{UID: b.idgen.New()}
 
 	case 1:
-		return b.makeLink(id, list[0])
-
+		out, err := b.makeLink(id, list[0])
+		if err != nil {
+			return nil, err
+		}
+		return b.newPCollectionNode(id, out)
 	default:
 		// Multiplex.
 
@@ -296,7 +303,16 @@ func (b *builder) makePCollection(id string) (Node, error) {
 		b.units = append(b.units, u)
 		u = &Flatten{UID: b.idgen.New(), N: count, Out: u}
 	}
+	b.units = append(b.units, u)
+	return b.newPCollectionNode(id, u)
+}
 
+func (b *builder) newPCollectionNode(id string, out Node) (*PCollection, error) {
+	ec, _, err := b.makeCoderForPCollection(id)
+	if err != nil {
+		return nil, err
+	}
+	u := &PCollection{UID: b.idgen.New(), Out: out, PColID: id, Coder: ec, Seed: rand.Int63()}
 	b.nodes[id] = u
 	b.units = append(b.units, u)
 	return u, nil
@@ -498,7 +514,11 @@ func (b *builder) makeLink(from string, id linkID) (Node, error) {
 			for _, dc := range c.Components[1:] {
 				decoders = append(decoders, MakeElementDecoder(dc))
 			}
-			u = &Expand{UID: b.idgen.New(), ValueDecoders: decoders, Out: out[0]}
+			// Strip PCollections from Expand nodes, as CoGBK metrics are handled by
+			// the DataSource that preceeds them.
+			trueOut := out[0].(*PCollection).Out
+			b.units = b.units[:len(b.units)-1]
+			u = &Expand{UID: b.idgen.New(), ValueDecoders: decoders, Out: trueOut}
 
 		case graphx.URNReshuffleInput:
 			c, w, err := b.makeCoderForPCollection(from)
@@ -521,7 +541,7 @@ func (b *builder) makeLink(from string, id linkID) (Node, error) {
 			u = &ReshuffleOutput{UID: b.idgen.New(), Coder: coder.NewW(c, w), Out: out[0]}
 
 		default:
-			return nil, errors.Errorf("unexpected payload: %v", tp)
+			return nil, errors.Errorf("unexpected payload: %v", &tp)
 		}
 
 	case graphx.URNWindow:
diff --git a/sdks/go/pkg/beam/core/runtime/harness/harness.go b/sdks/go/pkg/beam/core/runtime/harness/harness.go
index 4a93fc53414..bb1d5664f6c 100644
--- a/sdks/go/pkg/beam/core/runtime/harness/harness.go
+++ b/sdks/go/pkg/beam/core/runtime/harness/harness.go
@@ -24,6 +24,7 @@ import (
 	"sync/atomic"
 	"time"
 
+	"github.com/apache/beam/sdks/go/pkg/beam/core/metrics"
 	"github.com/apache/beam/sdks/go/pkg/beam/core/runtime/exec"
 	"github.com/apache/beam/sdks/go/pkg/beam/core/util/hooks"
 	"github.com/apache/beam/sdks/go/pkg/beam/internal/errors"
@@ -97,6 +98,7 @@ func Main(ctx context.Context, loggingEndpoint, controlEndpoint string) error {
 		plans:       make(map[bundleDescriptorID][]*exec.Plan),
 		active:      make(map[instructionID]*exec.Plan),
 		inactive:    newCircleBuffer(),
+		metStore:    make(map[instructionID]*metrics.Store),
 		failed:      make(map[instructionID]error),
 		data:        &DataChannelManager{},
 		state:       &StateChannelManager{},
@@ -221,6 +223,8 @@ type control struct {
 	// a plan that's either about to start or has finished recently
 	// instructions in this queue should return empty responses to control messages.
 	inactive circleBuffer // protected by mu
+	// metric stores for active plans.
+	metStore map[instructionID]*metrics.Store // protected by mu
 	// plans that have failed during execution
 	failed map[instructionID]error // protected by mu
 	mu     sync.Mutex
@@ -293,6 +297,10 @@ func (c *control) handleInstruction(ctx context.Context, req *fnpb.InstructionRe
 		c.mu.Lock()
 		c.inactive.Remove(instID)
 		c.active[instID] = plan
+		// Get the user metrics store for this bundle.
+		ctx = metrics.SetBundleID(ctx, string(instID))
+		store := metrics.GetStore(ctx)
+		c.metStore[instID] = store
 		c.mu.Unlock()
 
 		if err != nil {
@@ -305,7 +313,7 @@ func (c *control) handleInstruction(ctx context.Context, req *fnpb.InstructionRe
 		data.Close()
 		state.Close()
 
-		mons, pylds := monitoring(plan)
+		mons, pylds := monitoring(plan, store)
 		// Move the plan back to the candidate state
 		c.mu.Lock()
 		// Mark the instruction as failed.
@@ -316,10 +324,10 @@ func (c *control) handleInstruction(ctx context.Context, req *fnpb.InstructionRe
 			c.plans[bdID] = append(c.plans[bdID], plan)
 		}
 		delete(c.active, instID)
-
 		if removed, ok := c.inactive.Insert(instID); ok {
 			delete(c.failed, removed) // Also GC old failed bundles.
 		}
+		delete(c.metStore, instID)
 		c.mu.Unlock()
 
 		if err != nil {
@@ -341,7 +349,7 @@ func (c *control) handleInstruction(ctx context.Context, req *fnpb.InstructionRe
 
 		ref := instructionID(msg.GetInstructionId())
 
-		plan, resp := c.getPlanOrResponse(ctx, "progress", instID, ref)
+		plan, store, resp := c.getPlanOrResponse(ctx, "progress", instID, ref)
 		if resp != nil {
 			return resp
 		}
@@ -354,7 +362,7 @@ func (c *control) handleInstruction(ctx context.Context, req *fnpb.InstructionRe
 			}
 		}
 
-		mons, pylds := monitoring(plan)
+		mons, pylds := monitoring(plan, store)
 
 		return &fnpb.InstructionResponse{
 			InstructionId: string(instID),
@@ -372,7 +380,7 @@ func (c *control) handleInstruction(ctx context.Context, req *fnpb.InstructionRe
 		log.Debugf(ctx, "PB Split: %v", msg)
 		ref := instructionID(msg.GetInstructionId())
 
-		plan, resp := c.getPlanOrResponse(ctx, "split", instID, ref)
+		plan, _, resp := c.getPlanOrResponse(ctx, "split", instID, ref)
 		if resp != nil {
 			return resp
 		}
@@ -471,22 +479,23 @@ func (c *control) handleInstruction(ctx context.Context, req *fnpb.InstructionRe
 // them as a parameter here instead, and relying on those proto internal would be brittle.
 //
 // Since this logic is subtle, it's been abstracted to a method to scope the defer unlock.
-func (c *control) getPlanOrResponse(ctx context.Context, kind string, instID, ref instructionID) (*exec.Plan, *fnpb.InstructionResponse) {
+func (c *control) getPlanOrResponse(ctx context.Context, kind string, instID, ref instructionID) (*exec.Plan, *metrics.Store, *fnpb.InstructionResponse) {
 	c.mu.Lock()
 	plan, ok := c.active[ref]
 	err := c.failed[ref]
+	store, _ := c.metStore[ref]
 	defer c.mu.Unlock()
 
 	if err != nil {
-		return nil, fail(ctx, instID, "failed to return %v: instruction %v failed: %v", kind, ref, err)
+		return nil, nil, fail(ctx, instID, "failed to return %v: instruction %v failed: %v", kind, ref, err)
 	}
 	if !ok {
 		if c.inactive.Contains(ref) {
-			return nil, nil
+			return nil, nil, nil
 		}
-		return nil, fail(ctx, instID, "failed to return %v: instruction %v not active", kind, ref)
+		return nil, nil, fail(ctx, instID, "failed to return %v: instruction %v not active", kind, ref)
 	}
-	return plan, nil
+	return plan, store, nil
 }
 
 func fail(ctx context.Context, id instructionID, format string, args ...interface{}) *fnpb.InstructionResponse {
diff --git a/sdks/go/pkg/beam/core/runtime/harness/monitoring.go b/sdks/go/pkg/beam/core/runtime/harness/monitoring.go
index c9ddb801d95..c659e17ec11 100644
--- a/sdks/go/pkg/beam/core/runtime/harness/monitoring.go
+++ b/sdks/go/pkg/beam/core/runtime/harness/monitoring.go
@@ -72,7 +72,7 @@ func (c *shortIDCache) getShortID(l metrics.Labels, urn metricsx.Urn) string {
 	c.shortIds2Infos[s] = &pipepb.MonitoringInfo{
 		Urn:    metricsx.UrnToString(urn),
 		Type:   metricsx.UrnToType(urn),
-		Labels: userLabels(l),
+		Labels: l.Map(),
 	}
 	return s
 }
@@ -102,8 +102,7 @@ func shortIdsToInfos(shortids []string) map[string]*pipepb.MonitoringInfo {
 	return defaultShortIDCache.shortIdsToInfos(shortids)
 }
 
-func monitoring(p *exec.Plan) ([]*pipepb.MonitoringInfo, map[string][]byte) {
-	store := p.Store()
+func monitoring(p *exec.Plan, store *metrics.Store) ([]*pipepb.MonitoringInfo, map[string][]byte) {
 	if store == nil {
 		return nil, nil
 	}
@@ -125,7 +124,7 @@ func monitoring(p *exec.Plan) ([]*pipepb.MonitoringInfo, map[string][]byte) {
 				&pipepb.MonitoringInfo{
 					Urn:     metricsx.UrnToString(metricsx.UrnUserSumInt64),
 					Type:    metricsx.UrnToType(metricsx.UrnUserSumInt64),
-					Labels:  userLabels(l),
+					Labels:  l.Map(),
 					Payload: payload,
 				})
 		},
@@ -140,7 +139,7 @@ func monitoring(p *exec.Plan) ([]*pipepb.MonitoringInfo, map[string][]byte) {
 				&pipepb.MonitoringInfo{
 					Urn:     metricsx.UrnToString(metricsx.UrnUserDistInt64),
 					Type:    metricsx.UrnToType(metricsx.UrnUserDistInt64),
-					Labels:  userLabels(l),
+					Labels:  l.Map(),
 					Payload: payload,
 				})
 		},
@@ -155,7 +154,7 @@ func monitoring(p *exec.Plan) ([]*pipepb.MonitoringInfo, map[string][]byte) {
 				&pipepb.MonitoringInfo{
 					Urn:     metricsx.UrnToString(metricsx.UrnUserLatestMsInt64),
 					Type:    metricsx.UrnToType(metricsx.UrnUserLatestMsInt64),
-					Labels:  userLabels(l),
+					Labels:  l.Map(),
 					Payload: payload,
 				})
 
@@ -163,44 +162,65 @@ func monitoring(p *exec.Plan) ([]*pipepb.MonitoringInfo, map[string][]byte) {
 	}.ExtractFrom(store)
 
 	// Get the execution monitoring information from the bundle plan.
-	if snapshot, ok := p.Progress(); ok {
-		payload, err := metricsx.Int64Counter(snapshot.Count)
+
+	snapshot, ok := p.Progress()
+	if !ok {
+		return monitoringInfo, payloads
+	}
+	for _, pcol := range snapshot.PCols {
+		payload, err := metricsx.Int64Counter(pcol.ElementCount)
 		if err != nil {
 			panic(err)
 		}
 
 		// TODO(BEAM-9934): This metric should account for elements in multiple windows.
-		payloads[getShortID(metrics.PCollectionLabels(snapshot.PID), metricsx.UrnElementCount)] = payload
+		payloads[getShortID(metrics.PCollectionLabels(pcol.ID), metricsx.UrnElementCount)] = payload
+
 		monitoringInfo = append(monitoringInfo,
 			&pipepb.MonitoringInfo{
 				Urn:  metricsx.UrnToString(metricsx.UrnElementCount),
 				Type: metricsx.UrnToType(metricsx.UrnElementCount),
 				Labels: map[string]string{
-					"PCOLLECTION": snapshot.PID,
+					"PCOLLECTION": pcol.ID,
 				},
 				Payload: payload,
 			})
 
-		payloads[getShortID(metrics.PTransformLabels(snapshot.ID), metricsx.UrnDataChannelReadIndex)] = payload
-		monitoringInfo = append(monitoringInfo,
-			&pipepb.MonitoringInfo{
-				Urn:  metricsx.UrnToString(metricsx.UrnDataChannelReadIndex),
-				Type: metricsx.UrnToType(metricsx.UrnDataChannelReadIndex),
-				Labels: map[string]string{
-					"PTRANSFORM": snapshot.ID,
-				},
-				Payload: payload,
-			})
-	}
+		// Skip pcollections without size
+		if pcol.SizeCount != 0 {
+			payload, err := metricsx.Int64Distribution(pcol.SizeCount, pcol.SizeSum, pcol.SizeMin, pcol.SizeMax)
+			if err != nil {
+				panic(err)
+			}
+			payloads[getShortID(metrics.PCollectionLabels(pcol.ID), metricsx.UrnSampledByteSize)] = payload
 
-	return monitoringInfo,
-		payloads
-}
+			monitoringInfo = append(monitoringInfo,
+				&pipepb.MonitoringInfo{
+					Urn:  metricsx.UrnToString(metricsx.UrnSampledByteSize),
+					Type: metricsx.UrnToType(metricsx.UrnSampledByteSize),
+					Labels: map[string]string{
+						"PCOLLECTION": pcol.ID,
+					},
+					Payload: payload,
+				})
+		}
+	}
 
-func userLabels(l metrics.Labels) map[string]string {
-	return map[string]string{
-		"PTRANSFORM": l.Transform(),
-		"NAMESPACE":  l.Namespace(),
-		"NAME":       l.Name(),
+	payload, err := metricsx.Int64Counter(snapshot.Source.Count)
+	if err != nil {
+		panic(err)
 	}
+
+	payloads[getShortID(metrics.PTransformLabels(snapshot.Source.ID), metricsx.UrnDataChannelReadIndex)] = payload
+	monitoringInfo = append(monitoringInfo,
+		&pipepb.MonitoringInfo{
+			Urn:  metricsx.UrnToString(metricsx.UrnDataChannelReadIndex),
+			Type: metricsx.UrnToType(metricsx.UrnDataChannelReadIndex),
+			Labels: map[string]string{
+				"PTRANSFORM": snapshot.Source.ID,
+			},
+			Payload: payload,
+		})
+
+	return monitoringInfo, payloads
 }
diff --git a/sdks/go/pkg/beam/runners/direct/direct.go b/sdks/go/pkg/beam/runners/direct/direct.go
index d304443f76f..da9f6ca5fc6 100644
--- a/sdks/go/pkg/beam/runners/direct/direct.go
+++ b/sdks/go/pkg/beam/runners/direct/direct.go
@@ -47,6 +47,7 @@ func Execute(ctx context.Context, p *beam.Pipeline) (beam.PipelineResult, error)
 
 	log.Info(ctx, "Pipeline:")
 	log.Info(ctx, p)
+	ctx = metrics.SetBundleID(ctx, "direct") // Ensure a metrics.Store exists.
 
 	if *jobopts.Strict {
 		log.Info(ctx, "Strict mode enabled, applying additional validation.")
@@ -75,7 +76,7 @@ func Execute(ctx context.Context, p *beam.Pipeline) (beam.PipelineResult, error)
 	}
 	// TODO(lostluck) 2020/01/24: What's the right way to expose the
 	// metrics store for the direct runner?
-	metrics.DumpToLogFromStore(ctx, plan.Store())
+	metrics.DumpToLog(ctx)
 	return nil, nil
 }
 
