diff --git a/sdks/go/pkg/beam/core/runtime/harness/datamgr.go b/sdks/go/pkg/beam/core/runtime/harness/datamgr.go
index 3fb9401dd01..08e62f2233c 100644
--- a/sdks/go/pkg/beam/core/runtime/harness/datamgr.go
+++ b/sdks/go/pkg/beam/core/runtime/harness/datamgr.go
@@ -38,8 +38,6 @@ type ScopedDataManager struct {
 	mgr    *DataChannelManager
 	instID instructionID
 
-	// TODO(herohde) 7/20/2018: capture and force close open reads/writes. However,
-	// we would need the underlying Close to be idempotent or a separate method.
 	closed bool
 	mu     sync.Mutex
 }
@@ -82,9 +80,10 @@ func (s *ScopedDataManager) open(ctx context.Context, port exec.Port) (*DataChan
 // Close prevents new IO for this instruction.
 func (s *ScopedDataManager) Close() error {
 	s.mu.Lock()
+	defer s.mu.Unlock()
 	s.closed = true
+	s.mgr.closeInstruction(s.instID)
 	s.mgr = nil
-	s.mu.Unlock()
 	return nil
 }
 
@@ -125,6 +124,14 @@ func (m *DataChannelManager) Open(ctx context.Context, port exec.Port) (*DataCha
 	return ch, nil
 }
 
+func (m *DataChannelManager) closeInstruction(instID instructionID) {
+	m.mu.Lock()
+	defer m.mu.Unlock()
+	for _, ch := range m.ports {
+		ch.removeInstruction(instID)
+	}
+}
+
 // clientID identifies a client of a connected channel.
 type clientID struct {
 	ptransformID string
@@ -148,8 +155,13 @@ type DataChannel struct {
 	id     string
 	client dataClient
 
-	writers map[clientID]*dataWriter
-	readers map[clientID]*dataReader
+	writers map[instructionID]map[string]*dataWriter
+	readers map[instructionID]map[string]*dataReader
+
+	// recently terminated instructions
+	endedInstructions map[instructionID]struct{}
+	rmQueue           []instructionID
+
 	// readErr indicates a client.Recv error and is used to prevent new readers.
 	readErr error
 
@@ -178,11 +190,12 @@ func newDataChannel(ctx context.Context, port exec.Port) (*DataChannel, error) {
 
 func makeDataChannel(ctx context.Context, id string, client dataClient, cancelFn context.CancelFunc) *DataChannel {
 	ret := &DataChannel{
-		id:       id,
-		client:   client,
-		writers:  make(map[clientID]*dataWriter),
-		readers:  make(map[clientID]*dataReader),
-		cancelFn: cancelFn,
+		id:                id,
+		client:            client,
+		writers:           make(map[instructionID]map[string]*dataWriter),
+		readers:           make(map[instructionID]map[string]*dataReader),
+		endedInstructions: make(map[instructionID]struct{}),
+		cancelFn:          cancelFn,
 	}
 	go ret.read(ctx)
 
@@ -227,14 +240,16 @@ func (c *DataChannel) read(ctx context.Context) {
 			// close the r.buf channels twice, or send on a closed channel.
 			// Any other approach is racy, and may cause one of the above
 			// panics.
-			for _, r := range c.readers {
-				log.Errorf(ctx, "DataChannel.read %v reader %v closing due to error on channel", c.id, r.id)
-				if !r.completed {
-					r.completed = true
-					r.err = err
-					close(r.buf)
+			for _, m := range c.readers {
+				for _, r := range m {
+					log.Errorf(ctx, "DataChannel.read %v reader %v closing due to error on channel", c.id, r.id)
+					if !r.completed {
+						r.completed = true
+						r.err = err
+						close(r.buf)
+					}
+					delete(cache, r.id)
 				}
-				delete(cache, r.id)
 			}
 			c.terminateStreamOnError(err)
 			c.mu.Unlock()
@@ -319,31 +334,95 @@ func (r *errReader) Close() error {
 
 // makeReader creates a dataReader. It expects to be called while c.mu is held.
 func (c *DataChannel) makeReader(ctx context.Context, id clientID) *dataReader {
-	if r, ok := c.readers[id]; ok {
+	var m map[string]*dataReader
+	var ok bool
+	if m, ok = c.readers[id.instID]; !ok {
+		m = make(map[string]*dataReader)
+		c.readers[id.instID] = m
+	}
+
+	if r, ok := m[id.ptransformID]; ok {
 		return r
 	}
 
 	r := &dataReader{id: id, buf: make(chan []byte, bufElements), done: make(chan bool, 1), channel: c}
-	c.readers[id] = r
+
+	// Just in case initial data for an instruction arrives *after* an instructon has ended.
+	// eg. it was blocked by another reader being slow, or the other instruction failed.
+	// So we provide a pre-completed reader, and do not cache it, as there's no further cleanup for it.
+	if _, ok := c.endedInstructions[id.instID]; ok {
+		r.completed = true
+		close(r.buf)
+		r.err = io.EOF // In case of any actual data readers, so they terminate without error.
+		return r
+	}
+
+	m[id.ptransformID] = r
 	return r
 }
 
 func (c *DataChannel) removeReader(id clientID) {
 	c.mu.Lock()
-	delete(c.readers, id)
+	if m, ok := c.readers[id.instID]; ok {
+		delete(m, id.ptransformID)
+	}
 	c.mu.Unlock()
 }
 
+const endedInstructionCap = 32
+
+// removeInstruction closes all readers and writers registered for the instruction
+// and deletes this instruction from the channel's reader and writer maps.
+func (c *DataChannel) removeInstruction(instID instructionID) {
+	c.mu.Lock()
+
+	// We don't want to leak memory, so cap the endedInstructions list.
+	if len(c.rmQueue) >= endedInstructionCap {
+		toRemove := c.rmQueue[0]
+		c.rmQueue = c.rmQueue[1:]
+		delete(c.endedInstructions, toRemove)
+	}
+	c.endedInstructions[instID] = struct{}{}
+	c.rmQueue = append(c.rmQueue, instID)
+
+	rs := c.readers[instID]
+	ws := c.writers[instID]
+
+	// Prevent other users while we iterate.
+	delete(c.readers, instID)
+	delete(c.writers, instID)
+	c.mu.Unlock()
+
+	// Close grabs the channel lock, so this must be outside the critical section.
+	for _, r := range rs {
+		r.Close()
+	}
+	for _, w := range ws {
+		w.Close()
+	}
+}
+
 func (c *DataChannel) makeWriter(ctx context.Context, id clientID) *dataWriter {
 	c.mu.Lock()
 	defer c.mu.Unlock()
 
-	if w, ok := c.writers[id]; ok {
+	var m map[string]*dataWriter
+	var ok bool
+	if m, ok = c.writers[id.instID]; !ok {
+		m = make(map[string]*dataWriter)
+		c.writers[id.instID] = m
+	}
+
+	if w, ok := m[id.ptransformID]; ok {
 		return w
 	}
 
+	// We don't check for ended instructions for writers, as writers
+	// can only be created if an instruction is in scope, and aren't
+	// runner or user directed.
+
 	w := &dataWriter{ch: c, id: id}
-	c.writers[id] = w
+	m[id.ptransformID] = w
 	return w
 }
 
@@ -389,9 +468,6 @@ func (r *dataReader) Read(buf []byte) (int, error) {
 	return n, nil
 }
 
-// TODO(herohde) 7/20/2018: we should probably either not be tracking writers or
-// make dataWriter threadsafe. Either case is likely a corruption generator.
-
 type dataWriter struct {
 	buf []byte
 
@@ -431,7 +507,7 @@ func (w *dataWriter) Close() error {
 	// Now acquire the locks since we're sending.
 	w.ch.mu.Lock()
 	defer w.ch.mu.Unlock()
-	delete(w.ch.writers, w.id)
+	delete(w.ch.writers[w.id.instID], w.id.ptransformID)
 	msg := &fnpb.Elements{
 		Data: []*fnpb.Elements_Data{
 			{
diff --git a/sdks/go/pkg/beam/core/runtime/harness/datamgr_test.go b/sdks/go/pkg/beam/core/runtime/harness/datamgr_test.go
index 5601c3823fd..31172e8a04b 100644
--- a/sdks/go/pkg/beam/core/runtime/harness/datamgr_test.go
+++ b/sdks/go/pkg/beam/core/runtime/harness/datamgr_test.go
@@ -22,6 +22,7 @@ import (
 	"io/ioutil"
 	"log"
 	"strings"
+	"sync"
 	"testing"
 	"time"
 
@@ -38,9 +39,14 @@ type fakeDataClient struct {
 	calls          int
 	err            error
 	skipFirstError bool
+
+	blocked sync.Mutex // Prevent data from being read by the gotourtinr.
 }
 
 func (f *fakeDataClient) Recv() (*fnpb.Elements, error) {
+	f.blocked.Lock()
+	defer f.blocked.Unlock()
+
 	f.calls++
 	data := []byte{1, 2, 3, 4, 1, 2, 3, 4}
 	elemData := fnpb.Elements_Data{
@@ -130,6 +136,12 @@ func TestDataChannelTerminate_dataReader(t *testing.T) {
 				// Set the 2nd Recv call to have an error.
 				client.err = expectedError
 			},
+		}, {
+			name:          "onInstructionEnd",
+			expectedError: io.EOF,
+			caseFn: func(t *testing.T, r io.ReadCloser, client *fakeDataClient, c *DataChannel) {
+				c.removeInstruction("inst_ref")
+			},
 		},
 	}
 	for _, test := range tests {
@@ -170,7 +182,46 @@ func TestDataChannelTerminate_dataReader(t *testing.T) {
 			}
 		})
 	}
+}
+
+func TestDataChannelRemoveInstruction_dataAfterClose(t *testing.T) {
+	done := make(chan bool, 1)
+	client := &fakeDataClient{t: t, done: done}
+	client.blocked.Lock()
+
+	ctx, cancelFn := context.WithCancel(context.Background())
+	c := makeDataChannel(ctx, "id", client, cancelFn)
+	c.removeInstruction("inst_ref")
+
+	client.blocked.Unlock()
+
+	r := c.OpenRead(ctx, "ptr", "inst_ref")
+
+	dr := r.(*dataReader)
+	if !dr.completed || dr.err != io.EOF {
+		t.Errorf("Expected a closed reader, but was still open: completed: %v, err: %v", dr.completed, dr.err)
+	}
 
+	n, err := r.Read(make([]byte, 4))
+	if err != io.EOF {
+		t.Errorf("Unexpected error from read: %v, read %d bytes.", err, n)
+	}
+}
+
+func TestDataChannelRemoveInstruction_limitInstructionCap(t *testing.T) {
+	done := make(chan bool, 1)
+	client := &fakeDataClient{t: t, done: done}
+	ctx, cancelFn := context.WithCancel(context.Background())
+	c := makeDataChannel(ctx, "id", client, cancelFn)
+
+	for i := 0; i < endedInstructionCap+10; i++ {
+		instID := instructionID(fmt.Sprintf("inst_ref%d", i))
+		c.OpenRead(ctx, "ptr", instID)
+		c.removeInstruction(instID)
+	}
+	if got, want := len(c.endedInstructions), endedInstructionCap; got != want {
+		t.Errorf("unexpected len(endedInstructions) got %v, want %v,", got, want)
+	}
 }
 
 func TestDataChannelTerminate_Writes(t *testing.T) {
@@ -179,27 +230,34 @@ func TestDataChannelTerminate_Writes(t *testing.T) {
 
 	expectedError := fmt.Errorf("EXPECTED ERROR")
 
+	instID := instructionID("inst_ref")
 	tests := []struct {
 		name   string
-		caseFn func(t *testing.T, w io.WriteCloser, client *fakeDataClient) error
+		caseFn func(t *testing.T, w io.WriteCloser, client *fakeDataClient, c *DataChannel) error
 	}{
 		{
 			name: "onClose_Flush",
-			caseFn: func(t *testing.T, w io.WriteCloser, client *fakeDataClient) error {
+			caseFn: func(t *testing.T, w io.WriteCloser, client *fakeDataClient, c *DataChannel) error {
 				return w.Close()
 			},
 		}, {
 			name: "onClose_Sentinel",
-			caseFn: func(t *testing.T, w io.WriteCloser, client *fakeDataClient) error {
+			caseFn: func(t *testing.T, w io.WriteCloser, client *fakeDataClient, c *DataChannel) error {
 				client.skipFirstError = true
 				return w.Close()
 			},
 		}, {
 			name: "onWrite",
-			caseFn: func(t *testing.T, w io.WriteCloser, client *fakeDataClient) error {
+			caseFn: func(t *testing.T, w io.WriteCloser, client *fakeDataClient, c *DataChannel) error {
 				_, err := w.Write([]byte{'d', 'o', 'n', 'e'})
 				return err
 			},
+		}, {
+			name: "onInstructionEnd",
+			caseFn: func(t *testing.T, w io.WriteCloser, client *fakeDataClient, c *DataChannel) error {
+				c.removeInstruction(instID)
+				return expectedError
+			},
 		},
 	}
 	for _, test := range tests {
@@ -209,7 +267,7 @@ func TestDataChannelTerminate_Writes(t *testing.T) {
 			ctx, cancelFn := context.WithCancel(context.Background())
 			c := makeDataChannel(ctx, "id", client, cancelFn)
 
-			w := c.OpenWrite(ctx, "ptr", "inst_ref")
+			w := c.OpenWrite(ctx, "ptr", instID)
 
 			msg := []byte{'b', 'y', 't', 'e'}
 			var bufSize int
@@ -221,7 +279,7 @@ func TestDataChannelTerminate_Writes(t *testing.T) {
 				}
 			}
 
-			err := test.caseFn(t, w, client)
+			err := test.caseFn(t, w, client, c)
 
 			if got, want := err, expectedError; err == nil || !strings.Contains(err.Error(), expectedError.Error()) {
 				t.Errorf("Unexpected error: got %v, want %v", got, want)
@@ -229,7 +287,7 @@ func TestDataChannelTerminate_Writes(t *testing.T) {
 			// Verify that new readers return the same error for writes after stream termination.
 			// TODO(lostluck) 2019.11.26: use the the go 1.13 errors package to check this rather
 			// than a strings.Contains check once testing infrastructure can use go 1.13.
-			if n, err := c.OpenWrite(ctx, "ptr", "inst_ref").Write(msg); err != nil && !strings.Contains(err.Error(), expectedError.Error()) {
+			if n, err := c.OpenWrite(ctx, "ptr", instID).Write(msg); err != nil && !strings.Contains(err.Error(), expectedError.Error()) {
 				t.Errorf("Unexpected error from write: got %v, want, %v read %d bytes.", err, expectedError, n)
 			}
 			select {
@@ -240,5 +298,4 @@ func TestDataChannelTerminate_Writes(t *testing.T) {
 			}
 		})
 	}
-
 }
