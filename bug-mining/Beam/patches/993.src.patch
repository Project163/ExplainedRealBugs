diff --git a/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkPipelineExecutionEnvironment.java b/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkPipelineExecutionEnvironment.java
index ebd0af2f0ff..daddcd5e223 100644
--- a/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkPipelineExecutionEnvironment.java
+++ b/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkPipelineExecutionEnvironment.java
@@ -26,6 +26,8 @@ import org.apache.flink.api.common.JobExecutionResult;
 import org.apache.flink.api.java.ExecutionEnvironment;
 import org.apache.flink.runtime.jobgraph.JobGraph;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 /**
  * The class that instantiates and manages the execution of a given job. Depending on if the job is
@@ -37,6 +39,9 @@ import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
  */
 class FlinkPipelineExecutionEnvironment {
 
+  private static final Logger LOG =
+      LoggerFactory.getLogger(FlinkPipelineExecutionEnvironment.class);
+
   private final FlinkPipelineOptions options;
 
   /**
@@ -76,6 +81,8 @@ class FlinkPipelineExecutionEnvironment {
     this.flinkBatchEnv = null;
     this.flinkStreamEnv = null;
 
+    pipeline.replaceAll(FlinkTransformOverrides.getDefaultOverrides(options));
+
     PipelineTranslationModeOptimizer optimizer = new PipelineTranslationModeOptimizer(options);
     optimizer.translate(pipeline);
 
@@ -84,6 +91,11 @@ class FlinkPipelineExecutionEnvironment {
       this.flinkStreamEnv =
           FlinkExecutionEnvironments.createStreamExecutionEnvironment(
               options, options.getFilesToStage());
+      if (optimizer.hasUnboundedSources()
+          && !flinkStreamEnv.getCheckpointConfig().isCheckpointingEnabled()) {
+        LOG.warn(
+            "UnboundedSources present which rely on checkpointing, but checkpointing is disabled.");
+      }
       translator = new FlinkStreamingPipelineTranslator(flinkStreamEnv, options);
     } else {
       this.flinkBatchEnv =
@@ -92,7 +104,6 @@ class FlinkPipelineExecutionEnvironment {
       translator = new FlinkBatchPipelineTranslator(flinkBatchEnv, options);
     }
 
-    pipeline.replaceAll(FlinkTransformOverrides.getDefaultOverrides(options));
     prepareFilesToStageForRemoteClusterExecution(options);
 
     translator.translate(pipeline);
diff --git a/runners/flink/src/main/java/org/apache/beam/runners/flink/PipelineTranslationModeOptimizer.java b/runners/flink/src/main/java/org/apache/beam/runners/flink/PipelineTranslationModeOptimizer.java
index 7eb8b23ef13..f852ad7d769 100644
--- a/runners/flink/src/main/java/org/apache/beam/runners/flink/PipelineTranslationModeOptimizer.java
+++ b/runners/flink/src/main/java/org/apache/beam/runners/flink/PipelineTranslationModeOptimizer.java
@@ -17,12 +17,14 @@
  */
 package org.apache.beam.runners.flink;
 
+import org.apache.beam.sdk.Pipeline;
 import org.apache.beam.sdk.runners.AppliedPTransform;
 import org.apache.beam.sdk.runners.TransformHierarchy;
 import org.apache.beam.sdk.transforms.PTransform;
 import org.apache.beam.sdk.values.PCollection;
 import org.apache.beam.sdk.values.PCollection.IsBounded;
 import org.apache.beam.sdk.values.PValue;
+import org.apache.flink.util.Preconditions;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -35,11 +37,19 @@ class PipelineTranslationModeOptimizer extends FlinkPipelineTranslator {
   private static final Logger LOG = LoggerFactory.getLogger(PipelineTranslationModeOptimizer.class);
 
   private final FlinkPipelineOptions options;
+  private boolean hasRun;
+  private boolean hasUnboundedSources;
 
   public PipelineTranslationModeOptimizer(FlinkPipelineOptions options) {
     this.options = options;
   }
 
+  @Override
+  public void translate(Pipeline pipeline) {
+    super.translate(pipeline);
+    hasRun = true;
+  }
+
   @Override
   public CompositeBehavior enterCompositeTransform(TransformHierarchy.Node node) {
     return CompositeBehavior.ENTER_TRANSFORM;
@@ -55,6 +65,7 @@ class PipelineTranslationModeOptimizer extends FlinkPipelineTranslator {
       Class<? extends PTransform> transformClass = node.getTransform().getClass();
       LOG.info("Found {}. Switching to streaming execution.", transformClass);
       options.setStreaming(true);
+      hasUnboundedSources = true;
     }
   }
 
@@ -67,4 +78,9 @@ class PipelineTranslationModeOptimizer extends FlinkPipelineTranslator {
 
   @Override
   public void visitValue(PValue value, TransformHierarchy.Node producer) {}
+
+  boolean hasUnboundedSources() {
+    Preconditions.checkState(hasRun, "%s has not run yet.", getClass().getSimpleName());
+    return hasUnboundedSources;
+  }
 }
diff --git a/runners/flink/src/test/java/org/apache/beam/runners/flink/FlinkPipelineExecutionEnvironmentTest.java b/runners/flink/src/test/java/org/apache/beam/runners/flink/FlinkPipelineExecutionEnvironmentTest.java
index a06c4fb8c6a..f86a0dd57b4 100644
--- a/runners/flink/src/test/java/org/apache/beam/runners/flink/FlinkPipelineExecutionEnvironmentTest.java
+++ b/runners/flink/src/test/java/org/apache/beam/runners/flink/FlinkPipelineExecutionEnvironmentTest.java
@@ -19,13 +19,18 @@ package org.apache.beam.runners.flink;
 
 import static java.util.Arrays.asList;
 import static org.apache.beam.sdk.testing.RegexMatcher.matches;
+import static org.hamcrest.CoreMatchers.containsString;
 import static org.hamcrest.CoreMatchers.is;
 import static org.hamcrest.CoreMatchers.not;
+import static org.hamcrest.MatcherAssert.assertThat;
 import static org.hamcrest.core.Every.everyItem;
-import static org.junit.Assert.assertThat;
+import static org.junit.Assert.fail;
 
+import com.google.common.base.Charsets;
+import java.io.ByteArrayOutputStream;
 import java.io.File;
 import java.io.IOException;
+import java.io.PrintStream;
 import java.io.Serializable;
 import java.util.List;
 import org.apache.beam.sdk.Pipeline;
@@ -135,6 +140,43 @@ public class FlinkPipelineExecutionEnvironmentTest implements Serializable {
     }
   }
 
+  @Test
+  public void shouldLogWarningWhenCheckpointingIsDisabled() {
+    Pipeline pipeline = Pipeline.create();
+    pipeline.getOptions().setRunner(TestFlinkRunner.class);
+
+    pipeline
+        // Add an UnboundedSource to check for the warning if checkpointing is disabled
+        .apply(GenerateSequence.from(0))
+        .apply(
+            ParDo.of(
+                new DoFn<Long, Void>() {
+                  @ProcessElement
+                  public void processElement(ProcessContext ctx) {
+                    throw new RuntimeException("Failing here is ok.");
+                  }
+                }));
+
+    final PrintStream oldErr = System.err;
+    ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();
+    PrintStream replacementStdErr = new PrintStream(byteArrayOutputStream);
+    try {
+      System.setErr(replacementStdErr);
+      // Run pipeline and fail during execution
+      pipeline.run();
+      fail("Should have failed");
+    } catch (Exception e) {
+      // We want to fail here
+    } finally {
+      System.setErr(oldErr);
+    }
+    replacementStdErr.flush();
+    assertThat(
+        new String(byteArrayOutputStream.toByteArray(), Charsets.UTF_8),
+        containsString(
+            "UnboundedSources present which rely on checkpointing, but checkpointing is disabled."));
+  }
+
   private FlinkPipelineOptions testPreparingResourcesToStage(String flinkMaster)
       throws IOException {
     Pipeline pipeline = Pipeline.create();
