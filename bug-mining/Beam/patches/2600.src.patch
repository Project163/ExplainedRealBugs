diff --git a/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.java b/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.java
index 8bf1a22ae56..c6e28fd1b1e 100644
--- a/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.java
+++ b/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.java
@@ -1756,8 +1756,14 @@ public class BigQueryIO {
        * BigQuery</a>.
        */
       STREAMING_INSERTS,
-      /** Use the new, experimental Storage Write API. */
-      STORAGE_WRITE_API
+      /** Use the new, exactly-once Storage Write API. */
+      STORAGE_WRITE_API,
+      /**
+       * Use the new, Storage Write API without exactly once enabled. This will be cheaper and
+       * provide lower latency, however comes with the caveat that the output table may contain
+       * duplicates.
+       */
+      STORAGE_API_AT_LEAST_ONCE
     }
 
     abstract @Nullable ValueProvider<String> getJsonTableRef();
@@ -2508,8 +2514,11 @@ public class BigQueryIO {
       if (getMethod() != Write.Method.DEFAULT) {
         return getMethod();
       }
-      if (input.getPipeline().getOptions().as(BigQueryOptions.class).getUseStorageWriteApi()) {
-        return Write.Method.STORAGE_WRITE_API;
+      BigQueryOptions bqOptions = input.getPipeline().getOptions().as(BigQueryOptions.class);
+      if (bqOptions.getUseStorageWriteApi()) {
+        return bqOptions.getUseStorageWriteApiAtLeastOnce()
+            ? Method.STORAGE_API_AT_LEAST_ONCE
+            : Method.STORAGE_WRITE_API;
       }
       // By default, when writing an Unbounded PCollection, we use StreamingInserts and
       // BigQuery's streaming import API.
@@ -2857,7 +2866,7 @@ public class BigQueryIO {
           batchLoads.setNumFileShards(getNumFileShards());
         }
         return input.apply(batchLoads);
-      } else if (method == Write.Method.STORAGE_WRITE_API) {
+      } else if (method == Method.STORAGE_WRITE_API || method == Method.STORAGE_API_AT_LEAST_ONCE) {
         StorageApiDynamicDestinations<T, DestinationT> storageApiDynamicDestinations;
         if (getUseBeamSchema()) {
           // This ensures that the Beam rows are directly translated into protos for Sorage API
@@ -2884,7 +2893,8 @@ public class BigQueryIO {
                 getKmsKey(),
                 getStorageApiTriggeringFrequency(bqOptions),
                 getBigQueryServices(),
-                getStorageApiNumStreams(bqOptions));
+                getStorageApiNumStreams(bqOptions),
+                method == Method.STORAGE_API_AT_LEAST_ONCE);
         return input.apply("StorageApiLoads", storageApiLoads);
       } else {
         throw new RuntimeException("Unexpected write method " + method);
diff --git a/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryOptions.java b/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryOptions.java
index 165898599c8..e9df915f276 100644
--- a/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryOptions.java
+++ b/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryOptions.java
@@ -85,6 +85,13 @@ public interface BigQueryOptions
 
   void setUseStorageWriteApi(Boolean value);
 
+  @Description(
+      "If set, then BigQueryIO.Write will default to using the approximate Storage Write API.")
+  @Default.Boolean(false)
+  Boolean getUseStorageWriteApiAtLeastOnce();
+
+  void setUseStorageWriteApiAtLeastOnce(Boolean value);
+
   @Description(
       "If set, then BigQueryIO.Write will default to using this number of Storage Write API streams.")
   @Default.Integer(0)
diff --git a/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/SplittingIterable.java b/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/SplittingIterable.java
new file mode 100644
index 00000000000..cf367f3f427
--- /dev/null
+++ b/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/SplittingIterable.java
@@ -0,0 +1,69 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.beam.sdk.io.gcp.bigquery;
+
+import com.google.cloud.bigquery.storage.v1beta2.ProtoRows;
+import com.google.protobuf.ByteString;
+import java.util.Iterator;
+import java.util.NoSuchElementException;
+
+/**
+ * Takes in an iterable and batches the results into multiple ProtoRows objects. The splitSize
+ * parameter controls how many rows are batched into a single ProtoRows object before we move on to
+ * the next one.
+ */
+class SplittingIterable implements Iterable<ProtoRows> {
+  private final Iterable<byte[]> underlying;
+  private final long splitSize;
+
+  public SplittingIterable(Iterable<byte[]> underlying, long splitSize) {
+    this.underlying = underlying;
+    this.splitSize = splitSize;
+  }
+
+  @Override
+  public Iterator<ProtoRows> iterator() {
+    return new Iterator<ProtoRows>() {
+      final Iterator<byte[]> underlyingIterator = underlying.iterator();
+
+      @Override
+      public boolean hasNext() {
+        return underlyingIterator.hasNext();
+      }
+
+      @Override
+      public ProtoRows next() {
+        if (!hasNext()) {
+          throw new NoSuchElementException();
+        }
+
+        ProtoRows.Builder inserts = ProtoRows.newBuilder();
+        long bytesSize = 0;
+        while (underlyingIterator.hasNext()) {
+          ByteString byteString = ByteString.copyFrom(underlyingIterator.next());
+          inserts.addSerializedRows(byteString);
+          bytesSize += byteString.size();
+          if (bytesSize > splitSize) {
+            break;
+          }
+        }
+        return inserts.build();
+      }
+    };
+  }
+}
diff --git a/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/StorageApiLoads.java b/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/StorageApiLoads.java
index 3c27ddc8e42..2d079e3baa4 100644
--- a/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/StorageApiLoads.java
+++ b/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/StorageApiLoads.java
@@ -54,6 +54,7 @@ public class StorageApiLoads<DestinationT, ElementT>
   private final Duration triggeringFrequency;
   private final BigQueryServices bqServices;
   private final int numShards;
+  private final boolean allowInconsistentWrites;
 
   public StorageApiLoads(
       Coder<DestinationT> destinationCoder,
@@ -62,7 +63,8 @@ public class StorageApiLoads<DestinationT, ElementT>
       String kmsKey,
       Duration triggeringFrequency,
       BigQueryServices bqServices,
-      int numShards) {
+      int numShards,
+      boolean allowInconsistentWrites) {
     this.destinationCoder = destinationCoder;
     this.dynamicDestinations = dynamicDestinations;
     this.createDisposition = createDisposition;
@@ -70,11 +72,31 @@ public class StorageApiLoads<DestinationT, ElementT>
     this.triggeringFrequency = triggeringFrequency;
     this.bqServices = bqServices;
     this.numShards = numShards;
+    this.allowInconsistentWrites = allowInconsistentWrites;
   }
 
   @Override
   public WriteResult expand(PCollection<KV<DestinationT, ElementT>> input) {
-    return triggeringFrequency != null ? expandTriggered(input) : expandUntriggered(input);
+    if (allowInconsistentWrites) {
+      return expandInconsistent(input);
+    } else {
+      return triggeringFrequency != null ? expandTriggered(input) : expandUntriggered(input);
+    }
+  }
+
+  public WriteResult expandInconsistent(PCollection<KV<DestinationT, ElementT>> input) {
+    PCollection<KV<DestinationT, ElementT>> inputInGlobalWindow =
+        input.apply("rewindowIntoGlobal", Window.into(new GlobalWindows()));
+    PCollection<KV<DestinationT, byte[]>> convertedRecords =
+        inputInGlobalWindow
+            .apply("Convert", new StorageApiConvertMessages<>(dynamicDestinations))
+            .setCoder(KvCoder.of(destinationCoder, ByteArrayCoder.of()));
+    convertedRecords.apply(
+        "StorageApiWriteInconsistent",
+        new StorageApiWriteRecordsInconsistent<>(
+            dynamicDestinations, createDisposition, kmsKey, bqServices, destinationCoder));
+
+    return writeResult(input.getPipeline());
   }
 
   public WriteResult expandTriggered(PCollection<KV<DestinationT, ElementT>> input) {
@@ -130,7 +152,11 @@ public class StorageApiLoads<DestinationT, ElementT>
     PCollection<KV<DestinationT, ElementT>> inputInGlobalWindow =
         input.apply(
             "rewindowIntoGlobal", Window.<KV<DestinationT, ElementT>>into(new GlobalWindows()));
-    inputInGlobalWindow.apply(
+    PCollection<KV<DestinationT, byte[]>> convertedRecords =
+        inputInGlobalWindow
+            .apply("Convert", new StorageApiConvertMessages<>(dynamicDestinations))
+            .setCoder(KvCoder.of(destinationCoder, ByteArrayCoder.of()));
+    convertedRecords.apply(
         "StorageApiWriteUnsharded",
         new StorageApiWriteUnshardedRecords<>(
             dynamicDestinations, createDisposition, kmsKey, bqServices, destinationCoder));
diff --git a/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/StorageApiWriteRecordsInconsistent.java b/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/StorageApiWriteRecordsInconsistent.java
new file mode 100644
index 00000000000..5f65fd27360
--- /dev/null
+++ b/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/StorageApiWriteRecordsInconsistent.java
@@ -0,0 +1,75 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.beam.sdk.io.gcp.bigquery;
+
+import org.apache.beam.sdk.coders.Coder;
+import org.apache.beam.sdk.coders.VoidCoder;
+import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write.CreateDisposition;
+import org.apache.beam.sdk.transforms.Create;
+import org.apache.beam.sdk.transforms.PTransform;
+import org.apache.beam.sdk.transforms.ParDo;
+import org.apache.beam.sdk.values.KV;
+import org.apache.beam.sdk.values.PCollection;
+
+/**
+ * A transform to write sharded records to BigQuery using the Storage API. This transform uses the
+ * default stream to write the records. Records written will show up in BigQuery immediately,
+ * however exactly once is not guaranteed - duplicates may appear in the output. For exactly-once
+ * writes, use {@link StorageApiWritesShardedRecords} or {@link StorageApiWriteUnshardedRecords}.
+ */
+@SuppressWarnings("FutureReturnValueIgnored")
+public class StorageApiWriteRecordsInconsistent<DestinationT, ElementT>
+    extends PTransform<PCollection<KV<DestinationT, byte[]>>, PCollection<Void>> {
+  private final StorageApiDynamicDestinations<ElementT, DestinationT> dynamicDestinations;
+  private final CreateDisposition createDisposition;
+  private final String kmsKey;
+  private final BigQueryServices bqServices;
+  private final Coder<DestinationT> destinationCoder;
+
+  public StorageApiWriteRecordsInconsistent(
+      StorageApiDynamicDestinations<ElementT, DestinationT> dynamicDestinations,
+      CreateDisposition createDisposition,
+      String kmsKey,
+      BigQueryServices bqServices,
+      Coder<DestinationT> destinationCoder) {
+    this.dynamicDestinations = dynamicDestinations;
+    this.createDisposition = createDisposition;
+    this.kmsKey = kmsKey;
+    this.bqServices = bqServices;
+    this.destinationCoder = destinationCoder;
+  }
+
+  @Override
+  public PCollection<Void> expand(PCollection<KV<DestinationT, byte[]>> input) {
+    String operationName = input.getName() + "/" + getName();
+    // Append records to the Storage API streams.
+    input.apply(
+        "Write Records",
+        ParDo.of(
+                new StorageApiWriteUnshardedRecords.WriteRecordsDoFn<>(
+                    operationName,
+                    dynamicDestinations,
+                    bqServices,
+                    destinationCoder,
+                    createDisposition,
+                    kmsKey,
+                    true))
+            .withSideInputs(dynamicDestinations.getSideInputs()));
+    return input.getPipeline().apply("voids", Create.empty(VoidCoder.of()));
+  }
+}
diff --git a/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/StorageApiWriteUnshardedRecords.java b/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/StorageApiWriteUnshardedRecords.java
index 0b45b2010f5..b91720786c6 100644
--- a/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/StorageApiWriteUnshardedRecords.java
+++ b/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/StorageApiWriteUnshardedRecords.java
@@ -29,6 +29,7 @@ import java.util.List;
 import java.util.Map;
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.Executors;
+import java.util.concurrent.TimeUnit;
 import javax.annotation.Nullable;
 import org.apache.beam.sdk.coders.Coder;
 import org.apache.beam.sdk.coders.KvCoder;
@@ -39,6 +40,8 @@ import org.apache.beam.sdk.io.gcp.bigquery.BigQueryServices.StreamAppendClient;
 import org.apache.beam.sdk.io.gcp.bigquery.RetryManager.Operation.Context;
 import org.apache.beam.sdk.io.gcp.bigquery.RetryManager.RetryType;
 import org.apache.beam.sdk.io.gcp.bigquery.StorageApiDynamicDestinations.MessageConverter;
+import org.apache.beam.sdk.metrics.Counter;
+import org.apache.beam.sdk.metrics.Metrics;
 import org.apache.beam.sdk.options.PipelineOptions;
 import org.apache.beam.sdk.transforms.DoFn;
 import org.apache.beam.sdk.transforms.PTransform;
@@ -50,6 +53,9 @@ import org.apache.beam.sdk.values.KV;
 import org.apache.beam.sdk.values.PCollection;
 import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions;
 import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;
+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.Cache;
+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.CacheBuilder;
+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.RemovalNotification;
 import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Iterables;
 import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Lists;
 import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Maps;
@@ -57,7 +63,7 @@ import org.joda.time.Duration;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-@SuppressWarnings({"nullness"})
+@SuppressWarnings({"nullness", "FutureReturnValueIgnored"})
 /**
  * Write records to the Storage API using a standard batch approach. PENDING streams are used, which
  * do not become visible until they are finalized and committed. Each input bundle to the DoFn
@@ -65,7 +71,7 @@ import org.slf4j.LoggerFactory;
  * a finalize/commit operation at the end.
  */
 public class StorageApiWriteUnshardedRecords<DestinationT, ElementT>
-    extends PTransform<PCollection<KV<DestinationT, ElementT>>, PCollection<Void>> {
+    extends PTransform<PCollection<KV<DestinationT, byte[]>>, PCollection<Void>> {
   private static final Logger LOG = LoggerFactory.getLogger(StorageApiWriteUnshardedRecords.class);
 
   private final StorageApiDynamicDestinations<ElementT, DestinationT> dynamicDestinations;
@@ -73,6 +79,34 @@ public class StorageApiWriteUnshardedRecords<DestinationT, ElementT>
   private final String kmsKey;
   private final BigQueryServices bqServices;
   private final Coder<DestinationT> destinationCoder;
+  private static final ExecutorService closeWriterExecutor = Executors.newCachedThreadPool();
+
+  private static final Cache<String, StreamAppendClient> APPEND_CLIENTS =
+      CacheBuilder.newBuilder()
+          .expireAfterAccess(5, TimeUnit.MINUTES)
+          .removalListener(
+              (RemovalNotification<String, StreamAppendClient> removal) -> {
+                @Nullable final StreamAppendClient streamAppendClient = removal.getValue();
+                // Close the writer in a different thread so as not to block the main one.
+                runAsyncIgnoreFailure(closeWriterExecutor, streamAppendClient::close);
+              })
+          .build();
+
+  // Run a closure asynchronously, ignoring failures.
+  private interface ThrowingRunnable {
+    void run() throws Exception;
+  }
+
+  private static void runAsyncIgnoreFailure(ExecutorService executor, ThrowingRunnable task) {
+    executor.submit(
+        () -> {
+          try {
+            task.run();
+          } catch (Exception e) {
+            //
+          }
+        });
+  }
 
   public StorageApiWriteUnshardedRecords(
       StorageApiDynamicDestinations<ElementT, DestinationT> dynamicDestinations,
@@ -88,12 +122,20 @@ public class StorageApiWriteUnshardedRecords<DestinationT, ElementT>
   }
 
   @Override
-  public PCollection<Void> expand(PCollection<KV<DestinationT, ElementT>> input) {
+  public PCollection<Void> expand(PCollection<KV<DestinationT, byte[]>> input) {
     String operationName = input.getName() + "/" + getName();
     return input
         .apply(
             "Write Records",
-            ParDo.of(new WriteRecordsDoFn(operationName))
+            ParDo.of(
+                    new WriteRecordsDoFn<>(
+                        operationName,
+                        dynamicDestinations,
+                        bqServices,
+                        destinationCoder,
+                        createDisposition,
+                        kmsKey,
+                        false))
                 .withSideInputs(dynamicDestinations.getSideInputs()))
         .setCoder(KvCoder.of(StringUtf8Coder.of(), StringUtf8Coder.of()))
         // Calling Reshuffle makes the output stable - once this completes, the append operations
@@ -103,25 +145,8 @@ public class StorageApiWriteUnshardedRecords<DestinationT, ElementT>
         .apply("Finalize writes", ParDo.of(new StorageApiFinalizeWritesDoFn(bqServices)));
   }
 
-  private static final ExecutorService closeWriterExecutor = Executors.newCachedThreadPool();
-  // Run a closure asynchronously, ignoring failures.
-  private interface ThrowingRunnable {
-    void run() throws Exception;
-  }
-
-  @SuppressWarnings("FutureReturnValueIgnored")
-  private static void runAsyncIgnoreFailure(ExecutorService executor, ThrowingRunnable task) {
-    executor.submit(
-        () -> {
-          try {
-            task.run();
-          } catch (Exception e) {
-            //
-          }
-        });
-  }
-
-  class WriteRecordsDoFn extends DoFn<KV<DestinationT, ElementT>, KV<String, String>> {
+  static class WriteRecordsDoFn<DestinationT, ElementT>
+      extends DoFn<KV<DestinationT, byte[]>, KV<String, String>> {
     class DestinationState {
       private final String tableUrn;
       private final MessageConverter<ElementT> messageConverter;
@@ -129,16 +154,23 @@ public class StorageApiWriteUnshardedRecords<DestinationT, ElementT>
       private @Nullable StreamAppendClient streamAppendClient = null;
       private long currentOffset = 0;
       private List<ByteString> pendingMessages;
-      @Nullable private DatasetService datasetService;
+      private @Nullable DatasetService datasetService;
+      private final Counter recordsAppended =
+          Metrics.counter(WriteRecordsDoFn.class, "recordsAppended");
+      private final Counter appendFailures =
+          Metrics.counter(WriteRecordsDoFn.class, "appendFailures");
+      private final boolean useDefaultStream;
 
       public DestinationState(
           String tableUrn,
           MessageConverter<ElementT> messageConverter,
-          DatasetService datasetService) {
+          DatasetService datasetService,
+          boolean useDefaultStream) {
         this.tableUrn = tableUrn;
         this.messageConverter = messageConverter;
         this.pendingMessages = Lists.newArrayList();
         this.datasetService = datasetService;
+        this.useDefaultStream = useDefaultStream;
       }
 
       void close() {
@@ -152,16 +184,27 @@ public class StorageApiWriteUnshardedRecords<DestinationT, ElementT>
         }
       }
 
+      String getDefaultStreamName() {
+        return BigQueryHelpers.stripPartitionDecorator(tableUrn) + "/streams/_default";
+      }
+
       StreamAppendClient getWriteStream() {
         try {
           if (streamAppendClient == null) {
-            this.streamName =
-                Preconditions.checkNotNull(datasetService)
-                    .createWriteStream(tableUrn, Type.PENDING)
-                    .getName();
+            if (!useDefaultStream) {
+              this.streamName =
+                  Preconditions.checkNotNull(datasetService)
+                      .createWriteStream(tableUrn, Type.PENDING)
+                      .getName();
+            } else {
+              this.streamName = getDefaultStreamName();
+            }
             this.streamAppendClient =
-                Preconditions.checkNotNull(datasetService)
-                    .getStreamAppendClient(streamName, messageConverter.getSchemaDescriptor());
+                APPEND_CLIENTS.get(
+                    streamName,
+                    () ->
+                        datasetService.getStreamAppendClient(
+                            streamName, messageConverter.getSchemaDescriptor()));
             this.currentOffset = 0;
           }
           return streamAppendClient;
@@ -179,21 +222,13 @@ public class StorageApiWriteUnshardedRecords<DestinationT, ElementT>
         }
       }
 
-      void addMessage(ElementT element) throws Exception {
-        ByteString message = messageConverter.toMessage(element).toByteString();
-        pendingMessages.add(message);
-        if (shouldFlush()) {
-          flush();
-        }
-      }
-
-      boolean shouldFlush() {
-        // TODO: look at byte size too?
-        return pendingMessages.size() > 100;
+      void addMessage(byte[] message) throws Exception {
+        pendingMessages.add(ByteString.copyFrom(message));
       }
 
       @SuppressWarnings({"nullness"})
-      void flush() throws Exception {
+      void flush(RetryManager<AppendRowsResponse, Context<AppendRowsResponse>> retryManager)
+          throws Exception {
         if (pendingMessages.isEmpty()) {
           return;
         }
@@ -205,14 +240,16 @@ public class StorageApiWriteUnshardedRecords<DestinationT, ElementT>
         ProtoRows protoRows = inserts.build();
         pendingMessages.clear();
 
-        RetryManager<AppendRowsResponse, Context<AppendRowsResponse>> retryManager =
-            new RetryManager<>(Duration.standardSeconds(1), Duration.standardMinutes(1), 5);
         retryManager.addOperation(
             c -> {
               try {
-                long offset = currentOffset;
-                currentOffset += inserts.getSerializedRowsCount();
-                return getWriteStream().appendRows(offset, protoRows);
+                StreamAppendClient writeStream = getWriteStream();
+                long offset = -1;
+                if (!this.useDefaultStream) {
+                  offset = this.currentOffset;
+                  this.currentOffset += inserts.getSerializedRowsCount();
+                }
+                return writeStream.appendRows(offset, protoRows);
               } catch (Exception e) {
                 throw new RuntimeException(e);
               }
@@ -224,23 +261,72 @@ public class StorageApiWriteUnshardedRecords<DestinationT, ElementT>
                       + " failed with error "
                       + Iterables.getFirst(contexts, null).getError());
               invalidateWriteStream();
+              appendFailures.inc();
               return RetryType.RETRY_ALL_OPERATIONS;
             },
             response -> {
               LOG.info("Append to stream {} succeeded.", streamName);
+              recordsAppended.inc(protoRows.getSerializedRowsCount());
             },
             new Context<>());
-        // TODO: Do we have to wait on every append?
-        retryManager.run(true);
       }
     }
 
     private Map<DestinationT, DestinationState> destinations = Maps.newHashMap();
     private final TwoLevelMessageConverterCache<DestinationT, ElementT> messageConverters;
     private @Nullable DatasetService datasetService;
+    private int numPendingRecords = 0;
+    private int numPendingRecordBytes = 0;
+    private static final int FLUSH_THRESHOLD_RECORDS = 100;
+    private static final int FLUSH_THRESHOLD_RECORD_BYTES = 2 * 1024 * 1024;
+    private final StorageApiDynamicDestinations<ElementT, DestinationT> dynamicDestinations;
+    private final BigQueryServices bqServices;
+    private final Coder<DestinationT> destinationCoder;
+    private final CreateDisposition createDisposition;
+    private final String kmsKey;
+    private final boolean useDefaultStream;
 
-    WriteRecordsDoFn(String operationName) {
+    WriteRecordsDoFn(
+        String operationName,
+        StorageApiDynamicDestinations<ElementT, DestinationT> dynamicDestinations,
+        BigQueryServices bqServices,
+        Coder<DestinationT> destinationCoder,
+        CreateDisposition createDisposition,
+        String kmsKey,
+        boolean useDefaultStream) {
       this.messageConverters = new TwoLevelMessageConverterCache<>(operationName);
+      this.dynamicDestinations = dynamicDestinations;
+      this.bqServices = bqServices;
+      this.destinationCoder = destinationCoder;
+      this.createDisposition = createDisposition;
+      this.kmsKey = kmsKey;
+      this.useDefaultStream = useDefaultStream;
+    }
+
+    boolean shouldFlush() {
+      return numPendingRecords > FLUSH_THRESHOLD_RECORDS
+          || numPendingRecordBytes > FLUSH_THRESHOLD_RECORD_BYTES;
+    }
+
+    void flushIfNecessary() throws Exception {
+      if (shouldFlush()) {
+        // Too much memory being used. Flush the state and wait for it to drain out.
+        // TODO(reuvenlax): Consider waiting for memory usage to drop instead of waiting for all the
+        // appends to finish.
+        flushAll();
+      }
+    }
+
+    void flushAll() throws Exception {
+      RetryManager<AppendRowsResponse, RetryManager.Operation.Context<AppendRowsResponse>>
+          retryManager =
+              new RetryManager<>(Duration.standardSeconds(1), Duration.standardSeconds(10), 1000);
+      for (DestinationState destinationState : destinations.values()) {
+        destinationState.flush(retryManager);
+      }
+      retryManager.run(true);
+      numPendingRecords = 0;
+      numPendingRecordBytes = 0;
     }
 
     private void initializeDatasetService(PipelineOptions pipelineOptions) {
@@ -252,6 +338,8 @@ public class StorageApiWriteUnshardedRecords<DestinationT, ElementT>
     @StartBundle
     public void startBundle() throws IOException {
       destinations = Maps.newHashMap();
+      numPendingRecords = 0;
+      numPendingRecordBytes = 0;
     }
 
     DestinationState createDestinationState(ProcessContext c, DestinationT destination) {
@@ -279,37 +367,36 @@ public class StorageApiWriteUnshardedRecords<DestinationT, ElementT>
       } catch (Exception e) {
         throw new RuntimeException(e);
       }
-      return new DestinationState(createdTable.getTableUrn(), messageConverter, datasetService);
+      return new DestinationState(
+          createdTable.getTableUrn(), messageConverter, datasetService, useDefaultStream);
     }
 
     @ProcessElement
     public void process(
         ProcessContext c,
         PipelineOptions pipelineOptions,
-        @Element KV<DestinationT, ElementT> element)
+        @Element KV<DestinationT, byte[]> element)
         throws Exception {
       initializeDatasetService(pipelineOptions);
       dynamicDestinations.setSideInputAccessorFromProcessContext(c);
       DestinationState state =
           destinations.computeIfAbsent(element.getKey(), k -> createDestinationState(c, k));
-
-      if (state.shouldFlush()) {
-        // Too much memory being used. Flush the state and wait for it to drain out.
-        // TODO(reuvenlax): Consider waiting for memory usage to drop instead of waiting for all the
-        // appends to finish.
-        state.flush();
-      }
+      flushIfNecessary();
       state.addMessage(element.getValue());
+      ++numPendingRecords;
+      numPendingRecordBytes += element.getValue().length;
     }
 
     @FinishBundle
     public void finishBundle(FinishBundleContext context) throws Exception {
-      for (DestinationState state : destinations.values()) {
-        state.flush();
-        context.output(
-            KV.of(state.tableUrn, state.streamName),
-            BoundedWindow.TIMESTAMP_MAX_VALUE.minus(Duration.millis(1)),
-            GlobalWindow.INSTANCE);
+      flushAll();
+      if (!useDefaultStream) {
+        for (DestinationState state : destinations.values()) {
+          context.output(
+              KV.of(state.tableUrn, state.streamName),
+              BoundedWindow.TIMESTAMP_MAX_VALUE.minus(Duration.millis(1)),
+              GlobalWindow.INSTANCE);
+        }
       }
     }
 
@@ -318,6 +405,7 @@ public class StorageApiWriteUnshardedRecords<DestinationT, ElementT>
       for (DestinationState state : destinations.values()) {
         state.close();
       }
+      destinations.clear();
       try {
         if (datasetService != null) {
           datasetService.close();
diff --git a/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/StorageApiWritesShardedRecords.java b/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/StorageApiWritesShardedRecords.java
index 52871845ad6..c0009df792f 100644
--- a/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/StorageApiWritesShardedRecords.java
+++ b/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/StorageApiWritesShardedRecords.java
@@ -24,16 +24,13 @@ import com.google.api.services.bigquery.model.TableSchema;
 import com.google.cloud.bigquery.storage.v1beta2.AppendRowsResponse;
 import com.google.cloud.bigquery.storage.v1beta2.ProtoRows;
 import com.google.cloud.bigquery.storage.v1beta2.WriteStream.Type;
-import com.google.protobuf.ByteString;
 import com.google.protobuf.Descriptors.Descriptor;
 import io.grpc.Status;
 import io.grpc.Status.Code;
 import java.io.IOException;
 import java.time.Instant;
-import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
-import java.util.NoSuchElementException;
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.Executors;
 import java.util.concurrent.TimeUnit;
@@ -179,52 +176,6 @@ public class StorageApiWritesShardedRecords<DestinationT, ElementT>
             "Flush and finalize writes", ParDo.of(new StorageApiFlushAndFinalizeDoFn(bqServices)));
   }
 
-  /**
-   * Takes in an iterable and batches the results into multiple ProtoRows objects. The splitSize
-   * parameter controls how many rows are batched into a single ProtoRows object before we move on
-   * to the next one.
-   */
-  static class SplittingIterable implements Iterable<ProtoRows> {
-    private final Iterable<byte[]> underlying;
-    private final long splitSize;
-
-    public SplittingIterable(Iterable<byte[]> underlying, long splitSize) {
-      this.underlying = underlying;
-      this.splitSize = splitSize;
-    }
-
-    @Override
-    public Iterator<ProtoRows> iterator() {
-      return new Iterator<ProtoRows>() {
-        final Iterator<byte[]> underlyingIterator = underlying.iterator();
-
-        @Override
-        public boolean hasNext() {
-          return underlyingIterator.hasNext();
-        }
-
-        @Override
-        public ProtoRows next() {
-          if (!hasNext()) {
-            throw new NoSuchElementException();
-          }
-
-          ProtoRows.Builder inserts = ProtoRows.newBuilder();
-          long bytesSize = 0;
-          while (underlyingIterator.hasNext()) {
-            ByteString byteString = ByteString.copyFrom(underlyingIterator.next());
-            inserts.addSerializedRows(byteString);
-            bytesSize += byteString.size();
-            if (bytesSize > splitSize) {
-              break;
-            }
-          }
-          return inserts.build();
-        }
-      };
-    }
-  }
-
   class WriteRecordsDoFn
       extends DoFn<KV<ShardedKey<DestinationT>, Iterable<byte[]>>, KV<String, Operation>> {
     private final Counter recordsAppended =
diff --git a/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/testing/FakeDatasetService.java b/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/testing/FakeDatasetService.java
index eda8059ac2f..bb3ef9f0a63 100644
--- a/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/testing/FakeDatasetService.java
+++ b/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/testing/FakeDatasetService.java
@@ -86,13 +86,15 @@ public class FakeDatasetService implements DatasetService, Serializable {
   public void close() throws Exception {}
 
   static class Stream {
+    final String streamName;
     final List<TableRow> stream;
     final TableContainer tableContainer;
     final Type type;
     long nextFlushPosition;
     boolean finalized;
 
-    Stream(TableContainer tableContainer, Type type) {
+    Stream(String streamName, TableContainer tableContainer, Type type) {
+      this.streamName = streamName;
       this.stream = Lists.newArrayList();
       this.tableContainer = tableContainer;
       this.type = type;
@@ -110,9 +112,20 @@ public class FakeDatasetService implements DatasetService, Serializable {
         throw new RuntimeException("Stream already finalized.");
       }
       if (position != -1 && position != stream.size()) {
-        throw new RuntimeException("Bad append: " + position);
+        throw new RuntimeException(
+            "Bad append: "
+                + position
+                + " + for stream "
+                + streamName
+                + " expected "
+                + stream.size());
       }
       stream.addAll(rowsToAppend);
+      if (type == Type.COMMITTED) {
+        for (TableRow row : rowsToAppend) {
+          tableContainer.addRow(row, "");
+        }
+      }
     }
 
     void flush(long position) {
@@ -249,7 +262,21 @@ public class FakeDatasetService implements DatasetService, Serializable {
             "Tried to get a dataset %s:%s, but no such table was set",
             tableReference.getProjectId(), tableReference.getDatasetId());
       }
-      dataset.computeIfAbsent(tableReference.getTableId(), k -> new TableContainer(table));
+      dataset.computeIfAbsent(
+          tableReference.getTableId(),
+          k -> {
+            TableContainer tableContainer = new TableContainer(table);
+            // Create the default stream.
+            String streamName =
+                String.format(
+                    "projects/%s/datasets/%s/tables/%s/streams/_default",
+                    tableReference.getProjectId(),
+                    tableReference.getDatasetId(),
+                    BigQueryHelpers.stripPartitionDecorator(tableReference.getTableId()));
+            writeStreams.put(streamName, new Stream(streamName, tableContainer, Type.COMMITTED));
+
+            return tableContainer;
+          });
     }
   }
 
@@ -412,9 +439,6 @@ public class FakeDatasetService implements DatasetService, Serializable {
   @Override
   public WriteStream createWriteStream(String tableUrn, Type type)
       throws IOException, InterruptedException {
-    if (type != Type.PENDING && type != Type.BUFFERED) {
-      throw new RuntimeException("We only support PENDING or BUFFERED streams.");
-    }
     TableReference tableReference =
         BigQueryHelpers.parseTableUrn(BigQueryHelpers.stripPartitionDecorator(tableUrn));
     synchronized (tables) {
@@ -424,7 +448,7 @@ public class FakeDatasetService implements DatasetService, Serializable {
               tableReference.getDatasetId(),
               tableReference.getTableId());
       String streamName = UUID.randomUUID().toString();
-      writeStreams.put(streamName, new Stream(tableContainer, type));
+      writeStreams.put(streamName, new Stream(streamName, tableContainer, type));
       return WriteStream.newBuilder().setName(streamName).build();
     }
   }
diff --git a/sdks/java/io/google-cloud-platform/src/test/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIOWriteTest.java b/sdks/java/io/google-cloud-platform/src/test/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIOWriteTest.java
index 9d9de39243c..088a7c42d9b 100644
--- a/sdks/java/io/google-cloud-platform/src/test/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIOWriteTest.java
+++ b/sdks/java/io/google-cloud-platform/src/test/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIOWriteTest.java
@@ -159,16 +159,20 @@ public class BigQueryIOWriteTest implements Serializable {
   @Parameters
   public static Iterable<Object[]> data() {
     return ImmutableList.of(
-        new Object[] {false, false},
-        new Object[] {false, true},
-        new Object[] {true, false},
-        new Object[] {true, true});
+        new Object[] {false, false, false},
+        new Object[] {false, false, true},
+        new Object[] {true, false, false},
+        new Object[] {true, false, true},
+        new Object[] {true, true, true});
   }
 
   @Parameter(0)
   public boolean useStorageApi;
 
   @Parameter(1)
+  public boolean useStorageApiApproximate;
+
+  @Parameter(2)
   public boolean useStreaming;
 
   @Rule
@@ -194,6 +198,9 @@ public class BigQueryIOWriteTest implements Serializable {
                   bqOptions.setTempLocation(testFolder.getRoot().getAbsolutePath());
                   if (useStorageApi) {
                     bqOptions.setUseStorageWriteApi(true);
+                    if (useStorageApiApproximate) {
+                      bqOptions.setUseStorageWriteApiAtLeastOnce(true);
+                    }
                     if (useStreaming) {
                       bqOptions.setNumStorageWriteApiStreams(2);
                       bqOptions.setStorageWriteApiTriggeringFrequencySec(1);
@@ -477,7 +484,8 @@ public class BigQueryIOWriteTest implements Serializable {
   public void testTimePartitioning() throws Exception {
     BigQueryIO.Write.Method method;
     if (useStorageApi) {
-      method = Method.STORAGE_WRITE_API;
+      method =
+          useStorageApiApproximate ? Method.STORAGE_API_AT_LEAST_ONCE : Method.STORAGE_WRITE_API;
     } else if (useStreaming) {
       method = Method.STREAMING_INSERTS;
     } else {
@@ -497,7 +505,8 @@ public class BigQueryIOWriteTest implements Serializable {
   @Test
   public void testClusteringStorageApi() throws Exception {
     if (useStorageApi) {
-      testClustering(Method.STORAGE_WRITE_API);
+      testClustering(
+          useStorageApiApproximate ? Method.STORAGE_API_AT_LEAST_ONCE : Method.STORAGE_WRITE_API);
     }
   }
 
@@ -526,7 +535,12 @@ public class BigQueryIOWriteTest implements Serializable {
 
     // withMethod overrides the pipeline option, so we need to explicitly request
     // STORAGE_API_WRITES.
-    BigQueryIO.Write.Method method = useStorageApi ? Method.STORAGE_WRITE_API : Method.FILE_LOADS;
+    BigQueryIO.Write.Method method =
+        useStorageApi
+            ? (useStorageApiApproximate
+                ? Method.STORAGE_API_AT_LEAST_ONCE
+                : Method.STORAGE_WRITE_API)
+            : Method.FILE_LOADS;
     p.apply(Create.of(row1, row2))
         .apply(
             BigQueryIO.writeTableRows()
@@ -1085,7 +1099,12 @@ public class BigQueryIOWriteTest implements Serializable {
   public void testSchemaWriteLoads() throws Exception {
     // withMethod overrides the pipeline option, so we need to explicitly request
     // STORAGE_API_WRITES.
-    BigQueryIO.Write.Method method = useStorageApi ? Method.STORAGE_WRITE_API : Method.FILE_LOADS;
+    BigQueryIO.Write.Method method =
+        useStorageApi
+            ? (useStorageApiApproximate
+                ? Method.STORAGE_API_AT_LEAST_ONCE
+                : Method.STORAGE_WRITE_API)
+            : Method.FILE_LOADS;
     p.apply(
             Create.of(
                 new SchemaPojo("a", 1),
@@ -2097,7 +2116,11 @@ public class BigQueryIOWriteTest implements Serializable {
     // withMethod overrides the pipeline option, so we need to explicitly requiest
     // STORAGE_API_WRITES.
     BigQueryIO.Write.Method method =
-        useStorageApi ? Method.STORAGE_WRITE_API : Method.STREAMING_INSERTS;
+        useStorageApi
+            ? (useStorageApiApproximate
+                ? Method.STORAGE_API_AT_LEAST_ONCE
+                : Method.STORAGE_WRITE_API)
+            : Method.STREAMING_INSERTS;
     TableSchema schema =
         new TableSchema()
             .setFields(
