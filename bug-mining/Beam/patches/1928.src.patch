diff --git a/sdks/python/apache_beam/runners/portability/local_job_service.py b/sdks/python/apache_beam/runners/portability/local_job_service.py
index 4a54ca4120a..1bd5277659b 100644
--- a/sdks/python/apache_beam/runners/portability/local_job_service.py
+++ b/sdks/python/apache_beam/runners/portability/local_job_service.py
@@ -19,6 +19,7 @@
 from __future__ import absolute_import
 
 import concurrent.futures
+import itertools
 import logging
 import os
 import queue
@@ -150,8 +151,9 @@ class LocalJobServicer(abstract_job_service.AbstractJobServiceServicer):
 
     result = self._jobs[request.job_id].result
     monitoring_info_list = []
-    for mi in result._monitoring_infos_by_stage.values():
-      monitoring_info_list.extend(mi)
+    if result is not None:
+      for mi in result._monitoring_infos_by_stage.values():
+        monitoring_info_list.extend(mi)
 
     # Filter out system metrics
     user_monitoring_info_list = [
@@ -231,7 +233,7 @@ class BeamJob(abstract_job_service.AbstractBeamJob):
     self._artifact_staging_endpoint = artifact_staging_endpoint
     self._artifact_service = artifact_service
     self._state_queues = []  # type: List[queue.Queue]
-    self._log_queues = []  # type: List[queue.Queue]
+    self._log_queues = JobLogQueues()
     self.daemon = True
     self.result = None
 
@@ -256,7 +258,7 @@ class BeamJob(abstract_job_service.AbstractBeamJob):
 
   def _run_job(self):
     self.set_state(beam_job_api_pb2.JobState.RUNNING)
-    with JobLogHandler(self._log_queues):
+    with JobLogHandler(self._log_queues) as log_handler:
       self._update_dependencies()
       try:
         start = time.time()
@@ -268,8 +270,13 @@ class BeamJob(abstract_job_service.AbstractBeamJob):
         self.set_state(beam_job_api_pb2.JobState.DONE)
         self.result = result
       except:  # pylint: disable=bare-except
+        self._log_queues.put(
+            beam_job_api_pb2.JobMessage(
+                message_id=log_handler._next_id(),
+                time=time.strftime('%Y-%m-%d %H:%M:%S.'),
+                importance=beam_job_api_pb2.JobMessage.JOB_MESSAGE_ERROR,
+                message_text=traceback.format_exc()))
         _LOGGER.exception('Error running pipeline.')
-        _LOGGER.exception(traceback)
         self.set_state(beam_job_api_pb2.JobState.FAILED)
         raise
 
@@ -307,7 +314,8 @@ class BeamJob(abstract_job_service.AbstractBeamJob):
     self._log_queues.append(log_queue)
     self._state_queues.append(log_queue)
 
-    for msg in self.with_state_history(_iter_queue(log_queue)):
+    for msg in itertools.chain(self._log_queues.cache(),
+                               self.with_state_history(_iter_queue(log_queue))):
       if isinstance(msg, tuple):
         assert len(msg) == 2 and isinstance(msg[0], int)
         current_state = msg[0]
@@ -326,6 +334,38 @@ class BeamFnLoggingServicer(beam_fn_api_pb2_grpc.BeamFnLoggingServicer):
     return iter([])
 
 
+class JobLogQueues(object):
+  def __init__(self):
+    self._queues = []  # type: List[queue.Queue]
+    self._cache = []
+    self._cache_size = 10
+    self._lock = threading.Lock()
+
+  def cache(self):
+    with self._lock:
+      return list(self._cache)
+
+  def append(self, queue):
+    with self._lock:
+      self._queues.append(queue)
+
+  def put(self, msg):
+    with self._lock:
+      if len(self._cache) < self._cache_size:
+        self._cache.append(msg)
+      else:
+        min_level = min(m.importance for m in self._cache)
+        if msg.importance >= min_level:
+          self._cache.append(msg)
+          for ix, m in enumerate(self._cache):
+            if m.importance == min_level:
+              del self._cache[ix]
+              break
+
+      for queue in self._queues:
+        queue.put(msg)
+
+
 class JobLogHandler(logging.Handler):
   """Captures logs to be returned via the Beam Job API.
 
@@ -352,6 +392,7 @@ class JobLogHandler(logging.Handler):
     # running pipelines (as Python log handlers are global).
     self._logged_thread = threading.current_thread()
     logging.getLogger().addHandler(self)
+    return self
 
   def __exit__(self, *args):
     self._logged_thread = None
@@ -371,5 +412,4 @@ class JobLogHandler(logging.Handler):
           message_text=self.format(record))
 
       # Inform all message consumers.
-      for queue in self._log_queues:
-        queue.put(msg)
+      self._log_queues.put(msg)
diff --git a/sdks/python/apache_beam/runners/portability/local_job_service_test.py b/sdks/python/apache_beam/runners/portability/local_job_service_test.py
index ec164e0e8ff..5ba54271c4f 100644
--- a/sdks/python/apache_beam/runners/portability/local_job_service_test.py
+++ b/sdks/python/apache_beam/runners/portability/local_job_service_test.py
@@ -63,6 +63,42 @@ class LocalJobServerTest(unittest.TestCase):
     self.assertEqual([s.state_response.state for s in message_results],
                      expected_states)
 
+  def test_error_messages_after_pipeline_failure(self):
+    job_service = local_job_service.LocalJobServicer()
+    job_service.start_grpc_server()
+
+    plan = TestJobServicePlan(job_service)
+
+    job_id, message_stream, state_stream = plan.submit(
+        beam_runner_api_pb2.Pipeline(requirements=['unsupported_requirement']))
+
+    message_results = list(message_stream)
+    state_results = list(state_stream)
+
+    expected_states = [
+        beam_job_api_pb2.JobState.STOPPED,
+        beam_job_api_pb2.JobState.STARTING,
+        beam_job_api_pb2.JobState.RUNNING,
+        beam_job_api_pb2.JobState.FAILED,
+    ]
+    self.assertEqual([s.state for s in state_results], expected_states)
+    self.assertTrue(
+        any(
+            'unsupported_requirement' in m.message_response.message_text
+            for m in message_results),
+        message_results)
+
+    # Assert we still see the error message if we fetch error messages after
+    # the job has completed.
+    messages_again = list(
+        plan.job_service.GetMessageStream(
+            beam_job_api_pb2.JobMessagesRequest(job_id=job_id)))
+    self.assertTrue(
+        any(
+            'unsupported_requirement' in m.message_response.message_text
+            for m in message_results),
+        messages_again)
+
 
 if __name__ == '__main__':
   logging.getLogger().setLevel(logging.INFO)
