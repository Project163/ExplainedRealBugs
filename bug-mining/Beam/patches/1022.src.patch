diff --git a/runners/spark/src/main/java/org/apache/beam/runners/spark/coders/BeamSparkRunnerRegistrator.java b/runners/spark/src/main/java/org/apache/beam/runners/spark/coders/BeamSparkRunnerRegistrator.java
index ebd395a85b1..d1cb2a23cd8 100644
--- a/runners/spark/src/main/java/org/apache/beam/runners/spark/coders/BeamSparkRunnerRegistrator.java
+++ b/runners/spark/src/main/java/org/apache/beam/runners/spark/coders/BeamSparkRunnerRegistrator.java
@@ -23,6 +23,7 @@ import java.util.LinkedHashMap;
 import org.apache.beam.runners.spark.io.MicrobatchSource;
 import org.apache.beam.runners.spark.stateful.SparkGroupAlsoByWindowViaWindowSet.StateAndTimers;
 import org.apache.beam.runners.spark.translation.GroupCombineFunctions;
+import org.apache.beam.runners.spark.translation.GroupNonMergingWindowsFunctions.WindowedKey;
 import org.apache.beam.runners.spark.util.ByteArray;
 import org.apache.beam.sdk.transforms.windowing.PaneInfo;
 import org.apache.beam.sdk.values.KV;
@@ -61,6 +62,7 @@ public class BeamSparkRunnerRegistrator implements KryoRegistrator {
     kryo.register(HashBasedTable.class);
     kryo.register(KV.class);
     kryo.register(PaneInfo.class);
+    kryo.register(WindowedKey.class);
 
     try {
       kryo.register(
diff --git a/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/EvaluationContext.java b/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/EvaluationContext.java
index 88f706f256c..1c8a2eb7ab8 100644
--- a/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/EvaluationContext.java
+++ b/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/EvaluationContext.java
@@ -25,6 +25,7 @@ import java.util.LinkedHashSet;
 import java.util.Map;
 import java.util.Set;
 import java.util.stream.Collectors;
+import javax.annotation.Nullable;
 import org.apache.beam.runners.core.construction.SerializablePipelineOptions;
 import org.apache.beam.runners.core.construction.TransformInputs;
 import org.apache.beam.runners.spark.SparkPipelineOptions;
@@ -32,6 +33,7 @@ import org.apache.beam.sdk.Pipeline;
 import org.apache.beam.sdk.coders.Coder;
 import org.apache.beam.sdk.options.PipelineOptions;
 import org.apache.beam.sdk.runners.AppliedPTransform;
+import org.apache.beam.sdk.transforms.GroupByKey;
 import org.apache.beam.sdk.transforms.PTransform;
 import org.apache.beam.sdk.transforms.windowing.BoundedWindow;
 import org.apache.beam.sdk.util.WindowedValue;
@@ -137,30 +139,39 @@ public class EvaluationContext {
   }
 
   /**
-   * Cache PCollection if {@link #isCacheDisabled()} flag is false and PCollection is used more then
-   * once in Pipeline.
+   * Cache PCollection if {@link #isCacheDisabled()} flag is false or transform isn't GroupByKey
+   * transformation and PCollection is used more then once in Pipeline.
    *
-   * @param pvalue
+   * <p>PCollection is not cached in GroupByKey transformation, because Spark automatically persists
+   * some intermediate data in shuffle operations, even without users calling persist.
+   *
+   * @param pvalue output of transform
+   * @param transform
    * @return if PCollection will be cached
    */
-  public boolean shouldCache(PValue pvalue) {
-    if (isCacheDisabled()) {
+  public boolean shouldCache(PValue pvalue, PTransform<?, ? extends PValue> transform) {
+    if (isCacheDisabled() || transform instanceof GroupByKey) {
       return false;
     }
     return pvalue instanceof PCollection && cacheCandidates.getOrDefault(pvalue, 0L) > 1;
   }
 
   public void putDataset(PTransform<?, ? extends PValue> transform, Dataset dataset) {
-    putDataset(getOutput(transform), dataset);
+    putDataset(transform, getOutput(transform), dataset);
   }
 
   public void putDataset(PValue pvalue, Dataset dataset) {
+    putDataset(null, pvalue, dataset);
+  }
+
+  private void putDataset(
+      @Nullable PTransform<?, ? extends PValue> transform, PValue pvalue, Dataset dataset) {
     try {
       dataset.setName(pvalue.getName());
     } catch (IllegalStateException e) {
       // name not set, ignore
     }
-    if (shouldCache(pvalue)) {
+    if (shouldCache(pvalue, transform)) {
       // we cache only PCollection
       Coder<?> coder = ((PCollection<?>) pvalue).getCoder();
       Coder<? extends BoundedWindow> wCoder =
diff --git a/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/GroupNonMergingWindowsFunctions.java b/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/GroupNonMergingWindowsFunctions.java
new file mode 100644
index 00000000000..6ad50e0d912
--- /dev/null
+++ b/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/GroupNonMergingWindowsFunctions.java
@@ -0,0 +1,242 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.beam.runners.spark.translation;
+
+import java.io.Serializable;
+import java.util.Arrays;
+import java.util.Iterator;
+import java.util.Objects;
+import org.apache.beam.runners.spark.coders.CoderHelpers;
+import org.apache.beam.sdk.coders.ByteArrayCoder;
+import org.apache.beam.sdk.coders.Coder;
+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;
+import org.apache.beam.sdk.util.WindowedValue;
+import org.apache.beam.sdk.util.WindowedValue.FullWindowedValueCoder;
+import org.apache.beam.sdk.values.KV;
+import org.apache.beam.sdk.values.WindowingStrategy;
+import org.apache.beam.vendor.guava.v20_0.com.google.common.collect.AbstractIterator;
+import org.apache.beam.vendor.guava.v20_0.com.google.common.collect.Iterables;
+import org.apache.beam.vendor.guava.v20_0.com.google.common.collect.Iterators;
+import org.apache.beam.vendor.guava.v20_0.com.google.common.collect.PeekingIterator;
+import org.apache.beam.vendor.guava.v20_0.com.google.common.primitives.UnsignedBytes;
+import org.apache.spark.HashPartitioner;
+import org.apache.spark.api.java.JavaRDD;
+import org.joda.time.Instant;
+import scala.Tuple2;
+
+/** Functions for GroupByKey with Non-Merging windows translations to Spark. */
+public class GroupNonMergingWindowsFunctions {
+
+  static <K, V, W extends BoundedWindow>
+      JavaRDD<WindowedValue<KV<K, Iterable<V>>>> groupByKeyAndWindow(
+          JavaRDD<WindowedValue<KV<K, V>>> rdd,
+          Coder<K> keyCoder,
+          Coder<V> valueCoder,
+          WindowingStrategy<?, W> windowingStrategy) {
+    final Coder<W> windowCoder = windowingStrategy.getWindowFn().windowCoder();
+    final WindowedValue.FullWindowedValueCoder<byte[]> windowedValueCoder =
+        WindowedValue.getFullCoder(ByteArrayCoder.of(), windowCoder);
+    return rdd.flatMapToPair(
+            (WindowedValue<KV<K, V>> windowedValue) -> {
+              final byte[] keyBytes =
+                  CoderHelpers.toByteArray(windowedValue.getValue().getKey(), keyCoder);
+              final byte[] valueBytes =
+                  CoderHelpers.toByteArray(windowedValue.getValue().getValue(), valueCoder);
+              return Iterators.transform(
+                  windowedValue.explodeWindows().iterator(),
+                  item -> {
+                    Objects.requireNonNull(item, "Exploded window can not be null.");
+                    @SuppressWarnings("unchecked")
+                    final W window = (W) Iterables.getOnlyElement(item.getWindows());
+                    final byte[] windowBytes = CoderHelpers.toByteArray(window, windowCoder);
+                    final byte[] windowValueBytes =
+                        CoderHelpers.toByteArray(
+                            WindowedValue.of(
+                                valueBytes, item.getTimestamp(), window, item.getPane()),
+                            windowedValueCoder);
+                    final WindowedKey windowedKey = new WindowedKey(keyBytes, windowBytes);
+                    return new Tuple2<>(windowedKey, windowValueBytes);
+                  });
+            })
+        .repartitionAndSortWithinPartitions(new HashPartitioner(rdd.getNumPartitions()))
+        .mapPartitions(
+            it ->
+                new GroupByKeyIterator<>(
+                    it, keyCoder, valueCoder, windowingStrategy, windowedValueCoder))
+        .filter(Objects::nonNull); // filter last null element from GroupByKeyIterator
+  }
+
+  /**
+   * Transform stream of sorted key values into stream of value iterators for each key. This
+   * iterator can be iterated only once!
+   *
+   * @param <K> type of key iterator emits
+   * @param <V> type of value iterator emits
+   */
+  static class GroupByKeyIterator<K, V, W extends BoundedWindow>
+      implements Iterator<WindowedValue<KV<K, Iterable<V>>>> {
+
+    private final PeekingIterator<Tuple2<WindowedKey, byte[]>> inner;
+    private final Coder<K> keyCoder;
+    private final Coder<V> valueCoder;
+    private final WindowingStrategy<?, W> windowingStrategy;
+    private final FullWindowedValueCoder<byte[]> windowedValueCoder;
+    private boolean hasNext = true;
+
+    private WindowedKey currentKey = null;
+
+    GroupByKeyIterator(
+        Iterator<Tuple2<WindowedKey, byte[]>> inner,
+        Coder<K> keyCoder,
+        Coder<V> valueCoder,
+        WindowingStrategy<?, W> windowingStrategy,
+        WindowedValue.FullWindowedValueCoder<byte[]> windowedValueCoder) {
+      this.inner = Iterators.peekingIterator(inner);
+      this.keyCoder = keyCoder;
+      this.valueCoder = valueCoder;
+      this.windowingStrategy = windowingStrategy;
+      this.windowedValueCoder = windowedValueCoder;
+    }
+
+    @Override
+    public boolean hasNext() {
+      return hasNext;
+    }
+
+    @Override
+    public WindowedValue<KV<K, Iterable<V>>> next() {
+      while (inner.hasNext()) {
+        final WindowedKey nextKey = inner.peek()._1;
+        if (nextKey.equals(currentKey)) {
+          // we still did not see all values for a given key
+          inner.next();
+          continue;
+        }
+        currentKey = nextKey;
+        final WindowedValue<KV<K, V>> decodedItem = decodeItem(inner.peek());
+        return decodedItem.withValue(
+            KV.of(decodedItem.getValue().getKey(), new ValueIterator(inner, currentKey)));
+      }
+      hasNext = false;
+      return null;
+    }
+
+    class ValueIterator implements Iterable<V> {
+
+      boolean usedAsIterable = false;
+      private final PeekingIterator<Tuple2<WindowedKey, byte[]>> inner;
+      private WindowedKey currentKey;
+
+      ValueIterator(PeekingIterator<Tuple2<WindowedKey, byte[]>> inner, WindowedKey currentKey) {
+        this.inner = inner;
+        this.currentKey = currentKey;
+      }
+
+      @Override
+      public Iterator<V> iterator() {
+        if (usedAsIterable) {
+          throw new IllegalStateException(
+              "ValueIterator can't be iterated more than once,"
+                  + "otherwise there could be data lost");
+        }
+        usedAsIterable = true;
+        return new AbstractIterator<V>() {
+          @Override
+          protected V computeNext() {
+            if (inner.hasNext() && currentKey.equals(inner.peek()._1)) {
+              return decodeValue(inner.next()._2);
+            }
+            return endOfData();
+          }
+        };
+      }
+    }
+
+    private V decodeValue(byte[] windowedValueBytes) {
+      final WindowedValue<byte[]> windowedValue =
+          CoderHelpers.fromByteArray(windowedValueBytes, windowedValueCoder);
+      return CoderHelpers.fromByteArray(windowedValue.getValue(), valueCoder);
+    }
+
+    private WindowedValue<KV<K, V>> decodeItem(Tuple2<WindowedKey, byte[]> item) {
+      final K key = CoderHelpers.fromByteArray(item._1.getKey(), keyCoder);
+      final WindowedValue<byte[]> windowedValue =
+          CoderHelpers.fromByteArray(item._2, windowedValueCoder);
+      final V value = CoderHelpers.fromByteArray(windowedValue.getValue(), valueCoder);
+      @SuppressWarnings("unchecked")
+      final W window = (W) Iterables.getOnlyElement(windowedValue.getWindows());
+      final Instant timestamp =
+          windowingStrategy
+              .getTimestampCombiner()
+              .assign(
+                  window,
+                  windowingStrategy
+                      .getWindowFn()
+                      .getOutputTime(windowedValue.getTimestamp(), window));
+      return WindowedValue.of(KV.of(key, value), timestamp, window, windowedValue.getPane());
+    }
+  }
+
+  /** Composite key of key and window for groupByKey transformation. */
+  public static class WindowedKey implements Comparable<WindowedKey>, Serializable {
+
+    private final byte[] key;
+    private final byte[] window;
+
+    WindowedKey(byte[] key, byte[] window) {
+      this.key = key;
+      this.window = window;
+    }
+
+    @Override
+    public boolean equals(Object o) {
+      if (this == o) {
+        return true;
+      }
+      if (o == null || getClass() != o.getClass()) {
+        return false;
+      }
+      WindowedKey that = (WindowedKey) o;
+      return Arrays.equals(key, that.key) && Arrays.equals(window, that.window);
+    }
+
+    @Override
+    public int hashCode() {
+      int result = Arrays.hashCode(key);
+      result = 31 * result + Arrays.hashCode(window);
+      return result;
+    }
+
+    public byte[] getKey() {
+      return key;
+    }
+
+    public byte[] getWindow() {
+      return window;
+    }
+
+    @Override
+    public int compareTo(WindowedKey o) {
+      int keyCompare = UnsignedBytes.lexicographicalComparator().compare(this.getKey(), o.getKey());
+      if (keyCompare == 0) {
+        return UnsignedBytes.lexicographicalComparator().compare(this.getWindow(), o.getWindow());
+      }
+      return keyCompare;
+    }
+  }
+}
diff --git a/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/TransformTranslator.java b/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/TransformTranslator.java
index c6fc4345145..66de2f06690 100644
--- a/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/TransformTranslator.java
+++ b/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/TransformTranslator.java
@@ -52,6 +52,7 @@ import org.apache.beam.sdk.transforms.View;
 import org.apache.beam.sdk.transforms.reflect.DoFnSignature;
 import org.apache.beam.sdk.transforms.reflect.DoFnSignatures;
 import org.apache.beam.sdk.transforms.windowing.BoundedWindow;
+import org.apache.beam.sdk.transforms.windowing.TimestampCombiner;
 import org.apache.beam.sdk.transforms.windowing.Window;
 import org.apache.beam.sdk.transforms.windowing.WindowFn;
 import org.apache.beam.sdk.util.CombineFnUtil;
@@ -132,22 +133,32 @@ public final class TransformTranslator {
         final WindowedValue.WindowedValueCoder<V> wvCoder =
             WindowedValue.FullWindowedValueCoder.of(coder.getValueCoder(), windowFn.windowCoder());
 
-        // --- group by key only.
-        JavaRDD<WindowedValue<KV<K, Iterable<WindowedValue<V>>>>> groupedByKey =
-            GroupCombineFunctions.groupByKeyOnly(inRDD, keyCoder, wvCoder, getPartitioner(context));
-
-        // --- now group also by window.
-        // for batch, GroupAlsoByWindow uses an in-memory StateInternals.
-        JavaRDD<WindowedValue<KV<K, Iterable<V>>>> groupedAlsoByWindow =
-            groupedByKey.flatMap(
-                new SparkGroupAlsoByWindowViaOutputBufferFn<>(
-                    windowingStrategy,
-                    new TranslationUtils.InMemoryStateInternalsFactory<>(),
-                    SystemReduceFn.buffering(coder.getValueCoder()),
-                    context.getSerializableOptions(),
-                    accum));
-
-        context.putDataset(transform, new BoundedDataset<>(groupedAlsoByWindow));
+        JavaRDD<WindowedValue<KV<K, Iterable<V>>>> groupedByKey;
+        if (windowingStrategy.getWindowFn().isNonMerging()
+            && windowingStrategy.getTimestampCombiner() == TimestampCombiner.END_OF_WINDOW) {
+          // we can have a memory sensitive translation for non-merging windows
+          groupedByKey =
+              GroupNonMergingWindowsFunctions.groupByKeyAndWindow(
+                  inRDD, keyCoder, coder.getValueCoder(), windowingStrategy);
+        } else {
+
+          // --- group by key only.
+          Partitioner partitioner = getPartitioner(context);
+          JavaRDD<WindowedValue<KV<K, Iterable<WindowedValue<V>>>>> groupedByKeyOnly =
+              GroupCombineFunctions.groupByKeyOnly(inRDD, keyCoder, wvCoder, partitioner);
+
+          // --- now group also by window.
+          // for batch, GroupAlsoByWindow uses an in-memory StateInternals.
+          groupedByKey =
+              groupedByKeyOnly.flatMap(
+                  new SparkGroupAlsoByWindowViaOutputBufferFn<>(
+                      windowingStrategy,
+                      new TranslationUtils.InMemoryStateInternalsFactory<>(),
+                      SystemReduceFn.buffering(coder.getValueCoder()),
+                      context.getSerializableOptions(),
+                      accum));
+        }
+        context.putDataset(transform, new BoundedDataset<>(groupedByKey));
       }
 
       @Override
diff --git a/runners/spark/src/test/java/org/apache/beam/runners/spark/CacheTest.java b/runners/spark/src/test/java/org/apache/beam/runners/spark/CacheTest.java
index f2a5f1e8979..51facb466c6 100644
--- a/runners/spark/src/test/java/org/apache/beam/runners/spark/CacheTest.java
+++ b/runners/spark/src/test/java/org/apache/beam/runners/spark/CacheTest.java
@@ -30,6 +30,7 @@ import org.apache.beam.sdk.coders.Coder;
 import org.apache.beam.sdk.options.PipelineOptionsFactory;
 import org.apache.beam.sdk.transforms.Count;
 import org.apache.beam.sdk.transforms.Create;
+import org.apache.beam.sdk.transforms.Create.Values;
 import org.apache.beam.sdk.values.PCollection;
 import org.apache.spark.api.java.JavaSparkContext;
 import org.junit.Test;
@@ -70,15 +71,16 @@ public class CacheTest {
     options.setRunner(TestSparkRunner.class);
     options.setCacheDisabled(true);
     Pipeline pipeline = Pipeline.create(options);
-    PCollection<String> pCollection = pipeline.apply(Create.of("foo", "bar"));
+    Values<String> createInput = Create.of("foo", "bar");
+    PCollection<String> pCollection = pipeline.apply(createInput);
 
     JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);
     EvaluationContext ctxt = new EvaluationContext(jsc, pipeline, options);
     ctxt.getCacheCandidates().put(pCollection, 2L);
 
-    assertFalse(ctxt.shouldCache(pCollection));
+    assertFalse(ctxt.shouldCache(pCollection, createInput));
 
     options.setCacheDisabled(false);
-    assertTrue(ctxt.shouldCache(pCollection));
+    assertTrue(ctxt.shouldCache(pCollection, createInput));
   }
 }
diff --git a/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/GroupNonMergingWindowsFunctionsTest.java b/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/GroupNonMergingWindowsFunctionsTest.java
new file mode 100644
index 00000000000..728e813e781
--- /dev/null
+++ b/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/GroupNonMergingWindowsFunctionsTest.java
@@ -0,0 +1,133 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.beam.runners.spark.translation;
+
+import static org.junit.Assert.assertEquals;
+
+import java.util.Arrays;
+import java.util.Iterator;
+import java.util.List;
+import org.apache.beam.runners.spark.coders.CoderHelpers;
+import org.apache.beam.runners.spark.translation.GroupNonMergingWindowsFunctions.GroupByKeyIterator;
+import org.apache.beam.runners.spark.translation.GroupNonMergingWindowsFunctions.WindowedKey;
+import org.apache.beam.sdk.coders.BigEndianIntegerCoder;
+import org.apache.beam.sdk.coders.ByteArrayCoder;
+import org.apache.beam.sdk.coders.Coder;
+import org.apache.beam.sdk.coders.StringUtf8Coder;
+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;
+import org.apache.beam.sdk.transforms.windowing.GlobalWindows;
+import org.apache.beam.sdk.transforms.windowing.PaneInfo;
+import org.apache.beam.sdk.util.WindowedValue;
+import org.apache.beam.sdk.util.WindowedValue.FullWindowedValueCoder;
+import org.apache.beam.sdk.values.KV;
+import org.apache.beam.sdk.values.WindowingStrategy;
+import org.joda.time.Instant;
+import org.junit.Assert;
+import org.junit.Test;
+import scala.Tuple2;
+
+/** Unit tests of {@link GroupNonMergingWindowsFunctions}. */
+public class GroupNonMergingWindowsFunctionsTest {
+
+  @Test
+  public void testGroupByKeyIterator() {
+    GroupByKeyIterator<String, Integer, GlobalWindow> iteratorUnderTest = createGbkIterator();
+
+    Assert.assertTrue(iteratorUnderTest.hasNext());
+    WindowedValue<KV<String, Iterable<Integer>>> k1Win = iteratorUnderTest.next();
+    // testing that calling 2x hasNext doesn't move to next key iterator
+    Assert.assertTrue(iteratorUnderTest.hasNext());
+    Assert.assertTrue(iteratorUnderTest.hasNext());
+
+    Iterator<Integer> valuesIteratorForK1 = k1Win.getValue().getValue().iterator();
+
+    Assert.assertTrue("Now we expect first value for K1 to pop up.", valuesIteratorForK1.hasNext());
+    assertEquals(1L, valuesIteratorForK1.next().longValue());
+    Assert.assertTrue(valuesIteratorForK1.hasNext());
+    Assert.assertTrue(valuesIteratorForK1.hasNext());
+    assertEquals(2L, valuesIteratorForK1.next().longValue());
+
+    WindowedValue<KV<String, Iterable<Integer>>> k2Win = iteratorUnderTest.next();
+    Iterator<Integer> valuesIteratorForK2 = k2Win.getValue().getValue().iterator();
+    assertEquals(3L, valuesIteratorForK2.next().longValue());
+  }
+
+  @Test(expected = IllegalStateException.class)
+  public void testGbkIteratorValuesCannotBeReiterated() {
+    GroupByKeyIterator<String, Integer, GlobalWindow> iteratorUnderTest = createGbkIterator();
+    WindowedValue<KV<String, Iterable<Integer>>> firstEl = iteratorUnderTest.next();
+    Iterable<Integer> value = firstEl.getValue().getValue();
+    for (Integer i : value) {
+      // first iteration
+    }
+    for (Integer i : value) {
+      // second iteration should throw IllegalStateException
+    }
+  }
+
+  GroupByKeyIterator<String, Integer, GlobalWindow> createGbkIterator() {
+    StringUtf8Coder keyCoder = StringUtf8Coder.of();
+    BigEndianIntegerCoder valueCoder = BigEndianIntegerCoder.of();
+    WindowingStrategy<Object, GlobalWindow> winStrategy = WindowingStrategy.of(new GlobalWindows());
+
+    final WindowedValue.FullWindowedValueCoder<byte[]> winValCoder =
+        WindowedValue.getFullCoder(ByteArrayCoder.of(), winStrategy.getWindowFn().windowCoder());
+
+    ItemFactory<String, Integer> factory =
+        new ItemFactory<>(
+            keyCoder, valueCoder, winValCoder, winStrategy.getWindowFn().windowCoder());
+    List<Tuple2<WindowedKey, byte[]>> items =
+        Arrays.asList(
+            factory.create("k1", 1),
+            factory.create("k1", 2),
+            factory.create("k2", 3),
+            factory.create("k2", 4),
+            factory.create("k2", 5));
+    return new GroupByKeyIterator<>(
+        items.iterator(), keyCoder, valueCoder, winStrategy, winValCoder);
+  }
+
+  private static class ItemFactory<K, V> {
+    private final Coder<K> keyCoder;
+    private final Coder<V> valueCoder;
+    private final WindowedValue.FullWindowedValueCoder<byte[]> winValCoder;
+    private final byte[] globalWindow;
+
+    public ItemFactory(
+        Coder<K> keyCoder,
+        Coder<V> valueCoder,
+        FullWindowedValueCoder<byte[]> winValCoder,
+        Coder<GlobalWindow> winCoder) {
+      this.keyCoder = keyCoder;
+      this.valueCoder = valueCoder;
+      this.winValCoder = winValCoder;
+      this.globalWindow = CoderHelpers.toByteArray(GlobalWindow.INSTANCE, winCoder);
+    }
+
+    private Tuple2<WindowedKey, byte[]> create(K key, V value) {
+      WindowedKey kaw = new WindowedKey(CoderHelpers.toByteArray(key, keyCoder), globalWindow);
+
+      byte[] valueInbytes = CoderHelpers.toByteArray(value, valueCoder);
+
+      WindowedValue<byte[]> windowedValue =
+          WindowedValue.of(
+              valueInbytes, Instant.now(), GlobalWindow.INSTANCE, PaneInfo.ON_TIME_AND_ONLY_FIRING);
+      return new Tuple2<>(kaw, CoderHelpers.toByteArray(windowedValue, winValCoder));
+    }
+  }
+}
