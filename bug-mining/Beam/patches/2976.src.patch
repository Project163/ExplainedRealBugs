diff --git a/.test-infra/jenkins/Kubernetes.groovy b/.test-infra/jenkins/Kubernetes.groovy
index 67897616203..957c823cbd8 100644
--- a/.test-infra/jenkins/Kubernetes.groovy
+++ b/.test-infra/jenkins/Kubernetes.groovy
@@ -132,4 +132,34 @@ class Kubernetes {
       }
     }
   }
+
+  /**
+   * Specifies steps that will return an available port on the Kubernetes cluster,
+   * the value of the available port will be stored in job.properties using referenceName as key
+   *
+   * @param lowRangePort - low range port to be used
+   * @param highRangePort - high range port to be used
+   * @param referenceName - name of the environment variable
+   */
+  void availablePort(String lowRangePort, String highRangePort, String referenceName) {
+    job.steps {
+      String command = "${KUBERNETES_SCRIPT} getAvailablePort ${lowRangePort} ${highRangePort}"
+      shell("set -xo pipefail; eval ${command} | sed 's/^/${referenceName}=/' > job.properties")
+      environmentVariables {
+        propertiesFile('job.properties')
+      }
+    }
+  }
+
+  /**
+   * Specifies steps to wait until a job finishes
+   * @param jobName - job running in Kubernetes cluster
+   * @param timeout - max time to wait for job to finish
+   */
+  void waitForJob(String jobName, String timeout){
+    job.steps{
+      String command="${KUBERNETES_SCRIPT} waitForJob ${jobName} ${timeout}"
+      shell("eval ${command}")
+    }
+  }
 }
diff --git a/.test-infra/jenkins/job_PerformanceTests_KafkaIO_IT.groovy b/.test-infra/jenkins/job_PerformanceTests_KafkaIO_IT.groovy
index 7646a3c2889..8aaa59ebf40 100644
--- a/.test-infra/jenkins/job_PerformanceTests_KafkaIO_IT.groovy
+++ b/.test-infra/jenkins/job_PerformanceTests_KafkaIO_IT.groovy
@@ -21,11 +21,22 @@ import Kubernetes
 import InfluxDBCredentialsHelper
 
 String jobName = "beam_PerformanceTests_Kafka_IO"
+String HIGH_RANGE_PORT = "32767"
 
+/**
+ * This job runs the Kafka IO performance tests.
+ It runs on a kafka cluster that is build by applying the folder .test-infra/kubernetes/kafka-cluster,
+ in an existing kubernetes cluster (DEFAULT_CLUSTER in Kubernetes.groovy).
+ The services created to run this test are:
+ Pods: 3 kafka pods, 3 zookeeper pods, 1 kafka-config pod which run a job that creates topics.
+ Services: 1 bootstrap, 1 broker, 3 outside, 1 zookeeper
+ Job: job.batch/kafka-config-eff079ec
+ When the performance tests finish all resources are cleaned up by a postBuild step in Kubernetes.groovy
+ */
 job(jobName) {
-  common.setTopLevelMainJobProperties(delegate)
-  // TODO(https://github.com/apache/beam/issues/20333): Re-enable once fixed.
-  // common.setAutoJob(delegate, 'H H/6 * * *')
+  common.setTopLevelMainJobProperties(delegate, 'master', 120)
+  common.setAutoJob(delegate, 'H H/6 * * *')
+  // [Issue#21824] Disable trigger
   //  common.enablePhraseTriggeringFromPullRequest(
   //      delegate,
   //      'Java KafkaIO Performance Test',
@@ -35,9 +46,28 @@ job(jobName) {
   String namespace = common.getKubernetesNamespace(jobName)
   String kubeconfig = common.getKubeconfigLocationForNamespace(namespace)
   Kubernetes k8s = Kubernetes.create(delegate, kubeconfig, namespace)
-  k8s.apply(common.makePathAbsolute("src/.test-infra/kubernetes/kafka-cluster"))
 
+  String kafkaDir = common.makePathAbsolute("src/.test-infra/kubernetes/kafka-cluster")
+  String kafkaTopicJob = "job.batch/kafka-config-eff079ec"
+
+  /**
+   * Specifies steps to avoid port collisions when the Kafka outside services (1,2,3) are created.
+   Function k8s.availablePort finds unused ports in the Kubernetes cluster in a range from 32400
+   to 32767 by querying used ports, those ports are stored in env vars like KAFKA_SERVICE_PORT_${service},
+   which are used to replace default ports for outside-${service}.yml files, before the apply command.
+   */
+  steps {
+    String[] configuredPorts = ["32400", "32401", "32402"]
+    (0..2).each { service ->
+      k8s.availablePort(service == 0 ? configuredPorts[service] : "\$KAFKA_SERVICE_PORT_${service-1}",
+          HIGH_RANGE_PORT, "KAFKA_SERVICE_PORT_$service")
+      shell("sed -i -e s/${configuredPorts[service]}/\$KAFKA_SERVICE_PORT_$service/ \
+                  ${kafkaDir}/04-outside-services/outside-${service}.yml")
+    }
+  }
+  k8s.apply(kafkaDir)
   (0..2).each { k8s.loadBalancerIP("outside-$it", "KAFKA_BROKER_$it") }
+  k8s.waitForJob(kafkaTopicJob,"40m")
 
   Map pipelineOptions = [
     tempRoot                     : 'gs://temp-storage-for-perf-tests',
@@ -55,7 +85,8 @@ job(jobName) {
     influxMeasurement            : 'kafkaioit_results',
     influxDatabase               : InfluxDBCredentialsHelper.InfluxDBDatabaseName,
     influxHost                   : InfluxDBCredentialsHelper.InfluxDBHostUrl,
-    kafkaBootstrapServerAddresses: "\$KAFKA_BROKER_0:32400,\$KAFKA_BROKER_1:32401,\$KAFKA_BROKER_2:32402",
+    kafkaBootstrapServerAddresses: "\$KAFKA_BROKER_0:\$KAFKA_SERVICE_PORT_0,\$KAFKA_BROKER_1:\$KAFKA_SERVICE_PORT_1," +
+    "\$KAFKA_BROKER_2:\$KAFKA_SERVICE_PORT_2", //KAFKA_BROKER_ represents IP and KAFKA_SERVICE_ port of outside services
     kafkaTopic                   : 'beam',
     readTimeout                  : '900',
     numWorkers                   : '5',
diff --git a/.test-infra/kubernetes/kafka-cluster/05-kafka/10broker-config.yml b/.test-infra/kubernetes/kafka-cluster/05-kafka/10broker-config.yml
index 575458b6996..c465e026e48 100644
--- a/.test-infra/kubernetes/kafka-cluster/05-kafka/10broker-config.yml
+++ b/.test-infra/kubernetes/kafka-cluster/05-kafka/10broker-config.yml
@@ -45,7 +45,7 @@ data:
         [ -z "$OUTSIDE_HOST" ] && sleep 10
       done
       # OUTSIDE_HOST=$(kubectl get node "$NODE_NAME" -o jsonpath='{.status.addresses[?(@.type=="InternalIP")].address}')
-      OUTSIDE_PORT=3240${KAFKA_BROKER_ID}
+      OUTSIDE_PORT=$(kubectl get svc outside-${KAFKA_BROKER_ID} --template="{{range .spec.ports}}{{.port}}{{end}}")
       SEDS+=("s|#init#advertised.listeners=OUTSIDE://#init#|advertised.listeners=OUTSIDE://${OUTSIDE_HOST}:${OUTSIDE_PORT}|")
       ANNOTATIONS="$ANNOTATIONS kafka-listener-outside-host=$OUTSIDE_HOST kafka-listener-outside-port=$OUTSIDE_PORT"
 
diff --git a/.test-infra/kubernetes/kubernetes.sh b/.test-infra/kubernetes/kubernetes.sh
index fd2da72fd51..1ec3cd9364c 100755
--- a/.test-infra/kubernetes/kubernetes.sh
+++ b/.test-infra/kubernetes/kubernetes.sh
@@ -90,4 +90,44 @@ function loadBalancerIP() {
   retry "${command}" 36 10
 }
 
+# Gets Available NodePort to avoid conflicts with already allocated ports
+#
+# Usage: ./kubernetes.sh getAvailablePort <low range port> <high range port>
+function getAvailablePort() {
+  local lowRangePort=$(($1 + 1)) #not inclusive low range
+  local highRangePort=$2
+  local used=false
+  local command="$KUBECTL get svc --all-namespaces -o \
+  go-template='{{range .items}}{{range.spec.ports}}{{if .nodePort}}{{.nodePort}}{{\"\n\"}}{{end}}{{end}}{{end}}'"
+  local usedPorts
+  usedPorts=$(eval "${command}")
+  local availablePort=$lowRangePort
+
+  for i in $(seq $lowRangePort $highRangePort);
+    do
+      while IFS= read -r usedPort; do
+        if [ "$i" = "$usedPort" ]; then
+          used=true
+          break
+        fi
+      done <<< "$usedPorts"
+    if $used; then
+      availablePort=$((availablePort + 1))
+      used=false
+    else
+      echo $availablePort
+      return 0
+    fi
+    echo $availablePort
+  done
+}
+#Waits until a designated job finish to continue the workflow execution
+#Usage: ./kubernetes.sh waitForJob <kubernetes job name>  <timeout i.e: 30m, 20s etc.>
+function waitForJob(){
+  echo "Waiting for job completion..."
+  jobName=$1 
+  eval "$KUBECTL wait --for=condition=complete --timeout=$2 $jobName" 
+  echo "Job completed"
+}
+
 "$@"
diff --git a/buildSrc/src/main/groovy/org/apache/beam/gradle/BeamModulePlugin.groovy b/buildSrc/src/main/groovy/org/apache/beam/gradle/BeamModulePlugin.groovy
index a0185939ebc..ae1c581c554 100644
--- a/buildSrc/src/main/groovy/org/apache/beam/gradle/BeamModulePlugin.groovy
+++ b/buildSrc/src/main/groovy/org/apache/beam/gradle/BeamModulePlugin.groovy
@@ -669,7 +669,7 @@ class BeamModulePlugin implements Plugin<Project> {
         nemo_compiler_frontend_beam                 : "org.apache.nemo:nemo-compiler-frontend-beam:$nemo_version",
         netty_all                                   : "io.netty:netty-all:$netty_version",
         netty_handler                               : "io.netty:netty-handler:$netty_version",
-        netty_tcnative_boringssl_static             : "io.netty:netty-tcnative-boringssl-static:2.0.46.Final",
+        netty_tcnative_boringssl_static             : "io.netty:netty-tcnative-boringssl-static:2.0.47.Final",
         netty_transport_native_epoll                : "io.netty:netty-transport-native-epoll:$netty_version",
         postgres                                    : "org.postgresql:postgresql:$postgres_version",
         powermock                                   : "org.powermock:powermock-module-junit4:$powermock_version",
@@ -1776,7 +1776,7 @@ class BeamModulePlugin implements Plugin<Project> {
       }
     }
     def cleanUpTask = project.tasks.register('cleanUp') {
-      dependsOn ':runners:google-cloud-dataflow-java:cleanUpDockerImages'
+      dependsOn ':runners:google-cloud-dataflow-java:cleanUpDockerJavaImages'
     }
 
     // When applied in a module's build.gradle file, this closure provides task for running
@@ -1805,18 +1805,18 @@ class BeamModulePlugin implements Plugin<Project> {
 
         if (pipelineOptionsString && configuration.runner?.equalsIgnoreCase('dataflow')) {
           if (pipelineOptionsString.contains('use_runner_v2')) {
-            dependsOn ':runners:google-cloud-dataflow-java:buildAndPushDockerContainer'
+            dependsOn ':runners:google-cloud-dataflow-java:buildAndPushDockerJavaContainer'
           }
         }
 
-        // We construct the pipeline options during task execution time in order to get dockerImageName.
+        // We construct the pipeline options during task execution time in order to get dockerJavaImageName.
         doFirst {
           if (pipelineOptionsString && configuration.runner?.equalsIgnoreCase('dataflow')) {
             def dataflowRegion = project.findProperty('dataflowRegion') ?: 'us-central1'
             if (pipelineOptionsString.contains('use_runner_v2')) {
-              def dockerImageName = project.project(':runners:google-cloud-dataflow-java').ext.dockerImageName
+              def dockerJavaImageName = project.project(':runners:google-cloud-dataflow-java').ext.dockerJavaImageName
               allOptionsList.addAll([
-                "--sdkContainerImage=${dockerImageName}",
+                "--sdkContainerImage=${dockerJavaImageName}",
                 "--region=${dataflowRegion}"
               ])
             } else {
@@ -1847,7 +1847,7 @@ class BeamModulePlugin implements Plugin<Project> {
       project.afterEvaluate {
         // Ensure all tasks which use published docker images run before they are cleaned up
         project.tasks.each { t ->
-          if (t.dependsOn.contains(":runners:google-cloud-dataflow-java:buildAndPushDockerContainer")) {
+          if (t.dependsOn.contains(":runners:google-cloud-dataflow-java:buildAndPushDockerJavaContainer")) {
             t.finalizedBy cleanUpTask
           }
         }
