diff --git a/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java b/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java
index daf627782b9..b6fc933adbd 100644
--- a/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java
+++ b/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java
@@ -258,6 +258,7 @@ import org.slf4j.LoggerFactory;
   "nullness" // TODO(https://issues.apache.org/jira/browse/BEAM-10402)
 })
 public class ParquetIO {
+  private static final Logger LOG = LoggerFactory.getLogger(ParquetIO.class);
 
   /**
    * Reads {@link GenericRecord} from a Parquet file (or multiple Parquet files matching the
@@ -714,7 +715,6 @@ public class ParquetIO {
     @DoFn.BoundedPerElement
     static class SplitReadFn<T> extends DoFn<ReadableFile, T> {
       private final Class<? extends GenericData> modelClass;
-      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);
       private final String requestSchemaString;
       // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.
       private static final long SPLIT_LIMIT = 64000000;
@@ -735,7 +735,7 @@ public class ParquetIO {
         this.configuration = configuration;
       }
 
-      ParquetFileReader getParquetFileReader(ReadableFile file) throws Exception {
+      private ParquetFileReader getParquetFileReader(ReadableFile file) throws Exception {
         ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();
         return ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);
       }
@@ -762,83 +762,86 @@ public class ParquetIO {
               conf, new Schema.Parser().parse(requestSchemaString));
         }
         ParquetReadOptions options = HadoopReadOptions.builder(conf).build();
-        ParquetFileReader reader =
-            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);
-        Filter filter = checkNotNull(options.getRecordFilter(), "filter");
-        Configuration hadoopConf = ((HadoopReadOptions) options).getConf();
-        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();
-        MessageType fileSchema = parquetFileMetadata.getSchema();
-        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();
-        ReadSupport.ReadContext readContext =
-            readSupport.init(
-                new InitContext(
-                    hadoopConf, Maps.transformValues(fileMetadata, ImmutableSet::of), fileSchema));
-        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());
-
-        RecordMaterializer<GenericRecord> recordConverter =
-            readSupport.prepareForRead(hadoopConf, fileMetadata, fileSchema, readContext);
-        reader.setRequestedSchema(readContext.getRequestedSchema());
-        MessageColumnIO columnIO =
-            columnIOFactory.getColumnIO(readContext.getRequestedSchema(), fileSchema, true);
-        long currentBlock = tracker.currentRestriction().getFrom();
-        for (int i = 0; i < currentBlock; i++) {
-          reader.skipNextRowGroup();
-        }
-        while (tracker.tryClaim(currentBlock)) {
-          PageReadStore pages = reader.readNextRowGroup();
-          LOG.debug("block {} read in memory. row count = {}", currentBlock, pages.getRowCount());
-          currentBlock += 1;
-          RecordReader<GenericRecord> recordReader =
-              columnIO.getRecordReader(
-                  pages, recordConverter, options.useRecordFilter() ? filter : FilterCompat.NOOP);
-          long currentRow = 0;
-          long totalRows = pages.getRowCount();
-          while (currentRow < totalRows) {
-            try {
-              GenericRecord record;
-              currentRow += 1;
+        try (ParquetFileReader reader =
+            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options)) {
+          Filter filter = checkNotNull(options.getRecordFilter(), "filter");
+          Configuration hadoopConf = ((HadoopReadOptions) options).getConf();
+          FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();
+          MessageType fileSchema = parquetFileMetadata.getSchema();
+          Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();
+          ReadSupport.ReadContext readContext =
+              readSupport.init(
+                  new InitContext(
+                      hadoopConf,
+                      Maps.transformValues(fileMetadata, ImmutableSet::of),
+                      fileSchema));
+          ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());
+
+          RecordMaterializer<GenericRecord> recordConverter =
+              readSupport.prepareForRead(hadoopConf, fileMetadata, fileSchema, readContext);
+          reader.setRequestedSchema(readContext.getRequestedSchema());
+          MessageColumnIO columnIO =
+              columnIOFactory.getColumnIO(readContext.getRequestedSchema(), fileSchema, true);
+          long currentBlock = tracker.currentRestriction().getFrom();
+          for (int i = 0; i < currentBlock; i++) {
+            reader.skipNextRowGroup();
+          }
+          while (tracker.tryClaim(currentBlock)) {
+            PageReadStore pages = reader.readNextRowGroup();
+            LOG.debug("block {} read in memory. row count = {}", currentBlock, pages.getRowCount());
+            currentBlock += 1;
+            RecordReader<GenericRecord> recordReader =
+                columnIO.getRecordReader(
+                    pages, recordConverter, options.useRecordFilter() ? filter : FilterCompat.NOOP);
+            long currentRow = 0;
+            long totalRows = pages.getRowCount();
+            while (currentRow < totalRows) {
               try {
-                record = recordReader.read();
-              } catch (RecordMaterializer.RecordMaterializationException e) {
-                LOG.warn(
-                    "skipping a corrupt record at {} in block {} in file {}",
-                    currentRow,
-                    currentBlock,
-                    file.toString());
-                continue;
-              }
-              if (record == null) {
-                // only happens with FilteredRecordReader at end of block
-                LOG.debug(
-                    "filtered record reader reached end of block in block {} in file {}",
-                    currentBlock,
-                    file.toString());
-                break;
+                GenericRecord record;
+                currentRow += 1;
+                try {
+                  record = recordReader.read();
+                } catch (RecordMaterializer.RecordMaterializationException e) {
+                  LOG.warn(
+                      "skipping a corrupt record at {} in block {} in file {}",
+                      currentRow,
+                      currentBlock,
+                      file.toString());
+                  continue;
+                }
+                if (record == null) {
+                  // only happens with FilteredRecordReader at end of block
+                  LOG.debug(
+                      "filtered record reader reached end of block in block {} in file {}",
+                      currentBlock,
+                      file.toString());
+                  break;
+                }
+                if (recordReader.shouldSkipCurrentRecord()) {
+                  // this record is being filtered via the filter2 package
+                  LOG.debug(
+                      "skipping record at {} in block {} in file {}",
+                      currentRow,
+                      currentBlock,
+                      file.toString());
+                  continue;
+                }
+                outputReceiver.output(parseFn.apply(record));
+              } catch (RuntimeException e) {
+
+                throw new ParquetDecodingException(
+                    format(
+                        "Can not read value at %d in block %d in file %s",
+                        currentRow, currentBlock, file.toString()),
+                    e);
               }
-              if (recordReader.shouldSkipCurrentRecord()) {
-                // this record is being filtered via the filter2 package
-                LOG.debug(
-                    "skipping record at {} in block {} in file {}",
-                    currentRow,
-                    currentBlock,
-                    file.toString());
-                continue;
-              }
-              outputReceiver.output(parseFn.apply(record));
-            } catch (RuntimeException e) {
-
-              throw new ParquetDecodingException(
-                  format(
-                      "Can not read value at %d in block %d in file %s",
-                      currentRow, currentBlock, file.toString()),
-                  e);
             }
+            LOG.debug(
+                "Finish processing {} rows from block {} in file {}",
+                currentRow,
+                currentBlock - 1,
+                file.toString());
           }
-          LOG.debug(
-              "Finish processing {} rows from block {} in file {}",
-              currentRow,
-              currentBlock - 1,
-              file.toString());
         }
       }
 
@@ -857,8 +860,9 @@ public class ParquetIO {
 
       @GetInitialRestriction
       public OffsetRange getInitialRestriction(@Element ReadableFile file) throws Exception {
-        ParquetFileReader reader = getParquetFileReader(file);
-        return new OffsetRange(0, reader.getRowGroups().size());
+        try (ParquetFileReader reader = getParquetFileReader(file)) {
+          return new OffsetRange(0, reader.getRowGroups().size());
+        }
       }
 
       @SplitRestriction
@@ -867,12 +871,13 @@ public class ParquetIO {
           OutputReceiver<OffsetRange> out,
           @Element ReadableFile file)
           throws Exception {
-        ParquetFileReader reader = getParquetFileReader(file);
-        List<BlockMetaData> rowGroups = reader.getRowGroups();
-        for (OffsetRange offsetRange :
-            splitBlockWithLimit(
-                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT)) {
-          out.output(offsetRange);
+        try (ParquetFileReader reader = getParquetFileReader(file)) {
+          List<BlockMetaData> rowGroups = reader.getRowGroups();
+          for (OffsetRange offsetRange :
+              splitBlockWithLimit(
+                  restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT)) {
+            out.output(offsetRange);
+          }
         }
       }
 
@@ -918,15 +923,16 @@ public class ParquetIO {
 
       private CountAndSize getRecordCountAndSize(ReadableFile file, OffsetRange restriction)
           throws Exception {
-        ParquetFileReader reader = getParquetFileReader(file);
-        double size = 0;
-        double recordCount = 0;
-        for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {
-          BlockMetaData block = reader.getRowGroups().get((int) i);
-          recordCount += block.getRowCount();
-          size += block.getTotalByteSize();
+        try (ParquetFileReader reader = getParquetFileReader(file)) {
+          double size = 0;
+          double recordCount = 0;
+          for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {
+            BlockMetaData block = reader.getRowGroups().get((int) i);
+            recordCount += block.getRowCount();
+            size += block.getTotalByteSize();
+          }
+          return CountAndSize.create(recordCount, size);
         }
-        return CountAndSize.create(recordCount, size);
       }
 
       @AutoValue
@@ -988,9 +994,8 @@ public class ParquetIO {
       }
 
       @ProcessElement
-      public void processElement(ProcessContext processContext) throws Exception {
-        ReadableFile file = processContext.element();
-
+      public void processElement(@Element ReadableFile file, OutputReceiver<T> receiver)
+          throws Exception {
         if (!file.getMetadata().isReadSeekEfficient()) {
           ResourceId filename = file.getMetadata().resourceId();
           throw new RuntimeException(String.format("File has to be seekable: %s", filename));
@@ -1003,7 +1008,6 @@ public class ParquetIO {
                 AvroParquetReader.<GenericRecord>builder(
                         new BeamParquetInputFile(seekableByteChannel))
                     .withConf(SerializableConfiguration.newConfiguration(configuration));
-
         if (modelClass != null) {
           // all GenericData implementations have a static get method
           builder = builder.withDataModel(buildModelObject(modelClass));
@@ -1012,14 +1016,15 @@ public class ParquetIO {
         try (ParquetReader<GenericRecord> reader = builder.build()) {
           GenericRecord read;
           while ((read = reader.read()) != null) {
-            processContext.output(parseFn.apply(read));
+            receiver.output(parseFn.apply(read));
           }
         }
+
+        seekableByteChannel.close();
       }
     }
 
     private static class BeamParquetInputFile implements InputFile {
-
       private final SeekableByteChannel seekableByteChannel;
 
       BeamParquetInputFile(SeekableByteChannel seekableByteChannel) {
diff --git a/sdks/java/io/parquet/src/test/resources/log4j-test.properties b/sdks/java/io/parquet/src/test/resources/log4j-test.properties
new file mode 100644
index 00000000000..e28055ab941
--- /dev/null
+++ b/sdks/java/io/parquet/src/test/resources/log4j-test.properties
@@ -0,0 +1,31 @@
+################################################################################
+#  Licensed to the Apache Software Foundation (ASF) under one
+#  or more contributor license agreements.  See the NOTICE file
+#  distributed with this work for additional information
+#  regarding copyright ownership.  The ASF licenses this file
+#  to you under the Apache License, Version 2.0 (the
+#  "License"); you may not use this file except in compliance
+#  with the License.  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+# limitations under the License.
+################################################################################
+
+# Set root logger level to ERROR.
+# set manually to INFO for debugging purposes.
+log4j.rootLogger=ERROR, testlogger
+
+# ConsoleAppender.
+log4j.appender.testlogger=org.apache.log4j.ConsoleAppender
+log4j.appender.testlogger.target = System.err
+log4j.appender.testlogger.layout=org.apache.log4j.PatternLayout
+log4j.appender.testlogger.layout.ConversionPattern=%d [%t] %-5p %c %x - %m%n
+
+# Beam SDK logging
+log4j.logger.org.apache.beam=INFO
+#log4j.logger.org.apache.beam.sdk.io.parquet.ParquetIO=DEBUG
