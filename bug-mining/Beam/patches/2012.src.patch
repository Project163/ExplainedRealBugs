diff --git a/sdks/python/apache_beam/testing/benchmarks/nexmark/monitor.py b/sdks/python/apache_beam/testing/benchmarks/nexmark/monitor.py
new file mode 100644
index 00000000000..fab62188a5c
--- /dev/null
+++ b/sdks/python/apache_beam/testing/benchmarks/nexmark/monitor.py
@@ -0,0 +1,68 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+from __future__ import absolute_import
+
+from time import time
+
+import apache_beam as beam
+from apache_beam.metrics import Metrics
+
+
+class Monitor(object):
+  """
+  A monitor of elements with support for later retrieving their metrics
+
+  monitor objects contains a doFn to record metrics
+
+  Args:
+    namespace: the namespace all metrics within this Monitor uses
+    name_prefix: a prefix for this Monitor's metrics' names, intended to
+      be unique in per-monitor basis in pipeline
+  """
+  def __init__(self, namespace, name_prefix):
+    # type: (str, str) -> None
+    self.namespace = namespace
+    self.name_prefix = name_prefix
+    self.doFn = MonitorDoFn(namespace, name_prefix)
+
+
+class MonitorDoFn(beam.DoFn):
+  def __init__(self, namespace, prefix):
+    self.element_count = Metrics.counter(
+        namespace, prefix + MonitorSuffix.ELEMENT_COUNTER)
+    self.event_time = Metrics.distribution(
+        namespace, prefix + MonitorSuffix.EVENT_TIME)
+    self.event_timestamp = Metrics.distribution(
+        namespace, prefix + MonitorSuffix.EVENT_TIMESTAMP)
+
+  def start_bundle(self):
+    self.event_time.update(int(time() * 1000))
+
+  def process(self, element, timestamp=beam.DoFn.TimestampParam):
+    self.element_count.inc()
+    self.event_timestamp.update(timestamp)
+    yield element
+
+  def finish_bundle(self):
+    self.event_time.update(int(time() * 1000))
+
+
+class MonitorSuffix:
+  ELEMENT_COUNTER = '.elements'
+  EVENT_TIMESTAMP = '.event_timestamp'
+  EVENT_TIME = '.event_time'
diff --git a/sdks/python/apache_beam/testing/benchmarks/nexmark/nexmark_launcher.py b/sdks/python/apache_beam/testing/benchmarks/nexmark/nexmark_launcher.py
index 51526dd5a57..e7ea4182695 100644
--- a/sdks/python/apache_beam/testing/benchmarks/nexmark/nexmark_launcher.py
+++ b/sdks/python/apache_beam/testing/benchmarks/nexmark/nexmark_launcher.py
@@ -60,6 +60,7 @@ Usage
 # pytype: skip-file
 
 from __future__ import absolute_import
+from __future__ import division
 from __future__ import print_function
 
 import argparse
@@ -76,6 +77,8 @@ from apache_beam.options.pipeline_options import SetupOptions
 from apache_beam.options.pipeline_options import StandardOptions
 from apache_beam.options.pipeline_options import TestOptions
 from apache_beam.testing.benchmarks.nexmark import nexmark_util
+from apache_beam.testing.benchmarks.nexmark.monitor import Monitor
+from apache_beam.testing.benchmarks.nexmark.monitor import MonitorSuffix
 from apache_beam.testing.benchmarks.nexmark.nexmark_util import Command
 from apache_beam.testing.benchmarks.nexmark.queries import query0
 from apache_beam.testing.benchmarks.nexmark.queries import query1
@@ -216,15 +219,15 @@ class NexmarkLauncher(object):
       self.parse_args()
       self.pipeline = beam.Pipeline(options=self.pipeline_options)
       nexmark_util.setup_coder()
+
+      event_monitor = Monitor('.events', 'event')
+      result_monitor = Monitor('.results', 'result')
+
       events = self.generate_events()
+      events = events | 'event_monitor' >> beam.ParDo(event_monitor.doFn)
       output = query.load(events, query_args)
-      # print results
-      (  # pylint: disable=expression-not-assigned
-          output | beam.Map(repr)
-          | beam.io.WriteToText(
-              "py-query-result", file_name_suffix='.json', num_shards=1))
-      output | nexmark_util.CountAndLog()  # pylint: disable=expression-not-assigned
-      # end of results output
+      output | 'result_monitor' >> beam.ParDo(result_monitor.doFn)  # pylint: disable=expression-not-assigned
+
       result = self.pipeline.run()
       job_duration = (
           self.pipeline_options.view_as(TestOptions).wait_until_finish_duration)
@@ -237,6 +240,39 @@ class NexmarkLauncher(object):
       query_errors.append(str(exc))
       raise
 
+  @staticmethod
+  def get_performance(result, event_monitor, result_monitor):
+    event_count = nexmark_util.get_counter_metric(
+        result,
+        event_monitor.namespace,
+        event_monitor.name_prefix + MonitorSuffix.ELEMENT_COUNTER)
+    event_start = nexmark_util.get_start_time_metric(
+        result,
+        event_monitor.namespace,
+        event_monitor.name_prefix + MonitorSuffix.EVENT_TIME)
+    event_end = nexmark_util.get_end_time_metric(
+        result,
+        event_monitor.namespace,
+        event_monitor.name_prefix + MonitorSuffix.EVENT_TIME)
+    result_count = nexmark_util.get_counter_metric(
+        result,
+        result_monitor.namespace,
+        result_monitor.name_prefix + MonitorSuffix.ELEMENT_COUNTER)
+    result_end = nexmark_util.get_end_time_metric(
+        result,
+        result_monitor.namespace,
+        result_monitor.name_prefix + MonitorSuffix.EVENT_TIME)
+
+    effective_end = max(event_end, result_end)
+    runtime_sec = (effective_end - event_start) / 1000
+    event_per_sec = event_count / runtime_sec
+    logging.info(
+        'input event count: %d, output event count: %d' %
+        (event_count, result_count))
+    logging.info(
+        'query run took %.1f seconds and processed %.1f events per second' %
+        (runtime_sec, event_per_sec))
+
   def cleanup(self):
     publish_client = pubsub.Client(project=self.project)
     topic = publish_client.topic(self.topic_name)
diff --git a/sdks/python/apache_beam/testing/benchmarks/nexmark/nexmark_util.py b/sdks/python/apache_beam/testing/benchmarks/nexmark/nexmark_util.py
index 85634a7ec7d..3e1a93b7cda 100644
--- a/sdks/python/apache_beam/testing/benchmarks/nexmark/nexmark_util.py
+++ b/sdks/python/apache_beam/testing/benchmarks/nexmark/nexmark_util.py
@@ -42,6 +42,8 @@ import logging
 import threading
 
 import apache_beam as beam
+from apache_beam.metrics import MetricsFilter
+from apache_beam.runners.runner import PipelineResult  # pylint: disable=unused-import
 from apache_beam.testing.benchmarks.nexmark.models import auction_bid
 from apache_beam.testing.benchmarks.nexmark.models import nexmark_model
 from apache_beam.testing.benchmarks.nexmark.models.field_name import FieldNames
@@ -221,5 +223,34 @@ def unnest_to_json(cand):
 
 
 def millis_to_timestamp(millis):
+  # type: (int) -> Timestamp
   micro_second = millis * 1000
   return Timestamp(micros=micro_second)
+
+
+def get_counter_metric(result, namespace, name):
+  # type: (PipelineResult, str, str) -> int
+  metrics = result.metrics().query(
+      MetricsFilter().with_namespace(namespace).with_name(name))
+  counters = metrics['counters']
+  if len(counters) > 1:
+    raise RuntimeError(
+        '%d instead of one metric result matches name: %s in namespace %s' %
+        (len(counters), name, namespace))
+  return counters[0].result if len(counters) > 0 else -1
+
+
+def get_start_time_metric(result, namespace, name):
+  # type: (PipelineResult, str, str) -> int
+  distributions = result.metrics().query(
+      MetricsFilter().with_namespace(namespace).with_name(
+          name))['distributions']
+  return min(map(lambda m: m.result.min, distributions))
+
+
+def get_end_time_metric(result, namespace, name):
+  # type: (PipelineResult, str, str) -> int
+  distributions = result.metrics().query(
+      MetricsFilter().with_namespace(namespace).with_name(
+          name))['distributions']
+  return max(map(lambda m: m.result.max, distributions))
