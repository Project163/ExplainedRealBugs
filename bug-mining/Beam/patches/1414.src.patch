diff --git a/sdks/python/apache_beam/io/gcp/bigquery.py b/sdks/python/apache_beam/io/gcp/bigquery.py
index 1264ad3e01a..d3ac5ca6da4 100644
--- a/sdks/python/apache_beam/io/gcp/bigquery.py
+++ b/sdks/python/apache_beam/io/gcp/bigquery.py
@@ -236,6 +236,7 @@ import itertools
 import json
 import logging
 import time
+import uuid
 from builtins import object
 from builtins import zip
 
@@ -820,8 +821,8 @@ class BigQueryWriteFn(DoFn):
 
     destination = bigquery_tools.get_hashable_destination(destination)
 
-    row = element[1]
-    self._rows_buffer[destination].append(row)
+    row_and_insert_id = element[1]
+    self._rows_buffer[destination].append(row_and_insert_id)
     self._total_buffered_rows += 1
     if len(self._rows_buffer[destination]) >= self._max_batch_size:
       return self._flush_batch(destination)
@@ -842,7 +843,7 @@ class BigQueryWriteFn(DoFn):
   def _flush_batch(self, destination):
 
     # Flush the current batch of rows to BigQuery.
-    rows = self._rows_buffer[destination]
+    rows_and_insert_ids = self._rows_buffer[destination]
     table_reference = bigquery_tools.parse_table_reference(destination)
 
     if table_reference.projectId is None:
@@ -850,15 +851,18 @@ class BigQueryWriteFn(DoFn):
           'project', str, '')
 
     logging.debug('Flushing data to %s. Total %s rows.',
-                  destination, len(rows))
+                  destination, len(rows_and_insert_ids))
+
+    rows = [r[0] for r in rows_and_insert_ids]
+    insert_ids = [r[1] for r in rows_and_insert_ids]
 
     while True:
-      # TODO: Figure out an insertId to make calls idempotent.
       passed, errors = self.bigquery_wrapper.insert_rows(
           project_id=table_reference.projectId,
           dataset_id=table_reference.datasetId,
           table_id=table_reference.tableId,
           rows=rows,
+          insert_ids=insert_ids,
           skip_invalid_rows=True)
 
       logging.debug("Passed: %s. Errors are %s", passed, errors)
@@ -885,6 +889,68 @@ class BigQueryWriteFn(DoFn):
                                     (destination, row))) for row in failed_rows]
 
 
+class _StreamToBigQuery(PTransform):
+
+  def __init__(self,
+               table_reference,
+               table_side_inputs,
+               schema_side_inputs,
+               schema,
+               batch_size,
+               create_disposition,
+               write_disposition,
+               kms_key,
+               retry_strategy,
+               additional_bq_parameters,
+               test_client=None):
+    self.table_reference = table_reference
+    self.table_side_inputs = table_side_inputs
+    self.schema_side_inputs = schema_side_inputs
+    self.schema = schema
+    self.batch_size = batch_size
+    self.create_disposition = create_disposition
+    self.write_disposition = write_disposition
+    self.kms_key = kms_key
+    self.retry_strategy = retry_strategy
+    self.test_client = test_client
+    self.additional_bq_parameters = additional_bq_parameters
+
+  class InsertIdPrefixFn(DoFn):
+
+    def start_bundle(self):
+      self.prefix = str(uuid.uuid4())
+      self._row_count = 0
+
+    def process(self, element):
+      key = element[0]
+      value = element[1]
+
+      insert_id = '%s-%s' % (self.prefix, self._row_count)
+      self._row_count += 1
+      yield (key, (value, insert_id))
+
+  def expand(self, input):
+    bigquery_write_fn = BigQueryWriteFn(
+        schema=self.schema,
+        batch_size=self.batch_size,
+        create_disposition=self.create_disposition,
+        write_disposition=self.write_disposition,
+        kms_key=self.kms_key,
+        retry_strategy=self.retry_strategy,
+        test_client=self.test_client,
+        additional_bq_parameters=self.additional_bq_parameters)
+
+    return (input
+            | 'AppendDestination' >> beam.ParDo(
+                bigquery_tools.AppendDestinationsFn(self.table_reference),
+                *self.table_side_inputs)
+            | 'AddInsertIds' >> beam.ParDo(_StreamToBigQuery.InsertIdPrefixFn())
+            | 'CommitInsertIds' >> beam.Reshuffle()
+            | 'StreamInsertRows' >> ParDo(
+                bigquery_write_fn, *self.schema_side_inputs).with_outputs(
+                    BigQueryWriteFn.FAILED_ROWS, main='main'))
+
+
 # Flag to be passed to WriteToBigQuery to force schema autodetection
 SCHEMA_AUTODETECT = 'SCHEMA_AUTODETECT'
 
@@ -1157,23 +1223,18 @@ bigquery_v2_messages.TableSchema):
       if self.triggering_frequency:
         raise ValueError('triggering_frequency can only be used with '
                          'FILE_LOADS method of writing to BigQuery.')
-      bigquery_write_fn = BigQueryWriteFn(
-          schema=self.schema,
-          batch_size=self.batch_size,
-          create_disposition=self.create_disposition,
-          write_disposition=self.write_disposition,
-          kms_key=self.kms_key,
-          retry_strategy=self.insert_retry_strategy,
-          test_client=self.test_client,
-          additional_bq_parameters=self.additional_bq_parameters)
-
-      outputs = (pcoll
-                 | 'AppendDestination' >> beam.ParDo(
-                     bigquery_tools.AppendDestinationsFn(self.table_reference),
-                     *self.table_side_inputs)
-                 | 'StreamInsertRows' >> ParDo(
-                     bigquery_write_fn, *self.schema_side_inputs).with_outputs(
-                         BigQueryWriteFn.FAILED_ROWS, main='main'))
+
+      outputs = pcoll | _StreamToBigQuery(self.table_reference,
+                                          self.table_side_inputs,
+                                          self.schema_side_inputs,
+                                          self.schema,
+                                          self.batch_size,
+                                          self.create_disposition,
+                                          self.write_disposition,
+                                          self.kms_key,
+                                          self.insert_retry_strategy,
+                                          self.additional_bq_parameters,
+                                          test_client=self.test_client)
 
       return {BigQueryWriteFn.FAILED_ROWS: outputs[BigQueryWriteFn.FAILED_ROWS]}
     else:
diff --git a/sdks/python/apache_beam/io/gcp/bigquery_test.py b/sdks/python/apache_beam/io/gcp/bigquery_test.py
index f53121f6342..153635cf096 100644
--- a/sdks/python/apache_beam/io/gcp/bigquery_test.py
+++ b/sdks/python/apache_beam/io/gcp/bigquery_test.py
@@ -21,6 +21,7 @@ from __future__ import absolute_import
 import decimal
 import json
 import logging
+import os
 import random
 import re
 import time
@@ -38,6 +39,7 @@ from apache_beam.internal.gcp.json_value import to_json_value
 from apache_beam.io.gcp import bigquery_tools
 from apache_beam.io.gcp.bigquery import TableRowJsonCoder
 from apache_beam.io.gcp.bigquery import WriteToBigQuery
+from apache_beam.io.gcp.bigquery import _StreamToBigQuery
 from apache_beam.io.gcp.bigquery_file_loads_test import _ELEMENTS
 from apache_beam.io.gcp.bigquery_tools import JSON_COMPLIANCE_ERROR
 from apache_beam.io.gcp.internal.clients import bigquery
@@ -298,6 +300,19 @@ class TestBigQuerySink(unittest.TestCase):
 @unittest.skipIf(HttpError is None, 'GCP dependencies are not installed')
 class TestWriteToBigQuery(unittest.TestCase):
 
+  def _cleanup_files(self):
+    if os.path.exists('insert_calls1'):
+      os.remove('insert_calls1')
+
+    if os.path.exists('insert_calls2'):
+      os.remove('insert_calls2')
+
+  def setUp(self):
+    self._cleanup_files()
+
+  def tearDown(self):
+    self._cleanup_files()
+
   def test_noop_schema_parsing(self):
     expected_table_schema = None
     table_schema = beam.io.gcp.bigquery.BigQueryWriteFn.get_table_schema(
@@ -427,8 +442,8 @@ class BigQueryStreamingInsertTransformTests(unittest.TestCase):
         test_client=client)
 
     fn.start_bundle()
-    fn.process(('project_id:dataset_id.table_id', {'month': 1}))
-    fn.process(('project_id:dataset_id.table_id', {'month': 2}))
+    fn.process(('project_id:dataset_id.table_id', ({'month': 1}, 'insertid1')))
+    fn.process(('project_id:dataset_id.table_id', ({'month': 2}, 'insertid2')))
     # InsertRows called as batch size is hit
     self.assertTrue(client.tabledata.InsertAll.called)
 
@@ -453,7 +468,7 @@ class BigQueryStreamingInsertTransformTests(unittest.TestCase):
 
     # Destination is a tuple of (destination, schema) to ensure the table is
     # created.
-    fn.process(('project_id:dataset_id.table_id', {'month': 1}))
+    fn.process(('project_id:dataset_id.table_id', ({'month': 1}, 'insertid3')))
 
     self.assertTrue(client.tables.Get.called)
     # InsertRows not called as batch size is not hit
@@ -488,6 +503,47 @@ class BigQueryStreamingInsertTransformTests(unittest.TestCase):
     # InsertRows not called in finish bundle as no records
     self.assertFalse(client.tabledata.InsertAll.called)
 
+  def test_failure_has_same_insert_ids(self):
+
+    def store_callback(arg):
+      insert_ids = [r.insertId for r in arg.tableDataInsertAllRequest.rows]
+      colA_values = [r.json.additionalProperties[0].value.string_value
+                     for r in arg.tableDataInsertAllRequest.rows]
+      json_output = {'insertIds': insert_ids,
+                     'colA_values': colA_values}
+      # The first time we try to insert, we save those insertions in
+      # file insert_calls1.
+      if not os.path.exists('insert_calls1'):
+        json.dump(json_output, open('insert_calls1', 'w'))
+        raise Exception()
+      else:
+        json.dump(json_output, open('insert_calls2', 'w'))
+
+      res = mock.Mock()
+      res.insertErrors = []
+      return res
+
+    client = mock.Mock()
+    client.tabledata.InsertAll = mock.Mock(side_effect=store_callback)
+
+    with beam.Pipeline(runner='BundleBasedDirectRunner') as p:
+      _ = (p
+           | beam.Create([{'columnA':'value1', 'columnB':'value2'},
+                          {'columnA':'value3', 'columnB':'value4'},
+                          {'columnA':'value5', 'columnB':'value6'}])
+           | _StreamToBigQuery(
+               'project:dataset.table',
+               [], [],
+               'anyschema',
+               None,
+               'CREATE_NEVER', None,
+               None, None,
+               [], test_client=client))
+
+    self.assertEqual(
+        json.load(open('insert_calls1')),
+        json.load(open('insert_calls2')))
+
 
 class BigQueryStreamingInsertTransformIntegrationTests(unittest.TestCase):
   BIG_QUERY_DATASET_ID = 'python_bq_streaming_inserts_'
diff --git a/sdks/python/apache_beam/io/gcp/bigquery_tools.py b/sdks/python/apache_beam/io/gcp/bigquery_tools.py
index 9f30d5f755b..064970319ec 100644
--- a/sdks/python/apache_beam/io/gcp/bigquery_tools.py
+++ b/sdks/python/apache_beam/io/gcp/bigquery_tools.py
@@ -722,7 +722,7 @@ class BigQueryWrapper(object):
         break
       page_token = response.pageToken
 
-  def insert_rows(self, project_id, dataset_id, table_id, rows,
+  def insert_rows(self, project_id, dataset_id, table_id, rows, insert_ids=None,
                   skip_invalid_rows=False):
     """Inserts rows into the specified table.
 
@@ -747,25 +747,28 @@ class BigQueryWrapper(object):
     # can happen during retries on failures.
     # TODO(silviuc): Must add support to writing TableRow's instead of dicts.
     final_rows = []
-    for row in rows:
-      json_object = bigquery.JsonObject()
-      for k, v in iteritems(row):
-        if isinstance(v, decimal.Decimal):
-          # decimal values are converted into string because JSON does not
-          # support the precision that decimal supports. BQ is able to handle
-          # inserts into NUMERIC columns by receiving JSON with string attrs.
-          v = str(v)
-        json_object.additionalProperties.append(
-            bigquery.JsonObject.AdditionalProperty(
-                key=k, value=to_json_value(v)))
-      final_rows.append(
-          bigquery.TableDataInsertAllRequest.RowsValueListEntry(
-              insertId=str(self.unique_row_id),
-              json=json_object))
+    for i, row in enumerate(rows):
+      json_row = self._convert_to_json_row(row)
+      insert_id = str(self.unique_row_id) if not insert_ids else insert_ids[i]
+      final_rows.append(bigquery.TableDataInsertAllRequest.RowsValueListEntry(
+          insertId=insert_id, json=json_row))
     result, errors = self._insert_all_rows(
         project_id, dataset_id, table_id, final_rows, skip_invalid_rows)
     return result, errors
 
+  def _convert_to_json_row(self, row):
+    json_object = bigquery.JsonObject()
+    for k, v in iteritems(row):
+      if isinstance(v, decimal.Decimal):
+        # decimal values are converted into string because JSON does not
+        # support the precision that decimal supports. BQ is able to handle
+        # inserts into NUMERIC columns by receiving JSON with string attrs.
+        v = str(v)
+      json_object.additionalProperties.append(
+          bigquery.JsonObject.AdditionalProperty(
+              key=k, value=to_json_value(v)))
+    return json_object
+
   def _convert_cell_value_to_dict(self, value, field):
     if field.type == 'STRING':
       # Input: "XYZ" --> Output: "XYZ"
