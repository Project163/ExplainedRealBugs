diff --git a/runners/spark/src/main/java/org/apache/beam/runners/spark/SparkRunner.java b/runners/spark/src/main/java/org/apache/beam/runners/spark/SparkRunner.java
index 83ff4e56b2b..c4f17f8f140 100644
--- a/runners/spark/src/main/java/org/apache/beam/runners/spark/SparkRunner.java
+++ b/runners/spark/src/main/java/org/apache/beam/runners/spark/SparkRunner.java
@@ -21,6 +21,7 @@ import static org.apache.beam.runners.core.construction.PipelineResources.detect
 import static org.apache.beam.runners.spark.SparkPipelineOptions.prepareFilesToStage;
 
 import java.util.Collection;
+import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.concurrent.ExecutorService;
@@ -334,8 +335,12 @@ public final class SparkRunner extends PipelineRunner<SparkPipelineResult> {
       // we populate cache candidates by updating the map with inputs of each node.
       // The goal is to detect the PCollections accessed more than one time, and so enable cache
       // on the underlying RDDs or DStreams.
+      Map<TupleTag<?>, PValue> inputs = new HashMap<>(node.getInputs());
+      for (TupleTag<?> tupleTag : node.getTransform().getAdditionalInputs().keySet()) {
+        inputs.remove(tupleTag);
+      }
 
-      for (PValue value : node.getInputs().values()) {
+      for (PValue value : inputs.values()) {
         if (value instanceof PCollection) {
           long count = 1L;
           if (ctxt.getCacheCandidates().get(value) != null) {
diff --git a/runners/spark/src/test/java/org/apache/beam/runners/spark/CacheTest.java b/runners/spark/src/test/java/org/apache/beam/runners/spark/CacheTest.java
index 880a466db74..eb9dcd9f164 100644
--- a/runners/spark/src/test/java/org/apache/beam/runners/spark/CacheTest.java
+++ b/runners/spark/src/test/java/org/apache/beam/runners/spark/CacheTest.java
@@ -22,6 +22,7 @@ import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertTrue;
 import static org.mockito.Mockito.mock;
 
+import java.util.List;
 import org.apache.beam.runners.spark.translation.Dataset;
 import org.apache.beam.runners.spark.translation.EvaluationContext;
 import org.apache.beam.runners.spark.translation.SparkContextFactory;
@@ -32,8 +33,12 @@ import org.apache.beam.sdk.options.PipelineOptionsFactory;
 import org.apache.beam.sdk.transforms.Count;
 import org.apache.beam.sdk.transforms.Create;
 import org.apache.beam.sdk.transforms.Create.Values;
+import org.apache.beam.sdk.transforms.DoFn;
 import org.apache.beam.sdk.transforms.GroupByKey;
+import org.apache.beam.sdk.transforms.ParDo;
+import org.apache.beam.sdk.transforms.View;
 import org.apache.beam.sdk.values.PCollection;
+import org.apache.beam.sdk.values.PCollectionView;
 import org.apache.spark.api.java.JavaSparkContext;
 import org.junit.Test;
 
@@ -49,12 +54,28 @@ public class CacheTest {
     SparkPipelineOptions options = createOptions();
     Pipeline pipeline = Pipeline.create(options);
     PCollection<String> pCollection = pipeline.apply(Create.of("foo", "bar"));
-    // first read
-    pCollection.apply(Count.globally());
-    // second read
-    // as we access the same PCollection two times, the Spark runner does optimization and so
-    // will cache the RDD representing this PCollection
+
+    // First use of pCollection.
     pCollection.apply(Count.globally());
+    // Second use of pCollection.
+    PCollectionView<List<String>> view = pCollection.apply(View.asList());
+
+    // Internally View.asList() creates a PCollection that underlies the PCollectionView, that
+    // PCollection should not be cached as the SparkRunner does not access that PCollection to
+    // access the PCollectionView.
+    pipeline
+        .apply(Create.of("foo", "baz"))
+        .apply(
+            ParDo.of(
+                    new DoFn<String, String>() {
+                      @ProcessElement
+                      public void processElement(ProcessContext processContext) {
+                        if (processContext.sideInput(view).contains(processContext.element())) {
+                          processContext.output(processContext.element());
+                        }
+                      }
+                    })
+                .withSideInputs(view));
 
     JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);
     EvaluationContext ctxt = new EvaluationContext(jsc, pipeline, options);
@@ -62,6 +83,7 @@ public class CacheTest {
         new SparkRunner.CacheVisitor(new TransformTranslator.Translator(), ctxt);
     pipeline.traverseTopologically(cacheVisitor);
     assertEquals(2L, (long) ctxt.getCacheCandidates().get(pCollection));
+    assertEquals(1L, ctxt.getCacheCandidates().values().stream().filter(l -> l > 1).count());
   }
 
   @Test
