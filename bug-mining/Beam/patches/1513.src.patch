diff --git a/sdks/python/apache_beam/runners/common.py b/sdks/python/apache_beam/runners/common.py
index 8632cfdcf5d..c045231473a 100644
--- a/sdks/python/apache_beam/runners/common.py
+++ b/sdks/python/apache_beam/runners/common.py
@@ -50,13 +50,14 @@ from apache_beam.utils.windowed_value import WindowedValue
 class NameContext(object):
   """Holds the name information for a step."""
 
-  def __init__(self, step_name):
+  def __init__(self, step_name, transform_id=None):
     """Creates a new step NameContext.
 
     Args:
       step_name: The name of the step.
     """
     self.step_name = step_name
+    self.transform_id = transform_id
 
   def __eq__(self, other):
     return self.step_name == other.step_name
diff --git a/sdks/python/apache_beam/runners/worker/bundle_processor.py b/sdks/python/apache_beam/runners/worker/bundle_processor.py
index c62f194fab0..fd2528d2600 100644
--- a/sdks/python/apache_beam/runners/worker/bundle_processor.py
+++ b/sdks/python/apache_beam/runners/worker/bundle_processor.py
@@ -978,7 +978,7 @@ def create(factory, transform_id, transform_proto, grpc_port, consumers):
         transform_id)
     output_coder = factory.get_only_output_coder(transform_proto)
   return DataInputOperation(
-      transform_proto.unique_name,
+      common.NameContext(transform_proto.unique_name, transform_id),
       transform_proto.unique_name,
       consumers,
       factory.counter_factory,
@@ -1000,7 +1000,7 @@ def create(factory, transform_id, transform_proto, grpc_port, consumers):
         transform_id)
     output_coder = factory.get_only_input_coder(transform_proto)
   return DataOutputOperation(
-      transform_proto.unique_name,
+      common.NameContext(transform_proto.unique_name, transform_id),
       transform_proto.unique_name,
       consumers,
       factory.counter_factory,
@@ -1019,7 +1019,7 @@ def create(factory, transform_id, transform_proto, parameter, consumers):
       [factory.get_only_output_coder(transform_proto)])
   return factory.augment_oldstyle_op(
       operations.ReadOperation(
-          transform_proto.unique_name,
+          common.NameContext(transform_proto.unique_name, transform_id),
           spec,
           factory.counter_factory,
           factory.state_sampler),
@@ -1036,7 +1036,7 @@ def create(factory, transform_id, transform_proto, parameter, consumers):
       [WindowedValueCoder(source.default_output_coder())])
   return factory.augment_oldstyle_op(
       operations.ReadOperation(
-          transform_proto.unique_name,
+          common.NameContext(transform_proto.unique_name, transform_id),
           spec,
           factory.counter_factory,
           factory.state_sampler),
@@ -1048,7 +1048,7 @@ def create(factory, transform_id, transform_proto, parameter, consumers):
     python_urns.IMPULSE_READ_TRANSFORM, beam_runner_api_pb2.ReadPayload)
 def create(factory, transform_id, transform_proto, parameter, consumers):
   return operations.ImpulseReadOperation(
-      transform_proto.unique_name,
+      common.NameContext(transform_proto.unique_name, transform_id),
       factory.counter_factory,
       factory.state_sampler,
       consumers,
@@ -1227,7 +1227,7 @@ def _create_pardo_operation(
 
   result = factory.augment_oldstyle_op(
       operation_cls(
-          transform_proto.unique_name,
+          common.NameContext(transform_proto.unique_name, transform_id),
           spec,
           factory.counter_factory,
           factory.state_sampler,
@@ -1276,7 +1276,7 @@ def create(factory, transform_id, transform_proto, parameter, consumers):
 def create(factory, transform_id, transform_proto, unused_parameter, consumers):
   return factory.augment_oldstyle_op(
       operations.FlattenOperation(
-          transform_proto.unique_name,
+          common.NameContext(transform_proto.unique_name, transform_id),
           operation_specs.WorkerFlatten(
               None, [factory.get_only_output_coder(transform_proto)]),
           factory.counter_factory,
@@ -1294,7 +1294,7 @@ def create(factory, transform_id, transform_proto, payload, consumers):
        [], {}))
   return factory.augment_oldstyle_op(
       operations.PGBKCVOperation(
-          transform_proto.unique_name,
+          common.NameContext(transform_proto.unique_name, transform_id),
           operation_specs.WorkerPartialGroupByKey(
               serialized_combine_fn,
               None,
@@ -1310,7 +1310,7 @@ def create(factory, transform_id, transform_proto, payload, consumers):
     beam_runner_api_pb2.CombinePayload)
 def create(factory, transform_id, transform_proto, payload, consumers):
   return _create_combine_phase_operation(
-      factory, transform_proto, payload, consumers, 'merge')
+      factory, transform_id, transform_proto, payload, consumers, 'merge')
 
 
 @BeamTransformFactory.register_urn(
@@ -1318,7 +1318,7 @@ def create(factory, transform_id, transform_proto, payload, consumers):
     beam_runner_api_pb2.CombinePayload)
 def create(factory, transform_id, transform_proto, payload, consumers):
   return _create_combine_phase_operation(
-      factory, transform_proto, payload, consumers, 'extract')
+      factory, transform_id, transform_proto, payload, consumers, 'extract')
 
 
 @BeamTransformFactory.register_urn(
@@ -1326,17 +1326,17 @@ def create(factory, transform_id, transform_proto, payload, consumers):
     beam_runner_api_pb2.CombinePayload)
 def create(factory, transform_id, transform_proto, payload, consumers):
   return _create_combine_phase_operation(
-      factory, transform_proto, payload, consumers, 'all')
+      factory, transform_id, transform_proto, payload, consumers, 'all')
 
 
 def _create_combine_phase_operation(
-    factory, transform_proto, payload, consumers, phase):
+    factory, transform_id, transform_proto, payload, consumers, phase):
   serialized_combine_fn = pickler.dumps(
       (beam.CombineFn.from_runner_api(payload.combine_fn, factory.context),
        [], {}))
   return factory.augment_oldstyle_op(
       operations.CombineOperation(
-          transform_proto.unique_name,
+          common.NameContext(transform_proto.unique_name, transform_id),
           operation_specs.WorkerCombineFn(
               serialized_combine_fn,
               phase,
@@ -1352,7 +1352,7 @@ def _create_combine_phase_operation(
 def create(factory, transform_id, transform_proto, unused_parameter, consumers):
   return factory.augment_oldstyle_op(
       operations.FlattenOperation(
-          transform_proto.unique_name,
+          common.NameContext(transform_proto.unique_name, transform_id),
           operation_specs.WorkerFlatten(
               None,
               [factory.get_only_output_coder(transform_proto)]),
diff --git a/sdks/python/apache_beam/runners/worker/log_handler.py b/sdks/python/apache_beam/runners/worker/log_handler.py
index 08dac3a8923..12f162b8220 100644
--- a/sdks/python/apache_beam/runners/worker/log_handler.py
+++ b/sdks/python/apache_beam/runners/worker/log_handler.py
@@ -25,12 +25,13 @@ import queue
 import sys
 import threading
 import time
-from builtins import range
+import traceback
 
 import grpc
 
 from apache_beam.portability.api import beam_fn_api_pb2
 from apache_beam.portability.api import beam_fn_api_pb2_grpc
+from apache_beam.runners.worker import statesampler
 from apache_beam.runners.worker.channel_factory import GRPCChannelFactory
 from apache_beam.runners.worker.worker_id_interceptor import WorkerIdInterceptor
 
@@ -54,7 +55,8 @@ class FnApiLogRecordHandler(logging.Handler):
       logging.ERROR: beam_fn_api_pb2.LogEntry.Severity.ERROR,
       logging.WARNING: beam_fn_api_pb2.LogEntry.Severity.WARN,
       logging.INFO: beam_fn_api_pb2.LogEntry.Severity.INFO,
-      logging.DEBUG: beam_fn_api_pb2.LogEntry.Severity.DEBUG
+      logging.DEBUG: beam_fn_api_pb2.LogEntry.Severity.DEBUG,
+      -float('inf'): beam_fn_api_pb2.LogEntry.Severity.DEBUG,
   }
 
   def __init__(self, log_service_descriptor):
@@ -81,16 +83,37 @@ class FnApiLogRecordHandler(logging.Handler):
         self._log_channel)
     return self._logging_stub.Logging(self._write_log_entries())
 
+  def map_log_level(self, level):
+    try:
+      return self.LOG_LEVEL_MAP[level]
+    except KeyError:
+      return max(
+          beam_level for python_level, beam_level in self.LOG_LEVEL_MAP.items()
+          if python_level <= level)
+
   def emit(self, record):
     log_entry = beam_fn_api_pb2.LogEntry()
-    log_entry.severity = self.LOG_LEVEL_MAP[record.levelno]
+    log_entry.severity = self.map_log_level(record.levelno)
     log_entry.message = self.format(record)
     log_entry.thread = record.threadName
-    log_entry.log_location = record.module + '.' + record.funcName
+    log_entry.log_location = '%s:%s' % (
+        record.pathname or record.module, record.lineno or record.funcName)
     (fraction, seconds) = math.modf(record.created)
     nanoseconds = 1e9 * fraction
     log_entry.timestamp.seconds = int(seconds)
     log_entry.timestamp.nanos = int(nanoseconds)
+    if record.exc_info:
+      log_entry.trace = ''.join(traceback.format_exception(*record.exc_info))
+    instruction_id = statesampler.get_current_instruction_id()
+    if instruction_id:
+      log_entry.instruction_id = instruction_id
+    tracker = statesampler.get_current_tracker()
+    if tracker:
+      current_state = tracker.current_state()
+      if (current_state
+          and current_state.name_context
+          and current_state.name_context.transform_id):
+        log_entry.transform_id = current_state.name_context.transform_id
 
     try:
       self._log_entry_queue.put(log_entry, block=False)
diff --git a/sdks/python/apache_beam/runners/worker/log_handler_test.py b/sdks/python/apache_beam/runners/worker/log_handler_test.py
index a651409ddcd..c79ccf9c672 100644
--- a/sdks/python/apache_beam/runners/worker/log_handler_test.py
+++ b/sdks/python/apache_beam/runners/worker/log_handler_test.py
@@ -18,6 +18,7 @@
 from __future__ import absolute_import
 
 import logging
+import re
 import unittest
 from builtins import range
 
@@ -26,7 +27,9 @@ import grpc
 from apache_beam.portability.api import beam_fn_api_pb2
 from apache_beam.portability.api import beam_fn_api_pb2_grpc
 from apache_beam.portability.api import endpoints_pb2
+from apache_beam.runners.common import NameContext
 from apache_beam.runners.worker import log_handler
+from apache_beam.runners.worker import statesampler
 from apache_beam.utils.thread_pool_executor import UnboundedThreadPoolExecutor
 
 _LOGGER = logging.getLogger(__name__)
@@ -83,14 +86,58 @@ class FnApiLogRecordHandlerTest(unittest.TestCase):
                          log_entry.severity)
         self.assertEqual('%s: %s' % (msg, num_received_log_entries),
                          log_entry.message)
-        self.assertEqual(u'log_handler_test._verify_fn_log_handler',
-                         log_entry.log_location)
+        self.assertTrue(
+            re.match(r'.*/log_handler_test.py:\d+', log_entry.log_location),
+            log_entry.log_location)
         self.assertGreater(log_entry.timestamp.seconds, 0)
         self.assertGreaterEqual(log_entry.timestamp.nanos, 0)
         num_received_log_entries += 1
 
     self.assertEqual(num_received_log_entries, num_log_entries)
 
+  def assertContains(self, haystack, needle):
+    self.assertTrue(
+        needle in haystack, 'Expected %r to contain %r.' % (haystack, needle))
+
+  def test_exc_info(self):
+    try:
+      raise ValueError('some message')
+    except ValueError:
+      _LOGGER.error('some error', exc_info=True)
+
+    self.fn_log_handler.close()
+
+    log_entry = self.test_logging_service.log_records_received[0].log_entries[0]
+    self.assertContains(log_entry.message, 'some error')
+    self.assertContains(log_entry.trace, 'some message')
+    self.assertContains(log_entry.trace, 'log_handler_test.py')
+
+  def test_context(self):
+    try:
+      with statesampler.instruction_id('A'):
+        tracker = statesampler.for_test()
+        with tracker.scoped_state(NameContext('name', 'tid'), 'stage'):
+          _LOGGER.info('message a')
+      with statesampler.instruction_id('B'):
+        _LOGGER.info('message b')
+      _LOGGER.info('message c')
+
+      self.fn_log_handler.close()
+      a, b, c = sum(
+          [list(logs.log_entries)
+           for logs in self.test_logging_service.log_records_received], [])
+
+      self.assertEqual(a.instruction_id, 'A')
+      self.assertEqual(b.instruction_id, 'B')
+      self.assertEqual(c.instruction_id, '')
+
+      self.assertEqual(a.transform_id, 'tid')
+      self.assertEqual(b.transform_id, '')
+      self.assertEqual(c.transform_id, '')
+
+    finally:
+      statesampler.set_current_tracker(None)
+
 
 # Test cases.
 data = {
diff --git a/sdks/python/apache_beam/runners/worker/sdk_worker.py b/sdks/python/apache_beam/runners/worker/sdk_worker.py
index e0534ffb22b..00a0ac2e575 100644
--- a/sdks/python/apache_beam/runners/worker/sdk_worker.py
+++ b/sdks/python/apache_beam/runners/worker/sdk_worker.py
@@ -39,6 +39,7 @@ from apache_beam.portability.api import beam_fn_api_pb2
 from apache_beam.portability.api import beam_fn_api_pb2_grpc
 from apache_beam.runners.worker import bundle_processor
 from apache_beam.runners.worker import data_plane
+from apache_beam.runners.worker import statesampler
 from apache_beam.runners.worker.channel_factory import GRPCChannelFactory
 from apache_beam.runners.worker.statecache import StateCache
 from apache_beam.runners.worker.worker_id_interceptor import WorkerIdInterceptor
@@ -132,17 +133,18 @@ class SdkHarness(object):
     _LOGGER.info('Done consuming work.')
 
   def _execute(self, task, request):
-    try:
-      response = task()
-    except Exception:  # pylint: disable=broad-except
-      traceback_string = traceback.format_exc()
-      print(traceback_string, file=sys.stderr)
-      _LOGGER.error(
-          'Error processing instruction %s. Original traceback is\n%s\n',
-          request.instruction_id, traceback_string)
-      response = beam_fn_api_pb2.InstructionResponse(
-          instruction_id=request.instruction_id, error=traceback_string)
-    self._responses.put(response)
+    with statesampler.instruction_id(request.instruction_id):
+      try:
+        response = task()
+      except Exception:  # pylint: disable=broad-except
+        traceback_string = traceback.format_exc()
+        print(traceback_string, file=sys.stderr)
+        _LOGGER.error(
+            'Error processing instruction %s. Original traceback is\n%s\n',
+            request.instruction_id, traceback_string)
+        response = beam_fn_api_pb2.InstructionResponse(
+            instruction_id=request.instruction_id, error=traceback_string)
+      self._responses.put(response)
 
   def _request_register(self, request):
     # registration request is handled synchronously
diff --git a/sdks/python/apache_beam/runners/worker/statesampler.py b/sdks/python/apache_beam/runners/worker/statesampler.py
index 707ee1f5c18..e57815e34f5 100644
--- a/sdks/python/apache_beam/runners/worker/statesampler.py
+++ b/sdks/python/apache_beam/runners/worker/statesampler.py
@@ -19,6 +19,7 @@
 
 from __future__ import absolute_import
 
+import contextlib
 import threading
 from collections import namedtuple
 
@@ -49,6 +50,25 @@ def get_current_tracker():
     return None
 
 
+_INSTRUCTION_IDS = threading.local()
+
+
+def get_current_instruction_id():
+  try:
+    return _INSTRUCTION_IDS.instruction_id
+  except AttributeError:
+    return None
+
+
+@contextlib.contextmanager
+def instruction_id(id):
+  try:
+    _INSTRUCTION_IDS.instruction_id = id
+    yield
+  finally:
+    _INSTRUCTION_IDS.instruction_id = None
+
+
 def for_test():
   set_current_tracker(StateSampler('test', CounterFactory()))
   return get_current_tracker()
