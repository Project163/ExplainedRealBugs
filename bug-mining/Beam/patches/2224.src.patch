diff --git a/.test-infra/jenkins/job_LoadTests_Combine_Python.groovy b/.test-infra/jenkins/job_LoadTests_Combine_Python.groovy
index 0ea402c3454..8cd84e0379a 100644
--- a/.test-infra/jenkins/job_LoadTests_Combine_Python.groovy
+++ b/.test-infra/jenkins/job_LoadTests_Combine_Python.groovy
@@ -100,7 +100,7 @@ def loadTestConfigurations = { datasetName, mode ->
 
 def addStreamingOptions(test){
   test.pipelineOptions << [streaming: null,
-    experiments: "use_runner_v2"
+    experiments: "use_runner_v2, shuffle_mode=appliance"
   ]
 }
 
diff --git a/.test-infra/jenkins/job_LoadTests_GBK_Python.groovy b/.test-infra/jenkins/job_LoadTests_GBK_Python.groovy
index 2364ae2661f..e2b1bcbf4fd 100644
--- a/.test-infra/jenkins/job_LoadTests_GBK_Python.groovy
+++ b/.test-infra/jenkins/job_LoadTests_GBK_Python.groovy
@@ -156,7 +156,7 @@ def addStreamingOptions(test) {
     // Use the new Dataflow runner, which offers improved efficiency of Dataflow jobs.
     // See https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#dataflow-runner-v2
     // for more details.
-    experiments: 'use_runner_v2',
+    experiments: 'use_runner_v2, shuffle_mode=appliance',
   ]
 }
 
diff --git a/.test-infra/jenkins/job_LoadTests_GBK_Python_reiterate.groovy b/.test-infra/jenkins/job_LoadTests_GBK_Python_reiterate.groovy
index d1960abce17..1f40471e9a6 100644
--- a/.test-infra/jenkins/job_LoadTests_GBK_Python_reiterate.groovy
+++ b/.test-infra/jenkins/job_LoadTests_GBK_Python_reiterate.groovy
@@ -86,7 +86,7 @@ def addStreamingOptions(test) {
     // Use the new Dataflow runner, which offers improved efficiency of Dataflow jobs.
     // See https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#dataflow-runner-v2
     // for more details.
-    experiments: 'use_runner_v2',
+    experiments: 'use_runner_v2, shuffle_mode=appliance',
   ]
 }
 
diff --git a/.test-infra/jenkins/job_LoadTests_ParDo_Python.groovy b/.test-infra/jenkins/job_LoadTests_ParDo_Python.groovy
index 090361a21a5..746a553a2df 100644
--- a/.test-infra/jenkins/job_LoadTests_ParDo_Python.groovy
+++ b/.test-infra/jenkins/job_LoadTests_ParDo_Python.groovy
@@ -131,7 +131,7 @@ def addStreamingOptions(test) {
     // Use the new Dataflow runner, which offers improved efficiency of Dataflow jobs.
     // See https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#dataflow-runner-v2
     // for more details.
-    experiments: 'use_runner_v2',
+    experiments: 'use_runner_v2, shuffle_mode=appliance',
   ]
 }
 
diff --git a/.test-infra/jenkins/job_LoadTests_SideInput_Python.groovy b/.test-infra/jenkins/job_LoadTests_SideInput_Python.groovy
index 5ed7cc6381d..0b61ff0b2e5 100644
--- a/.test-infra/jenkins/job_LoadTests_SideInput_Python.groovy
+++ b/.test-infra/jenkins/job_LoadTests_SideInput_Python.groovy
@@ -39,7 +39,7 @@ def fromTemplate = { mode, name, id, datasetName, testSpecificOptions ->
       influx_measurement   : "python_${mode}_sideinput_${id}",
       num_workers          : 10,
       autoscaling_algorithm: 'NONE',
-      experiments          : 'use_runner_v2',
+      experiments          : 'use_runner_v2, shuffle_mode=appliance',
     ] << testSpecificOptions
   ]
 }
diff --git a/.test-infra/jenkins/job_PerformanceTests_KafkaIO_IT.groovy b/.test-infra/jenkins/job_PerformanceTests_KafkaIO_IT.groovy
index 0c6e6ae9ea6..2e98c79b96a 100644
--- a/.test-infra/jenkins/job_PerformanceTests_KafkaIO_IT.groovy
+++ b/.test-infra/jenkins/job_PerformanceTests_KafkaIO_IT.groovy
@@ -74,7 +74,7 @@ job(jobName) {
     kafkaTopic                   : 'beam-runnerv2',
     bigQueryTable                : 'kafkaioit_results_sdf_wrapper',
     influxMeasurement            : 'kafkaioit_results_sdf_wrapper',
-    experiments                  : 'beam_fn_api,use_runner_v2,use_unified_worker',
+    experiments                  : 'beam_fn_api,use_runner_v2,shuffle_mode=appliance,use_unified_worker',
   ]
 
   Map dataflowRunnerV2SdfPipelineOptions = pipelineOptions + [
@@ -88,7 +88,7 @@ job(jobName) {
     kafkaTopic                   : 'beam-sdf',
     bigQueryTable                : 'kafkaioit_results_runner_v2',
     influxMeasurement            : 'kafkaioit_results_runner_v2',
-    experiments                  : 'beam_fn_api,use_runner_v2,use_unified_worker,use_sdf_kafka_read',
+    experiments                  : 'beam_fn_api,use_runner_v2,shuffle_mode=appliance,use_unified_worker,use_sdf_kafka_read',
   ]
 
   steps {
diff --git a/runners/google-cloud-dataflow-java/build.gradle b/runners/google-cloud-dataflow-java/build.gradle
index e808e4d053c..67f75e44135 100644
--- a/runners/google-cloud-dataflow-java/build.gradle
+++ b/runners/google-cloud-dataflow-java/build.gradle
@@ -333,7 +333,7 @@ createCrossLanguageValidatesRunnerTask(
     "--project=${dataflowProject}",
     "--region=${dataflowRegion}",
     "--sdk_harness_container_image_overrides=.*java.*,${dockerImageContainer}:${dockerTag}",
-    "--experiments=use_runner_v2",
+    "--experiments=use_runner_v2,shuffle_mode=appliance",
   ],
   javaPipelineOptions: [
     "--runner=TestDataflowRunner",
@@ -341,7 +341,7 @@ createCrossLanguageValidatesRunnerTask(
     "--region=${dataflowRegion}",
     "--tempRoot=${dataflowValidatesTempRoot}",
     "--sdkHarnessContainerImageOverrides=.*java.*,${dockerImageContainer}:${dockerTag}",
-    "--experiments=use_runner_v2",
+    "--experiments=use_runner_v2,shuffle_mode=appliance",
   ],
   nosetestsOptions: [
     "--nocapture",
diff --git a/sdks/go/pkg/beam/runners/dataflow/dataflow.go b/sdks/go/pkg/beam/runners/dataflow/dataflow.go
index 474fb7adcdd..ae04ed6965c 100644
--- a/sdks/go/pkg/beam/runners/dataflow/dataflow.go
+++ b/sdks/go/pkg/beam/runners/dataflow/dataflow.go
@@ -140,6 +140,9 @@ func Execute(ctx context.Context, p *beam.Pipeline) (beam.PipelineResult, error)
 	if !v2set {
 		experiments = append(experiments, "use_unified_worker")
 	}
+	// BEAM-11779 use shuffle_mode=appliance with runner v2 until issue is resolved.
+	experiments = append(experiments, "shuffle_mode=appliance")
+
 	if *minCPUPlatform != "" {
 		experiments = append(experiments, fmt.Sprintf("min_cpu_platform=%v", *minCPUPlatform))
 	}
diff --git a/sdks/python/scripts/run_integration_test.sh b/sdks/python/scripts/run_integration_test.sh
index a60a141ea1a..ecda98dca8c 100755
--- a/sdks/python/scripts/run_integration_test.sh
+++ b/sdks/python/scripts/run_integration_test.sh
@@ -239,7 +239,7 @@ if [[ -z $PIPELINE_OPTS ]]; then
 
   # Add --runner_v2 if provided
   if [[ "$RUNNER_V2" = true ]]; then
-    opts+=("--experiments=use_runner_v2")
+    opts+=("--experiments=use_runner_v2,shuffle_mode=appliance")
     if [[ "$STREAMING" = true ]]; then
       # Dataflow Runner V2 only supports streaming engine.
       opts+=("--enable_streaming_engine")
