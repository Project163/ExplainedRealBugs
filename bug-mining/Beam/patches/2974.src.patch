diff --git a/sdks/java/io/kafka/build.gradle b/sdks/java/io/kafka/build.gradle
index 4db6077c115..889b6a40e89 100644
--- a/sdks/java/io/kafka/build.gradle
+++ b/sdks/java/io/kafka/build.gradle
@@ -43,6 +43,15 @@ def kafkaVersions = [
     '251': "2.5.1",
 ]
 
+def sdfKafkaVersions = [
+        '201',
+        '211',
+        '222',
+        '231',
+        '241',
+        '251'
+]
+
 kafkaVersions.each{k,v -> configurations.create("kafkaVersion$k")}
 
 dependencies {
@@ -135,7 +144,8 @@ kafkaVersions.each {kv ->
     include '**/KafkaIOIT.class'
 
     filter {
-      includeTestsMatching "*InBatch"
+      excludeTestsMatching "*InStreaming"
+      if (!(kv.key in sdfKafkaVersions)) excludeTestsMatching "*DynamicPartitions" //admin client create partitions does not exist in kafka 0.11.0.3 and kafka sdf does not appear to work for kafka versions <2.0.1
     }
   }
 }
diff --git a/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java b/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java
index 6ff07992504..e2d2369c684 100644
--- a/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java
+++ b/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java
@@ -94,7 +94,6 @@ import org.apache.beam.sdk.values.PDone;
 import org.apache.beam.sdk.values.Row;
 import org.apache.beam.sdk.values.TupleTag;
 import org.apache.beam.sdk.values.TypeDescriptor;
-import org.apache.beam.sdk.values.TypeDescriptors;
 import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;
 import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Joiner;
 import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;
@@ -190,7 +189,7 @@ import org.slf4j.LoggerFactory;
  *
  * For a given kafka bootstrap_server, KafkaIO is also able to detect and read from available {@link
  * TopicPartition} dynamically and stop reading from un. KafkaIO uses {@link
- * WatchKafkaTopicPartitionDoFn} to emit any new added {@link TopicPartition} and uses {@link
+ * WatchForKafkaTopicPartitions} to emit any new added {@link TopicPartition} and uses {@link
  * ReadFromKafkaDoFn} to read from each {@link KafkaSourceDescriptor}. Dynamic read is able to solve
  * 2 scenarios:
  *
@@ -215,22 +214,22 @@ import org.slf4j.LoggerFactory;
  *
  * When race condition happens, it will result in the stopped/removed TopicPartition failing to be
  * emitted to ReadFromKafkaDoFn again. Or ReadFromKafkaDoFn will output replicated records. The
- * major cause for such race condition is that both {@link WatchKafkaTopicPartitionDoFn} and {@link
+ * major cause for such race condition is that both {@link WatchForKafkaTopicPartitions} and {@link
  * ReadFromKafkaDoFn} react to the signal from removed/stopped {@link TopicPartition} but we cannot
  * guarantee that both DoFns perform related actions at the same time.
  *
  * <p>Here is one example for failing to emit new added {@link TopicPartition}:
  *
  * <ul>
- *   <li>A {@link WatchKafkaTopicPartitionDoFn} is configured with updating the current tracking set
+ *   <li>A {@link WatchForKafkaTopicPartitions} is configured with updating the current tracking set
  *       every 1 hour.
- *   <li>One TopicPartition A is tracked by the {@link WatchKafkaTopicPartitionDoFn} at 10:00AM and
+ *   <li>One TopicPartition A is tracked by the {@link WatchForKafkaTopicPartitions} at 10:00AM and
  *       {@link ReadFromKafkaDoFn} starts to read from TopicPartition A immediately.
- *   <li>At 10:30AM, the {@link WatchKafkaTopicPartitionDoFn} notices that the {@link
+ *   <li>At 10:30AM, the {@link WatchForKafkaTopicPartitions} notices that the {@link
  *       TopicPartition} has been stopped/removed, so it stops reading from it and returns {@code
  *       ProcessContinuation.stop()}.
  *   <li>At 10:45 the pipeline author wants to read from TopicPartition A again.
- *   <li>At 11:00AM when {@link WatchKafkaTopicPartitionDoFn} is invoked by firing timer, it doesn’t
+ *   <li>At 11:00AM when {@link WatchForKafkaTopicPartitions} is invoked by firing timer, it doesn’t
  *       know that TopicPartition A has been stopped/removed. All it knows is that TopicPartition A
  *       is still an active TopicPartition and it will not emit TopicPartition A again.
  * </ul>
@@ -241,9 +240,9 @@ import org.slf4j.LoggerFactory;
  *   <li>At 10:00AM, {@link ReadFromKafkaDoFn} is processing TopicPartition A
  *   <li>At 10:05AM, {@link ReadFromKafkaDoFn} starts to process other TopicPartitions(sdf-initiated
  *       checkpoint or runner-issued checkpoint happens)
- *   <li>At 10:10AM, {@link WatchKafkaTopicPartitionDoFn} knows that TopicPartition A is
+ *   <li>At 10:10AM, {@link WatchForKafkaTopicPartitions} knows that TopicPartition A is
  *       stopped/removed
- *   <li>At 10:15AM, {@link WatchKafkaTopicPartitionDoFn} knows that TopicPartition A is added again
+ *   <li>At 10:15AM, {@link WatchForKafkaTopicPartitions} knows that TopicPartition A is added again
  *       and emits TopicPartition A again
  *   <li>At 10:20AM, {@link ReadFromKafkaDoFn} starts to process resumed TopicPartition A but at the
  *       same time {@link ReadFromKafkaDoFn} is also processing the new emitted TopicPartitionA.
@@ -1208,7 +1207,7 @@ public class KafkaIO {
     }
 
     /**
-     * Configure the KafkaIO to use {@link WatchKafkaTopicPartitionDoFn} to detect and emit any new
+     * Configure the KafkaIO to use {@link WatchForKafkaTopicPartitions} to detect and emit any new
      * available {@link TopicPartition} for {@link ReadFromKafkaDoFn} to consume during pipeline
      * execution time. The KafkaIO will regularly check the availability based on the given
      * duration. If the duration is not specified as {@code null}, the default duration is 1 hour.
@@ -1267,7 +1266,7 @@ public class KafkaIO {
       return new TypedWithoutMetadata<>(this);
     }
 
-    PTransform<PBegin, PCollection<Row>> externalWithMetadata() {
+    public PTransform<PBegin, PCollection<Row>> externalWithMetadata() {
       return new RowsWithMetadata<>(this);
     }
 
@@ -1529,25 +1528,15 @@ public class KafkaIO {
             }
           }
           output =
-              input
-                  .getPipeline()
-                  .apply(Impulse.create())
-                  .apply(
-                      MapElements.into(
-                              TypeDescriptors.kvs(
-                                  new TypeDescriptor<byte[]>() {}, new TypeDescriptor<byte[]>() {}))
-                          .via(element -> KV.of(element, element)))
-                  .apply(
-                      ParDo.of(
-                          new WatchKafkaTopicPartitionDoFn(
-                              kafkaRead.getWatchTopicPartitionDuration(),
-                              kafkaRead.getConsumerFactoryFn(),
-                              kafkaRead.getCheckStopReadingFn(),
-                              kafkaRead.getConsumerConfig(),
-                              kafkaRead.getStartReadTime(),
-                              kafkaRead.getStopReadTime(),
-                              topics.stream().collect(Collectors.toList()))));
-
+              input.apply(
+                  new WatchForKafkaTopicPartitions(
+                      kafkaRead.getWatchTopicPartitionDuration(),
+                      kafkaRead.getConsumerFactoryFn(),
+                      kafkaRead.getConsumerConfig(),
+                      kafkaRead.getCheckStopReadingFn(),
+                      topics,
+                      kafkaRead.getStartReadTime(),
+                      kafkaRead.getStopReadTime()));
         } else {
           output =
               input
diff --git a/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/WatchForKafkaTopicPartitions.java b/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/WatchForKafkaTopicPartitions.java
new file mode 100644
index 00000000000..e5991222edf
--- /dev/null
+++ b/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/WatchForKafkaTopicPartitions.java
@@ -0,0 +1,185 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.beam.sdk.io.kafka;
+
+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.MoreObjects.firstNonNull;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.Objects;
+import java.util.Set;
+import org.apache.beam.sdk.annotations.Experimental;
+import org.apache.beam.sdk.metrics.Counter;
+import org.apache.beam.sdk.metrics.Metrics;
+import org.apache.beam.sdk.transforms.DoFn;
+import org.apache.beam.sdk.transforms.Impulse;
+import org.apache.beam.sdk.transforms.PTransform;
+import org.apache.beam.sdk.transforms.ParDo;
+import org.apache.beam.sdk.transforms.SerializableFunction;
+import org.apache.beam.sdk.transforms.Watch;
+import org.apache.beam.sdk.transforms.Watch.Growth.PollFn;
+import org.apache.beam.sdk.values.KV;
+import org.apache.beam.sdk.values.PBegin;
+import org.apache.beam.sdk.values.PCollection;
+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;
+import org.apache.kafka.clients.consumer.Consumer;
+import org.apache.kafka.common.PartitionInfo;
+import org.apache.kafka.common.TopicPartition;
+import org.checkerframework.checker.nullness.qual.Nullable;
+import org.joda.time.Duration;
+import org.joda.time.Instant;
+
+/**
+ * A {@link PTransform} for continuously querying Kafka for new partitions, and emitting those
+ * topics as {@link KafkaSourceDescriptor} This transform is implemented using the {@link Watch}
+ * transform, and modifications to this transform should keep that in mind.
+ *
+ * <p>Please see
+ * https://docs.google.com/document/d/1Io49s5LBs29HJyppKG3AlR-gHz5m5PC6CqO0CCoSqLs/edit?usp=sharing
+ * for design details
+ */
+@Experimental
+class WatchForKafkaTopicPartitions extends PTransform<PBegin, PCollection<KafkaSourceDescriptor>> {
+
+  private static final Duration DEFAULT_CHECK_DURATION = Duration.standardHours(1);
+  private static final String COUNTER_NAMESPACE = "watch_kafka_topic_partition";
+
+  private final Duration checkDuration;
+  private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>
+      kafkaConsumerFactoryFn;
+  private final Map<String, Object> kafkaConsumerConfig;
+  private final @Nullable SerializableFunction<TopicPartition, Boolean> checkStopReadingFn;
+  private final Set<String> topics;
+  private final @Nullable Instant startReadTime;
+  private final @Nullable Instant stopReadTime;
+
+  public WatchForKafkaTopicPartitions(
+      @Nullable Duration checkDuration,
+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> kafkaConsumerFactoryFn,
+      Map<String, Object> kafkaConsumerConfig,
+      @Nullable SerializableFunction<TopicPartition, Boolean> checkStopReadingFn,
+      Set<String> topics,
+      @Nullable Instant startReadTime,
+      @Nullable Instant stopReadTime) {
+    this.checkDuration = firstNonNull(checkDuration, DEFAULT_CHECK_DURATION);
+    this.kafkaConsumerFactoryFn = kafkaConsumerFactoryFn;
+    this.kafkaConsumerConfig = kafkaConsumerConfig;
+    this.checkStopReadingFn = checkStopReadingFn;
+    this.topics = topics;
+    this.startReadTime = startReadTime;
+    this.stopReadTime = stopReadTime;
+  }
+
+  @Override
+  public PCollection<KafkaSourceDescriptor> expand(PBegin input) {
+    return input
+        .apply(Impulse.create())
+        .apply(
+            "Match new TopicPartitions",
+            Watch.growthOf(
+                    new WatchPartitionFn(kafkaConsumerFactoryFn, kafkaConsumerConfig, topics))
+                .withPollInterval(checkDuration))
+        .apply(ParDo.of(new ConvertToDescriptor(checkStopReadingFn, startReadTime, stopReadTime)));
+  }
+
+  private static class ConvertToDescriptor
+      extends DoFn<KV<byte[], TopicPartition>, KafkaSourceDescriptor> {
+
+    private final @Nullable SerializableFunction<TopicPartition, Boolean> checkStopReadingFn;
+    private final @Nullable Instant startReadTime;
+    private final @Nullable Instant stopReadTime;
+
+    private ConvertToDescriptor(
+        @Nullable SerializableFunction<TopicPartition, Boolean> checkStopReadingFn,
+        @Nullable Instant startReadTime,
+        @Nullable Instant stopReadTime) {
+      this.checkStopReadingFn = checkStopReadingFn;
+      this.startReadTime = startReadTime;
+      this.stopReadTime = stopReadTime;
+    }
+
+    @ProcessElement
+    public void processElement(
+        @Element KV<byte[], TopicPartition> partition,
+        OutputReceiver<KafkaSourceDescriptor> receiver) {
+      TopicPartition topicPartition = Objects.requireNonNull(partition.getValue());
+      if (checkStopReadingFn == null || !checkStopReadingFn.apply(topicPartition)) {
+        Counter foundedTopicPartition =
+            Metrics.counter(COUNTER_NAMESPACE, topicPartition.toString());
+        foundedTopicPartition.inc();
+        receiver.output(
+            KafkaSourceDescriptor.of(
+                topicPartition, null, startReadTime, null, stopReadTime, null));
+      }
+    }
+  }
+
+  private static class WatchPartitionFn extends PollFn<byte[], TopicPartition> {
+
+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>
+        kafkaConsumerFactoryFn;
+    private final Map<String, Object> kafkaConsumerConfig;
+    private final Set<String> topics;
+
+    private WatchPartitionFn(
+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> kafkaConsumerFactoryFn,
+        Map<String, Object> kafkaConsumerConfig,
+        Set<String> topics) {
+      this.kafkaConsumerFactoryFn = kafkaConsumerFactoryFn;
+      this.kafkaConsumerConfig = kafkaConsumerConfig;
+      this.topics = topics;
+    }
+
+    @Override
+    public Watch.Growth.PollResult<TopicPartition> apply(byte[] element, Context c)
+        throws Exception {
+      Instant now = Instant.now();
+      return Watch.Growth.PollResult.incomplete(
+              now, getAllTopicPartitions(kafkaConsumerFactoryFn, kafkaConsumerConfig, topics))
+          .withWatermark(now);
+    }
+  }
+
+  @VisibleForTesting
+  static List<TopicPartition> getAllTopicPartitions(
+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> kafkaConsumerFactoryFn,
+      Map<String, Object> kafkaConsumerConfig,
+      Set<String> topics) {
+    List<TopicPartition> current = new ArrayList<>();
+    try (Consumer<byte[], byte[]> kafkaConsumer =
+        kafkaConsumerFactoryFn.apply(kafkaConsumerConfig)) {
+      if (topics != null && !topics.isEmpty()) {
+        for (String topic : topics) {
+          for (PartitionInfo partition : kafkaConsumer.partitionsFor(topic)) {
+            current.add(new TopicPartition(topic, partition.partition()));
+          }
+        }
+
+      } else {
+        for (Map.Entry<String, List<PartitionInfo>> topicInfo :
+            kafkaConsumer.listTopics().entrySet()) {
+          for (PartitionInfo partition : topicInfo.getValue()) {
+            current.add(new TopicPartition(topicInfo.getKey(), partition.partition()));
+          }
+        }
+      }
+    }
+    return current;
+  }
+}
diff --git a/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/WatchKafkaTopicPartitionDoFn.java b/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/WatchKafkaTopicPartitionDoFn.java
deleted file mode 100644
index bcd81d9d2fb..00000000000
--- a/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/WatchKafkaTopicPartitionDoFn.java
+++ /dev/null
@@ -1,189 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.beam.sdk.io.kafka;
-
-import java.util.HashSet;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-import org.apache.beam.sdk.annotations.Experimental;
-import org.apache.beam.sdk.metrics.Counter;
-import org.apache.beam.sdk.metrics.Metrics;
-import org.apache.beam.sdk.state.BagState;
-import org.apache.beam.sdk.state.StateSpec;
-import org.apache.beam.sdk.state.StateSpecs;
-import org.apache.beam.sdk.state.TimeDomain;
-import org.apache.beam.sdk.state.Timer;
-import org.apache.beam.sdk.state.TimerSpec;
-import org.apache.beam.sdk.state.TimerSpecs;
-import org.apache.beam.sdk.transforms.DoFn;
-import org.apache.beam.sdk.transforms.SerializableFunction;
-import org.apache.beam.sdk.values.KV;
-import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;
-import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Sets;
-import org.apache.kafka.clients.consumer.Consumer;
-import org.apache.kafka.common.PartitionInfo;
-import org.apache.kafka.common.TopicPartition;
-import org.checkerframework.checker.nullness.qual.Nullable;
-import org.joda.time.Duration;
-import org.joda.time.Instant;
-
-/**
- * A stateful {@linkl DoFn} that emits new available {@link TopicPartition} regularly.
- *
- * <p>Please refer to
- * https://docs.google.com/document/d/1FU3GxVRetHPLVizP3Mdv6mP5tpjZ3fd99qNjUI5DT5k/edit# for more
- * details.
- */
-@SuppressWarnings({
-  // TODO(https://github.com/apache/beam/issues/21230): Remove when new version of
-  // errorprone is released (2.11.0)
-  "unused"
-})
-@Experimental
-class WatchKafkaTopicPartitionDoFn extends DoFn<KV<byte[], byte[]>, KafkaSourceDescriptor> {
-
-  private static final Duration DEFAULT_CHECK_DURATION = Duration.standardHours(1);
-  private static final String TIMER_ID = "watch_timer";
-  private static final String STATE_ID = "topic_partition_set";
-  private final Duration checkDuration;
-
-  private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>
-      kafkaConsumerFactoryFn;
-  private final @Nullable SerializableFunction<TopicPartition, Boolean> checkStopReadingFn;
-  private final Map<String, Object> kafkaConsumerConfig;
-  private final @Nullable Instant startReadTime;
-  private final @Nullable Instant stopReadTime;
-
-  private static final String COUNTER_NAMESPACE = "watch_kafka_topic_partition";
-
-  private final List<String> topics;
-
-  WatchKafkaTopicPartitionDoFn(
-      @Nullable Duration checkDuration,
-      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> kafkaConsumerFactoryFn,
-      @Nullable SerializableFunction<TopicPartition, Boolean> checkStopReadingFn,
-      Map<String, Object> kafkaConsumerConfig,
-      @Nullable Instant startReadTime,
-      @Nullable Instant stopReadTime,
-      List<String> topics) {
-    this.checkDuration = checkDuration == null ? DEFAULT_CHECK_DURATION : checkDuration;
-    this.kafkaConsumerFactoryFn = kafkaConsumerFactoryFn;
-    this.checkStopReadingFn = checkStopReadingFn;
-    this.kafkaConsumerConfig = kafkaConsumerConfig;
-    this.startReadTime = startReadTime;
-    this.stopReadTime = stopReadTime;
-    this.topics = topics;
-  }
-
-  @TimerId(TIMER_ID)
-  private final TimerSpec timerSpec = TimerSpecs.timer(TimeDomain.PROCESSING_TIME);
-
-  @StateId(STATE_ID)
-  private final StateSpec<BagState<TopicPartition>> bagStateSpec =
-      StateSpecs.bag(new TopicPartitionCoder());
-
-  @VisibleForTesting
-  Set<TopicPartition> getAllTopicPartitions() {
-    Set<TopicPartition> current = new HashSet<>();
-    try (Consumer<byte[], byte[]> kafkaConsumer =
-        kafkaConsumerFactoryFn.apply(kafkaConsumerConfig)) {
-      if (topics != null && !topics.isEmpty()) {
-        for (String topic : topics) {
-          for (PartitionInfo partition : kafkaConsumer.partitionsFor(topic)) {
-            current.add(new TopicPartition(topic, partition.partition()));
-          }
-        }
-
-      } else {
-        for (Map.Entry<String, List<PartitionInfo>> topicInfo :
-            kafkaConsumer.listTopics().entrySet()) {
-          for (PartitionInfo partition : topicInfo.getValue()) {
-            current.add(new TopicPartition(topicInfo.getKey(), partition.partition()));
-          }
-        }
-      }
-    }
-    return current;
-  }
-
-  @ProcessElement
-  public void processElement(
-      @TimerId(TIMER_ID) Timer timer,
-      @StateId(STATE_ID) BagState<TopicPartition> existingTopicPartitions,
-      OutputReceiver<KafkaSourceDescriptor> outputReceiver) {
-    // For the first time, we emit all available TopicPartition and write them into State.
-    Set<TopicPartition> current = getAllTopicPartitions();
-    current.forEach(
-        topicPartition -> {
-          if (checkStopReadingFn == null || !checkStopReadingFn.apply(topicPartition)) {
-            Counter foundedTopicPartition =
-                Metrics.counter(COUNTER_NAMESPACE, topicPartition.toString());
-            foundedTopicPartition.inc();
-            existingTopicPartitions.add(topicPartition);
-            outputReceiver.output(
-                KafkaSourceDescriptor.of(
-                    topicPartition, null, startReadTime, null, stopReadTime, null));
-          }
-        });
-
-    timer.offset(checkDuration).setRelative();
-  }
-
-  @OnTimer(TIMER_ID)
-  public void onTimer(
-      @TimerId(TIMER_ID) Timer timer,
-      @StateId(STATE_ID) BagState<TopicPartition> existingTopicPartitions,
-      OutputReceiver<KafkaSourceDescriptor> outputReceiver) {
-    Set<TopicPartition> readingTopicPartitions = new HashSet<>();
-    existingTopicPartitions
-        .read()
-        .forEach(
-            topicPartition -> {
-              readingTopicPartitions.add(topicPartition);
-            });
-    existingTopicPartitions.clear();
-
-    Set<TopicPartition> currentAll = this.getAllTopicPartitions();
-
-    // Emit new added TopicPartitions.
-    Set<TopicPartition> newAdded = Sets.difference(currentAll, readingTopicPartitions);
-    newAdded.forEach(
-        topicPartition -> {
-          if (checkStopReadingFn == null || !checkStopReadingFn.apply(topicPartition)) {
-            Counter foundedTopicPartition =
-                Metrics.counter(COUNTER_NAMESPACE, topicPartition.toString());
-            foundedTopicPartition.inc();
-            outputReceiver.output(
-                KafkaSourceDescriptor.of(
-                    topicPartition, null, startReadTime, null, stopReadTime, null));
-          }
-        });
-
-    // Update the State.
-    currentAll.forEach(
-        topicPartition -> {
-          if (checkStopReadingFn == null || !checkStopReadingFn.apply(topicPartition)) {
-            existingTopicPartitions.add(topicPartition);
-          }
-        });
-
-    // Reset the timer.
-    timer.set(Instant.now().plus(Duration.millis(checkDuration.getMillis())));
-  }
-}
diff --git a/sdks/java/io/kafka/src/test/java/org/apache/beam/sdk/io/kafka/KafkaIOIT.java b/sdks/java/io/kafka/src/test/java/org/apache/beam/sdk/io/kafka/KafkaIOIT.java
index 68ce1e44192..211fa6f6137 100644
--- a/sdks/java/io/kafka/src/test/java/org/apache/beam/sdk/io/kafka/KafkaIOIT.java
+++ b/sdks/java/io/kafka/src/test/java/org/apache/beam/sdk/io/kafka/KafkaIOIT.java
@@ -23,14 +23,14 @@ import static org.junit.Assert.assertNull;
 
 import com.google.cloud.Timestamp;
 import java.io.IOException;
-import java.util.ArrayList;
 import java.util.Arrays;
-import java.util.List;
+import java.util.HashMap;
 import java.util.Map;
 import java.util.Set;
 import java.util.UUID;
 import java.util.function.BiFunction;
 import org.apache.beam.sdk.PipelineResult;
+import org.apache.beam.sdk.PipelineResult.State;
 import org.apache.beam.sdk.coders.ByteArrayCoder;
 import org.apache.beam.sdk.coders.NullableCoder;
 import org.apache.beam.sdk.io.Read;
@@ -43,10 +43,13 @@ import org.apache.beam.sdk.metrics.Counter;
 import org.apache.beam.sdk.metrics.Metrics;
 import org.apache.beam.sdk.options.Default;
 import org.apache.beam.sdk.options.Description;
+import org.apache.beam.sdk.options.ExperimentalOptions;
+import org.apache.beam.sdk.options.PipelineOptionsFactory;
 import org.apache.beam.sdk.options.StreamingOptions;
 import org.apache.beam.sdk.options.Validation;
 import org.apache.beam.sdk.testing.PAssert;
 import org.apache.beam.sdk.testing.TestPipeline;
+import org.apache.beam.sdk.testing.TestPipelineOptions;
 import org.apache.beam.sdk.testutils.NamedTestResult;
 import org.apache.beam.sdk.testutils.metrics.IOITMetrics;
 import org.apache.beam.sdk.testutils.metrics.MetricsReader;
@@ -55,16 +58,27 @@ import org.apache.beam.sdk.testutils.publishing.InfluxDBSettings;
 import org.apache.beam.sdk.transforms.Combine;
 import org.apache.beam.sdk.transforms.Create;
 import org.apache.beam.sdk.transforms.DoFn;
+import org.apache.beam.sdk.transforms.GroupByKey;
+import org.apache.beam.sdk.transforms.Keys;
 import org.apache.beam.sdk.transforms.MapElements;
 import org.apache.beam.sdk.transforms.ParDo;
 import org.apache.beam.sdk.transforms.SimpleFunction;
+import org.apache.beam.sdk.transforms.windowing.FixedWindows;
+import org.apache.beam.sdk.transforms.windowing.Window;
 import org.apache.beam.sdk.values.KV;
 import org.apache.beam.sdk.values.PCollection;
 import org.apache.beam.sdk.values.Row;
 import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;
 import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableSet;
+import org.apache.kafka.clients.admin.AdminClient;
+import org.apache.kafka.clients.admin.NewPartitions;
+import org.apache.kafka.clients.admin.NewTopic;
 import org.apache.kafka.common.serialization.ByteArrayDeserializer;
 import org.apache.kafka.common.serialization.ByteArraySerializer;
+import org.apache.kafka.common.serialization.IntegerDeserializer;
+import org.apache.kafka.common.serialization.IntegerSerializer;
+import org.apache.kafka.common.serialization.StringDeserializer;
+import org.apache.kafka.common.serialization.StringSerializer;
 import org.checkerframework.checker.nullness.qual.Nullable;
 import org.joda.time.Duration;
 import org.junit.AfterClass;
@@ -114,6 +128,17 @@ public class KafkaIOIT {
 
   @Rule public TestPipeline readPipeline = TestPipeline.create();
 
+  private static ExperimentalOptions sdfPipelineOptions;
+
+  static {
+    sdfPipelineOptions = PipelineOptionsFactory.create().as(ExperimentalOptions.class);
+    ExperimentalOptions.addExperiment(sdfPipelineOptions, "use_sdf_read");
+    ExperimentalOptions.addExperiment(sdfPipelineOptions, "beam_fn_api");
+    sdfPipelineOptions.as(TestPipelineOptions.class).setBlockOnRun(false);
+  }
+
+  @Rule public TestPipeline sdfReadPipeline = TestPipeline.fromOptions(sdfPipelineOptions);
+
   private static KafkaContainer kafkaContainer;
 
   @BeforeClass
@@ -185,11 +210,13 @@ public class KafkaIOIT {
     writePipeline
         .apply("Generate records", Read.from(new SyntheticBoundedSource(sourceOptions)))
         .apply("Measure write time", ParDo.of(new TimeMonitor<>(NAMESPACE, WRITE_TIME_METRIC_NAME)))
-        .apply("Write to Kafka", writeToKafka());
+        .apply("Write to Kafka", writeToKafka().withTopic(options.getKafkaTopic() + "-batch"));
 
     PCollection<String> hashcode =
         readPipeline
-            .apply("Read from bounded Kafka", readFromBoundedKafka())
+            .apply(
+                "Read from bounded Kafka",
+                readFromBoundedKafka().withTopic(options.getKafkaTopic() + "-batch"))
             .apply(
                 "Measure read time", ParDo.of(new TimeMonitor<>(NAMESPACE, READ_TIME_METRIC_NAME)))
             .apply("Map records to strings", MapElements.via(new MapKafkaRecordsToStrings()))
@@ -215,40 +242,24 @@ public class KafkaIOIT {
   // This test roundtrips a single KV<Null,Null> to verify that externalWithMetadata
   // can handle null keys and values correctly.
   @Test
-  public void testKafkaIOExternalRoundtripWithMetadataAndNullKeysAndValues() {
+  public void testKafkaIOExternalRoundtripWithMetadataAndNullKeysAndValues() throws IOException {
 
-    List<byte[]> nullList = new ArrayList<>();
-    nullList.add(null);
     writePipeline
-        .apply(Create.of(nullList))
+        .apply(Create.of(KV.<byte[], byte[]>of(null, null)))
         .apply(
-            ParDo.of(
-                new DoFn<byte[], KV<byte[], byte[]>>() {
-                  @ProcessElement
-                  public void processElement(
-                      @Element byte[] element, OutputReceiver<KV<byte[], byte[]>> receiver) {
-                    receiver.output(KV.of(element, element));
-                  }
-                }))
-        .apply(
-            KafkaIO.<byte[], byte[]>write()
-                .withBootstrapServers(options.getKafkaBootstrapServerAddresses())
-                .withTopic(options.getKafkaTopic())
-                .withKeySerializer(ByteArraySerializer.class)
-                .withValueSerializer(ByteArraySerializer.class));
-
-    PipelineResult writeResult = writePipeline.run();
-    writeResult.waitUntilFinish();
+            "Write to Kafka", writeToKafka().withTopic(options.getKafkaTopic() + "-nullRoundTrip"));
 
     PCollection<Row> rows =
         readPipeline.apply(
             KafkaIO.<byte[], byte[]>read()
                 .withBootstrapServers(options.getKafkaBootstrapServerAddresses())
-                .withTopic(options.getKafkaTopic())
+                .withTopic(options.getKafkaTopic() + "-nullRoundTrip")
+                .withConsumerConfigUpdates(ImmutableMap.of("auto.offset.reset", "earliest"))
                 .withKeyDeserializerAndCoder(
                     ByteArrayDeserializer.class, NullableCoder.of(ByteArrayCoder.of()))
                 .withValueDeserializerAndCoder(
                     ByteArrayDeserializer.class, NullableCoder.of(ByteArrayCoder.of()))
+                .withMaxNumRecords(1)
                 .externalWithMetadata());
 
     PAssert.thatSingleton(rows)
@@ -259,8 +270,111 @@ public class KafkaIOIT {
               return null;
             });
 
+    PipelineResult writeResult = writePipeline.run();
+    writeResult.waitUntilFinish();
+
     PipelineResult readResult = readPipeline.run();
-    readResult.waitUntilFinish();
+    PipelineResult.State readState =
+        readResult.waitUntilFinish(Duration.standardSeconds(options.getReadTimeout()));
+
+    cancelIfTimeouted(readResult, readState);
+  }
+
+  @Test
+  public void testKafkaWithDynamicPartitions() throws IOException {
+    AdminClient client =
+        AdminClient.create(
+            ImmutableMap.of("bootstrap.servers", options.getKafkaBootstrapServerAddresses()));
+    String topicName = "DynamicTopicPartition-" + UUID.randomUUID();
+    Map<Integer, String> records = new HashMap<>();
+    for (int i = 0; i < 100; i++) {
+      records.put(i, String.valueOf(i));
+    }
+    Map<Integer, String> moreRecords = new HashMap<>();
+    for (int i = 100; i < 200; i++) {
+      moreRecords.put(i, String.valueOf(i));
+    }
+    try {
+      client.createTopics(ImmutableSet.of(new NewTopic(topicName, 1, (short) 1)));
+      client.createPartitions(ImmutableMap.of(topicName, NewPartitions.increaseTo(1)));
+
+      writePipeline
+          .apply("Generate Write Elements", Create.of(records))
+          .apply(
+              "Write to Kafka",
+              KafkaIO.<Integer, String>write()
+                  .withBootstrapServers(options.getKafkaBootstrapServerAddresses())
+                  .withTopic(topicName)
+                  .withKeySerializer(IntegerSerializer.class)
+                  .withValueSerializer(StringSerializer.class));
+
+      writePipeline.run().waitUntilFinish(Duration.standardSeconds(15));
+
+      Thread delayedWriteThread =
+          new Thread(
+              () -> {
+                try {
+                  Thread.sleep(20 * 1000); // wait 20 seconds before changing kafka
+                } catch (InterruptedException e) {
+                  throw new RuntimeException(e);
+                }
+
+                client.createPartitions(ImmutableMap.of(topicName, NewPartitions.increaseTo(2)));
+
+                writePipeline
+                    .apply("Second Pass generate Write Elements", Create.of(moreRecords))
+                    .apply(
+                        "Write more to Kafka",
+                        KafkaIO.<Integer, String>write()
+                            .withBootstrapServers(options.getKafkaBootstrapServerAddresses())
+                            .withTopic(topicName)
+                            .withKeySerializer(IntegerSerializer.class)
+                            .withValueSerializer(StringSerializer.class));
+
+                writePipeline.run().waitUntilFinish(Duration.standardSeconds(15));
+              });
+
+      delayedWriteThread.start();
+
+      PCollection<Integer> values =
+          sdfReadPipeline
+              .apply(
+                  "Read from Kafka",
+                  KafkaIO.<Integer, String>read()
+                      .withBootstrapServers(options.getKafkaBootstrapServerAddresses())
+                      .withConsumerConfigUpdates(ImmutableMap.of("auto.offset.reset", "earliest"))
+                      .withTopic(topicName)
+                      .withDynamicRead(Duration.standardSeconds(5))
+                      .withKeyDeserializer(IntegerDeserializer.class)
+                      .withValueDeserializer(StringDeserializer.class))
+              .apply("Key by Partition", ParDo.of(new KeyByPartition()))
+              .apply(Window.into(FixedWindows.of(Duration.standardMinutes(1))))
+              .apply("Group by Partition", GroupByKey.create())
+              .apply("Get Partitions", Keys.create());
+
+      PAssert.that(values).containsInAnyOrder(0, 1);
+
+      PipelineResult readResult = sdfReadPipeline.run();
+
+      State readState =
+          readResult.waitUntilFinish(Duration.standardSeconds(options.getReadTimeout() / 2));
+
+      cancelIfTimeouted(readResult, readState);
+
+    } finally {
+      client.deleteTopics(ImmutableSet.of(topicName));
+    }
+  }
+
+  private static class KeyByPartition
+      extends DoFn<KafkaRecord<Integer, String>, KV<Integer, KafkaRecord<Integer, String>>> {
+
+    @ProcessElement
+    public void processElement(
+        @Element KafkaRecord<Integer, String> record,
+        OutputReceiver<KV<Integer, KafkaRecord<Integer, String>>> receiver) {
+      receiver.output(KV.of(record.getPartition(), record));
+    }
   }
 
   private long readElementMetric(PipelineResult result, String namespace, String name) {
@@ -300,7 +414,6 @@ public class KafkaIOIT {
   private KafkaIO.Write<byte[], byte[]> writeToKafka() {
     return KafkaIO.<byte[], byte[]>write()
         .withBootstrapServers(options.getKafkaBootstrapServerAddresses())
-        .withTopic(options.getKafkaTopic())
         .withKeySerializer(ByteArraySerializer.class)
         .withValueSerializer(ByteArraySerializer.class);
   }
@@ -312,8 +425,7 @@ public class KafkaIOIT {
   private KafkaIO.Read<byte[], byte[]> readFromKafka() {
     return KafkaIO.readBytes()
         .withBootstrapServers(options.getKafkaBootstrapServerAddresses())
-        .withConsumerConfigUpdates(ImmutableMap.of("auto.offset.reset", "earliest"))
-        .withTopic(options.getKafkaTopic());
+        .withConsumerConfigUpdates(ImmutableMap.of("auto.offset.reset", "earliest"));
   }
 
   private static class CountingFn extends DoFn<String, Void> {
diff --git a/sdks/java/io/kafka/src/test/java/org/apache/beam/sdk/io/kafka/KafkaMocks.java b/sdks/java/io/kafka/src/test/java/org/apache/beam/sdk/io/kafka/KafkaMocks.java
index 4ea1594f970..0844d71e710 100644
--- a/sdks/java/io/kafka/src/test/java/org/apache/beam/sdk/io/kafka/KafkaMocks.java
+++ b/sdks/java/io/kafka/src/test/java/org/apache/beam/sdk/io/kafka/KafkaMocks.java
@@ -17,11 +17,14 @@
  */
 package org.apache.beam.sdk.io.kafka;
 
+import java.io.Serializable;
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import java.util.concurrent.Future;
+import java.util.stream.Collectors;
 import org.apache.beam.sdk.transforms.SerializableFunction;
+import org.apache.beam.sdk.values.KV;
 import org.apache.kafka.clients.consumer.Consumer;
 import org.apache.kafka.clients.consumer.ConsumerConfig;
 import org.apache.kafka.clients.consumer.ConsumerRecords;
@@ -110,4 +113,29 @@ public class KafkaMocks {
       }
     }
   }
+
+  public static final class PartitionGrowthMockConsumer extends MockConsumer<byte[], byte[]>
+      implements Serializable {
+
+    private List<List<KV<String, Integer>>> partitions;
+    private int index = 0;
+
+    public PartitionGrowthMockConsumer() {
+      super(null);
+    }
+
+    public PartitionGrowthMockConsumer(List<List<KV<String, Integer>>> partitions) {
+      super(null);
+      this.partitions = partitions;
+    }
+
+    @Override
+    public synchronized List<PartitionInfo> partitionsFor(String topic) {
+      List<KV<String, Integer>> partitionInfos = partitions.get(index);
+      index++;
+      return partitionInfos.stream()
+          .map(kv -> new PartitionInfo(kv.getKey(), kv.getValue(), null, null, null))
+          .collect(Collectors.toList());
+    }
+  }
 }
diff --git a/sdks/java/io/kafka/src/test/java/org/apache/beam/sdk/io/kafka/WatchForKafkaTopicPartitionsTest.java b/sdks/java/io/kafka/src/test/java/org/apache/beam/sdk/io/kafka/WatchForKafkaTopicPartitionsTest.java
new file mode 100644
index 00000000000..78ec5772819
--- /dev/null
+++ b/sdks/java/io/kafka/src/test/java/org/apache/beam/sdk/io/kafka/WatchForKafkaTopicPartitionsTest.java
@@ -0,0 +1,187 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.beam.sdk.io.kafka;
+
+import static org.junit.Assert.assertEquals;
+import static org.mockito.Mockito.never;
+import static org.mockito.Mockito.verify;
+import static org.mockito.Mockito.when;
+
+import java.util.Set;
+import org.apache.beam.sdk.io.kafka.KafkaMocks.PartitionGrowthMockConsumer;
+import org.apache.beam.sdk.testing.PAssert;
+import org.apache.beam.sdk.testing.SerializableMatcher;
+import org.apache.beam.sdk.testing.TestPipeline;
+import org.apache.beam.sdk.testing.TestPipelineOptions;
+import org.apache.beam.sdk.values.KV;
+import org.apache.beam.sdk.values.PCollection;
+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;
+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;
+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableSet;
+import org.apache.kafka.clients.consumer.Consumer;
+import org.apache.kafka.common.PartitionInfo;
+import org.apache.kafka.common.TopicPartition;
+import org.hamcrest.BaseMatcher;
+import org.hamcrest.Description;
+import org.joda.time.Duration;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.JUnit4;
+import org.mockito.Mockito;
+
+@RunWith(JUnit4.class)
+public class WatchForKafkaTopicPartitionsTest {
+
+  public static final TestPipelineOptions OPTIONS =
+      TestPipeline.testingPipelineOptions().as(TestPipelineOptions.class);
+
+  static {
+    OPTIONS.setBlockOnRun(false);
+  }
+
+  @Rule public final transient TestPipeline p = TestPipeline.fromOptions(OPTIONS);
+
+  @Test
+  public void testGetAllTopicPartitions() throws Exception {
+    Consumer<byte[], byte[]> mockConsumer = Mockito.mock(Consumer.class);
+    when(mockConsumer.listTopics())
+        .thenReturn(
+            ImmutableMap.of(
+                "topic1",
+                ImmutableList.of(
+                    new PartitionInfo("topic1", 0, null, null, null),
+                    new PartitionInfo("topic1", 1, null, null, null)),
+                "topic2",
+                ImmutableList.of(
+                    new PartitionInfo("topic2", 0, null, null, null),
+                    new PartitionInfo("topic2", 1, null, null, null))));
+    assertEquals(
+        ImmutableList.of(
+            new TopicPartition("topic1", 0),
+            new TopicPartition("topic1", 1),
+            new TopicPartition("topic2", 0),
+            new TopicPartition("topic2", 1)),
+        WatchForKafkaTopicPartitions.getAllTopicPartitions((input) -> mockConsumer, null, null));
+  }
+
+  @Test
+  public void testGetAllTopicPartitionsWithGivenTopics() throws Exception {
+    Set<String> givenTopics = ImmutableSet.of("topic1", "topic2");
+
+    Consumer<byte[], byte[]> mockConsumer = Mockito.mock(Consumer.class);
+    when(mockConsumer.partitionsFor("topic1"))
+        .thenReturn(
+            ImmutableList.of(
+                new PartitionInfo("topic1", 0, null, null, null),
+                new PartitionInfo("topic1", 1, null, null, null)));
+    when(mockConsumer.partitionsFor("topic2"))
+        .thenReturn(
+            ImmutableList.of(
+                new PartitionInfo("topic2", 0, null, null, null),
+                new PartitionInfo("topic2", 1, null, null, null)));
+    verify(mockConsumer, never()).listTopics();
+    assertEquals(
+        ImmutableList.of(
+            new TopicPartition("topic1", 0),
+            new TopicPartition("topic1", 1),
+            new TopicPartition("topic2", 0),
+            new TopicPartition("topic2", 1)),
+        WatchForKafkaTopicPartitions.getAllTopicPartitions(
+            (input) -> mockConsumer, null, givenTopics));
+  }
+
+  @Test
+  public void testPartitionSingle() {
+    Set<String> givenTopics = ImmutableSet.of("topic1");
+
+    WatchForKafkaTopicPartitions watchForKafkaTopicPartitions =
+        new WatchForKafkaTopicPartitions(
+            Duration.millis(1L),
+            (input) ->
+                new PartitionGrowthMockConsumer(
+                    ImmutableList.of(ImmutableList.of(KV.of("topic1", 0)))),
+            null,
+            null,
+            givenTopics,
+            null,
+            null);
+
+    PCollection<KafkaSourceDescriptor> descriptors = p.apply(watchForKafkaTopicPartitions);
+
+    PAssert.that(descriptors).containsInAnyOrder(new KafkaSourceDescriptionMatcher("topic1", 0));
+
+    p.run().waitUntilFinish(Duration.millis(10));
+  }
+
+  @Test
+  public void testPartitionGrowth() {
+    Set<String> givenTopics = ImmutableSet.of("topic1");
+
+    WatchForKafkaTopicPartitions watchForKafkaTopicPartitions =
+        new WatchForKafkaTopicPartitions(
+            Duration.millis(1L),
+            (input) ->
+                new PartitionGrowthMockConsumer(
+                    ImmutableList.of(
+                        ImmutableList.of(KV.of("topic1", 0)),
+                        ImmutableList.of(KV.of("topic1", 0), KV.of("topic1", 1)))),
+            null,
+            null,
+            givenTopics,
+            null,
+            null);
+
+    PCollection<KafkaSourceDescriptor> descriptors = p.apply(watchForKafkaTopicPartitions);
+
+    PAssert.that(descriptors)
+        .containsInAnyOrder(
+            new KafkaSourceDescriptionMatcher("topic1", 0),
+            new KafkaSourceDescriptionMatcher("topic1", 1));
+
+    p.run().waitUntilFinish(Duration.millis(10));
+  }
+
+  private static class KafkaSourceDescriptionMatcher extends BaseMatcher<KafkaSourceDescriptor>
+      implements SerializableMatcher<KafkaSourceDescriptor> {
+
+    private String topic;
+    private int partition;
+
+    KafkaSourceDescriptionMatcher(String topic, int partition) {
+      this.topic = topic;
+      this.partition = partition;
+    }
+
+    @Override
+    public void describeMismatch(Object actual, Description mismatchDescription) {}
+
+    @Override
+    public void describeTo(Description description) {}
+
+    @Override
+    public boolean matches(Object o) {
+      if (o instanceof KafkaSourceDescriptor) {
+        KafkaSourceDescriptor descriptor = (KafkaSourceDescriptor) o;
+        return descriptor.getTopic().equals(topic) && descriptor.getPartition() == partition;
+      } else {
+        return false;
+      }
+    }
+  }
+}
diff --git a/sdks/java/io/kafka/src/test/java/org/apache/beam/sdk/io/kafka/WatchKafkaTopicPartitionDoFnTest.java b/sdks/java/io/kafka/src/test/java/org/apache/beam/sdk/io/kafka/WatchKafkaTopicPartitionDoFnTest.java
deleted file mode 100644
index 206228be484..00000000000
--- a/sdks/java/io/kafka/src/test/java/org/apache/beam/sdk/io/kafka/WatchKafkaTopicPartitionDoFnTest.java
+++ /dev/null
@@ -1,449 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.beam.sdk.io.kafka;
-
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertTrue;
-import static org.mockito.Mockito.never;
-import static org.mockito.Mockito.times;
-import static org.mockito.Mockito.verify;
-import static org.mockito.Mockito.when;
-import static org.powermock.api.mockito.PowerMockito.mockStatic;
-
-import java.util.ArrayList;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-import java.util.stream.Collectors;
-import org.apache.beam.sdk.state.BagState;
-import org.apache.beam.sdk.state.ReadableState;
-import org.apache.beam.sdk.state.Timer;
-import org.apache.beam.sdk.transforms.DoFn.OutputReceiver;
-import org.apache.beam.sdk.transforms.SerializableFunction;
-import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;
-import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;
-import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableSet;
-import org.apache.kafka.clients.consumer.Consumer;
-import org.apache.kafka.common.PartitionInfo;
-import org.apache.kafka.common.TopicPartition;
-import org.joda.time.Duration;
-import org.joda.time.Instant;
-import org.junit.Test;
-import org.junit.runner.RunWith;
-import org.mockito.Mock;
-import org.powermock.core.classloader.annotations.PrepareForTest;
-import org.powermock.modules.junit4.PowerMockRunner;
-
-@RunWith(PowerMockRunner.class)
-@PrepareForTest({Instant.class})
-public class WatchKafkaTopicPartitionDoFnTest {
-
-  @Mock Consumer<byte[], byte[]> mockConsumer;
-  @Mock Timer timer;
-
-  @SuppressWarnings("UnnecessaryAnonymousClass")
-  private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFn =
-      new SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>() {
-        @Override
-        public Consumer<byte[], byte[]> apply(Map<String, Object> input) {
-          return mockConsumer;
-        }
-      };
-
-  @Test
-  public void testGetAllTopicPartitions() throws Exception {
-    when(mockConsumer.listTopics())
-        .thenReturn(
-            ImmutableMap.of(
-                "topic1",
-                ImmutableList.of(
-                    new PartitionInfo("topic1", 0, null, null, null),
-                    new PartitionInfo("topic1", 1, null, null, null)),
-                "topic2",
-                ImmutableList.of(
-                    new PartitionInfo("topic2", 0, null, null, null),
-                    new PartitionInfo("topic2", 1, null, null, null))));
-    WatchKafkaTopicPartitionDoFn dofnInstance =
-        new WatchKafkaTopicPartitionDoFn(
-            Duration.millis(1L), consumerFn, null, ImmutableMap.of(), null, null, null);
-    assertEquals(
-        ImmutableSet.of(
-            new TopicPartition("topic1", 0),
-            new TopicPartition("topic1", 1),
-            new TopicPartition("topic2", 0),
-            new TopicPartition("topic2", 1)),
-        dofnInstance.getAllTopicPartitions());
-  }
-
-  @Test
-  public void testGetAllTopicPartitionsWithGivenTopics() throws Exception {
-    List<String> givenTopics = ImmutableList.of("topic1", "topic2");
-    when(mockConsumer.partitionsFor("topic1"))
-        .thenReturn(
-            ImmutableList.of(
-                new PartitionInfo("topic1", 0, null, null, null),
-                new PartitionInfo("topic1", 1, null, null, null)));
-    when(mockConsumer.partitionsFor("topic2"))
-        .thenReturn(
-            ImmutableList.of(
-                new PartitionInfo("topic2", 0, null, null, null),
-                new PartitionInfo("topic2", 1, null, null, null)));
-    WatchKafkaTopicPartitionDoFn dofnInstance =
-        new WatchKafkaTopicPartitionDoFn(
-            Duration.millis(1L), consumerFn, null, ImmutableMap.of(), null, null, givenTopics);
-    verify(mockConsumer, never()).listTopics();
-    assertEquals(
-        ImmutableSet.of(
-            new TopicPartition("topic1", 0),
-            new TopicPartition("topic1", 1),
-            new TopicPartition("topic2", 0),
-            new TopicPartition("topic2", 1)),
-        dofnInstance.getAllTopicPartitions());
-  }
-
-  @Test
-  public void testProcessElementWhenNoAvailableTopicPartition() throws Exception {
-    WatchKafkaTopicPartitionDoFn dofnInstance =
-        new WatchKafkaTopicPartitionDoFn(
-            Duration.millis(600L), consumerFn, null, ImmutableMap.of(), null, null, null);
-    MockOutputReceiver outputReceiver = new MockOutputReceiver();
-
-    when(mockConsumer.listTopics()).thenReturn(ImmutableMap.of());
-    MockBagState bagState = new MockBagState(ImmutableList.of());
-
-    when(timer.offset(Duration.millis(600L))).thenReturn(timer);
-    dofnInstance.processElement(timer, bagState, outputReceiver);
-    verify(timer, times(1)).setRelative();
-    assertTrue(outputReceiver.getOutputs().isEmpty());
-    assertTrue(bagState.getCurrentStates().isEmpty());
-  }
-
-  @Test
-  public void testProcessElementWithAvailableTopicPartitions() throws Exception {
-    Instant startReadTime = Instant.ofEpochMilli(1L);
-    WatchKafkaTopicPartitionDoFn dofnInstance =
-        new WatchKafkaTopicPartitionDoFn(
-            Duration.millis(600L), consumerFn, null, ImmutableMap.of(), startReadTime, null, null);
-    MockOutputReceiver outputReceiver = new MockOutputReceiver();
-
-    when(mockConsumer.listTopics())
-        .thenReturn(
-            ImmutableMap.of(
-                "topic1",
-                ImmutableList.of(
-                    new PartitionInfo("topic1", 0, null, null, null),
-                    new PartitionInfo("topic1", 1, null, null, null)),
-                "topic2",
-                ImmutableList.of(
-                    new PartitionInfo("topic2", 0, null, null, null),
-                    new PartitionInfo("topic2", 1, null, null, null))));
-    MockBagState bagState = new MockBagState(ImmutableList.of());
-
-    when(timer.offset(Duration.millis(600L))).thenReturn(timer);
-    dofnInstance.processElement(timer, bagState, outputReceiver);
-
-    verify(timer, times(1)).setRelative();
-    Set<TopicPartition> expectedOutputTopicPartitions =
-        ImmutableSet.of(
-            new TopicPartition("topic1", 0),
-            new TopicPartition("topic1", 1),
-            new TopicPartition("topic2", 0),
-            new TopicPartition("topic2", 1));
-    Set<KafkaSourceDescriptor> expectedOutputDescriptor =
-        generateDescriptorsFromTopicPartitions(expectedOutputTopicPartitions, startReadTime);
-    assertEquals(expectedOutputDescriptor, new HashSet<>(outputReceiver.getOutputs()));
-    assertEquals(expectedOutputTopicPartitions, bagState.getCurrentStates());
-  }
-
-  @Test
-  public void testProcessElementWithStoppingReadingTopicPartition() throws Exception {
-    Instant startReadTime = Instant.ofEpochMilli(1L);
-    SerializableFunction<TopicPartition, Boolean> checkStopReadingFn =
-        new SerializableFunction<TopicPartition, Boolean>() {
-          @Override
-          public Boolean apply(TopicPartition input) {
-            if (input.equals(new TopicPartition("topic1", 1))) {
-              return true;
-            }
-            return false;
-          }
-        };
-    WatchKafkaTopicPartitionDoFn dofnInstance =
-        new WatchKafkaTopicPartitionDoFn(
-            Duration.millis(600L),
-            consumerFn,
-            checkStopReadingFn,
-            ImmutableMap.of(),
-            startReadTime,
-            null,
-            null);
-    MockOutputReceiver outputReceiver = new MockOutputReceiver();
-
-    when(mockConsumer.listTopics())
-        .thenReturn(
-            ImmutableMap.of(
-                "topic1",
-                ImmutableList.of(
-                    new PartitionInfo("topic1", 0, null, null, null),
-                    new PartitionInfo("topic1", 1, null, null, null)),
-                "topic2",
-                ImmutableList.of(
-                    new PartitionInfo("topic2", 0, null, null, null),
-                    new PartitionInfo("topic2", 1, null, null, null))));
-    MockBagState bagState = new MockBagState(ImmutableList.of());
-
-    when(timer.offset(Duration.millis(600L))).thenReturn(timer);
-    dofnInstance.processElement(timer, bagState, outputReceiver);
-    verify(timer, times(1)).setRelative();
-
-    Set<TopicPartition> expectedOutputTopicPartitions =
-        ImmutableSet.of(
-            new TopicPartition("topic1", 0),
-            new TopicPartition("topic2", 0),
-            new TopicPartition("topic2", 1));
-    Set<KafkaSourceDescriptor> expectedOutputDescriptor =
-        generateDescriptorsFromTopicPartitions(expectedOutputTopicPartitions, startReadTime);
-    assertEquals(expectedOutputDescriptor, new HashSet<>(outputReceiver.getOutputs()));
-    assertEquals(expectedOutputTopicPartitions, bagState.getCurrentStates());
-  }
-
-  @Test
-  public void testOnTimerWithNoAvailableTopicPartition() throws Exception {
-    WatchKafkaTopicPartitionDoFn dofnInstance =
-        new WatchKafkaTopicPartitionDoFn(
-            Duration.millis(600L), consumerFn, null, ImmutableMap.of(), null, null, null);
-    MockOutputReceiver outputReceiver = new MockOutputReceiver();
-
-    when(mockConsumer.listTopics()).thenReturn(ImmutableMap.of());
-    MockBagState bagState = new MockBagState(ImmutableList.of(new TopicPartition("topic1", 0)));
-    Instant now = Instant.EPOCH;
-    mockStatic(Instant.class);
-    when(Instant.now()).thenReturn(now);
-
-    dofnInstance.onTimer(timer, bagState, outputReceiver);
-
-    verify(timer, times(1)).set(now.plus(Duration.millis(600L)));
-    assertTrue(outputReceiver.getOutputs().isEmpty());
-    assertTrue(bagState.getCurrentStates().isEmpty());
-  }
-
-  @Test
-  public void testOnTimerWithAdditionOnly() throws Exception {
-    Instant startReadTime = Instant.ofEpochMilli(1L);
-    WatchKafkaTopicPartitionDoFn dofnInstance =
-        new WatchKafkaTopicPartitionDoFn(
-            Duration.millis(600L), consumerFn, null, ImmutableMap.of(), startReadTime, null, null);
-    MockOutputReceiver outputReceiver = new MockOutputReceiver();
-
-    when(mockConsumer.listTopics())
-        .thenReturn(
-            ImmutableMap.of(
-                "topic1",
-                ImmutableList.of(
-                    new PartitionInfo("topic1", 0, null, null, null),
-                    new PartitionInfo("topic1", 1, null, null, null)),
-                "topic2",
-                ImmutableList.of(
-                    new PartitionInfo("topic2", 0, null, null, null),
-                    new PartitionInfo("topic2", 1, null, null, null))));
-    MockBagState bagState =
-        new MockBagState(
-            ImmutableList.of(new TopicPartition("topic1", 0), new TopicPartition("topic1", 1)));
-    Instant now = Instant.EPOCH;
-    mockStatic(Instant.class);
-    when(Instant.now()).thenReturn(now);
-
-    dofnInstance.onTimer(timer, bagState, outputReceiver);
-
-    verify(timer, times(1)).set(now.plus(Duration.millis(600L)));
-    Set<TopicPartition> expectedOutputTopicPartitions =
-        ImmutableSet.of(new TopicPartition("topic2", 0), new TopicPartition("topic2", 1));
-    Set<TopicPartition> expectedCurrentTopicPartitions =
-        ImmutableSet.of(
-            new TopicPartition("topic1", 0),
-            new TopicPartition("topic1", 1),
-            new TopicPartition("topic2", 0),
-            new TopicPartition("topic2", 1));
-    Set<KafkaSourceDescriptor> expectedOutputDescriptor =
-        generateDescriptorsFromTopicPartitions(expectedOutputTopicPartitions, startReadTime);
-    assertEquals(expectedOutputDescriptor, new HashSet<>(outputReceiver.getOutputs()));
-    assertEquals(expectedCurrentTopicPartitions, bagState.getCurrentStates());
-  }
-
-  @Test
-  public void testOnTimerWithRemovalOnly() throws Exception {
-    Instant startReadTime = Instant.ofEpochMilli(1L);
-    WatchKafkaTopicPartitionDoFn dofnInstance =
-        new WatchKafkaTopicPartitionDoFn(
-            Duration.millis(600L), consumerFn, null, ImmutableMap.of(), startReadTime, null, null);
-    MockOutputReceiver outputReceiver = new MockOutputReceiver();
-
-    when(mockConsumer.listTopics())
-        .thenReturn(
-            ImmutableMap.of(
-                "topic1",
-                ImmutableList.of(new PartitionInfo("topic1", 0, null, null, null)),
-                "topic2",
-                ImmutableList.of(
-                    new PartitionInfo("topic2", 0, null, null, null),
-                    new PartitionInfo("topic2", 1, null, null, null))));
-    MockBagState bagState =
-        new MockBagState(
-            ImmutableList.of(
-                new TopicPartition("topic1", 0),
-                new TopicPartition("topic1", 1),
-                new TopicPartition("topic2", 0),
-                new TopicPartition("topic2", 1)));
-    Instant now = Instant.EPOCH;
-    mockStatic(Instant.class);
-    when(Instant.now()).thenReturn(now);
-
-    dofnInstance.onTimer(timer, bagState, outputReceiver);
-
-    verify(timer, times(1)).set(now.plus(Duration.millis(600L)));
-    Set<TopicPartition> expectedCurrentTopicPartitions =
-        ImmutableSet.of(
-            new TopicPartition("topic1", 0),
-            new TopicPartition("topic2", 0),
-            new TopicPartition("topic2", 1));
-    assertTrue(outputReceiver.getOutputs().isEmpty());
-    assertEquals(expectedCurrentTopicPartitions, bagState.getCurrentStates());
-  }
-
-  @Test
-  public void testOnTimerWithStoppedTopicPartitions() throws Exception {
-    Instant startReadTime = Instant.ofEpochMilli(1L);
-    SerializableFunction<TopicPartition, Boolean> checkStopReadingFn =
-        new SerializableFunction<TopicPartition, Boolean>() {
-          @Override
-          public Boolean apply(TopicPartition input) {
-            if (input.equals(new TopicPartition("topic1", 1))) {
-              return true;
-            }
-            return false;
-          }
-        };
-    WatchKafkaTopicPartitionDoFn dofnInstance =
-        new WatchKafkaTopicPartitionDoFn(
-            Duration.millis(600L),
-            consumerFn,
-            checkStopReadingFn,
-            ImmutableMap.of(),
-            startReadTime,
-            null,
-            null);
-    MockOutputReceiver outputReceiver = new MockOutputReceiver();
-
-    when(mockConsumer.listTopics())
-        .thenReturn(
-            ImmutableMap.of(
-                "topic1",
-                ImmutableList.of(
-                    new PartitionInfo("topic1", 0, null, null, null),
-                    new PartitionInfo("topic1", 1, null, null, null)),
-                "topic2",
-                ImmutableList.of(
-                    new PartitionInfo("topic2", 0, null, null, null),
-                    new PartitionInfo("topic2", 1, null, null, null))));
-    MockBagState bagState =
-        new MockBagState(
-            ImmutableList.of(
-                new TopicPartition("topic1", 0),
-                new TopicPartition("topic2", 0),
-                new TopicPartition("topic2", 1)));
-    Instant now = Instant.EPOCH;
-    mockStatic(Instant.class);
-    when(Instant.now()).thenReturn(now);
-
-    dofnInstance.onTimer(timer, bagState, outputReceiver);
-
-    Set<TopicPartition> expectedCurrentTopicPartitions =
-        ImmutableSet.of(
-            new TopicPartition("topic1", 0),
-            new TopicPartition("topic2", 0),
-            new TopicPartition("topic2", 1));
-
-    verify(timer, times(1)).set(now.plus(Duration.millis(600L)));
-    assertTrue(outputReceiver.getOutputs().isEmpty());
-    assertEquals(expectedCurrentTopicPartitions, bagState.getCurrentStates());
-  }
-
-  private static class MockOutputReceiver implements OutputReceiver<KafkaSourceDescriptor> {
-
-    private List<KafkaSourceDescriptor> outputs = new ArrayList<>();
-
-    @Override
-    public void output(KafkaSourceDescriptor output) {
-      outputs.add(output);
-    }
-
-    @Override
-    public void outputWithTimestamp(KafkaSourceDescriptor output, Instant timestamp) {}
-
-    public List<KafkaSourceDescriptor> getOutputs() {
-      return outputs;
-    }
-  }
-
-  private static class MockBagState implements BagState<TopicPartition> {
-    private Set<TopicPartition> topicPartitions = new HashSet<>();
-
-    MockBagState(List<TopicPartition> readReturn) {
-      topicPartitions.addAll(readReturn);
-    }
-
-    @Override
-    public Iterable<TopicPartition> read() {
-      return topicPartitions;
-    }
-
-    @Override
-    public void add(TopicPartition value) {
-      topicPartitions.add(value);
-    }
-
-    @Override
-    public ReadableState<Boolean> isEmpty() {
-      return null;
-    }
-
-    @Override
-    public BagState<TopicPartition> readLater() {
-      return null;
-    }
-
-    @Override
-    public void clear() {
-      topicPartitions.clear();
-    }
-
-    public Set<TopicPartition> getCurrentStates() {
-      return topicPartitions;
-    }
-  }
-
-  private Set<KafkaSourceDescriptor> generateDescriptorsFromTopicPartitions(
-      Set<TopicPartition> topicPartitions, Instant startReadTime) {
-    return topicPartitions.stream()
-        .map(
-            topicPartition ->
-                KafkaSourceDescriptor.of(topicPartition, null, startReadTime, null, null, null))
-        .collect(Collectors.toSet());
-  }
-}
