<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 06:33:47 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF Jira</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[AMQ-4495] Improve cursor memory management</title>
                <link>https://issues.apache.org/jira/browse/AMQ-4495</link>
                <project id="12311210" key="AMQ">ActiveMQ Classic</project>
                    <description>&lt;p&gt;As currently stands, the store queue cursor will cache producer messages until it gets to the 70% (high watermark) of its usage. After that caching stops and messages goes only in store. When consumers comes, messages get dispatched to it, but memory isn&apos;t released until they are acked. The problem is with the use case where producer flow control is off and we have a prefetch large enough to get all our messages from the cache. Then, basically the cursor gets empty and as message acks release memory one by one, we go to the store and try to batch one message at the time. You can guess that things start to be really slow at that point. &lt;/p&gt;

&lt;p&gt;The solution for this scenario is to wait with batching until we have more space so that store access is optimized. We can do this by adding a new limit (smaller then the high watermark) which will be used as the limit after which we start filling cursor from the store again.&lt;/p&gt;

&lt;p&gt;All this led us to the following questions:&lt;/p&gt;

&lt;p&gt;1. Why do we use 70% as the limit (instead of 100%) when we stop caching producer messages?&lt;/p&gt;

&lt;p&gt;2. Would a solution that stop caching producer messages at 100% of usage and then start batching messages from the store when usage drops below high watermark value be enough. Of course, high watermark would be configurable, but 100% by default so we don&apos;t alter any behavior for regular use cases.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12645235">AMQ-4495</key>
            <summary>Improve cursor memory management</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="gtully">Gary Tully</assignee>
                                    <reporter username="dejanb">Dejan Bosanac</reporter>
                        <labels>
                    </labels>
                <created>Mon, 29 Apr 2013 13:44:21 +0000</created>
                <updated>Fri, 13 Sep 2019 15:12:39 +0000</updated>
                            <resolved>Tue, 8 Mar 2016 13:49:53 +0000</resolved>
                                                    <fixVersion>5.9.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="13647354" author="wangyin" created="Thu, 2 May 2013 06:54:16 +0000"  >&lt;p&gt;Before we can enjoy the improvement,is the optimization acknowledge option for consumers able to rescue the poor performance b/w of loading messages one by one from the store?&lt;/p&gt;</comment>
                            <comment id="13647381" author="dejanb" created="Thu, 2 May 2013 08:26:32 +0000"  >&lt;p&gt;Unfortunately not, as memory is released one message at the time even in that case (which triggers new store batch and basically just load one message)&lt;/p&gt;</comment>
                            <comment id="13647471" author="gtully" created="Thu, 2 May 2013 12:34:50 +0000"  >&lt;p&gt;allowing a batch load from the store to exceed the memory limit may be the simplest approach. The batch size is already configurable so the excess is controllable. In essence, once we go to the store we always load a full batch if we can so we don&apos;t check the usage on each message recovery.&lt;/p&gt;</comment>
                            <comment id="13648476" author="dejanb" created="Fri, 3 May 2013 14:55:08 +0000"  >&lt;p&gt;Fixed with svn revision 1478823&lt;/p&gt;

&lt;p&gt;The solution Gary mentioned has been implemented, basically allowing us to exceed a memory limit a bit for the more optimal store reads.&lt;/p&gt;</comment>
                            <comment id="13650655" author="wangyin" created="Tue, 7 May 2013 09:25:20 +0000"  >&lt;p&gt;there is only 2% performance improvement in my observation.&lt;/p&gt;</comment>
                            <comment id="13650656" author="wangyin" created="Tue, 7 May 2013 09:25:44 +0000"  >&lt;p&gt;I have tried to add a testcase to verify the improvement, but I can only see a little effect.&lt;br/&gt;
It would be better you can take a look at the testcase and give it a try to see the effect.&lt;/p&gt;</comment>
                            <comment id="13651765" author="wangyin" created="Wed, 8 May 2013 10:27:30 +0000"  >&lt;p&gt;More concurrence tests indicate load on the database server decreases noticeably than the original design when the memory usage of queues is exhausted.&lt;br/&gt;
Seems we can not see the dispatch speed increases while the load of the store server is lower than before.&lt;/p&gt;</comment>
                            <comment id="15172095" author="gtully" created="Mon, 29 Feb 2016 16:34:10 +0000"  >&lt;p&gt;There is a problem with this change. Going past the highWaterMark can have a negative influence on other consumers and destinations leading to the inability to page messages for dispatch.&lt;br/&gt;
This is a general problem for a shared limit but it is made worse with this fix.&lt;br/&gt;
Consider destinations that has stopped caching at 70% usage, so that&lt;br/&gt;
flow control does not kick in and producers can send directly to disk.&lt;br/&gt;
If the cache is less than the pageSize this is particularly problematic.&lt;br/&gt;
As messages page in, the usage goes past 100% and now producers will be flow controlled, rather than still going directly to disk. That is not what we want.&lt;/p&gt;

&lt;p&gt;I am thinking that paging in for dispatch should respect the highWaterMark, so it is not safe to just &quot;always page in a full page&quot;&lt;/p&gt;</comment>
                            <comment id="15173654" author="jira-bot" created="Tue, 1 Mar 2016 11:55:24 +0000"  >&lt;p&gt;Commit d8cf54b0a9eee4b86db1ffef2cb3dd1171067307 in activemq&apos;s branch refs/heads/master from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gtully&quot; class=&quot;user-hover&quot; rel=&quot;gtully&quot;&gt;gtully&lt;/a&gt;&lt;br/&gt;
[ &lt;a href=&quot;https://git-wip-us.apache.org/repos/asf?p=activemq.git;h=d8cf54b&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://git-wip-us.apache.org/repos/asf?p=activemq.git;h=d8cf54b&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/AMQ-4495&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/AMQ-4495&lt;/a&gt; - revisit. Reinstate check for space on pagein, so that highWaterMark is respected and full state is not reached, hense pfc is not triggered in error&lt;/p&gt;</comment>
                            <comment id="15173744" author="christopher.l.shannon" created="Tue, 1 Mar 2016 13:25:24 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gtully&quot; class=&quot;user-hover&quot; rel=&quot;gtully&quot;&gt;gtully&lt;/a&gt;,&lt;/p&gt;

&lt;p&gt;Two questions/comments,&lt;/p&gt;

&lt;p&gt;First, with this new commit, I assume that means there will be reduced performance again? Some of the brokers I&apos;ve run are on machines with relatively slow disk performance so that is a potential concern.&lt;/p&gt;

&lt;p&gt;Second, do you think your new change might reduce potential OOM errors?  I&apos;ve seen out of memory problems occasionally even though proper usage limits are set and I&apos;ve always thought that maybe that had something to do with the fact that paging in on dispatch could load more than 100% of the usage into memory.&lt;/p&gt;</comment>
                            <comment id="15173767" author="gtully" created="Tue, 1 Mar 2016 13:42:26 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cshannon&quot; class=&quot;user-hover&quot; rel=&quot;cshannon&quot;&gt;cshannon&lt;/a&gt; yes to both.&lt;/p&gt;

&lt;p&gt;It was oom that brought me back, then i saw the producer flow control (pfc) use case and was convinced.&lt;/p&gt;

&lt;p&gt;The original perf issue is a case where a consumer takes a page and as the acks come back, the page is empty so we go to the store, take in one message (as we again hit our limit (b/c the prefetched messages are not acked yet) and dispatch it. Then repeat.&lt;/p&gt;

&lt;p&gt;I think the pref issue can be addressed with some taking account of the prefetch, but it may be tricky.&lt;/p&gt;

&lt;p&gt;I am now thinking that we need a high and low water mark. using the low for caching on the producer side, and using the high for page in. That is sort of what we have with the high and full mark at the moment. so maybe there is no need to change much.&lt;br/&gt;
however the full trigger is what causes pfc to kick in so it is not isolated.&lt;br/&gt;
Thinking now, the pfc check could be conditional on the cursor caching messages. That may separate the concerns some which would help simplify.&lt;br/&gt;
Need to investigate that a bit. Thanks for your eyes on this, it is a tricky area, dispatch on memory limits. Not sure there is a perfect answer but I think it can be improved.&lt;/p&gt;</comment>
                            <comment id="15173781" author="gtully" created="Tue, 1 Mar 2016 13:54:53 +0000"  >&lt;p&gt;The second use case here is starvation, simplest when there is a shared memory limit, so just a broker usage limit.&lt;br/&gt;
Send to q1, it stops caching at &amp;lt; 70%. Send to q2, it stops caching immediately, same for q3.&lt;br/&gt;
Consume from q2, you get to page in some messages, brings you possibly &amp;gt; 70%, consume from q3, you get nothing, till q2 or q1 gets some acks.&lt;/p&gt;

&lt;p&gt;This is all expected, but with the usage based on %, and varying message size it is very random.&lt;/p&gt;</comment>
                            <comment id="15183230" author="jira-bot" created="Mon, 7 Mar 2016 16:35:51 +0000"  >&lt;p&gt;Commit 13ec9949397848c57653845b35e8003f8c490ebd in activemq&apos;s branch refs/heads/master from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gtully&quot; class=&quot;user-hover&quot; rel=&quot;gtully&quot;&gt;gtully&lt;/a&gt;&lt;br/&gt;
[ &lt;a href=&quot;https://git-wip-us.apache.org/repos/asf?p=activemq.git;h=13ec994&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://git-wip-us.apache.org/repos/asf?p=activemq.git;h=13ec994&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;Revert &quot;https://issues.apache.org/jira/browse/AMQ-4495 - revisit. Reinstate check for space on pagein, so that highWaterMark is respected and full state is not reached, hense pfc is not triggered in error&quot;&lt;/p&gt;

&lt;p&gt;This reverts commit d8cf54b0a9eee4b86db1ffef2cb3dd1171067307.&lt;/p&gt;</comment>
                            <comment id="15183267" author="gtully" created="Mon, 7 Mar 2016 17:01:11 +0000"  >&lt;p&gt;I have not found a straight forward way to deal with performance issue so I am reverting. Going to tackle the flow control issue on it own.&lt;/p&gt;</comment>
                            <comment id="15184926" author="gtully" created="Tue, 8 Mar 2016 13:49:53 +0000"  >&lt;p&gt;On review - i have retained the status quo. The performance implications of going to the store for a batch (page) of messages and dropping them due to no space is too high.&lt;/p&gt;

&lt;p&gt;Using a destination limit of X, and a cursorHighWaterMark of 70, only if usage is &amp;lt; 0.7*X will we page in from the store. If we page In, it will be a pageSize and we will accept the full page if a full page of messages exists. This may cause usage to increase past X. However we won&apos;t go back to page in till usage is again &amp;lt; 0.7*X.&lt;br/&gt;
The 0.7 is the cursorHighWaterMark and X is the memoryUsage (via policy entries). PageSize is also configurable.&lt;/p&gt;

&lt;p&gt;If per destinations limits are not set, then the global shared usage is available to all destinations which can lead to starvation of consumers due to the inability to page in because usage &amp;gt; 0.7*X due to messages in memory for other destinations. Their consumers may have dropped off for example.&lt;/p&gt;

&lt;p&gt;To divide a shared usage between N destinations:&lt;br/&gt;
 0.7*X will be used for caching if the cache is enabled (useCache=true). &lt;br/&gt;
Then be prepared for potential increase of pageSize*messageSize usage when we page in from the store for dispatch. This happens when the cache is exhausted and usage drops below 0.7*X. Ideally, the 0.3*X that remains for pageIn is &amp;lt; pageSize*messageSize. &lt;br/&gt;
For N destinations, the fraction available to each destinations needs to account for the above.&lt;br/&gt;
This will ensure that the global shared usage is respected so the JVM heap metrics can be sized sensibly.&lt;/p&gt;

&lt;p&gt;Mitigation:&lt;br/&gt;
lazyDispatch - where we only page in what can be consumed can help limit the pageSize dynamically if consumers are exact about their prefetch limit or use pull consumers. With 3 pull consumers, we will only pageIn 3 messages if lazyDispatch=true&lt;/p&gt;</comment>
                            <comment id="15185312" author="christopher.l.shannon" created="Tue, 8 Mar 2016 17:30:27 +0000"  >&lt;p&gt;Yes, this is another issue too. The size computation is currently an approximation as you pointed out in that ticket.  Another thing that helped in the queue case with OOM errors was setting the reduceMemoryFootprint flag to true.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12582066" name="FasterDispatchTest.java" size="3538" author="wangyin" created="Tue, 7 May 2013 09:25:20 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>325597</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 37 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1k68n:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>325942</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310080" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Regression</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10091"><![CDATA[Regression]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            </customfields>
    </item>
</channel>
</rss>