<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 04:12:50 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SOLR-8914] ZkStateReader&apos;s refreshLiveNodes(Watcher) is not thread safe</title>
                <link>https://issues.apache.org/jira/browse/SOLR-8914</link>
                <project id="12310230" key="SOLR">Solr</project>
                    <description>&lt;p&gt;Jenkin&apos;s encountered a failure in TestTolerantUpdateProcessorCloud over the weekend....&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;http://jenkins.thetaphi.de/job/Lucene-Solr-6.x-Solaris/32/consoleText
Checking out Revision c46d7686643e7503304cb35dfe546bce9c6684e7 (refs/remotes/origin/branch_6x)
Using Java: 64bit/jdk1.8.0 -XX:+UseCompressedOops -XX:+UseG1GC
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The failure happened during the static setup of the test, when a MiniSolrCloudCluster &amp;amp; several clients are initialized &amp;#8211; before any code related to TolerantUpdateProcessor is ever used.&lt;/p&gt;

&lt;p&gt;I can&apos;t reproduce this, or really make sense of what i&apos;m (not) seeing here in the logs, so i&apos;m filing this jira with my analysis in the hopes that someone else can help make sense of it.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12954025">SOLR-8914</key>
            <summary>ZkStateReader&apos;s refreshLiveNodes(Watcher) is not thread safe</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="dragonsinth">Scott Blum</assignee>
                                    <reporter username="hossman">Chris M. Hostetter</reporter>
                        <labels>
                    </labels>
                <created>Mon, 28 Mar 2016 18:12:31 +0000</created>
                <updated>Wed, 2 Oct 2019 17:23:26 +0000</updated>
                            <resolved>Wed, 20 Apr 2016 23:19:08 +0000</resolved>
                                    <version>5.4.1</version>
                    <version>5.5</version>
                    <version>6.0</version>
                                    <fixVersion>5.5.1</fixVersion>
                    <fixVersion>5.6</fixVersion>
                    <fixVersion>6.0.1</fixVersion>
                    <fixVersion>6.1</fixVersion>
                    <fixVersion>7.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>8</watches>
                                                                                                                <comments>
                            <comment id="15214609" author="hossman" created="Mon, 28 Mar 2016 18:15:38 +0000"  >
&lt;p&gt;The specific assertion that failed had to do with a sanity checking query for 2 docs that are added to ensure that some compositId routing prefixes used throughout the tests do actually corrispond to the expected shard names &amp;#8211; but for the purpose of discussing this particular jenkins failure, the key things to know about this test failure are...&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;there are 2 shards w/ repfactor=2 which are inited successfully &amp;amp; not undergoing recovery&lt;/li&gt;
	&lt;li&gt;2 docs are (successful) added, with prefixes such that one is located on each shard&lt;/li&gt;
	&lt;li&gt;a hard commit is (sucessfully) executed from the perspective of the CloudSolrClient
	&lt;ul&gt;
		&lt;li&gt;&lt;font color=&quot;red&quot;&gt;but only shard1 logs the commit &amp;amp; newSearcher&lt;/font&gt;&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;a &lt;tt&gt;&amp;#42;:&amp;#42;&lt;/tt&gt; query is executed...
	&lt;ul&gt;
		&lt;li&gt;only 1 doc is found by this query&lt;/li&gt;
		&lt;li&gt;an error is logged by a shard1 node that &lt;font color=&quot;red&quot;&gt;no servers hosting shard: shard2&lt;/font&gt;&lt;/li&gt;
		&lt;li&gt;&lt;font color=&quot;red&quot;&gt;a shard2 node does in fact log that it was consulted as part of this query&lt;/font&gt;&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Which raises 2 key questions I can&apos;t make sense of...&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Why didn&apos;t either shard2 node ever log (and execute) the commit &amp;amp; opening of a new searcher?&lt;/li&gt;
	&lt;li&gt;Why did a shard1 node log an error that &quot;no servers hosting shard: shard2&quot; even though the shard2&apos;s core_node2 clearly received the first phase of the distributed request &amp;amp; responded with a success?&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;I&apos;ve attached the full log file, and provided my detailed notes, line by line (of both the relevant code &amp;amp; the log file) below - NOTE: all &quot;Lxx&quot; line numbers refer to GIT SHA c46d768 since that&apos;s what jenkins was testing)&lt;/p&gt;

&lt;hr /&gt;

&lt;ul&gt;
	&lt;li&gt;TestTolerantUpdateProcessorCloud.createMiniSolrCloudCluster...
	&lt;ul&gt;
		&lt;li&gt;begins by creating a MiniSolrCloudCluster &amp;amp; initializes a collection of 5 nodes with 2 shards and refactor=2&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;timestamp #1302667 - #1302668
	&lt;ul&gt;
		&lt;li&gt;TestTolerantUpdateProcessorCloud.createMiniSolrCloudCluster @ L129&lt;/li&gt;
		&lt;li&gt;call to AbstractDistribZkTestBase.waitForRecoveriesToFinish&lt;/li&gt;
		&lt;li&gt;can see from the log that all 4 nodes are good to go...
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;1302667 INFO  (SUITE-TestTolerantUpdateProcessorCloud-seed#[F0FBD4E7705C29B5]-worker) [    ] o.a.s.c.AbstractDistribZkTestBase Wait for recoveries to finish - collection: test_col failOnTimeout:true timeout (sec):330
-
replica:core_node1 rstate:active live:true
replica:core_node4 rstate:active live:true
replica:core_node2 rstate:active live:true
replica:core_node3 rstate:active live:true
no one is recoverying
1302668 INFO  (SUITE-TestTolerantUpdateProcessorCloud-seed#[F0FBD4E7705C29B5]-worker) [    ] o.a.s.c.AbstractDistribZkTestBase Recoveries finished - collection: test_col
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;TestTolerantUpdateProcessorCloud.createMiniSolrCloudCluster @ L132 - L183
	&lt;ul&gt;
		&lt;li&gt;test is inspecting ZK state to init some HTTP SolrClients pointed at the URLs for specific leaders &amp;amp; replicas (these are used to test all possible forwarding situations in the various test methods)&lt;/li&gt;
		&lt;li&gt;timestamp #1302671 shows ZkStateReader used refreshing it&apos;s live nodes info for this purpose (L141)&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;TestTolerantUpdateProcessorCloud.createMiniSolrCloudCluster @ L184 - L188
	&lt;ul&gt;
		&lt;li&gt;adding 2 docs with specific routing prefixes&lt;/li&gt;
		&lt;li&gt;ultimate goal is to sanity check that the route prefixes match to the expected shards so other parts of test can assume this.&lt;/li&gt;
		&lt;li&gt;timestamp #1302696 - #1302719 record these 2 docs being added to both replicas of shard1 &amp;amp; shard2...
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;1302696 INFO  (qtp1904355206-7333) [n:127.0.0.1:34275_solr c:test_col s:shard1 r:core_node4 x:test_col_shard1_replica2] o.a.s.u.p.LogUpdateProcessorFactory [test_col_shard1_replica2]  webapp=/solr path=/update params={update.distrib=FROMLEADER&amp;amp;distrib.from=http://127.0.0.1:56361/solr/test_col_shard1_replica1/&amp;amp;wt=javabin&amp;amp;version=2}{add=[abc!447049285 (1529857595576156160)]} 0 4
1302697 INFO  (qtp1409631559-7341) [n:127.0.0.1:56361_solr c:test_col s:shard1 r:core_node1 x:test_col_shard1_replica1] o.a.s.u.p.LogUpdateProcessorFactory [test_col_shard1_replica1]  webapp=/solr path=/update params={wt=javabin&amp;amp;version=2}{add=[abc!447049285 (1529857595576156160)]} 0 14
1302719 INFO  (qtp797763926-7371) [n:127.0.0.1:51144_solr c:test_col s:shard2 r:core_node3 x:test_col_shard2_replica1] o.a.s.u.p.LogUpdateProcessorFactory [test_col_shard2_replica1]  webapp=/solr path=/update params={update.distrib=FROMLEADER&amp;amp;distrib.from=http://127.0.0.1:33616/solr/test_col_shard2_replica2/&amp;amp;wt=javabin&amp;amp;version=2}{add=[XYZ!701047101 (1529857595596079104)]} 0 7
1302719 INFO  (qtp316491708-7355) [n:127.0.0.1:33616_solr c:test_col s:shard2 r:core_node2 x:test_col_shard2_replica2] o.a.s.u.p.LogUpdateProcessorFactory [test_col_shard2_replica2]  webapp=/solr path=/update params={wt=javabin&amp;amp;version=2}{add=[XYZ!701047101 (1529857595596079104)]} 0 18
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;TestTolerantUpdateProcessorCloud.createMiniSolrCloudCluster @ L189
	&lt;ul&gt;
		&lt;li&gt;do a commit using the CloudSolrClient&lt;/li&gt;
		&lt;li&gt;timestamps #1302724 - #1302733 show both replicas of shard1 acting on this commit &amp;amp; opening new searchers...
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;1302724 INFO  (qtp1409631559-7343) [n:127.0.0.1:56361_solr c:test_col s:shard1 r:core_node1 x:test_col_shard1_replica1] o.a.s.u.DirectUpdateHandler2 start commit{,optimize=false,openSearcher=true,waitSearcher=true,expungeDeletes=false,softCommit=false,prepareCommit=false}
1302724 INFO  (qtp1904355206-7330) [n:127.0.0.1:34275_solr c:test_col s:shard1 r:core_node4 x:test_col_shard1_replica2] o.a.s.u.DirectUpdateHandler2 start commit{,optimize=false,openSearcher=true,waitSearcher=true,expungeDeletes=false,softCommit=false,prepareCommit=false}
1302727 INFO  (qtp1904355206-7330) [n:127.0.0.1:34275_solr c:test_col s:shard1 r:core_node4 x:test_col_shard1_replica2] o.a.s.c.SolrDeletionPolicy SolrDeletionPolicy.onCommit: commits: num=2
	commit{dir=MockDirectoryWrapper(RAMDirectory@21c6a821 lockFactory=org.apache.lucene.store.SingleInstanceLockFactory@26734241),segFN=segments_1,generation=1}
	commit{dir=MockDirectoryWrapper(RAMDirectory@21c6a821 lockFactory=org.apache.lucene.store.SingleInstanceLockFactory@26734241),segFN=segments_2,generation=2}
1302727 INFO  (qtp1409631559-7343) [n:127.0.0.1:56361_solr c:test_col s:shard1 r:core_node1 x:test_col_shard1_replica1] o.a.s.c.SolrDeletionPolicy SolrDeletionPolicy.onCommit: commits: num=2
	commit{dir=MockDirectoryWrapper(RAMDirectory@5616141f lockFactory=org.apache.lucene.store.SingleInstanceLockFactory@6180bc4c),segFN=segments_1,generation=1}
	commit{dir=MockDirectoryWrapper(RAMDirectory@5616141f lockFactory=org.apache.lucene.store.SingleInstanceLockFactory@6180bc4c),segFN=segments_2,generation=2}
1302728 INFO  (qtp1904355206-7330) [n:127.0.0.1:34275_solr c:test_col s:shard1 r:core_node4 x:test_col_shard1_replica2] o.a.s.c.SolrDeletionPolicy newest commit generation = 2
1302729 INFO  (qtp1409631559-7343) [n:127.0.0.1:56361_solr c:test_col s:shard1 r:core_node1 x:test_col_shard1_replica1] o.a.s.c.SolrDeletionPolicy newest commit generation = 2
1302730 INFO  (qtp1409631559-7343) [n:127.0.0.1:56361_solr c:test_col s:shard1 r:core_node1 x:test_col_shard1_replica1] o.a.s.s.SolrIndexSearcher Opening [Searcher@f331408[test_col_shard1_replica1] main]
1302730 INFO  (qtp1904355206-7330) [n:127.0.0.1:34275_solr c:test_col s:shard1 r:core_node4 x:test_col_shard1_replica2] o.a.s.s.SolrIndexSearcher Opening [Searcher@5d1967dd[test_col_shard1_replica2] main]
1302730 INFO  (qtp1409631559-7343) [n:127.0.0.1:56361_solr c:test_col s:shard1 r:core_node1 x:test_col_shard1_replica1] o.a.s.u.DirectUpdateHandler2 end_commit_flush
1302730 INFO  (qtp1904355206-7330) [n:127.0.0.1:34275_solr c:test_col s:shard1 r:core_node4 x:test_col_shard1_replica2] o.a.s.u.DirectUpdateHandler2 end_commit_flush
1302731 INFO  (searcherExecutor-3283-thread-1-processing-n:127.0.0.1:56361_solr x:test_col_shard1_replica1 s:shard1 c:test_col r:core_node1) [n:127.0.0.1:56361_solr c:test_col s:shard1 r:core_node1 x:test_col_shard1_replica1] o.a.s.c.SolrCore [test_col_shard1_replica1] Registered new searcher Searcher@f331408[test_col_shard1_replica1] main{ExitableDirectoryReader(UninvertingDirectoryReader(Uninverting(_0(6.1.0):c1)))}
1302732 INFO  (qtp1409631559-7343) [n:127.0.0.1:56361_solr c:test_col s:shard1 r:core_node1 x:test_col_shard1_replica1] o.a.s.u.p.LogUpdateProcessorFactory [test_col_shard1_replica1]  webapp=/solr path=/update params={update.distrib=FROMLEADER&amp;amp;waitSearcher=true&amp;amp;openSearcher=true&amp;amp;commit=true&amp;amp;softCommit=false&amp;amp;distrib.from=http://127.0.0.1:56361/solr/test_col_shard1_replica1/&amp;amp;commit_end_point=true&amp;amp;wt=javabin&amp;amp;version=2&amp;amp;expungeDeletes=false}{commit=} 0 7
1302732 INFO  (searcherExecutor-3284-thread-1-processing-n:127.0.0.1:34275_solr x:test_col_shard1_replica2 s:shard1 c:test_col r:core_node4) [n:127.0.0.1:34275_solr c:test_col s:shard1 r:core_node4 x:test_col_shard1_replica2] o.a.s.c.SolrCore [test_col_shard1_replica2] Registered new searcher Searcher@5d1967dd[test_col_shard1_replica2] main{ExitableDirectoryReader(UninvertingDirectoryReader(Uninverting(_0(6.1.0):c1)))}
1302732 INFO  (qtp1904355206-7330) [n:127.0.0.1:34275_solr c:test_col s:shard1 r:core_node4 x:test_col_shard1_replica2] o.a.s.u.p.LogUpdateProcessorFactory [test_col_shard1_replica2]  webapp=/solr path=/update params={update.distrib=FROMLEADER&amp;amp;waitSearcher=true&amp;amp;openSearcher=true&amp;amp;commit=true&amp;amp;softCommit=false&amp;amp;distrib.from=http://127.0.0.1:56361/solr/test_col_shard1_replica1/&amp;amp;commit_end_point=true&amp;amp;wt=javabin&amp;amp;version=2&amp;amp;expungeDeletes=false}{commit=} 0 8
1302733 INFO  (qtp1409631559-7345) [n:127.0.0.1:56361_solr c:test_col s:shard1 r:core_node1 x:test_col_shard1_replica1] o.a.s.u.p.LogUpdateProcessorFactory [test_col_shard1_replica1]  webapp=/solr path=/update params={_stateVer_=test_col:7&amp;amp;waitSearcher=true&amp;amp;commit=true&amp;amp;softCommit=false&amp;amp;wt=javabin&amp;amp;version=2}{commit=} 0 11
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
		&lt;li&gt;&lt;font color=&quot;red&quot;&gt;What the hell hapened to shard2 ?! why isn&apos;t shard#2 logging anything here?!&lt;/font&gt;&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;TestTolerantUpdateProcessorCloud.createMiniSolrCloudCluster @ L190
	&lt;ul&gt;
		&lt;li&gt;do a &lt;tt&gt;&amp;#42;:&amp;#42;&lt;/tt&gt; query using the CloudSolrClient&lt;/li&gt;
		&lt;li&gt;goal is to assert that the &lt;tt&gt;[shard]&lt;/tt&gt; value for each of our doc (prefixes) matches the expectation the rest of the test will have&lt;/li&gt;
		&lt;li&gt;timestamp #1302737 shows an error log from a shard1 node indicating that both replicas of shard2 have just vanished completley...
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;1302737 ERROR (qtp1409631559-7347) [n:127.0.0.1:56361_solr c:test_col s:shard1 r:core_node1 x:test_col_shard1_replica1] o.a.s.h.RequestHandlerBase org.apache.solr.common.SolrException: no servers hosting shard: shard2
	at org.apache.solr.handler.component.HttpShardHandler.prepDistributed(HttpShardHandler.java:451)
	at org.apache.solr.handler.component.SearchHandler.getAndPrepShardHandler(SearchHandler.java:224)
	at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:262)
	at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:155)
	at org.apache.solr.core.SolrCore.execute(SolrCore.java:2033)
	at org.apache.solr.servlet.HttpSolrCall.execute(HttpSolrCall.java:652)
	at org.apache.solr.servlet.HttpSolrCall.call(HttpSolrCall.java:460)
	at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:229)
	at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:184)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1676)
	at org.apache.solr.client.solrj.embedded.JettySolrRunner$DebugFilter.doFilter(JettySolrRunner.java:109)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1676)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:581)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:224)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1160)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:511)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1092)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:462)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.eclipse.jetty.server.Server.handle(Server.java:518)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:308)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:244)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:273)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:95)
	at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceAndRun(ExecuteProduceConsume.java:246)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:156)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:654)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:745)

1302737 INFO  (qtp1409631559-7347) [n:127.0.0.1:56361_solr c:test_col s:shard1 r:core_node1 x:test_col_shard1_replica1] o.a.s.c.S.Request [test_col_shard1_replica1]  webapp=/solr path=/select params={q=*:*&amp;amp;_stateVer_=test_col:7&amp;amp;fl=id,expected_shard_s,[shard]&amp;amp;wt=javabin&amp;amp;version=2} status=503 QTime=0
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
		&lt;li&gt;but 5ms latter, timestamp #1302742 shows shard2&apos;s core_node2 logging it&apos;s successful response to the initial phase of the distributed query (although with &lt;tt&gt;hits=0&lt;/tt&gt; since it never processed the commit or opened a new searcher after the documents where added) ...
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;1302742 INFO  (qtp316491708-7357) [n:127.0.0.1:33616_solr c:test_col s:shard2 r:core_node2 x:test_col_shard2_replica2] o.a.s.c.S.Request [test_col_shard2_replica2]  webapp=/solr path=/select params={distrib=false&amp;amp;_stateVer_=test_col:7&amp;amp;fl=id&amp;amp;fl=score&amp;amp;shards.purpose=4&amp;amp;start=0&amp;amp;fsv=true&amp;amp;shard.url=http://127.0.0.1:33616/solr/test_col_shard2_replica2/|http://127.0.0.1:51144/solr/test_col_shard2_replica1/&amp;amp;rows=10&amp;amp;version=2&amp;amp;q=*:*&amp;amp;NOW=1458985896717&amp;amp;isShard=true&amp;amp;wt=javabin} hits=0 status=0 QTime=0
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;The test ultimately fails at because the &lt;tt&gt;&amp;#42;:&amp;#42;&lt;/tt&gt; sanity check of numDocs=2 fails
	&lt;ul&gt;
		&lt;li&gt;TestTolerantUpdateProcessorCloud.createMiniSolrCloudCluster @ L192&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;other then the &lt;tt&gt;hits=0&lt;/tt&gt; logging mentioned above (#1302742), there is no logging from either of the shard2 nodes between timestamp #1302719 when both replicas accept the doc add, and #1302758 when the test starts shutting down nodes and closing cores.&lt;/li&gt;
&lt;/ul&gt;

</comment>
                            <comment id="15215090" author="dragonsinth" created="Mon, 28 Mar 2016 23:31:36 +0000"  >&lt;p&gt;I see something weird in there:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;   [junit4]   2&amp;gt; 1301674 INFO  (zkCallback-1136-thread-2-processing-n:127.0.0.1:34275_solr) [n:127.0.0.1:34275_solr    ] o.a.s.c.c.ZkStateReader A cluster state change: [WatchedEvent state:SyncConnected type:NodeDataChanged path:/collections/test_col/state.json] &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; collection [test_col] has occurred - updating... (live nodes size: [5])
   [junit4]   2&amp;gt; 1301674 INFO  (zkCallback-1143-thread-1-processing-n:127.0.0.1:33616_solr) [n:127.0.0.1:33616_solr    ] o.a.s.c.c.ZkStateReader A cluster state change: [WatchedEvent state:SyncConnected type:NodeDataChanged path:/collections/test_col/state.json] &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; collection [test_col] has occurred - updating... (live nodes size: [5])
   [junit4]   2&amp;gt; 1301674 INFO  (zkCallback-1135-thread-2-processing-n:127.0.0.1:56361_solr) [n:127.0.0.1:56361_solr    ] o.a.s.c.c.ZkStateReader A cluster state change: [WatchedEvent state:SyncConnected type:NodeDataChanged path:/collections/test_col/state.json] &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; collection [test_col] has occurred - updating... (live nodes size: [3])
   [junit4]   2&amp;gt; 1301675 INFO  (zkCallback-1142-thread-2-processing-n:127.0.0.1:51144_solr) [n:127.0.0.1:51144_solr    ] o.a.s.c.c.ZkStateReader A cluster state change: [WatchedEvent state:SyncConnected type:NodeDataChanged path:/collections/test_col/state.json] &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; collection [test_col] has occurred - updating... (live nodes size: [5])
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Why is one node seeing a live_nodes size 3, and the others seeing size 5? How many are there supposed to be?  I&apos;ve suspected for a while there&apos;s some weirdness involving live_nodes tracking, but I haven&apos;t been able to pin it down.&lt;/p&gt;</comment>
                            <comment id="15215237" author="hossman" created="Tue, 29 Mar 2016 01:53:08 +0000"  >&lt;p&gt;Nice catch Scott!&lt;/p&gt;

&lt;p&gt;FWIW, here&apos;s a one liner that pulls out all the &quot;live nodes size&quot; messages from the log and distills it down to just the timeestamp, threadIds (showing the port# of the jetty instance) and the current live node count...&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;grep &quot;live nodes size&quot; jenkins.thetaphi.de_Lucene-Solr-6.x-Solaris_32.log.txt | perl -ple &apos;s/.*2&amp;gt; (\d+).*?\((.*?)\).*?o.a.s.c.c.ZkStateReader.*?live nodes size: (\[\d+\]).*/$1 $2 $3/&apos;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;using the output from that command as a refrence point, we can revist some of my earlier observations...&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;NOTE: there are 5 nodes in this cluster, but only 2 shards x 2 replicas&lt;/li&gt;
	&lt;li&gt;L122: collection creation
	&lt;ul&gt;
		&lt;li&gt;when the 4 cores for the collection are created, the jetty node that does &lt;em&gt;NOT&lt;/em&gt; recieve any core is port #63099...
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;1291030 INFO  (OverseerThreadFactory-3272-thread-1-processing-n:127.0.0.1:34275_solr) [n:127.0.0.1:34275_solr    ] o.a.s.c.OverseerCollectionMessageHandler Creating core test_col_shard1_replica1 as part of shard shard1 of collection test_col on 127.0.0.1:56361_solr
1291031 INFO  (OverseerThreadFactory-3272-thread-1-processing-n:127.0.0.1:34275_solr) [n:127.0.0.1:34275_solr    ] o.a.s.c.OverseerCollectionMessageHandler Creating core test_col_shard2_replica2 as part of shard shard2 of collection test_col on 127.0.0.1:33616_solr
1291038 INFO  (OverseerThreadFactory-3272-thread-1-processing-n:127.0.0.1:34275_solr) [n:127.0.0.1:34275_solr    ] o.a.s.c.OverseerCollectionMessageHandler Creating core test_col_shard2_replica1 as part of shard shard2 of collection test_col on 127.0.0.1:51144_solr
1291041 INFO  (OverseerThreadFactory-3272-thread-1-processing-n:127.0.0.1:34275_solr) [n:127.0.0.1:34275_solr    ] o.a.s.c.OverseerCollectionMessageHandler Creating core test_col_shard1_replica2 as part of shard shard1 of collection test_col on 127.0.0.1:34275_solr
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;L129: call to AbstractDistribZkTestBase.waitForRecoveriesToFinish
	&lt;ul&gt;
		&lt;li&gt;this happens between timestamp #1302667 - #1302668&lt;/li&gt;
		&lt;li&gt;the most recent flurry of log messages mentioning &quot;live nodes size&quot; prior to this timestamp window are...
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;1301787 zkCallback-1136-thread-1-processing-n:127.0.0.1:34275_solr [5]
1301788 zkCallback-1142-thread-2-processing-n:127.0.0.1:51144_solr [5]
1301791 zkCallback-1143-thread-1-processing-n:127.0.0.1:33616_solr [5]
1301791 zkCallback-1135-thread-2-processing-n:127.0.0.1:56361_solr [3]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
		&lt;li&gt;NOTE: these log messages are watches on &lt;tt&gt;path:/collections/test_col/state.json&lt;/tt&gt; which port#63099 has no reason to watch (since it&apos;s not hosting any shards of &lt;tt&gt;test_col&lt;/tt&gt; so it&apos;s not unusual that it&apos;s not logging anything around these times&lt;/li&gt;
		&lt;li&gt;&lt;font color=&quot;red&quot;&gt;why does port #56361 think at this point that there are only 3 live nodes?&lt;/font&gt;&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;L184 - L188: adding docs
	&lt;ul&gt;
		&lt;li&gt;this is done with CloudSolrClient, so each doc goes to it&apos;s respective leader regardless of wether any 1 node has a broken view of live_nodes&lt;/li&gt;
		&lt;li&gt;port #56361 hosts test_col_shard1_replica1 which is aparently the leader of shard1 at this moment&lt;/li&gt;
		&lt;li&gt;the shard1 doc is forwarded correctly to test_col_shard1_replica2 (on port #34275)
		&lt;ul&gt;
			&lt;li&gt;so #34275 is evidently in port #56361&apos;s &quot;view&quot; of live_nodes, and &lt;em&gt;not&lt;/em&gt; one of the &quot;missing&quot; live_nodes&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;L189: commit
	&lt;ul&gt;
		&lt;li&gt;happens during timestamps #1302724 - #1302733&lt;/li&gt;
		&lt;li&gt;(no new &quot;live nodes size&quot; log messages prior to this time window since the ones mentioned previously)&lt;/li&gt;
		&lt;li&gt;IIUC CloudSolrClient correctly, this is considered a &quot;non routable&quot; update, so it will be sent to a (leader) node picked at random&lt;/li&gt;
		&lt;li&gt;from the logs, it looks like CloudSolrClient picked port #56361&lt;/li&gt;
		&lt;li&gt;port #56361&apos;s test_col_shard1_replica1 forwarded to it&apos;s replica test_col_shard1_replica2 on port #34275&lt;/li&gt;
		&lt;li&gt;as mentioned before shard2 never got the update - aparently because port #56361 didn&apos;t think that shard2&apos;s leader was in live_nodes&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;L190: the {{&amp;#42;:&amp;#42;} query
	&lt;ul&gt;
		&lt;li&gt;timestamp #1302737 is when port #56361 (who we know has a bogus view of live_nodes) logs that no servesr are hosting shard2, and returns a 503 error code&lt;/li&gt;
		&lt;li&gt;(there are still no new &quot;live nodes size&quot; log messages prior to this error since the ones mentioned previously)&lt;/li&gt;
		&lt;li&gt;Something that didn&apos;t occur to me before, is that IIRC CloudSolrServertimestamp will retry some error codes &amp;#8211; i&apos;m guessing 503 is one of them, because that would explain the subsequent log messages 5ms later...&lt;/li&gt;
		&lt;li&gt;timestamps #1302742 - 1302746, it appears that port #34275 was queried by CloudSolrClient on retry, and it did know that both replicas of shard2 were alive and queried them (but since they never got the commit, they had numFound=0)
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;1302742 INFO  (qtp316491708-7357) [n:127.0.0.1:33616_solr c:test_col s:shard2 r:core_node2 x:test_col_shard2_replica2] o.a.s.c.S.Request [test_col_shard2_replica2]  webapp=/solr path=/select params={distrib=false&amp;amp;_stateVer_=test_col:7&amp;amp;fl=id&amp;amp;fl=score&amp;amp;shards.purpose=4&amp;amp;start=0&amp;amp;fsv=true&amp;amp;shard.url=http://127.0.0.1:33616/solr/test_col_shard2_replica2/|http://127.0.0.1:51144/solr/test_col_shard2_replica1/&amp;amp;rows=10&amp;amp;version=2&amp;amp;q=*:*&amp;amp;NOW=1458985896717&amp;amp;isShard=true&amp;amp;wt=javabin} hits=0 status=0 QTime=0
1302742 INFO  (qtp1409631559-7346) [n:127.0.0.1:56361_solr c:test_col s:shard1 r:core_node1 x:test_col_shard1_replica1] o.a.s.c.S.Request [test_col_shard1_replica1]  webapp=/solr path=/select params={distrib=false&amp;amp;_stateVer_=test_col:7&amp;amp;fl=id&amp;amp;fl=score&amp;amp;shards.purpose=4&amp;amp;start=0&amp;amp;fsv=true&amp;amp;shard.url=http://127.0.0.1:56361/solr/test_col_shard1_replica1/|http://127.0.0.1:34275/solr/test_col_shard1_replica2/&amp;amp;rows=10&amp;amp;version=2&amp;amp;q=*:*&amp;amp;NOW=1458985896717&amp;amp;isShard=true&amp;amp;wt=javabin} hits=1 status=0 QTime=0
1302745 INFO  (qtp1409631559-7344) [n:127.0.0.1:56361_solr c:test_col s:shard1 r:core_node1 x:test_col_shard1_replica1] o.a.s.c.S.Request [test_col_shard1_replica1]  webapp=/solr path=/select params={q=*:*&amp;amp;distrib=false&amp;amp;_stateVer_=test_col:7&amp;amp;fl=id,expected_shard_s,[shard]&amp;amp;shards.purpose=64&amp;amp;NOW=1458985896717&amp;amp;ids=abc!447049285&amp;amp;isShard=true&amp;amp;shard.url=http://127.0.0.1:56361/solr/test_col_shard1_replica1/|http://127.0.0.1:34275/solr/test_col_shard1_replica2/&amp;amp;wt=javabin&amp;amp;version=2} status=0 QTime=0
1302746 INFO  (qtp1904355206-7332) [n:127.0.0.1:34275_solr c:test_col s:shard1 r:core_node4 x:test_col_shard1_replica2] o.a.s.c.S.Request [test_col_shard1_replica2]  webapp=/solr path=/select params={q=*:*&amp;amp;_stateVer_=test_col:7&amp;amp;fl=id,expected_shard_s,[shard]&amp;amp;wt=javabin&amp;amp;version=2} hits=1 status=0 QTime=7
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;So to sum up: it appears everything i noted before would be easily explained by a some broken code regarding live nodes that only affected port #56361 while all other nodes had a perfecly accurate view of the world.&lt;/p&gt;</comment>
                            <comment id="15215271" author="hossman" created="Tue, 29 Mar 2016 02:12:41 +0000"  >&lt;p&gt;Attaching another interesting view: live_nodes_mentions.log.txt...&lt;/p&gt;

&lt;p&gt;This is every log entry from ZkStateReader mentioning &quot;live nodes&quot; (not just &quot;live nodes size&quot;, so &quot;Updated live nodes from ZooKeeper...&quot; msgs also show up) distilled down to only the timestamp port# (or TEST if it&apos;s from the main testing thread via MiniSolrCloudCluster or waitForRecoveries) and log message.&lt;/p&gt;

&lt;p&gt;Generated via...&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;grep -i &lt;span class=&quot;code-quote&quot;&gt;&quot;live nodes&quot;&lt;/span&gt; jenkins.thetaphi.de_Lucene-Solr-6.x-Solaris_32.log.txt | perl -ple &lt;span class=&quot;code-quote&quot;&gt;&apos;s/.*2&amp;gt; (\d+).*?(\[[^\]]*? \]) o.a.s.c.c.ZkStateReader(.*)/$1 $2 $3/; s/\[n:127.0.0.1:(\d+)_solr\s*?\]/port $1/; s/\[\s+\]/     TEST /&apos;&lt;/span&gt; &amp;gt; live_nodes_mentions.log.txt
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;... use &lt;tt&gt;grep -v &quot;port 63099&quot;&lt;/tt&gt; if you want to only focus on the nodes that host a piece of the collection.&lt;/p&gt;</comment>
                            <comment id="15216972" author="hossman" created="Tue, 29 Mar 2016 22:26:38 +0000"  >
&lt;p&gt;Having read more of &lt;tt&gt;ZkStateReader&lt;/tt&gt;, and understanding a bit better when/why these various log messages are written, it now seems certain to me that ZkStateReader&apos;s callbacks to watching &lt;tt&gt;live_nodes&lt;/tt&gt; and updating that information in the (local) &lt;tt&gt;ClusterState&lt;/tt&gt; is &lt;b&gt;absolutely not thread safe&lt;/b&gt;.&lt;/p&gt;

&lt;p&gt;Key things to note about the ZkStateReader code related to live nodes and live nodes log msgs...&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;The following are examples of log message formats that occur when a ZK &lt;tt&gt;Watcher&lt;/tt&gt;&apos;s process method is triggered - in all of these cases the # of live nodes logged is the &lt;em&gt;current&lt;/em&gt; number, according to local state, prior to processing the Watcher event...
	&lt;ul&gt;
		&lt;li&gt;LegacyClusterStateWatcher (old multi-collections state.json)
		&lt;ul&gt;
			&lt;li&gt;&lt;tt&gt;&quot;A cluster state change: [{}] for collection [{}] has occurred - updating... (live nodes size: [{}])&quot;&lt;/tt&gt;&lt;/li&gt;
			&lt;li&gt;&lt;em&gt;NOTE: this is the current (local) &lt;tt&gt;liveNodes.size()&lt;/tt&gt;, w/ no attempt to refresh &lt;tt&gt;liveNodes&lt;/tt&gt; from ZK&lt;/em&gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
		&lt;li&gt;StateWatcher (per collection state.json)
		&lt;ul&gt;
			&lt;li&gt;&lt;tt&gt;&quot;A cluster state change: [{}] for collection [{}] has occurred - updating... (live nodes size: [{}])&quot;&lt;/tt&gt;&lt;/li&gt;
			&lt;li&gt;&lt;em&gt;NOTE: this is the current (local) &lt;tt&gt;liveNodes.size()&lt;/tt&gt;, w/ no attempt to refresh &lt;tt&gt;liveNodes&lt;/tt&gt; from ZK&lt;/em&gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
		&lt;li&gt;LiveNodeWatcher
		&lt;ul&gt;
			&lt;li&gt;&lt;tt&gt;&quot;A live node change: [{}], has occurred - updating... (live nodes size: [{}])&quot;&lt;/tt&gt;&lt;/li&gt;
			&lt;li&gt;&lt;b&gt;NOTE: This the &quot;old&quot; value when a LiveNodesWatcher is triggered, prior to refreshing from ZK&lt;/b&gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;The following is what gets logged &lt;em&gt;after&lt;/em&gt; the local cache of the live nodes data has been updated (either by an explicit method call to fetch the data from ZK on startup, or because a LiveNodeWatcher was triggered by ZK)
	&lt;ul&gt;
		&lt;li&gt;&lt;tt&gt;&quot;Updated live nodes from ZooKeeper... ({}) -&amp;gt; ({})&quot;&lt;/tt&gt;&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;The full code for how the local cache of &lt;tt&gt;liveNode&lt;/tt&gt; is refreshed from ZK (either by an explifit method call on startup or because a LiveNodeWatcher was triggered) is...
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
  List&amp;lt;&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;&amp;gt; nodeList = zkClient.getChildren(LIVE_NODES_ZKNODE, watcher, &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;);
  newLiveNodes = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; HashSet&amp;lt;&amp;gt;(nodeList);
} &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (KeeperException.NoNodeException e) {
  newLiveNodes = emptySet();
}
Set&amp;lt;&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;&amp;gt; oldLiveNodes;
&lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; (getUpdateLock()) {
  oldLiveNodes = &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.liveNodes;
  &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.liveNodes = newLiveNodes;
  &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (clusterState != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
    clusterState.setLiveNodes(newLiveNodes);
  }
}
LOG.info(&lt;span class=&quot;code-quote&quot;&gt;&quot;Updated live nodes from ZooKeeper... ({}) -&amp;gt; ({})&quot;&lt;/span&gt;, oldLiveNodes.size(), newLiveNodes.size());
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;With all of that info in mind if we revisi the log file, and focus on the threadIds that log each message for a single port we know is problematic (56361)...&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;grep -i &lt;span class=&quot;code-quote&quot;&gt;&quot;live nodes&quot;&lt;/span&gt; jenkins.thetaphi.de_Lucene-Solr-6.x-Solaris_32.log.txt | grep 127.0.0.1:56361_solr | perl -ple &lt;span class=&quot;code-quote&quot;&gt;&apos;s/.*2&amp;gt; (\d+).*?\((.*?-\d+)(?:-processing-n:127.*?)?\).*o.a.s.c.c.ZkStateReader(.*)/$1 $2 $3/&apos;&lt;/span&gt; &amp;gt; live_node_mentions_port56361_with_threadIds.log.txt
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;...it becomes really easy to spot the problem...&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;1290004 jetty-launcher-1118-thread-5  Updated live nodes from ZooKeeper... (0) -&amp;gt; (0)
1290033 zkCallback-1135-thread-1  A live node change: [WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/live_nodes], has occurred - updating... (live nodes size: [0])
1290047 zkCallback-1135-thread-2  A live node change: [WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/live_nodes], has occurred - updating... (live nodes size: [0])
1290052 zkCallback-1135-thread-3  A live node change: [WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/live_nodes], has occurred - updating... (live nodes size: [0])
1290070 zkCallback-1135-thread-3  Updated live nodes from ZooKeeper... (0) -&amp;gt; (4)
1290071 zkCallback-1135-thread-3  A live node change: [WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/live_nodes], has occurred - updating... (live nodes size: [4])
1290071 zkCallback-1135-thread-3  Updated live nodes from ZooKeeper... (4) -&amp;gt; (5)
1290075 zkCallback-1135-thread-1  Updated live nodes from ZooKeeper... (5) -&amp;gt; (1)
1290076 zkCallback-1135-thread-2  Updated live nodes from ZooKeeper... (1) -&amp;gt; (3)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
	&lt;li&gt;timestamp #1290004 is when jetty is first starting up, and forcibly asks ZK for the list of live nodes.&lt;/li&gt;
	&lt;li&gt;we then see 3 threads being spun up in numeric sequence order, to (start) processing LiveNodeWatcher events&lt;/li&gt;
	&lt;li&gt;thread-3 finishes and updates the live nodes to a list of size &quot;4&quot;&lt;/li&gt;
	&lt;li&gt;thread-3 is then re-used to (start) processing another LiveNodeWatcher event&lt;/li&gt;
	&lt;li&gt;thread-3 finishes again and updates the live nodes to a list of size &quot;5&quot;&lt;/li&gt;
	&lt;li&gt;thread-1 now finally finishes, and overwrites the live node list with a list of size &quot;1&quot; &lt;b&gt;even though it&apos;s data is almost certainly old&lt;/b&gt;&lt;/li&gt;
	&lt;li&gt;thread-1 now finally finishes, and also overwrites the live node list with a list of size &quot;3&quot; &lt;b&gt;even though this data is almost certainly also old&lt;/b&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Filling in the blanks, a valid sequence of events that fits the code and matches the logs could have been (colors matchg T# thread id)...&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;&lt;font color=&quot;blue&quot;&gt;T1 &lt;tt&gt;process()&lt;/tt&gt; called, logs &quot;A live node change: ...&quot;&lt;/font&gt;&lt;/li&gt;
	&lt;li&gt;&lt;font color=&quot;blue&quot;&gt;T1 &lt;tt&gt;zkClient.getChildren(...)&lt;/tt&gt; returns a list of 1 live nodes&lt;/font&gt;&lt;/li&gt;
	&lt;li&gt;&lt;font color=&quot;red&quot;&gt;T2 &lt;tt&gt;process()&lt;/tt&gt; called, logs &quot;A live node change: ...&quot;&lt;/font&gt;&lt;/li&gt;
	&lt;li&gt;&lt;font color=&quot;green&quot;&gt;T3 &lt;tt&gt;process()&lt;/tt&gt; called, logs &quot;A live node change: ...&quot;&lt;/font&gt;&lt;/li&gt;
	&lt;li&gt;&lt;font color=&quot;red&quot;&gt;T2 &lt;tt&gt;zkClient.getChildren(...)&lt;/tt&gt; returns a list of 3 live nodes&lt;/font&gt;&lt;/li&gt;
	&lt;li&gt;&lt;font color=&quot;green&quot;&gt;T3 &lt;tt&gt;zkClient.getChildren(...)&lt;/tt&gt; returns a list of 4 live nodes&lt;/font&gt;&lt;/li&gt;
	&lt;li&gt;&lt;font color=&quot;green&quot;&gt;T3 &lt;tt&gt;clusterState.setLiveNodes(&quot;4&quot;)&lt;/tt&gt; called, logs &quot;Updated live nodes...&quot;&lt;/font&gt;&lt;/li&gt;
	&lt;li&gt;&lt;font color=&quot;green&quot;&gt;T3 &lt;tt&gt;process()&lt;/tt&gt; is now finished, thread is free for re-use&lt;/font&gt;&lt;/li&gt;
	&lt;li&gt;&lt;font color=&quot;green&quot;&gt;T3 &lt;tt&gt;process()&lt;/tt&gt; called, logs &quot;A live node change: ...&quot;&lt;/font&gt;&lt;/li&gt;
	&lt;li&gt;&lt;font color=&quot;green&quot;&gt;T3 &lt;tt&gt;zkClient.getChildren(...)&lt;/tt&gt; returns a list of 5 live nodes&lt;/font&gt;&lt;/li&gt;
	&lt;li&gt;&lt;font color=&quot;green&quot;&gt;T3 &lt;tt&gt;clusterState.setLiveNodes(&quot;5&quot;)&lt;/tt&gt; called, logs &quot;Updated live nodes...&quot;&lt;/font&gt;&lt;/li&gt;
	&lt;li&gt;&lt;font color=&quot;green&quot;&gt;T3 &lt;tt&gt;process()&lt;/tt&gt; is now finished, thread is free for re-use&lt;/font&gt;&lt;/li&gt;
	&lt;li&gt;&lt;font color=&quot;blue&quot;&gt;T1 &lt;tt&gt;clusterState.setLiveNodes(&quot;1&quot;)&lt;/tt&gt; called, logs &quot;Updated live nodes...&quot;&lt;/font&gt;&lt;/li&gt;
	&lt;li&gt;&lt;font color=&quot;blue&quot;&gt;T1 &lt;tt&gt;process()&lt;/tt&gt; is now finished, thread is free for re-use&lt;/font&gt;&lt;/li&gt;
	&lt;li&gt;&lt;font color=&quot;red&quot;&gt;T2 &lt;tt&gt;clusterState.setLiveNodes(&quot;3&quot;)&lt;/tt&gt; called, logs &quot;Updated live nodes...&quot;&lt;/font&gt;&lt;/li&gt;
	&lt;li&gt;&lt;font color=&quot;red&quot;&gt;T2 &lt;tt&gt;process()&lt;/tt&gt; is now finished, thread is free for re-use&lt;/font&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;em&gt;(NOTE: one thing i&apos;m not clear on is why there are only 4 &lt;tt&gt;live_nodes&lt;/tt&gt; NodeChildrenChanged WatchedEvents triggered - i would expect 5 total unless ZK doesn&apos;t garuntee a unique event for each new child node, does it send a single event if 2 children are added very close together in time?)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;After the sequence of events above, port #56361 is hozed. In a real world situation, it will never update it&apos;s local state to match the &lt;em&gt;real&lt;/em&gt; list of live nodes in ZK unless some other node goes up/down triggering the LiveNodesWatcher.&lt;/p&gt;

&lt;p&gt;The rest of the &quot;live node&quot; log messages in our log file are either passively reporting the local cached data during other ZK events (specifically: collection creation trigging the state.json watcher) or they are the triggers to the LiveNodeWatcher when MiniSolrCloudCluster starts shutting down nodes after the test has failed.&lt;/p&gt;




</comment>
                            <comment id="15216977" author="hossman" created="Tue, 29 Mar 2016 22:30:03 +0000"  >&lt;p&gt;Updating summary.&lt;/p&gt;

&lt;p&gt;I suspect we either need to move the &lt;tt&gt;zkClient.getChildren(...)&lt;/tt&gt; call inside the existing &lt;tt&gt;synchronized (getUpdateLock())&lt;/tt&gt; block, or the entire &lt;tt&gt;refreshLiveNodes(Watcher watcher)&lt;/tt&gt; method needs to synchronize on some new &quot;liveNodesLock&quot;.&lt;/p&gt;</comment>
                            <comment id="15216979" author="dragonsinth" created="Tue, 29 Mar 2016 22:31:31 +0000"  >&lt;p&gt;This is super troubling....&lt;/p&gt;

&lt;p&gt;1290071 port 56361  Updated live nodes from ZooKeeper... (4) -&amp;gt; (5)&lt;br/&gt;
1290075 port 56361  Updated live nodes from ZooKeeper... (5) -&amp;gt; (1)&lt;br/&gt;
1290076 port 56361  Updated live nodes from ZooKeeper... (1) -&amp;gt; (3)&lt;/p&gt;

&lt;p&gt;WTF?&lt;/p&gt;</comment>
                            <comment id="15216989" author="hossman" created="Tue, 29 Mar 2016 22:35:14 +0000"  >&lt;blockquote&gt;&lt;p&gt;WTF?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;our comments seem to have the same problem as refreshLiveNodes &amp;#8211; concurrent updates.  see the additional analysis i just posted&lt;/p&gt;</comment>
                            <comment id="15217011" author="dragonsinth" created="Tue, 29 Mar 2016 22:55:24 +0000"  >&lt;p&gt;Hoss-- great analysis!  I was just looking at this code myself.  The part I don&apos;t understand is why this watcher is getting fired multiple times on different threads.  I (re)wrote some of this code, and one of my implicit assumptions that was that any given watcher would not get re-fired until the previous watcher invocation had returned.  But maybe that was a really bad assumption that I carried over from Curator, or perhaps the thread model in ZK has changed?&lt;/p&gt;

&lt;p&gt;Either way, I completely agree that this is broken if we can get multiple concurrent ZK callbacks on the same watcher.&lt;/p&gt;

&lt;p&gt;getUpdateLock() is an overly broad lock, we should not try to hold that lock while calling ZK.&lt;/p&gt;

&lt;p&gt;I have a few proposals on how to fix:&lt;/p&gt;

&lt;p&gt;1) Add a getChildren() variant that returns a Stat object, and use version tracking.  I could make an argument that this is the most sane way to break any versioning ties, since it completely eliminates client-side concurrency issues in the ZK framework code.  If we don&apos;t do this, we&apos;re always implicitly depending on our ability to linearly sequence getChildren calls.&lt;/p&gt;

&lt;p&gt;2) Make the various Watcher objects lock themselves for the duration of process(WatchedEvent event).  That way, subsequent callbacks will have to linearize.&lt;/p&gt;

&lt;p&gt;3) Similar idea, but put that same set of locks as top-level fields in ZkStateReader.  This is like #2, but also protects against a multiple-watchers scenario.  I&apos;ve tried to avoid situations of multiple watchers, there&apos;s no real guard against multiple calls to createClusterStateWatchersAndUpdate().&lt;/p&gt;</comment>
                            <comment id="15217032" author="dragonsinth" created="Tue, 29 Mar 2016 23:08:15 +0000"  >&lt;p&gt;Here&apos;s a pretty simple patch that I believe should fix.  I think #3 is probably the safest option here.&lt;/p&gt;</comment>
                            <comment id="15217033" author="hossman" created="Tue, 29 Mar 2016 23:09:35 +0000"  >&lt;blockquote&gt;
&lt;p&gt;The part I don&apos;t understand is why this watcher is getting fired multiple times on different threads. I (re)wrote some of this code, and one of my implicit assumptions that was that any given watcher would not get re-fired until the previous watcher invocation had returned. But maybe that was a really bad assumption that I carried over from Curator, or perhaps the thread model in ZK has changed?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I have no idea what the thread model for ZK is, but let&apos;s assume your implicit assumption was correct...&lt;/p&gt;

&lt;p&gt;That would still match the observed behavior, and a hypothetical sequence of events very similar to the one i outlined, since the &lt;tt&gt;zkClient.getChildren(...)&lt;/tt&gt; calls are passing the LiveNodeWatcher back to ZK.  so T1&apos;s call to &lt;tt&gt;zkClient.getChildren(...)&lt;/tt&gt; call adds the LiveNodeWatcher back, but before T1 has a chance to write it&apos;s local state that watcher fires and triggers T2, and T2&apos;s &lt;tt&gt;zkClient.getChildren(...)&lt;/tt&gt; call adds the LiveNodeWatcher backm but before either T1 or T2 can write to the local state that watcher fires and triggers T3, ... after T3 writes the local state, T1 and T2 overwrite it with their stale data.&lt;/p&gt;

&lt;p&gt;Your implicit assumption would alos explain the part i was confused about: why there are only 4 &lt;tt&gt;live_nodes&lt;/tt&gt; child events triggered instead of 5.  2 nodes must have come online add added themselves to &lt;tt&gt;live_nodes/&lt;/tt&gt; between the time T2&apos;s watch even was triggered and when T2 added a new watcher via the &lt;tt&gt;zkClient.getChildren(...)&lt;/tt&gt; call (explaining the jump from &quot;1&quot; to &quot;3&quot;)&lt;/p&gt;</comment>
                            <comment id="15217036" author="dragonsinth" created="Tue, 29 Mar 2016 23:11:22 +0000"  >&lt;p&gt;BTW, a million thanks to Hoss for digging into this.  We&apos;ve seen this in production in certain circumstances, I think it&apos;s triggered when we have multiple nodes experiencing long GC pauses at the same time, enough to drop ZK sessions.  The cluster can get into a state where it never fully recovers; but we&apos;ve found that if we perform a &quot;live_nodes tickle&quot; but dropping a dummy child into &quot;live_nodes&quot; then deleting it, things fix themselves.  I&apos;m so thankful to finally have an explanation, even if the root cause is my fault!&lt;/p&gt;</comment>
                            <comment id="15217043" author="dragonsinth" created="Tue, 29 Mar 2016 23:14:29 +0000"  >&lt;p&gt;Yeah, one of the things about ZK is it&apos;s impossible to view all changes as a strict linear sequence using the client-facing watcher protocol, because the watch events don&apos;t contain the data you want; by the time you can fetch the data in response to the event, it&apos;s already stale, so you can absolutely miss transient states. I assume that if you spoke ZK server replication protocol, then you could get a linearized change sequence.&lt;/p&gt;</comment>
                            <comment id="15217370" author="hossman" created="Wed, 30 Mar 2016 04:41:42 +0000"  >&lt;p&gt;I wrote up a stress test to demonstrate the bug.  I&apos;ve added it to the patch Scott already worked up &amp;amp; attached.&lt;/p&gt;

&lt;p&gt;Scott: Prior to incorporating your changes, hammering on this stress test would fail within the first 20 attempts.  But with your changes I&apos;m seeing deadlocks within the first 5 attempts every time i hammer on it...&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Found one Java-level deadlock:
=============================
&quot;zkCallback-7-thread-2-processing-n:127.0.0.1:48312_solr&quot;:
  waiting to lock monitor 0x00007f82d40076b8 (object 0x00000000ff3b5b38, a java.lang.Object),
  which is held by &quot;zkCallback-7-thread-1-processing-n:127.0.0.1:48312_solr&quot;
&quot;zkCallback-7-thread-1-processing-n:127.0.0.1:48312_solr&quot;:
  waiting to lock monitor 0x00007f82d400be38 (object 0x00000000ff3b5800, a org.apache.solr.common.cloud.ZkStateReader),
  which is held by &quot;OverseerStateUpdate-95637266046386179-127.0.0.1:48312_solr-n_0000000000&quot;
&quot;OverseerStateUpdate-95637266046386179-127.0.0.1:48312_solr-n_0000000000&quot;:
  waiting to lock monitor 0x00007f82d40076b8 (object 0x00000000ff3b5b38, a java.lang.Object),
  which is held by &quot;zkCallback-7-thread-1-processing-n:127.0.0.1:48312_solr&quot;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15217379" author="shalinmangar" created="Wed, 30 Mar 2016 04:58:05 +0000"  >&lt;p&gt;I&apos;m still digesting the discussions in this thread but..&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;...and one of my implicit assumptions that was that any given watcher would not get re-fired until the previous watcher invocation had returned. But maybe that was a really bad assumption that I carried over from Curator, or perhaps the thread model in ZK has changed?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I guess that&apos;s not true anymore because the watcher invocations are done asynchronously via a thread pool in Solr. So the original guarantees made by ZK don&apos;t apply anymore?&lt;/p&gt;</comment>
                            <comment id="15217413" author="noble.paul" created="Wed, 30 Mar 2016 05:18:53 +0000"  >&lt;blockquote&gt;&lt;p&gt;I guess that&apos;s not true anymore because the watcher invocations are done asynchronously via a thread pool in Solr. So the original guarantees made by ZK don&apos;t apply anymore?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1 shalin. Even if ZK does the event loop in a single thread, it fires the callback into &lt;tt&gt;SolrZkClient.zkCallbackExecutor&lt;/tt&gt; threadpool . So that single thread is free to fire more callbacks. So, we cannot guarantee thread safety in any watcher callback functions&lt;/p&gt;</comment>
                            <comment id="15217464" author="noble.paul" created="Wed, 30 Mar 2016 06:10:23 +0000"  >&lt;p&gt;hoss, what I would like to know is what if it is not threadsafe? The live nodes set is expected to change all the time. So, what if multiple threads update the Set one after other? &lt;/p&gt;</comment>
                            <comment id="15217494" author="dragonsinth" created="Wed, 30 Mar 2016 06:29:52 +0000"  >&lt;p&gt;The problem is inherently that if you getChildren() three times and get 3 different answers, there&apos;s currently no guarantee about which of those invocations and answers will ultimately populate the live_nodes var.  At that point, if no more server side changes happen, the watcher never re-fires and you&apos;re stuck with stale data.&lt;/p&gt;</comment>
                            <comment id="15217497" author="dragonsinth" created="Wed, 30 Mar 2016 06:31:53 +0000"  >&lt;p&gt;Should be easy enough to debug the deadlock.  I&apos;ll take a look tomorrow.  Thanks!&lt;/p&gt;</comment>
                            <comment id="15217507" author="noble.paul" created="Wed, 30 Mar 2016 06:41:56 +0000"  >&lt;p&gt;OK , That is understandable. It makes sense to make the whole &lt;tt&gt;LiveNodeWatcher.process()&lt;/tt&gt; synchronized. The current updateLock object is common for the entire cluster state. Should we not have  a separate lock for just livenodes, because live nodes are updated independently of the rest and it changes often.&lt;/p&gt;
</comment>
                            <comment id="15217585" author="dragonsinth" created="Wed, 30 Mar 2016 07:32:13 +0000"  >&lt;p&gt;Yep, that&apos;s what my patch does.  I just have to debug the deadlock.&lt;/p&gt;</comment>
                            <comment id="15217597" author="dragonsinth" created="Wed, 30 Mar 2016 07:39:58 +0000"  >&lt;p&gt;Oh.. I can see by inspection.  It&apos;s because createClusterStateWatchersAndUpdate() and updateClusterState() hold the update lock before calling the e.g. refreshLiveNodes().  Perhaps those two big methods should hold a more high-level lock instead of the update lock.&lt;/p&gt;</comment>
                            <comment id="15217612" author="dragonsinth" created="Wed, 30 Mar 2016 07:48:17 +0000"  >&lt;p&gt;Alternatively I could imagine a more golang channel style formulation like this..&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;  &lt;span class=&quot;code-comment&quot;&gt;// We don&apos;t get a Stat or track versions on getChildren() calls, so force linearization.
&lt;/span&gt;  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt; refreshLiveNodesLock = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;();
  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Queue&amp;lt;Set&amp;lt;&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;&amp;gt;&amp;gt; newLiveNodesQueue = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ConcurrentLinkedQueue&amp;lt;&amp;gt;();

  /**
   * Refresh live_nodes.
   */
  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; void refreshLiveNodes(Watcher watcher) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; KeeperException, InterruptedException {
    &lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; (refreshLiveNodesLock) {
      &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
        List&amp;lt;&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;&amp;gt; nodeList = zkClient.getChildren(LIVE_NODES_ZKNODE, watcher, &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;);
        newLiveNodesQueue.add(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; HashSet&amp;lt;&amp;gt;(nodeList));
      } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (KeeperException.NoNodeException e) {
        newLiveNodesQueue.add(emptySet());
      }
    }
    Set&amp;lt;&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;&amp;gt; oldLiveNodes;
    &lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; (getUpdateLock()) {
      Set&amp;lt;&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;&amp;gt; newLiveNodes = newLiveNodesQueue.remove();
      oldLiveNodes = &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.liveNodes;
      &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.liveNodes = newLiveNodes;
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (clusterState != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
        clusterState.setLiveNodes(newLiveNodes);
      }
      LOG.info(&lt;span class=&quot;code-quote&quot;&gt;&quot;Updated live nodes from ZooKeeper... ({}) -&amp;gt; ({})&quot;&lt;/span&gt;, oldLiveNodes.size(), newLiveNodes.size());
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (LOG.isDebugEnabled()) {
        LOG.debug(&lt;span class=&quot;code-quote&quot;&gt;&quot;Updated live nodes from ZooKeeper... {} -&amp;gt; {}&quot;&lt;/span&gt;, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; TreeSet&amp;lt;&amp;gt;(oldLiveNodes), &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; TreeSet&amp;lt;&amp;gt;(newLiveNodes));
      }
    }
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To avoid the updateLock -&amp;gt; liveNodesLock -&amp;gt; updateLock cycle, but still linearize the results of successive invocations.&lt;/p&gt;</comment>
                            <comment id="15218204" author="dragonsinth" created="Wed, 30 Mar 2016 15:57:45 +0000"  >&lt;p&gt;Updated patch, with a refinement of the channel formulation.  NOTE: I could not get TestStressLiveNodes to fail for me locally, but I believe this should fix the deadlock.&lt;/p&gt;</comment>
                            <comment id="15218286" author="noble.paul" created="Wed, 30 Mar 2016 16:44:31 +0000"  >&lt;p&gt;looked at the patch .The block &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;  &lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; (refreshLiveNodesLock) {
      Set&amp;lt;&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;&amp;gt; newLiveNodes;
      &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
        List&amp;lt;&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;&amp;gt; nodeList = zkClient.getChildren(LIVE_NODES_ZKNODE, watcher, &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;);
        newLiveNodes = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; HashSet&amp;lt;&amp;gt;(nodeList);
      } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (KeeperException.NoNodeException e) {
        newLiveNodes = emptySet();
      }
      lastFetchedLiveNodes.set(newLiveNodes);
    }

    &lt;span class=&quot;code-comment&quot;&gt;// Can&apos;t lock getUpdateLock() until we release the other, it would cause deadlock.
&lt;/span&gt;    Set&amp;lt;&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;&amp;gt; oldLiveNodes, newLiveNodes;
    &lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; (getUpdateLock()) {
      newLiveNodes = lastFetchedLiveNodes.getAndSet(&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;);
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (newLiveNodes == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
        &lt;span class=&quot;code-comment&quot;&gt;// Someone &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; won the race to apply the last update, just exit.
&lt;/span&gt;        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt;;
      }

      oldLiveNodes = &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.liveNodes;
      &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.liveNodes = newLiveNodes;
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (clusterState != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
        clusterState.setLiveNodes(newLiveNodes);
      }
    }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Why can&apos;t  the &lt;tt&gt;synchronized (refreshLiveNodesLock)&lt;/tt&gt; block be applied on the entire method and make the code simpler and avoid the race condition altogether.  &lt;/p&gt;</comment>
                            <comment id="15218306" author="hossman" created="Wed, 30 Mar 2016 16:52:31 +0000"  >&lt;p&gt;Scott: i&apos;m hammering on your updated patch now, so far 60 runs w/o failure of deadlock (3 times as long as i&apos;ve ever seen it go w/o failure, 10 times longer then your previous patch went w/o deadlocks)&lt;/p&gt;

&lt;p&gt;As someone unfamiliar with most of the ZkStateReader code, some of the changes are clearly relevant as far as the current bug goes (the lastFetchedLiveNodes AtomicRef and associated refreshLiveNodesLock) but it&apos;s not immediately obvious to me what the other changes (refreshCollectionListLock etc..) in your patch are for .. are these unrelated to the live nodes bug? &lt;/p&gt;

&lt;p&gt;Is this a similar bug pattern in watching the list of collections? (ie: two COLLECTIONS_ZKNODE, watchers may fire in rapid succession and updating the local collection states might happen out of order)&lt;/p&gt;
</comment>
                            <comment id="15218423" author="erickerickson" created="Wed, 30 Mar 2016 17:42:58 +0000"  >&lt;p&gt;I&apos;m beasting your latest patch too, I&apos;ll report anything that comes up. Just to make sure, I should be beasting StressTestLiveNodes, right?&lt;/p&gt;</comment>
                            <comment id="15218497" author="hossman" created="Wed, 30 Mar 2016 18:15:49 +0000"  >&lt;p&gt;i&apos;ve updated the patch to cleanup the test a bit &amp;#8211; besdies some cosmetic stuff it now does more iterations of smaller &quot;bursts&quot; with more variability in the number of threads used in each burst (which should increase the odds of it failing, eventually, on diff machines regardless of CPU count.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I&apos;m beasting your latest patch too, I&apos;ll report anything that comes up. Just to make sure, I should be beasting StressTestLiveNodes, right?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;TestStressLiveNodes, but otherwise yes.&lt;/p&gt;

&lt;p&gt;It would also be helpful to know if (and how quickly) you can get TestStressLiveNodes to fail on your machine when beasting w/o the rest of the patch (so far i&apos;m the only one that&apos;s been able to confirm the bug in practice w/o Scott&apos;s patch - hopefully these changes increase those odds)&lt;/p&gt;</comment>
                            <comment id="15218518" author="dragonsinth" created="Wed, 30 Mar 2016 18:23:24 +0000"  >&lt;p&gt;That was actually my first formulation, but that causes deadlock, see previous comments.&lt;/p&gt;</comment>
                            <comment id="15218529" author="dragonsinth" created="Wed, 30 Mar 2016 18:28:24 +0000"  >&lt;p&gt;Correct, the refreshCollectionListLock solves the same class of bug for the collections list as we needed to solve for live_nodes.  However, the implementation is simpler since we don&apos;t need to hold getUpdateLock() and therefore there&apos;s no deadlock potential.&lt;/p&gt;

&lt;p&gt;The code to check this.legacyClusterStateVersion &amp;gt;= stat.getVersion() solves that class of bug for clusterstate.json; we already have version guarding code for stateformat 2 collections, it was an oversight not to have it for this also.&lt;/p&gt;

&lt;p&gt;Net-net: my intent (hope?) is this patch should fix this category of race condition for all the major pieces of ZkStateReader.&lt;/p&gt;</comment>
                            <comment id="15218692" author="steve_rowe" created="Wed, 30 Mar 2016 19:47:20 +0000"  >&lt;p&gt;TestStressLiveNodes w/o the rest of the patch failed 84/100 iterations on my server.&lt;/p&gt;</comment>
                            <comment id="15218949" author="steve_rowe" created="Wed, 30 Mar 2016 21:55:56 +0000"  >&lt;p&gt;With the full patch 0/100 iterations failed.  I&apos;ll beast another 100 iterations.&lt;/p&gt;</comment>
                            <comment id="15219119" author="erickerickson" created="Thu, 31 Mar 2016 00:29:00 +0000"  >&lt;p&gt;I had 4/100 iterations fail. Let me try it all again to insure that I applied the right patch, saved the file and all that etc.&lt;/p&gt;</comment>
                            <comment id="15219154" author="erickerickson" created="Thu, 31 Mar 2016 01:12:08 +0000"  >&lt;p&gt;Nope, completely re-did the test and got another failure on run 31. Here are the failure snippets&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt; FAILURE 94.1s | TestStressLiveNodes.testStress &amp;lt;&amp;lt;&amp;lt;&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; Throwable #1: java.lang.AssertionError: iter1263: &lt;span class=&quot;error&quot;&gt;&amp;#91;127.0.0.1:53372_solr, thrasher-T1262_0-0&amp;#93;&lt;/span&gt; expected:&amp;lt;1&amp;gt; but was:&amp;lt;2&amp;gt;&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; 	at __randomizedtesting.SeedInfo.seed(&lt;span class=&quot;error&quot;&gt;&amp;#91;3E42073F9773C752:2B597AFE14843F29&amp;#93;&lt;/span&gt;:0)&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; 	at org.apache.solr.cloud.TestStressLiveNodes.testStress(TestStressLiveNodes.java:137)&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; 	at java.lang.Thread.run(Thread.java:745)&lt;/p&gt;


&lt;p&gt;   ***********************&lt;/p&gt;

&lt;p&gt;      &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt; FAILURE 75.7s | TestStressLiveNodes.testStress &amp;lt;&amp;lt;&amp;lt;&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; Throwable #1: java.lang.AssertionError: iter2373 6 != 1 expected:&amp;lt;&lt;span class=&quot;error&quot;&gt;&amp;#91;127.0.0.1:61240_solr, thrasher-T2373_0-0, thrasher-T2373_1-0, thrasher-T2373_2-0, thrasher-T2373_3-0, thrasher-T2373_4-0&amp;#93;&lt;/span&gt;&amp;gt; but was:&amp;lt;&lt;span class=&quot;error&quot;&gt;&amp;#91;127.0.0.1:61240_solr&amp;#93;&lt;/span&gt;&amp;gt;&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; 	at __randomizedtesting.SeedInfo.seed(&lt;span class=&quot;error&quot;&gt;&amp;#91;20645E3B72A746D3:357F23FAF150BEA8&amp;#93;&lt;/span&gt;:0)&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; 	at org.apache.solr.cloud.TestStressLiveNodes.testStress(TestStressLiveNodes.java:200)&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; 	at java.lang.Thread.run(Thread.java:745)&lt;/p&gt;


&lt;p&gt;**************************&lt;/p&gt;


&lt;p&gt;   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;   2&amp;gt; NOTE: reproduce with: ant test  -Dtestcase=TestStressLiveNodes -Dtests.method=testStress -Dtests.seed=E803236A2774DE4C -Dtests.nightly=true -Dtests.slow=true -Dtests.locale=sr-RS -Dtests.timezone=Pacific/Majuro -Dtests.asserts=true -Dtests.file.encoding=UTF-8&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt; ERROR   62.7s | TestStressLiveNodes.testStress &amp;lt;&amp;lt;&amp;lt;&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; Throwable #1: org.apache.solr.common.SolrException: java.util.concurrent.TimeoutException: Could not connect to ZooKeeper 127.0.0.1:62845/solr within 30000 ms&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; 	at __randomizedtesting.SeedInfo.seed(&lt;span class=&quot;error&quot;&gt;&amp;#91;E803236A2774DE4C:FD185EABA4832637&amp;#93;&lt;/span&gt;:0)&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; 	at org.apache.solr.common.cloud.SolrZkClient.&amp;lt;init&amp;gt;(SolrZkClient.java:181)&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; 	at org.apache.solr.common.cloud.SolrZkClient.&amp;lt;init&amp;gt;(SolrZkClient.java:115)&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; 	at org.apache.solr.common.cloud.SolrZkClient.&amp;lt;init&amp;gt;(SolrZkClient.java:110)&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; 	at org.apache.solr.common.cloud.SolrZkClient.&amp;lt;init&amp;gt;(SolrZkClient.java:97)&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; 	at org.apache.solr.cloud.TestStressLiveNodes.newSolrZkClient(TestStressLiveNodes.java:87)&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; 	at org.apache.solr.cloud.TestStressLiveNodes.access$000(TestStressLiveNodes.java:54)&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; 	at org.apache.solr.cloud.TestStressLiveNodes$LiveNodeTrasher.&amp;lt;init&amp;gt;(TestStressLiveNodes.java:225)&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; 	at org.apache.solr.cloud.TestStressLiveNodes.testStress(TestStressLiveNodes.java:174)&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; 	at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; Caused by: java.util.concurrent.TimeoutException: Could not connect to ZooKeeper 127.0.0.1:62845/solr within 30000 ms&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; 	at org.apache.solr.common.cloud.ConnectionManager.waitForConnected(ConnectionManager.java:228)&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; 	at org.apache.solr.common.cloud.SolrZkClient.&amp;lt;init&amp;gt;(SolrZkClient.java:173)&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; 	... 46 more&lt;br/&gt;
   [ju&lt;/p&gt;

&lt;p&gt;   ***********************************&lt;/p&gt;

&lt;p&gt;   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;   2&amp;gt; NOTE: reproduce with: ant test  -Dtestcase=TestStressLiveNodes -Dtests.method=testStress -Dtests.seed=6B315674F529C1E2 -Dtests.nightly=true -Dtests.slow=true -Dtests.locale=ar-IQ -Dtests.timezone=Europe/Bratislava -Dtests.asserts=true -Dtests.file.encoding=UTF-8&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt; ERROR    113s | TestStressLiveNodes.testStress &amp;lt;&amp;lt;&amp;lt;&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; Throwable #1: org.apache.solr.common.SolrException: java.util.concurrent.TimeoutException: Could not connect to ZooKeeper 127.0.0.1:62849/solr within 30000 ms&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; 	at __randomizedtesting.SeedInfo.seed(&lt;span class=&quot;error&quot;&gt;&amp;#91;6B315674F529C1E2:7E2A2BB576DE3999&amp;#93;&lt;/span&gt;:0)&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; 	at org.apache.solr.common.cloud.SolrZkClient.&amp;lt;init&amp;gt;(SolrZkClient.java:181)&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; 	at org.apache.solr.common.cloud.SolrZkClient.&amp;lt;init&amp;gt;(SolrZkClient.java:115)&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; 	at org.apache.solr.common.cloud.SolrZkClient.&amp;lt;init&amp;gt;(SolrZkClient.java:110)&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; 	at org.apache.solr.common.cloud.SolrZkClient.&amp;lt;init&amp;gt;(SolrZkClient.java:97)&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; 	at org.apache.solr.cloud.TestStressLiveNodes.newSolrZkClient(TestStressLiveNodes.java:87)&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; 	at org.apache.solr.cloud.TestStressLiveNodes.access$000(TestStressLiveNodes.java:54)&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; 	at org.apache.solr.cloud.TestStressLiveNodes$LiveNodeTrasher.&amp;lt;init&amp;gt;(TestStressLiveNodes.java:225)&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; 	at org.apache.solr.cloud.TestStressLiveNodes.testStress(TestStressLiveNodes.java:174)&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; 	at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; Caused by: java.util.concurrent.TimeoutException: Could not connect to ZooKeeper 127.0.0.1:62849/solr within 30000 ms&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; 	at org.apache.solr.common.cloud.ConnectionManager.waitForConnected(ConnectionManager.java:228)&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; 	at org.apache.solr.common.cloud.SolrZkClient.&amp;lt;init&amp;gt;(SolrZkClient.java:173)&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;    &amp;gt; 	... 46 more&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit4&amp;#93;&lt;/span&gt;   2&amp;gt; 116534 INFO  (jetty-launcher-1-thread-2) [    ] o.e.j.s.ServerConnector Stopped ServerConnector@5aa5196c&lt;/p&gt;
{HTTP/1.1,[http/1.1]}
{127.0.0.1:0}
&lt;p&gt;   [juni&lt;/p&gt;</comment>
                            <comment id="15219286" author="steve_rowe" created="Thu, 31 Mar 2016 03:53:22 +0000"  >&lt;p&gt;Zero failures on second 100 iterations.&lt;/p&gt;</comment>
                            <comment id="15220288" author="erickerickson" created="Thu, 31 Mar 2016 17:37:48 +0000"  >&lt;p&gt;Just chatting with Steve and there&apos;s a bit of confusion here. The mini-traces I pasted above are &lt;em&gt;with&lt;/em&gt; the latest patch AFAIK, &lt;em&gt;not&lt;/em&gt; with the fixes reverted.&lt;/p&gt;

&lt;p&gt;We&apos;re trying to figure out why my results are different.&lt;/p&gt;</comment>
                            <comment id="15220429" author="dragonsinth" created="Thu, 31 Mar 2016 18:42:51 +0000"  >&lt;p&gt;Erick, if all the errors you&apos;re seeing are related to Zookeeper errors, then I&apos;m not sure you&apos;re actually running into any of the issues we fixed.  ZkStateReader is meant to be fault tolerant with respect to ZK connectivity, but there&apos;s no reason to the think that the particular test expectations in TestStressLiveNodes would always hold true in the face of ZK connectivity loss; I would expect that TestStressLiveNodes assumes that the ZkStateReader -&amp;gt; ZK connection is always up.&lt;/p&gt;

&lt;p&gt;If you&apos;re seeing a either a deadlock problem, or a data problem in the absence of any ZK errors, then that would be a problem.&lt;/p&gt;</comment>
                            <comment id="15222382" author="dragonsinth" created="Fri, 1 Apr 2016 21:43:17 +0000"  >&lt;p&gt;What&apos;s the next step here?  I really think, despite the minority report, that this patch should be a drastic improvement on the current situation, and fix a real production bug.&lt;/p&gt;</comment>
                            <comment id="15222384" author="dragonsinth" created="Fri, 1 Apr 2016 21:43:54 +0000"  >&lt;p&gt;Maybe &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=shalin&quot; class=&quot;user-hover&quot; rel=&quot;shalin&quot;&gt;shalin&lt;/a&gt; or &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=noble.paul&quot; class=&quot;user-hover&quot; rel=&quot;noble.paul&quot;&gt;noble.paul&lt;/a&gt; would be willing to review/commit?&lt;/p&gt;</comment>
                            <comment id="15224511" author="shalinmangar" created="Mon, 4 Apr 2016 16:58:55 +0000"  >&lt;p&gt;I&apos;ll take a look this week if no one beats me to it.&lt;/p&gt;</comment>
                            <comment id="15224542" author="erickerickson" created="Mon, 4 Apr 2016 17:15:59 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dragonsinth&quot; class=&quot;user-hover&quot; rel=&quot;dragonsinth&quot;&gt;dragonsinth&lt;/a&gt; OK, aside from some connectivity issues, the only thing I see that &lt;em&gt;may&lt;/em&gt; be important is two cases that fail on TestStressLiveNodex&lt;span class=&quot;error&quot;&gt;&amp;#91;200&amp;#93;&lt;/span&gt;, these lines:&lt;br/&gt;
        // verify our local client knows the correct set of live nodes&lt;br/&gt;
        cachedLiveNodes = getCachedLiveNodesFromLocalState(actualLiveNodes.size());&lt;br/&gt;
        assertEquals(&quot;iter&quot;&lt;ins&gt;iter&lt;/ins&gt;&quot; &quot; + actualLiveNodes.size() + &quot; != &quot; + cachedLiveNodes.size(),&lt;br/&gt;
                     actualLiveNodes, cachedLiveNodes);&lt;/p&gt;

&lt;p&gt;16 != 1 on one test (seeds below) and 13 != 1.&lt;/p&gt;

&lt;p&gt;I tried the first one and it doesn&apos;t reliably fail, so I&apos;m not quite sure what to do next or if this test failure is even important to this JIRA, and I really can&apos;t pursue it more ATM.&lt;/p&gt;

&lt;p&gt;ant test  -Dtestcase=TestStressLiveNodes -Dtests.method=testStress -Dtests.seed=AAD31DA7247CE323 -Dtests.nightly=true -Dtests.slow=true -Dtests.locale=ar-AE -Dtests.timezone=America/Santo_Domingo -Dtests.asserts=true -Dtests.file.encoding=UTF-8&lt;/p&gt;

&lt;p&gt;ant test  -Dtestcase=TestStressLiveNodes -Dtests.method=testStress -Dtests.seed=B4786108141B49BD -Dtests.nightly=true -Dtests.slow=true -Dtests.locale=sr-Latn-BA -Dtests.timezone=Europe/Budapest -Dtests.asserts=true -Dtests.file.encoding=ISO-8859-1&lt;/p&gt;</comment>
                            <comment id="15235573" author="dragonsinth" created="Mon, 11 Apr 2016 17:44:04 +0000"  >&lt;p&gt;Thanks Erick!  I&apos;ll give this a shot myself.&lt;/p&gt;

&lt;p&gt;I&apos;m going to venture to say, however, that we should move forward on committing this patch.  Whether or not there are any lingering race conditions that could manifest in rare cases, the patch for sure fixes some race conditions that we can both see from inspection and reproduce with a lot of reliability.  So I think this patch makes things much better regardless of whether we&apos;ve ironed out every last race condition.&lt;/p&gt;</comment>
                            <comment id="15235604" author="dragonsinth" created="Mon, 11 Apr 2016 17:58:21 +0000"  >&lt;p&gt;FWIW these both passed for me.&lt;/p&gt;</comment>
                            <comment id="15239231" author="markrmiller@gmail.com" created="Wed, 13 Apr 2016 13:31:35 +0000"  >&lt;p&gt;I&apos;ll take it. I set my test beasting script on it, and it comes out solid for me.&lt;/p&gt;</comment>
                            <comment id="15239245" author="shalinmangar" created="Wed, 13 Apr 2016 13:39:08 +0000"  >&lt;p&gt;Okay, I&apos;ve been beasting it since morning. No fails yet.&lt;/p&gt;</comment>
                            <comment id="15239276" author="jira-bot" created="Wed, 13 Apr 2016 13:55:11 +0000"  >&lt;p&gt;Commit 744b419b42a5797700c0a3a5f859d86ae9d05325 in lucene-solr&apos;s branch refs/heads/master from markrmiller&lt;br/&gt;
[ &lt;a href=&quot;https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=744b419&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=744b419&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-8914&quot; title=&quot;ZkStateReader&amp;#39;s refreshLiveNodes(Watcher) is not thread safe&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-8914&quot;&gt;&lt;del&gt;SOLR-8914&lt;/del&gt;&lt;/a&gt;: ZkStateReader&apos;s refreshLiveNodes(Watcher) is not thread safe.&lt;/p&gt;</comment>
                            <comment id="15239328" author="jira-bot" created="Wed, 13 Apr 2016 14:24:37 +0000"  >&lt;p&gt;Commit fe858858686ba7e2738367bf4b7ef8c621987df9 in lucene-solr&apos;s branch refs/heads/branch_6x from markrmiller&lt;br/&gt;
[ &lt;a href=&quot;https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=fe85885&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=fe85885&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-8914&quot; title=&quot;ZkStateReader&amp;#39;s refreshLiveNodes(Watcher) is not thread safe&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-8914&quot;&gt;&lt;del&gt;SOLR-8914&lt;/del&gt;&lt;/a&gt;: ZkStateReader&apos;s refreshLiveNodes(Watcher) is not thread safe.&lt;/p&gt;</comment>
                            <comment id="15239957" author="dragonsinth" created="Wed, 13 Apr 2016 20:19:06 +0000"  >&lt;p&gt;Yay!  Thanks guys!&lt;/p&gt;</comment>
                            <comment id="15248103" author="hossman" created="Tue, 19 Apr 2016 16:28:47 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markrmiller%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;markrmiller@gmail.com&quot;&gt;markrmiller@gmail.com&lt;/a&gt; - is this resolved?&lt;/p&gt;

&lt;p&gt;is it targeting 6.1 only, or are you thinking it&apos;s safe/good to backport for 6.0.1 and or 5.5.1 ?&lt;/p&gt;</comment>
                            <comment id="15248325" author="dragonsinth" created="Tue, 19 Apr 2016 18:08:27 +0000"  >&lt;p&gt;I think it&apos;s definitely work backporting, and I&apos;m happy to do it.  I already backported it to our own 5.4.1 fork; the live code merges pretty clean but I had to do a little work on the test code as some test infra has changed.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markrmiller%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;markrmiller@gmail.com&quot;&gt;markrmiller@gmail.com&lt;/a&gt; Should I just do the backports and push commits?&lt;/p&gt;</comment>
                            <comment id="15249004" author="dragonsinth" created="Wed, 20 Apr 2016 00:15:28 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=anshumg&quot; class=&quot;user-hover&quot; rel=&quot;anshumg&quot;&gt;anshumg&lt;/a&gt; I think we should backport this to 5.5.1.  Happy to push a commit (i&apos;ve already done the work locally to backport the test code.)&lt;/p&gt;</comment>
                            <comment id="15249021" author="anshumg" created="Wed, 20 Apr 2016 00:23:24 +0000"  >&lt;p&gt;+1 for back porting this and thanks for volunteering &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15250879" author="jira-bot" created="Wed, 20 Apr 2016 22:46:44 +0000"  >&lt;p&gt;Commit b93cb2714b7a2ea43ae200d31cc5b04a0b0ecc7c in lucene-solr&apos;s branch refs/heads/&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-8914&quot; title=&quot;ZkStateReader&amp;#39;s refreshLiveNodes(Watcher) is not thread safe&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-8914&quot;&gt;&lt;del&gt;SOLR-8914&lt;/del&gt;&lt;/a&gt;-6_0 from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dragonsinth&quot; class=&quot;user-hover&quot; rel=&quot;dragonsinth&quot;&gt;dragonsinth&lt;/a&gt;&lt;br/&gt;
[ &lt;a href=&quot;https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=b93cb27&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=b93cb27&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-8914&quot; title=&quot;ZkStateReader&amp;#39;s refreshLiveNodes(Watcher) is not thread safe&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-8914&quot;&gt;&lt;del&gt;SOLR-8914&lt;/del&gt;&lt;/a&gt;: ZkStateReader&apos;s refreshLiveNodes(Watcher) is not thread safe.&lt;/p&gt;</comment>
                            <comment id="15250882" author="jira-bot" created="Wed, 20 Apr 2016 22:47:10 +0000"  >&lt;p&gt;Commit b93cb2714b7a2ea43ae200d31cc5b04a0b0ecc7c in lucene-solr&apos;s branch refs/heads/branch_6_0 from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dragonsinth&quot; class=&quot;user-hover&quot; rel=&quot;dragonsinth&quot;&gt;dragonsinth&lt;/a&gt;&lt;br/&gt;
[ &lt;a href=&quot;https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=b93cb27&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=b93cb27&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-8914&quot; title=&quot;ZkStateReader&amp;#39;s refreshLiveNodes(Watcher) is not thread safe&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-8914&quot;&gt;&lt;del&gt;SOLR-8914&lt;/del&gt;&lt;/a&gt;: ZkStateReader&apos;s refreshLiveNodes(Watcher) is not thread safe.&lt;/p&gt;</comment>
                            <comment id="15250886" author="jira-bot" created="Wed, 20 Apr 2016 22:48:37 +0000"  >&lt;p&gt;Commit 58f77fed2c45b8ce7da1831954f941dde39d98f9 in lucene-solr&apos;s branch refs/heads/branch_5x from markrmiller&lt;br/&gt;
[ &lt;a href=&quot;https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=58f77fe&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=58f77fe&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-8914&quot; title=&quot;ZkStateReader&amp;#39;s refreshLiveNodes(Watcher) is not thread safe&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-8914&quot;&gt;&lt;del&gt;SOLR-8914&lt;/del&gt;&lt;/a&gt;: ZkStateReader&apos;s refreshLiveNodes(Watcher) is not thread safe.&lt;/p&gt;</comment>
                            <comment id="15250912" author="jira-bot" created="Wed, 20 Apr 2016 23:13:42 +0000"  >&lt;p&gt;Commit f45fae5c81e68763c99826524bec8b9273dfefd4 in lucene-solr&apos;s branch refs/heads/branch_5_5 from markrmiller&lt;br/&gt;
[ &lt;a href=&quot;https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f45fae5&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f45fae5&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-8914&quot; title=&quot;ZkStateReader&amp;#39;s refreshLiveNodes(Watcher) is not thread safe&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-8914&quot;&gt;&lt;del&gt;SOLR-8914&lt;/del&gt;&lt;/a&gt;: ZkStateReader&apos;s refreshLiveNodes(Watcher) is not thread safe.&lt;/p&gt;</comment>
                            <comment id="15277242" author="hossman" created="Mon, 9 May 2016 22:54:01 +0000"  >
&lt;p&gt;Manually correcting fixVersion per Step #S5 of &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-7271&quot; title=&quot;Cleanup jira&amp;#39;s concept of &amp;#39;master&amp;#39; and &amp;#39;6.0&amp;#39;&quot; class=&quot;issue-link&quot; data-issue-key=&quot;LUCENE-7271&quot;&gt;&lt;del&gt;LUCENE-7271&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15305356" author="steve_rowe" created="Sat, 28 May 2016 13:38:22 +0000"  >&lt;p&gt;Bulk close issues included in the 6.0.1 release.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12796126" name="SOLR-8914.patch" size="16770" author="hossman" created="Wed, 30 Mar 2016 18:15:49 +0000"/>
                            <attachment id="12796101" name="SOLR-8914.patch" size="16566" author="dragonsinth" created="Wed, 30 Mar 2016 15:57:45 +0000"/>
                            <attachment id="12796012" name="SOLR-8914.patch" size="17666" author="hossman" created="Wed, 30 Mar 2016 04:41:42 +0000"/>
                            <attachment id="12795960" name="SOLR-8914.patch" size="6887" author="dragonsinth" created="Tue, 29 Mar 2016 23:08:15 +0000"/>
                            <attachment id="12795668" name="jenkins.thetaphi.de_Lucene-Solr-6.x-Solaris_32.log.txt" size="934891" author="hossman" created="Mon, 28 Mar 2016 18:15:38 +0000"/>
                            <attachment id="12795946" name="live_node_mentions_port56361_with_threadIds.log.txt" size="3712" author="hossman" created="Tue, 29 Mar 2016 22:26:38 +0000"/>
                            <attachment id="12795754" name="live_nodes_mentions.log.txt" size="13648" author="hossman" created="Tue, 29 Mar 2016 02:12:41 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>7.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 25 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2vaqn:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12313321" key="com.atlassian.jira.toolkit:message">
                        <customfieldname>Solr Mailing List Info</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>