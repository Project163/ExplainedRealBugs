<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 03:30:45 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SOLR-3939] An empty or just replicated index cannot become the leader of a shard after a leader goes down.</title>
                <link>https://issues.apache.org/jira/browse/SOLR-3939</link>
                <project id="12310230" key="SOLR">Solr</project>
                    <description>&lt;p&gt;When a leader core is unloaded using the core admin api, the followers in the shard go into recovery but do not come out. Leader election doesn&apos;t take place and the shard goes down.&lt;/p&gt;

&lt;p&gt;This effects the ability to move a micro-shard from one Solr instance to another Solr instance.&lt;/p&gt;

&lt;p&gt;The problem does not occur 100% of the time but a large % of the time. &lt;/p&gt;

&lt;p&gt;To setup a test, startup Solr Cloud with a single shard. Add cores to that shard as replicas using core admin. Then unload the leader core using core admin. &lt;/p&gt;</description>
                <environment></environment>
        <key id="12611527">SOLR-3939</key>
            <summary>An empty or just replicated index cannot become the leader of a shard after a leader goes down.</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="markrmiller@gmail.com">Mark Miller</assignee>
                                    <reporter username="jbernste">Joel Bernstein</reporter>
                        <labels>
                            <label>4.0.1_Candidate</label>
                    </labels>
                <created>Fri, 12 Oct 2012 14:13:34 +0000</created>
                <updated>Mon, 9 May 2016 18:58:22 +0000</updated>
                            <resolved>Fri, 26 Oct 2012 15:31:09 +0000</resolved>
                                    <version>4.0-BETA</version>
                    <version>4.0</version>
                                    <fixVersion>4.1</fixVersion>
                    <fixVersion>6.0</fixVersion>
                                    <component>SolrCloud</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>7</watches>
                                                                                                                <comments>
                            <comment id="13475136" author="markrmiller@gmail.com" created="Fri, 12 Oct 2012 16:40:11 +0000"  >&lt;p&gt;I think I see two issues so far:&lt;/p&gt;

&lt;p&gt;1. &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-3940&quot; title=&quot;Rejoining the leader election incorrectly triggers the code path for a fresh cluster start rather than fail over.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-3940&quot;&gt;&lt;del&gt;SOLR-3940&lt;/del&gt;&lt;/a&gt; - there can be a long wait that should not exist&lt;br/&gt;
2. We should consider a sync attempt from leader to replica that fails due to 404 a success. That is either a core that has been unloaded or a starting or stopping Solr instance - treating it as a fail in the unloaded core (404) case can cause our current leader choice strategy to fail to make progress. A stopping or starting Solr instance will move on to recovery.&lt;/p&gt;</comment>
                            <comment id="13475190" author="markrmiller@gmail.com" created="Fri, 12 Oct 2012 17:42:28 +0000"  >&lt;p&gt;Patch attached.&lt;/p&gt;

&lt;p&gt;Treat 404 on leader sync to replicas as success + test.&lt;/p&gt;

&lt;p&gt;Also includes fix for &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-3940&quot; title=&quot;Rejoining the leader election incorrectly triggers the code path for a fresh cluster start rather than fail over.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-3940&quot;&gt;&lt;del&gt;SOLR-3940&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="13475207" author="joel.bernstein" created="Fri, 12 Oct 2012 18:09:50 +0000"  >&lt;p&gt;The patch solved the issue. I was able to unload the leader core and one of the replicas became the leader without delay. Tested three times, worked each time. No error in the logs.&lt;/p&gt;

&lt;p&gt;Great job! Thanks!&lt;/p&gt;</comment>
                            <comment id="13475224" author="markrmiller@gmail.com" created="Fri, 12 Oct 2012 18:33:59 +0000"  >&lt;p&gt;Thanks Joel!&lt;/p&gt;</comment>
                            <comment id="13475905" author="joel.bernstein" created="Sun, 14 Oct 2012 21:28:12 +0000"  >&lt;p&gt;I think we need to re-open this issue.&lt;/p&gt;

&lt;p&gt;I tried unloading the shard leader when a replica is in another Solr instance and the leader election didn&apos;t take place. The initial test had the shard leader and replica in the same Solr instance, which works with this patch.&lt;/p&gt;

&lt;p&gt;Here is how to setup the test:&lt;/p&gt;

&lt;p&gt;1) Start initial solr instance, automatically creating collection1 and shard1.&lt;br/&gt;
java -Dbootstrap_confdir=./solr/collection1/conf -Dcollection.configName=myconf -DzkRun -jar start.jar&lt;/p&gt;


&lt;p&gt;2) Add shard2 to the same solr instance using coreadmin.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://localhost:8983/solr/admin/cores?action=CREATE&amp;amp;name=mycore&amp;amp;collection=collection1&amp;amp;shard=shard2&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://localhost:8983/solr/admin/cores?action=CREATE&amp;amp;name=mycore&amp;amp;collection=collection1&amp;amp;shard=shard2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;3) Feed exampledocs to collection1.&lt;/p&gt;

&lt;p&gt;4) Startup another solr instance and point to same zookeeper. This will create a replica for shard1 and replicate the data from shard1.&lt;/p&gt;

&lt;p&gt;5) Unload the shard1 leader (core collection1) on the first solr instance.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://localhost:8983/solr/admin/cores?action=UNLOAD&amp;amp;core=collection1&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://localhost:8983/solr/admin/cores?action=UNLOAD&amp;amp;core=collection1&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The leader election process doesn&apos;t take place. &lt;/p&gt;

&lt;p&gt;This would be the basic scenario for creating a micro-shard and then migrating it to another Solr instance.&lt;/p&gt;</comment>
                            <comment id="13475944" author="markrmiller@gmail.com" created="Mon, 15 Oct 2012 03:30:16 +0000"  >&lt;p&gt;Thanks for the report - we don&apos;t have much testing around unload with SolrCloud yet - this is good stuff to address.&lt;/p&gt;

&lt;p&gt;I&apos;ve setup a test that seems to show an issue. I&apos;ll try and look at it some more tomorrow.&lt;/p&gt;</comment>
                            <comment id="13476297" author="markrmiller@gmail.com" created="Mon, 15 Oct 2012 17:47:54 +0000"  >&lt;p&gt;is this with an empty index?&lt;/p&gt;</comment>
                            <comment id="13476316" author="joel.bernstein" created="Mon, 15 Oct 2012 18:06:39 +0000"  >&lt;p&gt;I tested with the exampledocs loaded. Step 3 in the test above. I loaded the shards before starting up the replica on the second solr instance.&lt;/p&gt;</comment>
                            <comment id="13476400" author="markrmiller@gmail.com" created="Mon, 15 Oct 2012 20:26:06 +0000"  >&lt;p&gt;interesting - I can see an issue when I run the test with empty indexes, but my current test is passing if I add some docs. The main reason I see for this at the moment is that a leader who tries to sync with his replicas will always fail with an empty tlog (no frame of reference).&lt;/p&gt;

&lt;p&gt;I&apos;ll have to dig deeper for the &apos;docs in index&apos; case.&lt;/p&gt;</comment>
                            <comment id="13476422" author="joel.bernstein" created="Mon, 15 Oct 2012 20:39:29 +0000"  >&lt;p&gt;No sure if this helps. Here is stack trace from my second solr instance. This is the instance that would be the leader after the leader core was unloaded on the first instance.&lt;/p&gt;

&lt;p&gt;SEVERE: There was a problem finding the leader in zk:org.apache.solr.common.SolrException: Could not get leader props&lt;br/&gt;
	at org.apache.solr.cloud.ZkController.getLeaderProps(ZkController.java:709)&lt;br/&gt;
	at org.apache.solr.cloud.ZkController.getLeaderProps(ZkController.java:673)&lt;br/&gt;
	at org.apache.solr.cloud.ZkController.waitForLeaderToSeeDownState(ZkController.java:1070)&lt;br/&gt;
	at org.apache.solr.cloud.ZkController.registerAllCoresAsDown(ZkController.java:273)&lt;br/&gt;
	at org.apache.solr.cloud.ZkController.access$100(ZkController.java:82)&lt;br/&gt;
	at org.apache.solr.cloud.ZkController$1.command(ZkController.java:190)&lt;br/&gt;
	at org.apache.solr.common.cloud.ConnectionManager$1.update(ConnectionManager.java:116)&lt;br/&gt;
	at org.apache.solr.common.cloud.DefaultConnectionStrategy.reconnect(DefaultConnectionStrategy.java:46)&lt;br/&gt;
	at org.apache.solr.common.cloud.ConnectionManager.process(ConnectionManager.java:90)&lt;br/&gt;
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:526)&lt;br/&gt;
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:502)&lt;br/&gt;
Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /collections/collection1/leaders/shard1&lt;br/&gt;
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:102)&lt;br/&gt;
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)&lt;br/&gt;
	at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:927)&lt;br/&gt;
	at org.apache.solr.common.cloud.SolrZkClient$7.execute(SolrZkClient.java:244)&lt;br/&gt;
	at org.apache.solr.common.cloud.SolrZkClient$7.execute(SolrZkClient.java:241)&lt;br/&gt;
	at org.apache.solr.common.cloud.ZkCmdExecutor.retryOperation(ZkCmdExecutor.java:63)&lt;br/&gt;
	at org.apache.solr.common.cloud.SolrZkClient.getData(SolrZkClient.java:241)&lt;br/&gt;
	at org.apache.solr.cloud.ZkController.getLeaderProps(ZkController.java:687)&lt;br/&gt;
	... 10 more&lt;/p&gt;

&lt;p&gt;Oct 15, 2012 3:39:18 PM org.apache.solr.common.SolrException log&lt;br/&gt;
SEVERE: :org.apache.solr.common.SolrException: There was a problem finding the leader in zk&lt;br/&gt;
	at org.apache.solr.cloud.ZkController.waitForLeaderToSeeDownState(ZkController.java:1080)&lt;br/&gt;
	at org.apache.solr.cloud.ZkController.registerAllCoresAsDown(ZkController.java:273)&lt;br/&gt;
	at org.apache.solr.cloud.ZkController.access$100(ZkController.java:82)&lt;br/&gt;
	at org.apache.solr.cloud.ZkController$1.command(ZkController.java:190)&lt;br/&gt;
	at org.apache.solr.common.cloud.ConnectionManager$1.update(ConnectionManager.java:116)&lt;br/&gt;
	at org.apache.solr.common.cloud.DefaultConnectionStrategy.reconnect(DefaultConnectionStrategy.java:46)&lt;br/&gt;
	at org.apache.solr.common.cloud.ConnectionManager.process(ConnectionManager.java:90)&lt;br/&gt;
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:526)&lt;br/&gt;
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:502)&lt;/p&gt;</comment>
                            <comment id="13476449" author="joel.bernstein" created="Mon, 15 Oct 2012 21:06:06 +0000"  >&lt;p&gt;I restarted the second solr instance and it came up as the leader for shard1, with no errors. &lt;/p&gt;

&lt;p&gt;I&apos;ll try to re-produce again.&lt;/p&gt;</comment>
                            <comment id="13476487" author="joel.bernstein" created="Mon, 15 Oct 2012 21:39:39 +0000"  >&lt;p&gt;I reproduced it again. I pulled again from the top of the 4x branch. I didn&apos;t apply the patch because it was committed, I believe.&lt;/p&gt;

&lt;p&gt;Same exact steps as described above. Attached is part of the log file from the second Solr instance that shows the replica going into recovery. It&apos;s looking for the collection1 core that was unloaded from the first solr instance.&lt;/p&gt;</comment>
                            <comment id="13476488" author="joel.bernstein" created="Mon, 15 Oct 2012 21:40:38 +0000"  >&lt;p&gt;The log output from solr.&lt;/p&gt;</comment>
                            <comment id="13477222" author="joel.bernstein" created="Tue, 16 Oct 2012 18:02:04 +0000"  >&lt;p&gt;It looks like after the leader is unloaded, the replica attempts to sync to the unloaded leader as part of the process to determine if it can be leader. When this fails, it thinks that there are better candidates to become leader. Then it goes into a recovery loop.&lt;/p&gt;</comment>
                            <comment id="13477247" author="markrmiller@gmail.com" created="Tue, 16 Oct 2012 18:29:47 +0000"  >&lt;p&gt;That&apos;s what I see when I have an empty index. The leader sync fails because sync always fails with no local versions.&lt;/p&gt;

&lt;p&gt;The case with docs is perhaps a bit trickier since my simple test passes. I&apos;ll take a look at the logs.&lt;/p&gt;</comment>
                            <comment id="13477250" author="markrmiller@gmail.com" created="Tue, 16 Oct 2012 18:33:29 +0000"  >&lt;p&gt;I think I see the issue. While we have talked about it, we don&apos;t currently try to populate the transaction log after a replication.&lt;/p&gt;

&lt;p&gt;So, the second core replica is replicating, it&apos;s got docs but no versions, then it tries to become the leader - but just like with the empty index, it cannot successfully sync with no versions as a frame of reference.&lt;/p&gt;</comment>
                            <comment id="13477252" author="markrmiller@gmail.com" created="Tue, 16 Oct 2012 18:35:33 +0000"  >&lt;p&gt;(My test was passing because I had the replica up initially, so it go the docs from the leader not through replication)&lt;/p&gt;</comment>
                            <comment id="13477898" author="joel.bernstein" created="Wed, 17 Oct 2012 13:54:35 +0000"  >&lt;p&gt;So that was the difference between the initial test with all the cores in the single Solr instance. I ran this test starting up all the cores and then loaded, which hit all the transaction logs.&lt;/p&gt;
</comment>
                            <comment id="13477906" author="markrmiller@gmail.com" created="Wed, 17 Oct 2012 14:11:29 +0000"  >&lt;p&gt;I&apos;m not sure - I&apos;d have to think about it. Could be as simple as timing. If the new leader candidate gets an updated cluster state fast enough, he won&apos;t even consider the core that just went down when trying to become the leader.&lt;/p&gt;

&lt;p&gt;Anyhow, for what I am seeing in my tests so far, I think have a solution that works for now - just have to test it out.&lt;/p&gt;</comment>
                            <comment id="13478081" author="yseeley@gmail.com" created="Wed, 17 Oct 2012 17:59:47 +0000"  >&lt;p&gt;Mark &amp;amp; I were chatting a little about this.  The easiest fix would seem to be&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if a single remaining replica is active, it should always become the leader (even if it has no recent versions)&lt;/li&gt;
	&lt;li&gt;if an existing replica comes back up and tries to sync with this new leader, it should fail (or somehow be forced to replicate from the new leader)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;That still begs the question... what if two replicas are in the situation of having no recent versions because they both just finished replicating?&lt;/p&gt;

&lt;p&gt;Anther solution (which is more difficult and would take longer) is to store some number of latest versions in the commit data.&lt;/p&gt;</comment>
                            <comment id="13478333" author="joel.bernstein" created="Wed, 17 Oct 2012 20:22:55 +0000"  >&lt;p&gt;Curious as to what the procedure is when the leader VM is taken down, rather then the leader core is unloaded. This scenario seems to work even when the replica was just be replicated to. &lt;/p&gt;</comment>
                            <comment id="13478386" author="joel.bernstein" created="Wed, 17 Oct 2012 21:08:34 +0000"  >&lt;p&gt;Actually just tested, and it&apos;s a problem when you take down the lead VM as well. So it&apos;s not specific to unloading lead core.&lt;/p&gt;</comment>
                            <comment id="13479322" author="markrmiller@gmail.com" created="Thu, 18 Oct 2012 20:22:15 +0000"  >&lt;p&gt;Joel: It could just be timing - live nodes are likely faster to update than the cluster state - when you just unload a core, the live node stays - so the unloaded core will only not take part in leader election when the leader candidate gets the new cluster state. When you take down a vm, the live node goes. Perhaps this is noticed faster. In either case, it&apos;s a race.&lt;/p&gt;

&lt;p&gt;Yonik:&lt;/p&gt;

&lt;p&gt;for case one, I&apos;d like to still sync if possible - but if there are no starting versions in the updatelog, don&apos;t worry if the sync was not a success?&lt;/p&gt;

&lt;p&gt;I still have to add a test and fix for case 2.&lt;/p&gt;</comment>
                            <comment id="13479350" author="markrmiller@gmail.com" created="Thu, 18 Oct 2012 20:58:48 +0000"  >&lt;p&gt;Sorry - not starting versions - recent versions.&lt;/p&gt;</comment>
                            <comment id="13479368" author="markrmiller@gmail.com" created="Thu, 18 Oct 2012 21:13:54 +0000"  >&lt;p&gt;Here is a patch that should address point 1 - point 2 still has no fix or test.&lt;/p&gt;</comment>
                            <comment id="13480076" author="joel.bernstein" created="Fri, 19 Oct 2012 15:17:31 +0000"  >&lt;p&gt;I applied the patch to a fresh pull of the 4x branch. &lt;/p&gt;

&lt;p&gt;Then I performed the test that I detailed in the Oct 14th comment. The versions issue still causes the recovery loop. I&apos;ve attached the log (cloud2.log) that shows the recovery loop.&lt;/p&gt;

&lt;p&gt;When I do a similar test, all in the same instance it works, but as you mentioned this is likely because of the race.  &lt;/p&gt;</comment>
                            <comment id="13480094" author="markrmiller@gmail.com" created="Fri, 19 Oct 2012 16:00:16 +0000"  >&lt;p&gt;I&apos;ve got a test for the second case yonik mentioned now.&lt;/p&gt;</comment>
                            <comment id="13480126" author="yseeley@gmail.com" created="Fri, 19 Oct 2012 16:54:03 +0000"  >&lt;blockquote&gt;&lt;p&gt;for case one, I&apos;d like to still sync if possible&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Case #1 only had a single active replica up, so there&apos;s no one to sync with (or you mean try to sync with even shards marked as down?)&lt;/p&gt;
</comment>
                            <comment id="13480162" author="markrmiller@gmail.com" created="Fri, 19 Oct 2012 17:38:02 +0000"  >&lt;p&gt;I mean, in the general case, don&apos;t just let someone be the leader if they are active. Make them sync and only let them be the leader if they are successful.&lt;/p&gt;

&lt;p&gt;However, if they have no recent versions, let them be the leader even if the sync fails. This lets us keep consistency in almost all cases. True, without the sync, there will still not be data loss, but I&apos;d still like to try and force consistency if possible as well.&lt;/p&gt;</comment>
                            <comment id="13481008" author="yseeley@gmail.com" created="Sun, 21 Oct 2012 18:12:36 +0000"  >&lt;p&gt;Actually, even if we someday replicate recent versions along with the index (by adding them to the commitData in the index, etc), it may still be good to support indexes w/o version info.  On the other side of the spectrum from having indexing completely automated, some people may want the ability to create a new shard off-line and then insert it into the cluster as read-only.&lt;/p&gt;</comment>
                            <comment id="13482774" author="markrmiller@gmail.com" created="Tue, 23 Oct 2012 22:54:13 +0000"  >&lt;p&gt;In two other issues I was working on, unrelated changes seemed to start causing test fails in one of the solrcloud tests - it&apos;s a fail I had seen sometimes in the past on Apache jenkins. A fail about waiting to notice a live node drop. It seems that was caused by this - it took some time to trace it back here. One of the nodes doesn&apos;t see a live node change because he is stuck in a leader election loop.&lt;/p&gt;

&lt;p&gt;Given that, I plan on committing what I have so far - so it stops blocking my other two issues. We can then iterate further on trunk.&lt;/p&gt;</comment>
                            <comment id="13483120" author="markrmiller@gmail.com" created="Wed, 24 Oct 2012 10:25:41 +0000"  >&lt;p&gt;Okay, I&apos;ve finally got all tests passing reliably for me. I had to add SocketException to the list of exceptions that are okay to consider a peer sync success. I&apos;ll try and get this committed tonight.&lt;/p&gt;</comment>
                            <comment id="13483567" author="yseeley@gmail.com" created="Wed, 24 Oct 2012 20:33:53 +0000"  >&lt;p&gt;Trying to think if this could happen when there are versions too... say that instead of having no versions, we just have old versions from before we did the replication.  This may argue for somehow marking the start of a replication in the transaction log and then never retrieving versions older than that.&lt;/p&gt;</comment>
                            <comment id="13483595" author="yseeley@gmail.com" created="Wed, 24 Oct 2012 21:11:10 +0000"  >&lt;p&gt;Thinking of some scenarios where this could happen:&lt;/p&gt;

&lt;p&gt;1. R1,R2 both up and active, add docs 1,2,3&lt;br/&gt;
2. bring R2 down&lt;br/&gt;
3. add docs 4 through 1million&lt;br/&gt;
4. bring R2 up, peersync fails, replication is kicked off&lt;br/&gt;
5. R2 finishes replication and becomes active, but it&apos;s recent version still list 1,2,3&lt;br/&gt;
6. bring R1 down, R2 becomes the leader&lt;br/&gt;
7. bring R2 up, it does a peer-sync with R1, which looks like it has really old versions (and succeeds because of that)&lt;br/&gt;
8. if the leader (R2) does a peer-sync back with R1, it will fail (not sure of the consequences of this)&lt;/p&gt;


&lt;p&gt;Another variation... if there&apos;s an update between 6 and 7:&lt;br/&gt;
6.5. add doc 1million+1&lt;/p&gt;

&lt;p&gt;This will cause recent versions of R2 to be 1,2,3,1000001&lt;br/&gt;
It would be good to verify that peersync to the leader will either fail (causing full replication), or pick up the new document.&lt;/p&gt;</comment>
                            <comment id="13483600" author="markrmiller@gmail.com" created="Wed, 24 Oct 2012 21:18:19 +0000"  >&lt;p&gt;Currently the leader does not peer sync back to a replica coming up because it would have to buffer updates.&lt;/p&gt;

&lt;p&gt;I think that if a replica is somehow ahead of the leader when coming back, peersync should fail and it should replicate. I think since this is not a common case, that is much simpler than trying to peersync back from the leder to the replica in this case.&lt;/p&gt;</comment>
                            <comment id="13483649" author="yseeley@gmail.com" created="Wed, 24 Oct 2012 22:08:55 +0000"  >&lt;blockquote&gt;&lt;p&gt;Currently the leader does not peer sync back to a replica coming up because it would have to buffer updates.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;peer sync doesn&apos;t require buffering updates.  AFAIK, we don&apos;t do that until we realize we need to replicate?&lt;/p&gt;</comment>
                            <comment id="13483664" author="markrmiller@gmail.com" created="Wed, 24 Oct 2012 22:26:43 +0000"  >&lt;p&gt;As far as I remember, if updates are coming in when you try and peer sync, we fail it? Isn&apos;t that what capturing the starting versions is all about?&lt;/p&gt;

&lt;p&gt;When a leader syncs with his replicas on leader election, we know docs are not coming in, so we don&apos;t worry about that starting versions check - but if you want to peer sync from the leader to a replica that is coming back up, if updates are coming in, you are going to force a replication anyway. Since it&apos;s already an uncommon case, it doesn&apos;t seem worth tackling. I mention buffering, because it seemed you would have to to be able to peer sync when updates are coming in (or block updates).&lt;/p&gt;
</comment>
                            <comment id="13484611" author="markrmiller@gmail.com" created="Fri, 26 Oct 2012 00:45:57 +0000"  >&lt;p&gt;I&apos;ve committed my latest work to 4x Joel - can you do a bit more testing with a recent checkout?&lt;/p&gt;</comment>
                            <comment id="13484652" author="joel.bernstein" created="Fri, 26 Oct 2012 02:17:10 +0000"  >&lt;p&gt;I ran the Oct 14th test and the leader election worked perfectly. Then I tested shutting down the leader VM instead of unloading the loader core and this worked fine.  &lt;/p&gt;

&lt;p&gt;Then I tried a leader with two replicas that had both just been replicated to. When I unloaded the leader neither replica became leader. But this was the case that was not yet accounted for I believe.&lt;/p&gt;

&lt;p&gt;I can&apos;t think of a use case where the second scenario would happen though.&lt;/p&gt;

&lt;p&gt;The first scenario though is critical for migrating micro-shards, so it&apos;s great that you committed this.&lt;/p&gt;

&lt;p&gt;Thanks for your work on this issue.&lt;/p&gt;

&lt;p&gt;Joel&lt;/p&gt;



</comment>
                            <comment id="13484683" author="yseeley@gmail.com" created="Fri, 26 Oct 2012 04:07:53 +0000"  >&lt;blockquote&gt;&lt;p&gt;Isn&apos;t that what capturing the starting versions is all about?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;For a node starting up, yeah.  For a leader syncing to someone else - I don&apos;t think it should matter.&lt;br/&gt;
edit: OK - I think I got what you&apos;re saying now - if the new node coming up did have an extra doc, then the only way to guarantee the leader pick it up would be if not too many updates came in for either.  We could require that a sync from the leader to the replica have the list of recent versions overlap enough (else the replica would be forced to replicate), but as you say... if updates are coming in fast enough (and that is prob pretty slow) you&apos;re going to force a replication anyway.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;but if you want to peer sync from the leader to a replica that is coming back up, if updates are coming in, you are going to force a replication anyway. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If updates were coming in fast enough during the &quot;bounce&quot;... I guess so.&lt;/p&gt;</comment>
                            <comment id="13485000" author="markrmiller@gmail.com" created="Fri, 26 Oct 2012 15:31:00 +0000"  >&lt;p&gt;Okay, I&apos;m going to resolve this - we can make a new issue for the case where a replica comes up and is ahead somehow.&lt;/p&gt;</comment>
                            <comment id="13494966" author="commit-tag-bot" created="Sun, 11 Nov 2012 20:27:56 +0000"  >&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; Mark Robert Miller&lt;br/&gt;
&lt;a href=&quot;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1397672&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1397672&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-3939&quot; title=&quot;An empty or just replicated index cannot become the leader of a shard after a leader goes down.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-3939&quot;&gt;&lt;del&gt;SOLR-3939&lt;/del&gt;&lt;/a&gt;: Consider a sync attempt from leader to replica that fails due to 404 a success.&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-3940&quot; title=&quot;Rejoining the leader election incorrectly triggers the code path for a fresh cluster start rather than fail over.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-3940&quot;&gt;&lt;del&gt;SOLR-3940&lt;/del&gt;&lt;/a&gt;: Rejoining the leader election incorrectly triggers the code path for a fresh cluster start rather than fail over.&lt;/p&gt;
</comment>
                            <comment id="13610637" author="commit-tag-bot" created="Fri, 22 Mar 2013 16:23:16 +0000"  >&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; Mark Robert Miller&lt;br/&gt;
&lt;a href=&quot;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1402362&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1402362&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-3933&quot; title=&quot;Distributed commits are not guaranteed to be ordered within a request.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-3933&quot;&gt;&lt;del&gt;SOLR-3933&lt;/del&gt;&lt;/a&gt;: Distributed commits are not guaranteed to be ordered within a request.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-3939&quot; title=&quot;An empty or just replicated index cannot become the leader of a shard after a leader goes down.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-3939&quot;&gt;&lt;del&gt;SOLR-3939&lt;/del&gt;&lt;/a&gt;: An empty or just replicated index cannot become the leader of a shard after a leader goes down. &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-3971&quot; title=&quot;A collection that is created with numShards=1 turns into a numShards=2 collection after starting up a second core and not specifying numShards.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-3971&quot;&gt;&lt;del&gt;SOLR-3971&lt;/del&gt;&lt;/a&gt;: A collection that is created with numShards=1 turns into a numShards=2 collection after starting up a second core and not specifying numShards. &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-3932&quot; title=&quot;SolrCmdDistributorTest either takes 3 seconds or 3 minutes.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-3932&quot;&gt;&lt;del&gt;SOLR-3932&lt;/del&gt;&lt;/a&gt;: SolrCmdDistributorTest either takes 3 seconds or 3 minutes.&lt;/p&gt;</comment>
                            <comment id="13610642" author="commit-tag-bot" created="Fri, 22 Mar 2013 16:23:37 +0000"  >&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; Mark Robert Miller&lt;br/&gt;
&lt;a href=&quot;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1402361&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1402361&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-3933&quot; title=&quot;Distributed commits are not guaranteed to be ordered within a request.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-3933&quot;&gt;&lt;del&gt;SOLR-3933&lt;/del&gt;&lt;/a&gt;: Distributed commits are not guaranteed to be ordered within a request.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-3939&quot; title=&quot;An empty or just replicated index cannot become the leader of a shard after a leader goes down.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-3939&quot;&gt;&lt;del&gt;SOLR-3939&lt;/del&gt;&lt;/a&gt;: An empty or just replicated index cannot become the leader of a shard after a leader goes down. &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-3971&quot; title=&quot;A collection that is created with numShards=1 turns into a numShards=2 collection after starting up a second core and not specifying numShards.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-3971&quot;&gt;&lt;del&gt;SOLR-3971&lt;/del&gt;&lt;/a&gt;: A collection that is created with numShards=1 turns into a numShards=2 collection after starting up a second core and not specifying numShards. &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-3932&quot; title=&quot;SolrCmdDistributorTest either takes 3 seconds or 3 minutes.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-3932&quot;&gt;&lt;del&gt;SOLR-3932&lt;/del&gt;&lt;/a&gt;: SolrCmdDistributorTest either takes 3 seconds or 3 minutes.&lt;/p&gt;</comment>
                            <comment id="13610678" author="commit-tag-bot" created="Fri, 22 Mar 2013 16:26:22 +0000"  >&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; Mark Robert Miller&lt;br/&gt;
&lt;a href=&quot;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1397672&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1397672&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-3939&quot; title=&quot;An empty or just replicated index cannot become the leader of a shard after a leader goes down.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-3939&quot;&gt;&lt;del&gt;SOLR-3939&lt;/del&gt;&lt;/a&gt;: Consider a sync attempt from leader to replica that fails due to 404 a success.&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-3940&quot; title=&quot;Rejoining the leader election incorrectly triggers the code path for a fresh cluster start rather than fail over.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-3940&quot;&gt;&lt;del&gt;SOLR-3940&lt;/del&gt;&lt;/a&gt;: Rejoining the leader election incorrectly triggers the code path for a fresh cluster start rather than fail over.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12611547">SOLR-3940</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12549754" name="SOLR-3939.patch" size="10910" author="markrmiller@gmail.com" created="Thu, 18 Oct 2012 21:13:54 +0000"/>
                            <attachment id="12548929" name="SOLR-3939.patch" size="11251" author="markrmiller@gmail.com" created="Fri, 12 Oct 2012 17:42:28 +0000"/>
                            <attachment id="12549220" name="cloud.log" size="14465" author="jbernste" created="Mon, 15 Oct 2012 21:40:38 +0000"/>
                            <attachment id="12549990" name="cloud2.log" size="25782" author="jbernste" created="Fri, 19 Oct 2012 15:17:52 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>4.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>247963</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            12 years, 35 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i095xr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>51394</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12313321" key="com.atlassian.jira.toolkit:message">
                        <customfieldname>Solr Mailing List Info</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>