<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 04:07:36 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SOLR-7819] ZkController.ensureReplicaInLeaderInitiatedRecovery does not respect retryOnConnLoss</title>
                <link>https://issues.apache.org/jira/browse/SOLR-7819</link>
                <project id="12310230" key="SOLR">Solr</project>
                    <description>&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-7245&quot; title=&quot;Temporary ZK election or connection loss should not stall indexing due to LIR&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-7245&quot;&gt;&lt;del&gt;SOLR-7245&lt;/del&gt;&lt;/a&gt; added a retryOnConnLoss parameter to ZkController.ensureReplicaInLeaderInitiatedRecovery so that indexing threads do not hang during a partition on ZK operations. However, some of those changes were unintentionally reverted by &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-7336&quot; title=&quot;Add State enum to Replica&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-7336&quot;&gt;&lt;del&gt;SOLR-7336&lt;/del&gt;&lt;/a&gt; in 5.2.&lt;/p&gt;

&lt;p&gt;I found this while running Jepsen tests on 5.2.1 where a hung update managed to put a leader into a &apos;down&apos; state (I&apos;m still investigating and will open a separate issue about this problem).&lt;/p&gt;</description>
                <environment></environment>
        <key id="12846868">SOLR-7819</key>
            <summary>ZkController.ensureReplicaInLeaderInitiatedRecovery does not respect retryOnConnLoss</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="shalin">Shalin Shekhar Mangar</assignee>
                                    <reporter username="shalin">Shalin Shekhar Mangar</reporter>
                        <labels>
                            <label>Jepsen</label>
                    </labels>
                <created>Wed, 22 Jul 2015 11:44:03 +0000</created>
                <updated>Mon, 9 May 2016 18:51:27 +0000</updated>
                            <resolved>Thu, 10 Sep 2015 11:09:46 +0000</resolved>
                                    <version>5.2</version>
                    <version>5.2.1</version>
                                    <fixVersion>5.4</fixVersion>
                    <fixVersion>6.0</fixVersion>
                                    <component>SolrCloud</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="14639380" author="shalinmangar" created="Thu, 23 Jul 2015 19:11:51 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=andyetitmoves&quot; class=&quot;user-hover&quot; rel=&quot;andyetitmoves&quot;&gt;andyetitmoves&lt;/a&gt; - It looks like the commits for &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-7245&quot; title=&quot;Temporary ZK election or connection loss should not stall indexing due to LIR&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-7245&quot;&gt;&lt;del&gt;SOLR-7245&lt;/del&gt;&lt;/a&gt; only added a retryOnConnLoss parameter but it was never used inside the ZkController.updateLeaderInitiatedRecoveryState method?&lt;/p&gt;

&lt;p&gt;Also, now that I am thinking about this change, is it really safe? For example, if a leader was not able to write to a &apos;live&apos; replica, and during the LIR process if the leader couldn&apos;t complete a ZK operation (because retryOnConnLoss=false) then  LIR won&apos;t be set and updates can be missed. Also, the code as it is currently written, bails on a ConnectionLossException and doesn&apos;t even start a LIR thread which is bad.&lt;/p&gt;

&lt;p&gt;I think not having a thread wait for LIR related activity is a noble cause but we should move the entire LIR logic to a background thread which must retry on connection loss until it either succeeds or a session expired exception is thrown.&lt;/p&gt;

&lt;p&gt;Thoughts?&lt;/p&gt;</comment>
                            <comment id="14639404" author="andyetitmoves" created="Thu, 23 Jul 2015 19:45:06 +0000"  >&lt;p&gt;Duh, this is why we need a good test for this (I gave up after trying a bit in the original ticket), and I need to pay attention to automated merges more. Looks like my initial patch had the change, but when I merged with &lt;a href=&quot;https://svn.apache.org/viewvc?view=revision&amp;amp;revision=1666825&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;your changes&lt;/a&gt; for &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-7109&quot; title=&quot;Indexing threads stuck during network partition can put leader into down state&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-7109&quot;&gt;&lt;del&gt;SOLR-7109&lt;/del&gt;&lt;/a&gt;, looks like the local variable use just got removed &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;I get your concern, I think we already do this, look at DistributedUpdateProcessor.java around line 883, if we are unable to set the LIR node, we start a thread to keep retrying the node set. We just need to return false in the connection loss case as well, we currently do it only if the node is not live (and hence we didnt even bother setting the node).&lt;/p&gt;</comment>
                            <comment id="14640506" author="shalinmangar" created="Fri, 24 Jul 2015 14:24:37 +0000"  >&lt;blockquote&gt;&lt;p&gt;I think we already do this, look at DistributedUpdateProcessor.java around line 883, if we are unable to set the LIR node, we start a thread to keep retrying the node set.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Umm, it looks the reverse to me. If we are unable to set the LIR node or if there is an exception then sendRecoveryCommand=false and we do not create the LeaderInitiatedRecoveryThread at all?&lt;/p&gt;</comment>
                            <comment id="14642603" author="shalinmangar" created="Mon, 27 Jul 2015 11:48:35 +0000"  >&lt;p&gt;Here&apos;s a patch which:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Adds retryOnConnLoss in ZkController&apos;s ensureReplicaInLeaderInitiatedRecovery, updateLeaderInitiatedRecoveryState and markShardAsDownIfLeader method.&lt;/li&gt;
	&lt;li&gt;Starts a LIR thread if leader cannot mark replica as down on connection loss. Earlier a session loss or connection loss both would skip starting the LIR thread.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;I&apos;m still running Solr&apos;s integration and jepsen tests.&lt;/p&gt;

&lt;p&gt;This causes a subtle change in behavior which is best analyzed with two different scenarios:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Leader fails to send an update to replica but also suffers a temporary blip in its ZK connection during the DistributedUpdateProcessor&apos;s doFinish method
	&lt;ol&gt;
		&lt;li&gt;Currently, a few indexing threads will hang but eventually succeed in marking the &apos;replica&apos; as down and the leader will start a new LIR thread to ask the replica to recover.&lt;/li&gt;
		&lt;li&gt;With this patch, the indexing threads do not hang but a connection loss exception is thrown. At this point, we started a new LIR thread to ask the replica to recover. Although this removes the safety of explicitly marking the &apos;replica&apos; as down, the LIR thread does provide us a timeout-based safety of making sure that the replica does recover from the leader.&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
	&lt;li&gt;Leader fails to send an update to replica but also suffers a long network partition between itself and ZK server during DUP.doFinish method.
	&lt;ol&gt;
		&lt;li&gt;Currently, a few indexing threads will hang in ZkController.ensureReplicaInLeaderInitiatedRecovery until the ZK operations time out because of connection loss or session loss and no LIR thread will be created. This seems okay because the current connection loss timeout value is higher than ZK session expiration time and session loss means that ZK has determined that our session has expired already. In both cases, a new leader election should have happened and there&apos;s no need to put the replica as &apos;down&apos;.&lt;/li&gt;
		&lt;li&gt;With this patch, the difference is that the indexing threads do not hang and the ensureReplicaInLeaderInitiatedRecovery returns immediately with a connection loss exception. A new LIR thread &lt;b&gt;is&lt;/b&gt; started in this scenario. This is also fine because we were not able to mark the replica as &apos;down&apos; and we aren&apos;t sure that the session has expired so it is important that we start the LIR thread to ask the replica to recover. Even if a new leader has been elected, there&apos;s no major harm done by asking the replica to recover.&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;So, net-net this patch doesn&apos;t seem to introduce any new problems in the system.&lt;/p&gt;</comment>
                            <comment id="14646066" author="shalinmangar" created="Wed, 29 Jul 2015 14:01:49 +0000"  >&lt;p&gt;Hmm, this last patch isn&apos;t quite right because it can create multiple LIR threads for the same replica on connection loss.&lt;/p&gt;

&lt;p&gt;For example, I found the following in the logs in one of the nodes. Here 4 LIR threads were created to ask the same replica to recover:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;2015-07-29 13:21:24.629 INFO  (updateExecutor-2-thread-18-processing-x:jepsen5x3_shard2_replica2 r:core_node1 http:&lt;span class=&quot;code-comment&quot;&gt;////n1:8983//solr//jepsen5x3_shard2_replica1// n:n5:8983_solr s:shard2 c:jepsen5x3) [c:jepsen5x3 s:shard2 r:core_node1 x:jepsen5x3_shard2_replica2] o.a.s.c.LeaderInitiatedRecoveryThread LeaderInitiatedRecoveryThread-jepsen5x3_shard2_replica1 completed successfully after running &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 0 secs
&lt;/span&gt;2015-07-29 13:21:24.978 INFO  (updateExecutor-2-thread-19-processing-x:jepsen5x3_shard2_replica2 r:core_node1 http:&lt;span class=&quot;code-comment&quot;&gt;////n1:8983//solr//jepsen5x3_shard2_replica1// n:n5:8983_solr s:shard2 c:jepsen5x3) [c:jepsen5x3 s:shard2 r:core_node1 x:jepsen5x3_shard2_replica2] o.a.s.c.c.ZkStateReader Updating data &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; jepsen5x3 to ver 95
&lt;/span&gt;2015-07-29 13:21:24.978 WARN  (updateExecutor-2-thread-19-processing-x:jepsen5x3_shard2_replica2 r:core_node1 http:&lt;span class=&quot;code-comment&quot;&gt;////n1:8983//solr//jepsen5x3_shard2_replica1// n:n5:8983_solr s:shard2 c:jepsen5x3) [c:jepsen5x3 s:shard2 r:core_node1 x:jepsen5x3_shard2_replica2] o.a.s.c.LeaderInitiatedRecoveryThread Stop trying to send recovery command to downed replica core=jepsen5x3_shard2_replica1,coreNodeName=core_node2 on n1:8983_solr because core_node1 is no longer the leader! New leader is core_node2
&lt;/span&gt;2015-07-29 13:21:24.978 INFO  (updateExecutor-2-thread-19-processing-x:jepsen5x3_shard2_replica2 r:core_node1 http:&lt;span class=&quot;code-comment&quot;&gt;////n1:8983//solr//jepsen5x3_shard2_replica1// n:n5:8983_solr s:shard2 c:jepsen5x3) [c:jepsen5x3 s:shard2 r:core_node1 x:jepsen5x3_shard2_replica2] o.a.s.c.LeaderInitiatedRecoveryThread LeaderInitiatedRecoveryThread-jepsen5x3_shard2_replica1 completed successfully after running &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 39 secs
&lt;/span&gt;2015-07-29 13:21:24.979 INFO  (updateExecutor-2-thread-21-processing-x:jepsen5x3_shard2_replica2 r:core_node1 http:&lt;span class=&quot;code-comment&quot;&gt;////n1:8983//solr//jepsen5x3_shard2_replica1// n:n5:8983_solr s:shard2 c:jepsen5x3) [c:jepsen5x3 s:shard2 r:core_node1 x:jepsen5x3_shard2_replica2] o.a.s.c.c.ZkStateReader Updating data &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; jepsen5x3 to ver 95
&lt;/span&gt;2015-07-29 13:21:24.979 WARN  (updateExecutor-2-thread-21-processing-x:jepsen5x3_shard2_replica2 r:core_node1 http:&lt;span class=&quot;code-comment&quot;&gt;////n1:8983//solr//jepsen5x3_shard2_replica1// n:n5:8983_solr s:shard2 c:jepsen5x3) [c:jepsen5x3 s:shard2 r:core_node1 x:jepsen5x3_shard2_replica2] o.a.s.c.LeaderInitiatedRecoveryThread Stop trying to send recovery command to downed replica core=jepsen5x3_shard2_replica1,coreNodeName=core_node2 on n1:8983_solr because core_node1 is no longer the leader! New leader is core_node2
&lt;/span&gt;2015-07-29 13:21:24.979 INFO  (updateExecutor-2-thread-21-processing-x:jepsen5x3_shard2_replica2 r:core_node1 http:&lt;span class=&quot;code-comment&quot;&gt;////n1:8983//solr//jepsen5x3_shard2_replica1// n:n5:8983_solr s:shard2 c:jepsen5x3) [c:jepsen5x3 s:shard2 r:core_node1 x:jepsen5x3_shard2_replica2] o.a.s.c.LeaderInitiatedRecoveryThread LeaderInitiatedRecoveryThread-jepsen5x3_shard2_replica1 completed successfully after running &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 28 secs
&lt;/span&gt;2015-07-29 13:21:24.981 INFO  (updateExecutor-2-thread-22-processing-x:jepsen5x3_shard2_replica2 r:core_node1 http:&lt;span class=&quot;code-comment&quot;&gt;////n1:8983//solr//jepsen5x3_shard2_replica1// n:n5:8983_solr s:shard2 c:jepsen5x3) [c:jepsen5x3 s:shard2 r:core_node1 x:jepsen5x3_shard2_replica2] o.a.s.c.c.ZkStateReader Updating data &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; jepsen5x3 to ver 95
&lt;/span&gt;2015-07-29 13:21:24.981 WARN  (updateExecutor-2-thread-22-processing-x:jepsen5x3_shard2_replica2 r:core_node1 http:&lt;span class=&quot;code-comment&quot;&gt;////n1:8983//solr//jepsen5x3_shard2_replica1// n:n5:8983_solr s:shard2 c:jepsen5x3) [c:jepsen5x3 s:shard2 r:core_node1 x:jepsen5x3_shard2_replica2] o.a.s.c.LeaderInitiatedRecoveryThread Stop trying to send recovery command to downed replica core=jepsen5x3_shard2_replica1,coreNodeName=core_node2 on n1:8983_solr because core_node1 is no longer the leader! New leader is core_node2
&lt;/span&gt;2015-07-29 13:21:24.981 INFO  (updateExecutor-2-thread-22-processing-x:jepsen5x3_shard2_replica2 r:core_node1 http:&lt;span class=&quot;code-comment&quot;&gt;////n1:8983//solr//jepsen5x3_shard2_replica1// n:n5:8983_solr s:shard2 c:jepsen5x3) [c:jepsen5x3 s:shard2 r:core_node1 x:jepsen5x3_shard2_replica2] o.a.s.c.LeaderInitiatedRecoveryThread LeaderInitiatedRecoveryThread-jepsen5x3_shard2_replica1 completed successfully after running &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 33 secs&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="14649671" author="shalinmangar" created="Fri, 31 Jul 2015 19:16:38 +0000"  >&lt;p&gt;This patch moves all LIR related activity inside the LIR thread. The LIR thread now publishes LIR state, publishes node state and then starts a recovery loop depending on whether LIR state was published successfully or if it failed because of session expiry or connection loss. The indexing thread only consults the local replica map to ensure that only 1 LIR thread is started for any given replica. This ensures that the indexing thread never needs to wait for ZK operations needed for LIR. All tests pass except for HttpPartitionTest.testLeaderInitiatedRecoveryCRUD whose assumptions about the LIR workflow are no longer correct.&lt;/p&gt;

&lt;p&gt;Still running more tests.&lt;/p&gt;</comment>
                            <comment id="14650674" author="andyetitmoves" created="Sun, 2 Aug 2015 09:41:51 +0000"  >&lt;p&gt;A couple of comments, looks sensible overall..&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;      log.info(&lt;span class=&quot;code-quote&quot;&gt;&quot;Node &quot;&lt;/span&gt; + replicaNodeName +
              &lt;span class=&quot;code-quote&quot;&gt;&quot; is not live, so skipping leader-initiated recovery &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; replica: core={} coreNodeName={}&quot;&lt;/span&gt;,
          replicaCoreName, replicaCoreNodeName);
      &lt;span class=&quot;code-comment&quot;&gt;// publishDownState will be &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; to avoid publishing the &lt;span class=&quot;code-quote&quot;&gt;&quot;down&quot;&lt;/span&gt; state too many times
&lt;/span&gt;      &lt;span class=&quot;code-comment&quot;&gt;// as many errors can occur together and will each call into &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; method (SOLR-6189)&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It goes ahead and does `publishDownState` still if `forcePublishState` is true, is that intentional? The caller does check for if the replica is live, but there could a race. Similarly, if our state is suspect due to zk disconnect/session (the block before this), should the force be respected?&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;      &lt;span class=&quot;code-comment&quot;&gt;// &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the replica&apos;s state is not DOWN right now, make it so ...
&lt;/span&gt;      &lt;span class=&quot;code-comment&quot;&gt;// we only really need to &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; to send the recovery command &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the node itself is &lt;span class=&quot;code-quote&quot;&gt;&quot;live&quot;&lt;/span&gt;
&lt;/span&gt;      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {

        LeaderInitiatedRecoveryThread lirThread =
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The comment doesn&apos;t make sense as the code has moved to LIRT.&lt;/p&gt;</comment>
                            <comment id="14713388" author="shalinmangar" created="Wed, 26 Aug 2015 13:14:27 +0000"  >&lt;p&gt;Bulk move to 5.4 after 5.3 release.&lt;/p&gt;</comment>
                            <comment id="14720979" author="shalinmangar" created="Sat, 29 Aug 2015 05:28:19 +0000"  >&lt;p&gt;Patch updated to trunk.&lt;/p&gt;

&lt;p&gt;Thanks for the review &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=andyetitmoves&quot; class=&quot;user-hover&quot; rel=&quot;andyetitmoves&quot;&gt;andyetitmoves&lt;/a&gt; and sorry for the delay in getting back to you.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;It goes ahead and does `publishDownState` still if `forcePublishState` is true, is that intentional?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, because if the replica somehow became &apos;active&apos; when the LIR state is still &apos;down&apos;, we want to force publish its state again. The forcePublishState=true is only set in this one scenario.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The caller does check for if the replica is live, but there could a race. Similarly, if our state is suspect due to zk disconnect/session (the block before this), should the force be respected?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think you&apos;re right. We should short-circuit the publishing part complete if replica is not live or if our state is suspect.&lt;/p&gt;

&lt;p&gt;This patch incorporates both of your review comments.&lt;/p&gt;</comment>
                            <comment id="14733730" author="shalinmangar" created="Mon, 7 Sep 2015 13:52:57 +0000"  >&lt;p&gt;My last patch had a merge error which was causing chaos monkey test failures. This patch fixes them.&lt;/p&gt;

&lt;p&gt;This still has a few nocommits &amp;#8211; the ZkControllerTest leaks threads which is due to LeaderInitiatedRecoveryThread taking on some of the responsibilities of ZkController. I think the LeaderInitiatedRecoveryThread has enough serious features that it should have its own test and not piggy back on ZkController. I&apos;ll attempt to refactor the class to make it more testable and write a unit test for it.&lt;/p&gt;</comment>
                            <comment id="14735444" author="shalinmangar" created="Tue, 8 Sep 2015 19:17:25 +0000"  >&lt;ol&gt;
	&lt;li&gt;Adds a new test: TestLeaderInitiatedRecoveryThread&lt;/li&gt;
	&lt;li&gt;Removes ZkControllerTest.testEnsureReplicaInLeaderInitiatedRecovery which is no longer correct&lt;/li&gt;
	&lt;li&gt;Removes portions of HttpPartitionTest.testLeaderInitiatedRecoveryCRUD which are no longer relevant to the new code&lt;/li&gt;
	&lt;li&gt;Fixed a bug in LeaderInitiatedRecoveryThread which would send recovery messages even when a node was not live. This is tested in the new test.&lt;/li&gt;
&lt;/ol&gt;
</comment>
                            <comment id="14736930" author="shalinmangar" created="Wed, 9 Sep 2015 14:27:51 +0000"  >&lt;ol&gt;
	&lt;li&gt;Removed some debug logging in ExecutorUtil that I accidentally left behind.&lt;/li&gt;
	&lt;li&gt;Removed the nocommit in DistributedUpdateProcessor which had a comment to the effect of &quot;not a StdNode, recovery command still gets sent once&quot; which isn&apos;t true because we short circuit errors on RetryNode at the beginning of the loop.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;I think this is ready.&lt;/p&gt;</comment>
                            <comment id="14737321" author="jira-bot" created="Wed, 9 Sep 2015 18:07:47 +0000"  >&lt;p&gt;Commit 1702067 from shalin@apache.org in branch &apos;dev/trunk&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1702067&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://svn.apache.org/r1702067&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-7819&quot; title=&quot;ZkController.ensureReplicaInLeaderInitiatedRecovery does not respect retryOnConnLoss&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-7819&quot;&gt;&lt;del&gt;SOLR-7819&lt;/del&gt;&lt;/a&gt;: ZK connection loss or session timeout do not stall indexing threads anymore and LIR activity is moved to a background thread&lt;/p&gt;</comment>
                            <comment id="14738572" author="jira-bot" created="Thu, 10 Sep 2015 10:48:20 +0000"  >&lt;p&gt;Commit 1702213 from shalin@apache.org in branch &apos;dev/branches/branch_5x&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1702213&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://svn.apache.org/r1702213&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-7819&quot; title=&quot;ZkController.ensureReplicaInLeaderInitiatedRecovery does not respect retryOnConnLoss&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-7819&quot;&gt;&lt;del&gt;SOLR-7819&lt;/del&gt;&lt;/a&gt;: ZK connection loss or session timeout do not stall indexing threads anymore and LIR activity is moved to a background thread&lt;/p&gt;</comment>
                            <comment id="14738589" author="shalinmangar" created="Thu, 10 Sep 2015 11:09:46 +0000"  >&lt;p&gt;Thanks Ramkumar for the review.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12787363">SOLR-7336</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12782000">SOLR-7245</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12754918" name="SOLR-7819.patch" size="44265" author="shalin" created="Wed, 9 Sep 2015 14:27:51 +0000"/>
                            <attachment id="12754707" name="SOLR-7819.patch" size="45520" author="shalin" created="Tue, 8 Sep 2015 19:17:25 +0000"/>
                            <attachment id="12754490" name="SOLR-7819.patch" size="30990" author="shalin" created="Mon, 7 Sep 2015 13:52:57 +0000"/>
                            <attachment id="12753120" name="SOLR-7819.patch" size="30709" author="shalin" created="Sat, 29 Aug 2015 05:28:19 +0000"/>
                            <attachment id="12748226" name="SOLR-7819.patch" size="30330" author="shalin" created="Fri, 31 Jul 2015 19:16:38 +0000"/>
                            <attachment id="12747321" name="SOLR-7819.patch" size="9394" author="shalin" created="Mon, 27 Jul 2015 11:48:35 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>6.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 10 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2hju7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12313321" key="com.atlassian.jira.toolkit:message">
                        <customfieldname>Solr Mailing List Info</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>