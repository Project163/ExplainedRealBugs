<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 03:47:14 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SOLR-5796] With many collections, leader re-election takes too long when a node dies or is rebooted, leading to some shards getting into a &quot;conflicting&quot; state about who is the leader.</title>
                <link>https://issues.apache.org/jira/browse/SOLR-5796</link>
                <project id="12310230" key="SOLR">Solr</project>
                    <description>&lt;p&gt;I&apos;m doing some testing with a 4-node SolrCloud cluster against the latest rev in branch_4x having many collections, 150 to be exact, each having 4 shards with rf=3, so 450 cores per node. Nodes are decent in terms of resources: -Xmx6g with 4 CPU - m3.xlarge&apos;s in EC2.&lt;/p&gt;

&lt;p&gt;The problem occurs when rebooting one of the nodes, say as part of a rolling restart of the cluster. If I kill one node and then wait for an extended period of time, such as 3 minutes, then all of the leaders on the downed node (roughly 150) have time to failover to another node in the cluster. When I restart the downed node, since leaders have all failed over successfully, the new node starts up and all cores assume the replica role in their respective shards. This is goodness and expected.&lt;/p&gt;

&lt;p&gt;However, if I don&apos;t wait long enough for the leader failover process to complete on the other nodes before restarting the downed node, &lt;br/&gt;
then some bad things happen. Specifically, when the dust settles, many of the previous leaders on the node I restarted get stuck in the &quot;conflicting&quot; state seen in the ZkController, starting around line 852 in branch_4x:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;852       while (!leaderUrl.equals(clusterStateLeaderUrl)) {&lt;br/&gt;
853         if (tries == 60) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {854           throw new SolrException(ErrorCode.SERVER_ERROR,855               &amp;quot;There is conflicting information about the leader of shard}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;859         Thread.sleep(1000);&lt;br/&gt;
860         tries++;&lt;br/&gt;
861         clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection, shardId,&lt;br/&gt;
862             timeoutms);&lt;br/&gt;
863         leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)&lt;br/&gt;
864             .getCoreUrl();&lt;br/&gt;
865       }&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;As you can see, the code is trying to give a little time for this problem to work itself out, 1 minute to be exact. Unfortunately, that doesn&apos;t seem to be long enough for a busy cluster that has many collections. Now, one might argue that 450 cores per node is asking too much of Solr, however I think this points to a bigger issue of the fact that a node coming up isn&apos;t aware that it went down and leader election is running on other nodes and is just being slow. Moreover, once this problem occurs, it&apos;s not clear how to fix it besides shutting the node down again and waiting for leader failover to complete.&lt;/p&gt;

&lt;p&gt;It&apos;s also interesting to me that /clusterstate.json was updated by the healthy node taking over the leader role but the /collections/&amp;lt;coll&amp;gt;leaders/shard# was not updated? I added some debugging and it seems like the overseer queue is extremely backed up with work.&lt;/p&gt;

&lt;p&gt;Maybe the solution here is to just wait longer but I also want to get some feedback from the community on other options? I know there are some plans to help scale the Overseer (i.e. &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-5476&quot; title=&quot;Overseer Role for nodes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-5476&quot;&gt;&lt;del&gt;SOLR-5476&lt;/del&gt;&lt;/a&gt;) so maybe that helps and I&apos;m trying to add more debug to see if this is really due to overseer backlog (which I suspect it is).&lt;/p&gt;

&lt;p&gt;In general, I&apos;m a little confused by the keeping of leader state in multiple places in ZK. Is there any background information on why we have leader state in /clusterstate.json and in the leader path znode?&lt;/p&gt;

&lt;p&gt;Also, here are some interesting side observations:&lt;/p&gt;

&lt;p&gt;a. If I use rf=2, then this problem doesn&apos;t occur as leader failover happens more quickly and there&apos;s less overseer work? &lt;br/&gt;
May be a red herring here, but I can consistently reproduce it with RF=3, but not with RF=2 ... suppose that is because there are only 300 cores per node versus 450 and that&apos;s just enough less work to make this issue work itself out.&lt;/p&gt;

&lt;p&gt;b. To support that many cores, I had to set -Xss256k to reduce the stack size as Solr uses a lot of threads during startup (high point was 800&apos;ish)              &lt;br/&gt;
Might be something we should recommend on the mailing list / wiki somewhere.&lt;/p&gt;</description>
                <environment>&lt;p&gt;Found on branch_4x&lt;/p&gt;</environment>
        <key id="12698003">SOLR-5796</key>
            <summary>With many collections, leader re-election takes too long when a node dies or is rebooted, leading to some shards getting into a &quot;conflicting&quot; state about who is the leader.</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="markrmiller@gmail.com">Mark Miller</assignee>
                                    <reporter username="tim.potter">Timothy Potter</reporter>
                        <labels>
                    </labels>
                <created>Fri, 28 Feb 2014 20:03:26 +0000</created>
                <updated>Mon, 9 May 2016 18:44:29 +0000</updated>
                            <resolved>Sat, 8 Mar 2014 14:59:15 +0000</resolved>
                                                    <fixVersion>4.7.1</fixVersion>
                    <fixVersion>4.8</fixVersion>
                    <fixVersion>6.0</fixVersion>
                                    <component>SolrCloud</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>10</watches>
                                                                                                                <comments>
                            <comment id="13916521" author="tim.potter" created="Fri, 28 Feb 2014 22:43:51 +0000"  >&lt;p&gt;Little more to this for today ...&lt;/p&gt;

&lt;p&gt;I tried this on more powerful nodes (m3.2xlarge) and changed the wait to tries == 180 (instead of 60) and viola, the restarted node came back as expected. This begs the question whether we should make that wait period configurable for those installations that have many collections in a cluster? To be clear, I&apos;m referring to the wait period in ZkController, while loop starting around line 852 (see above). I&apos;d prefer to have something more deterministic vs. an upper limit on waiting as that seems like a ticking time bomb in a busy cluster. I&apos;m going to try a few more ideas out over the weekend.&lt;/p&gt;</comment>
                            <comment id="13916575" author="markrmiller@gmail.com" created="Fri, 28 Feb 2014 23:35:07 +0000"  >&lt;p&gt;Cool - recently saw a user post a problem with this conflicting state - glad to see you already have a jump on it &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13916579" author="markrmiller@gmail.com" created="Fri, 28 Feb 2014 23:38:12 +0000"  >&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;This begs the question whether we should make that wait period configurable for those installations that have many collections in a cluster? To be clear, I&apos;m referring to the wait period in ZkController, while loop starting around line 852 (see above).
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I have no qualms with making any timeouts configurable, but it seems the defaults should be fairly high as well - we want to work out of the box with most reasonable setups if possible.&lt;/p&gt;

&lt;p&gt;+1 to making it configurable, but let&apos;s also crank it up.&lt;/p&gt;</comment>
                            <comment id="13918184" author="tim.potter" created="Mon, 3 Mar 2014 15:52:30 +0000"  >&lt;p&gt;Ok, sounds good. Also, let&apos;s assume that the timeout resolves this for most cores, but a couple stragglers on the end still end up in this state. I assume the solution there would be to send a core reload to those specific cores, which should re-run all the startup recovery logic. If so, I think a simple utility app that uses CloudSolrServer to find all the angry cores and reload them would be useful, which I&apos;ll also work on.&lt;/p&gt;</comment>
                            <comment id="13918249" author="tim.potter" created="Mon, 3 Mar 2014 16:47:57 +0000"  >&lt;p&gt;Quick and dirty patch (for trunk) that introduces this as a ConfigSolrXml property (sibling to leaderVoteWait). However, getting this property passed along to where it&apos;s eventually needed in ZkController is a bit ugly. Specifically,&lt;/p&gt;

&lt;p&gt;a. default value (DEFAULT_LEADER_CONFLICT_RESOLVE_WAIT) in ConfigSolr is private (did this to match the default for leaderVoteWait). This makes it hard to not break signatures on public methods, such as ZkContainer#initZooKeeper, because this class doesn&apos;t have access to the default value.&lt;/p&gt;

&lt;p&gt;b. ZkControllerTest doesn&apos;t have access to the default value either for constructing a ZkController; so I&apos;ve done something ugly and used a scalar value of 60000 in the test directly&lt;/p&gt;

&lt;p&gt;It seems like other config properties may be needed in the future, so I&apos;m wondering if it&apos;s better to just make the ConfigSolr object available from the CoreContainer, which means adding: &lt;/p&gt;

&lt;p&gt;public ConfigSolr getConfig() {&lt;br/&gt;
  return cfg;&lt;br/&gt;
}&lt;/p&gt;

&lt;p&gt;to the CoreContainer object. Is this a bad idea? If not, then ZkController can just pull this setting straight from the ConfigSolr object via the ref to CoreContainer.&lt;/p&gt;

&lt;p&gt;Or, maybe adding props like this is a rare-enough activity that I&apos;m over-thinking this and introducing a new property into the public method / ctor signatures is OK?&lt;/p&gt;</comment>
                            <comment id="13919529" author="tim.potter" created="Tue, 4 Mar 2014 15:39:30 +0000"  >&lt;p&gt;Hi Mark,&lt;/p&gt;

&lt;p&gt;So another thing I&apos;m curious about is why does leader failover take so long here? Even without increasing the wait period, 60 seconds seems like a long time for a new leader to be elected esp when it&apos;s failing over to a healthy node. I get technically why it takes so long (propagating the /clusterstate.json change around the cluster), so I guess my question is what we can do to speed that up? Is &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-5476&quot; title=&quot;Overseer Role for nodes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-5476&quot;&gt;&lt;del&gt;SOLR-5476&lt;/del&gt;&lt;/a&gt; (scaling the overseer) the solution here? Just wanted to get your thoughts on that as well.&lt;/p&gt;

&lt;p&gt;Thanks. Tim&lt;/p&gt;</comment>
                            <comment id="13919537" author="markrmiller@gmail.com" created="Tue, 4 Mar 2014 15:45:12 +0000"  >&lt;p&gt;Honestly, I don&apos;t know why it&apos;s taking so long. We should get to the bottom of it. &lt;/p&gt;

&lt;p&gt;Propagating the clusterstate.json on a fast network should not take long at all.&lt;/p&gt;

&lt;p&gt;ZooKeeper should be apple to handle a very high rate of updates and reads - I&apos;m not even sure why the Overseer is so slow at publishing state.&lt;/p&gt;

&lt;p&gt;We can find out though &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13919540" author="markrmiller@gmail.com" created="Tue, 4 Mar 2014 15:47:29 +0000"  >&lt;p&gt;When a leader tries to take over, it first tries to sync with the rest of the shard - we should look at how long this is taking. There are a variety of places we might be able to start logging time intervals to help get an idea how long various things take.&lt;/p&gt;</comment>
                            <comment id="13920645" author="andyetitmoves" created="Wed, 5 Mar 2014 08:19:01 +0000"  >&lt;p&gt;Doesn&apos;t Overseer throttle updates? It used to be a needless 1.5s before, but even now it does sleep 100ms between batches of updates I think (have to confirm with code). Maybe that&apos;s enough to slow things down enough in this case?&lt;/p&gt;</comment>
                            <comment id="13920995" author="markrmiller@gmail.com" created="Wed, 5 Mar 2014 16:06:42 +0000"  >&lt;p&gt;AFAIK, Noble and I changed that so that if the queue has the items, the overseer will plow through them.&lt;/p&gt;</comment>
                            <comment id="13921286" author="markrmiller@gmail.com" created="Wed, 5 Mar 2014 19:37:03 +0000"  >&lt;p&gt;Let&apos;s spin off the performance issue into a new JIRA issue - I&apos;ll wrap this one up.&lt;/p&gt;</comment>
                            <comment id="13921295" author="jira-bot" created="Wed, 5 Mar 2014 19:41:52 +0000"  >&lt;p&gt;Commit 1574638 from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markrmiller%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;markrmiller@gmail.com&quot;&gt;markrmiller@gmail.com&lt;/a&gt; in branch &apos;dev/trunk&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1574638&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://svn.apache.org/r1574638&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-5796&quot; title=&quot;With many collections, leader re-election takes too long when a node dies or is rebooted, leading to some shards getting into a &amp;quot;conflicting&amp;quot; state about who is the leader.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-5796&quot;&gt;&lt;del&gt;SOLR-5796&lt;/del&gt;&lt;/a&gt;: Increase how long we are willing to wait for a core to see the ZK advertised leader in it&apos;s local state.&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-5796&quot; title=&quot;With many collections, leader re-election takes too long when a node dies or is rebooted, leading to some shards getting into a &amp;quot;conflicting&amp;quot; state about who is the leader.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-5796&quot;&gt;&lt;del&gt;SOLR-5796&lt;/del&gt;&lt;/a&gt;: Make how long we are willing to wait for a core to see the ZK advertised leader in it&apos;s local state configurable.&lt;/p&gt;</comment>
                            <comment id="13921321" author="jira-bot" created="Wed, 5 Mar 2014 19:58:34 +0000"  >&lt;p&gt;Commit 1574641 from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markrmiller%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;markrmiller@gmail.com&quot;&gt;markrmiller@gmail.com&lt;/a&gt; in branch &apos;dev/branches/branch_4x&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1574641&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://svn.apache.org/r1574641&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-5796&quot; title=&quot;With many collections, leader re-election takes too long when a node dies or is rebooted, leading to some shards getting into a &amp;quot;conflicting&amp;quot; state about who is the leader.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-5796&quot;&gt;&lt;del&gt;SOLR-5796&lt;/del&gt;&lt;/a&gt;: Increase how long we are willing to wait for a core to see the ZK advertised leader in it&apos;s local state.&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-5796&quot; title=&quot;With many collections, leader re-election takes too long when a node dies or is rebooted, leading to some shards getting into a &amp;quot;conflicting&amp;quot; state about who is the leader.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-5796&quot;&gt;&lt;del&gt;SOLR-5796&lt;/del&gt;&lt;/a&gt;: Make how long we are willing to wait for a core to see the ZK advertised leader in it&apos;s local state configurable.&lt;/p&gt;</comment>
                            <comment id="13921411" author="jira-bot" created="Wed, 5 Mar 2014 21:12:42 +0000"  >&lt;p&gt;Commit 1574664 from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markrmiller%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;markrmiller@gmail.com&quot;&gt;markrmiller@gmail.com&lt;/a&gt; in branch &apos;dev/trunk&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1574664&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://svn.apache.org/r1574664&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-5796&quot; title=&quot;With many collections, leader re-election takes too long when a node dies or is rebooted, leading to some shards getting into a &amp;quot;conflicting&amp;quot; state about who is the leader.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-5796&quot;&gt;&lt;del&gt;SOLR-5796&lt;/del&gt;&lt;/a&gt;: Fix illegal API call to format.&lt;/p&gt;</comment>
                            <comment id="13921470" author="jira-bot" created="Wed, 5 Mar 2014 21:50:33 +0000"  >&lt;p&gt;Commit 1574682 from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markrmiller%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;markrmiller@gmail.com&quot;&gt;markrmiller@gmail.com&lt;/a&gt; in branch &apos;dev/branches/branch_4x&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1574682&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://svn.apache.org/r1574682&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-5796&quot; title=&quot;With many collections, leader re-election takes too long when a node dies or is rebooted, leading to some shards getting into a &amp;quot;conflicting&amp;quot; state about who is the leader.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-5796&quot;&gt;&lt;del&gt;SOLR-5796&lt;/del&gt;&lt;/a&gt;: Fix illegal API call to format.&lt;/p&gt;</comment>
                            <comment id="13922711" author="tim.potter" created="Thu, 6 Mar 2014 16:28:44 +0000"  >&lt;p&gt;I&apos;m still a little concern about a couple of things:&lt;/p&gt;

&lt;p&gt;1) why is the leader &quot;state&quot; stored in two places in ZooKeeper (/clusterstate.json and /collections/&amp;lt;coll&amp;gt;/leaders/shard#)? I&apos;m sure there is a good reason for this but can&apos;t see why &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;2) if the timeout still occurs (as we don&apos;t want to wait forever), can&apos;t the node with the conflict just favor what&apos;s in the leader path assuming that replica is active and agrees? In other words, instead of throwing an exception and then just ending up a &quot;down&quot; state, why can&apos;t the replica seeing the conflict just go with what ZooKeeper says?&lt;/p&gt;

&lt;p&gt;I&apos;m digging into leader failover timing / error handling today.&lt;/p&gt;

&lt;p&gt;Thanks. Tim&lt;/p&gt;</comment>
                            <comment id="13923209" author="markrmiller@gmail.com" created="Thu, 6 Mar 2014 22:55:15 +0000"  >&lt;ul&gt;
	&lt;li&gt;1: At one time, the leader was not actually in the cluster state I think...anyway, we do make use of it in one case in recovery where we make sure we have an update cloudstate that includes the latest leader for the shard. The ZK node is always up to date, clusterstate info can be stale. That is the main difference.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;2: Perhaps - if the clusterstate won&apos;t update though, seems like something is probably fairly wrong and we may want to favor not claiming we are active.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13924889" author="markrmiller@gmail.com" created="Sat, 8 Mar 2014 14:59:15 +0000"  >&lt;p&gt;Lets roll out a perf investigation and other concerns into new issues.&lt;/p&gt;</comment>
                            <comment id="13934039" author="tim.potter" created="Thu, 13 Mar 2014 20:53:38 +0000"  >&lt;p&gt;Just wanted to add a final comment here that it helps immensely to restart the node that is hosting the overseer last as part of a rolling restart. It&apos;s very subtle but there&apos;s a chance that each restart requires the overseer to need to be failed-over to a node each time you do a rolling restart. Consider 4 nodes: node1, node2, node3, node4. If the overseer is on node1, if I started my rolling restart using sequence: 2,3,4,1, then the overseer only needs to migrate once. This seems to really help stability during a rolling restart of the cluster (for obvious reasons).&lt;/p&gt;</comment>
                            <comment id="13937957" author="markrmiller@gmail.com" created="Mon, 17 Mar 2014 16:06:06 +0000"  >&lt;p&gt;Do we have a JIRA issue for the instability you mention in the failover? I can guess what it is... but we should track it and harden it.&lt;/p&gt;</comment>
                            <comment id="13945261" author="steve_rowe" created="Mon, 24 Mar 2014 15:53:38 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markrmiller%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;markrmiller@gmail.com&quot;&gt;markrmiller@gmail.com&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tim.potter&quot; class=&quot;user-hover&quot; rel=&quot;tim.potter&quot;&gt;tim.potter&lt;/a&gt;, any reason not to backport this to 4.7.1?&lt;/p&gt;</comment>
                            <comment id="13946064" author="jira-bot" created="Tue, 25 Mar 2014 02:59:23 +0000"  >&lt;p&gt;Commit 1581195 from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=steve_rowe&quot; class=&quot;user-hover&quot; rel=&quot;steve_rowe&quot;&gt;steve_rowe&lt;/a&gt; in branch &apos;dev/branches/lucene_solr_4_7&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1581195&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://svn.apache.org/r1581195&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-5796&quot; title=&quot;With many collections, leader re-election takes too long when a node dies or is rebooted, leading to some shards getting into a &amp;quot;conflicting&amp;quot; state about who is the leader.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-5796&quot;&gt;&lt;del&gt;SOLR-5796&lt;/del&gt;&lt;/a&gt;: Increase how long we are willing to wait for a core to see the ZK advertised leader in it&apos;s local state.&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-5796&quot; title=&quot;With many collections, leader re-election takes too long when a node dies or is rebooted, leading to some shards getting into a &amp;quot;conflicting&amp;quot; state about who is the leader.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-5796&quot;&gt;&lt;del&gt;SOLR-5796&lt;/del&gt;&lt;/a&gt;: Make how long we are willing to wait for a core to see the ZK advertised leader in it&apos;s local state configurable. &lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-5796&quot; title=&quot;With many collections, leader re-election takes too long when a node dies or is rebooted, leading to some shards getting into a &amp;quot;conflicting&amp;quot; state about who is the leader.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-5796&quot;&gt;&lt;del&gt;SOLR-5796&lt;/del&gt;&lt;/a&gt;: Fix illegal API call to format. (merged branch_4x revisions r1574641 and r1574682)&lt;/p&gt;</comment>
                            <comment id="13946068" author="jira-bot" created="Tue, 25 Mar 2014 03:00:07 +0000"  >&lt;p&gt;Commit 1581196 from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=steve_rowe&quot; class=&quot;user-hover&quot; rel=&quot;steve_rowe&quot;&gt;steve_rowe&lt;/a&gt; in branch &apos;dev/trunk&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1581196&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://svn.apache.org/r1581196&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-5796&quot; title=&quot;With many collections, leader re-election takes too long when a node dies or is rebooted, leading to some shards getting into a &amp;quot;conflicting&amp;quot; state about who is the leader.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-5796&quot;&gt;&lt;del&gt;SOLR-5796&lt;/del&gt;&lt;/a&gt;: move CHANGES.txt entries to 4.7.1 section&lt;/p&gt;</comment>
                            <comment id="13946069" author="jira-bot" created="Tue, 25 Mar 2014 03:01:25 +0000"  >&lt;p&gt;Commit 1581198 from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=steve_rowe&quot; class=&quot;user-hover&quot; rel=&quot;steve_rowe&quot;&gt;steve_rowe&lt;/a&gt; in branch &apos;dev/branches/branch_4x&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1581198&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://svn.apache.org/r1581198&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-5796&quot; title=&quot;With many collections, leader re-election takes too long when a node dies or is rebooted, leading to some shards getting into a &amp;quot;conflicting&amp;quot; state about who is the leader.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-5796&quot;&gt;&lt;del&gt;SOLR-5796&lt;/del&gt;&lt;/a&gt;: move CHANGES.txt entries to 4.7.1 section (merged trunk r1581196)&lt;/p&gt;</comment>
                            <comment id="13957731" author="steve_rowe" created="Wed, 2 Apr 2014 15:03:36 +0000"  >&lt;p&gt;Bulk close 4.7.1 issues&lt;/p&gt;</comment>
                            <comment id="13963658" author="rich mayfield" created="Wed, 9 Apr 2014 00:32:27 +0000"  >&lt;p&gt;I ran into this problem also (we have upwards of 1000 cores in each replica).&lt;/p&gt;

&lt;p&gt;I greatly appreciate the fix and the advice about rolling the overseer last. Can you point me to any other recommendations and caveats for doing a rolling restart?&lt;/p&gt;
</comment>
                    </comments>
                    <attachments>
                            <attachment id="12632300" name="SOLR-5796.patch" size="8494" author="tim.potter" created="Mon, 3 Mar 2014 16:49:39 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>376477</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            11 years, 33 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1svhb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>376773</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12313321" key="com.atlassian.jira.toolkit:message">
                        <customfieldname>Solr Mailing List Info</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>