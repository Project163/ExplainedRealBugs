<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 03:37:15 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SOLR-4744] Version conflict error during shard split test</title>
                <link>https://issues.apache.org/jira/browse/SOLR-4744</link>
                <project id="12310230" key="SOLR">Solr</project>
                    <description>&lt;p&gt;ShardSplitTest fails sometimes with the following error:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;[junit4:junit4]   1&amp;gt; INFO  - 2013-04-14 19:05:26.861; org.apache.solr.cloud.Overseer$ClusterStateUpdater; Update shard state invoked &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; collection: collection1
[junit4:junit4]   1&amp;gt; INFO  - 2013-04-14 19:05:26.861; org.apache.solr.cloud.Overseer$ClusterStateUpdater; Update shard state shard1 to inactive
[junit4:junit4]   1&amp;gt; INFO  - 2013-04-14 19:05:26.861; org.apache.solr.cloud.Overseer$ClusterStateUpdater; Update shard state shard1_0 to active
[junit4:junit4]   1&amp;gt; INFO  - 2013-04-14 19:05:26.861; org.apache.solr.cloud.Overseer$ClusterStateUpdater; Update shard state shard1_1 to active
[junit4:junit4]   1&amp;gt; INFO  - 2013-04-14 19:05:26.873; org.apache.solr.update.processor.LogUpdateProcessor; [collection1] webapp= path=/update params={wt=javabin&amp;amp;version=2} {add=[169 (1432319507166134272)]} 0 2
[junit4:junit4]   1&amp;gt; INFO  - 2013-04-14 19:05:26.877; org.apache.solr.common.cloud.ZkStateReader$2; A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/clusterstate.json, has occurred - updating... (live nodes size: 5)
[junit4:junit4]   1&amp;gt; INFO  - 2013-04-14 19:05:26.877; org.apache.solr.common.cloud.ZkStateReader$2; A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/clusterstate.json, has occurred - updating... (live nodes size: 5)
[junit4:junit4]   1&amp;gt; INFO  - 2013-04-14 19:05:26.877; org.apache.solr.common.cloud.ZkStateReader$2; A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/clusterstate.json, has occurred - updating... (live nodes size: 5)
[junit4:junit4]   1&amp;gt; INFO  - 2013-04-14 19:05:26.877; org.apache.solr.common.cloud.ZkStateReader$2; A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/clusterstate.json, has occurred - updating... (live nodes size: 5)
[junit4:junit4]   1&amp;gt; INFO  - 2013-04-14 19:05:26.877; org.apache.solr.common.cloud.ZkStateReader$2; A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/clusterstate.json, has occurred - updating... (live nodes size: 5)
[junit4:junit4]   1&amp;gt; INFO  - 2013-04-14 19:05:26.877; org.apache.solr.common.cloud.ZkStateReader$2; A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/clusterstate.json, has occurred - updating... (live nodes size: 5)
[junit4:junit4]   1&amp;gt; INFO  - 2013-04-14 19:05:26.884; org.apache.solr.update.processor.LogUpdateProcessor; [collection1_shard1_1_replica1] webapp= path=/update params={distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:41028/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;distrib.from.parent=shard1&amp;amp;version=2} {} 0 1
&lt;/span&gt;[junit4:junit4]   1&amp;gt; INFO  - 2013-04-14 19:05:26.885; org.apache.solr.update.processor.LogUpdateProcessor; [collection1] webapp= path=/update params={distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:41028/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;distrib.from.parent=shard1&amp;amp;version=2} {add=[169 (1432319507173474304)]} 0 2
&lt;/span&gt;[junit4:junit4]   1&amp;gt; ERROR - 2013-04-14 19:05:26.885; org.apache.solr.common.SolrException; shard update error StdNode: http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:41028/collection1_shard1_1_replica1/:org.apache.solr.common.SolrException: version conflict &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 169 expected=1432319507173474304 actual=-1
&lt;/span&gt;[junit4:junit4]   1&amp;gt; 	at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:404)
[junit4:junit4]   1&amp;gt; 	at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:181)
[junit4:junit4]   1&amp;gt; 	at org.apache.solr.update.SolrCmdDistributor$1.call(SolrCmdDistributor.java:332)
[junit4:junit4]   1&amp;gt; 	at org.apache.solr.update.SolrCmdDistributor$1.call(SolrCmdDistributor.java:306)
[junit4:junit4]   1&amp;gt; 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
[junit4:junit4]   1&amp;gt; 	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
[junit4:junit4]   1&amp;gt; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[junit4:junit4]   1&amp;gt; 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
[junit4:junit4]   1&amp;gt; 	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
[junit4:junit4]   1&amp;gt; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
[junit4:junit4]   1&amp;gt; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[junit4:junit4]   1&amp;gt; 	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:679)
[junit4:junit4]   1&amp;gt; 
[junit4:junit4]   1&amp;gt; INFO  - 2013-04-14 19:05:26.886; org.apache.solr.update.processor.DistributedUpdateProcessor; &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; and ask http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:41028 to recover&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The failure is hard to reproduce and very timing sensitive. These kind of failures have always been seen right after &quot;updateshardstate&quot; action.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12643813">SOLR-4744</key>
            <summary>Version conflict error during shard split test</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Minor</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="shalin">Shalin Shekhar Mangar</assignee>
                                    <reporter username="shalin">Shalin Shekhar Mangar</reporter>
                        <labels>
                    </labels>
                <created>Sun, 21 Apr 2013 08:33:00 +0000</created>
                <updated>Tue, 18 Jun 2013 16:52:39 +0000</updated>
                            <resolved>Thu, 6 Jun 2013 18:59:41 +0000</resolved>
                                    <version>4.3</version>
                                    <fixVersion>4.3.1</fixVersion>
                    <fixVersion>4.4</fixVersion>
                                    <component>SolrCloud</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="13655181" author="shalinmangar" created="Sat, 11 May 2013 07:02:02 +0000"  >&lt;p&gt;Consider the following scenario&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;The overseer collection processor asks overseer to update the state of parent to INACTIVE and the sub shards to ACTIVE&lt;/li&gt;
	&lt;li&gt;The parent shard leader receives an update request&lt;/li&gt;
	&lt;li&gt;The parent shard leader thinks that it is still the leader of an ACTIVE shard and therefore tries to send the request to the sub shard leaders (FROMLEADER update containing &quot;from.shard.parent&quot; param). This is done asynchronously so the client has already been given a success status.&lt;/li&gt;
	&lt;li&gt;The sub shard leader receives such a request but it&apos;s cluster state is already up to date and therefore rejects the update saying that it is already a leader and not in construction state any more.&lt;/li&gt;
	&lt;li&gt;The parent shard leader asks sub shard leader to recover which is basically no-op for sub shard leaders&lt;/li&gt;
	&lt;li&gt;The sub shard misses such a document update&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-4795&quot; title=&quot;Sub shard leader should not accept any updates from parent after it goes active&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-4795&quot;&gt;&lt;del&gt;SOLR-4795&lt;/del&gt;&lt;/a&gt; exposed the underlying problem clearly. The exceptions in the log on jenkins are now:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;[junit4:junit4]   1&amp;gt; INFO  - 2013-05-10 17:12:00.128; org.apache.solr.update.processor.LogUpdateProcessor; [collection1_shard1_1_replica1] webapp=/sx path=/update params={distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:47193/sx/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;distrib.from.parent=shard1&amp;amp;version=2} {} 0 1
&lt;/span&gt;[junit4:junit4]   1&amp;gt; INFO  - 2013-05-10 17:12:00.128; org.apache.solr.update.processor.LogUpdateProcessor; [collection1] webapp=/sx path=/update params={distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:47193/sx/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;distrib.from.parent=shard1&amp;amp;version=2} {add=[296 (1434667890899943424)]} 0 1
&lt;/span&gt;[junit4:junit4]   1&amp;gt; ERROR - 2013-05-10 17:12:00.129; org.apache.solr.common.SolrException; org.apache.solr.common.SolrException: Request says it is coming from parent shard leader but we are not in construction state
[junit4:junit4]   1&amp;gt; 	at org.apache.solr.update.processor.DistributedUpdateProcessor.doDefensiveChecks(DistributedUpdateProcessor.java:327)
[junit4:junit4]   1&amp;gt; 	at org.apache.solr.update.processor.DistributedUpdateProcessor.setupRequest(DistributedUpdateProcessor.java:232)
[junit4:junit4]   1&amp;gt; 	at org.apache.solr.update.processor.DistributedUpdateProcessor.processAdd(DistributedUpdateProcessor.java:394)
[junit4:junit4]   1&amp;gt; 	at org.apache.solr.update.processor.LogUpdateProcessor.processAdd(LogUpdateProcessorFactory.java:100)
[junit4:junit4]   1&amp;gt; 	at org.apache.solr.handler.loader.XMLLoader.processUpdate(XMLLoader.java:246)
[junit4:junit4]   1&amp;gt; 	at org.apache.solr.handler.loader.XMLLoader.load(XMLLoader.java:173)
[junit4:junit4]   1&amp;gt; 	at org.apache.solr.handler.UpdateRequestHandler$1.load(UpdateRequestHandler.java:92)
[junit4:junit4]   1&amp;gt; 	at org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:74)
[junit4:junit4]   1&amp;gt; 	at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:135)
[junit4:junit4]   1&amp;gt; 	at org.apache.solr.core.SolrCore.execute(SolrCore.java:1832)
[junit4:junit4]   1&amp;gt; 	at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:656)
[junit4:junit4]   1&amp;gt; 	at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:359)
[junit4:junit4]   1&amp;gt; 	at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:155)
[junit4:junit4]   1&amp;gt; 	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1419)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;These happen right after the state is switched until both parent leader and sub shard leader have the latest cluster state.&lt;/p&gt;

&lt;p&gt;The possible fixes are:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Create a new recovery strategy for sub shard replication&lt;/li&gt;
	&lt;li&gt;Replicate to sub shard leader synchronously (before local update)&lt;/li&gt;
	&lt;li&gt;Switch parent shard to INACTIVE first, wait for it to receive the cluster state and then switch sub shards to ACTIVE &amp;#8211; Clients would receive failures on updates for a short time but such failures should already be handled by clients (because of host failures), we should be okay. Sub shards failures must be handled so that we always end up with the shard range being available somewhere.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Thoughts? &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yseeley%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;yseeley@gmail.com&quot;&gt;yseeley@gmail.com&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markrmiller%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;markrmiller@gmail.com&quot;&gt;markrmiller@gmail.com&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=anshumg&quot; class=&quot;user-hover&quot; rel=&quot;anshumg&quot;&gt;anshumg&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13657460" author="yseeley@gmail.com" created="Tue, 14 May 2013 20:23:48 +0000"  >&lt;p&gt;Nice job tracking that down.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Replicate to sub shard leader synchronously (before local update)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This seems like the right fix (I had thought it was this way already).  This should include returning failure to the client of course.&lt;/p&gt;</comment>
                            <comment id="13667234" author="shalinmangar" created="Sun, 26 May 2013 08:51:49 +0000"  >&lt;p&gt;Changes:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;New syncAdd and syncDelete methods in SolrCmdDistributor which add/delete synchronously and propagate exceptions&lt;/li&gt;
	&lt;li&gt;DistributedUpdateProcessor calls cmdDistrib.syncAdd inside versionAdd because that&apos;s the only place where we have the version and the full doc and an opportunity to do remote synchronous add before local add&lt;/li&gt;
	&lt;li&gt;Similarly cmdDistrib.syncDelete is called by versionDelete and doDeleteByQuery&lt;/li&gt;
	&lt;li&gt;ShardSplitTest tests for delete-by-id&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;With these changes, any exception while forwarding updates to sub shard leaders, will result in an exception being thrown to the client. The client can then retry the operation. &lt;/p&gt;

&lt;p&gt;A code review would be helpful.&lt;/p&gt;

&lt;p&gt;Considering that without this fix, shard splitting can, in some cases, lead to data loss, we should add this to 4.3.1&lt;/p&gt;</comment>
                            <comment id="13667683" author="anshumg" created="Mon, 27 May 2013 10:31:43 +0000"  >&lt;p&gt;Looks fine to me other than one small change which I don&apos;t think is a part of your patch but would be good if fixed.&lt;/p&gt;

&lt;p&gt;DistributedUpdateProcessor.updateAdd(): Line 404&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt; if (isLeader) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {   params.set(&amp;quot;distrib.from&amp;quot;, ZkCoreNodeProps.getCoreUrl(       zkController.getBaseUrl(), req.getCore().getName()));   }&lt;/span&gt; &lt;/div&gt;

&lt;p&gt;   params.set(&quot;distrib.from&quot;, ZkCoreNodeProps.getCoreUrl(&lt;br/&gt;
       zkController.getBaseUrl(), req.getCore().getName()));&lt;/p&gt;&lt;/blockquote&gt;</comment>
                            <comment id="13668310" author="yseeley@gmail.com" created="Tue, 28 May 2013 13:54:41 +0000"  >&lt;p&gt;Hmmm, can we get away with fixing this with a much less invasive change?&lt;br/&gt;
What if we just send to sub-shards like we send to other replicas, and return a failure if the sub-shard fails (don&apos;t worry about trying to change the logic to send to a sub-shard before adding locally).  The shard is going to become inactive anyway, so it shouldn&apos;t matter if we accidentally add a document locally that goes on to be rejected by the sub-shard, right?&lt;/p&gt;
</comment>
                            <comment id="13668335" author="shalinmangar" created="Tue, 28 May 2013 14:34:10 +0000"  >&lt;blockquote&gt;&lt;p&gt;What if we just send to sub-shards like we send to other replicas, and return a failure if the sub-shard fails (don&apos;t worry about trying to change the logic to send to a sub-shard before adding locally). The shard is going to become inactive anyway, so it shouldn&apos;t matter if we accidentally add a document locally that goes on to be rejected by the sub-shard, right?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What happens with partial updates in that case? Suppose an increment operation is requested which succeeds locally but is not propagated to the sub shard. If the client retries, the index will have wrong values.&lt;/p&gt;</comment>
                            <comment id="13668344" author="yseeley@gmail.com" created="Tue, 28 May 2013 14:51:18 +0000"  >&lt;blockquote&gt;&lt;p&gt;What happens with partial updates in that case? Suppose an increment operation is requested which succeeds locally but is not propagated to the sub shard.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If we&apos;re talking about failures due to the sub-shard already being active when it receives an update from the old shard who thinks it&apos;s still the leader, then I think we&apos;re fine.  This isn&apos;t a new failure mode, but just another way that the old shard can be out of date.  For example, once a normal update is received by the new shard, the old shard will be out of date anyway.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;If the client retries, the index will have wrong values.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If the client retries to the same old shard that is no longer the leader, then the update will fail again because the sub-shard will reject it again?  We could perhaps return an error code suggesting that the client is using stale cluster state (i.e. re-read before trying the update again).&lt;/p&gt;</comment>
                            <comment id="13668351" author="shalinmangar" created="Tue, 28 May 2013 15:01:12 +0000"  >&lt;blockquote&gt;&lt;p&gt;If we&apos;re talking about failures due to the sub-shard already being active when it receives an update from the old shard who thinks it&apos;s still the leader, then I think we&apos;re fine.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, that&apos;s true. I was thinking of the general failure scenario but perhaps we can ignore it because both parent and sub shard leaders are on the same JVM?&lt;/p&gt;</comment>
                            <comment id="13668426" author="yseeley@gmail.com" created="Tue, 28 May 2013 16:37:49 +0000"  >&lt;blockquote&gt;&lt;p&gt;Yes, that&apos;s true. I was thinking of the general failure scenario but perhaps we can ignore it because both parent and sub shard leaders are on the same JVM?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah, I think that&apos;s best for now.  If it actually becomes an issue (which should be really rare), we could just cancel the split and maybe retry it from the start.&lt;/p&gt;</comment>
                            <comment id="13668522" author="shalinmangar" created="Tue, 28 May 2013 18:22:10 +0000"  >&lt;p&gt;Okay, I&apos;ll put up a patch.&lt;/p&gt;</comment>
                            <comment id="13671788" author="shalinmangar" created="Fri, 31 May 2013 20:02:00 +0000"  >&lt;p&gt;Changes:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Add and delete requests are processed after local operations but before distributed operations&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;This patch makes no changes to the versionAdd, versionDelete methods and is much less invasive.&lt;/p&gt;</comment>
                            <comment id="13673498" author="shalinmangar" created="Mon, 3 Jun 2013 20:07:36 +0000"  >&lt;p&gt;Committed.&lt;/p&gt;

&lt;p&gt;trunk r1489138&lt;br/&gt;
branch_4x 1489139&lt;br/&gt;
lucene_solr_4_3 r1489141&lt;/p&gt;</comment>
                            <comment id="13673673" author="hossman" created="Mon, 3 Jun 2013 22:14:09 +0000"  >&lt;p&gt;this change completlye broke almost every test of solr that doesn&apos;t use solrcloud...&lt;/p&gt;


&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.solr.common.SolrException: java.lang.NullPointerException
        at __randomizedtesting.SeedInfo.seed([E1CD32BA68ADB059:42A275854FFF05B6]:0)
        at org.apache.solr.util.TestHarness.update(TestHarness.java:271)
        at org.apache.solr.util.BaseTestHarness.checkUpdateStatus(BaseTestHarness.java:261)
        at org.apache.solr.util.BaseTestHarness.validateUpdate(BaseTestHarness.java:231)
        at org.apache.solr.SolrTestCaseJ4.checkUpdateU(SolrTestCaseJ4.java:481)
        at org.apache.solr.SolrTestCaseJ4.assertU(SolrTestCaseJ4.java:460)
        at org.apache.solr.SolrTestCaseJ4.assertU(SolrTestCaseJ4.java:454)
        at org.apache.solr.SolrTestCaseJ4.clearIndex(SolrTestCaseJ4.java:827)
        at org.apache.solr.BasicFunctionalityTest.testDefaultFieldValues(BasicFunctionalityTest.java:623)
...
Caused by: java.lang.NullPointerException
        at
org.apache.solr.update.processor.DistributedUpdateProcessor.doDeleteByQuery(DistributedUpdateProcessor.java:872)
        at
org.apache.solr.update.processor.DistributedUpdateProcessor.processDelete(DistributedUpdateProcessor.java:774)
        at org.apache.solr.update.processor.LogUpdateProcessor.processDelete(LogUpdateProcessorFactory.java:121)
        at org.apache.solr.handler.loader.XMLLoader.processDelete(XMLLoader.java:346)
        at org.apache.solr.handler.loader.XMLLoader.processUpdate(XMLLoader.java:277)
        at org.apache.solr.handler.loader.XMLLoader.load(XMLLoader.java:173)
        at org.apache.solr.handler.UpdateRequestHandler$1.load(UpdateRequestHandler.java:92)
        at org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:74)
        at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:135)
        at org.apache.solr.core.SolrCore.execute(SolrCore.java:1843)
        at org.apache.solr.servlet.DirectSolrConnection.request(DirectSolrConnection.java:131)
        at org.apache.solr.util.TestHarness.update(TestHarness.java:267)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Invetigating.&lt;/p&gt;</comment>
                            <comment id="13673697" author="hossman" created="Mon, 3 Jun 2013 22:32:27 +0000"  >&lt;p&gt;patch that seems to fix the NPE currenlty happening in 250+Solr tests w/o causing the shard splitting test to break  (still running full suite to be certain).&lt;/p&gt;</comment>
                            <comment id="13673747" author="hossman" created="Mon, 3 Jun 2013 23:04:23 +0000"  >&lt;p&gt;FWIW... full test on trunk seem to pass with my patch, so i&apos;ve start committing and backporting.&lt;/p&gt;

&lt;p&gt;i have no idea if this fix is &quot;correct&apos; but it is certainly better then what we had.&lt;/p&gt;

&lt;p&gt;I&apos;ll be leaving this issue open for Shalin to review &amp;amp; correct as needed.&lt;/p&gt;</comment>
                            <comment id="13673802" author="hossman" created="Mon, 3 Jun 2013 23:47:31 +0000"  >&lt;p&gt;Committed revision 1489222.&lt;br/&gt;
Committed revision 1489224.&lt;br/&gt;
Committed revision 1489231.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;i have no idea if this fix is &quot;correct&apos; but it is certainly better then what we had.&lt;/p&gt;&lt;/blockquote&gt;

&lt;blockquote&gt;&lt;p&gt;I&apos;ll be leaving this issue open for Shalin to review &amp;amp; correct as needed.&lt;/p&gt;&lt;/blockquote&gt;</comment>
                            <comment id="13673931" author="shalinmangar" created="Tue, 4 Jun 2013 01:46:35 +0000"  >&lt;p&gt;Sorry for breaking the build. Lesson learned. I&apos;m on a flight to Mumbai,  will take a look once I land in a couple of hours. &lt;/p&gt;

&lt;p&gt;Thanks Hoss for fixing this. &lt;/p&gt;</comment>
                            <comment id="13676370" author="yseeley@gmail.com" created="Wed, 5 Jun 2013 21:28:51 +0000"  >&lt;p&gt;Your changes look fine Hoss.&lt;/p&gt;

&lt;p&gt;It&apos;s not clear to me why the forward to subshard needs to be synchronous in the original committed patch, but I guess that can always be revisited later as an optimization.&lt;/p&gt;</comment>
                            <comment id="13677311" author="shalinmangar" created="Thu, 6 Jun 2013 17:56:01 +0000"  >&lt;blockquote&gt;&lt;p&gt;It&apos;s not clear to me why the forward to subshard needs to be synchronous in the original committed patch, but I guess that can always be revisited later as an optimization.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It does not need to be that way. The syncAdd and syncDelete method seemed to be the most straight forward way to achieve the goal.&lt;/p&gt;</comment>
                            <comment id="13686925" author="shalinmangar" created="Tue, 18 Jun 2013 16:52:39 +0000"  >&lt;p&gt;Bulk close after 4.3.1 release&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12604686">SOLR-3755</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12585658" name="SOLR-4744.patch" size="19907" author="shalin" created="Fri, 31 May 2013 20:02:00 +0000"/>
                            <attachment id="12584859" name="SOLR-4744.patch" size="14105" author="shalin" created="Sun, 26 May 2013 08:51:49 +0000"/>
                            <attachment id="12585972" name="SOLR-4744__no_more_NPE.patch" size="1463" author="hossman" created="Mon, 3 Jun 2013 22:32:27 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>324180</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            12 years, 23 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1jxif:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>324525</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12313321" key="com.atlassian.jira.toolkit:message">
                        <customfieldname>Solr Mailing List Info</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>