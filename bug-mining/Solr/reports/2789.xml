<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 04:10:52 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SOLR-5209] last replica removal cascades to remove shard from clusterstate</title>
                <link>https://issues.apache.org/jira/browse/SOLR-5209</link>
                <project id="12310230" key="SOLR">Solr</project>
                    <description>&lt;p&gt;The problem we saw was that unloading of an only replica of a shard deleted that shard&apos;s info from the clusterstate. Once it was gone then there was no easy way to re-create the shard (other than dropping and re-creating the whole collection&apos;s state).&lt;/p&gt;

&lt;p&gt;This seems like a bug?&lt;/p&gt;

&lt;p&gt;Overseer.java around line 600 has a comment and commented out code:&lt;br/&gt;
// TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it&lt;br/&gt;
// if (newReplicas.size() == 0 &amp;amp;&amp;amp; slice.getRange() == null) {&lt;br/&gt;
// if there are no replicas left for the slice remove it&lt;/p&gt;</description>
                <environment></environment>
        <key id="12666605">SOLR-5209</key>
            <summary>last replica removal cascades to remove shard from clusterstate</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="cpoerschke">Christine Poerschke</assignee>
                                    <reporter username="cpoerschke">Christine Poerschke</reporter>
                        <labels>
                    </labels>
                <created>Mon, 2 Sep 2013 17:16:09 +0000</created>
                <updated>Mon, 9 May 2016 18:49:33 +0000</updated>
                            <resolved>Tue, 12 Jan 2016 10:28:57 +0000</resolved>
                                    <version>4.4</version>
                                    <fixVersion>6.0</fixVersion>
                                    <component>SolrCloud</component>
                        <due></due>
                            <votes>2</votes>
                                    <watches>13</watches>
                                                                                                                <comments>
                            <comment id="13756189" author="cpoerschke" created="Mon, 2 Sep 2013 17:43:05 +0000"  >&lt;p&gt;&lt;ins&gt;original clusterstate (extract)&lt;/ins&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;shards : {
  &quot;shard1&quot;:{ &quot;range&quot;:&quot;...&quot;, &quot;replicas&quot;:{ { &quot;core&quot;:&quot;collection1_shard1&quot; } } },
  &quot;shard2&quot;:{ &quot;range&quot;:&quot;...&quot;, &quot;replicas&quot;:{ { &quot;core&quot;:&quot;collection1_shard2&quot; } } }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;ins&gt;actual clusterstate after UNLOAD of collection1_shard1&lt;/ins&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;shards : {
  &quot;shard2&quot;:{ &quot;range&quot;:&quot;...&quot;, &quot;replicas&quot;:{ { &quot;core&quot;:&quot;collection1_shard2&quot; } } }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;ins&gt;expected clusterstate after UNLOAD of collection1_shard1&lt;/ins&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;shards : {
  &quot;shard1&quot;:{ &quot;range&quot;:&quot;...&quot;, &quot;replicas&quot;:{} },
  &quot;shard2&quot;:{ &quot;range&quot;:&quot;...&quot;, &quot;replicas&quot;:{ { &quot;core&quot;:&quot;collection1_shard2&quot; } } }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13756446" author="dancollins" created="Tue, 3 Sep 2013 08:25:19 +0000"  >&lt;p&gt;When I had a look at that code, I was confused why &lt;tt&gt;removeCore&lt;/tt&gt; was even attempting to remove &quot;all empty pre allocated slices&quot;?  And it also tries to clean up collections, given we have the collections API to do that, why is a simple core unload going any further?  I can see that its &quot;nice&quot; and saves the user having to run the collections API to remove the collection, but isn&apos;t it potentially dangerous (like in this case) if we are second guessing what the user will do next?&lt;/p&gt;

&lt;p&gt;Say they are just removing all the replicas, and re-assigning them to different hosts?  They may want to leave the collection/shard ranges &quot;empty&quot; and then move the data to new hosts, and restart the cores to re-register them.  Yes, we could/should start the new ones before removing the old, but that&apos;s not enforced?&lt;/p&gt;</comment>
                            <comment id="13756714" author="shalinmangar" created="Tue, 3 Sep 2013 15:51:24 +0000"  >&lt;p&gt;I think this deserves another look. We have the deleteshard API now which can be used to completely remove a slice from the cluster state. We should remove this trappy behaviour.&lt;/p&gt;</comment>
                            <comment id="13756786" author="markrmiller@gmail.com" created="Tue, 3 Sep 2013 17:00:06 +0000"  >&lt;blockquote&gt;&lt;p&gt;given we have the collections API to do that&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We don&apos;t actually have the collections API to do that - it&apos;s simply a thin candy wrapper around SolrCore admin calls. Everything is driven by SolrCores being added or removed. There is work being done to migrate towards something where the collections API is actually large and in charge, but currently it&apos;s just a sugar wrapper.&lt;/p&gt;</comment>
                            <comment id="13756827" author="dancollins" created="Tue, 3 Sep 2013 17:46:14 +0000"  >&lt;p&gt;Ok, my bad, I wasn&apos;t clear enough.  At the user-level there is collections API and core API, and yes one is just a wrapper around the other.  But at the Overseer level, we seem to have various different sub-commands (not sure what the correct terminology for them is!): &lt;tt&gt;create_shard&lt;/tt&gt;, &lt;tt&gt;removeshard&lt;/tt&gt;, &lt;tt&gt;createcollection&lt;/tt&gt;, &lt;tt&gt;removecollection&lt;/tt&gt;, &lt;tt&gt;deletecore&lt;/tt&gt;, etc.  I appreciate this is probably historical code, but since we have these other methods, it felt like deletecore was overstepping its bounds  now &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Could submit an extra patch, but wasn&apos;t sure of the historical nature of this code, hence just a comment first to get an opinion/discussion.&lt;/p&gt;</comment>
                            <comment id="13756861" author="markrmiller@gmail.com" created="Tue, 3 Sep 2013 18:16:51 +0000"  >&lt;p&gt;Right, but the sub commands are just the wrapper calls - except the shard commands - those are new. Th delete core one is mostly about cleanup ini remember right. &lt;/p&gt;

&lt;p&gt;The problem is, the overseer and zk do not own the state. The individual cores do basically. Mostly that&apos;s due to historical stuff. We intend to change that, but it&apos;s no small feat. Until that is done, I think this is much trickier to get right than it looks. &lt;/p&gt;</comment>
                            <comment id="13867720" author="cpoerschke" created="Fri, 10 Jan 2014 11:09:32 +0000"  >&lt;p&gt;Hello. Just wanted to follow-up if we could proceed with this patch? To either reduce or completely remove the cascading behaviour UNLOAD currently has, between them DELETESHARD and DELETECOLLECTION should allow for shard and collection removal if that was to be required after UNLOAD-ing. &lt;/p&gt;

&lt;p&gt;&lt;ins&gt;original clusterstate&lt;/ins&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;&quot;collection1&quot; : {
  shards : {
    &quot;shard1&quot;:{ &quot;range&quot;:&quot;...&quot;, &quot;state&quot; : &quot;active&quot;, &quot;replicas&quot;:{ { &quot;core&quot;:&quot;collection1_shard1&quot; } } },
    &quot;shard2&quot;:{ &quot;range&quot;:&quot;...&quot;, &quot;state&quot; : &quot;active&quot;, &quot;replicas&quot;:{ { &quot;core&quot;:&quot;collection1_shard2&quot; } } }
  }, ...
},
&quot;collection2&quot; : {
  ...
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;ins&gt;current clusterstate after UNLOAD of collection1 shard1&lt;/ins&gt;&lt;br/&gt;
(The UNLOAD of the last shard1 replica cascade-triggered the removal shard1 itself.)&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;&quot;collection1&quot; : {
  shards : {
    &quot;shard2&quot;:{ &quot;range&quot;:&quot;...&quot;, &quot;state&quot; : &quot;active&quot;, &quot;replicas&quot;:{ { &quot;core&quot;:&quot;collection1_shard2&quot; } } }
  }, ...
},
&quot;collection2&quot; : {
  ...
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;ins&gt;expected clusterstate after UNLOAD of collection1 shard1&lt;/ins&gt;&lt;br/&gt;
(There are now no replica in shard1. If one wanted to get rid of the shard then DELETESHARD could be modified to allow removal of active shards without replicas (currently DELETESHARD only removes shards that are state=inactive or state=recovery or range=null).)&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;&quot;collection1&quot; : {
  shards : {
    &quot;shard1&quot;:{ &quot;range&quot;:&quot;...&quot;, &quot;state&quot; : &quot;active&quot;, &quot;replicas&quot;:{} },
    &quot;shard2&quot;:{ &quot;range&quot;:&quot;...&quot;, &quot;state&quot; : &quot;active&quot;, &quot;replicas&quot;:{ { &quot;core&quot;:&quot;collection1_shard2&quot; } } }
  }, ...
},
&quot;collection2&quot; : {
  ...
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;ins&gt;current clusterstate after UNLOAD of collection1 shard1 and UNLOAD of collection1 shard2&lt;/ins&gt;&lt;br/&gt;
(The UNLOAD of the last shard2 replica cascade-triggered the removal shard2 itself, and then the removal of the last shard cascade-triggered the removal of collection1 itself.)&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;&quot;collection2&quot; : {
  ...
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;ins&gt;expected clusterstate after UNLOAD of collection1 shard1 and UNLOAD of collection1 shard2&lt;/ins&gt;&lt;br/&gt;
(There are now no replica in shard1 and shard2. If one wanted to get rid of shard1 and shard2 then DELETESHARD could be modified and used as per above. If one wanted to get rid of collection1 itself then DELETECOLLECTION could be used.)&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;&quot;collection1&quot; : {
  shards : {
    &quot;shard1&quot;:{ &quot;range&quot;:&quot;...&quot;, &quot;state&quot; : &quot;active&quot;, &quot;replicas&quot;:{} },
    &quot;shard2&quot;:{ &quot;range&quot;:&quot;...&quot;, &quot;state&quot; : &quot;active&quot;, &quot;replicas&quot;:{} }
  }, ...
},
&quot;collection2&quot; : {
  ...
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13867762" author="noble.paul" created="Fri, 10 Jan 2014 12:30:11 +0000"  >&lt;p&gt;What is the resolution on this?&lt;/p&gt;

&lt;p&gt;+1 to not remove the slice when the last replica is gone&lt;/p&gt;

&lt;p&gt;I can take this up if this is the proposed resolution&lt;/p&gt;</comment>
                            <comment id="13867790" author="markrmiller@gmail.com" created="Fri, 10 Jan 2014 13:37:44 +0000"  >&lt;p&gt;This would need to be done in the new &quot;zk is truth&quot; mode, not in the legacy cloud mode.&lt;/p&gt;</comment>
                            <comment id="13867797" author="noble.paul" created="Fri, 10 Jan 2014 13:44:11 +0000"  >&lt;p&gt;I guess this should not matter because it would not break anything that people are doing. I would consider this a bug. Because creating a lost shard would be extremely hard because the &apos;range&apos; info would be missing and you will end up with  a &apos;broken&apos; cluster&lt;/p&gt;</comment>
                            <comment id="13867805" author="markrmiller@gmail.com" created="Fri, 10 Jan 2014 14:02:08 +0000"  >&lt;p&gt;It&apos;s not a bug, the behavior was added explicitly and has been around for a long time. If you want to remove it, its got to be in the new mode.&lt;/p&gt;</comment>
                            <comment id="13867810" author="markrmiller@gmail.com" created="Fri, 10 Jan 2014 14:06:19 +0000"  >&lt;p&gt;Just a note: The new zk=truth mode is the future. The old mode is never going to be great. It&apos;s not possible. It was built in a halfway world, and it will always seems halfway funny. We need to embrace the zk=truth mode to make things nice.&lt;/p&gt;</comment>
                            <comment id="13867840" author="markrmiller@gmail.com" created="Fri, 10 Jan 2014 14:32:25 +0000"  >&lt;p&gt;Of course, if we really need to, we can remove this, add lots of warnings about the break, notes about the new apis that allow the same thing, etc. Because there is now an alternate way to get this behavior, that is not an out of this world idea.&lt;/p&gt;

&lt;p&gt;Personally though, I&apos;d so much rather see that energy put into the new zk=truth mode, it&apos;s required for so many things to work as we want.&lt;/p&gt;</comment>
                            <comment id="13868054" author="andyetitmoves" created="Fri, 10 Jan 2014 18:04:32 +0000"  >&lt;p&gt;FWIW, newer API actually won&apos;t allow you to do exactly what this does. The shard delete API (sensibly) explicitly checks if we have a null range or if we are not an active shard. This potentially lops out an active shard with an assigned range leaving the cluster state broken. I actually don&apos;t know if there&apos;s a way to get the cluster state back to a sensible state without modifying it by hand or recreating it altogether by nuking ZK state &amp;#8211; when we tried bringing up the replica after this, it got assigned to a shard with an empty range..&lt;/p&gt;

&lt;p&gt;I guess if we plan to get to the ZK truth mode sometime soon, its fine to drop this patch. We mainly meant for this as an interim step to ensure someone doesn&apos;t accidentally burn their fingers..&lt;/p&gt;</comment>
                            <comment id="13868062" author="markrmiller@gmail.com" created="Fri, 10 Jan 2014 18:11:34 +0000"  >&lt;p&gt;This behavior existed long before hash ranges, so I&apos;d say that issue was not complete in this area.&lt;/p&gt;

&lt;p&gt;The zk=truth mode can be started quite simply - its just a new config param we add and we can say some behaviors will change over time as we complete it. I think Shalin has already created an issue for it.&lt;/p&gt;</comment>
                            <comment id="13868632" author="noble.paul" created="Sat, 11 Jan 2014 03:08:54 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-5610&quot; title=&quot;Support cluster-wide properties with an API called CLUSTERPROP&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-5610&quot;&gt;&lt;del&gt;SOLR-5610&lt;/del&gt;&lt;/a&gt; adds a the necessary property. I&apos;m committing i right away&lt;/p&gt;</comment>
                            <comment id="14237820" author="andyetitmoves" created="Mon, 8 Dec 2014 12:24:57 +0000"  >&lt;p&gt;This seems to be still relevant (in SliceMutator though rather than Overseer after the refactoring), 5.0 might be a good time to revisit this behaviour since the concerns above were more of back-compat..&lt;/p&gt;</comment>
                            <comment id="14237883" author="markrmiller@gmail.com" created="Mon, 8 Dec 2014 14:11:36 +0000"  >&lt;p&gt;Yup, now is a good time to change this. &lt;/p&gt;</comment>
                            <comment id="14258367" author="githubbot" created="Wed, 24 Dec 2014 16:44:04 +0000"  >&lt;p&gt;GitHub user cpoerschke opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/lucene-solr/pull/118&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/lucene-solr/pull/118&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-5209&quot; title=&quot;last replica removal cascades to remove shard from clusterstate&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-5209&quot;&gt;&lt;del&gt;SOLR-5209&lt;/del&gt;&lt;/a&gt;: last replica removal no longer cascades to remove shard from clusterstate&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/i#browse/SOLR-5209&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/i#browse/SOLR-5209&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/bloomberg/lucene-solr&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/bloomberg/lucene-solr&lt;/a&gt; trunk-solr-5209&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/lucene-solr/pull/118.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/lucene-solr/pull/118.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #118&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 90b1a394b50d4f456bfabab756cc64f28733e1a5&lt;br/&gt;
Author: Christine Poerschke &amp;lt;cpoerschke@bloomberg.net&amp;gt;&lt;br/&gt;
Date:   2014-12-24T12:07:56Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-5209&quot; title=&quot;last replica removal cascades to remove shard from clusterstate&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-5209&quot;&gt;&lt;del&gt;SOLR-5209&lt;/del&gt;&lt;/a&gt;: last replica removal no longer cascades to remove shard from clusterstate&lt;/p&gt;

&lt;p&gt;    old behaviour:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;last replica removal cascades to remove shard from clusterstate&lt;/li&gt;
	&lt;li&gt;last shard removal cascades to remove collection from clusterstate&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    new behaviour:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;last replica removal preserves shard within clusterstate&lt;/li&gt;
	&lt;li&gt;OverseerTest.testRemovalOfLastReplica added for replica removal logic&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    also:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;strings such as &quot;collection1&quot; and &quot;state&quot; in OverseerTest replaced with variable or enum equvivalent&lt;/li&gt;
&lt;/ul&gt;


&lt;hr /&gt;</comment>
                            <comment id="14258368" author="cpoerschke" created="Wed, 24 Dec 2014 16:46:44 +0000"  >&lt;p&gt;Replaced attached patch with Solr trunk pull request which also includes OverseerTest test case logic.&lt;/p&gt;</comment>
                            <comment id="14261246" author="andyetitmoves" created="Tue, 30 Dec 2014 16:58:24 +0000"  >&lt;p&gt;I think this might fail &lt;tt&gt;BasicDistributedZkTest.testFailedCoreCreateCleansUp&lt;/tt&gt;.. If we call core create, it happens to be the first core of the collection, and the core creation fails (due to say, a config issue) &amp;#8211; the test currently verifies if the rollback happens by checking for the absence of the collection. We could obviously change the test, but in such a case, should we be removing the collection as well? (or disallow creation of a core for a non-existent collection &amp;#8211; though that might be a bigger, disruptive end user change and not necessarily good)..&lt;/p&gt;</comment>
                            <comment id="14261249" author="markrmiller@gmail.com" created="Tue, 30 Dec 2014 17:03:55 +0000"  >&lt;p&gt;I think as part of making zk the truth we actually do want to prevent creation of cores that are not part of a zk collection. &lt;/p&gt;</comment>
                            <comment id="14277175" author="cpoerschke" created="Wed, 14 Jan 2015 16:33:47 +0000"  >&lt;p&gt;&lt;a href=&quot;https://github.com/apache/lucene-solr/pull/118&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/lucene-solr/pull/118&lt;/a&gt; now updated to remove the &lt;tt&gt;BasicDistributedZkTest.testFailedCoreCreateCleansUp&lt;/tt&gt; test and adjust &lt;tt&gt;UnloadDistributedZkTest.testUnloadShardAndCollection&lt;/tt&gt; and &lt;tt&gt;OverseerTest.testOverseerFailure&lt;/tt&gt; tests.&lt;/p&gt;

&lt;p&gt;Would agree that longer-term core creation should not auto-create a collection.&lt;/p&gt;</comment>
                            <comment id="14284062" author="anshumg" created="Tue, 20 Jan 2015 17:25:47 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markrmiller%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;markrmiller@gmail.com&quot;&gt;markrmiller@gmail.com&lt;/a&gt; is this a blocker?&lt;/p&gt;</comment>
                            <comment id="14284159" author="markrmiller@gmail.com" created="Tue, 20 Jan 2015 18:22:34 +0000"  >&lt;p&gt;Not fully. It would just stink to have to wait until 6 to remove this ugly back compat behavior. I thought I&apos;d have time to get to it last week or someone else might pick it up, but no such luck, so perhaps we just push it out. &lt;/p&gt;</comment>
                            <comment id="14743672" author="elyograg" created="Mon, 14 Sep 2015 15:30:40 +0000"  >&lt;p&gt;On IRC today:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;09:14 &amp;lt; yriveiro&amp;gt; Hi, I unloaded by accident the las replica of a shard in a
                  collection
09:14 &amp;lt; yriveiro&amp;gt; How can I recreate the shard?
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15025453" author="cpoerschke" created="Tue, 24 Nov 2015 21:29:31 +0000"  >&lt;p&gt;Am in the process of updating/rebasing the patch for this (&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-5209&quot; title=&quot;last replica removal cascades to remove shard from clusterstate&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-5209&quot;&gt;&lt;del&gt;SOLR-5209&lt;/del&gt;&lt;/a&gt;) ticket here. &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-8338&quot; title=&quot;in OverseerTest replace strings such as &amp;quot;collection1&amp;quot; and &amp;quot;state&amp;quot;&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-8338&quot;&gt;&lt;del&gt;SOLR-8338&lt;/del&gt;&lt;/a&gt; is towards that, just replacing magic strings so that the actual test changes required for &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-5209&quot; title=&quot;last replica removal cascades to remove shard from clusterstate&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-5209&quot;&gt;&lt;del&gt;SOLR-5209&lt;/del&gt;&lt;/a&gt; will then be simpler and clearer.&lt;/p&gt;</comment>
                            <comment id="15063929" author="cpoerschke" created="Fri, 18 Dec 2015 13:05:19 +0000"  >&lt;p&gt;Attaching updated patch against trunk.&lt;/p&gt;</comment>
                            <comment id="15069846" author="cpoerschke" created="Wed, 23 Dec 2015 16:40:12 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markrmiller%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;markrmiller@gmail.com&quot;&gt;markrmiller@gmail.com&lt;/a&gt; - if you have no objections then I will re-assign this ticket to myself with a view towards committing it in the first half of January or so, to trunk (and not branch_5x) in good time for the 6.0.0 release hopefully, after reviews of course.&lt;/p&gt;

&lt;p&gt;Everyone - the latest &lt;tt&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-5209&quot; title=&quot;last replica removal cascades to remove shard from clusterstate&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-5209&quot;&gt;&lt;del&gt;SOLR-5209&lt;/del&gt;&lt;/a&gt;.patch&lt;/tt&gt; attachment contains the (small) change itself, a new &lt;tt&gt;OverseerTest.testRemovalOfLastReplica()&lt;/tt&gt; test plus adjustments to existing tests. Reviews, comments, suggestions etc. welcome. Thank you.&lt;/p&gt;</comment>
                            <comment id="15092375" author="jira-bot" created="Mon, 11 Jan 2016 17:52:49 +0000"  >&lt;p&gt;Commit 1724098 from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cpoerschke&quot; class=&quot;user-hover&quot; rel=&quot;cpoerschke&quot;&gt;cpoerschke&lt;/a&gt; in branch &apos;dev/trunk&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1724098&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://svn.apache.org/r1724098&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-5209&quot; title=&quot;last replica removal cascades to remove shard from clusterstate&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-5209&quot;&gt;&lt;del&gt;SOLR-5209&lt;/del&gt;&lt;/a&gt;: Unloading or deleting the last replica of a shard now no longer cascades to remove the shard from the clusterstate.&lt;/p&gt;</comment>
                            <comment id="15093696" author="cpoerschke" created="Tue, 12 Jan 2016 10:28:57 +0000"  >&lt;p&gt;Committed to trunk only (for 6.x releases) and deliberately not committed to branch_5x since this changes existing behaviour.&lt;/p&gt;</comment>
                            <comment id="15094456" author="githubbot" created="Tue, 12 Jan 2016 18:42:01 +0000"  >&lt;p&gt;Github user cpoerschke commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/lucene-solr/pull/118#issuecomment-171007162&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/lucene-solr/pull/118#issuecomment-171007162&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-5209&quot; title=&quot;last replica removal cascades to remove shard from clusterstate&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-5209&quot;&gt;&lt;del&gt;SOLR-5209&lt;/del&gt;&lt;/a&gt; committed to trunk yesterday (&lt;a href=&quot;https://svn.apache.org/r1724098&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://svn.apache.org/r1724098&lt;/a&gt;).&lt;/p&gt;</comment>
                            <comment id="15094457" author="githubbot" created="Tue, 12 Jan 2016 18:42:02 +0000"  >&lt;p&gt;Github user cpoerschke closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/lucene-solr/pull/118&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/lucene-solr/pull/118&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15145531" author="noble.paul" created="Fri, 12 Feb 2016 23:17:52 +0000"  >&lt;p&gt;I guess we should get this into 5.5 . This looks like a serious enough problem that backward incompatibility should not be a problem&lt;/p&gt;</comment>
                            <comment id="15146134" author="markrmiller@gmail.com" created="Sat, 13 Feb 2016 19:00:21 +0000"  >&lt;p&gt;I don&apos;t agree for the same reasons we have already discussed.&lt;/p&gt;

&lt;p&gt;We reserved some leeway for 5x to fix things in this vein, but we didn&apos;t do any of it. To just make this large change in behavior out of the blue in a 5.5 release doesn&apos;t make a lot of sense to me. Best done in the 6 release.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10001">
                    <name>dependent</name>
                                            <outwardlinks description="depends upon">
                                        <issuelink>
            <issuekey id="12917056">SOLR-8352</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12915937">SOLR-8338</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12921706">SOLR-8414</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12778489" name="SOLR-5209.patch" size="17027" author="cpoerschke" created="Fri, 18 Dec 2015 13:05:19 +0000"/>
                            <attachment id="12601077" name="SOLR-5209.patch" size="1504" author="cpoerschke" created="Mon, 2 Sep 2013 17:20:14 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>346543</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 40 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1nrav:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>346844</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12313321" key="com.atlassian.jira.toolkit:message">
                        <customfieldname>Solr Mailing List Info</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>