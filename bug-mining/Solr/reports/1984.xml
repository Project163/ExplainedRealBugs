<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 03:53:55 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SOLR-6235] SyncSliceTest fails on jenkins with no live servers available error</title>
                <link>https://issues.apache.org/jira/browse/SOLR-6235</link>
                <project id="12310230" key="SOLR">Solr</project>
                    <description>&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;1 tests failed.
FAILED:  org.apache.solr.cloud.SyncSliceTest.testDistribSearch

Error Message:
No live SolrServers available to handle &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; request

Stack Trace:
org.apache.solr.client.solrj.SolrServerException: No live SolrServers available to handle &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; request
        at __randomizedtesting.SeedInfo.seed([685C57B3F25C854B:E9BAD9AB8503E577]:0)
        at org.apache.solr.client.solrj.impl.LBHttpSolrServer.request(LBHttpSolrServer.java:317)
        at org.apache.solr.client.solrj.impl.CloudSolrServer.request(CloudSolrServer.java:659)
        at org.apache.solr.client.solrj.request.QueryRequest.process(QueryRequest.java:91)
        at org.apache.solr.client.solrj.SolrServer.query(SolrServer.java:301)
        at org.apache.solr.cloud.AbstractFullDistribZkTestBase.checkShardConsistency(AbstractFullDistribZkTestBase.java:1149)
        at org.apache.solr.cloud.AbstractFullDistribZkTestBase.checkShardConsistency(AbstractFullDistribZkTestBase.java:1118)
        at org.apache.solr.cloud.SyncSliceTest.doTest(SyncSliceTest.java:236)
        at org.apache.solr.BaseDistributedSearchTestCase.testDistribSearch(BaseDistributedSearchTestCase.java:865)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="12726518">SOLR-6235</key>
            <summary>SyncSliceTest fails on jenkins with no live servers available error</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="shalin">Shalin Shekhar Mangar</assignee>
                                    <reporter username="shalin">Shalin Shekhar Mangar</reporter>
                        <labels>
                    </labels>
                <created>Thu, 10 Jul 2014 08:03:21 +0000</created>
                <updated>Mon, 9 May 2016 18:43:18 +0000</updated>
                            <resolved>Thu, 21 Aug 2014 22:36:52 +0000</resolved>
                                                    <fixVersion>4.10</fixVersion>
                    <fixVersion>6.0</fixVersion>
                                    <component>SolrCloud</component>
                    <component>Tests</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="14057467" author="shalinmangar" created="Thu, 10 Jul 2014 13:41:48 +0000"  >&lt;p&gt;Wow, crazy crazy bug! I finally found the root cause.&lt;/p&gt;

&lt;p&gt;The problem is with the leader initiated replica code which uses core name to set/get status. This works fine as long as the core names for all nodes are different but if they all happened to be &quot;collection1&quot; then we have this problem  &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;In this particular failure that I investigated:&lt;br/&gt;
&lt;a href=&quot;http://jenkins.thetaphi.de/job/Lucene-Solr-4.x-MacOSX/1667/consoleText&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://jenkins.thetaphi.de/job/Lucene-Solr-4.x-MacOSX/1667/consoleText&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here&apos;s the sequence of events:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;port:51916 - core_node1 was initially the leader, docs were indexed and then it was killed&lt;/li&gt;
	&lt;li&gt;port:51919 - core_node2 became the leader, peer sync happened, shards were checked for consistency&lt;/li&gt;
	&lt;li&gt;port:51916 - core_node1 was brought back online, it recovered from the leader, consistency check passed&lt;/li&gt;
	&lt;li&gt;port:51923 core_node3 and port:51932 core_node4 were added to the skipped servers&lt;/li&gt;
	&lt;li&gt;300 docs were indexed (to go beyond the peer sync limit)&lt;/li&gt;
	&lt;li&gt;port:51919 - core_node2 (the leader was killed)&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Here is where things get interesting:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;port:51923 core_node3 tries to become the leader and initiates sync with other replicas&lt;/li&gt;
	&lt;li&gt;In the meanwhile, a commit request from checkShardConsistency makes its way to port:51923 core_node3 (even though it&apos;s not clear whether it has indeed become the leader)&lt;/li&gt;
	&lt;li&gt;port:51923 core_node3 calls commit on all shards including port:51919 core_node2 which should&apos;ve been down but perhaps the local state at 51923 is not updated yet?&lt;/li&gt;
	&lt;li&gt;port:51923 core_node3 puts replica collection1 on 127.0.0.1:51919_ into leader-initiated recovery&lt;/li&gt;
	&lt;li&gt;port:51923 - core_node3 fails to peersync (because number of changes were too large) and rejoins election&lt;/li&gt;
	&lt;li&gt;After this point each shard that tries to become the leader fails because it thinks that it has been put under leader initiated recovery and goes into actual &quot;recovery&quot;&lt;/li&gt;
	&lt;li&gt;Of course, since there is no leader, recovery cannot happen and each shard eventually goes to &quot;recovery_failed&quot; state&lt;/li&gt;
	&lt;li&gt;Eventually the test gives up and throws an error saying that there are no live server available to handle the request.&lt;/li&gt;
&lt;/ol&gt;
</comment>
                            <comment id="14057469" author="shalinmangar" created="Thu, 10 Jul 2014 13:43:54 +0000"  >&lt;p&gt;We should use coreNode instead of core names for setting leader initiated recovery. I&apos;ll put up a patch.&lt;/p&gt;</comment>
                            <comment id="14057546" author="thelabdude" created="Thu, 10 Jul 2014 15:00:07 +0000"  >&lt;p&gt;Hi Shalin,&lt;/p&gt;

&lt;p&gt;Great find! Using coreNode is a good idea, but why would all the cores have the name &quot;collection1&quot;? Is that valid or an indication of a problem upstream from this code?&lt;/p&gt;

&lt;p&gt;Also, you raise a good point about all replicas thinking they are in leader-initiated recovery (LIR). In ElectionContext, when running shouldIBeLeader, the node will choose to not be the leader if it is in LIR. However, this could lead to no leader. My thinking there is the state is bad enough that we would need manual intervention to clear one of the LIR znodes to allow a replica to get past this point. But maybe we can do better here?&lt;/p&gt;</comment>
                            <comment id="14057566" author="markrmiller@gmail.com" created="Thu, 10 Jul 2014 15:16:48 +0000"  >&lt;blockquote&gt;&lt;p&gt;but why would all the cores have the name &quot;collection1&quot;?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It&apos;s probably historical. When we first where trying to make it easier to use SolrCloud and no Collections API existed, you could start up cores and have them be part of the same collection by giving them the same core name. This helped in trying to make a demo startup that required minimal extra work. So, most of the original tests probably just followed suit.&lt;/p&gt;

&lt;p&gt;As we get rid of predefined cores in SolrCloud and move to the collections API, that stuff will go away.&lt;/p&gt;</comment>
                            <comment id="14057613" author="shalinmangar" created="Thu, 10 Jul 2014 15:56:16 +0000"  >&lt;blockquote&gt;&lt;p&gt;but why would all the cores have the name &quot;collection1&quot;? Is that valid or an indication of a problem upstream from this code?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The reasons are what Mark said but it is a supported use-case and pretty common. Imagine stock solr running on 4 nodes - each node would have the same collection1 core name.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Also, you raise a good point about all replicas thinking they are in leader-initiated recovery (LIR). In ElectionContext, when running shouldIBeLeader, the node will choose to not be the leader if it is in LIR. However, this could lead to no leader. My thinking there is the state is bad enough that we would need manual intervention to clear one of the LIR znodes to allow a replica to get past this point. But maybe we can do better here?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Good question. With careful use of minRf, the user can retry operations and maintain consistency even if we arbitrarily elect a leader in this case. But most people won&apos;t use minRf and don&apos;t care about consistency as much as availability. For them there should be a way to get out of this mess easily. We can have a collection property (boolean + timeout value) to force elect a leader even if all shards were in LIR. What do you think?&lt;/p&gt;</comment>
                            <comment id="14057620" author="markrmiller@gmail.com" created="Thu, 10 Jul 2014 16:04:21 +0000"  >&lt;blockquote&gt;&lt;p&gt;you could start up cores and have them be part of the same collection by giving them the same core name.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If you don&apos;t specify a collection name, it also defaults to the core name - hence collection1 for the core name.&lt;/p&gt;</comment>
                            <comment id="14057622" author="shalinmangar" created="Thu, 10 Jul 2014 16:07:55 +0000"  >&lt;blockquote&gt;&lt;p&gt;We can have a collection property (boolean + timeout value) to force elect a leader even if all shards were in LIR&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In case it wasn&apos;t clear, I think it should be true by default.&lt;/p&gt;</comment>
                            <comment id="14057631" author="markrmiller@gmail.com" created="Thu, 10 Jul 2014 16:20:48 +0000"  >&lt;p&gt;Great work tracking this down!&lt;/p&gt;

&lt;p&gt;Indeed, it&apos;s a current limitation that you can have all nodes in a shard thinking they cannot be leader, even when all of them are available. This is not required by the distributed model we have at all, it&apos;s just a consequence of being over restrictive on the initial implementation - if all known replicas are participating, you should be able to get a leader. So I&apos;m not sure if this case should be optional. But iff not all known replicas are participating and you still want to force a leader, that should be optional - I think it should default to false though. I think the system should default to reasonable data safety in these cases.&lt;/p&gt;

&lt;p&gt;How best to solve this, I&apos;m not quite sure, but happy to look at a patch. How do you plan on monitoring and taking action? Via the Overseer? It seems tricky to do it from the replicas.&lt;/p&gt;</comment>
                            <comment id="14057637" author="markrmiller@gmail.com" created="Thu, 10 Jul 2014 16:22:20 +0000"  >&lt;p&gt;On another note, it almost seems we can do better than ask for a recovery on a failed commit.&lt;/p&gt;</comment>
                            <comment id="14057639" author="thelabdude" created="Thu, 10 Jul 2014 16:23:01 +0000"  >&lt;p&gt;We have a similar issue where a replica attempting to be the leader needs to wait a while to see other replicas before declaring itself the leader, see ElectionContext around line 200:&lt;/p&gt;

&lt;p&gt;    int leaderVoteWait = cc.getZkController().getLeaderVoteWait();&lt;br/&gt;
    if (!weAreReplacement) &lt;/p&gt;
{
      waitForReplicasToComeUp(weAreReplacement, leaderVoteWait);
    }

&lt;p&gt;So one quick idea might be to have the code that checks if it&apos;s in LIR see if all replicas are in LIR and if so, wait out the leaderVoteWait period and check again. If all are still in LIR, then move on with becoming the leader (in the spirit of availability).&lt;/p&gt;</comment>
                            <comment id="14057652" author="shalinmangar" created="Thu, 10 Jul 2014 16:45:43 +0000"  >&lt;blockquote&gt;&lt;p&gt;But iff not all known replicas are participating and you still want to force a leader, that should be optional - I think it should default to false though. I think the system should default to reasonable data safety in these cases.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That&apos;s the same case as the leaderVoteWait situation and we do go ahead after that amount of time even if all replicas aren&apos;t participating. Therefore, I think that we should handle it the same way. But to help people who care about consistency over availability, there should be a configurable property which bans this auto-promotion completely.&lt;/p&gt;

&lt;p&gt;In any case, we should switch to coreNodeName instead of coreName and open an issue to improve the leader election part.&lt;/p&gt;</comment>
                            <comment id="14057682" author="markrmiller@gmail.com" created="Thu, 10 Jul 2014 17:10:29 +0000"  >&lt;blockquote&gt;&lt;p&gt;That&apos;s the same case as the leaderVoteWait situation and we do go ahead after that amount of time even if all replicas aren&apos;t participating. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No, we don&apos;t - only if a new leader is elected does he try and do the wait. There are situations where that doesn&apos;t happen. This is like the issue where the leader loses the connection to zk after sending docs to replicas and then they fail and the leader asks them to recover and then you have no leader for the shard. We did a kind of workaround for that specific issue, but I&apos;ve seen it happen with other errors as well. You can certainly lose a whole shard when everyone is participating in the election - no one thinks they can be the leader because they all published recovery last.&lt;/p&gt;

&lt;p&gt;There are lots and lots of improvements to be made to recovery still - it&apos;s a baby.&lt;/p&gt;</comment>
                            <comment id="14057686" author="markrmiller@gmail.com" created="Thu, 10 Jul 2014 17:12:48 +0000"  >&lt;blockquote&gt;&lt;p&gt;only if a new leader is elected does he try and do the wait.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sorry - that line is confusing - the issue is that waiting for everyone doesn&apos;t matter. They might all be participating anyway, the wait is irrelevant. The issue comes after that code, when no one will become the leader.&lt;/p&gt;</comment>
                            <comment id="14057691" author="thelabdude" created="Thu, 10 Jul 2014 17:15:40 +0000"  >&lt;p&gt;I opened this ticket &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-6236&quot; title=&quot;Need an optional fallback mechanism for selecting a leader when all replicas are in leader-initiated recovery.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-6236&quot;&gt;&lt;del&gt;SOLR-6236&lt;/del&gt;&lt;/a&gt; for the leader election issue we&apos;re discussing, but the title might not be quite accurate &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="14057705" author="markrmiller@gmail.com" created="Thu, 10 Jul 2014 17:25:45 +0000"  >&lt;blockquote&gt;&lt;p&gt;there should be a configurable property which bans this auto-promotion completely.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That&apos;s why I&apos;m drawing the distinction between everyone participating and not everyone participating.&lt;/p&gt;

&lt;p&gt;Sometimes you can lose a shard and it&apos;s because the leader-&amp;gt;zk connection blinks. In this case, if you have all the replicas in a shard, it&apos;s safe to force an election anyway.&lt;/p&gt;

&lt;p&gt;Sometimes you lose a shard and you don&apos;t have all the replicas - in that case, it should be optional to force an election and default to false.&lt;/p&gt;</comment>
                            <comment id="14057964" author="mewmewball" created="Thu, 10 Jul 2014 21:11:55 +0000"  >&lt;p&gt;Why is core_node3 able to put core_node2 (the old leader) into LIR when core_node3 has not been elected a leader yet? (Actually, why is core_node3 processing any &quot;update&quot; at all when it&apos;s not a leader?)&lt;/p&gt;

&lt;p&gt;That&apos;s really more of a problem than the fact that the one LIR core_node3 wrote to &quot;collection1&quot; set everyone else in LIR, because what if really only core_node2 is up-to-date and it just went through a blip and came back--in this case the only right choice for leader is core_node2.&lt;/p&gt;</comment>
                            <comment id="14057969" author="mewmewball" created="Thu, 10 Jul 2014 21:13:02 +0000"  >&lt;p&gt;Obviously, this is not to say that &quot;the one LIR core_node3 wrote to &apos;collection1&apos; set everyone else in LIR&quot; is not a problem.&lt;/p&gt;</comment>
                            <comment id="14058001" author="shalinmangar" created="Thu, 10 Jul 2014 21:42:11 +0000"  >&lt;blockquote&gt;&lt;p&gt;Why is core_node3 able to put core_node2 (the old leader) into LIR when core_node3 has not been elected a leader yet? (Actually, why is core_node3 processing any &quot;update&quot; at all when it&apos;s not a leader?)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah, the discussion went in another direction but this is something I found odd and I&apos;m gonna find out why that happened.&lt;/p&gt;</comment>
                            <comment id="14058089" author="markrmiller@gmail.com" created="Thu, 10 Jul 2014 23:04:02 +0000"  >&lt;blockquote&gt;&lt;p&gt;Why is core_node3 able to put core_node2 (the old leader) into LIR when core_node3 has not been elected a leader yet? (Actually, why is core_node3 processing any &quot;update&quot; at all when it&apos;s not a leader?)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I have not followed the sequences that closely, but I would guess that it&apos;s because of how we implemented distrib commit.&lt;/p&gt;</comment>
                            <comment id="14058094" author="markrmiller@gmail.com" created="Thu, 10 Jul 2014 23:07:12 +0000"  >&lt;p&gt;That is part of my motivation for saying:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;On another note, it almost seems we can do better than ask for a recovery on a failed commit.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The current method was kind of just a least effort impl, so there might be some other things we can do as well. If I remember right, whoever gets the commit just broadcasts it out to everyone over http, including itself.&lt;/p&gt;</comment>
                            <comment id="14058096" author="mewmewball" created="Thu, 10 Jul 2014 23:08:01 +0000"  >&lt;blockquote&gt;&lt;p&gt;I would guess that it&apos;s because of how we implemented distrib commit.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;As in, anyone (non-leader) can distribute commits to everyone else? Is that why you commented earlier:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;On another note, it almost seems we can do better than ask for a recovery on a failed commit.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If so, that totally makes sense.&lt;/p&gt;</comment>
                            <comment id="14059139" author="markrmiller@gmail.com" created="Fri, 11 Jul 2014 18:24:48 +0000"  >&lt;p&gt;Right. I think a minimum, doing nothing is probably preferable in most cases. Perhaps a retry or two?&lt;/p&gt;

&lt;p&gt;Or perhaps we should look at sending to leaders to originate. We would still want to commit everywhere in parallel though, and I&apos;m not sure we can do anything that is that much better.&lt;/p&gt;

&lt;p&gt;The current situation doesn&apos;t seem good though.&lt;/p&gt;</comment>
                            <comment id="14059844" author="shalinmangar" created="Sat, 12 Jul 2014 16:43:42 +0000"  >&lt;p&gt;Patch which uses coreNodeName instead of coreName for leader initiated recoveries.&lt;/p&gt;</comment>
                            <comment id="14059845" author="shalinmangar" created="Sat, 12 Jul 2014 16:48:01 +0000"  >&lt;p&gt;I fixed another mistake that I found while fixing this problem. The call to ensureReplicaInLeaderInitiatedRecovery in ElectionContext.startLeaderInitiatedRecoveryOnReplicas had core name instead of replicaUrl. In a related note, the HttpPartitionTest can be improved to not rely on Thread.sleep so much &amp;#8211; I&apos;ll open a separate issue on that.&lt;/p&gt;</comment>
                            <comment id="14059883" author="shalinmangar" created="Sat, 12 Jul 2014 19:03:10 +0000"  >&lt;p&gt;I improved logging by adding coreName as well as coreNodeName everywhere in LIR code. This is ready.&lt;/p&gt;</comment>
                            <comment id="14059892" author="jira-bot" created="Sat, 12 Jul 2014 19:34:40 +0000"  >&lt;p&gt;Commit 1610028 from shalin@apache.org in branch &apos;dev/trunk&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1610028&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://svn.apache.org/r1610028&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-6235&quot; title=&quot;SyncSliceTest fails on jenkins with no live servers available error&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-6235&quot;&gt;&lt;del&gt;SOLR-6235&lt;/del&gt;&lt;/a&gt;: Leader initiated recovery should use coreNodeName instead of coreName to avoid marking all replicas having common core name as down&lt;/p&gt;</comment>
                            <comment id="14059894" author="jira-bot" created="Sat, 12 Jul 2014 19:37:43 +0000"  >&lt;p&gt;Commit 1610029 from shalin@apache.org in branch &apos;dev/branches/branch_4x&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1610029&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://svn.apache.org/r1610029&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-6235&quot; title=&quot;SyncSliceTest fails on jenkins with no live servers available error&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-6235&quot;&gt;&lt;del&gt;SOLR-6235&lt;/del&gt;&lt;/a&gt;: Leader initiated recovery should use coreNodeName instead of coreName to avoid marking all replicas having common core name as down&lt;/p&gt;</comment>
                            <comment id="14060407" author="jira-bot" created="Mon, 14 Jul 2014 07:47:00 +0000"  >&lt;p&gt;Commit 1610351 from shalin@apache.org in branch &apos;dev/trunk&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1610351&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://svn.apache.org/r1610351&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-6235&quot; title=&quot;SyncSliceTest fails on jenkins with no live servers available error&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-6235&quot;&gt;&lt;del&gt;SOLR-6235&lt;/del&gt;&lt;/a&gt;: Improved logging in RecoveryStrategy and fixed a mistake in ElectionContext logging that I had made earlier.&lt;/p&gt;</comment>
                            <comment id="14060408" author="jira-bot" created="Mon, 14 Jul 2014 07:47:24 +0000"  >&lt;p&gt;Commit 1610352 from shalin@apache.org in branch &apos;dev/branches/branch_4x&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1610352&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://svn.apache.org/r1610352&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-6235&quot; title=&quot;SyncSliceTest fails on jenkins with no live servers available error&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-6235&quot;&gt;&lt;del&gt;SOLR-6235&lt;/del&gt;&lt;/a&gt;: Improved logging in RecoveryStrategy and fixed a mistake in ElectionContext logging that I had made earlier.&lt;/p&gt;</comment>
                            <comment id="14060449" author="jira-bot" created="Mon, 14 Jul 2014 08:53:32 +0000"  >&lt;p&gt;Commit 1610361 from shalin@apache.org in branch &apos;dev/trunk&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1610361&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://svn.apache.org/r1610361&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-6235&quot; title=&quot;SyncSliceTest fails on jenkins with no live servers available error&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-6235&quot;&gt;&lt;del&gt;SOLR-6235&lt;/del&gt;&lt;/a&gt;: Fix comparison to use coreNodeName on both sides in ElectionContext.startLeaderInitiatedRecoveryOnReplicas&lt;/p&gt;</comment>
                            <comment id="14060453" author="jira-bot" created="Mon, 14 Jul 2014 08:54:24 +0000"  >&lt;p&gt;Commit 1610362 from shalin@apache.org in branch &apos;dev/branches/branch_4x&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1610362&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://svn.apache.org/r1610362&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-6235&quot; title=&quot;SyncSliceTest fails on jenkins with no live servers available error&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-6235&quot;&gt;&lt;del&gt;SOLR-6235&lt;/del&gt;&lt;/a&gt;: Fix comparison to use coreNodeName on both sides in ElectionContext.startLeaderInitiatedRecoveryOnReplicas&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12655412" name="SOLR-6235.patch" size="20014" author="shalin" created="Sat, 12 Jul 2014 19:03:10 +0000"/>
                            <attachment id="12655408" name="SOLR-6235.patch" size="15646" author="shalin" created="Sat, 12 Jul 2014 16:43:42 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>404625</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            11 years, 19 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1xmxj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>404663</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12313321" key="com.atlassian.jira.toolkit:message">
                        <customfieldname>Solr Mailing List Info</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>