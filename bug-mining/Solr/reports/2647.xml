<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 04:07:58 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SOLR-8085] Fix a variety of issues that can result in replicas getting out of sync.</title>
                <link>https://issues.apache.org/jira/browse/SOLR-8085</link>
                <project id="12310230" key="SOLR">Solr</project>
                    <description>&lt;p&gt;I&apos;ve been discussing this fail I found with Yonik.&lt;/p&gt;

&lt;p&gt;The problem seems to be that a replica tries to recover and publishes recovering - the attempt then fails, but docs are now coming in from the leader. The replica tries to recover again and has gotten enough docs to pass peery sync.&lt;/p&gt;

&lt;p&gt;I&apos;m trying a possible solution now where we won&apos;t allow peer sync after a recovery that is not successful.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12895504">SOLR-8085</key>
            <summary>Fix a variety of issues that can result in replicas getting out of sync.</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="markrmiller@gmail.com">Mark Miller</assignee>
                                    <reporter username="markrmiller@gmail.com">Mark Miller</reporter>
                        <labels>
                    </labels>
                <created>Tue, 22 Sep 2015 16:25:57 +0000</created>
                <updated>Mon, 9 May 2016 18:49:04 +0000</updated>
                            <resolved>Fri, 2 Oct 2015 14:46:18 +0000</resolved>
                                                    <fixVersion>5.4</fixVersion>
                    <fixVersion>6.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>8</watches>
                                                                                                                <comments>
                            <comment id="14902983" author="yseeley@gmail.com" created="Tue, 22 Sep 2015 17:00:38 +0000"  >&lt;p&gt;Here&apos;s a fail from HdfsChaosMonkeySafeLeaderTest on trunk (with some minor patches provided by Mark).&lt;/p&gt;

&lt;p&gt;Throwable #1: java.lang.AssertionError: shard3 is not consistent.  Got 658 from &lt;a href=&quot;http://127.0.0.1:44649/collection1lastClient&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://127.0.0.1:44649/collection1lastClient&lt;/a&gt; and got 330 from &lt;a href=&quot;http://127.0.0.1:38249/collection1&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://127.0.0.1:38249/collection1&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14903025" author="yseeley@gmail.com" created="Tue, 22 Sep 2015 17:18:25 +0000"  >&lt;p&gt;Another one that has a smaller log file.&lt;/p&gt;

&lt;p&gt;Throwable #1: java.lang.AssertionError: shard3 is not consistent.  Got 375 from &lt;a href=&quot;http://127.0.0.1:40940/collection1lastClient&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://127.0.0.1:40940/collection1lastClient&lt;/a&gt; and got 230 from &lt;a href=&quot;http://127.0.0.1:47239/collection1&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://127.0.0.1:47239/collection1&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14903479" author="markrmiller@gmail.com" created="Tue, 22 Sep 2015 21:33:52 +0000"  >&lt;blockquote&gt;&lt;p&gt;I&apos;m trying a possible solution now where we won&apos;t allow peer sync after a recovery that is not successful.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This seems to work out well. &lt;/p&gt;

&lt;p&gt;Most of these issues show up easier when running the more heavyweight hdfs tests though, so I am running those - because we truncate support in hdfs was not added until 2.7, when a replication fails, we still replay all the buffered docs. A badly timed server restart can then pass incorrectly on peer sync if this happens, regardless of my attempted fix that won&apos;t cross jetty restarts.&lt;/p&gt;</comment>
                            <comment id="14904585" author="yseeley@gmail.com" created="Wed, 23 Sep 2015 14:42:57 +0000"  >&lt;p&gt;OK, here&apos;s an analyisis of fail.150922_130608&lt;/p&gt;

&lt;p&gt;it looks like LIR happens during normal recovery after startup, and we finally end up doing recovery with recoverAfterStartup=false, which uses recent versions in peersync (and include docs that have been buffered while we were recovering) rather than the true startup versions.  This causes peersync to pass when it should not have.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
(add first appears, node 47239 appears to be coming up at the time)
  2&amp;gt; 74317 INFO  (qtp324612161-378) [n:127.0.0.1:40940_ c:collection1 s:shard3 r:core_node6 x:collection1] o.a.s.u.p.LogUpdateProcessor [collection1] webapp= path=/update params={update.distrib=FROMLEADER&amp;amp;distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:38911/collection1/&amp;amp;wt=javabin&amp;amp;version=2} {add=[0-333 (1513033816377131008)]} 0 19
&lt;/span&gt;  2&amp;gt; 74317 INFO  (qtp324612161-378) [n:127.0.0.1:40940_ c:collection1 s:shard3 r:core_node6 x:collection1] o.a.s.u.p.LogUpdateProcessor [collection1] webapp= path=/update params={update.distrib=FROMLEADER&amp;amp;distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:38911/collection1/&amp;amp;wt=javabin&amp;amp;version=2} {add=[0-333 (1513033816377131008)]} 0 19
&lt;/span&gt;  2&amp;gt; 74317 INFO  (qtp661063741-234) [n:127.0.0.1:38911_ c:collection1 s:shard3 r:core_node2 x:collection1] o.a.s.u.p.LogUpdateProcessor [collection1] webapp= path=/update params={wt=javabin&amp;amp;version=2} {add=[0-333 (1513033816377131008)]} 0 31
  
(node coming up)
  2&amp;gt; 75064 INFO  (coreLoadExecutor-196-thread-1-processing-n:127.0.0.1:47239_) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.VersionInfo Refreshing highest value of _version_ &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 256 version buckets from index
  2&amp;gt; 75065 INFO  (coreLoadExecutor-196-thread-1-processing-n:127.0.0.1:47239_) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.UpdateLog Took 31.0ms to seed version buckets with highest version 1513033812159758336
  2&amp;gt; 75120 INFO  (coreZkRegister-190-thread-1-processing-n:127.0.0.1:47239_ x:collection1 s:shard3 c:collection1 r:core_node10) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.c.ZkController Replaying tlog &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:47239/collection1/ during startup... NOTE: This can take a &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt;.
&lt;/span&gt;  2&amp;gt; 75162 DEBUG (recoveryExecutor-199-thread-1-processing-n:127.0.0.1:47239_ x:collection1 s:shard3 c:collection1 r:core_node10) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.UpdateLog add add{flags=a,_version_=1513033807303802880,id=1-3}
  2&amp;gt; 75162 DEBUG (recoveryExecutor-199-thread-1-processing-n:127.0.0.1:47239_ x:collection1 s:shard3 c:collection1 r:core_node10) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.p.LogUpdateProcessor PRE_UPDATE add{flags=a,_version_=1513033807303802880,id=1-3} LocalSolrQueryRequest{update.distrib=FROMLEADER&amp;amp;log_replay=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;}

(replay finished)
  2&amp;gt; 75280 DEBUG (recoveryExecutor-199-thread-1-processing-n:127.0.0.1:47239_ x:collection1 s:shard3 c:collection1 r:core_node10) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.p.LogUpdateProcessor PRE_UPDATE add{flags=a,_version_=1513033812159758336,id=1-132} LocalSolrQueryRequest{update.distrib=FROMLEADER&amp;amp;log_replay=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;}
 
(meanwhile, the leader is asking us to recover?)
  2&amp;gt; 75458 WARN  (updateExecutor-14-thread-5-processing-x:collection1 r:core_node2 http:&lt;span class=&quot;code-comment&quot;&gt;////127.0.0.1:47239//collection1// n:127.0.0.1:38911_ s:shard3 c:collection1) [n:127.0.0.1:38911_ c:collection1 s:shard3 r:core_node2 x:collection1] o.a.s.c.LeaderInitiatedRecoveryThread Asking core=collection1 coreNodeName=core_node10 on http://127.0.0.1:47239 to recover; unsuccessful after 2 of 120 attempts so far ...
&lt;/span&gt;  
(and we see the request to recover)
  2&amp;gt; 75475 INFO  (qtp2087242119-1282) [n:127.0.0.1:47239_    ] o.a.s.h.a.CoreAdminHandler It has been requested that we recover: core=collection1

(so we cancel the existing recovery)
  2&amp;gt; 75478 INFO  (&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;-1246) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.DefaultSolrCoreState Running recovery - first canceling any ongoing recovery

  2&amp;gt; 75552 INFO  (RecoveryThread-collection1) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.c.RecoveryStrategy Starting recovery process. recoveringAfterStartup=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;

  2&amp;gt; 75610 INFO  (RecoveryThread-collection1) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.c.RecoveryStrategy ###### startupVersions=[1513033812159758336, [...]
  2&amp;gt; 75611 INFO  (RecoveryThread-collection1) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.c.RecoveryStrategy Publishing state of core collection1 as recovering, leader is http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:38911/collection1/ and I am http://127.0.0.1:47239/collection1/
&lt;/span&gt;  2&amp;gt; 75611 INFO  (RecoveryThread-collection1) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.c.ZkController publishing state=recovering
 
  2&amp;gt; 75642 INFO  (RecoveryThread-collection1) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.c.RecoveryStrategy Sending prep recovery command to http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:38911; WaitForState: action=PREPRECOVERY&amp;amp;core=collection1&amp;amp;nodeName=127.0.0.1%3A47239_&amp;amp;coreNodeName=core_node10&amp;amp;state=recovering&amp;amp;checkLive=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&amp;amp;onlyIfLeader=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&amp;amp;onlyIfLeaderActive=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;
&lt;/span&gt;
(In the meantime, we still receive updates.  1-414 is one of the updates that we are missing!!!  I assume we are buffering these?)
  2&amp;gt; 75782 DEBUG (qtp2087242119-1281) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.p.LogUpdateProcessor PRE_UPDATE add{,id=1-414} {update.distrib=FROMLEADER&amp;amp;distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:38911/collection1/&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;  2&amp;gt; 75828 INFO  (qtp2087242119-1281) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.p.LogUpdateProcessor [collection1] webapp= path=/update params={update.distrib=FROMLEADER&amp;amp;distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:38911/collection1/&amp;amp;wt=javabin&amp;amp;version=2} {add=[1-414 (1513033817943703552)]} 0 45
&lt;/span&gt;
(what&apos;s &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; about?)
  2&amp;gt; 75923 INFO  (recoveryExecutor-199-thread-1-processing-n:127.0.0.1:47239_ x:collection1 s:shard3 c:collection1 r:core_node10) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.s.SolrIndexSearcher Opening Searcher@2ba72586[collection1] main

(wait... replay? is &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; from when we started up and we never stopped it?  should we have?)
  2&amp;gt; 76135 DEBUG (recoveryExecutor-199-thread-1-processing-n:127.0.0.1:47239_ x:collection1 s:shard3 c:collection1 r:core_node10) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.p.LogUpdateProcessor PRE_UPDATE FINISH LocalSolrQueryRequest{update.distrib=FROMLEADER&amp;amp;log_replay=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;}


(log replay finishes... the dbq *:* shows that &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; is the tlog from the beginning of &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; test.. i.e. we never got to &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; a commit at first)
  2&amp;gt; 76181 INFO  (recoveryExecutor-199-thread-1-processing-n:127.0.0.1:47239_ x:collection1 s:shard3 c:collection1 r:core_node10) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.p.LogUpdateProcessor [collection1] {deleteByQuery=*:* (-1513033805984694272),add=[1-3 (1513033807303802880), 0-5 (1513033807434874880), 1-6 (1513033807628861440), 1-7 (1513033807699116032), 1-9 (1513033807797682176), 0-14 (1513033807941337088), 0-15 (1513033807976988672), 1-14 (1513033807991668736), 1-15 (1513033808035708928), 0-17 (1513033808075554816), ... (61 adds)]} 0 1060
  2&amp;gt; 76182 WARN  (recoveryExecutor-199-thread-1-processing-n:127.0.0.1:47239_ x:collection1 s:shard3 c:collection1 r:core_node10) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.UpdateLog Log replay finished. recoveryInfo=RecoveryInfo{adds=61 deletes=0 deleteByQuery=1 errors=0 positionOfStart=0}

(we&apos;re canceling recoveryh again?)
  2&amp;gt; 76182 INFO  (coreZkRegister-190-thread-1-processing-n:127.0.0.1:47239_ x:collection1 s:shard3 c:collection1 r:core_node10) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.DefaultSolrCoreState Running recovery - first canceling any ongoing recovery
  2&amp;gt; 76186 WARN  (coreZkRegister-190-thread-1-processing-n:127.0.0.1:47239_ x:collection1 s:shard3 c:collection1 r:core_node10) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.c.RecoveryStrategy Stopping recovery &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; core=collection1 coreNodeName=core_node10
  
(did &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; fail because we canceled the recovery?)
  2&amp;gt; 76194 ERROR (RecoveryThread-collection1) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.c.RecoveryStrategy Error &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; trying to recover.:java.util.concurrent.ExecutionException: org.apache.solr.client.solrj.SolrServerException: IOException occured when talking to server at: http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:38911
&lt;/span&gt;  2&amp;gt; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
  2&amp;gt; 	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
  2&amp;gt; 	at org.apache.solr.cloud.RecoveryStrategy.sendPrepRecoveryCmd(RecoveryStrategy.java:598)
  2&amp;gt; 	at org.apache.solr.cloud.RecoveryStrategy.doRecovery(RecoveryStrategy.java:361)
  2&amp;gt; 	at org.apache.solr.cloud.RecoveryStrategy.run(RecoveryStrategy.java:227)
  2&amp;gt; Caused by: org.apache.solr.client.solrj.SolrServerException: IOException occured when talking to server at: http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:38911
&lt;/span&gt;  2&amp;gt; 	at org.apache.solr.client.solrj.impl.HttpSolrClient.executeMethod(HttpSolrClient.java:590)
  2&amp;gt; 	at org.apache.solr.client.solrj.impl.HttpSolrClient$1.call(HttpSolrClient.java:285)
  2&amp;gt; 	at org.apache.solr.client.solrj.impl.HttpSolrClient$1.call(HttpSolrClient.java:281)
  2&amp;gt; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  2&amp;gt; 	at org.apache.solr.common.util.ExecutorUtil$MDCAwareThreadPoolExecutor$1.run(ExecutorUtil.java:231)
  2&amp;gt; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  2&amp;gt; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  2&amp;gt; 	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
  2&amp;gt; Caused by: java.net.SocketException: Socket closed
  2&amp;gt; 76202 INFO  (RecoveryThread-collection1) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.c.RecoveryStrategy Finished recovery process.

(note: &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; is the last we hear of &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; thread... not clear &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; that&apos;s OK or not)
  2&amp;gt; 76202 INFO  (coreZkRegister-190-thread-1-processing-n:127.0.0.1:47239_ x:collection1 s:shard3 c:collection1 r:core_node10) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.c.ActionThrottle The last recovery attempt started 711ms ago.
  2&amp;gt; 76202 INFO  (coreZkRegister-190-thread-1-processing-n:127.0.0.1:47239_ x:collection1 s:shard3 c:collection1 r:core_node10) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.c.ActionThrottle Throttling recovery attempts - waiting &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 9288ms
  
 (still receiving updates, &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; we are waiting to &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; the next recovery attempt...  are these still buffering? how &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; we tell?)
  2&amp;gt; 76252 DEBUG (qtp2087242119-1283) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.p.LogUpdateProcessor PRE_UPDATE add{,id=0-432} {update.distrib=FROMLEADER&amp;amp;distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:38911/collection1/&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;

(OK, looks like we are &lt;span class=&quot;code-keyword&quot;&gt;finally&lt;/span&gt; trying the recovery process again... but note that &lt;span class=&quot;code-quote&quot;&gt;&quot;recoveringAfterStartup&quot;&lt;/span&gt; is now &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;!!!!)
  2&amp;gt; 85491 INFO  (RecoveryThread-collection1) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.c.RecoveryStrategy Starting recovery process. recoveringAfterStartup=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;
  2&amp;gt; 85531 INFO  (RecoveryThread-collection1) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.c.RecoveryStrategy Publishing state of core collection1 as recovering, leader is http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:38911/collection1/ and I am http://127.0.0.1:47239/collection1/
&lt;/span&gt;
(Peersync.  Because recoveringAfterStartup is &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;, it will use most recent versions to compare, rather than startup versions)
(As an example, version 1513033825996767232 is very recent, at timestamp 83465, &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; we were still not recovered)
  2&amp;gt; 92554 INFO  (RecoveryThread-collection1) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.c.RecoveryStrategy Attempting to PeerSync from http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:38911/collection1/ - recoveringAfterStartup=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;
&lt;/span&gt;  2&amp;gt; 92554 DEBUG (RecoveryThread-collection1) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.PeerSync PeerSync: core=collection1 url=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:47239 startingVersions=100 [1513033825996767232, [...]
&lt;/span&gt;  2&amp;gt; 92646 INFO  (RecoveryThread-collection1) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.PeerSync PeerSync: core=collection1 url=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:47239  Received 100 versions from http://127.0.0.1:38911/collection1/
&lt;/span&gt;  2&amp;gt; 92647 DEBUG (RecoveryThread-collection1) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.PeerSync PeerSync: core=collection1 url=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:47239  sorted versions from http://127.0.0.1:38911/collection1/ = [1513033825996767232, ...
&lt;/span&gt;  2&amp;gt; 92647 INFO  (RecoveryThread-collection1) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.PeerSync PeerSync: core=collection1 url=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:47239  Our versions are newer. ourLowThreshold=1513033822028955648 otherHigh=1513033825172586496
&lt;/span&gt;  2&amp;gt; 92647 INFO  (RecoveryThread-collection1) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.PeerSync PeerSync: core=collection1 url=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:47239 DONE. sync succeeded
&lt;/span&gt;
(And we pass peersync when we should not)
  2&amp;gt; 92796 INFO  (RecoveryThread-collection1) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.c.RecoveryStrategy PeerSync Recovery was successful - registering as Active.

  2&amp;gt; ######shard3 is not consistent.  Got 375 from http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:40940/collection1lastClient and got 230 from http://127.0.0.1:47239/collection1
&lt;/span&gt;  2&amp;gt; ###### sizes=375,230
  2&amp;gt; ###### Only in http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:40940/collection1: [{_version_=1513033815669342208, id=0-295}, [...], {_version_=1513033816377131008, id=0-333}]
&lt;/span&gt; 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="14904639" author="yseeley@gmail.com" created="Wed, 23 Sep 2015 15:18:40 +0000"  >&lt;p&gt;OK, here&apos;s one possible patch I think.&lt;/p&gt;</comment>
                            <comment id="14904653" author="markrmiller@gmail.com" created="Wed, 23 Sep 2015 15:24:18 +0000"  >&lt;blockquote&gt;&lt;p&gt;but note that &quot;recoveringAfterStartup&quot; is now false!!!!)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah, lot&apos;s of ways to lose the class and start over - so if you really want a field to persist it has to be static.&lt;/p&gt;</comment>
                            <comment id="14904662" author="markrmiller@gmail.com" created="Wed, 23 Sep 2015 15:28:56 +0000"  >&lt;p&gt;Looked at the patch - yeah, or put it on the core state &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="14905123" author="yseeley@gmail.com" created="Wed, 23 Sep 2015 19:48:28 +0000"  >&lt;p&gt;Yeah, It can&apos;t be static because each core needs it&apos;s own state.&lt;br/&gt;
We could also maintain it as a normal variable in RecoveryStrategy and either reuse RecoveryStrategy objects, or initialize future objects from past objects.  Thoughts on best approach?&lt;/p&gt;</comment>
                            <comment id="14905435" author="markrmiller@gmail.com" created="Wed, 23 Sep 2015 22:40:12 +0000"  >&lt;p&gt;Could be a static map or something in RecoveryStrategy too - but seeing as we already store another variable like this in the default state, made a lot of sense to me.&lt;/p&gt;

&lt;p&gt;With your patch and running on a patched version of 4.10.3, I was still only seeing one other type of fail.&lt;/p&gt;

&lt;p&gt;Docs that came in during recovery after publishing recovering and before buffering would end up interfering with and causing a false peer sync pass if enough of them came in.&lt;/p&gt;

&lt;p&gt;I seemed to have worked around this issue by buffering docs before peer sync and before publishing as RECOVERING (the signal for the leader to start sending updates).&lt;/p&gt;

&lt;p&gt;With my current runs using no deletes, I have not yet found a fail after this on this version of the code.&lt;/p&gt;</comment>
                            <comment id="14905439" author="markrmiller@gmail.com" created="Wed, 23 Sep 2015 22:42:18 +0000"  >&lt;p&gt;Should mention one other change as I am mostly testing Hdfs version of this chaosmonkey test - after talking to Yonik I also fixed an issue where because we don&apos;t have truncate support we were replaying buffered docs on fail to get past them - really we should not do that as it can lead to bad peer sync passes and I have a fix for that as well. I&apos;ll file a separate JIRA issue for that one.&lt;/p&gt;</comment>
                            <comment id="14906838" author="markrmiller@gmail.com" created="Thu, 24 Sep 2015 19:02:21 +0000"  >&lt;p&gt;For the last comment I filed &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-8094&quot; title=&quot;HdfsUpdateLog should not replay buffered documents as a replacement to dropping them.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-8094&quot;&gt;&lt;del&gt;SOLR-8094&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="14906848" author="markrmiller@gmail.com" created="Thu, 24 Sep 2015 19:10:15 +0000"  >&lt;p&gt;Here is a patch I just ported to trunk that starts buffering before peer sync and before we publish as RECOVERING. &lt;/p&gt;

&lt;p&gt;With &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yseeley%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;yseeley@gmail.com&quot;&gt;yseeley@gmail.com&lt;/a&gt;&apos;s fix and the couple I have, I&apos;ve been able to hit 200 HdfsChaosMonkey test runs without replica inconsistency on my 4.10.3 + backports based code for the first time.&lt;/p&gt;</comment>
                            <comment id="14933521" author="markrmiller@gmail.com" created="Mon, 28 Sep 2015 16:22:45 +0000"  >&lt;blockquote&gt;&lt;p&gt;Here is a patch I just ported to trunk that starts buffering before peer sync and before we publish as RECOVERING.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It looks like this may pose a problem for shard splitting.&lt;/p&gt;</comment>
                            <comment id="14933554" author="markrmiller@gmail.com" created="Mon, 28 Sep 2015 16:49:06 +0000"  >&lt;p&gt;NM - the patch was missing a change. Here is a new patch that combines with Yonik&apos;s.&lt;/p&gt;</comment>
                            <comment id="14941212" author="jira-bot" created="Fri, 2 Oct 2015 14:40:15 +0000"  >&lt;p&gt;Commit 1706423 from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markrmiller%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;markrmiller@gmail.com&quot;&gt;markrmiller@gmail.com&lt;/a&gt; in branch &apos;dev/trunk&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1706423&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://svn.apache.org/r1706423&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-8085&quot; title=&quot;Fix a variety of issues that can result in replicas getting out of sync.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-8085&quot;&gt;&lt;del&gt;SOLR-8085&lt;/del&gt;&lt;/a&gt;: Fix a variety of issues that can result in replicas getting out of sync.&lt;/p&gt;</comment>
                            <comment id="14941215" author="jira-bot" created="Fri, 2 Oct 2015 14:45:45 +0000"  >&lt;p&gt;Commit 1706424 from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markrmiller%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;markrmiller@gmail.com&quot;&gt;markrmiller@gmail.com&lt;/a&gt; in branch &apos;dev/branches/branch_5x&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1706424&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://svn.apache.org/r1706424&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-8085&quot; title=&quot;Fix a variety of issues that can result in replicas getting out of sync.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-8085&quot;&gt;&lt;del&gt;SOLR-8085&lt;/del&gt;&lt;/a&gt;: Fix a variety of issues that can result in replicas getting out of sync.&lt;/p&gt;</comment>
                            <comment id="14941216" author="markrmiller@gmail.com" created="Fri, 2 Oct 2015 14:46:08 +0000"  >&lt;p&gt;I also have a test change we may need to prevent false fails - I&apos;ll spin that off into it&apos;s own issue.&lt;/p&gt;</comment>
                            <comment id="14941771" author="joel.bernstein" created="Fri, 2 Oct 2015 21:12:04 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markrmiller%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;markrmiller@gmail.com&quot;&gt;markrmiller@gmail.com&lt;/a&gt;, I&apos;m getting errors in the TestSQLHandler test cases, which extends AbstractFullDistribZkTestBase, following this commit. In the test case it looks like one of the shards is not having documents added to it. Still investigating...&lt;/p&gt;</comment>
                            <comment id="14941803" author="joel.bernstein" created="Fri, 2 Oct 2015 21:33:50 +0000"  >&lt;p&gt;I think I&apos;m using the wrong method to index documents and this ticket just exposed that. &lt;/p&gt;</comment>
                            <comment id="14941841" author="joel.bernstein" created="Fri, 2 Oct 2015 21:56:36 +0000"  >&lt;p&gt;Ok, the issue was that I wasn&apos;t calling: waitForRecoveriesToFinish(false);&lt;/p&gt;

&lt;p&gt;Once I added this the test passes. &lt;/p&gt;</comment>
                            <comment id="14942554" author="shalinmangar" created="Sun, 4 Oct 2015 04:51:27 +0000"  >&lt;p&gt;Good catch guys! Thanks for fixing this. Special thanks to Yonik for analysing the failure logs.&lt;/p&gt;</comment>
                            <comment id="14942679" author="markrmiller@gmail.com" created="Sun, 4 Oct 2015 14:16:16 +0000"  >&lt;blockquote&gt;&lt;p&gt;I also have a test change we may need to prevent false fails - I&apos;ll spin that off into it&apos;s own issue.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-8121&quot; title=&quot;It looks like ChaosMonkeySafeLeader test can fail with replica inconsistency because waitForThingsToLevelOut can pass while state is still changing.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-8121&quot;&gt;&lt;del&gt;SOLR-8121&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12896201">SOLR-8094</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12764044" name="SOLR-8085.patch" size="6568" author="markrmiller@gmail.com" created="Mon, 28 Sep 2015 16:49:06 +0000"/>
                            <attachment id="12762209" name="SOLR-8085.patch" size="3217" author="markrmiller@gmail.com" created="Thu, 24 Sep 2015 19:10:15 +0000"/>
                            <attachment id="12761905" name="SOLR-8085.patch" size="2057" author="yseeley@gmail.com" created="Wed, 23 Sep 2015 15:18:40 +0000"/>
                            <attachment id="12761682" name="fail.150922_125320" size="15506886" author="yseeley@gmail.com" created="Tue, 22 Sep 2015 17:00:38 +0000"/>
                            <attachment id="12761685" name="fail.150922_130608" size="8398631" author="yseeley@gmail.com" created="Tue, 22 Sep 2015 17:18:25 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>5.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 7 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2let3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12313321" key="com.atlassian.jira.toolkit:message">
                        <customfieldname>Solr Mailing List Info</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>