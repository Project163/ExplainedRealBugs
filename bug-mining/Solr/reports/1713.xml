<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 03:44:56 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SOLR-4260] Inconsistent numDocs between leader and replica</title>
                <link>https://issues.apache.org/jira/browse/SOLR-4260</link>
                <project id="12310230" key="SOLR">Solr</project>
                    <description>&lt;p&gt;After wiping all cores and reindexing some 3.3 million docs from Nutch using CloudSolrServer we see inconsistencies between the leader and replica for some shards.&lt;/p&gt;

&lt;p&gt;Each core hold about 3.3k documents. For some reason 5 out of 10 shards have a small deviation in then number of documents. The leader and slave deviate for roughly 10-20 documents, not more.&lt;/p&gt;

&lt;p&gt;Results hopping ranks in the result set for identical queries got my attention, there were small IDF differences for exactly the same record causing a record to shift positions in the result set. During those tests no records were indexed. Consecutive catch all queries also return different number of numDocs.&lt;/p&gt;

&lt;p&gt;We&apos;re running a 10 node test cluster with 10 shards and a replication factor of two and frequently reindex using a fresh build from trunk. I&apos;ve not seen this issue for quite some time until a few days ago.&lt;/p&gt;</description>
                <environment>&lt;p&gt;5.0.0.2013.01.04.15.31.51&lt;/p&gt;</environment>
        <key id="12626072">SOLR-4260</key>
            <summary>Inconsistent numDocs between leader and replica</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="markrmiller@gmail.com">Mark Miller</assignee>
                                    <reporter username="markus17">Markus Jelsma</reporter>
                        <labels>
                    </labels>
                <created>Fri, 4 Jan 2013 17:53:01 +0000</created>
                <updated>Mon, 9 May 2016 18:51:53 +0000</updated>
                            <resolved>Sun, 2 Feb 2014 15:19:33 +0000</resolved>
                                                    <fixVersion>4.6.1</fixVersion>
                    <fixVersion>6.0</fixVersion>
                                    <component>SolrCloud</component>
                        <due></due>
                            <votes>6</votes>
                                    <watches>24</watches>
                                                                                                                <comments>
                            <comment id="13544077" author="yseeley@gmail.com" created="Fri, 4 Jan 2013 17:59:08 +0000"  >&lt;blockquote&gt;&lt;p&gt;For some reason 5 out of 10 shards have a small deviation in then number of documents. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;numDocs or maxDoc?&lt;br/&gt;
You &lt;b&gt;can&lt;/b&gt; expect variations in maxDoc (due to the same document being added more than once in recovery scenarios).  numDocs should be identical of course.&lt;/p&gt;

&lt;p&gt;Oh, and that variation in maxDoc will cause small differences in IDF too.&lt;/p&gt;</comment>
                            <comment id="13544078" author="markrmiller@gmail.com" created="Fri, 4 Jan 2013 17:59:18 +0000"  >&lt;p&gt;What&apos;s fresh? We fixed a couple issue for this within the last couple weeks.&lt;/p&gt;</comment>
                            <comment id="13544079" author="markrmiller@gmail.com" created="Fri, 4 Jan 2013 18:00:17 +0000"  >&lt;p&gt;Looks like fresh means from today?&lt;/p&gt;</comment>
                            <comment id="13544081" author="markus17" created="Fri, 4 Jan 2013 18:01:59 +0000"  >&lt;p&gt;Yonik, i explicitly look at numDocs. Shard_B has Num Docs: 335986, Max Doc: 336079 on one node and Num Docs: 335976 Max Doc: 336091 on the other. &lt;/p&gt;

&lt;p&gt;Mark: yes, this is today, 5.0.0.2013.01.04.15.31.51&lt;/p&gt;</comment>
                            <comment id="13546000" author="markus17" created="Mon, 7 Jan 2013 16:15:49 +0000"  >&lt;p&gt;I tried to reproduce this twice with today&apos;s check out of trunk manner but failed to consistently reproduce it. I did see a small deviation while no data was coming in and no recovery was &lt;b&gt;reported&lt;/b&gt; to be in progress, in the end all replicates where in sync and had, accoring to Luke, identical numDocs.&lt;/p&gt;

&lt;p&gt;I&apos;ll keep an eye on this in the next couple of days.&lt;/p&gt;

&lt;p&gt;btw: in this test cluster we use docCount and not maxDoc for our IDF calculation.&lt;/p&gt;</comment>
                            <comment id="13546027" author="markrmiller@gmail.com" created="Mon, 7 Jan 2013 16:49:06 +0000"  >&lt;p&gt;Thanks - would be good to make sure this is okay. FYI, seeing things off for a short time is somewhat expected - there is some &apos;eventual&apos; consistency here - but it should quickly consistent itself up - under heavy bulk indexing is when you would be most likely to have the most &quot;eventualness&quot;.&lt;/p&gt;</comment>
                            <comment id="13546196" author="markus17" created="Mon, 7 Jan 2013 19:41:55 +0000"  >&lt;p&gt;Yes, we see that behaviour all the time but it is expected indeed. The problem with with this issue is that it was not a dozen seconds or a few minutes but it was consistently wrong for almost an hour, then i wrote this issue and left the office &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13555980" author="markus17" created="Thu, 17 Jan 2013 08:31:28 +0000"  >&lt;p&gt;I&apos;ve got it again. This time numDocs is consistent but some facet counts are not consistent between leader and replica. Here are two facet counts for one node:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;   &amp;lt;lst name=&lt;span class=&quot;code-quote&quot;&gt;&quot;domain&quot;&lt;/span&gt;&amp;gt;
      &amp;lt;&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; name=&lt;span class=&quot;code-quote&quot;&gt;&quot;domain_a&quot;&lt;/span&gt;&amp;gt;238620&amp;lt;/&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;&amp;gt;
      &amp;lt;&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; name=&lt;span class=&quot;code-quote&quot;&gt;&quot;domain_b&quot;&lt;/span&gt;&amp;gt;218&amp;lt;/&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and the other:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;   &amp;lt;lst name=&lt;span class=&quot;code-quote&quot;&gt;&quot;domain&quot;&lt;/span&gt;&amp;gt;
      &amp;lt;&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; name=&lt;span class=&quot;code-quote&quot;&gt;&quot;domain_a&quot;&lt;/span&gt;&amp;gt;238621&amp;lt;/&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;&amp;gt;
      &amp;lt;&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; name=&lt;span class=&quot;code-quote&quot;&gt;&quot;domain_b&quot;&lt;/span&gt;&amp;gt;217&amp;lt;/&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13556261" author="markrmiller@gmail.com" created="Thu, 17 Jan 2013 15:16:34 +0000"  >&lt;p&gt;Not sure I&apos;d say it&apos;s &apos;it&apos; again yet - numdocs and facet counts are quite different.&lt;/p&gt;

&lt;p&gt;Interesting though - does that persist after a hard commit / open new searcher?&lt;/p&gt;</comment>
                            <comment id="13556315" author="markus17" created="Thu, 17 Jan 2013 15:37:35 +0000"  >&lt;p&gt;You&apos;re right, it is different, my bad. A hard commit is issues automatically once in a while and the issue persists and after a manual hard commit as well.&lt;/p&gt;

&lt;p&gt;Interestingly, we see that the docCounts returned by CollectionStatistics.docCount() is inconsistent between leader and replica for each shard. As Yonik said, it&apos;s normal when using maxDoc but we don&apos;t use maxDoc in this set up, docCount should be correct. Since it isn&apos;t, our IDF is sometimes skewed, causing docs to jump position in the result set.&lt;/p&gt;
</comment>
                            <comment id="13557211" author="markus17" created="Fri, 18 Jan 2013 13:53:12 +0000"  >&lt;p&gt;I&apos;ve removed domain_b from the index and as i expected the numDocs is now inconsistent indeed. By coincidence the what was missing in one replica from domain_a was replaced by an extra doc from domain_b and vice versa.&lt;/p&gt;

&lt;p&gt;The collection of a couple of million records has one replica that&apos;s missing one document.&lt;/p&gt;</comment>
                            <comment id="13558794" author="markus17" created="Mon, 21 Jan 2013 14:30:45 +0000"  >&lt;p&gt;Here&apos;s the debug output of the same query executed on both the leader and the replica. This set up uses and overridden BM25Similarity that returns docCount() for IDF instead of maxDoc. In this case both have the equal number of documents in the index so something else doesn&apos;t seem right. Facet counts add up, both leader and replicaa have the same number of documents per domain.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;43.960983 = (MATCH) sum of:
  43.960983 = (MATCH) max plus 0.35 times others of:
    29.059849 = (MATCH) weight(title_nl:amsterdam^6.4 in 14437) [], result of:
      29.059849 = score(doc=14437,freq=3.0 = termFreq=3.0
), product of:
        6.4 = boost
        2.889473 = idf(docFreq=18368, docCount=330335)
        1.5714288 = tfNorm, computed from:
          3.0 = termFreq=3.0
          1.2 = parameter k1
          0.0 = parameter b (norms omitted &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; field)
    4.651832 = (MATCH) weight(content_nl:amsterdam^1.6 in 14437) [], result of:
      4.651832 = score(doc=14437,freq=1.0 = termFreq=1.0
), product of:
        1.6 = boost
        2.9073951 = idf(docFreq=18039, docCount=330285)
        1.0 = tfNorm, computed from:
          1.0 = termFreq=1.0
          1.2 = parameter k1
          0.0 = parameter b (norms omitted &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; field)
    32.161896 = (MATCH) weight(url:amsterdam^3.64 in 14437) [], result of:
      32.161896 = score(doc=14437,freq=2.0 = termFreq=2.0
), product of:
        3.64 = boost
        6.328843 = idf(docFreq=608, docCount=341068)
        1.396098 = tfNorm, computed from:
          2.0 = termFreq=2.0
          1.2 = parameter k1
          0.75 = parameter b
          4.227131 = avgFieldLength
          4.0 = fieldLength
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;45.993042 = (MATCH) sum of:
  45.993042 = (MATCH) max plus 0.35 times others of:
    28.35725 = (MATCH) weight(title_nl:amsterdam^6.4 in 170479) [], result of:
      28.35725 = score(doc=170479,freq=3.0 = termFreq=3.0
), product of:
        6.4 = boost
        2.8196125 = idf(docFreq=16736, docCount=280676)
        1.5714288 = tfNorm, computed from:
          3.0 = termFreq=3.0
          1.2 = parameter k1
          0.0 = parameter b (norms omitted &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; field)
    4.577688 = (MATCH) weight(content_nl:amsterdam^1.6 in 170479) [], result of:
      4.577688 = score(doc=170479,freq=1.0 = termFreq=1.0
), product of:
        1.6 = boost
        2.8610551 = idf(docFreq=16054, docCount=280631)
        1.0 = tfNorm, computed from:
          1.0 = termFreq=1.0
          1.2 = parameter k1
          0.0 = parameter b (norms omitted &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; field)
    34.465813 = (MATCH) weight(url:amsterdam^3.64 in 170479) [], result of:
      34.465813 = score(doc=170479,freq=2.0 = termFreq=2.0
), product of:
        3.64 = boost
        6.798851 = idf(docFreq=323, docCount=290119)
        1.3926809 = tfNorm, computed from:
          2.0 = termFreq=2.0
          1.2 = parameter k1
          0.75 = parameter b
          4.189095 = avgFieldLength
          4.0 = fieldLength
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It&apos;s clear that not only docCount is different but also docFreq while both should be equal on the leader and replica. This makes a mess of the final score!&lt;/p&gt;

&lt;p&gt;Anyone else here that has seen this issue?&lt;/p&gt;</comment>
                            <comment id="13558910" author="markus17" created="Mon, 21 Jan 2013 17:03:02 +0000"  >&lt;p&gt;Ok, this is not a SolrCloud issue, i can also reproduce this in stand-alone and multi core set ups. This is also not a problem of BM25 since TFIDF has the same problem. Neither docCount vs. maxCount seems to be the problem.&lt;/p&gt;

&lt;p&gt;I now have two identical cores set up and index the same data to both, no problem, everything is very consistent. Then i&apos;ll reindex the same data again to only one of the two cores and then the trouble starts. There is a small variation in maxDoc which is expected but there is also a variation in docFreq which is very unexpected, docFreq must not change at all if i reindex the same data.&lt;/p&gt;

&lt;p&gt;Here&apos;s an debug snippet of the first core that did not receive reindexed data:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;910.47974 = (MATCH) sum of:
  910.47974 = (MATCH) max plus 0.35 times others of:
    793.99835 = (MATCH) weight(title_en:groningen^6.4 in 5132) [], result of:
      793.99835 = score(doc=5132,freq=1.0 = termFreq=1.0
), product of:
        71.28527 = queryWeight, product of:
          6.4 = boost
          11.138323 = idf(docFreq=1, maxDocs=50588)
          1.0 = queryNorm
        11.138323 = fieldWeight in 5132, product of:
          1.0 = tf(freq=1.0), with freq of:
            1.0 = termFreq=1.0
          11.138323 = idf(docFreq=1, maxDocs=50588)
          1.0 = fieldNorm(doc=5132)
    312.06528 = (MATCH) weight(content_en:groningen^1.6 in 5132) [], result of:
      312.06528 = score(doc=5132,freq=1.0 = termFreq=1.0
), product of:
        17.172573 = queryWeight, product of:
          1.6 = boost
          10.732858 = idf(docFreq=2, maxDocs=50588)
          1.0 = queryNorm
        18.172308 = fieldWeight in 5132, product of:
          1.6931472 = tf(freq=1.0), with freq of:
            1.0 = termFreq=1.0
          10.732858 = idf(docFreq=2, maxDocs=50588)
          1.0 = fieldNorm(doc=5132)
    20.73867 = (MATCH) weight(domain_grams:groningen^3.7 in 5132) [], result of:
      20.73867 = score(doc=5132,freq=1.0 = termFreq=1.0
), product of:
        26.48697 = queryWeight, product of:
          3.7 = boost
          7.158641 = idf(docFreq=106, maxDocs=50588)
          1.0 = queryNorm
        0.7829763 = fieldWeight in 5132, product of:
          1.0 = tf(freq=1.0), with freq of:
            1.0 = termFreq=1.0
          7.158641 = idf(docFreq=106, maxDocs=50588)
          0.109375 = fieldNorm(doc=5132)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here&apos;s the debug of the same doc on the core which i reindexed the same data to:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;928.31537 = (MATCH) sum of:
  928.31537 = (MATCH) max plus 0.35 times others of:
    815.29694 = (MATCH) weight(title_en:groningen^6.4 in 31881) [], result of:
      815.29694 = score(doc=31881,freq=1.0 = termFreq=1.0
), product of:
        72.23504 = queryWeight, product of:
          6.4 = boost
          11.286724 = idf(docFreq=1, maxDocs=58681)
          1.0 = queryNorm
        11.286724 = fieldWeight in 31881, product of:
          1.0 = tf(freq=1.0), with freq of:
            1.0 = termFreq=1.0
          11.286724 = idf(docFreq=1, maxDocs=58681)
          1.0 = fieldNorm(doc=31881)
    304.0185 = (MATCH) weight(content_en:groningen^1.6 in 31881) [], result of:
      304.0185 = score(doc=31881,freq=1.0 = termFreq=1.0
), product of:
        16.949724 = queryWeight, product of:
          1.6 = boost
          10.593577 = idf(docFreq=3, maxDocs=58681)
          1.0 = queryNorm
        17.936485 = fieldWeight in 31881, product of:
          1.6931472 = tf(freq=1.0), with freq of:
            1.0 = termFreq=1.0
          10.593577 = idf(docFreq=3, maxDocs=58681)
          1.0 = fieldNorm(doc=31881)
    18.891369 = (MATCH) weight(domain_grams:groningen^3.7 in 31881) [], result of:
      18.891369 = score(doc=31881,freq=1.0 = termFreq=1.0
), product of:
        25.279795 = queryWeight, product of:
          3.7 = boost
          6.832377 = idf(docFreq=171, maxDocs=58681)
          1.0 = queryNorm
        0.7472912 = fieldWeight in 31881, product of:
          1.0 = tf(freq=1.0), with freq of:
            1.0 = termFreq=1.0
          6.832377 = idf(docFreq=171, maxDocs=58681)
          0.109375 = fieldNorm(doc=31881)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As you can see, docFreq has changed but the number of documents is still the same. Since i now suspect the merging of segments has something to do with it i&apos;ll send an optimize command to the node that i reindexed data to.&lt;/p&gt;

&lt;p&gt;After optimizing (or forcing all segments to be merged) i get the same debug as i had for the first node that i didn&apos;t reindex to!&lt;/p&gt;

</comment>
                            <comment id="13558920" author="markus17" created="Mon, 21 Jan 2013 17:12:16 +0000"  >&lt;p&gt;I updated the title to reflect the new issue. Can anyone confirm that with trunk they see different values for CollectionStatistics.docCount and docFreq?&lt;/p&gt;

&lt;p&gt;I can confirm that the variations disappear after optimizing one of our SolrCloud clusters. The only time results swap places is when the score is identical and docID is used to score.&lt;/p&gt;</comment>
                            <comment id="13558929" author="yseeley@gmail.com" created="Mon, 21 Jan 2013 17:28:39 +0000"  >&lt;blockquote&gt;&lt;p&gt;There is a small variation in maxDoc which is expected but there is also a variation in docFreq which is very unexpected, docFreq must not change at all if i reindex the same data.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Unfortunately, deletions don&apos;t change index statistics like docFreq (this has been the case since the first version of Lucene).  This means that reindexing a document can artificially increase the docFreq until the deletion is really removed via merging/optimize.&lt;/p&gt;</comment>
                            <comment id="13558999" author="markus17" created="Mon, 21 Jan 2013 19:14:55 +0000"  >&lt;p&gt;Of course, you&apos;re right! I got distracted by that fact and in the process renamed this issue while i shouldn&apos;t. The inconsistency between leader and replica on one shard is still here.&lt;/p&gt;

&lt;p&gt;I&apos;ll rename it back and raise the docFreq and docCount issue on the list. Sorry for the mess &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13581189" author="markus17" created="Tue, 19 Feb 2013 10:16:44 +0000"  >&lt;p&gt;Here&apos;s the index information for two cores of the same shard, running on different nodes.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&amp;lt;?xml version=&lt;span class=&quot;code-quote&quot;&gt;&quot;1.0&quot;&lt;/span&gt; encoding=&lt;span class=&quot;code-quote&quot;&gt;&quot;UTF-8&quot;&lt;/span&gt;?&amp;gt;
&amp;lt;response&amp;gt;

&amp;lt;lst name=&lt;span class=&quot;code-quote&quot;&gt;&quot;responseHeader&quot;&lt;/span&gt;&amp;gt;
  &amp;lt;&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; name=&lt;span class=&quot;code-quote&quot;&gt;&quot;status&quot;&lt;/span&gt;&amp;gt;0&amp;lt;/&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;&amp;gt;
  &amp;lt;&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; name=&lt;span class=&quot;code-quote&quot;&gt;&quot;QTime&quot;&lt;/span&gt;&amp;gt;1&amp;lt;/&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;&amp;gt;
&amp;lt;/lst&amp;gt;
&amp;lt;lst name=&lt;span class=&quot;code-quote&quot;&gt;&quot;index&quot;&lt;/span&gt;&amp;gt;
  &amp;lt;&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; name=&lt;span class=&quot;code-quote&quot;&gt;&quot;numDocs&quot;&lt;/span&gt;&amp;gt;117744&amp;lt;/&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;&amp;gt;
  &amp;lt;&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; name=&lt;span class=&quot;code-quote&quot;&gt;&quot;maxDoc&quot;&lt;/span&gt;&amp;gt;118160&amp;lt;/&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;&amp;gt;
  &amp;lt;&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; name=&lt;span class=&quot;code-quote&quot;&gt;&quot;deletedDocs&quot;&lt;/span&gt;&amp;gt;416&amp;lt;/&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;&amp;gt;
  &amp;lt;&lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; name=&lt;span class=&quot;code-quote&quot;&gt;&quot;version&quot;&lt;/span&gt;&amp;gt;3802&amp;lt;/&lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt;&amp;gt;
  &amp;lt;&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; name=&lt;span class=&quot;code-quote&quot;&gt;&quot;segmentCount&quot;&lt;/span&gt;&amp;gt;15&amp;lt;/&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;&amp;gt;
  &amp;lt;bool name=&lt;span class=&quot;code-quote&quot;&gt;&quot;current&quot;&lt;/span&gt;&amp;gt;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&amp;lt;/bool&amp;gt;
  &amp;lt;bool name=&lt;span class=&quot;code-quote&quot;&gt;&quot;hasDeletions&quot;&lt;/span&gt;&amp;gt;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&amp;lt;/bool&amp;gt;
  &amp;lt;str name=&lt;span class=&quot;code-quote&quot;&gt;&quot;directory&quot;&lt;/span&gt;&amp;gt;org.apache.lucene.store.NRTCachingDirectory:NRTCachingDirectory(org.apache.lucene.store.MMapDirectory@/opt/solr/cores/shard_h/data/index.20130211094737738 lockFactory=org.apache.lucene.store.NativeFSLockFactory@2ca7563d; maxCacheMB=48.0 maxMergeSizeMB=4.0)&amp;lt;/str&amp;gt;
  &amp;lt;lst name=&lt;span class=&quot;code-quote&quot;&gt;&quot;userData&quot;&lt;/span&gt;&amp;gt;
    &amp;lt;str name=&lt;span class=&quot;code-quote&quot;&gt;&quot;commitTimeMSec&quot;&lt;/span&gt;&amp;gt;1361265544970&amp;lt;/str&amp;gt;
  &amp;lt;/lst&amp;gt;
  &amp;lt;date name=&lt;span class=&quot;code-quote&quot;&gt;&quot;lastModified&quot;&lt;/span&gt;&amp;gt;2013-02-19T09:19:04.97Z&amp;lt;/date&amp;gt;
&amp;lt;/lst&amp;gt;
&amp;lt;/response&amp;gt;

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&amp;lt;?xml version=&lt;span class=&quot;code-quote&quot;&gt;&quot;1.0&quot;&lt;/span&gt; encoding=&lt;span class=&quot;code-quote&quot;&gt;&quot;UTF-8&quot;&lt;/span&gt;?&amp;gt;
&amp;lt;response&amp;gt;

&amp;lt;lst name=&lt;span class=&quot;code-quote&quot;&gt;&quot;responseHeader&quot;&lt;/span&gt;&amp;gt;
  &amp;lt;&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; name=&lt;span class=&quot;code-quote&quot;&gt;&quot;status&quot;&lt;/span&gt;&amp;gt;0&amp;lt;/&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;&amp;gt;
  &amp;lt;&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; name=&lt;span class=&quot;code-quote&quot;&gt;&quot;QTime&quot;&lt;/span&gt;&amp;gt;0&amp;lt;/&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;&amp;gt;
&amp;lt;/lst&amp;gt;
&amp;lt;lst name=&lt;span class=&quot;code-quote&quot;&gt;&quot;index&quot;&lt;/span&gt;&amp;gt;
  &amp;lt;&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; name=&lt;span class=&quot;code-quote&quot;&gt;&quot;numDocs&quot;&lt;/span&gt;&amp;gt;117767&amp;lt;/&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;&amp;gt;
  &amp;lt;&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; name=&lt;span class=&quot;code-quote&quot;&gt;&quot;maxDoc&quot;&lt;/span&gt;&amp;gt;118181&amp;lt;/&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;&amp;gt;
  &amp;lt;&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; name=&lt;span class=&quot;code-quote&quot;&gt;&quot;deletedDocs&quot;&lt;/span&gt;&amp;gt;414&amp;lt;/&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;&amp;gt;
  &amp;lt;&lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; name=&lt;span class=&quot;code-quote&quot;&gt;&quot;version&quot;&lt;/span&gt;&amp;gt;3772&amp;lt;/&lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt;&amp;gt;
  &amp;lt;&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; name=&lt;span class=&quot;code-quote&quot;&gt;&quot;segmentCount&quot;&lt;/span&gt;&amp;gt;13&amp;lt;/&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;&amp;gt;
  &amp;lt;bool name=&lt;span class=&quot;code-quote&quot;&gt;&quot;current&quot;&lt;/span&gt;&amp;gt;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&amp;lt;/bool&amp;gt;
  &amp;lt;bool name=&lt;span class=&quot;code-quote&quot;&gt;&quot;hasDeletions&quot;&lt;/span&gt;&amp;gt;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&amp;lt;/bool&amp;gt;
  &amp;lt;str name=&lt;span class=&quot;code-quote&quot;&gt;&quot;directory&quot;&lt;/span&gt;&amp;gt;org.apache.lucene.store.NRTCachingDirectory:NRTCachingDirectory(org.apache.lucene.store.MMapDirectory@/opt/solr/cores/shard_h/data/index.20130211105622621 lockFactory=org.apache.lucene.store.NativeFSLockFactory@684b4388; maxCacheMB=48.0 maxMergeSizeMB=4.0)&amp;lt;/str&amp;gt;
  &amp;lt;lst name=&lt;span class=&quot;code-quote&quot;&gt;&quot;userData&quot;&lt;/span&gt;&amp;gt;
    &amp;lt;str name=&lt;span class=&quot;code-quote&quot;&gt;&quot;commitTimeMSec&quot;&lt;/span&gt;&amp;gt;1361265544937&amp;lt;/str&amp;gt;
  &amp;lt;/lst&amp;gt;
  &amp;lt;date name=&lt;span class=&quot;code-quote&quot;&gt;&quot;lastModified&quot;&lt;/span&gt;&amp;gt;2013-02-19T09:19:04.937Z&amp;lt;/date&amp;gt;
&amp;lt;/lst&amp;gt;
&amp;lt;/response&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We send updates/deletes to the cluster every 10-15 minutes. The shard will not become synchronized, unless i remove the index of one of the nodes.&lt;/p&gt;</comment>
                            <comment id="13581942" author="markrmiller@gmail.com" created="Wed, 20 Feb 2013 04:46:17 +0000"  >&lt;p&gt;No interesting exceptions in the logs? Perhaps dial them up to warn and run?&lt;/p&gt;</comment>
                            <comment id="13582030" author="markus17" created="Wed, 20 Feb 2013 08:30:32 +0000"  >&lt;p&gt;Nothing peculiar in the WARN logs. We don&apos;t log INFO usually unless something is really broken, that&apos;s too much data.&lt;/p&gt;</comment>
                            <comment id="13712297" author="markus17" created="Thu, 18 Jul 2013 13:11:12 +0000"  >&lt;p&gt;FYI: we&apos;re still seeing major inconsistencies, facet counts are off and when inspecting leaders and replica&apos;s we notice not all are in sync. This is on yesterday&apos;s trunk and with an empty index. There were no node failures during indexing. Shard_b&apos;s stats for example:&lt;/p&gt;

&lt;p&gt;node 2 shard b&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Last Modified:    about a minute ago
Num Docs:    158964
Max Doc:    158964
Deleted Docs:    0
Version:    4479
Segment Count:    1
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;node 3 shard b&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Last Modified:    2 minutes ago
Num Docs:    158298
Max Doc:    158298
Deleted Docs:    0
Version:    2886
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Size and versions are also different. Cluster is optimized/forceMerged but doesn&apos;t change the facts as expected. At least one other shard also has differences in its two replica&apos;s, i haven&apos;t manually checked the others.&lt;/p&gt;</comment>
                            <comment id="13712300" author="markrmiller@gmail.com" created="Thu, 18 Jul 2013 13:17:28 +0000"  >&lt;p&gt;See anything in the logs about zk expirations?&lt;/p&gt;</comment>
                            <comment id="13712314" author="markus17" created="Thu, 18 Jul 2013 13:21:01 +0000"  >&lt;p&gt;I&apos;ve already restarted the job and enabled logging! It&apos;s going to take a while &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13712400" author="markus17" created="Thu, 18 Jul 2013 15:12:07 +0000"  >&lt;p&gt;Alright, nothing looks like zookeeper expirations i grepped expirations in the error log but there&apos;s nothing there. This indexing session did not produce so many inconsistencies as the previous one; there is only 1 shard of which one replica has 2 more documents. It won&apos;t fix itself.&lt;/p&gt;

&lt;p&gt;During indexing there were, as usual, error such as autocommit causing a searcher too many and time outs talking to other nodes.&lt;/p&gt;

&lt;p&gt;Only 2 nodes report a Stopping Recovery For of which one node actually has a replica of the inconsistent core. The other shard is seems fine, both replica&apos;s have the same numDocs.&lt;/p&gt;</comment>
                            <comment id="13813827" author="yriveiro" created="Tue, 5 Nov 2013 10:31:33 +0000"  >&lt;p&gt;Hi, I hit this bug with solr 4.5.1&lt;/p&gt;

&lt;p&gt;replica 1:&lt;/p&gt;

&lt;p&gt;lastModified:20 minutes ago&lt;br/&gt;
version:80616&lt;br/&gt;
numDocs:6072661&lt;br/&gt;
maxDoc:6072841&lt;br/&gt;
deletedDocs:180&lt;/p&gt;

&lt;p&gt;replica 2 (leader)&lt;/p&gt;

&lt;p&gt;lastModified:20 minutes ago&lt;br/&gt;
version:77595&lt;br/&gt;
numDocs:6072575&lt;br/&gt;
maxDoc:6072771&lt;br/&gt;
deletedDocs:196&lt;/p&gt;

&lt;p&gt;I don&apos;t know when this happened, therefore I have no time frame to find in log valuable information on logs.&lt;/p&gt;</comment>
                            <comment id="13813850" author="yriveiro" created="Tue, 5 Nov 2013 11:11:17 +0000"  >&lt;p&gt;I attached some screenshots&lt;/p&gt;

&lt;p&gt;The shard is the shard11:&lt;/p&gt;

&lt;p&gt;1 - clusterstate: this screenshot shows replica2 192.168.20.104 as the leader&lt;br/&gt;
2 - the replica 2 has lower gen that replica1 and is the leader, is this correct?&lt;/p&gt;</comment>
                            <comment id="13820205" author="markus17" created="Tue, 12 Nov 2013 15:53:46 +0000"  >&lt;p&gt;I can confirm as well that is issue still exists. Since yesterday one of a shard&apos;s replica has one document less than it should have. Solr doesn&apos;t notice this and makes no attempt in recovering this issue. Around the time when i noticed it first we were shutting down and restarting nodes, it&apos;s likely that at that time some documents got indexed as well. &lt;/p&gt;</comment>
                            <comment id="13821300" author="markrmiller@gmail.com" created="Wed, 13 Nov 2013 13:15:29 +0000"  >&lt;p&gt;Yeah, I know that this can still happen - unfortunately, the debugging and testing required to make continued improvements requires a &lt;b&gt;lot&lt;/b&gt; of time, so I don&apos;t personally know when I can work on hardening it. Currently, if shards eventually get out of whack, the best you can do is trigger a new recovery against the leader. &lt;/p&gt;</comment>
                            <comment id="13821304" author="markrmiller@gmail.com" created="Wed, 13 Nov 2013 13:21:58 +0000"  >&lt;p&gt;I have done a bit of work on making it easier to turn on some debug logging that I use for this type of thing recently. I also have some specific local Jenkins jobs I run on dedicated hardware to help track down problems - I have been collecting a lot of logs over the last few weeks. There is a lot more that needs to be done though. I&apos;m hoping to start a wiki page on how I have gone about tracking this type of thing down in the past so that perhaps it&apos;s easier for others to get involved. Hopefully Yonik can add any of his useful tricks to that as well.&lt;/p&gt;</comment>
                            <comment id="13821375" author="yriveiro" created="Wed, 13 Nov 2013 14:26:54 +0000"  >&lt;blockquote&gt;&lt;p&gt; Currently, if shards eventually get out of whack, the best you can do is trigger a new recovery against the leader.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What happen when the leader is the shard with less docs? Is the replication done in the right way?&lt;/p&gt;</comment>
                            <comment id="13821421" author="markrmiller@gmail.com" created="Wed, 13 Nov 2013 15:31:08 +0000"  >&lt;blockquote&gt;&lt;p&gt;What happen when the leader is the shard with less docs? Is the replication done in the right way?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It really depends on how careful you are trying to be and what you can count on - you don&apos;t know if the leader is behind if deletes are involved. The safest thing to do is to stop the cluster and start it again - that triggers a process that tries to pick the most up to date replica and trades up to 100 updates or so among each other if some are on some replicas and not others.&lt;/p&gt;

&lt;p&gt;If you are sure the leader is simply behind, you can just bounce it and let a replica take over as leader.&lt;/p&gt;</comment>
                            <comment id="13823195" author="mewmewball" created="Fri, 15 Nov 2013 01:36:30 +0000"  >&lt;p&gt;We&apos;re seeing the same thing, running v4.5.0.&lt;/p&gt;

&lt;p&gt;Mark, if you don&apos;t mind clarifying...&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The safest thing to do is to stop the cluster and start it again - that triggers a process that tries to pick the most up to date replica and trades up to 100 updates or so among each other if some are on some replicas and not others.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;What if the difference is greater than 100? Is there any other way to figure out who is the &quot;truth&quot; and force that state onto the other replicas by doing a full sync?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If you are sure the leader is simply behind, you can just bounce it and let a replica take over as leader.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Newbie question: Why would the leader be behind? Aren&apos;t all updates sent to the leader first and then the leader distribute it to the replicas? Also, I was under the impression that this update call is synchronous, so once an update request returns successfully to the client, why would any replica be behind?&lt;/p&gt;
</comment>
                            <comment id="13823517" author="yriveiro" created="Fri, 15 Nov 2013 09:54:00 +0000"  >&lt;p&gt;Jessica, &lt;/p&gt;

&lt;p&gt;In some point of the process the leader can be downgraded to replica, the other replica whit less document will become the leader, in this case, the older leader (after the recovery) can be updated as usual and you get the leader behind the replica if the recovery doesn&apos;t fix the desviation.&lt;/p&gt;</comment>
                            <comment id="13823652" author="markrmiller@gmail.com" created="Fri, 15 Nov 2013 13:27:57 +0000"  >&lt;blockquote&gt;&lt;p&gt;What if the difference is greater than 100? Is there any other way to figure out who is the &quot;truth&quot; and force that state onto the other replicas by doing a full sync?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That is basically what should happen - everyone in the leader line will &quot;try&quot; and become the leader by trying to peer sync with everyone else - either they will be ahead of everyone else and the sync will succeed or they will be behind by less than 100 updates and trade and the sync will succeed. If the sync fails, the next guy in line tries. Eventually the most up to date guy should succeed and he forces everyone else to match him. That is the idea anyway.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Newbie question: Why would the leader be behind? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;ZooKeeper session timeouts (due to load, gc, whatever) can cause the leader to be bumped.&lt;/p&gt;

&lt;p&gt;You mainly only expect this stuff to happen if nodes go down (and perhaps come back) or session expirations.&lt;/p&gt;

&lt;p&gt;Unfortunately, for a while between 4.4 and 4.5, a couple of our important tests stopped working and I think a couple problems were introduced. I hope to have more time to look into it soon.&lt;/p&gt;</comment>
                            <comment id="13823834" author="mewmewball" created="Fri, 15 Nov 2013 17:31:08 +0000"  >&lt;p&gt;Thanks Yago and Mark. I really appreciate you guys spending time to answer my questions--it&apos;s definitely helping me understand more.&lt;/p&gt;

&lt;p&gt;I understand how leader roles can change, but not how replicas can be behind--I thought the updates are synchronously distributed (i.e. solrj&apos;s request doesn&apos;t return successfully until an update has been distributed to all replicas). Is this not the case?&lt;/p&gt;

&lt;p&gt;If any replica can fall behind, and it can be elected leader without having caught up, wouldn&apos;t we possibly end up in the following situation (where let&apos;s pretend docs are represented by monotonically increasing numbers):&lt;/p&gt;

&lt;p&gt;Old Leader: 1 2 3 4 5 (GC)&lt;br/&gt;
New Leader:  1 2 3 .. (elected) 6 7&lt;/p&gt;

&lt;p&gt;In this case, who&apos;s considered the most up-to-date guy? Would they figure out among themselves that Old Leader is missing 6 7 but the New Leader is missing 4 5? If so, how do they do the right &quot;merge&quot; if the difference is greater than 100 and they have to resort to full sync?&lt;/p&gt;</comment>
                            <comment id="13823867" author="yriveiro" created="Fri, 15 Nov 2013 18:00:07 +0000"  >&lt;blockquote&gt;&lt;p&gt;I thought the updates are synchronously distributed&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;My knowledge about how replication is done is very limited, for me replication is a distributed HTTP requests to all replicas, if all responses return the code 200, then the insertion was successful. I don&apos;t know if internally the 200 is returned when the document is written on tlog or in the open segment.&lt;/p&gt;

&lt;p&gt;Up-to-date in this case is none, you have your data compromised, you can&apos;t guarantee wich is the correct replica, the logic could be pick the replica with more docs and make a new replica using it, but still can know without check one by one if you have all data. An extreme case can be do a full reindex of the data (if you can).&lt;/p&gt;</comment>
                            <comment id="13823871" author="markrmiller@gmail.com" created="Fri, 15 Nov 2013 18:01:50 +0000"  >&lt;blockquote&gt;&lt;p&gt;I understand how leader roles can change, but not how replicas can be behind--I thought the updates are synchronously distributed (i.e. solrj&apos;s request doesn&apos;t return successfully until an update has been distributed to all replicas). Is this not the case?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;They could get behind because of a bug generally &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;
</comment>
                            <comment id="13823887" author="yriveiro" created="Fri, 15 Nov 2013 18:12:39 +0000"  >&lt;p&gt;Mark, &lt;/p&gt;

&lt;p&gt;I can confirm that I had session expirations in my logs in some point of time. My index rate is high and some times my boxes are under some &quot;pressure&quot;.&lt;/p&gt;

&lt;p&gt;My problem is that I don&apos;t know how deal with the situation. I&apos;m using a non java client and I don&apos;t know how I can do debug or the tools that I can use to give some information to help debug this issue.&lt;/p&gt;</comment>
                            <comment id="13823894" author="mewmewball" created="Fri, 15 Nov 2013 18:16:33 +0000"  >&lt;blockquote&gt;
&lt;p&gt;They could get behind because of a bug generally &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I see, so as designed, the scenario I described shouldn&apos;t really happen, because New Leader wouldn&apos;t have been missing 4 5?&lt;/p&gt;

&lt;p&gt;BTW, for our case, turns out the version numbers across the replicas matched even though numDocs didn&apos;t. That seems to suggest that there&#8217;s at least an issue somewhere in the replication flow where it&#8217;s possible to update the version but not have the matching documents. Not sure if this is a useful piece of information to you but thought I&apos;d mention.&lt;/p&gt;</comment>
                            <comment id="13823905" author="markrmiller@gmail.com" created="Fri, 15 Nov 2013 18:24:41 +0000"  >&lt;p&gt;I think of two failure scenarios:&lt;/p&gt;

&lt;p&gt;1. A replica goes down. Design solution: it comes back and uses peer sync or replication to catch up.&lt;/p&gt;

&lt;p&gt;2. A leader goes down. It might have been in the middle of sending updates and somehow a couple didn&apos;t make it to a replica. Design solution: that leader peer sync dance I talk about above.&lt;/p&gt;

&lt;p&gt;Basically, no one should ever be behind by more than 100 docs (a leader sends updates to replicas in parallel), and new leaders should always end up up to date. Obviously a bit more hardening to do though. Could also use more targeted testing - we count  a lot on the chaosmonkey tests for this (those are the tests that stopped working correctly for a while).&lt;/p&gt;

&lt;p&gt;A wrinkle is that zookeeper session timeouts also trigger the same thing as if the node had died - it comes back when the session is reestablished.&lt;/p&gt;</comment>
                            <comment id="13823922" author="markrmiller@gmail.com" created="Fri, 15 Nov 2013 18:40:11 +0000"  >&lt;blockquote&gt;&lt;p&gt;Basically, no one should ever be behind by more than 100 docs&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Of course if they are over 100 updates behind, they won&apos;t successfully sync and become leader, someone else will, and the behind node will be asked to catch up to the leader via replication.&lt;/p&gt;</comment>
                            <comment id="13823959" author="markrmiller@gmail.com" created="Fri, 15 Nov 2013 19:07:27 +0000"  >&lt;blockquote&gt;&lt;p&gt;My problem is that I don&apos;t know how deal with the situation. I&apos;m using a non java client and I don&apos;t know how I can do debug or the tools that I can use to give some information to help debug this issue.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m hoping to put together a guide on debugging some of the tests for this sort of thing soon. Perhaps some of that will also be useful for debugging a live installation. I sure could use the help - I have a lot on my plate at the moment. &lt;/p&gt;</comment>
                            <comment id="13824270" author="mewmewball" created="Sat, 16 Nov 2013 00:25:41 +0000"  >&lt;p&gt;Looking at the code a bit, I realized that the scenario I described can in fact happen if the Old Leader dies (or somehow becomes unreachable, for example due to tripping the kernel SYN flood detection, as ours did), because looks like during runLeaderProcess(), the sync that&apos;s run is called with cantReachIsSuccess=true. Since the New Leader can&apos;t reach Old Leader, it won&apos;t find out about 4 5 (assuming no other replicas have it either), but will successfully &quot;sync&quot; and become the new leader. This can be remedied if the &quot;// TODO: optionally fail if n replicas are not reached...&quot; on DistributedUpdateProcessor.doFinish() is implemented so that at least another replica must have 4 5 before the request would have been ack&apos;d to the user, but of course if New Leader can&apos;t reach this other replica either then it&apos;s not much help.&lt;/p&gt;

&lt;p&gt;I feel like in general the code may be trying too hard to find a new leader to take over, thereby compromising data consistency. This is probably the right thing to do for many, if not most, search solutions. However, if Solr is indeed moving toward being a possible NoSql solution or for use cases where reindexing the entire corpus is extremely expensive, then maybe a more consistent mode can be implemented where user can choose to trade availability for consistency.&lt;/p&gt;</comment>
                            <comment id="13824284" author="markrmiller@gmail.com" created="Sat, 16 Nov 2013 00:44:07 +0000"  >&lt;blockquote&gt;&lt;p&gt;Since the New Leader can&apos;t reach Old Leader, it won&apos;t find out about 4 5 (assuming no other replicas have it either)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This shouldn&apos;t be the case, because those updates will only have been ack&apos;d if each replica received them. And if they were not ack&apos;d a success, we don&apos;t care if we keep them - we just want to get consistent.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I feel like in general the code may be trying too hard to find a new leader to take over&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;A further protection is that a node will not become leader unless it&apos;s last state was active.&lt;/p&gt;

&lt;p&gt;I&apos;m not convinced it&apos;s too loose - I do know that the impl could use additional love and tests.&lt;/p&gt;</comment>
                            <comment id="13824295" author="mewmewball" created="Sat, 16 Nov 2013 00:56:07 +0000"  >&lt;blockquote&gt;
&lt;p&gt;This shouldn&apos;t be the case, because those updates will only have been ack&apos;d if each replica received them.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That&apos;s what I thought too, but doesn&apos;t seem to be the case in the code. If you take a look at DistributedUpdateProcessor.doFinish(),&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;    // if its a forward, any fail is a problem - &lt;br/&gt;
    // otherwise we assume things are fine if we got it locally&lt;br/&gt;
    // until we start allowing min replication param&lt;br/&gt;
    if (errors.size() &amp;gt; 0) {&lt;br/&gt;
      // if one node is a RetryNode, this was a forward request&lt;br/&gt;
      if (errors.get(0).req.node instanceof RetryNode) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {        rsp.setException(errors.get(0).e);      }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;      // else&lt;br/&gt;
      // for now we don&apos;t error - we assume if it was added locally, we&lt;br/&gt;
      // succeeded &lt;br/&gt;
    }&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It then starts a thread to urge the replica to recover, but if that fails, it just completely gives up.&lt;/p&gt;</comment>
                            <comment id="13824299" author="markrmiller@gmail.com" created="Sat, 16 Nov 2013 01:01:59 +0000"  >&lt;p&gt;Right - but that&apos;s just impl, not design. The idea is that, since we add locally first, there is not much reason it should fail on a replica - unless that replica has crashed or lost connectivity or something really bad. In that case, it will have to reconnect to zk and recover or restart and recover. Just in case, as a precaution, we try and tell it to recover - then if it&apos;s still got connectivity or it was an intermittent problem, it won&apos;t run around acting active. I think I have a note about perhaps doing more retries in background threads for that recovery request, but I&apos;ve never gotten to it.&lt;/p&gt;

&lt;p&gt;If you are finding a scenario that eludes that, we should strengthen the impl.&lt;/p&gt;</comment>
                            <comment id="13824921" author="markrmiller@gmail.com" created="Sun, 17 Nov 2013 18:20:11 +0000"  >&lt;p&gt;This could be related to &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-5397&quot; title=&quot;Replication can fail silently in some cases.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-5397&quot;&gt;&lt;del&gt;SOLR-5397&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13825107" author="markrmiller@gmail.com" created="Mon, 18 Nov 2013 04:38:04 +0000"  >&lt;p&gt;Would love if you guys could try with 4.6 and report back. &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-5397&quot; title=&quot;Replication can fail silently in some cases.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-5397&quot;&gt;&lt;del&gt;SOLR-5397&lt;/del&gt;&lt;/a&gt; was introduced when we fixed a similar issue, so that has really been an issue for a few releases.&lt;/p&gt;
</comment>
                            <comment id="13826350" author="markus17" created="Tue, 19 Nov 2013 09:45:28 +0000"  >&lt;p&gt;I updated our machines to include &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-5397&quot; title=&quot;Replication can fail silently in some cases.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-5397&quot;&gt;&lt;del&gt;SOLR-5397&lt;/del&gt;&lt;/a&gt;. Everything works fine now, it may take quite some time before we can say it is fixed &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13826356" author="yriveiro" created="Tue, 19 Nov 2013 10:00:20 +0000"  >&lt;p&gt;It&apos;s safe upgrade from 4.5.1 to 4.6?. I have docValues and I read that it&apos;s not linear upgraded and I can&apos;t reindex the data.&lt;/p&gt;</comment>
                            <comment id="13826529" author="markrmiller@gmail.com" created="Tue, 19 Nov 2013 13:58:53 +0000"  >&lt;p&gt;According to the wiki, it depends on the doc values impl you are using - the default one will upgrade fine. Others require that you forceMerge your index to rewrite it with the default and then upgrade, then I guess you can forceMerge back to that impl. Honestly, I have not had a chance to play with doc values yet though.&lt;/p&gt;</comment>
                            <comment id="13826533" author="yriveiro" created="Tue, 19 Nov 2013 14:10:55 +0000"  >&lt;p&gt;I&apos;m using  &amp;lt;codecFactory class=&quot;solr.SchemaCodecFactory&quot;/&amp;gt; to enable per-field DocValues formats.&lt;/p&gt;

&lt;p&gt;I think that this aspect about docValues it doesn&apos;t  explained on wiki in a proper way. There is no example how we can do the switch to default, do the forceMerge and switch back to the original implementation.&lt;/p&gt;

&lt;p&gt;If I can&apos;t have the security that all will work fine,  I can&apos;t do the upgrade.&lt;/p&gt;</comment>
                            <comment id="13826535" author="markrmiller@gmail.com" created="Tue, 19 Nov 2013 14:15:11 +0000"  >&lt;p&gt;Should probably bring it up on the user list - we need someone like &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rcmuir&quot; class=&quot;user-hover&quot; rel=&quot;rcmuir&quot;&gt;rcmuir&lt;/a&gt; to weigh in. I assume it all works the same way - you merge each field to the default impl and then back to what they were.&lt;/p&gt;</comment>
                            <comment id="13834760" author="markus17" created="Thu, 28 Nov 2013 11:58:59 +0000"  >&lt;p&gt;I&apos;ve got some bad news, it happened again on one of our clusters using a build of november 19th.Three replica&apos;s went out of sync.&lt;/p&gt;</comment>
                            <comment id="13835368" author="gro" created="Fri, 29 Nov 2013 13:40:24 +0000"  >&lt;p&gt;Happened to me two, collection with two four shards, each having a single replica. The replicas were our of sync.&lt;/p&gt;</comment>
                            <comment id="13835436" author="markrmiller@gmail.com" created="Fri, 29 Nov 2013 16:21:17 +0000"  >&lt;p&gt;What&apos;s the exact version / checkout?&lt;/p&gt;</comment>
                            <comment id="13835883" author="markrmiller@gmail.com" created="Sat, 30 Nov 2013 22:00:52 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markus17&quot; class=&quot;user-hover&quot; rel=&quot;markus17&quot;&gt;markus17&lt;/a&gt;, hopefully that&apos;s &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-5516&quot; title=&quot;ChaosMonkeyNothingIsSafeTest rare inconsistency fails.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-5516&quot;&gt;SOLR-5516&lt;/a&gt; then.&lt;/p&gt;</comment>
                            <comment id="13836414" author="markus17" created="Mon, 2 Dec 2013 10:54:10 +0000"  >&lt;p&gt;I&apos;ll check it out!&lt;/p&gt;</comment>
                            <comment id="13841162" author="markus17" created="Fri, 6 Dec 2013 10:42:00 +0000"  >&lt;p&gt;I&apos;m sorry, i&apos;ve got three replica&apos;s having one document less than the leader. We&apos;re on a december, 3th build.&lt;/p&gt;</comment>
                            <comment id="13841870" author="yriveiro" created="Fri, 6 Dec 2013 23:16:42 +0000"  >&lt;p&gt;Replicas are still losing docs in Solr 4.6 &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.&lt;/p&gt;

&lt;p&gt;I&apos;m wondering if we can&apos;t have a pair (version, numDocs) to track the increments of docs between versions. Also we can save the last 10 tlogs in each replica as backups after be commited and make a diff to see what is missing in case the replicas are out of sync, replay the transaction and avoid a not synchronized replica and a full-recovery that probably will be heaviest that make the diff.&lt;/p&gt;

&lt;p&gt;It&apos;s only and idea and of course find the bug must be the priority.&lt;/p&gt;

&lt;p&gt;This issue compromisse Solr to be &quot;the main&quot; storage. If re-index data is not possible, we can&apos;t guarantee that no data is missing,  and worse, we lost the data forever &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.&lt;/p&gt;</comment>
                            <comment id="13841896" author="markrmiller@gmail.com" created="Fri, 6 Dec 2013 23:37:14 +0000"  >&lt;p&gt;I&apos;ve fixed some things since 4.6 - I only had time to focus on the leader not going down case for 4.6, I spent a bunch more time on this case after 4.6 was released. Unfortunately, I think there are a couple of issues at play here - some of the new changes makes existing holes easier to spot and the chaos monkey tests where accidentally disabled for some time, so small issues may have crept in.&lt;/p&gt;

&lt;p&gt;I &lt;b&gt;think&lt;/b&gt; the remaining issue is mostly around &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-5516&quot; title=&quot;ChaosMonkeyNothingIsSafeTest rare inconsistency fails.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-5516&quot;&gt;SOLR-5516&lt;/a&gt;. Need to come up with a better idea than a really long wait though - but if someone wants to help test, putting in a long wait and stressing this would be useful to see if it is indeed the main remaining issue.&lt;/p&gt;

&lt;p&gt;I recently put in a lot of time improving the situation and I need to focus on other things for a bit, but that I&apos;ll keep coming back to this as I can.&lt;/p&gt;</comment>
                            <comment id="13845459" author="tim.potter" created="Wed, 11 Dec 2013 14:58:46 +0000"  >&lt;p&gt;I have some cycles to work on this issue over the next couple of days. I&apos;m starting by trying to reproduce it in my environment. Please let me know of any tasks that I can help out on (beyond the long wait stuff you mentioned above). &lt;/p&gt;</comment>
                            <comment id="13845847" author="tim.potter" created="Wed, 11 Dec 2013 23:35:00 +0000"  >&lt;p&gt;I don&apos;t have fix yet, but I wanted to post an update here to get some feedback on what I&apos;m seeing ...&lt;/p&gt;

&lt;p&gt;I have a simple SolrCloud configuration setup locally: 1 collection named &quot;cloud&quot; with 1 shard and replicationFactor 2, i.e. here&apos;s what I use to create it:&lt;br/&gt;
curl &quot;http://localhost:8984/solr/admin/collections?action=CREATE&amp;amp;name=cloud&amp;amp;replicationFactor=$REPFACT&amp;amp;numShards=1&amp;amp;collection.configName=cloud&quot;&lt;/p&gt;

&lt;p&gt;The collection gets distributed on two nodes: cloud84:8984 and cloud85:8985 with cloud84 being assigned the leader.&lt;/p&gt;

&lt;p&gt;Here&apos;s an outline of the process I used to get my collection out-of-sync during indexing:&lt;/p&gt;

&lt;p&gt;1) start indexing docs using CloudSolrServer in SolrJ - direct updates go to the leader and replica remains in sync for as long as I let this process run&lt;br/&gt;
2) kill -9 the process for the replica cloud85&lt;br/&gt;
3) let indexing continue against cloud84 for a few seconds (just to get the leader and replica out-of-sync once I bring the replica back online)&lt;br/&gt;
4) kill -9 the process for the leader cloud84 ... indexing halts of course as there are no running servers&lt;br/&gt;
5) start the replica cloud85 but do not start the previous leader cloud84&lt;/p&gt;

&lt;p&gt;Here are some key log messages as cloud85 - the replica - fires up ... my annotations of the log messages are prefixed by [TJP &amp;gt;&amp;gt;&lt;/p&gt;

&lt;p&gt;2013-12-11 11:43:22,076 &lt;span class=&quot;error&quot;&gt;&amp;#91;main-EventThread&amp;#93;&lt;/span&gt; INFO  common.cloud.ZkStateReader  - A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/clusterstate.json, has occurred - updating... (live nodes size: 1)&lt;br/&gt;
2013-12-11 11:43:23,370 &lt;span class=&quot;error&quot;&gt;&amp;#91;coreLoadExecutor-3-thread-1&amp;#93;&lt;/span&gt; INFO  solr.cloud.ShardLeaderElectionContext  - Waiting until we see more replicas up for shard shard1: total=2 found=1 timeoutin=139841&lt;/p&gt;

&lt;p&gt;[TJP &amp;gt;&amp;gt; This looks good and is expected because cloud85 was not the leader before it died, so it should not immediately assume it is the leader until it sees more replicas&lt;/p&gt;

&lt;p&gt;6) now start the previous leader cloud84 ...&lt;/p&gt;

&lt;p&gt;Here are some key log messages from cloud85 as the previous leader cloud84 is coming up ... &lt;/p&gt;

&lt;p&gt;2013-12-11 11:43:24,085 &lt;span class=&quot;error&quot;&gt;&amp;#91;main-EventThread&amp;#93;&lt;/span&gt; INFO  common.cloud.ZkStateReader  - Updating live nodes... (2)&lt;br/&gt;
2013-12-11 11:43:24,136 &lt;span class=&quot;error&quot;&gt;&amp;#91;main-EventThread&amp;#93;&lt;/span&gt; INFO  solr.cloud.DistributedQueue  - LatchChildWatcher fired on path: /overseer/queue state: SyncConnected type NodeChildrenChanged&lt;br/&gt;
2013-12-11 11:43:24,137 &lt;span class=&quot;error&quot;&gt;&amp;#91;Thread-13&amp;#93;&lt;/span&gt; INFO  common.cloud.ZkStateReader  - Updating cloud state from ZooKeeper... &lt;br/&gt;
2013-12-11 11:43:24,138 &lt;span class=&quot;error&quot;&gt;&amp;#91;Thread-13&amp;#93;&lt;/span&gt; INFO  solr.cloud.Overseer  - Update state numShards=1 message=&lt;/p&gt;
{
  &quot;operation&quot;:&quot;state&quot;,
  &quot;state&quot;:&quot;down&quot;,
  &quot;base_url&quot;:&quot;http://cloud84:8984/solr&quot;,
  &quot;core&quot;:&quot;cloud_shard1_replica2&quot;,
  &quot;roles&quot;:null,
  &quot;node_name&quot;:&quot;cloud84:8984_solr&quot;,
  &quot;shard&quot;:&quot;shard1&quot;,
  &quot;shard_range&quot;:null,
  &quot;shard_state&quot;:&quot;active&quot;,
  &quot;shard_parent&quot;:null,
  &quot;collection&quot;:&quot;cloud&quot;,
  &quot;numShards&quot;:&quot;1&quot;,
  &quot;core_node_name&quot;:&quot;core_node1&quot;}

&lt;p&gt;[TJP &amp;gt;&amp;gt; state of cloud84 looks correct as it is still initializing ...&lt;/p&gt;

&lt;p&gt;2013-12-11 11:43:24,140 &lt;span class=&quot;error&quot;&gt;&amp;#91;main-EventThread&amp;#93;&lt;/span&gt; INFO  solr.cloud.DistributedQueue  - LatchChildWatcher fired on path: /overseer/queue state: SyncConnected type NodeChildrenChanged&lt;br/&gt;
2013-12-11 11:43:24,141 &lt;span class=&quot;error&quot;&gt;&amp;#91;main-EventThread&amp;#93;&lt;/span&gt; INFO  common.cloud.ZkStateReader  - A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/clusterstate.json, has occurred - updating... (live nodes size: 2)&lt;/p&gt;

&lt;p&gt;2013-12-11 11:43:25,878 &lt;span class=&quot;error&quot;&gt;&amp;#91;coreLoadExecutor-3-thread-1&amp;#93;&lt;/span&gt; INFO  solr.cloud.ShardLeaderElectionContext  - Enough replicas found to continue.&lt;/p&gt;

&lt;p&gt;[TJP &amp;gt;&amp;gt; hmmmm ... cloud84 is listed in /live_nodes but it isn&apos;t &quot;active&quot; yet or even recovering (see state above - it&apos;s currently &quot;down&quot;) ... My thinking here is that the ShardLeaderElectionContext needs to take the state of the replica into account before deciding it should continue.&lt;/p&gt;


&lt;p&gt;2013-12-11 11:43:25,878 &lt;span class=&quot;error&quot;&gt;&amp;#91;coreLoadExecutor-3-thread-1&amp;#93;&lt;/span&gt; INFO  solr.cloud.ShardLeaderElectionContext  - I may be the new leader - try and sync&lt;br/&gt;
2013-12-11 11:43:25,878 &lt;span class=&quot;error&quot;&gt;&amp;#91;coreLoadExecutor-3-thread-1&amp;#93;&lt;/span&gt; INFO  solr.cloud.SyncStrategy  - Sync replicas to &lt;a href=&quot;http://cloud85:8985/solr/cloud_shard1_replica1/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://cloud85:8985/solr/cloud_shard1_replica1/&lt;/a&gt;&lt;br/&gt;
2013-12-11 11:43:25,880 &lt;span class=&quot;error&quot;&gt;&amp;#91;coreLoadExecutor-3-thread-1&amp;#93;&lt;/span&gt; INFO  solr.update.PeerSync  - PeerSync: core=cloud_shard1_replica1 url=&lt;a href=&quot;http://cloud85:8985/solr&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://cloud85:8985/solr&lt;/a&gt; START replicas=&lt;a href=&quot;http://cloud84:8984/solr/cloud_shard1_replica2/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://cloud84:8984/solr/cloud_shard1_replica2/&lt;/a&gt; nUpdates=100&lt;br/&gt;
2013-12-11 11:43:25,936 &lt;span class=&quot;error&quot;&gt;&amp;#91;coreLoadExecutor-3-thread-1&amp;#93;&lt;/span&gt; WARN  solr.update.PeerSync  - PeerSync: core=cloud_shard1_replica1 url=&lt;a href=&quot;http://cloud85:8985/solr&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://cloud85:8985/solr&lt;/a&gt;  couldn&apos;t connect to &lt;a href=&quot;http://cloud84:8984/solr/cloud_shard1_replica2/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://cloud84:8984/solr/cloud_shard1_replica2/&lt;/a&gt;, counting as success&lt;/p&gt;


&lt;p&gt;[TJP &amp;gt;&amp;gt; whoops! of course it couldn&apos;t connect to cloud84 as it&apos;s still initializing ...&lt;/p&gt;


&lt;p&gt;2013-12-11 11:43:25,936 &lt;span class=&quot;error&quot;&gt;&amp;#91;coreLoadExecutor-3-thread-1&amp;#93;&lt;/span&gt; INFO  solr.update.PeerSync  - PeerSync: core=cloud_shard1_replica1 url=&lt;a href=&quot;http://cloud85:8985/solr&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://cloud85:8985/solr&lt;/a&gt; DONE. sync succeeded&lt;br/&gt;
2013-12-11 11:43:25,937 &lt;span class=&quot;error&quot;&gt;&amp;#91;coreLoadExecutor-3-thread-1&amp;#93;&lt;/span&gt; INFO  solr.cloud.SyncStrategy  - Sync Success - now sync replicas to me&lt;br/&gt;
2013-12-11 11:43:25,937 &lt;span class=&quot;error&quot;&gt;&amp;#91;coreLoadExecutor-3-thread-1&amp;#93;&lt;/span&gt; INFO  solr.cloud.SyncStrategy  - &lt;a href=&quot;http://cloud85:8985/solr/cloud_shard1_replica1/:&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://cloud85:8985/solr/cloud_shard1_replica1/:&lt;/a&gt; try and ask &lt;a href=&quot;http://cloud84:8984/solr/cloud_shard1_replica2/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://cloud84:8984/solr/cloud_shard1_replica2/&lt;/a&gt; to sync&lt;br/&gt;
2013-12-11 11:43:25,938 &lt;span class=&quot;error&quot;&gt;&amp;#91;coreLoadExecutor-3-thread-1&amp;#93;&lt;/span&gt; ERROR solr.cloud.SyncStrategy  - Sync request error: org.apache.solr.client.solrj.SolrServerException: Server refused connection at: &lt;a href=&quot;http://cloud84:8984/solr/cloud_shard1_replica2&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://cloud84:8984/solr/cloud_shard1_replica2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[TJP &amp;gt;&amp;gt; ayep, cloud84 is still initializing so it can&apos;t respond to you Mr. Impatient cloud85!&lt;/p&gt;

&lt;p&gt;2013-12-11 11:43:25,939 &lt;span class=&quot;error&quot;&gt;&amp;#91;coreLoadExecutor-3-thread-1&amp;#93;&lt;/span&gt; INFO  solr.cloud.SyncStrategy  - &lt;a href=&quot;http://cloud85:8985/solr/cloud_shard1_replica1/:&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://cloud85:8985/solr/cloud_shard1_replica1/:&lt;/a&gt; Sync failed - asking replica (&lt;a href=&quot;http://cloud84:8984/solr/cloud_shard1_replica2/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://cloud84:8984/solr/cloud_shard1_replica2/&lt;/a&gt;) to recover.&lt;br/&gt;
2013-12-11 11:43:25,940 &lt;span class=&quot;error&quot;&gt;&amp;#91;coreLoadExecutor-3-thread-1&amp;#93;&lt;/span&gt; INFO  solr.cloud.ShardLeaderElectionContext  - I am the new leader: &lt;a href=&quot;http://cloud85:8985/solr/cloud_shard1_replica1/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://cloud85:8985/solr/cloud_shard1_replica1/&lt;/a&gt; shard1&lt;/p&gt;

&lt;p&gt;[TJP &amp;gt;&amp;gt; oh no! the collection is now out-of-sync ... my test harness periodically polls the replicas for their doc counts and at this point, we ended up with:&lt;br/&gt;
shard1: {&lt;br/&gt;
	&lt;a href=&quot;http://cloud85:8985/solr/cloud_shard1_replica1/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://cloud85:8985/solr/cloud_shard1_replica1/&lt;/a&gt; = 300800 LEADER&lt;br/&gt;
	&lt;a href=&quot;http://cloud84:8984/solr/cloud_shard1_replica2/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://cloud84:8984/solr/cloud_shard1_replica2/&lt;/a&gt; = 447600 diff:&lt;del&gt;146800  &amp;lt;&lt;/del&gt;-- this should be the real leader!&lt;br/&gt;
}&lt;/p&gt;

&lt;p&gt;Which of course is expected because cloud85 should &lt;b&gt;NOT&lt;/b&gt; be the leader&lt;/p&gt;

&lt;p&gt;So all that is interesting, but how to fix???&lt;/p&gt;

&lt;p&gt;My first idea was to go tackle the decision making process ShardLeaderElectionContext uses to decide if it has enough replicas to continue. &lt;/p&gt;

&lt;p&gt;It&apos;s easy enough to do something like the following:&lt;br/&gt;
        int notDownCount = 0;&lt;br/&gt;
        Map&amp;lt;String,Replica&amp;gt; replicasMap = slices.getReplicasMap();&lt;br/&gt;
        for (Replica replica : replicasMap.values()) {&lt;br/&gt;
          ZkCoreNodeProps replicaCoreProps = new ZkCoreNodeProps(replica);&lt;br/&gt;
          String replicaState = replicaCoreProps.getState();&lt;br/&gt;
          log.warn(&quot;&amp;gt;&amp;gt;&amp;gt;&amp;gt; State of replica &quot;&lt;ins&gt;replica.getName()&lt;/ins&gt;&quot; is &quot;&lt;ins&gt;replicaState&lt;/ins&gt;&quot; &amp;lt;&amp;lt;&amp;lt;&amp;lt;&quot;);&lt;br/&gt;
          if (&quot;active&quot;.equals(replicaState) || &quot;recovering&quot;.equals(replicaState)) &lt;/p&gt;
{
            ++notDownCount;
          }
&lt;p&gt;        }&lt;/p&gt;

&lt;p&gt;Was thinking I could use the notDownCount to make a better decision, but then I ran into another issue related to replica state being stale. In my cluster, if I have /clusterstate.json:&lt;/p&gt;

&lt;p&gt;{&quot;cloud&quot;:{&lt;br/&gt;
    &quot;shards&quot;:{&quot;shard1&quot;:{&lt;br/&gt;
        &quot;range&quot;:&quot;80000000-7fffffff&quot;,&lt;br/&gt;
        &quot;state&quot;:&quot;active&quot;,&lt;br/&gt;
        &quot;replicas&quot;:{&lt;br/&gt;
          &quot;core_node1&quot;:&lt;/p&gt;
{
            &quot;state&quot;:&quot;active&quot;,
            &quot;base_url&quot;:&quot;http://cloud84:8984/solr&quot;,
            &quot;core&quot;:&quot;cloud_shard1_replica2&quot;,
            &quot;node_name&quot;:&quot;cloud84:8984_solr&quot;,
            &quot;leader&quot;:&quot;true&quot;}
&lt;p&gt;,&lt;br/&gt;
          &quot;core_node2&quot;:{&lt;br/&gt;
            &quot;state&quot;:&quot;active&quot;,&lt;br/&gt;
            &quot;base_url&quot;:&quot;http://cloud85:8985/solr&quot;,&lt;br/&gt;
            &quot;core&quot;:&quot;cloud_shard1_replica1&quot;,&lt;br/&gt;
            &quot;node_name&quot;:&quot;cloud85:8985_solr&quot;}}}},&lt;br/&gt;
    &quot;maxShardsPerNode&quot;:&quot;1&quot;,&lt;br/&gt;
    &quot;router&quot;:&lt;/p&gt;
{&quot;name&quot;:&quot;compositeId&quot;}
&lt;p&gt;,&lt;br/&gt;
    &quot;replicationFactor&quot;:&quot;2&quot;}}&lt;/p&gt;

&lt;p&gt;If I kill the process using kill -9 PID for the Solr running on 8985 (the replica), core_node2&apos;s state remains &quot;active&quot; in /clusterstate.json&lt;/p&gt;

&lt;p&gt;When tailing the log on core_node1, I do see one notification coming in the watcher setup by ZkStateReader from ZooKeeper about live nodes having changed:&lt;br/&gt;
2013-12-11 15:42:46,010 &lt;span class=&quot;error&quot;&gt;&amp;#91;main-EventThread&amp;#93;&lt;/span&gt; INFO  common.cloud.ZkStateReader  - Updating live nodes... (1)&lt;/p&gt;

&lt;p&gt;So after killing the process, /live_nodes is updated to only have one node, but /clusterstate.json still thinks there are 2 healthy replicas for shard1, instead of just 1.&lt;/p&gt;

&lt;p&gt;Of course, if I restart 8985, then it goes through a series of state changes until it is marked active again, which looks correct.&lt;/p&gt;

&lt;p&gt;Bottom line ... it seems there is something in SolrCloud that does not update a replica&apos;s state when the node is killed. If a change to /live_nodes doesn&apos;t trigger a refresh of replica state, what does?&lt;/p&gt;

&lt;p&gt;I&apos;m seeing this stale replica state issue in Solr 4.6.0 and in revision 1550300 of branch_4x - the latest from svn.&lt;/p&gt;

&lt;p&gt;Not having a fresh state of a replica prevents my idea for fixing ShardLeaderElectionContext&apos;s decision making process. I&apos;m also curious about the decision to register a node under /live_nodes before it is fully initialized, but maybe that is a discussion for another time.&lt;/p&gt;

&lt;p&gt;In any case, I wanted to get some feedback on my findings before moving forward with a solution.&lt;/p&gt;</comment>
                            <comment id="13845887" author="markrmiller@gmail.com" created="Thu, 12 Dec 2013 00:13:41 +0000"  >&lt;p&gt;A lot there! I&apos;ll respond to most of it later.&lt;/p&gt;

&lt;p&gt;As far as the stale state, that is expected. You cannot tell the state just from clusterstate.json - it is a mix of clusterstate.json and the live_nodes list. If the livenode for anything in clusterstate.json is missing, it&apos;s considered not up. This is just currently by design - without live_nodes, you don&apos;t know the state.&lt;/p&gt;</comment>
                            <comment id="13845895" author="tim.potter" created="Thu, 12 Dec 2013 00:20:08 +0000"  >&lt;p&gt;ok - cool ... just wanted to make sure that &quot;stale&quot; situation was expected ...&lt;/p&gt;

&lt;p&gt;the more I dig into ShardLeaderElectionContext&apos;s decision making process, I think looking at state won&apos;t work because both are in the &quot;down&quot; state while this is happening. I think some determination of is the node &quot;reachable&quot; so that PeerSync can get good information from it is what needs to be factored into ShardLeaderElectionContext. Or maybe there is another state &quot;trying to figure out my role in the world as I come back up&quot; &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13846572" author="markrmiller@gmail.com" created="Thu, 12 Dec 2013 18:44:03 +0000"  >&lt;p&gt;The other issue is expected as well. It&apos;s the safety mechanism - we don&apos;t let you just start one node and let it becomes leader - ideally you want all replicas to be involved in the election to prevent data loss. You have to be explicit if you want to have this work with no wait. It might be nice if we added a startup sys prop that caused it not to wait on first startup. &lt;/p&gt;</comment>
                            <comment id="13846593" author="tim.potter" created="Thu, 12 Dec 2013 19:01:46 +0000"  >&lt;p&gt;Agreed on the wait being necessary (which I actually annotated in the comment above). The crux of the issue here is that the replica (cloud85) can&apos;t sync with the previous leader (cloud84) because they are waiting on each other; much like a dead-lock. Eventually, they both give up and one wins; unfortunately in my test case, cloud85 wins which leads to the shard being out-of-sync because the wrong leader is selected in this scenario (cloud84 should have been selected). &lt;/p&gt;

&lt;p&gt;I&apos;m continuing to dig into this but have come to the conclusion that tweaking the waitForReplicasToComeUp process is a dead end and it&apos;s working as well as it can.&lt;/p&gt;</comment>
                            <comment id="13846694" author="markrmiller@gmail.com" created="Thu, 12 Dec 2013 20:19:23 +0000"  >&lt;blockquote&gt;&lt;p&gt;because they are waiting on each other;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That doesn&apos;t make sense to me - the wait should be until all the replicas for a shard are up - so what exactly are they both waiting on? If they are both waiting, there should be enough replicas up to continue...&lt;/p&gt;</comment>
                            <comment id="13846755" author="tim.potter" created="Thu, 12 Dec 2013 21:25:46 +0000"  >&lt;p&gt;I&apos;m sorry for being unclear; &quot;waiting&quot; was probably the wrong term ... and they definitely continue right on down the path of selecting the wrong leader. &lt;/p&gt;

&lt;p&gt;Here&apos;s what I know so far, which admittedly isn&apos;t much:&lt;/p&gt;

&lt;p&gt;As cloud85 (replica before it crashed) is initializing, it enters the wait process in ShardLeaderElectionContext#waitForReplicasToComeUp; this is expected and a good thing.&lt;/p&gt;

&lt;p&gt;Some short amount of time in the future, cloud84 (leader before it crashed) begins initializing and gets to a point where it adds itself as a possible leader for the shard (by creating a znode under /collections/cloud/leaders_elect/shard1/election), which leads to cloud85 being able to return from waitForReplicasToComeUp and try to determine who should be the leader.&lt;/p&gt;

&lt;p&gt;cloud85 then tries to run the SyncStrategy, which can never work because in this scenario the Jetty HTTP listener is not active yet on either node, so all replication work that uses HTTP requests fails on both nodes ... PeerSync treats these failures as indicators that the other replicas in the shard are unavailable (or whatever) and assumes success. Here&apos;s the log message:&lt;/p&gt;

&lt;p&gt;2013-12-11 11:43:25,936 &lt;span class=&quot;error&quot;&gt;&amp;#91;coreLoadExecutor-3-thread-1&amp;#93;&lt;/span&gt; WARN solr.update.PeerSync - PeerSync: core=cloud_shard1_replica1 url=&lt;a href=&quot;http://cloud85:8985/solr&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://cloud85:8985/solr&lt;/a&gt; couldn&apos;t connect to &lt;a href=&quot;http://cloud84:8984/solr/cloud_shard1_replica2/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://cloud84:8984/solr/cloud_shard1_replica2/&lt;/a&gt;, counting as success&lt;/p&gt;

&lt;p&gt;The Jetty HTTP listener doesn&apos;t start accepting connections until long after this process has completed and already selected the wrong leader.&lt;/p&gt;

&lt;p&gt;From what I can see, we seem to have a leader recovery process that is based partly on HTTP requests to the other nodes, but the HTTP listener on those nodes isn&apos;t active yet. We need a leader recovery process that doesn&apos;t rely on HTTP requests. Perhaps, leader recovery for a shard w/o a current leader may need to work differently than leader election in a shard that has replicas that can respond to HTTP requests? All of what I&apos;m seeing makes perfect sense for leader election when there are active replicas and the current leader fails.&lt;/p&gt;

&lt;p&gt;All this aside, I&apos;m not asserting that this is the only cause for the out-of-sync issues reported in this ticket, but it definitely seems like it could happen in a real cluster.&lt;/p&gt;</comment>
                            <comment id="13846775" author="markrmiller@gmail.com" created="Thu, 12 Dec 2013 21:42:50 +0000"  >&lt;p&gt;Ah, thanks for the explanation.  I think we should roll that specific issue into a new JIRA issue. &lt;/p&gt;</comment>
                            <comment id="13846812" author="tim.potter" created="Thu, 12 Dec 2013 22:09:46 +0000"  >&lt;p&gt;Mark -&amp;gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-5552&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/SOLR-5552&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13855273" author="markrmiller@gmail.com" created="Sun, 22 Dec 2013 20:57:17 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-5552&quot; title=&quot;Leader recovery process can select the wrong leader if all replicas for a shard are down and trying to recover as well as lose updates that should have been recovered.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-5552&quot;&gt;&lt;del&gt;SOLR-5552&lt;/del&gt;&lt;/a&gt; investigation has also led to &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-5569&quot; title=&quot;A replica should not try and recover from a leader until it has published that it is ACTIVE.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-5569&quot;&gt;&lt;del&gt;SOLR-5569&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-5568&quot; title=&quot;A SolrCore cannot decide to be the leader just because the cluster state says no other SolrCore&amp;#39;s are active.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-5568&quot;&gt;&lt;del&gt;SOLR-5568&lt;/del&gt;&lt;/a&gt; &lt;/p&gt;</comment>
                            <comment id="13856002" author="tim.potter" created="Mon, 23 Dec 2013 23:05:41 +0000"  >&lt;p&gt;Found another interesting case that may or may not be valid depending on whether we think HTTP requests between a leader and replica can fail even if the ZooKeeper session on the replica does not drop?&lt;/p&gt;

&lt;p&gt;Specifically, what I&apos;m seeing is that if an update request between the leader and replica fails, but the replica doesn&apos;t lose it&apos;s session with ZK, then the replica can get out-of-sync with the leader. In a real network partition, the ZK connection would also likely be lost and the replica would get marked as down. So as long as the HTTP connection timeout between the leader and replica exceeds the ZK client timeout, the replica would probably recover correctly, rendering this test case invalid. So maybe the main question here is whether we think it&apos;s possible for HTTP requests between a leader and replica to fail even though the ZooKeeper connection stays alive?&lt;/p&gt;

&lt;p&gt;Here&apos;s the steps I used to reproduce this case (all using revision 1553150 in branch_4x):&lt;/p&gt;

&lt;p&gt;&lt;b&gt;&amp;gt; STEP 1: Setup a collection named &#8220;cloud&#8221; containing 1 shard and 2 replicas on hosts: cloud84 (127.0.0.1:8984) and cloud85 (127.0.0.1:8985)&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;SOLR_TOP=/home/ec2-user/branch_4x/solr&lt;br/&gt;
$SOLR_TOP/cloud84/cloud-scripts/zkcli.sh -zkhost $ZK_HOST -cmd upconfig -confdir $SOLR_TOP/cloud84/solr/cloud/conf -confname cloud&lt;br/&gt;
API=&lt;a href=&quot;http://localhost:8984/solr/admin/collections&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://localhost:8984/solr/admin/collections&lt;/a&gt;&lt;br/&gt;
curl -v &quot;$API?action=CREATE&amp;amp;name=cloud&amp;amp;replicationFactor=2&amp;amp;numShards=1&amp;amp;collection.configName=cloud&quot;&lt;/p&gt;

&lt;p&gt;Replica on cloud84 is elected as the initial leader. /clusterstate.json looks like:&lt;/p&gt;

&lt;p&gt;{&quot;cloud&quot;:{&lt;br/&gt;
    &quot;shards&quot;:{&quot;shard1&quot;:{&lt;br/&gt;
        &quot;range&quot;:&quot;80000000-7fffffff&quot;,&lt;br/&gt;
        &quot;state&quot;:&quot;active&quot;,&lt;br/&gt;
        &quot;replicas&quot;:{&lt;br/&gt;
          &quot;core_node1&quot;:&lt;/p&gt;
{
            &quot;state&quot;:&quot;active&quot;,
            &quot;base_url&quot;:&quot;http://cloud84:8984/solr&quot;,
            &quot;core&quot;:&quot;cloud_shard1_replica1&quot;,
            &quot;node_name&quot;:&quot;cloud84:8984_solr&quot;,
            &quot;leader&quot;:&quot;true&quot;}
&lt;p&gt;,&lt;br/&gt;
          &quot;core_node2&quot;:{&lt;br/&gt;
            &quot;state&quot;:&quot;active&quot;,&lt;br/&gt;
            &quot;base_url&quot;:&quot;http://cloud85:8985/solr&quot;,&lt;br/&gt;
            &quot;core&quot;:&quot;cloud_shard1_replica2&quot;,&lt;br/&gt;
            &quot;node_name&quot;:&quot;cloud85:8985_solr&quot;}}}},&lt;br/&gt;
    &quot;maxShardsPerNode&quot;:&quot;1&quot;,&lt;br/&gt;
    &quot;router&quot;:&lt;/p&gt;
{&quot;name&quot;:&quot;compositeId&quot;}
&lt;p&gt;,&lt;br/&gt;
    &quot;replicationFactor&quot;:&quot;2&quot;}}&lt;/p&gt;


&lt;p&gt;&lt;b&gt;&amp;gt; STEP 2: Simulate network partition&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;sudo iptables -I INPUT 1 -i lo -p tcp --sport 8985 -j DROP; sudo iptables -I INPUT 2 -i lo -p tcp --dport 8985 -j DROP&lt;/p&gt;

&lt;p&gt;Various ways to do this, but to keep it simple, I&apos;m just dropping inbound traffic on localhost to port 8985.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;&amp;gt; STEP 3: Send document with ID &#8220;doc1&#8221; to leader on cloud84&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;curl &quot;http://localhost:8984/solr/cloud/update&quot; -H &apos;Content-type:application/xml&apos; \&lt;br/&gt;
  --data-binary &apos;&amp;lt;add&amp;gt;&amp;lt;doc&amp;gt;&amp;lt;field name=&quot;id&quot;&amp;gt;doc1&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;foo_s&quot;&amp;gt;bar&amp;lt;/field&amp;gt;&amp;lt;/doc&amp;gt;&amp;lt;/add&amp;gt;&apos;&lt;/p&gt;

&lt;p&gt;The update request takes some time because the replica is down but ultimately succeeds on the leader. In the logs on the leader, we have (some stack trace lines removed for clarity):&lt;/p&gt;

&lt;p&gt;2013-12-23 10:59:33,688 &lt;span class=&quot;error&quot;&gt;&amp;#91;updateExecutor-1-thread-1&amp;#93;&lt;/span&gt; ERROR solr.update.StreamingSolrServers  - error&lt;br/&gt;
org.apache.http.conn.HttpHostConnectException: Connection to &lt;a href=&quot;http://cloud85:8985&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://cloud85:8985&lt;/a&gt; refused&lt;br/&gt;
        at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:190)&lt;br/&gt;
        ...&lt;br/&gt;
        at org.apache.solr.client.solrj.impl.ConcurrentUpdateSolrServer$Runner.run(ConcurrentUpdateSolrServer.java:232)&lt;br/&gt;
        ...&lt;br/&gt;
Caused by: java.net.ConnectException: Connection timed out&lt;br/&gt;
        ...&lt;br/&gt;
2013-12-23 10:59:33,695 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp1073932139-16&amp;#93;&lt;/span&gt; INFO  update.processor.LogUpdateProcessor  - &lt;span class=&quot;error&quot;&gt;&amp;#91;cloud_shard1_replica1&amp;#93;&lt;/span&gt; webapp=/solr path=/update params={} &lt;/p&gt;
{add=[doc1 (1455228778490888192)]}
&lt;p&gt; 0 63256&lt;br/&gt;
2013-12-23 10:59:33,702 &lt;span class=&quot;error&quot;&gt;&amp;#91;updateExecutor-1-thread-2&amp;#93;&lt;/span&gt; INFO  update.processor.DistributedUpdateProcessor  - try and ask &lt;a href=&quot;http://cloud85:8985/solr&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://cloud85:8985/solr&lt;/a&gt; to recover&lt;br/&gt;
2013-12-23 10:59:48,718 &lt;span class=&quot;error&quot;&gt;&amp;#91;updateExecutor-1-thread-2&amp;#93;&lt;/span&gt; ERROR update.processor.DistributedUpdateProcessor  - &lt;a href=&quot;http://cloud85:8985/solr:&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://cloud85:8985/solr:&lt;/a&gt; Could not tell a replica to recover:org.apache.solr.client.solrj.SolrServerException: IOException occured when talking to server at: &lt;a href=&quot;http://cloud85:8985/solr&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://cloud85:8985/solr&lt;/a&gt;&lt;br/&gt;
        at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:507)&lt;br/&gt;
        at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:199)&lt;br/&gt;
        at org.apache.solr.update.processor.DistributedUpdateProcessor$1.run(DistributedUpdateProcessor.java:657)&lt;br/&gt;
        ...&lt;br/&gt;
Caused by: org.apache.http.conn.ConnectTimeoutException: Connect to cloud85:8985 timed out&lt;br/&gt;
        ...&lt;/p&gt;

&lt;p&gt;Of course these log messages are expected. The key is that the leader accepted the update and now has one doc with ID &quot;doc1&quot;&lt;/p&gt;

&lt;p&gt;&amp;gt; STEP 4: Heal the network partition&lt;/p&gt;

&lt;p&gt;sudo service iptables restart (undoes the DROP rules we added above)&lt;/p&gt;

&lt;p&gt;&lt;b&gt;&amp;gt; STEP 5: Send document with ID &#8220;doc2&#8221; to leader on cloud84&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;curl &quot;http://localhost:8984/solr/cloud/update&quot; -H &apos;Content-type:application/xml&apos; \&lt;br/&gt;
  --data-binary &apos;&amp;lt;add&amp;gt;&amp;lt;doc&amp;gt;&amp;lt;field name=&quot;id&quot;&amp;gt;doc2&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;foo_s&quot;&amp;gt;bar&amp;lt;/field&amp;gt;&amp;lt;/doc&amp;gt;&amp;lt;/add&amp;gt;&apos;&lt;/p&gt;

&lt;p&gt;Of course this time the update gets sent successfully to replica ... here are some log messages ...&lt;/p&gt;

&lt;p&gt;from the log on cloud84:&lt;br/&gt;
2013-12-23 11:00:46,982 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp1073932139-18&amp;#93;&lt;/span&gt; INFO  update.processor.LogUpdateProcessor  - &lt;span class=&quot;error&quot;&gt;&amp;#91;cloud_shard1_replica1&amp;#93;&lt;/span&gt; webapp=/solr path=/update params={} &lt;/p&gt;
{add=[doc2 (1455228921389776896)]} 0 162&lt;br/&gt;
&lt;br/&gt;
from the log on cloud85 (out-of-sync replica):&lt;br/&gt;
2013-12-23 10:47:26,363 &lt;span class=&quot;error&quot;&gt;&amp;#91;main-EventThread&amp;#93;&lt;/span&gt; INFO  common.cloud.ZkStateReader  - A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/clusterstate.json, has occurred - updating... (live nodes size: 2)&lt;br/&gt;
&lt;br/&gt;
...&lt;br/&gt;
&lt;br/&gt;
*2013-12-23 11:00:46,979 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp2124890785-12&amp;#93;&lt;/span&gt; INFO  update.processor.LogUpdateProcessor  - &lt;span class=&quot;error&quot;&gt;&amp;#91;cloud_shard1_replica2&amp;#93;&lt;/span&gt; webapp=/solr path=/update params={distrib.from=http://cloud84:8984/solr/cloud_shard1_replica1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2} {add=[doc2 (1455228921389776896)]}
&lt;p&gt; 0 142*&lt;/p&gt;

&lt;p&gt;Notice that there is no logged activity on cloud85 between 10:47 and 11:00&lt;/p&gt;

&lt;p&gt;&lt;b&gt;&amp;gt; STEP 6: Commit updates&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;curl &quot;http://localhost:8984/solr/cloud/update&quot; -H &apos;Content-type:application/xml&apos; --data-binary &quot;&amp;lt;commit waitSearcher=\&quot;true\&quot;/&amp;gt;&quot;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;&amp;gt; STEP 7: Send non-distributed queries to each replica&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;curl &quot;http://localhost:8984/solr/cloud/select?q=foo_s:bar&amp;amp;rows=0&amp;amp;wt=json&amp;amp;distrib=false&quot;&lt;/p&gt;

&lt;p&gt;{&quot;responseHeader&quot;:{&quot;status&quot;:0,&quot;QTime&quot;:1,&quot;params&quot;:{&quot;q&quot;:&quot;foo_s:bar&quot;,&quot;distrib&quot;:&quot;false&quot;,&quot;wt&quot;:&quot;json&quot;,&quot;rows&quot;:&quot;0&quot;}},&quot;response&quot;:{&lt;b&gt;&quot;numFound&quot;:2&lt;/b&gt;,&quot;start&quot;:0,&quot;docs&quot;:[]}}&lt;/p&gt;

&lt;p&gt;curl &quot;http://localhost:8985/solr/cloud/select?q=foo_s:bar&amp;amp;rows=0&amp;amp;wt=json&amp;amp;distrib=false&quot;&lt;/p&gt;

&lt;p&gt;{&quot;responseHeader&quot;:{&quot;status&quot;:0,&quot;QTime&quot;:1,&quot;params&quot;:{&quot;q&quot;:&quot;foo_s:bar&quot;,&quot;distrib&quot;:&quot;false&quot;,&quot;wt&quot;:&quot;json&quot;,&quot;rows&quot;:&quot;0&quot;}},&quot;response&quot;:{&lt;b&gt;&quot;numFound&quot;:1&lt;/b&gt;,&quot;start&quot;:0,&quot;docs&quot;:[]}}&lt;/p&gt;

&lt;p&gt;Observe that the leader has 2 docs and the replica on cloud85 only has 1, but should have 2.&lt;/p&gt;

&lt;p&gt;From what I can tell, the replica that missed some updates because of a temporary network partition doesn&apos;t get any notification that it missed some documents. In other words, the out-of-sync replica doesn&apos;t know it&apos;s out-of-sync and it&apos;s state in ZooKeeper is active. As you can see from the log messages I posted in step 3 above, the leader tried to tell the replica to recovery, but due to the network partition, that request got dropped too.&lt;/p&gt;

&lt;p&gt;I&apos;m wondering if the leader should send a state version tracking ID along with each update request so that a replica can detect that it&apos;s view of state was stale? I could see the process working as follows:&lt;/p&gt;

&lt;p&gt;1. Shard leader now keeps track of a Slice state version tracking identifier that gets sent with every update request&lt;br/&gt;
2. Leader tries to send an update request (including the state version ID) to a replica and send fails&lt;br/&gt;
3. Leader updates the state version to a different value&lt;br/&gt;
4. Leader sends another update request to the replica; request includes the updated version ID; replica accepts the request but realizes its state version ID is out-of-date from what the leader sent&lt;br/&gt;
5. Replica enters recovery&lt;/p&gt;</comment>
                            <comment id="13856009" author="markrmiller@gmail.com" created="Mon, 23 Dec 2013 23:14:47 +0000"  >&lt;p&gt;Yeah, that&apos;s currently expected. We don&apos;t expect the case where you can talk to ZooKeeper but not your replicas to be common, so we kind of punted on this scenario for the first phase.&lt;/p&gt;

&lt;p&gt;Some related JIRA issues:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-5482&quot; title=&quot;We should add an option to the ChaosMonkey&amp;#39;s to do more complicated partition failures on Linux, as well as simulate hard fails of Jetty.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-5482&quot;&gt;SOLR-5482&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-5450&quot; title=&quot;Harden SolrCloud request for recoveries so that if the failure is not a connection exception, we retry a few times.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-5450&quot;&gt;SOLR-5450&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-5495&quot; title=&quot;Recovery strategy for leader partitioned from replica case.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-5495&quot;&gt;&lt;del&gt;SOLR-5495&lt;/del&gt;&lt;/a&gt; 	&lt;/p&gt;

&lt;p&gt;I think we should do all that, but the key is really, in this case, we need to pass the order to recover through ZooKeeper to the partitioned off replica. With an eventually consistent model, it can be off for a short time, but it needs to recover in a timely manner.&lt;/p&gt;

&lt;p&gt;I think this is the right solution because the replica is sure to either get the information to recover from ZooKeeper or lose it&apos;s connection to ZooKeeper in which case it will have to recover anyway.&lt;/p&gt;</comment>
                            <comment id="13856013" author="markrmiller@gmail.com" created="Mon, 23 Dec 2013 23:17:51 +0000"  >&lt;blockquote&gt;&lt;p&gt;so we kind of punted&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The other thing to note is that if you restart the shard or that node or the cluster, you should be able to do it without losing any data. It will recover from the leader when everything else is working correctly.&lt;/p&gt;</comment>
                            <comment id="13856052" author="tim.potter" created="Tue, 24 Dec 2013 00:36:47 +0000"  >&lt;p&gt;Thanks Mark, I suspected my test case was a little cherry picked ... something interesting happened when I also severed the connection between the replica and ZK (ie. same test as above but I also dropped the ZK connection on the replica).&lt;/p&gt;

&lt;p&gt;2013-12-23 15:39:57,170 &lt;span class=&quot;error&quot;&gt;&amp;#91;main-EventThread&amp;#93;&lt;/span&gt; INFO  common.cloud.ConnectionManager  - Watcher org.apache.solr.common.cloud.ConnectionManager@4f857c62 name:ZooKeeperConnection Watcher:ec2-54-197-0-103.compute-1.amazonaws.com:2181 got event WatchedEvent state:Disconnected type:None path:null path:null type:None&lt;br/&gt;
2013-12-23 15:39:57,170 &lt;span class=&quot;error&quot;&gt;&amp;#91;main-EventThread&amp;#93;&lt;/span&gt; INFO  common.cloud.ConnectionManager  - zkClient has disconnected&lt;/p&gt;

&lt;p&gt;&amp;gt;&amp;gt;&amp;gt; fixed the connection between replica and ZK here &amp;lt;&amp;lt;&amp;lt;&lt;/p&gt;

&lt;p&gt;2013-12-23 15:40:45,579 &lt;span class=&quot;error&quot;&gt;&amp;#91;main-EventThread&amp;#93;&lt;/span&gt; INFO  common.cloud.ConnectionManager  - Watcher org.apache.solr.common.cloud.ConnectionManager@4f857c62 name:ZooKeeperConnection Watcher:ec2-54-197-0-103.compute-1.amazonaws.com:2181 got event WatchedEvent state:Expired type:None path:null path:null type:None&lt;br/&gt;
2013-12-23 15:40:45,579 &lt;span class=&quot;error&quot;&gt;&amp;#91;main-EventThread&amp;#93;&lt;/span&gt; INFO  common.cloud.ConnectionManager  - Our previous ZooKeeper session was expired. Attempting to reconnect to recover relationship with ZooKeeper...&lt;br/&gt;
2013-12-23 15:40:45,580 &lt;span class=&quot;error&quot;&gt;&amp;#91;main-EventThread&amp;#93;&lt;/span&gt; INFO  common.cloud.DefaultConnectionStrategy  - Connection expired - starting a new one...&lt;br/&gt;
2013-12-23 15:40:45,586 &lt;span class=&quot;error&quot;&gt;&amp;#91;main-EventThread&amp;#93;&lt;/span&gt; INFO  common.cloud.ConnectionManager  - Waiting for client to connect to ZooKeeper&lt;br/&gt;
2013-12-23 15:40:45,595 &lt;span class=&quot;error&quot;&gt;&amp;#91;main-EventThread&amp;#93;&lt;/span&gt; INFO  common.cloud.ConnectionManager  - Watcher org.apache.solr.common.cloud.ConnectionManager@4f857c62 name:ZooKeeperConnection Watcher:ec2-54-197-0-103.compute-1.amazonaws.com:2181 got event WatchedEvent state:SyncConnected type:None path:null path:null type:None&lt;br/&gt;
2013-12-23 15:40:45,595 &lt;span class=&quot;error&quot;&gt;&amp;#91;main-EventThread&amp;#93;&lt;/span&gt; INFO  common.cloud.ConnectionManager  - Client is connected to ZooKeeper&lt;br/&gt;
2013-12-23 15:40:45,595 &lt;span class=&quot;error&quot;&gt;&amp;#91;main-EventThread&amp;#93;&lt;/span&gt; INFO  common.cloud.ConnectionManager  - Connection with ZooKeeper reestablished.&lt;br/&gt;
2013-12-23 15:40:45,596 &lt;span class=&quot;error&quot;&gt;&amp;#91;main-EventThread&amp;#93;&lt;/span&gt; WARN  solr.cloud.RecoveryStrategy  - Stopping recovery for zkNodeName=core_node3core=cloud_shard1_replica3&lt;br/&gt;
2013-12-23 15:40:45,597 &lt;span class=&quot;error&quot;&gt;&amp;#91;main-EventThread&amp;#93;&lt;/span&gt; INFO  solr.cloud.ZkController  - publishing core=cloud_shard1_replica3 state=down&lt;br/&gt;
2013-12-23 15:40:45,597 &lt;span class=&quot;error&quot;&gt;&amp;#91;main-EventThread&amp;#93;&lt;/span&gt; INFO  solr.cloud.ZkController  - numShards not found on descriptor - reading it from system property&lt;br/&gt;
2013-12-23 15:40:45,905 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp2124890785-14&amp;#93;&lt;/span&gt; INFO  handler.admin.CoreAdminHandler  - It has been requested that we recover&lt;br/&gt;
2013-12-23 15:40:45,906 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp2124890785-14&amp;#93;&lt;/span&gt; INFO  solr.servlet.SolrDispatchFilter  - &lt;span class=&quot;error&quot;&gt;&amp;#91;admin&amp;#93;&lt;/span&gt; webapp=null path=/admin/cores params=&lt;/p&gt;
{action=REQUESTRECOVERY&amp;amp;core=cloud_shard1_replica3&amp;amp;wt=javabin&amp;amp;version=2}
&lt;p&gt; status=0 QTime=2 &lt;br/&gt;
2013-12-23 15:40:45,909 &lt;span class=&quot;error&quot;&gt;&amp;#91;Thread-17&amp;#93;&lt;/span&gt; INFO  solr.cloud.ZkController  - publishing core=cloud_shard1_replica3 state=recovering&lt;br/&gt;
2013-12-23 15:40:45,909 &lt;span class=&quot;error&quot;&gt;&amp;#91;Thread-17&amp;#93;&lt;/span&gt; INFO  solr.cloud.ZkController  - numShards not found on descriptor - reading it from system property&lt;br/&gt;
2013-12-23 15:40:45,920 &lt;span class=&quot;error&quot;&gt;&amp;#91;Thread-17&amp;#93;&lt;/span&gt; INFO  solr.update.DefaultSolrCoreState  - Running recovery - first canceling any ongoing recovery&lt;br/&gt;
2013-12-23 15:40:45,921 &lt;span class=&quot;error&quot;&gt;&amp;#91;RecoveryThread&amp;#93;&lt;/span&gt; INFO  solr.cloud.RecoveryStrategy  - Starting recovery process.  core=cloud_shard1_replica3 recoveringAfterStartup=false&lt;br/&gt;
2013-12-23 15:40:45,924 &lt;span class=&quot;error&quot;&gt;&amp;#91;RecoveryThread&amp;#93;&lt;/span&gt; INFO  solr.cloud.ZkController  - publishing core=cloud_shard1_replica3 state=recovering&lt;br/&gt;
2013-12-23 15:40:45,924 &lt;span class=&quot;error&quot;&gt;&amp;#91;RecoveryThread&amp;#93;&lt;/span&gt; INFO  solr.cloud.ZkController  - numShards not found on descriptor - reading it from system property&lt;br/&gt;
2013-12-23 15:40:48,613 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp2124890785-15&amp;#93;&lt;/span&gt; INFO  solr.core.SolrCore  - &lt;span class=&quot;error&quot;&gt;&amp;#91;cloud_shard1_replica3&amp;#93;&lt;/span&gt; webapp=/solr path=/select params=&lt;/p&gt;
{q=foo_s:bar&amp;amp;distrib=false&amp;amp;wt=json&amp;amp;rows=0}
&lt;p&gt; hits=0 status=0 QTime=1 &lt;br/&gt;
2013-12-23 15:42:42,770 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp2124890785-13&amp;#93;&lt;/span&gt; INFO  solr.core.SolrCore  - &lt;span class=&quot;error&quot;&gt;&amp;#91;cloud_shard1_replica3&amp;#93;&lt;/span&gt; webapp=/solr path=/select params=&lt;/p&gt;
{q=foo_s:bar&amp;amp;distrib=false&amp;amp;wt=json&amp;amp;rows=0}
&lt;p&gt; hits=0 status=0 QTime=1 &lt;br/&gt;
2013-12-23 15:42:45,650 &lt;span class=&quot;error&quot;&gt;&amp;#91;main-EventThread&amp;#93;&lt;/span&gt; ERROR solr.cloud.ZkController  - There was a problem making a request to the leader:org.apache.solr.client.solrj.impl.HttpSolrServer$RemoteSolrException: I was asked to wait on state down for cloud86:8986_solr but I still do not see the requested state. I see state: recovering live:false&lt;br/&gt;
	at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:495)&lt;br/&gt;
	at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:199)&lt;br/&gt;
	at org.apache.solr.cloud.ZkController.waitForLeaderToSeeDownState(ZkController.java:1434)&lt;br/&gt;
	at org.apache.solr.cloud.ZkController.registerAllCoresAsDown(ZkController.java:347)&lt;br/&gt;
	at org.apache.solr.cloud.ZkController.access$100(ZkController.java:85)&lt;br/&gt;
	at org.apache.solr.cloud.ZkController$1.command(ZkController.java:225)&lt;br/&gt;
	at org.apache.solr.common.cloud.ConnectionManager$1.update(ConnectionManager.java:118)&lt;br/&gt;
	at org.apache.solr.common.cloud.DefaultConnectionStrategy.reconnect(DefaultConnectionStrategy.java:56)&lt;br/&gt;
	at org.apache.solr.common.cloud.ConnectionManager.process(ConnectionManager.java:93)&lt;br/&gt;
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:519)&lt;br/&gt;
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:495)&lt;/p&gt;

&lt;p&gt;2013-12-23 15:42:45,963 &lt;span class=&quot;error&quot;&gt;&amp;#91;RecoveryThread&amp;#93;&lt;/span&gt; ERROR solr.cloud.RecoveryStrategy  - Error while trying to recover. core=cloud_shard1_replica3:org.apache.solr.client.solrj.impl.HttpSolrServer$RemoteSolrException: I was asked to wait on state recovering for cloud86:8986_solr but I still do not see the requested state. I see state: recovering live:false&lt;br/&gt;
	at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:495)&lt;br/&gt;
	at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:199)&lt;br/&gt;
	at org.apache.solr.cloud.RecoveryStrategy.sendPrepRecoveryCmd(RecoveryStrategy.java:224)&lt;br/&gt;
	at org.apache.solr.cloud.RecoveryStrategy.doRecovery(RecoveryStrategy.java:371)&lt;br/&gt;
	at org.apache.solr.cloud.RecoveryStrategy.run(RecoveryStrategy.java:247)&lt;/p&gt;

&lt;p&gt;2013-12-23 15:42:45,964 &lt;span class=&quot;error&quot;&gt;&amp;#91;RecoveryThread&amp;#93;&lt;/span&gt; ERROR solr.cloud.RecoveryStrategy  - Recovery failed - trying again... (0) core=cloud_shard1_replica3&lt;br/&gt;
2013-12-23 15:42:45,964 &lt;span class=&quot;error&quot;&gt;&amp;#91;RecoveryThread&amp;#93;&lt;/span&gt; INFO  solr.cloud.RecoveryStrategy  - Wait 2.0 seconds before trying to recover again (1)&lt;br/&gt;
2013-12-23 15:42:47,964 &lt;span class=&quot;error&quot;&gt;&amp;#91;RecoveryThread&amp;#93;&lt;/span&gt; INFO  solr.cloud.ZkController  - publishing core=cloud_shard1_replica3 state=recovering&lt;/p&gt;</comment>
                            <comment id="13856078" author="markrmiller@gmail.com" created="Tue, 24 Dec 2013 01:57:47 +0000"  >&lt;p&gt;That&apos;s interesting. The logging makes it look like it&apos;s not creating it&apos;s new ephemeral live node for some reason...or the leader is not getting an updated view of the live node...&lt;/p&gt;</comment>
                            <comment id="13864141" author="markus17" created="Tue, 7 Jan 2014 12:00:43 +0000"  >&lt;p&gt;Ok, I followed all the great work here and in related tickets and yesterday i had the time to rebuild Solr and check for this issue. I hadn&apos;t seen it yesterday but it is right in front of me again, using a fresh build from January 6th.&lt;/p&gt;

&lt;p&gt;Leader has Num Docs: 379659&lt;br/&gt;
Replica has Num Docs: 379661&lt;/p&gt;</comment>
                            <comment id="13865724" author="tim.potter" created="Wed, 8 Jan 2014 18:32:25 +0000"  >&lt;p&gt;While doing some other testing of SolrCloud (branch4x - 4.7-SNAPSHOT rev. 1556055), I hit this issue and here&apos;s the kicker ... there were no errors in my replica&apos;s log, the tlogs are identical, and there was no significant GC activity during the time where the replica got out of sync with the leader. I&apos;m attaching the data directories (index + tlog) for both replicas (demo_shard1_replica1 &lt;span class=&quot;error&quot;&gt;&amp;#91;leader&amp;#93;&lt;/span&gt;, and demo_shard1_replica2) and their log files. When I do a doc-by-doc comparison of the two indexes, here&apos;s the result:&lt;/p&gt;

&lt;p&gt;&amp;gt;&amp;gt; finished querying replica1, found 33537 documents (33537)&lt;br/&gt;
&amp;gt;&amp;gt; finished querying replica2, found 33528 documents&lt;br/&gt;
Doc &lt;span class=&quot;error&quot;&gt;&amp;#91;82995&amp;#93;&lt;/span&gt; not found in replica2: &amp;lt;doc boost=&quot;1.0&quot;&amp;gt;&amp;lt;field name=&quot;id&quot;&amp;gt;82995&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;string_s&quot;&amp;gt;test&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;int_i&quot;&amp;gt;-274468088&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;float_f&quot;&amp;gt;0.90338105&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;double_d&quot;&amp;gt;0.6949391474539932&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;text_en&quot;&amp;gt;this is a test&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;&lt;em&gt;version&lt;/em&gt;&quot;&amp;gt;1456683668206518274&amp;lt;/field&amp;gt;&amp;lt;/doc&amp;gt;&lt;br/&gt;
Doc &lt;span class=&quot;error&quot;&gt;&amp;#91;82997&amp;#93;&lt;/span&gt; not found in replica2: &amp;lt;doc boost=&quot;1.0&quot;&amp;gt;&amp;lt;field name=&quot;id&quot;&amp;gt;82997&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;string_s&quot;&amp;gt;test&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;int_i&quot;&amp;gt;301737117&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;float_f&quot;&amp;gt;0.6746266&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;double_d&quot;&amp;gt;0.26034065188918565&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;text_en&quot;&amp;gt;this is a test&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;&lt;em&gt;version&lt;/em&gt;&quot;&amp;gt;1456683668206518276&amp;lt;/field&amp;gt;&amp;lt;/doc&amp;gt;&lt;br/&gt;
Doc &lt;span class=&quot;error&quot;&gt;&amp;#91;82996&amp;#93;&lt;/span&gt; not found in replica2: &amp;lt;doc boost=&quot;1.0&quot;&amp;gt;&amp;lt;field name=&quot;id&quot;&amp;gt;82996&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;string_s&quot;&amp;gt;test&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;int_i&quot;&amp;gt;-1768315588&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;float_f&quot;&amp;gt;0.6641093&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;double_d&quot;&amp;gt;0.23708033183534993&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;text_en&quot;&amp;gt;this is a test&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;&lt;em&gt;version&lt;/em&gt;&quot;&amp;gt;1456683668206518275&amp;lt;/field&amp;gt;&amp;lt;/doc&amp;gt;&lt;br/&gt;
Doc &lt;span class=&quot;error&quot;&gt;&amp;#91;82991&amp;#93;&lt;/span&gt; not found in replica2: &amp;lt;doc boost=&quot;1.0&quot;&amp;gt;&amp;lt;field name=&quot;id&quot;&amp;gt;82991&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;string_s&quot;&amp;gt;test&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;int_i&quot;&amp;gt;-2057280061&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;float_f&quot;&amp;gt;0.27617514&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;double_d&quot;&amp;gt;0.7885214691953506&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;text_en&quot;&amp;gt;this is a test&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;&lt;em&gt;version&lt;/em&gt;&quot;&amp;gt;1456683668206518273&amp;lt;/field&amp;gt;&amp;lt;/doc&amp;gt;&lt;br/&gt;
Doc &lt;span class=&quot;error&quot;&gt;&amp;#91;82987&amp;#93;&lt;/span&gt; not found in replica2: &amp;lt;doc boost=&quot;1.0&quot;&amp;gt;&amp;lt;field name=&quot;id&quot;&amp;gt;82987&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;string_s&quot;&amp;gt;test&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;int_i&quot;&amp;gt;1051456320&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;float_f&quot;&amp;gt;0.51863414&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;double_d&quot;&amp;gt;0.7881255443862878&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;text_en&quot;&amp;gt;this is a test&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;&lt;em&gt;version&lt;/em&gt;&quot;&amp;gt;1456683668206518272&amp;lt;/field&amp;gt;&amp;lt;/doc&amp;gt;&lt;br/&gt;
Doc &lt;span class=&quot;error&quot;&gt;&amp;#91;82986&amp;#93;&lt;/span&gt; not found in replica2: &amp;lt;doc boost=&quot;1.0&quot;&amp;gt;&amp;lt;field name=&quot;id&quot;&amp;gt;82986&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;string_s&quot;&amp;gt;test&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;int_i&quot;&amp;gt;-1356807889&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;float_f&quot;&amp;gt;0.2762279&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;double_d&quot;&amp;gt;0.003657816979820372&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;text_en&quot;&amp;gt;this is a test&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;&lt;em&gt;version&lt;/em&gt;&quot;&amp;gt;1456683668205469699&amp;lt;/field&amp;gt;&amp;lt;/doc&amp;gt;&lt;br/&gt;
Doc &lt;span class=&quot;error&quot;&gt;&amp;#91;82984&amp;#93;&lt;/span&gt; not found in replica2: &amp;lt;doc boost=&quot;1.0&quot;&amp;gt;&amp;lt;field name=&quot;id&quot;&amp;gt;82984&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;string_s&quot;&amp;gt;test&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;int_i&quot;&amp;gt;732678870&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;float_f&quot;&amp;gt;0.31199205&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;double_d&quot;&amp;gt;0.9848865821766198&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;text_en&quot;&amp;gt;this is a test&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;&lt;em&gt;version&lt;/em&gt;&quot;&amp;gt;1456683668205469698&amp;lt;/field&amp;gt;&amp;lt;/doc&amp;gt;&lt;br/&gt;
Doc &lt;span class=&quot;error&quot;&gt;&amp;#91;82970&amp;#93;&lt;/span&gt; not found in replica2: &amp;lt;doc boost=&quot;1.0&quot;&amp;gt;&amp;lt;field name=&quot;id&quot;&amp;gt;82970&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;string_s&quot;&amp;gt;test&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;int_i&quot;&amp;gt;283693979&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;float_f&quot;&amp;gt;0.6119651&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;double_d&quot;&amp;gt;0.04142006867388914&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;text_en&quot;&amp;gt;this is a test&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;&lt;em&gt;version&lt;/em&gt;&quot;&amp;gt;1456683668205469696&amp;lt;/field&amp;gt;&amp;lt;/doc&amp;gt;&lt;br/&gt;
Doc &lt;span class=&quot;error&quot;&gt;&amp;#91;82973&amp;#93;&lt;/span&gt; not found in replica2: &amp;lt;doc boost=&quot;1.0&quot;&amp;gt;&amp;lt;field name=&quot;id&quot;&amp;gt;82973&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;string_s&quot;&amp;gt;test&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;int_i&quot;&amp;gt;1343103920&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;float_f&quot;&amp;gt;0.5855809&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;double_d&quot;&amp;gt;0.6575904716584224&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;text_en&quot;&amp;gt;this is a test&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;&lt;em&gt;version&lt;/em&gt;&quot;&amp;gt;1456683668205469697&amp;lt;/field&amp;gt;&amp;lt;/doc&amp;gt;&lt;/p&gt;

&lt;p&gt;No amount of committing or reloading of these cores helps. Also, restarting replica2 doesn&apos;t lead to it being in-sync either, most likely because the tlog is identical to the leader? Here&apos;s the log messages on replica2 after restarting it.&lt;/p&gt;

&lt;p&gt;2014-01-08 13:28:20,112 &lt;span class=&quot;error&quot;&gt;&amp;#91;searcherExecutor-5-thread-1&amp;#93;&lt;/span&gt; INFO  solr.core.SolrCore  - &lt;span class=&quot;error&quot;&gt;&amp;#91;demo_shard1_replica2&amp;#93;&lt;/span&gt; Registered new searcher Searcher@4345de8a main&lt;/p&gt;
{StandardDirectoryReader(segments_e:38:nrt _d(4.7):C26791 _e(4.7):C3356 _f(4.7):C3381)}
&lt;p&gt;2014-01-08 13:28:21,298 &lt;span class=&quot;error&quot;&gt;&amp;#91;RecoveryThread&amp;#93;&lt;/span&gt; INFO  solr.cloud.RecoveryStrategy  - Attempting to PeerSync from &lt;a href=&quot;http://ec2-54-209-223-12.compute-1.amazonaws.com:8984/solr/demo_shard1_replica1/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://ec2-54-209-223-12.compute-1.amazonaws.com:8984/solr/demo_shard1_replica1/&lt;/a&gt; core=demo_shard1_replica2 - recoveringAfterStartup=true&lt;br/&gt;
2014-01-08 13:28:21,302 &lt;span class=&quot;error&quot;&gt;&amp;#91;RecoveryThread&amp;#93;&lt;/span&gt; INFO  solr.update.PeerSync  - PeerSync: core=demo_shard1_replica2 url=&lt;a href=&quot;http://ec2-54-209-97-145.compute-1.amazonaws.com:8984/solr&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://ec2-54-209-97-145.compute-1.amazonaws.com:8984/solr&lt;/a&gt; START replicas=&lt;a href=&quot;http://ec2-54-209-223-12.compute-1.amazonaws.com:8984/solr/demo_shard1_replica1/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://ec2-54-209-223-12.compute-1.amazonaws.com:8984/solr/demo_shard1_replica1/&lt;/a&gt; nUpdates=100&lt;br/&gt;
2014-01-08 13:28:21,330 &lt;span class=&quot;error&quot;&gt;&amp;#91;RecoveryThread&amp;#93;&lt;/span&gt; INFO  solr.update.PeerSync  - PeerSync: core=demo_shard1_replica2 url=&lt;a href=&quot;http://ec2-54-209-97-145.compute-1.amazonaws.com:8984/solr&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://ec2-54-209-97-145.compute-1.amazonaws.com:8984/solr&lt;/a&gt;  Received 99 versions from ec2-54-209-223-12.compute-1.amazonaws.com:8984/solr/demo_shard1_replica1/&lt;br/&gt;
2014-01-08 13:28:21,331 &lt;span class=&quot;error&quot;&gt;&amp;#91;RecoveryThread&amp;#93;&lt;/span&gt; INFO  solr.update.PeerSync  - PeerSync: core=demo_shard1_replica2 url=&lt;a href=&quot;http://ec2-54-209-97-145.compute-1.amazonaws.com:8984/solr&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://ec2-54-209-97-145.compute-1.amazonaws.com:8984/solr&lt;/a&gt;  Our versions are newer. ourLowThreshold=1456683689417113603 otherHigh=1456683689602711553&lt;br/&gt;
2014-01-08 13:28:21,331 &lt;span class=&quot;error&quot;&gt;&amp;#91;RecoveryThread&amp;#93;&lt;/span&gt; INFO  solr.update.PeerSync  - PeerSync: core=demo_shard1_replica2 url=&lt;a href=&quot;http://ec2-54-209-97-145.compute-1.amazonaws.com:8984/solr&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://ec2-54-209-97-145.compute-1.amazonaws.com:8984/solr&lt;/a&gt; DONE. sync succeeded&lt;br/&gt;
2014-01-08 13:28:21,332 &lt;span class=&quot;error&quot;&gt;&amp;#91;RecoveryThread&amp;#93;&lt;/span&gt; INFO  solr.update.UpdateHandler  - start commit&lt;/p&gt;
{,optimize=false,openSearcher=true,waitSearcher=true,expungeDeletes=false,softCommit=false,prepareCommit=false}
&lt;p&gt;2014-01-08 13:28:21,332 &lt;span class=&quot;error&quot;&gt;&amp;#91;RecoveryThread&amp;#93;&lt;/span&gt; INFO  solr.update.LoggingInfoStream  - &lt;span class=&quot;error&quot;&gt;&amp;#91;DW&amp;#93;&lt;/span&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;RecoveryThread&amp;#93;&lt;/span&gt;: anyChanges? numDocsInRam=0 deletes=false hasTickets:false pendingChangesInFullFlush: false&lt;br/&gt;
2014-01-08 13:28:21,333 &lt;span class=&quot;error&quot;&gt;&amp;#91;RecoveryThread&amp;#93;&lt;/span&gt; INFO  solr.update.UpdateHandler  - No uncommitted changes. Skipping IW.commit.&lt;br/&gt;
2014-01-08 13:28:21,334 &lt;span class=&quot;error&quot;&gt;&amp;#91;RecoveryThread&amp;#93;&lt;/span&gt; INFO  solr.update.LoggingInfoStream  - &lt;span class=&quot;error&quot;&gt;&amp;#91;DW&amp;#93;&lt;/span&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;RecoveryThread&amp;#93;&lt;/span&gt;: anyChanges? numDocsInRam=0 deletes=false hasTickets:false pendingChangesInFullFlush: false&lt;br/&gt;
2014-01-08 13:28:21,334 &lt;span class=&quot;error&quot;&gt;&amp;#91;RecoveryThread&amp;#93;&lt;/span&gt; INFO  solr.update.LoggingInfoStream  - &lt;span class=&quot;error&quot;&gt;&amp;#91;IW&amp;#93;&lt;/span&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;RecoveryThread&amp;#93;&lt;/span&gt;: nrtIsCurrent: infoVersion matches: true; DW changes: false; BD changes: false&lt;br/&gt;
2014-01-08 13:28:21,335 &lt;span class=&quot;error&quot;&gt;&amp;#91;RecoveryThread&amp;#93;&lt;/span&gt; INFO  solr.update.LoggingInfoStream  - &lt;span class=&quot;error&quot;&gt;&amp;#91;DW&amp;#93;&lt;/span&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;RecoveryThread&amp;#93;&lt;/span&gt;: anyChanges? numDocsInRam=0 deletes=false hasTickets:false pendingChangesInFullFlush: false&lt;br/&gt;
2014-01-08 13:28:21,335 &lt;span class=&quot;error&quot;&gt;&amp;#91;RecoveryThread&amp;#93;&lt;/span&gt; INFO  solr.search.SolrIndexSearcher  - Opening Searcher@5fc2a9d main&lt;br/&gt;
2014-01-08 13:28:21,338 &lt;span class=&quot;error&quot;&gt;&amp;#91;searcherExecutor-5-thread-1&amp;#93;&lt;/span&gt; INFO  solr.core.SolrCore  - QuerySenderListener sending requests to Searcher@5fc2a9d main&lt;/p&gt;
{StandardDirectoryReader(segments_e:38:nrt _d(4.7):C26791 _e(4.7):C3356 _f(4.7):C3381)}
&lt;p&gt;2014-01-08 13:28:21,338 &lt;span class=&quot;error&quot;&gt;&amp;#91;searcherExecutor-5-thread-1&amp;#93;&lt;/span&gt; INFO  solr.core.SolrCore  - QuerySenderListener done.&lt;br/&gt;
2014-01-08 13:28:21,338 &lt;span class=&quot;error&quot;&gt;&amp;#91;searcherExecutor-5-thread-1&amp;#93;&lt;/span&gt; INFO  solr.core.SolrCore  - &lt;span class=&quot;error&quot;&gt;&amp;#91;demo_shard1_replica2&amp;#93;&lt;/span&gt; Registered new searcher Searcher@5fc2a9d main&lt;/p&gt;
{StandardDirectoryReader(segments_e:38:nrt _d(4.7):C26791 _e(4.7):C3356 _f(4.7):C3381)}
&lt;p&gt;2014-01-08 13:28:21,339 &lt;span class=&quot;error&quot;&gt;&amp;#91;RecoveryThread&amp;#93;&lt;/span&gt; INFO  solr.update.UpdateHandler  - end_commit_flush&lt;br/&gt;
2014-01-08 13:28:21,339 &lt;span class=&quot;error&quot;&gt;&amp;#91;RecoveryThread&amp;#93;&lt;/span&gt; INFO  solr.cloud.RecoveryStrategy  - PeerSync Recovery was successful - registering as Active. core=demo_shard1_replica2&lt;br/&gt;
2014-01-08 13:28:21,339 &lt;span class=&quot;error&quot;&gt;&amp;#91;RecoveryThread&amp;#93;&lt;/span&gt; INFO  solr.cloud.ZkController  - publishing core=demo_shard1_replica2 state=active&lt;br/&gt;
2014-01-08 13:28:21,370 &lt;span class=&quot;error&quot;&gt;&amp;#91;main-EventThread&amp;#93;&lt;/span&gt; INFO  common.cloud.ZkStateReader  - A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/clusterstate.json, has occurred - updating... (live nodes size: 6)&lt;/p&gt;


&lt;p&gt;Thus, it would seem there might be some code that&apos;s outright losing documents (almost feels like a last batch not flushed error but more subtle as it&apos;s not easy to reproduce this all the time)&lt;/p&gt;
</comment>
                            <comment id="13865742" author="markrmiller@gmail.com" created="Wed, 8 Jan 2014 18:50:40 +0000"  >&lt;p&gt;I&apos;ve noticed something like this too - but nothing i could reproduce easily. I imagine it&apos;s likely an issue in SolrCmdDistributor.&lt;/p&gt;</comment>
                            <comment id="13865744" author="markrmiller@gmail.com" created="Wed, 8 Jan 2014 18:51:31 +0000"  >&lt;p&gt;Although that doesn&apos;t really jive with the tran logs being identical...hmm...&lt;/p&gt;</comment>
                            <comment id="13865769" author="markrmiller@gmail.com" created="Wed, 8 Jan 2014 19:19:04 +0000"  >&lt;p&gt;No, wait, it could jive. We only check the last 99 docs on peer sync - if bunch of docs just didn&apos;t show up well before that, it wouldn&apos;t be detected by peer sync. I still think SolrCmdDistributor is the first place to look.&lt;/p&gt;</comment>
                            <comment id="13865963" author="tim.potter" created="Wed, 8 Jan 2014 22:06:55 +0000"  >&lt;p&gt;Still digging into it ... I&apos;m curious why a batch of 34 adds on the leader gets processed as several sub-batches on the replica? Here&apos;s what I&apos;m seeing the logs around the documents that are missing from the replica. Basically, there are 34 docs on the leader and only 25 processed in 4 separate batches (from my counting of the logs) on the replica. Why wouldn&apos;t it just be one for one? The docs are all roughly the same size ... and what&apos;s breaking it up? Having trouble seeing that in the logs / code &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;On the leader:&lt;/p&gt;

&lt;p&gt;2014-01-08 12:23:21,501 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp604104855-17&amp;#93;&lt;/span&gt; INFO  update.processor.LogUpdateProcessor  - &lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;demo_shard1_replica1&amp;#93;&lt;/span&gt; webapp=/solr path=/update params=&lt;/p&gt;
{wt=javabin&amp;amp;version=2} {add=[82900 (1456683668174012416), 82901 (1456683668181352448), 82903 (1456683668181352449), 82904 (1456683668181352450), 82912 (1456683668187643904), 82913 (1456683668188692480), 82914 (1456683668188692481), 82916 (1456683668188692482), 82917 (1456683668188692483), 82918 (1456683668188692484), ... (34 adds)]} 0 34&lt;br/&gt;
&lt;br/&gt;
&amp;gt;&amp;gt;&amp;gt;&amp;gt; NOT ALL OF THE 34 DOCS MENTIONED ABOVE MAKE IT TO THE REPLICA &amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&lt;br/&gt;
&lt;br/&gt;
2014-01-08 12:23:21,600 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp604104855-17&amp;#93;&lt;/span&gt; INFO  update.processor.LogUpdateProcessor  - &lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;demo_shard1_replica1&amp;#93;&lt;/span&gt; webapp=/solr path=/update params={wt=javabin&amp;amp;version=2}
&lt;p&gt; &lt;/p&gt;
{add=[83002 (1456683668280967168), 83005 (1456683668286210048), 83008 (1456683668286210049), 83011 (1456683668286210050), 83012 (1456683668286210051), 83013 (1456683668287258624), 83018 (1456683668287258625), 83019 (1456683668289355776), 83023 (1456683668289355777), 83024 (1456683668289355778), ... (43 adds)]}
&lt;p&gt; 0 32&lt;/p&gt;


&lt;p&gt;On the replica:&lt;/p&gt;

&lt;p&gt;2014-01-08 12:23:21,126 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp604104855-22&amp;#93;&lt;/span&gt; INFO  update.processor.LogUpdateProcessor  - &lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;demo_shard1_replica2&amp;#93;&lt;/span&gt; webapp=/solr path=/update params=&lt;/p&gt;
{distrib.from=http://ec2-54-209-223-12.compute-1.amazonaws.com:8984/solr/demo_shard1_replica1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;p&gt; &lt;/p&gt;
{add=[82900 (1456683668174012416), 82901 (1456683668181352448), 82903 (1456683668181352449)]}
&lt;p&gt; 0 1&lt;/p&gt;

&lt;p&gt;2014-01-08 12:23:21,134 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp604104855-22&amp;#93;&lt;/span&gt; INFO  update.processor.LogUpdateProcessor  - &lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;demo_shard1_replica2&amp;#93;&lt;/span&gt; webapp=/solr path=/update params=&lt;/p&gt;
{distrib.from=http://ec2-54-209-223-12.compute-1.amazonaws.com:8984/solr/demo_shard1_replica1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;p&gt; &lt;/p&gt;
{add=[82904 (1456683668181352450), 82912 (1456683668187643904), 82913 (1456683668188692480), 82914 (1456683668188692481), 82916 (1456683668188692482), 82917 (1456683668188692483), 82918 (1456683668188692484), 82919 (1456683668188692485), 82922 (1456683668188692486)]}
&lt;p&gt; 0 2&lt;/p&gt;

&lt;p&gt;2014-01-08 12:23:21,139 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp604104855-22&amp;#93;&lt;/span&gt; INFO  update.processor.LogUpdateProcessor  - &lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;demo_shard1_replica2&amp;#93;&lt;/span&gt; webapp=/solr path=/update params=&lt;/p&gt;
{distrib.from=http://ec2-54-209-223-12.compute-1.amazonaws.com:8984/solr/demo_shard1_replica1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;p&gt; &lt;/p&gt;
{add=[82923 (1456683668188692487), 82926 (1456683668190789632), 82928 (1456683668190789633), 82932 (1456683668190789634), 82939 (1456683668192886784), 82945 (1456683668192886785), 82946 (1456683668192886786), 82947 (1456683668193935360), 82952 (1456683668193935361), 82962 (1456683668193935362), ... (12 adds)]}
&lt;p&gt; 0 3&lt;/p&gt;

&lt;p&gt;2014-01-08 12:23:21,144 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp604104855-22&amp;#93;&lt;/span&gt; INFO  update.processor.LogUpdateProcessor  - &lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;demo_shard1_replica2&amp;#93;&lt;/span&gt; webapp=/solr path=/update params=&lt;/p&gt;
{distrib.from=http://ec2-54-209-223-12.compute-1.amazonaws.com:8984/solr/demo_shard1_replica1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;p&gt; &lt;/p&gt;
{add=[82967 (1456683668199178240)]}
&lt;p&gt; 0 0&lt;/p&gt;


&lt;p&gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; 9 Docs Missing here &amp;lt;&amp;lt;&amp;lt;&amp;lt;&lt;/p&gt;

&lt;p&gt;2014-01-08 12:23:21,227 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp604104855-22&amp;#93;&lt;/span&gt; INFO  update.processor.LogUpdateProcessor  - &lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;demo_shard1_replica2&amp;#93;&lt;/span&gt; webapp=/solr path=/update params=&lt;/p&gt;
{distrib.from=http://ec2-54-209-223-12.compute-1.amazonaws.com:8984/solr/demo_shard1_replica1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;p&gt; &lt;/p&gt;
{add=[83002 (1456683668280967168), 83005 (1456683668286210048), 83008 (1456683668286210049), 83011 (1456683668286210050), 83012 (1456683668286210051), 83013 (1456683668287258624)]}
&lt;p&gt; 0 2&lt;/p&gt;


&lt;p&gt;Note the add log message starting with doc ID 83002 is just included here for context to show where the leader / replica got out of sync.&lt;/p&gt;</comment>
                            <comment id="13865984" author="yseeley@gmail.com" created="Wed, 8 Jan 2014 22:29:04 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Basically, there are 34 docs on the leader and only 25 processed in 4 separate batches (from my counting of the logs) on the replica. Why wouldn&apos;t it just be one for one? The docs are all roughly the same size ... and what&apos;s breaking it up? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;ConcurrentUpdateSolrServer?  If another doc doesn&apos;t come in quickly enough (250ms by default), it ends the batch.&lt;br/&gt;
I thought there used to be a doc count limit too or something... but after a quick scan, I&apos;m not seeing it.&lt;/p&gt;</comment>
                            <comment id="13866031" author="tim.potter" created="Wed, 8 Jan 2014 23:15:45 +0000"  >&lt;p&gt;Cuss on CUSS &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; Thanks, I sometimes forget that the client-side batch gets broken into individual AddUpdateCommands when sending on to the replicas.&lt;/p&gt;</comment>
                            <comment id="13869134" author="markrmiller@gmail.com" created="Sun, 12 Jan 2014 20:27:20 +0000"  >&lt;p&gt;In this case there is no wait due to the massive penalty it puts on doc per request speed.&lt;/p&gt;</comment>
                            <comment id="13869137" author="markrmiller@gmail.com" created="Sun, 12 Jan 2014 20:38:16 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-5625&quot; title=&quot;Add to testing for SolrCmdDistributor&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-5625&quot;&gt;&lt;del&gt;SOLR-5625&lt;/del&gt;&lt;/a&gt;: Add to testing for SolrCmdDistributor&lt;/p&gt;</comment>
                            <comment id="13869916" author="tim.potter" created="Mon, 13 Jan 2014 20:12:05 +0000"  >&lt;p&gt;Make sense about not waiting because of the penalty now that I&apos;ve had a chance to get into the details of that code.&lt;/p&gt;

&lt;p&gt;I spent a lot of time on Friday and over the weekend trying to track down the docs getting dropped. Unfortunately have not been able to track down the source of the issue yet. I&apos;m fairly certain the issue happens before docs get submitted to CUSS, meaning that the lost docs never seemed to hit the queue in ConcurrentUpdateSolrServer. My original thinking was that given the complex nature of CUSS, there might be some sort of race condition but after having added a log of what hit the queue, it seems that the documents that get lost never hit the queue. Not to mention that the actual use of CUSS is mostly single-threaded because StreamingSolrServers construct them with a threadCount of 1.&lt;/p&gt;

&lt;p&gt;As a side note, one thing I noticed while is that direct updates don&apos;t necessarily hit the correct core initially when a Solr node hosts more than one shard per collection. In other words, if host X had shard1 and shard3 of collection foo, then some update requests would hit shard1 on host X when they should go to shard3 on the same host; shard1 correctly forwards them on but it&apos;s still an extra hop. Of course that is probably not a big deal in production as it would be rare to host multiple shards of the same collection in the same Solr host, unless they are over-sharding.&lt;/p&gt;

&lt;p&gt;In terms of this issue, here&apos;s what I&apos;m seeing:&lt;/p&gt;

&lt;p&gt;Assume a SolrCloud environment with shard1 having replicas on host A and B; A is the current leader&lt;br/&gt;
client sends direct update request to shard1 on host A containing 3 docs (1,2,3) (for example)&lt;br/&gt;
batch from client gets broken up into individual docs (during request parsing)&lt;br/&gt;
docs 1,2,3 get indexed on host A (the leader)&lt;br/&gt;
docs 1 and 2 get queued into CUSS and sent on to the replica on host B (sometimes in the same request, sometimes in separate requests)&lt;br/&gt;
doc 3 never makes it and from what I can tell, never hits the queue&lt;/p&gt;

&lt;p&gt;This may be anecdotal but from what I can tell, it&apos;s always docs on the end of a batch and not in the middle. Meaning that I haven&apos;t seen a case where 1 and 3 make it and 2 not ... maybe useful, maybe not. The only other thing I&apos;ll mention is it does seem timing / race condition related as it&apos;s almost impossible to reproduce this on my Mac when running 2 shards across 2 nodes but much easier to trigger if I ramp up to say 8 shards on 2 nodes, i.e. the busier my CPU is, the easier it is to see docs getting dropped.&lt;/p&gt;
</comment>
                            <comment id="13870068" author="markrmiller@gmail.com" created="Mon, 13 Jan 2014 22:26:14 +0000"  >&lt;p&gt;How many threads are you using to load docs? How large are the batches?&lt;/p&gt;</comment>
                            <comment id="13870197" author="tim.potter" created="Tue, 14 Jan 2014 00:49:16 +0000"  >&lt;p&gt;Oddly enough, just 1 indexing thread on the client side and batches of around 30-40 docs per shard (ie I set my batch size so that direct updates send about 30-40 per shard to the leaders from the client side).&lt;/p&gt;</comment>
                            <comment id="13870858" author="markrmiller@gmail.com" created="Tue, 14 Jan 2014 16:22:18 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markus17&quot; class=&quot;user-hover&quot; rel=&quot;markus17&quot;&gt;markus17&lt;/a&gt;, are you loading docs via the bulk methods or cuss or what?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tim.potter&quot; class=&quot;user-hover&quot; rel=&quot;tim.potter&quot;&gt;tim.potter&lt;/a&gt;, I think I&apos;m seeing your issue. Have not gotten to the bottom of it yet, but if I am seeing the same thing, it seems those docs are being setup to send to 0 replicas. Trying to figure out why/how.&lt;/p&gt;</comment>
                            <comment id="13870859" author="markrmiller@gmail.com" created="Tue, 14 Jan 2014 16:23:08 +0000"  >&lt;p&gt;FYI, I also had to overshard to see anything.&lt;/p&gt;</comment>
                            <comment id="13870865" author="markus17" created="Tue, 14 Jan 2014 16:28:09 +0000"  >&lt;p&gt;Mark - We use CloudSolrServer and send batches of around 380 documents from Nutch. I am not sure what actual implementation we get back when connecting.&lt;/p&gt;</comment>
                            <comment id="13870868" author="markus17" created="Tue, 14 Jan 2014 16:33:02 +0000"  >&lt;p&gt;I also think i&apos;m seeing this happening right now with a trunk build of yesterday. I am slowly indexing few hundred docs every few minutes for quite some time for fixing a Nutch issue. Looks like i can restart it because replica&apos;s are already out of sync &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13870984" author="markrmiller@gmail.com" created="Tue, 14 Jan 2014 18:11:43 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markus17&quot; class=&quot;user-hover&quot; rel=&quot;markus17&quot;&gt;markus17&lt;/a&gt;, are you indexing to an oversharded cluster?&lt;/p&gt;</comment>
                            <comment id="13871878" author="markus17" created="Wed, 15 Jan 2014 09:59:46 +0000"  >&lt;p&gt;Mark, no, each node holds a single JVM and single core. We have 5 leaders and five replica&apos;s, so ten nodes.&lt;/p&gt;</comment>
                            <comment id="13872666" author="markrmiller@gmail.com" created="Wed, 15 Jan 2014 22:07:20 +0000"  >&lt;p&gt;Well the affects I was seeing related to having a control collection with a core named collection1 and another collection called collection1. Over shard, and that causes some similar looking effects.&lt;/p&gt;

&lt;p&gt;I&apos;ve addressed that and will see if ramping up my tests can spot anything - so far cannot replicate in a test though.&lt;/p&gt;</comment>
                            <comment id="13873303" author="markus17" created="Thu, 16 Jan 2014 11:56:06 +0000"  >&lt;p&gt;Did something crucial change recently? Since at least the 13th, maybe earlier, indexing small segments from Nutch in several cycles (few hundred per cycle), some shards get out of sync really quick! I did lots of tests before that but didn&apos;t see it happening before.&lt;/p&gt;

&lt;p&gt;Ok, someting did change. I reverted back to a build of the 6th and everything is fine. I can run the index process from Nutch for many cycles and more data. No shard is going out of sync with itself.&lt;/p&gt;</comment>
                            <comment id="13873334" author="markus17" created="Thu, 16 Jan 2014 12:59:55 +0000"  >&lt;p&gt;Correction: it happens on a build of the 6th as well, although it doens&apos;t look that bad as when index to a 13th build.&lt;/p&gt;</comment>
                            <comment id="13873457" author="markus17" created="Thu, 16 Jan 2014 14:56:03 +0000"  >&lt;p&gt;Seems autocommit has something to do with triggering the problem, at least in my case.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;13th build without autocommit: out of sync very soon&lt;/li&gt;
	&lt;li&gt;13th build with autocommit: out of sync after a while&lt;/li&gt;
	&lt;li&gt;6th build without autocommit: out of sync after a while&lt;/li&gt;
	&lt;li&gt;6th build with autocommit: out of sync after many more documents&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13873638" author="joel.bernstein" created="Thu, 16 Jan 2014 17:29:33 +0000"  >&lt;p&gt;The commit behavior is interesting. I&apos;m seeing docs flushing from the leader to replica following a manual hard commit issued long after indexing has stopped. That means somewhere along the way docs are buffered and waiting for an event to flush them to the replica. I haven&apos;t figured out just yet where the buffering is occurring but I&apos;m trying to track it down. &lt;/p&gt;</comment>
                            <comment id="13873657" author="markrmiller@gmail.com" created="Thu, 16 Jan 2014 17:48:09 +0000"  >&lt;p&gt;I spent some time a while back trying to find a fault in ConcurrentSolrServer#blockUntilFinished - didn&apos;t uncover anything yet though.&lt;/p&gt;</comment>
                            <comment id="13873788" author="joel.bernstein" created="Thu, 16 Jan 2014 19:10:35 +0000"  >&lt;p&gt;I&apos;m betting it&apos;s something in the streaming. This afternoon I&apos;m going to put some debugging in to see if the docs being flushed by the commit were already written to the stream. My bet is that they were, and that the commit is pushing them all the way through to the replica. &lt;/p&gt;</comment>
                            <comment id="13873841" author="tim.potter" created="Thu, 16 Jan 2014 19:53:10 +0000"  >&lt;p&gt;I was able to reproduce this issue on EC2 without any over-sharding (on latest rev on branch_4x) ... basically 6 Solr nodes with 3 shards and RF=2, i.e. each replica gets its own Solr instance. Here&apos;s the output from my client app that traps the inconsistency:&lt;/p&gt;

&lt;p&gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&lt;br/&gt;
Found 1 shards with mis-matched doc counts.&lt;br/&gt;
At January 16, 2014 12:18:08 PM MST&lt;br/&gt;
shard2: {&lt;br/&gt;
	&lt;a href=&quot;http://ec2-54-236-245-61.compute-1.amazonaws.com:8985/solr/test_shard2_replica2/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://ec2-54-236-245-61.compute-1.amazonaws.com:8985/solr/test_shard2_replica2/&lt;/a&gt; = 62984 LEADER&lt;br/&gt;
	&lt;a href=&quot;http://ec2-107-21-55-0.compute-1.amazonaws.com:8985/solr/test_shard2_replica1/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://ec2-107-21-55-0.compute-1.amazonaws.com:8985/solr/test_shard2_replica1/&lt;/a&gt; = 62980 diff:4&lt;br/&gt;
}&lt;br/&gt;
Details:&lt;br/&gt;
shard2&lt;br/&gt;
&amp;gt;&amp;gt; finished querying leader, found 62984 documents (62984)&lt;br/&gt;
&amp;gt;&amp;gt; finished querying &lt;a href=&quot;http://ec2-107-21-55-0.compute-1.amazonaws.com:8985/solr/test_shard2_replica1/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://ec2-107-21-55-0.compute-1.amazonaws.com:8985/solr/test_shard2_replica1/&lt;/a&gt;, found 62980 documents&lt;br/&gt;
Doc &lt;span class=&quot;error&quot;&gt;&amp;#91;182866&amp;#93;&lt;/span&gt; not found in replica: &amp;lt;doc boost=&quot;1.0&quot;&amp;gt;&amp;lt;field name=&quot;id&quot;&amp;gt;182866&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;string_s&quot;&amp;gt;test&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;int_i&quot;&amp;gt;-1257345242&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;float_f&quot;&amp;gt;0.92657363&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;double_d&quot;&amp;gt;0.5259114828332452&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;text_en&quot;&amp;gt;this is a test&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;&lt;em&gt;version&lt;/em&gt;&quot;&amp;gt;1457415570117885953&amp;lt;/field&amp;gt;&amp;lt;/doc&amp;gt;&lt;br/&gt;
Doc &lt;span class=&quot;error&quot;&gt;&amp;#91;182859&amp;#93;&lt;/span&gt; not found in replica: &amp;lt;doc boost=&quot;1.0&quot;&amp;gt;&amp;lt;field name=&quot;id&quot;&amp;gt;182859&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;string_s&quot;&amp;gt;test&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;int_i&quot;&amp;gt;991366909&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;float_f&quot;&amp;gt;0.5311716&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;double_d&quot;&amp;gt;0.10846350752086309&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;text_en&quot;&amp;gt;this is a test&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;&lt;em&gt;version&lt;/em&gt;&quot;&amp;gt;1457415570117885952&amp;lt;/field&amp;gt;&amp;lt;/doc&amp;gt;&lt;br/&gt;
Doc &lt;span class=&quot;error&quot;&gt;&amp;#91;182872&amp;#93;&lt;/span&gt; not found in replica: &amp;lt;doc boost=&quot;1.0&quot;&amp;gt;&amp;lt;field name=&quot;id&quot;&amp;gt;182872&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;string_s&quot;&amp;gt;test&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;int_i&quot;&amp;gt;824512897&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;float_f&quot;&amp;gt;0.830366&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;double_d&quot;&amp;gt;0.6560223698806142&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;text_en&quot;&amp;gt;this is a test&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;&lt;em&gt;version&lt;/em&gt;&quot;&amp;gt;1457415570117885954&amp;lt;/field&amp;gt;&amp;lt;/doc&amp;gt;&lt;br/&gt;
Doc &lt;span class=&quot;error&quot;&gt;&amp;#91;182876&amp;#93;&lt;/span&gt; not found in replica: &amp;lt;doc boost=&quot;1.0&quot;&amp;gt;&amp;lt;field name=&quot;id&quot;&amp;gt;182876&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;string_s&quot;&amp;gt;test&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;int_i&quot;&amp;gt;-1657831473&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;float_f&quot;&amp;gt;0.4877965&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;double_d&quot;&amp;gt;0.9214420679315872&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;text_en&quot;&amp;gt;this is a test&amp;lt;/field&amp;gt;&amp;lt;field name=&quot;&lt;em&gt;version&lt;/em&gt;&quot;&amp;gt;1457415570117885955&amp;lt;/field&amp;gt;&amp;lt;/doc&amp;gt;&lt;br/&gt;
Sending hard commit after mis-match and then will wait for user to handle it ...&lt;br/&gt;
&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&lt;/p&gt;

&lt;p&gt;So four missing docs: 182866, 182859, 182872, 182876&lt;/p&gt;

&lt;p&gt;Now I&apos;m thinking this might be in the ConcurrentUpdateSolrServer logic. I added some detailed logging to show when JavabinLoader unmarshals a doc and when it is offered on the CUSS queue (to be sent to the replica). On the leader, here&apos;s the log around some messages that were lost:&lt;/p&gt;

&lt;p&gt;2014-01-16 14:16:37,534 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp417447538-17&amp;#93;&lt;/span&gt; INFO  handler.loader.JavabinLoader  - test_shard2_replica2 add: 182857&lt;br/&gt;
2014-01-16 14:16:37,534 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp417447538-17&amp;#93;&lt;/span&gt; INFO  solrj.impl.ConcurrentUpdateSolrServer  - test_shard2_replica2 queued: 182857&lt;br/&gt;
/////////////////////////////////////&lt;br/&gt;
2014-01-16 14:16:37,552 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp417447538-17&amp;#93;&lt;/span&gt; INFO  handler.loader.JavabinLoader  - test_shard2_replica2 add: 182859&lt;br/&gt;
2014-01-16 14:16:37,552 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp417447538-17&amp;#93;&lt;/span&gt; INFO  solrj.impl.ConcurrentUpdateSolrServer  - test_shard2_replica2 queued: 182859&lt;br/&gt;
2014-01-16 14:16:37,552 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp417447538-17&amp;#93;&lt;/span&gt; INFO  handler.loader.JavabinLoader  - test_shard2_replica2 add: 182866&lt;br/&gt;
2014-01-16 14:16:37,552 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp417447538-17&amp;#93;&lt;/span&gt; INFO  solrj.impl.ConcurrentUpdateSolrServer  - test_shard2_replica2 queued: 182866&lt;br/&gt;
2014-01-16 14:16:37,552 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp417447538-17&amp;#93;&lt;/span&gt; INFO  handler.loader.JavabinLoader  - test_shard2_replica2 add: 182872&lt;br/&gt;
2014-01-16 14:16:37,552 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp417447538-17&amp;#93;&lt;/span&gt; INFO  solrj.impl.ConcurrentUpdateSolrServer  - test_shard2_replica2 queued: 182872&lt;br/&gt;
2014-01-16 14:16:37,552 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp417447538-17&amp;#93;&lt;/span&gt; INFO  handler.loader.JavabinLoader  - test_shard2_replica2 add: 182876&lt;br/&gt;
2014-01-16 14:16:37,552 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp417447538-17&amp;#93;&lt;/span&gt; INFO  solrj.impl.ConcurrentUpdateSolrServer  - test_shard2_replica2 queued: 182876&lt;br/&gt;
2014-01-16 14:16:37,558 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp417447538-17&amp;#93;&lt;/span&gt; INFO  update.processor.LogUpdateProcessor  - &lt;span class=&quot;error&quot;&gt;&amp;#91;test_shard2_replica2&amp;#93;&lt;/span&gt; webapp=/solr path=/update params=&lt;/p&gt;
{wt=javabin&amp;amp;version=2}
&lt;p&gt; &lt;/p&gt;
{add=[182704 (1457415570048679936), 182710 (1457415570049728512), 182711 (1457415570049728513), 182717 (1457415570056019968), 182720 (1457415570056019969), 182722 (1457415570057068544), 182723 (1457415570057068545), 182724 (1457415570058117120), 182730 (1457415570058117121), 182735 (1457415570059165696), ... (61 adds)]}
&lt;p&gt; 0 72&lt;br/&gt;
/////////////////////////////////////&lt;br/&gt;
2014-01-16 14:16:37,764 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp417447538-17&amp;#93;&lt;/span&gt; INFO  handler.loader.JavabinLoader  - test_shard2_replica2 add: 182880&lt;br/&gt;
2014-01-16 14:16:37,764 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp417447538-17&amp;#93;&lt;/span&gt; INFO  solrj.impl.ConcurrentUpdateSolrServer  - test_shard2_replica2 queued: 182880&lt;/p&gt;


&lt;p&gt;As you can see, the leader received doc with ID:182859 at 2014-01-16 14:16:37,552 and the queued it on the CUSS queue to be sent to the replica. On the replica, the log shows it receiving 182857 and then 182880 ... the 4 missing docs (182866, 182859, 182872, 182876) were definitely queued in CUSS on the leader. I&apos;ve checked the logs on all the other replicas and the docs didn&apos;t go there either.&lt;/p&gt;


&lt;p&gt;2014-01-16 14:16:37,292 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp417447538-14&amp;#93;&lt;/span&gt; INFO  handler.loader.JavabinLoader  - test_shard2_replica1 add: 182857&lt;br/&gt;
2014-01-16 14:16:37,293 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp417447538-14&amp;#93;&lt;/span&gt; INFO  update.processor.LogUpdateProcessor  - &lt;span class=&quot;error&quot;&gt;&amp;#91;test_shard2_replica1&amp;#93;&lt;/span&gt; webapp=/solr path=/update params=&lt;/p&gt;
{distrib.from=http://ec2-54-236-245-61.compute-1.amazonaws.com:8985/solr/test_shard2_replica2/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;p&gt; &lt;/p&gt;
{add=[182841 (1457415570096914432), 182842 (1457415570096914433), 182843 (1457415570096914434), 182844 (1457415570096914435), 182846 (1457415570097963008), 182848 (1457415570097963009), 182850 (1457415570099011584), 182854 (1457415570099011585), 182857 (1457415570099011586)]}
&lt;p&gt; 0 2&lt;br/&gt;
2014-01-16 14:16:37,521 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp417447538-14&amp;#93;&lt;/span&gt; INFO  handler.loader.JavabinLoader  - test_shard2_replica1 add: 182880&lt;/p&gt;


&lt;p&gt;So it seems like a &quot;batch&quot; of docs queued on the leader just got missed ...&lt;/p&gt;</comment>
                            <comment id="13873851" author="shikhar" created="Thu, 16 Jan 2014 19:58:58 +0000"  >&lt;p&gt;This may be unrelated - I have not done much digging or looked at the full context, but was just looking at CUSS out of curiosity.&lt;/p&gt;

&lt;p&gt;Why do we flush() the OutputStream, but then write() on stuff like ending tags? Shouldn&apos;t the flush be after all those writes()&apos;s?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/lucene-solr/blob/lucene_solr_4_6/solr/solrj/src/java/org/apache/solr/client/solrj/impl/ConcurrentUpdateSolrServer.java#L205&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/lucene-solr/blob/lucene_solr_4_6/solr/solrj/src/java/org/apache/solr/client/solrj/impl/ConcurrentUpdateSolrServer.java#L205&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13873868" author="markrmiller@gmail.com" created="Thu, 16 Jan 2014 20:04:41 +0000"  >&lt;blockquote&gt;&lt;p&gt;I&apos;ve checked the logs on all the other replicas and the docs didn&apos;t go there either.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;So strange - it would be a different CUSS instance used for each server....&lt;/p&gt;</comment>
                            <comment id="13873874" author="joel.bernstein" created="Thu, 16 Jan 2014 20:11:36 +0000"  >&lt;p&gt;That was a blind alley, a faulty test was causing the effect I described above.&lt;/p&gt;</comment>
                            <comment id="13873882" author="markrmiller@gmail.com" created="Thu, 16 Jan 2014 20:17:38 +0000"  >&lt;p&gt;I have various theories, but without a test that fails, it&apos;s hard to test out anything - so I&apos;ve been putting most of my efforts into a unit test that can get this, but it&apos;s been surprisingly difficult for me to trigger in a test.&lt;/p&gt;</comment>
                            <comment id="13873888" author="tim.potter" created="Thu, 16 Jan 2014 20:25:22 +0000"  >&lt;blockquote&gt;&lt;p&gt;So strange - it would be a different CUSS instance used for each server....&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;right, I was just mentioning that I did check to make sure there wasn&apos;t a bug in the routing logic or anything like that but I now see that was silly because it wouldn&apos;t be able to go to the other replicas because the message was on the correct queue &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;agreed on the need for a unit test to reproduce this and am working on the same.&lt;/p&gt;</comment>
                            <comment id="13873951" author="tim.potter" created="Thu, 16 Jan 2014 21:15:09 +0000"  >&lt;p&gt;Added some more logging on the leader ... as a bit of context, the replica received doc with ID 41029 and then 41041 and didn&apos;t receive 41033 and 41038 in between ... here&apos;s the log on the leader of activity between 41029 and then 41041.&lt;/p&gt;

&lt;p&gt;2014-01-16 16:03:02,523 &lt;span class=&quot;error&quot;&gt;&amp;#91;updateExecutor-1-thread-1&amp;#93;&lt;/span&gt; INFO  solrj.impl.ConcurrentUpdateSolrServer  - sent docs to &lt;a href=&quot;http://ec2-107-21-55-0.compute-1.amazonaws.com:8985/solr/test3_shard3_replica1&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://ec2-107-21-55-0.compute-1.amazonaws.com:8985/solr/test3_shard3_replica1&lt;/a&gt; , 41003, 41&lt;br/&gt;
005, 41007, 41010, 41014, 41015, 41026, 41029&lt;br/&gt;
2014-01-16 16:03:02,527 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp417447538-16&amp;#93;&lt;/span&gt; INFO  handler.loader.JavabinLoader  - test3_shard3_replica2 add: 41033&lt;br/&gt;
2014-01-16 16:03:02,527 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp417447538-16&amp;#93;&lt;/span&gt; INFO  update.processor.DistributedUpdateProcessor  - doLocalAdd 41033&lt;br/&gt;
2014-01-16 16:03:02,527 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp417447538-16&amp;#93;&lt;/span&gt; INFO  solrj.impl.ConcurrentUpdateSolrServer  - test3_shard3_replica2 queued (to: &lt;a href=&quot;http://ec2-107-21-55-0.compute-1.amazonaws.com:8985/solr/test3_shard3_replica1):&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://ec2-107-21-55-0.compute-1.amazonaws.com:8985/solr/test3_shard3_replica1):&lt;/a&gt; 41033&lt;br/&gt;
2014-01-16 16:03:02,528 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp417447538-16&amp;#93;&lt;/span&gt; INFO  handler.loader.JavabinLoader  - test3_shard3_replica2 add: 41038&lt;br/&gt;
2014-01-16 16:03:02,528 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp417447538-16&amp;#93;&lt;/span&gt; INFO  update.processor.DistributedUpdateProcessor  - doLocalAdd 41038&lt;br/&gt;
2014-01-16 16:03:02,528 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp417447538-16&amp;#93;&lt;/span&gt; INFO  solrj.impl.ConcurrentUpdateSolrServer  - test3_shard3_replica2 queued (to: &lt;a href=&quot;http://ec2-107-21-55-0.compute-1.amazonaws.com:8985/solr/test3_shard3_replica1):&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://ec2-107-21-55-0.compute-1.amazonaws.com:8985/solr/test3_shard3_replica1):&lt;/a&gt; 41038&lt;br/&gt;
2014-01-16 16:03:02,559 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp417447538-16&amp;#93;&lt;/span&gt; INFO  solrj.impl.ConcurrentUpdateSolrServer  - blockUntilFinished starting &lt;a href=&quot;http://ec2-107-21-55-0.compute-1.amazonaws.com:8985/solr/test3_shard3_replica1&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://ec2-107-21-55-0.compute-1.amazonaws.com:8985/solr/test3_shard3_replica1&lt;/a&gt;&lt;br/&gt;
2014-01-16 16:03:02,559 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp417447538-16&amp;#93;&lt;/span&gt; INFO  solrj.impl.ConcurrentUpdateSolrServer  - blockUntilFinished is done for &lt;a href=&quot;http://ec2-107-21-55-0.compute-1.amazonaws.com:8985/solr/test3_shard3_replica1&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://ec2-107-21-55-0.compute-1.amazonaws.com:8985/solr/test3_shard3_replica1&lt;/a&gt;&lt;br/&gt;
2014-01-16 16:03:02,559 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp417447538-16&amp;#93;&lt;/span&gt; INFO  solrj.impl.ConcurrentUpdateSolrServer  - shutting down CUSS for &lt;a href=&quot;http://ec2-107-21-55-0.compute-1.amazonaws.com:8985/solr/test3_shard3_replica1&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://ec2-107-21-55-0.compute-1.amazonaws.com:8985/solr/test3_shard3_replica1&lt;/a&gt;&lt;br/&gt;
2014-01-16 16:03:02,559 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp417447538-16&amp;#93;&lt;/span&gt; INFO  solrj.impl.ConcurrentUpdateSolrServer  - shut down CUSS for &lt;a href=&quot;http://ec2-107-21-55-0.compute-1.amazonaws.com:8985/solr/test3_shard3_replica1&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://ec2-107-21-55-0.compute-1.amazonaws.com:8985/solr/test3_shard3_replica1&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Not quite sure what this means but I think you&apos;re hunch about blockUntilFinished being involved is getting warmer&lt;/p&gt;</comment>
                            <comment id="13873964" author="markrmiller@gmail.com" created="Thu, 16 Jan 2014 21:21:12 +0000"  >&lt;p&gt;For a long time, I&apos;ve wanted to try putting in a check that the queue is empty as well for blockUntilFinished when we use it in this case - I just need a test that sees this so I can check if it works &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Without that, it seems there is a window where we can bail before we are done sending everything in the queue. Shutdown doesn&apos;t help much, because it can&apos;t even wait for the executor to shutdown in this case.&lt;/p&gt;</comment>
                            <comment id="13873988" author="markrmiller@gmail.com" created="Thu, 16 Jan 2014 21:39:17 +0000"  >&lt;p&gt;Patch attached that does the above.&lt;/p&gt;</comment>
                            <comment id="13874031" author="markrmiller@gmail.com" created="Thu, 16 Jan 2014 22:09:12 +0000"  >&lt;p&gt;That&apos;s all I have come up with so far - though I&apos;m not even completely sold on it. Because we are using CUSS with a single thread, all the previous doc adds should have hit the request method and so a Runner should be going for them if necessary.&lt;/p&gt;

&lt;p&gt;It&apos;s all pretty tricky logic to understand clearly though.&lt;/p&gt;</comment>
                            <comment id="13874098" author="tim.potter" created="Thu, 16 Jan 2014 22:49:44 +0000"  >&lt;p&gt;So far so good, Mark! I applied the patch to latest rev of branch_4x and have indexed about 3M docs without hitting the issue, before the patch, I would see this issue within a few minutes. So jury is still out and I&apos;ll keep stress testing it, but looks promising. Nice work!&lt;/p&gt;</comment>
                            <comment id="13874120" author="tim.potter" created="Thu, 16 Jan 2014 23:09:12 +0000"  >&lt;p&gt;Did another couple of million docs in an oversharded env. 24 replicas on 6 nodes (m1.mediums so I didn&apos;t want to overload them too much) ... still looking good.&lt;/p&gt;</comment>
                            <comment id="13874194" author="joel.bernstein" created="Fri, 17 Jan 2014 00:10:09 +0000"  >&lt;p&gt;I installed the patch and ran it. &lt;/p&gt;

&lt;p&gt;I&apos;m getting some intermittent null pointers:&lt;/p&gt;

&lt;p&gt;1578995 &lt;span class=&quot;error&quot;&gt;&amp;#91;qtp433857665-17&amp;#93;&lt;/span&gt; ERROR org.apache.solr.servlet.SolrDispatchFilter  &#8211; null:java.lang.NullPointerException&lt;br/&gt;
	at org.apache.solr.client.solrj.impl.ConcurrentUpdateSolrServer.blockUntilFinished(ConcurrentUpdateSolrServer.java:401)&lt;br/&gt;
	at org.apache.solr.update.StreamingSolrServers.blockUntilFinished(StreamingSolrServers.java:99)&lt;br/&gt;
	at org.apache.solr.update.SolrCmdDistributor.finish(SolrCmdDistributor.java:69)&lt;br/&gt;
	at org.apache.solr.update.processor.DistributedUpdateProcessor.doFinish(DistributedUpdateProcessor.java:606)&lt;br/&gt;
	at org.apache.solr.update.processor.DistributedUpdateProcessor.finish(DistributedUpdateProcessor.java:1449)&lt;br/&gt;
	at org.apache.solr.update.processor.LogUpdateProcessor.finish(LogUpdateProcessorFactory.java:179)&lt;br/&gt;
	at org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:83)&lt;br/&gt;
	at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:135)&lt;br/&gt;
	at org.apache.solr.core.SolrCore.execute(SolrCore.java:1915)&lt;br/&gt;
	at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:764)&lt;br/&gt;
	at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:418)&lt;br/&gt;
	at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:203)&lt;br/&gt;
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1419)&lt;/p&gt;</comment>
                            <comment id="13874204" author="joel.bernstein" created="Fri, 17 Jan 2014 00:22:52 +0000"  >&lt;p&gt;In the code snippet below it looks like this line is the culprit:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; ((runner == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; &amp;amp;&amp;amp; queue.isEmpty()) || scheduler.isTerminated())
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt; &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; void blockUntilFinished(&lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; waitForEmptyQueue) {
    lock = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; CountDownLatch(1);
    &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
      &lt;span class=&quot;code-comment&quot;&gt;// Wait until no runners are running
&lt;/span&gt;      &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (;;) {
        Runner runner;
        &lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; (runners) {
          runner = runners.peek();
        }
        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (waitForEmptyQueue) {
          &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; ((runner == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; &amp;amp;&amp;amp; queue.isEmpty()) || scheduler.isTerminated())
            &lt;span class=&quot;code-keyword&quot;&gt;break&lt;/span&gt;;
        } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
          &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (runner == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; || scheduler.isTerminated())
            &lt;span class=&quot;code-keyword&quot;&gt;break&lt;/span&gt;;
        }
        runner.runnerLock.lock();
        runner.runnerLock.unlock();
      }
    } &lt;span class=&quot;code-keyword&quot;&gt;finally&lt;/span&gt; {
      lock.countDown();
      lock = &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;;
    }
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13874270" author="markrmiller@gmail.com" created="Fri, 17 Jan 2014 01:24:52 +0000"  >&lt;p&gt;Strange Joel - queue and scheduler are both final and set in the constructor.&lt;/p&gt;</comment>
                            <comment id="13874294" author="joel.bernstein" created="Fri, 17 Jan 2014 01:43:19 +0000"  >&lt;p&gt;It&apos;s actually the runner that is null:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;runner.runnerLock.lock();
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The conditions in this statement have changed and I think made it possible for the null pointer to appear.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; ((runner == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; &amp;amp;&amp;amp; queue.isEmpty()) || scheduler.isTerminated())
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13874300" author="jira-bot" created="Fri, 17 Jan 2014 01:48:43 +0000"  >&lt;p&gt;Commit 1558978 from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markrmiller%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;markrmiller@gmail.com&quot;&gt;markrmiller@gmail.com&lt;/a&gt; in branch &apos;dev/trunk&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1558978&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://svn.apache.org/r1558978&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-4260&quot; title=&quot;Inconsistent numDocs between leader and replica&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-4260&quot;&gt;&lt;del&gt;SOLR-4260&lt;/del&gt;&lt;/a&gt;: ConcurrentUpdateSolrServer#blockUntilFinished can return before all previously added updates have finished. This could cause distributed updates meant for replicas to be lost.&lt;/p&gt;</comment>
                            <comment id="13874301" author="markrmiller@gmail.com" created="Fri, 17 Jan 2014 01:49:28 +0000"  >&lt;p&gt;Well, this is important for 4.6.1 - given Potter&apos;s feedback, in it goes. Please help test and review this guys. Especially around this possible NPE.&lt;/p&gt;</comment>
                            <comment id="13874302" author="jira-bot" created="Fri, 17 Jan 2014 01:50:58 +0000"  >&lt;p&gt;Commit 1558979 from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markrmiller%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;markrmiller@gmail.com&quot;&gt;markrmiller@gmail.com&lt;/a&gt; in branch &apos;dev/branches/branch_4x&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1558979&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://svn.apache.org/r1558979&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-4260&quot; title=&quot;Inconsistent numDocs between leader and replica&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-4260&quot;&gt;&lt;del&gt;SOLR-4260&lt;/del&gt;&lt;/a&gt;: ConcurrentUpdateSolrServer#blockUntilFinished can return before all previously added updates have finished. This could cause distributed updates meant for replicas to be lost.&lt;/p&gt;</comment>
                            <comment id="13874303" author="jira-bot" created="Fri, 17 Jan 2014 01:51:09 +0000"  >&lt;p&gt;Commit 1558980 from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markrmiller%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;markrmiller@gmail.com&quot;&gt;markrmiller@gmail.com&lt;/a&gt; in branch &apos;dev/trunk&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1558980&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://svn.apache.org/r1558980&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-4260&quot; title=&quot;Inconsistent numDocs between leader and replica&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-4260&quot;&gt;&lt;del&gt;SOLR-4260&lt;/del&gt;&lt;/a&gt;: Add name to CHANGES&lt;/p&gt;</comment>
                            <comment id="13874305" author="jira-bot" created="Fri, 17 Jan 2014 01:52:10 +0000"  >&lt;p&gt;Commit 1558981 from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markrmiller%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;markrmiller@gmail.com&quot;&gt;markrmiller@gmail.com&lt;/a&gt; in branch &apos;dev/branches/branch_4x&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1558981&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://svn.apache.org/r1558981&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-4260&quot; title=&quot;Inconsistent numDocs between leader and replica&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-4260&quot;&gt;&lt;del&gt;SOLR-4260&lt;/del&gt;&lt;/a&gt;: Add name to CHANGES&lt;/p&gt;</comment>
                            <comment id="13874311" author="markrmiller@gmail.com" created="Fri, 17 Jan 2014 01:59:45 +0000"  >&lt;blockquote&gt;&lt;p&gt;The conditions in this statement have changed and I think made it possible for the null pointer to appear.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ah, nice - thanks. I had already made some changes so couldn&apos;t line up the src lines - thought you meant the line that was the culprit was the one that the NPE came from.&lt;/p&gt;

&lt;p&gt;I&apos;ll take a closer look.&lt;/p&gt;</comment>
                            <comment id="13874314" author="jira-bot" created="Fri, 17 Jan 2014 02:00:58 +0000"  >&lt;p&gt;Commit 1558982 from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markrmiller%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;markrmiller@gmail.com&quot;&gt;markrmiller@gmail.com&lt;/a&gt; in branch &apos;dev/branches/lucene_solr_4_6&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1558982&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://svn.apache.org/r1558982&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-4260&quot; title=&quot;Inconsistent numDocs between leader and replica&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-4260&quot;&gt;&lt;del&gt;SOLR-4260&lt;/del&gt;&lt;/a&gt;: ConcurrentUpdateSolrServer#blockUntilFinished can return before all previously added updates have finished. This could cause distributed updates meant for replicas to be lost.&lt;/p&gt;</comment>
                            <comment id="13874317" author="jira-bot" created="Fri, 17 Jan 2014 02:03:07 +0000"  >&lt;p&gt;Commit 1558983 from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markrmiller%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;markrmiller@gmail.com&quot;&gt;markrmiller@gmail.com&lt;/a&gt; in branch &apos;dev/branches/lucene_solr_4_6&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1558983&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://svn.apache.org/r1558983&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-4260&quot; title=&quot;Inconsistent numDocs between leader and replica&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-4260&quot;&gt;&lt;del&gt;SOLR-4260&lt;/del&gt;&lt;/a&gt;: Add name to CHANGES&lt;/p&gt;</comment>
                            <comment id="13874320" author="jira-bot" created="Fri, 17 Jan 2014 02:06:32 +0000"  >&lt;p&gt;Commit 1558985 from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markrmiller%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;markrmiller@gmail.com&quot;&gt;markrmiller@gmail.com&lt;/a&gt; in branch &apos;dev/trunk&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1558985&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://svn.apache.org/r1558985&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-4260&quot; title=&quot;Inconsistent numDocs between leader and replica&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-4260&quot;&gt;&lt;del&gt;SOLR-4260&lt;/del&gt;&lt;/a&gt;: Guard against NPE.&lt;/p&gt;</comment>
                            <comment id="13874321" author="jira-bot" created="Fri, 17 Jan 2014 02:08:01 +0000"  >&lt;p&gt;Commit 1558986 from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markrmiller%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;markrmiller@gmail.com&quot;&gt;markrmiller@gmail.com&lt;/a&gt; in branch &apos;dev/branches/branch_4x&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1558986&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://svn.apache.org/r1558986&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-4260&quot; title=&quot;Inconsistent numDocs between leader and replica&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-4260&quot;&gt;&lt;del&gt;SOLR-4260&lt;/del&gt;&lt;/a&gt;: Guard against NPE.&lt;/p&gt;</comment>
                            <comment id="13874323" author="jira-bot" created="Fri, 17 Jan 2014 02:10:17 +0000"  >&lt;p&gt;Commit 1558988 from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markrmiller%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;markrmiller@gmail.com&quot;&gt;markrmiller@gmail.com&lt;/a&gt; in branch &apos;dev/branches/lucene_solr_4_6&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1558988&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://svn.apache.org/r1558988&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-4260&quot; title=&quot;Inconsistent numDocs between leader and replica&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-4260&quot;&gt;&lt;del&gt;SOLR-4260&lt;/del&gt;&lt;/a&gt;: Guard against NPE.&lt;/p&gt;</comment>
                            <comment id="13874340" author="markrmiller@gmail.com" created="Fri, 17 Jan 2014 02:28:39 +0000"  >&lt;p&gt;ChaosMonkeyNothingIsSafeTest is exposing an issue now with ConcurrentUpdateSolrServer - it looks like it&apos;s getting stuck in blockUntilFinished because the queue is not empty and no runners are being spawned to empty it.&lt;/p&gt;

&lt;p&gt;It may be that NPE that would occurred before in this case just kept the docs from being lost &apos;silently&apos;, and this is closer to the actual bug?&lt;/p&gt;</comment>
                            <comment id="13874345" author="jira-bot" created="Fri, 17 Jan 2014 02:36:27 +0000"  >&lt;p&gt;Commit 1558996 from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markrmiller%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;markrmiller@gmail.com&quot;&gt;markrmiller@gmail.com&lt;/a&gt; in branch &apos;dev/trunk&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1558996&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://svn.apache.org/r1558996&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-4260&quot; title=&quot;Inconsistent numDocs between leader and replica&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-4260&quot;&gt;&lt;del&gt;SOLR-4260&lt;/del&gt;&lt;/a&gt;: If in blockUntilFinished and there are no Runners running and the queue is not empty, start a new Runner.&lt;/p&gt;</comment>
                            <comment id="13874348" author="jira-bot" created="Fri, 17 Jan 2014 02:37:36 +0000"  >&lt;p&gt;Commit 1558997 from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markrmiller%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;markrmiller@gmail.com&quot;&gt;markrmiller@gmail.com&lt;/a&gt; in branch &apos;dev/branches/branch_4x&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1558997&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://svn.apache.org/r1558997&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-4260&quot; title=&quot;Inconsistent numDocs between leader and replica&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-4260&quot;&gt;&lt;del&gt;SOLR-4260&lt;/del&gt;&lt;/a&gt;: If in blockUntilFinished and there are no Runners running and the queue is not empty, start a new Runner.&lt;/p&gt;</comment>
                            <comment id="13874351" author="markrmiller@gmail.com" created="Fri, 17 Jan 2014 02:40:30 +0000"  >&lt;p&gt;Committed something for that.&lt;/p&gt;

&lt;p&gt;As a separate issue, it seems to me that CUSS#shutdown should probably call blockUntilFinished as it&apos;s first order of business.&lt;/p&gt;</comment>
                            <comment id="13874352" author="jira-bot" created="Fri, 17 Jan 2014 02:40:58 +0000"  >&lt;p&gt;Commit 1558998 from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markrmiller%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;markrmiller@gmail.com&quot;&gt;markrmiller@gmail.com&lt;/a&gt; in branch &apos;dev/branches/lucene_solr_4_6&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1558998&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://svn.apache.org/r1558998&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-4260&quot; title=&quot;Inconsistent numDocs between leader and replica&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-4260&quot;&gt;&lt;del&gt;SOLR-4260&lt;/del&gt;&lt;/a&gt;: If in blockUntilFinished and there are no Runners running and the queue is not empty, start a new Runner.&lt;/p&gt;</comment>
                            <comment id="13874364" author="markrmiller@gmail.com" created="Fri, 17 Jan 2014 02:55:48 +0000"  >&lt;p&gt;This is a fine fix for SolrCloud, especially for 4.6.1 - but there may be a better general fix hidden still - what seems to happen is that we have docs that enter the queue that don&apos;t spawn a runner. The current fix means docs can be added that will sit in the queue until you call blockUntilFinished.&lt;/p&gt;</comment>
                            <comment id="13874434" author="elyograg" created="Fri, 17 Jan 2014 04:59:06 +0000"  >&lt;p&gt;This might be old news by now, but I noticed it while updating my test system, so I&apos;m reporting it.&lt;/p&gt;

&lt;p&gt;The lucene_solr_4_6 branch fails to compile with these fixes committed.  One of the changes removes the import for RemoteSolrException from SolrCmdDistributor, but the doRetries method still uses this exception.  That method is very different in 4.6 than it is in branch_4x.  Everything&apos;s good on branch_4x.  Re-adding the import fixes the problem, but the discrepancy between the two branches needs some investigation.&lt;/p&gt;

&lt;p&gt;The specific code that fails to compile with the removed import seems to have been initially added to trunk by revision 1545464 (2013/11/25) and removed from trunk by revision 1546670 (2013/11/29).  It was then re-added to lucene_solr_4_6 by revision 1554122 (2013/12/29).&lt;/p&gt;</comment>
                            <comment id="13874667" author="markus17" created="Fri, 17 Jan 2014 11:10:37 +0000"  >&lt;p&gt;I believe the whole building now knows i cannot reproduce the problem!&lt;/p&gt;</comment>
                            <comment id="13874686" author="mkhludnev" created="Fri, 17 Jan 2014 11:24:52 +0000"  >&lt;p&gt;What a great hunt, guys! Thanks a lot!&lt;/p&gt;</comment>
                            <comment id="13874835" author="joel.bernstein" created="Fri, 17 Jan 2014 14:45:19 +0000"  >&lt;p&gt;Ok, just had two clean test runs with trunk. The NPE is no longer occurring and the leaders and replicas are in sync. Running through some more stress tests this morning, but so far so good.&lt;/p&gt;
</comment>
                            <comment id="13874839" author="jira-bot" created="Fri, 17 Jan 2014 14:54:40 +0000"  >&lt;p&gt;Commit 1559125 from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markrmiller%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;markrmiller@gmail.com&quot;&gt;markrmiller@gmail.com&lt;/a&gt; in branch &apos;dev/branches/lucene_solr_4_6&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1559125&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://svn.apache.org/r1559125&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-4260&quot; title=&quot;Inconsistent numDocs between leader and replica&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-4260&quot;&gt;&lt;del&gt;SOLR-4260&lt;/del&gt;&lt;/a&gt;: Bring back import still used on 4.6 branch.&lt;/p&gt;</comment>
                            <comment id="13874841" author="markrmiller@gmail.com" created="Fri, 17 Jan 2014 15:03:26 +0000"  >&lt;p&gt;Thanks Shawn - fixed.&lt;/p&gt;</comment>
                            <comment id="13875755" author="markrmiller@gmail.com" created="Sat, 18 Jan 2014 21:57:41 +0000"  >&lt;p&gt;Thanks everyone. I&apos;ll make a new JIRA issue to properly fix this. I&apos;m not sure we should remove this logic, it&apos;s a good failsafe, but ideally, we don&apos;t want to run out of runners when there are still updates in the queue. Calling blockUntilFinished is not supposed to be required to make sure the queue is emptied.&lt;/p&gt;</comment>
                            <comment id="13888951" author="markrmiller@gmail.com" created="Sun, 2 Feb 2014 15:19:33 +0000"  >&lt;p&gt;Calling this done for 4.6.1. Let&apos;s open a new issue for anything further.&lt;/p&gt;</comment>
                            <comment id="13897716" author="markus17" created="Tue, 11 Feb 2014 10:39:36 +0000"  >&lt;p&gt;&lt;del&gt;ignore&lt;/del&gt; apparently one node did not receive the update. forgive my stupidity&lt;/p&gt;</comment>
                            <comment id="14496055" author="harisekhon" created="Wed, 15 Apr 2015 11:15:16 +0000"  >&lt;p&gt;I&apos;ve seen discrepancies between leader and followers of much higher numbers on newer versions of Solr than in this ticket when running on HDFS, it might be a separate issue, raised as &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-7395&quot; title=&quot;Major numDocs inconsistency between leader and follower replicas in SolrCloud on HDFS&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-7395&quot;&gt;&lt;del&gt;SOLR-7395&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="14499853" author="markrmiller@gmail.com" created="Fri, 17 Apr 2015 13:51:17 +0000"  >&lt;p&gt;This ticket addressed specific issues - please open a new ticket for any further reports.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12676206">SOLR-5397</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12684361">SOLR-5552</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12686103">SOLR-5573</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12688464">SOLR-5625</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12689682">SOLR-5643</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12612155" name="192.168.20.102-replica1.png" size="28990" author="yriveiro" created="Tue, 5 Nov 2013 11:08:43 +0000"/>
                            <attachment id="12612154" name="192.168.20.104-replica2.png" size="28728" author="yriveiro" created="Tue, 5 Nov 2013 11:08:43 +0000"/>
                            <attachment id="12623488" name="SOLR-4260.patch" size="4007" author="markrmiller@gmail.com" created="Thu, 16 Jan 2014 21:39:17 +0000"/>
                            <attachment id="12612153" name="clusterstate.png" size="76269" author="yriveiro" created="Tue, 5 Nov 2013 11:08:43 +0000"/>
                            <attachment id="12622006" name="demo_shard1_replicas_out_of_sync.tgz" size="4222957" author="tim.potter" created="Wed, 8 Jan 2014 18:32:25 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>5.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>302655</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 31 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i174jr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>249705</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12313321" key="com.atlassian.jira.toolkit:message">
                        <customfieldname>Solr Mailing List Info</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>