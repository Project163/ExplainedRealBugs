<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 03:29:40 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SOLR-3721] Multiple concurrent recoveries of same shard?</title>
                <link>https://issues.apache.org/jira/browse/SOLR-3721</link>
                <project id="12310230" key="SOLR">Solr</project>
                    <description>&lt;p&gt;We run a performance/endurance test on a 7 Solr instance SolrCloud setup and eventually Solrs lose ZK connections and go into recovery. BTW the recovery often does not ever succeed, but we are looking into that. While doing that I noticed that, according to logs, multiple recoveries are in progress at the same time for the same shard. That cannot be intended and I can certainly imagine that it will cause some problems.&lt;br/&gt;
It is just the logs that are wrong, did I make some mistake, or is this a real bug?&lt;/p&gt;

&lt;p&gt;See attached grep from log, grepping only on &quot;Finished recovery&quot; and &quot;Starting recovery&quot; logs.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;grep -B 1 &lt;span class=&quot;code-quote&quot;&gt;&quot;Finished recovery\|Starting recovery&quot;&lt;/span&gt; solr9.log solr8.log solr7.log solr6.log solr5.log solr4.log solr3.log solr2.log solr1.log solr0.log &amp;gt; recovery_start_finish.log
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It can be hard to get an overview of the log, but I have generated a graph showing (based alone on &quot;Started recovery&quot; and &quot;Finished recovery&quot; logs) how many recoveries are in progress at any time for the different shards. See attached recovery_in_progress.png. The graph is also a little hard to get an overview of (due to the many shards) but it is clear that for several shards there are multiple recoveries going on at the same time, and that several recoveries never succeed.&lt;/p&gt;

&lt;p&gt;Regards, Per Steffensen&lt;/p&gt;</description>
                <environment>&lt;p&gt;Using our own Solr release based on Apache revision 1355667 from 4.x branch. Our changes to the Solr version is our solutions to TLT-3178 etc., and should have no effect on this issue.&lt;/p&gt;</environment>
        <key id="12602132">SOLR-3721</key>
            <summary>Multiple concurrent recoveries of same shard?</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="steff1193">Per Steffensen</reporter>
                        <labels>
                            <label>concurrency</label>
                            <label>multicore</label>
                            <label>recovery</label>
                            <label>solrcloud</label>
                    </labels>
                <created>Wed, 8 Aug 2012 07:46:13 +0000</created>
                <updated>Tue, 9 May 2017 20:33:30 +0000</updated>
                                                                            <component>multicore</component>
                    <component>SolrCloud</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="13431203" author="markrmiller@gmail.com" created="Wed, 8 Aug 2012 16:31:22 +0000"  >&lt;p&gt;Correct, only one recovery should run at a time. I&apos;ll try and look into this.&lt;/p&gt;</comment>
                            <comment id="13432948" author="steff1193" created="Mon, 13 Aug 2012 06:19:11 +0000"  >&lt;p&gt;What if e.g. you lose the ZK connection while in recovery - will it then start another recovery without checking if one is already going on.&lt;/p&gt;</comment>
                            <comment id="13433241" author="markrmiller@gmail.com" created="Mon, 13 Aug 2012 16:10:19 +0000"  >&lt;p&gt;It should not, no. Each core has a recovery lock that should only allow one recovery to happen at a time. An further recovery attempts should line up and procede one at a time. That&apos;s not saying there is not a bug here - but that&apos;s the intention.&lt;/p&gt;</comment>
                            <comment id="13435953" author="steff1193" created="Thu, 16 Aug 2012 13:37:27 +0000"  >&lt;p&gt;What if two Solrs, respectively running leader and replica for the same slice (only one replica), lose their ZK connection at about the same time. Then there will be no active shard that either of them can recover from. Could it be in such scenarios that multiple concurrent recoveries of the same shard somehow get started?&lt;/p&gt;

&lt;p&gt;BTW, the scenario above shouldnt end in a situation where the slice is just dead. The two shards in the same slice ought to find out who has the newest version of the shard-data (will probably be the one that was leader last), make that shard the leader (without recovering) and let the other shard recover from it. Is this scenarios handled (in the way I suggest or in another way) already in Solr 4.0 (beta - tip of branch) or is that a future thing (e.g. on 4.1 or 5.0)?&lt;/p&gt;

&lt;p&gt;Regards, Per Steffensen&lt;/p&gt;</comment>
                            <comment id="13435980" author="markrmiller@gmail.com" created="Thu, 16 Aug 2012 14:00:09 +0000"  >&lt;blockquote&gt;&lt;p&gt;What if two Solrs, respectively running leader and replica for the same slice (only one replica), lose their ZK connection at about the same time. Then there will be no active shard that either of them can recover from. Could it be in such scenarios that multiple concurrent recoveries of the same shard somehow get started?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No recovery will be started if a node cannot talk to zookeeper. So nothing would happen until one or both of the nodes reconnected to ZooKeeper. That would trigger a leader election, that leader node would attempt to sync up with all the other nodes for that shard and any recoveries would procede against him.&lt;/p&gt;

&lt;p&gt;There is a lock for each core that only allows one recovery per core to happen at a time. I&apos;m not saying there is no bug in this, but that is the intention.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;BTW, the scenario above shouldnt end in a situation where the slice is just dead. The two shards in the same slice ought to find out who has the newest version of the shard-data (will probably be the one that was leader last), make that shard the leader (without recovering) and let the other shard recover from it. Is this scenarios handled (in the way I suggest or in another way) already in Solr 4.0 (beta - tip of branch) or is that a future thing (e.g. on 4.1 or 5.0)?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It happens as I mention above. A little more detail on the &quot;leader attempts to sync up&quot;:&lt;/p&gt;

&lt;p&gt;When a new node is elected as a leader by ZooKeeper it first tries to do a peer sync against every other live node. So lets say the first node in your two node situation comes back and he is behind the other node, but he comes back first and is elected leader. The second node has the latest updates, but is second in line to be leader and a few updates ahead. The potential leader will try and peer sync with the other node and get those missing updates if it&apos;s fewer than 100 or fail because the other node is ahead by too much. If the peer sync is a fail, the potential leader will give up his leader role, realizing that it seems there is a better candidate. The other node, being the next in line to be leader, will now try and peer sync with the other nodes in the shard. In this case, that will be a success since he is ahead of the first node. He will then ask the other nodes to peer sync to him. If they are less than 100 docs behind, it will succeed. If any sync back attempts fail, the leader tries to ask them to recover and they will replicate. Only after this sync process is completed does the leader advertise that he is now the leader in the cloud state.&lt;/p&gt;

&lt;p&gt;That is the current process - we will continually be hardening and improving it I&apos;m sure.&lt;/p&gt;</comment>
                            <comment id="13436154" author="steff1193" created="Thu, 16 Aug 2012 17:53:23 +0000"  >&lt;blockquote&gt;&lt;p&gt; No recovery will be started if a node cannot talk to zookeeper. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well, I knew that. I meant that the two Solrs where disconnected from ZK at the same time, but of course both got their connection reestablished - after session timeout (believe (kinda hope) that a session timeout has to have happened before Solr needs to go into recovery after a ZK connection loss)&lt;/p&gt;

&lt;p&gt;When it gets prioritized on my side, I will try to investigate further what causes the log to claim that many recoveries goes on for the same shard concurrently.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt; When a new node is elected as a leader by ZooKeeper it first tries to do a peer sync against every other live node. So lets say the first node in your two node situation comes back and he is behind the other node, but he comes back first and is elected leader. The second node has the latest updates, but is second in line to be leader and a few updates ahead. The potential leader will try and peer sync with the other node and get those missing updates if it&apos;s fewer than 100 or fail because the other node is ahead by too much. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well we shouldnt let this issue (&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-3721&quot; title=&quot;Multiple concurrent recoveries of same shard?&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-3721&quot;&gt;SOLR-3721&lt;/a&gt;) become about many other issues, but when the &quot;behind&quot; node has reconnected and become leader and the one with the latest updates does not come back live right away, isnt the new leader (which is behind) allowed to start handling update-requests. If yes, then it will be possible that both shards have documents/updates that the other one doesnt, and it is possible to come up with scenarios where there is no good algorithm for generating the &quot;correct&quot; merged union of the data in both shards. So what to do when the other shard (which used to have a later version than the current leader) comes live? Believe there is nothing solid to do!&lt;/p&gt;

&lt;p&gt;How to avoid that? I was thinking about keeping the latest version for every slice in ZK, so that a &quot;behind&quot; shard will know if it has the latest version of a slice, and therefore if it is allowed to take the role as leader. Of course the writing of this &quot;latest version&quot; to ZK and the writing of the corresponding update in leaders transaction-log would have to be atomic (like the A in ACID) as much as possible. And it would be nice if writing of the update in replica transaction-log would also be atomic with the leader-writing and the ZK writing, in order to increase the chance that a replica is actually allowed to take over the leader role if the leader dies (or both dies and replica comes back first). But all that is just an idea on top of my head. &lt;br/&gt;
Do you already have a solution implemented or a solution on the drawing board or how do you/we prevent such a problem? As far as I understand &quot;the drill&quot; during leader-election/recovery (whether its peer-sync or file-copy-replication) from the little code-reading I have done and from what you explain, there is not a current solution. But I might be wrong?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt; The other node, being the next in line to be leader, will now try and peer sync with the other nodes in the shard &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Guess/hope you mean &quot;...with the other shards (running on different nodes) in the slice&quot;. As I understand Solr terminology a logical chunk of the &quot;entire data&quot; (a collection in Solr) is a &quot;slice&quot;, and the data in a slice might physically exist more than one place (in more shards - if replication is used). Back when I started my interest in Solr I used a considerable amount of time understanding Solr terminology - mainly because it is different that what I have been used to (in my pre-Solr-world a &quot;shard&quot; is what you call a &quot;slice&quot;) - so now please dont tell me that I misunderstood &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt; That is the current process - we will continually be hardening and improving it I&apos;m sure. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I will probably stick around for that. The correctness and robusteness of this live-replication feature is (currently) very important to us.&lt;/p&gt;</comment>
                            <comment id="13436171" author="markrmiller@gmail.com" created="Thu, 16 Aug 2012 18:03:55 +0000"  >&lt;blockquote&gt;&lt;p&gt; Back when I started my interest in Solr I used a considerable amount of time understanding Solr terminology - mainly because it is different that what I have been used to (in my pre-Solr-world a &quot;shard&quot; is what you call a &quot;slice&quot;) - so now please dont tell me that I misunderstood &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We never had agreement on this. It ended up being we use slice in code and shard means one thing or the other depending on context. Can it be confusing? I think so.&lt;/p&gt;</comment>
                            <comment id="13436175" author="markrmiller@gmail.com" created="Thu, 16 Aug 2012 18:06:41 +0000"  >&lt;blockquote&gt;&lt;p&gt;Believe there is nothing solid to do!&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well, we can do some practical things right? I don&apos;t think we need to support a node coming back from the dead a year later and it had some updates the cluster doesn&apos;t have. A node coming up 2 minutes later is something we want to worry about though.&lt;/p&gt;

&lt;p&gt;So basically we either need something timing based or admin command based that lets you start a cold shard (slice &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;) and each node waits around for X amount of time or until command X is received, and then leader election begins. &lt;/p&gt;</comment>
                            <comment id="13436383" author="janhoy" created="Thu, 16 Aug 2012 22:44:24 +0000"  >&lt;p&gt;ElasticSearch has some settings to control when recovery starts after cluster restart, see &lt;a href=&quot;http://www.elasticsearch.org/guide/reference/modules/gateway/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Guide&lt;/a&gt;. This approach looks reasonable. If we know that we expect N nodes in our cluster we can start recovery when we see N nodes up. If fewer than N nodes up, we wait for X time (running on local data, not accepting new updates) before recovery and leader election starts.&lt;/p&gt;</comment>
                            <comment id="13436525" author="markrmiller@gmail.com" created="Fri, 17 Aug 2012 02:38:26 +0000"  >&lt;p&gt;I also forgot to mention - not only is there the lock that is there to only allow one recovery to procede at a time, but every recovery cancels any in progress recovery before it starts - this does a Thread#join on the existing recovery thread. Then a new recovery thread starts.&lt;/p&gt;</comment>
                            <comment id="13438518" author="steff1193" created="Tue, 21 Aug 2012 07:28:09 +0000"  >&lt;p&gt;Took the comments (parts) on this issue that is really not around &quot;multiple concurrent recoveries of same shard&quot;, but which is basically more around &quot;Avoid losing data on ZK connection-loss/session-timeout&quot;, copy-pasted them to a mail with subject &quot;Avoid losing data on ZK connection-loss/session-timeout&quot; and sent it to dev@lucene.apache.com. Lets continue the discussion there.&lt;/p&gt;</comment>
                            <comment id="13439176" author="markrmiller@gmail.com" created="Wed, 22 Aug 2012 00:02:47 +0000"  >&lt;p&gt;connection-loss should not be involved here - the general move for connection-loss is to retry - not go into any recoveries - until you hit a session-timeout. There is an exception in the leader election agl where you don&apos;t want to retry, but that is another story).&lt;/p&gt;

&lt;p&gt;(which you may know, but for clarification to those following along).&lt;/p&gt;</comment>
                            <comment id="13440531" author="markrmiller@gmail.com" created="Thu, 23 Aug 2012 18:37:07 +0000"  >&lt;p&gt;I&apos;ve been reviewing this code, and so far there is only way I can see that would seem to allow multiple recoveries at once - if there is an interrupt when cancelRecovery is doing a join, it seems another recovery could be started and briefly overlap. Since we already call close on the recovery thread, the overlap would be brief at best. An interrupt like this should be somewhat rare - but interrupts do happen on jetty shutdown. I&apos;m going to guess this is not what you were seeing, but I&apos;ll plug it.&lt;/p&gt;</comment>
                            <comment id="13445001" author="markrmiller@gmail.com" created="Thu, 30 Aug 2012 15:11:08 +0000"  >&lt;p&gt;I&apos;ve committed the fix to the issue I mention in the above comment.&lt;/p&gt;</comment>
                            <comment id="13445219" author="steff1193" created="Thu, 30 Aug 2012 19:25:04 +0000"  >&lt;p&gt;Cool. I dont believe this was our issue, as you also mention. But any fix of a potential problem is a step in the right direction.&lt;/p&gt;

&lt;p&gt;It will probably be a while before I get back to this issue, because we have changed our strategy. Before we where aming at including replication in first production-ready version of our application. We have had too many problems with the immature (IMHO) new way of doing replication in 4.0. Now we aim at not including replication, so I will not be dealing with replication to much for the next few months.&lt;/p&gt;</comment>
                            <comment id="13451184" author="hossman" created="Sat, 8 Sep 2012 00:35:53 +0000"  >&lt;p&gt;assigning to mark since it sounds like he is actively working on this&lt;/p&gt;</comment>
                            <comment id="13511072" author="markrmiller@gmail.com" created="Thu, 6 Dec 2012 03:51:24 +0000"  >&lt;p&gt;This still an issue you see?&lt;/p&gt;</comment>
                            <comment id="16003494" author="mdrob" created="Tue, 9 May 2017 20:33:30 +0000"  >&lt;p&gt;Lots of additional work has gone into this in other issues, I think it&apos;s probably ok to close?&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12539788" name="recovery_in_progress.png" size="73993" author="steff1193" created="Wed, 8 Aug 2012 07:47:44 +0000"/>
                            <attachment id="12539789" name="recovery_start_finish.log" size="35601" author="steff1193" created="Wed, 8 Aug 2012 07:47:44 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>243263</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 28 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i03d07:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>17540</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12313321" key="com.atlassian.jira.toolkit:message">
                        <customfieldname>Solr Mailing List Info</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>