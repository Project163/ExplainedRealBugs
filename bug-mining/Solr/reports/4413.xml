<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 04:42:37 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SOLR-13486] race condition between leader&apos;s &quot;replay on startup&quot; and non-leader&apos;s &quot;recover from leader&quot; can leave replicas out of sync (TestTlogReplayVsRecovery)</title>
                <link>https://issues.apache.org/jira/browse/SOLR-13486</link>
                <project id="12310230" key="SOLR">Solr</project>
                    <description>&lt;p&gt;There is a bug in solr cloud that can result in replicas being out of sync with the leader if:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;The leader has uncommitted docs (in the tlog) that didn&apos;t make it to the replica&lt;/li&gt;
	&lt;li&gt;The leader restarts&lt;/li&gt;
	&lt;li&gt;The replica begins to peer sync from the leader before the leader finishes it&apos;s own tlog replay on startup&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;A &quot;rolling restart&quot; situation is when this is most likeley to affect real world users&lt;/p&gt;

&lt;p&gt;This was first discovered via hard to reproduce TestCloudConsistency failures in jenkins, but that test has since been modified to work around this bug, and a new test &quot;TestTlogReplayVsRecovery&quot; has been added that more aggressively demonstrates this error.&lt;/p&gt;

&lt;p&gt;Original jira description below...&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;I&apos;ve been investigating some jenkins failures from TestCloudConsistency, which at first glance suggest a problem w/replica(s) recovering after a network partition from the leader - but in digging into the logs the root cause acturally seems to be a thread race conditions when a replica (the leader) is first registered...&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;The &lt;tt&gt;ZkContainer.registerInZk(...)&lt;/tt&gt; method (which is called by &lt;tt&gt;CoreContainer.registerCore(...)&lt;/tt&gt; &amp;amp; &lt;tt&gt;CoreContainer.load()&lt;/tt&gt;) is typically run in a background thread (via the &lt;tt&gt;ZkContainer.coreZkRegister&lt;/tt&gt; ExecutorService)&lt;/li&gt;
	&lt;li&gt;&lt;tt&gt;ZkContainer.registerInZk(...)&lt;/tt&gt; delegates to &lt;tt&gt;ZKController.register(...)&lt;/tt&gt; which is ultimately responsible for checking if there are any &quot;old&quot; tlogs on disk, and if so handling the &quot;Replaying tlog for &amp;lt;URL&amp;gt; during startup&quot; logic&lt;/li&gt;
	&lt;li&gt;Because this happens in a background thread, other logic/requests can be handled by this core/replica in the meantime - before it starts (or while in the middle of) replaying the tlogs
	&lt;ul&gt;
		&lt;li&gt;Notably: &lt;b&gt;leader&apos;s that have not yet replayed tlogs on startup will erroneously respond to RTG / Fingerprint / PeerSync requests from other replicas w/incomplete data&lt;/b&gt;&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;...In general, it seems scary / fishy to me that a replica can (aparently) become &lt;b&gt;ACTIVE&lt;/b&gt; before it&apos;s finished it&apos;s &lt;tt&gt;registerInZk&lt;/tt&gt; + &quot;Replaying tlog ... during startup&quot; logic ... particularly since this can happen even for replicas that are/become leaders. It seems like this could potentially cause a whole host of problems, only one of which manifests in this particular test failure:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;&lt;b&gt;BEFORE&lt;/b&gt; replicaX&apos;s &quot;coreZkRegister&quot; thread reaches the &quot;Replaying tlog ... during startup&quot; check:
	&lt;ul&gt;
		&lt;li&gt;replicaX can recognize (via zk terms) that it should be the leader(X)&lt;/li&gt;
		&lt;li&gt;this leaderX can then instruct some other replicaY to recover from it&lt;/li&gt;
		&lt;li&gt;replicaY can send RTG / PeerSync / FetchIndex requests to the leaderX (either on it&apos;s own volition, or because it was instructed to by leaderX) in an attempt to recover
		&lt;ul&gt;
			&lt;li&gt;the responses to these recovery requests will not include updates in the tlog files that existed on leaderX prior to startup that hvae not yet been replayed&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;AFTER&lt;/b&gt; replicaY has finished it&apos;s recovery, leaderX&apos;s &quot;Replaying tlog ... during startup&quot; can finish
	&lt;ul&gt;
		&lt;li&gt;replicaY now thinks it is in sync with leaderX, but leaderX has (replayed) updates the other replicas know nothing about&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;
</description>
                <environment></environment>
        <key id="13234669">SOLR-13486</key>
            <summary>race condition between leader&apos;s &quot;replay on startup&quot; and non-leader&apos;s &quot;recover from leader&quot; can leave replicas out of sync (TestTlogReplayVsRecovery)</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="hossman">Chris M. Hostetter</reporter>
                        <labels>
                    </labels>
                <created>Tue, 21 May 2019 16:53:07 +0000</created>
                <updated>Fri, 24 Jul 2020 11:23:57 +0000</updated>
                                                                                <due></due>
                            <votes>2</votes>
                                    <watches>12</watches>
                                                                                                                <comments>
                            <comment id="16845023" author="hossman" created="Tue, 21 May 2019 16:54:27 +0000"  >
&lt;p&gt;I&apos;m attaching the full apache_Lucene-Solr-BadApples-NightlyTests-master_61.log.txt jenkins log file.&lt;/p&gt;

&lt;p&gt;Here are the most pertinant entries, in chronological order, annotated...&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;# collection creation...

   [junit4]   2&amp;gt; 1700487 INFO  (qtp888794333-18006) [n:127.0.0.1:33720_solr    ] o.a.s.h.a.CollectionsHandler Invoked Collection Action :create with params pullReplicas=0&amp;amp;name=outOfSyncReplicasCannotBecomeLeader-false&amp;amp;nrtReplicas=3&amp;amp;action=CREATE&amp;amp;numShards=1&amp;amp;tlogReplicas=0&amp;amp;createNodeSet=&amp;amp;wt=javabin&amp;amp;version=2 and sendToOCPQueue=true

# core_node2 is created on port 34940 and becomes shard1 leader...

   [junit4]   2&amp;gt; 1701016 INFO  (qtp232935839-17864) [n:127.0.0.1:34940_solr    x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] o.a.s.h.a.CoreAdminOperation core create command qt=/admin/cores&amp;amp;coreNodeName=core_node2&amp;amp;collection.configName=outOfSyncReplicasCannotBecomeLeader-false.AUTOCREATED&amp;amp;name=outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1&amp;amp;action=CREATE&amp;amp;collection=outOfSyncReplicasCannotBecomeLeader-false&amp;amp;shard=shard1&amp;amp;wt=javabin&amp;amp;version=2&amp;amp;replicaType=NRT
...
   [junit4]   2&amp;gt; 1701604 INFO  (qtp232935839-17864) [n:127.0.0.1:34940_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] o.a.s.c.ZkController I am the leader, no recovery necessary

# docs #1-3 are added/commited to shard1 and successfully federated by core_node2 to the other 2 replicas...

   [junit4]   2&amp;gt; 1704043 INFO  (qtp1957279984-17955) [n:127.0.0.1:40376_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node6 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n5] o.a.s.u.p.LogUpdateProcessorFactory [outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n5]  webapp=/solr path=/update params={update.distrib=FROMLEADER&amp;amp;distrib.from=http://127.0.0.1:34940/solr/outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1/&amp;amp;wt=javabin&amp;amp;version=2}{add=[1 (1633216782169800704), 2 (1633216782171897856), 3 (1633216782172946432)]} 0 38
   [junit4]   2&amp;gt; 1704044 INFO  (qtp456989520-17911) [n:127.0.0.1:35087_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3] o.a.s.u.p.LogUpdateProcessorFactory [outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3]  webapp=/solr path=/update params={update.distrib=FROMLEADER&amp;amp;distrib.from=http://127.0.0.1:34940/solr/outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1/&amp;amp;wt=javabin&amp;amp;version=2}{add=[1 (1633216782169800704), 2 (1633216782171897856), 3 (1633216782172946432)]} 0 38
   [junit4]   2&amp;gt; 1704045 INFO  (qtp232935839-17864) [n:127.0.0.1:34940_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] o.a.s.u.p.LogUpdateProcessorFactory [outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1]  webapp=/solr path=/update params={wt=javabin&amp;amp;version=2}{add=[1 (1633216782169800704), 2 (1633216782171897856), 3 (1633216782172946432)]} 0 43
   [junit4]   2&amp;gt; 1704047 INFO  (qtp232935839-17861) [n:127.0.0.1:34940_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] o.a.s.u.DirectUpdateHandler2 start commit{_version_=1633216782216986624,optimize=false,openSearcher=true,waitSearcher=true,expungeDeletes=false,softCommit=false,prepareCommit=false}
   [junit4]   2&amp;gt; 1704047 INFO  (qtp232935839-17861) [n:127.0.0.1:34940_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] o.a.s.u.SolrIndexWriter Calling setCommitData with IW:org.apache.solr.update.SolrIndexWriter@74aef16f commitCommandVersion:1633216782216986624
   [junit4]   2&amp;gt; 1704049 INFO  (qtp456989520-17912) [n:127.0.0.1:35087_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3] o.a.s.u.DirectUpdateHandler2 start commit{_version_=1633216782219083776,optimize=false,openSearcher=true,waitSearcher=true,expungeDeletes=false,softCommit=false,prepareCommit=false}
   [junit4]   2&amp;gt; 1704049 INFO  (qtp456989520-17912) [n:127.0.0.1:35087_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3] o.a.s.u.SolrIndexWriter Calling setCommitData with IW:org.apache.solr.update.SolrIndexWriter@7bfb5d7d commitCommandVersion:1633216782219083776
   [junit4]   2&amp;gt; 1704051 INFO  (qtp1957279984-17957) [n:127.0.0.1:40376_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node6 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n5] o.a.s.u.DirectUpdateHandler2 start commit{_version_=1633216782221180928,optimize=false,openSearcher=true,waitSearcher=true,expungeDeletes=false,softCommit=false,prepareCommit=false}
   [junit4]   2&amp;gt; 1704052 INFO  (qtp1957279984-17957) [n:127.0.0.1:40376_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node6 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n5] o.a.s.u.SolrIndexWriter Calling setCommitData with IW:org.apache.solr.update.SolrIndexWriter@56ea55e1 commitCommandVersion:1633216782221180928

# the test introduces network partitions (via proxy close) and sends doc #4 to the leader
# which fails to forward the update to the other replicas, but does successfully &quot;accept&quot; the add

   [junit4]   2&amp;gt; 1704132 WARN  (TEST-TestCloudConsistency.testOutOfSyncReplicasCannotBecomeLeader-seed#[C551697D6DBDF3BB]) [    ] o.a.s.c.s.c.SocketProxy Closing 4 connections to: http://127.0.0.1:34940/solr, target: http://127.0.0.1:46348/solr
   [junit4]   2&amp;gt; 1704133 WARN  (TEST-TestCloudConsistency.testOutOfSyncReplicasCannotBecomeLeader-seed#[C551697D6DBDF3BB]) [    ] o.a.s.c.s.c.SocketProxy Closing 2 connections to: http://127.0.0.1:35087/solr, target: http://127.0.0.1:45490/solr
   [junit4]   2&amp;gt; 1704133 WARN  (TEST-TestCloudConsistency.testOutOfSyncReplicasCannotBecomeLeader-seed#[C551697D6DBDF3BB]) [    ] o.a.s.c.s.c.SocketProxy Closing 3 connections to: http://127.0.0.1:40376/solr, target: http://127.0.0.1:35605/solr
   [junit4]   2&amp;gt; 1704142 ERROR (updateExecutor-6525-thread-2-processing-x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1 r:core_node2 null n:127.0.0.1:34940_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1) [n:127.0.0.1:34940_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] o.a.s.u.ErrorReportingConcurrentUpdateSolrClient Error when calling SolrCmdDistributor$Req: cmd=add{,id=(null)}; node=StdNode: http://127.0.0.1:35087/solr/outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3/ to http://127.0.0.1:35087/solr/outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3/
   [junit4]   2&amp;gt;           =&amp;gt; java.io.IOException: java.net.ConnectException: Connection refused
...
   [junit4]   2&amp;gt; 1704143 ERROR (updateExecutor-6525-thread-1-processing-x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1 r:core_node2 null n:127.0.0.1:34940_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1) [n:127.0.0.1:34940_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] o.a.s.u.ErrorReportingConcurrentUpdateSolrClient Error when calling SolrCmdDistributor$Req: cmd=add{,id=(null)}; node=StdNode: http://127.0.0.1:40376/solr/outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n5/ to http://127.0.0.1:40376/solr/outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n5/
   [junit4]   2&amp;gt;           =&amp;gt; java.io.IOException: java.net.ConnectException: Connection refused
...
   [junit4]   2&amp;gt; 1704145 INFO  (qtp232935839-17862) [n:127.0.0.1:34940_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] o.a.s.u.SolrCmdDistributor SolrCmdDistributor found 2 errors
...
   [junit4]   2&amp;gt; 1704151 INFO  (qtp232935839-17862) [n:127.0.0.1:34940_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] o.a.s.c.ZkShardTerms Successful update of terms at /collections/outOfSyncReplicasCannotBecomeLeader-false/terms/shard1 to Terms{values={core_node6=1, core_node2=2, core_node4=1}, version=4}
   [junit4]   2&amp;gt; 1704151 INFO  (qtp232935839-17862) [n:127.0.0.1:34940_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] o.a.s.u.p.LogUpdateProcessorFactory [outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1]  webapp=/solr path=/update params={wt=javabin&amp;amp;version=2}{add=[4 (1633216782312407040)]} 0 13

# replica&apos;s detect that they need to go into recovery (via ZK terms), but this is throttled for now...

   [junit4]   2&amp;gt; 1704152 INFO  (zkCallback-6553-thread-3) [    ] o.a.s.c.RecoveringCoreTermWatcher Start recovery on core_node4 because core&apos;s term is less than leader&apos;s term
   [junit4]   2&amp;gt; 1704152 INFO  (updateExecutor-6548-thread-1) [n:127.0.0.1:35087_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3] o.a.s.u.DefaultSolrCoreState Running recovery
   [junit4]   2&amp;gt; 1704152 INFO  (updateExecutor-6548-thread-1) [n:127.0.0.1:35087_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3] o.a.s.c.ActionThrottle Throttling recovery attempts - waiting for 8280ms
   [junit4]   2&amp;gt; 1704152 INFO  (zkCallback-6576-thread-3) [    ] o.a.s.c.RecoveringCoreTermWatcher Start recovery on core_node6 because core&apos;s term is less than leader&apos;s term
   [junit4]   2&amp;gt; 1704152 INFO  (updateExecutor-6571-thread-1) [n:127.0.0.1:40376_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node6 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n5] o.a.s.u.DefaultSolrCoreState Running recovery
   [junit4]   2&amp;gt; 1704152 INFO  (updateExecutor-6571-thread-1) [n:127.0.0.1:40376_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node6 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n5] o.a.s.c.ActionThrottle Throttling recovery attempts - waiting for 9013ms


# shard leader&apos;s node is shut down...

   [junit4]   2&amp;gt; 1704153 INFO  (TEST-TestCloudConsistency.testOutOfSyncReplicasCannotBecomeLeader-seed#[C551697D6DBDF3BB]) [    ] o.a.s.c.CoreContainer Shutting down CoreContainer instance=1473387280
   [junit4]   2&amp;gt; 1704153 INFO  (TEST-TestCloudConsistency.testOutOfSyncReplicasCannotBecomeLeader-seed#[C551697D6DBDF3BB]) [    ] o.a.s.c.ZkController Remove node as live in ZooKeeper:/live_nodes/127.0.0.1:34940_solr
...

# test re-enables sock proxy &amp;amp; re-starts port 34940 (hosting the shard leader) ...

   [junit4]   2&amp;gt; 1704219 INFO  (TEST-TestCloudConsistency.testOutOfSyncReplicasCannotBecomeLeader-seed#[C551697D6DBDF3BB]) [    ] o.a.s.c.s.c.SocketProxy Re-opening connectivity to http://127.0.0.1:35087/solr
   [junit4]   2&amp;gt; 1704219 INFO  (TEST-TestCloudConsistency.testOutOfSyncReplicasCannotBecomeLeader-seed#[C551697D6DBDF3BB]) [    ] o.a.s.c.s.c.SocketProxy Re-opening connectivity to http://127.0.0.1:40376/solr
...
   [junit4]   2&amp;gt; 1714228 INFO  (TEST-TestCloudConsistency.testOutOfSyncReplicasCannotBecomeLeader-seed#[C551697D6DBDF3BB]) [    ] o.a.s.c.s.c.SocketProxy Re-opening connectivity to http://127.0.0.1:34940/solr
   [junit4]   2&amp;gt; 1714228 INFO  (TEST-TestCloudConsistency.testOutOfSyncReplicasCannotBecomeLeader-seed#[C551697D6DBDF3BB]) [    ] o.a.s.c.s.e.JettySolrRunner Start Jetty (original configured port=0)
...
   [junit4]   2&amp;gt; 1714331 INFO  (TEST-TestCloudConsistency.testOutOfSyncReplicasCannotBecomeLeader-seed#[C551697D6DBDF3BB]) [    ] o.a.s.c.ZkController Register node as live in ZooKeeper:/live_nodes/127.0.0.1:34940_solr


# core_node2 is re-loaded, and it&apos;s &quot;coreZkRegister&quot; thread starts up...

   [junit4]   2&amp;gt; 1715118 INFO  (coreZkRegister-5116-thread-1-processing-n:127.0.0.1:34940_solr x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1 c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2) [n:127.0.0.1:34940_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] o.a.s.c.ShardLeaderElectionContextBase make sure parent is created /collections/outOfSyncReplicasCannotBecomeLeader-false/leaders/shard1

# core_node2 realizes it should be the leader, and other replicas should sync to it

   [junit4]   2&amp;gt; 1719735 INFO  (zkCallback-6627-thread-1) [n:127.0.0.1:34940_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] o.a.s.c.ShardLeaderElectionContext I may be the new leader - try and sync
...
   [junit4]   2&amp;gt; 1722241 INFO  (zkCallback-6627-thread-1) [n:127.0.0.1:34940_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] o.a.s.u.PeerSync PeerSync: core=outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1 url=http://127.0.0.1:34940/solr  Received 3 versions from http://127.0.0.1:40376/solr/outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n5/ fingerprint:null
   [junit4]   2&amp;gt; 1722242 INFO  (qtp1957279984-17952) [n:127.0.0.1:40376_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node6 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n5] o.a.s.c.S.Request [outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n5]  webapp=/solr path=/get params={distrib=false&amp;amp;qt=/get&amp;amp;checkCanHandleVersionRanges=false&amp;amp;wt=javabin&amp;amp;version=2} status=0 QTime=0
   [junit4]   2&amp;gt; 1722243 INFO  (zkCallback-6627-thread-1) [n:127.0.0.1:34940_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] o.a.s.u.PeerSync PeerSync: core=outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1 url=http://127.0.0.1:34940/solr  No additional versions requested. ourHighThreshold=1633216782312407040 otherLowThreshold=1633216782169800704 ourHighest=1633216782312407040 otherHighest=1633216782172946432
   [junit4]   2&amp;gt; 1722243 INFO  (zkCallback-6627-thread-1) [n:127.0.0.1:34940_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] o.a.s.u.PeerSync PeerSync: core=outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1 url=http://127.0.0.1:34940/solr  Received 3 versions from http://127.0.0.1:35087/solr/outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3/ fingerprint:null
   [junit4]   2&amp;gt; 1722244 INFO  (qtp456989520-17912) [n:127.0.0.1:35087_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3] o.a.s.c.S.Request [outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3]  webapp=/solr path=/get params={distrib=false&amp;amp;qt=/get&amp;amp;checkCanHandleVersionRanges=false&amp;amp;wt=javabin&amp;amp;version=2} status=0 QTime=0
   [junit4]   2&amp;gt; 1722244 INFO  (zkCallback-6627-thread-1) [n:127.0.0.1:34940_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] o.a.s.u.PeerSync PeerSync: core=outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1 url=http://127.0.0.1:34940/solr  No additional versions requested. ourHighThreshold=1633216782312407040 otherLowThreshold=1633216782169800704 ourHighest=1633216782312407040 otherHighest=1633216782172946432
   [junit4]   2&amp;gt; 1722244 INFO  (zkCallback-6627-thread-1) [n:127.0.0.1:34940_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] o.a.s.u.PeerSync PeerSync: core=outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1 url=http://127.0.0.1:34940/solr DONE. sync succeeded
   [junit4]   2&amp;gt; 1722244 INFO  (zkCallback-6627-thread-1) [n:127.0.0.1:34940_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] o.a.s.c.SyncStrategy Sync Success - now sync replicas to me
   [junit4]   2&amp;gt; 1722244 INFO  (zkCallback-6627-thread-1) [n:127.0.0.1:34940_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] o.a.s.c.SyncStrategy http://127.0.0.1:34940/solr/outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1/: try and ask http://127.0.0.1:35087/solr/outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3/ to sync
   [junit4]   2&amp;gt; 1722244 INFO  (zkCallback-6627-thread-1) [n:127.0.0.1:34940_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] o.a.s.c.SyncStrategy http://127.0.0.1:34940/solr/outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1/: try and ask http://127.0.0.1:40376/solr/outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n5/ to sync

# core_node4 attempts recovery, and via PeerSync / fingerprint comparisons w/leader (core_node2)
# decides it&apos;s up to date and doesn&apos;t need to do anything...

   [junit4]   2&amp;gt; 1723046 INFO  (recoveryExecutor-6550-thread-1-processing-n:127.0.0.1:35087_solr x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3 c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4) [n:127.0.0.1:35087_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3] o.a.s.u.PeerSyncWithLeader PeerSync: core=outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3 url=http://127.0.0.1:35087/solr START leader=http://127.0.0.1:34940/solr/outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1/ nUpdates=100
   [junit4]   2&amp;gt; 1723048 INFO  (recoveryExecutor-6550-thread-1-processing-n:127.0.0.1:35087_solr x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3 c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4) [n:127.0.0.1:35087_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3] o.a.s.u.IndexFingerprint IndexFingerprint millis:0.0 result:{maxVersionSpecified=9223372036854775807, maxVersionEncountered=1633216782172946432, maxInHash=1633216782172946432, versionsHash=7090876931860977394, numVersions=3, numDocs=3, maxDoc=3}
   [junit4]   2&amp;gt; 1723048 INFO  (recoveryExecutor-6550-thread-1-processing-n:127.0.0.1:35087_solr x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3 c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4) [n:127.0.0.1:35087_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3] o.a.s.u.PeerSyncWithLeader Fingerprint comparison result: 0
   [junit4]   2&amp;gt; 1723048 INFO  (recoveryExecutor-6550-thread-1-processing-n:127.0.0.1:35087_solr x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3 c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4) [n:127.0.0.1:35087_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3] o.a.s.u.DirectUpdateHandler2 start commit{,optimize=false,openSearcher=true,waitSearcher=true,expungeDeletes=false,softCommit=false,prepareCommit=false}
   [junit4]   2&amp;gt; 1723049 INFO  (recoveryExecutor-6550-thread-1-processing-n:127.0.0.1:35087_solr x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3 c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4) [n:127.0.0.1:35087_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3] o.a.s.u.DirectUpdateHandler2 No uncommitted changes. Skipping IW.commit.


# Only *NOW* does the &quot;coreZkRegister&quot; thread for core_node2 (the leader) get around to
# taking care of &quot;Replaying tlog for ... during startup&quot;
#
# This is where the (re-started) core_node2 (leader) becomes aware of doc #4
# Which core_node4 will never learn about because it already &quot;successfully&quot; finished recovery

   [junit4]   2&amp;gt; 1723134 INFO  (coreZkRegister-5116-thread-1-processing-n:127.0.0.1:34940_solr x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1 c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2) [n:127.0.0.1:34940_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] o.a.s.c.ZkController Replaying tlog for http://127.0.0.1:34940/solr/outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1/ during startup... NOTE: This can take a while.
   [junit4]   2&amp;gt; 1723134 WARN  (recoveryExecutor-5127-thread-1-processing-n:127.0.0.1:34940_solr x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1 c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2) [n:127.0.0.1:34940_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] o.a.s.u.UpdateLog Starting log replay tlog{file=/x1/jenkins/jenkins-slave/workspace/Lucene-Solr-BadApples-NightlyTests-master/checkout/solr/build/solr-core/test/J0/temp/solr.cloud.TestCloudConsistency_C551697D6DBDF3BB-001/tempDir-002/node4/outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1/data/tlog/tlog.0000000000000000003 refcount=2} active=false starting pos=0 inSortedOrder=false
...
   [junit4]   2&amp;gt; 1723416 INFO  (recoveryExecutor-5127-thread-1-processing-n:127.0.0.1:34940_solr x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1 c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2) [n:127.0.0.1:34940_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] o.a.s.u.p.LogUpdateProcessorFactory [outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] {add=[4 (1633216782312407040)]} 0 282
   [junit4]   2&amp;gt; 1723416 WARN  (recoveryExecutor-5127-thread-1-processing-n:127.0.0.1:34940_solr x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1 c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2) [n:127.0.0.1:34940_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] o.a.s.u.UpdateLog Log replay finished. recoveryInfo=RecoveryInfo{adds=1 deletes=0 deleteByQuery=0 errors=0 positionOfStart=0}

# core_node6 get&apos;s lucky: it&apos;s PeerSync / fingerprint checks with the leader (core_node2)
# happen *after* core_node2 finishes it&apos;s &quot;Replaying tlog for ... during startup&quot; so it does learn about
# doc #4

   [junit4]   2&amp;gt; 1723775 INFO  (recoveryExecutor-6573-thread-1-processing-n:127.0.0.1:40376_solr x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n5 c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node6) [n:127.0.0.1:40376_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node6 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n5] o.a.s.c.RecoveryStrategy Attempting to PeerSync from [http://127.0.0.1:34940/solr/outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1/] - recoveringAfterStartup=[false]
   [junit4]   2&amp;gt; 1723775 INFO  (recoveryExecutor-6573-thread-1-processing-n:127.0.0.1:40376_solr x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n5 c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node6) [n:127.0.0.1:40376_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node6 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n5] o.a.s.u.PeerSyncWithLeader PeerSync: core=outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n5 url=http://127.0.0.1:40376/solr START leader=http://127.0.0.1:34940/solr/outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1/ nUpdates=100
   [junit4]   2&amp;gt; 1723777 INFO  (qtp1060679012-18152) [n:127.0.0.1:34940_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] o.a.s.u.IndexFingerprint IndexFingerprint millis:0.0 result:{maxVersionSpecified=9223372036854775807, maxVersionEncountered=1633216782312407040, maxInHash=1633216782312407040, versionsHash=5716225272730900171, numVersions=4, numDocs=4, maxDoc=3}

# the test ultimately fails because core_node4 doesn&apos;t know that doc #4 should exist...

   [junit4]   2&amp;gt; 1723970 INFO  (qtp456989520-17914) [n:127.0.0.1:35087_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3] o.a.s.c.S.Request [outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3]  webapp=/solr path=/get params={distrib=false&amp;amp;qt=/get&amp;amp;id=4&amp;amp;wt=javabin&amp;amp;version=2} status=0 QTime=0
...
   [junit4] FAILURE 35.8s J0 | TestCloudConsistency.testOutOfSyncReplicasCannotBecomeLeader &amp;lt;&amp;lt;&amp;lt;
   [junit4]    &amp;gt; Throwable #1: java.lang.AssertionError: Doc with id=4 not found in http://127.0.0.1:35087/solr/outOfSyncReplicasCannotBecomeLeader-false due to: Path not found: /id; rsp={doc=null}


&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
</comment>
                            <comment id="16845034" author="hossman" created="Tue, 21 May 2019 17:00:15 +0000"  >&lt;p&gt;I&apos;m sure there are good reasons why the &lt;tt&gt;registerInZk&lt;/tt&gt; is done in a background thread, but it seems like there needs to be some rethinking of how to reconcile all the logic related to the overlap of of a replica that &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;has not finished replay&lt;/li&gt;
	&lt;li&gt;has determined it should be shard leader&lt;/li&gt;
	&lt;li&gt;is ACTIVE&lt;/li&gt;
	&lt;li&gt;is responding to peersync/fingerprint/fetchindex requests&lt;/li&gt;
	&lt;li&gt;is responding to updates&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="16845052" author="hossman" created="Tue, 21 May 2019 17:18:05 +0000"  >&lt;p&gt;Another example in apache_Lucene-Solr-BadApples-Tests-8.x_102.log.txt&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;...

# core_node2 is restarted and realizes itshould be the leader...

   [junit4]   2&amp;gt; 453455 INFO  (qtp90823460-8520) [n:127.0.0.1:43027_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] o.a.s.c.ZkShardTerms Successful update of terms at /collections/outOfSyncReplicasCannotBecomeLeader-false/terms/shard1 to Terms{values={core_node2=0}, version=0}
   [junit4]   2&amp;gt; 453455 INFO  (qtp90823460-8520) [n:127.0.0.1:43027_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] o.a.s.c.ShardLeaderElectionContextBase make sure parent is created /collections/outOfSyncReplicasCannotBecomeLeader-false/leaders/shard1
   [junit4]   2&amp;gt; 453460 INFO  (qtp90823460-8520) [n:127.0.0.1:43027_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] o.a.s.c.ShardLeaderElectionContext Enough replicas found to continue.
   [junit4]   2&amp;gt; 453460 INFO  (qtp90823460-8520) [n:127.0.0.1:43027_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] o.a.s.c.ShardLeaderElectionContext I may be the new leader - try and sync
   [junit4]   2&amp;gt; 453460 INFO  (qtp90823460-8520) [n:127.0.0.1:43027_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] o.a.s.c.SyncStrategy Sync replicas to https://127.0.0.1:43027/solr/outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1/
   [junit4]   2&amp;gt; 453460 INFO  (qtp90823460-8520) [n:127.0.0.1:43027_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] o.a.s.c.SyncStrategy Sync Success - now sync replicas to me


# core_node4 tries to peer sync...

   [junit4]   2&amp;gt; 455090 INFO  (recoveryExecutor-2903-thread-1-processing-n:127.0.0.1:38151_solr x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3 c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4) [n:127.0.0.1:38151_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3] o.a.s.c.RecoveryStrategy Attempting to PeerSync from [https://127.0.0.1:43027/solr/outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1/] - recoveringAfterStartup=[true]
   [junit4]   2&amp;gt; 455091 WARN  (recoveryExecutor-2903-thread-1-processing-n:127.0.0.1:38151_solr x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3 c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4) [n:127.0.0.1:38151_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3] o.a.s.u.PeerSyncWithLeader no frame of reference to tell if we&apos;ve missed updates
   [junit4]   2&amp;gt; 455091 INFO  (recoveryExecutor-2903-thread-1-processing-n:127.0.0.1:38151_solr x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3 c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4) [n:127.0.0.1:38151_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3] o.a.s.c.RecoveryStrategy PeerSync Recovery was not successful - trying replication.

# core_node4&apos;s recovery didn&apos;t work, so it uses replication...

   [junit4]   2&amp;gt; 455091 INFO  (recoveryExecutor-2903-thread-1-processing-n:127.0.0.1:38151_solr x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3 c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4) [n:127.0.0.1:38151_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3] o.a.s.c.RecoveryStrategy Starting Replication Recovery.
   [junit4]   2&amp;gt; 455091 INFO  (recoveryExecutor-2903-thread-1-processing-n:127.0.0.1:38151_solr x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3 c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4) [n:127.0.0.1:38151_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3] o.a.s.c.RecoveryStrategy Attempting to replicate from [https://127.0.0.1:43027/solr/outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1/].
...
   [junit4]   2&amp;gt; 455162 INFO  (recoveryExecutor-2903-thread-1-processing-n:127.0.0.1:38151_solr x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3 c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4) [n:127.0.0.1:38151_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3] o.a.s.h.IndexFetcher New index in Master. Deleting mine...
...

   [junit4]   2&amp;gt; 455168 INFO  (recoveryExecutor-2903-thread-1-processing-n:127.0.0.1:38151_solr x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3 c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4) [n:127.0.0.1:38151_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3] o.a.s.c.RecoveryStrategy No replay needed.
   [junit4]   2&amp;gt; 455168 INFO  (recoveryExecutor-2903-thread-1-processing-n:127.0.0.1:38151_solr x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3 c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4) [n:127.0.0.1:38151_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3] o.a.s.c.RecoveryStrategy Replication Recovery was successful.
   [junit4]   2&amp;gt; 455168 INFO  (recoveryExecutor-2903-thread-1-processing-n:127.0.0.1:38151_solr x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3 c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4) [n:127.0.0.1:38151_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3] o.a.s.c.RecoveryStrategy Registering as Active after recovery.
   [junit4]   2&amp;gt; 455169 INFO  (recoveryExecutor-2903-thread-1-processing-n:127.0.0.1:38151_solr x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3 c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4) [n:127.0.0.1:38151_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3] o.a.s.c.RecoveryStrategy Updating version bucket highest from index after successful recovery.
   [junit4]   2&amp;gt; 455169 INFO  (recoveryExecutor-2903-thread-1-processing-n:127.0.0.1:38151_solr x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3 c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4) [n:127.0.0.1:38151_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3] o.a.s.u.UpdateLog Could not find max version in index or recent updates, using new clock 1633646091644698624
   [junit4]   2&amp;gt; 455171 INFO  (recoveryExecutor-2903-thread-1-processing-n:127.0.0.1:38151_solr x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3 c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4) [n:127.0.0.1:38151_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3] o.a.s.c.RecoveryStrategy Finished recovery process, successful=[true]


# only now does (leader) core_node2 get around to replaying it&apos;s tlogs
# &quot;on startup&quot; HA!

   [junit4]   2&amp;gt; 475523 INFO  (coreZkRegister-2758-thread-1-processing-n:127.0.0.1:43027_solr x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1 c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2) [n:127.0.0.1:43027_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] o.a.s.c.ZkController Replaying tlog for https://127.0.0.1:43027/solr/outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1/ during startup... NOTE: This can take a while.
...
   [junit4]   2&amp;gt; 475523 WARN  (recoveryExecutor-2769-thread-1-processing-n:127.0.0.1:43027_solr x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1 c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2) [n:127.0.0.1:43027_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] o.a.s.u.UpdateLog Starting log replay tlog{file=/x1/jenkins/jenkins-slave/workspace/Lucene-Solr-BadApples-Tests-8.x/solr/build/solr-core/test/J0/temp/solr.cloud.TestCloudConsistency_B3AA42F6379701AA-001/tempDir-001/node2/outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1/data/tlog/tlog.0000000000000000003 refcount=2} active=false starting pos=0 inSortedOrder=false
...
   [junit4]   2&amp;gt; 475797 WARN  (recoveryExecutor-2769-thread-1-processing-n:127.0.0.1:43027_solr x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1 c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2) [n:127.0.0.1:43027_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1] o.a.s.u.UpdateLog Log replay finished. recoveryInfo=RecoveryInfo{adds=1 deletes=0 deleteByQuery=0 errors=0 positionOfStart=0}



# core_node4 on port 38151 never learns that doc #4 exists

   [junit4]   2&amp;gt; 476117 INFO  (qtp933340360-8561) [n:127.0.0.1:38151_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3] o.a.s.c.S.Request [outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3]  webapp=/solr path=/get params={distrib=false&amp;amp;qt=/get&amp;amp;id=4&amp;amp;wt=javabin&amp;amp;version=2} status=0 QTime=0
...
   [junit4] FAILURE 35.6s J0 | TestCloudConsistency.testOutOfSyncReplicasCannotBecomeLeader &amp;lt;&amp;lt;&amp;lt;
   [junit4]    &amp;gt; Throwable #1: java.lang.AssertionError: Doc with id=4 not found in https://127.0.0.1:38151/solr/outOfSyncReplicasCannotBecomeLeader-false due to: Path not found: /id; rsp={doc=null}
   [junit4]    &amp;gt;        at __randomizedtesting.SeedInfo.seed([B3AA42F6379701AA:CD4162E6F4F00E90]:0)

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="16887575" author="caomanhdat" created="Thu, 18 Jul 2019 03:14:45 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hossman&quot; class=&quot;user-hover&quot; rel=&quot;hossman&quot;&gt;hossman&lt;/a&gt; it is indeed a serious problem. Should we introduce a lock/flag to prevent /replication if tlog replay is not finished? (&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=shalin&quot; class=&quot;user-hover&quot; rel=&quot;shalin&quot;&gt;shalin&lt;/a&gt; wdyt?)&lt;/p&gt;</comment>
                            <comment id="17004269" author="dweiss" created="Fri, 27 Dec 2019 16:39:45 +0000"  >&lt;p&gt;I hit this very often on Linux (org.apache.solr.cloud.TestCloudConsistency) and this prevents regular sanity-checks of the gradle branch. Is there anything that can be done about it?&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
org.apache.solr.cloud.TestCloudConsistency &amp;gt; testOutOfSyncReplicasCannotBecomeLeader FAILED
    java.lang.AssertionError: Doc with id=4 not found in http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:40399/solr/outOfSyncReplicasCannotBecomeLeader-&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; due to: Path not found: /id; rsp={doc=&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;}
&lt;/span&gt;        at __randomizedtesting.SeedInfo.seed([542A0C718771D678:2AC12C614416D942]:0)
        at org.junit.Assert.fail(Assert.java:88)
        at org.junit.Assert.assertTrue(Assert.java:41)
        at org.apache.solr.cloud.TestCloudConsistency.assertDocExists(TestCloudConsistency.java:285)
        at org.apache.solr.cloud.TestCloudConsistency.assertDocsExistInAllReplicas(TestCloudConsistency.java:269)
        at org.apache.solr.cloud.TestCloudConsistency.testOutOfSyncReplicasCannotBecomeLeader(TestCloudConsistency.java:140)
        at org.apache.solr.cloud.TestCloudConsistency.testOutOfSyncReplicasCannotBecomeLeader(TestCloudConsistency.java:99)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="17004577" author="erickerickson" created="Sat, 28 Dec 2019 21:00:40 +0000"  >&lt;p&gt;At least I can get this to reproduce fairly reliably, digging. Although I&apos;m getting different errors.....&lt;/p&gt;</comment>
                            <comment id="17004878" author="erickerickson" created="Sun, 29 Dec 2019 19:51:21 +0000"  >&lt;p&gt;Making some progress, but it&apos;s slow &apos;cause it takes a long time to generate...&lt;/p&gt;

&lt;p&gt;The cluster setup and teardown is done for each test &apos;cause they&apos;re annotated with `@Before` and `@After`. Changing these to `@BeforeClass` and `@AfterClass`, which at least lessens the confusion. I don&apos;t think this is a real fix given the comments from Chris and Dat, so I&apos;ll see if I can still generate errors with this change.&lt;/p&gt;

&lt;p&gt;I found one issue, spamming people who&apos;ve been in the code for opinions. I don&apos;t think this is the problem Dawid had though.&lt;/p&gt;

&lt;p&gt;SolrClientNodeStateProvider.fetchReplicaMetrics is generating an NPE in some of the tests at this line:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt; v = Utils.getObjectByPath(frsp.nl, &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, Arrays.asList(&lt;span class=&quot;code-quote&quot;&gt;&quot;metrics&quot;&lt;/span&gt;, key));&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt; because `rsp` is null. It&apos;s simple to test for null, and maybe log a warning, but is this indicative of some deeper problem? &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ab&quot; class=&quot;user-hover&quot; rel=&quot;ab&quot;&gt;ab&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=noble&quot; class=&quot;user-hover&quot; rel=&quot;noble&quot;&gt;noble&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=shalin&quot; class=&quot;user-hover&quot; rel=&quot;shalin&quot;&gt;shalin&lt;/a&gt;.&lt;/p&gt;
</comment>
                            <comment id="17004893" author="ab" created="Sun, 29 Dec 2019 20:24:10 +0000"  >&lt;p&gt;The logic in &lt;tt&gt;SolrClientNodeStateProvider.fetchReplicaMetrics&lt;/tt&gt; is a bit convoluted. I don&apos;t think it properly handles repeating IOException-s - it simply exits the &lt;tt&gt;while&lt;/tt&gt; loop in line 199 after 3 iterations and the &lt;tt&gt;rsp&lt;/tt&gt; object may very well be null if there were 3+ repeating IOExceptions.&lt;/p&gt;

&lt;p&gt;IMHO this should produce a warning but should not throw an exception (and certainly not an NPE!), it should simply return - this is the intention of the code at the end of this method.&lt;/p&gt;</comment>
                            <comment id="17004897" author="noble.paul" created="Sun, 29 Dec 2019 20:30:36 +0000"  >&lt;p&gt;Is it that metrics is not up and running and it returned an empty response ? &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ab&quot; class=&quot;user-hover&quot; rel=&quot;ab&quot;&gt;ab&lt;/a&gt; ?&lt;/p&gt;</comment>
                            <comment id="17005057" author="erickerickson" created="Sun, 29 Dec 2019 22:18:26 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ab&quot; class=&quot;user-hover&quot; rel=&quot;ab&quot;&gt;ab&lt;/a&gt;, that&apos;s what I thought too, I&apos;ll do so.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=noble.paul&quot; class=&quot;user-hover&quot; rel=&quot;noble.paul&quot;&gt;noble.paul&lt;/a&gt; What Hoss saw &lt;em&gt;may&lt;/em&gt; be an artifact of the fact that the cluster was being created/destroyed &lt;em&gt;between&lt;/em&gt; tests. So far when I only run a single test at a time I&apos;m not seeing failures, but that&apos;s not very conclusive at this point.&lt;/p&gt;

&lt;p&gt;I intend to run both tests on a couple of machines several thousand times overnight, will post an update.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hossman&quot; class=&quot;user-hover&quot; rel=&quot;hossman&quot;&gt;hossman&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=caomanhdat&quot; class=&quot;user-hover&quot; rel=&quot;caomanhdat&quot;&gt;caomanhdat&lt;/a&gt; Let&apos;s claim that I can&apos;t reproduce this overnight  in 3,000-4,000 runs when both tests are enabled (and note that I was able to fairly reliably make something bad happen within a few hundred runs up until this change). I propose that I open a new JIRA for the test failures and push my changes from there, then rename this one because what I&apos;m doing does nothing to address the analysis Hoss did. It&apos;s still possible that what Hoss saw was a result of destroying/recreating the cluster between tests, but if so that&apos;s something that seems less of a concern.&lt;/p&gt;</comment>
                            <comment id="17005179" author="dweiss" created="Mon, 30 Dec 2019 07:43:09 +0000"  >&lt;p&gt;What&apos;s interesting is that I get the exception I mentioned almost all of the time but only when I run the full test suite. It doesn&apos;t reproduce in isolation.&lt;/p&gt;</comment>
                            <comment id="17005284" author="ab" created="Mon, 30 Dec 2019 12:23:54 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=noble.paul&quot; class=&quot;user-hover&quot; rel=&quot;noble.paul&quot;&gt;noble.paul&lt;/a&gt; &lt;tt&gt;SolrMetricManager&lt;/tt&gt;&#160;is initialized and the &lt;tt&gt;MetricsHandler&lt;/tt&gt; is initialized &lt;b&gt;before&lt;/b&gt; any core is loaded, so in this sense metrics are always &quot;up&quot; - however, if the metrics for this specific core are missing (e.g. because the core is not there) then an empty response is returned.&lt;/p&gt;

&lt;p&gt;However, in this case because we&apos;re getting an NPE I think it was an IOException due to some other error that caused this.&lt;/p&gt;

&lt;p&gt;Can you guys check the logs of the failing runs and see if there are any INFO lines starting with &quot;Error on getting remote info, trying again:&quot;?&lt;/p&gt;</comment>
                            <comment id="17005313" author="dweiss" created="Mon, 30 Dec 2019 13:22:40 +0000"  >&lt;p&gt;I attached an example log report file from the failure, perhaps it&apos;ll be of some help.&lt;/p&gt;</comment>
                            <comment id="17005322" author="erickerickson" created="Mon, 30 Dec 2019 13:48:54 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ab&quot; class=&quot;user-hover&quot; rel=&quot;ab&quot;&gt;ab&lt;/a&gt; Yes, I do get that error on some failed tests. I notice that this can be logged in two places, and the second one (AutoScalingSnitch.getRemoteInfo) throws an explicit exception. My guess is that one should continue to throw a SERVER_ERROR, exception correct?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dweiss&quot; class=&quot;user-hover&quot; rel=&quot;dweiss&quot;&gt;dweiss&lt;/a&gt; I&apos;m seeing two distinct errors now, but neither one of them is exactly yours. One is coming from assertDocExists like yours, but the root cause seems to be &quot;Server refused connection&quot;. Do you see that in your stack traces?&lt;/p&gt;

&lt;p&gt;The other one I&apos;m getting is a timeout waiting for the collection to become active.&lt;/p&gt;

&lt;p&gt;I&apos;m getting about a 130 of these, or about 65 retries for each replica trying to recover:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
109782 ERROR (recoveryExecutor-173-thread-1-processing-n:127.0.0.1:64320_solr x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3 c:outOfSyncReplicasCannotBecomeLeader-&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; s:shard1 r:core_node4) [n:127.0.0.1:64320_solr c:outOfSyncReplicasCannotBecomeLeader-&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; s:shard1 r:core_node4 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3 ] o.a.s.c.RecoveryStrategy Failed to connect leader http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:64314/solr on recovery, &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; again&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This comes from RecoveryStrategy, which pauses half a second then rebuilds the connection, so I&apos;m a bit mystified. I wonder if this has anything to do with the proxy in the test...&lt;/p&gt;
</comment>
                            <comment id="17005323" author="dweiss" created="Mon, 30 Dec 2019 13:50:44 +0000"  >&lt;p&gt;The one I attached is by far the one I see most often. Like I said &amp;#8211; this happens when running full test suite.&lt;/p&gt;</comment>
                            <comment id="17005357" author="erickerickson" created="Mon, 30 Dec 2019 14:26:36 +0000"  >&lt;p&gt;Right. WDYT about you just @Ignore the test for now locally? Hoss&apos; rollups show it failing once in the last week in 30 runs, and trying to run the entire suite enough times to have a hope of showing it fixed will take forever, there&apos;s no sense in you being burdened with this wonkiness.&lt;/p&gt;

&lt;p&gt;So I&apos;ll try to clean up what I can reproduce, then push it to the repo and maybe then go from there.&lt;/p&gt;</comment>
                            <comment id="17005364" author="dweiss" created="Mon, 30 Dec 2019 14:34:16 +0000"  >&lt;p&gt;You can leave that in &amp;#8211; I&apos;ll post more stack traces once I have them, no problem. I just wanted to let you guys know this one has been pretty bad for me recently.&lt;/p&gt;</comment>
                            <comment id="17007007" author="hossman" created="Thu, 2 Jan 2020 18:54:35 +0000"  >&lt;p&gt;Ok, I&apos;m playing catchup on all the comments from the last 2 weeks &#8211; most of which are confusing hte hell out of me &#8211; so forgive me if this is all over the place...&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;This Jira is &lt;b&gt;NOT&lt;/b&gt; a generic tracker for &quot;TestCloudConsistency can sometimes fail&quot; ... it is tracking a very specific bug, that can impact real world solr instances in the wild.&lt;/p&gt;

&lt;p&gt;The fact that it was discovered via TestCloudConsistency, and that that test appears to be (AFAICT) the only test we currently have in the code base for reproducing this &lt;em&gt;code&lt;/em&gt; bug does not mean we should use this jira to discuss every possible failure people encounter w/TestCloudConsistency. Please pay attention to the specifics of the failures you encounter, and when in doubt file a new jira (they can always be linked later, but confusing off topic comments are hard to de-tangle from a jira after the fact)&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=erickerickson&quot; class=&quot;user-hover&quot; rel=&quot;erickerickson&quot;&gt;erickerickson&lt;/a&gt; : if you are seeing NPEs in your logs when you run TestCloudConsistency then that most certainly has nothing to do with the this specific bug being tracked here.&lt;/p&gt;

&lt;p&gt;(BUt it&apos;s impossible to be certain since you dind&apos;t attach any logs or specifics)&lt;/p&gt;

&lt;p&gt;Please open a new jira and move the disucssion of this unrelated problem you have found to that jira. (or open multiple new jiras if you think you&apos;ve found multiple new problems)&lt;/p&gt;
&lt;hr /&gt;
&lt;blockquote&gt;&lt;p&gt;The cluster setup and teardown is done for each test &apos;cause they&apos;re annotated with `@Before` and `@After`. Changing these to `@BeforeClass` and `@AfterClass`, which at least lessens the confusion. I don&apos;t think this is a real fix given the comments from Chris and Dat, so I&apos;ll see if I can still generate errors with this change.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Please &lt;b&gt;DO NOT&lt;/b&gt; change this test to setup/teardown the cluster in &lt;tt&gt;@BeforeClass&lt;/tt&gt; and &lt;tt&gt;@AfterClass&lt;/tt&gt; &#8211; the fact that this test (and others like it) uses a &quot;pristine&quot; cluster for each test method is very specific &#8211; because the test is mucking with the collection nodes, and this way a failure from one test (that might leve the cluster in a bad state) won&apos;t &quot;bleed over&quot; into another test and cause spurrious failures.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;What Hoss saw may be an artifact of the fact that the cluster was being created/destroyed between tests. So far when I only run a single test at a time I&apos;m not seeing failures, but that&apos;s not very conclusive at this point.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The bug being tracked here has &lt;b&gt;NOTHING&lt;/b&gt; to do with a new cluster being recreated for each test. please note all of the logs &amp;amp; analysis i&apos;ve already posted &#8211; everything about this bug has to do with when/how the leader is partitioned &amp;amp; shutdown &lt;em&gt;during&lt;/em&gt; the test, and the race condition thta exists as a result when nodes are trying to recover from that leader while it is trying to recover from it&apos;s tlog.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Dawid&apos;s &lt;tt&gt;org.apache.solr.cloud.TestCloudConsistency.zip&lt;/tt&gt; attachment includes a log that does in fact show the specific problem tracked in this isssue &#8211; note the time stamps of the log messages in the last 2 grep comments...&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;# here&apos;s the replica that is failing to locate doc#4...

hossman@slate:~/tmp$ grep outOfSyncReplicasCannotBecomeLeader-false org.apache.solr.cloud.TestCloudConsistency.html | grep &apos;Doc with id=4 not found&apos;
&amp;lt;pre&amp;gt;java.lang.AssertionError: Doc with id=4 not found in http://127.0.0.1:40399/solr/outOfSyncReplicasCannotBecomeLeader-false due to: Path not found: /id; rsp={doc=null}

# here&apos;s the recovery via replication logging from that replica showing who the leader is...

hossman@slate:~/tmp$ grep outOfSyncReplicasCannotBecomeLeader-false org.apache.solr.cloud.TestCloudConsistency.html | grep &apos;n:127.0.0.1:40399_solr&apos; | grep recoveryExecutor | grep &apos;Attempting to replicate from&apos;
1500569 INFO  (recoveryExecutor-10537-thread-1-processing-n:127.0.0.1:40399_solr x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3 c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4) [n:127.0.0.1:40399_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node4 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n3 ] o.a.s.c.RecoveryStrategy Attempting to replicate from [http://127.0.0.1:33461/solr/outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1/].

# here&apos;s when that leader is doing it&apos;s tlog replay...

hossman@slate:~/tmp$ grep outOfSyncReplicasCannotBecomeLeader-false org.apache.solr.cloud.TestCloudConsistency.html | grep &apos;n:127.0.0.1:33461_solr&apos; | grep &apos;Replaying tlog&apos;
1515376 INFO  (coreZkRegister-10093-thread-1-processing-n:127.0.0.1:33461_solr x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1 c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2) [n:127.0.0.1:33461_solr c:outOfSyncReplicasCannotBecomeLeader-false s:shard1 r:core_node2 x:outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1 ] o.a.s.c.ZkController Replaying tlog for http://127.0.0.1:33461/solr/outOfSyncReplicasCannotBecomeLeader-false_shard1_replica_n1/ during startup... NOTE: This can take a while.

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;...so in that case at least, we are in fact still on topic talking about the correct bug.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dweiss&quot; class=&quot;user-hover&quot; rel=&quot;dweiss&quot;&gt;dweiss&lt;/a&gt; : if you&apos;ve encountered other failures that don&apos;t match this pattern, then please file new jiras for those failure patterns &#8211; I honestly can&apos;t tell if the other failures you mentioned seeing are the same as the other failures Erick mentioned seeing or not.&lt;/p&gt;
&lt;hr /&gt;
&lt;blockquote&gt;&lt;p&gt;I propose that I open a new JIRA for the test failures and push my changes from there, then rename this one because what I&apos;m doing does nothing to address the analysis Hoss did.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=erickerickson&quot; class=&quot;user-hover&quot; rel=&quot;erickerickson&quot;&gt;erickerickson&lt;/a&gt; : you haven&apos;t attached any logs showing the errors you got, nor have you attached any patch showing what changes you are suggesting &#8211; but unless those changes address the underlying bug I identified in the description of this jira, then yes, as i said before: you should open a new jira.&lt;/p&gt;

&lt;p&gt;But i really can&apos;t understand why you think this jira should be renamed &#8211; this jira tracks a bug and the summary/description make it very clera what that bug is. If you have unrelated improvements for the test, or have found an unreltaed bug that manifests in the same test, then those belong in a new jira.&lt;/p&gt;</comment>
                            <comment id="17007143" author="erickerickson" created="Fri, 3 Jan 2020 00:36:43 +0000"  >&lt;p&gt;OK, I&apos;ll talk about it over on 14159. Most of this discussion is me trying to understand where this failure could possibly be coming from anyway.&lt;/p&gt;</comment>
                            <comment id="17007668" author="hossman" created="Fri, 3 Jan 2020 18:02:07 +0000"  >&lt;p&gt;Based on the renewed interest in this issue recently, I set aside some time last night to try and do 2 things:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Fork TestCloudConsistency into a new simplified test that focused on triggering this &quot;replica peer syncs from leader before leader does tlog replay&quot; more reliably
	&lt;ul&gt;
		&lt;li&gt;Start with a simplified 1 shard 2 replica collection&lt;/li&gt;
		&lt;li&gt;add more docs during the partition&lt;/li&gt;
		&lt;li&gt;Use TestInjection to excessively delay the tlog replay&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;Find a work around for this problem in TestCloudConsistency so it passes more reliably&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;While working on #1, I realized there was a question I hadn&apos;t considered during my initial analyssi of the jenkins logs showing this failure:&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;#de350b&quot;&gt;&lt;b&gt;Why does the leader &lt;em&gt;need&lt;/em&gt; to do tlog replay in the test at all?&lt;/b&gt;&lt;/font&gt;&lt;/p&gt;

&lt;p&gt;Even if the client doesn&apos;t expilicitly commit all docs, the &quot;Commit on Close&quot; semantics of Solr&apos;s IndexWriter should ensure that a clean shutdown of the leader means all uncommitted docs in the tlog will be automaticaly committed before the Directory is closed &#8211; nothing in the test &quot;kills&quot; the leader before this should happen.&lt;/p&gt;

&lt;p&gt;So WTF?&lt;/p&gt;

&lt;p&gt;I still haven&apos;t gotten to the bottom of that, but I did confirm that:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;unlike the &quot;normal&quot; adds for docs 1-3, the code path in TestCloudConsistency that was adding doc #4 (during hte network partition) was &lt;b&gt;NOT&lt;/b&gt; committing doc#4.&lt;/li&gt;
	&lt;li&gt;in the test logs where TestCloudConsistency failed, we never see the normal &quot;Committing on IndexWriter close.&quot; i would expect from an oderly shutdown of the leader
	&lt;ul&gt;
		&lt;li&gt;This message does appear in the expected location of the logs for a TestCloudConsistency run that passes&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;At first I thought the problem was some other test class running earlier in the same jenkins JVM mucking with the value of the (public static) &lt;tt&gt;DirectUpdateHandler2.commitOnClose&lt;/tt&gt; prior to the test running &#8211; but even when running a single test class locally, with &lt;tt&gt;DirectUpdateHandler2.commitOnClose = true;&lt;/tt&gt; i was able to continue to reproduce the problem in my new test.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;The attached patch includes:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;my new TestTlogReplayVsRecovery test, marked as &lt;tt&gt;AwaitsFix&lt;/tt&gt; on this jira.&lt;/li&gt;
	&lt;li&gt;a trivial fix/workaround for this bug in TestCloudConsistency
	&lt;ul&gt;
		&lt;li&gt;it now ensures that doc #4 is committed before shutting down the leader &#8211; eliminating the risk of tlog replay affecting the leader when it restarts&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I still want to dig into WTF is happening to cause &quot;commit on close&quot; from not working reliably by default, but that seems like a distinct bug/issue.&lt;/p&gt;

&lt;p&gt;In the mean time this patch should help ensure we don&apos;t get hard to reproduce errors in TestCloudConsistency as a result of this bug; while also providing a new test, that more reliably proves this bug exists, that can be used to test whatever fixes people suggest in the future.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dweiss&quot; class=&quot;user-hover&quot; rel=&quot;dweiss&quot;&gt;dweiss&lt;/a&gt; / &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=erickerickson&quot; class=&quot;user-hover&quot; rel=&quot;erickerickson&quot;&gt;erickerickson&lt;/a&gt; does this patch resolve most of the failures you are seeing from TestCloudConsistency ? If you are still seeing the &quot; &lt;tt&gt;Doc with id=4 not found&lt;/tt&gt; &quot; failures after applying this patch can you please post your logs? (if you are seeing other failures can you please file new jiras and attach the logs there?)&lt;/p&gt;

&lt;p&gt;If there are no objections I&apos;ll check precommit (I&apos;m sure i have some unused imports) and commit soon, then update this jira summary/description to refer to the new test&lt;/p&gt;</comment>
                            <comment id="17007744" author="erickerickson" created="Fri, 3 Jan 2020 20:30:48 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hossman&quot; class=&quot;user-hover&quot; rel=&quot;hossman&quot;&gt;hossman&lt;/a&gt; What I&apos;m seeing is different, and I haven&apos;t a clue what is happening. On the surface, it looks similar because the &quot;assertDocExists&quot; method fails. However, what I can reliably reproduce in asertDocExists is:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Server refused connection at: https:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:49190/solr/outOfSyncReplicasCannotBecomeLeader-&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; org.apache.solr.client.solrj.SolrServerException: Server refused connection at: https://127.0.0.1:49190/solr/outOfSyncReplicasCannotBecomeLeader-&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This problem happens on one of the &lt;em&gt;followers&lt;/em&gt;, BTW, after it apparently successfully syncs after being restarted.&lt;/p&gt;

&lt;p&gt;So that&apos;s what I&apos;m looking at over on the linked JIRA. And I can only reliably make it happen on my MBP, my Mac Pro doesn&apos;t seem to generate this. &quot;Reliably&quot; is a bit of a misnomer, it&apos;s really between 3 and 10 times per thousand tests. I can also make the &quot;connection refused&quot; error happen when beasting only a single test. &lt;/p&gt;

&lt;p&gt;So please go ahead and push anything you think will help for the commit issue. AFAIK, there are multiple issues and it looks like I conflated what I&apos;m seeing with the original problem. I&apos;ll take whatever else I find over to the linked JIRA.&lt;/p&gt;

&lt;p&gt;It&apos;d be really weird if this fix also fixed my other issue...&lt;/p&gt;</comment>
                            <comment id="17007791" author="jira-bot" created="Fri, 3 Jan 2020 22:12:35 +0000"  >&lt;p&gt;Commit 0fac7c1a26395ed21f14e02a471e6350144074c7 in lucene-solr&apos;s branch refs/heads/master from Chris M. Hostetter&lt;br/&gt;
[ &lt;a href=&quot;https://gitbox.apache.org/repos/asf?p=lucene-solr.git;h=0fac7c1&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://gitbox.apache.org/repos/asf?p=lucene-solr.git;h=0fac7c1&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-13486&quot; title=&quot;race condition between leader&amp;#39;s &amp;quot;replay on startup&amp;quot; and non-leader&amp;#39;s &amp;quot;recover from leader&amp;quot; can leave replicas out of sync (TestTlogReplayVsRecovery)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-13486&quot;&gt;SOLR-13486&lt;/a&gt;: Text improvements&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;force a hard commit of all docs in TestCloudConsistency to work around bug in that test&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;add new AwaitsFix&apos;ed TestTlogReplayVsRecovery that more explicitly demonstrates the bug via TestInjection.updateLogReplayRandomPause&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="17007800" author="jira-bot" created="Fri, 3 Jan 2020 22:25:38 +0000"  >&lt;p&gt;Commit a8ab65b1860eaa10605209a060807c292f5497b1 in lucene-solr&apos;s branch refs/heads/branch_8x from Chris M. Hostetter&lt;br/&gt;
[ &lt;a href=&quot;https://gitbox.apache.org/repos/asf?p=lucene-solr.git;h=a8ab65b&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://gitbox.apache.org/repos/asf?p=lucene-solr.git;h=a8ab65b&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-13486&quot; title=&quot;race condition between leader&amp;#39;s &amp;quot;replay on startup&amp;quot; and non-leader&amp;#39;s &amp;quot;recover from leader&amp;quot; can leave replicas out of sync (TestTlogReplayVsRecovery)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-13486&quot;&gt;SOLR-13486&lt;/a&gt;: Text improvements&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;force a hard commit of all docs in TestCloudConsistency to work around bug in that test&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;add new AwaitsFix&apos;ed TestTlogReplayVsRecovery that more explicitly demonstrates the bug via TestInjection.updateLogReplayRandomPause&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;(cherry picked from commit 0fac7c1a26395ed21f14e02a471e6350144074c7)&lt;/p&gt;</comment>
                            <comment id="17008134" author="dweiss" created="Sat, 4 Jan 2020 19:17:18 +0000"  >&lt;p&gt;I can&apos;t claim I understand what&apos;s going on in that code, Chris &amp;#8211; if you think it&apos;s going to help, commit it to master. I&apos;ll let you know if I encounter the same problem again.&lt;/p&gt;</comment>
                            <comment id="17008184" author="erickerickson" created="Sat, 4 Jan 2020 23:10:45 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dweiss&quot; class=&quot;user-hover&quot; rel=&quot;dweiss&quot;&gt;dweiss&lt;/a&gt; It is committed to master, the fix for TestCloudConsistency is an added commit after the 4th doc is added. The new test is @AwaitsFix&apos;ed so shouldn&apos;t affect the Gradle build.&lt;/p&gt;

&lt;p&gt;NOTE: I&apos;ve seen three distinct errors with this test. Since I can&apos;t reliably reproduce the &quot;Doc with id=4 not found in...&quot; message, please let us know if you see it. The other two errors I&apos;ve seen are &quot;Server refused connection&quot; with no mention of the &quot;Doc with id=4 not found...&quot; message and another waiting for the collection to be created...&lt;/p&gt;</comment>
                            <comment id="17013597" author="hossman" created="Sat, 11 Jan 2020 23:07:30 +0000"  >&lt;p&gt;I&apos;ve been revisiting this aspect of my earlier investigation into this bug...&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;font color=&quot;#de350b&quot;&gt;&lt;b&gt;Why does the leader &lt;em&gt;need&lt;/em&gt; to do tlog replay in the test at all?&lt;/b&gt;&lt;/font&gt;&lt;/p&gt;

&lt;p&gt;Even if the client doesn&apos;t expilicitly commit all docs, the &quot;Commit on Close&quot; semantics of Solr&apos;s IndexWriter should ensure that a clean shutdown of the leader means all uncommitted docs in the tlog will be automaticaly committed before the Directory is closed &#8211; nothing in the test &quot;kills&quot; the leader before this should happen.&lt;/p&gt;

&lt;p&gt;So WTF?&lt;/p&gt;

&lt;p&gt;I still haven&apos;t gotten to the bottom of that, but I did confirm that:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;unlike the &quot;normal&quot; adds for docs 1-3, the code path in TestCloudConsistency that was adding doc #4 (during hte network partition) was &lt;b&gt;NOT&lt;/b&gt; committing doc#4.&lt;/li&gt;
	&lt;li&gt;in the test logs where TestCloudConsistency failed, we never see the normal &quot;Committing on IndexWriter close.&quot; i would expect from an oderly shutdown of the leader
	&lt;ul&gt;
		&lt;li&gt;This message does appear in the expected location of the logs for a TestCloudConsistency run that passes&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;At first I thought the problem was some other test class running earlier in the same jenkins JVM mucking with the value of the (public static) &lt;tt&gt;DirectUpdateHandler2.commitOnClose&lt;/tt&gt; prior to the test running &#8211; but even when running a single test class locally, with &lt;tt&gt;DirectUpdateHandler2.commitOnClose = true;&lt;/tt&gt; i was able to continue to reproduce the problem in my new test.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I&apos;ve been trying to get to the bottom of this by modifying &lt;tt&gt;TestTlogReplayVsRecovery&lt;/tt&gt; to explicitly use &lt;tt&gt;DirectUpdateHandler2.commitOnClose = true;&lt;/tt&gt; (as mentioned above) along with more detailed logging from org.apache.solr.update (particularly DUH2)&lt;/p&gt;

&lt;p&gt;The first thing I realized is that there&apos;s a bug in the test where it&apos;s expecting to find &lt;tt&gt;uncommittedDocs + uncommittedDocs&lt;/tt&gt; docs, not just &lt;tt&gt;committedDocs + uncommittedDocs&lt;/tt&gt;, which is why it so easily/quickly failed for me before.&lt;/p&gt;

&lt;p&gt;With that trivial test bug fixed, I have &lt;b&gt;NOT&lt;/b&gt; been able to reproduce the situation that was observed in &lt;tt&gt;TestCloudConsistency&lt;/tt&gt; when this jira was filed: That the leader shutdown (evidently) w/o doing a commitOnClose, necessitating tlog replay on startup, which then happenes after a replica did recovery.&lt;/p&gt;

&lt;p&gt;The only way I can seem to trigger this situation is when &lt;tt&gt;DirectUpdateHandler2.commitOnClose = false;&lt;/tt&gt; (ie: simulating an unclean shutdown) suggesting that maybe my original guess about some other test in the same JVM borking this seeing was correct ... but I still haven&apos;t been able to find a test that ran in the same JVM which might be broken in that way&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;The only failure type I&apos;ve been able to trigger is a new one AFAICT:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;(partitioned) leader successfully indexes some docs &amp;amp; commits on shutdown&lt;/li&gt;
	&lt;li&gt;leader re-starts, and sends &lt;tt&gt;REQUESTRECOVERY&lt;/tt&gt; to replica&lt;/li&gt;
	&lt;li&gt;leader marks itself as active&lt;/li&gt;
	&lt;li&gt;test thread detects &quot;all replicas are active&quot; &lt;b&gt;before&lt;/b&gt; replica has a chance to actually go into recovery&lt;/li&gt;
	&lt;li&gt;test thread checks replica for docs that only leader has, and fails&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;...ironically I&apos;ve only been able to reproduce this using &lt;tt&gt;TestTlogReplayVsRecovery&lt;/tt&gt; &#8211; I&apos;ve never seen it in &lt;tt&gt;TestCloudConsistency&lt;/tt&gt; even though it seems like that test establishes the same preconditions? (Successful logs of &lt;tt&gt;TestCloudConsistency&lt;/tt&gt; never show a &lt;tt&gt;REQUESTRECOVERY&lt;/tt&gt; command sent to the replicas from the leader, like I see in (both success and failure) logs for &lt;tt&gt;TestTlogReplayVsRecovery&lt;/tt&gt;, so I&apos;m guessing it has to do with how many docs are out of sync and what type of recovery is done? ... not certain)&lt;/p&gt;

&lt;p&gt;My next steps are:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Commit a fix for the &lt;tt&gt;uncommittedDocs + uncommittedDocs&lt;/tt&gt; bug in &lt;tt&gt;TestTlogReplayVsRecovery&lt;/tt&gt;
	&lt;ul&gt;
		&lt;li&gt;This will also include some TODOs about making the test more robust with more randomized committed &amp;amp; uncommitted docs before/after the network partition
		&lt;ul&gt;
			&lt;li&gt;These TODOs aren&apos;t really worth pursuing until the underlying bug is fixed&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;Open new jiras for:
	&lt;ul&gt;
		&lt;li&gt;Replacing &lt;tt&gt;DirectUpdateHandler2.commitOnClose&lt;/tt&gt; with something in &lt;tt&gt;TestInjection&lt;/tt&gt; (per comment there)
		&lt;ul&gt;
			&lt;li&gt;so we can be more confident tests aren&apos;t leaving it in a bad state&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
		&lt;li&gt;Consider setting replica to &lt;tt&gt;State.RECOVERYING&lt;/tt&gt; synchronously when processing &lt;tt&gt;REQUESTRECOVERY&lt;/tt&gt; command.
		&lt;ul&gt;
			&lt;li&gt;w/o this, even if we fix the bug tracked in this issue, it&apos;s still impossible for tests like &lt;tt&gt;TestTlogReplayVsRecovery&lt;/tt&gt; &#8211; or end users &#8211; to set CollectionState watchers to know when a collection is healthy in situations like the one being tracked in this jira.&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;After that, i don&apos;t think there&apos;s any thing else to do until someone smarter then me can chime in about fixing the underlying race condition of (leader) &quot;tlog replay on startup&quot; vs (replica) &quot;recover from leader&quot;.&lt;/p&gt;</comment>
                            <comment id="17013603" author="jira-bot" created="Sat, 11 Jan 2020 23:48:17 +0000"  >&lt;p&gt;Commit 9a2497f6377601d396b1b3b8b83ffcab0fd331a3 in lucene-solr&apos;s branch refs/heads/master from Chris M. Hostetter&lt;br/&gt;
[ &lt;a href=&quot;https://gitbox.apache.org/repos/asf?p=lucene-solr.git;h=9a2497f&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://gitbox.apache.org/repos/asf?p=lucene-solr.git;h=9a2497f&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-13486&quot; title=&quot;race condition between leader&amp;#39;s &amp;quot;replay on startup&amp;quot; and non-leader&amp;#39;s &amp;quot;recover from leader&amp;quot; can leave replicas out of sync (TestTlogReplayVsRecovery)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-13486&quot;&gt;SOLR-13486&lt;/a&gt;: Fix trivial test bug in TestTlogReplayVsRecovery&lt;/p&gt;

&lt;p&gt;Add TODOs for future test improvements once underlying race condition is fixed in core code&lt;/p&gt;</comment>
                            <comment id="17013613" author="jira-bot" created="Sun, 12 Jan 2020 00:09:55 +0000"  >&lt;p&gt;Commit 23fab1b6ebc08dab54f2937d2886fdc9c270711c in lucene-solr&apos;s branch refs/heads/branch_8x from Chris M. Hostetter&lt;br/&gt;
[ &lt;a href=&quot;https://gitbox.apache.org/repos/asf?p=lucene-solr.git;h=23fab1b&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://gitbox.apache.org/repos/asf?p=lucene-solr.git;h=23fab1b&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-13486&quot; title=&quot;race condition between leader&amp;#39;s &amp;quot;replay on startup&amp;quot; and non-leader&amp;#39;s &amp;quot;recover from leader&amp;quot; can leave replicas out of sync (TestTlogReplayVsRecovery)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-13486&quot;&gt;SOLR-13486&lt;/a&gt;: Fix trivial test bug in TestTlogReplayVsRecovery&lt;/p&gt;

&lt;p&gt;Add TODOs for future test improvements once underlying race condition is fixed in core code&lt;/p&gt;

&lt;p&gt;(cherry picked from commit 9a2497f6377601d396b1b3b8b83ffcab0fd331a3)&lt;/p&gt;</comment>
                            <comment id="17013614" author="hossman" created="Sun, 12 Jan 2020 00:11:20 +0000"  >&lt;p&gt;New linked jiras:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-14183&quot; title=&quot;replicas do not immediately/synchronously reflect state=RECOVERYING when recieving REQUESTRECOVERY commands&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-14183&quot;&gt;SOLR-14183&lt;/a&gt;: replicas do not immediately/synchronously reflect state=RECOVERYING when recieving REQUESTRECOVERY commands&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-14184&quot; title=&quot;replace DirectUpdateHandler2.commitOnClose with (negated) TestInjection.skipIndexWriterCommitOnClose&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-14184&quot;&gt;&lt;del&gt;SOLR-14184&lt;/del&gt;&lt;/a&gt;: replace DirectUpdateHandler2.commitOnClose with something in TestInjection&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="17014090" author="jira-bot" created="Mon, 13 Jan 2020 07:44:52 +0000"  >&lt;p&gt;Commit 9a2497f6377601d396b1b3b8b83ffcab0fd331a3 in lucene-solr&apos;s branch refs/heads/gradle-master from Chris M. Hostetter&lt;br/&gt;
[ &lt;a href=&quot;https://gitbox.apache.org/repos/asf?p=lucene-solr.git;h=9a2497f&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://gitbox.apache.org/repos/asf?p=lucene-solr.git;h=9a2497f&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-13486&quot; title=&quot;race condition between leader&amp;#39;s &amp;quot;replay on startup&amp;quot; and non-leader&amp;#39;s &amp;quot;recover from leader&amp;quot; can leave replicas out of sync (TestTlogReplayVsRecovery)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-13486&quot;&gt;SOLR-13486&lt;/a&gt;: Fix trivial test bug in TestTlogReplayVsRecovery&lt;/p&gt;

&lt;p&gt;Add TODOs for future test improvements once underlying race condition is fixed in core code&lt;/p&gt;</comment>
                            <comment id="17164379" author="erickerickson" created="Fri, 24 Jul 2020 11:23:57 +0000"  >&lt;p&gt;This is at least in the same ballpark, whether it&apos;s the same root cause is TBD&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="13278885">SOLR-14183</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13278886">SOLR-14184</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13277285">SOLR-14159</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13319157">SOLR-14679</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12989901" name="SOLR-13486__test.patch" size="12701" author="hossman" created="Fri, 3 Jan 2020 17:59:32 +0000"/>
                            <attachment id="12969301" name="apache_Lucene-Solr-BadApples-NightlyTests-master_61.log.txt.gz" size="4851376" author="hossman" created="Tue, 21 May 2019 16:54:05 +0000"/>
                            <attachment id="12969302" name="apache_Lucene-Solr-BadApples-Tests-8.x_102.log.txt.gz" size="260856" author="hossman" created="Tue, 21 May 2019 17:17:10 +0000"/>
                            <attachment id="12989672" name="org.apache.solr.cloud.TestCloudConsistency.zip" size="78987" author="dweiss" created="Mon, 30 Dec 2019 13:22:11 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>4.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            5 years, 16 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z02x3s:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12313321" key="com.atlassian.jira.toolkit:message">
                        <customfieldname>Solr Mailing List Info</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>