<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 03:33:10 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SOLR-3180] ChaosMonkey test failures</title>
                <link>https://issues.apache.org/jira/browse/SOLR-3180</link>
                <project id="12310230" key="SOLR">Solr</project>
                    <description>&lt;p&gt;Handle intermittent failures in the ChaosMonkey tests.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12544625">SOLR-3180</key>
            <summary>ChaosMonkey test failures</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="yseeley@gmail.com">Yonik Seeley</assignee>
                                    <reporter username="yseeley@gmail.com">Yonik Seeley</reporter>
                        <labels>
                    </labels>
                <created>Wed, 29 Feb 2012 14:27:11 +0000</created>
                <updated>Mon, 9 May 2016 18:54:19 +0000</updated>
                            <resolved>Mon, 25 Nov 2013 01:04:37 +0000</resolved>
                                                    <fixVersion>4.4</fixVersion>
                    <fixVersion>6.0</fixVersion>
                                    <component>SolrCloud</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                                                                                <comments>
                            <comment id="13219246" author="yseeley@gmail.com" created="Wed, 29 Feb 2012 14:37:55 +0000"  >&lt;p&gt;We&apos;ve come a long way and the monkey has uncovered a number of bugs that we&apos;ve fixed and is helping to make a really solid solution.&lt;/p&gt;

&lt;p&gt;I just uncovered another one having to do with races on shutdown.&lt;br/&gt;
When we kill the Jetty instance, it can cause an interrupted exception that closes the underlying NIO files under lucene.&lt;br/&gt;
If a commit is happening concurrently then what can happen is that we can end up with more than one unfinished transaction log.&lt;/p&gt;

&lt;p&gt;We call preCommit, which move the current tlog to prevTlog.&lt;br/&gt;
The commit fails, but concurrently other updates are coming in and they cause a new tlog to be created.&lt;br/&gt;
Even other updates coming in after this point can also succeed since they are simply buffered in memory by the IW. &lt;/p&gt;</comment>
                            <comment id="13221300" author="yseeley@gmail.com" created="Fri, 2 Mar 2012 22:14:40 +0000"  >&lt;p&gt;Just checked in a fix for this as well as a test that recovers from more than one tlog at startup. &lt;/p&gt;</comment>
                            <comment id="13221712" author="yseeley@gmail.com" created="Sat, 3 Mar 2012 22:05:08 +0000"  >&lt;p&gt;Failures are &lt;b&gt;much&lt;/b&gt; less frequent, but I still got one after about 7 hours I think.&lt;br/&gt;
I saw a commit fail (due to the interrupted exception), but then I later saw the IW.close() succeed (which caused Solr to cap the log file, assuming that everything was in the index).&lt;/p&gt;

&lt;p&gt;As a result, I just committed a change to the shutdown code to do an explicit commit.&lt;/p&gt;</comment>
                            <comment id="13534598" author="yseeley@gmail.com" created="Tue, 18 Dec 2012 03:34:47 +0000"  >&lt;p&gt;Unfortunately, the chaos monkey tests have gathered some rust due to not being run regularly.&lt;/p&gt;

&lt;p&gt;I&apos;ve added some instrumentation for one particular type of fail (number of docs mismatch between control and collection) and ran into something peculiar.&lt;/p&gt;

&lt;p&gt;selected lines from test_report_1.txt&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;6952 T39 oasc.ZkController.register Register shard - core:c
ollection1 address:http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:60440/ij/pf shardId:shard1
&lt;/span&gt;
9734 T56 oasc.ZkController.register Register shard - core:c
ollection1 address:http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:48048/ij/pf shardId:shard1
&lt;/span&gt;
12184 T71 oasc.ZkController.register Register shard - core:
collection1 address:http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:33264/ij/pf shardId:shard2
&lt;/span&gt;
59069 T27 C12 P60440 /update {wt=javabin&amp;amp;version=2} {add=[50067 (1421643212854919168)]} 0 3

89123 T48 C1 P48048 /update {wt=javabin&amp;amp;version=2} {add=[50067 (1421643244360433664)]} 0 10

89354 T25 C12 P60440 /update {wt=javabin&amp;amp;version=2} {delete=[50067 (-1421643244613140480)]} 0 1

89364 T47 C1 P48048 /update {wt=javabin&amp;amp;version=2} {delete=[50067 (-1421643244618383360)]} 0 6

90114 T63 C3 P33264 /update {wt=javabin&amp;amp;version=2} {add=[50067]} 0 31041

217860 T10 oasc.AbstractFullDistribZkTestBase.checkShardConsistency SEVERE document count mismatch.  control=1460 sum(shards)=1461 cloudClient=1461
[junit4:junit4]

###### Only in cloudDocList: [{id=50067}]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The only updates we see from the logs for id 50067 are to collection1 (none to the control shard).&lt;/p&gt;

&lt;p&gt;Further, grepping for &quot;Register shard&quot; shows that they are all for collection1 and none for the control collection.  Not sure how that&apos;s possible and only be off by 1, so I&apos;m probably mis-interpreting the logging somehow...&lt;/p&gt;</comment>
                            <comment id="13534659" author="commit-tag-bot" created="Tue, 18 Dec 2012 05:28:30 +0000"  >&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; Yonik Seeley&lt;br/&gt;
&lt;a href=&quot;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1423275&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1423275&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-3180&quot; title=&quot;ChaosMonkey test failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-3180&quot;&gt;&lt;del&gt;SOLR-3180&lt;/del&gt;&lt;/a&gt;: improve logging&lt;/p&gt;</comment>
                            <comment id="13534668" author="commit-tag-bot" created="Tue, 18 Dec 2012 05:36:50 +0000"  >&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; Yonik Seeley&lt;br/&gt;
&lt;a href=&quot;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1423276&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1423276&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-3180&quot; title=&quot;ChaosMonkey test failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-3180&quot;&gt;&lt;del&gt;SOLR-3180&lt;/del&gt;&lt;/a&gt;: improve logging&lt;/p&gt;</comment>
                            <comment id="13535200" author="commit-tag-bot" created="Tue, 18 Dec 2012 19:36:36 +0000"  >&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; Yonik Seeley&lt;br/&gt;
&lt;a href=&quot;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1423591&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1423591&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-3180&quot; title=&quot;ChaosMonkey test failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-3180&quot;&gt;&lt;del&gt;SOLR-3180&lt;/del&gt;&lt;/a&gt;: improve logging&lt;/p&gt;</comment>
                            <comment id="13535215" author="commit-tag-bot" created="Tue, 18 Dec 2012 19:44:45 +0000"  >&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; Yonik Seeley&lt;br/&gt;
&lt;a href=&quot;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1423597&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1423597&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-3180&quot; title=&quot;ChaosMonkey test failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-3180&quot;&gt;&lt;del&gt;SOLR-3180&lt;/del&gt;&lt;/a&gt;: improve logging&lt;/p&gt;</comment>
                            <comment id="13538876" author="yseeley@gmail.com" created="Sat, 22 Dec 2012 18:45:22 +0000"  >&lt;p&gt;Uploading fail.inconsistent.txt - I had to truncate the start of the log file to get it under the limit for JIRA.&lt;/p&gt;

&lt;p&gt;Analysis:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  2&amp;gt; ASYNC  NEW_CORE C6 name=collection1 org.apache.solr.core.SolrCore@eaecb09 url=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:58270/collection1 node=127.0.0.1:58270_ C6_STATE=coll:control_collection core:collection1 props:{shard=shard1, roles=&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;, state=active, core=collection1, collection=control_collection, node_name=127.0.0.1:58270_, base_url=http://127.0.0.1:58270, leader=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;}
&lt;/span&gt;
  2&amp;gt; ASYNC  NEW_CORE C5 name=collection1 org.apache.solr.core.SolrCore@54eeabe8 url=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:37198/collection1 node=127.0.0.1:37198_ C5_STATE=coll:collection1 core:collection1 props:{shard=shard3, roles=&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;, state=active, core=collection1, collection=collection1, node_name=127.0.0.1:37198_, base_url=http://127.0.0.1:37198, leader=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;}
&lt;/span&gt;  2&amp;gt; 25510 T80 C5 P37198 REQ /get {distrib=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&amp;amp;qt=/get&amp;amp;wt=javabin&amp;amp;version=2&amp;amp;getVersions=100} status=0 QTime=0 


  2&amp;gt; 187637 T669 C21 P39620 oasu.PeerSync.sync PeerSync: core=collection1 url=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:39620 DONE. sync succeeded
&lt;/span&gt;  2&amp;gt; 188653 T669 C21 P39620 oasc.RecoveryStrategy.doRecovery PeerSync Recovery was successful - registering as Active. core=collection1

#
# C21 (the replica) is recovering around the same time that the update &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; id:52720 comes in (we only
# see when the update finishes below, not when it starts).
#

# update control finished
  2&amp;gt; 187923 T24 C6 P58270 /update {wt=javabin&amp;amp;version=2} {add=[52720 (1422073056556220416)]} 0 1
# update leader &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; shard3 finished
  2&amp;gt; 187927 T77 C5 P37198 /update {wt=javabin&amp;amp;version=2} {add=[52720 (1422073056559366144)]} 0 1
# these are the only adds &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; id:52720 in the logs...
# TODO: verify that there was no replica &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; C5 to forward to?


--------------------
  2&amp;gt; 225993 T77 C5 P37198 REQ /select {tests=checkShardConsistency&amp;amp;q=*:*&amp;amp;distrib=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&amp;amp;wt=javabin&amp;amp;rows=0&amp;amp;version=2} hits=835 status=0 QTime=1 
# Note that C5 is still the leader - &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; means that C21 recovered from it at some point?

  2&amp;gt; 225997 T658 C21 P39620 REQ /select {tests=checkShardConsistency&amp;amp;q=*:*&amp;amp;distrib=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&amp;amp;wt=javabin&amp;amp;rows=0&amp;amp;version=2} hits=833 status=0 QTime=1 
  2&amp;gt;  live:&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;
  2&amp;gt;  num:833
  2&amp;gt; 
  2&amp;gt; ######shard3 is not consistent.  Got 835 from http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:37198/collection1lastClient and got 833 from http://127.0.0.1:39620/collection1
&lt;/span&gt; 
  2&amp;gt; ###### sizes=835,833
  2&amp;gt; ###### Only in http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:37198/collection1: [{id=52720, _version_=1422073056559366144}, {id=52710, _version_=1422073056325533696}, {id=52717, _version_=1422073056485965825}, {id=2225, _version_=1422073056602357760}, {id=52709, _version_=1422073056298270720}, {id=2226, _version_=1422073056612843520}, {id=2219, _version_=1422073056477577216}, {id=52723, _version_=1422073056605503488}]
&lt;/span&gt;  2&amp;gt; ###### Only in http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:39620/collection1: [{id=52680, _version_=1422073042480136192}, {id=52669, _version_=1422073042276712448}, {id=52676, _version_=1422073042420367360}, {id=2204, _version_=1422073042912149504}, {id=2198, _version_=1422073042778980352}, {id=2207, _version_=1422073053454532608}]&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So what looks to be the case is that we came up and did a peersync with the leader and succeeded, but the leader hadn&apos;t noticed us yet and so didn&apos;t forward us a new update in the meantime.&lt;/p&gt;

&lt;p&gt;If we don&apos;t already, we need to make sure to do some of the same type of stuff that we do with replication recovery.  The replica needs to ensure that the leader sees it (and hence will forward future updates) before it peersyncs with the leader.&lt;/p&gt;</comment>
                            <comment id="13538908" author="commit-tag-bot" created="Sat, 22 Dec 2012 21:08:22 +0000"  >&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; Mark Robert Miller&lt;br/&gt;
&lt;a href=&quot;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1425342&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1425342&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-3180&quot; title=&quot;ChaosMonkey test failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-3180&quot;&gt;&lt;del&gt;SOLR-3180&lt;/del&gt;&lt;/a&gt;: wait for leader to see our recovering state before peer sync as well&lt;/p&gt;</comment>
                            <comment id="13538909" author="markrmiller@gmail.com" created="Sat, 22 Dec 2012 21:10:40 +0000"  >&lt;p&gt;I&apos;ve got fullmetaljenkins.org on chaos monkey duty: &lt;a href=&quot;http://fullmetaljenkins.org/view/Chaos/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://fullmetaljenkins.org/view/Chaos/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I&apos;m running the test with a variety of situations to help narrow in on the problem.&lt;/p&gt;

&lt;p&gt;I&apos;ve also started checking old checkouts - did that some last night, but with somewhat inconclusive results. Hope to make more progress.&lt;/p&gt;</comment>
                            <comment id="13538910" author="commit-tag-bot" created="Sat, 22 Dec 2012 21:14:11 +0000"  >&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; Mark Robert Miller&lt;br/&gt;
&lt;a href=&quot;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1425343&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1425343&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-3180&quot; title=&quot;ChaosMonkey test failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-3180&quot;&gt;&lt;del&gt;SOLR-3180&lt;/del&gt;&lt;/a&gt;: wait for leader to see our recovering state before peer sync as well&lt;/p&gt;</comment>
                            <comment id="13539120" author="commit-tag-bot" created="Sun, 23 Dec 2012 23:00:16 +0000"  >&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; Mark Robert Miller&lt;br/&gt;
&lt;a href=&quot;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1425554&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1425554&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-3180&quot; title=&quot;ChaosMonkey test failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-3180&quot;&gt;&lt;del&gt;SOLR-3180&lt;/del&gt;&lt;/a&gt;: remove bad assumption&lt;/p&gt;</comment>
                            <comment id="13539121" author="commit-tag-bot" created="Sun, 23 Dec 2012 23:08:12 +0000"  >&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; Mark Robert Miller&lt;br/&gt;
&lt;a href=&quot;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1425555&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1425555&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-3180&quot; title=&quot;ChaosMonkey test failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-3180&quot;&gt;&lt;del&gt;SOLR-3180&lt;/del&gt;&lt;/a&gt;: remove bad assumption&lt;/p&gt;</comment>
                            <comment id="13539130" author="commit-tag-bot" created="Mon, 24 Dec 2012 00:16:11 +0000"  >&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; Mark Robert Miller&lt;br/&gt;
&lt;a href=&quot;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1425561&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1425561&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-4638&quot; title=&quot;If IndexWriter is interrupted on close and is using a channel (mmap/nio), it can throw a ClosedByInterruptException and prevent you from opening a new IndexWriter in the same proceses if you are using Native locks.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;LUCENE-4638&quot;&gt;LUCENE-4638&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-3180&quot; title=&quot;ChaosMonkey test failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-3180&quot;&gt;&lt;del&gt;SOLR-3180&lt;/del&gt;&lt;/a&gt;: try using the IW&apos;s writeLock to unlock&lt;/p&gt;</comment>
                            <comment id="13539137" author="commit-tag-bot" created="Mon, 24 Dec 2012 00:29:05 +0000"  >&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; Mark Robert Miller&lt;br/&gt;
&lt;a href=&quot;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1425563&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1425563&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-4638&quot; title=&quot;If IndexWriter is interrupted on close and is using a channel (mmap/nio), it can throw a ClosedByInterruptException and prevent you from opening a new IndexWriter in the same proceses if you are using Native locks.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;LUCENE-4638&quot;&gt;LUCENE-4638&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-3180&quot; title=&quot;ChaosMonkey test failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-3180&quot;&gt;&lt;del&gt;SOLR-3180&lt;/del&gt;&lt;/a&gt;: try using the IW&apos;s writeLock to unlock&lt;/p&gt;</comment>
                            <comment id="13539152" author="commit-tag-bot" created="Mon, 24 Dec 2012 02:12:11 +0000"  >&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; Mark Robert Miller&lt;br/&gt;
&lt;a href=&quot;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1425574&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1425574&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-4638&quot; title=&quot;If IndexWriter is interrupted on close and is using a channel (mmap/nio), it can throw a ClosedByInterruptException and prevent you from opening a new IndexWriter in the same proceses if you are using Native locks.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;LUCENE-4638&quot;&gt;LUCENE-4638&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-3180&quot; title=&quot;ChaosMonkey test failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-3180&quot;&gt;&lt;del&gt;SOLR-3180&lt;/del&gt;&lt;/a&gt;: revert for now (try using the IW&apos;s writeLock to unlock)&lt;/p&gt;</comment>
                            <comment id="13539155" author="commit-tag-bot" created="Mon, 24 Dec 2012 02:18:25 +0000"  >&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; Mark Robert Miller&lt;br/&gt;
&lt;a href=&quot;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1425576&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1425576&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-4638&quot; title=&quot;If IndexWriter is interrupted on close and is using a channel (mmap/nio), it can throw a ClosedByInterruptException and prevent you from opening a new IndexWriter in the same proceses if you are using Native locks.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;LUCENE-4638&quot;&gt;LUCENE-4638&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-3180&quot; title=&quot;ChaosMonkey test failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-3180&quot;&gt;&lt;del&gt;SOLR-3180&lt;/del&gt;&lt;/a&gt;: revert for now (try using the IW&apos;s writeLock to unlock)&lt;/p&gt;</comment>
                            <comment id="13539459" author="markrmiller@gmail.com" created="Wed, 26 Dec 2012 03:36:15 +0000"  >&lt;p&gt;The removed bad assumption commit takes care of the final shard consistency issues. Two holes that I was somehow not seeing when running this test previously. We should probably spin them out into their own issues for CHANGES.&lt;/p&gt;

&lt;p&gt;There is still a very rare cloud doc count is off by one with control issue to resolve - still unclear if its a real issue or test issue.&lt;/p&gt;

&lt;p&gt;The last set of commits was to try and take care of a somewhat unrelated issue around index locks - it didnt work, it just moved the blow up point. It&apos;s only related to this issue in that I suspected it as part of an indexsearcher not being closed fail that made &apos;false&apos; fails annoying to wade through.&lt;/p&gt;</comment>
                            <comment id="13540526" author="yseeley@gmail.com" created="Fri, 28 Dec 2012 17:32:59 +0000"  >&lt;p&gt;I got a hang running the CMSL test.&lt;br/&gt;
Here&apos;s a thread dump.&lt;/p&gt;

&lt;p&gt;I took another thread dump a few minutes later to see what changed, and it wasn&apos;t much:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;/opt/code/misc$ diff -u CMSL_hang.txt CMSL_hang2.txt
--- CMSL_hang.txt	2012-12-28 12:21:47.000000000 -0500
+++ CMSL_hang2.txt	2012-12-28 12:24:37.000000000 -0500
@@ -1,4 +1,4 @@
-2012-12-28 12:21:47
+2012-12-28 12:24:38
 Full thread dump Java HotSpot(TM) 64-Bit Server VM (20.4-b02 mixed mode):
 
 &lt;span class=&quot;code-quote&quot;&gt;&quot;Attach Listener&quot;&lt;/span&gt; daemon prio=10 tid=0x0000000042790000 nid=0x1df4 waiting on condition [0x0000000000000000]
@@ -1559,7 +1559,7 @@
    java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.State: TIMED_WAITING (on object monitor)
 	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;.wait(Native Method)
 	at org.apache.solr.cloud.DistributedQueue$LatchChildWatcher.await(DistributedQueue.java:182)
-	- locked &amp;lt;0x00000000fd60df70&amp;gt; (a java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;)
+	- locked &amp;lt;0x00000000f7ed6b70&amp;gt; (a java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;)
 	at org.apache.solr.cloud.DistributedQueue.peek(DistributedQueue.java:281)
 	at org.apache.solr.cloud.OverseerCollectionProcessor.run(OverseerCollectionProcessor.java:100)
 	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:662)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13540959" author="yseeley@gmail.com" created="Sat, 29 Dec 2012 18:33:41 +0000"  >&lt;p&gt;Here&apos;s another hang.&lt;br/&gt;
Diff between 2 snapshots at different times is:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.State: TIMED_WAITING (on object monitor)
 	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;.wait(Native Method)
 	at org.apache.solr.cloud.DistributedQueue$LatchChildWatcher.await(DistributedQueue.java:182)
-	- locked &amp;lt;0x00000000f5821d30&amp;gt; (a java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;)
+	- locked &amp;lt;0x00000000f5f2c7c0&amp;gt; (a java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;)
 	at org.apache.solr.cloud.DistributedQueue.peek(DistributedQueue.java:281)
 	at org.apache.solr.cloud.OverseerCollectionProcessor.run(OverseerCollectionProcessor.java:100)
 	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:662)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13541020" author="yseeley@gmail.com" created="Sat, 29 Dec 2012 23:34:13 +0000"  >&lt;p&gt;Progress!&lt;br/&gt;
Here&apos;s my analysis of one of the failures where the control doesn&apos;t match the cloud:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  2&amp;gt; 115826 T24 C12 P44177 /update {wt=javabin&amp;amp;version=2} {add=[50247 (1422479033427296256)]} 0 11

  2&amp;gt; 145877 T61 C4 P38042 /update {wt=javabin&amp;amp;version=2} {add=[50247 (1422479064932810752)]} 0 17

  2&amp;gt; 146263 T26 C12 P44177 /update {wt=javabin&amp;amp;version=2} {delete=[50247 (-1422479065354338304)]} 0 0

  2&amp;gt; 146268 T63 C4 P38042 /update {wt=javabin&amp;amp;version=2} {delete=[50247 (-1422479065356435456)]} 0 3

  2&amp;gt; 146867 T46 C1 P60593 /update {wt=javabin&amp;amp;version=2} {add=[50247]} 0 31033

  2&amp;gt; ###### Only in cloudDocList: [{id=50247}]
  2&amp;gt; 190158 T10 oasc.AbstractFullDistribZkTestBase.checkShardConsistency SEVERE controlClient :{numFound=0,start=0,docs=[]}
  2&amp;gt; 		cloudClient :{numFound=1,start=0,docs=[SolrDocument{id=50247, _version_=1422479065981386752}]}
 
  NOTE: The version we found does not match any version previously added, and it does come logically after the last delete!

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;edit: the upshot is that it looks like there&apos;s a real bug here somewhere - this is not just a test issue.&lt;/p&gt;</comment>
                            <comment id="13541895" author="yseeley@gmail.com" created="Tue, 1 Jan 2013 18:15:59 +0000"  >&lt;p&gt;With the logging fixed, I can see more of what the problem is.  It appears if two updates for document 50030 are happening simultaneously, which shouldn&apos;t happen since the tests don&apos;t do this.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;  2&amp;gt; 65234 T28 C12 P56307 oasu.DirectUpdateHandler2.checkDocument LOCAL_ADD: id=50030 version=1422948614358958080
  2&amp;gt; 65235 T28 C12 P56307 /update {wt=javabin&amp;amp;version=2} {add=[50030 (1422948614358958080)]} 0 2
 
  2&amp;gt; 65242 T61 C4 P44328 oasu.DirectUpdateHandler2.checkDocument LOCAL_ADD: id=50030 version=1422948614366298112
  2&amp;gt; 65245 T103 C1 P59742 oasu.DirectUpdateHandler2.checkDocument LOCAL_ADD: id=50030 version=1422948614366298112
  2&amp;gt; 65246 T160 C7 P51177 oasu.DirectUpdateHandler2.checkDocument LOCAL_ADD: id=50030 version=1422948614366298112
  2&amp;gt; 65247 T103 C1 P59742 /update {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:44328/fa/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2} {add=[50030 (1422948614366298112)]} 0 2
&lt;/span&gt;  2&amp;gt; 65247 T160 C7 P51177 /update {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:44328/fa/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2} {add=[50030 (1422948614366298112)]} 0 2
&lt;/span&gt; 
  2&amp;gt; 80763 T59 C4 P44328 oasu.DirectUpdateHandler2.checkDocument LOCAL_ADD: id=50030 version=1422948630641246208
  2&amp;gt; 80773 T158 C7 P51177 oasu.DirectUpdateHandler2.checkDocument LOCAL_ADD: id=50030 version=1422948630641246208
  2&amp;gt; 80776 T158 C7 P51177 /update {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:44328/fa/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2} {add=[50030 (1422948630641246208)]} 0 5
&lt;/span&gt; 
  2&amp;gt; 85279 T61 C4 P44328 /update {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:36869/fa/collection1/&amp;amp;update.distrib=TOLEADER&amp;amp;wt=javabin&amp;amp;version=2} {add=[50030 (1422948614366298112)]} 0 20038
&lt;/span&gt;
  2&amp;gt; 87884 T213 C10 P42076 oasu.DirectUpdateHandler2.checkDocument LOCAL_ADD: id=50030 version=1422948630641246208
  2&amp;gt; 87885 T213 C10 P42076 /update {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:44328/fa/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2} {add=[50030 (1422948630641246208)]} 0 7116
&lt;/span&gt;
  2&amp;gt; 87973 T216 C10 P42076 /update {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:44328/fa/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2} {add=[50030 (1422948614366298112)]} 0 22728
&lt;/span&gt;
  2&amp;gt; 95272 T61 C4 P44328 oasu.DirectUpdateHandler2.checkDocument LOCAL_ADD: id=50030 version=1422948645855035392
  2&amp;gt; 95277 T158 C7 P51177 oasu.DirectUpdateHandler2.checkDocument LOCAL_ADD: id=50030 version=1422948645855035392
  2&amp;gt; 95281 T158 C7 P51177 /update {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:44328/fa/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2} {add=[50030 (1422948645855035392)]} 0 5
&lt;/span&gt;  2&amp;gt; 95283 T61 C4 P44328 /update {wt=javabin&amp;amp;version=2} {add=[50030 (1422948645855035392)]} 0 12
 

  2&amp;gt; 95326 T27 C12 P56307 /update {wt=javabin&amp;amp;version=2} {delete=[50030 (-1422948645912707072)]} 0 0
  2&amp;gt; 95336 T159 C7 P51177 /update {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:44328/fa/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2} {delete=[50030 (-1422948645918998528)]} 0 1
&lt;/span&gt;  2&amp;gt; 95338 T61 C4 P44328 /update {wt=javabin&amp;amp;version=2} {delete=[50030 (-1422948645918998528)]} 0 7


  2&amp;gt; 96276 T475 C4 P44328 oasu.DirectUpdateHandler2.checkDocument LOCAL_ADD: id=50030 version=1422948646907805696
  2&amp;gt; 96286 T156 C7 P51177 oasu.DirectUpdateHandler2.checkDocument LOCAL_ADD: id=50030 version=1422948646907805696
  2&amp;gt; 96290 T156 C7 P51177 /update {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:44328/fa/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2} {add=[50030 (1422948646907805696)]} 0 5
&lt;/span&gt;  2&amp;gt; 96292 T475 C4 P44328 /update {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:36869/fa/collection1/&amp;amp;update.distrib=TOLEADER&amp;amp;wt=javabin&amp;amp;version=2} {add=[50030 (1422948646907805696)]} 0 18
&lt;/span&gt;  2&amp;gt; 96293 T77 C6 P36869 /update {wt=javabin&amp;amp;version=2} {add=[50030]} 0 31054


  2&amp;gt; 97738 T59 C4 P44328 /update {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:36869/fa/collection1/&amp;amp;update.distrib=TOLEADER&amp;amp;wt=javabin&amp;amp;version=2} {add=[50030 (1422948630641246208)]} 0 16977
&lt;/span&gt;

  2&amp;gt; 98396 T103 C1 P59742 oasu.DirectUpdateHandler2.checkDocument LOCAL_ADD: id=50030 version=1422948630641246208
  2&amp;gt; 98397 T103 C1 P59742 /update {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:44328/fa/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2} {add=[50030 (1422948630641246208)]} 0 17627
&lt;/span&gt;
  2&amp;gt; 105349 T488 C1 P59742 oasu.PeerSync.handleUpdates FINE PeerSync: core=collection1 url=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:59742/fa raw update record [1, 1422948645855035392, SolrInputDocument[id=50030, a_si=50, other_tl1=50, a_t=to come to the aid of their country., rnd_b=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, _version_=1422948645855035392]]
&lt;/span&gt;  2&amp;gt; 105350 T488 C1 P59742 oasu.PeerSync.handleUpdates FINE PeerSync: core=collection1 url=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:59742/fa add add{flags=12,_version_=1422948645855035392,id=50030}
&lt;/span&gt;  2&amp;gt; 105351 T488 C1 P59742 oasu.DirectUpdateHandler2.checkDocument LOCAL_ADD: id=50030 version=1422948645855035392
 

  2&amp;gt; 105356 T488 C1 P59742 oasu.PeerSync.handleUpdates FINE PeerSync: core=collection1 url=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:59742/fa raw update record [2, -1422948645918998528, [B@3b76982e]
&lt;/span&gt;  2&amp;gt; 105356 T488 C1 P59742 oasu.PeerSync.handleUpdates FINE PeerSync: core=collection1 url=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:59742/fa delete delete{flags=12,_version_=-1422948645918998528,indexedId=50030,commitWithin=-1}
&lt;/span&gt;  2&amp;gt; 105356 T488 C1 P59742 oasu.PeerSync.handleUpdates FINE PeerSync: core=collection1 url=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:59742/fa raw update record [1, 1422948646907805696, SolrInputDocument[id=50030, a_si=50, other_tl1=50, a_t=to come to the aid of their country., rnd_b=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, _version_=1422948646907805696]]
&lt;/span&gt;  2&amp;gt; 105356 T488 C1 P59742 oasu.PeerSync.handleUpdates FINE PeerSync: core=collection1 url=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:59742/fa add add{flags=12,_version_=1422948646907805696,id=50030}
&lt;/span&gt;


  2&amp;gt; ###### Only in cloudDocList: [{id=50030}]
  2&amp;gt; 203443 T10 oasc.AbstractFullDistribZkTestBase.checkShardConsistency SEVERE controlClient :{numFound=0,start=0,docs=[]}
  2&amp;gt; 		cloudClient :{numFound=1,start=0,docs=[SolrDocument{id=50030, _version_=1422948646907805696}]}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As usual C12 is the control.&lt;br/&gt;
You can see an add complete on C4 and on C6&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;  2&amp;gt; 95283 T61 C4 P44328 /update {wt=javabin&amp;amp;version=2} {add=[50030 (1422948645855035392)]} 0 12
  2&amp;gt; 96293 T77 C6 P36869 /update {wt=javabin&amp;amp;version=2} {add=[50030]} 0 31054
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13541944" author="commit-tag-bot" created="Tue, 1 Jan 2013 23:26:33 +0000"  >&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; Mark Robert Miller&lt;br/&gt;
&lt;a href=&quot;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1427589&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1427589&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-3180&quot; title=&quot;ChaosMonkey test failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-3180&quot;&gt;&lt;del&gt;SOLR-3180&lt;/del&gt;&lt;/a&gt;: remove bad method call&lt;/p&gt;</comment>
                            <comment id="13541945" author="commit-tag-bot" created="Tue, 1 Jan 2013 23:34:19 +0000"  >&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; Mark Robert Miller&lt;br/&gt;
&lt;a href=&quot;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1427590&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1427590&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-3180&quot; title=&quot;ChaosMonkey test failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-3180&quot;&gt;&lt;del&gt;SOLR-3180&lt;/del&gt;&lt;/a&gt;: remove bad method call&lt;/p&gt;</comment>
                            <comment id="13542022" author="commit-tag-bot" created="Wed, 2 Jan 2013 06:27:10 +0000"  >&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; Mark Robert Miller&lt;br/&gt;
&lt;a href=&quot;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1427658&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1427658&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-3180&quot; title=&quot;ChaosMonkey test failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-3180&quot;&gt;&lt;del&gt;SOLR-3180&lt;/del&gt;&lt;/a&gt;: update node props for retry node, not just url&lt;/p&gt;</comment>
                            <comment id="13542023" author="commit-tag-bot" created="Wed, 2 Jan 2013 06:27:13 +0000"  >&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; Mark Robert Miller&lt;br/&gt;
&lt;a href=&quot;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1427657&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1427657&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-3180&quot; title=&quot;ChaosMonkey test failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-3180&quot;&gt;&lt;del&gt;SOLR-3180&lt;/del&gt;&lt;/a&gt;: update node props for retry node, not just url&lt;/p&gt;</comment>
                            <comment id="13542240" author="yseeley@gmail.com" created="Wed, 2 Jan 2013 17:12:10 +0000"  >&lt;p&gt;OK, here&apos;s a fail with more logging.&lt;/p&gt;

&lt;p&gt;What finally manifests as an error the test detects seems to be caused by retries (but there are still threads hanging around from the first time that eventually execute).&lt;/p&gt;

&lt;p&gt;I didn&apos;t follow this fail all the way through to the end since I saw enough potentially wonky stuff earlier.&lt;/p&gt;

&lt;p&gt;C14 tries to publish success, fails because CM caused a connection loss, and then future updates to C14 hang for a long time.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
###  Since we know C14 (P56842) seems problematic we look back in time before the problematic update.
  2&amp;gt; ASYNC  NEW_CORE C14 name=collection1 org.apache.solr.core.SolrCore@15c617e0 url=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:56842/collection1 node=127.0.0.1:56842_ C14_STATE=coll:collection1 core:collection1 props:{shard=shard1, roles=&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;, state=active, core=collection1, collection=collection1, node_name=127.0.0.1:56842_, base_url=http://127.0.0.1:56842}
&lt;/span&gt;  2&amp;gt; 73226 T323 C14 P56842 oasc.RecoveryStrategy.run Starting recovery process.  core=collection1 recoveringAfterStartup=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;
 
  2&amp;gt; 73233 T323 C14 P56842 oasc.ZkController.publish publishing core=collection1 state=recovering

  2&amp;gt; 75058 T256 oasc.ChaosMonkey.monkeyLog monkey: chose a victim! 56842
  2&amp;gt; 75059 T256 oasc.ChaosMonkey.monkeyLog monkey: expire session &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 56842 !
  2&amp;gt; 75059 T256 oasc.ChaosMonkey.monkeyLog monkey: cause connection loss!
 
  2&amp;gt;  C14_STATE=coll:collection1 core:collection1 props:{shard=shard1, roles=&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;, state=recovering, core=collection1, collection=collection1, node_name=127.0.0.1:56842_, base_url=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:56842}
&lt;/span&gt;  2&amp;gt; 77316 T323 C14 P56842 oasc.RecoveryStrategy.doRecovery Attempting to PeerSync from http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:41533/collection1/ core=collection1 - recoveringAfterStartup=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;
&lt;/span&gt;

  2&amp;gt; 77328 T323 C14 P56842 oasu.PeerSync.handleVersions PeerSync: core=collection1 url=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:56842  Our versions are newer. ourLowThreshold=1423033399394697216 otherHigh=1423033403051081728
&lt;/span&gt;  2&amp;gt; 77329 T323 C14 P56842 oasu.PeerSync.sync PeerSync: core=collection1 url=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:56842 DONE. sync succeeded
&lt;/span&gt; 
##### OK, perhaps the real problem starts here, when we &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; to publish as &lt;span class=&quot;code-quote&quot;&gt;&quot;active&quot;&lt;/span&gt; but
##### we have issues talking to ZK (because CM caused a connection loss previously?)
  2&amp;gt; 77478 T323 C14 P56842 oasc.ZkController.publish publishing core=collection1 state=active
  2&amp;gt; 77480 T319 oaz.ClientCnxn$SendThread.run WARNING Session 0x13bfa14a03d0020 &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; server &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;, unexpected error, closing socket connection and attempting reconnect java.nio.channels.CancelledKeyException

##### The first request that comes in &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; us in on T314, but &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; is the last we hear from &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; thread until time 109571 (same as the problematic add)
  2&amp;gt; 82034 T314 C14 P56842 oasup.LogUpdateProcessor.processDelete PRE_UPDATE DELETE delete{flags=0,_version_=-1423033432318935040,id=60072,commitWithin=-1} {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:41533/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;
 
############## start of troublesome update

  2&amp;gt; 82115 T28 C12 P49621 oasup.LogUpdateProcessor.processAdd PRE_UPDATE ADD add{flags=0,_version_=0,id=10079} {wt=javabin&amp;amp;version=2}
  2&amp;gt; 82116 T28 C12 P49621 oasu.DirectUpdateHandler2.checkDocument LOCAL_ADD: id=10079 version=1423033432421695488
  2&amp;gt; 82117 T28 C12 P49621 PRE_UPDATE FINISH  {wt=javabin&amp;amp;version=2}
  2&amp;gt; 82118 T28 C12 P49621 /update {wt=javabin&amp;amp;version=2} {add=[10079 (1423033432421695488)]} 0 3


##### Below, C4 receives the update, forwards it to the leader C2, which forwards it to C14 and C0.  Looking ahead, it looks like C14 is a problem node - &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; update takes a &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; time to complete.

  2&amp;gt; 82121 T59 C4 P48210 oasup.LogUpdateProcessor.processAdd PRE_UPDATE ADD add{flags=0,_version_=0,id=10079} {wt=javabin&amp;amp;version=2}
  2&amp;gt; 82121 T59 C4 P48210 PRE_UPDATE FINISH  {wt=javabin&amp;amp;version=2}
  2&amp;gt; 82124 T44 C2 P41533 oasup.LogUpdateProcessor.processAdd PRE_UPDATE ADD add{flags=0,_version_=0,id=10079} {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:48210/collection1/&amp;amp;update.distrib=TOLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;  2&amp;gt; 82124 T44 C2 P41533 oasu.DirectUpdateHandler2.checkDocument LOCAL_ADD: id=10079 version=1423033432430084096
  2&amp;gt; 82125 T44 C2 P41533 PRE_UPDATE FINISH  {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:48210/collection1/&amp;amp;update.distrib=TOLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;  2&amp;gt; 82127 T315 C14 P56842 oasup.LogUpdateProcessor.processAdd PRE_UPDATE ADD add{flags=0,_version_=0,id=10079} {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:41533/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;  2&amp;gt; 82128 T89 C0 P56785 oasup.LogUpdateProcessor.processAdd PRE_UPDATE ADD add{flags=0,_version_=0,id=10079} {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:41533/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;  2&amp;gt; 82128 T89 C0 P56785 oasu.DirectUpdateHandler2.checkDocument LOCAL_ADD: id=10079 version=1423033432430084096
  2&amp;gt; 82129 T89 C0 P56785 PRE_UPDATE FINISH  {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:41533/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;  2&amp;gt; 82129 T89 C0 P56785 /update {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:41533/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2} {add=[10079 (1423033432430084096)]} 0 1
&lt;/span&gt;
  2&amp;gt; 82247 T256 oasc.ChaosMonkey.monkeyLog monkey: chose a victim! 56785
  2&amp;gt; 82247 T256 oasc.ChaosMonkey.monkeyLog monkey: expire session &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 56785 !
  2&amp;gt; 82248 T256 oasc.ChaosMonkey.monkeyLog monkey: cause connection loss!
##### 56785 is C0, but it looks like we already completed our update forwarded from C2 (although it&apos;s always possible the response was not completely written yet)



##### So C14 hung &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; some reason, but it doesn&apos;t look like the chaos monkey touched him!
  2&amp;gt; 97029 T46 C2 P41533 oasc.SolrException.log SEVERE shard update error StdNode: http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:56842/collection1/:org.apache.solr.client.solrj.SolrServerException: Timeout occured &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; waiting response from server at: http://127.0.0.1:56842/collection1
&lt;/span&gt;  2&amp;gt; 97031 T46 C2 P41533 oasup.DistributedUpdateProcessor.doFinish &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; and ask http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:56842 to recover
&lt;/span&gt;
##### I presume &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; is C14, and it got the request to recover
  2&amp;gt; 97044 T316 oasha.CoreAdminHandler.handleRequestRecoveryAction It has been requested that we recover

##### It looks like C2 &lt;span class=&quot;code-keyword&quot;&gt;finally&lt;/span&gt; returned failure to C4, which retries sending to the leader
  2&amp;gt; 97133 T59 C4 P48210 oasc.SolrException.log SEVERE forwarding update to http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:41533/collection1/ failed - retrying ... 
&lt;/span&gt;
##### Another timeout from the leader to that node - not sure what request &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; was...
  2&amp;gt; 97135 T44 C2 P41533 oasc.SolrException.log SEVERE shard update error StdNode: http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:56842/collection1/:org.apache.solr.client.solrj.SolrServerException: Timeout occured &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; waiting response from server at: http://127.0.0.1:56842/collection1
&lt;/span&gt;
##### But we ask the node to recover again, and it looks like the node gets that request again
  2&amp;gt; 97137 T44 C2 P41533 oasup.DistributedUpdateProcessor.doFinish &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; and ask http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:56842 to recover
&lt;/span&gt;  2&amp;gt; 97138 T44 C2 P41533 oascsi.HttpClientUtil.createClient Creating &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; http client, config:maxConnections=128&amp;amp;maxConnectionsPerHost=32&amp;amp;followRedirects=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;
  2&amp;gt; 97148 T311 oasha.CoreAdminHandler.handleRequestRecoveryAction It has been requested that we recover
 

##### Below, the request was re-sent to the leader C2 from C4
##### Either the top level cloud client timed things out and re-sent, or the command distributor in C4 re-sent.
  2&amp;gt; 97639 T47 C2 P41533 oasup.LogUpdateProcessor.processAdd PRE_UPDATE ADD add{flags=0,_version_=0,id=10079} {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:48210/collection1/&amp;amp;update.distrib=TOLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;  2&amp;gt; 97641 T47 C2 P41533 oasu.DirectUpdateHandler2.checkDocument LOCAL_ADD: id=10079 version=1423033448699789312
  2&amp;gt; 97643 T47 C2 P41533 PRE_UPDATE FINISH  {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:48210/collection1/&amp;amp;update.distrib=TOLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;  2&amp;gt; 97648 T313 C14 P56842 oasup.LogUpdateProcessor.processAdd PRE_UPDATE ADD add{flags=0,_version_=0,id=10079} {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:41533/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;  2&amp;gt; 97648 T93 C0 P56785 oasup.LogUpdateProcessor.processAdd PRE_UPDATE ADD add{flags=0,_version_=0,id=10079} {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:41533/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt; 

##### Why did &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; time out?  We saw C14 get the request... perhaps more needs to be done asynchronously on the receiving end of such a request? 
##### Also, what&apos;s happening on C14 and C0 right now???
   2&amp;gt; 102044 T46 C2 P41533 oasup.DistributedUpdateProcessor.doFinish Could not tell a replica to recover org.apache.solr.client.solrj.SolrServerException: Timeout occured &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; waiting response from server at: http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:56842
&lt;/span&gt;

##### The original hung update to C14 &lt;span class=&quot;code-keyword&quot;&gt;finally&lt;/span&gt; finished after 27 sec.
  2&amp;gt; 109516 T419 oascc.ConnectionManager.process Watcher org.apache.solr.common.cloud.ConnectionManager@58046530 name:ZooKeeperConnection Watcher:127.0.0.1:40151/solr got event WatchedEvent state:SyncConnected type:None path:&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; path:&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; type:None
  2&amp;gt; 109518 T320 oascc.ConnectionManager.waitForConnected Client is connected to ZooKeeper
  2&amp;gt; 109518 T320 oascc.ConnectionManager$1.update Connection with ZooKeeper reestablished.
  2&amp;gt; 109519 T320 oasc.RecoveryStrategy.close WARNING Stopping recovery &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; zkNodeName=127.0.0.1:56842__collection1core=collection1
 
  2&amp;gt; 109564 T313 C14 P56842 oasu.DirectUpdateHandler2.checkDocument LOCAL_ADD: id=10079 version=1423033448699789312
  2&amp;gt; 109571 T314 C14 P56842 PRE_UPDATE FINISH  {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:41533/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;  2&amp;gt; 109571 T313 C14 P56842 PRE_UPDATE FINISH  {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:41533/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;  2&amp;gt; 109571 T315 C14 P56842 PRE_UPDATE FINISH  {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:41533/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;  2&amp;gt; 109572 T314 C14 P56842 /update {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:41533/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2} {delete=[60072 (-1423033432318935040)]} 0 27538
&lt;/span&gt;  2&amp;gt; 109572 T313 C14 P56842 /update {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:41533/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2} {add=[10079 (1423033448699789312)]} 0 11925
&lt;/span&gt;  2&amp;gt; 109573 T315 C14 P56842 /update {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:41533/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2} {add=[10079 (1423033432430084096)]} 0 27446
&lt;/span&gt; 

  2&amp;gt; ###### Only in cloudDocList: [{id=10079}]

  2&amp;gt; 207369 T545 C22 P57160 REQ /select {fl=id,_version_&amp;amp;shard.url=127.0.0.1:41533/collection1/|127.0.0.1:57160/collection1/&amp;amp;NOW=1357110561123&amp;amp;tests=checkShardConsistency(vsControl)/getVers&amp;amp;q=id:(+10079)&amp;amp;ids=10079&amp;amp;distrib=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&amp;amp;isShard=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&amp;amp;wt=javabin&amp;amp;rows=100000&amp;amp;version=2} status=0 QTime=1 
  2&amp;gt; 207372 T10 oasc.AbstractFullDistribZkTestBase.checkShardConsistency SEVERE controlClient :{numFound=0,start=0,docs=[]}
  2&amp;gt; 		cloudClient :{numFound=1,start=0,docs=[SolrDocument{id=10079, _version_=1423033464957960192}]}
 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13542693" author="yseeley@gmail.com" created="Thu, 3 Jan 2013 03:55:50 +0000"  >&lt;p&gt;I&apos;ve added a testing hook for failures in the command distributor, and hooked it up for the chaos monkey safe leader test to do a thread dump on socket timeout.  Hopefully this will help nail down why some of the requests are taking so long.&lt;/p&gt;</comment>
                            <comment id="13543164" author="yseeley@gmail.com" created="Thu, 3 Jan 2013 18:31:46 +0000"  >&lt;p&gt;Here&apos;s a log with some notes on the first timeout.  Chaos monkey decides to cause connection loss to ZK at 48 sec.  At 60 sec, two requests start that don&apos;t seem to finish until 83 sec into the test.  They seem to be blocked in zkCheck().&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;  2&amp;gt; 48972 T260 oasc.ChaosMonkey.monkeyLog monkey: chose a victim! 42854
  2&amp;gt; 48973 T260 oasc.ChaosMonkey.monkeyLog monkey: expire session &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 42854 !
  2&amp;gt; 48975 T260 oasc.ChaosMonkey.monkeyLog monkey: cause connection loss!

 
  2&amp;gt; 52127 T122 C3 P42854 /update {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:51342/r_/f/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2} {delete=[60058 (-1423156803769729024)]} 0 1
&lt;/span&gt;  2&amp;gt; 60518 T120 C3 P42854 oasup.LogUpdateProcessor.processAdd PRE_UPDATE ADD add{flags=0,_version_=0,id=10063} {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:51342/r_/f/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;  2&amp;gt; 60786 T124 C3 P42854 oasup.LogUpdateProcessor.processDelete PRE_UPDATE DELETE delete{flags=0,_version_=-1423156812849348608,id=60059,commitWithin=-1} {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:51342/r_/f/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;
  2&amp;gt; 75667 T74 C6 P51342 oasc.Diagnostics.logThreadDumps SEVERE REQUESTING THREAD DUMP DUE TO TIMEOUT: Timeout occured &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; waiting response from server at: http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:42854/r_/f/collection1
&lt;/span&gt;[...]
  2&amp;gt; 	&lt;span class=&quot;code-quote&quot;&gt;&quot;qtp1333272771-124&quot;&lt;/span&gt; Id=124 TIMED_WAITING
  2&amp;gt; 		at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.sleep(Native Method)
  2&amp;gt; 		at org.apache.solr.update.processor.DistributedUpdateProcessor.zkCheck(DistributedUpdateProcessor.java:925)
  2&amp;gt; 		at org.apache.solr.update.processor.DistributedUpdateProcessor.processDelete(DistributedUpdateProcessor.java:699)
  2&amp;gt; 		at org.apache.solr.update.processor.LogUpdateProcessor.processDelete(LogUpdateProcessor.java:97)
  2&amp;gt; 		at org.apache.solr.handler.loader.XMLLoader.processDelete(XMLLoader.java:346)
  2&amp;gt; 		at org.apache.solr.handler.loader.XMLLoader.processUpdate(XMLLoader.java:277)
  2&amp;gt; 		at org.apache.solr.handler.loader.XMLLoader.load(XMLLoader.java:173)
  2&amp;gt; 		at org.apache.solr.handler.UpdateRequestHandler$1.load(UpdateRequestHandler.java:92)
[...]
  2&amp;gt; 	&lt;span class=&quot;code-quote&quot;&gt;&quot;qtp1333272771-120&quot;&lt;/span&gt; Id=120 TIMED_WAITING
  2&amp;gt; 		at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.sleep(Native Method)
  2&amp;gt; 		at org.apache.solr.update.processor.DistributedUpdateProcessor.zkCheck(DistributedUpdateProcessor.java:925)
  2&amp;gt; 		at org.apache.solr.update.processor.DistributedUpdateProcessor.processAdd(DistributedUpdateProcessor.java:330)
  2&amp;gt; 		at org.apache.solr.update.processor.LogUpdateProcessor.processAdd(LogUpdateProcessor.java:76)
  2&amp;gt; 		at org.apache.solr.handler.loader.XMLLoader.processUpdate(XMLLoader.java:246)
  2&amp;gt; 		at org.apache.solr.handler.loader.XMLLoader.load(XMLLoader.java:173)
  2&amp;gt; 		at org.apache.solr.handler.UpdateRequestHandler$1.load(UpdateRequestHandler.java:92)
  2&amp;gt; 		at org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:74)

  2&amp;gt; 83174 T124 C3 P42854 PRE_UPDATE FINISH  {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:51342/r_/f/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;  2&amp;gt; 83174 T120 C3 P42854 PRE_UPDATE FINISH  {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:51342/r_/f/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13543173" author="markrmiller@gmail.com" created="Thu, 3 Jan 2013 18:38:25 +0000"  >&lt;p&gt;zkCheck currently waits up to the length of the client timeout to see if the connection comes back before expiration...&lt;/p&gt;</comment>
                            <comment id="13543186" author="yseeley@gmail.com" created="Thu, 3 Jan 2013 18:50:04 +0000"  >&lt;p&gt;Here&apos;s another interesting tidbit:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;  2&amp;gt;    &lt;span class=&quot;code-quote&quot;&gt;&quot;recoveryExecutor-140-thread-1&quot;&lt;/span&gt; Id=320 TIMED_WAITING
  2&amp;gt;            at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.sleep(Native Method)
  2&amp;gt;            at org.apache.solr.update.processor.DistributedUpdateProcessor.zkCheck(DistributedUpdateProcessor.java:925)
  2&amp;gt;            at org.apache.solr.update.processor.DistributedUpdateProcessor.processAdd(DistributedUpdateProcessor.java:330)
  2&amp;gt;            at org.apache.solr.update.UpdateLog$LogReplayer.doReplay(UpdateLog.java:1268)
  2&amp;gt;            at org.apache.solr.update.UpdateLog$LogReplayer.run(UpdateLog.java:1159)
  2&amp;gt;            at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
  2&amp;gt;            at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
  2&amp;gt;            at java.util.concurrent.FutureTask.run(FutureTask.java:138)
  2&amp;gt;            ...
  2&amp;gt;    
  2&amp;gt;            &lt;span class=&quot;code-object&quot;&gt;Number&lt;/span&gt; of locked synchronizers = 1
  2&amp;gt;            - java.util.concurrent.locks.ReentrantLock$NonfairSync@1f2eb71e
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Seems like we shouldn&apos;t check ZK if the update is from log replay or from peersync (basically if distrib=false?)&lt;/p&gt;

&lt;p&gt;edit: I just committed &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-4257&quot; title=&quot;Don&amp;#39;t wait for ZK connection to apply replay updates or peersync updates&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-4257&quot;&gt;&lt;del&gt;SOLR-4257&lt;/del&gt;&lt;/a&gt;, which should handle these cases.&lt;/p&gt;</comment>
                            <comment id="13543979" author="yseeley@gmail.com" created="Fri, 4 Jan 2013 16:06:10 +0000"  >&lt;p&gt;Here&apos;s an analyzed log that I traced all the way to the end.&lt;br/&gt;
The issues involved are all timeout related (socket timeouts).&lt;br/&gt;
Timing out an update request in general is bad, since the request itself normally continues on and can finish at some point in the future.&lt;br/&gt;
We should strive to only time out requests that are truely / hopelessly hung.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  2&amp;gt; 54461 T256 oasc.ChaosMonkey.monkeyLog monkey: chose a victim! 35930
  2&amp;gt; 54462 T256 oasc.ChaosMonkey.monkeyLog monkey: expire session &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 35930 !
  2&amp;gt; 54473 T256 oasc.ChaosMonkey.monkeyLog monkey: cause connection loss!

# NOTE: &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; some reason, &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; didn&lt;span class=&quot;code-quote&quot;&gt;&apos;t seem to slow 35930 down... it&apos;&lt;/span&gt;s still accepting and processing updates at &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; point!

  2&amp;gt; 59545 T256 oasc.ChaosMonkey.monkeyLog monkey: chose a victim! 35930
  2&amp;gt; 59545 T256 oasc.ChaosMonkey.monkeyLog monkey: expire session &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 35930 !
  2&amp;gt; 59546 T256 oasc.ChaosMonkey.monkeyLog monkey: cause connection loss!
 

  2&amp;gt; 65367 T48 C2 P47815 PRE_UPDATE FINISH  {wt=javabin&amp;amp;version=2}
  2&amp;gt; 65367 T48 C2 P47815 oasup.LogUpdateProcessor.processAdd PRE_UPDATE ADD add{flags=0,_version_=0,id=10070} {wt=javabin&amp;amp;version=2}
# C2 is forwarding to the leader, C6 (P59996)
# NOTE: &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; is the same thread that &lt;span class=&quot;code-keyword&quot;&gt;finally&lt;/span&gt; times out

  2&amp;gt; 65372 T75 C6 P59996 oasup.LogUpdateProcessor.processAdd PRE_UPDATE ADD add{flags=0,_version_=0,id=10070} {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:47815/to_/y/collection1/&amp;amp;update.distrib=TOLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;  2&amp;gt; 65374 T75 C6 P59996 oasu.DirectUpdateHandler2.checkDocument LOCAL_ADD: id=10070 version=1423189915618770944

  2&amp;gt; 65384 T179 C8 P40531 oasup.LogUpdateProcessor.processAdd PRE_UPDATE ADD add{flags=0,_version_=0,id=10070} {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:59996/to_/y/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;  2&amp;gt; 65384 T229 C11 P44046 oasup.LogUpdateProcessor.processAdd PRE_UPDATE ADD add{flags=0,_version_=0,id=10070} {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:59996/to_/y/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;  2&amp;gt; 65384 T124 C3 P35930 oasup.LogUpdateProcessor.processAdd PRE_UPDATE ADD add{flags=0,_version_=0,id=10070} {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:59996/to_/y/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;  2&amp;gt; 65385 T179 C8 P40531 oasu.DirectUpdateHandler2.checkDocument LOCAL_ADD: id=10070 version=1423189915618770944
  2&amp;gt; 65385 T229 C11 P44046 PRE_UPDATE FINISH  {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:59996/to_/y/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;  2&amp;gt; 65387 T229 C11 P44046 /update {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:59996/to_/y/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2} {add=[10070 (1423189915618770944)]} 0 3
&lt;/span&gt;  2&amp;gt; 65445 T179 C8 P40531 /update {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:59996/to_/y/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2} {add=[10070 (1423189915618770944)]} 0 62
&lt;/span&gt;# C8 and C11 finish, but C3 (P35930) does not


  2&amp;gt; 80583 T48 C2 P47815 oasc.Diagnostics.logThreadDumps SEVERE REQUESTING THREAD DUMP DUE TO TIMEOUT: Timeout occured &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; waiting response from server at: http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:59996/to_/y/collection1
&lt;/span&gt;
  2&amp;gt; 	&lt;span class=&quot;code-quote&quot;&gt;&quot;qtp330001436-124&quot;&lt;/span&gt; Id=124 TIMED_WAITING
  2&amp;gt; 		at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.sleep(Native Method)
  2&amp;gt; 		at org.apache.solr.update.processor.DistributedUpdateProcessor.zkCheck(DistributedUpdateProcessor.java:944)
  2&amp;gt; 		at org.apache.solr.update.processor.DistributedUpdateProcessor.processAdd(DistributedUpdateProcessor.java:342)
  2&amp;gt; 		at org.apache.solr.update.processor.LogUpdateProcessor.processAdd(LogUpdateProcessor.java:76)
  2&amp;gt; 		at org.apache.solr.handler.loader.XMLLoader.processUpdate(XMLLoader.java:246)
  2&amp;gt; 		at org.apache.solr.handler.loader.XMLLoader.load(XMLLoader.java:173)
  2&amp;gt; 		at org.apache.solr.handler.UpdateRequestHandler$1.load(UpdateRequestHandler.java:92)
  2&amp;gt; 		at org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:74)
 
# C3 is waiting &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; the ZK connection

  2&amp;gt; 80926 T75 C6 P59996 oasc.SolrException.log SEVERE shard update error StdNode: http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:35930/to_/y/collection1/:org.apache.solr.client.solrj.SolrServerException: Timeout occured &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; waiting response from server at: http://127.0.0.1:35930/to_/y/collection1
&lt;/span&gt;  2&amp;gt; 80632 T73 C6 P59996 oasc.SolrException.log SEVERE shard update error StdNode: http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:35930/to_/y/collection1/:org.apache.solr.client.solrj.SolrServerException: Timeout occured &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; waiting response from server at: http://127.0.0.1:35930/to_/y/collection1
&lt;/span&gt;
  2&amp;gt; 80603 T48 C2 P47815 oasc.SolrException.log SEVERE forwarding update to http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:59996/to_/y/collection1/ failed - retrying ... 
&lt;/span&gt;  2&amp;gt; 80955 T75 C6 P59996 oasup.DistributedUpdateProcessor.doFinish &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; and ask http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:35930/to_/y to recover
&lt;/span&gt;  2&amp;gt; 80956 T73 C6 P59996 oasup.DistributedUpdateProcessor.doFinish &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; and ask http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:35930/to_/y to recover
&lt;/span&gt; 
# there are actually 2 updates failing, hence the 2 requests to recover
# It looks like C2 retries forwarding to the leader just at about the same time that the leader times out the requests to the replicas and asks them to recover

  2&amp;gt; 80965 T120 oasha.CoreAdminHandler.handleRequestRecoveryAction It has been requested that we recover
  2&amp;gt; 80971 T121 oasha.CoreAdminHandler.handleRequestRecoveryAction It has been requested that we recover


  2&amp;gt; 80972 T75 C6 P59996 /update {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:47815/to_/y/collection1/&amp;amp;update.distrib=TOLEADER&amp;amp;wt=javabin&amp;amp;version=2} {add=[10070 (1423189915618770944)]} 0 15600
&lt;/span&gt;######### This is success because we added the doc to multiple shards and asked the shard that failed to recover?

  2&amp;gt; 81461 T76 C6 P59996 oasup.LogUpdateProcessor.processAdd PRE_UPDATE ADD add{flags=0,_version_=0,id=10070} {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:47815/to_/y/collection1/&amp;amp;update.distrib=TOLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;######### Now &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; is the retry from C2 sending to the leader, but we&apos;ve already counted the previous update a success on C6 (but C2 had already timed it out)

  2&amp;gt; 81463 T76 C6 P59996 oasu.DirectUpdateHandler2.checkDocument LOCAL_ADD: id=10070 version=1423189932489310208
  2&amp;gt; 81465 T76 C6 P59996 PRE_UPDATE FINISH  {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:47815/to_/y/collection1/&amp;amp;update.distrib=TOLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;
  2&amp;gt; 81471 T122 C3 P35930 oasup.LogUpdateProcessor.processAdd PRE_UPDATE ADD add{flags=0,_version_=0,id=10070} {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:59996/to_/y/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;  2&amp;gt; 81486 T324 C15 P44046 oasup.LogUpdateProcessor.processAdd PRE_UPDATE ADD add{flags=0,_version_=0,id=10070} {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:59996/to_/y/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;  2&amp;gt; 81489 T324 C15 P44046 oasu.DirectUpdateHandler2.checkDocument LOCAL_ADD: id=10070 version=1423189932489310208
  2&amp;gt; 81497 T324 C15 P44046 /update {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:59996/to_/y/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2} {add=[10070 (1423189932489310208)]} 0 11
&lt;/span&gt;

############# The following update doesn&apos;t have &lt;span class=&quot;code-quote&quot;&gt;&quot;forward to leader&quot;&lt;/span&gt; on it!  This must be a retry from
############# the cloud client?
  2&amp;gt; 95400 T74 C6 P59996 oasup.LogUpdateProcessor.processAdd PRE_UPDATE ADD add{flags=0,_version_=0,id=10070} {wt=javabin&amp;amp;version=2}
  2&amp;gt; 95402 T74 C6 P59996 oasu.DirectUpdateHandler2.checkDocument LOCAL_ADD: id=10070 version=1423189947105411072
  2&amp;gt; 95404 T74 C6 P59996 PRE_UPDATE FINISH  {wt=javabin&amp;amp;version=2}

  2&amp;gt; 95410 T327 C15 P44046 oasup.LogUpdateProcessor.processAdd PRE_UPDATE ADD add{flags=0,_version_=0,id=10070} {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:59996/to_/y/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;  2&amp;gt; 95411 T327 C15 P44046 oasu.DirectUpdateHandler2.checkDocument LOCAL_ADD: id=10070 version=1423189947105411072
  2&amp;gt; 95414 T327 C15 P44046 PRE_UPDATE FINISH  {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:59996/to_/y/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;  2&amp;gt; 95415 T327 C15 P44046 /update {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:59996/to_/y/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2} {add=[10070 (1423189947105411072)]} 0 6
&lt;/span&gt; 
  2&amp;gt;  C16_STATE=coll:collection1 core:collection1 props:{shard=shard3, roles=&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;, state=active, core=collection1, collection=collection1, node_name=127.0.0.1:40531_to_%2Fy, base_url=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:40531/to_/y}
&lt;/span&gt;  2&amp;gt; 95420 T358 C16 P40531 oasup.LogUpdateProcessor.processAdd PRE_UPDATE ADD add{flags=0,_version_=0,id=10070} {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:59996/to_/y/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;  2&amp;gt; 95424 T358 C16 P40531 oasu.DirectUpdateHandler2.checkDocument LOCAL_ADD: id=10070 version=1423189947105411072
  2&amp;gt; 95432 T358 C16 P40531 PRE_UPDATE FINISH  {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:59996/to_/y/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;  2&amp;gt; 95433 T358 C16 P40531 /update {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:59996/to_/y/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2} {add=[10070 (1423189947105411072)]} 0 13
&lt;/span&gt;
  2&amp;gt; 95436 T74 C6 P59996 /update {wt=javabin&amp;amp;version=2} {add=[10070 (1423189947105411072)]} 0 36
############## The update is a success because apparently only C15 and C16 are active replcias of the shard?  


  2&amp;gt; 95465 T124 C3 P35930 PRE_UPDATE FINISH  {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:59996/to_/y/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;
############# Here comes the delete &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; the document, since the second add finished! (even though the first is still hung up)
############# This must mean that the cloud client timed out the request?

  2&amp;gt; 96296 T25 C12 P52041 oasup.LogUpdateProcessor.processDelete PRE_UPDATE DELETE delete{flags=0,_version_=0,id=10070,commitWithin=-1} {wt=javabin&amp;amp;version=2}
  2&amp;gt; 96297 T25 C12 P52041 /update {wt=javabin&amp;amp;version=2} {delete=[10070 (-1423189948044935168)]} 0 1
  2&amp;gt; 96300 T75 C6 P59996 oasup.LogUpdateProcessor.processDelete PRE_UPDATE DELETE delete{flags=0,_version_=0,id=10070,commitWithin=-1} {wt=javabin&amp;amp;version=2}


  2&amp;gt; 96757 T48 C2 P47815 oasc.SolrException.log SEVERE forwarding update to http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:59996/to_/y/collection1/ failed - retrying ... 
&lt;/span&gt;#### &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; to forward to the leader again, and &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; is the update that &lt;span class=&quot;code-keyword&quot;&gt;finally&lt;/span&gt; sticks

  2&amp;gt; 97306 T77 C6 P59996 oasup.LogUpdateProcessor.processAdd PRE_UPDATE ADD add{flags=0,_version_=0,id=10070} {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:47815/to_/y/collection1/&amp;amp;update.distrib=TOLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;  2&amp;gt; 97308 T77 C6 P59996 oasu.DirectUpdateHandler2.checkDocument LOCAL_ADD: id=10070 version=1423189949105045504

  2&amp;gt; 97315 T326 C15 P44046 oasup.LogUpdateProcessor.processAdd PRE_UPDATE ADD add{flags=0,_version_=0,id=10070} {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:59996/to_/y/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;  2&amp;gt; 97314 T359 C16 P40531 oasup.LogUpdateProcessor.processAdd PRE_UPDATE ADD add{flags=0,_version_=0,id=10070} {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:59996/to_/y/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;  2&amp;gt; 97321 T359 C16 P40531 oasu.DirectUpdateHandler2.checkDocument LOCAL_ADD: id=10070 version=1423189949105045504
  2&amp;gt; 97323 T359 C16 P40531 PRE_UPDATE FINISH  {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:59996/to_/y/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;  2&amp;gt; 97323 T359 C16 P40531 /update {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:59996/to_/y/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2} {add=[10070 (1423189949105045504)]} 0 10
&lt;/span&gt;  2&amp;gt; 97321 T326 C15 P44046 oasu.DirectUpdateHandler2.checkDocument LOCAL_ADD: id=10070 version=1423189949105045504
  2&amp;gt; 97325 T326 C15 P44046 PRE_UPDATE FINISH  {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:59996/to_/y/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2}
&lt;/span&gt;  2&amp;gt; 97325 T326 C15 P44046 /update {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:59996/to_/y/collection1/&amp;amp;update.distrib=FROMLEADER&amp;amp;wt=javabin&amp;amp;version=2} {add=[10070 (1423189949105045504)]} 0 10
&lt;/span&gt; 
  2&amp;gt; 97326 T77 C6 P59996 /update {distrib.from=http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:47815/to_/y/collection1/&amp;amp;update.distrib=TOLEADER&amp;amp;wt=javabin&amp;amp;version=2} {add=[10070 (1423189949105045504)]} 0 20
&lt;/span&gt;##### And now &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; update succeeds (which was a retry between non-leader and leader of the original update)


  2&amp;gt; 97328 T48 C2 P47815 /update {wt=javabin&amp;amp;version=2} {add=[10070]} 0 31962
#### This is the first (and only) time that the original update succeeded!

  2&amp;gt; ###### Only in cloudDocList: [{id=60073}, {id=10070}]
  2&amp;gt; 209316 T76 C6 P59996 REQ /select {fl=id,_version_&amp;amp;shard.url=127.0.0.1:59996/to_/y/collection1/|127.0.0.1:35930/to_/y/collection1/&amp;amp;NOW=1357259813791&amp;amp;tests=checkShardConsistency(vsControl)/getVers&amp;amp;q=id:(+60073+10070)&amp;amp;ids=60073,10070&amp;amp;distrib=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&amp;amp;isShard=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&amp;amp;wt=javabin&amp;amp;rows=100000&amp;amp;version=2} status=0 QTime=1 
  2&amp;gt; 209318 T10 oasc.AbstractFullDistribZkTestBase.checkShardConsistency SEVERE controlClient :{numFound=0,start=0,docs=[]}
  2&amp;gt; 		cloudClient :{numFound=2,start=0,docs=[SolrDocument{id=10070, _version_=1423189949105045504}, SolrDocument{id=60073, _version_=1423189949103996928}]}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;There was a lot of timeout / retry activity that could cause problems for other tests / scenarios, but this test is simpler&lt;br/&gt;
because it waits for a response to the add before moving on to possibly delete that add.  For this scenario, the&lt;br/&gt;
retry that caused the issue was from the cloud client.  It timed out it&apos;s original update and retried the update.  The retry completed.  Then the test deleted that document.  Then the &lt;b&gt;original&lt;/b&gt; update succeeded and added the doc back.&lt;/p&gt;

&lt;p&gt;Having the same timeouts on forwards to leaders as forwards from leaders has turned out to be not-so-good.  Because the former happens &lt;b&gt;before&lt;/b&gt; the latter, if a replica update hangs, the to_leader update will timeout and retry &lt;b&gt;slightly&lt;/b&gt; before the from_leader times out to the replica (and maybe succeeds by asking that replica to recover!).&lt;/p&gt;

&lt;p&gt;Q) A replica receiving a forward &lt;b&gt;from&lt;/b&gt; a leader - do we really need to have a ZK connection to accept that update?&lt;br/&gt;
Maybe so for defensive check reasons?&lt;/p&gt;

&lt;p&gt;Here&apos;s how I think we need to fix this:&lt;br/&gt;
A) We need to figure out how long an update to a replica forwarded by the leader can reasonably take.  Then we need to make the socket timeout be greater than that.&lt;br/&gt;
B) We need to figure out how long an update to a leader can take (taking into account (A)), and make the socket timeout to the leader greater than that.&lt;br/&gt;
C) We need to figure out how long an update to a non-leader (which is then forwarded to a leader) can take, and make the socket timeout from SolrJ servers to be greater than that.&lt;br/&gt;
D) Make sure that the generic Jetty socket timeouts are greater than all of the above?&lt;/p&gt;

&lt;p&gt;If it&apos;s too hard to separate all these different socket timeouts now, then the best approximation&lt;br/&gt;
would be to try and minimize the time any update can take, and raise all of the timeouts up high enough&lt;br/&gt;
such that we should never see them.&lt;/p&gt;

&lt;p&gt;We should probably also take care to only retry in certain scenarios.  For instance if we try to forward to a leader, but can&apos;t reach the leader.  We should retry on connect timeout, but never on socket timeout.&lt;/p&gt;</comment>
                            <comment id="13544018" author="markrmiller@gmail.com" created="Fri, 4 Jan 2013 16:51:28 +0000"  >&lt;p&gt;I just noticed the update so and conn timeouts being set for tests were reversed and fixed.&lt;/p&gt;</comment>
                            <comment id="13544110" author="markrmiller@gmail.com" created="Fri, 4 Jan 2013 19:04:21 +0000"  >&lt;p&gt;Since making the above change, fullmetaljenkins in &apos;choas duty mode&apos; seems to as happy as its ever been &lt;a href=&quot;http://fullmetaljenkins.org/view/Chaos/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://fullmetaljenkins.org/view/Chaos/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I&apos;ve also independently run 45 runs with an infinite timeout with no failures.&lt;/p&gt;

&lt;p&gt;Looks like we can move on to the leader kill test...&lt;/p&gt;</comment>
                            <comment id="13544112" author="markrmiller@gmail.com" created="Fri, 4 Jan 2013 19:06:08 +0000"  >&lt;p&gt;FYI, ive also raised the timeout from 30s to 60s just to be safe - we will see if that makes freebsd cry or not.&lt;/p&gt;</comment>
                            <comment id="13544826" author="commit-tag-bot" created="Sat, 5 Jan 2013 20:27:46 +0000"  >&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; Mark Robert Miller&lt;br/&gt;
&lt;a href=&quot;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1429029&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1429029&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-3180&quot; title=&quot;ChaosMonkey test failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-3180&quot;&gt;&lt;del&gt;SOLR-3180&lt;/del&gt;&lt;/a&gt;: tests: timeouts from 30 sec to 60 sec&lt;/p&gt;</comment>
                            <comment id="13544856" author="commit-tag-bot" created="Sat, 5 Jan 2013 20:28:13 +0000"  >&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; Mark Robert Miller&lt;br/&gt;
&lt;a href=&quot;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1429027&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1429027&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-3180&quot; title=&quot;ChaosMonkey test failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-3180&quot;&gt;&lt;del&gt;SOLR-3180&lt;/del&gt;&lt;/a&gt;: tests: timeouts from 30 sec to 60 sec&lt;/p&gt;</comment>
                            <comment id="13717355" author="steve_rowe" created="Tue, 23 Jul 2013 18:47:58 +0000"  >&lt;p&gt;Bulk move 4.4 issues to 4.5 and 5.0&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12625900">SOLR-4257</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12562700" name="CMSL_fail1.log" size="4246456" author="yseeley@gmail.com" created="Sat, 29 Dec 2012 23:34:13 +0000"/>
                            <attachment id="12562590" name="CMSL_hang.txt" size="132243" author="yseeley@gmail.com" created="Fri, 28 Dec 2012 17:32:59 +0000"/>
                            <attachment id="12562687" name="CMSL_hang_2.txt" size="135066" author="yseeley@gmail.com" created="Sat, 29 Dec 2012 18:33:41 +0000"/>
                            <attachment id="12562852" name="fail.130101_034142.txt" size="4764056" author="yseeley@gmail.com" created="Tue, 1 Jan 2013 18:15:59 +0000"/>
                            <attachment id="12562926" name="fail.130102_020942.txt" size="7479569" author="yseeley@gmail.com" created="Wed, 2 Jan 2013 17:12:10 +0000"/>
                            <attachment id="12563123" name="fail.130103_105104.txt" size="10223502" author="yseeley@gmail.com" created="Thu, 3 Jan 2013 18:31:46 +0000"/>
                            <attachment id="12563305" name="fail.130103_193722.txt" size="8848142" author="yseeley@gmail.com" created="Fri, 4 Jan 2013 16:06:10 +0000"/>
                            <attachment id="12562240" name="fail.inconsistent.txt" size="10046631" author="yseeley@gmail.com" created="Sat, 22 Dec 2012 18:45:22 +0000"/>
                            <attachment id="12561410" name="test_report_1.txt" size="7065256" author="yseeley@gmail.com" created="Tue, 18 Dec 2012 03:34:47 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>229811</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            12 years, 18 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i03g87:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>18062</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12313321" key="com.atlassian.jira.toolkit:message">
                        <customfieldname>Solr Mailing List Info</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>