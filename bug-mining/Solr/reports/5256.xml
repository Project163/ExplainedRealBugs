<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 05:11:33 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SOLR-17652] PULL replicas can be stuck permemantly in DOWN state if leader election takes too long</title>
                <link>https://issues.apache.org/jira/browse/SOLR-17652</link>
                <project id="12310230" key="SOLR">Solr</project>
                    <description>&lt;p&gt;A bug exists in &lt;tt&gt;ZkController&lt;/tt&gt; that can cause PULL replicas to be permanently stuck in a DOWN state (such that even a core RELOAD can not fix it) if that PULL replica was initially loaded during a leader election that takes a significant amount of time.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Details to follow in comments&lt;/p&gt;</description>
                <environment></environment>
        <key id="13607329">SOLR-17652</key>
            <summary>PULL replicas can be stuck permemantly in DOWN state if leader election takes too long</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="hossman">Chris M. Hostetter</assignee>
                                    <reporter username="hossman">Chris M. Hostetter</reporter>
                        <labels>
                    </labels>
                <created>Tue, 4 Feb 2025 20:08:54 +0000</created>
                <updated>Fri, 17 Oct 2025 20:05:05 +0000</updated>
                            <resolved>Wed, 5 Feb 2025 20:58:23 +0000</resolved>
                                                    <fixVersion>9.9</fixVersion>
                    <fixVersion>9.8.1</fixVersion>
                    <fixVersion>10.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="17923821" author="hossman" created="Tue, 4 Feb 2025 20:11:50 +0000"  >&lt;p&gt;Every now and then over the past few years, I&apos;ve heard vague rumors of weird situations involving TLOG+PULL replica based collections after &quot;catastrophic&quot; failure situations with a Solr cloud cluster&apos;s impacting the shard leader (network partitions, disk errors on leader, multi-node failures, etc...).&lt;/p&gt;

&lt;p&gt;Typically, once the underlying problem is dealt with, and the leader election eventually &quot;succeeds&quot; (either on it&apos;s own after nodes get restarted, or sometimes with a manual FORCELEADER) all of the replicas should eventually get healthy again ... BUT! ... in some of these vague rumored situations, I&apos;m told there are occasionally PULL replicas that never recover no matter how much time passes &#8211; which never really made sense to me since I know the recovery code operates in an (exponential back off loop) and because the anecdotal reports only ever described PULL replicas being affect &#8211; not TLOG replicas, but PULL replicas should never really care which TLOG replica is the leader (They recheck the current leader on every PULL attempt)&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Today I was doing some experiments with PULL replicas, and reading through some of the core initialization code, and I think i figured out where/how this kind of bug can occur:&lt;/p&gt;

&lt;p&gt;1. A node hosting a PULL replica gets restarted (as part of whatever broader catastrophic problem impacts the cluster)&lt;br/&gt;
2. the PULL replica comes online while there is no elected leader&lt;br/&gt;
3. the leader election takes longer then ~13 min (due to whatever catastrophic problem is still being dealt with, and/or slow TLOG replay on the leader)&lt;/p&gt;

&lt;p&gt;...in this case, the PULL replica will be left in a &quot;DOWN&quot; and will never attempt to recover.&lt;/p&gt;

&lt;p&gt;The cause of this problem is code in &lt;tt&gt;ZkController.register(...)&lt;/tt&gt; which expects that it can do the following in sequence:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;&lt;tt&gt;joinElection(...)&lt;/tt&gt; (If replica type is&#160; eligible)&lt;/li&gt;
	&lt;li&gt;call &lt;tt&gt;getLeader(cloudDesc, leaderVoteWait + 600000)&lt;/tt&gt; to see if the current replica is the leader
	&lt;ul&gt;
		&lt;li&gt;If this call throws a SolrException, &lt;tt&gt;ZkController.register(...)&lt;/tt&gt; propagates the exception w/o any sort of retry.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;For NRT or TLOG replicas, this logic is fine: We&apos;ve triggered an election and even if no other replicas come along and join the election, eventually we&apos;ll recognize that we&apos;re the leader and put our props into the leader znode so we can read them back. But in the case of a PULL replica, there is no guarantee if/when another replica comes along to become the leader &#8211; so if that &lt;tt&gt;getLeader(...)&lt;/tt&gt; times out, we get a log message that looks like this...&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;2025-02-04 18:38:50.124 ERROR (coreZkRegister-1-thread-3-processing-localhost:8983_solr techproducts_shard1_replica_p3 techproducts shard1 core_node4) [c:techproducts s:shard1 r:core_node4 x:techproducts_shard1_replica_p3 t:] o.a.s.c.ZkCon
troller Error getting leader from zk =&amp;gt; org.apache.solr.common.SolrException: Could not get leader props
        at org.apache.solr.cloud.ZkController.getLeaderProps(ZkController.java:1554)
org.apache.solr.common.SolrException: Could not get leader props
        at org.apache.solr.cloud.ZkController.getLeaderProps(ZkController.java:1554) ~[?:?]
        at org.apache.solr.cloud.ZkController.getLeaderProps(ZkController.java:1514) ~[?:?]
        at org.apache.solr.cloud.ZkController.getLeader(ZkController.java:1456) ~[?:?]
        at org.apache.solr.cloud.ZkController.register(ZkController.java:1318) ~[?:?]
        at org.apache.solr.cloud.ZkController.register(ZkController.java:1229) ~[?:?]
        at org.apache.solr.core.ZkContainer.lambda$registerInZk$0(ZkContainer.java:208) ~[?:?]
        at org.apache.solr.common.util.ExecutorUtil$MDCAwareThreadPoolExecutor.lambda$execute$0(ExecutorUtil.java:380) ~[?:?]
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]
        at java.base/java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /collections/techproducts/leaders/shard1/leader
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:117) ~[?:?]
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:53) ~[?:?]
        at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1972) ~[?:?]
        at org.apache.solr.common.cloud.SolrZkClient.lambda$getData$6(SolrZkClient.java:452) ~[?:?]
        at org.apache.solr.common.cloud.ZkCmdExecutor.retryOperation(ZkCmdExecutor.java:70) ~[?:?]
        at org.apache.solr.common.cloud.SolrZkClient.getData(SolrZkClient.java:452) ~[?:?]
        at org.apache.solr.cloud.ZkController.getLeaderProps(ZkController.java:1534) ~[?:?]
        ... 9 more
2025-02-04 18:38:50.125 ERROR (coreZkRegister-1-thread-3-processing-localhost:8983_solr techproducts_shard1_replica_p3 techproducts shard1 core_node4) [c:techproducts s:shard1 r:core_node4 x:techproducts_shard1_replica_p3 t:] o.a.s.c.ZkContainer Exception registering core techproducts_shard1_replica_p3 =&amp;gt; org.apache.solr.common.SolrException: Error getting leader from zk for shard shard1
        at org.apache.solr.cloud.ZkController.getLeader(ZkController.java:1499)
org.apache.solr.common.SolrException: Error getting leader from zk for shard shard1
        at org.apache.solr.cloud.ZkController.getLeader(ZkController.java:1499) ~[?:?]
        at org.apache.solr.cloud.ZkController.register(ZkController.java:1318) ~[?:?]
        at org.apache.solr.cloud.ZkController.register(ZkController.java:1229) ~[?:?]
        at org.apache.solr.core.ZkContainer.lambda$registerInZk$0(ZkContainer.java:208) ~[?:?]
        at org.apache.solr.common.util.ExecutorUtil$MDCAwareThreadPoolExecutor.lambda$execute$0(ExecutorUtil.java:380) ~[?:?]
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]
        at java.base/java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: org.apache.solr.common.SolrException: Could not get leader props
        at org.apache.solr.cloud.ZkController.getLeaderProps(ZkController.java:1554) ~[?:?]
        at org.apache.solr.cloud.ZkController.getLeaderProps(ZkController.java:1514) ~[?:?]
        at org.apache.solr.cloud.ZkController.getLeader(ZkController.java:1456) ~[?:?]
        ... 7 more
Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /collections/techproducts/leaders/shard1/leader
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:117) ~[?:?]
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:53) ~[?:?]
        at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1972) ~[?:?]
        at org.apache.solr.common.cloud.SolrZkClient.lambda$getData$6(SolrZkClient.java:452) ~[?:?]
        at org.apache.solr.common.cloud.ZkCmdExecutor.retryOperation(ZkCmdExecutor.java:70) ~[?:?]
        at org.apache.solr.common.cloud.SolrZkClient.getData(SolrZkClient.java:452) ~[?:?]
        at org.apache.solr.cloud.ZkController.getLeaderProps(ZkController.java:1534) ~[?:?]
        at org.apache.solr.cloud.ZkController.getLeaderProps(ZkController.java:1514) ~[?:?]
        at org.apache.solr.cloud.ZkController.getLeader(ZkController.java:1456) ~[?:?]
        ... 7 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;...and we get stuck in limbo where we are DOWN and stay down and never recover even if/when another replica &lt;b&gt;does&lt;/b&gt; become a leader later.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;EVEN IF YOU RELOAD THE CORE, THE REPLICA STAYS IN A DOWN STATE!&lt;/b&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;The fix (see attached patch) is fairly simple: Since the only reason &lt;tt&gt;ZkController.register(...)&lt;/tt&gt; calls &lt;tt&gt;getLeader(...)&lt;/tt&gt; is to see if the current replica &lt;em&gt;might&lt;/em&gt; be the leader, skip that when we know from the replica type that we &lt;em&gt;can&apos;t&lt;/em&gt; be the leader, and skip straight ahead to doing recovery (which &lt;tt&gt;RecoveryStrategy&lt;/tt&gt; does in a perpetual loop &#8211; even if it can&apos;t find a leader)&lt;/p&gt;</comment>
                            <comment id="17923822" author="hossman" created="Tue, 4 Feb 2025 20:13:46 +0000"  >&lt;p&gt;FYI, here&apos;s a way to demonstrate the bug using solr&apos;s example mode (commands below is from 9x, commands need modified slightly to work on main)...&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;./solr/packaging/build/dev/bin/solr start -e cloud -noprompt

# Setup our collection and both types of replicas
curl -sS &apos;http://localhost:8983/solr/admin/collections?action=CREATE&amp;amp;name=techproducts&amp;amp;numShards=1&amp;amp;tlogReplicas=1&amp;amp;createNodeSet=localhost:7574_solr&apos;
curl -sS &apos;http://localhost:8983/solr/admin/collections?action=ADDREPLICA&amp;amp;collection=techproducts&amp;amp;shard=shard1&amp;amp;node=localhost:8983_solr&amp;amp;type=PULL&apos;

./solr/packaging/build/dev/bin/post -c techproducts ./solr/packaging/build/dev/example/exampledocs/*.xml

# shut down pod hosting TLOG leader
./solr/packaging/build/dev/bin/solr stop -p 7574

# stop &amp;amp; re-start pod hosting PULL replica (and embedded zk)

./solr/packaging/build/dev/bin/solr stop -p 8983

./solr/packaging/build/dev/bin/solr start --cloud -p 8983 --solr-home &quot;/home/hossman/lucene/solr/solr/packaging/build/dev/example/cloud/node1/solr&quot; --server-dir &quot;/home/hossman/lucene/solr/solr/packaging/build/dev/server&quot;

# Wait ~13 min (until you see an exception like the one above in the logs)

# Bring back the TLOG leader pod...
./solr/packaging/build/dev/bin/solr start --cloud -p 7574 --solr-home &quot;/home/hossman/lucene/solr/solr/packaging/build/dev/example/cloud/node2/solr&quot; --server-dir &quot;/home/hossman/lucene/solr/solr/packaging/build/dev/server&quot; -z 127.0.0.1:9983

# PULL replica will still stay DOWN forever

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17923864" author="hossman" created="Tue, 4 Feb 2025 23:40:36 +0000"  >&lt;blockquote&gt;&lt;p&gt;The fix (see attached patch) is fairly simple: ...&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;All existing tests pass for me w/this patch, but i can&apos;t really think of a decent way to write a test for this specific change that wouldn&apos;t involve sleeping for at least 10 minutes.&lt;/p&gt;

&lt;p&gt;Unless someone has a specific suggestion for a test (and/or feels strongly that we shouldn&apos;t fix this bug unless we make that delay configurable?) I think this is safe to commit as is?&lt;/p&gt;</comment>
                            <comment id="17924224" author="jira-bot" created="Wed, 5 Feb 2025 19:11:40 +0000"  >&lt;p&gt;Commit bca4cd630b9cff66ecc0431397a99f5289a6462b in solr&apos;s branch refs/heads/main from Chris M. Hostetter&lt;br/&gt;
[ &lt;a href=&quot;https://gitbox.apache.org/repos/asf?p=solr.git;h=bca4cd630b9&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://gitbox.apache.org/repos/asf?p=solr.git;h=bca4cd630b9&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-17652&quot; title=&quot;PULL replicas can be stuck permemantly in DOWN state if leader election takes too long&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-17652&quot;&gt;&lt;del&gt;SOLR-17652&lt;/del&gt;&lt;/a&gt;: Fix a bug that could cause long leader elections to leave PULL replicas in DOWN state forever&lt;/p&gt;</comment>
                            <comment id="17924247" author="dsmiley" created="Wed, 5 Feb 2025 20:24:49 +0000"  >&lt;p&gt;Great investigation and simple fix!&lt;/p&gt;</comment>
                            <comment id="17924265" author="jira-bot" created="Wed, 5 Feb 2025 20:57:51 +0000"  >&lt;p&gt;Commit 9f6d7a8274ce344ae17d5405cc174085cf6be430 in solr&apos;s branch refs/heads/branch_9x from Chris M. Hostetter&lt;br/&gt;
[ &lt;a href=&quot;https://gitbox.apache.org/repos/asf?p=solr.git;h=9f6d7a8274c&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://gitbox.apache.org/repos/asf?p=solr.git;h=9f6d7a8274c&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-17652&quot; title=&quot;PULL replicas can be stuck permemantly in DOWN state if leader election takes too long&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-17652&quot;&gt;&lt;del&gt;SOLR-17652&lt;/del&gt;&lt;/a&gt;: Fix a bug that could cause long leader elections to leave PULL replicas in DOWN state forever&lt;/p&gt;

&lt;p&gt;(cherry picked from commit bca4cd630b9cff66ecc0431397a99f5289a6462b)&lt;/p&gt;</comment>
                            <comment id="17927214" author="houston" created="Fri, 14 Feb 2025 18:13:50 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hossman&quot; class=&quot;user-hover&quot; rel=&quot;hossman&quot;&gt;hossman&lt;/a&gt; should this make it into 9.8.1 (which will happen next week)?&lt;/p&gt;</comment>
                            <comment id="17927301" author="hossman" created="Fri, 14 Feb 2025 23:13:22 +0000"  >&lt;p&gt;yeah, it&apos;s a low risk fix .... testing now on &lt;tt&gt;branch_9_8&lt;/tt&gt;&lt;/p&gt;</comment>
                            <comment id="17927307" author="jira-bot" created="Sat, 15 Feb 2025 00:33:47 +0000"  >&lt;p&gt;Commit 59aaa8b75e8c7820af50a59adbfafa7398e96d24 in solr&apos;s branch refs/heads/branch_9_8 from Chris M. Hostetter&lt;br/&gt;
[ &lt;a href=&quot;https://gitbox.apache.org/repos/asf?p=solr.git;h=59aaa8b75e8&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://gitbox.apache.org/repos/asf?p=solr.git;h=59aaa8b75e8&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-17652&quot; title=&quot;PULL replicas can be stuck permemantly in DOWN state if leader election takes too long&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-17652&quot;&gt;&lt;del&gt;SOLR-17652&lt;/del&gt;&lt;/a&gt;: Fix a bug that could cause long leader elections to leave PULL replicas in DOWN state forever&lt;/p&gt;

&lt;p&gt;(cherry picked from commit bca4cd630b9cff66ecc0431397a99f5289a6462b)&lt;br/&gt;
(cherry picked from commit 9f6d7a8274ce344ae17d5405cc174085cf6be430)&lt;/p&gt;</comment>
                            <comment id="17934381" author="houston" created="Tue, 11 Mar 2025 23:42:57 +0000"  >&lt;p&gt;Closing after the 9.8.1 release&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310760">
                    <name>Testing</name>
                                                                <inwardlinks description="Discovered while testing">
                                        <issuelink>
            <issuekey id="13607679">SOLR-17656</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10001">
                    <name>dependent</name>
                                                                <inwardlinks description="is depended upon by">
                                        <issuelink>
            <issuekey id="13607679">SOLR-17656</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="13074483" name="SOLR-17652.patch" size="2114" author="hossman" created="Tue, 4 Feb 2025 20:11:48 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            35 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z1u0m0:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12313321" key="com.atlassian.jira.toolkit:message">
                        <customfieldname>Solr Mailing List Info</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>