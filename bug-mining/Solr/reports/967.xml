<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 03:29:00 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SOLR-1781] Replication index directories not always cleaned up</title>
                <link>https://issues.apache.org/jira/browse/SOLR-1781</link>
                <project id="12310230" key="SOLR">Solr</project>
                    <description>&lt;p&gt;We had the same problem as someone described in &lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/lucene-solr-user/201001.mbox/%3c222A518D-DDF5-4FC8-A02A-74D4F232B339@snooth.com%3e&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://mail-archives.apache.org/mod_mbox/lucene-solr-user/201001.mbox/%3c222A518D-DDF5-4FC8-A02A-74D4F232B339@snooth.com%3e&lt;/a&gt;. A partial copy of that message:&lt;/p&gt;

&lt;p&gt;We&apos;re using the new replication and it&apos;s working pretty well. There&apos;s  &lt;br/&gt;
one detail I&apos;d like to get some more information about.&lt;/p&gt;

&lt;p&gt;As the replication works, it creates versions of the index in the data  &lt;br/&gt;
directory. Originally we had index/, but now there are dated versions  &lt;br/&gt;
such as index.20100127044500/, which are the replicated versions.&lt;/p&gt;

&lt;p&gt;Each copy is sized in the vicinity of 65G. With our current hard drive  &lt;br/&gt;
it&apos;s fine to have two around, but 3 gets a little dicey. Sometimes  &lt;br/&gt;
we&apos;re finding that the replication doesn&apos;t always clean up after  &lt;br/&gt;
itself. I would like to understand this better, or to not have this  &lt;br/&gt;
happen. It could be a configuration issue.&lt;/p&gt;
</description>
                <environment>&lt;p&gt;Windows Server 2003 R2, Java 6b18&lt;/p&gt;</environment>
        <key id="12456787">SOLR-1781</key>
            <summary>Replication index directories not always cleaned up</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="markrmiller@gmail.com">Mark Miller</assignee>
                                    <reporter username="terjesb">Terje Sten Bjerkseth</reporter>
                        <labels>
                    </labels>
                <created>Fri, 19 Feb 2010 15:13:21 +0000</created>
                <updated>Mon, 9 May 2016 18:46:58 +0000</updated>
                            <resolved>Thu, 26 Jul 2012 14:38:04 +0000</resolved>
                                    <version>1.4</version>
                                    <fixVersion>4.0-BETA</fixVersion>
                    <fixVersion>6.0</fixVersion>
                                    <component>replication (java)</component>
                    <component>SolrCloud</component>
                        <due></due>
                            <votes>3</votes>
                                    <watches>10</watches>
                                                                                                                <comments>
                            <comment id="12835770" author="terjesb" created="Fri, 19 Feb 2010 15:15:03 +0000"  >&lt;p&gt;This patch seems to fix the problem, mostly.&lt;/p&gt;</comment>
                            <comment id="12836519" author="noble.paul" created="Mon, 22 Feb 2010 05:51:13 +0000"  >&lt;p&gt;The problem is that while you are trying to delete the original index, there may be requests in pipleline which uses the old index. If the index files are deleted those requests may fail.&lt;/p&gt;</comment>
                            <comment id="12872455" author="hossman" created="Thu, 27 May 2010 22:05:39 +0000"  >&lt;p&gt;Bulk updating 240 Solr issues to set the Fix Version to &quot;next&quot; per the process outlined in this email...&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/lucene-dev/201005.mbox/%3Calpine.DEB.1.10.1005251052040.24672@radix.cryptio.net%3E&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://mail-archives.apache.org/mod_mbox/lucene-dev/201005.mbox/%3Calpine.DEB.1.10.1005251052040.24672@radix.cryptio.net%3E&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Selection criteria was &quot;Unresolved&quot; with a Fix Version of 1.5, 1.6, 3.1, or 4.0.  email notifications were suppressed.&lt;/p&gt;

&lt;p&gt;A unique token for finding these 240 issues in the future: hossversioncleanup20100527&lt;/p&gt;</comment>
                            <comment id="12971296" author="elyograg" created="Tue, 14 Dec 2010 16:32:01 +0000"  >&lt;p&gt;I have been having this problem too, but most of the time it&apos;s fine.  After a discussion today on the mailing list, I believe it is related to a replication initiated by swapping cores on the master.&lt;/p&gt;</comment>
                            <comment id="13043723" author="rcmuir" created="Fri, 3 Jun 2011 16:46:45 +0000"  >&lt;p&gt;Bulk move 3.2 -&amp;gt; 3.3&lt;/p&gt;</comment>
                            <comment id="13106421" author="rcmuir" created="Fri, 16 Sep 2011 14:51:00 +0000"  >&lt;p&gt;3.4 -&amp;gt; 3.5&lt;/p&gt;</comment>
                            <comment id="13412136" author="hossman" created="Wed, 11 Jul 2012 22:26:07 +0000"  >&lt;p&gt;bulk fixing the version info for 4.0-ALPHA and 4.0 all affected issues have &quot;hoss20120711-bulk-40-change&quot; in comment&lt;/p&gt;</comment>
                            <comment id="13416005" author="markus17" created="Tue, 17 Jul 2012 08:32:07 +0000"  >&lt;p&gt;This happens almost daily on our SolrCloud (trunk) test cluster, we sometimes see four surpluss index directories created in a day.&lt;/p&gt;</comment>
                            <comment id="13416169" author="markrmiller@gmail.com" created="Tue, 17 Jul 2012 13:31:58 +0000"  >&lt;p&gt;Markus: on windows?&lt;/p&gt;</comment>
                            <comment id="13416183" author="markus17" created="Tue, 17 Jul 2012 13:43:49 +0000"  >&lt;p&gt;We don&apos;t have that, i should have included it in my comment. All servers run Debian GNU/Linux 6.0 and the cloud test cluster always runs with a very recent build from trunk.&lt;/p&gt;</comment>
                            <comment id="13416231" author="markrmiller@gmail.com" created="Tue, 17 Jul 2012 14:10:44 +0000"  >&lt;p&gt;I&apos;ll look into taking care of this.&lt;/p&gt;</comment>
                            <comment id="13416255" author="markrmiller@gmail.com" created="Tue, 17 Jul 2012 14:50:31 +0000"  >&lt;p&gt;I&apos;m not sure who or when, but it looks like a form of the above patch has already been committed, except instead of:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;+        &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (successfulInstall) {
+          delTree(indexDir);
+        }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;it&apos;s&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;+        &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
+          delTree(indexDir);
+        }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I think there is a problem with both. Anyway, I can duplicate this issue easy enough - I&apos;ll figure out the right fix.&lt;/p&gt;</comment>
                            <comment id="13416309" author="markrmiller@gmail.com" created="Tue, 17 Jul 2012 15:46:08 +0000"  >&lt;p&gt;Hmm...so I think I have a fix...but it involves reloading the core even if you do not replicate conf files.&lt;/p&gt;

&lt;p&gt;The problem is, unless you reload the core, it&apos;s very difficult to know when searchers are done with the old index.&lt;/p&gt;</comment>
                            <comment id="13416318" author="markus17" created="Tue, 17 Jul 2012 15:55:42 +0000"  >&lt;p&gt;Perhaps they could be cleaned up on core start or after some time has passed? &lt;/p&gt;</comment>
                            <comment id="13416330" author="markrmiller@gmail.com" created="Tue, 17 Jul 2012 16:16:35 +0000"  >&lt;blockquote&gt;&lt;p&gt;after some time has passed?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This was tempting for a moment - but it feels dirty.&lt;/p&gt;

&lt;p&gt;You could just say, hey, do it on the next core restart - but that may not come for a long time.&lt;/p&gt;

&lt;p&gt;You&apos;d really like to be able to tell when all the indexreaders on the old dir are done, but that is very difficult it turns out. The only easy way to do it is to reload the core.&lt;/p&gt;

&lt;p&gt;If you replicate configuration files, you have to reload the core anyway. I&apos;d say it&apos;s probably the way to go. It should not be that much more expensive than reloading a searcher in the larger scheme of replication.&lt;/p&gt;</comment>
                            <comment id="13416453" author="markrmiller@gmail.com" created="Tue, 17 Jul 2012 18:59:23 +0000"  >&lt;p&gt;Patch that starts reloading on replication even when config is not replicated.&lt;/p&gt;

&lt;p&gt;Also, so that existing tests will pass, the dataDir is now consistant across reloads - it won&apos;t pick up a new setting from config. &lt;br/&gt;
I conferred about this latter change with hossman in IRC, so for now, that&apos;s the way I&apos;m going.&lt;/p&gt;</comment>
                            <comment id="13417132" author="markrmiller@gmail.com" created="Wed, 18 Jul 2012 14:43:27 +0000"  >&lt;p&gt;Spoke to yonik about this real quick and he brought up the ref counting we have for Directories as an alternative to using reload. It took a couple new methods to our DirectoryFactory, but I think I have something working using that now.&lt;/p&gt;</comment>
                            <comment id="13417192" author="markrmiller@gmail.com" created="Wed, 18 Jul 2012 15:55:07 +0000"  >&lt;p&gt;Latest patch - I&apos;ll commit this soon.&lt;/p&gt;</comment>
                            <comment id="13417212" author="markrmiller@gmail.com" created="Wed, 18 Jul 2012 16:19:44 +0000"  >&lt;p&gt;Committed.&lt;/p&gt;</comment>
                            <comment id="13419032" author="markus17" created="Fri, 20 Jul 2012 10:10:25 +0000"  >&lt;p&gt;Hi, is the core reloading still part of this? I get a lot of firstSearcher events on a test node now and it won&apos;t get online. Going back to July 18th (before this patch) build works fine. Other nodes won&apos;t come online with a build from the 19th (after this patch).&lt;/p&gt;</comment>
                            <comment id="13419099" author="markrmiller@gmail.com" created="Fri, 20 Jul 2012 12:53:37 +0000"  >&lt;p&gt;No, no reload. Can you please elaborate on what not going online means. Can you share logs?&lt;/p&gt;</comment>
                            <comment id="13419114" author="markus17" created="Fri, 20 Jul 2012 13:19:27 +0000"  >&lt;p&gt;The node will never respond to HTTP requests, all ZK connections time out, very high resource consumption. I&apos;ll try provide a log snippet soon. I tried running today&apos;s build several times but one specific node refuses to `come online`. Another node did well and runs today&apos;s build.&lt;/p&gt;

&lt;p&gt;I cannot attach a file to a resolved issue. Send over mail?&lt;/p&gt;</comment>
                            <comment id="13419116" author="markrmiller@gmail.com" created="Fri, 20 Jul 2012 13:24:02 +0000"  >&lt;p&gt;Ill reopen - email is fine as well.&lt;/p&gt;</comment>
                            <comment id="13419127" author="markus17" created="Fri, 20 Jul 2012 13:31:35 +0000"  >&lt;p&gt;Log sent.&lt;/p&gt;

&lt;p&gt;This node has two shards on it and executed 2x 512 warmup queries which adds up. It won&apos;t talk to ZK (tail of the log). Restarting the node with an 18th&apos;s build works fine. Did it three times today.&lt;br/&gt;
Thanks&lt;/p&gt;</comment>
                            <comment id="13419380" author="markrmiller@gmail.com" created="Fri, 20 Jul 2012 17:44:44 +0000"  >&lt;p&gt;Hmm...i can&apos;t replicate this issue so far.&lt;/p&gt;

&lt;p&gt;Another change around then was updating to ZooKeeper 3.3.5 (bug fix update).&lt;/p&gt;

&lt;p&gt;I wouldnt expect that to be an issue - but are you just upgrading one node and not all of them?&lt;/p&gt;</comment>
                            <comment id="13419385" author="markus17" created="Fri, 20 Jul 2012 17:50:11 +0000"  >&lt;p&gt;Strange indeed. I can/could replicate it on one machine consistently and not on others. Machines weren&apos;t upgraded at the same time to prevent cluster downtime.&lt;/p&gt;

&lt;p&gt;I&apos;ll check back monday, there are two other machines left to upgrade plus the bad node.&lt;/p&gt;</comment>
                            <comment id="13419427" author="markrmiller@gmail.com" created="Fri, 20 Jul 2012 18:46:44 +0000"  >&lt;blockquote&gt;&lt;p&gt;Machines weren&apos;t upgraded at the same time to prevent cluster downtime.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah, makes sense, just wasn&apos;t sure how you went about it. I&apos;d expect a bugfix  release of zookeeper to work no problem with the previous nodes, but it&apos;s the other variable I think. They recommend upgrading with rolling restarts, so it shouldn&apos;t be the problem...&lt;/p&gt;</comment>
                            <comment id="13420495" author="markus17" created="Mon, 23 Jul 2012 08:05:06 +0000"  >&lt;p&gt;The problem is resolved. The `bad` node created several new index.&lt;span class=&quot;error&quot;&gt;&amp;#91;0-9&amp;#93;&lt;/span&gt; directories, even with this patch, and caused high I/O. I deleted the complete data directory so also the index.properties file. It loaded its index from the other nodes and no longer created many index dirs.&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;</comment>
                            <comment id="13421323" author="markus17" created="Tue, 24 Jul 2012 11:00:13 +0000"  >&lt;p&gt;One of the nodes ended up with two index directories today. Later some other nodes also didn&apos;t clean up after they got restarted.&lt;/p&gt;</comment>
                            <comment id="13421557" author="markrmiller@gmail.com" created="Tue, 24 Jul 2012 17:20:01 +0000"  >&lt;p&gt;Odd - in the first case, you are sure the indexes were there over time? For brief periods, more than once can exist...it should just end up being cleaned up when no longer in use.&lt;/p&gt;

&lt;p&gt;I can try and dig in some more, but I&apos;ll have to think a little - I don&apos;t really know where to start.&lt;/p&gt;

&lt;p&gt;My test for this issue is a test that runs a lot of instances and randomly starts and stop them. I then monitor the index directories for these 6-12 instances - I run these tests for a long time and monitor that each keeps ending up with one index dir. At some points, there are two indexes - but it always drops to one shortly later.&lt;/p&gt;

&lt;p&gt;So there still may be some hole, but I don&apos;t know where or how.&lt;/p&gt;

&lt;p&gt;If you look in the logs, perhaps you will see a bunch of &quot;Unable to delete directory : &quot; entries? It might be that it&apos;s trying but cannot. It might make sense to start using deleteOnExit as a last resort if delete fails - I just looked and the del call in SnapPuller does not do this.&lt;/p&gt;</comment>
                            <comment id="13421565" author="markus17" created="Tue, 24 Jul 2012 17:34:57 +0000"  >&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I&apos;ve deleted both today and yesterday indexes of more than a few hours old.&lt;/p&gt;

&lt;p&gt;Some are being removed indeed and some persist. I just restarted all nodes (introduced new fieldTypes and one field) and at least one node has three index directories. Others had two, some just one. Not a single node has a `unable to delete` string in the logs.&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;</comment>
                            <comment id="13421661" author="markrmiller@gmail.com" created="Tue, 24 Jul 2012 19:06:49 +0000"  >&lt;p&gt;Could you tell me a little bit about the operations you are performing on your cluster - perhaps I am missing some activity that is needed to trigger the remaining issue(s)?&lt;/p&gt;</comment>
                            <comment id="13421675" author="markus17" created="Tue, 24 Jul 2012 19:16:01 +0000"  >&lt;p&gt;Besides search and index only the occasional restart when i change some config or deploy a new build. Sometimes i need to start ZK 3.4 again because it died for some reason. Restarting Tomcat a few times in a row may be a clue here. I&apos;ll check again tomorrow if whether it&apos;s consistent.&lt;/p&gt;</comment>
                            <comment id="13421707" author="markrmiller@gmail.com" created="Tue, 24 Jul 2012 19:49:43 +0000"  >&lt;blockquote&gt;&lt;p&gt;occasional restart when i change some config&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You can also do that by reloading all the cores in the cluster - the latest collections api work has a collection reload command.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Restarting Tomcat a few times in a row may be a clue here.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah, if any docs where added while a node was down, it will replicate and perhaps get a new index dir. I&apos;m restarting jetty in my tests literally hundreds of times in a row and have not seen this yet. I also just added the search thread, but so far no luck.&lt;/p&gt;</comment>
                            <comment id="13422135" author="markus17" created="Wed, 25 Jul 2012 10:16:47 +0000"  >&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I&apos;ll restart one node with two cores.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;#cat cores/openindex_b/data/index.properties 
#index properties
#Wed Jul 25 09:58:26 UTC 2012
index=index.20120725095644707
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;# du -h cores/
4.0K    cores/lib
46M     cores/openindex_b/data/index.20120725095644707
404K    cores/openindex_b/data/tlog
46M     cores/openindex_b/data
46M     cores/openindex_b
98M     cores/openindex_a/data/index.20120725095843731
124K    cores/openindex_a/data/tlog
98M     cores/openindex_a/data
98M     cores/openindex_a
144M    cores/
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;2012-07-25 10:01:09,176 WARN &lt;span class=&quot;error&quot;&gt;&amp;#91;solr.core.SolrCore&amp;#93;&lt;/span&gt; - &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; - : New index directory detected: old=null new=/opt/solr/cores/openindex_b/data/index.20120725095644707&lt;br/&gt;
...&lt;br/&gt;
2012-07-25 10:01:17,303 WARN &lt;span class=&quot;error&quot;&gt;&amp;#91;solr.core.SolrCore&amp;#93;&lt;/span&gt; - &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; - : New index directory detected: old=null new=/opt/solr/cores/openindex_a/data/index.20120725095843731&lt;br/&gt;
...&lt;br/&gt;
2012-07-25 10:01:55,016 WARN &lt;span class=&quot;error&quot;&gt;&amp;#91;solr.core.SolrCore&amp;#93;&lt;/span&gt; - &lt;span class=&quot;error&quot;&gt;&amp;#91;RecoveryThread&amp;#93;&lt;/span&gt; - : New index directory detected: old=/opt/solr/cores/openindex_b/data/index.20120725095644707 new=/opt/solr/cores/openindex_b/data/index.20120725100120496&lt;br/&gt;
...&lt;br/&gt;
2012-07-25 10:03:35,236 WARN &lt;span class=&quot;error&quot;&gt;&amp;#91;solr.core.SolrCore&amp;#93;&lt;/span&gt; - &lt;span class=&quot;error&quot;&gt;&amp;#91;RecoveryThread&amp;#93;&lt;/span&gt; - : New index directory detected: old=/opt/solr/cores/openindex_a/data/index.20120725100220706 new=/opt/solr/cores/openindex_a/data/index.20120725100321897&lt;/p&gt;


&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;# du -h cores/
4.0K    cores/lib
46M     cores/openindex_b/data/index.20120725095644707
404K    cores/openindex_b/data/tlog
46M     cores/openindex_b/data/index.20120725100120496
91M     cores/openindex_b/data
91M     cores/openindex_b
98M     cores/openindex_a/data/index.20120725100321897
98M     cores/openindex_a/data/index.20120725100220706
124K    cores/openindex_a/data/tlog
196M    cores/openindex_a/data
196M    cores/openindex_a
287M    cores/
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A few minutes later we still have multiple index directories. No updates have been sent to the cluster during this whole scenario. Each time another directory appears it comes with a lot of I/O, on these RAM limited machines it&apos;s almost trashing because of the additional directory. It does not create another directory on each restart but sometimes does, it restarted the same machine again and now i have three dirs for each core.&lt;/p&gt;

&lt;p&gt;I&apos;ll turn on INFO logging for the node and restart it again without deleting the surpluss dirs. The master and slave versions are still the same.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;# du -h cores/
4.0K    cores/lib
46M     cores/openindex_b/data/index.20120725100813961
42M     cores/openindex_b/data/index.20120725101349376
46M     cores/openindex_b/data/index.20120725095644707
46M     cores/openindex_b/data/index.20120725101231289
404K    cores/openindex_b/data/tlog
46M     cores/openindex_b/data/index.20120725100120496
223M    cores/openindex_b/data
223M    cores/openindex_b
98M     cores/openindex_a/data/index.20120725101252920
98M     cores/openindex_a/data/index.20120725100220706
124K    cores/openindex_a/data/tlog
196M    cores/openindex_a/data
196M    cores/openindex_a
418M    cores/
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Maybe it cannot find the current index directory on start up (in my case).&lt;/p&gt;


&lt;p&gt;2012-07-25 10:13:36,125 WARN &lt;span class=&quot;error&quot;&gt;&amp;#91;solr.core.SolrCore&amp;#93;&lt;/span&gt; - &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; - : New index directory detected: old=null new=/opt/solr/cores/openindex_b/data/index.20120725101231289&lt;br/&gt;
2012-07-25 10:13:45,840 WARN &lt;span class=&quot;error&quot;&gt;&amp;#91;solr.core.SolrCore&amp;#93;&lt;/span&gt; - &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; - : New index directory detected: old=null new=/opt/solr/cores/openindex_a/data/index.20120725101252920&lt;br/&gt;
2012-07-25 10:15:41,393 WARN &lt;span class=&quot;error&quot;&gt;&amp;#91;solr.core.SolrCore&amp;#93;&lt;/span&gt; - &lt;span class=&quot;error&quot;&gt;&amp;#91;RecoveryThread&amp;#93;&lt;/span&gt; - : New index directory detected: old=/opt/solr/cores/openindex_b/data/index.20120725101231289 new=/opt/solr/cores/openindex_b/data/index.20120725101349376&lt;br/&gt;
2012-07-25 10:15:46,895 WARN &lt;span class=&quot;error&quot;&gt;&amp;#91;solr.cloud.RecoveryStrategy&amp;#93;&lt;/span&gt; - &lt;span class=&quot;error&quot;&gt;&amp;#91;main-EventThread&amp;#93;&lt;/span&gt; - : Stopping recovery for core openindex_b zkNodeName=nl2.index.openindex.io:8080_solr_openindex_b&lt;br/&gt;
2012-07-25 10:15:46,952 WARN &lt;span class=&quot;error&quot;&gt;&amp;#91;solr.core.SolrCore&amp;#93;&lt;/span&gt; - &lt;span class=&quot;error&quot;&gt;&amp;#91;RecoveryThread&amp;#93;&lt;/span&gt; - : &lt;span class=&quot;error&quot;&gt;&amp;#91;openindex_a&amp;#93;&lt;/span&gt; Error opening new searcher. exceeded limit of maxWarmingSearchers=1, try again later.&lt;br/&gt;
2012-07-25 10:15:47,298 ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;solr.cloud.RecoveryStrategy&amp;#93;&lt;/span&gt; - &lt;span class=&quot;error&quot;&gt;&amp;#91;RecoveryThread&amp;#93;&lt;/span&gt; - : Error while trying to recover.&lt;br/&gt;
org.apache.solr.common.SolrException: Error opening new searcher. exceeded limit of maxWarmingSearchers=1, try again later.&lt;br/&gt;
        at org.apache.solr.core.SolrCore.getSearcher(SolrCore.java:1365)&lt;br/&gt;
        at org.apache.solr.core.SolrCore.getSearcher(SolrCore.java:1157)&lt;br/&gt;
        at org.apache.solr.update.DirectUpdateHandler2.commit(DirectUpdateHandler2.java:560)&lt;br/&gt;
        at org.apache.solr.cloud.RecoveryStrategy.doRecovery(RecoveryStrategy.java:316)&lt;br/&gt;
        at org.apache.solr.cloud.RecoveryStrategy.run(RecoveryStrategy.java:210)&lt;br/&gt;
2012-07-25 10:15:47,299 ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;solr.cloud.RecoveryStrategy&amp;#93;&lt;/span&gt; - &lt;span class=&quot;error&quot;&gt;&amp;#91;RecoveryThread&amp;#93;&lt;/span&gt; - : Recovery failed - trying again...&lt;/p&gt;

&lt;p&gt;This is crazy &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;btw: this is today&apos;s build.&lt;/p&gt;</comment>
                            <comment id="13422257" author="markrmiller@gmail.com" created="Wed, 25 Jul 2012 13:35:22 +0000"  >&lt;p&gt;Thanks for all the info - I think perhaps this is a different issue - I&apos;ll try and look into.&lt;/p&gt;</comment>
                            <comment id="13422298" author="markrmiller@gmail.com" created="Wed, 25 Jul 2012 14:18:48 +0000"  >&lt;p&gt;Can I get more of that log? If you are not doing updates, I&apos;m surprised it&apos;s going into replication at all on restart...&lt;/p&gt;</comment>
                            <comment id="13422308" author="markus17" created="Wed, 25 Jul 2012 14:28:48 +0000"  >&lt;p&gt;Ok, I purged the logs, enabled info and started a tomcat. New indexes are created shortly after :&lt;br/&gt;
2012-07-25 10:13:36,125 WARN &lt;span class=&quot;error&quot;&gt;&amp;#91;solr.core.SolrCore&amp;#93;&lt;/span&gt; - &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; - : New index directory detected: old=null new=/opt/solr/cores/openindex_b/data/index.20120725101231289&lt;/p&gt;

&lt;p&gt;I&apos;ll send it right now.&lt;/p&gt;</comment>
                            <comment id="13422353" author="markrmiller@gmail.com" created="Wed, 25 Jul 2012 15:32:04 +0000"  >&lt;p&gt;Thanks - there is some sort of race here around opening new searches and what index dir appears. I&apos;m not sure why you see it and I don&apos;t in my tests, but it&apos;s fairly clear in your logs.&lt;/p&gt;

&lt;p&gt;I think I can address it with a simple change that ensures we are picking up the right directory to remove. I&apos;ll commit that change in a moment.&lt;/p&gt;</comment>
                            <comment id="13422526" author="markus17" created="Wed, 25 Jul 2012 19:11:42 +0000"  >&lt;p&gt;It seems this fixes the issue. I&apos;ll double check tomorrow!&lt;/p&gt;</comment>
                            <comment id="13423022" author="markus17" created="Thu, 26 Jul 2012 12:05:09 +0000"  >&lt;p&gt;Yes, the problem no longer occurs!&lt;br/&gt;
Great work! Thanks&lt;/p&gt;</comment>
                            <comment id="13423113" author="markrmiller@gmail.com" created="Thu, 26 Jul 2012 14:38:05 +0000"  >&lt;p&gt;Thansk Markus!&lt;/p&gt;</comment>
                            <comment id="13423779" author="markus17" created="Fri, 27 Jul 2012 10:17:57 +0000"  >&lt;p&gt;There&apos;s still a problem with old index directories not being cleaned up and strange replication on start up. I&apos;ll write to the ML for this, the problem is likely larger than just cleaning up.&lt;/p&gt;</comment>
                            <comment id="13586918" author="zhuojunjian" created="Tue, 26 Feb 2013 08:17:39 +0000"  >&lt;p&gt;hi&lt;br/&gt;
sure this issue is fixed in solr4.0 ?&lt;br/&gt;
we find the issue in solr4.0 in our enviroment . &lt;br/&gt;
I have created a new JIRA &quot;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-4506&quot; title=&quot;[solr4.0.0] many index.{date} dir in replication node &quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-4506&quot;&gt;&lt;del&gt;SOLR-4506&lt;/del&gt;&lt;/a&gt;&quot;.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12597245">SOLR-3592</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12436328" name="0001-Replication-does-not-always-clean-up-old-directories.patch" size="2095" author="terjesb" created="Fri, 19 Feb 2010 15:15:03 +0000"/>
                            <attachment id="12537010" name="SOLR-1781.patch" size="10223" author="markrmiller@gmail.com" created="Wed, 18 Jul 2012 15:55:07 +0000"/>
                            <attachment id="12536865" name="SOLR-1781.patch" size="3439" author="markrmiller@gmail.com" created="Tue, 17 Jul 2012 18:59:23 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2530</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            12 years, 39 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i03opj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>19436</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12313321" key="com.atlassian.jira.toolkit:message">
                        <customfieldname>Solr Mailing List Info</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>