<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 03:29:22 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SOLR-3685] Solr Cloud sometimes skipped peersync attempt and replicated instead due to tlog flags not being cleared when no updates were buffered during a previous replication.</title>
                <link>https://issues.apache.org/jira/browse/SOLR-3685</link>
                <project id="12310230" key="SOLR">Solr</project>
                    <description>&lt;p&gt;There&apos;s a serious problem with restarting nodes, not cleaning old or unused index directories and sudden replication and Java being killed by the OS due to excessive memory allocation. Since &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-1781&quot; title=&quot;Replication index directories not always cleaned up&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-1781&quot;&gt;&lt;del&gt;SOLR-1781&lt;/del&gt;&lt;/a&gt; was fixed index directories get cleaned up when a node is being restarted cleanly, however, old or unused index directories still pile up if Solr crashes or is being killed by the OS, happening here.&lt;/p&gt;

&lt;p&gt;We have a six-node 64-bit Linux test cluster with each node having two shards. There&apos;s 512MB RAM available and no swap. Each index is roughly 27MB so about 50MB per node, this fits easily and works fine. However, if a node is being restarted, Solr will consistently crash because it immediately eats up all RAM. If swap is enabled Solr will eat an additional few 100MB&apos;s right after start up.&lt;/p&gt;

&lt;p&gt;This cannot be solved by restarting Solr, it will just crash again and leave index directories in place until the disk is full. The only way i can restart a node safely is to delete the index directories and have it replicate from another node. If i then restart the node it will crash almost consistently.&lt;/p&gt;

&lt;p&gt;I&apos;ll attach a log of one of the nodes.&lt;/p&gt;</description>
                <environment>&lt;p&gt;Debian GNU/Linux Squeeze 64bit&lt;br/&gt;
Solr 5.0-SNAPSHOT 1365667M - markus - 2012-07-25 19:09:43&lt;/p&gt;</environment>
        <key id="12600450">SOLR-3685</key>
            <summary>Solr Cloud sometimes skipped peersync attempt and replicated instead due to tlog flags not being cleared when no updates were buffered during a previous replication.</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="yseeley@gmail.com">Yonik Seeley</assignee>
                                    <reporter username="markus17">Markus Jelsma</reporter>
                        <labels>
                    </labels>
                <created>Fri, 27 Jul 2012 10:28:29 +0000</created>
                <updated>Mon, 9 May 2016 18:47:19 +0000</updated>
                            <resolved>Sat, 22 Sep 2012 13:12:33 +0000</resolved>
                                    <version>4.0-ALPHA</version>
                                    <fixVersion>4.0</fixVersion>
                    <fixVersion>6.0</fixVersion>
                                    <component>replication (java)</component>
                    <component>SolrCloud</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="13423781" author="markus17" created="Fri, 27 Jul 2012 10:33:29 +0000"  >&lt;p&gt;Here&apos;s a log for a node where the Java process is being killed by the OS. I can reproduce this consistently.&lt;/p&gt;</comment>
                            <comment id="13423782" author="markus17" created="Fri, 27 Jul 2012 10:38:33 +0000"  >&lt;p&gt;I forgot to add that it doesn&apos;t matter if updates are sent to the cluster. A node will start to replicate on startup when it&apos;s update to date as well and crash subsequently.&lt;/p&gt;</comment>
                            <comment id="13423785" author="thetaphi" created="Fri, 27 Jul 2012 10:42:54 +0000"  >&lt;p&gt;How much heap do you assign to Solr&apos;s Java process (-Xmx)? 512 MB physical RAM is very few. The Jetty default is as far as I remember larger. If the OS kills processes in its OOM process killer, we cannot do so much, as those processes are killed with a hard sigkill (-9) not sigterm.&lt;/p&gt;</comment>
                            <comment id="13423786" author="markus17" created="Fri, 27 Jul 2012 10:45:16 +0000"  >&lt;p&gt;I should have added this. I allocate just 98MB to the heap and 32 to the permgen so there just 130MB allocated.&lt;/p&gt;</comment>
                            <comment id="13423787" author="thetaphi" created="Fri, 27 Jul 2012 10:46:13 +0000"  >&lt;p&gt;Is it 32 bit or 64 bit JVM?&lt;/p&gt;</comment>
                            <comment id="13423794" author="markus17" created="Fri, 27 Jul 2012 11:01:31 +0000"  >&lt;p&gt;Java 1.6.0-26 64bit, just as Linux.&lt;/p&gt;

&lt;p&gt;I should also note now that i made an error in the configuration. I thought i had reduced the DocumentCache size to 64 but the node it was testing on had a size of 1024 configured and redistributed the config over the cluster via config bootstrap.&lt;/p&gt;

&lt;p&gt;This still leaves the problem that Solr itself should run out of memory and not the OS as the cache is part of the heap. It also should clean old index directories. So this issue may consist of multiple problems.&lt;/p&gt;</comment>
                            <comment id="13423796" author="thetaphi" created="Fri, 27 Jul 2012 11:09:06 +0000"  >&lt;p&gt;OK, I wanted to come back:&lt;br/&gt;
From what I see, 96 MB of heap is very few for Solr. Tests are running with -Xmx512. But regarding memory consumtion (Java&apos;s heap OOMs), Mark Miller might know better.&lt;/p&gt;

&lt;p&gt;But Solr will not use all available RAM, as you are on 64 bit Java, Solr defaults to MMapDirectory - I recommend to read: &lt;a href=&quot;http://blog.thetaphi.de/2012/07/use-lucenes-mmapdirectory-on-64bit.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://blog.thetaphi.de/2012/07/use-lucenes-mmapdirectory-on-64bit.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It will allocate from the system only heap + what Java itsself needs. Everything else is only allocated as adress space to directly acces file system cache. So the real memory usage of Solr is not what &quot;top&quot; reports in column &quot;VIRT&quot; but in column &quot;RES&quot; (resident memory). VIRT can be much higher (multiples of system RAM) with MMapDirectoy, as it only shows virtual address space allocated. This cannot cause Kernel-OOM to get active and kill processes, if that happens you have too few RAM for kernel, Solr + tools, sorry.&lt;/p&gt;</comment>
                            <comment id="13423801" author="markus17" created="Fri, 27 Jul 2012 11:19:10 +0000"  >&lt;p&gt;Hi - i don&apos;t look at virtual memory but RESident memory. My Solr install here will eat up to 512MB RESIDENT MEMORY and is killed by the OS. The virtual memory will then be almost 800MB, while both indexes are just 27MB in size. This sounds a lot of VIRT and RES for a tiny index and tiny heap.&lt;/p&gt;

&lt;p&gt;Also, Solr will run fine and fast with just 100MB of memory, the index is still very small.&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;</comment>
                            <comment id="13423821" author="thetaphi" created="Fri, 27 Jul 2012 11:48:05 +0000"  >&lt;p&gt;I have no idea what libraries bundled by Solr do outside, but as you poroblem seems to be related to cloud, it might be another thing in JVMs: DirectMemory (allocated by ByteBuffer.allocateDirect()). By default the JVM allows up to the heap size to be allocated on this space external to heap, so your -Xmx is only half of the truth. Solr by itsself does not use direct memory (only mmapped memory, but that is not resident), but I am not sure about Zookeeper and all that cloud stuff (and maybe plugins like TIKA-extraction).&lt;/p&gt;

&lt;p&gt;You can limit direct memory with: -XX:MaxDirectMemorySize=&amp;lt;size&amp;gt;&lt;/p&gt;

&lt;p&gt;The VIRT column can contain aditionally 2-3 times your index size depending on pending commits, merges,...&lt;/p&gt;

&lt;p&gt;Please report back what this changes!&lt;/p&gt;</comment>
                            <comment id="13423841" author="markus17" created="Fri, 27 Jul 2012 12:41:27 +0000"  >&lt;p&gt;Ok, i have increased my DocumentCache again to reproduce the problem and configured from -XX:MaxDirectMemorySize=100m to 10m but RES is still climbing at the same rate as before so no change. We don&apos;t use Tika only Zookeeper.&lt;/p&gt;

&lt;p&gt;About virtual memory. That also climbes to ~800Mb which is many times more than the index size. There are no pending commits or merges right after start up.&lt;/p&gt;

&lt;p&gt;There may be some cloud replication related process that eats the RAM.&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;</comment>
                            <comment id="13424026" author="markrmiller@gmail.com" created="Fri, 27 Jul 2012 18:09:08 +0000"  >&lt;p&gt;Seems this is perhaps two or three issues here.&lt;/p&gt;

&lt;p&gt;1. The resource usage. This may just be because replication causes two searchers to be open at the same time briefly? I really don&apos;t have any guesses at the moment. &lt;/p&gt;

&lt;p&gt;2. On a non graceful shutdown, old index dirs may end up left behind. We could look at cleaning them up on startup, but that should be it&apos;s own issue.&lt;/p&gt;

&lt;p&gt;3. You claim you are replicating on startup even though the shards should be in sync. You should not be replicating in that case.&lt;/p&gt;</comment>
                            <comment id="13424785" author="markus17" created="Mon, 30 Jul 2012 10:07:49 +0000"  >&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;1. Yes, but we allow only one searcher at the same time to be warmed. This resource usage also belongs to the Java heap, it cannot cause 5x as much heap being allocated.&lt;/p&gt;

&lt;p&gt;2. Yes, i&apos;ll open a new issue and refer to this.&lt;/p&gt;

&lt;p&gt;3. Well, in some logs i clearly see a core is attempting to download and judging from the multiple index directories it&apos;s true. I am very sure no updates have been added to the cluster for a long time yet it still attempts to recover. Below is a core recovering.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;2012-07-30 09:48:36,970 INFO [solr.cloud.ZkController] - [main] - : We are http:&lt;span class=&quot;code-comment&quot;&gt;//nl2.index.openindex.io:8080/solr/openindex_a/ and leader is http://nl1.index.openindex.io:8080/solr/openindex_a/
&lt;/span&gt;2012-07-30 09:48:36,970 INFO [solr.cloud.ZkController] - [main] - : No LogReplay needed &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; core=openindex_a baseURL=http:&lt;span class=&quot;code-comment&quot;&gt;//nl2.index.openindex.io:8080/solr
&lt;/span&gt;2012-07-30 09:48:36,970 INFO [solr.cloud.ZkController] - [main] - : Core needs to recover:openindex_a
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Something noteworthy may be that for some reasons the index versions of all cores and their replica&apos;s don&apos;t match. After a restart the generation of a core is also different while it shouldn&apos;t have changed. The size in bytes is also slightly different (~20 bytes).&lt;/p&gt;

&lt;p&gt;The main thing that&apos;s concerning that Solr consumes 5x the allocated heap space in the RESident memory. Caches and such are in the heap and the MMapped index dir should be in VIRTual memory and not cause the kernel to kill the process. I&apos;m not yet sure what&apos;s going on here. Also, according to Uwe virtual memory should not be more than 2-3 times index size. In our case we see ~800Mb virtual memory for two 26Mb cores right after start up.&lt;/p&gt;

&lt;p&gt;We have only allocated 98Mb to the heap for now and this is enough for such a small index.&lt;/p&gt;</comment>
                            <comment id="13428255" author="markrmiller@gmail.com" created="Fri, 3 Aug 2012 17:43:51 +0000"  >&lt;p&gt;Is it 2 or 3 cores you have? One thing is that it won&apos;t be just one extra searcher and index - it will be that times the number of cores. All of them will attempt to recover at the same time. So you will see a bump in RAM reqs. You are talking about off heap RAM though - I don&apos;t think SolrCloud will have much to do with that.&lt;/p&gt;

&lt;p&gt;Looking at your logs, it appears that you are replicating because the transaction logs look suspect - probably because of a hard power down. If you shutdown gracefully, you would get a peer sync instead which should determine you are up to date.&lt;/p&gt;

&lt;p&gt;The comment for the path you are taking says:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;        // last operation at the time of startup had the GAP flag set...&lt;br/&gt;
        // this means we were previously doing a full index replication&lt;br/&gt;
        // that probably didn&apos;t complete and buffering updates in the meantime.&lt;/p&gt;&lt;/blockquote&gt;</comment>
                            <comment id="13428310" author="markrmiller@gmail.com" created="Fri, 3 Aug 2012 19:20:43 +0000"  >&lt;blockquote&gt;&lt;p&gt;Looking at your logs, it appears that you are replicating because the transaction logs look suspect - probably because of a hard power down. If you shutdown gracefully, you would get a peer sync instead which should determine you are up to date.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Alright, I just saw a similar thing happen even shutting everyone down gracefully.&lt;/p&gt;

&lt;p&gt;I think it&apos;s likely our kind of un-orderly cluster shutdown. If you shutdown all the nodes at once, depending on some timing differences, some recoveries may trigger as the leader goes down. Then the replica would go down.&lt;/p&gt;

&lt;p&gt;In my case though, I was working with a 3 shard 2 replica cluster - so I don&apos;t think that was likely the issue. If one node goes down, there is no one to recover from.&lt;/p&gt;

&lt;p&gt;We need to investigate a bit more.&lt;/p&gt;</comment>
                            <comment id="13429132" author="markus17" created="Mon, 6 Aug 2012 13:11:57 +0000"  >&lt;p&gt;Each node has two cores and allow only one warming searcher at any time. The problem is triggered on start up after graceful shutdown as well as a hard power off. I&apos;ve seen it happening not only when the whole cluster if restarted (i don&apos;t think i&apos;ve ever done that) but just one node of the 6 shard 2 replica test cluster.&lt;/p&gt;

&lt;p&gt;The attached log is of one node being restarted out of the whole cluster.&lt;/p&gt;

&lt;p&gt;Could the off-heap RAM be part of data being sent over the wire?&lt;/p&gt;

&lt;p&gt;We&apos;ve worked around the problem for now by getting more RAM.&lt;/p&gt;</comment>
                            <comment id="13430400" author="markrmiller@gmail.com" created="Tue, 7 Aug 2012 16:25:45 +0000"  >&lt;p&gt;I was off a bit - even a non graceful shutdown should not cause this - if you are not indexing when you shutdown, at worst nodes should sync - not replicate.&lt;/p&gt;

&lt;p&gt;In my testing, I could easily replicate this though - replication recoveries when it should be a sync.&lt;/p&gt;

&lt;p&gt;Yonik recently committed a fix to this on trunk.&lt;/p&gt;</comment>
                            <comment id="13430438" author="markus17" created="Tue, 7 Aug 2012 17:05:02 +0000"  >&lt;p&gt;When exactly? Do you have an issue?&lt;/p&gt;</comment>
                            <comment id="13430448" author="markrmiller@gmail.com" created="Tue, 7 Aug 2012 17:13:17 +0000"  >&lt;p&gt;It was tagged to this issue number:&lt;/p&gt;

&lt;p&gt;+* &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-3685&quot; title=&quot;Solr Cloud sometimes skipped peersync attempt and replicated instead due to tlog flags not being cleared when no updates were buffered during a previous replication.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-3685&quot;&gt;&lt;del&gt;SOLR-3685&lt;/del&gt;&lt;/a&gt;: Solr Cloud sometimes skipped peersync attempt and replicated instead due&lt;br/&gt;
+  to tlog flags not being cleared when no updates were buffered during a previous&lt;br/&gt;
+  replication.  (Markus Jelsma, Mark Miller, yonik)&lt;/p&gt;</comment>
                            <comment id="13430450" author="markrmiller@gmail.com" created="Tue, 7 Aug 2012 17:15:41 +0000"  >&lt;p&gt;I think we still need to make an issue for cleaning up replication directories on non graceful shutdown.&lt;/p&gt;

&lt;p&gt;I&apos;ll rename this issue to match the recovery issue.&lt;/p&gt;

&lt;p&gt;And we can create a new issue for the memory thing (I tried to spot that locally, but have not yet).&lt;/p&gt;</comment>
                            <comment id="13436182" author="markus17" created="Thu, 16 Aug 2012 18:15:07 +0000"  >&lt;p&gt;Finally! Two nodes failed again and got killed by the OS. All nodes have a lot of off-heap RES memory, sometimes 3x higher than the heap which is a meager 256MB.&lt;/p&gt;

&lt;p&gt;Got a name suggestion for the memory issue? I&apos;ll open one tomorrow and link to this one.&lt;/p&gt;</comment>
                            <comment id="13436187" author="markrmiller@gmail.com" created="Thu, 16 Aug 2012 18:19:52 +0000"  >&lt;p&gt;Are there any crash dump files? I don&apos;t think I&apos;ve seen a java process crash without seeing one of these.&lt;/p&gt;</comment>
                            <comment id="13436194" author="markus17" created="Thu, 16 Aug 2012 18:23:50 +0000"  >&lt;p&gt;One node also got rsyslogd killed but the other survived. I assume the OOMkiller output of Linux is what you refer to?&lt;/p&gt;</comment>
                            <comment id="13436200" author="markus17" created="Thu, 16 Aug 2012 18:31:11 +0000"  >&lt;p&gt;Here&apos;s the relevant part of syslog for a node where Tomcat is killed by the OS. There is 1G or available RAM, no configured swap and the heap size is 256MB. The node has two running cores.&lt;/p&gt;

&lt;p&gt;The off-heap RES memory for the Java process sometimes gets so large that Linux decides to kill it.&lt;/p&gt;</comment>
                            <comment id="13436209" author="yseeley@gmail.com" created="Thu, 16 Aug 2012 18:40:25 +0000"  >&lt;p&gt;May also want to try specifying NIOFSDirectoryFactory in solrconfig.xml to see if it&apos;s related to mmap?&lt;/p&gt;</comment>
                            <comment id="13436214" author="markus17" created="Thu, 16 Aug 2012 18:47:25 +0000"  >&lt;p&gt;We didn&apos;t think mmap could be the cause but nevertheless we tried that once on a smaller cluster and got a lot of memory consumption again, after which it got killed.&lt;br/&gt;
I can see if i can run one or two of the nodes with NIOFS but let the other run with mmap. We don&apos;t automatically restart cores so it should run fine if we temporarily change the config in zookeeper and restart two nodes.&lt;/p&gt;

&lt;p&gt;edit: each core has a ~2.5GB index.&lt;/p&gt;</comment>
                            <comment id="13436243" author="thetaphi" created="Thu, 16 Aug 2012 19:13:18 +0000"  >&lt;p&gt;Hi,&lt;br/&gt;
I also don&apos;t think MMap is the reason for this, but it&apos;s good that you test it. You are saying that this happened with NIOFS, too, so my only guess is:&lt;/p&gt;

&lt;p&gt;As noted before (in my last comment), there seems to be something using off-heap memory (RES does not contain mmap, so if RES raises, its definitely not mmap), but other &quot;direct memory&quot;. I am not sure about other components in solr, that might use direct memory. Maybe Zookeeper? Its hard to find those things in external libraries. Can you try to limit the -XX:MaxDirectMemorySize to zero and see if exceptions occur? Also it would be good to have the output of &quot;pmap &amp;lt;pid&amp;gt;&quot;, this shows allocated and mapped memory, we should look at anonymous mappings and how many are there. Pmap is in procutils package.&lt;/p&gt;</comment>
                            <comment id="13436648" author="markus17" created="Fri, 17 Aug 2012 10:25:17 +0000"  >&lt;p&gt;Here&apos;s the pmap for one node. Heap Xmx is still 256M. I also just noticed this node still has OpenJDK 6 running instead of Sun Java 6 like the other nodes. Despite that difference the memory consumption is equal.&lt;br/&gt;
I&apos;ll also restart a node with NIOFS but i still expect memory to increase as with mmap.&lt;/p&gt;</comment>
                            <comment id="13437790" author="markus17" created="Mon, 20 Aug 2012 11:19:25 +0000"  >&lt;p&gt;To my surprise the RES for all nodes except the NIOFS node increased slowly over the past three days and were still increasing today. The mmapped nodes used sometimes up to three times the Xmx and, for some reason, about 1/2 Xmx of shared memory. We just restarted all nodes with 9 using mmap and one using NIO, after restart the mmapped nodes immediately start to use a lot more RES than the NIO node. The NIO node also uses much less shared memory.&lt;/p&gt;

&lt;p&gt;Perhaps what i&apos;ve seen before with NIO also crashing was due to some other issue.&lt;/p&gt;

&lt;p&gt;So what we&apos;re seeing here is the mmapped nodes use more RES and SHR than the NIO node. VIRT is as expected. I&apos;ll change another node to NIO and keep them running again for the next few days and keep sending documents and firing queries.&lt;/p&gt;

&lt;p&gt;All nodes are using august 20th trunk from now on.&lt;/p&gt;
</comment>
                            <comment id="13439666" author="thetaphi" created="Wed, 22 Aug 2012 16:40:56 +0000"  >&lt;p&gt;Hi,&lt;br/&gt;
another thing to take into account: Java allocates on Linux for every thread a thread-local amount of memory (you can see this in the pmap output as additional &quot;[ anon ]&quot; mappings with a fixed size - it is only strange that you pmap does not populate the first hunman-readable column; I think you used -x instead of no option). This sums up to a lot of memory!&lt;/p&gt;

&lt;p&gt;See &lt;a href=&quot;http://mail.openjdk.java.net/pipermail/hotspot-compiler-dev/2012-August/008270.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://mail.openjdk.java.net/pipermail/hotspot-compiler-dev/2012-August/008270.html&lt;/a&gt; (I don&apos;t think it is 64 MB, but for sure 1 MB)&lt;/p&gt;

&lt;p&gt;On my machine, the threads take each 1 MB. Depending on your Tomcat config and its thread pools this can take lot of memory, too.&lt;/p&gt;</comment>
                            <comment id="13457914" author="rcmuir" created="Tue, 18 Sep 2012 16:17:19 +0000"  >&lt;p&gt;Whats happening with this issue: is it still one? should it be critical/block 4.0?&lt;/p&gt;</comment>
                            <comment id="13458679" author="markus17" created="Wed, 19 Sep 2012 13:27:34 +0000"  >&lt;blockquote&gt;&lt;p&gt;So what we&apos;re seeing here is the mmapped nodes use more RES and SHR than the NIO node. VIRT is as expected. I&apos;ll change another node to NIO and keep them running again for the next few days and keep sending documents and firing queries.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;there is still an issue with mmap and high RES opposed to NIOFS but the actual issue is already resolved. I&apos;ll open a new issue.&lt;/p&gt;</comment>
                            <comment id="13654248" author="thetaphi" created="Fri, 10 May 2013 10:34:32 +0000"  >&lt;p&gt;Closed after release.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12538147" name="info.log" size="448585" author="markus17" created="Fri, 27 Jul 2012 10:33:29 +0000"/>
                            <attachment id="12541260" name="oom-killer.log" size="14443" author="markus17" created="Thu, 16 Aug 2012 18:31:11 +0000"/>
                            <attachment id="12541345" name="pmap.log" size="50451" author="markus17" created="Fri, 17 Aug 2012 10:25:17 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>243298</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            12 years, 28 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i03d7z:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>17575</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12313321" key="com.atlassian.jira.toolkit:message">
                        <customfieldname>Solr Mailing List Info</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>