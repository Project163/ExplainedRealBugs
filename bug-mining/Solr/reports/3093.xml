<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 04:16:17 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SOLR-9438] Shard split can lose data</title>
                <link>https://issues.apache.org/jira/browse/SOLR-9438</link>
                <project id="12310230" key="SOLR">Solr</project>
                    <description>&lt;p&gt;Solr&#8217;s shard split can lose documents if the parent/sub-shard leader is killed (or crashes) between the time that the new sub-shard replica is created and before it recovers. In such a case the slice has already been set to &#8216;recovery&#8217; state, the sub-shard replica comes up, finds that no other replica is up, waits until the leader vote wait time and then proceeds to become the leader as well as publish itself as active. If the former leader node comes back online, the overseer seeing that all replicas of the sub-shard are now &#8216;active&#8217;, sets the parent slice as &#8216;inactive&#8217; and the new sub-shard as &#8216;active&#8217;.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12999801">SOLR-9438</key>
            <summary>Shard split can lose data</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="shalin">Shalin Shekhar Mangar</assignee>
                                    <reporter username="shalin">Shalin Shekhar Mangar</reporter>
                        <labels>
                            <label>difficulty-medium</label>
                            <label>impact-high</label>
                    </labels>
                <created>Wed, 24 Aug 2016 21:00:34 +0000</created>
                <updated>Wed, 2 Oct 2019 17:23:22 +0000</updated>
                            <resolved>Mon, 12 Sep 2016 11:53:44 +0000</resolved>
                                    <version>4.10.4</version>
                    <version>5.5.2</version>
                    <version>6.1</version>
                    <version>6.2</version>
                                    <fixVersion>6.2.1</fixVersion>
                    <fixVersion>6.3</fixVersion>
                    <fixVersion>7.0</fixVersion>
                                    <component>SolrCloud</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                                                            <comments>
                            <comment id="15435682" author="shalinmangar" created="Wed, 24 Aug 2016 21:04:12 +0000"  >&lt;p&gt;A simple fix is for the overseer to check live node information before setting the parent shard as &#8216;invalid&#8217;. This will work because by the time the leader vote wait period expires, the killed former-leader&#8217;s ephemeral nodes should have expired.&lt;/p&gt;

&lt;p&gt;But it gets trickier if the leader comes back online and recovers from this new (incomplete) replica. This will again mark the sub-shard as active. To prevent this, the overseer must ensure that the live node of the sub-shard leader still exists (with the same sequence number assigned at the time of split) before changing the sub-slice state to active.&lt;/p&gt;</comment>
                            <comment id="15435688" author="shalinmangar" created="Wed, 24 Aug 2016 21:07:40 +0000"  >&lt;p&gt;We should also mark the slice to recovery_failed in case we find the live_node has changed. Any sub-shard in this new state should not be forwarded updates. It will also be a clear indication that the shard split operation has failed and must be re-tried.&lt;/p&gt;</comment>
                            <comment id="15435733" author="shalinmangar" created="Wed, 24 Aug 2016 21:23:22 +0000"  >&lt;p&gt;The test which attempts to tickle this. This is not deterministic as it tries to kill the leader at just the right time. We retry a few times and timeout after 2 minutes. Beasting this test around 50 times has reproduced the bug once. This test works around &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-9440&quot; title=&quot;ZkStateReader on a client can cache collection state and never refresh it&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-9440&quot;&gt;&lt;del&gt;SOLR-9440&lt;/del&gt;&lt;/a&gt; by calling cloudClient.getZkStateReader().registerCore(&quot;collection1&quot;);&lt;/p&gt;</comment>
                            <comment id="15446247" author="shalinmangar" created="Mon, 29 Aug 2016 15:52:04 +0000"  >&lt;p&gt;Patch with the test updated to master.&lt;/p&gt;</comment>
                            <comment id="15450041" author="shalinmangar" created="Tue, 30 Aug 2016 20:19:19 +0000"  >&lt;p&gt;Beasting this test sometimes fails with nodes not recovering even after working around &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-9440&quot; title=&quot;ZkStateReader on a client can cache collection state and never refresh it&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-9440&quot;&gt;&lt;del&gt;SOLR-9440&lt;/del&gt;&lt;/a&gt;. I finally found the cause:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;  [beaster]   2&amp;gt; 232316 ERROR (coreZkRegister-123-thread-1-processing-n:127.0.0.1:54683_ x:collection1_shard1_0_replica0 s:shard1_0 c:collection1 r:core_node7) [n:127.0.0.1:54683_ c:collection1 s:shard1_0 r:core_node7 x:collection1_shard1_0_replica0] o.a.s.c.ZkContainer :org.apache.solr.common.SolrException: Error getting leader from zk &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; shard shard1_0
  [beaster]   2&amp;gt; 	at org.apache.solr.cloud.ZkController.getLeader(ZkController.java:994)
  [beaster]   2&amp;gt; 	at org.apache.solr.cloud.ZkController.register(ZkController.java:900)
  [beaster]   2&amp;gt; 	at org.apache.solr.cloud.ZkController.register(ZkController.java:843)
  [beaster]   2&amp;gt; 	at org.apache.solr.core.ZkContainer.lambda$registerInZk$0(ZkContainer.java:181)
  [beaster]   2&amp;gt; 	at org.apache.solr.common.util.ExecutorUtil$MDCAwareThreadPoolExecutor.lambda$execute$0(ExecutorUtil.java:229)
  [beaster]   2&amp;gt; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  [beaster]   2&amp;gt; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  [beaster]   2&amp;gt; 	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
  [beaster]   2&amp;gt; Caused by: org.apache.solr.common.SolrException: There is conflicting information about the leader of shard: shard1_0 our state says:http:&lt;span class=&quot;code-comment&quot;&gt;//127.0.0.1:54683/collection1_shard1_0_replica1/ but zookeeper says:http://127.0.0.1:49547/collection1_shard1_0_replica1/
&lt;/span&gt;  [beaster]   2&amp;gt; 	at org.apache.solr.cloud.ZkController.getLeader(ZkController.java:975)
  [beaster]   2&amp;gt; 	... 7 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The problem is that restarting the node (which assigns a new port number) sometimes confuses the hell out of SolrCloud and then such nodes keep their old port number in cluster state and never recover, can&apos;t elect leaders etc. I have a suspicion that this behavior is intentional. I&apos;ll keep digging.&lt;/p&gt;</comment>
                            <comment id="15465468" author="shalinmangar" created="Mon, 5 Sep 2016 17:40:14 +0000"  >&lt;p&gt;This log is from a run which reproduced this bug. A newly created replica logs the following:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;38737 INFO  (parallelCoreAdminExecutor-8-thread-1-processing-n:127.0.0.1:42309_ 9208de91-9c97-4a42-94f5-9e00e3b6189b388000949708638 CREATE) [n:127.0.0.1:42309_ c:collection1 s:shard1_1 r:core_node8 x:collection1_shard1_1_replica0] o.a.s.c.ShardLeaderElectionContext Was waiting &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; replicas to come up, but they are taking too &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; - assuming they won&apos;t come back till later
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After this point, this replica becomes the leader (with 0 docs inside!) and eventually when the old replica comes back up, it syncs with this empty index and loses all data except for whatever was indexed after the split.&lt;/p&gt;</comment>
                            <comment id="15467650" author="shalinmangar" created="Tue, 6 Sep 2016 15:11:00 +0000"  >&lt;p&gt;The attached &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-9438&quot; title=&quot;Shard split can lose data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-9438&quot;&gt;&lt;del&gt;SOLR-9438&lt;/del&gt;&lt;/a&gt;-false-replication.log shows another kind of failure. &lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;We create the sub-shard replica and restart the leader node&lt;/li&gt;
	&lt;li&gt;The leader comes back online. The replica tries to recover.&lt;/li&gt;
	&lt;li&gt;Leader reports its version as 0&lt;/li&gt;
	&lt;li&gt;Replica seeing the master version as 0, assumes it is an empty index, reports the replication successful&lt;/li&gt;
	&lt;li&gt;sub-shard becomes active.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;The root cause is that after split we do not commit and so the commit timestamp (used for version checks) is not written to the index. If the leader is restarted, the IndexWriter.close calls a commit on close. Upon restart, the leader will report its version as 0 even though it contains data.&lt;/p&gt;</comment>
                            <comment id="15467657" author="shalinmangar" created="Tue, 6 Sep 2016 15:14:01 +0000"  >&lt;p&gt;A scenario related to the above that I ran into is that if this new replica becomes the leader, any one else trying to replicate from the leader will also report replication successful with 0 docs.&lt;/p&gt;</comment>
                            <comment id="15468555" author="shalinmangar" created="Tue, 6 Sep 2016 21:01:42 +0000"  >&lt;p&gt;Patch with a simplified testSplitWithChaosMonkey() that doesn&apos;t restart repeatedly (to avoid the port change problems) and yet reproduces all bugs on beasting.&lt;/p&gt;

&lt;p&gt;I also added another test called testSplitStaticIndexReplication which tickles the bug related to commit data not being present. A fix for this is also included.&lt;/p&gt;

&lt;p&gt;There are still a few nocommits.&lt;/p&gt;</comment>
                            <comment id="15474984" author="shalinmangar" created="Thu, 8 Sep 2016 20:59:25 +0000"  >&lt;p&gt;Changes:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;We record the parent leader node name and the ephemeral owner of its live node (zk session id which created the live node) at the start of the split process.&lt;/li&gt;
	&lt;li&gt;These two pieces of information called &quot;shard_parent_node&quot; and &quot;shard_parent_zk_session&quot; respectively, are stored in the cluster state along with the slice information.&lt;/li&gt;
	&lt;li&gt;When all replicas of all sub-shards are live, the overseer checks if the parent leader node is still live and if its ephemeral owner is still the same. If yes, it switches the sub-shard states to active and parent to inactive. If not, it  changes the sub-shard state to a newly introduced &quot;recovery_failed&quot; state.&lt;/li&gt;
	&lt;li&gt;Any shard in &quot;recovery_failed&quot; state does not receive any indexing or querying traffic.&lt;/li&gt;
	&lt;li&gt;I beefed up the test to check for both outcomes and to assert that all documents that were successfully indexed are visible on a distributed search. Additionally, if the split succeeds, we also assert that all replicas of the sub-shards are consistent i.e. have the same number of docs.&lt;/li&gt;
	&lt;li&gt;Fixed a test bug where concurrent watcher invocations on collection state would shutdown the leader node again even after the test had restarted it already to assert document counts.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Results of beasting are looking good as far as this particular bug is concerned, but there is a curious failure where one and only core stays down and times out the waiting for recovery check. I&apos;m still digging.&lt;/p&gt;</comment>
                            <comment id="15477607" author="shalinmangar" created="Fri, 9 Sep 2016 17:05:50 +0000"  >&lt;p&gt;The test failure due to only one node remaining down was because sometimes the parent leader node itself is selected to host the new sub-shard replica. When we shutdown that node at the right time, the add replica call fails but the replica has already been created in the cluster state. Since the physical core doesn&apos;t actually exist, it will never recover and stay in down state.&lt;/p&gt;

&lt;p&gt;Changes:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;The test now checks if all replicas actually exist as a core before we wait for recovery and for sub-shards to switch states.&lt;/li&gt;
	&lt;li&gt;The split shard API puts sub-shards into recovery_failed state itself if the parent leader changes before any replicas can be created.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;I&apos;ve been beasting this test and so far everything looks good but I&apos;ll continue beasting for a little while more.&lt;/p&gt;</comment>
                            <comment id="15483810" author="shalinmangar" created="Mon, 12 Sep 2016 11:06:27 +0000"  >&lt;p&gt;Fixed comments and javadocs in a few places. I beasted this test overnight for more than 500 runs and everything looks good! I&apos;ll commit this shortly.&lt;/p&gt;</comment>
                            <comment id="15483857" author="jira-bot" created="Mon, 12 Sep 2016 11:31:05 +0000"  >&lt;p&gt;Commit f177a660f5745350207dc61b46396b49404fd383 in lucene-solr&apos;s branch refs/heads/master from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=shalin&quot; class=&quot;user-hover&quot; rel=&quot;shalin&quot;&gt;shalin&lt;/a&gt;&lt;br/&gt;
[ &lt;a href=&quot;https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f177a66&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f177a66&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-9438&quot; title=&quot;Shard split can lose data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-9438&quot;&gt;&lt;del&gt;SOLR-9438&lt;/del&gt;&lt;/a&gt;: Shard split can be marked successful and sub-shard states switched to &apos;active&apos; even when one or more sub-shards replicas do not recover due to the leader crashing or restarting between the time the replicas are created and before they can recover&lt;/p&gt;</comment>
                            <comment id="15483874" author="jira-bot" created="Mon, 12 Sep 2016 11:38:54 +0000"  >&lt;p&gt;Commit 1b03c940398de384c60fb0083a82ddb601db3909 in lucene-solr&apos;s branch refs/heads/branch_6x from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=shalin&quot; class=&quot;user-hover&quot; rel=&quot;shalin&quot;&gt;shalin&lt;/a&gt;&lt;br/&gt;
[ &lt;a href=&quot;https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=1b03c94&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=1b03c94&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-9438&quot; title=&quot;Shard split can lose data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-9438&quot;&gt;&lt;del&gt;SOLR-9438&lt;/del&gt;&lt;/a&gt;: Shard split can be marked successful and sub-shard states switched to &apos;active&apos; even when one or more sub-shards replicas do not recover due to the leader crashing or restarting between the time the replicas are created and before they can recover&lt;/p&gt;

&lt;p&gt;(cherry picked from commit f177a66)&lt;/p&gt;</comment>
                            <comment id="15483908" author="jira-bot" created="Mon, 12 Sep 2016 11:51:28 +0000"  >&lt;p&gt;Commit 8027eb9803c2248a427f45c3771d1cb88d57f4b1 in lucene-solr&apos;s branch refs/heads/branch_6_2 from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=shalin&quot; class=&quot;user-hover&quot; rel=&quot;shalin&quot;&gt;shalin&lt;/a&gt;&lt;br/&gt;
[ &lt;a href=&quot;https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=8027eb9&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=8027eb9&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-9438&quot; title=&quot;Shard split can lose data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-9438&quot;&gt;&lt;del&gt;SOLR-9438&lt;/del&gt;&lt;/a&gt;: Shard split can be marked successful and sub-shard states switched to &apos;active&apos; even when one or more sub-shards replicas do not recover due to the leader crashing or restarting between the time the replicas are created and before they can recover&lt;/p&gt;

&lt;p&gt;(cherry picked from commit f177a66)&lt;/p&gt;

&lt;p&gt;(cherry picked from commit 1b03c94)&lt;/p&gt;</comment>
                            <comment id="15508554" author="shalinmangar" created="Wed, 21 Sep 2016 03:03:30 +0000"  >&lt;p&gt;Closing after 6.2.1 release&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12999805">SOLR-9439</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13000413">SOLR-9445</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13001032">SOLR-9455</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12827206" name="SOLR-9438-false-replication.log" size="1031587" author="shalin" created="Tue, 6 Sep 2016 15:11:00 +0000"/>
                            <attachment id="12827089" name="SOLR-9438-split-data-loss.log" size="1048577" author="shalin" created="Mon, 5 Sep 2016 17:40:14 +0000"/>
                            <attachment id="12828022" name="SOLR-9438.patch" size="27403" author="shalin" created="Mon, 12 Sep 2016 11:06:27 +0000"/>
                            <attachment id="12827783" name="SOLR-9438.patch" size="26583" author="shalin" created="Fri, 9 Sep 2016 17:05:50 +0000"/>
                            <attachment id="12827653" name="SOLR-9438.patch" size="22487" author="shalin" created="Thu, 8 Sep 2016 20:59:25 +0000"/>
                            <attachment id="12827253" name="SOLR-9438.patch" size="21977" author="shalin" created="Tue, 6 Sep 2016 21:01:42 +0000"/>
                            <attachment id="12826022" name="SOLR-9438.patch" size="8720" author="shalin" created="Mon, 29 Aug 2016 15:52:04 +0000"/>
                            <attachment id="12825340" name="SOLR-9438.patch" size="8901" author="shalin" created="Wed, 24 Aug 2016 21:23:22 +0000"/>
                    </attachments>
                <subtasks>
                            <subtask id="13003231">SOLR-9488</subtask>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>8.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 9 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i32qyn:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12313321" key="com.atlassian.jira.toolkit:message">
                        <customfieldname>Solr Mailing List Info</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>