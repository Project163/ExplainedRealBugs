<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 04:07:46 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SOLR-8069] Ensure that only the valid ZooKeeper registered leader can put a replica into Leader Initiated Recovery.</title>
                <link>https://issues.apache.org/jira/browse/SOLR-8069</link>
                <project id="12310230" key="SOLR">Solr</project>
                    <description>&lt;p&gt;I&apos;ve seen this twice now. Need to work on a test.&lt;/p&gt;

&lt;p&gt;When some issues hit all the replicas at once, you can end up in a situation where the rightful leader was put or put itself into LIR. Even on restart, this rightful leader won&apos;t take leadership and you have to manually clear the LIR nodes.&lt;/p&gt;

&lt;p&gt;It seems that if all the replicas participate in election on startup, LIR should just be cleared.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12875394">SOLR-8069</key>
            <summary>Ensure that only the valid ZooKeeper registered leader can put a replica into Leader Initiated Recovery.</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="markrmiller@gmail.com">Mark Miller</assignee>
                                    <reporter username="markrmiller@gmail.com">Mark Miller</reporter>
                        <labels>
                    </labels>
                <created>Thu, 17 Sep 2015 15:25:51 +0000</created>
                <updated>Wed, 2 Oct 2019 17:24:10 +0000</updated>
                            <resolved>Wed, 11 Nov 2015 14:52:30 +0000</resolved>
                                                    <fixVersion>5.4</fixVersion>
                    <fixVersion>6.0</fixVersion>
                                        <due></due>
                            <votes>1</votes>
                                    <watches>10</watches>
                                                                                                                <comments>
                            <comment id="14803093" author="markrmiller@gmail.com" created="Thu, 17 Sep 2015 15:28:03 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=thelabdude&quot; class=&quot;user-hover&quot; rel=&quot;thelabdude&quot;&gt;thelabdude&lt;/a&gt;, any immediate thoughts on this?&lt;/p&gt;</comment>
                            <comment id="14803172" author="mewmewball" created="Thu, 17 Sep 2015 16:28:14 +0000"  >&lt;p&gt;We have definitely seen this as well, even after commit for &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-7109&quot; title=&quot;Indexing threads stuck during network partition can put leader into down state&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-7109&quot;&gt;&lt;del&gt;SOLR-7109&lt;/del&gt;&lt;/a&gt; added zookeeper multi transaction to ZkController.markShardAsDownIfLeader, which is supposed to predicate setting the LiR node on the setter&apos;s still having the same election znode it thinks it has when it&apos;s a leader.&lt;/p&gt;

&lt;p&gt;Hmmm, reading the code now I&apos;m not sure it&apos;s doing exactly the right thing since it calls getLeaderSeqPath, which just takes the current ElectionContext from electionContexts, which isn&apos;t necessarily the one the node had when it decided to mark someone else down, right? &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=shalin&quot; class=&quot;user-hover&quot; rel=&quot;shalin&quot;&gt;shalin&lt;/a&gt; thoughts?&lt;/p&gt;</comment>
                            <comment id="14803264" author="thelabdude" created="Thu, 17 Sep 2015 17:23:24 +0000"  >&lt;p&gt;Immediate thought is ugh! I&apos;m surprised to hear this is still happening after 7109. I&apos;d like to dig in a bit more, but agreed on:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;It seems that if all the replicas participate in election on startup, LIR should just be cleared.&lt;/p&gt;&lt;/blockquote&gt;
</comment>
                            <comment id="14804320" author="markrmiller@gmail.com" created="Thu, 17 Sep 2015 19:25:57 +0000"  >&lt;p&gt;Not quite working yet, but what about this approach instead?&lt;/p&gt;</comment>
                            <comment id="14804407" author="mewmewball" created="Thu, 17 Sep 2015 20:10:32 +0000"  >&lt;p&gt;I still struggle with the safety of getting the ElectionContext from electionContexts, because what&apos;s mapped there could change from under this thread. What about if we write down the election node path (e.g.  238121947050958365-core_node2-n_0000000006) into the leader znode as a leader props, so that whenever we&apos;re actually checking that we&apos;re the leader, we can get that election node path back and do the zk multi checking for that particular election node path?&lt;/p&gt;

&lt;p&gt;Ugh, but then I guess lots of places are actually looking at the cluster state&apos;s leader instead of the leader node. &amp;gt;_&amp;lt; Why are there separate places for marking the leader? I don&apos;t know how to reason with the asynchronous nature of cluster state&apos;s update wrt actual leader election...&lt;/p&gt;</comment>
                            <comment id="14804438" author="mewmewball" created="Thu, 17 Sep 2015 20:26:14 +0000"  >&lt;p&gt;Actually, thinking about it &amp;#8211; why do we have the leader property in cluster state at all? If it&apos;s simply to publish leadership to solrj, it seems that on the server-side we should still use the leader znode as the &quot;source of truth&quot; so that we can have guarantees of consistent view along with the zk transactions. If solrj&apos;s view falls behind due to the asynchronous nature of having the Overseer update the state, at least on the server side we can check the leader znode.&lt;/p&gt;

&lt;p&gt;Any historical reason why leadership information is in two places?&lt;/p&gt;</comment>
                            <comment id="14804446" author="markrmiller@gmail.com" created="Thu, 17 Sep 2015 20:31:31 +0000"  >&lt;blockquote&gt;&lt;p&gt;I still struggle with the safety of getting the ElectionContext from electionContexts, because what&apos;s mapped there could change from under this thread. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That is why I check before and after we get the context that we locally think we are the leader. The idea is, if we locally are connected to zk and think we are leader before and after getting the latest context, we have near real confidence that we are the leader and can do still do as we please.&lt;/p&gt;

&lt;p&gt;There really is nothing tricky about the leader being advertised in clusterstate - it&apos;s simply slightly stale state that is updated by Overseer. I don&apos;t see how it complicates an approach to this?&lt;/p&gt;</comment>
                            <comment id="14804543" author="markrmiller@gmail.com" created="Thu, 17 Sep 2015 21:27:31 +0000"  >&lt;p&gt;Well here is one approach.&lt;/p&gt;

&lt;p&gt;In either case, I&apos;d like to add the checks against CloudDescriptor#isLeader - this is our most up to date and real time information about whether or not we are connected to zk and think we are the leader - if this is false, we don&apos;t want to do anything that a leader should do. If we are going to have best effort checks against zk itself to ensure we are still the leader, this important defensive check should also generally be included.&lt;/p&gt;

&lt;p&gt;Otherwise, I think the current code does not really work - it&apos;s really easy to  grab the latest context and the latest context should usually look right? This code addresses that rather large hole. I&apos;m open to doing more, but I have not grasped the full implementation of it yet given the state available to the various LIR methods.&lt;/p&gt;</comment>
                            <comment id="14804599" author="thelabdude" created="Thu, 17 Sep 2015 21:56:03 +0000"  >&lt;p&gt;Quick pass over the patch looks good to me (a few non-related changes in HdfsCollectionsAPIDistributedZkTest.java leaked into this patch). I&apos;m focused on other un-related issue at the moment so will take a closer look in the AM when I&apos;m fresh, but I like the approach.&lt;/p&gt;</comment>
                            <comment id="14804677" author="mewmewball" created="Thu, 17 Sep 2015 23:01:03 +0000"  >&lt;p&gt;Yes, I think this is definitely an improvement. I&apos;m just not sure if it gets everything covered. I suppose &quot;we have near real confidence that we are the leader and can do still do as we please&quot; is probably good enough &amp;#8211; though I haven&apos;t convinced myself yet through playing with complex scenarios of repeated leadership changes &amp;#8211; thus I prefer the simple logic of &quot;do this action only if our zookeeper session state is exactly what it was when we decided to do it&quot;. Anyhow, this is probably beyond the scope of this JIRA.&lt;/p&gt;

&lt;p&gt;BTW, we tend to see this most when a &quot;bad&quot; query is issued (e.g. doing non-cursorMark deep paging of page 50,000). Presumably it creates GC on each replica it hits (since the request is retried) and a series of leadership changes happen. Along with complication of GC pauses, the states are quite difficult to reason through. &lt;/p&gt;</comment>
                            <comment id="14804686" author="markrmiller@gmail.com" created="Thu, 17 Sep 2015 23:08:41 +0000"  >&lt;p&gt;I think the thought game comes down to:&lt;/p&gt;

&lt;p&gt;We check if locally think we are the leader (which requires being connected to zk).&lt;/p&gt;

&lt;p&gt;We get the current leader context.&lt;/p&gt;

&lt;p&gt;We check if locally think we are the leader.&lt;/p&gt;

&lt;p&gt;If all that passes, we assume we have context for when we were the leader. Now publishing only works if that same leader is registered.&lt;/p&gt;

&lt;p&gt;So where are the holes?&lt;/p&gt;

&lt;p&gt;There does not seem to be a lot of room to get the wrong context? In what scenario could we think we are the leader before and after the getContext call and end up with the wrong context?&lt;/p&gt;

&lt;p&gt; And if we have the leaders context, the multi update ensures the update only happens if that context is still the leader.&lt;/p&gt;</comment>
                            <comment id="14804934" author="mewmewball" created="Fri, 18 Sep 2015 04:15:22 +0000"  >&lt;p&gt;The scenario that I have in mind is if somehow we&apos;re switching leadership back and forth due to nodes going into GC after receiving retries of an expensive query, what if a node is a leader at time T1, decided to set another node in LiR but went to GC before it did, so that it lost the leadership. Then, the other node briefly gained leadership at T2 and maybe processed an update or two but then also went to GC and lost its leadership. Then, the first node wakes up from GC and became the leader once more at T3--and then this code execute. My question is if it&apos;s absolutely safe for this node to set the other node in LiR simply because it&apos;s the leader now, even though when it decided to set the LiR, it was the leader  at T1.&lt;/p&gt;</comment>
                            <comment id="14847287" author="shalinmangar" created="Fri, 18 Sep 2015 11:39:56 +0000"  >&lt;blockquote&gt;&lt;p&gt;Hmmm, reading the code now I&apos;m not sure it&apos;s doing exactly the right thing since it calls getLeaderSeqPath, which just takes the current ElectionContext from electionContexts, which isn&apos;t necessarily the one the node had when it decided to mark someone else down, right? Shalin Shekhar Mangar thoughts?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, we should acquire the leader sequence path at the beginning of the update instead of so late in the game. I believe Mark&apos;s patch has the same problem but it is somewhat diluted by checking against CloudDescriptor.isLeader.&lt;/p&gt;</comment>
                            <comment id="14852717" author="markrmiller@gmail.com" created="Fri, 18 Sep 2015 12:41:19 +0000"  >&lt;p&gt;The difference is that this patch ensures we are still the leader when we get the context - rather than blindly getting the current context.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;is somewhat diluted &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think it goes from being a large hole still to closed really. Someone might have another idea for an improvement, but I don&apos;t see the scenario that really sneaks by this yet.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;My question is if it&apos;s absolutely safe for this node to set the other node in LiR simply because it&apos;s the leader now,&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think of course it is. It&apos;s valid for the leader and only the leader to set anyone as down.&lt;/p&gt;</comment>
                            <comment id="14866031" author="markrmiller@gmail.com" created="Fri, 18 Sep 2015 13:24:47 +0000"  >&lt;blockquote&gt;&lt;p&gt;thus I prefer the simple logic of &quot;do this action only if our zookeeper session state is exactly what it was when we decided to do it&quot;. Anyhow, this is probably beyond the scope of this JIRA.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t see an easy way to do that in this case. Almost all the solutions that fit with the code have the exact same holes / races. I think the local leader check around getting the leader context is the strongest thing I can think of so far other than adding further defensive checks.&lt;/p&gt;

&lt;p&gt;I don&apos;t know that much more is needed though. If the context returned is from the leader, great, its zkparentversion will will match. If the context is somehow not the right one, it won&apos;t match. We get a context and only if it&apos;s the context for the leader in ZK do we do anything rather than just if the context has a node in line. I&apos;d say that is a pretty strong improvement.&lt;/p&gt;

&lt;p&gt;This should only work if the node is a valid leader by its local state and by ZooKeeper.&lt;/p&gt;</comment>
                            <comment id="14875931" author="mewmewball" created="Fri, 18 Sep 2015 16:47:13 +0000"  >&lt;blockquote&gt;&lt;p&gt;I think of course it is. It&apos;s valid for the leader and only the leader to set anyone as down.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It&apos;s definitely only valid for the leader to set anyone down, but it doesn&apos;t mean that the leader should set someone down based on old leadership decision. This is the only place I&apos;m unsure about.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I don&apos;t see an easy way to do that in this case. Almost all the solutions that fit with the code have the exact same holes / races.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If we&apos;re willing to make more changes, one way I see this work is to write down the election node path as a prop in the leader znode (this is now written via zk transaction from your other commit). Then, have the isLeader logic in DistributedUpdateProcessor be based on reading the leader znode, and at that point record down the election node path as well. Then, when setting LiR, predicate the ZK transaction on the election node path read in the beginning of DistributedUpdateProcessor.&lt;/p&gt;</comment>
                            <comment id="14875960" author="markrmiller@gmail.com" created="Fri, 18 Sep 2015 17:06:25 +0000"  >&lt;blockquote&gt;&lt;p&gt;but it doesn&apos;t mean that the leader should set someone down based on old leadership decision.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think it does. A leader can do this. It doesn&apos;t matter if it had a valid reason to do it or not.&lt;/p&gt;</comment>
                            <comment id="14876022" author="markrmiller@gmail.com" created="Fri, 18 Sep 2015 17:36:47 +0000"  >&lt;blockquote&gt;&lt;p&gt;one way I see this &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That really seems the same as just getting the context earlier in the request.&lt;/p&gt;

&lt;p&gt;Given the different ways LIR might be started and used, it really seemed simpler to try and localize the changes rather than tie them more into the request lifecycle.&lt;/p&gt;</comment>
                            <comment id="14876057" author="markrmiller@gmail.com" created="Fri, 18 Sep 2015 17:56:54 +0000"  >&lt;blockquote&gt;&lt;p&gt;predicate the ZK transaction on the election node &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;As I think about this, I think I really prefer the approach in the patch - with that, we use ZK to ensure &lt;b&gt;ONLY&lt;/b&gt; the leader can put a replica into LIR. It doesn&apos;t matter what clumsy things happen elsewhere in the code, with this multi, only one replica in the shard, only the leader as recently properly enforced by ZK will be able to put a replica into LIR. I like that property vs a multi on election nodes.&lt;/p&gt;
</comment>
                            <comment id="14876133" author="andyetitmoves" created="Fri, 18 Sep 2015 18:48:43 +0000"  >&lt;p&gt;Late to the party here.. We experienced the same issue, and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cpoerschke&quot; class=&quot;user-hover&quot; rel=&quot;cpoerschke&quot;&gt;cpoerschke&lt;/a&gt; was trying to create a test case for this. My initial thought was why we even check LIR when we are about to become the leader? Shouldn&apos;t the double way sync cover us even if we are behind due to losing documents?&lt;/p&gt;</comment>
                            <comment id="14876156" author="andyetitmoves" created="Fri, 18 Sep 2015 19:01:31 +0000"  >&lt;p&gt;The case we hit was when we cold stopped/started the cloud. This was on 4.10.4, so may not be valid now. Let&apos;s say you have R1 and R2.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;R1 is the leader and both R1 and R2 are stopped at the same time.&lt;/li&gt;
	&lt;li&gt;R2&apos;s stops accepting requests but hasn&apos;t updated ZK as yet, when R1 sends a update to R2, it fails and puts R2 in LIR.&lt;/li&gt;
	&lt;li&gt;R2 shuts down first, then R1.&lt;/li&gt;
	&lt;li&gt;R1 starts up first, finds it should be the leader.&lt;/li&gt;
	&lt;li&gt;R2 decides it should follow and tries to recover.&lt;/li&gt;
	&lt;li&gt;R1 decides it can&apos;t be leader due to LIR and steps down. But by then R2 is in recovery, doesn&apos;t step up, and we have no one stepping forward.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14876184" author="markrmiller@gmail.com" created="Fri, 18 Sep 2015 19:17:46 +0000"  >&lt;blockquote&gt;&lt;p&gt;initial thought was why we even check LIR when we are about to become the leader?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think if everyone participates in the election that makes sense. I&apos;ve started working on that as a separate patch.&lt;/p&gt;

&lt;p&gt;I still like the idea of making it so that by zk decree only the current leader can put a replica into LIR as one of two improvements.&lt;/p&gt;</comment>
                            <comment id="14876205" author="andyetitmoves" created="Fri, 18 Sep 2015 19:27:55 +0000"  >&lt;p&gt;That makes sense, but for my understanding, why is it a bad idea even if not everyone is participating?&lt;/p&gt;</comment>
                            <comment id="14876236" author="markrmiller@gmail.com" created="Fri, 18 Sep 2015 19:47:08 +0000"  >&lt;p&gt;I guess I worry about cases where a bad replica was marked as LIR by the leader and the shard goes down. It comes back with two nodes that were LIR but not the good replicas - do we want one of them to become the leader and lose data? We know they are probably not good actual leader candidates and the best way to prevent data loss is manual intervention if possible.&lt;/p&gt;</comment>
                            <comment id="14876271" author="andyetitmoves" created="Fri, 18 Sep 2015 20:08:53 +0000"  >&lt;p&gt;Got it. You&apos;ve to include those in recovery along with the participants since the ones which have gone into recovery are not going to help in anyway (an alternative would be for them to abort recovery and rejoin of no one is around). But in your case, if I understand it right, detecting that there are down replicas (which might come back as good leaders) would certainly be a good idea.&lt;/p&gt;</comment>
                            <comment id="14876419" author="thelabdude" created="Fri, 18 Sep 2015 21:09:07 +0000"  >&lt;p&gt;Hi Ram,&lt;/p&gt;

&lt;p&gt;In your scenario, why would R1 be in LIR? What put it there?&lt;/p&gt;</comment>
                            <comment id="14876697" author="andyetitmoves" created="Fri, 18 Sep 2015 23:37:52 +0000"  >&lt;p&gt;R1 was not in LIR, but it came up while R2 was still at the lead and decided to recover, before R2 stepped down due to being in LIR.&lt;/p&gt;</comment>
                            <comment id="14876879" author="mewmewball" created="Sat, 19 Sep 2015 05:02:54 +0000"  >&lt;blockquote&gt;&lt;p&gt;I think it does. A leader can do this. It doesn&apos;t matter if it had a valid reason to do it or not.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;If you believe that this is true, I do agree that your patch will accomplish the check that at the moment you&apos;re setting someone else down, you&apos;re the leader. If we&apos;re going with this policy though, I think if at this moment it realizes that it&apos;s not the leader, it should actually fail the request because it shouldn&apos;t accept it on the real leader&apos;s behalf. E.g. if it&apos;s a node that was a leader but has just been network-partitioned off (but clusterstate change hasn&apos;t been made since it&apos;s asynchronous) and wasn&apos;t able to actually forward the request to the real leader.&lt;/p&gt;</comment>
                            <comment id="14877082" author="markrmiller@gmail.com" created="Sat, 19 Sep 2015 12:28:25 +0000"  >&lt;blockquote&gt;&lt;p&gt;If you believe that this is true, I do agree that your patch will accomplish the check that at the moment you&apos;re setting someone else down, you&apos;re the leader. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If the leader cannot set a replica into LIR at any time for any reason, I think we have trouble in general.&lt;/p&gt;

&lt;p&gt;I&apos;m not sure I fully follow the rest. I can&apos;t wrap my head around LIR causing requests to fail or not...that doesn&apos;t make a lot of sense to me.&lt;/p&gt;</comment>
                            <comment id="14899967" author="markrmiller@gmail.com" created="Sun, 20 Sep 2015 15:08:50 +0000"  >&lt;blockquote&gt;&lt;p&gt;I think if everyone participates in the election that makes sense. I&apos;ve started working on that as a separate patch.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;ve spun off this part of the issue to &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-8075&quot; title=&quot;Leader Initiated Recovery should not stop a leader that participated in an election with all of it&amp;#39;s replicas from becoming a valid leader.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-8075&quot;&gt;&lt;del&gt;SOLR-8075&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="14901352" author="markrmiller@gmail.com" created="Mon, 21 Sep 2015 20:40:33 +0000"  >&lt;p&gt;Having had some time to consider this patch, I think this is the right place to commit for now. I think further improvements should be spun out into other JIRA issues. &lt;/p&gt;</comment>
                            <comment id="14901449" author="anshumg" created="Mon, 21 Sep 2015 21:33:01 +0000"  >&lt;p&gt;This makes sense and it&apos;s also pretty contained. Here are a suggestions:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;That should be CoreDescriptor in the comment.
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;&quot;&gt;&lt;b&gt;ZkController.java&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;+                leaderCd); &lt;span class=&quot;code-comment&quot;&gt;// core node name of current leader&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
	&lt;li&gt;Unused import MockCoreContainer in HttpPartitionTest&lt;/li&gt;
	&lt;li&gt;In ZkController.markShardAsDownIfLeader(), was the move from using getLeaderSeqPath to &lt;tt&gt;new org.apache.hadoop.fs.Path(((ShardLeaderElectionContextBase)context).leaderPath).getParent().toString()&lt;/tt&gt; intentional ?&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14901572" author="markrmiller@gmail.com" created="Mon, 21 Sep 2015 22:50:20 +0000"  >&lt;p&gt;Thanks for taking a look Anshum.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;intentional ?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yup, that&apos;s really all of the magic. The CloudDescriptor#isLeader stuff is really just a little extra sugar on top.&lt;/p&gt;</comment>
                            <comment id="14901583" author="anshumg" created="Mon, 21 Sep 2015 23:03:56 +0000"  >&lt;p&gt;Just looked at it again, it indeed is &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.&lt;/p&gt;

&lt;p&gt;In that case let&apos;s yank out getLeaderSeqPath. It&apos;s not needed elsewhere anyways.&lt;/p&gt;</comment>
                            <comment id="14901817" author="markrmiller@gmail.com" created="Tue, 22 Sep 2015 02:38:51 +0000"  >&lt;blockquote&gt;&lt;p&gt;if it&apos;s a node that was a leader but has just been network-partitioned off (but clusterstate change hasn&apos;t been made since it&apos;s asynchronous) and wasn&apos;t able to actually forward the request to the real leader.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t think we really protect against such cases where there is only a single leader that can accept an update because all its replicas go bad and then it goes away but the replicas come back. That is what min replication factor on the request is meant to handle. For full data promises, you want to use it - an achieved replication factor of 1 is not going to be fault tolerant.&lt;/p&gt;</comment>
                            <comment id="14902021" author="mewmewball" created="Tue, 22 Sep 2015 06:19:18 +0000"  >&lt;blockquote&gt;&lt;p&gt;I don&apos;t think we really protect against such cases where there is only a single leader that can accept an update&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;This is not the scenario I&apos;m describing. If you have 3 replicas and one that was the leader gets partitioned off, one of the other 2 will get elected and they can carry on. However, during this transition time, because the cluster state update hasn&apos;t been completed or propagated through watches, the old leader can still get trailing updates from the client. In a normal case where the updates are successfully forwarded to all replicas, no one cares. But in this case, the old leader cannot forward the update to others (because it&apos;s partitioned off), so it should not reply success to the client because that would be wrong (it is not the leader and it does not have the right to tell the others to recover).&lt;/p&gt;</comment>
                            <comment id="14902517" author="markrmiller@gmail.com" created="Tue, 22 Sep 2015 12:41:38 +0000"  >&lt;p&gt;The old leader has defensive checks that should make that pretty unlikely.&lt;/p&gt;

&lt;p&gt;But yes, it&apos;s the same thing. Only one node ack&apos;d your update. We don&apos;t protect against that and you have to use min rep factor.&lt;/p&gt;</comment>
                            <comment id="14903626" author="gchanan" created="Tue, 22 Sep 2015 23:06:00 +0000"  >&lt;p&gt;Most of this seems like just adding defensive checks, which seem reasonable.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;List&amp;lt;Op&amp;gt; ops = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ArrayList&amp;lt;&amp;gt;(2);&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;nit: this should be 3 in two places.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;+      ops.add(Op.check(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; org.apache.hadoop.fs.Path(((ShardLeaderElectionContextBase)context).leaderPath).getParent().toString(), leaderZkNodeParentVersion));
       ops.add(Op.setData(znodePath, znodeData, -1));
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;What happens if the leaderZkNodeParentVersion doesn&apos;t match?  Presumably that&apos;s a possibility or else why add the check.  We don&apos;t want to loop and see if we get an updated version in electionContexts?  I&apos;m certainly not well versed in this area of the code but checking isLeader seems a little roundabount &amp;#8211; isn&apos;t the leaderZkNodeParentVersion what we actually care about?  What happens if we think we are the leader but the version doesn&apos;t match?  What does that mean?  Certainly we can optimistically try whatever we pulled out of electionContexts the first time, as you&apos;ve done here to avoid a zk trip.&lt;/p&gt;</comment>
                            <comment id="14903807" author="markrmiller@gmail.com" created="Wed, 23 Sep 2015 02:05:43 +0000"  >&lt;p&gt;The defensive checks are just sugar like I said. If we are going to check zk if we are still leader it makes sense to check our local reckoning first. &lt;/p&gt;

&lt;p&gt;The meat of the change is the parent version check. If it fails, we don&apos;t care. The leader has moved on - we don&apos;t care about retries. &lt;/p&gt;</comment>
                            <comment id="14904408" author="markrmiller@gmail.com" created="Wed, 23 Sep 2015 12:12:43 +0000"  >&lt;p&gt;I was out on the phone last night - a fuller reply:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;What happens if the leaderZkNodeParentVersion doesn&apos;t match? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The leader cannot update the zk node as we want.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Presumably that&apos;s a possibility or else why add the check.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It&apos;s the whole point of the patch?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I&apos;m certainly not well versed in this area of the code but checking isLeader seems a little roundabount&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;There is no reason to go to zk if we already know we are not the leader locally - what is roundabout about it?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;What does that mean?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That the fix worked??&lt;/p&gt;


</comment>
                            <comment id="14904431" author="jira-bot" created="Wed, 23 Sep 2015 12:36:04 +0000"  >&lt;p&gt;Commit 1704836 from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markrmiller%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;markrmiller@gmail.com&quot;&gt;markrmiller@gmail.com&lt;/a&gt; in branch &apos;dev/trunk&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1704836&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://svn.apache.org/r1704836&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-8069&quot; title=&quot;Ensure that only the valid ZooKeeper registered leader can put a replica into Leader Initiated Recovery.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-8069&quot;&gt;&lt;del&gt;SOLR-8069&lt;/del&gt;&lt;/a&gt;: Ensure that only the valid ZooKeeper registered leader can put a replica into Leader Initiated Recovery.&lt;/p&gt;</comment>
                            <comment id="14904435" author="markrmiller@gmail.com" created="Wed, 23 Sep 2015 12:42:02 +0000"  >&lt;p&gt;So this adds sensible local isLeader checks where we were already checking ZK, it passes the core descriptor instead of just a name to LIR so it has a lot more context to work with, and it ensures that only the registered ZK leader can put a replica into LIR.&lt;/p&gt;

&lt;p&gt;Barring any bugs in the current code, let&apos;s open further issues for other changes / improvements.&lt;/p&gt;</comment>
                            <comment id="14904442" author="jira-bot" created="Wed, 23 Sep 2015 12:45:51 +0000"  >&lt;p&gt;Commit 1704837 from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markrmiller%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;markrmiller@gmail.com&quot;&gt;markrmiller@gmail.com&lt;/a&gt; in branch &apos;dev/branches/branch_5x&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1704837&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://svn.apache.org/r1704837&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-8069&quot; title=&quot;Ensure that only the valid ZooKeeper registered leader can put a replica into Leader Initiated Recovery.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-8069&quot;&gt;&lt;del&gt;SOLR-8069&lt;/del&gt;&lt;/a&gt;: Ensure that only the valid ZooKeeper registered leader can put a replica into Leader Initiated Recovery.&lt;/p&gt;</comment>
                            <comment id="14968664" author="shalinmangar" created="Thu, 22 Oct 2015 07:03:44 +0000"  >&lt;p&gt;There&apos;s a reproducible failure in the test added by &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-8075&quot; title=&quot;Leader Initiated Recovery should not stop a leader that participated in an election with all of it&amp;#39;s replicas from becoming a valid leader.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-8075&quot;&gt;&lt;del&gt;SOLR-8075&lt;/del&gt;&lt;/a&gt; caused by assertion error on asserts added in this issue.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;1 tests failed.
FAILED:  org.apache.solr.cloud.LeaderInitiatedRecoveryOnShardRestartTest.testRestartWithAllInLIR

Error Message:
Captured an uncaught exception in thread: &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;[id=43491, name=coreZkRegister-5997-thread-1, state=RUNNABLE, group=TGRP-LeaderInitiatedRecoveryOnShardRestartTest]

Stack Trace:
com.carrotsearch.randomizedtesting.UncaughtExceptionError: Captured an uncaught exception in thread: &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;[id=43491, name=coreZkRegister-5997-thread-1, state=RUNNABLE, group=TGRP-LeaderInitiatedRecoveryOnShardRestartTest]
Caused by: java.lang.AssertionError
        at __randomizedtesting.SeedInfo.seed([7F78F76DDF75FAD1]:0)
        at org.apache.solr.cloud.ZkController.updateLeaderInitiatedRecoveryState(ZkController.java:2133)
        at org.apache.solr.cloud.ShardLeaderElectionContext.runLeaderProcess(ElectionContext.java:434)
        at org.apache.solr.cloud.LeaderElector.runIamLeaderProcess(LeaderElector.java:197)
        at org.apache.solr.cloud.LeaderElector.checkIfIamLeader(LeaderElector.java:157)
        at org.apache.solr.cloud.LeaderElector.joinElection(LeaderElector.java:346)
        at org.apache.solr.cloud.ZkController.joinElection(ZkController.java:1113)
        at org.apache.solr.cloud.ZkController.register(ZkController.java:926)
        at org.apache.solr.cloud.ZkController.register(ZkController.java:881)
        at org.apache.solr.core.ZkContainer$2.run(ZkContainer.java:183)
        at org.apache.solr.common.util.ExecutorUtil$MDCAwareThreadPoolExecutor$1.run(ExecutorUtil.java:231)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The assertion is that leaderCd != null fails because ShardLeaderElectionContext.runLeaderProcess calls ZkController.updateLeaderInitiatedRecoveryState with a null core descriptor  which is by design because if you are marking a replica as &apos;active&apos; then you don&apos;t necessarily need to be a leader.&lt;/p&gt;</comment>
                            <comment id="15000453" author="markrmiller@gmail.com" created="Wed, 11 Nov 2015 14:52:18 +0000"  >&lt;p&gt;The problem is unrelated to this issue. That assert is correct and it&apos;s catching a bug with &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-8075&quot; title=&quot;Leader Initiated Recovery should not stop a leader that participated in an election with all of it&amp;#39;s replicas from becoming a valid leader.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-8075&quot;&gt;&lt;del&gt;SOLR-8075&lt;/del&gt;&lt;/a&gt; or something. It&apos;s passing null when it should pass the core descriptor.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12774965">SOLR-7109</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12894987">SOLR-8075</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12757164" name="SOLR-8069.patch" size="21987" author="markrmiller@gmail.com" created="Thu, 17 Sep 2015 21:27:31 +0000"/>
                            <attachment id="12757136" name="SOLR-8069.patch" size="19212" author="markrmiller@gmail.com" created="Thu, 17 Sep 2015 19:25:57 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 1 week, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2kb4v:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12313321" key="com.atlassian.jira.toolkit:message">
                        <customfieldname>Solr Mailing List Info</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>