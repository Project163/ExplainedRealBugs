<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 04:24:17 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SOLR-10704] REPLACENODE can make the collection lost data which replicaFactor is 1 </title>
                <link>https://issues.apache.org/jira/browse/SOLR-10704</link>
                <project id="12310230" key="SOLR">Solr</project>
                    <description>&lt;p&gt;When some replicas which the relative collection&apos;s replicaFactor is 1, it will lost data after executing the REPLACENODE cmd. &lt;/p&gt;

&lt;p&gt;It may be the new replica on the target node does not complete revovering, but the old replica on the source node  was already be deleted.&lt;/p&gt;

&lt;p&gt;At last the target revocery failed for the following exception:&lt;br/&gt;
2017-05-18 17:08:48,587 | ERROR | recoveryExecutor-3-thread-2-processing-n:192.168.229.137:21103_solr x:replace-hdfs-coll1_shard1_replica2 s:shard1 c:replace-hdfs-coll1 r:core_node3 | Error while trying to recover. core=replace-hdfs-coll1_shard1_replica2:java.lang.NullPointerException&lt;br/&gt;
        at org.apache.solr.update.PeerSync.alreadyInSync(PeerSync.java:339)&lt;/p&gt;

</description>
                <environment>&lt;p&gt;Red Hat 4.8.3-9, JDK 1.8.0_121&lt;/p&gt;</environment>
        <key id="13073088">SOLR-10704</key>
            <summary>REPLACENODE can make the collection lost data which replicaFactor is 1 </summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="ab">Andrzej Bialecki</assignee>
                                    <reporter username="daisy_yu">Daisy.Yuan</reporter>
                        <labels>
                    </labels>
                <created>Thu, 18 May 2017 11:17:37 +0000</created>
                <updated>Sat, 8 Jun 2019 15:19:53 +0000</updated>
                            <resolved>Tue, 13 Jun 2017 15:59:47 +0000</resolved>
                                    <version>6.2</version>
                                    <fixVersion>6.7</fixVersion>
                    <fixVersion>7.0</fixVersion>
                                    <component>SolrCloud</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="16018369" author="daisy_yu" created="Sat, 20 May 2017 08:35:04 +0000"  >&lt;p&gt;The details: &lt;br/&gt;
1. Collections&apos; replicas distribution: &lt;br/&gt;
replace-hdfs-coll1 has two shards, each shard has one replica and the index files was stored on hdfs. &lt;br/&gt;
replace-hdfs-coll1_shard1_replica1  on node 192.168.229.219&lt;br/&gt;
replace-hdfs-coll1_shard2_replica1  on node 192.168.228.193&lt;/p&gt;

&lt;p&gt;replace-hdfs-coll2 has two shards, each shard has two replica and the index files was stored on hdfs.&lt;br/&gt;
replace-hdfs-coll2_shard1_replica1  on node 192.168.229.219&lt;br/&gt;
replace-hdfs-coll2_shard1_replica2  on node 192.168.229.193&lt;/p&gt;

&lt;p&gt;replace-hdfs-coll2_shard2_replica1  on node 192.168.228.193&lt;br/&gt;
replace-hdfs-coll2_shard2_replica2  on node 192.168.229.219&lt;/p&gt;

&lt;p&gt;replace-local-coll1 has two shards, each shard has one replica and the index files was stored on disk.&lt;br/&gt;
replace-local-coll1_shard1_replica1  on node 192.168.228.193&lt;br/&gt;
replace-local-coll1_shard2_replica1  on node 192.168.229.219&lt;/p&gt;

&lt;p&gt;replace-local-coll2 has two shards, each shard has two replica and the index files was stored on disk.&lt;br/&gt;
replace-local-coll2_shard1_replica1  on node 192.168.229.193&lt;br/&gt;
replace-local-coll2_shard1_replica2 on node 192.168.229.219&lt;/p&gt;

&lt;p&gt;replace-local-coll2_shard2_replica1  on node 192.168.228.193&lt;br/&gt;
replace-local-coll2_shard2_replica2  on node 192.168.229.219&lt;/p&gt;

&lt;p&gt;2. Execute REPLACENODE to replace node 192.168.229.219 with node 192.168.229.137&lt;/p&gt;

&lt;p&gt;3. The REPLACENODE request was executed successfully&lt;/p&gt;

&lt;p&gt;4. The target replace-hdfs-coll1_shard1_replica2 does not complete revovering, but the source replace-hdfs-coll1_shard1_replica1 was already be deleted. At last the target revocery failed for the following exception:&lt;br/&gt;
2017-05-18 17:08:48,587 | ERROR | recoveryExecutor-3-thread-2-processing-n:192.168.229.137:21103_solr x:replace-hdfs-coll1_shard1_replica2 s:shard1 c:replace-hdfs-coll1 r:core_node3 | Error while trying to recover. core=replace-hdfs-coll1_shard1_replica2:java.lang.NullPointerException&lt;br/&gt;
        at org.apache.solr.update.PeerSync.alreadyInSync(PeerSync.java:339)&lt;/p&gt;

&lt;p&gt;5. The main process log messages&lt;br/&gt;
log in 193.log, the node 192.168.228.193 is overseer role.&lt;br/&gt;
step 1. node 192.168.229.193 recevied the REPLACENODE request&lt;br/&gt;
2017-05-18 17:08:32,717 | INFO  | http-nio-21100-exec-6 | Invoked Collection Action :replacenode with params action=REPLACENODE&amp;amp;source=192.168.229.219:21100_solr&amp;amp;wt=json&amp;amp;target=192.168.229.137:21103_solr and sendToOCPQueue=true | org.apache.solr.handler.admin.CollectionsHandler.handleRequestBody(CollectionsHandler.java:203)&lt;/p&gt;

&lt;p&gt;step 2. OverseerCollectionConfigSetProcessor get the task msg and process REPLACENODE&lt;/p&gt;

&lt;p&gt;step 3.  add replica&lt;br/&gt;
2017-05-18 17:08:36,592 | INFO  | OverseerStateUpdate-1225069473835599708-192.168.228.193:21100_solr-n_0000000063 | processMessage: queueSize: 1, message = &lt;/p&gt;
{
  &quot;core&quot;:&quot;replace-hdfs-coll1_shard1_replica2&quot;,
  &quot;roles&quot;:null,
  &quot;base_url&quot;:&quot;http://192.168.229.137:21103/solr&quot;,
  &quot;node_name&quot;:&quot;192.168.229.137:21103_solr&quot;,
  &quot;state&quot;:&quot;down&quot;,
  &quot;shard&quot;:&quot;shard1&quot;,
  &quot;collection&quot;:&quot;replace-hdfs-coll1&quot;,
  &quot;operation&quot;:&quot;state&quot;}
&lt;p&gt; current state version: 42 | org.apache.solr.cloud.Overseer$ClusterStateUpdater.run(Overseer.java:221)&lt;/p&gt;

&lt;p&gt; 2017-05-18 17:08:40,540 | INFO  | OverseerStateUpdate-1225069473835599708-192.168.228.193:21100_solr-n_0000000063 | processMessage: queueSize: 1, message = &lt;/p&gt;
{
  &quot;core&quot;:&quot;replace-hdfs-coll1_shard1_replica2&quot;,
  &quot;core_node_name&quot;:&quot;core_node3&quot;,
  &quot;dataDir&quot;:&quot;hdfs://hacluster//user/solr//SolrServer1/replace-hdfs-coll1/core_node3/data/&quot;,
  &quot;roles&quot;:null,
  &quot;base_url&quot;:&quot;http://192.168.229.137:21103/solr&quot;,
  &quot;node_name&quot;:&quot;192.168.229.137:21103_solr&quot;,
  &quot;state&quot;:&quot;recovering&quot;,
  &quot;shard&quot;:&quot;shard1&quot;,
  &quot;collection&quot;:&quot;replace-hdfs-coll1&quot;,
  &quot;operation&quot;:&quot;state&quot;,
  &quot;ulogDir&quot;:&quot;hdfs://hacluster/user/solr/SolrServer1/replace-hdfs-coll1/core_node3/data/tlog&quot;}
&lt;p&gt; current state version: 42 | org.apache.solr.cloud.Overseer$ClusterStateUpdater.run(Overseer.java:221)&lt;br/&gt;
 step 4.  deletecore&lt;br/&gt;
2017-05-18 17:08:47,552 | INFO  | OverseerStateUpdate-1225069473835599708-192.168.228.193:21100_solr-n_0000000063 | processMessage: queueSize: 1, message = &lt;/p&gt;
{
  &quot;operation&quot;:&quot;deletecore&quot;,
  &quot;core&quot;:&quot;replace-hdfs-coll1_shard1_replica1&quot;,
  &quot;node_name&quot;:&quot;192.168.229.219:21100_solr&quot;,
  &quot;collection&quot;:&quot;replace-hdfs-coll1&quot;,
  &quot;core_node_name&quot;:&quot;core_node2&quot;}
&lt;p&gt; current state version: 42 | org.apache.solr.cloud.Overseer$ClusterStateUpdater.run(Overseer.java:221)&lt;/p&gt;

&lt;p&gt; 192.168.229.219 is the source node.&lt;br/&gt;
  2017-05-18 17:08:47,484 | INFO  | http-nio-21100-exec-6 | Removing directory before core close: hdfs://hacluster//user/solr//SolrServerAdmin/replace-hdfs-coll1/core_node2/data/index | org.apache.solr.core.CachingDirectoryFactory.closeCacheValue(CachingDirectoryFactory.java:271)&lt;br/&gt;
2017-05-18 17:08:47,515 | INFO  | http-nio-21100-exec-6 | Removing directory after core close: hdfs://hacluster//user/solr//SolrServerAdmin/replace-hdfs-coll1/core_node2/data | org.apache.solr.core.CachingDirectoryFactory.close(CachingDirectoryFactory.java:204)&lt;/p&gt;

&lt;p&gt; 192.168.229.137 is the target node, but  replace-hdfs-coll1_shard1_replica2 recovering is not finished&lt;br/&gt;
 2017-05-18 17:08:48,547 | INFO  | recoveryExecutor-3-thread-2-processing-n:192.168.229.137:21103_solr x:replace-hdfs-coll1_shard1_replica2 s:shard1 c:replace-hdfs-coll1 r:core_node3 | Attempting to PeerSync from &lt;a href=&quot;http://192.168.229.219:21100/solr/replace-hdfs-coll1_shard1_replica1/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://192.168.229.219:21100/solr/replace-hdfs-coll1_shard1_replica1/&lt;/a&gt; - recoveringAfterStartup=&lt;span class=&quot;error&quot;&gt;&amp;#91;true&amp;#93;&lt;/span&gt; | org.apache.solr.cloud.RecoveryStrategy.doRecovery(RecoveryStrategy.java:370)&lt;br/&gt;
2017-05-18 17:08:48,547 | INFO  | recoveryExecutor-3-thread-2-processing-n:192.168.229.137:21103_solr x:replace-hdfs-coll1_shard1_replica2 s:shard1 c:replace-hdfs-coll1 r:core_node3 | PeerSync: core=replace-hdfs-coll1_shard1_replica2 url=&lt;a href=&quot;http://192.168.229.137:21103/solr&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://192.168.229.137:21103/solr&lt;/a&gt; START replicas=&lt;a href=&quot;http://192.168.229.219:21100/solr/replace-hdfs-coll1_shard1_replica1/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://192.168.229.219:21100/solr/replace-hdfs-coll1_shard1_replica1/&lt;/a&gt; nUpdates=100 | org.apache.solr.update.PeerSync.sync(PeerSync.java:214)&lt;br/&gt;
2017-05-18 17:08:48,587 | ERROR | recoveryExecutor-3-thread-2-processing-n:192.168.229.137:21103_solr x:replace-hdfs-coll1_shard1_replica2 s:shard1 c:replace-hdfs-coll1 r:core_node3 | Error while trying to recover. core=replace-hdfs-coll1_shard1_replica2:java.lang.NullPointerException&lt;br/&gt;
        at org.apache.solr.update.PeerSync.alreadyInSync(PeerSync.java:339)&lt;br/&gt;
        at org.apache.solr.update.PeerSync.sync(PeerSync.java:222)&lt;br/&gt;
        at org.apache.solr.cloud.RecoveryStrategy.doRecovery(RecoveryStrategy.java:376)&lt;br/&gt;
        at org.apache.solr.cloud.RecoveryStrategy.run(RecoveryStrategy.java:221)&lt;br/&gt;
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)&lt;br/&gt;
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)&lt;br/&gt;
        at org.apache.solr.common.util.ExecutorUtil$MDCAwareThreadPoolExecutor.lambda$execute$0(ExecutorUtil.java:229)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:745)&lt;/p&gt;
&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; org.apache.solr.common.SolrException.log(SolrException.java:159)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;


&lt;p&gt;2017-05-18 17:08:48,587 | INFO  | recoveryExecutor-3-thread-2-processing-n:192.168.229.137:21103_solr x:replace-hdfs-coll1_shard1_replica2 s:shard1 c:replace-hdfs-coll1 r:core_node3 | Replay not started, or was not successful... still buffering updates. | org.apache.solr.cloud.RecoveryStrategy.doRecovery(RecoveryStrategy.java:441)&lt;br/&gt;
2017-05-18 17:08:48,587 | ERROR | recoveryExecutor-3-thread-2-processing-n:192.168.229.137:21103_solr x:replace-hdfs-coll1_shard1_replica2 s:shard1 c:replace-hdfs-coll1 r:core_node3 | Recovery failed - trying again... (0) | org.apache.solr.cloud.RecoveryStrategy.doRecovery(RecoveryStrategy.java:478)&lt;/p&gt;
</comment>
                            <comment id="16042621" author="ab" created="Thu, 8 Jun 2017 12:25:44 +0000"  >&lt;p&gt;I can reproduce this also using a 2 node setup, using HDFS collection with 2 shards and 1 replica. It appears that the REPLACENODE deletes the original replica (which is also the replica leader) while the new replica is starting the recovery - at which point the recovery fails. Then it tries to find a shard leader, which no longer exists...&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;2017-06-08 12:11:49.760 INFO  (recoveryExecutor-3-thread-1-processing-n:192.168.0.202:8983_solr x:gettingstarted_shard1_replica2 s:shard1 c:gettingstarted r:core_node3) [c:gettingstarted s:shard1 r:core_node3 x:gettingstarted_shard1_replica2] o.a.s.c.RecoveryStrategy Attempting to PeerSync from [http:&lt;span class=&quot;code-comment&quot;&gt;//192.168.0.201:8983/solr/gettingstarted_shard1_replica1/] - recoveringAfterStartup=[&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;]
&lt;/span&gt;2017-06-08 12:11:49.789 INFO  (recoveryExecutor-3-thread-1-processing-n:192.168.0.202:8983_solr x:gettingstarted_shard1_replica2 s:shard1 c:gettingstarted r:core_node3) [c:gettingstarted s:shard1 r:core_node3 x:gettingstarted_shard1_replica2] o.a.s.u.PeerSync PeerSync: core=gettingstarted_shard1_replica2 url=http:&lt;span class=&quot;code-comment&quot;&gt;//192.168.0.202:8983/solr START replicas=[http://192.168.0.201:8983/solr/gettingstarted_shard1_replica1/] nUpdates=100
&lt;/span&gt;2017-06-08 12:11:49.856 ERROR (recoveryExecutor-3-thread-1-processing-n:192.168.0.202:8983_solr x:gettingstarted_shard1_replica2 s:shard1 c:gettingstarted r:core_node3) [c:gettingstarted s:shard1 r:core_node3 x:gettingstarted_shard1_replica2] o.a.s.c.RecoveryStrategy Error &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; trying to recover. core=gettingstarted_shard1_replica2:java.lang.NullPointerException
        at org.apache.solr.update.PeerSync.alreadyInSync(PeerSync.java:340)
        at org.apache.solr.update.PeerSync.sync(PeerSync.java:223)
        at org.apache.solr.cloud.RecoveryStrategy.doRecovery(RecoveryStrategy.java:376)
        at org.apache.solr.cloud.RecoveryStrategy.run(RecoveryStrategy.java:221)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at org.apache.solr.common.util.ExecutorUtil$MDCAwareThreadPoolExecutor.lambda$execute$0(ExecutorUtil.java:229)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)

...

2017-06-08 12:12:03.826 INFO  (zkCallback-4-thread-2-processing-n:192.168.0.202:8983_solr) [c:gettingstarted s:shard2 r:core_node4 x:gettingstarted_shard2_replica2] o.a.s.c.ActionThrottle The last leader attempt started 34ms ago.
2017-06-08 12:12:03.827 INFO  (zkCallback-4-thread-2-processing-n:192.168.0.202:8983_solr) [c:gettingstarted s:shard2 r:core_node4 x:gettingstarted_shard2_replica2] o.a.s.c.ActionThrottle Throttling leader attempts - waiting &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 4965ms
2017-06-08 12:12:03.873 ERROR (recoveryExecutor-3-thread-1-processing-n:192.168.0.202:8983_solr x:gettingstarted_shard1_replica2 s:shard1 c:gettingstarted r:core_node3) [c:gettingstarted s:shard1 r:core_node3 x:gettingstarted_shard1_replica2] o.a.s.c.RecoveryStrategy Error &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; trying to recover. core=gettingstarted_shard1_replica2:org.apache.solr.common.SolrException: No registered leader was found after waiting &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 4000ms , collection: gettingstarted slice: shard1
        at org.apache.solr.common.cloud.ZkStateReader.getLeaderRetry(ZkStateReader.java:747)
        at org.apache.solr.common.cloud.ZkStateReader.getLeaderRetry(ZkStateReader.java:733)
        at org.apache.solr.cloud.RecoveryStrategy.doRecovery(RecoveryStrategy.java:305)
        at org.apache.solr.cloud.RecoveryStrategy.run(RecoveryStrategy.java:221)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at org.apache.solr.common.util.ExecutorUtil$MDCAwareThreadPoolExecutor.lambda$execute$0(ExecutorUtil.java:229)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The main problem here is that the original replica is deleted before the new replica fully recovers - the code should wait until this happens when there&apos;s only one active replica left in the cluster. This should also consider a scenario when there are several replicas of the same shard on the same node, and again the code has to wait with deleting them before at least one new replica has fully recovered.&lt;/p&gt;</comment>
                            <comment id="16047009" author="ab" created="Mon, 12 Jun 2017 20:11:39 +0000"  >&lt;p&gt;This patch waits for all replicas that we&apos;re moving that were leaders (which covers also the case of replicationFactor=1) until they are recovered and only then proceeds to deleting the old replicas.&lt;/p&gt;</comment>
                            <comment id="16047556" author="shalinmangar" created="Tue, 13 Jun 2017 08:26:56 +0000"  >&lt;p&gt;Thanks Andrzej.&lt;/p&gt;

&lt;p&gt;A few comments:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;The key for the watcher should be collection_name + coreNodeName &amp;#8211; that is necessary and sufficient to be unique across the cluster&lt;/li&gt;
	&lt;li&gt;Instead of &lt;tt&gt;if (replica.getState().equals(Replica.State.ACTIVE))&lt;/tt&gt;, you should use replica.isActive(liveNodes) to check if replica is active &amp;#8211; another gotcha of SolrCloud that we really should fix at some point&lt;/li&gt;
	&lt;li&gt;The RecoveryWatcher&apos;s latch is counted down even if there exists at least one replica other than the one being moved &amp;#8211; which is completely fine but a bit confusing reading the code &amp;#8211; perhaps a code comment is pertinent.&lt;/li&gt;
	&lt;li&gt;The RecoveryWatcher should have additional checks for replica types e.g. there must be at least 1 active NRT or TLOG replicas somewhere otherwise the slice will be left leaderless as PULL type replicas cannot become leaders.&lt;/li&gt;
	&lt;li&gt;Unrelated to these changes &amp;#8211; it looks like if anyOneFailed is true, then we delete all newly created replicas from the target AND continue to delete the source node as well?&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Overall, this kind of operation is hard to guarantee in the current state of SolrCloud because at any time, the leader can put another replica in LIR. If that happens after we checked for the replica to be active, then deleting the leader will make that slice leader-less as replicas in LIR cannot become leaders without recoverying first. However, at this point, this is the best we can do.&lt;/p&gt;</comment>
                            <comment id="16047735" author="ab" created="Tue, 13 Jun 2017 11:07:41 +0000"  >&lt;p&gt;Updated patch that addresses the issues from Shalin&apos;s review. If there are no objections I&apos;ll commit this shortly.&lt;/p&gt;</comment>
                            <comment id="16047765" author="shalinmangar" created="Tue, 13 Jun 2017 11:44:14 +0000"  >&lt;p&gt;The replicaName in your patch is actually the coreNodeName. The replica ZkNodeProps does not have a property for &lt;tt&gt;ZkStateReader.CORE_NODE_NAME_PROP&lt;/tt&gt;. Just use &lt;tt&gt;collectionName + &quot;_&quot; + replicaName&lt;/tt&gt;. Rest looks good.&lt;/p&gt;</comment>
                            <comment id="16048044" author="jira-bot" created="Tue, 13 Jun 2017 15:40:04 +0000"  >&lt;p&gt;Commit 232eff0893bccb93d01042f26a00e50870be2f29 in lucene-solr&apos;s branch refs/heads/master from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ab&quot; class=&quot;user-hover&quot; rel=&quot;ab&quot;&gt;ab&lt;/a&gt;&lt;br/&gt;
[ &lt;a href=&quot;https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=232eff0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=232eff0&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-10704&quot; title=&quot;REPLACENODE can make the collection lost data which replicaFactor is 1 &quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-10704&quot;&gt;&lt;del&gt;SOLR-10704&lt;/del&gt;&lt;/a&gt; Wait until all leader replicas are recovered before deleting&lt;br/&gt;
the originals.&lt;/p&gt;</comment>
                            <comment id="16048057" author="jira-bot" created="Tue, 13 Jun 2017 15:56:33 +0000"  >&lt;p&gt;Commit ccd1f45b3ba3fbb862cae5d0ab0ce821b966026a in lucene-solr&apos;s branch refs/heads/branch_6x from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ab&quot; class=&quot;user-hover&quot; rel=&quot;ab&quot;&gt;ab&lt;/a&gt;&lt;br/&gt;
[ &lt;a href=&quot;https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=ccd1f45&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=ccd1f45&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-10704&quot; title=&quot;REPLACENODE can make the collection lost data which replicaFactor is 1 &quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-10704&quot;&gt;&lt;del&gt;SOLR-10704&lt;/del&gt;&lt;/a&gt; Wait until all leader replicas are recovered before deleting&lt;br/&gt;
the originals.&lt;/p&gt;</comment>
                            <comment id="16048059" author="ab" created="Tue, 13 Jun 2017 15:59:47 +0000"  >&lt;p&gt;Fix it so that we wait until the new replica completes recovery in case of leader replicas.&lt;/p&gt;</comment>
                            <comment id="16048105" author="jira-bot" created="Tue, 13 Jun 2017 16:53:51 +0000"  >&lt;p&gt;Commit cdccbfb92f30cc70a3fee4c1b3655b1aaf1acb7a in lucene-solr&apos;s branch refs/heads/master from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ab&quot; class=&quot;user-hover&quot; rel=&quot;ab&quot;&gt;ab&lt;/a&gt;&lt;br/&gt;
[ &lt;a href=&quot;https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=cdccbfb&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=cdccbfb&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-10704&quot; title=&quot;REPLACENODE can make the collection lost data which replicaFactor is 1 &quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-10704&quot;&gt;&lt;del&gt;SOLR-10704&lt;/del&gt;&lt;/a&gt; Update the ref guide.&lt;/p&gt;</comment>
                            <comment id="16048114" author="jira-bot" created="Tue, 13 Jun 2017 17:09:09 +0000"  >&lt;p&gt;Commit 9612e318568e48ea187700accc1f6e88b1afea43 in lucene-solr&apos;s branch refs/heads/branch_6x from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ab&quot; class=&quot;user-hover&quot; rel=&quot;ab&quot;&gt;ab&lt;/a&gt;&lt;br/&gt;
[ &lt;a href=&quot;https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=9612e31&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=9612e31&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-10704&quot; title=&quot;REPLACENODE can make the collection lost data which replicaFactor is 1 &quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-10704&quot;&gt;&lt;del&gt;SOLR-10704&lt;/del&gt;&lt;/a&gt; Update the ref guide.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12868736" name="219.log" size="9090" author="daisy_yu" created="Thu, 18 May 2017 11:27:03 +0000"/>
                            <attachment id="12872835" name="SOLR-10704.patch" size="9508" author="ab" created="Tue, 13 Jun 2017 11:06:56 +0000"/>
                            <attachment id="12872768" name="SOLR-10704.patch" size="8022" author="ab" created="Mon, 12 Jun 2017 20:10:08 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 23 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3f5wn:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12313321" key="com.atlassian.jira.toolkit:message">
                        <customfieldname>Solr Mailing List Info</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>