diff --git a/solr/CHANGES.txt b/solr/CHANGES.txt
index 8fc3f137d84..78456ae7052 100644
--- a/solr/CHANGES.txt
+++ b/solr/CHANGES.txt
@@ -332,6 +332,9 @@ New Features
 
 * SOLR-11093: Add support for PointFields for {!graph} query. (yonik)
 
+* SOLR-10845: Add support for PointFields to {!graphTerms} query that is internally
+  used by some graph traversal streaming expressions.  (yonik)
+
 Bug Fixes
 ----------------------
 * SOLR-9262: Connection and read timeouts are being ignored by UpdateShardHandler after SOLR-4509.
diff --git a/solr/core/src/java/org/apache/solr/search/GraphTermsQParserPlugin.java b/solr/core/src/java/org/apache/solr/search/GraphTermsQParserPlugin.java
index d659265b1e4..4656afe0928 100644
--- a/solr/core/src/java/org/apache/solr/search/GraphTermsQParserPlugin.java
+++ b/solr/core/src/java/org/apache/solr/search/GraphTermsQParserPlugin.java
@@ -20,13 +20,22 @@ package org.apache.solr.search;
 
 import java.io.IOException;
 import java.util.ArrayList;
+import java.util.Arrays;
 import java.util.List;
 import java.util.Set;
 
+import org.apache.lucene.document.DoublePoint;
+import org.apache.lucene.document.FloatPoint;
+import org.apache.lucene.document.IntPoint;
+import org.apache.lucene.document.LongPoint;
 import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.index.PointValues;
 import org.apache.lucene.index.PostingsEnum;
+import org.apache.lucene.index.PrefixCodedTerms;
+import org.apache.lucene.index.ReaderUtil;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermContext;
 import org.apache.lucene.index.TermState;
@@ -45,13 +54,18 @@ import org.apache.lucene.search.Scorer;
 import org.apache.lucene.search.Weight;
 import org.apache.lucene.util.ArrayUtil;
 import org.apache.lucene.util.BitDocIdSet;
+import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.BytesRefBuilder;
+import org.apache.lucene.util.BytesRefIterator;
 import org.apache.lucene.util.DocIdSetBuilder;
 import org.apache.lucene.util.FixedBitSet;
+import org.apache.lucene.util.StringHelper;
 import org.apache.solr.common.params.SolrParams;
 import org.apache.solr.request.SolrQueryRequest;
 import org.apache.solr.schema.FieldType;
+import org.apache.solr.schema.NumberType;
+import org.apache.solr.schema.SchemaField;
 
 /**
  *  The GraphTermsQuery builds a disjunction query from a list of terms. The terms are first filtered by the maxDocFreq parameter.
@@ -79,6 +93,49 @@ public class GraphTermsQParserPlugin extends QParserPlugin {
 
         final String[] splitVals = qstr.split(",");
 
+        SchemaField sf = req.getSchema().getField(fname);
+
+        // if we don't limit by maxDocFreq, then simply use a normal set query
+        if (maxDocFreq == Integer.MAX_VALUE) {
+          return sf.getType().getSetQuery(this, sf, Arrays.asList(splitVals));
+        }
+
+        if (sf.getType().isPointField()) {
+          PointSetQuery setQ = null;
+          if (sf.getType().getNumberType() == NumberType.INTEGER) {
+            int[] vals = new int[splitVals.length];
+            for (int i=0; i<vals.length; i++) {
+              vals[i] = Integer.parseInt(splitVals[i]);
+            }
+            Arrays.sort(vals);
+            setQ = PointSetQuery.newSetQuery(sf.getName(), vals);
+          } else if (sf.getType().getNumberType() == NumberType.LONG || sf.getType().getNumberType() == NumberType.DATE) {
+            long[] vals = new long[splitVals.length];
+            for (int i=0; i<vals.length; i++) {
+              vals[i] = Long.parseLong(splitVals[i]);
+            }
+            Arrays.sort(vals);
+            setQ = PointSetQuery.newSetQuery(sf.getName(), vals);
+          } else if (sf.getType().getNumberType() == NumberType.FLOAT) {
+            float[] vals = new float[splitVals.length];
+            for (int i=0; i<vals.length; i++) {
+              vals[i] = Float.parseFloat(splitVals[i]);
+            }
+            Arrays.sort(vals);
+            setQ = PointSetQuery.newSetQuery(sf.getName(), vals);
+          } else if (sf.getType().getNumberType() == NumberType.DOUBLE) {
+            double[] vals = new double[splitVals.length];
+            for (int i=0; i<vals.length; i++) {
+              vals[i] = Double.parseDouble(splitVals[i]);
+            }
+            Arrays.sort(vals);
+            setQ = PointSetQuery.newSetQuery(sf.getName(), vals);
+          }
+
+          setQ.setMaxDocFreq(maxDocFreq);
+          return setQ;
+        }
+
         Term[] terms = new Term[splitVals.length];
         BytesRefBuilder term = new BytesRefBuilder();
         for (int i = 0; i < splitVals.length; i++) {
@@ -312,3 +369,410 @@ public class GraphTermsQParserPlugin extends QParserPlugin {
     }
   }
 }
+
+
+
+// modified version of PointInSetQuery
+abstract class PointSetQuery extends Query implements DocSetProducer {
+  // A little bit overkill for us, since all of our "terms" are always in the same field:
+  final PrefixCodedTerms sortedPackedPoints;
+  final int sortedPackedPointsHashCode;
+  final String field;
+  final int bytesPerDim;
+  final int numDims;
+  int maxDocFreq = Integer.MAX_VALUE;
+
+  /**
+   * Iterator of encoded point values.
+   */
+  // TODO: if we want to stream, maybe we should use jdk stream class?
+  public static abstract class Stream implements BytesRefIterator {
+    @Override
+    public abstract BytesRef next();
+  };
+
+  public void setMaxDocFreq(int maxDocFreq) {
+    this.maxDocFreq = maxDocFreq;
+  }
+
+  public static PointSetQuery newSetQuery(String field, float... sortedValues) {
+
+    final BytesRef encoded = new BytesRef(new byte[Float.BYTES]);
+
+    return new PointSetQuery(field, 1, Float.BYTES,
+        new PointSetQuery.Stream() {
+
+          int upto;
+
+          @Override
+          public BytesRef next() {
+            if (upto == sortedValues.length) {
+              return null;
+            } else {
+              FloatPoint.encodeDimension(sortedValues[upto], encoded.bytes, 0);
+              upto++;
+              return encoded;
+            }
+          }
+        }) {
+      @Override
+      protected String toString(byte[] value) {
+        assert value.length == Float.BYTES;
+        return Float.toString(FloatPoint.decodeDimension(value, 0));
+      }
+    };
+  }
+
+  public static PointSetQuery newSetQuery(String field, long... sortedValues) {
+    final BytesRef encoded = new BytesRef(new byte[Long.BYTES]);
+
+    return new PointSetQuery(field, 1, Long.BYTES,
+        new PointSetQuery.Stream() {
+
+          int upto;
+
+          @Override
+          public BytesRef next() {
+            if (upto == sortedValues.length) {
+              return null;
+            } else {
+              LongPoint.encodeDimension(sortedValues[upto], encoded.bytes, 0);
+              upto++;
+              return encoded;
+            }
+          }
+        }) {
+      @Override
+      protected String toString(byte[] value) {
+        assert value.length == Long.BYTES;
+        return Long.toString(LongPoint.decodeDimension(value, 0));
+      }
+    };
+  }
+
+  public static PointSetQuery newSetQuery(String field, int... sortedValues) {
+    final BytesRef encoded = new BytesRef(new byte[Integer.BYTES]);
+
+    return new PointSetQuery(field, 1, Integer.BYTES,
+        new PointSetQuery.Stream() {
+
+          int upto;
+
+          @Override
+          public BytesRef next() {
+            if (upto == sortedValues.length) {
+              return null;
+            } else {
+              IntPoint.encodeDimension(sortedValues[upto], encoded.bytes, 0);
+              upto++;
+              return encoded;
+            }
+          }
+        }) {
+      @Override
+      protected String toString(byte[] value) {
+        assert value.length == Integer.BYTES;
+        return Integer.toString(IntPoint.decodeDimension(value, 0));
+      }
+    };
+  }
+
+  public static PointSetQuery newSetQuery(String field, double... values) {
+
+    // Don't unexpectedly change the user's incoming values array:
+    double[] sortedValues = values.clone();
+    Arrays.sort(sortedValues);
+
+    final BytesRef encoded = new BytesRef(new byte[Double.BYTES]);
+
+    return new PointSetQuery(field, 1, Double.BYTES,
+        new PointSetQuery.Stream() {
+
+          int upto;
+
+          @Override
+          public BytesRef next() {
+            if (upto == sortedValues.length) {
+              return null;
+            } else {
+              DoublePoint.encodeDimension(sortedValues[upto], encoded.bytes, 0);
+              upto++;
+              return encoded;
+            }
+          }
+        }) {
+      @Override
+      protected String toString(byte[] value) {
+        assert value.length == Double.BYTES;
+        return Double.toString(DoublePoint.decodeDimension(value, 0));
+      }
+    };
+  }
+
+  public PointSetQuery(String field, int numDims, int bytesPerDim, Stream packedPoints) {
+    this.field = field;
+    this.bytesPerDim = bytesPerDim;
+    this.numDims = numDims;
+
+    // In the 1D case this works well (the more points, the more common prefixes they share, typically), but in
+    // the > 1 D case, where we are only looking at the first dimension's prefix bytes, it can at worst not hurt:
+    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();
+    BytesRefBuilder previous = null;
+    BytesRef current;
+    while ((current = packedPoints.next()) != null) {
+      if (current.length != numDims * bytesPerDim) {
+        throw new IllegalArgumentException("packed point length should be " + (numDims * bytesPerDim) + " but got " + current.length + "; field=\"" + field + "\" numDims=" + numDims + " bytesPerDim=" + bytesPerDim);
+      }
+      if (previous == null) {
+        previous = new BytesRefBuilder();
+      } else {
+        int cmp = previous.get().compareTo(current);
+        if (cmp == 0) {
+          continue; // deduplicate
+        } else if (cmp > 0) {
+          throw new IllegalArgumentException("values are out of order: saw " + previous + " before " + current);
+        }
+      }
+      builder.add(field, current);
+      previous.copyBytes(current);
+    }
+    sortedPackedPoints = builder.finish();
+    sortedPackedPointsHashCode = sortedPackedPoints.hashCode();
+  }
+
+  private FixedBitSet getLiveDocs(IndexSearcher searcher) throws IOException {
+    if (searcher instanceof SolrIndexSearcher) {
+      BitDocSet liveDocs = ((SolrIndexSearcher) searcher).getLiveDocs();
+      FixedBitSet liveBits = liveDocs.size() == ((SolrIndexSearcher) searcher).maxDoc() ? null : liveDocs.getBits();
+      return liveBits;
+    } else {
+      if (searcher.getTopReaderContext().reader().maxDoc() == searcher.getTopReaderContext().reader().numDocs()) return null;
+      FixedBitSet bs = new FixedBitSet(searcher.getTopReaderContext().reader().maxDoc());
+      for (LeafReaderContext ctx : searcher.getTopReaderContext().leaves()) {
+        Bits liveDocs = ctx.reader().getLiveDocs();
+        int max = ctx.reader().maxDoc();
+        int base = ctx.docBase;
+        for (int i=0; i<max; i++) {
+          if (liveDocs.get(i)) bs.set(i + base);
+        }
+      }
+      return bs;
+    }
+  }
+
+  @Override
+  public DocSet createDocSet(SolrIndexSearcher searcher) throws IOException {
+    return getDocSet(searcher);
+  }
+
+  public DocSet getDocSet(IndexSearcher searcher) throws IOException {
+    IndexReaderContext top = ReaderUtil.getTopLevelContext(searcher.getTopReaderContext());
+    List<LeafReaderContext> segs = top.leaves();
+    DocSetBuilder builder = new DocSetBuilder(top.reader().maxDoc(), Math.min(64,(top.reader().maxDoc()>>>10)+4));
+    PointValues[] segPoints = new PointValues[segs.size()];
+    for (int i=0; i<segPoints.length; i++) {
+      segPoints[i] = segs.get(i).reader().getPointValues(field);
+    }
+
+    int maxCollect = Math.min(maxDocFreq, top.reader().maxDoc());
+
+    PointSetQuery.CutoffPointVisitor visitor = new PointSetQuery.CutoffPointVisitor(maxCollect);
+    PrefixCodedTerms.TermIterator iterator = sortedPackedPoints.iterator();
+    outer: for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {
+      visitor.setPoint(point);
+      for (int i=0; i<segs.size(); i++) {
+        if (segPoints[i] == null) continue;
+        visitor.setBase(segs.get(i).docBase);
+        segPoints[i].intersect(visitor);
+        if (visitor.getCount() > maxDocFreq) {
+          continue outer;
+        }
+      }
+      int collected = visitor.getCount();
+      int[] ids = visitor.getGlobalIds();
+      for (int i=0; i<collected; i++) {
+        builder.add( ids[i] );
+      }
+    }
+
+    FixedBitSet liveDocs = getLiveDocs(searcher);
+    DocSet set = builder.build(liveDocs);
+    return set;
+  }
+
+
+  @Override
+  public final Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {
+    return new ConstantScoreWeight(this, boost) {
+      Filter filter;
+
+      @Override
+      public Scorer scorer(LeafReaderContext context) throws IOException {
+        if (filter == null) {
+          DocSet set = getDocSet(searcher);
+          filter = set.getTopFilter();
+        }
+
+        // Although this set only includes live docs, other filters can be pushed down to queries.
+        DocIdSet readerSet = filter.getDocIdSet(context, null);
+        if (readerSet == null) {
+          return null;
+        }
+        DocIdSetIterator readerSetIterator = readerSet.iterator();
+        if (readerSetIterator == null) {
+          return null;
+        }
+        return new ConstantScoreScorer(this, score(), readerSetIterator);
+      }
+    };
+  }
+
+
+  /** Cutoff point visitor that collects a maximum number of points before stopping. */
+  private class CutoffPointVisitor implements PointValues.IntersectVisitor {
+    int[] ids;
+    int base;
+    int count;
+    private final byte[] pointBytes;
+
+    public CutoffPointVisitor(int sz) {
+      this.pointBytes = new byte[bytesPerDim * numDims];
+      ids = new int[sz];
+    }
+
+    private void add(int id) {
+      if (count < ids.length) {
+        ids[count] = id + base;
+      }
+      count++;
+    }
+
+    public int getCount() { return count; }
+
+    public int[] getGlobalIds() { return ids; }
+
+    public void setPoint(BytesRef point) {
+      // we verified this up front in query's ctor:
+      assert point.length == pointBytes.length;
+      System.arraycopy(point.bytes, point.offset, pointBytes, 0, pointBytes.length);
+      count = 0;
+    }
+
+    public void setBase(int base) {
+      this.base = base;
+    }
+
+    @Override
+    public void grow(int count) {
+    }
+
+    @Override
+    public void visit(int docID) {
+      add(docID);
+    }
+
+    @Override
+    public void visit(int docID, byte[] packedValue) {
+      if (Arrays.equals(packedValue, pointBytes)) {
+        add(docID);
+      }
+    }
+
+    @Override
+    public PointValues.Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {
+
+      boolean crosses = false;
+
+      for(int dim=0;dim<numDims;dim++) {
+        int offset = dim*bytesPerDim;
+
+        int cmpMin = StringHelper.compare(bytesPerDim, minPackedValue, offset, pointBytes, offset);
+        if (cmpMin > 0) {
+          return PointValues.Relation.CELL_OUTSIDE_QUERY;
+        }
+
+        int cmpMax = StringHelper.compare(bytesPerDim, maxPackedValue, offset, pointBytes, offset);
+        if (cmpMax < 0) {
+          return PointValues.Relation.CELL_OUTSIDE_QUERY;
+        }
+
+        if (cmpMin != 0 || cmpMax != 0) {
+          crosses = true;
+        }
+      }
+
+      if (crosses) {
+        return PointValues.Relation.CELL_CROSSES_QUERY;
+      } else {
+        // NOTE: we only hit this if we are on a cell whose min and max values are exactly equal to our point,
+        // which can easily happen if many docs share this one value
+        return PointValues.Relation.CELL_INSIDE_QUERY;
+      }
+    }
+  }
+
+  public String getField() {
+    return field;
+  }
+
+  public int getNumDims() {
+    return numDims;
+  }
+
+  public int getBytesPerDim() {
+    return bytesPerDim;
+  }
+
+  @Override
+  public final int hashCode() {
+    int hash = classHash();
+    hash = 31 * hash + field.hashCode();
+    hash = 31 * hash + sortedPackedPointsHashCode;
+    hash = 31 * hash + numDims;
+    hash = 31 * hash + bytesPerDim;
+    hash = 31 * hash + maxDocFreq;
+    return hash;
+  }
+
+  @Override
+  public final boolean equals(Object other) {
+    return sameClassAs(other) &&
+        equalsTo(getClass().cast(other));
+  }
+
+  private boolean equalsTo(PointSetQuery other) {
+    return other.field.equals(field) &&
+        other.numDims == numDims &&
+        other.bytesPerDim == bytesPerDim &&
+        other.sortedPackedPointsHashCode == sortedPackedPointsHashCode &&
+        other.sortedPackedPoints.equals(sortedPackedPoints) &&
+        other.maxDocFreq == maxDocFreq;
+  }
+
+  @Override
+  public final String toString(String field) {
+    final StringBuilder sb = new StringBuilder();
+    if (this.field.equals(field) == false) {
+      sb.append(this.field);
+      sb.append(':');
+    }
+
+    sb.append("{");
+
+    PrefixCodedTerms.TermIterator iterator = sortedPackedPoints.iterator();
+    byte[] pointBytes = new byte[numDims * bytesPerDim];
+    boolean first = true;
+    for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {
+      if (first == false) {
+        sb.append(" ");
+      }
+      first = false;
+      System.arraycopy(point.bytes, point.offset, pointBytes, 0, pointBytes.length);
+      sb.append(toString(pointBytes));
+    }
+    sb.append("}");
+    return sb.toString();
+  }
+
+  protected abstract String toString(byte[] value);
+}
diff --git a/solr/core/src/test/org/apache/solr/search/TestGraphTermsQParserPlugin.java b/solr/core/src/test/org/apache/solr/search/TestGraphTermsQParserPlugin.java
index f62a8bf98bb..1cb927d3644 100644
--- a/solr/core/src/test/org/apache/solr/search/TestGraphTermsQParserPlugin.java
+++ b/solr/core/src/test/org/apache/solr/search/TestGraphTermsQParserPlugin.java
@@ -19,7 +19,6 @@ package org.apache.solr.search;
 
 import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
 import org.apache.solr.SolrTestCaseJ4;
-import org.apache.solr.SolrTestCaseJ4.SuppressPointFields;
 import org.apache.solr.common.params.ModifiableSolrParams;
 import org.junit.Before;
 import org.junit.BeforeClass;
@@ -27,7 +26,6 @@ import org.junit.Test;
 
 //We want codecs that support DocValues, and ones supporting blank/empty values.
 @SuppressCodecs({"Appending","Lucene3x","Lucene40","Lucene41","Lucene42"})
-@SuppressPointFields(bugUrl="https://issues.apache.org/jira/browse/SOLR-10845")
 public class TestGraphTermsQParserPlugin extends SolrTestCaseJ4 {
 
   @BeforeClass
