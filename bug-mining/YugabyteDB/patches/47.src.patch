diff --git a/src/yb/master/CMakeLists.txt b/src/yb/master/CMakeLists.txt
index 2e43422588..ca96efea30 100644
--- a/src/yb/master/CMakeLists.txt
+++ b/src/yb/master/CMakeLists.txt
@@ -79,6 +79,7 @@ set(MASTER_SRCS
   catalog_manager_util.cc
   catalog_entity_base.cc
   catalog_entity_info.cc
+  catalog_entity_tasks.cc
   catalog_manager_bg_tasks.cc
   catalog_loaders.cc
   xcluster_consumer_registry_service.cc
@@ -244,6 +245,7 @@ ADD_YB_TEST(flush_manager-test)
 ADD_YB_TEST(load_balancer_mocked-test)
 ADD_YB_TEST(master-test)
 ADD_YB_TEST(master_xrepl-test)
+ADD_YB_TEST(multi_step_monitored_task-test)
 ADD_YB_TEST(restore_sys_catalog_state_test)
 ADD_YB_TEST(stateful_service-test)
 ADD_YB_TEST(sys_catalog-test)
diff --git a/src/yb/master/catalog_entity_base.cc b/src/yb/master/catalog_entity_base.cc
index 5ed2f7fa89..5afe4009d4 100644
--- a/src/yb/master/catalog_entity_base.cc
+++ b/src/yb/master/catalog_entity_base.cc
@@ -21,6 +21,11 @@ namespace yb::master {
 CatalogEntityWithTasks::CatalogEntityWithTasks(scoped_refptr<TasksTracker> tasks_tracker)
     : tasks_tracker_(tasks_tracker) {}
 
+CatalogEntityWithTasks::~CatalogEntityWithTasks() {
+  CloseAndWaitForAllTasksToAbort();
+  DCHECK(!HasTasks());
+}
+
 std::size_t CatalogEntityWithTasks::NumTasks() const {
   SharedLock l(mutex_);
   return pending_tasks_.size();
@@ -144,4 +149,12 @@ std::unordered_set<server::MonitoredTaskPtr> CatalogEntityWithTasks::GetTasks()
   return pending_tasks_;
 }
 
+void CatalogEntityWithTasks::CloseAndWaitForAllTasksToAbort() {
+  VLOG(1) << "Aborting tasks";
+  AbortTasksAndClose();
+  VLOG(1) << "Waiting on Aborting tasks";
+  WaitTasksCompletion();
+  VLOG(1) << "Waiting on Aborting tasks done";
+}
+
 }  // namespace yb::master
diff --git a/src/yb/master/catalog_entity_base.h b/src/yb/master/catalog_entity_base.h
index 51d1e70bb1..5c9f13b03b 100644
--- a/src/yb/master/catalog_entity_base.h
+++ b/src/yb/master/catalog_entity_base.h
@@ -106,7 +106,7 @@ class SingletonMetadataCowWrapper : public MetadataCowWrapper<PersistentDataEntr
 class CatalogEntityWithTasks {
  public:
   explicit CatalogEntityWithTasks(scoped_refptr<TasksTracker> tasks_tracker);
-  virtual ~CatalogEntityWithTasks() = default;
+  virtual ~CatalogEntityWithTasks();
 
   bool HasTasks() const EXCLUDES(mutex_);
   bool HasTasks(server::MonitoredTaskType type) const EXCLUDES(mutex_);
@@ -124,6 +124,22 @@ class CatalogEntityWithTasks {
   // Wait for all inflight tasks to complete.
   void WaitTasksCompletion() EXCLUDES(mutex_);
 
+  void CloseAndWaitForAllTasksToAbort() EXCLUDES(mutex_);
+
+  template <typename IterableCatalogEntityWithTasks>
+  static void CloseAbortAndWaitForAllTasks(
+      const IterableCatalogEntityWithTasks& entity_collection) {
+    for (const auto& entity : entity_collection) {
+      VLOG(1) << entity->ToString() << ": Closing and aborting tasks";
+      entity->AbortTasksAndClose();
+    }
+    for (const auto& entity : entity_collection) {
+      VLOG(1) << entity->ToString() << ": Waiting for tasks for complete";
+      entity->WaitTasksCompletion();
+      VLOG(1) << entity->ToString() << ": Completed wait for tasks to complete";
+    }
+  }
+
  private:
   void AbortTasksAndCloseIfRequested(bool close) EXCLUDES(mutex_);
 
diff --git a/src/yb/master/catalog_entity_info-test.cc b/src/yb/master/catalog_entity_info-test.cc
index 48a56aed65..602a4fbd19 100644
--- a/src/yb/master/catalog_entity_info-test.cc
+++ b/src/yb/master/catalog_entity_info-test.cc
@@ -23,7 +23,8 @@ class CatalogEntityInfoTest : public YBTest {};
 
 // Verify that data mutations are not available from metadata() until commit.
 TEST_F(CatalogEntityInfoTest, TestNamespaceInfoCommit) {
-  scoped_refptr<NamespaceInfo> ns(new NamespaceInfo("deadbeafdeadbeafdeadbeafdeadbeaf"));
+  scoped_refptr<NamespaceInfo> ns(
+      new NamespaceInfo("deadbeafdeadbeafdeadbeafdeadbeaf", /*tasks_tracker=*/nullptr));
 
   // Mutate the namespace, under the write lock.
   auto writer_lock = ns->LockForWrite();
diff --git a/src/yb/master/catalog_entity_info.cc b/src/yb/master/catalog_entity_info.cc
index c1f232f43d..4fa509af50 100644
--- a/src/yb/master/catalog_entity_info.cc
+++ b/src/yb/master/catalog_entity_info.cc
@@ -1069,7 +1069,8 @@ void DeletedTableInfo::AddTabletsToMap(DeletedTabletMap* tablet_map) {
 // NamespaceInfo
 // ================================================================================================
 
-NamespaceInfo::NamespaceInfo(NamespaceId ns_id) : namespace_id_(std::move(ns_id)) {}
+NamespaceInfo::NamespaceInfo(NamespaceId ns_id, scoped_refptr<TasksTracker> tasks_tracker)
+    : CatalogEntityWithTasks(std::move(tasks_tracker)), namespace_id_(std::move(ns_id)) {}
 
 const NamespaceName NamespaceInfo::name() const {
   return LockForRead()->pb.name();
diff --git a/src/yb/master/catalog_entity_info.h b/src/yb/master/catalog_entity_info.h
index 616dce4d5e..77c7cdf0ba 100644
--- a/src/yb/master/catalog_entity_info.h
+++ b/src/yb/master/catalog_entity_info.h
@@ -840,9 +840,10 @@ struct PersistentNamespaceInfo : public Persistent<
 // This object uses copy-on-write techniques similarly to TabletInfo.
 // Please see the TabletInfo class doc above for more information.
 class NamespaceInfo : public RefCountedThreadSafe<NamespaceInfo>,
-                      public MetadataCowWrapper<PersistentNamespaceInfo> {
+                      public MetadataCowWrapper<PersistentNamespaceInfo>,
+                      public CatalogEntityWithTasks {
  public:
-  explicit NamespaceInfo(NamespaceId ns_id);
+  explicit NamespaceInfo(NamespaceId ns_id, scoped_refptr<TasksTracker> tasks_tracker);
 
   virtual const NamespaceId& id() const override { return namespace_id_; }
 
diff --git a/src/yb/master/catalog_entity_tasks.cc b/src/yb/master/catalog_entity_tasks.cc
new file mode 100644
index 0000000000..5fd08c91da
--- /dev/null
+++ b/src/yb/master/catalog_entity_tasks.cc
@@ -0,0 +1,38 @@
+// Copyright (c) YugabyteDB, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
+// in compliance with the License.  You may obtain a copy of the License at
+//
+// http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software distributed under the License
+// is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
+// or implied.  See the License for the specific language governing permissions and limitations
+// under the License.
+//
+
+#include "yb/master/catalog_entity_tasks.h"
+
+#include "yb/master/catalog_entity_info.h"
+#include "yb/master/catalog_manager.h"
+
+namespace yb::master {
+
+MultiStepTableTaskBase::MultiStepTableTaskBase(
+    CatalogManager& catalog_manager, ThreadPool& async_task_pool, rpc::Messenger& messenger,
+    TableInfoPtr table_info, const LeaderEpoch& epoch)
+    : MultiStepCatalogEntityTask(
+          catalog_manager.GetValidateEpochFunc(), async_task_pool, messenger, *table_info, epoch),
+      catalog_manager_(catalog_manager),
+      table_info_(std::move(table_info)) {}
+
+MultiStepNamespaceTaskBase::MultiStepNamespaceTaskBase(
+    CatalogManager& catalog_manager, ThreadPool& async_task_pool, rpc::Messenger& messenger,
+    NamespaceInfo& namespace_info, const LeaderEpoch& epoch)
+    : MultiStepCatalogEntityTask(
+          catalog_manager.GetValidateEpochFunc(), async_task_pool, messenger, namespace_info,
+          epoch),
+      catalog_manager_(catalog_manager),
+      namespace_info_(namespace_info) {}
+
+}  // namespace yb::master
diff --git a/src/yb/master/catalog_entity_tasks.h b/src/yb/master/catalog_entity_tasks.h
new file mode 100644
index 0000000000..fdb59fa792
--- /dev/null
+++ b/src/yb/master/catalog_entity_tasks.h
@@ -0,0 +1,44 @@
+// Copyright (c) YugabyteDB, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
+// in compliance with the License.  You may obtain a copy of the License at
+//
+// http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software distributed under the License
+// is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
+// or implied.  See the License for the specific language governing permissions and limitations
+// under the License.
+//
+
+#pragma once
+
+#include "yb/master/master_fwd.h"
+#include "yb/master/multi_step_monitored_task.h"
+
+namespace yb {
+
+namespace master {
+
+class MultiStepTableTaskBase : public MultiStepCatalogEntityTask {
+ protected:
+  MultiStepTableTaskBase(
+      CatalogManager& catalog_manager, ThreadPool& async_task_pool, rpc::Messenger& messenger,
+      TableInfoPtr table_info, const LeaderEpoch& epoch);
+
+  CatalogManager& catalog_manager_;
+  const TableInfoPtr table_info_;
+};
+
+class MultiStepNamespaceTaskBase : public MultiStepCatalogEntityTask {
+ protected:
+  MultiStepNamespaceTaskBase(
+      CatalogManager& catalog_manager, ThreadPool& async_task_pool, rpc::Messenger& messenger,
+      NamespaceInfo& namespace_info, const LeaderEpoch& epoch);
+
+  CatalogManager& catalog_manager_;
+  NamespaceInfo& namespace_info_;
+};
+
+}  // namespace master
+}  // namespace yb
diff --git a/src/yb/master/catalog_loaders.cc b/src/yb/master/catalog_loaders.cc
index 82c38e2e73..289e9bed9d 100644
--- a/src/yb/master/catalog_loaders.cc
+++ b/src/yb/master/catalog_loaders.cc
@@ -416,7 +416,7 @@ Status NamespaceLoader::Visit(const NamespaceId& ns_id, const SysNamespaceEntryP
     << "Namespace already exists: " << ns_id;
 
   // Setup the namespace info.
-  scoped_refptr<NamespaceInfo> ns = new NamespaceInfo(ns_id);
+  scoped_refptr<NamespaceInfo> ns = new NamespaceInfo(ns_id, catalog_manager_->GetTasksTracker());
   auto l = ns->LockForWrite();
   const auto& pb_data = l->pb;
 
diff --git a/src/yb/master/catalog_manager.cc b/src/yb/master/catalog_manager.cc
index cef80b6651..d8bb8fa1a4 100644
--- a/src/yb/master/catalog_manager.cc
+++ b/src/yb/master/catalog_manager.cc
@@ -900,19 +900,6 @@ GetMoreEligibleSysCatalogLeaders(
   return {scored_masters, my_score == 0 || my_score < affinitized_zones.size()};
 }
 
-template <typename IterableTables>
-void AbortAndWaitForAllTasksImplementation(const IterableTables& tables) {
-  for (const auto& t : tables) {
-    VLOG(1) << "Aborting tasks for table " << t->ToString();
-    t->AbortTasksAndClose();
-  }
-  for (const auto& t : tables) {
-    VLOG(1) << "Waiting on Aborting tasks for table " << t->ToString();
-    t->WaitTasksCompletion();
-  }
-  VLOG(1) << "Waiting on Aborting tasks done";
-}
-
 // Sets basic fields in the TabletLocationsPB proto that are always filled regardless of the
 // PartitionsOnly parameter.
 void InitializeTabletLocationsPB(
@@ -1009,6 +996,19 @@ void CatalogManager::NamespaceNameMapper::clear() {
   }
 }
 
+std::vector<scoped_refptr<NamespaceInfo>> CatalogManager::NamespaceNameMapper::GetAll() const {
+  std::vector<scoped_refptr<NamespaceInfo>> result;
+  for (const auto& map : typed_maps_) {
+    for (const auto& [_, ns_info] : map) {
+      if (ns_info) {
+        result.emplace_back(ns_info);
+      }
+    }
+  }
+
+  return result;
+}
+
 CatalogManager::CatalogManager(Master* master)
     : master_(DCHECK_NOTNULL(master)),
       tablet_exists_(false),
@@ -1383,9 +1383,8 @@ Status CatalogManager::VisitSysCatalog(SysCatalogLoadingState* state) {
     LockGuard lock(mutex_);
     VLOG_WITH_FUNC(3) << "Acquired the catalog manager lock";
 
-    // Abort any outstanding tasks. All TableInfos are orphaned below, so
-    // it's important to end their tasks now; otherwise Shutdown() will
-    // destroy master state used by these tasks.
+    // Abort any outstanding tasks. All CatalogEntities are orphaned below, so
+    // it's important to end their tasks now.
     AbortAndWaitForAllTasksUnlocked();
 
     // Clear internal maps and run data loaders.
@@ -2093,7 +2092,7 @@ Status CatalogManager::PrepareNamespace(
   ns_entry.set_state(SysNamespaceEntryPB::RUNNING);
 
   // Create in memory object.
-  ns = new NamespaceInfo(id);
+  ns = new NamespaceInfo(id, tasks_tracker_);
 
   // Prepare write.
   auto l = ns->LockForWrite();
@@ -2224,6 +2223,12 @@ Status CatalogManager::CheckIsLeaderAndReady() const {
   return Status::OK();
 }
 
+Status CatalogManager::IsEpochValid(const LeaderEpoch& epoch) const {
+  SCHECK_EQ(epoch, GetLeaderEpochInternal(), IllegalState, "Master leader epoch has changed");
+  RETURN_NOT_OK(CheckIsLeaderAndReady());
+  return Status::OK();
+}
+
 std::shared_ptr<tablet::TabletPeer> CatalogManager::tablet_peer() const {
   DCHECK(sys_catalog_);
   return sys_catalog_->tablet_peer();
@@ -2288,19 +2293,9 @@ void CatalogManager::CompleteShutdown() {
     async_task_pool_->Shutdown();
   }
 
-  // Mark all outstanding table tasks as aborted and wait for them to fail.
-  //
-  // There may be an outstanding table visitor thread modifying the table map,
-  // so we must make a copy of it before we iterate. It's OK if the visitor
-  // adds more entries to the map even after we finish; it won't start any new
-  // tasks for those entries.
-  vector<scoped_refptr<TableInfo>> copy;
-  {
-    SharedLock lock(mutex_);
-    auto tables_it = tables_->GetAllTables();
-    copy = std::vector(std::begin(tables_it), std::end(tables_it));
-  }
-  AbortAndWaitForAllTasks(copy);
+  // It's OK if the visitor adds more entries even after we finish; it won't start any new tasks for
+  // those entries.
+  AbortAndWaitForAllTasks();
 
   cdc_state_table_.reset();
 
@@ -2339,8 +2334,7 @@ Status CatalogManager::AbortTableCreation(TableInfo* table,
   // Since this is a failed creation attempt, it's safe to just abort
   // all tasks, as (by definition) no tasks may be pending against a
   // table that has failed to successfully create.
-  table->AbortTasksAndClose();
-  table->WaitTasksCompletion();
+  table->CloseAndWaitForAllTasksToAbort();
   {
     LockGuard lock(mutex_);
 
@@ -8941,7 +8935,7 @@ Status CatalogManager::CreateNamespace(const CreateNamespaceRequestPB* req,
     // Create unique id for this new namespace.
     NamespaceId new_id = !req->namespace_id().empty()
         ? req->namespace_id() : GenerateIdUnlocked(SysRowEntryType::NAMESPACE);
-    ns = new NamespaceInfo(new_id);
+    ns = new NamespaceInfo(new_id, tasks_tracker_);
     ns->mutable_metadata()->StartMutation();
     SysNamespaceEntryPB *metadata = &ns->mutable_metadata()->mutable_dirty()->pb;
     metadata->set_name(req->name());
@@ -12883,12 +12877,22 @@ void CatalogManager::ResetTasksTrackers() {
   VLOG_WITH_FUNC(1) << "End";
 }
 
-void CatalogManager::AbortAndWaitForAllTasks(const vector<scoped_refptr<TableInfo>>& tables) {
-  AbortAndWaitForAllTasksImplementation(tables);
+void CatalogManager::AbortAndWaitForAllTasks() {
+  std::vector<scoped_refptr<TableInfo>> tables;
+  std::vector<scoped_refptr<NamespaceInfo>> namespaces;
+  {
+    SharedLock lock(mutex_);
+    auto tables_it = tables_->GetAllTables();
+    tables = std::vector(std::begin(tables_it), std::end(tables_it));
+    namespaces = namespace_names_mapper_.GetAll();
+  }
+  CatalogEntityWithTasks::CloseAbortAndWaitForAllTasks(tables);
+  CatalogEntityWithTasks::CloseAbortAndWaitForAllTasks(namespaces);
 }
 
 void CatalogManager::AbortAndWaitForAllTasksUnlocked() {
-  AbortAndWaitForAllTasksImplementation(tables_->GetAllTables());
+  CatalogEntityWithTasks::CloseAbortAndWaitForAllTasks(tables_->GetAllTables());
+  CatalogEntityWithTasks::CloseAbortAndWaitForAllTasks(namespace_names_mapper_.GetAll());
 }
 
 void CatalogManager::HandleNewTableId(const TableId& table_id) {
diff --git a/src/yb/master/catalog_manager.h b/src/yb/master/catalog_manager.h
index 7a0149b11b..65ab9d56f1 100644
--- a/src/yb/master/catalog_manager.h
+++ b/src/yb/master/catalog_manager.h
@@ -211,6 +211,7 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
     NamespaceInfoMap& operator[](YQLDatabase db_type);
     const NamespaceInfoMap& operator[](YQLDatabase db_type) const;
     void clear();
+    std::vector<scoped_refptr<NamespaceInfo>> GetAll() const;
 
    private:
     std::array<NamespaceInfoMap, 4> typed_maps_;
@@ -858,6 +859,12 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
   // initialized before calling this method.
   Status CheckIsLeaderAndReady() const override;
 
+  Status IsEpochValid(const LeaderEpoch& epoch) const;
+
+  auto GetValidateEpochFunc() const {
+    return std::bind(&CatalogManager::IsEpochValid, this, std::placeholders::_1);
+  }
+
   // Returns this CatalogManager's role in a consensus configuration. CatalogManager
   // must be initialized before calling this method.
   PeerRole Role() const;
@@ -1559,6 +1566,8 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
       const std::vector<TableId>& table_ids, const std::optional<const NamespaceId>& namespace_id,
       CreateCDCStreamResponsePB* resp, const LeaderEpoch& epoch, rpc::RpcContext* rpc);
 
+  auto GetTasksTracker() { return tasks_tracker_; }
+
  protected:
   // TODO Get rid of these friend classes and introduce formal interface.
   friend class TableLoader;
@@ -2004,7 +2013,7 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
   // Removes all tasks from jobs_tracker_ and tasks_tracker_.
   void ResetTasksTrackers();
   // Aborts all tasks belonging to 'tables' and waits for them to finish.
-  void AbortAndWaitForAllTasks(const std::vector<scoped_refptr<TableInfo>>& tables);
+  void AbortAndWaitForAllTasks() EXCLUDES(mutex_);
   void AbortAndWaitForAllTasksUnlocked() REQUIRES_SHARED(mutex_);
 
   // Can be used to create background_tasks_ field for this master.
diff --git a/src/yb/master/multi_step_monitored_task-test.cc b/src/yb/master/multi_step_monitored_task-test.cc
new file mode 100644
index 0000000000..a9a4a458d4
--- /dev/null
+++ b/src/yb/master/multi_step_monitored_task-test.cc
@@ -0,0 +1,175 @@
+// Copyright (c) YugabyteDB, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
+// in compliance with the License.  You may obtain a copy of the License at
+//
+// http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software distributed under the License
+// is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
+// or implied.  See the License for the specific language governing permissions and limitations
+// under the License.
+//
+
+#include <gtest/gtest.h>
+
+#include "yb/master/catalog_entity_base.h"
+#include "yb/master/multi_step_monitored_task.h"
+#include "yb/master/tasks_tracker.h"
+#include "yb/rpc/messenger.h"
+#include "yb/util/scope_exit.h"
+#include "yb/util/sync_point.h"
+#include "yb/util/test_macros.h"
+#include "yb/util/threadpool.h"
+
+using namespace std::placeholders;
+
+DECLARE_bool(TEST_enable_sync_points);
+
+namespace yb::master {
+
+class CatalogEntityWithTasksMock : public CatalogEntityWithTasks {
+ public:
+  CatalogEntityWithTasksMock() : CatalogEntityWithTasks(/*tasks_tracker=*/nullptr) {}
+};
+
+const auto kTaskType = server::MonitoredTaskType::kAddServer;
+const auto kTaskDelay = MonoDelta::FromSeconds(2);
+LeaderEpoch leader_epoch = LeaderEpoch(1, 1);
+
+class TestCatalogEntityTask : public MultiStepCatalogEntityTask {
+ public:
+  TestCatalogEntityTask(
+      CatalogEntityWithTasks& catalog_entity, ThreadPool& async_task_pool,
+      rpc::Messenger& messenger, const LeaderEpoch& epoch)
+      : MultiStepCatalogEntityTask(
+            std::bind(&TestCatalogEntityTask::ValidateEpoch, this, _1), async_task_pool, messenger,
+            catalog_entity, epoch) {
+    completion_callback_ = [this](const Status& status) {
+      completion_sync_.AsStatusFunctor()(status);
+    };
+  }
+
+  ~TestCatalogEntityTask() = default;
+
+  Status ValidateEpoch(const LeaderEpoch& epoch) {
+    SCHECK_EQ(epoch, leader_epoch, IllegalState, "Epoch does not match");
+    return Status::OK();
+  }
+
+  server::MonitoredTaskType type() const override { return kTaskType; }
+
+  std::string type_name() const override { return "Test task"; }
+
+  std::string description() const override { return "Test task"; }
+
+  Status FirstStep() override {
+    current_step_ = 1;
+    ScheduleNextStep(std::bind(&TestCatalogEntityTask::SecondStep, this), "second step");
+    return Status::OK();
+  }
+
+  Status SecondStep() {
+    current_step_ = 2;
+    TEST_SYNC_POINT("SecondStep");
+    TEST_SYNC_POINT("SecondStepBeforeDelay");
+    ScheduleNextStepWithDelay(
+        std::bind(&TestCatalogEntityTask::ThirdStep, this), "third step", kTaskDelay * 2);
+    return Status::OK();
+  }
+
+  Status ThirdStep() {
+    current_step_ = 3;
+    Complete();
+    return Status::OK();
+  }
+
+  uint32 current_step_ = 0;
+  Synchronizer completion_sync_;
+};
+
+TEST(CatalogEntityTaskTest, TestMultipleSteps) {
+  google::SetVLOGLevel("multi_step*", 4);
+
+  ANNOTATE_UNPROTECTED_WRITE(FLAGS_TEST_enable_sync_points) = true;
+  yb::SyncPoint::GetInstance()->LoadDependency(
+      {{"SecondStep", "ReachedSecondStep"}, {"ResumeSecondStep", "SecondStepBeforeDelay"}});
+  yb::SyncPoint::GetInstance()->EnableProcessing();
+
+  std::unique_ptr<ThreadPool> thread_pool;
+  ThreadPoolBuilder thread_pool_builder("Test");
+  ASSERT_OK(thread_pool_builder.Build(&thread_pool));
+
+  rpc::MessengerBuilder messenger_builder("Test");
+  auto messenger = ASSERT_RESULT(messenger_builder.Build());
+
+  CatalogEntityWithTasksMock catalog_entity;
+
+  auto task = std::make_shared<TestCatalogEntityTask>(
+      catalog_entity, *thread_pool.get(), *messenger.get(), leader_epoch);
+  task->Start();
+  ASSERT_TRUE(catalog_entity.HasTasks());
+  ASSERT_EQ(catalog_entity.NumTasks(), 1);
+  ASSERT_EQ(catalog_entity.HasTasks(kTaskType), 1);
+
+  TEST_SYNC_POINT("ReachedSecondStep");
+  ASSERT_EQ(task->current_step_, 2);
+
+  auto start_time = CoarseMonoClock::now();
+  TEST_SYNC_POINT("ResumeSecondStep");
+
+  catalog_entity.WaitTasksCompletion();
+  ASSERT_OK(task->completion_sync_.Wait());
+  ASSERT_EQ(task->current_step_, 3);
+  ASSERT_GE(MonoDelta(CoarseMonoClock::now() - start_time), kTaskDelay);
+
+  ASSERT_FALSE(catalog_entity.HasTasks());
+  ASSERT_EQ(catalog_entity.NumTasks(), 0);
+  ASSERT_EQ(catalog_entity.HasTasks(kTaskType), 0);
+
+  messenger->Shutdown();
+  thread_pool->Shutdown();
+}
+
+TEST(CatalogEntityTaskTest, TestEpochChanges) {
+  google::SetVLOGLevel("multi_step*", 4);
+
+  ANNOTATE_UNPROTECTED_WRITE(FLAGS_TEST_enable_sync_points) = true;
+  yb::SyncPoint::GetInstance()->LoadDependency({
+      {"SecondStep", "ReachedSecondStep"},
+      {"ResumeSecondStep", "SecondStepBeforeDelay"},
+  });
+  yb::SyncPoint::GetInstance()->EnableProcessing();
+
+  std::unique_ptr<ThreadPool> thread_pool;
+  ThreadPoolBuilder thread_pool_builder("Test");
+  ASSERT_OK(thread_pool_builder.Build(&thread_pool));
+
+  rpc::MessengerBuilder messenger_builder("Test");
+  auto messenger = ASSERT_RESULT(messenger_builder.Build());
+  auto se = ScopeExit([messenger = messenger.get(), thread_pool = thread_pool.get()] {
+    messenger->Shutdown();
+    thread_pool->Shutdown();
+  });
+
+  CatalogEntityWithTasksMock catalog_entity;
+
+  auto task = std::make_shared<TestCatalogEntityTask>(
+      catalog_entity, *thread_pool.get(), *messenger.get(), leader_epoch);
+  task->Start();
+
+  TEST_SYNC_POINT("ReachedSecondStep");
+  leader_epoch.leader_term++;
+
+  TEST_SYNC_POINT("ResumeSecondStep");
+
+  catalog_entity.WaitTasksCompletion();
+  ASSERT_NOK(task->completion_sync_.Wait());
+  ASSERT_EQ(task->current_step_, 2);
+
+  ASSERT_FALSE(catalog_entity.HasTasks());
+  ASSERT_EQ(catalog_entity.NumTasks(), 0);
+  ASSERT_EQ(catalog_entity.HasTasks(kTaskType), 0);
+}
+
+}  // namespace yb::master
diff --git a/src/yb/master/multi_step_monitored_task.cc b/src/yb/master/multi_step_monitored_task.cc
index 6c95076107..1238fdf681 100644
--- a/src/yb/master/multi_step_monitored_task.cc
+++ b/src/yb/master/multi_step_monitored_task.cc
@@ -48,6 +48,10 @@ bool MultiStepMonitoredTask::TrySetState(server::MonitoredTaskState new_state) {
   return false;
 }
 
+void MultiStepMonitoredTask::TaskCompleted(const Status& status) {
+  // NoOp
+}
+
 void MultiStepMonitoredTask::Complete() {
   if (TrySetState(server::MonitoredTaskState::kComplete)) {
     EndTask(Status::OK());
@@ -196,7 +200,7 @@ void MultiStepMonitoredTask::EndTask(const Status& status) {
   LOG_WITH_PREFIX(INFO) << this << " task ended" << (status.ok() ? " successfully" : "");
 
   UnregisterTask();
-  // Unsafe to use this beyond this point.
+  // Unsafe to use 'this' beyond this point.
 
   if (callback) {
     callback(status);
@@ -273,30 +277,26 @@ Status MultiStepMonitoredTask::StartTasks(
   return Status::OK();
 }
 
-MultiStepTableTask::MultiStepTableTask(
-    CatalogManager& catalog_manager, ThreadPool& async_task_pool, rpc::Messenger& messenger,
-    TableInfoPtr table_info, const LeaderEpoch& epoch)
+MultiStepCatalogEntityTask::MultiStepCatalogEntityTask(
+    std::function<Status(const LeaderEpoch& epoch)> validate_epoch_func,
+    ThreadPool& async_task_pool, rpc::Messenger& messenger, CatalogEntityWithTasks& catalog_entity,
+    const LeaderEpoch& epoch)
     : MultiStepMonitoredTask(async_task_pool, messenger),
-      catalog_manager_(catalog_manager),
       epoch_(std::move(epoch)),
-      table_info_(std::move(table_info)) {}
+      catalog_entity_(catalog_entity),
+      validate_epoch_func_(std::move(validate_epoch_func)) {}
 
-Status MultiStepTableTask::RegisterTask() {
-  table_info_->AddTask(shared_from_this());
+Status MultiStepCatalogEntityTask::RegisterTask() {
+  catalog_entity_.AddTask(shared_from_this());
   return Status::OK();
 }
 
-void MultiStepTableTask::UnregisterTask() {
-  DCHECK(table_info_->HasTasks(type()));
+void MultiStepCatalogEntityTask::UnregisterTask() {
+  DCHECK(catalog_entity_.HasTasks(type()));
 
-  table_info_->RemoveTask(shared_from_this());
+  catalog_entity_.RemoveTask(shared_from_this());
 }
 
-Status MultiStepTableTask::ValidateRunnable() {
-  RETURN_NOT_OK(catalog_manager_.CheckIsLeaderAndReady());
-  SCHECK_EQ(
-      epoch_, catalog_manager_.GetLeaderEpochInternal(), IllegalState, "Epoch is no longer valid");
-  return Status::OK();
-}
+Status MultiStepCatalogEntityTask::ValidateRunnable() { return validate_epoch_func_(epoch_); }
 
 }  // namespace yb::master
diff --git a/src/yb/master/multi_step_monitored_task.h b/src/yb/master/multi_step_monitored_task.h
index fefe5293ca..f51d38655d 100644
--- a/src/yb/master/multi_step_monitored_task.h
+++ b/src/yb/master/multi_step_monitored_task.h
@@ -14,7 +14,6 @@
 #pragma once
 
 #include "yb/master/leader_epoch.h"
-#include "yb/master/master_fwd.h"
 #include "yb/rpc/rpc_fwd.h"
 #include "yb/server/monitored_task.h"
 #include "yb/util/status_callback.h"
@@ -25,6 +24,9 @@ class ThreadPool;
 
 namespace master {
 
+class CatalogManager;
+class CatalogEntityWithTasks;
+
 // Tasks that contain multiple asynchronous steps.
 // - All steps run on the passed in async thread pool. Steps can be scheduled to run immediately or
 // with a delay.
@@ -76,7 +78,7 @@ class MultiStepMonitoredTask : public server::RunnableMonitoredTask {
   // This is invoked when the task reaches a terminal state and before it is Unregistered. Status
   // will be OK if the task succeeded and not OK if it failed. Note: Do not invoke this directly. It
   // will be invoked when any step returns a bad status.
-  virtual void TaskCompleted(const Status& status) = 0;
+  virtual void TaskCompleted(const Status& status);
 
   // ==================================================================
   // Helper methods that derived class steps can invoke.
@@ -125,22 +127,24 @@ class MultiStepMonitoredTask : public server::RunnableMonitoredTask {
   std::function<Status()> next_step_ GUARDED_BY(schedule_task_mutex_) = nullptr;
 };
 
-// A MultiStepMonitoredTask that is tied to a single Table and the master leader epoch.
-class MultiStepTableTask : public MultiStepMonitoredTask {
- public:
+// A MultiStepMonitoredTask that is tied to a single CatalogEntity object (ex: Table) and the master
+// leader epoch.
+class MultiStepCatalogEntityTask : public MultiStepMonitoredTask {
  protected:
-  MultiStepTableTask(
-      CatalogManager& catalog_manager, ThreadPool& async_task_pool, rpc::Messenger& messenger,
-      TableInfoPtr table_info, const LeaderEpoch& epoch);
+  MultiStepCatalogEntityTask(
+      std::function<Status(const LeaderEpoch& epoch)> validate_epoch_func,
+      ThreadPool& async_task_pool, rpc::Messenger& messenger,
+      CatalogEntityWithTasks& catalog_entity, const LeaderEpoch& epoch);
   Status ValidateRunnable() override;
 
-  CatalogManager& catalog_manager_;
   const LeaderEpoch epoch_;
-  const TableInfoPtr table_info_;
+  CatalogEntityWithTasks& catalog_entity_;
 
  private:
   Status RegisterTask() override;
   void UnregisterTask() override;
+
+  const std::function<Status(const LeaderEpoch& epoch)> validate_epoch_func_;
 };
 
 }  // namespace master
diff --git a/src/yb/master/post_tablet_create_task_base.cc b/src/yb/master/post_tablet_create_task_base.cc
index 0f4219e541..3961bbada1 100644
--- a/src/yb/master/post_tablet_create_task_base.cc
+++ b/src/yb/master/post_tablet_create_task_base.cc
@@ -41,11 +41,11 @@ void MarkTableAsRunningIfHealthy(
 PostTabletCreateTaskBase::PostTabletCreateTaskBase(
     CatalogManager& catalog_manager, ThreadPool& async_task_pool, rpc::Messenger& messenger,
     TableInfoPtr table_info, const LeaderEpoch& epoch)
-    : MultiStepTableTask(
+    : MultiStepTableTaskBase(
           catalog_manager, async_task_pool, messenger, std::move(table_info), std::move(epoch)) {}
 
 Status PostTabletCreateTaskBase::ValidateRunnable() {
-  RETURN_NOT_OK(MultiStepTableTask::ValidateRunnable());
+  RETURN_NOT_OK(MultiStepTableTaskBase::ValidateRunnable());
 
   SCHECK(
       table_info_->IsPreparing(), IllegalState,
diff --git a/src/yb/master/post_tablet_create_task_base.h b/src/yb/master/post_tablet_create_task_base.h
index 35eb32f8c5..cf9dd046cc 100644
--- a/src/yb/master/post_tablet_create_task_base.h
+++ b/src/yb/master/post_tablet_create_task_base.h
@@ -13,7 +13,7 @@
 
 #pragma once
 
-#include "yb/master/multi_step_monitored_task.h"
+#include "yb/master/catalog_entity_tasks.h"
 
 namespace yb::master {
 
@@ -21,7 +21,7 @@ namespace yb::master {
 // created and marked as RUNNING. Once all tasks of this type complete, the table will be marked as
 // RUNNING, which completes the table creation workflow.
 // - Check MultiStepMonitoredTask for information about scheduling.
-class PostTabletCreateTaskBase : public MultiStepTableTask {
+class PostTabletCreateTaskBase : public MultiStepTableTaskBase {
  public:
   // Start all the tasks at once. Once the last task completes it will transition the table to
   // RUNNING state if healthy.
diff --git a/src/yb/master/sys_catalog-test.cc b/src/yb/master/sys_catalog-test.cc
index 7a26617543..328b09844b 100644
--- a/src/yb/master/sys_catalog-test.cc
+++ b/src/yb/master/sys_catalog-test.cc
@@ -377,7 +377,8 @@ TEST_F(SysCatalogTest, TestSysCatalogNamespacesOperations) {
 
   // 2. CHECK ADD_NAMESPACE
   // Create new namespace.
-  scoped_refptr<NamespaceInfo> ns(new NamespaceInfo("deadbeafdeadbeafdeadbeafdeadbeaf"));
+  scoped_refptr<NamespaceInfo> ns(
+      new NamespaceInfo("deadbeafdeadbeafdeadbeafdeadbeaf", /*tasks_tracker=*/nullptr));
   {
     auto l = ns->LockForWrite();
     l.mutable_data()->pb.set_name("test_ns");
@@ -806,7 +807,8 @@ TEST_F(SysCatalogTest, TestNamespaceNameMigration) {
   ASSERT_EQ(kNumSystemNamespaces, ns_loader->namespaces.size());
 
   auto epoch = LeaderEpoch(kLeaderTerm, sys_catalog_->pitr_count());
-  scoped_refptr<NamespaceInfo> ns(new NamespaceInfo("deadbeafdeadbeafdeadbeafdeadbeaf"));
+  scoped_refptr<NamespaceInfo> ns(
+      new NamespaceInfo("deadbeafdeadbeafdeadbeafdeadbeaf", /*tasks_tracker=*/nullptr));
   {
     auto l = ns->LockForWrite();
     l.mutable_data()->pb.set_name("test_ns");
diff --git a/src/yb/master/sys_catalog-test_base.h b/src/yb/master/sys_catalog-test_base.h
index af90dff471..d92ee91e57 100644
--- a/src/yb/master/sys_catalog-test_base.h
+++ b/src/yb/master/sys_catalog-test_base.h
@@ -243,7 +243,7 @@ class TestNamespaceLoader : public Visitor<PersistentNamespaceInfo> {
 
   Status Visit(const std::string& ns_id, const SysNamespaceEntryPB& metadata) override {
     // Setup the namespace info
-    NamespaceInfo* const ns = new NamespaceInfo(ns_id);
+    NamespaceInfo* const ns = new NamespaceInfo(ns_id, /*tasks_tracker=*/nullptr);
     auto l = ns->LockForWrite();
     l.mutable_data()->pb.CopyFrom(metadata);
     l.Commit();
diff --git a/src/yb/master/xcluster/xcluster_outbound_replication_group-test.cc b/src/yb/master/xcluster/xcluster_outbound_replication_group-test.cc
index 91f2d0afea..5fd8282840 100644
--- a/src/yb/master/xcluster/xcluster_outbound_replication_group-test.cc
+++ b/src/yb/master/xcluster/xcluster_outbound_replication_group-test.cc
@@ -72,7 +72,8 @@ class XClusterOutboundReplicationGroupMocked : public XClusterOutboundReplicatio
  public:
   explicit XClusterOutboundReplicationGroupMocked(
       const xcluster::ReplicationGroupId& replication_group_id, HelperFunctions helper_functions)
-      : XClusterOutboundReplicationGroup(replication_group_id, {}, std::move(helper_functions)) {
+      : XClusterOutboundReplicationGroup(
+            replication_group_id, {}, std::move(helper_functions), /*tasks_tracker=*/nullptr) {
     remote_client_ = std::make_shared<XClusterRemoteClientMocked>();
   }
 
diff --git a/src/yb/master/xcluster/xcluster_outbound_replication_group.cc b/src/yb/master/xcluster/xcluster_outbound_replication_group.cc
index 0e304bf728..18ad70f8ee 100644
--- a/src/yb/master/xcluster/xcluster_outbound_replication_group.cc
+++ b/src/yb/master/xcluster/xcluster_outbound_replication_group.cc
@@ -37,8 +37,9 @@ struct TableSchemaNamePairHash {
 XClusterOutboundReplicationGroup::XClusterOutboundReplicationGroup(
     const xcluster::ReplicationGroupId& replication_group_id,
     const SysXClusterOutboundReplicationGroupEntryPB& outbound_replication_group_pb,
-    HelperFunctions helper_functions)
-    : helper_functions_(std::move(helper_functions)) {
+    HelperFunctions helper_functions, scoped_refptr<TasksTracker> tasks_tracker)
+    : CatalogEntityWithTasks(std::move(tasks_tracker)),
+      helper_functions_(std::move(helper_functions)) {
   outbound_rg_info_ = std::make_unique<XClusterOutboundReplicationGroupInfo>(replication_group_id);
   outbound_rg_info_->Load(outbound_replication_group_pb);
 }
diff --git a/src/yb/master/xcluster/xcluster_outbound_replication_group.h b/src/yb/master/xcluster/xcluster_outbound_replication_group.h
index a3123f75c3..5d9393617d 100644
--- a/src/yb/master/xcluster/xcluster_outbound_replication_group.h
+++ b/src/yb/master/xcluster/xcluster_outbound_replication_group.h
@@ -29,7 +29,8 @@ class XClusterRemoteClient;
 namespace master {
 
 class XClusterOutboundReplicationGroup
-    : public std::enable_shared_from_this<XClusterOutboundReplicationGroup> {
+    : public std::enable_shared_from_this<XClusterOutboundReplicationGroup>,
+      public CatalogEntityWithTasks {
  public:
   struct HelperFunctions {
     const std::function<Result<NamespaceId>(YQLDatabase, const NamespaceName&)>
@@ -54,7 +55,7 @@ class XClusterOutboundReplicationGroup
   explicit XClusterOutboundReplicationGroup(
       const xcluster::ReplicationGroupId& replication_group_id,
       const SysXClusterOutboundReplicationGroupEntryPB& outbound_replication_group_pb,
-      HelperFunctions helper_functions);
+      HelperFunctions helper_functions, scoped_refptr<TasksTracker> tasks_tracker);
 
   virtual ~XClusterOutboundReplicationGroup() = default;
 
diff --git a/src/yb/master/xcluster/xcluster_source_manager.cc b/src/yb/master/xcluster/xcluster_source_manager.cc
index a18f7797ca..7cf509d07b 100644
--- a/src/yb/master/xcluster/xcluster_source_manager.cc
+++ b/src/yb/master/xcluster/xcluster_source_manager.cc
@@ -61,8 +61,12 @@ XClusterSourceManager::XClusterSourceManager(
 XClusterSourceManager::~XClusterSourceManager() {}
 
 void XClusterSourceManager::Clear() {
-  std::lock_guard l(outbound_replication_group_map_mutex_);
-  outbound_replication_group_map_.clear();
+  CatalogEntityWithTasks::CloseAbortAndWaitForAllTasks(GetAllOutboundGroups());
+
+  {
+    std::lock_guard l(outbound_replication_group_map_mutex_);
+    outbound_replication_group_map_.clear();
+  }
 }
 
 Status XClusterSourceManager::RunLoaders() {
@@ -81,22 +85,35 @@ void XClusterSourceManager::DumpState(std::ostream& out, bool on_disk_dump) cons
     return;
   }
 
-  std::lock_guard l(outbound_replication_group_map_mutex_);
-  if (outbound_replication_group_map_.empty()) {
+  auto all_outbound_groups = GetAllOutboundGroups();
+  if (all_outbound_groups.empty()) {
     return;
   }
   out << "XClusterOutboundReplicationGroups:\n";
 
-  for (const auto& [replication_group_id, outbound_rg] : outbound_replication_group_map_) {
+  for (const auto& outbound_rg : all_outbound_groups) {
     auto metadata = outbound_rg->GetMetadata();
     if (metadata.ok()) {
-      out << "  ReplicationGroupId: " << replication_group_id
+      out << "  ReplicationGroupId: " << outbound_rg->Id()
           << "\n  metadata: " << metadata->ShortDebugString() << "\n";
     }
     // else deleted.
   }
 }
 
+std::vector<std::shared_ptr<XClusterOutboundReplicationGroup>>
+XClusterSourceManager::GetAllOutboundGroups() const {
+  std::vector<std::shared_ptr<XClusterOutboundReplicationGroup>> result;
+  SharedLock l(outbound_replication_group_map_mutex_);
+  for (auto& [_, outbound_rg] : outbound_replication_group_map_) {
+    if (outbound_rg) {
+      result.emplace_back(outbound_rg);
+    }
+  }
+
+  return result;
+}
+
 Status XClusterSourceManager::InsertOutboundReplicationGroup(
     const std::string& replication_group_id,
     const SysXClusterOutboundReplicationGroupEntryPB& metadata) {
@@ -158,7 +175,8 @@ XClusterSourceManager::InitOutboundReplicationGroup(
   };
 
   return std::make_shared<XClusterOutboundReplicationGroup>(
-      replication_group_id, metadata, std::move(helper_functions));
+      replication_group_id, metadata, std::move(helper_functions),
+      catalog_manager_.GetTasksTracker());
 }
 
 Result<std::shared_ptr<XClusterOutboundReplicationGroup>>
@@ -256,9 +274,8 @@ XClusterSourceManager::GetPostTabletCreateTasks(
   // tables namespace.
   std::vector<std::shared_ptr<PostTabletCreateTaskBase>> tasks;
   const auto namespace_id = table_info->namespace_id();
-  SharedLock l(outbound_replication_group_map_mutex_);
-  for (const auto& [_, outbound_replication_group] : outbound_replication_group_map_) {
-    if (outbound_replication_group && outbound_replication_group->HasNamespace(namespace_id)) {
+  for (const auto& outbound_replication_group : GetAllOutboundGroups()) {
+    if (outbound_replication_group->HasNamespace(namespace_id)) {
       tasks.emplace_back(std::make_shared<AddTableToXClusterSourceTask>(
           outbound_replication_group, catalog_manager_, *master_.messenger(), table_info, epoch));
     }
@@ -269,9 +286,8 @@ XClusterSourceManager::GetPostTabletCreateTasks(
 
 std::optional<uint32> XClusterSourceManager::GetDefaultWalRetentionSec(
     const NamespaceId& namespace_id) const {
-  SharedLock l(outbound_replication_group_map_mutex_);
-  for (const auto& [_, outbound_replication_group] : outbound_replication_group_map_) {
-    if (outbound_replication_group && outbound_replication_group->HasNamespace(namespace_id)) {
+  for (const auto& outbound_replication_group : GetAllOutboundGroups()) {
+    if (outbound_replication_group->HasNamespace(namespace_id)) {
       return FLAGS_cdc_wal_retention_time_secs;
     }
   }
diff --git a/src/yb/master/xcluster/xcluster_source_manager.h b/src/yb/master/xcluster/xcluster_source_manager.h
index 71c9d8ed45..54c1346840 100644
--- a/src/yb/master/xcluster/xcluster_source_manager.h
+++ b/src/yb/master/xcluster/xcluster_source_manager.h
@@ -88,6 +88,10 @@ class XClusterSourceManager {
       const std::vector<HostPort>& target_master_addresses, const LeaderEpoch& epoch);
 
  private:
+  // Get all the outbound replication groups. Shared pointer are guaranteed not to be null.
+  std::vector<std::shared_ptr<XClusterOutboundReplicationGroup>> GetAllOutboundGroups() const
+      EXCLUDES(outbound_replication_group_map_mutex_);
+
   Status InsertOutboundReplicationGroup(
       const std::string& replication_group_id,
       const SysXClusterOutboundReplicationGroupEntryPB& metadata)
diff --git a/src/yb/util/sync_point.cc b/src/yb/util/sync_point.cc
index 64f898f380..29ab0f1551 100644
--- a/src/yb/util/sync_point.cc
+++ b/src/yb/util/sync_point.cc
@@ -70,6 +70,7 @@ void SyncPoint::ClearAllCallBacks() {
 }
 
 void SyncPoint::EnableProcessing() {
+  DCHECK(FLAGS_TEST_enable_sync_points);
   std::unique_lock lock(mutex_);
   enabled_ = true;
 }
