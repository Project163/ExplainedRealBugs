diff --git a/src/yb/integration-tests/xcluster/xcluster-test.cc b/src/yb/integration-tests/xcluster/xcluster-test.cc
index e3fcbeb8bc..dd00f3c1e6 100644
--- a/src/yb/integration-tests/xcluster/xcluster-test.cc
+++ b/src/yb/integration-tests/xcluster/xcluster-test.cc
@@ -148,6 +148,7 @@ DECLARE_double(TEST_xcluster_simulate_random_failure_after_apply);
 DECLARE_uint32(cdcsdk_retention_barrier_no_revision_interval_secs);
 DECLARE_int32(heartbeat_interval_ms);
 DECLARE_bool(TEST_xcluster_fail_setup_stream_update);
+DECLARE_bool(xcluster_skip_health_check_on_replication_setup);
 
 namespace yb {
 
@@ -2705,6 +2706,9 @@ TEST_P(XClusterTest, TestAlterUniverseRemoveTableAndDrop) {
 TEST_P(XClusterTest, TestNonZeroLagMetricsWithoutGetChange) {
   ASSERT_OK(SetUpWithParams({1}, 1));
 
+  // Disable health check on setup since we are stopping all consumer tservers.
+  ANNOTATE_UNPROTECTED_WRITE(FLAGS_xcluster_skip_health_check_on_replication_setup) = true;
+
   // Stop the consumer tserver before setting up replication.
   consumer_cluster()->mini_tablet_server(0)->Shutdown();
 
@@ -3482,6 +3486,8 @@ TEST_F_EX(XClusterTest, ListMetaCacheAfterXClusterSetup, XClusterTestNoParam) {
 TEST_F_EX(XClusterTest, PollerShutdownWithLongPollDelay, XClusterTestNoParam) {
   // Make Pollers enter a long sleep state.
   ANNOTATE_UNPROTECTED_WRITE(FLAGS_TEST_cdc_skip_replication_poll) = true;
+  ANNOTATE_UNPROTECTED_WRITE(FLAGS_xcluster_skip_health_check_on_replication_setup) = true;
+
   ASSERT_OK(SET_FLAG(async_replication_idle_delay_ms, 10 * 60 * 1000));  // 10 minutes
   const auto kTimeout = 10s * kTimeMultiplier;
 
@@ -3856,9 +3862,24 @@ TEST_F_EX(XClusterTest, TestStats, XClusterTestNoParam) {
   ASSERT_EQ(source_stats[0].sent_index, target_stats[0].received_index);
 }
 
+// If the Pollers are not able to successfully poll then Setup should fail.
+TEST_F_EX(XClusterTest, FailedSetupOnReplicationError, XClusterTestNoParam) {
+  // Disable polling so that errors don't get cleared by successful polls.
+  ANNOTATE_UNPROTECTED_WRITE(FLAGS_TEST_cdc_skip_replication_poll) = true;
+  ASSERT_OK(SetUpWithParams(
+      {1}, {1}, /* replication_factor */ 1, /* num_masters */ 1, /* num_tservers */ 1));
+
+  ASSERT_NOK(SetupReplication());
+  ASSERT_OK(CorrectlyPollingAllTablets(1));
+  const auto stream_id = ASSERT_RESULT(GetCDCStreamID(producer_table_->id()));
+  ASSERT_OK(VerifyReplicationError(
+      consumer_table_->id(), stream_id, ReplicationErrorPb::REPLICATION_ERROR_UNINITIALIZED));
+}
+
 TEST_F_EX(XClusterTest, VerifyReplicationError, XClusterTestNoParam) {
   // Disable polling so that errors don't get cleared by successful polls.
   ANNOTATE_UNPROTECTED_WRITE(FLAGS_TEST_cdc_skip_replication_poll) = true;
+  ANNOTATE_UNPROTECTED_WRITE(FLAGS_xcluster_skip_health_check_on_replication_setup) = true;
   ASSERT_OK(SetUpWithParams(
       {1}, {1}, /* replication_factor */ 1, /* num_masters */ 3, /* num_tservers */ 1));
   ASSERT_OK(SetupReplication());
diff --git a/src/yb/integration-tests/xcluster/xcluster_consistency-test.cc b/src/yb/integration-tests/xcluster/xcluster_consistency-test.cc
index f6039c51c8..036b487bf6 100644
--- a/src/yb/integration-tests/xcluster/xcluster_consistency-test.cc
+++ b/src/yb/integration-tests/xcluster/xcluster_consistency-test.cc
@@ -35,6 +35,7 @@ DECLARE_string(TEST_xcluster_simulated_lag_tablet_filter);
 DECLARE_string(ysql_yb_xcluster_consistency_level);
 DECLARE_uint32(xcluster_safe_time_log_outliers_interval_secs);
 DECLARE_uint32(xcluster_safe_time_slow_tablet_delta_secs);
+DECLARE_bool(xcluster_skip_health_check_on_replication_setup);
 
 using namespace std::chrono_literals;
 
@@ -329,6 +330,8 @@ class XClusterConsistencyNoSafeTimeTest : public XClusterConsistencyTest {
  public:
   void SetUp() override {
     ASSERT_OK(SET_FLAG(TEST_xcluster_simulated_lag_ms, -1));
+    ANNOTATE_UNPROTECTED_WRITE(FLAGS_xcluster_skip_health_check_on_replication_setup) = true;
+
     XClusterConsistencyTest::SetUp();
   }
 
diff --git a/src/yb/master/catalog_manager.h b/src/yb/master/catalog_manager.h
index 27ebf885a6..5a25d1e884 100644
--- a/src/yb/master/catalog_manager.h
+++ b/src/yb/master/catalog_manager.h
@@ -1630,6 +1630,9 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
 
   std::shared_ptr<YsqlTablespaceManager> GetTablespaceManager() const;
 
+  Result<bool> HasReplicationGroupErrors(
+      const xcluster::ReplicationGroupId& replication_group_id) const;
+
  protected:
   // TODO Get rid of these friend classes and introduce formal interface.
   friend class TableLoader;
diff --git a/src/yb/master/xcluster/xcluster_replication_group.cc b/src/yb/master/xcluster/xcluster_replication_group.cc
index ee71d14a0d..62d750626c 100644
--- a/src/yb/master/xcluster/xcluster_replication_group.cc
+++ b/src/yb/master/xcluster/xcluster_replication_group.cc
@@ -32,6 +32,9 @@
 DECLARE_int32(cdc_read_rpc_timeout_ms);
 DECLARE_string(certs_for_cdc_dir);
 
+DEFINE_RUNTIME_bool(xcluster_skip_health_check_on_replication_setup, false,
+    "Skip health check on xCluster replication setup");
+
 namespace yb::master {
 
 namespace {
@@ -148,6 +151,40 @@ Status ValidateAutoFlagsInternal(
 
   return Status::OK();
 }
+
+Result<bool> IsSafeTimeReady(
+    const SysUniverseReplicationEntryPB& universe_pb, const XClusterManagerIf& xcluster_manager) {
+  if (!IsDbScoped(universe_pb)) {
+    // Only valid in Db scoped replication.
+    return true;
+  }
+
+  const auto safe_time_map = VERIFY_RESULT(xcluster_manager.GetXClusterNamespaceToSafeTimeMap());
+  for (const auto& namespace_info : universe_pb.db_scoped_info().namespace_infos()) {
+    const auto& namespace_id = namespace_info.consumer_namespace_id();
+    auto* it = FindOrNull(safe_time_map, namespace_id);
+    if (!it || it->is_special()) {
+      VLOG_WITH_FUNC(1) << "Safe time for namespace " << namespace_id
+                        << " is not yet ready: " << (it ? it->ToString() : "NA");
+      return false;
+    }
+  }
+
+  return true;
+}
+
+Result<bool> IsReplicationGroupReady(
+    const UniverseReplicationInfo& universe, CatalogManager& catalog_manager) {
+  // The replication group must be in a healthy state.
+  if (!FLAGS_xcluster_skip_health_check_on_replication_setup &&
+      VERIFY_RESULT(catalog_manager.HasReplicationGroupErrors(universe.ReplicationGroupId()))) {
+    return false;
+  }
+
+  auto l = universe.LockForRead();
+  return IsSafeTimeReady(l->pb, *catalog_manager.GetXClusterManager());
+}
+
 }  // namespace
 
 Result<uint32> GetAutoFlagConfigVersionIfCompatible(
@@ -373,27 +410,6 @@ Result<std::shared_ptr<client::XClusterRemoteClient>> GetXClusterRemoteClient(
   return xcluster_client;
 }
 
-Result<bool> IsSafeTimeReady(
-    const SysUniverseReplicationEntryPB& universe_pb, const XClusterManagerIf& xcluster_manager) {
-  if (!IsDbScoped(universe_pb)) {
-    // Only valid in Db scoped replication.
-    return true;
-  }
-
-  const auto safe_time_map = VERIFY_RESULT(xcluster_manager.GetXClusterNamespaceToSafeTimeMap());
-  for (const auto& namespace_info : universe_pb.db_scoped_info().namespace_infos()) {
-    const auto& namespace_id = namespace_info.consumer_namespace_id();
-    auto* it = FindOrNull(safe_time_map, namespace_id);
-    if (!it || it->is_special()) {
-      VLOG_WITH_FUNC(1) << "Safe time for namespace " << namespace_id
-                        << " is not yet ready: " << (it ? it->ToString() : "NA");
-      return false;
-    }
-  }
-
-  return true;
-}
-
 Result<IsOperationDoneResult> IsSetupUniverseReplicationDone(
     const xcluster::ReplicationGroupId& replication_group_id, CatalogManager& catalog_manager) {
   // Cases for completion:
@@ -437,8 +453,7 @@ Result<IsOperationDoneResult> IsSetupUniverseReplicationDone(
 
   bool is_done = false;
   if (!is_alter_request && state == SysUniverseReplicationEntryPB::ACTIVE) {
-    auto l = universe->LockForRead();
-    is_done = VERIFY_RESULT(IsSafeTimeReady(l->pb, *catalog_manager.GetXClusterManager()));
+    is_done = VERIFY_RESULT(IsReplicationGroupReady(*universe, catalog_manager));
   }
 
   return is_done ? IsOperationDoneResult::Done() : IsOperationDoneResult::NotDone();
diff --git a/src/yb/master/xrepl_catalog_manager.cc b/src/yb/master/xrepl_catalog_manager.cc
index 6378f42684..5f33d195d6 100644
--- a/src/yb/master/xrepl_catalog_manager.cc
+++ b/src/yb/master/xrepl_catalog_manager.cc
@@ -5832,9 +5832,13 @@ void CatalogManager::SyncXClusterConsumerReplicationStatusMap(
     return;
   }
 
+  auto& replication_group_errors = xcluster_consumer_replication_error_map_[replication_group_id];
   auto& producer_entry = producer_map.at(replication_group_id.ToString());
 
+  std::unordered_set<TableId> all_consumer_table_ids;
   for (auto& [_, stream_map] : producer_entry.stream_map()) {
+    const auto& consumer_table_id = stream_map.consumer_table_id();
+
     std::unordered_set<TabletId> all_producer_tablet_ids;
     for (auto& [_, producer_tablet_ids] : stream_map.consumer_producer_tablet_map()) {
       all_producer_tablet_ids.insert(
@@ -5842,30 +5846,41 @@ void CatalogManager::SyncXClusterConsumerReplicationStatusMap(
     }
 
     if (all_producer_tablet_ids.empty()) {
-      if (xcluster_consumer_replication_error_map_.contains(replication_group_id)) {
-        xcluster_consumer_replication_error_map_.at(replication_group_id)
-            .erase(stream_map.consumer_table_id());
-      }
       continue;
     }
+    all_consumer_table_ids.insert(consumer_table_id);
 
-    auto& consumer_error_map =
-        xcluster_consumer_replication_error_map_[replication_group_id]
-                                                [stream_map.consumer_table_id()];
-    // Remove entries that are no longer part of replication.
-    std::erase_if(consumer_error_map, [&all_producer_tablet_ids](const auto& entry) {
+    auto& tablet_error_map = replication_group_errors[consumer_table_id];
+    // Remove tablets that are no longer part of replication.
+    std::erase_if(tablet_error_map, [&all_producer_tablet_ids](const auto& entry) {
       return !all_producer_tablet_ids.contains(entry.first);
     });
 
-    // Add new entries.
+    // Add new tablets.
     for (const auto& producer_tablet_id : all_producer_tablet_ids) {
-      if (!consumer_error_map.contains(producer_tablet_id)) {
+      if (!tablet_error_map.contains(producer_tablet_id)) {
         // Default to UNINITIALIZED error. Once the Pollers send the status, this will be updated.
-        consumer_error_map[producer_tablet_id].error =
+        tablet_error_map[producer_tablet_id].error =
             ReplicationErrorPb::REPLICATION_ERROR_UNINITIALIZED;
       }
     }
   }
+
+  // Remove tables that are no longer part of replication.
+  std::erase_if(replication_group_errors, [&all_consumer_table_ids](const auto& entry) {
+    return !all_consumer_table_ids.contains(entry.first);
+  });
+
+  if (VLOG_IS_ON(1)) {
+    LOG(INFO) << "Synced xcluster_consumer_replication_error_map_ for replication group "
+              << replication_group_id;
+    for (const auto& [consumer_table_id, tablet_error_map] : replication_group_errors) {
+      LOG(INFO) << "Table: " << consumer_table_id;
+      for (const auto& [tablet_id, error] : tablet_error_map) {
+        LOG(INFO) << "Tablet: " << tablet_id << ", Error: " << ReplicationErrorPb_Name(error.error);
+      }
+    }
+  }
 }
 
 void CatalogManager::StoreXClusterConsumerReplicationStatus(
@@ -6099,6 +6114,32 @@ Status CatalogManager::TEST_CDCSDKFailCreateStreamRequestIfNeeded(const std::str
   }
   return Status::OK();
 }
+Result<bool> CatalogManager::HasReplicationGroupErrors(
+    const xcluster::ReplicationGroupId& replication_group_id) const {
+  yb::SharedLock l(xcluster_consumer_replication_error_map_mutex_);
+
+  auto* replication_error_map =
+      FindOrNull(xcluster_consumer_replication_error_map_, replication_group_id);
+  SCHECK(
+      replication_error_map, NotFound, "Could not find replication group $0",
+      replication_group_id.ToString());
+
+  std::unordered_map<TableId, std::unordered_set<std::string>> table_errors;
+  for (const auto& [consumer_table_id, tablet_error_map] : *replication_error_map) {
+    for (const auto& [_, error_info] : tablet_error_map) {
+      if (error_info.error != ReplicationErrorPb::REPLICATION_OK) {
+        table_errors[consumer_table_id].insert(ReplicationErrorPb_Name(error_info.error));
+      }
+    }
+  }
+
+  if (!table_errors.empty()) {
+    YB_LOG_EVERY_N_SECS(WARNING, 60)
+        << "Replication group " << replication_group_id
+        << " has errors for the following tables:" << AsString(table_errors);
+  }
+  return !table_errors.empty();
+}
 
 Status CatalogManager::PopulateReplicationGroupErrors(
     const xcluster::ReplicationGroupId& replication_group_id,
@@ -6137,7 +6178,7 @@ Status CatalogManager::PopulateReplicationGroupErrors(
     resp_status->set_stream_id(stream_id.ToString());
     for (const auto& [error_pb, tablet_ids] : errors) {
       if (error_pb == ReplicationErrorPb::REPLICATION_OK) {
-        // Do not report healthy tablets.
+        // Do not add errors for healthy tablets.
         continue;
       }
 
