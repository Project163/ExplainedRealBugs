diff --git a/src/yb/integration-tests/xcluster/xcluster_outbound_replication_group-itest.cc b/src/yb/integration-tests/xcluster/xcluster_outbound_replication_group-itest.cc
index 61711d97f9..83d957197a 100644
--- a/src/yb/integration-tests/xcluster/xcluster_outbound_replication_group-itest.cc
+++ b/src/yb/integration-tests/xcluster/xcluster_outbound_replication_group-itest.cc
@@ -16,8 +16,10 @@
 #include "yb/master/catalog_manager.h"
 #include "yb/master/master_ddl.pb.h"
 #include "yb/master/mini_master.h"
+#include "yb/tablet/tablet_peer.h"
 
 DECLARE_bool(TEST_enable_xcluster_api_v2);
+DECLARE_uint32(cdc_wal_retention_time_secs);
 
 namespace yb {
 namespace master {
@@ -115,6 +117,21 @@ class XClusterOutboundReplicationGroupTest : public XClusterYsqlTestBase {
     return promise.get_future().get();
   }
 
+  Status VerifyWalRetentionOfTable(
+      const TableId& table_id, uint32 wal_retention_secs = FLAGS_cdc_wal_retention_time_secs) {
+    auto tablets = ListTableActiveTabletLeadersPeers(producer_cluster(), table_id);
+    SCHECK_GE(
+        tablets.size(), static_cast<size_t>(1), IllegalState,
+        Format("No active tablets found for table $0", table_id));
+    for (const auto& tablet : tablets) {
+      SCHECK_EQ(
+          tablet->tablet_metadata()->wal_retention_secs(), wal_retention_secs, IllegalState,
+          Format("Tablet: $0", tablet->tablet_metadata()->LogPrefix()));
+    }
+
+    return Status::OK();
+  }
+
   CatalogManager* catalog_manager_;
   LeaderEpoch epoch_;
   YBClient* client_;
@@ -162,6 +179,9 @@ TEST_F(XClusterOutboundReplicationGroupTest, TestMultipleTable) {
   ASSERT_EQ(resp.table_infos(0).table_name(), kTableName2);
   ASSERT_EQ(resp.table_infos(1).table_name(), kTableName1);
 
+  ASSERT_OK(VerifyWalRetentionOfTable(table_id_1));
+  ASSERT_OK(VerifyWalRetentionOfTable(table_id_2));
+
   ASSERT_OK(client_->XClusterDeleteOutboundReplicationGroup(kReplicationGroupId));
   ASSERT_NOK(GetXClusterStreams(kReplicationGroupId, namespace_id_));
 
@@ -238,21 +258,26 @@ TEST_F(XClusterOutboundReplicationGroupTest, AddDeleteNamespaces) {
 }
 
 TEST_F(XClusterOutboundReplicationGroupTest, AddTable) {
-  auto ns1_table_id_1 = ASSERT_RESULT(CreateYsqlTable(kNamespaceName, kTableName1));
+  auto table_id_1 = ASSERT_RESULT(CreateYsqlTable(kNamespaceName, kTableName1));
+  ASSERT_OK(VerifyWalRetentionOfTable(table_id_1, 0));
 
   ASSERT_OK(client_->XClusterCreateOutboundReplicationGroup(kReplicationGroupId, {kNamespaceName}));
 
   auto all_xcluster_streams_initial = CleanupAndGetAllXClusterStreams();
   ASSERT_EQ(all_xcluster_streams_initial.size(), 1);
 
-  auto ns1_table_id_2 = ASSERT_RESULT(CreateYsqlTable(kNamespaceName, kTableName2));
+  ASSERT_OK(VerifyWalRetentionOfTable(table_id_1));
+
+  auto table_id_2 = ASSERT_RESULT(CreateYsqlTable(kNamespaceName, kTableName2));
 
   all_xcluster_streams_initial = CleanupAndGetAllXClusterStreams();
   ASSERT_EQ(all_xcluster_streams_initial.size(), 2);
 
   auto ns1_info = ASSERT_RESULT(GetXClusterStreams(kReplicationGroupId, namespace_id_));
   ASSERT_NO_FATALS(VerifyNamespaceCheckpointInfo(
-      ns1_table_id_1, ns1_table_id_2, all_xcluster_streams_initial, ns1_info));
+      table_id_1, table_id_2, all_xcluster_streams_initial, ns1_info));
+
+  ASSERT_OK(VerifyWalRetentionOfTable(table_id_2));
 }
 
 }  // namespace master
diff --git a/src/yb/master/async_rpc_tasks.cc b/src/yb/master/async_rpc_tasks.cc
index 899f3047ed..5297a4065c 100644
--- a/src/yb/master/async_rpc_tasks.cc
+++ b/src/yb/master/async_rpc_tasks.cc
@@ -720,23 +720,27 @@ AsyncCreateReplica::AsyncCreateReplica(Master *master,
   deadline_.AddDelta(MonoDelta::FromMilliseconds(FLAGS_tablet_creation_timeout_ms));
 
   auto table_lock = tablet->table()->LockForRead();
-  const SysTabletsEntryPB& tablet_pb = tablet->metadata().dirty().pb;
+  const auto& table_pb = table_lock->pb;
+  const auto& tablet_pb = tablet->metadata().dirty().pb;
 
   req_.set_dest_uuid(permanent_uuid);
   req_.set_table_id(tablet->table()->id());
   req_.set_tablet_id(tablet->tablet_id());
   req_.set_table_type(tablet->table()->metadata().state().pb.table_type());
   req_.mutable_partition()->CopyFrom(tablet_pb.partition());
-  req_.set_namespace_id(table_lock->pb.namespace_id());
-  req_.set_namespace_name(table_lock->pb.namespace_name());
-  req_.set_pg_table_id(table_lock->pb.pg_table_id());
-  req_.set_table_name(table_lock->pb.name());
-  req_.mutable_schema()->CopyFrom(table_lock->pb.schema());
-  req_.mutable_partition_schema()->CopyFrom(table_lock->pb.partition_schema());
+  req_.set_namespace_id(table_pb.namespace_id());
+  req_.set_namespace_name(table_pb.namespace_name());
+  req_.set_pg_table_id(table_pb.pg_table_id());
+  req_.set_table_name(table_pb.name());
+  req_.mutable_schema()->CopyFrom(table_pb.schema());
+  req_.mutable_partition_schema()->CopyFrom(table_pb.partition_schema());
   req_.mutable_config()->CopyFrom(tablet_pb.committed_consensus_state().config());
   req_.set_colocated(tablet_pb.colocated());
-  if (table_lock->pb.has_index_info()) {
-    req_.mutable_index_info()->CopyFrom(table_lock->pb.index_info());
+  if (table_pb.has_index_info()) {
+    req_.mutable_index_info()->CopyFrom(table_pb.index_info());
+  }
+  if (table_pb.has_wal_retention_secs()) {
+    req_.set_wal_retention_secs(table_pb.wal_retention_secs());
   }
   auto& req_schedules = *req_.mutable_snapshot_schedules();
   req_schedules.Reserve(narrow_cast<int>(snapshot_schedules.size()));
@@ -744,7 +748,7 @@ AsyncCreateReplica::AsyncCreateReplica(Master *master,
     req_schedules.Add()->assign(id.AsSlice().cdata(), id.size());
   }
 
-  req_.mutable_hosted_stateful_services()->CopyFrom(table_lock->pb.hosted_stateful_services());
+  req_.mutable_hosted_stateful_services()->CopyFrom(table_pb.hosted_stateful_services());
 }
 
 std::string AsyncCreateReplica::description() const {
diff --git a/src/yb/master/catalog_manager.cc b/src/yb/master/catalog_manager.cc
index 330556237d..4571d6d327 100644
--- a/src/yb/master/catalog_manager.cc
+++ b/src/yb/master/catalog_manager.cc
@@ -4248,10 +4248,14 @@ Status CatalogManager::CreateTable(const CreateTableRequestPB* orig_req,
   TRACE("Inserted new table and tablet info into CatalogManager maps");
   VLOG_WITH_PREFIX(1) << "Inserted new table and tablet info into CatalogManager maps";
 
-  // Write tablets to sys-tablets.
-  // If new tablets are created, they will be in PREPARING state.
   if (!joining_colocation_group) {
+    auto opt_wal_retention = xcluster_manager_->GetDefaultWalRetentionSec(namespace_id);
+    if (opt_wal_retention) {
+      table->mutable_metadata()->mutable_dirty()->pb.set_wal_retention_secs(*opt_wal_retention);
+    }
+
     for (const auto& tablet : tablets) {
+      // If new tablets are created, they will be in PREPARING state.
       CHECK_EQ(SysTabletsEntryPB::PREPARING, tablet->metadata().dirty().pb.state());
     }
   }
diff --git a/src/yb/master/xcluster/xcluster_source_manager.cc b/src/yb/master/xcluster/xcluster_source_manager.cc
index 7f0efc5dad..a18f7797ca 100644
--- a/src/yb/master/xcluster/xcluster_source_manager.cc
+++ b/src/yb/master/xcluster/xcluster_source_manager.cc
@@ -22,6 +22,8 @@
 #include "yb/rpc/rpc_context.h"
 #include "yb/util/scope_exit.h"
 
+DECLARE_uint32(cdc_wal_retention_time_secs);
+
 using namespace std::placeholders;
 
 namespace yb::master {
@@ -191,7 +193,6 @@ Result<std::vector<xrepl::StreamId>> XClusterSourceManager::BootstrapTables(
       master::CreateCDCStreamResponsePB create_stream_resp;
       create_stream_req.set_table_id(table_info->id());
 
-      // TODO: #20769 Apply appropriate WAL retention on the table.
       RETURN_NOT_OK(catalog_manager_.CreateNewXReplStream(
           create_stream_req, CreateNewCDCStreamMode::kXClusterTableIds, {table_id},
           /*namespace_id=*/std::nullopt, &create_stream_resp, epoch, /*rpc=*/nullptr));
@@ -266,6 +267,18 @@ XClusterSourceManager::GetPostTabletCreateTasks(
   return tasks;
 }
 
+std::optional<uint32> XClusterSourceManager::GetDefaultWalRetentionSec(
+    const NamespaceId& namespace_id) const {
+  SharedLock l(outbound_replication_group_map_mutex_);
+  for (const auto& [_, outbound_replication_group] : outbound_replication_group_map_) {
+    if (outbound_replication_group && outbound_replication_group->HasNamespace(namespace_id)) {
+      return FLAGS_cdc_wal_retention_time_secs;
+    }
+  }
+
+  return std::nullopt;
+}
+
 Result<std::vector<NamespaceId>> XClusterSourceManager::CreateOutboundReplicationGroup(
     const xcluster::ReplicationGroupId& replication_group_id,
     const std::vector<NamespaceName>& namespace_names, const LeaderEpoch& epoch,
diff --git a/src/yb/master/xcluster/xcluster_source_manager.h b/src/yb/master/xcluster/xcluster_source_manager.h
index 2bddf21f68..71c9d8ed45 100644
--- a/src/yb/master/xcluster/xcluster_source_manager.h
+++ b/src/yb/master/xcluster/xcluster_source_manager.h
@@ -36,6 +36,8 @@ class SysCatalogTable;
 
 class XClusterSourceManager {
  public:
+  std::optional<uint32> GetDefaultWalRetentionSec(const NamespaceId& namespace_id) const;
+
  protected:
   explicit XClusterSourceManager(
       Master& master, CatalogManager& catalog_manager, SysCatalogTable& sys_catalog);
diff --git a/src/yb/tserver/tablet_service.cc b/src/yb/tserver/tablet_service.cc
index 9a7ecf3083..fcff9e844f 100644
--- a/src/yb/tserver/tablet_service.cc
+++ b/src/yb/tserver/tablet_service.cc
@@ -1584,6 +1584,11 @@ Status TabletServiceAdminImpl::DoCreateTablet(const CreateTabletRequestPB* req,
       req->table_type(), schema, qlexpr::IndexMap(),
       req->has_index_info() ? boost::optional<qlexpr::IndexInfo>(req->index_info()) : boost::none,
       0 /* schema_version */, partition_schema, req->pg_table_id());
+
+  if (req->has_wal_retention_secs()) {
+    table_info->wal_retention_secs = req->wal_retention_secs();
+  }
+
   std::vector<SnapshotScheduleId> snapshot_schedules;
   snapshot_schedules.reserve(req->snapshot_schedules().size());
   for (const auto& id : req->snapshot_schedules()) {
diff --git a/src/yb/tserver/tserver_admin.proto b/src/yb/tserver/tserver_admin.proto
index 2db8dba254..87cbc5b8e0 100644
--- a/src/yb/tserver/tserver_admin.proto
+++ b/src/yb/tserver/tserver_admin.proto
@@ -160,6 +160,8 @@ message CreateTabletRequestPB {
   // In case the table was rewritten, explicitly store the TableId containing the PG table OID
   // (as the table's TableId no longer matches).
   optional bytes pg_table_id = 18;
+
+  optional uint32 wal_retention_secs = 19;
 }
 
 message CreateTabletResponsePB {
