diff --git a/src/yb/integration-tests/xcluster/xcluster_ysql_index-test.cc b/src/yb/integration-tests/xcluster/xcluster_ysql_index-test.cc
index 812ca8abd4..cb89e174f0 100644
--- a/src/yb/integration-tests/xcluster/xcluster_ysql_index-test.cc
+++ b/src/yb/integration-tests/xcluster/xcluster_ysql_index-test.cc
@@ -48,7 +48,12 @@ class XClusterYsqlIndexTest : public XClusterYsqlTestBase {
   void SetUp() override {
     YB_SKIP_TEST_IN_TSAN();
     XClusterYsqlTestBase::SetUp();
-    ASSERT_OK(SET_FLAG(vmodule, "backfill_index*=4,xrepl*=4,xcluster*=4,add_table*=4,catalog*=4"));
+    google::SetVLOGLevel("backfill_index*", 4);
+    google::SetVLOGLevel("xrepl*", 4);
+    google::SetVLOGLevel("xcluster*", 4);
+    google::SetVLOGLevel("add_table*", 4);
+    google::SetVLOGLevel("catalog*", 4);
+    google::SetVLOGLevel("table_creation_task*", 4);
 
     ANNOTATE_UNPROTECTED_WRITE(FLAGS_TEST_user_ddl_operation_timeout_sec) = NonTsanVsTsan(60, 90);
 
@@ -307,11 +312,11 @@ TEST_F(XClusterYsqlIndexTest, MasterFailoverRetryAddTableToXcluster) {
   ASSERT_OK(CreateIndex(*producer_conn_));
 
   SyncPoint::GetInstance()->LoadDependency(
-      {{"AddTableToXClusterTask::RunInternal::BeforeBootstrap",
+      {{"AddTableToXClusterTargetTask::RunInternal::BeforeBootstrap",
         "MasterFailoverRetryAddTableToXcluster::BeforeStepDown"}});
 
   SyncPoint::GetInstance()->SetCallBack(
-      "AddTableToXClusterTask::RunInternal::BeforeBootstrap",
+      "AddTableToXClusterTargetTask::RunInternal::BeforeBootstrap",
       [](void* stuck_add_table_to_xcluster) {
         *(reinterpret_cast<bool*>(stuck_add_table_to_xcluster)) = true;
       });
diff --git a/src/yb/master/CMakeLists.txt b/src/yb/master/CMakeLists.txt
index d099bd26d0..24573b7945 100644
--- a/src/yb/master/CMakeLists.txt
+++ b/src/yb/master/CMakeLists.txt
@@ -108,6 +108,8 @@ set(MASTER_SRCS
   master-path-handlers.cc
   mini_master.cc
   master_snapshot_coordinator.cc
+  multi_step_monitored_task.cc
+  post_tablet_create_task_base.cc
   restoration_state.cc
   restore_sys_catalog_state.cc
   snapshot_coordinator_context.cc
@@ -128,7 +130,7 @@ set(MASTER_SRCS
   ts_manager.cc
   universe_key_registry_service.cc
   util/yql_vtable_helpers.cc
-  xcluster/add_table_to_xcluster_task.cc
+  xcluster/add_table_to_xcluster_target_task.cc
   xcluster/xcluster_catalog_entity.cc
   xcluster/xcluster_config.cc
   xcluster/xcluster_consumer_metrics.cc
diff --git a/src/yb/master/catalog_manager.cc b/src/yb/master/catalog_manager.cc
index 644b540d3c..bd454f271a 100644
--- a/src/yb/master/catalog_manager.cc
+++ b/src/yb/master/catalog_manager.cc
@@ -138,11 +138,11 @@
 #include "yb/master/master_replication.pb.h"
 #include "yb/master/master_util.h"
 #include "yb/master/permissions_manager.h"
+#include "yb/master/post_tablet_create_task_base.h"
 #include "yb/master/scoped_leader_shared_lock-internal.h"
 #include "yb/master/sys_catalog.h"
 #include "yb/master/sys_catalog_constants.h"
 #include "yb/master/ts_descriptor.h"
-#include "yb/master/xcluster/add_table_to_xcluster_task.h"
 #include "yb/master/xcluster/xcluster_manager.h"
 #include "yb/master/xcluster/xcluster_safe_time_service.h"
 #include "yb/master/yql_aggregates_vtable.h"
@@ -8307,7 +8307,6 @@ Status CatalogManager::ProcessTabletReportBatch(
   // appear in 'full_report', but that has no bearing on correctness.
   vector<TabletInfo*> mutated_tablets; // refcount protected by ReportedTablet::info
   std::unordered_map<TableId, std::set<TabletId>> new_running_tablets;
-  vector<TableInfo*> mutated_tables;  // refcount protected by 'table_info_map'
   for (auto it = begin; it != end; ++it) {
     const auto& tablet_id = it->tablet_id;
     const TabletInfoPtr& tablet = it->info;
@@ -8439,29 +8438,27 @@ Status CatalogManager::ProcessTabletReportBatch(
     }
   } // Finished one round of batch processing.
 
-  // Update the table state if all its tablets are now running.
-  for (auto& [table_id, tablets] : new_running_tablets) {
-    auto& table_info = table_info_map[table_id];
-    if (VERIFY_RESULT(HandleNewRunningTabletsForTable(table_info, tablets, epoch))) {
-      mutated_tables.push_back(table_info.get());
-    }
-  }
-
   // Write all tablet mutations to the catalog table.
   //
   // SysCatalogTable::Write will short-circuit the case where the data has not
   // in fact changed since the previous version and avoid any unnecessary mutations.
-  if (!mutated_tablets.empty() || !mutated_tables.empty()) {
-    Status s = sys_catalog_->Upsert(epoch, mutated_tablets, mutated_tables);
+  if (!mutated_tablets.empty()) {
+    Status s = sys_catalog_->Upsert(epoch, mutated_tablets);
     if (!s.ok()) {
       LOG(WARNING) << "Error updating tablets: " << s;
       return s;
     }
   }
+
+  // Update the table state if all its tablets are now running.
+  for (auto& [table_id, tablets] : new_running_tablets) {
+    SchedulePostTabletCreationTasks(table_info_map[table_id], epoch, tablets);
+  }
+
   // Filter the mutated tablets to find which tablets were modified. Need to actually commit the
   // state of the tablets before updating the system.partitions table, so get this first.
   vector<TabletInfoPtr> yql_partitions_mutated_tablets =
-      VERIFY_RESULT(GetYqlPartitionsVtable().FilterRelevantTablets(mutated_tablets));
+      GetYqlPartitionsVtable().FilterRelevantTablets(mutated_tablets);
 
   // Publish the in-memory tablet mutations and release the locks.
   for (auto& l : tablet_write_locks) {
@@ -13518,7 +13515,7 @@ void CatalogManager::SysCatalogLoaded(SysCatalogLoadingState&& state) {
   snapshot_coordinator_.SysCatalogLoaded(state.epoch.leader_term);
 
   xcluster_manager_->SysCatalogLoaded();
-  ScheduleAddTableToXClusterTaskForAllTables(state.epoch);
+  SchedulePostTabletCreationTasksForPendingTables(state.epoch);
 }
 
 Status CatalogManager::UpdateLastFullCompactionRequestTime(
@@ -13656,46 +13653,30 @@ void CatalogManager::WriteTabletToSysCatalog(const TabletId& tablet_id) {
       "Failed to upsert migrated colocated tablet into sys catalog.");
 }
 
-bool CatalogManager::ScheduleAddTableToXClusterTaskIfNeeded(
-    TableInfoPtr table_info, const SysTablesEntryPB& pb, const LeaderEpoch& epoch) {
-  DCHECK(table_info->mutable_metadata()->is_dirty());
-  if (table_info->HasTasks(server::MonitoredTaskType::kAddTableToXClusterReplication)) {
-    // Task already scheduled.
-    return true;
-  }
-
-  if (!ShouldAddTableToXClusterReplication(*table_info, pb)) {
-    return false;
-  }
-
-  VLOG(1) << "Scheduling AddTableToXClusterTask for " << table_info->id();
-  auto call = std::make_shared<AddTableToXClusterTask>(this, table_info, epoch);
-  table_info->AddTask(call);
-  // Task will be removed from table_info if the scheduling fails.
-  auto s = ScheduleTask(call);
-  if (!s.ok()) {
-    table_info->SetCreateTableErrorStatus(s);
-    LOG(WARNING) << "Failed to start AddTableToXClusterTask: " << s;
-  }
-
-  return true;
-}
+void CatalogManager::SchedulePostTabletCreationTasks(
+    const TableInfoPtr& table_info, const LeaderEpoch& epoch,
+    const std::set<TabletId>& new_running_tablets) {
+  auto& mutable_table_info = *table_info->mutable_metadata();
+  DCHECK(mutable_table_info.HasWriteLock());
 
-Result<bool> CatalogManager::HandleNewRunningTabletsForTable(
-    TableInfoPtr table_info, const std::set<TabletId>& new_running_tablets,
-    const LeaderEpoch& epoch) {
-  auto* mutable_table_info = table_info->mutable_metadata()->mutable_dirty();
-  if (!mutable_table_info->IsPreparing() ||
+  if (!mutable_table_info.mutable_dirty()->IsPreparing() ||
       !table_info->AreAllTabletsRunning(new_running_tablets)) {
-    return false;
+    return;
   }
 
-  if (ScheduleAddTableToXClusterTaskIfNeeded(table_info, mutable_table_info->pb, epoch)) {
-    return false;
+  // Collect all PostTabletCreateTasks for this table.
+  auto table_creation_tasks = xcluster_manager_->GetPostTabletCreateTasks(table_info, epoch);
+  // TODO: Get cdcsdk tasks
+
+  if (table_creation_tasks.empty()) {
+    // Schedule a simple task that will mark this table as RUNNING when it completes.
+    table_creation_tasks.emplace_back(std::make_shared<MarkTableAsRunningTask>(
+        *this, *AsyncTaskPool(), *master_->messenger(), table_info, epoch));
   }
 
-  mutable_table_info->pb.set_state(SysTablesEntryPB::RUNNING);
-  return true;
+  WARN_NOT_OK(
+      PostTabletCreateTaskBase::StartTasks(table_creation_tasks, this, table_info, epoch),
+      "Failed to schedule PostTabletCreateTasks");
 }
 
 Status CatalogManager::PromoteTableToRunningState(
@@ -13703,7 +13684,7 @@ Status CatalogManager::PromoteTableToRunningState(
   auto l = table_info->LockForWrite();
   SCHECK(
       l.mutable_data()->IsPreparing(), IllegalState,
-      "Table $0 should be in PREPARING state. Current state: $2", table_info->ToString(),
+      "Table $0 should be in PREPARING state. Current state: $1", table_info->ToString(),
       l.mutable_data()->pb.state());
 
   l.mutable_data()->pb.set_state(SysTablesEntryPB::RUNNING);
@@ -13715,7 +13696,7 @@ Status CatalogManager::PromoteTableToRunningState(
   return Status::OK();
 }
 
-void CatalogManager::ScheduleAddTableToXClusterTaskForAllTables(const LeaderEpoch& epoch) {
+void CatalogManager::SchedulePostTabletCreationTasksForPendingTables(const LeaderEpoch& epoch) {
   std::vector<TableId> table_ids;
   {
     SharedLock lock(mutex_);
@@ -13728,16 +13709,12 @@ void CatalogManager::ScheduleAddTableToXClusterTaskForAllTables(const LeaderEpoc
   for (auto& table_id : table_ids) {
     auto table_info_result = FindTableById(table_id);
     if (!table_info_result) {
-      LOG(WARNING) << "Failed to find table " << table_id << " to schedule AddTableToXClusterTask";
+      LOG(WARNING) << "Failed to find table " << table_id << " to schedule PostTabletCreationTasks";
       continue;
     }
     auto& table_info = *table_info_result;
-    auto l = table_info->LockForRead();
-    if (l->IsPreparing() && table_info->AreAllTabletsRunning()) {
-      l.Unlock();
-      auto wl = table_info->LockForWrite();
-      ScheduleAddTableToXClusterTaskIfNeeded(table_info, wl.mutable_data()->pb, epoch);
-    }
+    auto wl = table_info->LockForWrite();
+    SchedulePostTabletCreationTasks(table_info, epoch);
   }
 }
 
diff --git a/src/yb/master/catalog_manager.h b/src/yb/master/catalog_manager.h
index 1bb4ac0f06..cb25fa6756 100644
--- a/src/yb/master/catalog_manager.h
+++ b/src/yb/master/catalog_manager.h
@@ -1377,6 +1377,9 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
       GetUniverseReplicationResponsePB* resp,
       rpc::RpcContext* rpc);
 
+  scoped_refptr<UniverseReplicationInfo> GetUniverseReplication(
+      const xcluster::ReplicationGroupId& replication_group_id);
+
   // Checks if the universe is in an active state or has failed during setup.
   Status IsSetupUniverseReplicationDone(
       const IsSetupUniverseReplicationDoneRequestPB* req,
@@ -1528,6 +1531,15 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
 
   void NotifyAutoFlagsConfigChanged();
 
+  // Maps replication group id to the corresponding xrepl stream for a table.
+  using XClusterConsumerTableStreamIds =
+      std::unordered_map<xcluster::ReplicationGroupId, xrepl::StreamId>;
+  // Gets the set of CDC stream info for an xCluster consumer table.
+  XClusterConsumerTableStreamIds GetXClusterConsumerStreamIdsForTable(const TableId& table_id) const
+      EXCLUDES(mutex_);
+
+  std::shared_ptr<ClusterConfigInfo> ClusterConfig() const;
+
  protected:
   // TODO Get rid of these friend classes and introduce formal interface.
   friend class TableLoader;
@@ -1545,7 +1557,7 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
   friend class BackfillTablet;
   friend class YsqlBackendsManager;
   friend class BackendsCatalogVersionJob;
-  friend class AddTableToXClusterTask;
+  friend class AddTableToXClusterTargetTask;
 
   FRIEND_TEST(yb::MasterPartitionedTest, VerifyOldLeaderStepsDown);
 
@@ -2081,8 +2093,6 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
                            rpc::RpcContext* rpc,
                            const LeaderEpoch& epoch);
 
-  std::shared_ptr<ClusterConfigInfo> ClusterConfig() const;
-
   Result<TableInfoPtr> GetGlobalTransactionStatusTable();
 
   Result<bool> IsCreateTableDone(const TableInfoPtr& table);
@@ -2902,14 +2912,6 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
 
   std::unordered_set<xrepl::StreamId> GetCDCSDKStreamsForTable(const TableId& table_id) const;
 
-  // Maps replication group id to the corresponding xrepl stream for a table.
-  using XClusterConsumerTableStreamIds =
-      std::unordered_map<xcluster::ReplicationGroupId, xrepl::StreamId>;
-
-  // Gets the set of CDC stream info for an xCluster consumer table.
-  XClusterConsumerTableStreamIds GetXClusterConsumerStreamIdsForTable(const TableId& table_id) const
-      EXCLUDES(mutex_);
-
   Status CreateTransactionAwareSnapshot(
       const CreateSnapshotRequestPB& req, CreateSnapshotResponsePB* resp, CoarseTimePoint deadline);
 
@@ -3014,16 +3016,18 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
 
   Status DeleteCDCStreamsForTables(const std::unordered_set<TableId>& table_ids) EXCLUDES(mutex_);
 
-  bool ShouldAddTableToXClusterReplication(const TableInfo& index_info, const SysTablesEntryPB& pb)
-      EXCLUDES(mutex_);
-
-  // Schedule AddTableToXClusterTask if needed. Returns true if task is needed and false otherwise.
-  // Write lock is required on the table.
-  bool ScheduleAddTableToXClusterTaskIfNeeded(
-      TableInfoPtr table_info, const SysTablesEntryPB& pb, const LeaderEpoch& epoch)
-      EXCLUDES(mutex_);
+  // For a table that is currently in PREPARING state, if all its tablets have transitioned to
+  // RUNNING state, then collect and start the required post tablet creation async tasks. Table is
+  // advanced to the RUNNING state after all of these tasks complete successfully.
+  // new_running_tablets is the new set of tablets that are being transitioned to RUNNING state
+  // (dirty copy is modified) and yet to be persisted. These should be persisted before the table
+  // lock is released. Note:
+  //    WriteLock on the table is required.
+  void SchedulePostTabletCreationTasks(
+      const TableInfoPtr& table_info, const LeaderEpoch& epoch,
+      const std::set<TabletId>& new_running_tablets = {});
 
-  void ScheduleAddTableToXClusterTaskForAllTables(const LeaderEpoch& epoch) EXCLUDES(mutex_);
+  void SchedulePostTabletCreationTasksForPendingTables(const LeaderEpoch& epoch) EXCLUDES(mutex_);
 
   Result<xcluster::ReplicationGroupId> GetIndexesTableReplicationGroup(const TableInfo& index_info);
 
@@ -3039,18 +3043,6 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
 
   Status RemoveTableFromXcluster(const std::vector<TabletId>& table_ids);
 
-  // For a table that is currently in PREPARING state, if all its tablets have transitioned to
-  // RUNNING state, either advance the table from PREPARING to RUNNING state, or start any async
-  // tasks if needed (ex: AddTableToXClusterTask).
-  // new_running_tablets is the new set of tablets that are being transitioned to RUNNING state
-  // (dirty copy is modified) and yet to be persisted. Returns true if the table state has changed.
-  // Note:
-  //    WriteLock on the table is required.
-  //    Caller is responsible for persisting the table to sys_catalog if its state has changed.
-  Result<bool> HandleNewRunningTabletsForTable(
-      TableInfoPtr table_info, const std::set<TabletId>& new_running_tablets,
-      const LeaderEpoch& epoch);
-
   // Background task that refreshes the in-memory map for YSQL pg_yb_catalog_version table.
   void RefreshPgCatalogVersionInfoPeriodically()
       EXCLUDES(heartbeat_pg_catalog_versions_cache_mutex_);
diff --git a/src/yb/master/leader_epoch.h b/src/yb/master/leader_epoch.h
index e4da28af99..6be481fa6f 100644
--- a/src/yb/master/leader_epoch.h
+++ b/src/yb/master/leader_epoch.h
@@ -32,6 +32,9 @@
 #pragma once
 
 #include <cstdint>
+#include <string>
+
+#include "yb/util/tostring.h"
 
 namespace yb {
 namespace master {
@@ -53,6 +56,12 @@ struct LeaderEpoch {
   explicit LeaderEpoch(int64_t term) : LeaderEpoch(term, 0) {}
 
   LeaderEpoch() : LeaderEpoch(-1, 0) {}
+
+  bool operator==(const LeaderEpoch& rhs) const {
+    return leader_term == rhs.leader_term && pitr_count == rhs.pitr_count;
+  }
+
+  std::string ToString() const { return YB_STRUCT_TO_STRING(leader_term, pitr_count); }
 };
 
 }  // namespace master
diff --git a/src/yb/master/multi_step_monitored_task.cc b/src/yb/master/multi_step_monitored_task.cc
new file mode 100644
index 0000000000..6c95076107
--- /dev/null
+++ b/src/yb/master/multi_step_monitored_task.cc
@@ -0,0 +1,302 @@
+// Copyright (c) YugabyteDB, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
+// in compliance with the License.  You may obtain a copy of the License at
+//
+// http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software distributed under the License
+// is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
+// or implied.  See the License for the specific language governing permissions and limitations
+// under the License.
+//
+
+#include "yb/master/multi_step_monitored_task.h"
+
+#include "yb/master/catalog_entity_info.h"
+#include "yb/master/catalog_manager.h"
+
+#include "yb/rpc/messenger.h"
+
+#include "yb/util/source_location.h"
+#include "yb/util/threadpool.h"
+
+namespace yb::master {
+
+MultiStepMonitoredTask::MultiStepMonitoredTask(
+    ThreadPool& async_task_pool, rpc::Messenger& messenger)
+    : messenger_(messenger), async_task_pool_(async_task_pool) {}
+
+void MultiStepMonitoredTask::Start() {
+  LOG_WITH_PREFIX(INFO) << "Starting task " << this;
+  auto status = RegisterTask();
+  if (!status.ok()) {
+    AbortAndReturnPrevState(status.CloneAndPrepend("Failed to register task"));
+  }
+
+  ScheduleNextStep(std::bind(&MultiStepMonitoredTask::FirstStep, this), "FirstStep");
+}
+
+bool MultiStepMonitoredTask::TrySetState(server::MonitoredTaskState new_state) {
+  auto old_state = state();
+  while (!IsStateTerminal(old_state)) {
+    if (state_.compare_exchange_strong(old_state, new_state)) {
+      return true;
+    }
+  }
+
+  return false;
+}
+
+void MultiStepMonitoredTask::Complete() {
+  if (TrySetState(server::MonitoredTaskState::kComplete)) {
+    EndTask(Status::OK());
+  }
+  // else Task already finished.
+}
+
+server::MonitoredTaskState MultiStepMonitoredTask::AbortAndReturnPrevState(const Status& status) {
+  DCHECK(!status.ok());
+  auto old_state = state();
+  if (TrySetState(server::MonitoredTaskState::kAborted)) {
+    AbortReactorTaskIfScheduled();
+    EndTask(status);
+  }
+
+  return old_state;
+}
+
+void MultiStepMonitoredTask::AbortReactorTaskIfScheduled() {
+  rpc::ScheduledTaskId reactor_task_id;
+  {
+    std::lock_guard l(schedule_task_mutex_);
+    reactor_task_id = reactor_task_id_;
+    reactor_task_id_ = rpc::kInvalidTaskId;
+  }
+
+  if (reactor_task_id != rpc::kInvalidTaskId) {
+    VLOG_WITH_PREFIX_AND_FUNC(2) << "Aborting reactor task id: " << reactor_task_id;
+    messenger_.AbortOnReactor(reactor_task_id);
+  }
+}
+
+void MultiStepMonitoredTask::ScheduleNextStepWithDelay(
+    std::function<Status()> next_step, std::string step_description, MonoDelta delay) {
+  UniqueLock l(schedule_task_mutex_);
+  if (!TrySetState(server::MonitoredTaskState::kWaiting)) {
+    VLOG_WITH_PREFIX_AND_FUNC(1) << "Task has ended. State: " << state();
+    return;
+  }
+
+  VLOG_WITH_PREFIX(1) << "Scheduling " << step_description << " on reactor with delay " << delay;
+
+  next_step_description_ = std::move(step_description);
+  next_step_ = std::move(next_step);
+
+  auto task_id_result = messenger_.ScheduleOnReactor(
+      std::bind(&MultiStepMonitoredTask::ScheduleInternal, shared_from(this)), delay,
+      SOURCE_LOCATION());
+
+  if (!task_id_result.ok()) {
+    l.unlock();
+    AbortAndReturnPrevState(
+        task_id_result.status().CloneAndPrepend("Failed to submit task to reactor"));
+    return;
+  }
+
+  VLOG_WITH_PREFIX_AND_FUNC(2) << "Reactor task id: " << *task_id_result;
+  reactor_task_id_ = *task_id_result;
+}
+
+void MultiStepMonitoredTask::ScheduleNextStep(
+    std::function<Status()> next_step, std::string step_description) {
+  {
+    std::lock_guard l(schedule_task_mutex_);
+    if (next_step_) {
+      LOG(DFATAL) << "Attempt to schedule a step '" << step_description
+                  << "' before the previous step '" << next_step_description_ << "' started.";
+    }
+    next_step_description_ = std::move(step_description);
+    next_step_ = std::move(next_step);
+  }
+  ScheduleInternal();
+}
+
+void MultiStepMonitoredTask::ScheduleInternal() {
+  if (!TrySetState(server::MonitoredTaskState::kScheduling)) {
+    LOG_WITH_PREFIX(WARNING) << "Task already ended. Unable to schedule more work on it";
+    return;
+  }
+
+  Status status;
+  {
+    std::lock_guard l(schedule_task_mutex_);
+    VLOG_WITH_PREFIX(1) << "Scheduling " << next_step_description_ << " on async task pool";
+
+    status = async_task_pool_.SubmitFunc([task = shared_from(this)]() {
+      WARN_NOT_OK(task->Run(), Format("Failed task $0", task->LogPrefix()));
+    });
+  }
+
+  // If we are not able to enqueue, abort the task.
+  if (!status.ok()) {
+    AbortAndReturnPrevState(status.CloneAndPrepend("Failed to submit task to async task pool"));
+  }
+}
+
+Status MultiStepMonitoredTask::Run() {
+  SCHECK(
+      TrySetState(server::MonitoredTaskState::kRunning), IllegalState,
+      "Task already ended. Unable to run more work on it");
+
+  auto status = RunInternal();
+  if (!status.ok()) {
+    AbortAndReturnPrevState(status);
+  }
+  return status;
+}
+
+Status MultiStepMonitoredTask::RunInternal() {
+  std::function<Status()> step;
+
+  // Grab the step_execution_mutex_, since one step schedules the next step, and we dont want the
+  // second one to start before the first once goes fully out of scope.
+  std::lock_guard step_execution_l(step_execution_mutex_);
+  {
+    std::lock_guard schedule_task_l(schedule_task_mutex_);
+    reactor_task_id_ = rpc::kInvalidTaskId;
+    std::swap(next_step_, step);
+
+    if (VLOG_IS_ON(1) && !next_step_description_.empty()) {
+      VLOG_WITH_PREFIX(1) << "Running " << next_step_description_;
+    }
+  }
+
+  if (!step) {
+    LOG_WITH_PREFIX(DFATAL) << "No further steps to run for task";
+    return STATUS(IllegalState, "Task has no steps to run");
+  }
+
+  RETURN_NOT_OK(ValidateRunnable());
+
+  return step();
+}
+
+void MultiStepMonitoredTask::EndTask(const Status& status) {
+  if (!status.ok()) {
+    LOG_WITH_PREFIX(WARNING) << this << " task failed: " << status;
+  }
+
+  TaskCompleted(status);
+
+  StdStatusCallback callback;
+  callback.swap(completion_callback_);
+
+  completion_timestamp_ = MonoTime::Now();
+  LOG_WITH_PREFIX(INFO) << this << " task ended" << (status.ok() ? " successfully" : "");
+
+  UnregisterTask();
+  // Unsafe to use this beyond this point.
+
+  if (callback) {
+    callback(status);
+  }
+}
+
+namespace {
+
+// Helper task to keep track of a group of tasks and execute a callback exactly once when all tasks
+// complete.
+class TaskGroupInfo {
+ public:
+  static std::shared_ptr<TaskGroupInfo> Create(
+      size_t task_count, StdStatusCallback completion_callback) {
+    return std::shared_ptr<TaskGroupInfo>(
+        new TaskGroupInfo(task_count, std::move(completion_callback)));
+  }
+
+  void TaskCompleted(const Status& status) {
+    auto first_error_status = SetStatusAndGetFirstError(status);
+
+    if (task_count_.fetch_sub(1) == 1) {
+      // This is the last task in the group to complete.
+      completion_callback_(first_error_status);
+    }
+  }
+
+ private:
+  TaskGroupInfo(size_t task_count, StdStatusCallback completion_callback)
+      : task_count_(task_count), completion_callback_(std::move(completion_callback)) {
+    DCHECK_GT(task_count_, 0);
+  }
+
+  Status SetStatusAndGetFirstError(const Status& status) {
+    std::lock_guard l(status_mutex_);
+    if (status_.ok() && !status.ok()) {
+      status_ = status;
+    }
+    return status_;
+  }
+
+  std::atomic<size_t> task_count_;
+  StdStatusCallback completion_callback_;
+
+  std::mutex status_mutex_;
+  Status status_ GUARDED_BY(status_mutex_);
+
+  DISALLOW_COPY_AND_ASSIGN(TaskGroupInfo);
+};
+}  // namespace
+
+Status MultiStepMonitoredTask::StartTasks(
+    const std::vector<MultiStepMonitoredTask*>& tasks, StdStatusCallback group_completion_cb) {
+  SCHECK(!tasks.empty(), InvalidArgument, "No tasks provided");
+
+  auto task_group_info = TaskGroupInfo::Create(tasks.size(), std::move(group_completion_cb));
+
+  for (auto* task : tasks) {
+    CHECK(task);
+    // Preserve exiting callback of the task and run that at the end of the new callback.
+    task->completion_callback_ =
+        [task_group_info, task_cb = std::move(task->completion_callback_)](const Status& status) {
+          if (task_cb) {
+            task_cb(status);
+          }
+          task_group_info->TaskCompleted(status);
+        };
+
+    // If the last task failed to schedule then it will be aborted. If all the other tasks completed
+    // before this, then group_completion_cb in invoked on this same thread.
+    task->Start();
+  }
+
+  return Status::OK();
+}
+
+MultiStepTableTask::MultiStepTableTask(
+    CatalogManager& catalog_manager, ThreadPool& async_task_pool, rpc::Messenger& messenger,
+    TableInfoPtr table_info, const LeaderEpoch& epoch)
+    : MultiStepMonitoredTask(async_task_pool, messenger),
+      catalog_manager_(catalog_manager),
+      epoch_(std::move(epoch)),
+      table_info_(std::move(table_info)) {}
+
+Status MultiStepTableTask::RegisterTask() {
+  table_info_->AddTask(shared_from_this());
+  return Status::OK();
+}
+
+void MultiStepTableTask::UnregisterTask() {
+  DCHECK(table_info_->HasTasks(type()));
+
+  table_info_->RemoveTask(shared_from_this());
+}
+
+Status MultiStepTableTask::ValidateRunnable() {
+  RETURN_NOT_OK(catalog_manager_.CheckIsLeaderAndReady());
+  SCHECK_EQ(
+      epoch_, catalog_manager_.GetLeaderEpochInternal(), IllegalState, "Epoch is no longer valid");
+  return Status::OK();
+}
+
+}  // namespace yb::master
diff --git a/src/yb/master/multi_step_monitored_task.h b/src/yb/master/multi_step_monitored_task.h
new file mode 100644
index 0000000000..fefe5293ca
--- /dev/null
+++ b/src/yb/master/multi_step_monitored_task.h
@@ -0,0 +1,147 @@
+// Copyright (c) YugabyteDB, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
+// in compliance with the License.  You may obtain a copy of the License at
+//
+// http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software distributed under the License
+// is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
+// or implied.  See the License for the specific language governing permissions and limitations
+// under the License.
+//
+
+#pragma once
+
+#include "yb/master/leader_epoch.h"
+#include "yb/master/master_fwd.h"
+#include "yb/rpc/rpc_fwd.h"
+#include "yb/server/monitored_task.h"
+#include "yb/util/status_callback.h"
+
+namespace yb {
+
+class ThreadPool;
+
+namespace master {
+
+// Tasks that contain multiple asynchronous steps.
+// - All steps run on the passed in async thread pool. Steps can be scheduled to run immediately or
+// with a delay.
+// - Invoking Start will register the task and schedule it in the async thread pool.
+// - StartTasks will start a group of tasks together and guarantees the group_completion_cb is
+//      called exactly once after all tasks complete.
+//
+// Notes for Derived classes:
+// - Complete must be called when all steps of the task are done. No further steps can be scheduled
+// after this point.
+// - If any step returns a bad status the task is aborted.
+// - At least one shared pointer to the task must be held until UnregisterTask completes.
+class MultiStepMonitoredTask : public server::RunnableMonitoredTask {
+ public:
+  virtual void Start();
+
+  // Abort this task and return its state before it was successfully aborted. If the task
+  // entered a different terminal state before we were able to abort it, return that state.
+  server::MonitoredTaskState AbortAndReturnPrevState(const Status& status) override
+      EXCLUDES(schedule_task_mutex_);
+
+  // Schedule a group of tasks and invoke the callback when all tasks in the group complete.
+  // group_completion_cb is invoked with the first bad status if any of the tasks failed or an
+  // OK status if all tasks succeeded. group_completion_cb may be invoked on the calling thread
+  // with a bad status.
+  static Status StartTasks(
+      const std::vector<MultiStepMonitoredTask*>& tasks, StdStatusCallback group_completion_cb);
+
+ protected:
+  explicit MultiStepMonitoredTask(ThreadPool& async_task_pool, rpc::Messenger& messenger);
+
+  virtual ~MultiStepMonitoredTask() = default;
+
+  // ==================================================================
+  // Virtual methods that should be implemented in derived classes.
+  // ==================================================================
+
+  virtual Status FirstStep() = 0;
+  // Register the task before it starts. Typically the task's shared pointer is registered to a
+  // CatalogEntity object, so that it can be tracked, and aborted when the object is dropped or the
+  // master leader changes.
+  virtual Status RegisterTask() = 0;
+  // Unregister the task after it completes.
+  virtual void UnregisterTask() = 0;
+
+  // Check if the task is still valid to run.
+  virtual Status ValidateRunnable() = 0;
+
+  // This is invoked when the task reaches a terminal state and before it is Unregistered. Status
+  // will be OK if the task succeeded and not OK if it failed. Note: Do not invoke this directly. It
+  // will be invoked when any step returns a bad status.
+  virtual void TaskCompleted(const Status& status) = 0;
+
+  // ==================================================================
+  // Helper methods that derived class steps can invoke.
+  // ==================================================================
+
+  void Complete();
+
+  std::string LogPrefix() { return Format("$0: ", description()); }
+
+  void ScheduleNextStep(std::function<Status()> next_step, std::string step_description)
+      EXCLUDES(schedule_task_mutex_);
+
+  // If the task needs to run with a delay we schedule on the reactor with the delay and when
+  // that executes we schedule the actual step on the async thread pool.
+  void ScheduleNextStepWithDelay(
+      std::function<Status()> next_step, std::string step_description, MonoDelta delay)
+      EXCLUDES(schedule_task_mutex_);
+
+  // Optional callback invoked when the task finishes. Callback is not allowed to access the task.
+  StdStatusCallback completion_callback_;
+
+ private:
+  Status Run() override EXCLUDES(schedule_task_mutex_, step_execution_mutex_);
+  Status RunInternal() EXCLUDES(schedule_task_mutex_, step_execution_mutex_);
+  void ScheduleInternal() EXCLUDES(schedule_task_mutex_);
+  void AbortReactorTaskIfScheduled() EXCLUDES(schedule_task_mutex_);
+
+  // Attempts to set the state to the specified value.
+  // The state cannot be changed once it is set to a terminal value.
+  // If new_state is terminal then returns true if and only if we reached that terminal state for
+  // the first time.
+  // Otherwise, returns true if state is now new_state.
+  bool TrySetState(server::MonitoredTaskState new_state);
+  void EndTask(const Status& s);
+
+  rpc::Messenger& messenger_;
+  ThreadPool& async_task_pool_;
+
+  // Mutex to ensure sure we execute steps sequentially.
+  std::mutex step_execution_mutex_;
+
+  std::mutex schedule_task_mutex_;
+  rpc::ScheduledTaskId reactor_task_id_ GUARDED_BY(schedule_task_mutex_) = rpc::kInvalidTaskId;
+
+  std::string next_step_description_ GUARDED_BY(schedule_task_mutex_);
+  std::function<Status()> next_step_ GUARDED_BY(schedule_task_mutex_) = nullptr;
+};
+
+// A MultiStepMonitoredTask that is tied to a single Table and the master leader epoch.
+class MultiStepTableTask : public MultiStepMonitoredTask {
+ public:
+ protected:
+  MultiStepTableTask(
+      CatalogManager& catalog_manager, ThreadPool& async_task_pool, rpc::Messenger& messenger,
+      TableInfoPtr table_info, const LeaderEpoch& epoch);
+  Status ValidateRunnable() override;
+
+  CatalogManager& catalog_manager_;
+  const LeaderEpoch epoch_;
+  const TableInfoPtr table_info_;
+
+ private:
+  Status RegisterTask() override;
+  void UnregisterTask() override;
+};
+
+}  // namespace master
+}  // namespace yb
diff --git a/src/yb/master/post_tablet_create_task_base.cc b/src/yb/master/post_tablet_create_task_base.cc
new file mode 100644
index 0000000000..0f4219e541
--- /dev/null
+++ b/src/yb/master/post_tablet_create_task_base.cc
@@ -0,0 +1,98 @@
+// Copyright (c) YugabyteDB, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
+// in compliance with the License.  You may obtain a copy of the License at
+//
+// http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software distributed under the License
+// is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
+// or implied.  See the License for the specific language governing permissions and limitations
+// under the License.
+//
+
+#include "yb/master/post_tablet_create_task_base.h"
+
+#include "yb/master/catalog_manager.h"
+
+using namespace std::placeholders;
+
+namespace yb::master {
+
+namespace {
+
+void MarkTableAsRunningIfHealthy(
+    CatalogManager* catalog_manager, TableInfoPtr table_info, const LeaderEpoch& epoch,
+    const Status& status) {
+  if (!status.ok() || !table_info->GetCreateTableErrorStatus().ok()) {
+    return;
+  }
+
+  auto promote_status = catalog_manager->PromoteTableToRunningState(table_info, epoch);
+  if (!promote_status.ok()) {
+    table_info->SetCreateTableErrorStatus(promote_status);
+    LOG(WARNING) << "Failed to promote table " << table_info->ToString()
+                 << " to RUNNING state: " << promote_status;
+  }
+}
+
+}  // namespace
+
+PostTabletCreateTaskBase::PostTabletCreateTaskBase(
+    CatalogManager& catalog_manager, ThreadPool& async_task_pool, rpc::Messenger& messenger,
+    TableInfoPtr table_info, const LeaderEpoch& epoch)
+    : MultiStepTableTask(
+          catalog_manager, async_task_pool, messenger, std::move(table_info), std::move(epoch)) {}
+
+Status PostTabletCreateTaskBase::ValidateRunnable() {
+  RETURN_NOT_OK(MultiStepTableTask::ValidateRunnable());
+
+  SCHECK(
+      table_info_->IsPreparing(), IllegalState,
+      "Task $0 failed since table is in state $1 when it was expected to be in state $2",
+      description(), table_info_->LockForRead()->state_name(),
+      SysTablesEntryPB::State_Name(SysTablesEntryPB::PREPARING));
+  return Status::OK();
+}
+
+Status PostTabletCreateTaskBase::StartTasks(
+    const std::vector<std::shared_ptr<PostTabletCreateTaskBase>>& table_creation_tasks,
+    CatalogManager* catalog_manager, TableInfoPtr table_info, const LeaderEpoch& epoch) {
+  SCHECK(!table_creation_tasks.empty(), IllegalState, "At least one task must be scheduled");
+
+  std::vector<MultiStepMonitoredTask*> tasks;
+  for (const auto& task : table_creation_tasks) {
+    SCHECK_EQ(
+        task->table_info_->id(), table_info->id(), IllegalState,
+        "All tasks in group must belong to the same table");
+    tasks.emplace_back(task.get());
+  }
+
+  return MultiStepMonitoredTask::StartTasks(
+      tasks, std::bind(&MarkTableAsRunningIfHealthy, catalog_manager, table_info, epoch, _1));
+}
+
+void PostTabletCreateTaskBase::TaskCompleted(const Status& status) {
+  if (!status.ok() && table_info_->GetCreateTableErrorStatus().ok()) {
+    // There is a small race between the Get and the Set which can be ignored since we never reset a
+    // bad status with OK status.
+    table_info_->SetCreateTableErrorStatus(status);
+  }
+}
+
+MarkTableAsRunningTask::MarkTableAsRunningTask(
+    CatalogManager& catalog_manager, ThreadPool& async_task_pool, rpc::Messenger& messenger,
+    TableInfoPtr table_info, const LeaderEpoch& epoch)
+    : PostTabletCreateTaskBase(
+          catalog_manager, async_task_pool, messenger, std::move(table_info), std::move(epoch)) {}
+
+std::string MarkTableAsRunningTask::description() const {
+  return Format("MarkTableAsRunningTask [$0]", table_info_->id());
+}
+
+Status MarkTableAsRunningTask::FirstStep() {
+  Complete();
+  return Status::OK();
+}
+
+}  // namespace yb::master
diff --git a/src/yb/master/post_tablet_create_task_base.h b/src/yb/master/post_tablet_create_task_base.h
new file mode 100644
index 0000000000..35eb32f8c5
--- /dev/null
+++ b/src/yb/master/post_tablet_create_task_base.h
@@ -0,0 +1,67 @@
+// Copyright (c) YugabyteDB, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
+// in compliance with the License.  You may obtain a copy of the License at
+//
+// http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software distributed under the License
+// is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
+// or implied.  See the License for the specific language governing permissions and limitations
+// under the License.
+//
+
+#pragma once
+
+#include "yb/master/multi_step_monitored_task.h"
+
+namespace yb::master {
+
+// Tasks that run on a newly created table (PREPARING state) after all its tablets have been
+// created and marked as RUNNING. Once all tasks of this type complete, the table will be marked as
+// RUNNING, which completes the table creation workflow.
+// - Check MultiStepMonitoredTask for information about scheduling.
+class PostTabletCreateTaskBase : public MultiStepTableTask {
+ public:
+  // Start all the tasks at once. Once the last task completes it will transition the table to
+  // RUNNING state if healthy.
+  // NOTE: Do not call this directly. Add your task to
+  // `CatalogManager::SchedulePostTabletCreationTasks`.
+  static Status StartTasks(
+      const std::vector<std::shared_ptr<PostTabletCreateTaskBase>>& table_creation_tasks,
+      CatalogManager* catalog_manager, TableInfoPtr table_info, const LeaderEpoch& epoch);
+
+ protected:
+  explicit PostTabletCreateTaskBase(
+      CatalogManager& catalog_manager, ThreadPool& async_task_pool, rpc::Messenger& messenger,
+      TableInfoPtr table_info, const LeaderEpoch& epoch);
+
+  virtual ~PostTabletCreateTaskBase() = default;
+
+ private:
+  void Start() override { MultiStepMonitoredTask::Start(); }  // Marking private.
+  Status ValidateRunnable() override;
+  void TaskCompleted(const Status& s) override;
+};
+
+// A simple task that will directly complete. This is used to transition the table to RUNNING
+// state in a async manner.
+class MarkTableAsRunningTask : public PostTabletCreateTaskBase {
+ public:
+  MarkTableAsRunningTask(
+      CatalogManager& catalog_manager, ThreadPool& async_task_pool, rpc::Messenger& messenger,
+      TableInfoPtr table_info, const LeaderEpoch& epoch);
+
+  server::MonitoredTaskType type() const override {
+    return server::MonitoredTaskType::kMarkTableAsRunning;
+  }
+
+  std::string type_name() const override { return "Marking table as RUNNNING"; }
+
+  std::string description() const override;
+
+ private:
+  Status FirstStep() override;
+};
+
+}  // namespace yb::master
diff --git a/src/yb/master/xcluster/add_table_to_xcluster_target_task.cc b/src/yb/master/xcluster/add_table_to_xcluster_target_task.cc
new file mode 100644
index 0000000000..20583061ec
--- /dev/null
+++ b/src/yb/master/xcluster/add_table_to_xcluster_target_task.cc
@@ -0,0 +1,202 @@
+// Copyright (c) YugabyteDB, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
+// in compliance with the License.  You may obtain a copy of the License at
+//
+// http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software distributed under the License
+// is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
+// or implied.  See the License for the specific language governing permissions and limitations
+// under the License.
+//
+
+#include "yb/master/xcluster/add_table_to_xcluster_target_task.h"
+
+#include "yb/master/catalog_manager.h"
+#include "yb/master/xcluster/xcluster_manager_if.h"
+#include "yb/master/xcluster/xcluster_safe_time_service.h"
+#include "yb/rpc/messenger.h"
+#include "yb/util/logging.h"
+#include "yb/util/sync_point.h"
+#include "yb/util/trace.h"
+
+DEFINE_test_flag(bool, xcluster_fail_table_create_during_bootstrap, false,
+    "Fail the table or index creation during xcluster bootstrap stage.");
+
+#define SCHEDULE_WITH_DELAY(task, ...) \
+  ScheduleNextStepWithDelay( \
+      std::bind(&AddTableToXClusterTargetTask::task, this, ##__VA_ARGS__), #task, kScheduleDelay);
+
+#define SCHEDULE(task, ...) \
+  ScheduleNextStep(std::bind(&AddTableToXClusterTargetTask::task, this, ##__VA_ARGS__), #task);
+
+using namespace std::placeholders;
+
+namespace yb::master {
+
+namespace {
+const MonoDelta kScheduleDelay = MonoDelta::FromMilliseconds(200);
+}
+
+AddTableToXClusterTargetTask::AddTableToXClusterTargetTask(
+    CatalogManager& catalog_manager, rpc::Messenger& messenger, TableInfoPtr table_info,
+    const LeaderEpoch& epoch)
+    : PostTabletCreateTaskBase(
+          catalog_manager, *catalog_manager.AsyncTaskPool(), messenger, std::move(table_info),
+          std::move(epoch)) {}
+
+std::string AddTableToXClusterTargetTask::description() const {
+  return Format("AddTableToXClusterTargetTask [$0]", table_info_->id());
+}
+
+Status AddTableToXClusterTargetTask::FirstStep() {
+  auto stream_ids = catalog_manager_.GetXClusterConsumerStreamIdsForTable(table_info_->id());
+  if (!stream_ids.empty()) {
+    LOG_WITH_PREFIX(INFO) << "Table is already part of xcluster replication "
+                          << yb::ToString(stream_ids);
+    Complete();
+    return Status::OK();
+  }
+
+  SCHECK(
+      !FLAGS_TEST_xcluster_fail_table_create_during_bootstrap, IllegalState,
+      "FLAGS_TEST_xcluster_fail_table_create_during_bootstrap");
+
+  bool abandon_task = false;
+  TEST_SYNC_POINT_CALLBACK(
+      "AddTableToXClusterTargetTask::RunInternal::BeforeBootstrap", &abandon_task);
+  if (abandon_task) {
+    LOG_WITH_PREFIX(WARNING) << "Task will be stuck";
+    // Just exit without scheduling work or completing the task.
+    return Status::OK();
+  }
+
+  replication_group_id_ =
+      VERIFY_RESULT(catalog_manager_.GetIndexesTableReplicationGroup(*table_info_));
+  DCHECK(!replication_group_id_.empty());
+
+  return catalog_manager_.BootstrapTable(
+      replication_group_id_, *table_info_,
+      std::bind(&AddTableToXClusterTargetTask::BootstrapTableCallback, shared_from(this), _1));
+}
+
+Status AddTableToXClusterTargetTask::BootstrapTableCallback(
+    client::BootstrapProducerResult bootstrap_result) {
+  auto [producer_table_ids, bootstrap_ids, bootstrap_time] =
+      VERIFY_RESULT(std::move(bootstrap_result));
+  CHECK_EQ(producer_table_ids.size(), 1);
+  CHECK_EQ(bootstrap_ids.size(), 1);
+  SCHECK(!bootstrap_time.is_special(), IllegalState, "Failed to get a valid bootstrap time");
+  bootstrap_time_ = bootstrap_time;
+
+  SCHEDULE(AddTableToReplicationGroup, producer_table_ids[0], bootstrap_ids[0]);
+  return Status::OK();
+}
+
+Status AddTableToXClusterTargetTask::AddTableToReplicationGroup(
+    TableId producer_table_id, std::string bootstrap_id) {
+  LOG_WITH_PREFIX_AND_FUNC(INFO) << "Adding table to xcluster universe replication "
+                                 << replication_group_id_ << " with bootstrap_id:" << bootstrap_id
+                                 << ", bootstrap_time:" << bootstrap_time_
+                                 << " and producer_table_id:" << producer_table_id;
+  AlterUniverseReplicationRequestPB alter_universe_req;
+  AlterUniverseReplicationResponsePB alter_universe_resp;
+  alter_universe_req.set_replication_group_id(replication_group_id_.ToString());
+  alter_universe_req.add_producer_table_ids_to_add(producer_table_id);
+  alter_universe_req.add_producer_bootstrap_ids_to_add(bootstrap_id);
+  RETURN_NOT_OK(catalog_manager_.AlterUniverseReplication(
+      &alter_universe_req, &alter_universe_resp, nullptr /* rpc */));
+
+  if (alter_universe_resp.has_error()) {
+    return StatusFromPB(alter_universe_resp.error().status());
+  }
+
+  VLOG_WITH_PREFIX(1) << "Waiting for xcluster safe time of namespace "
+                      << table_info_->namespace_id() << " to get past bootstrap_time "
+                      << bootstrap_time_;
+
+  SCHEDULE_WITH_DELAY(WaitForSetupUniverseReplicationToFinish);
+  return Status::OK();
+}
+
+Status AddTableToXClusterTargetTask::WaitForSetupUniverseReplicationToFinish() {
+  IsSetupUniverseReplicationDoneRequestPB check_req;
+  IsSetupUniverseReplicationDoneResponsePB check_resp;
+  check_req.set_replication_group_id(replication_group_id_.ToString());
+  auto status = catalog_manager_.IsSetupUniverseReplicationDone(
+      &check_req, &check_resp, /* RpcContext */ nullptr);
+  if (status.ok() && check_resp.has_error()) {
+    status = StatusFromPB(check_resp.error().status());
+  }
+
+  if (!status.ok()) {
+    YB_LOG_EVERY_N_SECS(WARNING, 10)
+        << LogPrefix() << "Failed to check if setup universe replication is done: " << status;
+    SCHEDULE_WITH_DELAY(WaitForSetupUniverseReplicationToFinish);
+    return Status::OK();
+  }
+
+  if (!check_resp.done()) {
+    VLOG_WITH_PREFIX(2) << "Waiting for setup universe replication to finish";
+    // If this takes too long the table creation will timeout and abort the task.
+    SCHEDULE_WITH_DELAY(WaitForSetupUniverseReplicationToFinish);
+    return Status::OK();
+  }
+
+  if (check_resp.has_replication_error()) {
+    RETURN_NOT_OK(StatusFromPB(check_resp.replication_error()));
+  }
+
+  SCHEDULE(RefreshAndGetXClusterSafeTime);
+  return Status::OK();
+}
+
+Status AddTableToXClusterTargetTask::RefreshAndGetXClusterSafeTime() {
+  CHECK(bootstrap_time_.is_valid());
+  // Force a refresh of the xCluster safe time map so that it accounts for all tables under
+  // replication.
+  auto namespace_id = table_info_->namespace_id();
+  auto initial_safe_time = VERIFY_RESULT(
+      catalog_manager_.GetXClusterManager()->RefreshAndGetXClusterNamespaceToSafeTimeMap(
+          catalog_manager_.GetLeaderEpochInternal()));
+  if (!initial_safe_time.contains(namespace_id)) {
+    VLOG_WITH_PREFIX(2) << "Namespace " << namespace_id
+                        << " is no longer part of any xCluster replication";
+    Complete();
+    return Status::OK();
+  }
+
+  initial_xcluster_safe_time_ = initial_safe_time[namespace_id];
+  initial_xcluster_safe_time_.MakeAtLeast(bootstrap_time_);
+
+  // Wait for the xCluster safe time to advance beyond the initial value. This ensures all tables
+  // under replication are part of the safe time computation.
+  SCHEDULE_WITH_DELAY(WaitForXClusterSafeTimeCaughtUp);
+  return Status::OK();
+}
+
+Status AddTableToXClusterTargetTask::WaitForXClusterSafeTimeCaughtUp() {
+  if (initial_xcluster_safe_time_.is_valid()) {
+    // TODO: Handle the case when replication was dropped.
+    auto ht = VERIFY_RESULT(
+        catalog_manager_.GetXClusterManager()->GetXClusterSafeTime(table_info_->namespace_id()));
+
+    auto caught_up = ht > initial_xcluster_safe_time_;
+    if (!caught_up) {
+      YB_LOG_EVERY_N_SECS(WARNING, 10) << LogPrefix() << "Waiting for xCluster safe time " << ht
+                                       << " to advance beyond " << initial_xcluster_safe_time_;
+      // If this takes too long the table creation will timeout and abort the task.
+      SCHEDULE_WITH_DELAY(WaitForXClusterSafeTimeCaughtUp);
+      return Status::OK();
+    }
+  }
+
+  LOG(INFO) << "Table " << table_info_->ToString()
+            << " successfully added to xCluster universe replication";
+
+  Complete();
+  return Status::OK();
+}
+
+}  // namespace yb::master
diff --git a/src/yb/master/xcluster/add_table_to_xcluster_target_task.h b/src/yb/master/xcluster/add_table_to_xcluster_target_task.h
new file mode 100644
index 0000000000..0c7c52b5d8
--- /dev/null
+++ b/src/yb/master/xcluster/add_table_to_xcluster_target_task.h
@@ -0,0 +1,58 @@
+// Copyright (c) YugabyteDB, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
+// in compliance with the License.  You may obtain a copy of the License at
+//
+// http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software distributed under the License
+// is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
+// or implied.  See the License for the specific language governing permissions and limitations
+// under the License.
+//
+
+#pragma once
+
+#include <string>
+
+#include "yb/cdc/xcluster_types.h"
+#include "yb/client/client_fwd.h"
+#include "yb/common/hybrid_time.h"
+#include "yb/master/leader_epoch.h"
+#include "yb/master/post_tablet_create_task_base.h"
+
+namespace yb::master {
+
+// This task adds a newly created table in the consumer xCluster universe to transactional
+// replication group. The table must be in PREPARING state with all tablets created at the start of
+// the task. The task performs bootstrap of the producer table, adds it to the replication group,
+// waits for the xCluster Safe Time to include the new table, and finally marks the table as
+// RUNNING.
+class AddTableToXClusterTargetTask : public PostTabletCreateTaskBase {
+ public:
+  AddTableToXClusterTargetTask(
+      CatalogManager& catalog_manager, rpc::Messenger& messenger, TableInfoPtr table_info,
+      const LeaderEpoch& epoch);
+
+  server::MonitoredTaskType type() const override {
+    return server::MonitoredTaskType::kAddTableToXClusterTarget;
+  }
+
+  std::string type_name() const override { return "Add Table to xCluster Target replication"; }
+
+  std::string description() const override;
+
+ private:
+  Status FirstStep() override;
+  Status BootstrapTableCallback(client::BootstrapProducerResult bootstrap_result);
+  Status AddTableToReplicationGroup(TableId producer_table_id, std::string bootstrap_id);
+  Status WaitForSetupUniverseReplicationToFinish();
+  Status RefreshAndGetXClusterSafeTime();
+  Status WaitForXClusterSafeTimeCaughtUp();
+
+  HybridTime bootstrap_time_ = HybridTime::kInvalid;
+  HybridTime initial_xcluster_safe_time_ = HybridTime::kInvalid;
+  xcluster::ReplicationGroupId replication_group_id_;
+};
+
+}  // namespace yb::master
diff --git a/src/yb/master/xcluster/add_table_to_xcluster_task.cc b/src/yb/master/xcluster/add_table_to_xcluster_task.cc
deleted file mode 100644
index 29b7856e5a..0000000000
--- a/src/yb/master/xcluster/add_table_to_xcluster_task.cc
+++ /dev/null
@@ -1,326 +0,0 @@
-// Copyright (c) YugaByte, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
-// in compliance with the License.  You may obtain a copy of the License at
-//
-// http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software distributed under the License
-// is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
-// or implied.  See the License for the specific language governing permissions and limitations
-// under the License.
-//
-
-#include "yb/master/xcluster/add_table_to_xcluster_task.h"
-#include "yb/master/catalog_manager.h"
-#include "yb/master/master.h"
-#include "yb/master/xcluster/xcluster_manager_if.h"
-#include "yb/master/xcluster/xcluster_safe_time_service.h"
-#include "yb/rpc/messenger.h"
-#include "yb/util/source_location.h"
-#include "yb/util/logging.h"
-#include "yb/util/sync_point.h"
-#include "yb/util/trace.h"
-
-DEFINE_test_flag(bool, xcluster_fail_table_create_during_bootstrap, false,
-    "Fail the table or index creation during xcluster bootstrap stage.");
-
-#define FAIL_TASK_AND_RETURN_IF(cond, status) \
-  do { \
-    if ((cond)) { \
-      AbortAndReturnPrevState((status)); \
-      return; \
-    } \
-  } while (false)
-
-#define FAIL_TASK_AND_RETURN_IF_NOT_OK(status) \
-  FAIL_TASK_AND_RETURN_IF(!status.ok(), MoveStatus(status))
-
-#define VERIFY_RESULT_AND_FAIL_TASK(expr) \
-  RESULT_CHECKER_HELPER(expr, FAIL_TASK_AND_RETURN_IF_NOT_OK(__result))
-
-#define SCHEDULE_ON_REACTOR_AND_RETURN(task, ...) \
-  ScheduleOnReactor(std::bind(&AddTableToXClusterTask::task, this, ##__VA_ARGS__), #task); \
-  return
-
-#define SCHEDULE_ON_ASYNC_POOL_AND_RETURN(task, ...) \
-  ScheduleOnAsyncTaskPool(std::bind(&AddTableToXClusterTask::task, this, ##__VA_ARGS__), #task); \
-  return
-
-using namespace std::placeholders;
-
-namespace yb::master {
-
-const auto kDefaultReactorDelay = MonoDelta::FromMilliseconds(200);
-
-AddTableToXClusterTask::AddTableToXClusterTask(
-    CatalogManager* catalog_manager, TableInfoPtr table_info, LeaderEpoch epoch)
-    : catalog_manager_(catalog_manager), table_info_(table_info), epoch_(std::move(epoch)) {}
-
-std::string AddTableToXClusterTask::description() const {
-  return Format("AddTableToXClusterTask [$0]", table_info_->id());
-}
-
-Status AddTableToXClusterTask::Run() {
-  TransitionToRunningAndExecute([this]() { RunInternal(); }, "RunInternal");
-  return Status::OK();
-}
-
-void AddTableToXClusterTask::RunInternal() {
-  {
-    TRACE("Locking table");
-    auto l = table_info_->LockForRead();
-    FAIL_TASK_AND_RETURN_IF(
-        !l->IsPreparing(),
-        STATUS(
-            IllegalState,
-            "Table $0 should be in PREPARING state to be added to xCluster replication.",
-            table_info_->ToString()));
-  }
-
-  auto stream_ids = catalog_manager_->GetXClusterConsumerStreamIdsForTable(table_info_->id());
-  if (!stream_ids.empty()) {
-    LOG_WITH_PREFIX(INFO) << "Table is already part of xcluster replication "
-                          << yb::ToString(stream_ids);
-    CompleteTableCreation();
-    return;
-  }
-
-  FAIL_TASK_AND_RETURN_IF(
-      FLAGS_TEST_xcluster_fail_table_create_during_bootstrap,
-      STATUS(IllegalState, "FLAGS_TEST_xcluster_fail_table_create_during_bootstrap"));
-
-  bool stuck_add_table_to_xcluster = false;
-  TEST_SYNC_POINT_CALLBACK(
-      "AddTableToXClusterTask::RunInternal::BeforeBootstrap", &stuck_add_table_to_xcluster);
-  if (stuck_add_table_to_xcluster) {
-    LOG_WITH_PREFIX(WARNING) << "Task will be stuck";
-    // Just exit without scheduling work or completing the task.
-    return;
-  }
-
-  replication_group_id_ =
-      VERIFY_RESULT_AND_FAIL_TASK(catalog_manager_->GetIndexesTableReplicationGroup(*table_info_));
-  DCHECK(!replication_group_id_.empty());
-
-  FAIL_TASK_AND_RETURN_IF_NOT_OK(catalog_manager_->BootstrapTable(
-      replication_group_id_,
-      *table_info_,
-      std::bind(&AddTableToXClusterTask::BootstrapTableCallback, shared_from(this), _1)));
-}
-
-void AddTableToXClusterTask::BootstrapTableCallback(
-    client::BootstrapProducerResult bootstrap_result) {
-  auto [producer_table_ids, bootstrap_ids, bootstrap_time] =
-      VERIFY_RESULT_AND_FAIL_TASK(std::move(bootstrap_result));
-  CHECK_EQ(producer_table_ids.size(), 1);
-  CHECK_EQ(bootstrap_ids.size(), 1);
-  FAIL_TASK_AND_RETURN_IF(
-      bootstrap_time.is_special(), STATUS(IllegalState, "Failed to get a valid bootstrap time"));
-  bootstrap_time_ = bootstrap_time;
-
-  SCHEDULE_ON_ASYNC_POOL_AND_RETURN(
-      AddTableToReplicationGroup, producer_table_ids[0], bootstrap_ids[0]);
-}
-
-void AddTableToXClusterTask::AddTableToReplicationGroup(
-    TableId producer_table_id, std::string bootstrap_id) {
-  LOG_WITH_PREFIX_AND_FUNC(INFO) << "Adding table to xcluster universe replication "
-                                 << replication_group_id_ << " with bootstrap_id " << bootstrap_id
-                                 << " and bootstrap_time " << bootstrap_time_;
-  AlterUniverseReplicationRequestPB alter_universe_req;
-  AlterUniverseReplicationResponsePB alter_universe_resp;
-  alter_universe_req.set_replication_group_id(replication_group_id_.ToString());
-  alter_universe_req.add_producer_table_ids_to_add(producer_table_id);
-  alter_universe_req.add_producer_bootstrap_ids_to_add(bootstrap_id);
-  FAIL_TASK_AND_RETURN_IF_NOT_OK(catalog_manager_->AlterUniverseReplication(
-      &alter_universe_req, &alter_universe_resp, nullptr /* rpc */));
-
-  FAIL_TASK_AND_RETURN_IF(
-      alter_universe_resp.has_error(), StatusFromPB(alter_universe_resp.error().status()));
-
-  VLOG_WITH_PREFIX(1) << "Waiting for xcluster safe time of namespace "
-                      << table_info_->namespace_id() << " to get past bootstrap_time "
-                      << bootstrap_time_;
-
-  SCHEDULE_ON_REACTOR_AND_RETURN(WaitForSetupUniverseReplicationToFinish);
-}
-
-void AddTableToXClusterTask::WaitForSetupUniverseReplicationToFinish() {
-  IsSetupUniverseReplicationDoneRequestPB check_req;
-  IsSetupUniverseReplicationDoneResponsePB check_resp;
-  check_req.set_replication_group_id(replication_group_id_.ToString());
-  auto status = catalog_manager_->IsSetupUniverseReplicationDone(
-      &check_req, &check_resp, /* RpcContext */ nullptr);
-  if (status.ok() && check_resp.has_error()) {
-    status = StatusFromPB(check_resp.error().status());
-  }
-
-  if (!status.ok()) {
-    YB_LOG_EVERY_N_SECS(WARNING, 10)
-        << LogPrefix() << "Failed to check if setup universe replication is done: " << status;
-    SCHEDULE_ON_REACTOR_AND_RETURN(WaitForSetupUniverseReplicationToFinish);
-  }
-
-  if (check_resp.has_done() && check_resp.done()) {
-    status = StatusFromPB(check_resp.replication_error());
-  }
-  FAIL_TASK_AND_RETURN_IF_NOT_OK(status);
-
-  SCHEDULE_ON_ASYNC_POOL_AND_RETURN(RefreshAndGetXClusterSafeTime);
-}
-
-void AddTableToXClusterTask::RefreshAndGetXClusterSafeTime() {
-  CHECK(bootstrap_time_.is_valid());
-  // Force a refresh of the xCluster safe time map so that it accounts for all tables under
-  // replication.
-  auto namespace_id = table_info_->namespace_id();
-  auto initial_safe_time = VERIFY_RESULT_AND_FAIL_TASK(
-      catalog_manager_->GetXClusterManager()->RefreshAndGetXClusterNamespaceToSafeTimeMap(
-          catalog_manager_->GetLeaderEpochInternal()));
-  if (!initial_safe_time.contains(namespace_id)) {
-    // Namespace is no longer part of any xCluster replication.
-    CompleteTableCreation();
-    return;
-  }
-
-  initial_xcluster_safe_time_ = initial_safe_time[namespace_id];
-  initial_xcluster_safe_time_.MakeAtLeast(bootstrap_time_);
-
-  // Wait for the xCluster safe time to advance beyond the initial value. This ensures all tables
-  // under replication are part of the safe time computation.
-  SCHEDULE_ON_REACTOR_AND_RETURN(WaitForXClusterSafeTimeCaughtUp);
-}
-
-void AddTableToXClusterTask::WaitForXClusterSafeTimeCaughtUp() {
-  if (initial_xcluster_safe_time_.is_valid()) {
-    auto ht = VERIFY_RESULT_AND_FAIL_TASK(
-        catalog_manager_->GetXClusterManager()->GetXClusterSafeTime(table_info_->namespace_id()));
-
-    auto caught_up = ht > initial_xcluster_safe_time_;
-    if (!caught_up) {
-      YB_LOG_EVERY_N_SECS(WARNING, 10) << LogPrefix() << "Waiting for xCluster safe time " << ht
-                                       << " to advance beyond " << initial_xcluster_safe_time_;
-      SCHEDULE_ON_REACTOR_AND_RETURN(WaitForXClusterSafeTimeCaughtUp);
-    }
-  }
-
-  // CompleteTableCreation requires a lock with wait. Since we cannot wait on the reactor thread,
-  // run it in background_tasks_thread_pool .
-  SCHEDULE_ON_ASYNC_POOL_AND_RETURN(CompleteTableCreation);
-}
-
-void AddTableToXClusterTask::CompleteTableCreation() {
-  FAIL_TASK_AND_RETURN_IF_NOT_OK(catalog_manager_->PromoteTableToRunningState(table_info_, epoch_));
-
-  LOG(INFO) << "Table " << table_info_->ToString()
-            << " successfully added to xcluster universe replication";
-
-  FinishTask(Status::OK());
-  TransitionState(server::MonitoredTaskState::kComplete);
-}
-
-void AddTableToXClusterTask::ScheduleOnReactor(
-    std::function<void()> task, std::string task_description) {
-  std::lock_guard l(schedule_task_mutex_);
-  if (!TransitionState(server::MonitoredTaskState::kWaiting)) {
-    VLOG_WITH_PREFIX_AND_FUNC(1) << "Task has been aborted";
-    return;
-  }
-
-  VLOG_WITH_PREFIX(2) << "Scheduling " << task_description << " on reactor";
-
-  auto task_id =
-      VERIFY_RESULT_AND_FAIL_TASK(catalog_manager_->master_->messenger()->ScheduleOnReactor(
-          std::bind(
-              &AddTableToXClusterTask::TransitionToRunningAndExecute, shared_from(this),
-              std::move(task), std::move(task_description)),
-          kDefaultReactorDelay, SOURCE_LOCATION()));
-
-  VLOG_WITH_PREFIX_AND_FUNC(2) << "Reactor task id: " << task_id;
-  reactor_task_id_ = task_id;
-}
-
-void AddTableToXClusterTask::ScheduleOnAsyncTaskPool(
-    std::function<void()> task, std::string task_description) {
-  std::lock_guard l(schedule_task_mutex_);
-  if (!TransitionState(server::MonitoredTaskState::kWaiting)) {
-    LOG_WITH_PREFIX_AND_FUNC(WARNING) << "Task has been aborted";
-    return;
-  }
-  VLOG_WITH_PREFIX(2) << "Scheduling " << task_description << " on async task pool";
-
-  const auto status = catalog_manager_->async_task_pool_->SubmitFunc(std::bind(
-      &AddTableToXClusterTask::TransitionToRunningAndExecute, shared_from(this), std::move(task),
-      std::move(task_description)));
-
-  FAIL_TASK_AND_RETURN_IF(
-      !status.ok(), status.CloneAndPrepend("Failed to submit task to async task pool"));
-}
-
-void AddTableToXClusterTask::TransitionToRunningAndExecute(
-    std::function<void()> task, std::string task_description) {
-  VLOG_WITH_PREFIX(2) << "Running " << task_description;
-  if (!TransitionState(server::MonitoredTaskState::kRunning)) {
-    LOG_WITH_PREFIX(WARNING) << "Task has been aborted";
-    return;
-  }
-
-  {
-    std::lock_guard l(schedule_task_mutex_);
-    reactor_task_id_ = rpc::kInvalidTaskId;
-  }
-
-  task();
-}
-
-void AddTableToXClusterTask::FinishTask(const Status& s) {
-  if (!s.ok()) {
-    table_info_->SetCreateTableErrorStatus(s);
-  }
-
-  table_info_->RemoveTask(shared_from_this());
-  completion_timestamp_ = MonoTime::Now();
-}
-
-void AddTableToXClusterTask::AbortReactorTaskIfScheduled() {
-  rpc::ScheduledTaskId reactor_task_id;
-  {
-    std::lock_guard l(schedule_task_mutex_);
-    reactor_task_id = reactor_task_id_;
-    reactor_task_id_ = rpc::kInvalidTaskId;
-  }
-
-  if (reactor_task_id != rpc::kInvalidTaskId) {
-    VLOG_WITH_PREFIX_AND_FUNC(2) << "Aborting reactor task id: " << reactor_task_id;
-    catalog_manager_->master_->messenger()->AbortOnReactor(reactor_task_id);
-  }
-}
-
-server::MonitoredTaskState AddTableToXClusterTask::AbortAndReturnPrevState(const Status& status) {
-  auto old_state = state();
-  while (!IsStateTerminal(old_state)) {
-    if (state_.compare_exchange_strong(old_state, server::MonitoredTaskState::kAborted)) {
-      VLOG_WITH_PREFIX_AND_FUNC(1) << "Aborted due to: " << status;
-      AbortReactorTaskIfScheduled();
-      FinishTask(status);
-      return old_state;
-    }
-    old_state = state();
-  }
-  return old_state;
-}
-
-bool AddTableToXClusterTask::TransitionState(server::MonitoredTaskState new_state) {
-  auto old_state = state();
-  while (!IsStateTerminal(old_state)) {
-    if (state_.compare_exchange_strong(old_state, new_state)) {
-      return true;
-    }
-    old_state = state();
-  }
-
-  return false;
-}
-
-}  // namespace yb::master
diff --git a/src/yb/master/xcluster/add_table_to_xcluster_task.h b/src/yb/master/xcluster/add_table_to_xcluster_task.h
deleted file mode 100644
index 79b64284fb..0000000000
--- a/src/yb/master/xcluster/add_table_to_xcluster_task.h
+++ /dev/null
@@ -1,83 +0,0 @@
-// Copyright (c) YugaByte, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
-// in compliance with the License.  You may obtain a copy of the License at
-//
-// http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software distributed under the License
-// is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
-// or implied.  See the License for the specific language governing permissions and limitations
-// under the License.
-//
-
-#pragma once
-
-#include <string>
-
-#include "yb/cdc/xcluster_types.h"
-#include "yb/client/client_fwd.h"
-#include "yb/common/hybrid_time.h"
-#include "yb/gutil/thread_annotations.h"
-#include "yb/master/leader_epoch.h"
-#include "yb/master/master_fwd.h"
-#include "yb/rpc/rpc_fwd.h"
-#include "yb/server/monitored_task.h"
-
-namespace yb::master {
-// This task adds a newly created table in the consumer xCluster universe to transactional
-// replication group. The table must be in PREPARING state with all tablets created at the start of
-// the task. The task performs bootstrap of the producer table, adds it to the replication group,
-// waits for the xCluster Safe Time to include the new table, and finally marks the table as
-// RUNNING.
-class AddTableToXClusterTask : public server::RunnableMonitoredTask {
- public:
-  AddTableToXClusterTask(
-      CatalogManager* catalog_manager, TableInfoPtr table_info, LeaderEpoch epoch);
-
-  server::MonitoredTaskType type() const override {
-    return server::MonitoredTaskType::kAddTableToXClusterReplication;
-  }
-  // Abort this task and return its value before it was successfully aborted. If the task entered
-  // a different terminal state before we were able to abort it, return that state.
-  server::MonitoredTaskState AbortAndReturnPrevState(const Status& status) override;
-
-  // Task Type Identifier.
-  std::string type_name() const override { return "Add Table to xCluster replication"; }
-
-  // Task description.
-  std::string description() const override;
-
-  Status Run() override;
-
- private:
-  std::string LogPrefix() { return Format("$0: ", description()); }
-
-  void RunInternal();
-  void BootstrapTableCallback(client::BootstrapProducerResult bootstrap_result);
-  void AddTableToReplicationGroup(TableId producer_table_id, std::string bootstrap_id);
-  void WaitForSetupUniverseReplicationToFinish();
-  void RefreshAndGetXClusterSafeTime();
-  void WaitForXClusterSafeTimeCaughtUp();
-  void CompleteTableCreation();
-
-  // Methods related to scheduling and task completion.
-  // If the task needs to run with a delay we schedule on the reactor. If the task needs to run
-  // immediately or waits on locks we schedule on the background thread.
-  void ScheduleOnReactor(std::function<void()> task, std::string task_description);
-  void ScheduleOnAsyncTaskPool(std::function<void()> task, std::string task_description);
-  void TransitionToRunningAndExecute(std::function<void()> task, std::string task_description);
-  void AbortReactorTaskIfScheduled();
-  void FinishTask(const Status& s);
-  bool TransitionState(server::MonitoredTaskState new_state);
-
-  CatalogManager* catalog_manager_;
-  TableInfoPtr table_info_;
-  std::mutex schedule_task_mutex_;
-  rpc::ScheduledTaskId reactor_task_id_ GUARDED_BY(schedule_task_mutex_) = rpc::kInvalidTaskId;
-  HybridTime bootstrap_time_ = HybridTime::kInvalid;
-  HybridTime initial_xcluster_safe_time_ = HybridTime::kInvalid;
-  xcluster::ReplicationGroupId replication_group_id_;
-  LeaderEpoch epoch_;
-};
-}  // namespace yb::master
diff --git a/src/yb/master/xcluster/xcluster_manager.cc b/src/yb/master/xcluster/xcluster_manager.cc
index 7ebdbe918b..5d641bafe2 100644
--- a/src/yb/master/xcluster/xcluster_manager.cc
+++ b/src/yb/master/xcluster/xcluster_manager.cc
@@ -17,9 +17,12 @@
 
 #include "yb/common/hybrid_time.h"
 
-#include "yb/master/master.h"
+#include "yb/master/catalog_entity_info.h"
 #include "yb/master/master_cluster.pb.h"
 #include "yb/master/ts_descriptor.h"
+#include "yb/master/master.h"
+#include "yb/master/post_tablet_create_task_base.h"
+#include "yb/master/xcluster/add_table_to_xcluster_target_task.h"
 #include "yb/master/xcluster/xcluster_config.h"
 #include "yb/master/xcluster/xcluster_safe_time_service.h"
 
@@ -30,6 +33,9 @@
 
 #include "yb/cdc/cdc_service.proxy.h"
 
+DEFINE_RUNTIME_bool(disable_auto_add_index_to_xcluster, false,
+    "Disables the automatic addition of indexes to transactional xCluster replication.");
+
 DEFINE_test_flag(bool, enable_xcluster_api_v2, false, "Allow the usage of new xCluster APIs");
 
 using namespace std::placeholders;
@@ -556,4 +562,102 @@ Result<std::vector<xrepl::StreamId>> XClusterManager::BootstrapTables(
   return stream_ids;
 }
 
+bool XClusterManager::ShouldAddTableToXClusterTarget(const TableInfo& table) const {
+  if (FLAGS_disable_auto_add_index_to_xcluster) {
+    return false;
+  }
+
+  const auto& pb = table.metadata().dirty().pb;
+
+  // Only user created YSQL Indexes should be automatically added to xCluster replication.
+  // For Colocated tables, this function will return false since it is only called on the parent
+  // colocated table, which cannot be an index.
+  if (pb.colocated() || pb.table_type() != PGSQL_TABLE_TYPE || !IsIndex(pb) ||
+      !catalog_manager_->IsUserCreatedTable(table)) {
+    return false;
+  }
+
+  auto indexed_table_stream_ids =
+      catalog_manager_->GetXClusterConsumerStreamIdsForTable(table.id());
+  if (!indexed_table_stream_ids.empty()) {
+    VLOG(1) << "Index " << table.ToString() << " is already part of xcluster replication "
+            << yb::ToString(indexed_table_stream_ids);
+    return false;
+  }
+
+  auto indexed_table = catalog_manager_->GetTableInfo(GetIndexedTableId(pb));
+  if (!indexed_table) {
+    LOG(WARNING) << "Indexed table for " << table.id() << " not found";
+    return false;
+  }
+
+  auto stream_ids = catalog_manager_->GetXClusterConsumerStreamIdsForTable(indexed_table->id());
+  if (stream_ids.empty()) {
+    return false;
+  }
+
+  if (stream_ids.size() > 1) {
+    LOG(WARNING) << "Skipping adding index " << table.ToString()
+                 << " to xCluster replication as the base table" << indexed_table->ToString()
+                 << " is part of multiple replication streams " << yb::ToString(stream_ids);
+    return false;
+  }
+
+  const auto& replication_group_id = stream_ids.begin()->first;
+  auto cluster_config = catalog_manager_->ClusterConfig();
+  {
+    auto l = cluster_config->LockForRead();
+    const auto& consumer_registry = l.data().pb.consumer_registry();
+    // Only add if we are in a transactional replication with STANDBY mode.
+    if (consumer_registry.role() != cdc::XClusterRole::STANDBY ||
+        !consumer_registry.transactional()) {
+      return false;
+    }
+
+    auto producer_entry =
+        FindOrNull(consumer_registry.producer_map(), replication_group_id.ToString());
+    if (producer_entry) {
+      // Check if the table is already part of replication.
+      // This is needed despite the check for GetXClusterConsumerStreamIdsForTable as the in-memory
+      // list is not atomically updated.
+      for (auto& stream_info : producer_entry->stream_map()) {
+        if (stream_info.second.consumer_table_id() == table.id()) {
+          VLOG(1) << "Index " << table.ToString() << " is already part of xcluster replication "
+                  << stream_info.first;
+          return false;
+        }
+      }
+    }
+  }
+
+  scoped_refptr<UniverseReplicationInfo> universe;
+  {
+    auto universe = catalog_manager_->GetUniverseReplication(replication_group_id);
+    if (universe == nullptr) {
+      LOG(WARNING) << "Skip adding index " << table.ToString()
+                   << " to xCluster replication as the universe " << replication_group_id
+                   << " was not found";
+      return false;
+    }
+
+    if (universe->LockForRead()->is_deleted_or_failed()) {
+      LOG(WARNING) << "Skip adding index " << table.ToString()
+                   << " to xCluster replication as the universe " << replication_group_id
+                   << " is in a deleted or failed state";
+      return false;
+    }
+  }
+
+  return true;
+}
+
+std::vector<std::shared_ptr<PostTabletCreateTaskBase>> XClusterManager::GetPostTabletCreateTasks(
+    const TableInfoPtr& table_info, const LeaderEpoch& epoch) {
+  if (!ShouldAddTableToXClusterTarget(*table_info)) {
+    return {};
+  }
+
+  return {std::make_shared<AddTableToXClusterTargetTask>(
+      *catalog_manager_, *master_->messenger(), table_info, epoch)};
+}
 }  // namespace yb::master
diff --git a/src/yb/master/xcluster/xcluster_manager.h b/src/yb/master/xcluster/xcluster_manager.h
index d54bf79dc4..e76dbf4aee 100644
--- a/src/yb/master/xcluster/xcluster_manager.h
+++ b/src/yb/master/xcluster/xcluster_manager.h
@@ -31,6 +31,7 @@ class CDCStreamInfo;
 class GetMasterXClusterConfigResponsePB;
 class PauseResumeXClusterProducerStreamsRequestPB;
 class PauseResumeXClusterProducerStreamsResponsePB;
+class PostTabletCreateTaskBase;
 class TSHeartbeatRequestPB;
 class TSHeartbeatResponsePB;
 class XClusterConfig;
@@ -92,6 +93,9 @@ class XClusterManager : public XClusterManagerIf {
   Result<XClusterNamespaceToSafeTimeMap> RefreshAndGetXClusterNamespaceToSafeTimeMap(
       const LeaderEpoch& epoch) override;
 
+  std::vector<std::shared_ptr<PostTabletCreateTaskBase>> GetPostTabletCreateTasks(
+      const TableInfoPtr& table_info, const LeaderEpoch& epoch);
+
   XClusterSafeTimeService* TEST_xcluster_safe_time_service() {
     return xcluster_safe_time_service_.get();
   }
@@ -151,6 +155,8 @@ class XClusterManager : public XClusterManagerIf {
   Result<std::vector<xrepl::StreamId>> BootstrapTables(
       const std::vector<TableInfoPtr>& table_infos, CoarseTimePoint deadline);
 
+  bool ShouldAddTableToXClusterTarget(const TableInfo& table_info) const;
+
   Master* const master_;
   CatalogManager* const catalog_manager_;
   SysCatalogTable* const sys_catalog_;
diff --git a/src/yb/master/xrepl_catalog_manager.cc b/src/yb/master/xrepl_catalog_manager.cc
index 45c9bbab7a..c6dfb867e1 100644
--- a/src/yb/master/xrepl_catalog_manager.cc
+++ b/src/yb/master/xrepl_catalog_manager.cc
@@ -102,9 +102,6 @@ DEFINE_RUNTIME_int32(cdc_parent_tablet_deletion_task_retry_secs, 30,
     "Frequency at which the background task will verify parent tablets retained for xCluster or "
     "CDCSDK replication and determine if they can be cleaned up.");
 
-DEFINE_RUNTIME_bool(disable_auto_add_index_to_xcluster, false,
-    "Disables the automatic addition of indexes to transactional xCluster replication.");
-
 DEFINE_NON_RUNTIME_uint32(max_replication_slots, 10,
     "Controls the maximum number of replication slots that are allowed to exist.");
 
@@ -7111,95 +7108,6 @@ Status CatalogManager::BumpVersionAndStoreClusterConfig(
   return Status::OK();
 }
 
-bool CatalogManager::ShouldAddTableToXClusterReplication(
-    const TableInfo& table, const SysTablesEntryPB& pb) {
-  if (FLAGS_disable_auto_add_index_to_xcluster) {
-    return false;
-  }
-
-  // Only user created YSQL Indexes should be automatically added to xCluster replication.
-  // For Colocated tables, this function will return false since it is only called on the parent
-  // colocated table which cannot be an index.
-  if (pb.colocated() || pb.table_type() != PGSQL_TABLE_TYPE || !IsIndex(pb) ||
-      !IsUserCreatedTable(table)) {
-    return false;
-  }
-
-  auto indexed_table_stream_ids = GetXClusterConsumerStreamIdsForTable(table.id());
-  if (!indexed_table_stream_ids.empty()) {
-    VLOG(1) << "Index " << table.ToString() << " is already part of xcluster replication "
-            << yb::ToString(indexed_table_stream_ids);
-    return false;
-  }
-
-  auto indexed_table = GetTableInfo(GetIndexedTableId(pb));
-  if (!indexed_table) {
-    LOG(WARNING) << "Indexed table for " << table.id() << " not found";
-    return false;
-  }
-
-  auto stream_ids = GetXClusterConsumerStreamIdsForTable(indexed_table->id());
-  if (stream_ids.empty()) {
-    return false;
-  }
-
-  if (stream_ids.size() > 1) {
-    LOG(WARNING) << "Skip adding index " << table.ToString()
-                 << " to xCluster replication as the base table" << indexed_table->ToString()
-                 << " is part of multiple replication streams " << yb::ToString(stream_ids);
-    return false;
-  }
-
-  const auto& replication_group_id = stream_ids.begin()->first;
-  auto cluster_config = ClusterConfig();
-  {
-    auto l = cluster_config->LockForRead();
-    auto consumer_registry = l.data().pb.consumer_registry();
-    // Only add if we are in a transactional replication with STANDBY mode.
-    if (consumer_registry.role() != cdc::XClusterRole::STANDBY ||
-        !consumer_registry.transactional()) {
-      return false;
-    }
-
-    auto producer_entry =
-        FindOrNull(consumer_registry.producer_map(), replication_group_id.ToString());
-    if (producer_entry) {
-      // Check if the table is already part of replication.
-      // This is needed despite the check for GetXClusterConsumerStreamIdsForTable as the in-memory
-      // list is not atomically updated.
-      for (auto& stream_info : producer_entry->stream_map()) {
-        if (stream_info.second.consumer_table_id() == table.id()) {
-          VLOG(1) << "Index " << table.ToString() << " is already part of xcluster replication "
-                  << stream_info.first;
-          return false;
-        }
-      }
-    }
-  }
-
-  scoped_refptr<UniverseReplicationInfo> universe;
-  {
-    TRACE("Acquired catalog manager lock");
-    SharedLock lock(mutex_);
-    universe = FindPtrOrNull(universe_replication_map_, replication_group_id);
-    if (universe == nullptr) {
-      LOG(WARNING) << "Skip adding index " << table.ToString()
-                   << " to xCluster replication as the universe " << replication_group_id
-                   << " was not found";
-      return false;
-    }
-
-    if (universe->LockForRead()->is_deleted_or_failed()) {
-      LOG(WARNING) << "Skip adding index " << table.ToString()
-                   << " to xCluster replication as the universe " << replication_group_id
-                   << " is in a deleted or failed state";
-      return false;
-    }
-  }
-
-  return true;
-}
-
 Result<xcluster::ReplicationGroupId> CatalogManager::GetIndexesTableReplicationGroup(
     const TableInfo& index_info) {
   const auto indexed_table_id = GetIndexedTableId(index_info.LockForRead()->pb);
@@ -7516,5 +7424,12 @@ Status CatalogManager::XClusterRefreshLocalAutoFlagConfig(const LeaderEpoch& epo
   return Status::OK();
 }
 
+scoped_refptr<UniverseReplicationInfo> CatalogManager::GetUniverseReplication(
+    const xcluster::ReplicationGroupId& replication_group_id) {
+  SharedLock lock(mutex_);
+  TRACE("Acquired catalog manager lock");
+  return FindPtrOrNull(universe_replication_map_, replication_group_id);
+}
+
 }  // namespace master
 }  // namespace yb
diff --git a/src/yb/master/yql_partitions_vtable.cc b/src/yb/master/yql_partitions_vtable.cc
index 27b6263448..353eaa443c 100644
--- a/src/yb/master/yql_partitions_vtable.cc
+++ b/src/yb/master/yql_partitions_vtable.cc
@@ -297,7 +297,7 @@ bool HasRelevantPbChanges(const SysTabletsEntryPB& old_pb, const SysTabletsEntry
   return false;
 }
 
-Result<std::vector<TabletInfoPtr>> YQLPartitionsVTable::FilterRelevantTablets(
+std::vector<TabletInfoPtr> YQLPartitionsVTable::FilterRelevantTablets(
     const std::vector<TabletInfo*>& mutated_tablets) const {
   std::vector<TabletInfoPtr> tablets;
   if (!ShouldGeneratePartitionsVTableOnChanges()) {
diff --git a/src/yb/master/yql_partitions_vtable.h b/src/yb/master/yql_partitions_vtable.h
index 1ab950f371..9152015b31 100644
--- a/src/yb/master/yql_partitions_vtable.h
+++ b/src/yb/master/yql_partitions_vtable.h
@@ -38,7 +38,7 @@ class YQLPartitionsVTable : public YQLVirtualTable {
 
   // Filter only tablets that have relevant system.partitions changes from a list of tablets that
   // have heartbeated in and are being processed.
-  Result<std::vector<TabletInfoPtr>> FilterRelevantTablets(
+  std::vector<TabletInfoPtr> FilterRelevantTablets(
       const std::vector<TabletInfo*>& mutated_tablets) const;
 
   // Process a filtered list of tablets and add them to the system.partitions vtable.
diff --git a/src/yb/server/monitored_task.h b/src/yb/server/monitored_task.h
index 57f9d7ac28..167ca70d7d 100644
--- a/src/yb/server/monitored_task.h
+++ b/src/yb/server/monitored_task.h
@@ -35,6 +35,7 @@
 #include <memory>
 #include <string>
 #include <type_traits>
+#include <unordered_set>
 
 #include "yb/gutil/ref_counted.h"
 
@@ -82,7 +83,8 @@ YB_DEFINE_ENUM(MonitoredTaskType,
   (kTruncateTablet)
   (kTryStepDown)
   (kUpdateTransactionTablesVersion)
-  (kAddTableToXClusterReplication));
+  (kAddTableToXClusterTarget)
+  (kMarkTableAsRunning));
 
 class MonitoredTask : public std::enable_shared_from_this<MonitoredTask> {
  public:
@@ -95,7 +97,7 @@ class MonitoredTask : public std::enable_shared_from_this<MonitoredTask> {
   virtual MonitoredTaskState AbortAndReturnPrevState(const Status& status) = 0;
 
   // Task State.
-  virtual MonitoredTaskState state() const { return state_.load(std::memory_order_acquire); }
+  MonitoredTaskState state() const { return state_.load(std::memory_order_acquire); }
 
   virtual MonitoredTaskType type() const = 0;
 
