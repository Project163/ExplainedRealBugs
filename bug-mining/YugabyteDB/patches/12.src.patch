diff --git a/src/yb/cdc/cdc_types.h b/src/yb/cdc/cdc_types.h
index 05517de4e9..d54e8d5910 100644
--- a/src/yb/cdc/cdc_types.h
+++ b/src/yb/cdc/cdc_types.h
@@ -39,11 +39,6 @@ static const char* const kTableId = "TABLEID";
 
 YB_STRONGLY_TYPED_STRING(ReplicationGroupId);
 
-// Maps a tablet id -> stream id -> replication error -> error detail.
-typedef std::unordered_map<ReplicationErrorPb, std::string> ReplicationErrorMap;
-typedef std::unordered_map<xrepl::StreamId, ReplicationErrorMap> StreamReplicationErrorMap;
-typedef std::unordered_map<TabletId, StreamReplicationErrorMap> TabletReplicationErrorMap;
-
 typedef std::unordered_map<SchemaVersion, SchemaVersion> XClusterSchemaVersionMap;
 typedef std::unordered_map<uint32_t, XClusterSchemaVersionMap> ColocatedSchemaVersionMap;
 typedef std::unordered_map<xrepl::StreamId, XClusterSchemaVersionMap> StreamSchemaVersionMap;
diff --git a/src/yb/cdc/xcluster_types.h b/src/yb/cdc/xcluster_types.h
new file mode 100644
index 0000000000..1c11da7b77
--- /dev/null
+++ b/src/yb/cdc/xcluster_types.h
@@ -0,0 +1,30 @@
+// Copyright (c) YugabyteDB, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
+// in compliance with the License.  You may obtain a copy of the License at
+//
+// http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software distributed under the License
+// is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
+// or implied.  See the License for the specific language governing permissions and limitations
+// under the License.
+//
+
+#pragma once
+
+#include "yb/common/common_types.pb.h"
+#include "yb/common/entity_ids_types.h"
+
+namespace yb::xcluster {
+
+struct TabletReplicationError {
+  int64_t consumer_term = 0;
+  ReplicationErrorPb error;
+};
+
+// Maps Consumer TableId, Producer TabletId to an error. This is per Replication Group.
+using ReplicationGroupErrors =
+    std::unordered_map<TableId, std::unordered_map<TabletId, TabletReplicationError>>;
+
+}  // namespace yb::xcluster
diff --git a/src/yb/common/common_types.proto b/src/yb/common/common_types.proto
index cbd99e8f84..f6f8b7d88b 100644
--- a/src/yb/common/common_types.proto
+++ b/src/yb/common/common_types.proto
@@ -149,6 +149,12 @@ enum ReplicationErrorPb {
 
   // There is a colocated table missing on the consumer side.
   REPLICATION_MISSING_TABLE = 3;
+
+  // Replication state has not yet been initialized.
+  REPLICATION_ERROR_UNINITIALIZED = 4;
+
+  // Replication is healthy.
+  REPLICATION_OK = 5;
 }
 
 // Stateful services.
diff --git a/src/yb/integration-tests/cdc_test_util.cc b/src/yb/integration-tests/cdc_test_util.cc
index e270f78152..0e683c77b6 100644
--- a/src/yb/integration-tests/cdc_test_util.cc
+++ b/src/yb/integration-tests/cdc_test_util.cc
@@ -23,7 +23,7 @@
 #include "yb/tablet/tablet_metadata.h"
 #include "yb/tablet/tablet_peer.h"
 
-#include "yb/tserver/xcluster_consumer.h"
+#include "yb/tserver/xcluster_consumer_if.h"
 #include "yb/tserver/mini_tablet_server.h"
 #include "yb/tserver/tablet_server.h"
 #include "yb/tserver/ts_tablet_manager.h"
@@ -106,7 +106,7 @@ size_t NumProducerTabletsPolled(MiniCluster* cluster) {
   for (const auto& mini_tserver : cluster->mini_tablet_servers()) {
     size_t new_size = 0;
     auto* tserver = mini_tserver->server();
-    tserver::XClusterConsumer* xcluster_consumer;
+    tserver::XClusterConsumerIf* xcluster_consumer;
     if (tserver && (xcluster_consumer = tserver->GetXClusterConsumer()) &&
         mini_tserver->is_started()) {
       auto tablets_running = xcluster_consumer->TEST_producer_tablets_running();
diff --git a/src/yb/integration-tests/xcluster/xcluster-tablet-split-itest.cc b/src/yb/integration-tests/xcluster/xcluster-tablet-split-itest.cc
index afdfa099f5..a15f7e92a5 100644
--- a/src/yb/integration-tests/xcluster/xcluster-tablet-split-itest.cc
+++ b/src/yb/integration-tests/xcluster/xcluster-tablet-split-itest.cc
@@ -39,7 +39,6 @@
 #include "yb/tablet/tablet_metadata.h"
 #include "yb/tablet/tablet_peer.h"
 #include "yb/tools/admin-test-base.h"
-#include "yb/tserver/xcluster_consumer.h"
 #include "yb/tserver/mini_tablet_server.h"
 #include "yb/tserver/tablet_server.h"
 #include "yb/util/backoff_waiter.h"
diff --git a/src/yb/integration-tests/xcluster/xcluster-test.cc b/src/yb/integration-tests/xcluster/xcluster-test.cc
index 86ccfa8765..00e37449c6 100644
--- a/src/yb/integration-tests/xcluster/xcluster-test.cc
+++ b/src/yb/integration-tests/xcluster/xcluster-test.cc
@@ -69,12 +69,12 @@
 #include "yb/tablet/tablet_peer.h"
 #include "yb/tablet/tablet.h"
 
+#include "yb/tserver/xcluster_consumer_if.h"
+#include "yb/tserver/xcluster_poller.h"
 #include "yb/tserver/mini_tablet_server.h"
 #include "yb/tserver/tablet_server.h"
 #include "yb/tserver/ts_tablet_manager.h"
 #include "yb/tserver/tserver_service.pb.h"
-#include "yb/tserver/xcluster_consumer.h"
-#include "yb/tserver/xcluster_poller.h"
 
 #include "yb/util/atomic.h"
 #include "yb/util/backoff_waiter.h"
@@ -299,66 +299,6 @@ class XClusterTestNoParam : public XClusterYcqlTestBase {
 
   Status SetupReplication() { return SetupUniverseReplication(producer_tables_); }
 
-  Status VerifyReplicationError(
-      const std::string& consumer_table_id, const xrepl::StreamId& stream_id,
-      const boost::optional<ReplicationErrorPb> expected_replication_error) {
-    // 1. Verify that the RPC contains the expected error.
-    master::GetReplicationStatusRequestPB req;
-    master::GetReplicationStatusResponsePB resp;
-
-    req.set_replication_group_id(kReplicationGroupId.ToString());
-
-    auto master_proxy = std::make_shared<master::MasterReplicationProxy>(
-        &consumer_client()->proxy_cache(),
-        VERIFY_RESULT(consumer_cluster()->GetLeaderMiniMaster())->bound_rpc_addr());
-
-    rpc::RpcController rpc;
-    RETURN_NOT_OK(WaitFor(
-        [&]() -> Result<bool> {
-          rpc.Reset();
-          rpc.set_timeout(MonoDelta::FromSeconds(kRpcTimeout));
-          if (!master_proxy->GetReplicationStatus(req, &resp, &rpc).ok()) {
-            return false;
-          }
-
-          if (resp.has_error()) {
-            return false;
-          }
-
-          if (resp.statuses_size() == 0 ||
-              (resp.statuses()[0].table_id() != consumer_table_id &&
-               resp.statuses()[0].stream_id() != stream_id.ToString())) {
-            return false;
-          }
-
-          if (expected_replication_error) {
-            return resp.statuses()[0].errors_size() == 1 &&
-                   resp.statuses()[0].errors()[0].error() == *expected_replication_error;
-          } else {
-            return resp.statuses()[0].errors_size() == 0;
-          }
-        },
-        MonoDelta::FromSeconds(30), "Waiting for replication error"));
-
-    // 2. Verify that the yb-admin output contains the expected error.
-    auto admin_out =
-        VERIFY_RESULT(CallAdmin(consumer_cluster(), "get_replication_status", kReplicationGroupId));
-    if (expected_replication_error) {
-      SCHECK_FORMAT(
-          admin_out.find(
-              Format("error: $0", ReplicationErrorPb_Name(*expected_replication_error))) !=
-              std::string::npos,
-          IllegalState, "Expected error '$0' not found in yb-admin output '$1'",
-          expected_replication_error, admin_out);
-    } else {
-      SCHECK_FORMAT(
-          admin_out.find("error:") == std::string::npos, IllegalState,
-          "Unexpected error found in yb-admin output '$0'", admin_out);
-    }
-
-    return Status::OK();
-  }
-
   Result<xrepl::StreamId> GetCDCStreamID(const std::string& producer_table_id) {
     master::ListCDCStreamsResponsePB stream_resp;
     RETURN_NOT_OK(GetCDCStreamForTable(producer_table_id, &stream_resp));
@@ -2374,8 +2314,7 @@ TEST_P(XClusterTest, TestAlterDDLBasic) {
   ASSERT_OK(VerifyRowsMatch());
 
   // Verify that the replication status for the consumer table does not contain an error.
-  ASSERT_OK(VerifyReplicationError(
-      consumer_table_->id(), stream_id, boost::optional<ReplicationErrorPb>()));
+  ASSERT_OK(VerifyReplicationError(consumer_table_->id(), stream_id, std::nullopt));
 
   // Stop replication on the Consumer.
   ASSERT_OK(DeleteUniverseReplication());
@@ -3257,7 +3196,7 @@ TEST_P(XClusterTestWaitForReplicationDrain, TestProducerChange) {
   ASSERT_OK(drain_api_future.get());
 }
 
-TEST_P(XClusterTest, TestPrematureLogGC) {
+TEST_F_EX(XClusterTest, TestPrematureLogGC, XClusterTestNoParam) {
   // Allow WAL segments to be garbage collected regardless of their lifetime.
   ANNOTATE_UNPROTECTED_WRITE(FLAGS_TEST_disable_wal_retention_time) = true;
 
@@ -3876,4 +3815,52 @@ TEST_F_EX(XClusterTest, TestStats, XClusterTestNoParam) {
   ASSERT_EQ(source_stats[0].mbs_sent, target_stats[0].mbs_received);
   ASSERT_EQ(source_stats[0].sent_index, target_stats[0].received_index);
 }
+
+TEST_F_EX(XClusterTest, VerifyReplicationError, XClusterTestNoParam) {
+  // Disable polling so that errors don't get cleared by successful polls.
+  ANNOTATE_UNPROTECTED_WRITE(FLAGS_TEST_cdc_skip_replication_poll) = true;
+  ASSERT_OK(SetUpWithParams(
+      {1}, {1}, /* replication_factor */ 1, /* num_masters */ 3, /* num_tservers */ 1));
+  ASSERT_OK(SetupReplication());
+  ASSERT_OK(CorrectlyPollingAllTablets(1));
+  const auto stream_id = ASSERT_RESULT(GetCDCStreamID(producer_table_->id()));
+  ASSERT_OK(VerifyReplicationError(
+      consumer_table_->id(), stream_id, ReplicationErrorPb::REPLICATION_ERROR_UNINITIALIZED));
+
+  const auto replication_error1 = ReplicationErrorPb::REPLICATION_SCHEMA_MISMATCH;
+  const auto replication_error2 = ReplicationErrorPb::REPLICATION_MISSING_OP_ID;
+
+  // Store an error in the Poller.
+  auto xcluster_consumer =
+      consumer_cluster()->mini_tablet_server(0)->server()->GetXClusterConsumer();
+  auto pollers = xcluster_consumer->TEST_ListPollers();
+  ASSERT_EQ(pollers.size(), 1);
+  auto poller = pollers[0];
+  poller->StoreReplicationError(replication_error1);
+
+  // Verify the error propagated to master.
+  ASSERT_OK(VerifyReplicationError(consumer_table_->id(), stream_id, replication_error1));
+
+  // Verify the error persists across master fail overs and restarts.
+  auto old_master = ASSERT_RESULT(producer_cluster()->GetLeaderMiniMaster());
+  ASSERT_OK(producer_cluster()->StepDownMasterLeader());
+  ASSERT_OK(VerifyReplicationError(consumer_table_->id(), stream_id, replication_error1));
+
+  // Set a different error in the Poller.
+  poller->StoreReplicationError(replication_error2);
+
+  // Verify the new error propagated to the master.
+  ASSERT_OK(VerifyReplicationError(consumer_table_->id(), stream_id, replication_error2));
+
+  // Fallback to old master and make sure it is updated.
+  ASSERT_OK(producer_cluster()->StepDownMasterLeader(old_master->permanent_uuid()));
+  ASSERT_OK(VerifyReplicationError(consumer_table_->id(), stream_id, replication_error2));
+
+  // Clear the error in the Poller.
+  poller->StoreReplicationError(ReplicationErrorPb::REPLICATION_OK);
+
+  // Verify the error is cleared in the master.
+  ASSERT_OK(VerifyReplicationError(consumer_table_->id(), stream_id, std::nullopt));
+}
+
 }  // namespace yb
diff --git a/src/yb/integration-tests/xcluster/xcluster_test_base.cc b/src/yb/integration-tests/xcluster/xcluster_test_base.cc
index 144bbeaa94..b2f84182da 100644
--- a/src/yb/integration-tests/xcluster/xcluster_test_base.cc
+++ b/src/yb/integration-tests/xcluster/xcluster_test_base.cc
@@ -36,7 +36,7 @@
 #include "yb/master/master_replication.proxy.h"
 #include "yb/master/mini_master.h"
 #include "yb/rpc/rpc_controller.h"
-#include "yb/tserver/xcluster_consumer.h"
+#include "yb/tserver/xcluster_consumer_if.h"
 #include "yb/tserver/mini_tablet_server.h"
 #include "yb/tserver/tablet_server.h"
 #include "yb/util/backoff_waiter.h"
@@ -60,7 +60,6 @@ namespace yb {
 
 using client::YBClient;
 using client::YBTableName;
-using tserver::XClusterConsumer;
 
 Status XClusterTestBase::PostSetUp() {
   if (!producer_tables_.empty()) {
@@ -545,7 +544,7 @@ uint32_t XClusterTestBase::GetSuccessfulWriteOps(MiniCluster* cluster) {
   uint32_t size = 0;
   for (const auto& mini_tserver : cluster->mini_tablet_servers()) {
     auto* tserver = mini_tserver->server();
-    XClusterConsumer* xcluster_consumer;
+    tserver::XClusterConsumerIf* xcluster_consumer;
     if (tserver && (xcluster_consumer = tserver->GetXClusterConsumer())) {
       size += xcluster_consumer->TEST_GetNumSuccessfulWriteRpcs();
     }
@@ -784,11 +783,9 @@ Status XClusterTestBase::WaitForReplicationDrain(
   return Status::OK();
 }
 
-void XClusterTestBase::VerifyReplicationError(
-    const std::string& consumer_table_id,
-    const std::string& stream_id,
-    const boost::optional<ReplicationErrorPb>
-        expected_replication_error) {
+Status XClusterTestBase::VerifyReplicationError(
+    const std::string& consumer_table_id, const xrepl::StreamId& stream_id,
+    const std::optional<ReplicationErrorPb> expected_replication_error) {
   // 1. Verify that the RPC contains the expected error.
   master::GetReplicationStatusRequestPB req;
   master::GetReplicationStatusResponsePB resp;
@@ -797,10 +794,10 @@ void XClusterTestBase::VerifyReplicationError(
 
   auto master_proxy = std::make_shared<master::MasterReplicationProxy>(
       &consumer_client()->proxy_cache(),
-      ASSERT_RESULT(consumer_cluster()->GetLeaderMiniMaster())->bound_rpc_addr());
+      VERIFY_RESULT(consumer_cluster()->GetLeaderMiniMaster())->bound_rpc_addr());
 
   rpc::RpcController rpc;
-  ASSERT_OK(WaitFor(
+  RETURN_NOT_OK(WaitFor(
       [&]() -> Result<bool> {
         rpc.Reset();
         rpc.set_timeout(MonoDelta::FromSeconds(kRpcTimeout));
@@ -813,7 +810,7 @@ void XClusterTestBase::VerifyReplicationError(
         }
 
         if (resp.statuses_size() == 0 || (resp.statuses()[0].table_id() != consumer_table_id &&
-                                          resp.statuses()[0].stream_id() != stream_id)) {
+                                          resp.statuses()[0].stream_id() != stream_id.ToString())) {
           return false;
         }
 
@@ -828,14 +825,20 @@ void XClusterTestBase::VerifyReplicationError(
 
   // 2. Verify that the yb-admin output contains the expected error.
   auto admin_out =
-      ASSERT_RESULT(CallAdmin(consumer_cluster(), "get_replication_status", kReplicationGroupId));
+      VERIFY_RESULT(CallAdmin(consumer_cluster(), "get_replication_status", kReplicationGroupId));
   if (expected_replication_error) {
-    ASSERT_TRUE(
+    SCHECK_FORMAT(
         admin_out.find(Format("error: $0", ReplicationErrorPb_Name(*expected_replication_error))) !=
-        std::string::npos);
+            std::string::npos,
+        IllegalState, "Expected error '$0' not found in yb-admin output '$1'",
+        expected_replication_error, admin_out);
   } else {
-    ASSERT_TRUE(admin_out.find("error:") == std::string::npos);
+    SCHECK_FORMAT(
+        admin_out.find("error:") == std::string::npos, IllegalState,
+        "Unexpected error found in yb-admin output '$0'", admin_out);
   }
+
+  return Status::OK();
 }
 
 Result<xrepl::StreamId> XClusterTestBase::GetCDCStreamID(const TableId& producer_table_id) {
@@ -990,5 +993,4 @@ Result<TableId> XClusterTestBase::GetColocatedDatabaseParentTableId() {
   // Colocated database
   return GetTablegroupParentTable(&producer_cluster_, namespace_name);
 }
-
 } // namespace yb
diff --git a/src/yb/integration-tests/xcluster/xcluster_test_base.h b/src/yb/integration-tests/xcluster/xcluster_test_base.h
index 990882aa23..75c4dd8bb4 100644
--- a/src/yb/integration-tests/xcluster/xcluster_test_base.h
+++ b/src/yb/integration-tests/xcluster/xcluster_test_base.h
@@ -315,11 +315,9 @@ class XClusterTestBase : public YBTest {
 
   Status WaitForSafeTime(const NamespaceId& namespace_id, const HybridTime& min_safe_time);
 
-  void VerifyReplicationError(
-      const std::string& consumer_table_id,
-      const std::string& stream_id,
-      const boost::optional<ReplicationErrorPb>
-          expected_replication_error);
+  Status VerifyReplicationError(
+      const std::string& consumer_table_id, const xrepl::StreamId& stream_id,
+      const std::optional<ReplicationErrorPb> expected_replication_error);
 
   Result<xrepl::StreamId> GetCDCStreamID(const TableId& producer_table_id);
 
diff --git a/src/yb/integration-tests/xcluster/xcluster_topologies-test.cc b/src/yb/integration-tests/xcluster/xcluster_topologies-test.cc
index 4568629ada..4040ca9fec 100644
--- a/src/yb/integration-tests/xcluster/xcluster_topologies-test.cc
+++ b/src/yb/integration-tests/xcluster/xcluster_topologies-test.cc
@@ -43,7 +43,6 @@
 #include "yb/master/xcluster_consumer_registry_service.h"
 #include "yb/rpc/rpc_controller.h"
 #include "yb/server/hybrid_clock.h"
-#include "yb/tserver/xcluster_consumer.h"
 #include "yb/tserver/mini_tablet_server.h"
 
 #include "yb/util/atomic.h"
diff --git a/src/yb/integration-tests/xcluster/xcluster_ycql_test_base.cc b/src/yb/integration-tests/xcluster/xcluster_ycql_test_base.cc
index 7cac3ecc12..7519248349 100644
--- a/src/yb/integration-tests/xcluster/xcluster_ycql_test_base.cc
+++ b/src/yb/integration-tests/xcluster/xcluster_ycql_test_base.cc
@@ -44,7 +44,6 @@
 #include "yb/master/xcluster_consumer_registry_service.h"
 #include "yb/rpc/rpc_controller.h"
 #include "yb/server/hybrid_clock.h"
-#include "yb/tserver/xcluster_consumer.h"
 #include "yb/tserver/mini_tablet_server.h"
 
 #include "yb/util/atomic.h"
diff --git a/src/yb/integration-tests/xcluster/xcluster_ysql-test.cc b/src/yb/integration-tests/xcluster/xcluster_ysql-test.cc
index 1ff563791c..9aae2c3ea1 100644
--- a/src/yb/integration-tests/xcluster/xcluster_ysql-test.cc
+++ b/src/yb/integration-tests/xcluster/xcluster_ysql-test.cc
@@ -1314,7 +1314,7 @@ TEST_F(XClusterYsqlTest, ReplicationWithBasicDDL) {
 
   // 2. Setup replication.
   ASSERT_OK(SetupUniverseReplication({producer_table_}));
-  auto stream_id = ASSERT_RESULT(GetCDCStreamID(producer_table_->id())).ToString();
+  auto stream_id = ASSERT_RESULT(GetCDCStreamID(producer_table_->id()));
 
   // 3. Verify everything is setup correctly.
   ASSERT_OK(CorrectlyPollingAllTablets(
@@ -1362,7 +1362,7 @@ TEST_F(XClusterYsqlTest, ReplicationWithBasicDDL) {
       ToggleUniverseReplication(consumer_cluster(), consumer_client(), kReplicationGroupId, true));
 
   // We read the first batch of writes with the old schema, but not the new schema writes.
-  ASSERT_NO_FATALS(VerifyReplicationError(
+  ASSERT_OK(VerifyReplicationError(
       consumer_table_->id(), stream_id, ReplicationErrorPb::REPLICATION_SCHEMA_MISMATCH));
   ASSERT_OK(data_replicated_correctly(count));
 
@@ -1376,7 +1376,7 @@ TEST_F(XClusterYsqlTest, ReplicationWithBasicDDL) {
 
   // 5. Verify Replication continued and new schema Producer entries are added to Consumer.
   count += kRecordBatch;
-  ASSERT_NO_FATALS(VerifyReplicationError(consumer_table_->id(), stream_id, boost::none));
+  ASSERT_OK(VerifyReplicationError(consumer_table_->id(), stream_id, std::nullopt));
   ASSERT_OK(data_replicated_correctly(count));
 
   /***************************/
@@ -1419,7 +1419,7 @@ TEST_F(XClusterYsqlTest, ReplicationWithBasicDDL) {
   //  2. Expectations: Replication should add data 1x, then block because IDs don't match.
   //                   DROP is non-blocking,
   //                   re-ADD blocks until IDs match even though the Name & Type match.
-  ASSERT_NO_FATALS(VerifyReplicationError(
+  ASSERT_OK(VerifyReplicationError(
       consumer_table_->id(), stream_id, ReplicationErrorPb::REPLICATION_SCHEMA_MISMATCH));
   ASSERT_OK(data_replicated_correctly(count));
 
@@ -1429,7 +1429,7 @@ TEST_F(XClusterYsqlTest, ReplicationWithBasicDDL) {
       "ALTER TABLE $0 ADD COLUMN $1 VARCHAR", consumer_tbl_name, new_column));
 
   count += kRecordBatch;
-  ASSERT_NO_FATALS(VerifyReplicationError(consumer_table_->id(), stream_id, boost::none));
+  ASSERT_OK(VerifyReplicationError(consumer_table_->id(), stream_id, std::nullopt));
   ASSERT_OK(data_replicated_correctly(count));
 
   /***************************/
@@ -1447,7 +1447,7 @@ TEST_F(XClusterYsqlTest, ReplicationWithBasicDDL) {
 
   // 3. Verify ALTER was parsed by Consumer, which stopped replication and hasn't read the new
   // data.
-  ASSERT_NO_FATALS(VerifyReplicationError(
+  ASSERT_OK(VerifyReplicationError(
       consumer_table_->id(), stream_id, ReplicationErrorPb::REPLICATION_SCHEMA_MISMATCH));
   ASSERT_OK(data_replicated_correctly(count));
 
@@ -1461,7 +1461,7 @@ TEST_F(XClusterYsqlTest, ReplicationWithBasicDDL) {
 
   // 5. Verify Replication continued and new schema Producer entries are added to Consumer.
   count += kRecordBatch;
-  ASSERT_NO_FATALS(VerifyReplicationError(consumer_table_->id(), stream_id, boost::none));
+  ASSERT_OK(VerifyReplicationError(consumer_table_->id(), stream_id, std::nullopt));
   ASSERT_OK(data_replicated_correctly(count));
 
   /***************************/
diff --git a/src/yb/master/catalog_entity_info.cc b/src/yb/master/catalog_entity_info.cc
index fbe37f1fce..32412b841b 100644
--- a/src/yb/master/catalog_entity_info.cc
+++ b/src/yb/master/catalog_entity_info.cc
@@ -1316,44 +1316,6 @@ Status UniverseReplicationInfo::GetSetupUniverseReplicationErrorStatus() const {
   return setup_universe_replication_error_;
 }
 
-void UniverseReplicationInfo::StoreReplicationError(
-    const TableId& consumer_table_id,
-    const xrepl::StreamId& stream_id,
-    const ReplicationErrorPb error,
-    const std::string& error_detail) {
-  std::lock_guard l(lock_);
-  table_replication_error_map_[consumer_table_id][stream_id][error] = error_detail;
-}
-
-void UniverseReplicationInfo::ClearReplicationError(
-    const TableId& consumer_table_id,
-    const xrepl::StreamId& stream_id,
-    const ReplicationErrorPb error) {
-  std::lock_guard l(lock_);
-
-  if (table_replication_error_map_.count(consumer_table_id) == 0 ||
-      table_replication_error_map_[consumer_table_id].count(stream_id) == 0 ||
-      table_replication_error_map_[consumer_table_id][stream_id].count(error) == 0) {
-    return;
-  }
-
-  table_replication_error_map_[consumer_table_id][stream_id].erase(error);
-
-  if (table_replication_error_map_[consumer_table_id][stream_id].empty()) {
-    table_replication_error_map_[consumer_table_id].erase(stream_id);
-  }
-
-  if (table_replication_error_map_[consumer_table_id].empty()) {
-    table_replication_error_map_.erase(consumer_table_id);
-  }
-}
-
-UniverseReplicationInfo::TableReplicationErrorMap
-UniverseReplicationInfo::GetReplicationErrors() const {
-  SharedLock<decltype(lock_)> l(lock_);
-  return table_replication_error_map_;
-}
-
 // ================================================================================================
 // PersistentUniverseReplicationBootstrapInfo
 // ================================================================================================
diff --git a/src/yb/master/catalog_entity_info.h b/src/yb/master/catalog_entity_info.h
index b2717d5f8c..9c82f5209e 100644
--- a/src/yb/master/catalog_entity_info.h
+++ b/src/yb/master/catalog_entity_info.h
@@ -1209,22 +1209,6 @@ class UniverseReplicationInfo : public UniverseReplicationInfoBase,
   // Get the Status of the last error from the current SetupUniverseReplication.
   Status GetSetupUniverseReplicationErrorStatus() const;
 
-  void StoreReplicationError(
-      const TableId& consumer_table_id,
-      const xrepl::StreamId& stream_id,
-      ReplicationErrorPb error,
-      const std::string& error_detail);
-
-  void ClearReplicationError(
-      const TableId& consumer_table_id, const xrepl::StreamId& stream_id, ReplicationErrorPb error);
-
-  // Maps from a table id -> stream id -> replication error -> error detail.
-  typedef std::unordered_map<ReplicationErrorPb, std::string> ReplicationErrorMap;
-  typedef std::unordered_map<xrepl::StreamId, ReplicationErrorMap> StreamReplicationErrorMap;
-  typedef std::unordered_map<TableId, StreamReplicationErrorMap> TableReplicationErrorMap;
-
-  TableReplicationErrorMap GetReplicationErrors() const;
-
  private:
   friend class RefCountedThreadSafe<UniverseReplicationInfo>;
   virtual ~UniverseReplicationInfo() = default;
@@ -1233,8 +1217,6 @@ class UniverseReplicationInfo : public UniverseReplicationInfoBase,
   // constructed object, or if the SetupUniverseReplication was successful.
   Status setup_universe_replication_error_ = Status::OK();
 
-  TableReplicationErrorMap table_replication_error_map_;
-
   DISALLOW_COPY_AND_ASSIGN(UniverseReplicationInfo);
 };
 
diff --git a/src/yb/master/catalog_manager.h b/src/yb/master/catalog_manager.h
index adcb5c8f7b..d79c75c2bd 100644
--- a/src/yb/master/catalog_manager.h
+++ b/src/yb/master/catalog_manager.h
@@ -44,6 +44,7 @@
 #include <boost/functional/hash.hpp>
 #include <gtest/internal/gtest-internal.h>
 
+#include "yb/cdc/xcluster_types.h"
 #include "yb/common/constants.h"
 #include "yb/common/entity_ids.h"
 #include "yb/master/leader_epoch.h"
@@ -1084,8 +1085,13 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
       const TabletDriveStorageMetadataPB& storage_metadata,
       const std::optional<TabletLeaderMetricsPB>& leader_metrics);
 
-  Status ProcessTabletReplicationStatus(const TabletReplicationStatusPB& replication_state)
-      EXCLUDES(mutex_);
+  void SyncXClusterConsumerReplicationStatusMap(
+      const cdc::ReplicationGroupId& replication_group_id,
+      const google::protobuf::Map<std::string, cdc::ProducerEntryPB>& producer_map)
+      EXCLUDES(xcluster_consumer_replication_error_map_mutex_);
+
+  void StoreXClusterConsumerReplicationStatus(
+      const XClusterConsumerReplicationStatusPB& consumer_replication_status) EXCLUDES(mutex_);
 
   void ProcessTabletReplicaFullCompactionStatus(
       const TabletServerId& ts_uuid, const FullCompactionStatusPB& full_compaction_status);
@@ -2882,35 +2888,11 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
 
   void LoadCDCRetainedTabletsSet() REQUIRES(mutex_);
 
-  void PopulateUniverseReplicationStatus(
-      const UniverseReplicationInfo& universe, GetReplicationStatusResponsePB* resp) const
-      REQUIRES_SHARED(mutex_);
-
-  Status StoreReplicationErrors(
-      const cdc::ReplicationGroupId& replication_group_id,
-      const TableId& consumer_table_id,
-      const xrepl::StreamId& stream_id,
-      const std::vector<std::pair<ReplicationErrorPb, std::string>>& replication_errors)
-      EXCLUDES(mutex_);
-
-  Status StoreReplicationErrorsUnlocked(
+  // Populate the response with the errors for the given replication group.
+  Status PopulateReplicationGroupErrors(
       const cdc::ReplicationGroupId& replication_group_id,
-      const TableId& consumer_table_id,
-      const xrepl::StreamId& stream_id,
-      const std::vector<std::pair<ReplicationErrorPb, std::string>>& replication_errors)
-      REQUIRES_SHARED(mutex_);
-
-  Status ClearReplicationErrors(
-      const cdc::ReplicationGroupId& replication_group_id,
-      const TableId& consumer_table_id,
-      const xrepl::StreamId& stream_id,
-      const std::vector<ReplicationErrorPb>& replication_error_codes) EXCLUDES(mutex_);
-
-  Status ClearReplicationErrorsUnlocked(
-      const cdc::ReplicationGroupId& replication_group_id,
-      const TableId& consumer_table_id,
-      const xrepl::StreamId& stream_id,
-      const std::vector<ReplicationErrorPb>& replication_error_codes) REQUIRES_SHARED(mutex_);
+      GetReplicationStatusResponsePB* resp) const
+      REQUIRES_SHARED(mutex_, xcluster_consumer_replication_error_map_mutex_);
 
   // Update the UniverseReplicationInfo object when toggling replication.
   Status SetUniverseReplicationInfoEnabled(
@@ -3139,6 +3121,11 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
 
   std::unique_ptr<cdc::CDCStateTable> cdc_state_table_;
 
+  mutable std::shared_mutex xcluster_consumer_replication_error_map_mutex_ ACQUIRED_AFTER(mutex_);
+  std::unordered_map<cdc::ReplicationGroupId, xcluster::ReplicationGroupErrors>
+      xcluster_consumer_replication_error_map_
+          GUARDED_BY(xcluster_consumer_replication_error_map_mutex_);
+
   DISALLOW_COPY_AND_ASSIGN(CatalogManager);
 };
 
diff --git a/src/yb/master/master_heartbeat.proto b/src/yb/master/master_heartbeat.proto
index e718e1158e..094e54db46 100644
--- a/src/yb/master/master_heartbeat.proto
+++ b/src/yb/master/master_heartbeat.proto
@@ -18,6 +18,7 @@ option java_package = "org.yb.master";
 
 import "yb/cdc/cdc_consumer.proto";
 import "yb/cdc/xcluster_producer.proto";
+import "yb/common/common_types.proto";
 import "yb/common/wire_protocol.proto";
 import "yb/consensus/metadata.proto";
 import "yb/consensus/consensus_types.proto";
@@ -107,11 +108,21 @@ message TabletLeaderMetricsPB {
   optional fixed64 ht_lease_expiration = 3;
 }
 
-message TabletReplicationStatusPB {
-  required bytes tablet_id = 1;
+message XClusterConsumerStreamTabletStatusPB {
+  required bytes producer_tablet_id = 1;
+  required int64 consumer_term = 2;
+  required ReplicationErrorPb error = 3;
+}
 
-  // Maps a stream id -> replication status.
-  map<string, StreamReplicationStatusPB> stream_replication_statuses = 2;
+message XClusterConsumerTableStatusPB {
+  required bytes consumer_table_id = 1;
+  repeated XClusterConsumerStreamTabletStatusPB stream_tablet_status = 2;
+}
+
+message XClusterConsumerReplicationStatusPB {
+  // ReplicationGroupId type represented in bytes.
+  required bytes replication_group_id = 1;
+  repeated XClusterConsumerTableStatusPB table_status = 2;
 }
 
 message FullCompactionStatusPB {
@@ -221,8 +232,8 @@ message TSHeartbeatRequestPB {
   // Tablet server has at least one faulty drive.
   optional bool faulty_drive = 15;
 
-  // Replication status. Only set in the metrics heartbeat.
-  repeated TabletReplicationStatusPB replication_state = 16;
+  // DEPRECATED TabletReplicationStatusPB.
+  reserved 16;
 
   optional uint32 auto_flags_config_version = 17;
 
@@ -235,6 +246,9 @@ message TSHeartbeatRequestPB {
   optional uint64 ysql_db_catalog_versions_fingerprint = 21;
 
   optional string universe_uuid = 22;
+
+  repeated XClusterConsumerReplicationStatusPB
+      xcluster_consumer_replication_status = 23;
 }
 
 message TSHeartbeatResponsePB {
diff --git a/src/yb/master/master_heartbeat_service.cc b/src/yb/master/master_heartbeat_service.cc
index dc9279b2bc..fe56d94be0 100644
--- a/src/yb/master/master_heartbeat_service.cc
+++ b/src/yb/master/master_heartbeat_service.cc
@@ -263,15 +263,9 @@ class MasterHeartbeatServiceImpl : public MasterServiceBase, public MasterHeartb
         }
       }
 
-      // Only process the replication status if we have plenty of time to process the work (> 50% of
-      // timeout).
-      safe_time_left = CoarseMonoClock::Now() + (FLAGS_heartbeat_rpc_timeout_ms * 1ms / 2);
-      if (rpc.GetClientDeadline() > safe_time_left) {
-        for (const auto& replication_state : req->replication_state()) {
-          ERROR_NOT_OK(
-            server_->catalog_manager_impl()->ProcessTabletReplicationStatus(replication_state),
-            "Failed to process tablet replication status");
-        }
+      for (const auto& consumer_replication_state : req->xcluster_consumer_replication_status()) {
+        server_->catalog_manager_impl()->StoreXClusterConsumerReplicationStatus(
+            consumer_replication_state);
       }
 
       // Only process the full compaction statuses if we have plenty of time to process the work (>
diff --git a/src/yb/master/master_types.proto b/src/yb/master/master_types.proto
index 555e3f6e87..d825d1dd7d 100644
--- a/src/yb/master/master_types.proto
+++ b/src/yb/master/master_types.proto
@@ -217,8 +217,3 @@ message TServerMetricsPB {
   optional bool disable_tablet_split_if_default_ttl = 9;
 }
 
-message StreamReplicationStatusPB {
-  // The keys are expected to be of type 'ReplicationErrorPb'. This works around the limitation that
-  // enums can not be used as map keys in the protobuf spec.
-  map<int32, string> replication_errors = 1;
-}
diff --git a/src/yb/master/xrepl_catalog_manager.cc b/src/yb/master/xrepl_catalog_manager.cc
index e970d1e828..c490436eec 100644
--- a/src/yb/master/xrepl_catalog_manager.cc
+++ b/src/yb/master/xrepl_catalog_manager.cc
@@ -294,6 +294,10 @@ void CatalogManager::ClearXReplState() {
   // Clear universe replication map.
   universe_replication_map_.clear();
   xcluster_consumer_table_stream_ids_map_.clear();
+  {
+    std::lock_guard l(xcluster_consumer_replication_error_map_mutex_);
+    xcluster_consumer_replication_error_map_.clear();
+  }
 }
 
 Status CatalogManager::LoadXReplStream() {
@@ -307,6 +311,18 @@ Status CatalogManager::LoadXReplStream() {
   // loops.
   LoadCDCRetainedTabletsSet();
 
+  // Refresh the Consumer registry.
+  if (cluster_config_) {
+    auto l = cluster_config_->LockForRead();
+    if (l->pb.has_consumer_registry()) {
+      auto& producer_map = l->pb.consumer_registry().producer_map();
+      for (const auto& [replication_group_id, _] : producer_map) {
+        SyncXClusterConsumerReplicationStatusMap(
+            cdc::ReplicationGroupId(replication_group_id), producer_map);
+      }
+    }
+  }
+
   return Status::OK();
 }
 
@@ -3632,6 +3648,8 @@ Status CatalogManager::InitXClusterConsumer(
   RETURN_NOT_OK(CheckStatus(
       sys_catalog_->Upsert(leader_ready_term(), cluster_config.get()),
       "updating cluster config in sys-catalog"));
+
+  SyncXClusterConsumerReplicationStatusMap(replication_group_id, *replication_group_map);
   l.Commit();
 
   xcluster_manager_->CreateXClusterSafeTimeTableAndStartService();
@@ -3710,6 +3728,7 @@ void CatalogManager::MergeUniverseReplication(
           "Updating universe replication entries and cluster config in sys-catalog");
     }
 
+    SyncXClusterConsumerReplicationStatusMap(cdc::ReplicationGroupId(original_universe->id()), *pm);
     alter_lock.Commit();
     cl.Commit();
     original_lock.Commit();
@@ -3769,6 +3788,8 @@ Status CatalogManager::DeleteUniverseReplication(
       RETURN_NOT_OK(CheckStatus(
           sys_catalog_->Upsert(leader_ready_term(), cluster_config.get()),
           "updating cluster config in sys-catalog"));
+
+      SyncXClusterConsumerReplicationStatusMap(replication_group_id, *replication_group_map);
       cl.Commit();
     }
   }
@@ -4320,6 +4341,9 @@ Status CatalogManager::RemoveTablesFromReplication(
       }
     }
 
+    SyncXClusterConsumerReplicationStatusMap(
+        cdc::ReplicationGroupId(req->replication_group_id()), *pm);
+
     l.Commit();
     cl.Commit();
 
@@ -4771,6 +4795,9 @@ Status CatalogManager::UpdateConsumerOnProducerSplit(
   RETURN_NOT_OK(CheckStatus(
       sys_catalog_->Upsert(leader_ready_term(), cluster_config.get()),
       "Updating cluster config in sys-catalog"));
+
+  SyncXClusterConsumerReplicationStatusMap(
+      cdc::ReplicationGroupId(req->replication_group_id()), *replication_group_map);
   l.Commit();
 
   xcluster_manager_->CreateXClusterSafeTimeTableAndStartService();
@@ -4911,73 +4938,6 @@ Status CatalogManager::UpdateConsumerOnProducerMetadata(
       replication_group_id, stream_id, req->colocation_id(), current_producer_schema_version,
       current_consumer_schema_version, old_producer_schema_version, old_consumer_schema_version,
       replication_group_id);
-
-  // Clear replication errors related to schema mismatch.
-  WARN_NOT_OK(
-      ClearReplicationErrors(
-          replication_group_id, consumer_table_id, stream_id, {REPLICATION_SCHEMA_MISMATCH}),
-      "Failed to store schema mismatch replication error");
-  return Status::OK();
-}
-
-Status CatalogManager::StoreReplicationErrors(
-    const cdc::ReplicationGroupId& replication_group_id,
-    const TableId& consumer_table_id,
-    const xrepl::StreamId& stream_id,
-    const std::vector<std::pair<ReplicationErrorPb, std::string>>& replication_errors) {
-  SharedLock lock(mutex_);
-  return StoreReplicationErrorsUnlocked(
-      replication_group_id, consumer_table_id, stream_id, replication_errors);
-}
-
-Status CatalogManager::StoreReplicationErrorsUnlocked(
-    const cdc::ReplicationGroupId& replication_group_id,
-    const TableId& consumer_table_id,
-    const xrepl::StreamId& stream_id,
-    const std::vector<std::pair<ReplicationErrorPb, std::string>>& replication_errors) {
-  const auto& universe = FindPtrOrNull(universe_replication_map_, replication_group_id);
-  if (universe == nullptr) {
-    return STATUS(
-        NotFound, Format("Could not locate universe $0", replication_group_id),
-        MasterError(MasterErrorPB::UNKNOWN_ERROR));
-  }
-
-  for (const auto& error_kv : replication_errors) {
-    const ReplicationErrorPb& replication_error = error_kv.first;
-    const std::string& replication_error_detail = error_kv.second;
-    universe->StoreReplicationError(
-        consumer_table_id, stream_id, replication_error, replication_error_detail);
-  }
-
-  return Status::OK();
-}
-
-Status CatalogManager::ClearReplicationErrors(
-    const cdc::ReplicationGroupId& replication_group_id,
-    const TableId& consumer_table_id,
-    const xrepl::StreamId& stream_id,
-    const std::vector<ReplicationErrorPb>& replication_error_codes) {
-  SharedLock lock(mutex_);
-  return ClearReplicationErrorsUnlocked(
-      replication_group_id, consumer_table_id, stream_id, replication_error_codes);
-}
-
-Status CatalogManager::ClearReplicationErrorsUnlocked(
-    const cdc::ReplicationGroupId& replication_group_id,
-    const TableId& consumer_table_id,
-    const xrepl::StreamId& stream_id,
-    const std::vector<ReplicationErrorPb>& replication_error_codes) {
-  const auto& universe = FindPtrOrNull(universe_replication_map_, replication_group_id);
-  if (universe == nullptr) {
-    return STATUS(
-        NotFound, Format("Could not locate universe $0", replication_group_id),
-        MasterError(MasterErrorPB::UNKNOWN_ERROR));
-  }
-
-  for (const auto& replication_error_code : replication_error_codes) {
-    universe->ClearReplicationError(consumer_table_id, stream_id, replication_error_code);
-  }
-
   return Status::OK();
 }
 
@@ -5270,71 +5230,190 @@ Status CatalogManager::SetupNSUniverseReplication(
   return Status::OK();
 }
 
+// Sync xcluster_consumer_replication_error_map_ with the streams we have in our producer_map.
+void CatalogManager::SyncXClusterConsumerReplicationStatusMap(
+    const cdc::ReplicationGroupId& replication_group_id,
+    const google::protobuf::Map<std::string, cdc::ProducerEntryPB>& producer_map) {
+  std::lock_guard lock(xcluster_consumer_replication_error_map_mutex_);
+
+  if (producer_map.count(replication_group_id.ToString()) == 0) {
+    // Replication group has been deleted.
+    xcluster_consumer_replication_error_map_.erase(replication_group_id);
+    return;
+  }
+
+  auto& producer_entry = producer_map.at(replication_group_id.ToString());
+
+  for (auto& [_, stream_map] : producer_entry.stream_map()) {
+    std::unordered_set<TabletId> all_producer_tablet_ids;
+    for (auto& [_, producer_tablet_ids] : stream_map.consumer_producer_tablet_map()) {
+      all_producer_tablet_ids.insert(
+          producer_tablet_ids.tablets().begin(), producer_tablet_ids.tablets().end());
+    }
+
+    if (all_producer_tablet_ids.empty()) {
+      if (xcluster_consumer_replication_error_map_.contains(replication_group_id)) {
+        xcluster_consumer_replication_error_map_.at(replication_group_id)
+            .erase(stream_map.consumer_table_id());
+      }
+      continue;
+    }
+
+    auto& consumer_error_map =
+        xcluster_consumer_replication_error_map_[replication_group_id]
+                                                [stream_map.consumer_table_id()];
+    // Remove entries that are no longer part of replication.
+    std::erase_if(consumer_error_map, [&all_producer_tablet_ids](const auto& entry) {
+      return !all_producer_tablet_ids.contains(entry.first);
+    });
+
+    // Add new entries.
+    for (const auto& producer_tablet_id : all_producer_tablet_ids) {
+      if (!consumer_error_map.contains(producer_tablet_id)) {
+        // Default to UNINITIALIZED error. Once the Pollers send the status, this will be updated.
+        consumer_error_map[producer_tablet_id].error =
+            ReplicationErrorPb::REPLICATION_ERROR_UNINITIALIZED;
+      }
+    }
+  }
+}
+
+void CatalogManager::StoreXClusterConsumerReplicationStatus(
+    const XClusterConsumerReplicationStatusPB& consumer_replication_status) {
+  const auto& replication_group_id = consumer_replication_status.replication_group_id();
+
+  std::lock_guard lock(xcluster_consumer_replication_error_map_mutex_);
+  // Heartbeats can report stale entries. So we skip anything that is not in
+  // xcluster_consumer_replication_error_map_.
+
+  auto* replication_error_map = FindOrNull(
+      xcluster_consumer_replication_error_map_, cdc::ReplicationGroupId(replication_group_id));
+  if (!replication_error_map) {
+    VLOG_WITH_FUNC(2) << "Skipping deleted replication group " << replication_group_id;
+    return;
+  }
+
+  for (const auto& table_status : consumer_replication_status.table_status()) {
+    const auto& consumer_table_id = table_status.consumer_table_id();
+    auto* consumer_table_map = FindOrNull(*replication_error_map, consumer_table_id);
+    if (!consumer_table_map) {
+      VLOG_WITH_FUNC(2) << "Skipping removed table " << consumer_table_id
+                        << " in replication group " << replication_group_id;
+      continue;
+    }
+
+    for (const auto& stream_tablet_status : table_status.stream_tablet_status()) {
+      const auto& producer_tablet_id = stream_tablet_status.producer_tablet_id();
+      auto* tablet_status_map = FindOrNull(*consumer_table_map, producer_tablet_id);
+      if (!tablet_status_map) {
+        VLOG_WITH_FUNC(2) << "Skipping removed tablet " << producer_tablet_id
+                          << " in replication group " << replication_group_id;
+        continue;
+      }
+
+      // Get status from highest term only. When consumer leaders move we may get stale status
+      // from older leaders.
+      if (tablet_status_map->consumer_term <= stream_tablet_status.consumer_term()) {
+        DCHECK_NE(
+            stream_tablet_status.error(), ReplicationErrorPb::REPLICATION_ERROR_UNINITIALIZED);
+        tablet_status_map->consumer_term = stream_tablet_status.consumer_term();
+        tablet_status_map->error = stream_tablet_status.error();
+        VLOG_WITH_FUNC(2) << "Storing error for replication group: " << replication_group_id
+                          << ", consumer table: " << consumer_table_id
+                          << ", tablet: " << producer_tablet_id
+                          << ", term: " << stream_tablet_status.consumer_term()
+                          << ", error: " << stream_tablet_status.error();
+      } else {
+        VLOG_WITH_FUNC(2) << "Skipping stale error for  replication group: " << replication_group_id
+                          << ", consumer table: " << consumer_table_id
+                          << ", tablet: " << producer_tablet_id
+                          << ", term: " << stream_tablet_status.consumer_term();
+      }
+    }
+  }
+}
+
 Status CatalogManager::GetReplicationStatus(
-    const GetReplicationStatusRequestPB* req,
-    GetReplicationStatusResponsePB* resp,
+    const GetReplicationStatusRequestPB* req, GetReplicationStatusResponsePB* resp,
     rpc::RpcContext* rpc) {
   LOG(INFO) << "GetReplicationStatus from " << RequestorString(rpc) << ": " << req->DebugString();
 
+  SharedLock lock(mutex_);
+  yb::SharedLock l(xcluster_consumer_replication_error_map_mutex_);
+
   // If the 'replication_group_id' is given, only populate the status for the streams in that
-  // universe. Otherwise, populate all the status for all streams.
+  // ReplicationGroup. Otherwise, populate all the status for all groups.
   if (!req->replication_group_id().empty()) {
-    SharedLock lock(mutex_);
-    auto universe = FindPtrOrNull(
-        universe_replication_map_, cdc::ReplicationGroupId(req->replication_group_id()));
-    SCHECK(
-        universe, InvalidArgument,
-        Format("Could not find universe $0", req->replication_group_id()));
-    PopulateUniverseReplicationStatus(*universe, resp);
-  } else {
-    SharedLock lock(mutex_);
-    for (const auto& [_, replication_info] : universe_replication_map_) {
-      PopulateUniverseReplicationStatus(*replication_info, resp);
-    }
+    return PopulateReplicationGroupErrors(
+        cdc::ReplicationGroupId(req->replication_group_id()), resp);
+  }
+
+  for (const auto& [replication_id, _] : xcluster_consumer_replication_error_map_) {
+    RETURN_NOT_OK(PopulateReplicationGroupErrors(replication_id, resp));
   }
 
   return Status::OK();
 }
 
-void CatalogManager::PopulateUniverseReplicationStatus(
-    const UniverseReplicationInfo& universe, GetReplicationStatusResponsePB* resp) const {
-  // Fetch the replication error map for this universe.
-  auto table_replication_error_map = universe.GetReplicationErrors();
+Status CatalogManager::PopulateReplicationGroupErrors(
+    const cdc::ReplicationGroupId& replication_group_id,
+    GetReplicationStatusResponsePB* resp) const {
+  auto* replication_error_map =
+      FindOrNull(xcluster_consumer_replication_error_map_, replication_group_id);
+  SCHECK(
+      replication_error_map, NotFound, "Could not find replication group $0",
+      replication_group_id.ToString());
+
+  for (const auto& [consumer_table_id, tablet_error_map] : *replication_error_map) {
+    if (!xcluster_consumer_table_stream_ids_map_.contains(consumer_table_id) ||
+        !xcluster_consumer_table_stream_ids_map_.at(consumer_table_id)
+             .contains(replication_group_id)) {
+      // This is not expected. The two maps should be kept in sync.
+      LOG(DFATAL) << "xcluster_consumer_replication_error_map_ contains consumer table "
+                  << consumer_table_id << " in replication group " << replication_group_id
+                  << " but xcluster_consumer_table_stream_ids_map_ does not.";
+      continue;
+    }
 
-  // Populate an entry for each table/stream pair that belongs to 'universe'.
-  for (const auto& table_stream : xcluster_consumer_table_stream_ids_map_) {
-    const auto& table_id = table_stream.first;
-    const auto& stream_map = table_stream.second;
+    // Map from error to list of producer tablet IDs/Pollers reporting them.
+    std::unordered_map<ReplicationErrorPb, std::vector<TabletId>> errors;
+    for (const auto& [tablet_id, error_info] : tablet_error_map) {
+      errors[error_info.error].push_back(tablet_id);
+    }
 
-    auto stream_map_iter = stream_map.find(universe.ReplicationGroupId());
-    if (stream_map_iter == stream_map.end()) {
+    if (errors.empty()) {
       continue;
     }
 
-    const auto& stream_id = stream_map_iter->second;
-
     auto resp_status = resp->add_statuses();
-    resp_status->set_table_id(table_id);
+    resp_status->set_table_id(consumer_table_id);
+    const auto& stream_id =
+        xcluster_consumer_table_stream_ids_map_.at(consumer_table_id).at(replication_group_id);
     resp_status->set_stream_id(stream_id.ToString());
+    for (const auto& [error_pb, tablet_ids] : errors) {
+      if (error_pb == ReplicationErrorPb::REPLICATION_OK) {
+        // Do not report healthy tablets.
+        continue;
+      }
 
-    // Store any replication errors associated with this table/stream pair.
-    auto table_error_map_iter = table_replication_error_map.find(table_id);
-    if (table_error_map_iter != table_replication_error_map.end()) {
-      const auto& stream_replication_error_map = table_error_map_iter->second;
-      auto stream_error_map_iter = stream_replication_error_map.find(stream_id);
-      if (stream_error_map_iter != stream_replication_error_map.end()) {
-        const auto& error_map = stream_error_map_iter->second;
-        for (const auto& error_kv : error_map) {
-          const auto& error = error_kv.first;
-          const auto& detail = error_kv.second;
-
-          auto status_error = resp_status->add_errors();
-          status_error->set_error(error);
-          status_error->set_error_detail(detail);
+      auto* resp_error = resp_status->add_errors();
+      resp_error->set_error(error_pb);
+      // Only include the first 20 tablet IDs to limit response size. VLOG(4) will log all tablet to
+      // the log.
+      resp_error->set_error_detail(
+          Format("Producer Tablet IDs: $0", JoinStringsLimitCount(tablet_ids, ",", 20)));
+      if (VLOG_IS_ON(4)) {
+        VLOG(4) << "Replication error " << error_pb
+                << " for ReplicationGroup: " << replication_group_id << ", stream id: " << stream_id
+                << ", consumer table: " << consumer_table_id << ", producer tablet IDs:";
+        for (const auto& tablet_id : tablet_ids) {
+          VLOG(4) << tablet_id;
         }
       }
     }
   }
+
+  return Status::OK();
 }
 
 bool CatalogManager::IsTableXClusterProducer(const TableInfo& table_info) const {
@@ -5525,11 +5604,6 @@ Status CatalogManager::ResumeXClusterConsumerAfterNewSchema(
         producer_schema_pb->clear_pending_schema();
         // Bump the ClusterConfig version so we'll broadcast new schema version & resume operation.
         l.mutable_data()->pb.set_version(l.mutable_data()->pb.version() + 1);
-
-        WARN_NOT_OK(
-            ClearReplicationErrors(
-                replication_group_id, table_info.id(), stream_id, {REPLICATION_SCHEMA_MISMATCH}),
-            "Failed to store schema mismatch replication error");
       } else {
         LOG(INFO) << "Consumer schema not compatible for data copy of next Producer schema.";
       }
@@ -6368,63 +6442,6 @@ Result<scoped_refptr<TableInfo>> CatalogManager::GetTableById(const TableId& tab
   return FindTableById(table_id);
 }
 
-Status CatalogManager::ProcessTabletReplicationStatus(
-    const TabletReplicationStatusPB& tablet_replication_state) {
-  // Lookup the tablet info for the given tablet id.
-  const string& tablet_id = tablet_replication_state.tablet_id();
-  scoped_refptr<TabletInfo> tablet;
-  {
-    SharedLock lock(mutex_);
-    tablet = FindPtrOrNull(*tablet_map_, tablet_id);
-    SCHECK(tablet, NotFound, Format("Could not locate tablet info for tablet id $0", tablet_id));
-
-    // Lookup the streams that this tablet belongs to.
-    const std::string& consumer_table_id = tablet->table()->id();
-    auto stream_map_iter = xcluster_consumer_table_stream_ids_map_.find(consumer_table_id);
-    SCHECK(
-        stream_map_iter != xcluster_consumer_table_stream_ids_map_.end(), NotFound,
-        Format("Could not locate stream map for table id $0", consumer_table_id));
-
-    for (const auto& kv : stream_map_iter->second) {
-      const auto& replication_group_id = kv.first;
-
-      for (const auto& [tablet_stream_id, tablet_stream_replication_status] :
-           tablet_replication_state.stream_replication_statuses()) {
-        // Build a list of replication errors to store for this stream.
-        std::vector<std::pair<ReplicationErrorPb, std::string>> replication_errors;
-        replication_errors.reserve(tablet_stream_replication_status.replication_errors_size());
-        for (const auto& kv : tablet_stream_replication_status.replication_errors()) {
-          const auto& error_code = kv.first;
-          const auto& error_detail = kv.second;
-
-          // The protobuf spec does not allow enums to be used as keys in maps. To work around this,
-          // the keys in 'replication_errors' are of type int32_t that correspond to values in the
-          // 'ReplicationErrorPb' enum.
-          if (ReplicationErrorPb_IsValid(error_code)) {
-            replication_errors.emplace_back(
-                static_cast<ReplicationErrorPb>(error_code), error_detail);
-          } else {
-            LOG(WARNING) << Format(
-                "Received replication status error code ($0) that does not "
-                "correspond to an enum value in ReplicationErrorPb.");
-            replication_errors.emplace_back(REPLICATION_UNKNOWN_ERROR, error_detail);
-          }
-        }
-
-        WARN_NOT_OK(
-            StoreReplicationErrorsUnlocked(
-                replication_group_id, consumer_table_id,
-                VERIFY_RESULT(xrepl::StreamId::FromString(tablet_stream_id)), replication_errors),
-            Format(
-                "Failed to store replication error for universe=$0 table=$1 stream=$2",
-                replication_group_id, consumer_table_id, tablet_stream_id));
-      }
-    }
-  }
-
-  return Status::OK();
-}
-
 Status CatalogManager::FillHeartbeatResponseCDC(
     const SysClusterConfigEntryPB& cluster_config,
     const TSHeartbeatRequestPB* req,
diff --git a/src/yb/tserver/CMakeLists.txt b/src/yb/tserver/CMakeLists.txt
index 4faa93d93a..6f3b3b6b69 100644
--- a/src/yb/tserver/CMakeLists.txt
+++ b/src/yb/tserver/CMakeLists.txt
@@ -246,7 +246,9 @@ set(TSERVER_SRCS
   xcluster_async_executor.cc
   xcluster_safe_time_map.cc
   xcluster_consumer.cc
+  xcluster_consumer_replication_error.cc
   xcluster_poller.cc
+  xcluster_poller_id.cc
   xcluster_poller_stats.cc
   xcluster_output_client.cc
   xcluster_write_implementations.cc)
@@ -363,3 +365,5 @@ YB_TEST_TARGET_LINK_LIBRARIES(encrypted_sstable-test encryption_test_util tserve
 
 ADD_YB_TEST(remote_bootstrap_rocksdb_session-test_ent)
 ADD_YB_TEST(remote_bootstrap_rocksdb_client-test_ent)
+
+ADD_YB_TEST(xcluster_consumer_replication_error-test)
diff --git a/src/yb/tserver/heartbeater.cc b/src/yb/tserver/heartbeater.cc
index df1d2996cf..81581c9d43 100644
--- a/src/yb/tserver/heartbeater.cc
+++ b/src/yb/tserver/heartbeater.cc
@@ -531,34 +531,7 @@ Status Heartbeater::Thread::TryHeartbeat() {
       RETURN_NOT_OK(server_->SetUniverseKeyRegistry(resp.universe_key_registry()));
     }
 
-    // Check for CDC Universe Replication.
-    if (resp.has_consumer_registry()) {
-      int32_t cluster_config_version = -1;
-      if (!resp.has_cluster_config_version()) {
-        YB_LOG_EVERY_N_SECS(INFO, 30)
-            << "Invalid heartbeat response without a cluster config version";
-      } else {
-        cluster_config_version = resp.cluster_config_version();
-      }
-      RETURN_NOT_OK(server_->SetConfigVersionAndConsumerRegistry(
-          cluster_config_version, &resp.consumer_registry()));
-      server_->SetXClusterDDLOnlyMode(resp.consumer_registry().role() != cdc::XClusterRole::ACTIVE);
-    } else if (resp.has_cluster_config_version()) {
-      RETURN_NOT_OK(
-          server_->SetConfigVersionAndConsumerRegistry(resp.cluster_config_version(), nullptr));
-    }
-
-    // Check whether the cluster is a producer of a CDC stream.
-    if (resp.has_xcluster_enabled_on_producer() &&
-        resp.xcluster_enabled_on_producer()) {
-      RETURN_NOT_OK(server_->SetCDCServiceEnabled());
-    }
-
-    if (resp.has_xcluster_producer_registry() && resp.has_xcluster_config_version()) {
-      RETURN_NOT_OK(server_->SetPausedXClusterProducerStreams(
-          resp.xcluster_producer_registry().paused_producer_stream_ids(),
-          resp.xcluster_config_version()));
-    }
+    RETURN_NOT_OK(server_->XClusterHandleMasterHeartbeatResponse(resp));
 
     // At this point we know resp is a successful heartbeat response from the master so set it as
     // the last heartbeat response. This invalidates resp so we should use last_hb_response_ instead
diff --git a/src/yb/tserver/tablet_server.cc b/src/yb/tserver/tablet_server.cc
index 9e10df6549..0d1c5b46b4 100644
--- a/src/yb/tserver/tablet_server.cc
+++ b/src/yb/tserver/tablet_server.cc
@@ -86,7 +86,7 @@
 #include "yb/tserver/ts_tablet_manager.h"
 #include "yb/tserver/tserver-path-handlers.h"
 #include "yb/tserver/tserver_service.proxy.h"
-#include "yb/tserver/xcluster_consumer.h"
+#include "yb/tserver/xcluster_consumer_if.h"
 #include "yb/tserver/backup_service.h"
 
 #include "yb/cdc/cdc_service.h"
@@ -1151,7 +1151,7 @@ Status TabletServer::SetupMessengerBuilder(rpc::MessengerBuilder* builder) {
   return Status::OK();
 }
 
-XClusterConsumer* TabletServer::GetXClusterConsumer() const {
+XClusterConsumerIf* TabletServer::GetXClusterConsumer() const {
   std::lock_guard l(xcluster_consumer_mutex_);
   return xcluster_consumer_.get();
 }
@@ -1168,13 +1168,8 @@ Status TabletServer::SetUniverseKeyRegistry(
 }
 
 Status TabletServer::CreateXClusterConsumer() {
-  auto is_leader_clbk = [this](const string& tablet_id) {
-    auto tablet_peer = tablet_manager_->LookupTablet(tablet_id);
-    if (!tablet_peer) {
-      return false;
-    }
-    return tablet_peer->LeaderStatus() == consensus::LeaderStatus::LEADER_AND_READY;
-  };
+  std::lock_guard l(xcluster_consumer_mutex_);
+  SCHECK(!xcluster_consumer_, IllegalState, "XCluster consumer already exists");
 
   auto get_leader_term = [this](const TabletId& tablet_id) {
     auto tablet_peer = tablet_manager_->LookupTablet(tablet_id);
@@ -1184,22 +1179,51 @@ Status TabletServer::CreateXClusterConsumer() {
     return tablet_peer->LeaderTerm();
   };
 
-  xcluster_consumer_ = VERIFY_RESULT(XClusterConsumer::Create(
-      std::move(is_leader_clbk), std::move(get_leader_term), proxy_cache_.get(), this));
+  xcluster_consumer_ = VERIFY_RESULT(
+      tserver::CreateXClusterConsumer(std::move(get_leader_term), proxy_cache_.get(), this));
   return Status::OK();
 }
 
-Status TabletServer::SetConfigVersionAndConsumerRegistry(
-    int32_t cluster_config_version, const cdc::ConsumerRegistryPB* consumer_registry) {
-  std::lock_guard l(xcluster_consumer_mutex_);
+Status TabletServer::XClusterHandleMasterHeartbeatResponse(
+    const master::TSHeartbeatResponsePB& resp) {
+  auto* xcluster_consumer = GetXClusterConsumer();
+
+  // Only create a xcluster consumer if consumer_registry is not null.
+  const cdc::ConsumerRegistryPB* consumer_registry = nullptr;
+  if (resp.has_consumer_registry()) {
+    consumer_registry = &resp.consumer_registry();
 
-  // Only create a cdc consumer if consumer_registry is not null.
-  if (!xcluster_consumer_ && consumer_registry) {
-    RETURN_NOT_OK(CreateXClusterConsumer());
+    if (!xcluster_consumer) {
+      RETURN_NOT_OK(CreateXClusterConsumer());
+      xcluster_consumer = GetXClusterConsumer();
+    }
+
+    SetXClusterDDLOnlyMode(consumer_registry->role() != cdc::XClusterRole::ACTIVE);
   }
-  if (xcluster_consumer_) {
-    xcluster_consumer_->RefreshWithNewRegistryFromMaster(consumer_registry, cluster_config_version);
+
+  if (xcluster_consumer) {
+    int32_t cluster_config_version = -1;
+    if (!resp.has_cluster_config_version()) {
+      YB_LOG_EVERY_N_SECS(WARNING, 30)
+          << "Invalid heartbeat response without a cluster config version";
+    } else {
+      cluster_config_version = resp.cluster_config_version();
+    }
+
+    xcluster_consumer->HandleMasterHeartbeatResponse(consumer_registry, cluster_config_version);
   }
+
+  // Check whether the cluster is a producer of a CDC stream.
+  if (resp.has_xcluster_enabled_on_producer() && resp.xcluster_enabled_on_producer()) {
+    RETURN_NOT_OK(SetCDCServiceEnabled());
+  }
+
+  if (resp.has_xcluster_producer_registry() && resp.has_xcluster_config_version()) {
+    RETURN_NOT_OK(SetPausedXClusterProducerStreams(
+        resp.xcluster_producer_registry().paused_producer_stream_ids(),
+        resp.xcluster_config_version()));
+  }
+
   return Status::OK();
 }
 
diff --git a/src/yb/tserver/tablet_server.h b/src/yb/tserver/tablet_server.h
index 8f11cc2ce2..032565baba 100644
--- a/src/yb/tserver/tablet_server.h
+++ b/src/yb/tserver/tablet_server.h
@@ -77,8 +77,8 @@ class AutoFlagsManager;
 
 namespace tserver {
 
-class XClusterConsumer;
 class PgClientServiceImpl;
+class XClusterConsumerIf;
 
 class TabletServer : public DbServerBase, public TabletServerIf {
  public:
@@ -277,12 +277,11 @@ class TabletServer : public DbServerBase, public TabletServerIf {
 
   encryption::UniverseKeyManager* GetUniverseKeyManager();
 
-  Status SetConfigVersionAndConsumerRegistry(
-      int32_t cluster_config_version, const cdc::ConsumerRegistryPB* consumer_registry);
+  Status XClusterHandleMasterHeartbeatResponse(const master::TSHeartbeatResponsePB& resp);
 
   Status ValidateAndMaybeSetUniverseUuid(const UniverseUuid& universe_uuid);
 
-  XClusterConsumer* GetXClusterConsumer() const;
+  XClusterConsumerIf* GetXClusterConsumer() const;
 
   // Mark the CDC service as enabled via heartbeat.
   Status SetCDCServiceEnabled();
@@ -420,14 +419,14 @@ class TabletServer : public DbServerBase, public TabletServerIf {
 
   PgConfigReloader pg_config_reloader_;
 
-  Status CreateXClusterConsumer() REQUIRES(xcluster_consumer_mutex_);
+  Status CreateXClusterConsumer() EXCLUDES(xcluster_consumer_mutex_);
 
   std::unique_ptr<rpc::SecureContext> secure_context_;
   std::vector<CertificateReloader> certificate_reloaders_;
 
   // xCluster consumer.
   mutable std::mutex xcluster_consumer_mutex_;
-  std::unique_ptr<XClusterConsumer> xcluster_consumer_ GUARDED_BY(xcluster_consumer_mutex_);
+  std::unique_ptr<XClusterConsumerIf> xcluster_consumer_ GUARDED_BY(xcluster_consumer_mutex_);
 
   // CDC service.
   std::shared_ptr<cdc::CDCServiceImpl> cdc_service_;
diff --git a/src/yb/tserver/tserver-path-handlers.cc b/src/yb/tserver/tserver-path-handlers.cc
index c4e1ef3e7c..a946670d52 100644
--- a/src/yb/tserver/tserver-path-handlers.cc
+++ b/src/yb/tserver/tserver-path-handlers.cc
@@ -69,7 +69,7 @@
 #include "yb/tserver/tablet_server.h"
 #include "yb/tserver/ts_tablet_manager.h"
 
-#include "yb/tserver/xcluster_consumer.h"
+#include "yb/tserver/xcluster_consumer_if.h"
 #include "yb/tserver/xcluster_poller_stats.h"
 #include "yb/util/jsonwriter.h"
 #include "yb/util/url-coding.h"
diff --git a/src/yb/tserver/tserver_metrics_heartbeat_data_provider.cc b/src/yb/tserver/tserver_metrics_heartbeat_data_provider.cc
index 55fe083a69..f2f767774d 100644
--- a/src/yb/tserver/tserver_metrics_heartbeat_data_provider.cc
+++ b/src/yb/tserver/tserver_metrics_heartbeat_data_provider.cc
@@ -24,7 +24,7 @@
 #include "yb/tablet/tablet_metadata.h"
 #include "yb/tablet/tablet_peer.h"
 
-#include "yb/tserver/xcluster_consumer.h"
+#include "yb/tserver/xcluster_consumer_if.h"
 #include "yb/tserver/tablet_server.h"
 #include "yb/tserver/ts_tablet_manager.h"
 #include "yb/tserver/tserver_service.service.h"
@@ -74,8 +74,6 @@ void TServerMetricsHeartbeatDataProvider::DoAddData(
   bool no_full_tablet_report = !req->has_tablet_report() || req->tablet_report().is_incremental();
   bool should_add_tablet_data =
       FLAGS_tserver_heartbeat_metrics_add_drive_data && no_full_tablet_report;
-  bool should_add_replication_status =
-      FLAGS_tserver_heartbeat_metrics_add_replication_status && no_full_tablet_report;
 
   for (const auto& tablet_peer : server().tablet_manager()->GetTabletPeers()) {
     if (tablet_peer) {
@@ -124,42 +122,13 @@ void TServerMetricsHeartbeatDataProvider::DoAddData(
         }
       }
     }
+  }
 
-    // Report replication errors from the xCluster consumer.
+  // Report xCluster consumer heartbeat info.
+  if (FLAGS_tserver_heartbeat_metrics_add_replication_status) {
     auto xcluster_consumer = server().GetXClusterConsumer();
-    if (xcluster_consumer != nullptr && should_add_replication_status) {
-      const auto tablet_replication_error_map = xcluster_consumer->GetReplicationErrors();
-      for (const auto& tablet_kv : tablet_replication_error_map) {
-        const TabletId& tablet_id = tablet_kv.first;
-
-        auto replication_state = req->add_replication_state();
-        replication_state->set_tablet_id(tablet_id);
-
-        auto& stream_to_status = *replication_state->mutable_stream_replication_statuses();
-        const auto& stream_replication_error_map = tablet_kv.second;
-        for (const auto& [stream_id, replication_error_map] : stream_replication_error_map) {
-          auto& error_to_detail =
-              *stream_to_status[stream_id.ToString()].mutable_replication_errors();
-          for (const auto& error_kv : replication_error_map) {
-            const ReplicationErrorPb error = error_kv.first;
-            const std::string& detail = error_kv.second;
-
-            // Do not report this error if we have already reported it, unless the master needs a
-            // full tablet report.
-            if (no_full_tablet_report &&
-                prev_replication_error_map_.count(tablet_id) == 1 &&
-                prev_replication_error_map_[tablet_id].count(stream_id) == 1 &&
-                prev_replication_error_map_[tablet_id][stream_id].count(error) == 1 &&
-                prev_replication_error_map_[tablet_id][stream_id][error] == detail) {
-              continue;
-            }
-
-            error_to_detail[static_cast<int32_t>(error)] = detail;
-          }
-        }
-      }
-
-      prev_replication_error_map_ = tablet_replication_error_map;
+    if (xcluster_consumer != nullptr) {
+      xcluster_consumer->PopulateMasterHeartbeatRequest(req, last_resp.needs_full_tablet_report());
     }
   }
 
diff --git a/src/yb/tserver/tserver_metrics_heartbeat_data_provider.h b/src/yb/tserver/tserver_metrics_heartbeat_data_provider.h
index a952d1b21d..cbb3de5d1a 100644
--- a/src/yb/tserver/tserver_metrics_heartbeat_data_provider.h
+++ b/src/yb/tserver/tserver_metrics_heartbeat_data_provider.h
@@ -15,7 +15,6 @@
 
 #include <memory>
 
-#include "yb/cdc/cdc_types.h"
 #include "yb/tserver/heartbeater.h"
 
 namespace yb {
@@ -36,9 +35,6 @@ class TServerMetricsHeartbeatDataProvider : public PeriodicalHeartbeatDataProvid
   // Stores the total read and writes ops for computing iops.
   uint64_t prev_reads_ = 0;
   uint64_t prev_writes_ = 0;
-
-  // Stores the previously reported replication errors.
-  cdc::TabletReplicationErrorMap prev_replication_error_map_;
 };
 
 } // namespace tserver
diff --git a/src/yb/tserver/xcluster_consumer.cc b/src/yb/tserver/xcluster_consumer.cc
index f2f0017bb3..009c7c38be 100644
--- a/src/yb/tserver/xcluster_consumer.cc
+++ b/src/yb/tserver/xcluster_consumer.cc
@@ -22,6 +22,7 @@
 #include "yb/common/wire_protocol.h"
 
 #include "yb/master/master_defaults.h"
+#include "yb/master/master_heartbeat.pb.h"
 
 #include "yb/rpc/messenger.h"
 #include "yb/rpc/proxy.h"
@@ -66,7 +67,7 @@ DEFINE_RUNTIME_int32(xcluster_safe_time_update_interval_secs, 1,
     "in the replication, then it will add to the overall staleness of the data.");
 
 DEFINE_RUNTIME_int32(apply_changes_max_send_rate_mbps, 100,
-                    "Server-wide max apply rate for xcluster traffic.");
+    "Server-wide max apply rate for xcluster traffic.");
 
 static bool ValidateXClusterSafeTimeUpdateInterval(const char* flagname, int32 value) {
   if (value <= 0) {
@@ -116,11 +117,8 @@ void XClusterClient::Shutdown() {
   }
 }
 
-Result<std::unique_ptr<XClusterConsumer>> XClusterConsumer::Create(
-    std::function<bool(const std::string&)> is_leader_for_tablet,
-    std::function<int64_t(const TabletId&)>
-        get_leader_term,
-    rpc::ProxyCache* proxy_cache,
+Result<std::unique_ptr<XClusterConsumerIf>> CreateXClusterConsumer(
+    std::function<int64_t(const TabletId&)> get_leader_term, rpc::ProxyCache* proxy_cache,
     TabletServer* tserver) {
   auto master_addrs = tserver->options().GetMasterAddresses();
   std::vector<std::string> hostport_strs;
@@ -147,28 +145,17 @@ Result<std::unique_ptr<XClusterConsumer>> XClusterConsumer::Create(
 
   local_client->client->SetLocalTabletServer(tserver->permanent_uuid(), tserver->proxy(), tserver);
   auto xcluster_consumer = std::make_unique<XClusterConsumer>(
-      std::move(is_leader_for_tablet), std::move(get_leader_term), proxy_cache,
-      tserver->permanent_uuid(), std::move(local_client));
+      std::move(get_leader_term), proxy_cache, tserver->permanent_uuid(), std::move(local_client));
 
-  // TODO(NIC): Unify xcluster_consumer thread_pool & remote_client_ threadpools
-  RETURN_NOT_OK(yb::Thread::Create(
-      "XClusterConsumer", "Poll", &XClusterConsumer::RunThread, xcluster_consumer.get(),
-      &xcluster_consumer->run_trigger_poll_thread_));
-  ThreadPoolBuilder cdc_consumer_thread_pool_builder("XClusterConsumerHandler");
-  if (FLAGS_cdc_consumer_handler_thread_pool_size > 0) {
-    cdc_consumer_thread_pool_builder.set_max_threads(FLAGS_cdc_consumer_handler_thread_pool_size);
-  }
-  RETURN_NOT_OK(cdc_consumer_thread_pool_builder.Build(&xcluster_consumer->thread_pool_));
+  RETURN_NOT_OK(xcluster_consumer->Init());
 
   return xcluster_consumer;
 }
 
 XClusterConsumer::XClusterConsumer(
-    std::function<bool(const std::string&)> is_leader_for_tablet,
     std::function<int64_t(const TabletId&)> get_leader_term, rpc::ProxyCache* proxy_cache,
     const string& ts_uuid, std::unique_ptr<XClusterClient> local_client)
-    : is_leader_for_tablet_(std::move(is_leader_for_tablet)),
-      get_leader_term_(std::move(get_leader_term)),
+    : get_leader_term_func_(std::move(get_leader_term)),
       rpcs_(new rpc::Rpcs),
       log_prefix_(Format("[TS $0]: ", ts_uuid)),
       local_client_(std::move(local_client)),
@@ -189,6 +176,18 @@ XClusterConsumer::~XClusterConsumer() {
   DCHECK(pollers_map_.empty());
 }
 
+Status XClusterConsumer::Init() {
+  // TODO(NIC): Unify xcluster_consumer thread_pool & remote_client_ threadpools
+  RETURN_NOT_OK(yb::Thread::Create(
+      "XClusterConsumer", "Poll", &XClusterConsumer::RunThread, this, &run_trigger_poll_thread_));
+  ThreadPoolBuilder cdc_consumer_thread_pool_builder("XClusterConsumerHandler");
+  if (FLAGS_cdc_consumer_handler_thread_pool_size > 0) {
+    cdc_consumer_thread_pool_builder.set_max_threads(FLAGS_cdc_consumer_handler_thread_pool_size);
+  }
+
+  return cdc_consumer_thread_pool_builder.Build(&thread_pool_);
+}
+
 void XClusterConsumer::Shutdown() {
   LOG_WITH_PREFIX(INFO) << "Shutting down XClusterConsumer";
   is_shutdown_ = true;
@@ -267,13 +266,7 @@ void XClusterConsumer::RunThread() {
   }
 }
 
-void XClusterConsumer::RefreshWithNewRegistryFromMaster(
-    const cdc::ConsumerRegistryPB* consumer_registry, int32_t cluster_config_version) {
-  UpdateInMemoryState(consumer_registry, cluster_config_version);
-  run_thread_cond_.notify_all();
-}
-
-std::vector<TabletId> XClusterConsumer::TEST_producer_tablets_running() {
+std::vector<std::string> XClusterConsumer::TEST_producer_tablets_running() const {
   SharedLock read_lock(pollers_map_mutex_);
 
   std::vector<TabletId> tablets;
@@ -283,7 +276,7 @@ std::vector<TabletId> XClusterConsumer::TEST_producer_tablets_running() {
   return tablets;
 }
 
-std::vector<std::shared_ptr<XClusterPoller>> XClusterConsumer::TEST_ListPollers() {
+std::vector<std::shared_ptr<XClusterPoller>> XClusterConsumer::TEST_ListPollers() const {
   std::vector<std::shared_ptr<XClusterPoller>> ret;
   {
     SharedLock read_lock(pollers_map_mutex_);
@@ -296,18 +289,19 @@ std::vector<std::shared_ptr<XClusterPoller>> XClusterConsumer::TEST_ListPollers(
 
 std::vector<XClusterPollerStats> XClusterConsumer::GetPollerStats() const {
   std::vector<XClusterPollerStats> ret;
-  {
-    SharedLock read_lock(pollers_map_mutex_);
-    for (const auto& [_, poller] : pollers_map_) {
-      ret.push_back(poller->GetStats());
-    }
+  SharedLock read_lock(pollers_map_mutex_);
+  for (const auto& [_, poller] : pollers_map_) {
+    ret.push_back(poller->GetStats());
   }
   return ret;
 }
 
-// NOTE: This happens on TS.heartbeat, so it needs to finish quickly
-void XClusterConsumer::UpdateInMemoryState(
+void XClusterConsumer::HandleMasterHeartbeatResponse(
     const cdc::ConsumerRegistryPB* consumer_registry, int32_t cluster_config_version) {
+  // This function is called from the Master heartbeat response which means the errors in sending
+  // state have been processed by the Master leader.
+  error_collector_.TransitionErrorsFromSendingToSent();
+
   std::lock_guard write_lock_master(master_data_mutex_);
   if (is_shutdown_) {
     return;
@@ -343,95 +337,99 @@ void XClusterConsumer::UpdateInMemoryState(
   for (const auto& [replication_group_id_str, producer_entry_pb] :
        DCHECK_NOTNULL(consumer_registry)->producer_map()) {
     const cdc::ReplicationGroupId replication_group_id(replication_group_id_str);
-    // recreate the UUID connection information
-    if (!ContainsKey(uuid_master_addrs_, replication_group_id)) {
-      std::vector<HostPort> hp;
-      HostPortsFromPBs(producer_entry_pb.master_addrs(), &hp);
-      uuid_master_addrs_[replication_group_id] = HostPort::ToCommaSeparatedString(hp);
 
+    std::vector<HostPort> hp;
+    HostPortsFromPBs(producer_entry_pb.master_addrs(), &hp);
+    auto master_addrs = HostPort::ToCommaSeparatedString(std::move(hp));
+    if (ContainsKey(old_uuid_master_addrs, replication_group_id) &&
+        old_uuid_master_addrs[replication_group_id] != master_addrs) {
       // If master addresses changed, mark for YBClient update.
-      if (ContainsKey(old_uuid_master_addrs, replication_group_id) &&
-          uuid_master_addrs_[replication_group_id] != old_uuid_master_addrs[replication_group_id]) {
-        changed_master_addrs_.insert(cdc::ReplicationGroupId(replication_group_id));
-      }
+      changed_master_addrs_.insert(replication_group_id);
     }
-    // recreate the set of XClusterPoller
-    for (const auto& [stream_id_str, stream_entry_pb] : producer_entry_pb.stream_map()) {
-      auto stream_id_result = xrepl::StreamId::FromString(stream_id_str);
-      if (!stream_id_result) {
-        LOG_WITH_PREFIX_AND_FUNC(WARNING) << "Invalid stream id: " << stream_id_str;
-        continue;
-      }
-      auto& stream_id = *stream_id_result;
+    uuid_master_addrs_[replication_group_id] = std::move(master_addrs);
 
-      if (stream_entry_pb.local_tserver_optimized()) {
-        LOG_WITH_PREFIX(INFO) << Format(
-            "Stream $0 will use local tserver optimization", stream_id);
-        streams_with_local_tserver_optimization_.insert(stream_id);
-      }
-      if (stream_entry_pb.has_producer_schema()) {
-        stream_to_schema_version_[stream_id] = std::make_pair(
-            stream_entry_pb.producer_schema().validated_schema_version(),
-            stream_entry_pb.producer_schema().last_compatible_consumer_schema_version());
-      }
+    UpdateReplicationGroupInMemState(replication_group_id, producer_entry_pb);
+  }
+  // Wake up the background thread to stop old pollers and start new ones.
+  run_thread_cond_.notify_all();
+}
 
-      if (stream_entry_pb.has_schema_versions()) {
-        auto& schema_version_map = stream_schema_version_map_[stream_id];
-        auto schema_versions = stream_entry_pb.schema_versions();
-        schema_version_map[schema_versions.current_producer_schema_version()] =
-            schema_versions.current_consumer_schema_version();
-        // Update the old producer schema version, only if it is not the same as
-        // current producer schema version.
-        if (schema_versions.old_producer_schema_version() !=
-            schema_versions.current_producer_schema_version()) {
-          DCHECK(schema_versions.old_producer_schema_version() <
-              schema_versions.current_producer_schema_version());
-          schema_version_map[schema_versions.old_producer_schema_version()] =
-              schema_versions.old_consumer_schema_version();
-        }
+// NOTE: This happens on TS.heartbeat, so it needs to finish quickly
+void XClusterConsumer::UpdateReplicationGroupInMemState(
+    const cdc::ReplicationGroupId& replication_group_id,
+    const yb::cdc::ProducerEntryPB& producer_entry_pb) {
+  for (const auto& [stream_id_str, stream_entry_pb] : producer_entry_pb.stream_map()) {
+    auto stream_id_result = xrepl::StreamId::FromString(stream_id_str);
+    if (!stream_id_result) {
+      LOG_WITH_PREFIX_AND_FUNC(WARNING) << "Invalid stream id: " << stream_id_str;
+      continue;
+    }
+    auto& stream_id = *stream_id_result;
+
+    if (stream_entry_pb.local_tserver_optimized()) {
+      LOG_WITH_PREFIX(INFO) << Format("Stream $0 will use local tserver optimization", stream_id);
+      streams_with_local_tserver_optimization_.insert(stream_id);
+    }
+    if (stream_entry_pb.has_producer_schema()) {
+      stream_to_schema_version_[stream_id] = std::make_pair(
+          stream_entry_pb.producer_schema().validated_schema_version(),
+          stream_entry_pb.producer_schema().last_compatible_consumer_schema_version());
+    }
 
-        auto min_schema_version = schema_versions.old_consumer_schema_version() > 0 ?
-            schema_versions.old_consumer_schema_version() :
-            schema_versions.current_consumer_schema_version();
-        min_schema_version_map_[std::make_pair(
-            stream_entry_pb.consumer_table_id(), kColocationIdNotSet)] = min_schema_version;
+    if (stream_entry_pb.has_schema_versions()) {
+      auto& schema_version_map = stream_schema_version_map_[stream_id];
+      auto schema_versions = stream_entry_pb.schema_versions();
+      schema_version_map[schema_versions.current_producer_schema_version()] =
+          schema_versions.current_consumer_schema_version();
+      // Update the old producer schema version, only if it is not the same as
+      // current producer schema version.
+      if (schema_versions.old_producer_schema_version() !=
+          schema_versions.current_producer_schema_version()) {
+        DCHECK(
+            schema_versions.old_producer_schema_version() <
+            schema_versions.current_producer_schema_version());
+        schema_version_map[schema_versions.old_producer_schema_version()] =
+            schema_versions.old_consumer_schema_version();
       }
 
-      for (const auto& [colocated_id, versions] : stream_entry_pb.colocated_schema_versions()) {
-        auto& schema_version_map =
-            stream_colocated_schema_version_map_[stream_id][colocated_id];
-        schema_version_map[versions.current_producer_schema_version()] =
-            versions.current_consumer_schema_version();
+      auto min_schema_version = schema_versions.old_consumer_schema_version() > 0
+                                    ? schema_versions.old_consumer_schema_version()
+                                    : schema_versions.current_consumer_schema_version();
+      min_schema_version_map_[std::make_pair(
+          stream_entry_pb.consumer_table_id(), kColocationIdNotSet)] = min_schema_version;
+    }
+
+    for (const auto& [colocated_id, versions] : stream_entry_pb.colocated_schema_versions()) {
+      auto& schema_version_map = stream_colocated_schema_version_map_[stream_id][colocated_id];
+      schema_version_map[versions.current_producer_schema_version()] =
+          versions.current_consumer_schema_version();
 
         // Update the old producer schema version, only if it is not the same as
         // current producer schema version - handles the case where versions are 0.
-        if (versions.old_producer_schema_version() != versions.current_producer_schema_version()) {
-          DCHECK(versions.old_producer_schema_version() <
-              versions.current_producer_schema_version());
-          schema_version_map[versions.old_producer_schema_version()] =
-              versions.old_consumer_schema_version();
-        }
-
-        auto min_schema_version = versions.old_consumer_schema_version() > 0 ?
-            versions.old_consumer_schema_version() : versions.current_consumer_schema_version();
-        min_schema_version_map_[std::make_pair(
-            stream_entry_pb.consumer_table_id(), colocated_id)] = min_schema_version;
+      if (versions.old_producer_schema_version() != versions.current_producer_schema_version()) {
+        DCHECK(versions.old_producer_schema_version() < versions.current_producer_schema_version());
+        schema_version_map[versions.old_producer_schema_version()] =
+            versions.old_consumer_schema_version();
       }
 
-      for (const auto& [consumer_tablet_id, producer_tablet_list] :
-           stream_entry_pb.consumer_producer_tablet_map()) {
-        for (const auto& producer_tablet_id : producer_tablet_list.tablets()) {
-          auto xCluster_tablet_info = cdc::XClusterTabletInfo{
-              .producer_tablet_info =
-                  {cdc::ReplicationGroupId(replication_group_id), stream_id, producer_tablet_id},
-              .consumer_tablet_info = {consumer_tablet_id, stream_entry_pb.consumer_table_id()},
-              .disable_stream = producer_entry_pb.disable_stream()};
-          producer_consumer_tablet_map_from_master_.emplace(std::move(xCluster_tablet_info));
-        }
+      auto min_schema_version = versions.old_consumer_schema_version() > 0
+                                    ? versions.old_consumer_schema_version()
+                                    : versions.current_consumer_schema_version();
+      min_schema_version_map_[std::make_pair(stream_entry_pb.consumer_table_id(), colocated_id)] =
+          min_schema_version;
+    }
+
+    for (const auto& [consumer_tablet_id, producer_tablet_list] :
+         stream_entry_pb.consumer_producer_tablet_map()) {
+      for (const auto& producer_tablet_id : producer_tablet_list.tablets()) {
+        auto xCluster_tablet_info = cdc::XClusterTabletInfo{
+            .producer_tablet_info = {replication_group_id, stream_id, producer_tablet_id},
+            .consumer_tablet_info = {consumer_tablet_id, stream_entry_pb.consumer_table_id()},
+            .disable_stream = producer_entry_pb.disable_stream()};
+        producer_consumer_tablet_map_from_master_.emplace(std::move(xCluster_tablet_info));
       }
     }
   }
-  run_thread_cond_.notify_all();
 }
 
 SchemaVersion XClusterConsumer::GetMinXClusterSchemaVersion(const TableId& table_id,
@@ -469,8 +467,10 @@ void XClusterConsumer::TriggerPollForNewTablets() {
     bool start_polling;
     {
       SharedLock read_lock_pollers(pollers_map_mutex_);
-      start_polling = !pollers_map_.contains(producer_tablet_info) &&
-                      is_leader_for_tablet_(consumer_tablet_info.tablet_id);
+      // Iff we are the leader we will get a valid term.
+      start_polling =
+          !pollers_map_.contains(producer_tablet_info) &&
+          get_leader_term_func_(consumer_tablet_info.tablet_id) != yb::OpId::kUnknownTerm;
 
       // Update the Master Addresses, if altered after setup.
       if (ContainsKey(remote_clients_, replication_group_id) &&
@@ -487,10 +487,11 @@ void XClusterConsumer::TriggerPollForNewTablets() {
     }
     if (start_polling) {
       std::lock_guard write_lock_pollers(pollers_map_mutex_);
+      const auto leader_term = get_leader_term_func_(consumer_tablet_info.tablet_id);
 
       // Check again, since we unlocked.
-      start_polling = !pollers_map_.contains(producer_tablet_info) &&
-                      is_leader_for_tablet_(consumer_tablet_info.tablet_id);
+      start_polling =
+          !pollers_map_.contains(producer_tablet_info) && leader_term != yb::OpId::kUnknownTerm;
       if (start_polling) {
         // This is a new tablet, trigger a poll.
         // See if we need to create a new client connection
@@ -554,7 +555,7 @@ void XClusterConsumer::TriggerPollForNewTablets() {
         std::shared_ptr<XClusterPoller> xcluster_poller = std::make_unique<XClusterPoller>(
             producer_tablet_info, consumer_tablet_info, thread_pool_.get(), rpcs_.get(),
             local_client_, remote_clients_[replication_group_id], this,
-            last_compatible_consumer_schema_version, get_leader_term_);
+            last_compatible_consumer_schema_version, leader_term, get_leader_term_func_);
         xcluster_poller->Init(use_local_tserver, rate_limiter_.get());
 
         UpdatePollerSchemaVersionMaps(xcluster_poller, producer_tablet_info.stream_id);
@@ -563,6 +564,8 @@ void XClusterConsumer::TriggerPollForNewTablets() {
             "Start polling for producer tablet $0, consumer tablet $1", producer_tablet_info,
             consumer_tablet_info.tablet_id);
         pollers_map_[producer_tablet_info] = xcluster_poller;
+
+        error_collector_.AddPoller(xcluster_poller->GetPollerId());
         xcluster_poller->SchedulePoll();
       }
     }
@@ -610,7 +613,8 @@ void XClusterConsumer::TriggerDeletionOfOldPollers() {
       const ProducerTabletInfo producer_info = it->first;
       const std::shared_ptr<XClusterPoller> poller = it->second;
       // Check if we need to delete this poller.
-      if (ShouldContinuePolling(producer_info, *poller)) {
+      std::string reason;
+      if (ShouldContinuePolling(producer_info, *poller, reason)) {
         ++it;
         continue;
       }
@@ -618,8 +622,8 @@ void XClusterConsumer::TriggerDeletionOfOldPollers() {
       const cdc::ConsumerTabletInfo& consumer_info = poller->GetConsumerTabletInfo();
 
       LOG_WITH_PREFIX(INFO) << Format(
-          "Stop polling for producer tablet $0, consumer tablet $1", producer_info,
-          consumer_info.tablet_id);
+          "Stop polling for producer tablet $0, consumer tablet $1. Reason: $2.", producer_info,
+          consumer_info.tablet_id, reason);
       pollers_to_shutdown.emplace_back(poller);
       it = pollers_map_.erase(it);
 
@@ -642,33 +646,44 @@ void XClusterConsumer::TriggerDeletionOfOldPollers() {
     poller->CompleteShutdown();
   }
 
+  for (const auto& poller : pollers_to_shutdown) {
+    error_collector_.RemovePoller(poller->GetPollerId());
+  }
+
   for (const auto& client : clients_to_delete) {
     client->Shutdown();
   }
 }
 
 bool XClusterConsumer::ShouldContinuePolling(
-    const ProducerTabletInfo producer_tablet_info, const XClusterPoller& poller) {
+    const ProducerTabletInfo producer_tablet_info, const XClusterPoller& poller,
+    std::string& reason) {
   if (FLAGS_TEST_xcluster_disable_delete_old_pollers) {
     return true;
   }
 
-  if (poller.IsFailed() || poller.IsStuck()) {
-    // All failed and stuck pollers need to be deleted. If the tablet leader is still on this node
-    // they will be recreated.
+  if (!poller.ShouldContinuePolling()) {
+    reason = "the Poller failed or is stuck";
     return false;
   }
 
-  const auto& consumer_tablet_info = poller.GetConsumerTabletInfo();
   const auto& it = producer_consumer_tablet_map_from_master_.find(producer_tablet_info);
   // We either no longer need to poll for this tablet, or a different tablet should be polling
   // for it now instead of this one (due to a local tablet split).
-  if (it == producer_consumer_tablet_map_from_master_.end() ||
-      it->consumer_tablet_info.tablet_id != consumer_tablet_info.tablet_id || it->disable_stream) {
-    // We no longer want to poll for this tablet, abort the cycle.
+  if (it == producer_consumer_tablet_map_from_master_.end()) {
+    reason = "the Poller is no longer needed";
+    return false;
+  }
+  if (it->consumer_tablet_info.tablet_id != poller.GetConsumerTabletInfo().tablet_id) {
+    reason = "the consumer tablet changed";
+    return false;
+  }
+  if (it->disable_stream) {
+    reason = "the stream is paused";
     return false;
   }
-  return is_leader_for_tablet_(it->consumer_tablet_info.tablet_id);
+
+  return true;
 }
 
 std::string XClusterConsumer::LogPrefix() { return log_prefix_; }
@@ -776,28 +791,30 @@ Status XClusterConsumer::PublishXClusterSafeTime() {
 }
 
 void XClusterConsumer::StoreReplicationError(
-    const TabletId& tablet_id,
-    const xrepl::StreamId& stream_id,
-    const ReplicationErrorPb error,
-    const std::string& detail) {
-  std::lock_guard lock(tablet_replication_error_map_lock_);
-  tablet_replication_error_map_[tablet_id][stream_id][error] = detail;
+    const XClusterPollerId& poller_id, ReplicationErrorPb error) {
+  error_collector_.StoreError(poller_id, error);
 }
 
-void XClusterConsumer::ClearReplicationError(
-    const TabletId& tablet_id, const xrepl::StreamId& stream_id) {
-  std::lock_guard l(tablet_replication_error_map_lock_);
-  if (!tablet_replication_error_map_.contains(tablet_id)) {
-    return;
+// This happens on TS.heartbeat request, so it needs to finish quickly.
+void XClusterConsumer::PopulateMasterHeartbeatRequest(
+    master::TSHeartbeatRequestPB* req, bool needs_full_tablet_report) {
+  // Map of ReplicationGroupId, consumer TableId, producer TabletId to consumer term and error.
+  auto errors_to_send = error_collector_.GetErrorsToSend(needs_full_tablet_report);
+
+  for (const auto& [replication_group_id, table_map] : errors_to_send) {
+    auto* replication_group_status = req->add_xcluster_consumer_replication_status();
+    replication_group_status->set_replication_group_id(std::move(replication_group_id.ToString()));
+    for (const auto& [consumer_table_id, producer_tablet_map] : table_map) {
+      auto* table_status = replication_group_status->add_table_status();
+      table_status->set_consumer_table_id(std::move(consumer_table_id));
+      for (const auto& [producer_tablet_id, error_info] : producer_tablet_map) {
+        auto* stream_tablet_status = table_status->add_stream_tablet_status();
+        stream_tablet_status->set_producer_tablet_id(std::move(producer_tablet_id));
+        stream_tablet_status->set_consumer_term(error_info.consumer_term);
+        stream_tablet_status->set_error(error_info.error);
+      }
+    }
   }
-
-  tablet_replication_error_map_[tablet_id].erase(stream_id);
-  tablet_replication_error_map_.erase(tablet_id);
-}
-
-cdc::TabletReplicationErrorMap XClusterConsumer::GetReplicationErrors() const {
-  std::lock_guard lock(tablet_replication_error_map_lock_);
-  return tablet_replication_error_map_;
 }
 
 }  // namespace tserver
diff --git a/src/yb/tserver/xcluster_consumer.h b/src/yb/tserver/xcluster_consumer.h
index e733abcc10..0a407341e7 100644
--- a/src/yb/tserver/xcluster_consumer.h
+++ b/src/yb/tserver/xcluster_consumer.h
@@ -23,42 +23,43 @@
 #include <boost/multi_index/member.hpp>
 #include <boost/multi_index_container.hpp>
 
+#include "yb/cdc/cdc_consumer.pb.h"
 #include "yb/cdc/cdc_types.h"
 #include "yb/cdc/cdc_util.h"
+#include "yb/cdc/xcluster_types.h"
 #include "yb/client/client_fwd.h"
 #include "yb/common/common_types.pb.h"
 #include "yb/tablet/tablet_types.pb.h"
-#include "yb/cdc/cdc_consumer.pb.h"
+
+#include "yb/tserver/xcluster_consumer_if.h"
+#include "yb/tserver/xcluster_consumer_replication_error.h"
 #include "yb/tserver/xcluster_poller_stats.h"
+
 #include "yb/util/flags/flags_callback.h"
 #include "yb/util/locks.h"
 #include "yb/util/monotime.h"
 
 namespace rocksdb {
-
 class RateLimiter;
-
-}
+}  // namespace rocksdb
 
 namespace yb {
-
 class Thread;
 class ThreadPool;
 
 namespace rpc {
-
 class Messenger;
-class ProxyCache;
 class Rpcs;
 class SecureContext;
-
-} // namespace rpc
+}  // namespace rpc
 
 namespace cdc {
-
 class ConsumerRegistryPB;
+}  // namespace cdc
 
-} // namespace cdc
+namespace master {
+class TSHeartbeatRequestPB;
+}  // namespace master
 
 namespace tserver {
 class XClusterPoller;
@@ -73,62 +74,51 @@ struct XClusterClient {
   void Shutdown();
 };
 
-class XClusterConsumer {
+class XClusterConsumer : public XClusterConsumerIf {
  public:
-  static Result<std::unique_ptr<XClusterConsumer>> Create(
-      std::function<bool(const std::string&)> is_leader_for_tablet,
-      std::function<int64_t(const TabletId&)> get_leader_term, rpc::ProxyCache* proxy_cache,
-      TabletServer* tserver);
-
   XClusterConsumer(
-      std::function<bool(const std::string&)> is_leader_for_tablet,
       std::function<int64_t(const TabletId&)> get_leader_term, rpc::ProxyCache* proxy_cache,
       const std::string& ts_uuid, std::unique_ptr<XClusterClient> local_client);
 
   ~XClusterConsumer();
-  void Shutdown() EXCLUDES(shutdown_mutex_);
+  Status Init();
+  void Shutdown() override EXCLUDES(shutdown_mutex_);
 
   // Refreshes the in memory state when we receive a new registry from master.
-  void RefreshWithNewRegistryFromMaster(const cdc::ConsumerRegistryPB* consumer_registry,
-                                        int32_t cluster_config_version);
+  void HandleMasterHeartbeatResponse(
+      const cdc::ConsumerRegistryPB* consumer_registry, int32_t cluster_config_version) override;
 
-  std::vector<TabletId> TEST_producer_tablets_running();
+  std::vector<TabletId> TEST_producer_tablets_running() const override;
 
-  std::vector<std::shared_ptr<XClusterPoller>> TEST_ListPollers();
+  std::vector<std::shared_ptr<XClusterPoller>> TEST_ListPollers() const override;
 
-  std::vector<XClusterPollerStats> GetPollerStats() const;
+  std::vector<XClusterPollerStats> GetPollerStats() const override;
 
   std::string LogPrefix();
 
   // Return the value stored in cluster_config_version_. Since we are reading an atomic variable,
   // we don't need to hold the mutex.
-  int32_t cluster_config_version() const NO_THREAD_SAFETY_ANALYSIS;
+  int32_t cluster_config_version() const override NO_THREAD_SAFETY_ANALYSIS;
 
   void TEST_IncrementNumSuccessfulWriteRpcs() { TEST_num_successful_write_rpcs_++; }
 
-  uint32_t TEST_GetNumSuccessfulWriteRpcs() {
+  uint32_t TEST_GetNumSuccessfulWriteRpcs() override {
     return TEST_num_successful_write_rpcs_.load(std::memory_order_acquire);
   }
 
-  Status ReloadCertificates();
+  Status ReloadCertificates() override;
 
   Status PublishXClusterSafeTime();
 
-  SchemaVersion GetMinXClusterSchemaVersion(const TableId& table_id,
-      const ColocationId& colocation_id);
-
-  // Stores a replication error and detail. This overwrites a previously stored 'error'.
-  void StoreReplicationError(
-      const TabletId& tablet_id, const xrepl::StreamId& stream_id, ReplicationErrorPb error,
-      const std::string& detail);
+  SchemaVersion GetMinXClusterSchemaVersion(
+      const TableId& table_id, const ColocationId& colocation_id) override;
 
-  // Clears replication errors.
-  void ClearReplicationError(const TabletId& tablet_id, const xrepl::StreamId& stream_id);
+  cdc::XClusterRole TEST_GetXClusterRole() const override { return consumer_role_; }
 
-  // Returns the replication error map.
-  cdc::TabletReplicationErrorMap GetReplicationErrors() const;
+  void PopulateMasterHeartbeatRequest(
+      master::TSHeartbeatRequestPB* req, bool needs_full_tablet_report) override;
 
-  cdc::XClusterRole TEST_GetXClusterRole() const { return consumer_role_; }
+  void StoreReplicationError(const XClusterPollerId& poller_id, ReplicationErrorPb error);
 
  private:
   // Runs a thread that periodically polls for any new threads.
@@ -136,8 +126,9 @@ class XClusterConsumer {
 
   // Loops through all the entries in the registry and creates a producer -> consumer tablet
   // mapping.
-  void UpdateInMemoryState(const cdc::ConsumerRegistryPB* consumer_producer_map,
-                           int32_t cluster_config_version);
+  void UpdateReplicationGroupInMemState(
+      const cdc::ReplicationGroupId& replication_group_id,
+      const yb::cdc::ProducerEntryPB& producer_entry_pb) REQUIRES(master_data_mutex_);
 
   // Loops through all entries in registry from master to check if all producer tablets are being
   // polled for.
@@ -147,8 +138,8 @@ class XClusterConsumer {
   void TriggerDeletionOfOldPollers() EXCLUDES (master_data_mutex_);
 
   bool ShouldContinuePolling(
-      const cdc::ProducerTabletInfo producer_tablet_info,
-      const XClusterPoller& poller) REQUIRES_SHARED(master_data_mutex_);
+      const cdc::ProducerTabletInfo producer_tablet_info, const XClusterPoller& poller,
+      std::string& reason) REQUIRES_SHARED(master_data_mutex_);
 
   void UpdatePollerSchemaVersionMaps(
       std::shared_ptr<XClusterPoller> xcluster_poller, const xrepl::StreamId& stream_id) const
@@ -169,8 +160,7 @@ class XClusterConsumer {
   // Mutex for producer_pollers_map_.
   mutable rw_spinlock pollers_map_mutex_ ACQUIRED_AFTER(master_data_mutex_);
 
-  std::function<bool(const std::string&)> is_leader_for_tablet_;
-  std::function<int64_t(const TabletId&)> get_leader_term_;
+  std::function<int64_t(const TabletId&)> get_leader_term_func_;
 
   class TabletTag;
   using ProducerConsumerTabletMap = boost::multi_index_container <
@@ -246,13 +236,11 @@ class XClusterConsumer {
   bool xcluster_safe_time_table_ready_ GUARDED_BY(safe_time_update_mutex_) = false;
   std::unique_ptr<client::TableHandle> safe_time_table_ GUARDED_BY(safe_time_update_mutex_);
 
-  mutable simple_spinlock tablet_replication_error_map_lock_;
-  cdc::TabletReplicationErrorMap tablet_replication_error_map_
-      GUARDED_BY(tablet_replication_error_map_lock_);
+  XClusterConsumerReplicationErrorCollector error_collector_;
 
   std::unique_ptr<rocksdb::RateLimiter> rate_limiter_;
   FlagCallbackRegistration rate_limiter_callback_;
 };
 
 } // namespace tserver
-} // namespace yb
+}  // namespace yb
diff --git a/src/yb/tserver/xcluster_consumer_if.h b/src/yb/tserver/xcluster_consumer_if.h
new file mode 100644
index 0000000000..7295be1f0f
--- /dev/null
+++ b/src/yb/tserver/xcluster_consumer_if.h
@@ -0,0 +1,71 @@
+// Copyright (c) YugaByte, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
+// in compliance with the License.  You may obtain a copy of the License at
+//
+// http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software distributed under the License
+// is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
+// or implied.  See the License for the specific language governing permissions and limitations
+// under the License.
+//
+
+#pragma once
+
+#include <memory>
+
+#include "yb/cdc/cdc_consumer.pb.h"
+#include "yb/common/common_fwd.h"
+#include "yb/common/entity_ids_types.h"
+#include "yb/util/result.h"
+
+namespace yb {
+
+struct XClusterPollerStats;
+
+namespace cdc {
+class ConsumerRegistryPB;
+}  // namespace cdc
+
+namespace master {
+class TSHeartbeatRequestPB;
+}  // namespace master
+
+namespace rpc {
+class ProxyCache;
+}  // namespace rpc
+
+namespace tserver {
+
+class TabletServer;
+class XClusterPoller;
+
+class XClusterConsumerIf {
+ public:
+  virtual ~XClusterConsumerIf() = default;
+
+  virtual void Shutdown() = 0;
+
+  virtual void HandleMasterHeartbeatResponse(
+      const cdc::ConsumerRegistryPB* consumer_registry, int32_t cluster_config_version) = 0;
+  virtual SchemaVersion GetMinXClusterSchemaVersion(
+      const TableId& table_id, const ColocationId& colocation_id) = 0;
+  virtual int32_t cluster_config_version() const = 0;
+  virtual Status ReloadCertificates() = 0;
+  virtual void PopulateMasterHeartbeatRequest(
+      master::TSHeartbeatRequestPB* req, bool needs_full_tablet_report) = 0;
+  virtual std::vector<XClusterPollerStats> GetPollerStats() const = 0;
+
+  virtual cdc::XClusterRole TEST_GetXClusterRole() const = 0;
+  virtual std::vector<TabletId> TEST_producer_tablets_running() const = 0;
+  virtual uint32_t TEST_GetNumSuccessfulWriteRpcs() = 0;
+  virtual std::vector<std::shared_ptr<XClusterPoller>> TEST_ListPollers() const = 0;
+};
+
+Result<std::unique_ptr<XClusterConsumerIf>> CreateXClusterConsumer(
+    std::function<int64_t(const TabletId&)> get_leader_term, rpc::ProxyCache* proxy_cache,
+    TabletServer* tserver);
+
+}  // namespace tserver
+}  // namespace yb
diff --git a/src/yb/tserver/xcluster_consumer_replication_error-test.cc b/src/yb/tserver/xcluster_consumer_replication_error-test.cc
new file mode 100644
index 0000000000..d95c61768c
--- /dev/null
+++ b/src/yb/tserver/xcluster_consumer_replication_error-test.cc
@@ -0,0 +1,164 @@
+// Copyright (c) YugaByte, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
+// in compliance with the License.  You may obtain a copy of the License at
+//
+// http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software distributed under the License
+// is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
+// or implied.  See the License for the specific language governing permissions and limitations
+// under the License.
+//
+
+#include "yb/tserver/xcluster_consumer_replication_error.h"
+#include "yb/util/test_util.h"
+
+namespace yb::tserver {
+
+using XClusterReplicationErrorSendState::kNotSent;
+using XClusterReplicationErrorSendState::kSending;
+using XClusterReplicationErrorSendState::kSent;
+
+void ValidateErrorInMap(
+    const XClusterReplicationErrorsToSendMap& errors_map, const XClusterPollerId& poller_id,
+    ReplicationErrorPb error) {
+  ASSERT_TRUE(
+      errors_map.contains(poller_id.replication_group_id) &&
+      errors_map.at(poller_id.replication_group_id).contains(poller_id.consumer_table_id) &&
+      errors_map.at(poller_id.replication_group_id)
+          .at(poller_id.consumer_table_id)
+          .contains(poller_id.producer_tablet_id));
+
+  auto& poller_error = errors_map.at(poller_id.replication_group_id)
+                           .at(poller_id.consumer_table_id)
+                           .at(poller_id.producer_tablet_id);
+
+  ASSERT_EQ(poller_error.consumer_term, poller_id.leader_term);
+  ASSERT_EQ(poller_error.error, error);
+}
+
+TEST(XClusterConsumerReplicationError, TestCollector) {
+  XClusterConsumerReplicationErrorCollector collector;
+
+  // Setup 3 pollers across 2 Replication Groups, 2 tables and 3 tablets.
+  const XClusterPollerId poller1(
+      cdc::ReplicationGroupId("Group1"), "TableA", "Tablet1", /*leader_term=*/1);
+  const XClusterPollerId poller2(
+      cdc::ReplicationGroupId("Group1"), "TableA", "Tablet2", /*leader_term=*/2);
+  const XClusterPollerId poller3(
+      cdc::ReplicationGroupId("Group2"), "TableA", "Tablet1", /*leader_term=*/1);
+  collector.AddPoller(poller1);
+  collector.AddPoller(poller2);
+  collector.AddPoller(poller3);
+
+  // Validate the initial state.
+  ASSERT_EQ(collector.TEST_GetSendState(), kSent);
+  auto error_map = collector.TEST_GetErrorMap();
+  ASSERT_EQ(error_map.size(), 3);
+  ASSERT_EQ(error_map[poller1].send_state, kSent);
+  ASSERT_EQ(error_map[poller2].send_state, kSent);
+  ASSERT_EQ(error_map[poller3].send_state, kSent);
+  auto result = collector.GetErrorsToSend(/* get_all_errors */ true);
+  ASSERT_EQ(result.size(), 0);
+  ASSERT_EQ(collector.TEST_GetSendState(), kSent);
+
+  // Store error for Poller1.
+  collector.StoreError(poller1, ReplicationErrorPb::REPLICATION_MISSING_OP_ID);
+  ASSERT_EQ(collector.TEST_GetSendState(), kNotSent);
+  error_map = collector.TEST_GetErrorMap();
+  ASSERT_EQ(error_map[poller1].send_state, kNotSent);
+  ASSERT_EQ(error_map[poller2].send_state, kSent);
+  ASSERT_EQ(error_map[poller3].send_state, kSent);
+
+  // Verify that we only get the one error.
+  result = collector.GetErrorsToSend(/* get_all_errors */ false);
+  ASSERT_EQ(result.size(), 1);
+  ASSERT_NO_FATALS(
+      ValidateErrorInMap(result, poller1, ReplicationErrorPb::REPLICATION_MISSING_OP_ID));
+  ASSERT_EQ(collector.TEST_GetSendState(), kSending);
+  error_map = collector.TEST_GetErrorMap();
+  ASSERT_EQ(error_map[poller1].send_state, kSending);
+  ASSERT_EQ(error_map[poller2].send_state, kSent);
+  ASSERT_EQ(error_map[poller3].send_state, kSent);
+
+  // Store error for Poller3.
+  collector.StoreError(poller3, ReplicationErrorPb::REPLICATION_SCHEMA_MISMATCH);
+  ASSERT_EQ(collector.TEST_GetSendState(), kNotSent);
+  error_map = collector.TEST_GetErrorMap();
+  ASSERT_EQ(error_map[poller1].send_state, kSending);
+  ASSERT_EQ(error_map[poller2].send_state, kSent);
+  ASSERT_EQ(error_map[poller3].send_state, kNotSent);
+
+  // Verify that we get two errors.
+  result = collector.GetErrorsToSend(/* get_all_errors */ false);
+  ASSERT_EQ(result.size(), 2);
+  ASSERT_NO_FATALS(
+      ValidateErrorInMap(result, poller1, ReplicationErrorPb::REPLICATION_MISSING_OP_ID));
+  ASSERT_NO_FATALS(
+      ValidateErrorInMap(result, poller3, ReplicationErrorPb::REPLICATION_SCHEMA_MISMATCH));
+  ASSERT_EQ(collector.TEST_GetSendState(), kSending);
+  error_map = collector.TEST_GetErrorMap();
+  ASSERT_EQ(error_map[poller1].send_state, kSending);
+  ASSERT_EQ(error_map[poller2].send_state, kSent);
+  ASSERT_EQ(error_map[poller3].send_state, kSending);
+
+  // Validate TransitionErrorsFromSendingToSent.
+  collector.TransitionErrorsFromSendingToSent();
+  ASSERT_EQ(collector.TEST_GetSendState(), kSent);
+  error_map = collector.TEST_GetErrorMap();
+  ASSERT_EQ(error_map[poller1].send_state, kSent);
+  ASSERT_EQ(error_map[poller2].send_state, kSent);
+  ASSERT_EQ(error_map[poller3].send_state, kSent);
+
+  // Store error for Poller2.
+  collector.StoreError(poller2, ReplicationErrorPb::REPLICATION_OK);
+  ASSERT_EQ(collector.TEST_GetSendState(), kNotSent);
+  error_map = collector.TEST_GetErrorMap();
+  ASSERT_EQ(error_map[poller1].send_state, kSent);
+  ASSERT_EQ(error_map[poller2].send_state, kNotSent);
+  ASSERT_EQ(error_map[poller3].send_state, kSent);
+
+  // Verify that we get only one error.
+  result = collector.GetErrorsToSend(/* get_all_errors */ false);
+  ASSERT_EQ(result.size(), 1);
+  ASSERT_NO_FATALS(ValidateErrorInMap(result, poller2, ReplicationErrorPb::REPLICATION_OK));
+  ASSERT_EQ(collector.TEST_GetSendState(), kSending);
+  error_map = collector.TEST_GetErrorMap();
+  ASSERT_EQ(error_map[poller1].send_state, kSent);
+  ASSERT_EQ(error_map[poller2].send_state, kSending);
+  ASSERT_EQ(error_map[poller3].send_state, kSent);
+
+  // Validate TransitionErrorsFromSendingToSent.
+  collector.TransitionErrorsFromSendingToSent();
+  ASSERT_EQ(collector.TEST_GetSendState(), kSent);
+  error_map = collector.TEST_GetErrorMap();
+  ASSERT_EQ(error_map[poller1].send_state, kSent);
+  ASSERT_EQ(error_map[poller2].send_state, kSent);
+  ASSERT_EQ(error_map[poller3].send_state, kSent);
+
+  // Verify that we get no errors.
+  result = collector.GetErrorsToSend(/* get_all_errors */ false);
+  ASSERT_EQ(result.size(), 0);
+  ASSERT_EQ(collector.TEST_GetSendState(), kSent);
+  error_map = collector.TEST_GetErrorMap();
+  ASSERT_EQ(error_map[poller1].send_state, kSent);
+  ASSERT_EQ(error_map[poller2].send_state, kSent);
+  ASSERT_EQ(error_map[poller3].send_state, kSent);
+
+  // Get all errors after they have been marked as Sent.
+  result = collector.GetErrorsToSend(/* get_all_errors */ true);
+  ASSERT_EQ(result.size(), 2);
+  ASSERT_NO_FATALS(
+      ValidateErrorInMap(result, poller1, ReplicationErrorPb::REPLICATION_MISSING_OP_ID));
+  ASSERT_NO_FATALS(ValidateErrorInMap(result, poller2, ReplicationErrorPb::REPLICATION_OK));
+  ASSERT_NO_FATALS(
+      ValidateErrorInMap(result, poller3, ReplicationErrorPb::REPLICATION_SCHEMA_MISMATCH));
+  ASSERT_EQ(collector.TEST_GetSendState(), kSending);
+  error_map = collector.TEST_GetErrorMap();
+  ASSERT_EQ(error_map[poller1].send_state, kSending);
+  ASSERT_EQ(error_map[poller2].send_state, kSending);
+  ASSERT_EQ(error_map[poller3].send_state, kSending);
+}
+
+}  // namespace yb::tserver
diff --git a/src/yb/tserver/xcluster_consumer_replication_error.cc b/src/yb/tserver/xcluster_consumer_replication_error.cc
new file mode 100644
index 0000000000..50f92e3628
--- /dev/null
+++ b/src/yb/tserver/xcluster_consumer_replication_error.cc
@@ -0,0 +1,105 @@
+// Copyright (c) YugabyteDB, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
+// in compliance with the License.  You may obtain a copy of the License at
+//
+// http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software distributed under the License
+// is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
+// or implied.  See the License for the specific language governing permissions and limitations
+// under the License.
+//
+
+#include "yb/tserver/xcluster_consumer_replication_error.h"
+
+#include "yb/gutil/map-util.h"
+
+namespace yb::tserver {
+
+XClusterConsumerReplicationErrorCollector::XClusterConsumerReplicationErrorCollector() {}
+
+void XClusterConsumerReplicationErrorCollector::AddPoller(const XClusterPollerId& poller_id) {
+  std::lock_guard l(mutex_);
+  InsertOrDie(&error_map_, poller_id, ReplicationError());
+}
+
+void XClusterConsumerReplicationErrorCollector::RemovePoller(const XClusterPollerId& poller_id) {
+  std::lock_guard l(mutex_);
+  error_map_.erase(poller_id);
+}
+
+void XClusterConsumerReplicationErrorCollector::StoreError(
+    const XClusterPollerId& poller_id, ReplicationErrorPb error) {
+  std::lock_guard l(mutex_);
+  DCHECK(error_map_.contains(poller_id));
+  if (!error_map_.contains(poller_id)) {
+    LOG(WARNING) << "xCluster Poller '" << poller_id
+                 << "' reporting error when it is not expected to";
+    return;
+  }
+  auto& poller_entry = error_map_.at(poller_id);
+  poller_entry.error = error;
+  poller_entry.send_state = XClusterReplicationErrorSendState::kNotSent;
+
+  errors_send_state_ = XClusterReplicationErrorSendState::kNotSent;
+}
+
+void XClusterConsumerReplicationErrorCollector::TransitionErrorsFromSendingToSent() {
+  std::lock_guard l(mutex_);
+  if (errors_send_state_ == XClusterReplicationErrorSendState::kSent) {
+    return;
+  }
+
+  for (auto& [_, error_info] : error_map_) {
+    if (error_info.send_state == XClusterReplicationErrorSendState::kSending) {
+      error_info.send_state = XClusterReplicationErrorSendState::kSent;
+    }
+  }
+
+  if (errors_send_state_ == XClusterReplicationErrorSendState::kSending) {
+    errors_send_state_ = XClusterReplicationErrorSendState::kSent;
+  }
+  // If errors_send_state_ is kNotSent, then it means we have more errors to send.
+}
+
+XClusterReplicationErrorsToSendMap XClusterConsumerReplicationErrorCollector::GetErrorsToSend(
+    bool get_all_errors) {
+  XClusterReplicationErrorsToSendMap errors_to_send;
+  std::lock_guard l(mutex_);
+
+  if (!get_all_errors && errors_send_state_ == XClusterReplicationErrorSendState::kSent) {
+    // Nothing new to send.
+    return errors_to_send;
+  }
+
+  for (auto& [poller_id, error_info] : error_map_) {
+    if (error_info.error == ReplicationErrorPb::REPLICATION_ERROR_UNINITIALIZED) {
+      continue;
+    }
+    if (get_all_errors || error_info.send_state != XClusterReplicationErrorSendState::kSent) {
+      errors_to_send[poller_id.replication_group_id][poller_id.consumer_table_id]
+                    [poller_id.producer_tablet_id] = {poller_id.leader_term, error_info.error};
+      error_info.send_state = XClusterReplicationErrorSendState::kSending;
+    }
+  }
+
+  if (!errors_to_send.empty()) {
+    errors_send_state_ = XClusterReplicationErrorSendState::kSending;
+  }
+
+  return errors_to_send;
+}
+
+XClusterReplicationErrorSendState XClusterConsumerReplicationErrorCollector::TEST_GetSendState()
+    const {
+  std::lock_guard l(mutex_);
+  return errors_send_state_;
+}
+
+std::unordered_map<XClusterPollerId, XClusterConsumerReplicationErrorCollector::ReplicationError>
+XClusterConsumerReplicationErrorCollector::TEST_GetErrorMap() const {
+  std::lock_guard l(mutex_);
+  return error_map_;
+}
+}  // namespace yb::tserver
diff --git a/src/yb/tserver/xcluster_consumer_replication_error.h b/src/yb/tserver/xcluster_consumer_replication_error.h
new file mode 100644
index 0000000000..51b24fb93a
--- /dev/null
+++ b/src/yb/tserver/xcluster_consumer_replication_error.h
@@ -0,0 +1,75 @@
+// Copyright (c) YugabyteDB, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
+// in compliance with the License.  You may obtain a copy of the License at
+//
+// http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software distributed under the License
+// is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
+// or implied.  See the License for the specific language governing permissions and limitations
+// under the License.
+//
+
+#pragma once
+
+#include "yb/cdc/cdc_types.h"
+#include "yb/cdc/xcluster_types.h"
+#include "yb/common/common_types.pb.h"
+
+#include "yb/tserver/xcluster_poller_id.h"
+
+#include "yb/util/enums.h"
+
+namespace yb::tserver {
+
+YB_DEFINE_ENUM(
+    XClusterReplicationErrorSendState,
+    (kNotSent)  // Error has not been sent to master yet.
+    (kSending)  // Error has been sent but master has not yet acknowledged it yet.
+    (kSent));   // Error has been sent and master has acknowledged it.
+
+using XClusterReplicationErrorsToSendMap =
+    std::unordered_map<cdc::ReplicationGroupId, xcluster::ReplicationGroupErrors>;
+
+// Collection of errors from the various Pollers running on this tserver. This class helps keep
+// track of which errors were sent to master.
+class XClusterConsumerReplicationErrorCollector {
+ public:
+  XClusterConsumerReplicationErrorCollector();
+
+  // Add a new Poller to the map. If the poller already exists, this will FATAL.
+  void AddPoller(const XClusterPollerId& poller_id) EXCLUDES(mutex_);
+  void RemovePoller(const XClusterPollerId& poller_id) EXCLUDES(mutex_);
+
+  void StoreError(const XClusterPollerId& poller_id, ReplicationErrorPb error) EXCLUDES(mutex_);
+
+  void TransitionErrorsFromSendingToSent() EXCLUDES(mutex_);
+
+  XClusterReplicationErrorsToSendMap GetErrorsToSend(bool get_all_errors) EXCLUDES(mutex_);
+
+ private:
+  FRIEND_TEST(XClusterConsumerReplicationError, TestCollector);
+
+  struct ReplicationError {
+    ReplicationErrorPb error = ReplicationErrorPb::REPLICATION_ERROR_UNINITIALIZED;
+    // No need to send UNINITIALIZED error to master.
+    XClusterReplicationErrorSendState send_state = XClusterReplicationErrorSendState::kSent;
+  };
+
+  XClusterReplicationErrorSendState TEST_GetSendState() const EXCLUDES(mutex_);
+  std::unordered_map<XClusterPollerId, ReplicationError> TEST_GetErrorMap() const EXCLUDES(mutex_);
+
+  mutable std::mutex mutex_;
+
+  // Stores the error status of each poller. This is accessed on master heartbeat where we need to
+  // be fast. Storing it per poller would require atomics or locks per poller which can slow us
+  // down, hence storing in a dedicated map.
+  std::unordered_map<XClusterPollerId, ReplicationError> error_map_ GUARDED_BY(mutex_);
+
+  // Used to prevent unnecessary scan of the map in performance critical code.
+  XClusterReplicationErrorSendState errors_send_state_ GUARDED_BY(mutex_) =
+      XClusterReplicationErrorSendState::kSent;
+};
+
+}  // namespace yb::tserver
diff --git a/src/yb/tserver/xcluster_output_client.cc b/src/yb/tserver/xcluster_output_client.cc
index d81936fbb3..2efbef640b 100644
--- a/src/yb/tserver/xcluster_output_client.cc
+++ b/src/yb/tserver/xcluster_output_client.cc
@@ -38,6 +38,7 @@
 #include "yb/rpc/rpc.h"
 #include "yb/rpc/rpc_fwd.h"
 #include "yb/tserver/xcluster_consumer.h"
+#include "yb/tserver/xcluster_poller.h"
 #include "yb/tserver/tserver_service.proxy.h"
 #include "yb/tserver/xcluster_write_interface.h"
 #include "yb/util/flags.h"
@@ -62,6 +63,8 @@ DECLARE_int32(cdc_read_rpc_timeout_ms);
 DEFINE_test_flag(bool, xcluster_consumer_fail_after_process_split_op, false,
     "Whether or not to fail after processing a replicated split_op on the consumer.");
 
+DECLARE_bool(TEST_running_test);
+
 #define HANDLE_ERROR_AND_RETURN_IF_NOT_OK(status) \
   do { \
     auto&& _s = (status); \
@@ -71,34 +74,34 @@ DEFINE_test_flag(bool, xcluster_consumer_fail_after_process_split_op, false,
     } \
   } while (0)
 
-#define ACQUIRE_MUTEX_IF_ONLINE \
+#define ACQUIRE_MUTEX_IF_ONLINE_ELSE_RETURN \
   std::lock_guard l(lock_); \
   if (IsOffline()) { \
-    VLOG_WITH_PREFIX(1) << "Aborting ApplyChanges since the output client is shutting down."; \
+    VLOG_WITH_PREFIX(1) << "xCluster output client is shutting down."; \
     return; \
   } \
   do { \
   } while (false)
 
+#define ACQUIRE_MUTEX_IF_ONLINE_ELSE_RETURN_STATUS \
+  std::lock_guard l(lock_); \
+  SCHECK(!IsOffline(), Aborted, LogPrefix(), "xCluster output client went offline")
+
 using namespace std::placeholders;
 
 namespace yb {
 namespace tserver {
 
 XClusterOutputClient::XClusterOutputClient(
-    XClusterConsumer* xcluster_consumer, const cdc::ConsumerTabletInfo& consumer_tablet_info,
+    XClusterPoller* xcluster_poller, const cdc::ConsumerTabletInfo& consumer_tablet_info,
     const cdc::ProducerTabletInfo& producer_tablet_info,
     const std::shared_ptr<XClusterClient>& local_client, ThreadPool* thread_pool, rpc::Rpcs* rpcs,
-    std::function<void(const XClusterOutputClientResponse& response)> apply_changes_clbk,
-    std::function<void(const std::string& reason, const Status& status)> mark_fail_clbk,
     bool use_local_tserver, rocksdb::RateLimiter* rate_limiter)
     : XClusterAsyncExecutor(thread_pool, local_client->messenger.get(), rpcs),
-      xcluster_consumer_(xcluster_consumer),
+      xcluster_poller_(xcluster_poller),
       consumer_tablet_info_(consumer_tablet_info),
       producer_tablet_info_(producer_tablet_info),
       local_client_(local_client),
-      apply_changes_clbk_(std::move(apply_changes_clbk)),
-      mark_fail_clbk_(std::move(mark_fail_clbk)),
       use_local_tserver_(use_local_tserver),
       all_tablets_result_(STATUS(Uninitialized, "Result has not been initialized.")),
       rate_limiter_(rate_limiter) {}
@@ -124,8 +127,13 @@ void XClusterOutputClient::CompleteShutdown() {
 }
 
 void XClusterOutputClient::MarkFailed(const std::string& reason, const Status& status) {
+  ACQUIRE_MUTEX_IF_ONLINE_ELSE_RETURN;
+  MarkFailedUnlocked(reason, status);
+}
+
+void XClusterOutputClient::MarkFailedUnlocked(const std::string& reason, const Status& status) {
   LOG_WITH_PREFIX(WARNING) << "xCluster Output client failed as " << reason;
-  mark_fail_clbk_(reason, status);
+  xcluster_poller_->MarkFailed(reason, status);
 }
 
 void XClusterOutputClient::SetLastCompatibleConsumerSchemaVersion(SchemaVersion schema_version) {
@@ -141,7 +149,7 @@ void XClusterOutputClient::SetLastCompatibleConsumerSchemaVersion(SchemaVersion
 void XClusterOutputClient::UpdateSchemaVersionMappings(
     const cdc::XClusterSchemaVersionMap& schema_version_map,
     const cdc::ColocatedSchemaVersionMap& colocated_schema_version_map) {
-  std::lock_guard l(lock_);
+  ACQUIRE_MUTEX_IF_ONLINE_ELSE_RETURN;
 
   // Incoming schema version map is typically a superset of what we have so merge should be ok.
   for(const auto& [producer_version, consumer_version] : schema_version_map) {
@@ -167,7 +175,7 @@ void XClusterOutputClient::ApplyChanges(std::shared_ptr<cdc::GetChangesResponseP
 
   // Init class variables that threads will use.
   {
-    std::lock_guard l(lock_);
+    ACQUIRE_MUTEX_IF_ONLINE_ELSE_RETURN;
     DCHECK(consensus::OpIdEquals(op_id_, consensus::MinimumOpId()));
     op_id_ = poller_resp->checkpoint().op_id();
     error_status_ = Status::OK();
@@ -257,7 +265,7 @@ Status XClusterOutputClient::SendUserTableWrites() {
   // Send out the buffered writes.
   std::unique_ptr<WriteRequestPB> write_request;
   {
-    std::lock_guard l(lock_);
+    ACQUIRE_MUTEX_IF_ONLINE_ELSE_RETURN_STATUS;
     write_request = write_strategy_->FetchNextRequest();
   }
   if (!write_request) {
@@ -300,7 +308,7 @@ Result<cdc::XClusterSchemaVersionMap> XClusterOutputClient::GetSchemaVersionMap(
 
 Status XClusterOutputClient::ProcessRecord(
     const std::vector<std::string>& tablet_ids, const cdc::CDCRecordPB& record) {
-  std::lock_guard l(lock_);
+  ACQUIRE_MUTEX_IF_ONLINE_ELSE_RETURN_STATUS;
   for (const auto& tablet_id : tablet_ids) {
     SCHECK(!IsOffline(), Aborted, LogPrefix(), "xCluster output client went offline");
 
@@ -380,7 +388,7 @@ Result<bool> XClusterOutputClient::ProcessChangeMetadataOp(const cdc::CDCRecordP
                 record.change_metadata_request().add_table().schema() :
                 record.change_metadata_request().schema();
   {
-    std::lock_guard l(lock_);
+    ACQUIRE_MUTEX_IF_ONLINE_ELSE_RETURN_STATUS;
 
     // If this is a request to add a table and we have already have a mapping for the producer
     // schema version, we can safely ignore the add table as the table and mapping was present at
@@ -446,7 +454,7 @@ Result<bool> XClusterOutputClient::ProcessMetaOp(const cdc::CDCRecordPB& record)
   // Increment processed records, and check for completion.
   bool done = false;
   {
-    std::lock_guard l(lock_);
+    ACQUIRE_MUTEX_IF_ONLINE_ELSE_RETURN_STATUS;
     done = IncProcessedRecordCount();
   }
   return done;
@@ -461,12 +469,12 @@ void XClusterOutputClient::SendNextCDCWriteToTablet(std::unique_ptr<WriteRequest
   auto deadline =
       CoarseMonoClock::Now() + MonoDelta::FromMilliseconds(FLAGS_cdc_write_rpc_timeout_ms);
 
-  ACQUIRE_MUTEX_IF_ONLINE;
+  ACQUIRE_MUTEX_IF_ONLINE_ELSE_RETURN;
 
   auto handle = rpcs_->Prepare();
   if (handle == rpcs_->InvalidHandle()) {
     DCHECK(IsOffline());
-    MarkFailed("we could not prepare rpc as it is shutting down");
+    MarkFailedUnlocked("we could not prepare rpc as it is shutting down");
     return;
   }
 
@@ -490,12 +498,12 @@ void XClusterOutputClient::UpdateSchemaVersionMapping(
   auto deadline =
       CoarseMonoClock::Now() + MonoDelta::FromMilliseconds(FLAGS_cdc_read_rpc_timeout_ms);
 
-  ACQUIRE_MUTEX_IF_ONLINE;
+  ACQUIRE_MUTEX_IF_ONLINE_ELSE_RETURN;
 
   auto handle = rpcs_->Prepare();
   if (handle == rpcs_->InvalidHandle()) {
     DCHECK(IsOffline());
-    MarkFailed("we could not prepare rpc as it is shutting down");
+    MarkFailedUnlocked("we could not prepare rpc as it is shutting down");
     return;
   }
 
@@ -522,7 +530,7 @@ void XClusterOutputClient::DoSchemaVersionCheckDone(
   SchemaVersion producer_schema_version = cdc::kInvalidSchemaVersion;
   ColocationId colocation_id = 0;
   {
-    ACQUIRE_MUTEX_IF_ONLINE;
+    ACQUIRE_MUTEX_IF_ONLINE_ELSE_RETURN;
 
     producer_schema_version = producer_schema_version_;
     colocation_id = colocation_id_;
@@ -537,13 +545,10 @@ void XClusterOutputClient::DoSchemaVersionCheckDone(
     auto msg = Format(
         "XCluster schema mismatch. No matching schema for producer schema $0 with version $1",
         req.schema().DebugString(), producer_schema_version);
-    LOG(WARNING) << msg;
+    LOG(WARNING) << msg << ": " << status;
     if (resp.error().code() == TabletServerErrorPB::MISMATCHED_SCHEMA) {
-      xcluster_consumer_->StoreReplicationError(
-          consumer_tablet_info_.tablet_id,
-          producer_tablet_info_.stream_id,
-          replication_error,
-          msg);
+      ACQUIRE_MUTEX_IF_ONLINE_ELSE_RETURN;
+      xcluster_poller_->StoreReplicationError(replication_error);
     }
     HandleError(status);
     return;
@@ -570,10 +575,10 @@ void XClusterOutputClient::DoSchemaVersionCheckDone(
 
   {
     // Since we are done processing the meta op, we can increment the processed count.
-    std::lock_guard l(lock_);
+    ACQUIRE_MUTEX_IF_ONLINE_ELSE_RETURN;
     // Update the local cache with the response. This should happen automatically on the heartbeat
     // as well when the poller gets updated, but we opportunistically update here.
-    cdc::XClusterSchemaVersionMap *schema_version_map = nullptr;
+    cdc::XClusterSchemaVersionMap* schema_version_map = nullptr;
     if (colocation_id != kColocationIdNotSet) {
       schema_version_map = &colocated_schema_version_map_[colocation_id];
     } else {
@@ -597,12 +602,10 @@ void XClusterOutputClient::DoSchemaVersionCheckDone(
     }
 
     IncProcessedRecordCount();
+    xcluster_poller_->ClearReplicationError();
   }
 
   // Clear out any replication errors.
-  xcluster_consumer_->ClearReplicationError(
-      consumer_tablet_info_.tablet_id,
-      producer_tablet_info_.stream_id);
 
   // MetaOps should be the last ones in a batch, so we should be done at this point.
   HandleResponse();
@@ -617,12 +620,14 @@ void XClusterOutputClient::DoWriteCDCRecordDone(
     HandleError(StatusFromPB(response.error().status()));
     return;
   }
-  xcluster_consumer_->TEST_IncrementNumSuccessfulWriteRpcs();
 
   // See if we need to handle any more writes.
   std::unique_ptr<WriteRequestPB> write_request;
   {
-    ACQUIRE_MUTEX_IF_ONLINE;
+    ACQUIRE_MUTEX_IF_ONLINE_ELSE_RETURN;
+    if (FLAGS_TEST_running_test) {
+      xcluster_poller_->TEST_IncrementNumSuccessfulWriteRpcs();
+    }
     write_request = write_strategy_->FetchNextRequest();
   }
 
@@ -660,7 +665,7 @@ void XClusterOutputClient::HandleError(const Status& s) {
                << ", consumer tablet: " << consumer_tablet_info_.tablet_id;
   }
   {
-    ACQUIRE_MUTEX_IF_ONLINE;
+    ACQUIRE_MUTEX_IF_ONLINE_ELSE_RETURN;
     error_status_ = s;
     // In case of a consumer side tablet split, need to refresh the partitions.
     if (client::ClientError(error_status_) == client::ClientErrorCode::kTablePartitionListIsStale) {
@@ -672,23 +677,21 @@ void XClusterOutputClient::HandleError(const Status& s) {
 }
 
 void XClusterOutputClient::HandleResponse() {
+  // If we're shutting down, then don't try to call the callback as that object might be deleted.
+  ACQUIRE_MUTEX_IF_ONLINE_ELSE_RETURN;
+
   XClusterOutputClientResponse response;
-  {
-    // If we're shutting down, then don't try to call the callback as that object might be deleted.
-    ACQUIRE_MUTEX_IF_ONLINE;
-
-    response.status = error_status_;
-    response.get_changes_response = std::move(get_changes_resp_);
-    if (response.status.ok()) {
-      response.last_applied_op_id = op_id_;
-      response.processed_record_count = processed_record_count_;
-      response.wait_for_version = wait_for_version_;
-    }
-    op_id_ = consensus::MinimumOpId();
-    processed_record_count_ = 0;
+  response.status = error_status_;
+  response.get_changes_response = std::move(get_changes_resp_);
+  if (response.status.ok()) {
+    response.last_applied_op_id = op_id_;
+    response.processed_record_count = processed_record_count_;
+    response.wait_for_version = wait_for_version_;
   }
+  op_id_ = consensus::MinimumOpId();
+  processed_record_count_ = 0;
 
-  apply_changes_clbk_(std::move(response));
+  xcluster_poller_->ApplyChangesCallback(std::move(response));
 }
 
 bool XClusterOutputClient::IncProcessedRecordCount() {
@@ -701,16 +704,13 @@ bool XClusterOutputClient::IncProcessedRecordCount() {
 }
 
 std::shared_ptr<XClusterOutputClient> CreateXClusterOutputClient(
-    XClusterConsumer* xcluster_consumer, const cdc::ConsumerTabletInfo& consumer_tablet_info,
+    XClusterPoller* xcluster_poller, const cdc::ConsumerTabletInfo& consumer_tablet_info,
     const cdc::ProducerTabletInfo& producer_tablet_info,
     const std::shared_ptr<XClusterClient>& local_client, ThreadPool* thread_pool, rpc::Rpcs* rpcs,
-    std::function<void(const XClusterOutputClientResponse& response)> apply_changes_clbk,
-    std::function<void(const std::string& reason, const Status& status)> mark_fail_clbk,
     bool use_local_tserver, rocksdb::RateLimiter* rate_limiter) {
   return std::make_unique<XClusterOutputClient>(
-      xcluster_consumer, consumer_tablet_info, producer_tablet_info, local_client, thread_pool,
-      rpcs, std::move(apply_changes_clbk), std::move(mark_fail_clbk), use_local_tserver,
-      rate_limiter);
+      xcluster_poller, consumer_tablet_info, producer_tablet_info, local_client, thread_pool, rpcs,
+      use_local_tserver, rate_limiter);
 }
 
 }  // namespace tserver
diff --git a/src/yb/tserver/xcluster_output_client.h b/src/yb/tserver/xcluster_output_client.h
index 11cdcfe32f..5ada1dd618 100644
--- a/src/yb/tserver/xcluster_output_client.h
+++ b/src/yb/tserver/xcluster_output_client.h
@@ -41,17 +41,15 @@ struct XClusterOutputClientResponse {
   std::shared_ptr<cdc::GetChangesResponsePB> get_changes_response;
 };
 
-class XClusterConsumer;
+class XClusterPoller;
 struct XClusterClient;
 
 class XClusterOutputClient : public XClusterAsyncExecutor {
  public:
   XClusterOutputClient(
-      XClusterConsumer* xcluster_consumer, const cdc::ConsumerTabletInfo& consumer_tablet_info,
+      XClusterPoller* xcluster_poller, const cdc::ConsumerTabletInfo& consumer_tablet_info,
       const cdc::ProducerTabletInfo& producer_tablet_info,
       const std::shared_ptr<XClusterClient>& local_client, ThreadPool* thread_pool, rpc::Rpcs* rpcs,
-      std::function<void(const XClusterOutputClientResponse& response)> apply_changes_clbk,
-      std::function<void(const std::string& reason, const Status& status)> mark_fail_clbk,
       bool use_local_tserver, rocksdb::RateLimiter* rate_limiter);
   ~XClusterOutputClient();
   void StartShutdown() override;
@@ -75,7 +73,10 @@ class XClusterOutputClient : public XClusterAsyncExecutor {
         consumer_tablet_info_.table_id, consumer_tablet_info_.tablet_id);
   }
   bool IsOffline() override { return shutdown_; }
-  void MarkFailed(const std::string& reason, const Status& status = Status::OK()) override;
+  void MarkFailed(const std::string& reason, const Status& status = Status::OK()) override
+      EXCLUDES(lock_);
+  void MarkFailedUnlocked(const std::string& reason, const Status& status = Status::OK())
+      REQUIRES(lock_);
 
   // Process all records in get_changes_resp_ starting from the start index. If we find a ddl
   // record, then we process the current changes first, wait for those to complete, then process
@@ -123,16 +124,16 @@ class XClusterOutputClient : public XClusterAsyncExecutor {
 
   bool UseLocalTserver();
 
-  XClusterConsumer* xcluster_consumer_;
+  // Even though this is a const we guard it with a lock, since it is unsafe to use after shutdown.
+  // TODO: Once we move the async execution logic to the Poller, it will guarantee that our lifetime
+  // is less than the pollers lifetime, making this always safe to use.
+  XClusterPoller* const xcluster_poller_ GUARDED_BY(lock_);
   const cdc::ConsumerTabletInfo consumer_tablet_info_;
   const cdc::ProducerTabletInfo producer_tablet_info_;
   cdc::XClusterSchemaVersionMap schema_versions_ GUARDED_BY(lock_);
   cdc::ColocatedSchemaVersionMap colocated_schema_version_map_ GUARDED_BY(lock_);
   std::shared_ptr<XClusterClient> local_client_;
 
-  std::function<void(const XClusterOutputClientResponse& response)> apply_changes_clbk_;
-  std::function<void(const std::string& reason, const Status& status)> mark_fail_clbk_;
-
   bool use_local_tserver_;
 
   std::shared_ptr<client::YBTable> table_;
@@ -166,11 +167,9 @@ class XClusterOutputClient : public XClusterAsyncExecutor {
 };
 
 std::shared_ptr<XClusterOutputClient> CreateXClusterOutputClient(
-    XClusterConsumer* xcluster_consumer, const cdc::ConsumerTabletInfo& consumer_tablet_info,
+    XClusterPoller* xcluster_poller, const cdc::ConsumerTabletInfo& consumer_tablet_info,
     const cdc::ProducerTabletInfo& producer_tablet_info,
     const std::shared_ptr<XClusterClient>& local_client, ThreadPool* thread_pool, rpc::Rpcs* rpcs,
-    std::function<void(const XClusterOutputClientResponse& response)> apply_changes_clbk,
-    std::function<void(const std::string& reason, const Status& status)> mark_fail_clbk,
     bool use_local_tserver, rocksdb::RateLimiter* rate_limiter);
 
 } // namespace tserver
diff --git a/src/yb/tserver/xcluster_poller.cc b/src/yb/tserver/xcluster_poller.cc
index 1c43b431c6..a27d2b44e9 100644
--- a/src/yb/tserver/xcluster_poller.cc
+++ b/src/yb/tserver/xcluster_poller.cc
@@ -86,9 +86,11 @@ using namespace std::placeholders;
     } \
   } while (false)
 
-#define ACQUIRE_MUTEX_IF_ONLINE \
+// CompleteShutdown gets and releases the lock after setting shutdown_. Here we lock the mutex
+// before checking shutdown_ to make sure we do not run after CompleteShutdown.
+#define ACQUIRE_MUTEX_IF_ONLINE_ELSE_RETURN \
+  std::lock_guard data_mutex_lock(data_mutex_); \
   RETURN_WHEN_OFFLINE; \
-  std::lock_guard l(data_mutex_); \
   do { \
     if (!IsLeaderTermValid()) { \
       return; \
@@ -105,11 +107,14 @@ XClusterPoller::XClusterPoller(
     const cdc::ConsumerTabletInfo& consumer_tablet_info, ThreadPool* thread_pool, rpc::Rpcs* rpcs,
     const std::shared_ptr<XClusterClient>& local_client,
     const std::shared_ptr<XClusterClient>& producer_client, XClusterConsumer* xcluster_consumer,
-    SchemaVersion last_compatible_consumer_schema_version,
+    SchemaVersion last_compatible_consumer_schema_version, int64_t leader_term,
     std::function<int64_t(const TabletId&)> get_leader_term)
     : XClusterAsyncExecutor(thread_pool, local_client->messenger.get(), rpcs),
       producer_tablet_info_(producer_tablet_info),
       consumer_tablet_info_(consumer_tablet_info),
+      poller_id_(
+          producer_tablet_info.replication_group_id, consumer_tablet_info.table_id,
+          producer_tablet_info.tablet_id, leader_term),
       op_id_(consensus::MinimumOpId()),
       validated_schema_version_(0),
       last_compatible_consumer_schema_version_(last_compatible_consumer_schema_version),
@@ -117,7 +122,9 @@ XClusterPoller::XClusterPoller(
       local_client_(local_client),
       producer_client_(producer_client),
       xcluster_consumer_(xcluster_consumer),
-      producer_safe_time_(HybridTime::kInvalid) {}
+      producer_safe_time_(HybridTime::kInvalid) {
+  DCHECK_NE(GetLeaderTerm(), yb::OpId::kUnknownTerm);
+}
 
 XClusterPoller::~XClusterPoller() {
   VLOG(1) << "Destroying XClusterPoller";
@@ -125,23 +132,10 @@ XClusterPoller::~XClusterPoller() {
 }
 
 void XClusterPoller::Init(bool use_local_tserver, rocksdb::RateLimiter* rate_limiter) {
-  ACQUIRE_MUTEX_IF_ONLINE;
+  ACQUIRE_MUTEX_IF_ONLINE_ELSE_RETURN;
 
   output_client_ = CreateXClusterOutputClient(
-      xcluster_consumer_, consumer_tablet_info_, producer_tablet_info_, local_client_, thread_pool_,
-      rpcs_,
-      [weak_ptr = weak_from_this(), this](const XClusterOutputClientResponse& response) {
-        WeakPtrCallback(
-            weak_ptr, BIND_FUNCTION_AND_ARGS(
-                          XClusterPoller::ApplyChangesCallback,
-                          std::reference_wrapper<const XClusterOutputClientResponse>(response)));
-      },
-      [weak_ptr = weak_from_this(), this](const std::string& reason, const Status& status) {
-        WeakPtrCallback(
-            weak_ptr,
-            BIND_FUNCTION_AND_ARGS(
-                XClusterPoller::MarkFailed, reason, std::reference_wrapper<const Status>(status)));
-      },
+      this, consumer_tablet_info_, producer_tablet_info_, local_client_, thread_pool_, rpcs_,
       use_local_tserver, rate_limiter);
 }
 
@@ -237,7 +231,7 @@ void XClusterPoller::ScheduleSetSchemaVersionIfNeeded(
 
 void XClusterPoller::DoSetSchemaVersion(
     SchemaVersion cur_version, SchemaVersion current_consumer_schema_version) {
-  ACQUIRE_MUTEX_IF_ONLINE;
+  ACQUIRE_MUTEX_IF_ONLINE_ELSE_RETURN;
 
   if (last_compatible_consumer_schema_version_ < current_consumer_schema_version) {
     last_compatible_consumer_schema_version_ = current_consumer_schema_version;
@@ -261,10 +255,6 @@ HybridTime XClusterPoller::GetSafeTime() const {
   return producer_safe_time_;
 }
 
-cdc::ConsumerTabletInfo XClusterPoller::GetConsumerTabletInfo() const {
-  return consumer_tablet_info_;
-}
-
 void XClusterPoller::UpdateSafeTime(int64 new_time) {
   HybridTime new_hybrid_time(new_time);
   if (!new_hybrid_time.is_special()) {
@@ -296,7 +286,7 @@ void XClusterPoller::DoPoll() {
     poll_stats_history_.RecordBeginPoll();
   }
 
-  ACQUIRE_MUTEX_IF_ONLINE;
+  ACQUIRE_MUTEX_IF_ONLINE_ELSE_RETURN;
 
   if (PREDICT_FALSE(FLAGS_TEST_cdc_skip_replication_poll)) {
     idle_polls_ = GetAtomicFlag(&FLAGS_async_replication_max_idle_wait);
@@ -389,20 +379,18 @@ void XClusterPoller::HandleGetChangesResponse(
   }
 
   {
-    ACQUIRE_MUTEX_IF_ONLINE;
+    ACQUIRE_MUTEX_IF_ONLINE_ELSE_RETURN;
 
     if (status.ok()) {
       if (resp->has_error()) {
         LOG_WITH_PREFIX(WARNING) << "XClusterPoller GetChanges failure response: code="
                                  << resp->error().code()
                                  << ", status=" << resp->error().status().DebugString();
-        status = StatusFromPB(resp->error().status());
+        status = StatusFromPB(resp->error().status())
+                     .CloneAndPrepend("Unable to find expected op id on the producer");
 
         if (resp->error().code() == cdc::CDCErrorPB::CHECKPOINT_TOO_OLD) {
-          xcluster_consumer_->StoreReplicationError(
-              consumer_tablet_info_.tablet_id, producer_tablet_info_.stream_id,
-              ReplicationErrorPb::REPLICATION_MISSING_OP_ID,
-              "Unable to find expected op id on the producer");
+          StoreReplicationError(ReplicationErrorPb::REPLICATION_MISSING_OP_ID);
         }
       } else if (!resp->has_checkpoint()) {
         static const auto no_checkpoint_status =
@@ -432,7 +420,7 @@ void XClusterPoller::HandleGetChangesResponse(
   ScheduleApplyChanges(std::move(resp));
 }
 
-void XClusterPoller::ApplyChangesCallback(XClusterOutputClientResponse response) {
+void XClusterPoller::ApplyChangesCallback(XClusterOutputClientResponse&& response) {
   if (FLAGS_enable_xcluster_stat_collection) {
     poll_stats_history_.RecordEndApplyChanges();
   }
@@ -441,7 +429,7 @@ void XClusterPoller::ApplyChangesCallback(XClusterOutputClientResponse response)
       BIND_FUNCTION_AND_ARGS(XClusterPoller::HandleApplyChangesResponse, std::move(response)));
 }
 
-void XClusterPoller::HandleApplyChangesResponse(XClusterOutputClientResponse response) {
+void XClusterPoller::HandleApplyChangesResponse(XClusterOutputClientResponse& response) {
   if (!response.status.ok() ||
       RandomActWithProbability(FLAGS_TEST_xcluster_simulate_random_failure_after_apply)) {
     LOG_WITH_PREFIX(WARNING) << "ApplyChanges failure: " << response.status;
@@ -469,8 +457,10 @@ void XClusterPoller::HandleApplyChangesResponse(XClusterOutputClientResponse res
     poll_stats_history_.RecordEndPoll(num_records, received_index, resp_size);
   }
 
+  ClearReplicationError();
+
   {
-    ACQUIRE_MUTEX_IF_ONLINE;
+    ACQUIRE_MUTEX_IF_ONLINE_ELSE_RETURN;
 
     // Recover slowly if we've gotten congested.
     apply_failures_ = static_cast<uint32>(
@@ -501,7 +491,7 @@ void XClusterPoller::ScheduleApplyChanges(
     std::shared_ptr<cdc::GetChangesResponsePB> get_changes_response) {
   DCHECK(get_changes_response);
 
-  ACQUIRE_MUTEX_IF_ONLINE;
+  ACQUIRE_MUTEX_IF_ONLINE_ELSE_RETURN;
 
   int64_t delay_ms = 0;
   if (apply_failures_ > 0) {
@@ -520,7 +510,7 @@ void XClusterPoller::ApplyChanges(std::shared_ptr<cdc::GetChangesResponsePB> get
     poll_stats_history_.RecordBeginApplyChanges();
   }
 
-  ACQUIRE_MUTEX_IF_ONLINE;
+  ACQUIRE_MUTEX_IF_ONLINE_ELSE_RETURN;
 
   UpdateSchemaVersionsForApply();
   output_client_->ApplyChanges(get_changes_response);
@@ -536,17 +526,8 @@ bool XClusterPoller::IsLeaderTermValid() {
   }
 
   auto current_term = get_leader_term_(consumer_tablet_info_.tablet_id);
-  if (leader_term_ == OpId::kUnknownTerm) {
-    leader_term_ = current_term;
-  }
-
-  if (current_term == OpId::kUnknownTerm || current_term != leader_term_) {
-    std::string msg = "leader term is unknown";
-    if (current_term != OpId::kUnknownTerm) {
-      msg = Format("leader term changed. Expected: $0, Current: $1", leader_term_, current_term);
-    }
-
-    MarkFailed(msg);
+  if (current_term != GetLeaderTerm()) {
+    MarkFailed(Format("leader term changed. Old: $0, New: $1", GetLeaderTerm(), current_term));
     return false;
   }
 
@@ -576,6 +557,17 @@ std::string XClusterPoller::State() const {
   return "Running";
 }
 
+bool XClusterPoller::ShouldContinuePolling() const {
+  if (is_failed_ || IsStuck()) {
+    // All failed and stuck pollers need to be deleted. If the tablet leader is still on this node
+    // they will be recreated.
+    return false;
+  }
+
+  // If we are not the leader, we should not poll.
+  return GetLeaderTerm() == get_leader_term_(consumer_tablet_info_.tablet_id);
+}
+
 void XClusterPoller::MarkFailed(const std::string& reason, const Status& status) {
   LOG_WITH_PREFIX(WARNING) << "Stopping xCluster Poller as " << reason
                            << (status.ok() ? "" : Format(": $0", status));
@@ -590,5 +582,24 @@ XClusterPollerStats XClusterPoller::GetStats() const {
   return stats;
 }
 
+void XClusterPoller::StoreReplicationError(ReplicationErrorPb error) {
+  DCHECK_NE(error, ReplicationErrorPb::REPLICATION_ERROR_UNINITIALIZED);
+
+  std::lock_guard l(replication_error_mutex_);
+  if (previous_replication_error_ != error) {
+    // Avoid unnecessarily storing same errors since this is used in perf critical master heartbeat
+    // path.
+    xcluster_consumer_->StoreReplicationError(GetPollerId(), error);
+    previous_replication_error_ = error;
+  }
+}
+
+void XClusterPoller::ClearReplicationError() {
+  StoreReplicationError(ReplicationErrorPb::REPLICATION_OK);
+}
+
+void XClusterPoller::TEST_IncrementNumSuccessfulWriteRpcs() {
+  xcluster_consumer_->TEST_IncrementNumSuccessfulWriteRpcs();
+}
 }  // namespace tserver
 }  // namespace yb
diff --git a/src/yb/tserver/xcluster_poller.h b/src/yb/tserver/xcluster_poller.h
index fedc1a7d93..4bdece2a42 100644
--- a/src/yb/tserver/xcluster_poller.h
+++ b/src/yb/tserver/xcluster_poller.h
@@ -18,7 +18,7 @@
 #include "yb/tserver/xcluster_async_executor.h"
 #include "yb/tserver/xcluster_output_client.h"
 #include "yb/common/hybrid_time.h"
-#include "yb/tserver/xcluster_consumer.h"
+#include "yb/tserver/xcluster_poller_id.h"
 #include "yb/tserver/xcluster_poller_stats.h"
 #include "yb/tserver/tablet_server.h"
 #include "yb/util/locks.h"
@@ -53,7 +53,7 @@ class XClusterPoller : public XClusterAsyncExecutor {
       const cdc::ConsumerTabletInfo& consumer_tablet_info, ThreadPool* thread_pool, rpc::Rpcs* rpcs,
       const std::shared_ptr<XClusterClient>& local_client,
       const std::shared_ptr<XClusterClient>& producer_client, XClusterConsumer* xcluster_consumer,
-      SchemaVersion last_compatible_consumer_schema_version,
+      SchemaVersion last_compatible_consumer_schema_version, int64_t leader_term,
       std::function<int64_t(const TabletId&)> get_leader_term);
   ~XClusterPoller();
 
@@ -62,7 +62,7 @@ class XClusterPoller : public XClusterAsyncExecutor {
   void StartShutdown() override;
   void CompleteShutdown() override;
 
-  bool IsFailed() const { return is_failed_.load(); }
+  bool ShouldContinuePolling() const;
 
   // Begins poll process for a producer tablet.
   void SchedulePoll();
@@ -78,16 +78,27 @@ class XClusterPoller : public XClusterAsyncExecutor {
       EXCLUDES(schema_version_lock_);
 
   std::string LogPrefix() const override;
+  const XClusterPollerId& GetPollerId() const { return poller_id_; }
 
   HybridTime GetSafeTime() const EXCLUDES(safe_time_lock_);
 
-  cdc::ConsumerTabletInfo GetConsumerTabletInfo() const;
+  const cdc::ConsumerTabletInfo& GetConsumerTabletInfo() const { return consumer_tablet_info_; }
+  const cdc::ProducerTabletInfo& GetProducerTabletInfo() const { return producer_tablet_info_; }
 
   bool IsStuck() const;
   std::string State() const;
 
   XClusterPollerStats GetStats() const;
 
+  int64_t GetLeaderTerm() const { return poller_id_.leader_term; }
+
+  void MarkFailed(const std::string& reason, const Status& status = Status::OK()) override;
+  // Stores a replication error and detail. This overwrites a previously stored 'error'.
+  void StoreReplicationError(ReplicationErrorPb error) EXCLUDES(replication_error_mutex_);
+  void ClearReplicationError() EXCLUDES(replication_error_mutex_);
+  void TEST_IncrementNumSuccessfulWriteRpcs();
+  void ApplyChangesCallback(XClusterOutputClientResponse&& response);
+
  private:
   bool IsOffline() override;
 
@@ -101,26 +112,20 @@ class XClusterPoller : public XClusterAsyncExecutor {
   void ScheduleApplyChanges(std::shared_ptr<cdc::GetChangesResponsePB> get_changes_response);
   void ApplyChanges(std::shared_ptr<cdc::GetChangesResponsePB> get_changes_response)
       EXCLUDES(data_mutex_);
-  void ApplyChangesCallback(XClusterOutputClientResponse response);
-  void HandleApplyChangesResponse(XClusterOutputClientResponse response) EXCLUDES(data_mutex_);
+  void HandleApplyChangesResponse(XClusterOutputClientResponse& response) EXCLUDES(data_mutex_);
   void UpdateSafeTime(int64 new_time) EXCLUDES(safe_time_lock_);
   void UpdateSchemaVersionsForApply() EXCLUDES(schema_version_lock_);
   bool IsLeaderTermValid() REQUIRES(data_mutex_);
 
-  void MarkFailed(const std::string& reason, const Status& status = Status::OK()) override;
-
   const cdc::ProducerTabletInfo producer_tablet_info_;
   const cdc::ConsumerTabletInfo consumer_tablet_info_;
+  const XClusterPollerId poller_id_;
 
   mutable rw_spinlock schema_version_lock_;
   cdc::XClusterSchemaVersionMap schema_version_map_ GUARDED_BY(schema_version_lock_);
   cdc::ColocatedSchemaVersionMap colocated_schema_version_map_ GUARDED_BY(schema_version_lock_);
 
-  // Although this is processing serially, it might be on a different thread in the ThreadPool.
-  // Using mutex to guarantee cache flush, preventing TSAN warnings.
-  // Recursive, since when we abort the CDCReadRpc, that will also call the callback within the
-  // same thread (HandlePoll()) which needs to Unregister poll_handle_ from rpcs_.
-  std::mutex data_mutex_;
+  mutable std::mutex data_mutex_;
 
   std::atomic<bool> shutdown_ = false;
   // In failed state we do not poll for changes and are awaiting shutdown.
@@ -137,7 +142,8 @@ class XClusterPoller : public XClusterAsyncExecutor {
   std::shared_ptr<XClusterOutputClient> output_client_;
   std::shared_ptr<XClusterClient> producer_client_;
 
-  XClusterConsumer* xcluster_consumer_ GUARDED_BY(data_mutex_);
+  // Unsafe to use after shutdown.
+  XClusterConsumer* const xcluster_consumer_;
 
   mutable rw_spinlock safe_time_lock_;
   HybridTime producer_safe_time_ GUARDED_BY(safe_time_lock_);
@@ -147,7 +153,10 @@ class XClusterPoller : public XClusterAsyncExecutor {
   std::atomic<uint32> apply_failures_ = 0;
   std::atomic<uint32> idle_polls_ = 0;
 
-  int64_t leader_term_ GUARDED_BY(data_mutex_) = OpId::kUnknownTerm;
+  // Replication errors that are tracked and reported to the master.
+  std::mutex replication_error_mutex_;
+  ReplicationErrorPb previous_replication_error_ GUARDED_BY(replication_error_mutex_) =
+      ReplicationErrorPb::REPLICATION_ERROR_UNINITIALIZED;
 
   PollStatsHistory poll_stats_history_;
 };
diff --git a/src/yb/tserver/xcluster_poller_id.cc b/src/yb/tserver/xcluster_poller_id.cc
new file mode 100644
index 0000000000..f5d3ecec76
--- /dev/null
+++ b/src/yb/tserver/xcluster_poller_id.cc
@@ -0,0 +1,47 @@
+// Copyright (c) YugabyteDB, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
+// in compliance with the License.  You may obtain a copy of the License at
+//
+// http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software distributed under the License
+// is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
+// or implied.  See the License for the specific language governing permissions and limitations
+// under the License.
+//
+
+#include "yb/tserver/xcluster_poller_id.h"
+
+namespace yb {
+
+XClusterPollerId::XClusterPollerId(
+    cdc::ReplicationGroupId replication_group_id, TableId consumer_table_id,
+    TabletId producer_tablet_id, int64 leader_term)
+    : replication_group_id(replication_group_id),
+      consumer_table_id(consumer_table_id),
+      producer_tablet_id(producer_tablet_id),
+      leader_term(leader_term) {}
+
+bool XClusterPollerId::operator==(const XClusterPollerId& other) const {
+  return replication_group_id == other.replication_group_id &&
+         consumer_table_id == other.consumer_table_id &&
+         producer_tablet_id == other.producer_tablet_id && leader_term == other.leader_term;
+}
+
+std::string XClusterPollerId::ToString() const {
+  return YB_STRUCT_TO_STRING(
+      replication_group_id, consumer_table_id, producer_tablet_id, leader_term);
+}
+
+std::size_t XClusterPollerId::GetHash() const {
+  std::size_t hash = 0;
+  boost::hash_combine(hash, replication_group_id);
+  boost::hash_combine(hash, consumer_table_id);
+  boost::hash_combine(hash, producer_tablet_id);
+  boost::hash_combine(hash, leader_term);
+
+  return hash;
+}
+
+}  // namespace yb
diff --git a/src/yb/tserver/xcluster_poller_id.h b/src/yb/tserver/xcluster_poller_id.h
new file mode 100644
index 0000000000..0bdb83e614
--- /dev/null
+++ b/src/yb/tserver/xcluster_poller_id.h
@@ -0,0 +1,55 @@
+// Copyright (c) YugabyteDB, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
+// in compliance with the License.  You may obtain a copy of the License at
+//
+// http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software distributed under the License
+// is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
+// or implied.  See the License for the specific language governing permissions and limitations
+// under the License.
+//
+
+#pragma once
+
+#include "yb/cdc/cdc_types.h"
+
+namespace yb {
+
+// The minimum required information to uniquely identify a xCluster Poller instance across the
+// universe. Each time the tablet leader peer changes the term is incremented. The Poller is tied to
+// a single term.
+// Each consumer table is linked to one producer table by one stream, so producer table id and
+// stream id are redundant information that is ignored. When the tablet counts between the producer
+// and consumer do not match, a single Poller can write to multiple consumer tablets, and multiple
+// Pollers belonging to the same ReplicationGroup can write to the same consumer tablet.
+struct XClusterPollerId {
+  cdc::ReplicationGroupId replication_group_id;
+  TableId consumer_table_id;
+  TabletId producer_tablet_id;
+  int64 leader_term;
+
+  explicit XClusterPollerId(
+      cdc::ReplicationGroupId replication_group_id, TableId consumer_table_id,
+      TabletId producer_tablet_id, int64 leader_term);
+
+  bool operator==(const XClusterPollerId& other) const;
+
+  std::string ToString() const;
+
+  std::size_t GetHash() const;
+};
+
+inline std::ostream& operator<<(std::ostream& out, const XClusterPollerId& poller_info) {
+  return out << poller_info.ToString();
+}
+
+}  // namespace yb
+
+namespace std {
+template <>
+struct hash<yb::XClusterPollerId> {
+  size_t operator()(const yb::XClusterPollerId& poller_id) const { return poller_id.GetHash(); }
+};
+}  // namespace std
