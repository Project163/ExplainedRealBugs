diff --git a/CMakeLists.txt b/CMakeLists.txt
index 021187b7e5..38b5b1d9bc 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -363,6 +363,7 @@ if(IS_CLANG)
   ADD_CXX_FLAGS("-Werror=implicit-fallthrough")
   ADD_CXX_FLAGS("-D_LIBCPP_ENABLE_THREAD_SAFETY_ANNOTATIONS")
   ADD_CXX_FLAGS("-Wthread-safety-analysis")
+  ADD_CXX_FLAGS("-Wthread-safety-reference")
   ADD_CXX_FLAGS("-Wshorten-64-to-32")
 endif()
 
diff --git a/src/yb/client/transaction.cc b/src/yb/client/transaction.cc
index e506c60292..817f9b131d 100644
--- a/src/yb/client/transaction.cc
+++ b/src/yb/client/transaction.cc
@@ -1181,12 +1181,11 @@ class YBTransaction::Impl final : public internal::TxnBatcherIf {
   void DoCommit(
       CoarseTimePoint deadline, SealOnly seal_only, const Status& status,
       const YBTransactionPtr& transaction) EXCLUDES(mutex_) {
+    UniqueLock lock(mutex_);
     VLOG_WITH_PREFIX(1)
         << Format("Commit, seal_only: $0, tablets: $1, status: $2",
                   seal_only, tablets_, status);
 
-    UniqueLock lock(mutex_);
-
     if (!status.ok()) {
       VLOG_WITH_PREFIX(4) << "Commit failed: " << status;
       auto commit_callback = std::move(commit_callback_);
@@ -1511,9 +1510,9 @@ class YBTransaction::Impl final : public internal::TxnBatcherIf {
         client::UseCache::kTrue);
   }
 
-  void LookupTabletDone(const Result<client::internal::RemoteTabletPtr>& result,
-                        const YBTransactionPtr& transaction,
-                        TransactionPromoting promoting) {
+  void LookupTabletDone(
+      const Result<client::internal::RemoteTabletPtr>& result, const YBTransactionPtr& transaction,
+      TransactionPromoting promoting) EXCLUDES(mutex_) {
     TRACE_TO(trace_, __func__);
     VLOG_WITH_PREFIX(1) << "Lookup tablet done: " << yb::ToString(result);
 
@@ -1527,7 +1526,12 @@ class YBTransaction::Impl final : public internal::TxnBatcherIf {
 
     if (status == TransactionStatus::ABORTED) {
       DCHECK(promoting);
-      SendAbortToOldStatusTabletIfNeeded(TransactionRpcDeadline(), transaction, old_status_tablet_);
+      decltype(old_status_tablet_) old_status_tablet;
+      {
+        SharedLock lock(mutex_);
+        old_status_tablet = old_status_tablet_;
+      }
+      SendAbortToOldStatusTabletIfNeeded(TransactionRpcDeadline(), transaction, old_status_tablet);
     } else {
       SendHeartbeat(status, metadata_.transaction_id, transaction_->shared_from_this(),
                     SendHeartbeatToNewTablet(promoting));
diff --git a/src/yb/consensus/log.cc b/src/yb/consensus/log.cc
index 36ef7882c8..44f6966d87 100644
--- a/src/yb/consensus/log.cc
+++ b/src/yb/consensus/log.cc
@@ -1300,6 +1300,11 @@ Status Log::GetSegmentsToGCUnlocked(int64_t min_op_idx, SegmentSequence* segment
   return Status::OK();
 }
 
+Status Log::GetSegmentsToGC(int64_t min_op_idx, SegmentSequence* segments_to_gc) const {
+  PerCpuRwSharedLock read_lock(state_lock_);
+  return GetSegmentsToGCUnlocked(min_op_idx, segments_to_gc);
+}
+
 void Log::ApplyTimeRetentionPolicy(SegmentSequence* segments_to_gc) const {
   // Don't GC segments that are newer than the configured time-based retention.
   int64_t now = GetCurrentTimeMicros() + FLAGS_time_based_wal_gc_clock_delta_usec;
diff --git a/src/yb/consensus/log.h b/src/yb/consensus/log.h
index 9b85c39a96..9972d5396c 100644
--- a/src/yb/consensus/log.h
+++ b/src/yb/consensus/log.h
@@ -446,7 +446,10 @@ class Log : public RefCountedThreadSafe<Log> {
   Status UpdateSegmentReadableOffset() EXCLUDES(active_segment_mutex_);
 
   // Helper method to get the segment sequence to GC based on the provided min_op_idx.
-  Status GetSegmentsToGCUnlocked(int64_t min_op_idx, SegmentSequence* segments_to_gc) const;
+  Status GetSegmentsToGCUnlocked(int64_t min_op_idx, SegmentSequence* segments_to_gc) const
+      REQUIRES_SHARED(state_lock_);
+  Status GetSegmentsToGC(int64_t min_op_idx, SegmentSequence* segments_to_gc) const
+      EXCLUDES(state_lock_);
 
   // Discards segments from 'segments_to_gc' if they have not yet met the minimim retention time.
   void ApplyTimeRetentionPolicy(SegmentSequence* segments_to_gc) const;
diff --git a/src/yb/integration-tests/cdc_service-int-test.cc b/src/yb/integration-tests/cdc_service-int-test.cc
index 3ccad183ba..622b9346f3 100644
--- a/src/yb/integration-tests/cdc_service-int-test.cc
+++ b/src/yb/integration-tests/cdc_service-int-test.cc
@@ -1788,16 +1788,16 @@ TEST_F(CDCServiceTestMaxRentionTime, TestLogRetentionByOpId_MaxRentionTime) {
   // Since we haven't updated the minimum cdc index, and the elapsed time is less than
   // kMaxSecondsToRetain, no log files should be returned.
   log::SegmentSequence segment_sequence;
-  ASSERT_OK(tablet_peer->log()->GetSegmentsToGCUnlocked(std::numeric_limits<int64_t>::max(),
-                                                        &segment_sequence));
+  ASSERT_OK(
+      tablet_peer->log()->GetSegmentsToGC(std::numeric_limits<int64_t>::max(), &segment_sequence));
   ASSERT_EQ(segment_sequence.size(), 0);
   LOG(INFO) << "No segments to be GCed because less than " << kMaxSecondsToRetain
             << " seconds have elapsed";
 
   SleepFor(time_to_sleep);
 
-  ASSERT_OK(tablet_peer->log()->GetSegmentsToGCUnlocked(std::numeric_limits<int64_t>::max(),
-                                                              &segment_sequence));
+  ASSERT_OK(
+      tablet_peer->log()->GetSegmentsToGC(std::numeric_limits<int64_t>::max(), &segment_sequence));
   ASSERT_GT(segment_sequence.size(), 0);
   const auto& segments_violate =
       *(tablet_peer->log()->reader_->TEST_segments_violate_max_time_policy_);
@@ -1979,14 +1979,14 @@ TEST_F(CDCServiceTestMinSpace, TestLogRetentionByOpId_MinSpace) {
   }
 
   log::SegmentSequence segment_sequence;
-  ASSERT_OK(tablet_peer->log()->GetSegmentsToGCUnlocked(std::numeric_limits<int64_t>::max(),
-                                                        &segment_sequence));
+  ASSERT_OK(
+      tablet_peer->log()->GetSegmentsToGC(std::numeric_limits<int64_t>::max(), &segment_sequence));
   ASSERT_EQ(segment_sequence.size(), 0);
 
   ANNOTATE_UNPROTECTED_WRITE(FLAGS_TEST_simulate_free_space_bytes) = 128;
 
-  ASSERT_OK(tablet_peer->log()->GetSegmentsToGCUnlocked(std::numeric_limits<int64_t>::max(),
-                                                        &segment_sequence));
+  ASSERT_OK(
+      tablet_peer->log()->GetSegmentsToGC(std::numeric_limits<int64_t>::max(), &segment_sequence));
   ASSERT_GT(segment_sequence.size(), 0);
   const auto& segments_violate =
       *(tablet_peer->log()->reader_->TEST_segments_violate_min_space_policy_);
diff --git a/src/yb/master/backfill_index.cc b/src/yb/master/backfill_index.cc
index 029a9b3788..85b74db290 100644
--- a/src/yb/master/backfill_index.cc
+++ b/src/yb/master/backfill_index.cc
@@ -779,8 +779,7 @@ Status BackfillTable::LaunchComputeSafeTimeForRead() {
       SCHECK(!res->is_special(), InvalidArgument, "Invalid xCluster safe time for namespace ",
              indexed_table_->namespace_id());
 
-      LOG_WITH_PREFIX(INFO) << "Using xCluster safe time " << read_time_for_backfill_
-                            << " as the backfill read time";
+      LOG_WITH_PREFIX(INFO) << "Using xCluster safe time " << *res << " as the backfill read time";
       return SetSafeTimeAndStartBackfill(*res);
     } else {
       if (res.status().IsNotFound()) {
@@ -958,8 +957,10 @@ Status BackfillTable::DoBackfill() {
     LOG(INFO) << Format("Blocking $0 for $1", __func__, kSpinWait);
     SleepFor(kSpinWait);
   }
-  VLOG_WITH_PREFIX(1) << "starting backfill with timestamp: "
-                      << read_time_for_backfill_;
+  if (VLOG_IS_ON(1)) {
+    std::lock_guard l(mutex_);
+    VLOG_WITH_PREFIX(1) << "starting backfill with timestamp: " << read_time_for_backfill_;
+  }
 
   num_tablets_.store(tablets.size(), std::memory_order_release);
   tablets_pending_.store(tablets.size(), std::memory_order_release);
diff --git a/src/yb/master/backfill_index.h b/src/yb/master/backfill_index.h
index ee861fe32f..7101a16814 100644
--- a/src/yb/master/backfill_index.h
+++ b/src/yb/master/backfill_index.h
@@ -163,7 +163,7 @@ class BackfillTable : public std::enable_shared_from_this<BackfillTable> {
   void LaunchBackfillOrAbort();
   Status WaitForTabletSplitting();
   Status DoLaunchBackfill();
-  Status LaunchComputeSafeTimeForRead();
+  Status LaunchComputeSafeTimeForRead() EXCLUDES(mutex_);
   Status DoBackfill();
 
   Status MarkAllIndexesAsFailed();
diff --git a/src/yb/master/catalog_manager.cc b/src/yb/master/catalog_manager.cc
index 80e2c9221c..6cdb63998c 100644
--- a/src/yb/master/catalog_manager.cc
+++ b/src/yb/master/catalog_manager.cc
@@ -2660,7 +2660,7 @@ void CatalogManager::RefreshTablespaceInfoPeriodically() {
     return;
   }
 
-  LeaderEpoch epoch(-1);
+  LeaderEpoch epoch;
   {
     SCOPED_LEADER_SHARED_LOCK(l, this);
     if (!l.IsInitializedAndIsLeader()) {
@@ -9830,12 +9830,13 @@ Status CatalogManager::GetUDTypeInfo(const GetUDTypeInfoRequestPB* req,
   }
 
   if (req->type().has_type_id()) {
+    SharedLock lock(mutex_);
     tp = FindPtrOrNull(udtype_ids_map_, req->type().type_id());
   } else if (req->type().has_type_name() && req->type().has_namespace_()) {
     // Lookup the type and verify if it exists.
     TRACE("Looking up namespace");
     ns = VERIFY_NAMESPACE_FOUND(FindNamespace(req->type().namespace_()), resp);
-
+    SharedLock lock(mutex_);
     tp = FindPtrOrNull(udtype_names_map_, std::make_pair(ns->id(), req->type().type_name()));
   }
 
diff --git a/src/yb/master/catalog_manager.h b/src/yb/master/catalog_manager.h
index f5bc1d05ed..646869e14c 100644
--- a/src/yb/master/catalog_manager.h
+++ b/src/yb/master/catalog_manager.h
@@ -1926,7 +1926,7 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
   Status EnableBgTasks();
 
   // Helper function for RebuildYQLSystemPartitions to get the system.partitions tablet.
-  Status GetYQLPartitionsVTable(std::shared_ptr<SystemTablet>* tablet);
+  Status GetYQLPartitionsVTable(std::shared_ptr<SystemTablet>* tablet) REQUIRES(mutex_);
   // Background task for automatically rebuilding system.partitions every
   // partitions_vtable_cache_refresh_secs seconds.
   void RebuildYQLSystemPartitions();
diff --git a/src/yb/master/cluster_balance.cc b/src/yb/master/cluster_balance.cc
index 039969646d..fbf1760c6b 100644
--- a/src/yb/master/cluster_balance.cc
+++ b/src/yb/master/cluster_balance.cc
@@ -287,8 +287,7 @@ bool ClusterLoadBalancer::IsLoadBalancerEnabled() const {
 ClusterLoadBalancer::ClusterLoadBalancer(CatalogManager* cm)
     : random_(GetRandomSeed32()),
       is_enabled_(FLAGS_enable_load_balancing),
-      cbuf_activities_(FLAGS_load_balancer_num_idle_runs),
-      epoch_(LeaderEpoch(-1)) {
+      cbuf_activities_(FLAGS_load_balancer_num_idle_runs) {
   ResetGlobalState(false /* initialize_ts_descs */);
 
   catalog_manager_ = cm;
diff --git a/src/yb/master/leader_epoch.h b/src/yb/master/leader_epoch.h
index 4edb7412e8..e4da28af99 100644
--- a/src/yb/master/leader_epoch.h
+++ b/src/yb/master/leader_epoch.h
@@ -51,6 +51,8 @@ struct LeaderEpoch {
       : leader_term(term), pitr_count(pitr_count) {}
 
   explicit LeaderEpoch(int64_t term) : LeaderEpoch(term, 0) {}
+
+  LeaderEpoch() : LeaderEpoch(-1, 0) {}
 };
 
 }  // namespace master
diff --git a/src/yb/master/master_snapshot_coordinator.cc b/src/yb/master/master_snapshot_coordinator.cc
index 7c64db525f..f3b48eaf82 100644
--- a/src/yb/master/master_snapshot_coordinator.cc
+++ b/src/yb/master/master_snapshot_coordinator.cc
@@ -57,6 +57,7 @@
 #include "yb/util/status_format.h"
 #include "yb/util/status_log.h"
 #include "yb/util/stopwatch.h"
+#include "yb/util/unique_lock.h"
 
 using std::vector;
 using std::string;
@@ -143,7 +144,7 @@ struct NoOp {
 // Finds appropriate entry in passed collection and invokes Done on it.
 template <class Collection, class PostProcess = NoOp>
 auto MakeDoneCallback(
-    std::mutex* mutex, const Collection& collection, const typename Collection::key_type& key,
+    std::mutex* mutex, const Collection* collection, const typename Collection::key_type& key,
     const TabletId& tablet_id, const PostProcess& post_process = PostProcess()) {
   struct DoneFunctor {
     std::mutex& mutex;
@@ -165,12 +166,12 @@ auto MakeDoneCallback(
     }
   };
 
-  return DoneFunctor {
-    .mutex = *mutex,
-    .collection = collection,
-    .key = key,
-    .tablet_id = tablet_id,
-    .post_process = post_process,
+  return DoneFunctor{
+      .mutex = *mutex,
+      .collection = *collection,
+      .key = key,
+      .tablet_id = tablet_id,
+      .post_process = post_process,
   };
 }
 
@@ -335,12 +336,11 @@ class MasterSnapshotCoordinator::Impl {
     return Status::OK();
   }
 
-  void UpdateSnapshotIfPresent(const TxnSnapshotId& id, int64_t leader_term)
-      NO_THREAD_SAFETY_ANALYSIS EXCLUDES(mutex_) {
-    std::unique_lock<std::mutex> lock(mutex_);
+  void UpdateSnapshotIfPresent(const TxnSnapshotId& id, int64_t leader_term) EXCLUDES(mutex_) {
+    UniqueLock lock(mutex_);
     auto it = snapshots_.find(id);
     if (it != snapshots_.end()) {
-      UpdateSnapshot(it->get(), leader_term, &lock);
+      UpdateSnapshot(it->get(), leader_term, &GetLockForCondition(&lock));
     }
   }
 
@@ -348,14 +348,14 @@ class MasterSnapshotCoordinator::Impl {
   Status LoadEntryOfType(
       tablet::Tablet* tablet, const SysRowEntryType& type, Map* m) REQUIRES(mutex_) {
     return EnumerateSysCatalog(tablet, context_.schema(), type,
-        [this, &m](const Slice& id, const Slice& data) NO_THREAD_SAFETY_ANALYSIS -> Status {
+        [this, &m](const Slice& id, const Slice& data) REQUIRES(mutex_) -> Status {
           return LoadEntry<Pb>(id, data, m);
     });
   }
 
   Status LoadSnapshotEntry(tablet::Tablet* tablet) REQUIRES(mutex_) {
     return EnumerateSysCatalog(tablet, context_.schema(), SysRowEntryType::SNAPSHOT,
-        [this](const Slice& id, const Slice& data) NO_THREAD_SAFETY_ANALYSIS -> Status {
+        [this](const Slice& id, const Slice& data) REQUIRES(mutex_) -> Status {
           RETURN_NOT_OK(LoadEntry<SysSnapshotEntryPB>(id, data, &snapshots_));
           auto snapshot_id = TryFullyDecodeTxnSnapshotId(id);
           UpdateCoveringMap(snapshot_id);
@@ -1262,7 +1262,7 @@ class MasterSnapshotCoordinator::Impl {
       const TabletSnapshotOperation& operation, const TabletInfoPtr& tablet_info,
       int64_t leader_term) {
     auto callback = MakeDoneCallback(
-        &mutex_, snapshots_, operation.snapshot_id, operation.tablet_id,
+        &mutex_, &snapshots_, operation.snapshot_id, operation.tablet_id,
         std::bind(&Impl::UpdateSnapshot, this, _1, leader_term, _2));
     if (!tablet_info) {
       callback(STATUS_EC_FORMAT(NotFound, MasterError(MasterErrorPB::TABLET_NOT_RUNNING),
@@ -1332,7 +1332,7 @@ class MasterSnapshotCoordinator::Impl {
       const TabletRestoreOperation& operation, const TabletInfoPtr& tablet_info,
       int64_t leader_term) {
     auto callback = MakeDoneCallback(
-        &mutex_, restorations_, operation.restoration_id, operation.tablet_id,
+        &mutex_, &restorations_, operation.restoration_id, operation.tablet_id,
         std::bind(&Impl::FinishRestoration, this, _1, leader_term));
     if (!tablet_info) {
       callback(STATUS_EC_FORMAT(
@@ -1429,10 +1429,11 @@ class MasterSnapshotCoordinator::Impl {
         }
         r->PrepareOperations(&restore_operations, tablets_snapshot, db_oid);
       }
+      for (const auto& id : cleanup_snapshots) {
+        CleanupObject(leader_term, id, snapshots_, EncodedSnapshotKey(id, &context_));
+      }
     }
-    for (const auto& id : cleanup_snapshots) {
-      CleanupObject(leader_term, id, snapshots_, EncodedSnapshotKey(id, &context_));
-    }
+
     ExecuteOperations(operations, leader_term);
     PollSchedulesComplete(schedules_data, l.epoch());
     ExecuteRestoreOperations(restore_operations, leader_term);
@@ -1504,11 +1505,13 @@ class MasterSnapshotCoordinator::Impl {
           WARN_NOT_OK(ExecuteScheduleOperation(operation, epoch.leader_term),
                       Format("Failed to execute operation on $0", operation.schedule_id));
           break;
-        case SnapshotScheduleOperationType::kCleanup:
+        case SnapshotScheduleOperationType::kCleanup: {
+          std::lock_guard l(mutex_);
           CleanupObject(
               epoch.leader_term, operation.schedule_id, schedules_,
               SnapshotScheduleState::EncodedKey(operation.schedule_id, &context_));
           break;
+        }
         default:
           LOG(DFATAL) << "Unexpected operation type: " << operation.type;
           break;
diff --git a/src/yb/master/scoped_leader_shared_lock.cc b/src/yb/master/scoped_leader_shared_lock.cc
index 6d10715ec9..c76adbd5f8 100644
--- a/src/yb/master/scoped_leader_shared_lock.cc
+++ b/src/yb/master/scoped_leader_shared_lock.cc
@@ -79,7 +79,6 @@ ScopedLeaderSharedLock::ScopedLeaderSharedLock(
     : catalog_(DCHECK_NOTNULL(catalog)),
       leader_shared_lock_(catalog->leader_lock_, std::try_to_lock),
       start_(std::chrono::steady_clock::now()),
-      epoch_(LeaderEpoch(-1)),
       file_name_(file_name),
       line_number_(line_number),
       function_name_(function_name) {
diff --git a/src/yb/master/ysql_backends_manager.cc b/src/yb/master/ysql_backends_manager.cc
index 37fa0ae151..00880495d7 100644
--- a/src/yb/master/ysql_backends_manager.cc
+++ b/src/yb/master/ysql_backends_manager.cc
@@ -473,7 +473,7 @@ Status BackendsCatalogVersionJob::Launch(LeaderEpoch epoch) {
     std::lock_guard l(mutex_);
 
     // Commit term now.
-    epoch_ = std::move(epoch);
+    epoch_ = epoch;
 
     for (const auto& ts_desc : descs) {
       if (!ts_desc->IsLive()) {
@@ -489,7 +489,7 @@ Status BackendsCatalogVersionJob::Launch(LeaderEpoch epoch) {
   }
 
   for (const auto& ts_uuid : ts_uuids) {
-    RETURN_NOT_OK(LaunchTS(ts_uuid, -1 /* num_lagging_backends */, epoch_));
+    RETURN_NOT_OK(LaunchTS(ts_uuid, -1 /* num_lagging_backends */, epoch));
   }
 
   return Status::OK();
@@ -498,7 +498,7 @@ Status BackendsCatalogVersionJob::Launch(LeaderEpoch epoch) {
 Status BackendsCatalogVersionJob::LaunchTS(
     TabletServerId ts_uuid, int num_lagging_backends, const LeaderEpoch& epoch) {
   auto task = std::make_shared<BackendsCatalogVersionTS>(
-      shared_from_this(), ts_uuid, num_lagging_backends, epoch_);
+      shared_from_this(), ts_uuid, num_lagging_backends, epoch);
   Status s = threadpool()->SubmitFunc([this, &ts_uuid, task]() {
     Status s = task->Run();
     if (!s.ok()) {
@@ -610,12 +610,14 @@ void BackendsCatalogVersionJob::Update(TabletServerId ts_uuid, Result<int> num_l
     auto s = num_lagging_backends.status();
     if (s.IsTryAgain()) {
       int last_known_num_lagging_backends;
+      LeaderEpoch epoch;
       {
         std::lock_guard l(mutex_);
         last_known_num_lagging_backends = ts_map_[ts_uuid];
+        epoch = epoch_;
       }
       // Ignore returned status since it is already logged/handled.
-      (void)LaunchTS(ts_uuid, last_known_num_lagging_backends, epoch_);
+      (void)LaunchTS(ts_uuid, last_known_num_lagging_backends, epoch);
     } else {
       LOG_WITH_PREFIX(WARNING) << "got bad status " << s.ToString() << " from TS " << ts_uuid;
       master_->ysql_backends_manager()->TerminateJob(
@@ -626,8 +628,10 @@ void BackendsCatalogVersionJob::Update(TabletServerId ts_uuid, Result<int> num_l
   DCHECK_GE(*num_lagging_backends, 0);
 
   // Update num_lagging_backends.
+  LeaderEpoch epoch;
   {
     std::lock_guard l(mutex_);
+    epoch = epoch_;
 
 #ifndef NDEBUG
     if (ts_map_[ts_uuid] != -1) {
@@ -644,7 +648,7 @@ void BackendsCatalogVersionJob::Update(TabletServerId ts_uuid, Result<int> num_l
     VLOG_WITH_PREFIX(2) << "still waiting on " << *num_lagging_backends << " backends of TS "
                         << ts_uuid;
     // Ignore returned status since it is already logged/handled.
-    (void)LaunchTS(ts_uuid, *num_lagging_backends, epoch_);
+    (void)LaunchTS(ts_uuid, *num_lagging_backends, epoch);
     return;
   }
   DCHECK_EQ(*num_lagging_backends, 0);
diff --git a/src/yb/rpc/connection.cc b/src/yb/rpc/connection.cc
index 9bb5ce7b3a..16a010be1d 100644
--- a/src/yb/rpc/connection.cc
+++ b/src/yb/rpc/connection.cc
@@ -189,7 +189,7 @@ void Connection::Shutdown(const Status& provided_status) {
       if (shutdown_initiated_.exchange(true, std::memory_order_release)) {
         LOG_WITH_PREFIX(WARNING)
             << "Connection shutdown invoked multiple times. Previously with status "
-            << ShutdownStatus() << " and now with status " << provided_status
+            << shutdown_status_ << " and now with status " << provided_status
             << ", completed=" << shutdown_completed() << ". Skipping repeated shutdown.";
         return;
       }
@@ -519,7 +519,7 @@ void Connection::DumpConnectionState(int32_t call_id, const void* call_ptr) cons
       /* $4 */ call_ptr,
       /* $5 */ call_id,
       /* $6 */ found_call_id,
-      /* $7 */ shutdown_status_,
+      /* $7 */ ShutdownStatus(),
       /* $8 */ ToStringRelativeToNow(shutdown_time_, now),
       /* $9 */ calls_queued_after_shutdown_.load(std::memory_order_acquire),
       /* $10 */ responses_queued_after_shutdown_.load(std::memory_order_acquire));
diff --git a/src/yb/rpc/connection.h b/src/yb/rpc/connection.h
index f426a3f107..0ea26523d2 100644
--- a/src/yb/rpc/connection.h
+++ b/src/yb/rpc/connection.h
@@ -134,13 +134,14 @@ class Connection final : public StreamContext, public std::enable_shared_from_th
   // Fail any calls which are currently queued or awaiting response.
   // Prohibits any future calls (they will be failed immediately with this
   // same Status).
-  void Shutdown(const Status& status) ON_REACTOR_THREAD;
+  void Shutdown(const Status& status) ON_REACTOR_THREAD EXCLUDES(outbound_data_queue_mtx_);
 
   // Queue a new call to be made. If the queuing fails, the call will be
   // marked failed.
   // Takes ownership of the 'call' object regardless of whether it succeeds or fails.
   // This may be called from a non-reactor thread.
-  void QueueOutboundCall(const OutboundCallPtr& call) ON_REACTOR_THREAD;
+  void QueueOutboundCall(const OutboundCallPtr& call) ON_REACTOR_THREAD
+      EXCLUDES(outbound_data_queue_mtx_);
 
   // The address of the remote end of the connection.
   const Endpoint& remote() const;
@@ -170,9 +171,10 @@ class Connection final : public StreamContext, public std::enable_shared_from_th
   //
   // In case is called outside of the reactor thread, it might return an error if it fails to submit
   // a task to the reactor thread, e.g. when the reactor is shutting down.
-  Status QueueOutboundData(OutboundDataPtr outbound_data);
+  Status QueueOutboundData(OutboundDataPtr outbound_data) EXCLUDES(outbound_data_queue_mtx_);
 
-  void QueueOutboundDataBatch(const OutboundDataBatch& batch) ON_REACTOR_THREAD;
+  void QueueOutboundDataBatch(const OutboundDataBatch& batch) ON_REACTOR_THREAD
+      EXCLUDES(outbound_data_queue_mtx_);
 
   Reactor* reactor() const { return reactor_; }
 
@@ -180,7 +182,7 @@ class Connection final : public StreamContext, public std::enable_shared_from_th
 
   // Do appropriate actions after adding outbound call. If the connection is shutting down,
   // returns the connection's shutdown status.
-  Status OutboundQueued() ON_REACTOR_THREAD;
+  Status OutboundQueued() ON_REACTOR_THREAD EXCLUDES(outbound_data_queue_mtx_);
 
   // An incoming packet has completed on the client side. This parses the
   // call response, looks up the CallAwaitingResponse, and calls the
@@ -201,7 +203,7 @@ class Connection final : public StreamContext, public std::enable_shared_from_th
   }
 
   // Returns the connection's shutdown status, or OK if shutdown has not happened yet.
-  Status ShutdownStatus() const;
+  Status ShutdownStatus() const EXCLUDES(outbound_data_queue_mtx_);
 
   bool shutdown_initiated() const {
     return shutdown_initiated_.load(std::memory_order_acquire);
@@ -228,9 +230,10 @@ class Connection final : public StreamContext, public std::enable_shared_from_th
   //
   // Returns the handle corresponding to the queued call, or std::numeric_limits<size_t>::max() in
   // case the handle is unknown, or an error in case the connection is shutting down.
-  Result<size_t> DoQueueOutboundData(OutboundDataPtr call, bool batch) ON_REACTOR_THREAD;
+  Result<size_t> DoQueueOutboundData(OutboundDataPtr call, bool batch) ON_REACTOR_THREAD
+      EXCLUDES(outbound_data_queue_mtx_);
 
-  void ProcessResponseQueue() ON_REACTOR_THREAD;
+  void ProcessResponseQueue() ON_REACTOR_THREAD EXCLUDES(outbound_data_queue_mtx_);
 
   // Stream context implementation
   void UpdateLastRead() override;
@@ -246,7 +249,8 @@ class Connection final : public StreamContext, public std::enable_shared_from_th
 
   void CleanupExpirationQueue(CoarseTimePoint now) ON_REACTOR_THREAD;
   // call_ptr is used only for logging to correlate with OutboundCall trace.
-  void DumpConnectionState(int32_t call_id, const void* call_ptr) const ON_REACTOR_THREAD;
+  void DumpConnectionState(int32_t call_id, const void* call_ptr) const ON_REACTOR_THREAD
+      EXCLUDES(outbound_data_queue_mtx_);
 
   std::string LogPrefix() const;
 
diff --git a/src/yb/rpc/outbound_call.h b/src/yb/rpc/outbound_call.h
index 5bc947fb44..9b97798feb 100644
--- a/src/yb/rpc/outbound_call.h
+++ b/src/yb/rpc/outbound_call.h
@@ -389,9 +389,13 @@ class OutboundCall : public RpcCall {
 
   size_t ObjectSize() const override { return sizeof(*this); }
 
-  size_t DynamicMemoryUsage() const override {
-    return DynamicMemoryUsageAllowSizeOf(error_pb_) +
-           DynamicMemoryUsageOf(buffer_, call_response_, trace_);
+  size_t DynamicMemoryUsage() const override ON_REACTOR_THREAD EXCLUDES(mtx_) {
+    size_t allow_size_of;
+    {
+      std::lock_guard lock(mtx_);
+      allow_size_of = DynamicMemoryUsageAllowSizeOf(error_pb_);
+    }
+    return allow_size_of + DynamicMemoryUsageOf(buffer_, call_response_, trace_);
   }
 
   CoarseTimePoint CallStartTime() const { return start_; }
diff --git a/src/yb/tablet/tablet_metadata.cc b/src/yb/tablet/tablet_metadata.cc
index 673c0eed85..0524138ad7 100644
--- a/src/yb/tablet/tablet_metadata.cc
+++ b/src/yb/tablet/tablet_metadata.cc
@@ -1255,17 +1255,13 @@ void RaftGroupMetadata::SetSchemaAndTableName(
   SetTableNameUnlocked(namespace_name, table_name, op_id, table_id);
 }
 
-void RaftGroupMetadata::AddTable(const std::string& table_id,
-                                 const std::string& namespace_name,
-                                 const std::string& table_name,
-                                 const TableType table_type,
-                                 const Schema& schema,
-                                 const IndexMap& index_map,
-                                 const dockv::PartitionSchema& partition_schema,
-                                 const boost::optional<IndexInfo>& index_info,
-                                 const SchemaVersion schema_version,
-                                 const OpId& op_id) {
+void RaftGroupMetadata::AddTable(
+    const std::string& table_id, const std::string& namespace_name, const std::string& table_name,
+    const TableType table_type, const Schema& schema, const IndexMap& index_map,
+    const dockv::PartitionSchema& partition_schema, const boost::optional<IndexInfo>& index_info,
+    const SchemaVersion schema_version, const OpId& op_id) {
   DCHECK(schema.has_column_ids());
+  std::lock_guard lock(data_mutex_);
   Primary primary(table_id == primary_table_id_);
   TableInfoPtr new_table_info = std::make_shared<TableInfo>(log_prefix_,
                                                             primary,
@@ -1285,7 +1281,6 @@ void RaftGroupMetadata::AddTable(const std::string& table_id,
       new_table_info->doc_read_context->SetCotableId(new_table_info->cotable_id);
     }
   }
-  std::lock_guard lock(data_mutex_);
   auto& tables = kv_store_.tables;
   auto[iter, inserted] = tables.emplace(table_id, new_table_info);
   OnChangeMetadataOperationAppliedUnlocked(op_id);
@@ -1677,6 +1672,7 @@ Result<docdb::CompactionSchemaInfo> RaftGroupMetadata::ColocationPacking(
 }
 
 std::string RaftGroupMetadata::GetSubRaftGroupWalDir(const RaftGroupId& raft_group_id) const {
+  std::lock_guard lock(data_mutex_);
   return JoinPathSegments(DirName(wal_dir_), MakeTabletDirName(raft_group_id));
 }
 
diff --git a/src/yb/tablet/tablet_metadata.h b/src/yb/tablet/tablet_metadata.h
index 8253e9a494..8da77f2b94 100644
--- a/src/yb/tablet/tablet_metadata.h
+++ b/src/yb/tablet/tablet_metadata.h
@@ -484,7 +484,7 @@ class RaftGroupMetadata : public RefCountedThreadSafe<RaftGroupMetadata>,
                 const dockv::PartitionSchema& partition_schema,
                 const boost::optional<qlexpr::IndexInfo>& index_info,
                 const SchemaVersion schema_version,
-                const OpId& op_id);
+                const OpId& op_id) EXCLUDES(data_mutex_);
 
   void RemoveTable(const TableId& table_id, const OpId& op_id);
 
@@ -562,7 +562,7 @@ class RaftGroupMetadata : public RefCountedThreadSafe<RaftGroupMetadata>,
   // Returns a new WAL dir path to be used for new Raft group `raft_group_id` which will be created
   // as a result of this Raft group splitting.
   // Uses the same root dir as for `this` Raft group.
-  std::string GetSubRaftGroupWalDir(const RaftGroupId& raft_group_id) const;
+  std::string GetSubRaftGroupWalDir(const RaftGroupId& raft_group_id) const EXCLUDES(data_mutex_);
 
   // Returns a new Data dir path to be used for new Raft group `raft_group_id` which will be created
   // as a result of this Raft group splitting.
@@ -682,7 +682,7 @@ class RaftGroupMetadata : public RefCountedThreadSafe<RaftGroupMetadata>,
   // Requires 'data_mutex_'.
   void ToSuperBlockUnlocked(RaftGroupReplicaSuperBlockPB* superblock) const REQUIRES(data_mutex_);
 
-  const TableInfoPtr primary_table_info_unlocked() const {
+  const TableInfoPtr primary_table_info_unlocked() const REQUIRES(data_mutex_) {
     const auto& tables = kv_store_.tables;
     const auto itr = tables.find(primary_table_id_);
     CHECK(itr != tables.end());
@@ -710,7 +710,11 @@ class RaftGroupMetadata : public RefCountedThreadSafe<RaftGroupMetadata>,
   // If taken together with 'data_mutex_', must be acquired first.
   mutable Mutex flush_lock_;
 
-  RaftGroupId raft_group_id_ GUARDED_BY(data_mutex_);
+  // No thread safety annotations on raft_group_id_ because it is a constant after the object is
+  // fully created. We cannot mark it as const since CreateSubtabletMetadata sets it to its parents
+  // id in order to call LoadFromSuperBlock and then updates it to the right value.
+  RaftGroupId raft_group_id_;
+
   std::shared_ptr<dockv::Partition> partition_ GUARDED_BY(data_mutex_);
 
   // The primary table id. Primary table is the first table this Raft group is created for.
@@ -759,6 +763,8 @@ class RaftGroupMetadata : public RefCountedThreadSafe<RaftGroupMetadata>,
 
   std::vector<TxnSnapshotRestorationId> active_restorations_;
 
+  // No thread safety annotations on log_prefix_ because it is a constant after the object is
+  // fully created. Check the comment on raft_group_id_ for more info.
   std::string log_prefix_;
 
   std::unordered_set<StatefulServiceKind> hosted_services_;
diff --git a/src/yb/tablet/transaction_participant.cc b/src/yb/tablet/transaction_participant.cc
index 6cd77f2839..1cd675b63a 100644
--- a/src/yb/tablet/transaction_participant.cc
+++ b/src/yb/tablet/transaction_participant.cc
@@ -222,14 +222,13 @@ class TransactionParticipant::Impl
     return true;
   }
 
-  void CompleteShutdown() {
+  void CompleteShutdown() EXCLUDES(mutex_, status_resolvers_mutex_) {
     LOG_IF_WITH_PREFIX(DFATAL, !Closing()) << __func__ << " w/o StartShutdown";
 
     if (wait_queue_) {
       wait_queue_->CompleteShutdown();
     }
 
-    decltype(status_resolvers_) status_resolvers;
     {
       UniqueLock lock(mutex_);
       WaitOnConditionVariable(
@@ -240,10 +239,16 @@ class TransactionParticipant::Impl
       DumpClear(RemoveReason::kShutdown);
       transactions_.clear();
       TransactionsModifiedUnlocked(&min_running_notifier);
-      status_resolvers.swap(status_resolvers_);
+
       mem_tracker_->UnregisterFromParent();
     }
 
+    decltype(status_resolvers_) status_resolvers;
+    {
+      std::lock_guard lock(status_resolvers_mutex_);
+      status_resolvers.swap(status_resolvers_);
+    }
+
     rpcs_.Shutdown();
     loader_.Shutdown();
     for (auto& resolver : status_resolvers) {
@@ -1285,7 +1290,8 @@ class TransactionParticipant::Impl
     TransactionsModifiedUnlocked(&min_running_notifier);
   }
 
-  void LoadFinished(const ApplyStatesMap& pending_applies) override {
+  void LoadFinished(const ApplyStatesMap& pending_applies) override
+      EXCLUDES(status_resolvers_mutex_) {
     // The start_latch will be hit either from a CountDown from Start, or from Shutdown, so make
     // sure that at the end of Load, we unblock shutdown.
     auto se = ScopeExit([&] {
@@ -1529,7 +1535,7 @@ class TransactionParticipant::Impl
     bool recently_removed;
     Status deadlock_status;
     {
-      std::unique_lock<std::mutex> lock(mutex_);
+      UniqueLock<std::mutex> lock(mutex_);
       auto it = transactions_.find(id);
       if (it != transactions_.end()) {
         if ((**it).start_ht() <= ignore_all_transactions_started_before_) {
@@ -1538,7 +1544,7 @@ class TransactionParticipant::Impl
               << ignore_all_transactions_started_before_ << ", txn: " << AsString(**it);
           return LockAndFindResult{};
         }
-        return LockAndFindResult{ std::move(lock), it };
+        return LockAndFindResult{std::move(GetLockForCondition(&lock)), it};
       }
       recently_removed = WasTransactionRecentlyRemoved(id);
       deadlock_status = GetTransactionDeadlockStatusUnlocked(id);
diff --git a/src/yb/tserver/ts_tablet_manager.cc b/src/yb/tserver/ts_tablet_manager.cc
index d7d9764fac..1fea050240 100644
--- a/src/yb/tserver/ts_tablet_manager.cc
+++ b/src/yb/tserver/ts_tablet_manager.cc
@@ -1346,10 +1346,10 @@ Status TSTabletManager::StartRemoteSnapshotTransfer(
   // ScopeExit before calling RegisterRemoteClientAndLookupTablet so that if it fails,
   // we cleanup as expected.
   auto decrement_num_session = ScopeExit([this, &private_addr]() {
-    DecrementRemoteSessionCount(private_addr, &snapshot_transfer_clients);
+    DecrementRemoteSessionCount(private_addr, &snapshot_transfer_clients_);
   });
   TabletPeerPtr tablet = VERIFY_RESULT(RegisterRemoteClientAndLookupTablet(
-      tablet_id, private_addr, kLogPrefix, &snapshot_transfer_clients));
+      tablet_id, private_addr, kLogPrefix, &snapshot_transfer_clients_));
 
   SCHECK(tablet, InvalidArgument, Format("Could not find tablet $0", tablet_id));
   const auto& rocksdb_dir = tablet->tablet_metadata()->rocksdb_dir();
@@ -1538,8 +1538,8 @@ Status TSTabletManager::StartTabletStateTransition(
 }
 
 bool TSTabletManager::IsTabletInTransition(const TabletId& tablet_id) const {
-  std::unique_lock<std::mutex> lock(transition_in_progress_mutex_);
-  return ContainsKey(transition_in_progress_, tablet_id);
+  std::lock_guard lock(transition_in_progress_mutex_);
+  return transition_in_progress_.contains(tablet_id);
 }
 
 Status TSTabletManager::OpenTabletMeta(const string& tablet_id,
@@ -1821,8 +1821,9 @@ void TSTabletManager::StartShutdown() {
   full_compaction_manager_->Shutdown();
 
   // Wait for all remote operations to finish.
-  WaitForRemoteSessionsToEnd(remote_bootstrap_clients_, kDebugBootstrapString);
-  WaitForRemoteSessionsToEnd(snapshot_transfer_clients, kDebugSnapshotTransferString);
+  WaitForRemoteSessionsToEnd(TabletRemoteSessionType::kBootstrap, kDebugBootstrapString);
+  WaitForRemoteSessionsToEnd(
+      TabletRemoteSessionType::kSnapshotTransfer, kDebugSnapshotTransferString);
 
   // Shut down the bootstrap pool, so new tablets are registered after this point.
   open_tablet_pool_->Shutdown();
@@ -2862,7 +2863,7 @@ void TSTabletManager::FlushDirtySuperblocks() {
 }
 
 void TSTabletManager::WaitForRemoteSessionsToEnd(
-    const RemoteClients& remote_clients, const std::string& debug_session_string) const {
+    TabletRemoteSessionType session_type, const std::string& debug_session_string) const {
   const MonoDelta kSingleWait = 10ms;
   const MonoDelta kReportInterval = 5s;
   const MonoDelta kMaxWait = 30s;
@@ -2871,6 +2872,9 @@ void TSTabletManager::WaitForRemoteSessionsToEnd(
   while (true) {
     {
       std::lock_guard lock(mutex_);
+      auto& remote_clients = session_type == TabletRemoteSessionType::kBootstrap
+                                 ? remote_bootstrap_clients_
+                                 : snapshot_transfer_clients_;
       const auto& remaining_sessions = remote_clients.num_clients_;
       const auto& source_addresses = remote_clients.source_addresses_;
       if (remaining_sessions == 0) return;
diff --git a/src/yb/tserver/ts_tablet_manager.h b/src/yb/tserver/ts_tablet_manager.h
index 9f02279d7f..bbe7816381 100644
--- a/src/yb/tserver/ts_tablet_manager.h
+++ b/src/yb/tserver/ts_tablet_manager.h
@@ -123,6 +123,7 @@ typedef Callback<void(tablet::TabletPeerPtr)> ConsensusChangeCallback;
 
 // Type of tablet directory.
 YB_DEFINE_ENUM(TabletDirType, (kData)(kWal));
+YB_DEFINE_ENUM(TabletRemoteSessionType, (kBootstrap)(kSnapshotTransfer));
 
 // Keeps track of the tablets hosted on the tablet server side.
 //
@@ -572,7 +573,8 @@ class TSTabletManager : public tserver::TabletPeerLookupIf, public tablet::Table
       std::function<Status()> callback = [] { return Status::OK(); });
 
   void WaitForRemoteSessionsToEnd(
-      const RemoteClients& remote_clients, const std::string& debug_session_string) const;
+      TabletRemoteSessionType session_type, const std::string& debug_session_string) const
+      EXCLUDES(mutex_);
 
   void DecrementRemoteSessionCount(const std::string& private_addr, RemoteClients* remote_clients);
 
@@ -698,7 +700,7 @@ class TSTabletManager : public tserver::TabletPeerLookupIf, public tablet::Table
   std::shared_ptr<TabletMemoryManager> mem_manager_;
 
   RemoteClients remote_bootstrap_clients_ GUARDED_BY(mutex_);
-  RemoteClients snapshot_transfer_clients GUARDED_BY(mutex_);
+  RemoteClients snapshot_transfer_clients_ GUARDED_BY(mutex_);
 
   // Gauge to monitor applied split operations.
   scoped_refptr<yb::AtomicGauge<uint64_t>> ts_split_op_apply_;
diff --git a/src/yb/util/priority_thread_pool.cc b/src/yb/util/priority_thread_pool.cc
index 58e4558b56..48910d02bf 100644
--- a/src/yb/util/priority_thread_pool.cc
+++ b/src/yb/util/priority_thread_pool.cc
@@ -101,7 +101,13 @@ class PriorityThreadPoolInternalTask {
   // This is called get_worker_unsafe because in order to call this function, the caller needs
   // to turn off thread safety analysis, so we only do it inside the GetWorker wrapper function.
   PriorityThreadPoolWorker* get_worker_unsafe() const REQUIRES(thread_pool_mutex_) {
-    return worker_;
+    return worker_.load(std::memory_order_relaxed);
+  }
+
+  // Another getter for worker_ but this time the mutex is not required. The value returned should
+  // only be used for ToString.
+  PriorityThreadPoolWorker* get_worker_relaxed() const NO_THREAD_SAFETY_ANALYSIS {
+    return worker_.load(std::memory_order_relaxed);
   }
 
   size_t serial_no() const {
@@ -139,7 +145,7 @@ class PriorityThreadPoolInternalTask {
     // So it is safe to avoid state caching for logging.
     LOG_IF(DFATAL, state() != PriorityThreadPoolTaskState::kNotStarted)
         << "Wrong task state " << state() << " in " << __PRETTY_FUNCTION__;
-    worker_ = worker;
+    worker_.store(worker, std::memory_order_release);
     SetState(PriorityThreadPoolTaskState::kRunning);
   }
 
@@ -171,10 +177,13 @@ class PriorityThreadPoolInternalTask {
     return PriorityThreadPoolPriorities{task_priority_.load(), group_no_priority()};
   }
 
+  // Reading the value of worker_ requires thread_pool_mutex_. But since this is only used for
+  // logging and we are just printing the pointer value we do not need to lock.
   std::string ToString() const {
     return Format(
-      "{ task: $0 worker: $1 state: $2 task_priority: $3 group_no_priority: $4 serial_no: $5 }",
-      TaskToString(), worker_, state(), task_priority(), group_no_priority(), serial_no_);
+        "{ task: $0 worker: $1 state: $2 task_priority: $3 group_no_priority: $4 serial_no: $5 }",
+        TaskToString(), get_worker_relaxed(), state(), task_priority(), group_no_priority(),
+        serial_no_);
   }
 
  private:
@@ -197,7 +206,7 @@ class PriorityThreadPoolInternalTask {
   std::atomic<PriorityThreadPoolTaskState> state_{PriorityThreadPoolTaskState::kNotStarted};
   mutable TaskPtr task_;
 
-  mutable PriorityThreadPoolWorker* worker_ GUARDED_BY(thread_pool_mutex_);
+  mutable std::atomic<PriorityThreadPoolWorker*> worker_ GUARDED_BY(thread_pool_mutex_);
 
   mutable std::atomic<bool> task_to_string_ready_{false};
   mutable simple_spinlock task_to_string_mutex_;
diff --git a/src/yb/util/unique_lock.h b/src/yb/util/unique_lock.h
index 2aeed80f04..561e6e5272 100644
--- a/src/yb/util/unique_lock.h
+++ b/src/yb/util/unique_lock.h
@@ -29,6 +29,7 @@ namespace yb {
 template<typename Mutex>
 class SCOPED_CAPABILITY UniqueLock {
  public:
+  UniqueLock() = default;
   explicit UniqueLock(Mutex &mutex) ACQUIRE(mutex) : unique_lock_(mutex) {}
 
   explicit UniqueLock(Mutex &mutex, std::defer_lock_t defer) : unique_lock_(mutex, defer) {}
@@ -41,6 +42,8 @@ class SCOPED_CAPABILITY UniqueLock {
 
   std::unique_lock<Mutex>& internal_unique_lock() { return unique_lock_; }
 
+  bool owns_lock() const { return unique_lock_.owns_lock(); }
+
   Mutex* mutex() RETURN_CAPABILITY(unique_lock_.mutex()) { return unique_lock_.mutex(); }
 
  private:
