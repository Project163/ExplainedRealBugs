diff --git a/src/yb/integration-tests/xcluster/xcluster_ddl_replication-test.cc b/src/yb/integration-tests/xcluster/xcluster_ddl_replication-test.cc
index f144696091..6eeec2b8db 100644
--- a/src/yb/integration-tests/xcluster/xcluster_ddl_replication-test.cc
+++ b/src/yb/integration-tests/xcluster/xcluster_ddl_replication-test.cc
@@ -40,6 +40,7 @@
 
 DECLARE_uint32(ysql_oid_cache_prefetch_size);
 DECLARE_uint32(xcluster_consistent_wal_safe_time_frequency_ms);
+DECLARE_int32(timestamp_history_retention_interval_sec);
 DECLARE_int32(xcluster_ddl_queue_max_retries_per_ddl);
 DECLARE_int32(ysql_sequence_cache_minval);
 
@@ -1986,6 +1987,223 @@ TEST_F(XClusterDDLReplicationAddDropColumnTest, AddDropColumns) {
   }
 }
 
+// TODO(#27064): Enable this test once the underlying bug is fixed.
+TEST_F(XClusterDDLReplicationTest, YB_DISABLE_TEST(DocdbNextColumnAboveLastUsedColumn)) {
+  // This test checks the hard case of whether or not backup and
+  // restore preserves the next DocDB column ID correctly: when the
+  // next DocDB column ID is higher than the last undeleted Postgres
+  // column ID.
+
+  if (!UseYbController()) {
+    GTEST_SKIP() << "This test does not work with yb_backup.py";
+  }
+  ASSERT_OK(SetUpClusters(/*is_colocated=*/false, /*start_yb_controller_servers=*/true));
+
+  {
+    auto conn = ASSERT_RESULT(producer_cluster_.ConnectToDB(namespace_name));
+    ASSERT_OK(conn.Execute("CREATE TABLE my_table (x INT);"));
+    ASSERT_OK(conn.Execute("ALTER TABLE my_table ADD COLUMN y INT;"));
+    ASSERT_OK(conn.Execute("ALTER TABLE my_table ADD COLUMN z INT;"));
+    ASSERT_OK(conn.Execute("ALTER TABLE my_table DROP COLUMN z;"));
+  }
+
+  ASSERT_OK(CheckpointReplicationGroupOnNamespaces({namespace_name}));
+  ASSERT_OK(BackupFromProducer());
+  ASSERT_OK(RestoreToConsumer());
+  ASSERT_OK(CreateReplicationFromCheckpoint());
+
+  auto conn = ASSERT_RESULT(producer_cluster_.ConnectToDB(namespace_name));
+  ASSERT_OK(conn.Execute("ALTER TABLE my_table ADD COLUMN q INT;"));
+  ASSERT_OK(conn.Execute("INSERT INTO my_table (x, y, q) VALUES (1,2,3), (4,5,6);"));
+  ASSERT_OK(WaitForSafeTimeToAdvanceToNow());
+
+  auto GetRows = [&](Cluster& cluster) -> Result<std::string> {
+    auto conn = VERIFY_RESULT(cluster.ConnectToDB(namespace_name));
+    return conn.FetchAllAsString("SELECT * FROM my_table ORDER BY x", ", ", "\n");
+  };
+
+  auto producer_rows = ASSERT_RESULT(GetRows(producer_cluster_));
+  auto consumer_rows = ASSERT_RESULT(GetRows(consumer_cluster_));
+  ASSERT_EQ(producer_rows, consumer_rows);
+}
+
+TEST_F(XClusterDDLReplicationTest, FailedSchemaChangeOnSource) {
+  ASSERT_OK(SetUpClustersAndCheckpointReplicationGroup());
+  ASSERT_OK(CreateReplicationFromCheckpoint());
+
+  auto conn = ASSERT_RESULT(producer_cluster_.ConnectToDB(namespace_name));
+  ASSERT_OK(conn.Execute("CREATE TABLE my_table (x INT);"));
+
+  // Do a few failing ADD COLUMN DDLs, which might bump the next DocDB column ID.
+  ASSERT_OK(conn.Execute("SET yb_test_fail_next_ddl=true;"));
+  ASSERT_NOK(conn.Execute("ALTER TABLE my_table ADD COLUMN y TEXT;"));
+  ASSERT_OK(conn.Execute("SET yb_test_fail_next_ddl=true;"));
+  ASSERT_NOK(conn.Execute("ALTER TABLE my_table ADD COLUMN y TEXT;"));
+
+  // In the absence of rollback of the next DocDB column ID counter,
+  // this column will have different DocDB column IDs on source and
+  // target.
+  ASSERT_OK(conn.Execute("ALTER TABLE my_table ADD COLUMN z INT;"));
+
+  ASSERT_OK(conn.Execute("INSERT INTO my_table (x, z) VALUES (1,2), (3,4);"));
+  ASSERT_OK(WaitForSafeTimeToAdvanceToNow());
+
+  auto GetRows = [&](Cluster& cluster) -> Result<std::string> {
+    auto conn = VERIFY_RESULT(cluster.ConnectToDB(namespace_name));
+    return conn.FetchAllAsString("SELECT * FROM my_table ORDER BY x", ", ", "\n");
+  };
+
+  auto producer_rows = ASSERT_RESULT(GetRows(producer_cluster_));
+  auto consumer_rows = ASSERT_RESULT(GetRows(consumer_cluster_));
+  ASSERT_EQ(producer_rows, consumer_rows);
+}
+
+TEST_F(XClusterDDLReplicationTest, FailedSchemaChangeOnSourceWithPartitioning) {
+  ASSERT_OK(SetUpClustersAndCheckpointReplicationGroup());
+  ASSERT_OK(CreateReplicationFromCheckpoint());
+
+  auto conn = ASSERT_RESULT(producer_cluster_.ConnectToDB(namespace_name));
+  ASSERT_OK(conn.Execute("CREATE TABLE my_table (x INT) PARTITION BY RANGE (x);"));
+  ASSERT_OK(conn.Execute(
+      "CREATE TABLE my_table_1 PARTITION OF my_table FOR VALUES FROM (1) TO (100000);"));
+
+  // Do a few failing ADD COLUMN DDLs, which might bump the next DocDB column ID.
+  ASSERT_OK(conn.Execute("SET yb_test_fail_next_ddl=true;"));
+  ASSERT_NOK(conn.Execute("ALTER TABLE my_table ADD COLUMN y INT;"));
+  ASSERT_OK(conn.Execute("SET yb_test_fail_next_ddl=true;"));
+  ASSERT_NOK(conn.Execute("ALTER TABLE my_table ADD COLUMN y INT;"));
+
+  // In the absence of rollback of the next DocDB column ID counter,
+  // this column will have different DocDB column IDs on source and
+  // target.
+  ASSERT_OK(conn.Execute("ALTER TABLE my_table ADD COLUMN z INT;"));
+
+  ANNOTATE_UNPROTECTED_WRITE(FLAGS_TEST_docdb_log_write_batches) = true;
+  ASSERT_OK(conn.Execute("INSERT INTO my_table (x, z) VALUES (1,2), (33333,44444);"));
+  ASSERT_OK(WaitForSafeTimeToAdvanceToNow());
+
+  auto GetRows = [&](Cluster& cluster) -> Result<std::string> {
+    auto conn = VERIFY_RESULT(cluster.ConnectToDB(namespace_name));
+    return conn.FetchAllAsString("SELECT * FROM my_table ORDER BY x", ", ", "\n");
+  };
+
+  auto producer_rows = ASSERT_RESULT(GetRows(producer_cluster_));
+  auto consumer_rows = ASSERT_RESULT(GetRows(consumer_cluster_));
+  ASSERT_EQ(producer_rows, consumer_rows);
+}
+
+TEST_F(XClusterDDLReplicationTest, FailedSchemaChangeOnTarget) {
+  ASSERT_OK(SetUpClustersAndCheckpointReplicationGroup());
+  ASSERT_OK(CreateReplicationFromCheckpoint());
+
+  auto conn = ASSERT_RESULT(producer_cluster_.ConnectToDB(namespace_name));
+  ASSERT_OK(conn.Execute("CREATE TABLE my_table (x INT);"));
+  ASSERT_OK(WaitForSafeTimeToAdvanceToNow());
+
+  // Fail the ADD COLUMN DDL at least once on the target before letting it succeed:
+  ANNOTATE_UNPROTECTED_WRITE(FLAGS_TEST_xcluster_ddl_queue_handler_fail_ddl) = true;
+  ANNOTATE_UNPROTECTED_WRITE(FLAGS_xcluster_ddl_queue_max_retries_per_ddl) = 1'000'000;
+  ASSERT_OK(conn.Execute("ALTER TABLE my_table ADD COLUMN y INT;"));
+  ASSERT_OK(StringWaiterLogSink("Failed DDL operation as requested").WaitFor(kTimeout));
+  ASSERT_OK(StringWaiterLogSink("Failed DDL operation as requested").WaitFor(kTimeout));
+  ANNOTATE_UNPROTECTED_WRITE(FLAGS_TEST_xcluster_ddl_queue_handler_fail_ddl) = false;
+  ASSERT_OK(WaitForSafeTimeToAdvanceToNow());
+
+  ASSERT_OK(conn.Execute("ALTER TABLE my_table ADD COLUMN z INT;"));
+  ASSERT_OK(conn.Execute("INSERT INTO my_table (x, y, z) VALUES (1,2,3), (4,5,6);"));
+  ASSERT_OK(WaitForSafeTimeToAdvanceToNow());
+
+  auto GetRows = [&](Cluster& cluster) -> Result<std::string> {
+    auto conn = VERIFY_RESULT(cluster.ConnectToDB(namespace_name));
+    return conn.FetchAllAsString("SELECT * FROM my_table ORDER BY x", ", ", "\n");
+  };
+
+  auto producer_rows = ASSERT_RESULT(GetRows(producer_cluster_));
+  auto consumer_rows = ASSERT_RESULT(GetRows(consumer_cluster_));
+  ASSERT_EQ(producer_rows, consumer_rows);
+}
+
+TEST_F(XClusterDDLReplicationTest, ColumnIdsOnFailover) {
+  ASSERT_OK(SetUpClustersAndCheckpointReplicationGroup());
+  ASSERT_OK(CreateReplicationFromCheckpoint());
+
+  ASSERT_OK(EnablePITROnClusters());
+
+  auto conn = ASSERT_RESULT(producer_cluster_.ConnectToDB(namespace_name));
+  ASSERT_OK(conn.Execute("CREATE TABLE my_table (x INT);"));
+  ASSERT_OK(WaitForSafeTimeToAdvanceToNow());
+
+  auto hybrid_time =
+      consumer_cluster_.mini_cluster_->mini_tablet_server(0)->server()->Clock()->Now();
+
+  // Fail the DDL apply, but let replication to the tablet keep running.
+  ANNOTATE_UNPROTECTED_WRITE(FLAGS_TEST_xcluster_ddl_queue_handler_fail_at_start) = true;
+  ANNOTATE_UNPROTECTED_WRITE(FLAGS_TEST_docdb_log_write_batches) = true;
+
+  ASSERT_OK(conn.Execute("ALTER TABLE my_table ADD COLUMN q TEXT DEFAULT 'default_foobar';"));
+  ASSERT_OK(conn.Execute("INSERT INTO my_table (x) VALUES (777777);"));
+
+  // Wait for writes to propagate but do not wait for the DDL to be successfully replicated.
+  ASSERT_OK(WaitForSafeTimeToAdvanceToNowWithoutDDLQueue());
+
+  // Perform failover:
+  ASSERT_OK(ToggleUniverseReplication(
+      consumer_cluster(), consumer_client(), kReplicationGroupId, false /* is_enabled */));
+  ASSERT_OK(PerformPITROnConsumerCluster(hybrid_time));
+  ASSERT_OK(DeleteUniverseReplication(
+      kReplicationGroupId, consumer_client(), consumer_cluster_.mini_cluster_.get()));
+  // Wait for role change/readability to propagate to consumer TServer.
+  auto* consumer_tablet_server = consumer_cluster_.mini_cluster_->mini_tablet_server(0)->server();
+  auto namespace_id = ASSERT_RESULT(GetNamespaceId(consumer_client()));
+  auto& consumer_xcluster_context = consumer_tablet_server->GetXClusterContext();
+  ASSERT_OK(WaitFor(
+      [&]() -> bool {
+        return !consumer_xcluster_context.IsTargetAndInAutomaticMode(namespace_id) &&
+               !consumer_xcluster_context.IsReadOnlyMode(namespace_id);
+      },
+      1s, "Wait for TServer to know that it is no longer a target"));
+
+  auto conn2 = ASSERT_RESULT(consumer_cluster_.ConnectToDB(namespace_name));
+  ASSERT_OK(conn2.Execute("ALTER TABLE my_table ADD COLUMN q INT;"));
+
+  auto result = ASSERT_RESULT(conn2.FetchAllAsString("SELECT * FROM my_table", ", ", "\n"));
+  ASSERT_EQ(result, "");
+
+  ASSERT_OK(conn2.Execute("INSERT INTO my_table (x,q) VALUES (777777, 666666);"));
+  result = ASSERT_RESULT(conn2.FetchAllAsString("SELECT * FROM my_table", ", ", "\n"));
+  ASSERT_EQ(result, "777777, 666666");
+}
+
+TEST_F(XClusterDDLReplicationTest, RollbackPreservesDeletedColumns) {
+  // Set up xCluster automatic mode replication so we will be rolling back next DocDB column IDs
+  // counter as part of failed DDLs.
+  ASSERT_OK(SetUpClustersAndCheckpointReplicationGroup());
+  ASSERT_OK(CreateReplicationFromCheckpoint());
+
+  auto conn = ASSERT_RESULT(producer_cluster_.ConnectToDB(namespace_name));
+  ASSERT_OK(conn.Execute("CREATE TABLE my_table (x INT);"));
+  ASSERT_OK(conn.Execute("SET yb_test_fail_next_ddl=true;"));
+  ASSERT_NOK(conn.Execute("ALTER TABLE my_table ADD COLUMN y INT;"));
+  ASSERT_OK(conn.Execute("ALTER TABLE my_table ADD COLUMN y INT;"));
+
+  ANNOTATE_UNPROTECTED_WRITE(FLAGS_TEST_docdb_log_write_batches) = true;
+  ASSERT_OK(conn.Execute("INSERT INTO my_table (x, y) VALUES (1,2), (33333,44444);"));
+  ASSERT_OK(conn.Execute("UPDATE my_table SET y = 55555 WHERE x = 1;"));
+
+  // Ensure the writes and column changes are outside the history retention interval we are going to
+  // compact with.
+  std::this_thread::sleep_for(20s);
+
+  ANNOTATE_UNPROTECTED_WRITE(FLAGS_timestamp_history_retention_interval_sec) = 10;
+  ASSERT_OK(producer_cluster()->FlushTablets());
+  ASSERT_OK(producer_cluster()->CompactTablets());
+  ANNOTATE_UNPROTECTED_WRITE(FLAGS_timestamp_history_retention_interval_sec) = 30000;
+
+  auto result =
+      ASSERT_RESULT(conn.FetchAllAsString("SELECT * FROM my_table ORDER BY x", ", ", "\n"));
+  ASSERT_EQ(result, "1, 55555\n33333, 44444");
+}
+
 // Make sure we can create Colocated db and table on both clusters that is not affected by an the
 // replication of a different database.
 TEST_F(XClusterDDLReplicationTest, CreateNonXClusterColocatedDb) {
diff --git a/src/yb/integration-tests/xcluster/xcluster_ysql_test_base.cc b/src/yb/integration-tests/xcluster/xcluster_ysql_test_base.cc
index d7fffb1d4f..4f24a37002 100644
--- a/src/yb/integration-tests/xcluster/xcluster_ysql_test_base.cc
+++ b/src/yb/integration-tests/xcluster/xcluster_ysql_test_base.cc
@@ -24,17 +24,19 @@
 #include "yb/master/master_cluster.proxy.h"
 #include "yb/master/master_ddl.pb.h"
 #include "yb/master/master_ddl.proxy.h"
-#include "yb/master/master_replication.proxy.h"
 #include "yb/master/mini_master.h"
 #include "yb/master/sys_catalog_initialization.h"
 
 #include "yb/server/server_base.h"
 
+#include "yb/tools/yb-admin_client.h"
+
 #include "yb/tserver/mini_tablet_server.h"
 #include "yb/tserver/tablet_server.h"
 
 #include "yb/util/backoff_waiter.h"
 #include "yb/util/is_operation_done_result.h"
+#include "yb/util/logging_test_util.h"
 #include "yb/util/thread.h"
 
 #include "yb/yql/pgwrapper/libpq_utils.h"
@@ -339,7 +341,7 @@ Result<YBTableName> XClusterYsqlTestBase::CreateYsqlTable(
   bool verify_schema_name =
       !schema_name.empty() && !FLAGS_TEST_create_table_with_empty_pgschema_name;
   return GetYsqlTable(
-      cluster, namespace_name, schema_name, table_name, true /* verify_table_name */,
+      cluster, namespace_name, schema_name, table_name, /*verify_table_name=*/true,
       verify_schema_name);
 }
 
@@ -1164,4 +1166,18 @@ Status XClusterYsqlTestBase::EnablePITROnClusters() {
     return Status::OK();
   });
 }
+
+Status XClusterYsqlTestBase::PerformPITROnConsumerCluster(HybridTime time) {
+  auto yb_admin_client = std::make_unique<tools::ClusterAdminClient>(
+      consumer_cluster_.mini_cluster_->GetMasterAddresses(), MonoDelta::FromSeconds(30));
+  RETURN_NOT_OK(yb_admin_client->Init());
+  auto j = VERIFY_RESULT(yb_admin_client->ListSnapshotSchedules(SnapshotScheduleId::Nil()));
+  auto snapshot_schedule_id =
+      VERIFY_RESULT(SnapshotScheduleIdFromString(j["schedules"].GetArray()[0]["id"].GetString()));
+  auto sink = RegexWaiterLogSink(".*Marking restoration.*as complete in sys catalog");
+  RETURN_NOT_OK(yb_admin_client->RestoreSnapshotSchedule(snapshot_schedule_id, time));
+  RETURN_NOT_OK(sink.WaitFor(300s));
+  LOG(INFO) << "PITR has been completed";
+  return Status::OK();
+}
 }  // namespace yb
diff --git a/src/yb/integration-tests/xcluster/xcluster_ysql_test_base.h b/src/yb/integration-tests/xcluster/xcluster_ysql_test_base.h
index c9e10411ae..2b43ab1fab 100644
--- a/src/yb/integration-tests/xcluster/xcluster_ysql_test_base.h
+++ b/src/yb/integration-tests/xcluster/xcluster_ysql_test_base.h
@@ -200,6 +200,7 @@ class XClusterYsqlTestBase : public XClusterTestBase {
   Status VerifyDDLExtensionTablesDeletion(const NamespaceName& db_name, bool only_source = false);
 
   Status EnablePITROnClusters();
+  Status PerformPITROnConsumerCluster(HybridTime time);
 
  protected:
   void TestReplicationWithSchemaChanges(TableId producer_table_id, bool bootstrap);
diff --git a/src/yb/master/async_rpc_tasks.cc b/src/yb/master/async_rpc_tasks.cc
index 86902704f1..3c60d71dd2 100644
--- a/src/yb/master/async_rpc_tasks.cc
+++ b/src/yb/master/async_rpc_tasks.cc
@@ -42,8 +42,7 @@ DECLARE_int32(TEST_slowdown_alter_table_rpcs_ms);
 
 DECLARE_bool(ysql_yb_enable_alter_table_rewrite);
 
-namespace yb {
-namespace master {
+namespace yb::master {
 
 using namespace std::placeholders;
 
@@ -600,6 +599,18 @@ bool AsyncAlterTable::SendRequest(int attempt) {
     req.mutable_indexes()->CopyFrom(l->pb.indexes());
     req.set_propagated_hybrid_time(master_->clock()->Now().ToUint64());
 
+    // First provisional column ID is the earliest column ID we can set next_column_id to.
+    int32_t first_provisional_column_id = l->pb.next_column_id();
+    if (l->pb.ysql_ddl_txn_verifier_state().size() > 0) {
+      DCHECK_EQ(l->pb.ysql_ddl_txn_verifier_state().size(), 1);
+      const auto& state = l->pb.ysql_ddl_txn_verifier_state()[0];
+      if (state.has_previous_next_column_id()) {
+        // If we rollback, we will move next_column_id back to this.
+        first_provisional_column_id = state.previous_next_column_id();
+      }
+    }
+    req.set_first_provisional_column_id(first_provisional_column_id);
+
     if (table_type() == TableType::PGSQL_TABLE_TYPE && !transaction_id_.IsNil()) {
       VLOG_WITH_PREFIX(1) << "Transaction ID is provided for tablet " << tablet_->tablet_id()
                           << " with ID " << transaction_id_.ToString()
@@ -1172,7 +1183,9 @@ bool AsyncGetTabletSplitKey::SendRequest(int attempt) {
 void AsyncGetTabletSplitKey::Finished(const Status& status) {
   if (result_cb_) {
     if (status.ok()) {
-      result_cb_(Data{resp_.split_encoded_key(), resp_.split_partition_key()});
+      result_cb_(Data{
+          .split_encoded_key = resp_.split_encoded_key(),
+          .split_partition_key = resp_.split_partition_key()});
     } else {
       result_cb_(status);
     }
@@ -1343,5 +1356,4 @@ bool AsyncMasterTestRetry::SendRequest(int attempt) {
   return true;
 }
 
-}  // namespace master
-}  // namespace yb
+} // namespace yb::master
diff --git a/src/yb/master/async_rpc_tasks.h b/src/yb/master/async_rpc_tasks.h
index efbbf0746e..d9d85796e1 100644
--- a/src/yb/master/async_rpc_tasks.h
+++ b/src/yb/master/async_rpc_tasks.h
@@ -26,8 +26,7 @@
 
 #include "yb/util/status_callback.h"
 
-namespace yb {
-namespace master {
+namespace yb::master {
 
 // Fire off the async create tablet.
 // This requires that the new tablet info is locked for write, and the
@@ -111,7 +110,7 @@ class AsyncTserverTabletHealthTask : public RetrySpecificTSRpcTask {
 
  protected:
   // Not associated with a tablet.
-  TabletId tablet_id() const override { return TabletId(); }
+  TabletId tablet_id() const override { return {}; }
 
   void HandleResponse(int attempt) override;
   bool SendRequest(int attempt) override;
@@ -254,18 +253,18 @@ class AsyncAlterTable : public AsyncTabletLeaderTask {
 
   AsyncAlterTable(
       Master* master, ThreadPool* callback_pool, const TabletInfoPtr& tablet,
-      const scoped_refptr<TableInfo>& table, const TransactionId transaction_id, LeaderEpoch epoch)
-    : AsyncTabletLeaderTask(master, callback_pool, tablet, table, std::move(epoch)),
+      const scoped_refptr<TableInfo>& table, TransactionId transaction_id, LeaderEpoch epoch)
+      : AsyncTabletLeaderTask(master, callback_pool, tablet, table, std::move(epoch)),
         transaction_id_(transaction_id) {}
 
   AsyncAlterTable(
       Master* master, ThreadPool* callback_pool, const TabletInfoPtr& tablet,
-      const scoped_refptr<TableInfo>& table, const TransactionId transaction_id, LeaderEpoch epoch,
-      const xrepl::StreamId& cdc_sdk_stream_id, const bool cdc_sdk_require_history_cutoff)
+      const scoped_refptr<TableInfo>& table, TransactionId transaction_id, LeaderEpoch epoch,
+      const xrepl::StreamId& cdc_sdk_stream_id, bool cdc_sdk_require_history_cutoff)
       : AsyncTabletLeaderTask(master, callback_pool, tablet, table, std::move(epoch)),
-          transaction_id_(transaction_id),
-          cdc_sdk_stream_id_(cdc_sdk_stream_id),
-          cdc_sdk_require_history_cutoff_(cdc_sdk_require_history_cutoff) {}
+        transaction_id_(transaction_id),
+        cdc_sdk_stream_id_(cdc_sdk_stream_id),
+        cdc_sdk_require_history_cutoff_(cdc_sdk_require_history_cutoff) {}
 
   server::MonitoredTaskType type() const override {
     return server::MonitoredTaskType::kAlterTable;
@@ -624,7 +623,7 @@ class AsyncTsTestRetry : public RetrySpecificTSRpcTask {
   std::string description() const override;
 
  private:
-  TabletId tablet_id() const override { return TabletId(); }
+  TabletId tablet_id() const override { return {}; }
 
   void HandleResponse(int attempt) override;
   bool SendRequest(int attempt) override;
@@ -675,7 +674,7 @@ class AsyncUpdateTransactionTablesVersion: public RetrySpecificTSRpcTask {
   std::string description() const override;
 
  private:
-  TabletId tablet_id() const override { return TabletId(); }
+  TabletId tablet_id() const override { return {}; }
 
   void HandleResponse(int attempt) override;
   bool SendRequest(int attempt) override;
@@ -686,5 +685,4 @@ class AsyncUpdateTransactionTablesVersion: public RetrySpecificTSRpcTask {
   tserver::UpdateTransactionTablesVersionResponsePB resp_;
 };
 
-} // namespace master
-} // namespace yb
+} // namespace yb::master
diff --git a/src/yb/master/catalog_entity_info.proto b/src/yb/master/catalog_entity_info.proto
index ed00120e89..f061be1641 100644
--- a/src/yb/master/catalog_entity_info.proto
+++ b/src/yb/master/catalog_entity_info.proto
@@ -54,6 +54,7 @@ message BackfillJobPB {
 
     optional SchemaPB previous_schema = 4;
     optional string previous_table_name = 5;
+    optional int32 previous_next_column_id = 6;
   }
 
 // The on-disk entry in the sys.catalog table ("metadata" column) for
diff --git a/src/yb/master/catalog_manager.cc b/src/yb/master/catalog_manager.cc
index da66f86438..294ad6eae6 100644
--- a/src/yb/master/catalog_manager.cc
+++ b/src/yb/master/catalog_manager.cc
@@ -594,8 +594,7 @@ DECLARE_bool(enable_truncate_cdcsdk_table);
 DECLARE_bool(TEST_enable_object_locking_for_table_locks);
 DECLARE_bool(ysql_yb_enable_replica_identity);
 
-namespace yb {
-namespace master {
+namespace yb::master {
 
 using std::shared_ptr;
 using std::string;
@@ -4769,7 +4768,7 @@ Status CatalogManager::CreateTransactionStatusTable(
     rpc::RpcContext *rpc, const LeaderEpoch& epoch) {
   const string& table_name = req->table_name();
   Status s = CreateTransactionStatusTableInternal(
-      rpc, table_name, nullptr /* tablespace_id */, epoch,
+      rpc, table_name, /*tablespace_id=*/nullptr, epoch,
       req->has_replication_info() ? &req->replication_info() : nullptr);
   if (s.IsAlreadyPresent()) {
     return SetupError(resp->mutable_error(), MasterErrorPB::OBJECT_ALREADY_PRESENT, s);
@@ -4914,7 +4913,7 @@ Status CatalogManager::CreateLocalTransactionStatusTableIfNeeded(
   }
 
   return CreateTransactionStatusTableInternal(
-      rpc, table_name, &tablespace_id, epoch, nullptr /* replication_info */);
+      rpc, table_name, &tablespace_id, epoch, /*replication_info=*/nullptr);
 }
 
 Status CatalogManager::CreateGlobalTransactionStatusTableIfNeededForNewTable(
@@ -4941,8 +4940,8 @@ Status CatalogManager::CreateGlobalTransactionStatusTableIfNeededForNewTable(
 Status CatalogManager::CreateGlobalTransactionStatusTableIfNotPresent(
     rpc::RpcContext* rpc, const LeaderEpoch& epoch) {
   Status s = CreateTransactionStatusTableInternal(
-      rpc, kGlobalTransactionsTableName, nullptr /* tablespace_id */, epoch,
-      nullptr /* replication_info */);
+      rpc, kGlobalTransactionsTableName, /*tablespace_id=*/nullptr, epoch,
+      /*replication_info=*/nullptr);
   if (s.IsAlreadyPresent()) {
     VLOG(1) << "Transaction status table already exists, not creating.";
     return Status::OK();
@@ -7402,8 +7401,8 @@ Status CatalogManager::AlterTable(const AlterTableRequestPB* req,
   if (table->GetTableType() == PGSQL_TABLE_TYPE) {
     LOG_WITH_PREFIX(INFO) << "PG table OID for AlterTable request: " << table->GetPgTableOid();
   }
-  NamespaceId new_namespace_id;
 
+  NamespaceId new_namespace_id;
   if (req->has_new_namespace()) {
     // Lookup the new namespace and verify if it exists.
     TRACE("Looking up new namespace");
@@ -7452,6 +7451,14 @@ Status CatalogManager::AlterTable(const AlterTableRequestPB* req,
     SleepFor(kSleepFor);
   }
 
+  // IsNamespaceInAutomaticDDLMode can't be called once we have the
+  // catalog manager lock in exclusive mode, so check for automatic mode now.
+  bool is_automatic_mode = false;
+  if (table->GetTableType() == PGSQL_TABLE_TYPE) {
+    is_automatic_mode =
+        xcluster_manager_.get()->IsNamespaceInAutomaticDDLMode(table->namespace_id());
+  }
+
   UniqueLock lock(mutex_);
   VLOG_WITH_FUNC(3) << "Acquired the catalog manager lock";
 
@@ -7490,7 +7497,8 @@ Status CatalogManager::AlterTable(const AlterTableRequestPB* req,
   Schema previous_schema;
   RETURN_NOT_OK(SchemaFromPB(l->pb.schema(), &previous_schema));
   string previous_table_name = l->pb.name();
-  ColumnId next_col_id = ColumnId(l->pb.next_column_id());
+  auto next_col_id = ColumnId(l->pb.next_column_id());
+  ColumnId initial_next_col_id = next_col_id;
   if (req->alter_schema_steps_size() || req->has_alter_properties() || req->has_pgschema_name()) {
     TRACE("Apply alter schema");
     Status s = ApplyAlterSteps(
@@ -7636,6 +7644,9 @@ Status CatalogManager::AlterTable(const AlterTableRequestPB* req,
         auto *ddl_state = table_pb.add_ysql_ddl_txn_verifier_state();
         SchemaToPB(previous_schema, ddl_state->mutable_previous_schema());
         ddl_state->set_previous_table_name(previous_table_name);
+        if (is_automatic_mode) {
+          ddl_state->set_previous_next_column_id(initial_next_col_id);
+        }
         schedule_ysql_txn_verifier = true;
       }
       txn = VERIFY_RESULT(TransactionMetadata::FromPB(req->transaction()));
@@ -10620,20 +10631,19 @@ Status CatalogManager::SendAlterTableRequestInternal(
     const scoped_refptr<TableInfo>& table, const TransactionId& txn_id, const LeaderEpoch& epoch,
     const AlterTableRequestPB* req) {
   for (const auto& tablet : VERIFY_RESULT(table->GetTablets())) {
-     std::shared_ptr<AsyncAlterTable> call;
+    std::shared_ptr<AsyncAlterTable> call;
 
     // CDC SDK Create Stream context
     if (req && req->has_cdc_sdk_stream_id()) {
       LOG(INFO) << " CDC stream id context : " << req->cdc_sdk_stream_id();
       xrepl::StreamId stream_id =
-        VERIFY_RESULT(xrepl::StreamId::FromString(req->cdc_sdk_stream_id()));
-      call = std::make_shared<AsyncAlterTable>(master_, AsyncTaskPool(), tablet, table,
-                                               txn_id, epoch,
-                                               stream_id,
-                                               req->cdc_sdk_require_history_cutoff());
+          VERIFY_RESULT(xrepl::StreamId::FromString(req->cdc_sdk_stream_id()));
+      call = std::make_shared<AsyncAlterTable>(
+          master_, AsyncTaskPool(), tablet, table, txn_id, epoch, stream_id,
+          req->cdc_sdk_require_history_cutoff());
     } else {
-      call = std::make_shared<AsyncAlterTable>(master_, AsyncTaskPool(), tablet, table,
-                                               txn_id, epoch);
+      call =
+          std::make_shared<AsyncAlterTable>(master_, AsyncTaskPool(), tablet, table, txn_id, epoch);
     }
     table->AddTask(call);
     if (PREDICT_FALSE(FLAGS_TEST_slowdown_alter_table_rpcs_ms > 0)) {
@@ -13712,5 +13722,4 @@ void CatalogManager::RemoveNamespaceFromMaps(
   }
 }
 
-}  // namespace master
-}  // namespace yb
+} // namespace yb::master
diff --git a/src/yb/master/xcluster/xcluster_manager.h b/src/yb/master/xcluster/xcluster_manager.h
index df51714f5b..9e5e462aa4 100644
--- a/src/yb/master/xcluster/xcluster_manager.h
+++ b/src/yb/master/xcluster/xcluster_manager.h
@@ -279,7 +279,7 @@ class XClusterManager : public XClusterManagerIf,
 
   bool IsTableReplicated(const TableId& table_id) const;
 
-  bool IsNamespaceInAutomaticDDLMode(const NamespaceId& namespace_id) const;
+  bool IsNamespaceInAutomaticDDLMode(const NamespaceId& namespace_id) const override;
   bool IsNamespaceInAutomaticModeSource(const NamespaceId& namespace_id) const override;
   bool IsNamespaceInAutomaticModeTarget(const NamespaceId& namespace_id) const override;
 
diff --git a/src/yb/master/xcluster/xcluster_manager_if.h b/src/yb/master/xcluster/xcluster_manager_if.h
index 1ccc4b9593..128eb47f6c 100644
--- a/src/yb/master/xcluster/xcluster_manager_if.h
+++ b/src/yb/master/xcluster/xcluster_manager_if.h
@@ -13,8 +13,6 @@
 
 #pragma once
 
-#include <memory>
-
 #include "yb/master/master_fwd.h"
 #include "yb/master/xcluster/master_xcluster_types.h"
 
@@ -83,6 +81,7 @@ class XClusterManagerIf {
 
   virtual bool IsTableReplicationConsumer(const TableId& table_id) const = 0;
 
+  virtual bool IsNamespaceInAutomaticDDLMode(const NamespaceId& namespace_id) const = 0;
   virtual bool IsNamespaceInAutomaticModeSource(const NamespaceId& namespace_id) const = 0;
   virtual bool IsNamespaceInAutomaticModeTarget(const NamespaceId& namespace_id) const = 0;
 
diff --git a/src/yb/master/xrepl_catalog_manager.cc b/src/yb/master/xrepl_catalog_manager.cc
index a4ee2e9a8d..9ee4b630b9 100644
--- a/src/yb/master/xrepl_catalog_manager.cc
+++ b/src/yb/master/xrepl_catalog_manager.cc
@@ -385,8 +385,8 @@ class UniverseReplicationLoader : public Visitor<PersistentUniverseReplicationIn
       : catalog_manager_(catalog_manager) {}
 
   Status Visit(
-      const std::string& replication_group_id_str, const SysUniverseReplicationEntryPB& metadata)
-      REQUIRES(catalog_manager_->mutex_) {
+      const std::string& replication_group_id_str,
+      const SysUniverseReplicationEntryPB& metadata) override REQUIRES(catalog_manager_->mutex_) {
     const xcluster::ReplicationGroupId replication_group_id(replication_group_id_str);
     DCHECK(!ContainsKey(
         catalog_manager_->universe_replication_map_,
@@ -467,7 +467,8 @@ class UniverseReplicationBootstrapLoader
 
   Status Visit(
       const std::string& replication_group_id_str,
-      const SysUniverseReplicationBootstrapEntryPB& metadata) REQUIRES(catalog_manager_->mutex_) {
+      const SysUniverseReplicationBootstrapEntryPB& metadata) override
+      REQUIRES(catalog_manager_->mutex_) {
     const xcluster::ReplicationGroupId replication_group_id(replication_group_id_str);
     DCHECK(!ContainsKey(
         catalog_manager_->universe_replication_bootstrap_map_,
diff --git a/src/yb/master/ysql_ddl_handler.cc b/src/yb/master/ysql_ddl_handler.cc
index 42820ac759..2722439085 100644
--- a/src/yb/master/ysql_ddl_handler.cc
+++ b/src/yb/master/ysql_ddl_handler.cc
@@ -20,9 +20,7 @@
 #include "yb/master/xcluster/xcluster_manager_if.h"
 #include "yb/master/ysql_ddl_verification_task.h"
 
-#include "yb/util/backoff_waiter.h"
 #include "yb/util/sync_point.h"
-#include "yb/util/trace.h"
 
 DEFINE_RUNTIME_bool(retry_if_ddl_txn_verification_pending, true,
     "Whether to retry a transaction if it fails verification.");
@@ -62,8 +60,7 @@ using std::shared_ptr;
 using std::string;
 using std::vector;
 
-namespace yb {
-namespace master {
+namespace yb::master {
 
 /*
  * This file contains all the logic required for YSQL DDL transaction verification. This is done
@@ -265,7 +262,7 @@ Status CatalogManager::YsqlDdlTxnCompleteCallback(TableInfoPtr table,
     // For example, a DDL like "alter table mytable add constraint x_unique unique(x)"
     // does not change the table mytable's DocDB schema, but it creates a new index
     // x_unique in DocDB. We cannot use mytable to decide whether the transaction has
-    // committed or aborted, but we can use x_uniqe for that purpose.
+    // committed or aborted, but we can use x_unique for that purpose.
     if (!is_committed.has_value() &&
       // Try to find a table that may have its schema changed before/after the DDL.
         std::find_if(
@@ -416,7 +413,7 @@ Status CatalogManager::HandleSuccessfulYsqlDdlTxn(
   // TABLE and DROP COLUMN.
   auto& l = txn_data.write_lock;
   if (l->is_being_deleted_by_ysql_ddl_txn()) {
-    return YsqlDdlTxnDropTableHelper(txn_data, true /* success */);
+    return YsqlDdlTxnDropTableHelper(txn_data, /*success=*/true);
   }
 
   vector<string> cols_being_dropped;
@@ -444,7 +441,7 @@ Status CatalogManager::HandleSuccessfulYsqlDdlTxn(
   }
   SchemaToPB(builder.Build(), mutable_pb.mutable_schema());
   return YsqlDdlTxnAlterTableHelper(
-      txn_data, ddl_log_entries, "" /* new_table_name */, true /* success */);
+      txn_data, ddl_log_entries, /*new_table_name=*/"", /*success=*/true);
 }
 
 Status CatalogManager::HandleAbortedYsqlDdlTxn(const YsqlTableDdlTxnState txn_data) {
@@ -452,7 +449,7 @@ Status CatalogManager::HandleAbortedYsqlDdlTxn(const YsqlTableDdlTxnState txn_da
   const auto& ddl_state = mutable_pb.ysql_ddl_txn_verifier_state(0);
   if (ddl_state.contains_create_table_op()) {
     // This table was created in this aborted transaction. Drop the xCluster streams and the table.
-    RETURN_NOT_OK(YsqlDdlTxnDropTableHelper(txn_data, false /* success */));
+    RETURN_NOT_OK(YsqlDdlTxnDropTableHelper(txn_data, /*success=*/false));
 
     return DropXClusterStreamsOfTables({txn_data.table->id()});
   }
@@ -466,8 +463,11 @@ Status CatalogManager::HandleAbortedYsqlDdlTxn(const YsqlTableDdlTxnState txn_da
     mutable_pb.mutable_schema()->CopyFrom(ddl_state.previous_schema());
     const string new_table_name = ddl_state.previous_table_name();
     mutable_pb.set_name(new_table_name);
+    if (ddl_state.has_previous_next_column_id()) {
+      mutable_pb.set_next_column_id(ddl_state.previous_next_column_id());
+    }
     return YsqlDdlTxnAlterTableHelper(
-        txn_data, ddl_log_entries, new_table_name, false /* success */);
+        txn_data, ddl_log_entries, new_table_name, /*success=*/false);
   }
 
   // This must be a failed Delete transaction.
@@ -533,8 +533,8 @@ Status CatalogManager::YsqlDdlTxnAlterTableHelper(const YsqlTableDdlTxnState txn
   table->AddDdlTxnWaitingForSchemaVersion(target_schema_version, txn_data.ddl_txn_id);
 
   auto action = success ? "roll forward" : "rollback";
-  LOG(INFO) << "Sending Alter Table request as part of " << action
-            << " for table " << table->name();
+  LOG(INFO) << "Sending Alter Table request as part of " << action << " for table "
+            << table->name();
   if (RandomActWithProbability(FLAGS_TEST_ysql_ddl_rollback_failure_probability)) {
     return STATUS(InternalError, "Injected random failure for testing.");
   }
@@ -687,7 +687,7 @@ Status CatalogManager::TriggerDdlVerificationIfNeeded(
       return Status::OK();
     }
 
-    if (verifier_state->txn_state == TxnState::kCommitted  ||
+    if (verifier_state->txn_state == TxnState::kCommitted ||
         verifier_state->txn_state == TxnState::kAborted ||
         (verifier_state->txn_state == TxnState::kNoChange && verifier_state->tables.size() == 1)) {
       // (1) For kCommitted and kAborted, we already know whether this transaction is a success or
@@ -716,8 +716,7 @@ Status CatalogManager::TriggerDdlVerificationIfNeeded(
           // all of its tablets yet. Call SendAlterTableRequestInternal to
           // sync them up. If fails, reschedule TriggerDdlVerificationIfNeeded
           // with a delay.
-          auto s = SendAlterTableRequestInternal(
-                   table, TransactionId::Nil(), epoch);
+          auto s = SendAlterTableRequestInternal(table, TransactionId::Nil(), epoch);
           if (!s.ok()) {
             LOG(WARNING) << "SendAlterTableRequestInternal failed, table: " << table->id();
             ScheduleTriggerDdlVerificationIfNeeded(txn, epoch, 500 /* delay_ms */);
@@ -736,35 +735,31 @@ Status CatalogManager::TriggerDdlVerificationIfNeeded(
           // In this case we clear the table from txn.transaction_id.
           // The new DDL transaction txn_id will take care of syncing up the
           // table's schema version with its tablets.
-          LOG(WARNING) << "pb_txn_id " << txn_id << " on table "
-                       << table->id() << " differs from txn.transaction_id "
-                       << txn.transaction_id;
+          LOG(WARNING) << "pb_txn_id " << txn_id << " on table " << table->id()
+                       << " differs from txn.transaction_id " << txn.transaction_id;
           remove_table_ids.push_back(table->id());
           continue;
         }
-        return background_tasks_thread_pool_->SubmitFunc(
-          [this, table, pb_txn_id, is_committed, epoch, debug_caller_info = __FUNCTION__]() {
-              WARN_NOT_OK(YsqlDdlTxnCompleteCallback(table, pb_txn_id, is_committed, epoch,
-                                                     debug_caller_info),
-                          Format("YsqlDdlTxnCompleteCallback failed, table: $0",
-                                 table->id()));
-          }
-        );
+        return background_tasks_thread_pool_->SubmitFunc([this, table, pb_txn_id, is_committed,
+                                                          epoch,
+                                                          debug_caller_info = __FUNCTION__]() {
+          WARN_NOT_OK(
+              YsqlDdlTxnCompleteCallback(table, pb_txn_id, is_committed, epoch, debug_caller_info),
+              Format("YsqlDdlTxnCompleteCallback failed, table: $0", table->id()));
+        });
       }
       for (const auto& table_id : remove_table_ids) {
         RemoveDdlTransactionStateUnlocked(table_id, {txn.transaction_id});
       }
-      VLOG(3) << "All tables " << VectorToString(table_ids)
-              << " in transaction " << txn << " have pb_txn_id cleared"
-              << " or have a new txn_id";
+      VLOG(3) << "All tables " << VectorToString(table_ids) << " in transaction " << txn
+              << " have pb_txn_id cleared or have a new txn_id";
       return Status::OK();
     }
     // Pick a table that is not in nochange_tables.
     for (size_t index = 0; index < verifier_state->tables.size(); ++index) {
       table = verifier_state->tables[index];
       if (!verifier_state->nochange_tables.contains(table->id())) {
-        VLOG(3) << "Picked table at index " << index << " out of "
-                << verifier_state->tables.size();
+        VLOG(3) << "Picked table at index " << index << " out of " << verifier_state->tables.size();
         break;
       }
     }
@@ -812,5 +807,4 @@ void CatalogManager::DoReleaseObjectLocksIfNecessary(const TransactionId& txn_id
   object_lock_info_manager_->ReleaseLocksForTxn(txn_id);
 }
 
-} // namespace master
-} // namespace yb
+} // namespace yb::master
diff --git a/src/yb/tablet/operations.proto b/src/yb/tablet/operations.proto
index 53ae36ab02..2f49ed2801 100644
--- a/src/yb/tablet/operations.proto
+++ b/src/yb/tablet/operations.proto
@@ -152,6 +152,12 @@ message ChangeMetadataRequestPB {
   // [schema_version - 1], and then reinsert the current packing schema with [schema_version].
   // Schema version must be bumped by 2 before this call is made.
   optional bool insert_packed_schema = 20;
+
+  // If present, holds the first column ID that may be provisional.  Columns with this ID or later
+  // may be rolled back then possibly rolled forward with different types.
+  //
+  // If missing, then all existing columns may safely be treated as non-provisional.
+  optional int32 first_provisional_column_id = 21;
 }
 
 message SplitTabletRequestPB {
diff --git a/src/yb/tablet/tablet.cc b/src/yb/tablet/tablet.cc
index 5ff65ac4ed..58a9277505 100644
--- a/src/yb/tablet/tablet.cc
+++ b/src/yb/tablet/tablet.cc
@@ -36,29 +36,21 @@
 
 #include <boost/container/static_vector.hpp>
 
-#include "yb/server/auto_flags_manager_base.h"
 #include "yb/client/client.h"
-#include "yb/client/error.h"
 #include "yb/client/meta_data_cache.h"
 #include "yb/client/session.h"
 #include "yb/client/table.h"
 #include "yb/client/transaction.h"
-#include "yb/client/transaction_manager.h"
 #include "yb/client/yb_op.h"
 
-#include "yb/qlexpr/index_column.h"
 #include "yb/common/pgsql_error.h"
-#include "yb/qlexpr/ql_rowblock.h"
-#include "yb/common/row_mark.h"
 #include "yb/common/schema.h"
-#include "yb/common/transaction.h"
-#include "yb/common/transaction_error.h"
 #include "yb/common/schema_pbutil.h"
+#include "yb/common/transaction.h"
 
 #include "yb/consensus/consensus.messages.h"
 #include "yb/consensus/log.h"
 #include "yb/consensus/log_anchor_registry.h"
-#include "yb/consensus/opid_util.h"
 
 #include "yb/docdb/compaction_file_filter.h"
 #include "yb/docdb/conflict_resolution.h"
@@ -88,8 +80,12 @@
 
 #include "yb/rocksutil/yb_rocksdb.h"
 
+#include "yb/qlexpr/index_column.h"
+#include "yb/qlexpr/ql_rowblock.h"
+
 #include "yb/rpc/thread_pool.h"
 
+#include "yb/server/auto_flags_manager_base.h"
 #include "yb/server/hybrid_clock.h"
 
 #include "yb/tablet/operations/change_metadata_operation.h"
@@ -110,7 +106,6 @@
 #include "yb/tablet/transaction_participant.h"
 #include "yb/tablet/write_query.h"
 
-#include "yb/tserver/tserver.pb.h"
 #include "yb/tserver/tserver_error.h"
 #include "yb/tserver/ysql_advisory_lock_table.h"
 
@@ -125,7 +120,6 @@
 #include "yb/util/scope_exit.h"
 #include "yb/util/status_format.h"
 #include "yb/util/status_log.h"
-#include "yb/util/std_util.h"
 #include "yb/util/stopwatch.h"
 #include "yb/util/sync_point.h"
 #include "yb/util/trace.h"
@@ -834,18 +828,18 @@ void Tablet::ResetYBMetaDataCache() {
 template <class F>
 auto MakeMemTableFlushFilterFactory(const F& f) {
   // Trick to get type of mem_table_flush_filter_factory field.
-  typedef typename decltype(
-      static_cast<rocksdb::Options*>(nullptr)->mem_table_flush_filter_factory)::element_type
-      MemTableFlushFilterFactoryType;
+  using MemTableFlushFilterFactoryType =
+      typename decltype(static_cast<rocksdb::Options*>(nullptr)
+                            ->mem_table_flush_filter_factory)::element_type;
   return std::make_shared<MemTableFlushFilterFactoryType>(f);
 }
 
 template <class F>
 auto MakeMaxFileSizeWithTableTTLFunction(const F& f) {
   // Trick to get type of max_file_size_for_compaction field.
-  typedef typename decltype(
-      static_cast<rocksdb::Options*>(nullptr)->max_file_size_for_compaction)::element_type
-      MaxFileSizeWithTableTTLFunction;
+  using MaxFileSizeWithTableTTLFunction =
+      typename decltype(static_cast<rocksdb::Options*>(nullptr)
+                            ->max_file_size_for_compaction)::element_type;
   return std::make_shared<MaxFileSizeWithTableTTLFunction>(f);
 }
 
@@ -1122,7 +1116,7 @@ Status Tablet::OpenRegularDB(const rocksdb::Options& common_options) {
     return rocksdb_open_status;
   }
   regular_db_.reset(db);
-  regular_db_->ListenFilesChanged(std::bind(&Tablet::RegularDbFilesChanged, this));
+  regular_db_->ListenFilesChanged([this] { RegularDbFilesChanged(); });
 
   return Status::OK();
 }
@@ -1170,7 +1164,7 @@ Status Tablet::OpenIntentsDB(const rocksdb::Options& common_options) {
   rocksdb::DB* intents_db = nullptr;
   RETURN_NOT_OK(rocksdb::DB::Open(intents_rocksdb_options, intents_dir, &intents_db));
   intents_db_.reset(intents_db);
-  intents_db_->ListenFilesChanged(std::bind(&Tablet::CleanupIntentFiles, this));
+  intents_db_->ListenFilesChanged([this] { CleanupIntentFiles(); });
 
   // We need to set the "cdc_sdk_min_checkpoint_op_id" so that intents don't get
   // garbage collected after transactions are loaded.
@@ -1214,7 +1208,7 @@ void Tablet::CleanupIntentFiles() {
   }
 
   WARN_NOT_OK(
-      cleanup_intent_files_token_->SubmitFunc(std::bind(&Tablet::DoCleanupIntentFiles, this)),
+      cleanup_intent_files_token_->SubmitFunc([this] { DoCleanupIntentFiles(); }),
       "Submit cleanup intent files failed");
 }
 
@@ -2775,10 +2769,10 @@ Status Tablet::MarkBackfillDone(const OpId& op_id, const TableId& table_id) {
   return metadata_->Flush();
 }
 
-Status Tablet::AlterSchema(ChangeMetadataOperation *operation) {
+Status Tablet::AlterSchema(ChangeMetadataOperation* operation) {
+  const auto* request = operation->request();
   auto current_table_info = VERIFY_RESULT(metadata_->GetTableInfo(
-        operation->request()->has_alter_table_id() ?
-        operation->request()->alter_table_id().ToBuffer() : ""));
+      request->has_alter_table_id() ? request->alter_table_id().ToBuffer() : ""));
   auto key_schema = current_table_info->schema().CreateKeyProjection();
 
   RSTATUS_DCHECK_NE(operation->schema(), static_cast<void*>(nullptr), InvalidArgument,
@@ -2795,7 +2789,7 @@ Status Tablet::AlterSchema(ChangeMetadataOperation *operation) {
   }
 
   // Handle insert_packed_schema separately.
-  if (operation->request()->insert_packed_schema()) {
+  if (request->insert_packed_schema()) {
     return InsertPackedSchemaForXClusterTarget(operation, current_table_info);
   }
 
@@ -2805,9 +2799,20 @@ Status Tablet::AlterSchema(ChangeMetadataOperation *operation) {
                         << " version " << operation->schema_version();
 
   // Find out which columns have been deleted in this schema change, and add them to metadata.
+  int32_t first_provisional_column_id = std::numeric_limits<int32_t>::max();
+  if (request->has_first_provisional_column_id()) {
+    first_provisional_column_id = request->first_provisional_column_id();
+    DCHECK_GE(first_provisional_column_id, 0);
+  }
   vector<DeletedColumn> deleted_cols;
   for (const auto& col : current_table_info->schema().column_ids()) {
-    if (operation->schema()->find_column_by_id(col) == Schema::kColumnNotFound) {
+    if (operation->schema()->find_column_by_id(col) != Schema::kColumnNotFound) {
+      continue;
+    }
+    if (col.ToUint64() >= (uint32_t)first_provisional_column_id) {
+      LOG_WITH_PREFIX(INFO) << "Column " << col
+                            << " not recorded as deleted because of its provisional nature.";
+    } else {
       deleted_cols.emplace_back(col, clock_->Now());
       LOG_WITH_PREFIX(INFO) << "Column " << col << " recorded as deleted.";
     }
diff --git a/src/yb/util/logging_test_util.cc b/src/yb/util/logging_test_util.cc
index ff8b7675fe..2b0f0c43b6 100644
--- a/src/yb/util/logging_test_util.cc
+++ b/src/yb/util/logging_test_util.cc
@@ -13,7 +13,6 @@
 #include "yb/util/logging_test_util.h"
 
 #include "yb/util/backoff_waiter.h"
-#include "yb/util/result.h"
 
 namespace yb {
 
@@ -41,17 +40,17 @@ void PatternWaiterLogSink<std::string>::send(
 }
 
 template<>
-void PatternWaiterLogSink<std::regex>::send(
+void PatternWaiterLogSink<boost::regex>::send(
     google::LogSeverity severity, const char* full_filename, const char* base_filename, int line,
     const struct ::tm* tm_time, const char* message, size_t message_len) {
   auto log_message = ToString(severity, base_filename, line, tm_time, message, message_len);
-  if (std::regex_match(log_message, pattern_to_wait_for_) &&
+  if (boost::regex_match(log_message, pattern_to_wait_for_) &&
       log_message.find(kWaitingMessage) == std::string::npos) {
     event_occurred_ = true;
   }
 }
 
 template class PatternWaiterLogSink<std::string>;
-template class PatternWaiterLogSink<std::regex>;
+template class PatternWaiterLogSink<boost::regex>;
 
 }  // namespace yb
diff --git a/src/yb/util/logging_test_util.h b/src/yb/util/logging_test_util.h
index 523107f7b7..e38b0b7ce2 100644
--- a/src/yb/util/logging_test_util.h
+++ b/src/yb/util/logging_test_util.h
@@ -33,13 +33,12 @@
 #pragma once
 
 #include <atomic>
-#include <chrono>
-#include <regex>
 #include <string>
 #include <vector>
 
-#include "yb/util/logging.h"
+#include <boost/regex.hpp>
 
+#include "yb/util/logging.h"
 #include "yb/util/monotime.h"
 #include "yb/util/status_fwd.h"
 
@@ -84,7 +83,7 @@ class PatternWaiterLogSink : public google::LogSink {
 
   bool IsEventOccurred() { return event_occurred_; }
 
-  ~PatternWaiterLogSink() { google::RemoveLogSink(this); }
+  ~PatternWaiterLogSink() override { google::RemoveLogSink(this); }
 
  private:
   static const char* kWaitingMessage;
@@ -97,7 +96,9 @@ class PatternWaiterLogSink : public google::LogSink {
 };
 
 using StringWaiterLogSink = PatternWaiterLogSink<std::string>;
-using RegexWaiterLogSink = PatternWaiterLogSink<std::regex>;
+// We use boost::regex here instead of std::regex to avoid a bug where gcc's std::regex crashes due
+// to running out of stack space when it is applied to too large of an input.
+using RegexWaiterLogSink = PatternWaiterLogSink<boost::regex>;
 
 // RAII wrapper around registering a LogSink with GLog.
 struct ScopedRegisterSink {
