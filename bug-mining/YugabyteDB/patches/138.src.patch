diff --git a/src/yb/client/client-internal.cc b/src/yb/client/client-internal.cc
index 9996f617db..e28a3b6b4f 100644
--- a/src/yb/client/client-internal.cc
+++ b/src/yb/client/client-internal.cc
@@ -136,9 +136,8 @@ using master::MasterErrorPB;
 
 namespace client {
 
-using internal::GetTableSchemaRpc;
 using internal::GetTablegroupSchemaRpc;
-using internal::GetColocatedTabletSchemaRpc;
+using internal::GetTableSchemaRpc;
 using internal::RemoteTablet;
 using internal::RemoteTabletServer;
 using internal::UpdateLocalTsState;
@@ -1417,40 +1416,6 @@ class GetTablegroupSchemaRpc
   vector<YBTableInfo>* info_;
 };
 
-// Gets all table schemas for a colocated tablet from the leader master. See ClientMasterRpc.
-class GetColocatedTabletSchemaRpc : public ClientMasterRpc<GetColocatedTabletSchemaRequestPB,
-    GetColocatedTabletSchemaResponsePB> {
- public:
-  GetColocatedTabletSchemaRpc(YBClient* client,
-                              StatusCallback user_cb,
-                              const YBTableName& parent_colocated_table,
-                              vector<YBTableInfo>* info,
-                              CoarseTimePoint deadline);
-  GetColocatedTabletSchemaRpc(YBClient* client,
-                              StatusCallback user_cb,
-                              const TableId& parent_colocated_table_id,
-                              vector<YBTableInfo>* info,
-                              CoarseTimePoint deadline);
-
-  std::string ToString() const override;
-
-  virtual ~GetColocatedTabletSchemaRpc();
-
- private:
-  GetColocatedTabletSchemaRpc(YBClient* client,
-                              StatusCallback user_cb,
-                              const master::TableIdentifierPB& parent_colocated_table_identifier,
-                              vector<YBTableInfo>* info,
-                              CoarseTimePoint deadline);
-
-  void CallRemoteMethod() override;
-  void ProcessResponse(const Status& status) override;
-
-  StatusCallback user_cb_;
-  master::TableIdentifierPB table_identifier_;
-  vector<YBTableInfo>* info_;
-};
-
 namespace {
 
 master::TableIdentifierPB ToTableIdentifierPB(const YBTableName& table_name) {
@@ -1637,68 +1602,6 @@ void GetTablegroupSchemaRpc::ProcessResponse(const Status& status) {
   user_cb_.Run(new_status);
 }
 
-GetColocatedTabletSchemaRpc::GetColocatedTabletSchemaRpc(YBClient* client,
-                                                         StatusCallback user_cb,
-                                                         const YBTableName& table_name,
-                                                         vector<YBTableInfo>* info,
-                                                         CoarseTimePoint deadline)
-    : GetColocatedTabletSchemaRpc(
-          client, user_cb, ToTableIdentifierPB(table_name), info, deadline) {
-}
-
-GetColocatedTabletSchemaRpc::GetColocatedTabletSchemaRpc(YBClient* client,
-                                                         StatusCallback user_cb,
-                                                         const TableId& table_id,
-                                                         vector<YBTableInfo>* info,
-                                                         CoarseTimePoint deadline)
-    : GetColocatedTabletSchemaRpc(
-          client, user_cb, ToTableIdentifierPB(table_id), info, deadline) {}
-
-GetColocatedTabletSchemaRpc::GetColocatedTabletSchemaRpc(
-    YBClient* client,
-    StatusCallback user_cb,
-    const master::TableIdentifierPB& table_identifier,
-    vector<YBTableInfo>* info,
-    CoarseTimePoint deadline)
-    : ClientMasterRpc(client, deadline),
-      user_cb_(std::move(user_cb)),
-      table_identifier_(table_identifier),
-      info_(DCHECK_NOTNULL(info)) {
-  req_.mutable_parent_colocated_table()->CopyFrom(table_identifier_);
-}
-
-GetColocatedTabletSchemaRpc::~GetColocatedTabletSchemaRpc() {
-}
-
-void GetColocatedTabletSchemaRpc::CallRemoteMethod() {
-  master_ddl_proxy()->GetColocatedTabletSchemaAsync(
-      req_, &resp_, mutable_retrier()->mutable_controller(),
-      std::bind(&GetColocatedTabletSchemaRpc::Finished, this, Status::OK()));
-}
-
-string GetColocatedTabletSchemaRpc::ToString() const {
-  return Format(
-      "GetColocatedTabletSchemaRpc(table_identifier: $0, num_attempts: $1)",
-      table_identifier_.ShortDebugString(), num_attempts());
-}
-
-void GetColocatedTabletSchemaRpc::ProcessResponse(const Status& status) {
-  auto new_status = status;
-  if (new_status.ok()) {
-    for (const auto& resp : resp_.get_table_schema_response_pbs()) {
-      info_->emplace_back();
-      new_status = CreateTableInfoFromTableSchemaResp(resp, &info_->back());
-      if (!new_status.ok()) {
-        break;
-      }
-    }
-  }
-  if (!new_status.ok()) {
-    LOG(WARNING) << ToString() << " failed: " << new_status.ToString();
-  }
-  user_cb_.Run(new_status);
-}
-
 class CreateXClusterStreamRpc
     : public ClientMasterRpc<CreateCDCStreamRequestPB, CreateCDCStreamResponsePB> {
  public:
@@ -2419,21 +2322,6 @@ Status YBClient::Data::GetTablegroupSchemaById(
   return Status::OK();
 }
 
-Status YBClient::Data::GetColocatedTabletSchemaByParentTableId(
-    YBClient* client,
-    const TableId& parent_colocated_table_id,
-    CoarseTimePoint deadline,
-    std::shared_ptr<std::vector<YBTableInfo>> info,
-    StatusCallback callback) {
-  auto rpc = StartRpc<GetColocatedTabletSchemaRpc>(
-      client,
-      callback,
-      parent_colocated_table_id,
-      info.get(),
-      deadline);
-  return Status::OK();
-}
-
 Result<IndexPermissions> YBClient::Data::GetIndexPermissions(
     YBClient* client,
     const TableId& table_id,
diff --git a/src/yb/client/client-internal.h b/src/yb/client/client-internal.h
index f633d8134f..086a5a8bcf 100644
--- a/src/yb/client/client-internal.h
+++ b/src/yb/client/client-internal.h
@@ -280,12 +280,6 @@ class YBClient::Data {
                                  CoarseTimePoint deadline,
                                  std::shared_ptr<std::vector<YBTableInfo>> info,
                                  StatusCallback callback);
-  Status GetColocatedTabletSchemaByParentTableId(
-      YBClient* client,
-      const TableId& parent_colocated_table_id,
-      CoarseTimePoint deadline,
-      std::shared_ptr<std::vector<YBTableInfo>> info,
-      StatusCallback callback);
 
   Result<IndexPermissions> GetIndexPermissions(
       YBClient* client,
diff --git a/src/yb/client/client.cc b/src/yb/client/client.cc
index 39811a5beb..917b79bbed 100644
--- a/src/yb/client/client.cc
+++ b/src/yb/client/client.cc
@@ -836,18 +836,6 @@ Status YBClient::GetTablegroupSchemaById(const TablegroupId& tablegroup_id,
                                         callback);
 }
 
-Status YBClient::GetColocatedTabletSchemaByParentTableId(
-    const TableId& parent_colocated_table_id,
-    std::shared_ptr<std::vector<YBTableInfo>> info,
-    StatusCallback callback) {
-  auto deadline = CoarseMonoClock::Now() + default_admin_operation_timeout();
-  return data_->GetColocatedTabletSchemaByParentTableId(this,
-                                                         parent_colocated_table_id,
-                                                         deadline,
-                                                         info,
-                                                         callback);
-}
-
 Result<IndexPermissions> YBClient::GetIndexPermissions(
     const TableId& table_id,
     const TableId& index_id) {
diff --git a/src/yb/client/client.h b/src/yb/client/client.h
index d3585cb7d3..b0a60af85e 100644
--- a/src/yb/client/client.h
+++ b/src/yb/client/client.h
@@ -410,11 +410,6 @@ class YBClient {
                                  std::shared_ptr<std::vector<YBTableInfo>> info,
                                  StatusCallback callback);
 
-  Status GetColocatedTabletSchemaByParentTableId(
-      const TableId& parent_colocated_table_id,
-      std::shared_ptr<std::vector<YBTableInfo>> info,
-      StatusCallback callback);
-
   Result<IndexPermissions> GetIndexPermissions(
       const TableId& table_id,
       const TableId& index_id);
diff --git a/src/yb/integration-tests/xcluster/xcluster-tablet-split-itest.cc b/src/yb/integration-tests/xcluster/xcluster-tablet-split-itest.cc
index a2664db2be..90c3579b2d 100644
--- a/src/yb/integration-tests/xcluster/xcluster-tablet-split-itest.cc
+++ b/src/yb/integration-tests/xcluster/xcluster-tablet-split-itest.cc
@@ -87,10 +87,16 @@ class XClusterTabletSplitITestBase : public TabletSplitBase {
  protected:
   Status SetupReplication(const string& bootstrap_id = "") {
     SwitchToProducer();
-    VERIFY_RESULT(tools::RunAdminToolCommand(
-        consumer_cluster_->GetMasterAddresses(), "setup_universe_replication", kProducerClusterId,
-        TabletSplitBase::cluster_->GetMasterAddresses(), TabletSplitBase::table_->id(),
-        bootstrap_id));
+    if (!bootstrap_id.empty()) {
+      VERIFY_RESULT(tools::RunAdminToolCommand(
+          consumer_cluster_->GetMasterAddresses(), "setup_universe_replication", kProducerClusterId,
+          TabletSplitBase::cluster_->GetMasterAddresses(), TabletSplitBase::table_->id(),
+          bootstrap_id));
+    } else {
+      VERIFY_RESULT(tools::RunAdminToolCommand(
+          consumer_cluster_->GetMasterAddresses(), "setup_universe_replication", kProducerClusterId,
+          TabletSplitBase::cluster_->GetMasterAddresses(), TabletSplitBase::table_->id()));
+    }
     return Status::OK();
   }
 
diff --git a/src/yb/integration-tests/xcluster/xcluster-test.cc b/src/yb/integration-tests/xcluster/xcluster-test.cc
index e79cb759e0..75f1a52586 100644
--- a/src/yb/integration-tests/xcluster/xcluster-test.cc
+++ b/src/yb/integration-tests/xcluster/xcluster-test.cc
@@ -650,8 +650,10 @@ TEST_P(XClusterTest, SetupUniverseReplicationErrorChecking) {
     ASSERT_OK(
         master_proxy->SetupUniverseReplication(setup_universe_req, &setup_universe_resp, &rpc));
     ASSERT_TRUE(setup_universe_resp.has_error());
-    std::string prefix = "Producer universe ID must be provided";
-    ASSERT_TRUE(setup_universe_resp.error().status().message().substr(0, prefix.size()) == prefix);
+    ASSERT_STR_CONTAINS(
+        setup_universe_resp.error().status().message(),
+        "Empty required arguments: [replication_group_id, producer_master_addresses, "
+        "producer_table_ids]");
   }
 
   {
@@ -663,8 +665,9 @@ TEST_P(XClusterTest, SetupUniverseReplicationErrorChecking) {
     ASSERT_OK(
         master_proxy->SetupUniverseReplication(setup_universe_req, &setup_universe_resp, &rpc));
     ASSERT_TRUE(setup_universe_resp.has_error());
-    std::string prefix = "Producer master address must be provided";
-    ASSERT_TRUE(setup_universe_resp.error().status().message().substr(0, prefix.size()) == prefix);
+    ASSERT_STR_CONTAINS(
+        setup_universe_resp.error().status().message(),
+        "Empty required arguments: [producer_master_addresses, producer_table_ids]");
   }
 
   {
@@ -677,13 +680,14 @@ TEST_P(XClusterTest, SetupUniverseReplicationErrorChecking) {
     HostPortsToPBs(hp_vec, setup_universe_req.mutable_producer_master_addresses());
     setup_universe_req.add_producer_table_ids("a");
     setup_universe_req.add_producer_table_ids("b");
-    setup_universe_req.add_producer_bootstrap_ids("c");
+    setup_universe_req.add_producer_bootstrap_ids(xrepl::StreamId::GenerateRandom().ToString());
     master::SetupUniverseReplicationResponsePB setup_universe_resp;
     ASSERT_OK(
         master_proxy->SetupUniverseReplication(setup_universe_req, &setup_universe_resp, &rpc));
     ASSERT_TRUE(setup_universe_resp.has_error());
-    std::string prefix = "Number of bootstrap ids must be equal to number of tables";
-    ASSERT_TRUE(setup_universe_resp.error().status().message().substr(0, prefix.size()) == prefix);
+    ASSERT_STR_CONTAINS(
+        setup_universe_resp.error().status().message(),
+        "Number of bootstrap ids must be equal to number of tables");
   }
 
   {
@@ -699,14 +703,14 @@ TEST_P(XClusterTest, SetupUniverseReplicationErrorChecking) {
 
     setup_universe_req.add_producer_table_ids("prod_table_id_1");
     setup_universe_req.add_producer_table_ids("prod_table_id_2");
-    setup_universe_req.add_producer_bootstrap_ids("prod_bootstrap_id_1");
-    setup_universe_req.add_producer_bootstrap_ids("prod_bootstrap_id_2");
+    setup_universe_req.add_producer_bootstrap_ids(xrepl::StreamId::GenerateRandom().ToString());
+    setup_universe_req.add_producer_bootstrap_ids(xrepl::StreamId::GenerateRandom().ToString());
 
     ASSERT_OK(
         master_proxy->SetupUniverseReplication(setup_universe_req, &setup_universe_resp, &rpc));
     ASSERT_TRUE(setup_universe_resp.has_error());
-    std::string substring = "belongs to the target universe";
-    ASSERT_TRUE(setup_universe_resp.error().status().message().find(substring) != string::npos);
+    ASSERT_STR_CONTAINS(
+        setup_universe_resp.error().status().message(), "belongs to the target universe");
   }
 
   {
@@ -725,14 +729,15 @@ TEST_P(XClusterTest, SetupUniverseReplicationErrorChecking) {
 
     setup_universe_req.add_producer_table_ids("prod_table_id_1");
     setup_universe_req.add_producer_table_ids("prod_table_id_2");
-    setup_universe_req.add_producer_bootstrap_ids("prod_bootstrap_id_1");
-    setup_universe_req.add_producer_bootstrap_ids("prod_bootstrap_id_2");
+    setup_universe_req.add_producer_bootstrap_ids(xrepl::StreamId::GenerateRandom().ToString());
+    setup_universe_req.add_producer_bootstrap_ids(xrepl::StreamId::GenerateRandom().ToString());
 
     ASSERT_OK(
         master_proxy->SetupUniverseReplication(setup_universe_req, &setup_universe_resp, &rpc));
     ASSERT_TRUE(setup_universe_resp.has_error());
-    std::string prefix = "The request UUID and cluster UUID are identical.";
-    ASSERT_TRUE(setup_universe_resp.error().status().message().substr(0, prefix.size()) == prefix);
+    ASSERT_STR_CONTAINS(
+        setup_universe_resp.error().status().message(),
+        "Replication group Id cannot be the same as the cluster UUID");
   }
 }
 
@@ -2531,10 +2536,12 @@ TEST_P(XClusterTest, TestAlterWhenProducerIsInaccessible) {
 
   // Ensure that we just return an error and don't have a fatal.
   ASSERT_OK(master_proxy->AlterUniverseReplication(alter_req, &alter_resp, &rpc));
-  ASSERT_TRUE(alter_resp.has_error());
+  auto is_done = ASSERT_RESULT(
+      WaitForSetupUniverseReplication(xcluster::GetAlterReplicationGroupId(kReplicationGroupId)));
+  ASSERT_FALSE(is_done.status().ok());
 }
 
-TEST_P(XClusterTest, TestFailedUniverseDeletionOnRestart) {
+TEST_P(XClusterTest, TestFailedUniverseDeletion) {
   ASSERT_OK(SetUpWithParams({8, 4}, {6, 6}, 3));
 
   auto master_proxy = std::make_shared<master::MasterReplicationProxy>(
@@ -2554,25 +2561,17 @@ TEST_P(XClusterTest, TestFailedUniverseDeletionOnRestart) {
   rpc::RpcController rpc;
   rpc.set_timeout(MonoDelta::FromSeconds(kRpcTimeout));
   ASSERT_OK(master_proxy->SetupUniverseReplication(req, &resp, &rpc));
-  // Sleep to allow the universe to be marked as failed
-  std::this_thread::sleep_for(2s);
+
+  auto is_done = ASSERT_RESULT(WaitForSetupUniverseReplication());
+  ASSERT_TRUE(is_done.status().IsNotFound());
 
   master::GetUniverseReplicationRequestPB new_req;
   new_req.set_replication_group_id(kReplicationGroupId.ToString());
   master::GetUniverseReplicationResponsePB new_resp;
   rpc.Reset();
   ASSERT_OK(master_proxy->GetUniverseReplication(new_req, &new_resp, &rpc));
-  ASSERT_TRUE(new_resp.entry().state() == master::SysUniverseReplicationEntryPB::FAILED);
-
-  // Restart the ENTIRE Consumer cluster.
-  ASSERT_OK(consumer_cluster()->RestartSync());
-
-  // Should delete on restart
-  ASSERT_OK(WaitForSetupUniverseReplicationCleanUp(kReplicationGroupId));
-  rpc.Reset();
-  Status s = master_proxy->GetUniverseReplication(new_req, &new_resp, &rpc);
-  ASSERT_OK(s);
   ASSERT_TRUE(new_resp.has_error());
+  ASSERT_TRUE(StatusFromPB(new_resp.error().status()).IsNotFound());
 }
 
 TEST_P(XClusterTest, TestFailedDeleteOnRestart) {
@@ -2656,26 +2655,24 @@ TEST_P(XClusterTest, TestFailedAlterUniverseOnRestart) {
 
   ASSERT_OK(master_proxy->AlterUniverseReplication(alter_req, &alter_resp, &rpc));
 
+  ASSERT_OK(
+      WaitForSetupUniverseReplication(xcluster::GetAlterReplicationGroupId(kReplicationGroupId)));
+
   // Restart the ENTIRE Consumer cluster.
   ASSERT_OK(consumer_cluster()->RestartSync());
 
-  // Wait for alter universe to be deleted on start up
-  ASSERT_OK(WaitForSetupUniverseReplicationCleanUp(
-      xcluster::GetAlterReplicationGroupId(kReplicationGroupId)));
-
   // Change should not have gone through
-  new_req.set_replication_group_id(kReplicationGroupId.ToString());
   rpc.Reset();
   ASSERT_OK(master_proxy->GetUniverseReplication(new_req, &new_resp, &rpc));
-  ASSERT_NE(new_resp.entry().tables_size(), 2);
+  ASSERT_NE(new_resp.entry().tables_size(), 2) << new_resp.ShortDebugString();
 
   // Check that the unfinished alter universe was deleted on start up
   rpc.Reset();
   new_req.set_replication_group_id(
       xcluster::GetAlterReplicationGroupId(kReplicationGroupId).ToString());
-  Status s = master_proxy->GetUniverseReplication(new_req, &new_resp, &rpc);
-  ASSERT_OK(s);
+  ASSERT_OK(master_proxy->GetUniverseReplication(new_req, &new_resp, &rpc));
   ASSERT_TRUE(new_resp.has_error());
+  ASSERT_TRUE(StatusFromPB(new_resp.error().status()).IsNotFound());
 }
 
 TEST_P(XClusterTest, TestAlterUniverseRemoveTableAndDrop) {
@@ -4074,7 +4071,7 @@ TEST_F_EX(XClusterTest, FailedSetupStreamUpdate, XClusterTestNoParam) {
 
   ASSERT_NOK_STR_CONTAINS(
       SetupUniverseReplication(producer_tables_, bootstrap_ids),
-      "Unable to update xrepl stream options on source universe");
+      "Test flag to fail setup stream update is set");
 
   master::ListCDCStreamsResponsePB stream_resp;
   ASSERT_OK(GetCDCStreamForTable(producer_table_->id(), &stream_resp));
diff --git a/src/yb/integration-tests/xcluster/xcluster_test_base.cc b/src/yb/integration-tests/xcluster/xcluster_test_base.cc
index 239ae1934e..0547784b32 100644
--- a/src/yb/integration-tests/xcluster/xcluster_test_base.cc
+++ b/src/yb/integration-tests/xcluster/xcluster_test_base.cc
@@ -15,6 +15,7 @@
 
 #include <string>
 
+#include "yb/client/xcluster_client.h"
 #include "yb/common/xcluster_util.h"
 
 #include "yb/client/client.h"
@@ -360,6 +361,10 @@ Status XClusterTestBase::SetupUniverseReplication(
   rpc.set_timeout(MonoDelta::FromSeconds(kRpcTimeout));
 
   RETURN_NOT_OK(master_proxy->SetupUniverseReplication(req, &resp, &rpc));
+  if (resp.has_error()) {
+    return StatusFromPB(resp.error().status());
+  }
+
   // Verify that replication was setup correctly.
   master::GetUniverseReplicationResponsePB verify_resp;
   RETURN_NOT_OK(VerifyUniverseReplication(
@@ -460,6 +465,9 @@ Result<master::GetUniverseReplicationResponsePB> XClusterTestBase::GetUniverseRe
   rpc.set_timeout(MonoDelta::FromSeconds(kRpcTimeout));
 
   RETURN_NOT_OK(master_proxy->GetUniverseReplication(req, &resp, &rpc));
+  if (resp.has_error()) {
+    return StatusFromPB(resp.error().status());
+  }
   return resp;
 }
 
@@ -508,6 +516,27 @@ Status XClusterTestBase::WaitForSetupUniverseReplication(
       MonoDelta::FromSeconds(kRpcTimeout), "Is setup replication done");
 }
 
+Result<IsOperationDoneResult> XClusterTestBase::WaitForSetupUniverseReplication(
+    const xcluster::ReplicationGroupId& replication_group_id, MiniCluster* consumer_cluster,
+    YBClient* consumer_client) {
+  if (!consumer_cluster) {
+    consumer_cluster = consumer_cluster_.mini_cluster_.get();
+  }
+  if (!consumer_client) {
+    consumer_client = consumer_cluster_.client_.get();
+  }
+  master::IsSetupUniverseReplicationDoneResponsePB resp;
+  RETURN_NOT_OK(WaitForSetupUniverseReplication(
+      consumer_cluster, consumer_client, replication_group_id, &resp));
+
+  Status done_status;
+  if (resp.has_replication_error()) {
+    done_status = StatusFromPB(resp.replication_error());
+  }
+
+  return IsOperationDoneResult::Done(done_status);
+}
+
 Status XClusterTestBase::GetCDCStreamForTable(
     const TableId& table_id, master::ListCDCStreamsResponsePB* resp) {
   return LoggedWaitFor([this, table_id, resp]() -> Result<bool> {
@@ -591,7 +620,6 @@ Status XClusterTestBase::AlterUniverseReplication(
   RETURN_NOT_OK(WaitForSetupUniverseReplication(
       consumer_cluster(), consumer_client(), alter_replication_group_id, &setup_resp));
   SCHECK(!setup_resp.has_error(), IllegalState, alter_resp.ShortDebugString());
-  RETURN_NOT_OK(WaitForSetupUniverseReplicationCleanUp(alter_replication_group_id));
   master::GetUniverseReplicationResponsePB get_resp;
   return VerifyUniverseReplication(replication_group_id, &get_resp);
 }
diff --git a/src/yb/integration-tests/xcluster/xcluster_test_base.h b/src/yb/integration-tests/xcluster/xcluster_test_base.h
index 53f7547e15..113d0d4e6d 100644
--- a/src/yb/integration-tests/xcluster/xcluster_test_base.h
+++ b/src/yb/integration-tests/xcluster/xcluster_test_base.h
@@ -28,6 +28,7 @@
 
 #include "yb/master/master_replication.fwd.h"
 
+#include "yb/util/is_operation_done_result.h"
 #include "yb/util/string_util.h"
 #include "yb/util/test_util.h"
 #include "yb/util/tsan_util.h"
@@ -229,6 +230,10 @@ class XClusterTestBase : public YBTest {
       const xcluster::ReplicationGroupId& replication_group_id,
       master::IsSetupUniverseReplicationDoneResponsePB* resp);
 
+  Result<IsOperationDoneResult> WaitForSetupUniverseReplication(
+      const xcluster::ReplicationGroupId& replication_group_id = kReplicationGroupId,
+      MiniCluster* consumer_cluster = nullptr, YBClient* consumer_client = nullptr);
+
   Status GetCDCStreamForTable(const TableId& table_id, master::ListCDCStreamsResponsePB* resp);
 
   uint32_t GetSuccessfulWriteOps(MiniCluster* cluster);
diff --git a/src/yb/integration-tests/xcluster/xcluster_upgrade-test.cc b/src/yb/integration-tests/xcluster/xcluster_upgrade-test.cc
index 6e7850e95d..04d36a74fe 100644
--- a/src/yb/integration-tests/xcluster/xcluster_upgrade-test.cc
+++ b/src/yb/integration-tests/xcluster/xcluster_upgrade-test.cc
@@ -193,11 +193,7 @@ TEST_F(XClusterUpgradeTest, SetupWithLowerTargetUniverse) {
   ASSERT_NOK(s);
   ASSERT_STR_CONTAINS(s.ToString(), kAutoFlagsMismatchError);
   master::GetUniverseReplicationResponsePB resp;
-  s = VerifyUniverseReplication(replication_group_id1, &resp);
-  ASSERT_NOK(s);
-  ASSERT_STR_CONTAINS(s.ToString(), kAutoFlagsMismatchError);
-
-  ASSERT_OK(DeleteUniverseReplication(replication_group_id1));
+  ASSERT_NOK(GetUniverseReplicationInfo(consumer_cluster_, replication_group_id1));
 
   // Without AutoFlag validation enabled it should work. SourceWithoutAutoFlagCompatiblity and
   // TargetWithoutAutoFlagCompatiblity tests validate this more thoroughly.
diff --git a/src/yb/integration-tests/xcluster/xcluster_ysql_colocated-test.cc b/src/yb/integration-tests/xcluster/xcluster_ysql_colocated-test.cc
index a29acb61ff..2146444b7f 100644
--- a/src/yb/integration-tests/xcluster/xcluster_ysql_colocated-test.cc
+++ b/src/yb/integration-tests/xcluster/xcluster_ysql_colocated-test.cc
@@ -295,7 +295,9 @@ TEST_F(XClusterYsqlColocatedTest, DatabaseReplication) { ASSERT_OK(TestDatabaseR
 
 TEST_F(XClusterYsqlColocatedTest, LegacyColocatedDatabaseReplication) {
   ANNOTATE_UNPROTECTED_WRITE(FLAGS_ysql_legacy_colocated_database_creation) = true;
-  ASSERT_OK(TestDatabaseReplication());
+  ASSERT_NOK_STR_CONTAINS(
+      TestDatabaseReplication(),
+      "Pre GA colocated databases are not supported with xCluster replication");
 }
 
 TEST_F(XClusterYsqlColocatedTest, DatabaseReplicationWithPacked) {
@@ -313,12 +315,6 @@ TEST_F(XClusterYsqlColocatedTest, PackedDatabaseReplicationWithTransactions) {
   ASSERT_OK(TestDatabaseReplication(/* compact= */ true, /* use_transaction = */ true));
 }
 
-TEST_F(XClusterYsqlColocatedTest, LegacyColocatedDatabaseReplicationWithPacked) {
-  ANNOTATE_UNPROTECTED_WRITE(FLAGS_ysql_enable_packed_row) = true;
-  ANNOTATE_UNPROTECTED_WRITE(FLAGS_ysql_legacy_colocated_database_creation) = true;
-  ASSERT_OK(TestDatabaseReplication(/*compact=*/true));
-}
-
 TEST_F(XClusterYsqlColocatedTest, TestTablesReplicationWithLargeTableCount) {
   constexpr int kNTabletsPerColocatedTable = 1;
   std::vector<uint32_t> tables_vector;
diff --git a/src/yb/master/CMakeLists.txt b/src/yb/master/CMakeLists.txt
index 33a6850367..16b1a66136 100644
--- a/src/yb/master/CMakeLists.txt
+++ b/src/yb/master/CMakeLists.txt
@@ -151,7 +151,7 @@ set(MASTER_SRCS
   xcluster/xcluster_source_manager.cc
   xcluster/xcluster_target_manager.cc
   xcluster/xcluster_universe_replication_alter_helper.cc
-  xcluster/xcluster_universe_replication_setup_helper.cc
+  xcluster/xcluster_inbound_replication_group_setup_task.cc
   xrepl_catalog_manager.cc
   yql_aggregates_vtable.cc
   yql_auth_resource_role_permissions_index.cc
diff --git a/src/yb/master/catalog_entity_info.proto b/src/yb/master/catalog_entity_info.proto
index eb74b1685b..fe2282d44f 100644
--- a/src/yb/master/catalog_entity_info.proto
+++ b/src/yb/master/catalog_entity_info.proto
@@ -694,8 +694,8 @@ message SysUniverseReplicationEntryPB {
   // 7, 8, and 9 were used by Namespace-level replication but never populated in
   // production so they are valid for reuse.
 
-  // Mapping from Producer Table ID to Producer->Consumer schema version mappings
-  map<string, SchemaVersionMappingEntryPB> schema_version_mappings = 10;
+  // DEPRECATED schema_version_mappings
+  reserved 10;
 
   // Set when consistent transactions are enabled for a replication group.
   optional bool transactional = 11; // [default = false]
diff --git a/src/yb/master/catalog_manager.cc b/src/yb/master/catalog_manager.cc
index e3d9cbdf37..d1c5ad363b 100644
--- a/src/yb/master/catalog_manager.cc
+++ b/src/yb/master/catalog_manager.cc
@@ -2192,6 +2192,8 @@ bool CatalogManager::StartShutdown() {
 
   refresh_ysql_pg_catalog_versions_task_.StartShutdown();
 
+  xcluster_manager_->StartShutdown();
+
   if (sys_catalog_) {
     sys_catalog_->StartShutdown();
   }
@@ -2204,7 +2206,7 @@ void CatalogManager::CompleteShutdown() {
   refresh_ysql_tablespace_info_task_.CompleteShutdown();
   xrepl_parent_tablet_deletion_task_.CompleteShutdown();
   refresh_ysql_pg_catalog_versions_task_.CompleteShutdown();
-  xcluster_manager_->Shutdown();
+  xcluster_manager_->CompleteShutdown();
 
   if (background_tasks_) {
     background_tasks_->Shutdown();
@@ -12927,23 +12929,6 @@ CatalogManager::GetStatefulServicesStatus() const {
   return result;
 }
 
-Status CatalogManager::GetTableGroupAndColocationInfo(
-    const TableId& table_id, TablegroupId& out_tablegroup_id, bool& out_colocated_database) {
-  SharedLock lock(mutex_);
-  const auto* tablegroup = tablegroup_manager_->FindByTable(table_id);
-  SCHECK_FORMAT(tablegroup, NotFound, "No tablegroup found for table: $0", table_id);
-
-  out_tablegroup_id = tablegroup->id();
-
-  auto ns = FindPtrOrNull(namespace_ids_map_, tablegroup->database_id());
-  SCHECK(
-      ns, NotFound,
-      Format("Could not find namespace by namespace id $0", tablegroup->database_id()));
-  out_colocated_database = ns->colocated();
-
-  return Status::OK();
-}
-
 Result<std::vector<SysCatalogEntryDumpPB>> CatalogManager::FetchFromSysCatalog(
     SysRowEntryType type, const std::string& item_id_filter) {
   SCHECK_NOTNULL(sys_catalog_);
@@ -13022,5 +13007,12 @@ Status CatalogManager::WriteSysCatalogEntry(
       req->entry_type(), req->entity_id(), req->pb_debug_string(), statement_type);
 }
 
+Result<TablegroupId> CatalogManager::GetTablegroupId(const TableId& table_id) {
+  SharedLock lock(mutex_);
+  const auto* tablegroup = tablegroup_manager_->FindByTable(table_id);
+  SCHECK_FORMAT(tablegroup, NotFound, "No tablegroup found for table: $0", table_id);
+  return tablegroup->id();
+}
+
 }  // namespace master
 }  // namespace yb
diff --git a/src/yb/master/catalog_manager.h b/src/yb/master/catalog_manager.h
index 4c90663439..a7c784cc96 100644
--- a/src/yb/master/catalog_manager.h
+++ b/src/yb/master/catalog_manager.h
@@ -1582,9 +1582,7 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
 
   Result<SchemaVersion> GetTableSchemaVersion(const TableId& table_id);
 
-  Status GetTableGroupAndColocationInfo(
-      const TableId& table_id, TablegroupId& out_tablegroup_id, bool& out_colocated_database)
-      EXCLUDES(mutex_);
+  Result<TablegroupId> GetTablegroupId(const TableId& table_id) EXCLUDES(mutex_);
 
   void InsertNewUniverseReplication(UniverseReplicationInfo& replication_group) EXCLUDES(mutex_);
 
diff --git a/src/yb/master/master_xrepl-test.cc b/src/yb/master/master_xrepl-test.cc
index 0f6a1966a3..611dc6d615 100644
--- a/src/yb/master/master_xrepl-test.cc
+++ b/src/yb/master/master_xrepl-test.cc
@@ -19,8 +19,6 @@
 #include "yb/common/schema.h"
 #include "yb/common/wire_protocol.h"
 
-#include "yb/gutil/casts.h"
-
 #include "yb/master/master-test_base.h"
 #include "yb/master/master_ddl.proxy.h"
 #include "yb/master/master_defaults.h"
@@ -97,12 +95,6 @@ class MasterTestXRepl  : public MasterTestBase {
   Result<ListCDCStreamsResponsePB> ListCDCSDKStreams();
   Result<bool> IsObjectPartOfXRepl(const TableId& table_id);
 
-  Status SetupUniverseReplication(
-      const std::string& producer_id, const std::vector<std::string>& master_addr,
-      const std::vector<std::string>& tables);
-  Status DeleteUniverseReplication(const std::string& producer_id);
-  Result<GetUniverseReplicationResponsePB> GetUniverseReplication(const std::string& producer_id);
-
   Status CreateTableWithTableId(TableId* table_id);
   Status DeleteNamespace(std::string namespace_id, YQLDatabase db_type);
   Result<bool> IsDeleteNamespaceDone(std::string namespace_id);
@@ -290,56 +282,6 @@ Result<bool> MasterTestXRepl::IsObjectPartOfXRepl(const TableId& table_id) {
       Result<bool>(resp.is_object_part_of_xrepl());
 }
 
-Status MasterTestXRepl::SetupUniverseReplication(
-    const std::string& producer_id, const std::vector<std::string>& producer_master_addrs,
-    const std::vector<TableId>& tables) {
-  SetupUniverseReplicationRequestPB req;
-  SetupUniverseReplicationResponsePB resp;
-
-  req.set_replication_group_id(producer_id);
-  req.mutable_producer_master_addresses()->Reserve(narrow_cast<int>(producer_master_addrs.size()));
-  for (const auto& addr : producer_master_addrs) {
-    std::vector<std::string> hp;
-    boost::split(hp, addr, boost::is_any_of(":"));
-    CHECK_EQ(hp.size(), 2);
-    auto* master = req.add_producer_master_addresses();
-    master->set_host(hp[0]);
-    master->set_port(boost::lexical_cast<uint32_t>(hp[1]));
-  }
-  req.mutable_producer_table_ids()->Reserve(narrow_cast<int>(tables.size()));
-  for (const auto& table : tables) {
-    req.add_producer_table_ids(table);
-  }
-
-  RETURN_NOT_OK(proxy_replication_->SetupUniverseReplication(req, &resp, ResetAndGetController()));
-  if (resp.has_error()) {
-    RETURN_NOT_OK(StatusFromPB(resp.error().status()));
-  }
-  return Status::OK();
-}
-
-Result<GetUniverseReplicationResponsePB> MasterTestXRepl::GetUniverseReplication(
-    const std::string& producer_id) {
-  GetUniverseReplicationRequestPB req;
-  GetUniverseReplicationResponsePB resp;
-  req.set_replication_group_id(producer_id);
-
-  RETURN_NOT_OK(proxy_replication_->GetUniverseReplication(req, &resp, ResetAndGetController()));
-  return resp;
-}
-
-Status MasterTestXRepl::DeleteUniverseReplication(const std::string& producer_id) {
-  DeleteUniverseReplicationRequestPB req;
-  DeleteUniverseReplicationResponsePB resp;
-  req.set_replication_group_id(producer_id);
-
-  RETURN_NOT_OK(proxy_replication_->DeleteUniverseReplication(req, &resp, ResetAndGetController()));
-  if (resp.has_error()) {
-    RETURN_NOT_OK(StatusFromPB(resp.error().status()));
-  }
-  return Status::OK();
-}
-
 Status MasterTestXRepl::CreateTableWithTableId(TableId* table_id) {
   return CreateTable(kTableName, kTableSchema, table_id);
 }
@@ -1091,45 +1033,6 @@ TEST_F(MasterTestXRepl, TestIsObjectPartOfXRepl) {
   ASSERT_TRUE(ASSERT_RESULT(IsObjectPartOfXRepl(table_id)));
 }
 
-TEST_F(MasterTestXRepl, TestSetupUniverseReplication) {
-  std::string producer_id = "producer_universe";
-  std::vector<std::string> producer_masters {"127.0.0.1:7100"};
-  std::vector<std::string> tables {"some_table_id"};
-  // Always fails because we don't have actual producer.
-  ASSERT_NOK(SetupUniverseReplication(producer_id, producer_masters, tables));
-
-  auto resp = ASSERT_RESULT(GetUniverseReplication(producer_id));
-  ASSERT_EQ(resp.entry().replication_group_id(), producer_id);
-
-  ASSERT_EQ(resp.entry().producer_master_addresses_size(), 1);
-  std::string addr;
-  const auto& hp = resp.entry().producer_master_addresses(0);
-  addr = hp.host() + ":" + std::to_string(hp.port());
-  ASSERT_EQ(addr, "127.0.0.1:7100");
-
-  ASSERT_EQ(resp.entry().tables_size(), 1);
-  ASSERT_EQ(resp.entry().tables(0), "some_table_id");
-}
-
-TEST_F(MasterTestXRepl, TestDeleteUniverseReplication) {
-  std::string producer_id = "producer_universe";
-  std::vector<std::string> producer_masters {"127.0.0.1:7100"};
-  std::vector<std::string> tables {"some_table_id"};
-  // Always fails because we don't have actual producer.
-  ASSERT_NOK(SetupUniverseReplication(producer_id, producer_masters, tables));
-
-  // Verify that universe was created.
-  auto resp = ASSERT_RESULT(GetUniverseReplication(producer_id));
-  ASSERT_EQ(resp.entry().replication_group_id(), producer_id);
-
-  ASSERT_OK(DeleteUniverseReplication(producer_id));
-
-  resp.Clear();
-  resp = ASSERT_RESULT(GetUniverseReplication(producer_id));
-  ASSERT_TRUE(resp.has_error());
-  ASSERT_EQ(MasterErrorPB::OBJECT_NOT_FOUND, resp.error().code());
-}
-
 TEST_F(MasterTestXRepl, DropNamespaceWithLiveCDCStream) {
   CreateNamespaceResponsePB create_namespace_resp;
   ASSERT_OK(CreatePgsqlNamespace(kNamespaceName, kPgsqlNamespaceId, &create_namespace_resp));
diff --git a/src/yb/master/multi_step_monitored_task.h b/src/yb/master/multi_step_monitored_task.h
index 14e5a76101..dd1b7477ac 100644
--- a/src/yb/master/multi_step_monitored_task.h
+++ b/src/yb/master/multi_step_monitored_task.h
@@ -55,6 +55,17 @@ class MultiStepMonitoredTask : public server::RunnableMonitoredTask {
   static Status StartTasks(
       const std::vector<MultiStepMonitoredTask*>& tasks, StdStatusCallback group_completion_cb);
 
+  template <typename T>
+  static Status StartTasks(
+      const std::vector<std::shared_ptr<T>>& tasks, StdStatusCallback group_completion_cb) {
+    std::vector<MultiStepMonitoredTask*> raw_tasks;
+    raw_tasks.reserve(tasks.size());
+    for (const auto& task : tasks) {
+      raw_tasks.push_back(task.get());
+    }
+    return StartTasks(raw_tasks, group_completion_cb);
+  }
+
  protected:
   explicit MultiStepMonitoredTask(ThreadPool& async_task_pool, rpc::Messenger& messenger);
 
diff --git a/src/yb/master/post_tablet_create_task_base.cc b/src/yb/master/post_tablet_create_task_base.cc
index 3961bbada1..c14e84c642 100644
--- a/src/yb/master/post_tablet_create_task_base.cc
+++ b/src/yb/master/post_tablet_create_task_base.cc
@@ -60,16 +60,15 @@ Status PostTabletCreateTaskBase::StartTasks(
     CatalogManager* catalog_manager, TableInfoPtr table_info, const LeaderEpoch& epoch) {
   SCHECK(!table_creation_tasks.empty(), IllegalState, "At least one task must be scheduled");
 
-  std::vector<MultiStepMonitoredTask*> tasks;
   for (const auto& task : table_creation_tasks) {
     SCHECK_EQ(
         task->table_info_->id(), table_info->id(), IllegalState,
         "All tasks in group must belong to the same table");
-    tasks.emplace_back(task.get());
   }
 
   return MultiStepMonitoredTask::StartTasks(
-      tasks, std::bind(&MarkTableAsRunningIfHealthy, catalog_manager, table_info, epoch, _1));
+      table_creation_tasks,
+      std::bind(&MarkTableAsRunningIfHealthy, catalog_manager, table_info, epoch, _1));
 }
 
 void PostTabletCreateTaskBase::TaskCompleted(const Status& status) {
diff --git a/src/yb/master/xcluster/add_table_to_xcluster_target_task.cc b/src/yb/master/xcluster/add_table_to_xcluster_target_task.cc
index f1025a8a0d..d601673bd6 100644
--- a/src/yb/master/xcluster/add_table_to_xcluster_target_task.cc
+++ b/src/yb/master/xcluster/add_table_to_xcluster_target_task.cc
@@ -179,8 +179,10 @@ Status AddTableToXClusterTargetTask::AddTableToReplicationGroup(
 }
 
 Status AddTableToXClusterTargetTask::WaitForSetupUniverseReplicationToFinish() {
-  auto operation_result = VERIFY_RESULT(IsSetupUniverseReplicationDone(
-      xcluster::GetAlterReplicationGroupId(universe_->ReplicationGroupId()), catalog_manager_));
+  // Skip the health checks since we will wait for the safe time to advance in the next step.
+  auto operation_result = VERIFY_RESULT(xcluster_manager_.IsSetupUniverseReplicationDone(
+      xcluster::GetAlterReplicationGroupId(universe_->ReplicationGroupId()),
+      /*skip_health_check=*/true));
 
   if (!operation_result.done()) {
     VLOG_WITH_PREFIX(2) << "Waiting for setup universe replication to finish";
diff --git a/src/yb/master/xcluster/xcluster_bootstrap_helper.cc b/src/yb/master/xcluster/xcluster_bootstrap_helper.cc
index 5bd052dd07..14b6793a2d 100644
--- a/src/yb/master/xcluster/xcluster_bootstrap_helper.cc
+++ b/src/yb/master/xcluster/xcluster_bootstrap_helper.cc
@@ -19,6 +19,7 @@
 #include "yb/master/catalog_manager.h"
 #include "yb/master/master.h"
 #include "yb/master/snapshot_transfer_manager.h"
+#include "yb/master/xcluster/xcluster_manager.h"
 #include "yb/master/xcluster_rpc_tasks.h"
 #include "yb/master/xcluster/xcluster_universe_replication_setup_helper.h"
 #include "yb/util/backoff_waiter.h"
@@ -213,8 +214,7 @@ Status SetupUniverseReplicationWithBootstrapHelper::ValidateReplicationBootstrap
   }
 
   RETURN_NOT_OK_PREPEND(
-      SetupUniverseReplicationHelper::ValidateMasterAddressesBelongToDifferentCluster(
-          master_, req->producer_master_addresses()),
+      ValidateMasterAddressesBelongToDifferentCluster(master_, req->producer_master_addresses()),
       req->ShortDebugString());
 
   auto universe =
@@ -281,8 +281,9 @@ void SetupUniverseReplicationWithBootstrapHelper::DoReplicationBootstrap(
 
   SetReplicationBootstrapState(
       bootstrap_info, SysUniverseReplicationBootstrapEntryPB::SETUP_REPLICATION);
-  MARK_BOOTSTRAP_FAILED_NOT_OK(SetupUniverseReplicationHelper::Setup(
-      master_, catalog_manager_, &replication_req, &replication_resp, epoch_));
+
+  MARK_BOOTSTRAP_FAILED_NOT_OK(xcluster_manager_.SetupUniverseReplication(
+      &replication_req, &replication_resp, /*rpc=*/nullptr, epoch_));
 
   LOG(INFO) << Format(
       "Successfully completed replication bootstrap for $0", replication_id.ToString());
diff --git a/src/yb/master/xcluster/xcluster_inbound_replication_group_setup_task.cc b/src/yb/master/xcluster/xcluster_inbound_replication_group_setup_task.cc
new file mode 100644
index 0000000000..ed0e7a6d8b
--- /dev/null
+++ b/src/yb/master/xcluster/xcluster_inbound_replication_group_setup_task.cc
@@ -0,0 +1,1122 @@
+// Copyright (c) YugabyteDB, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
+// in compliance with the License.  You may obtain a copy of the License at
+//
+// http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software distributed under the License
+// is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
+// or implied.  See the License for the specific language governing permissions and limitations
+// under the License.
+//
+
+#include "yb/master/xcluster/xcluster_inbound_replication_group_setup_task.h"
+
+#include "yb/client/table_info.h"
+#include "yb/client/table.h"
+#include "yb/client/xcluster_client.h"
+
+#include "yb/common/colocated_util.h"
+#include "yb/common/common_net.pb.h"
+#include "yb/common/xcluster_util.h"
+
+#include "yb/gutil/bind.h"
+
+#include "yb/master/catalog_manager.h"
+#include "yb/master/leader_epoch.h"
+#include "yb/master/master_ddl.pb.h"
+#include "yb/master/master_replication.pb.h"
+#include "yb/master/master_util.h"
+#include "yb/master/master.h"
+#include "yb/master/xcluster_consumer_registry_service.h"
+#include "yb/master/xcluster/master_xcluster_util.h"
+#include "yb/master/xcluster/xcluster_manager.h"
+#include "yb/master/xcluster/xcluster_replication_group.h"
+
+#include "yb/util/flags/auto_flags_util.h"
+#include "yb/util/status.h"
+#include "yb/util/status_format.h"
+
+// TODO: Bootstrap check sends multiple RPCs. Consider converting it to one RPC when enabling
+// check_bootstrap_required flag by default.
+DEFINE_RUNTIME_bool(check_bootstrap_required, false,
+    "Is it necessary to check whether bootstrap is required for Universe Replication.");
+
+DEFINE_test_flag(bool, allow_ycql_transactional_xcluster, false,
+    "Determines if xCluster transactional replication on YCQL tables is allowed.");
+
+DEFINE_test_flag(bool, fail_universe_replication_merge, false,
+    "Causes MergeUniverseReplication to fail with an error.");
+
+DEFINE_test_flag(bool, exit_unfinished_merging, false,
+    "Whether to exit part way through the merging universe process.");
+
+DECLARE_bool(enable_xcluster_auto_flag_validation);
+
+using namespace std::placeholders;
+
+namespace yb::master {
+
+Result<std::shared_ptr<XClusterInboundReplicationGroupSetupTaskIf>>
+CreateSetupUniverseReplicationTask(
+    Master& master, CatalogManager& catalog_manager, const SetupUniverseReplicationRequestPB* req,
+    const LeaderEpoch& epoch) {
+  SCHECK_PB_FIELDS_NOT_EMPTY(
+      *req, replication_group_id, producer_master_addresses, producer_table_ids);
+
+  std::vector<xrepl::StreamId> stream_ids;
+  for (const auto& bootstrap_id : req->producer_bootstrap_ids()) {
+    stream_ids.push_back(VERIFY_RESULT(xrepl::StreamId::FromString(bootstrap_id)));
+  }
+
+  std::vector<NamespaceId> source_namespace_ids, target_namespace_ids;
+  for (const auto& source_ns_id : req->producer_namespaces()) {
+    SCHECK(!source_ns_id.id().empty(), InvalidArgument, "Invalid Namespace Id");
+    SCHECK(!source_ns_id.name().empty(), InvalidArgument, "Invalid Namespace name");
+    SCHECK_EQ(
+        source_ns_id.database_type(), YQLDatabase::YQL_DATABASE_PGSQL, InvalidArgument,
+        "Invalid Namespace database_type");
+
+    source_namespace_ids.push_back(source_ns_id.id());
+
+    NamespaceIdentifierPB target_ns_id;
+    target_ns_id.set_database_type(YQLDatabase::YQL_DATABASE_PGSQL);
+    target_ns_id.set_name(source_ns_id.name());
+    auto ns_info = VERIFY_RESULT(catalog_manager.FindNamespace(target_ns_id));
+    target_namespace_ids.push_back(ns_info->id());
+  }
+
+  std::vector<TableId> source_table_ids;
+  source_table_ids.insert(
+      source_table_ids.begin(), req->producer_table_ids().begin(), req->producer_table_ids().end());
+
+  auto setup_task = std::shared_ptr<XClusterInboundReplicationGroupSetupTask>(
+      new XClusterInboundReplicationGroupSetupTask(
+          master, catalog_manager, epoch, xcluster::ReplicationGroupId(req->replication_group_id()),
+          req->producer_master_addresses(), std::move(source_table_ids), std::move(stream_ids),
+          req->transactional(), std::move(source_namespace_ids), std::move(target_namespace_ids)));
+  RETURN_NOT_OK(setup_task->ValidateInputArguments());
+
+  return setup_task;
+}
+
+XClusterInboundReplicationGroupSetupTask::XClusterInboundReplicationGroupSetupTask(
+    Master& master, CatalogManager& catalog_manager, const LeaderEpoch& epoch,
+    xcluster::ReplicationGroupId&& replication_group_id,
+    const google::protobuf::RepeatedPtrField<HostPortPB>& source_masters,
+    std::vector<TableId>&& source_table_ids, std::vector<xrepl::StreamId>&& stream_ids,
+    bool transactional, std::vector<NamespaceId>&& source_namespace_ids,
+    std::vector<NamespaceId>&& target_namespace_ids)
+    : MultiStepMonitoredTask(*catalog_manager.AsyncTaskPool(), *master.messenger()),
+      master_(master),
+      catalog_manager_(catalog_manager),
+      sys_catalog_(*catalog_manager.sys_catalog()),
+      xcluster_manager_(*catalog_manager.GetXClusterManagerImpl()),
+      epoch_(epoch),
+      replication_group_id_(std::move(replication_group_id)),
+      source_masters_(source_masters),
+      transactional_(transactional),
+      source_table_ids_(std::move(source_table_ids)),
+      stream_ids_(std::move(stream_ids)),
+      source_namespace_ids_(std::move(source_namespace_ids)),
+      target_namespace_ids_(std::move(target_namespace_ids)),
+      is_alter_replication_(xcluster::IsAlterReplicationGroupId(replication_group_id_)),
+      is_db_scoped_(!source_namespace_ids_.empty()),
+      stream_ids_provided_(!stream_ids_.empty()) {
+  log_prefix_ = Format(
+      "xCluster InboundReplicationGroup [$0] $1: ", replication_group_id_,
+      (is_alter_replication_ ? "Alter" : "Setup"));
+}
+
+IsOperationDoneResult XClusterInboundReplicationGroupSetupTask::DoneResult() const {
+  SharedLock l(done_result_mutex_);
+  return done_result_;
+}
+
+client::YBClient& XClusterInboundReplicationGroupSetupTask::GetYbClient() {
+  return remote_client_->GetYbClient();
+}
+
+client::XClusterClient& XClusterInboundReplicationGroupSetupTask::GetXClusterClient() {
+  return remote_client_->GetXClusterClient();
+}
+
+Status XClusterInboundReplicationGroupSetupTask::RegisterTask() {
+  VLOG_WITH_PREFIX_AND_FUNC(1);
+  return xcluster_manager_.RegisterMonitoredTask(shared_from(this));
+}
+
+void XClusterInboundReplicationGroupSetupTask::UnregisterTask() {
+  VLOG_WITH_PREFIX_AND_FUNC(1);
+  xcluster_manager_.UnRegisterMonitoredTask(shared_from(this));
+}
+
+Status XClusterInboundReplicationGroupSetupTask::ValidateRunnable() {
+  if (DoneResult().done()) {
+    return STATUS(Aborted, LogPrefix(), "Task already completed");
+  }
+
+  return Status::OK();
+}
+
+void XClusterInboundReplicationGroupSetupTask::TaskCompleted(const Status& status) {
+  if (remote_client_) {
+    // Stop any inflight RPCs on child tasks.
+    remote_client_->Shutdown();
+  }
+
+  std::lock_guard l(done_result_mutex_);
+
+  if (!status.ok()) {
+    LOG_WITH_PREFIX(WARNING) << "Failed: " << status.ToString();
+
+    DCHECK(!done_result_) << "We cannot fail after we successfully completed";
+    if (!done_result_.done()) {
+      // Only record the first error.
+      done_result_ = IsOperationDoneResult::Done(std::move(status));
+    }
+    return;
+  }
+
+  LOG_IF(DFATAL, !done_result_.done())
+      << "done_result_ should be marked as done before successful task completion";
+}
+
+bool XClusterInboundReplicationGroupSetupTask::TryCancel() {
+  static auto status = STATUS(Aborted, LogPrefix(), "Cancelled");
+  {
+    std::lock_guard l(done_result_mutex_);
+    if (done_result_.done()) {
+      return !done_result_.status().ok();
+    }
+
+    done_result_ = IsOperationDoneResult::Done(status);
+  }
+
+  AbortAndReturnPrevState(status);
+  return true;
+}
+
+Status XClusterInboundReplicationGroupSetupTask::ValidateInputArguments() {
+  SCHECK(!replication_group_id_.empty(), InvalidArgument, "Invalid Replication Group Id");
+  SCHECK(!source_table_ids_.empty(), InvalidArgument, "No tables provided");
+
+  for (const auto& source_table_id : source_table_ids_) {
+    SCHECK(!source_table_id.empty(), InvalidArgument, "Invalid Table Id");
+    SCHECK(
+        !IsColocatedDbParentTableId(source_table_id), NotSupported,
+        "Pre GA colocated databases are not supported with xCluster replication: $0",
+        source_table_id);
+  }
+
+  if (stream_ids_provided_) {
+    SCHECK_EQ(
+        stream_ids_.size(), source_table_ids_.size(), InvalidArgument,
+        "Number of bootstrap ids must be equal to number of tables");
+
+    for (const auto& stream_id : stream_ids_) {
+      SCHECK(stream_id, InvalidArgument, "Invalid Stream Id");
+    }
+  }
+
+  {
+    auto l = catalog_manager_.ClusterConfig()->LockForRead();
+    SCHECK_NE(
+        l->pb.cluster_uuid(), replication_group_id_, InvalidArgument,
+        "Replication group Id cannot be the same as the cluster UUID");
+  }
+
+  RETURN_NOT_OK(ValidateMasterAddressesBelongToDifferentCluster(master_, source_masters_));
+
+  SCHECK(
+      source_namespace_ids_.empty() || transactional_, InvalidArgument,
+      "Transactional flag must be set for Db scoped replication groups");
+
+  SCHECK_EQ(
+      source_namespace_ids_.size(), target_namespace_ids_.size(), InvalidArgument,
+      "Source and target namespace ids must be of the same size");
+
+  auto universe_replication = catalog_manager_.GetUniverseReplication(
+      xcluster::GetOriginalReplicationGroupId(replication_group_id_));
+  if (is_alter_replication_) {
+    SCHECK(universe_replication, NotFound, "Replication group $0 not found", replication_group_id_);
+  } else {
+    SCHECK(
+        !universe_replication, AlreadyPresent, "Replication group $0 already present",
+        replication_group_id_);
+  }
+
+  RETURN_NOT_OK(ValidateNamespaceListForDbScoped());
+
+  IF_DEBUG_MODE(argument_validation_done_ = true);
+
+  return Status::OK();
+}
+
+Status XClusterInboundReplicationGroupSetupTask::FirstStep() {
+  IF_DEBUG_MODE(CHECK(argument_validation_done_));
+
+  {
+    std::vector<HostPort> hp;
+    HostPortsFromPBs(source_masters_, &hp);
+    remote_client_ =
+        VERIFY_RESULT(client::XClusterRemoteClientHolder::Create(replication_group_id_, hp));
+  }
+
+  if (FLAGS_enable_xcluster_auto_flag_validation && !is_alter_replication_) {
+    // Sanity check AutoFlags compatibility before we start further work.
+    RETURN_NOT_OK(GetAutoFlagConfigVersionIfCompatible());
+  }
+
+  LOG_WITH_PREFIX(INFO) << "Started schema validation for " << source_table_ids_.size()
+                        << " table(s)";
+
+  std::vector<std::shared_ptr<MultiStepMonitoredTask>> child_tasks;
+  for (size_t i = 0; i < source_table_ids_.size(); i++) {
+    const auto stream_id = stream_ids_provided_ ? stream_ids_[i] : xrepl::StreamId::Nil();
+    auto table_setup_info = std::shared_ptr<XClusterTableSetupTask>(
+        new XClusterTableSetupTask(shared_from(this), source_table_ids_[i], stream_id));
+
+    table_setup_info->Start();
+  }
+
+  return Status::OK();
+}
+
+void XClusterInboundReplicationGroupSetupTask::TableTaskCompletionCallback(
+    const TableId& source_table_id, const Result<XClusterTableSetupInfo>& table_setup_result) {
+  VLOG_WITH_PREFIX_AND_FUNC(1) << table_setup_result.ToString();
+
+  if (!table_setup_result) {
+    AbortAndReturnPrevState(table_setup_result.status());
+    return;
+  }
+
+  {
+    std::lock_guard l(mutex_);
+    if (source_table_infos_.contains(source_table_id)) {
+      LOG_WITH_PREFIX(DFATAL) << "TableTaskCompletionCallback called multiple times for table "
+                              << source_table_id;
+      return;
+    }
+
+    source_table_infos_[source_table_id] = std::move(*table_setup_result);
+
+    VLOG_WITH_PREFIX(1) << "Processed " << source_table_infos_.size() << " tables out of "
+                        << source_table_ids_.size();
+
+    if (source_table_infos_.size() != source_table_ids_.size()) {
+      // We still have uncompleted tasks. The final task will proceed with the setup.
+      return;
+    }
+  }
+
+  ScheduleNextStep(
+      std::bind(
+          &XClusterInboundReplicationGroupSetupTask::SetupReplicationAfterProcessingAllTables,
+          shared_from(this)),
+      "Process replication group after tables validated");
+}
+
+Status XClusterInboundReplicationGroupSetupTask::SetupReplicationAfterProcessingAllTables() {
+  if (stream_ids_provided_) {
+    RETURN_NOT_OK_PREPEND(
+        UpdateSourceStreamOptions(), "Failed to update xCluster stream options on source universe");
+  }
+
+  LOG_WITH_PREFIX(INFO) << "Table validation and stream setup completed successfully for "
+                        << source_table_ids_.size() << " table(s)";
+
+  return SetupReplicationGroup();
+}
+
+Status XClusterInboundReplicationGroupSetupTask::UpdateSourceStreamOptions() {
+  std::vector<xrepl::StreamId> update_bootstrap_ids;
+  std::vector<SysCDCStreamEntryPB> update_entries;
+
+  {
+    std::lock_guard l(mutex_);
+    for (const auto& [source_table_id, table_info] : source_table_infos_) {
+      SysCDCStreamEntryPB new_entry;
+      new_entry.add_table_id(source_table_id);
+      new_entry.mutable_options()->Reserve(narrow_cast<int>(table_info.stream_options.size()));
+      for (const auto& [key, value] : table_info.stream_options) {
+        if (key == cdc::kStreamState) {
+          // We will set state explicitly.
+          continue;
+        }
+        auto new_option = new_entry.add_options();
+        new_option->set_key(key);
+        new_option->set_value(value);
+      }
+      new_entry.set_state(master::SysCDCStreamEntryPB::ACTIVE);
+      new_entry.set_transactional(transactional_);
+
+      update_bootstrap_ids.push_back(table_info.stream_id);
+      update_entries.push_back(new_entry);
+    }
+  }
+
+  RETURN_NOT_OK_PREPEND(
+      remote_client_->GetYbClient().UpdateCDCStream(update_bootstrap_ids, update_entries),
+      "Unable to update xCluster stream options on source universe");
+
+  return Status::OK();
+}
+
+Status XClusterInboundReplicationGroupSetupTask::SetupReplicationGroup() {
+  std::lock_guard l(mutex_);
+
+  // Last minute validations.
+  // TODO: Block DDLs and other xCluster setup/alter operations between this step till the end of
+  // this function.
+  RETURN_NOT_OK(ValidateNamespaceListForDbScoped());
+  RETURN_NOT_OK(ValidateTableListForDbScoped());
+
+  const auto original_id = xcluster::GetOriginalReplicationGroupId(replication_group_id_);
+
+  auto universe = catalog_manager_.GetUniverseReplication(original_id);
+  std::optional<UniverseReplicationInfo::WriteLock> universe_lock;
+  if (is_alter_replication_) {
+    SCHECK(universe != nullptr, NotFound, "Replication group $0 not found", original_id);
+    universe_lock = universe->LockForWrite();
+  } else {
+    SCHECK(
+        universe == nullptr, AlreadyPresent, "Replication group $0 already present",
+        replication_group_id_);
+
+    universe = VERIFY_RESULT(CreateNewUniverseReplicationInfo());
+  }
+
+  auto& universe_pb = universe->mutable_metadata()->mutable_dirty()->pb;
+  PopulateUniverseReplication(universe_pb);
+
+  auto cluster_config = catalog_manager_.ClusterConfig();
+  auto cluster_config_l = cluster_config->LockForWrite();
+  auto& cluster_config_producer_map =
+      *cluster_config_l.mutable_data()->pb.mutable_consumer_registry()->mutable_producer_map();
+
+  if (is_alter_replication_) {
+    SCHECK_EQ(
+        cluster_config_producer_map.count(original_id.ToString()), 1, InvalidArgument,
+        Format("ClusterConfig producer_map missing for ReplicationGroup $0", original_id));
+
+    LOG(INFO) << "Merging xCluster ReplicationGroup: " << replication_group_id_ << " into "
+              << original_id;
+
+    SCHECK(
+        !FLAGS_TEST_fail_universe_replication_merge, IllegalState,
+        "TEST_fail_universe_replication_merge");
+
+    if (FLAGS_TEST_exit_unfinished_merging) {
+      LOG_WITH_PREFIX(INFO) << "Completed successfully due to FLAGS_TEST_exit_unfinished_merging";
+      {
+        std::lock_guard done_l(done_result_mutex_);
+        done_result_ = IsOperationDoneResult::Done();
+      }
+      Complete();
+      return Status::OK();
+    }
+  } else {
+    SCHECK_EQ(
+        cluster_config_producer_map.count(original_id.ToString()), 0, InvalidArgument,
+        Format("ReplicationGroup $0 already exists in ClusterConfig producer_map", original_id));
+
+    cdc::ProducerEntryPB producer_entry;
+    producer_entry.mutable_master_addrs()->CopyFrom(source_masters_);
+
+    if (FLAGS_enable_xcluster_auto_flag_validation) {
+      auto auto_flag_config_version = VERIFY_RESULT(GetAutoFlagConfigVersionIfCompatible());
+
+      universe_pb.set_validated_local_auto_flags_config_version(auto_flag_config_version);
+      producer_entry.set_compatible_auto_flag_config_version(auto_flag_config_version);
+      producer_entry.set_validated_auto_flags_config_version(auto_flag_config_version);
+    }
+
+    cluster_config_producer_map[original_id.ToString()].Swap(&producer_entry);
+  }
+
+  auto& stream_map = *cluster_config_producer_map.at(original_id.ToString()).mutable_stream_map();
+  for (auto& [source_table_id, table_info] : source_table_infos_) {
+    stream_map[table_info.stream_id.ToString()].Swap(&table_info.stream_entry);
+  }
+
+  cluster_config_l.mutable_data()->pb.set_version(
+      cluster_config_l.mutable_data()->pb.version() + 1);
+
+  // Grab the done mutex, since we can no longer be cancelled.
+  UniqueLock done_l(done_result_mutex_);
+  if (done_result_.done()) {
+    return STATUS(Aborted, LogPrefix(), "Task already completed");
+  }
+
+  RETURN_NOT_OK_PREPEND(
+      sys_catalog_.Upsert(epoch_, universe, cluster_config), "Failed to wite to sys-catalog");
+
+  LOG_WITH_PREFIX(INFO) << "Replication Map: "
+                        << cluster_config_producer_map.at(original_id.ToString()).DebugString();
+
+  // Update the in-memory states now that data is persistent in sys catalog.
+  if (!is_alter_replication_) {
+    catalog_manager_.InsertNewUniverseReplication(*universe);
+  }
+
+  for (const auto& [_, table_info] : source_table_infos_) {
+    DCHECK(table_info.stream_id);
+    xcluster_manager_.RecordTableConsumerStream(
+        table_info.target_table_id, original_id, table_info.stream_id);
+  }
+
+  xcluster_manager_.SyncConsumerReplicationStatusMap(original_id, cluster_config_producer_map);
+  cluster_config_l.Commit();
+
+  if (universe_lock) {
+    universe_lock->Commit();
+  } else {
+    universe->mutable_metadata()->CommitMutation();
+  }
+
+  xcluster_manager_.CreateXClusterSafeTimeTableAndStartService();
+
+  done_result_ = IsOperationDoneResult::Done();
+  done_l.unlock();
+
+  Complete();
+
+  return Status::OK();
+}
+
+Result<uint32> XClusterInboundReplicationGroupSetupTask::GetAutoFlagConfigVersionIfCompatible() {
+  DCHECK(!is_alter_replication_);
+
+  auto local_config = master_.GetAutoFlagsConfig();
+  VLOG_WITH_FUNC(2) << "Validating AutoFlags config for replication group: "
+                    << replication_group_id_
+                    << " with target config version: " << local_config.config_version();
+
+  auto validate_result =
+      VERIFY_RESULT(master::ValidateAutoFlagsConfig(*remote_client_, local_config));
+
+  if (!validate_result) {
+    VLOG_WITH_FUNC(2)
+        << "Source universe of replication group " << replication_group_id_
+        << " is running a version that does not support the AutoFlags compatibility check yet";
+    return kInvalidAutoFlagsConfigVersion;
+  }
+
+  auto& [is_valid, source_version] = *validate_result;
+
+  SCHECK(
+      is_valid, IllegalState,
+      "AutoFlags between the universes are not compatible. Upgrade the target universe to a "
+      "version higher than or equal to the source universe");
+
+  return source_version;
+}
+
+Status XClusterInboundReplicationGroupSetupTask::ValidateNamespaceListForDbScoped() {
+  if (!is_db_scoped_) {
+    return Status::OK();
+  }
+
+  for (const auto& universe : catalog_manager_.GetAllUniverseReplications()) {
+    for (const auto& target_namespace_id : target_namespace_ids_) {
+      SCHECK_FORMAT(
+          !IncludesConsumerNamespace(*universe, target_namespace_id), AlreadyPresent,
+          "Namespace $0 already included in replication group $1", target_namespace_id,
+          universe->ReplicationGroupId());
+    }
+  }
+
+  return Status::OK();
+}
+
+Status XClusterInboundReplicationGroupSetupTask::ValidateTableListForDbScoped() {
+  if (!is_db_scoped_) {
+    return Status::OK();
+  }
+
+  std::set<TableId> target_table_ids;
+  for (const auto& [source_table_id, table_info] : source_table_infos_) {
+    target_table_ids.insert(table_info.target_table_id);
+  }
+
+  std::set<TableId> validated_tables;
+  for (const auto& namespace_id : target_namespace_ids_) {
+    auto table_infos =
+        VERIFY_RESULT(GetTablesEligibleForXClusterReplication(catalog_manager_, namespace_id));
+
+    std::vector<TableId> missing_tables;
+
+    for (const auto& table_info : table_infos) {
+      const auto& table_id = table_info->id();
+      if (target_table_ids.contains(table_id)) {
+        validated_tables.insert(table_id);
+      } else {
+        missing_tables.push_back(table_id);
+      }
+    }
+
+    SCHECK_FORMAT(
+        missing_tables.empty(), IllegalState,
+        "Namespace $0 has additional tables that were not added to xCluster DB Scoped replication "
+        "group $1: $2",
+        namespace_id, replication_group_id_, yb::ToString(missing_tables));
+  }
+
+  auto diff = STLSetSymmetricDifference(target_table_ids, validated_tables);
+  SCHECK_FORMAT(
+      diff.empty(), IllegalState,
+      "xCluster DB Scoped replication group $0 contains tables $1 that do not belong to replicated "
+      "namespaces $2",
+      replication_group_id_, yb::ToString(diff), yb::ToString(target_namespace_ids_));
+
+  return Status::OK();
+}
+
+Result<scoped_refptr<UniverseReplicationInfo>>
+XClusterInboundReplicationGroupSetupTask::CreateNewUniverseReplicationInfo() {
+  scoped_refptr<UniverseReplicationInfo> ri = new UniverseReplicationInfo(replication_group_id_);
+  ri->mutable_metadata()->StartMutation();
+  SysUniverseReplicationEntryPB* metadata = &ri->mutable_metadata()->mutable_dirty()->pb;
+  metadata->set_replication_group_id(replication_group_id_.ToString());
+  metadata->mutable_producer_master_addresses()->CopyFrom(source_masters_);
+
+  metadata->set_state(SysUniverseReplicationEntryPB::ACTIVE);
+  metadata->set_transactional(transactional_);
+
+  return ri;
+}
+
+void XClusterInboundReplicationGroupSetupTask::PopulateUniverseReplication(
+    SysUniverseReplicationEntryPB& universe_pb) {
+  if (!source_namespace_ids_.empty()) {
+    auto* db_scoped_info = universe_pb.mutable_db_scoped_info();
+    for (size_t i = 0; i < source_namespace_ids_.size(); i++) {
+      auto* ns_info = db_scoped_info->mutable_namespace_infos()->Add();
+      ns_info->set_producer_namespace_id(source_namespace_ids_[i]);
+      ns_info->set_consumer_namespace_id(target_namespace_ids_[i]);
+    }
+  }
+
+  // We need to preserve the input table order, so loop on source_table_ids_ list instead of the
+  // source_table_infos_ map. This has been the behavior since the beginning, and tests rely on
+  // this.
+  for (const auto& source_table_id : source_table_ids_) {
+    const auto& table_info = source_table_infos_[source_table_id];
+
+    universe_pb.add_tables(source_table_id);
+    (*universe_pb.mutable_table_streams())[source_table_id] = table_info.stream_id.ToString();
+    (*universe_pb.mutable_validated_tables())[source_table_id] = table_info.target_table_id;
+  }
+}
+
+XClusterTableSetupTask::XClusterTableSetupTask(
+    std::shared_ptr<XClusterInboundReplicationGroupSetupTask> parent_task,
+    const TableId& source_table_id, const xrepl::StreamId& stream_id)
+    : MultiStepMonitoredTask(
+          *parent_task->catalog_manager_.AsyncTaskPool(), *parent_task->master_.messenger()),
+      parent_task_(parent_task),
+      source_table_id_(source_table_id) {
+  table_setup_info_.stream_id = stream_id;
+  log_prefix_ = Format(
+      "xCluster InboundReplicationGroup [$0] Source Table [$1]: ",
+      parent_task_->replication_group_id_, source_table_id_);
+}
+
+Status XClusterTableSetupTask::RegisterTask() {
+  VLOG_WITH_PREFIX_AND_FUNC(1);
+  return parent_task_->xcluster_manager_.RegisterMonitoredTask(shared_from(this));
+}
+
+void XClusterTableSetupTask::UnregisterTask() {
+  VLOG_WITH_PREFIX_AND_FUNC(1);
+  parent_task_->xcluster_manager_.UnRegisterMonitoredTask(shared_from(this));
+}
+
+Status XClusterTableSetupTask::ValidateRunnable() { return parent_task_->ValidateRunnable(); }
+
+void XClusterTableSetupTask::TaskCompleted(const Status& status) {
+  VLOG_WITH_PREFIX_AND_FUNC(1) << status;
+
+  if (!status.ok()) {
+    parent_task_->TableTaskCompletionCallback(
+        source_table_id_,
+        status.CloneAndPrepend(Format("Error processing source table $0", source_table_id_)));
+    return;
+  }
+
+  parent_task_->TableTaskCompletionCallback(source_table_id_, std::move(table_setup_info_));
+}
+
+Status XClusterTableSetupTask::FirstStep() {
+  if (IsTablegroupParentTableId(source_table_id_)) {
+    auto source_tablegroup_id = GetTablegroupIdFromParentTableId(source_table_id_);
+    auto tables_info = std::make_shared<std::vector<client::YBTableInfo>>();
+    return parent_task_->GetYbClient().GetTablegroupSchemaById(
+        source_tablegroup_id, tables_info,
+        Bind(&XClusterTableSetupTask::GetTablegroupSchemaCallback, shared_from(this), tables_info));
+  }
+
+  auto table_info = std::make_shared<client::YBTableInfo>();
+  return parent_task_->GetYbClient().GetTableSchemaById(
+      source_table_id_, table_info,
+      Bind(&XClusterTableSetupTask::GetTableSchemaCallback, shared_from(this), table_info));
+}
+
+void XClusterTableSetupTask::GetTableSchemaCallback(
+    std::shared_ptr<XClusterTableSetupTask> shared_this,
+    const std::shared_ptr<client::YBTableInfo>& source_table_info, const Status& s) {
+  shared_this->ScheduleNextStep(
+      std::bind(&XClusterTableSetupTask::ProcessTable, shared_this, source_table_info, s),
+      "Processing table");
+}
+
+Status XClusterTableSetupTask::ProcessTable(
+    const std::shared_ptr<client::YBTableInfo>& source_info, const Status& s) {
+  VLOG_WITH_PREFIX_AND_FUNC(1) << s;
+  RETURN_NOT_OK_PREPEND(s, "Error from source universe");
+  SCHECK(source_info != nullptr, InvalidArgument, "Received null table info from source universe");
+
+  SCHECK_NE(
+      source_info->table_name.namespace_name(), master::kSystemNamespaceName, NotSupported,
+      "Cannot replicate system tables");
+
+  auto target_schema = VERIFY_RESULT(ValidateSourceSchemaAndGetTargetSchema(*source_info));
+
+  const auto& target_table_id = target_schema.identifier().table_id();
+  RSTATUS_DCHECK_NE(
+      source_info->schema.version(), cdc::kInvalidSchemaVersion, IllegalState,
+      Format("Invalid source schema version for target table $0", target_table_id));
+
+  auto* schema_versions = table_setup_info_.stream_entry.mutable_schema_versions();
+  schema_versions->set_current_producer_schema_version(source_info->schema.version());
+  schema_versions->set_current_consumer_schema_version(target_schema.version());
+
+  RETURN_NOT_OK(
+      PopulateTableStreamEntry(target_schema.identifier().table_id(), target_schema.version()));
+
+  return ValidateBootstrapAndSetupStreams();
+}
+
+void XClusterTableSetupTask::GetTablegroupSchemaCallback(
+    std::shared_ptr<XClusterTableSetupTask> shared_this,
+    const std::shared_ptr<std::vector<client::YBTableInfo>>& source_table_infos, const Status& s) {
+  shared_this->ScheduleNextStep(
+      std::bind(&XClusterTableSetupTask::ProcessTablegroup, shared_this, source_table_infos, s),
+      "Processing tablegroup");
+}
+
+Status XClusterTableSetupTask::ProcessTablegroup(
+    const std::shared_ptr<std::vector<client::YBTableInfo>>& source_table_infos, const Status& s) {
+  VLOG_WITH_PREFIX_AND_FUNC(1) << s;
+
+  RETURN_NOT_OK_PREPEND(s, "Error from source universe");
+
+  SCHECK(
+      source_table_infos != nullptr, InvalidArgument,
+      "Received null table info from source universe");
+  SCHECK(
+      !source_table_infos->empty(), IllegalState,
+      "Received empty list of tables to validate from source universe");
+
+  const auto source_tablegroup_id = GetTablegroupIdFromParentTableId(source_table_id_);
+  std::unordered_set<TableId> validated_target_tables;
+  for (const auto& source_table_info : *source_table_infos) {
+    SCHECK(
+        source_table_info.colocated, InvalidArgument,
+        "Received non-colocated table $0 from source universe", source_table_info.table_id);
+
+    auto target_schema = VERIFY_RESULT(ValidateSourceSchemaAndGetTargetSchema(source_table_info));
+
+    const auto& colocation_id = target_schema.schema().colocated_table_id().colocation_id();
+    auto& schema_versions =
+        (*table_setup_info_.stream_entry.mutable_colocated_schema_versions())[colocation_id];
+    schema_versions.set_current_producer_schema_version(source_table_info.schema.version());
+    schema_versions.set_current_consumer_schema_version(target_schema.version());
+
+    validated_target_tables.insert(target_schema.identifier().table_id());
+  }
+
+  // Make sure the list of tables in our tablegroup matches the source.
+  auto target_tablegroup_id = VERIFY_RESULT(
+      parent_task_->catalog_manager_.GetTablegroupId(*validated_target_tables.begin()));
+
+  std::unordered_set<TableId> tables_in_consumer_tablegroup;
+  {
+    GetTablegroupSchemaRequestPB req;
+    GetTablegroupSchemaResponsePB resp;
+    req.mutable_tablegroup()->set_id(target_tablegroup_id);
+    auto status = parent_task_->catalog_manager_.GetTablegroupSchema(&req, &resp);
+    if (status.ok() && resp.has_error()) {
+      status = StatusFromPB(resp.error().status());
+    }
+    RETURN_NOT_OK_PREPEND(
+        status, Format("Error when getting target tablegroup schema: $0", target_tablegroup_id));
+
+    for (const auto& info : resp.get_table_schema_response_pbs()) {
+      tables_in_consumer_tablegroup.insert(info.identifier().table_id());
+    }
+  }
+
+  SCHECK_EQ(
+      validated_target_tables, tables_in_consumer_tablegroup, IllegalState,
+      Format(
+          "Mismatch between tables associated with source tablegroup $0 and "
+          "tables in target tablegroup $1: ($2) vs ($3).",
+          source_tablegroup_id, target_tablegroup_id, AsString(validated_target_tables),
+          AsString(tables_in_consumer_tablegroup)));
+
+  const auto target_parent_table_id = IsColocatedDbTablegroupParentTableId(source_table_id_)
+                                          ? GetColocationParentTableId(target_tablegroup_id)
+                                          : GetTablegroupParentTableId(target_tablegroup_id);
+
+  auto target_schema_version =
+      VERIFY_RESULT(parent_task_->catalog_manager_.GetTableSchemaVersion(target_parent_table_id));
+
+  RETURN_NOT_OK(PopulateTableStreamEntry(target_parent_table_id, target_schema_version));
+
+  return ValidateBootstrapAndSetupStreams();
+}
+
+Result<GetTableSchemaResponsePB> XClusterTableSetupTask::ValidateSourceSchemaAndGetTargetSchema(
+    const client::YBTableInfo& source_table_info) {
+  bool is_ysql_table = source_table_info.table_type == client::YBTableType::PGSQL_TABLE_TYPE;
+  if (parent_task_->transactional_ &&
+      !GetAtomicFlag(&FLAGS_TEST_allow_ycql_transactional_xcluster) && !is_ysql_table) {
+    return STATUS_FORMAT(
+        NotSupported, "Transactional replication is not supported for non-YSQL tables: $0",
+        source_table_info.table_name.ToString());
+  }
+
+  // Get corresponding table schema on local universe.
+  GetTableSchemaRequestPB table_schema_req;
+  GetTableSchemaResponsePB table_schema_resp;
+
+  auto* table = table_schema_req.mutable_table();
+  table->set_table_name(source_table_info.table_name.table_name());
+  table->mutable_namespace_()->set_name(source_table_info.table_name.namespace_name());
+  table->mutable_namespace_()->set_database_type(
+      GetDatabaseTypeForTable(client::ClientToPBTableType(source_table_info.table_type)));
+
+  // Since YSQL tables are not present in table map, we first need to list tables to get the table
+  // ID and then get table schema.
+  // Remove this once table maps are fixed for YSQL.
+  ListTablesRequestPB list_req;
+  ListTablesResponsePB list_resp;
+
+  list_req.set_name_filter(source_table_info.table_name.table_name());
+  Status status = parent_task_->catalog_manager_.ListTables(&list_req, &list_resp);
+  SCHECK(
+      status.ok() && !list_resp.has_error(), NotFound,
+      Format("Error while listing table: $0", status.ToString()));
+
+  const auto& source_schema = client::internal::GetSchema(source_table_info.schema);
+  for (const auto& t : list_resp.tables()) {
+    // Check that table name and namespace both match.
+    if (t.name() != source_table_info.table_name.table_name() ||
+        t.namespace_().name() != source_table_info.table_name.namespace_name()) {
+      continue;
+    }
+
+    // Check that schema name matches for YSQL tables, if the field is empty, fill in that
+    // information during GetTableSchema call later.
+    bool has_valid_pgschema_name = !t.pgschema_name().empty();
+    if (is_ysql_table && has_valid_pgschema_name &&
+        t.pgschema_name() != source_schema.SchemaName()) {
+      continue;
+    }
+
+    // Get the table schema.
+    table->set_table_id(t.id());
+    status = parent_task_->catalog_manager_.GetTableSchema(&table_schema_req, &table_schema_resp);
+    SCHECK(
+        status.ok() && !table_schema_resp.has_error(), NotFound,
+        Format("Error while getting table schema: $0", status.ToString()));
+
+    // Double-check schema name here if the previous check was skipped.
+    if (is_ysql_table && !has_valid_pgschema_name) {
+      std::string target_schema_name = table_schema_resp.schema().pgschema_name();
+      if (target_schema_name != source_schema.SchemaName()) {
+        table->clear_table_id();
+        continue;
+      }
+    }
+
+    // Verify that the table on the target side supports replication.
+    if (is_ysql_table && t.has_relation_type() && t.relation_type() == MATVIEW_TABLE_RELATION) {
+      return STATUS_FORMAT(
+          NotSupported, "Replication is not supported for materialized view: $0",
+          source_table_info.table_name.ToString());
+    }
+
+    Schema target_schema;
+    RETURN_NOT_OK(SchemaFromPB(table_schema_resp.schema(), &target_schema));
+
+    // We now have a table match. Validate the schema.
+    SCHECK(
+        target_schema.EquivalentForDataCopy(source_schema), IllegalState,
+        Format(
+            "Source and target schemas don't match: "
+            "Source: $0, Target: $1, Source schema: $2, Target schema: $3",
+            source_table_info.table_id, table_schema_resp.identifier().table_id(),
+            source_table_info.schema.ToString(), table_schema_resp.schema().DebugString()));
+    break;
+  }
+
+  SCHECK(
+      table->has_table_id(), NotFound,
+      Format(
+          "Could not find matching table for $0$1", source_table_info.table_name.ToString(),
+          (is_ysql_table ? " pgschema_name: " + source_schema.SchemaName() : "")));
+
+  if (source_table_info.colocated) {
+    // We require that colocated tables have the same colocation ID.
+    //
+    // Backward compatibility: tables created prior to #7378 use YSQL table OID as a colocation
+    // ID.
+    SCHECK_FORMAT(
+        source_table_info.schema.has_colocation_id(), NotFound,
+        "Missing colocation ID for source table $0", source_table_info.table_id);
+    SCHECK_FORMAT(
+        table_schema_resp.schema().has_colocated_table_id() &&
+            table_schema_resp.schema().colocated_table_id().has_colocation_id(),
+        NotFound, "Missing colocation ID for target table $0",
+        table_schema_resp.identifier().table_id());
+
+    auto source_clc_id = source_table_info.schema.colocation_id();
+    auto target_clc_id = table_schema_resp.schema().colocated_table_id().colocation_id();
+    SCHECK_EQ(
+        source_clc_id, target_clc_id, IllegalState,
+        Format(
+            "Source and target colocation IDs don't match for colocated table: "
+            "Source: $0, Target: $1, Source colocation ID: $2, Target colocation ID: $3",
+            source_table_info.table_id, table_schema_resp.identifier().table_id(), source_clc_id,
+            target_clc_id));
+  }
+
+  return table_schema_resp;
+}
+
+Status XClusterTableSetupTask::PopulateTableStreamEntry(
+    const TableId& target_table_id, const SchemaVersion& target_schema_version) {
+  VLOG_WITH_PREFIX_AND_FUNC(1) << YB_STRUCT_TO_STRING(target_table_id, target_schema_version);
+
+  SCHECK(
+      !parent_task_->xcluster_manager_.IsTableReplicationConsumer(target_table_id), IllegalState,
+      "N:1 replication topology not supported");
+
+  table_setup_info_.target_table_id = target_table_id;
+
+  auto& stream_entry = table_setup_info_.stream_entry;
+  stream_entry.set_consumer_table_id(target_table_id);
+  stream_entry.set_producer_table_id(source_table_id_);
+
+  RSTATUS_DCHECK_NE(
+      target_schema_version, cdc::kInvalidSchemaVersion, IllegalState,
+      Format(
+          "Invalid target schema version for source table $0, target table $1", source_table_id_,
+          target_table_id));
+
+  stream_entry.mutable_producer_schema()->set_last_compatible_consumer_schema_version(
+      target_schema_version);
+
+  // Mark this stream as special if it is for the ddl_queue table.
+  auto yb_table_info = parent_task_->catalog_manager_.GetTableInfo(target_table_id);
+  stream_entry.set_is_ddl_queue_table(
+      yb_table_info->GetTableType() == PGSQL_TABLE_TYPE &&
+      yb_table_info->name() == xcluster::kDDLQueueTableName &&
+      yb_table_info->pgschema_name() == xcluster::kDDLQueuePgSchemaName);
+
+  return Status::OK();
+}
+
+Status XClusterTableSetupTask::ValidateBootstrapAndSetupStreams() {
+  if (FLAGS_check_bootstrap_required) {
+    RETURN_NOT_OK_PREPEND(
+        ValidateBootstrapNotRequired(), "Error checking if bootstrap is required");
+  }
+
+  SetupStreams();
+
+  return Status::OK();
+}
+
+Status XClusterTableSetupTask::ValidateBootstrapNotRequired() {
+  boost::optional<xrepl::StreamId> bootstrap_id;
+
+  if (table_setup_info_.stream_id) {
+    bootstrap_id = table_setup_info_.stream_id;
+  }
+
+  // TODO: When FLAGS_check_bootstrap_required is enabled by default we need to convert this to a
+  // async rpc call.
+  if (VERIFY_RESULT(
+          parent_task_->GetYbClient().IsBootstrapRequired({source_table_id_}, bootstrap_id))) {
+    return STATUS(
+        IllegalState, LogPrefix(),
+        Format(
+            "Bootstrap is required for Table $0$1", source_table_id_,
+            (bootstrap_id ? Format(", stream $0", bootstrap_id->ToString()) : "")));
+  }
+  return Status::OK();
+}
+
+void XClusterTableSetupTask::SetupStreams() {
+  if (!table_setup_info_.stream_id) {
+    // Streams are used as soon as they are created so set state to active.
+    parent_task_->GetXClusterClient().CreateXClusterStreamAsync(
+        source_table_id_, /*active=*/true,
+        cdc::StreamModeTransactional(parent_task_->transactional_),
+        std::bind(&XClusterTableSetupTask::CreateXClusterStreamCallback, shared_from(this), _1));
+    return;
+  }
+
+  auto received_table_id = std::make_shared<TableId>();
+  auto stream_options = std::make_shared<std::unordered_map<std::string, std::string>>();
+  parent_task_->GetYbClient().GetCDCStream(
+      table_setup_info_.stream_id, received_table_id, stream_options,
+      std::bind(
+          &XClusterTableSetupTask::GetStreamCallback, shared_from(this), received_table_id,
+          stream_options, _1));
+}
+
+void XClusterTableSetupTask::GetStreamCallback(
+    std::shared_ptr<TableId> received_table_id,
+    std::shared_ptr<std::unordered_map<std::string, std::string>> options, const Status& s) {
+  VLOG_WITH_PREFIX_AND_FUNC(1) << YB_STRUCT_TO_STRING(received_table_id, options, s);
+  ScheduleNextStep(
+      std::bind(
+          &XClusterTableSetupTask::ProcessStreamOptions, shared_from(this), received_table_id,
+          options, s),
+      "Processing stream options");
+}
+
+Status XClusterTableSetupTask::ProcessStreamOptions(
+    std::shared_ptr<TableId> received_table_id,
+    std::shared_ptr<std::unordered_map<std::string, std::string>> options, const Status& s) {
+  RETURN_NOT_OK_PREPEND(s, "Error from source universe");
+  SCHECK(
+      received_table_id != nullptr, InvalidArgument, "Received null table id from source universe");
+
+  SCHECK_EQ(
+      *received_table_id, source_table_id_, InvalidArgument,
+      Format(
+          "Invalid xCluster Stream id for table $0. Stream id $1 belongs to table $2",
+          source_table_id_, table_setup_info_.stream_id, *received_table_id));
+
+  // Store the stream options for later use. XClusterInboundReplicationGroupSetupTask::
+  // UpdateSourceStreamOptions will process options of tables in one batch.
+  table_setup_info_.stream_options.swap(*options);
+
+  PopulateTabletMapping();
+
+  return Status::OK();
+}
+
+void XClusterTableSetupTask::CreateXClusterStreamCallback(
+    const Result<xrepl::StreamId>& stream_id) {
+  VLOG_WITH_PREFIX_AND_FUNC(1) << stream_id.ToString();
+  ScheduleNextStep(
+      std::bind(&XClusterTableSetupTask::ProcessNewStream, shared_from(this), std::move(stream_id)),
+      "Processing new stream");
+}
+
+Status XClusterTableSetupTask::ProcessNewStream(const Result<xrepl::StreamId>& stream_id) {
+  VLOG_WITH_PREFIX_AND_FUNC(1) << stream_id;
+
+  RETURN_NOT_OK_PREPEND(stream_id, "Error creating xCluster Stream on source universe");
+  SCHECK(*stream_id, InvalidArgument, "Received invalid xCluster Stream id from source universe");
+
+  table_setup_info_.stream_id = *stream_id;
+
+  PopulateTabletMapping();
+
+  return Status::OK();
+}
+
+void XClusterTableSetupTask::PopulateTabletMapping() {
+  VLOG_WITH_PREFIX_AND_FUNC(1);
+
+  parent_task_->GetYbClient().GetTableLocations(
+      source_table_id_, /* max_tablets = */ std::numeric_limits<int32_t>::max(),
+      RequireTabletsRunning::kTrue, PartitionsOnly::kTrue,
+      std::bind(&XClusterTableSetupTask::PopulateTabletMappingCallback, shared_from(this), _1),
+      IncludeInactive::kFalse);
+}
+
+void XClusterTableSetupTask::PopulateTabletMappingCallback(
+    const Result<master::GetTableLocationsResponsePB*>& result) {
+  VLOG_WITH_PREFIX_AND_FUNC(1) << result.ToString();
+
+  auto schedule_func = [this](Result<master::GetTableLocationsResponsePB>&& result) {
+    ScheduleNextStep(
+        std::bind(
+            &XClusterTableSetupTask::ProcessTabletMapping, shared_from(this), std::move(result)),
+        "Processing new stream");
+  };
+
+  if (!result.ok()) {
+    schedule_func(result.status());
+    return;
+  }
+
+  master::GetTableLocationsResponsePB resp;
+  resp.Swap(*result);
+  schedule_func(std::move(resp));
+}
+
+Status XClusterTableSetupTask::ProcessTabletMapping(
+    const Result<master::GetTableLocationsResponsePB>& result) {
+  RETURN_NOT_OK_PREPEND(result, "Error from source universe");
+  const auto& resp = *result;
+  if (resp.has_error()) {
+    return StatusFromPB(resp.error().status()).CloneAndPrepend("Error from source universe");
+  }
+
+  auto& tablets = resp.tablet_locations();
+
+  auto target_tablet_keys = VERIFY_RESULT(
+      parent_task_->catalog_manager_.GetTableKeyRanges(table_setup_info_.target_table_id));
+
+  RETURN_NOT_OK(PopulateXClusterStreamEntryTabletMapping(
+      source_table_id_, table_setup_info_.target_table_id, target_tablet_keys,
+      &table_setup_info_.stream_entry, tablets));
+
+  Complete();
+
+  return Status::OK();
+}
+
+Status ValidateMasterAddressesBelongToDifferentCluster(
+    Master& master, const google::protobuf::RepeatedPtrField<HostPortPB>& master_addresses) {
+  std::vector<ServerEntryPB> cluster_master_addresses;
+  RETURN_NOT_OK(master.ListMasters(&cluster_master_addresses));
+  std::unordered_set<HostPort, HostPortHash> cluster_master_hps;
+
+  for (const auto& cluster_elem : cluster_master_addresses) {
+    if (cluster_elem.has_registration()) {
+      auto p_rpc_addresses = cluster_elem.registration().private_rpc_addresses();
+      for (const auto& p_rpc_elem : p_rpc_addresses) {
+        cluster_master_hps.insert(HostPort::FromPB(p_rpc_elem));
+      }
+
+      auto broadcast_addresses = cluster_elem.registration().broadcast_addresses();
+      for (const auto& bc_elem : broadcast_addresses) {
+        cluster_master_hps.insert(HostPort::FromPB(bc_elem));
+      }
+    }
+
+    for (const auto& master_address : master_addresses) {
+      auto master_hp = HostPort::FromPB(master_address);
+      SCHECK(
+          !cluster_master_hps.contains(master_hp), InvalidArgument,
+          "Master address $0 belongs to the target universe", master_hp);
+    }
+  }
+
+  return Status::OK();
+}
+
+}  // namespace yb::master
diff --git a/src/yb/master/xcluster/xcluster_inbound_replication_group_setup_task.h b/src/yb/master/xcluster/xcluster_inbound_replication_group_setup_task.h
new file mode 100644
index 0000000000..610543190e
--- /dev/null
+++ b/src/yb/master/xcluster/xcluster_inbound_replication_group_setup_task.h
@@ -0,0 +1,271 @@
+// Copyright (c) YugabyteDB, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
+// in compliance with the License.  You may obtain a copy of the License at
+//
+// http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software distributed under the License
+// is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
+// or implied.  See the License for the specific language governing permissions and limitations
+// under the License.
+//
+
+#pragma once
+
+#include <shared_mutex>
+
+#include "yb/cdc/xcluster_types.h"
+
+#include "yb/common/common_fwd.h"
+#include "yb/master/master_fwd.h"
+#include "yb/master/multi_step_monitored_task.h"
+#include "yb/master/xcluster/xcluster_universe_replication_setup_helper.h"
+
+namespace yb {
+
+namespace client {
+class XClusterClient;
+class XClusterRemoteClientHolder;
+class YBClient;
+struct YBTableInfo;
+}  // namespace client
+
+namespace master {
+
+class GetTableLocationsResponsePB;
+class GetTableSchemaResponsePB;
+class UniverseReplicationInfo;
+
+// Table specific setup information.
+struct XClusterTableSetupInfo {
+  TableId target_table_id;
+  cdc::StreamEntryPB stream_entry;
+
+  xrepl::StreamId stream_id = xrepl::StreamId::Nil();
+  std::unordered_map<std::string, std::string> stream_options = {};
+
+  std::string ToString() const { return YB_STRUCT_TO_STRING(target_table_id, stream_id); }
+};
+
+// Helper container to track colocationId and the source to target schema version mapping.
+typedef std::vector<std::tuple<ColocationId, SchemaVersion, SchemaVersion>>
+    ColocationSchemaVersions;
+
+class XClusterInboundReplicationGroupSetupTask;
+
+// The task that will setup a single table for xCluster. All the info is collected into the
+// XClusterTableSetupInfo object and returned via the callback.
+// Task will stop running if parent_task.ValidateRunnable returns false.
+//
+// The table setup involves the following stages:
+// 1. TableSchema: Get the table schema from the source universe and validate it is compatible
+//    with the target universe. (GetTableSchemaCallback, GetColocatedTabletSchemaCallback).
+// 2. ValidateAndSetupStreams: Check if the source table needs to be bootstrapped. Create new
+//    xCluster streams, or validate passed in stream id. (GetStreamCallback,
+//    CreateXClusterStreamCallback).
+// 4. TableLocations: Get the tablet info for the source table and map it to the target tablets
+//    (PopulateTabletMapping).
+class XClusterTableSetupTask : public MultiStepMonitoredTask {
+ public:
+  XClusterTableSetupTask(
+      std::shared_ptr<XClusterInboundReplicationGroupSetupTask> parent_task,
+      const TableId& source_table_id, const xrepl::StreamId& stream_id);
+
+  ~XClusterTableSetupTask() = default;
+
+  server::MonitoredTaskType type() const override {
+    return server::MonitoredTaskType::kXClusterTableSetup;
+  }
+
+  std::string type_name() const override {
+    return "Setup table for xCluster inbound replication group";
+  }
+
+  std::string description() const override { return log_prefix_; }
+
+  TableId SourceTableId() const { return source_table_id_; }
+
+ private:
+  std::string LogPrefix() const { return log_prefix_; }
+
+  Status FirstStep() override;
+
+  Status RegisterTask() override;
+  // Release the parent task reference.
+  void UnregisterTask() override;
+
+  Status ValidateRunnable() override;
+
+  void TaskCompleted(const Status& status) override;
+
+  static void GetTableSchemaCallback(
+      std::shared_ptr<XClusterTableSetupTask> shared_this,
+      const std::shared_ptr<client::YBTableInfo>& source_table_info, const Status& s);
+  Status ProcessTable(
+      const std::shared_ptr<client::YBTableInfo>& source_table_info, const Status& s);
+
+  static void GetTablegroupSchemaCallback(
+      std::shared_ptr<XClusterTableSetupTask> shared_this,
+      const std::shared_ptr<std::vector<client::YBTableInfo>>& source_table_infos, const Status& s);
+  Status ProcessTablegroup(
+      const std::shared_ptr<std::vector<client::YBTableInfo>>& source_table_infos, const Status& s);
+
+  Result<GetTableSchemaResponsePB> ValidateSourceSchemaAndGetTargetSchema(
+      const client::YBTableInfo& source_table_info);
+
+  Status ValidateBootstrapAndSetupStreams();
+
+  Status ValidateBootstrapNotRequired();
+
+  void SetupStreams();
+
+  Status PopulateTableStreamEntry(
+      const TableId& target_table_id, const SchemaVersion& target_schema_version);
+
+  void GetStreamCallback(
+      std::shared_ptr<TableId> received_table_id,
+      std::shared_ptr<std::unordered_map<std::string, std::string>> options, const Status& s);
+  Status ProcessStreamOptions(
+      std::shared_ptr<TableId> received_table_id,
+      std::shared_ptr<std::unordered_map<std::string, std::string>> options, const Status& s);
+
+  void CreateXClusterStreamCallback(const Result<xrepl::StreamId>& stream_id);
+  Status ProcessNewStream(const Result<xrepl::StreamId>& stream_id);
+
+  void PopulateTabletMapping();
+  void PopulateTabletMappingCallback(const Result<master::GetTableLocationsResponsePB*>& result);
+  Status ProcessTabletMapping(const Result<master::GetTableLocationsResponsePB>& result);
+
+  std::shared_ptr<XClusterInboundReplicationGroupSetupTask> parent_task_;
+  const TableId& source_table_id_;
+  XClusterTableSetupInfo table_setup_info_;
+  std::string log_prefix_;
+
+  DISALLOW_COPY_AND_ASSIGN(XClusterTableSetupTask);
+};
+
+// The replication group level task that performs the setup.
+// Uses XClusterTableSetupTask to run the per table steps, then performs batched operations like
+// UpdateCDCStream and finally writes/updates the UniverseReplicationInfo and ClusterConfig.
+// ValidateInputArguments must be called before StartSetup.
+// ALTER: If the replication group id has suffix '.ALTER' then performs an alter operation. Alter
+// operations add new tables, and namespaces to an existing replication group.
+//
+// Object lifetime: The caller (XClusterTargetManager) holds a reference to this task until it gets
+// a completion result. In the case of failures the caller is free to release the reference to us,
+// but we might still have pending work, and child tasks. We keep ourself alive while there is work
+// by registering ourself with XClusterManager. We start the child tasks that hold a shared ref to
+// us keeping us alive while they are still running. Child tasks themself also register to
+// XClusterManager to keep themself alive.
+class XClusterInboundReplicationGroupSetupTask : public XClusterInboundReplicationGroupSetupTaskIf,
+                                                 public MultiStepMonitoredTask {
+ public:
+  XClusterInboundReplicationGroupSetupTask(
+      Master& master, CatalogManager& catalog_manager, const LeaderEpoch& epoch,
+      xcluster::ReplicationGroupId&& replication_group_id,
+      const google::protobuf::RepeatedPtrField<HostPortPB>& source_masters,
+      std::vector<TableId>&& source_table_ids, std::vector<xrepl::StreamId>&& stream_ids,
+      bool transactional, std::vector<NamespaceId>&& source_namespace_ids,
+      std::vector<NamespaceId>&& target_namespace_ids);
+
+  ~XClusterInboundReplicationGroupSetupTask() = default;
+
+  server::MonitoredTaskType type() const override {
+    return server::MonitoredTaskType::kXClusterInboundReplicationGroupSetup;
+  }
+
+  std::string type_name() const override { return "Setup xCluster inbound replication group"; }
+
+  std::string description() const override { return log_prefix_; }
+
+  xcluster::ReplicationGroupId Id() const override { return replication_group_id_; }
+
+  Status ValidateInputArguments();
+
+  void StartSetup() override { Start(); }
+
+  IsOperationDoneResult DoneResult() const override EXCLUDES(done_result_mutex_);
+
+  // Returns true if the setup was cancelled or it already failed, and false if the setup
+  // already completed successfully.
+  bool TryCancel() EXCLUDES(done_result_mutex_) override;
+
+ private:
+  friend class XClusterTableSetupTask;
+
+  std::string LogPrefix() const { return log_prefix_; }
+
+  client::YBClient& GetYbClient();
+  client::XClusterClient& GetXClusterClient();
+
+  Status ValidateRunnable() override EXCLUDES(done_result_mutex_);
+
+  Status RegisterTask() override;
+  void UnregisterTask() override;
+
+  Status FirstStep() override EXCLUDES(mutex_);
+
+  void TaskCompleted(const Status& status) override EXCLUDES(done_result_mutex_);
+
+  // Check if the local AutoFlags config is compatible with the source universe and returns the
+  // source universe AutoFlags config version if they are compatible. If they are not
+  // compatible, returns a bad status. If the source universe is running an older version which
+  // does not support AutoFlags compatiblity check, returns an invalid AutoFlags config version.
+  Result<uint32> GetAutoFlagConfigVersionIfCompatible();
+
+  void TableTaskCompletionCallback(
+      const TableId& source_table_id, const Result<XClusterTableSetupInfo>& table_setup_result)
+      EXCLUDES(mutex_);
+
+  Status SetupReplicationAfterProcessingAllTables() EXCLUDES(mutex_);
+
+  // Update the source to let it know that the bootstrapping is complete. We set the stream
+  // state to ACTIVE and set the transactional flag.
+  Status UpdateSourceStreamOptions() EXCLUDES(mutex_);
+
+  Status ValidateNamespaceListForDbScoped();
+  Status ValidateTableListForDbScoped() REQUIRES(mutex_);
+
+  Status SetupReplicationGroup() EXCLUDES(mutex_);
+
+  Result<scoped_refptr<UniverseReplicationInfo>> CreateNewUniverseReplicationInfo()
+      REQUIRES(mutex_);
+  void PopulateUniverseReplication(SysUniverseReplicationEntryPB& universe_pb) REQUIRES(mutex_);
+
+  Master& master_;
+  CatalogManager& catalog_manager_;
+  SysCatalogTable& sys_catalog_;
+  XClusterManager& xcluster_manager_;
+  const LeaderEpoch epoch_;
+
+  const xcluster::ReplicationGroupId replication_group_id_;
+  const google::protobuf::RepeatedPtrField<HostPortPB> source_masters_;
+  const bool transactional_;
+  // The following lists are preserved in the same order they were provided.
+  const std::vector<TableId> source_table_ids_;
+  const std::vector<xrepl::StreamId> stream_ids_;
+  const std::vector<NamespaceId> source_namespace_ids_;
+  const std::vector<NamespaceId> target_namespace_ids_;
+  const bool is_alter_replication_;
+  const bool is_db_scoped_;
+  const bool stream_ids_provided_;
+  std::string log_prefix_;
+
+  std::shared_ptr<client::XClusterRemoteClientHolder> remote_client_;
+
+  IF_DEBUG_MODE(bool argument_validation_done_ = false);
+
+  mutable std::shared_mutex done_result_mutex_;
+  IsOperationDoneResult done_result_ GUARDED_BY(done_result_mutex_) =
+      IsOperationDoneResult::NotDone();
+
+  // Map of source table id to XClusterTableSetupInfo.
+  mutable std::shared_mutex mutex_;
+  std::unordered_map<TableId, XClusterTableSetupInfo> source_table_infos_ GUARDED_BY(mutex_);
+
+  DISALLOW_COPY_AND_ASSIGN(XClusterInboundReplicationGroupSetupTask);
+};
+
+}  // namespace master
+}  // namespace yb
diff --git a/src/yb/master/xcluster/xcluster_manager.cc b/src/yb/master/xcluster/xcluster_manager.cc
index 8cc30f97f4..9faa8154cc 100644
--- a/src/yb/master/xcluster/xcluster_manager.cc
+++ b/src/yb/master/xcluster/xcluster_manager.cc
@@ -21,6 +21,7 @@
 #include "yb/master/catalog_manager.h"
 #include "yb/master/master_cluster.pb.h"
 #include "yb/master/xcluster/xcluster_status.h"
+#include "yb/util/backoff_waiter.h"
 #include "yb/util/is_operation_done_result.h"
 #include "yb/master/xcluster/xcluster_config.h"
 
@@ -74,7 +75,33 @@ XClusterManager::XClusterManager(
 
 XClusterManager::~XClusterManager() {}
 
-void XClusterManager::Shutdown() { XClusterTargetManager::Shutdown(); }
+void XClusterManager::StartShutdown() {
+  XClusterTargetManager::StartShutdown();
+
+  // Copy the tasks out since Abort will trigger UnRegister which needs the mutex.
+  decltype(monitored_tasks_) tasks;
+  {
+    std::lock_guard l(monitored_tasks_mutex_);
+    tasks = monitored_tasks_;
+  }
+  for (auto& task : tasks) {
+    task->AbortAndReturnPrevState(STATUS(Aborted, "Master is shutting down"));
+  }
+}
+
+void XClusterManager::CompleteShutdown() {
+  XClusterTargetManager::CompleteShutdown();
+
+  CHECK_OK(WaitFor(
+      [this]() {
+        std::lock_guard l(monitored_tasks_mutex_);
+        YB_LOG_EVERY_N_SECS(WARNING, 10)
+            << "Waiting for " << monitored_tasks_.size() << " monitored tasks to complete";
+        return monitored_tasks_.empty();
+      },
+      MonoDelta::kMax, "Waiting for monitored tasks to complete",
+      MonoDelta::FromMilliseconds(100)));
+}
 
 Status XClusterManager::Init() {
   RETURN_NOT_OK(XClusterSourceManager::Init());
@@ -715,14 +742,20 @@ Status XClusterManager::IsSetupUniverseReplicationDone(
 
   SCHECK_PB_FIELDS_NOT_EMPTY(*req, replication_group_id);
 
-  auto is_operation_done = VERIFY_RESULT(XClusterTargetManager::IsSetupUniverseReplicationDone(
-      xcluster::ReplicationGroupId(req->replication_group_id())));
+  auto is_operation_done = VERIFY_RESULT(IsSetupUniverseReplicationDone(
+      xcluster::ReplicationGroupId(req->replication_group_id()), /*skip_health_check=*/false));
 
   resp->set_done(is_operation_done.done());
   StatusToPB(is_operation_done.status(), resp->mutable_replication_error());
   return Status::OK();
 }
 
+Result<IsOperationDoneResult> XClusterManager::IsSetupUniverseReplicationDone(
+    const xcluster::ReplicationGroupId& replication_group_id, bool skip_health_check) {
+  return XClusterTargetManager::IsSetupUniverseReplicationDone(
+      replication_group_id, skip_health_check);
+}
+
 Status XClusterManager::SetupNamespaceReplicationWithBootstrap(
     const SetupNamespaceReplicationWithBootstrapRequestPB* req,
     SetupNamespaceReplicationWithBootstrapResponsePB* resp, rpc::RpcContext* rpc,
@@ -776,4 +809,17 @@ Status XClusterManager::DeleteUniverseReplication(
   return Status::OK();
 }
 
+Status XClusterManager::RegisterMonitoredTask(server::MonitoredTaskPtr task) {
+  std::lock_guard l(monitored_tasks_mutex_);
+  SCHECK_FORMAT(
+      monitored_tasks_.insert(task).second, AlreadyPresent, "Task $0 already registered",
+      task->description());
+  return Status::OK();
+}
+
+void XClusterManager::UnRegisterMonitoredTask(server::MonitoredTaskPtr task) {
+  std::lock_guard l(monitored_tasks_mutex_);
+  monitored_tasks_.erase(task);
+}
+
 }  // namespace yb::master
diff --git a/src/yb/master/xcluster/xcluster_manager.h b/src/yb/master/xcluster/xcluster_manager.h
index 60c51867b3..62e4be8364 100644
--- a/src/yb/master/xcluster/xcluster_manager.h
+++ b/src/yb/master/xcluster/xcluster_manager.h
@@ -50,7 +50,8 @@ class XClusterManager : public XClusterManagerIf,
 
   Status Init();
 
-  void Shutdown();
+  void StartShutdown() EXCLUDES(monitored_tasks_mutex_);
+  void CompleteShutdown() EXCLUDES(monitored_tasks_mutex_);
 
   void Clear();
 
@@ -109,6 +110,9 @@ class XClusterManager : public XClusterManagerIf,
       const IsSetupUniverseReplicationDoneRequestPB* req,
       IsSetupUniverseReplicationDoneResponsePB* resp, rpc::RpcContext* rpc);
 
+  Result<IsOperationDoneResult> IsSetupUniverseReplicationDone(
+      const xcluster::ReplicationGroupId& replication_group_id, bool skip_health_check) override;
+
   Status SetupNamespaceReplicationWithBootstrap(
       const SetupNamespaceReplicationWithBootstrapRequestPB* req,
       SetupNamespaceReplicationWithBootstrapResponsePB* resp, rpc::RpcContext* rpc,
@@ -248,6 +252,9 @@ class XClusterManager : public XClusterManagerIf,
   Status HandleTabletSchemaVersionReport(
       const TableInfo& table_info, SchemaVersion consumer_schema_version, const LeaderEpoch& epoch);
 
+  Status RegisterMonitoredTask(server::MonitoredTaskPtr task) EXCLUDES(monitored_tasks_mutex_);
+  void UnRegisterMonitoredTask(server::MonitoredTaskPtr task) EXCLUDES(monitored_tasks_mutex_);
+
  private:
   CatalogManager& catalog_manager_;
   SysCatalogTable& sys_catalog_;
@@ -255,6 +262,9 @@ class XClusterManager : public XClusterManagerIf,
   bool in_memory_state_cleared_ = true;
 
   std::unique_ptr<XClusterConfig> xcluster_config_;
+
+  std::mutex monitored_tasks_mutex_;
+  std::unordered_set<server::MonitoredTaskPtr> monitored_tasks_ GUARDED_BY(monitored_tasks_mutex_);
 };
 
 }  // namespace master
diff --git a/src/yb/master/xcluster/xcluster_manager_if.h b/src/yb/master/xcluster/xcluster_manager_if.h
index 1d7d696ac8..0679e0e38b 100644
--- a/src/yb/master/xcluster/xcluster_manager_if.h
+++ b/src/yb/master/xcluster/xcluster_manager_if.h
@@ -23,6 +23,7 @@
 namespace yb {
 
 class HybridTime;
+class IsOperationDoneResult;
 class JsonWriter;
 
 namespace rpc {
@@ -93,6 +94,9 @@ class XClusterManagerIf {
       const AlterUniverseReplicationRequestPB* req, AlterUniverseReplicationResponsePB* resp,
       rpc::RpcContext* rpc, const LeaderEpoch& epoch) = 0;
 
+  virtual Result<IsOperationDoneResult> IsSetupUniverseReplicationDone(
+      const xcluster::ReplicationGroupId& replication_group_id, bool skip_health_check) = 0;
+
  protected:
   virtual ~XClusterManagerIf() = default;
 };
diff --git a/src/yb/master/xcluster/xcluster_replication_group.cc b/src/yb/master/xcluster/xcluster_replication_group.cc
index 2b47c7bd57..503728070e 100644
--- a/src/yb/master/xcluster/xcluster_replication_group.cc
+++ b/src/yb/master/xcluster/xcluster_replication_group.cc
@@ -143,8 +143,9 @@ Result<bool> IsSafeTimeReady(
     const auto& namespace_id = namespace_info.consumer_namespace_id();
     auto* it = FindOrNull(safe_time_map, namespace_id);
     if (!it || it->is_special()) {
-      VLOG_WITH_FUNC(1) << "Safe time for namespace " << namespace_id
-                        << " is not yet ready: " << (it ? it->ToString() : "NA");
+      YB_LOG_EVERY_N_SECS_OR_VLOG(INFO, 10, 1)
+          << "xCluster safe time for namespace " << namespace_id
+          << " is not yet ready: " << (it ? it->ToString() : "NA");
       return false;
     }
   }
@@ -157,6 +158,8 @@ Result<bool> IsReplicationGroupReady(
   // The replication group must be in a healthy state.
   if (!FLAGS_xcluster_skip_health_check_on_replication_setup &&
       VERIFY_RESULT(xcluster_manager.HasReplicationGroupErrors(universe.ReplicationGroupId()))) {
+    YB_LOG_EVERY_N_SECS_OR_VLOG(INFO, 10, 1)
+        << "xCluster replication group " << universe.ReplicationGroupId() << " is not yet healthy";
     return false;
   }
 
@@ -178,28 +181,37 @@ Status ReturnErrorOrAddWarning(
   return s;
 }
 
-}  // namespace
-
 Result<std::optional<std::pair<bool, uint32>>> ValidateAutoFlagsConfig(
-    UniverseReplicationInfo& replication_info, const AutoFlagsConfigPB& local_config) {
-  auto master_addresses = replication_info.LockForRead()->pb.producer_master_addresses();
-  auto xcluster_rpc = VERIFY_RESULT(replication_info.GetOrCreateXClusterRpcTasks(master_addresses));
-  auto result = VERIFY_RESULT(
-      xcluster_rpc->client()->ValidateAutoFlagsConfig(local_config, AutoFlagClass::kExternal));
+    client::YBClient& client, const AutoFlagsConfigPB& local_config) {
+  auto result =
+      VERIFY_RESULT(client.ValidateAutoFlagsConfig(local_config, AutoFlagClass::kExternal));
 
   if (!result) {
     return std::nullopt;
   }
 
   auto& [is_valid, source_version] = *result;
-  VLOG(2) << "ValidateAutoFlagsConfig for replication group: "
-          << replication_info.ReplicationGroupId() << ", is_valid: " << is_valid
+  VLOG(2) << "ValidateAutoFlagsConfig is_valid: " << is_valid
           << ", source universe version: " << source_version
           << ", target universe version: " << local_config.config_version();
 
   return result;
 }
 
+}  // namespace
+
+Result<std::optional<std::pair<bool, uint32>>> ValidateAutoFlagsConfig(
+    UniverseReplicationInfo& replication_info, const AutoFlagsConfigPB& local_config) {
+  auto master_addresses = replication_info.LockForRead()->pb.producer_master_addresses();
+  auto xcluster_rpc = VERIFY_RESULT(replication_info.GetOrCreateXClusterRpcTasks(master_addresses));
+  return ValidateAutoFlagsConfig(*xcluster_rpc->client(), local_config);
+}
+
+Result<std::optional<std::pair<bool, uint32>>> ValidateAutoFlagsConfig(
+    client::XClusterRemoteClientHolder& xcluster_client, const AutoFlagsConfigPB& local_config) {
+  return ValidateAutoFlagsConfig(xcluster_client.GetYbClient(), local_config);
+}
+
 Status RefreshAutoFlagConfigVersion(
     SysCatalogTable& sys_catalog, UniverseReplicationInfo& replication_info,
     ClusterConfigInfo& cluster_config, uint32 requested_auto_flag_version,
@@ -439,7 +451,12 @@ Result<IsOperationDoneResult> IsSetupUniverseReplicationDone(
         VERIFY_RESULT(IsReplicationGroupReady(*universe, *catalog_manager.GetXClusterManager()));
   }
 
-  return is_done ? IsOperationDoneResult::Done() : IsOperationDoneResult::NotDone();
+  if (is_done) {
+    LOG(INFO) << "xCluster replication group " << replication_group_id << " is ready";
+    return IsOperationDoneResult::Done();
+  }
+
+  return IsOperationDoneResult::NotDone();
 }
 
 Status RemoveNamespaceFromReplicationGroup(
diff --git a/src/yb/master/xcluster/xcluster_replication_group.h b/src/yb/master/xcluster/xcluster_replication_group.h
index 066056394e..658835f4d2 100644
--- a/src/yb/master/xcluster/xcluster_replication_group.h
+++ b/src/yb/master/xcluster/xcluster_replication_group.h
@@ -37,6 +37,9 @@ namespace master {
 Result<std::optional<std::pair<bool, uint32>>> ValidateAutoFlagsConfig(
     UniverseReplicationInfo& replication_info, const AutoFlagsConfigPB& local_config);
 
+Result<std::optional<std::pair<bool, uint32>>> ValidateAutoFlagsConfig(
+    client::XClusterRemoteClientHolder& xcluster_client, const AutoFlagsConfigPB& local_config);
+
 // Reruns the AutoFlags compatiblity validation when source universe AutoFlags config version has
 // changed.
 // Compatiblity validation is needed only if the requested_auto_flag_version is greater than the
diff --git a/src/yb/master/xcluster/xcluster_target_manager.cc b/src/yb/master/xcluster/xcluster_target_manager.cc
index 3271248b28..db12c45e16 100644
--- a/src/yb/master/xcluster/xcluster_target_manager.cc
+++ b/src/yb/master/xcluster/xcluster_target_manager.cc
@@ -47,7 +47,16 @@ XClusterTargetManager::XClusterTargetManager(
 
 XClusterTargetManager::~XClusterTargetManager() {}
 
-void XClusterTargetManager::Shutdown() {
+void XClusterTargetManager::StartShutdown() {
+  {
+    std::lock_guard l(replication_setup_tasks_mutex_);
+    for (auto& [_, task] : replication_setup_tasks_) {
+      task->TryCancel();
+    }
+  }
+}
+
+void XClusterTargetManager::CompleteShutdown() {
   if (safe_time_service_) {
     safe_time_service_->Shutdown();
   }
@@ -235,11 +244,12 @@ XClusterTargetManager::GetPostTabletCreateTasks(
 }
 
 Status XClusterTargetManager::WaitForSetupUniverseReplicationToFinish(
-    const xcluster::ReplicationGroupId& replication_group_id, CoarseTimePoint deadline) {
+    const xcluster::ReplicationGroupId& replication_group_id, CoarseTimePoint deadline,
+    bool skip_health_check) {
   return Wait(
       [&]() -> Result<bool> {
         auto is_operation_done =
-            VERIFY_RESULT(IsSetupUniverseReplicationDone(replication_group_id));
+            VERIFY_RESULT(IsSetupUniverseReplicationDone(replication_group_id, skip_health_check));
 
         if (is_operation_done.done()) {
           RETURN_NOT_OK(is_operation_done.status());
@@ -1107,11 +1117,61 @@ Status XClusterTargetManager::ProcessPendingSchemaChanges(const LeaderEpoch& epo
 Status XClusterTargetManager::SetupUniverseReplication(
     const SetupUniverseReplicationRequestPB* req, SetupUniverseReplicationResponsePB* resp,
     const LeaderEpoch& epoch) {
-  return SetupUniverseReplicationHelper::Setup(master_, catalog_manager_, req, resp, epoch);
+  // We should set the universe uuid even if we fail with AlreadyPresent error.
+  {
+    auto universe_uuid = catalog_manager_.GetUniverseUuidIfExists();
+    if (universe_uuid) {
+      resp->set_universe_uuid(universe_uuid->ToString());
+    }
+  }
+
+  auto setup_replication_task =
+      VERIFY_RESULT(CreateSetupUniverseReplicationTask(master_, catalog_manager_, req, epoch));
+
+  {
+    std::lock_guard l(replication_setup_tasks_mutex_);
+    SCHECK(
+        !replication_setup_tasks_.contains(setup_replication_task->Id()), AlreadyPresent,
+        "Setup already running for xCluster ReplicationGroup $0", setup_replication_task->Id());
+    replication_setup_tasks_[setup_replication_task->Id()] = setup_replication_task;
+  }
+
+  setup_replication_task->StartSetup();
+
+  return Status::OK();
 }
 
 Result<IsOperationDoneResult> XClusterTargetManager::IsSetupUniverseReplicationDone(
-    const xcluster::ReplicationGroupId& replication_group_id) {
+    const xcluster::ReplicationGroupId& replication_group_id, bool skip_health_check) {
+  std::shared_ptr<XClusterInboundReplicationGroupSetupTaskIf> setup_replication_task;
+  {
+    SharedLock l(replication_setup_tasks_mutex_);
+    setup_replication_task = FindPtrOrNull(replication_setup_tasks_, replication_group_id);
+  }
+
+  if (setup_replication_task) {
+    auto is_done = setup_replication_task->DoneResult();
+    if (!is_done.done()) {
+      return is_done;
+    }
+
+    {
+      std::lock_guard l(replication_setup_tasks_mutex_);
+      // Forget about the task now that we've responded to the user.
+      replication_setup_tasks_.erase(replication_group_id);
+    }
+
+    if (!is_done.status().ok()) {
+      // Setup failed.
+      return is_done;
+    }
+  }
+
+  if (skip_health_check) {
+    return IsOperationDoneResult::Done();
+  }
+
+  // Setup completed successfully. Wait for the ReplicationGroup be become healthy.
   return master::IsSetupUniverseReplicationDone(replication_group_id, catalog_manager_);
 }
 
@@ -1170,6 +1230,24 @@ Status XClusterTargetManager::DeleteUniverseReplication(
     const xcluster::ReplicationGroupId& replication_group_id, bool ignore_errors,
     bool skip_producer_stream_deletion, DeleteUniverseReplicationResponsePB* resp,
     const LeaderEpoch& epoch) {
+  {
+    // If a setup is in progress, then cancel it.
+    std::lock_guard l(replication_setup_tasks_mutex_);
+    auto setup_helper = FindPtrOrNull(replication_setup_tasks_, replication_group_id);
+    if (setup_helper) {
+      replication_setup_tasks_.erase(replication_group_id);
+
+      if (setup_helper->TryCancel()) {
+        // Either we successfully cancelled the Setup or it already failed. There wont be any
+        // UniverseReplication to delete, so we succeeded.
+        return Status::OK();
+      }
+
+      // Setup already completed successfully, but IsSetupUniverseReplicationDone hasn't been
+      // called yet. Proceed with the delete.
+    }
+  }
+
   auto ri = catalog_manager_.GetUniverseReplication(replication_group_id);
   SCHECK(ri != nullptr, NotFound, "Universe replication $0 does not exist", replication_group_id);
 
diff --git a/src/yb/master/xcluster/xcluster_target_manager.h b/src/yb/master/xcluster/xcluster_target_manager.h
index 93ce74705f..a9b1b7de56 100644
--- a/src/yb/master/xcluster/xcluster_target_manager.h
+++ b/src/yb/master/xcluster/xcluster_target_manager.h
@@ -27,6 +27,7 @@ class TSHeartbeatRequestPB;
 class TSHeartbeatResponsePB;
 
 class PostTabletCreateTaskBase;
+class XClusterInboundReplicationGroupSetupTaskIf;
 class UniverseReplicationInfo;
 class XClusterSafeTimeService;
 struct XClusterInboundReplicationGroupStatus;
@@ -63,7 +64,8 @@ class XClusterTargetManager {
       EXCLUDES(table_stream_ids_map_mutex_);
 
   Status WaitForSetupUniverseReplicationToFinish(
-      const xcluster::ReplicationGroupId& replication_group_id, CoarseTimePoint deadline);
+      const xcluster::ReplicationGroupId& replication_group_id, CoarseTimePoint deadline,
+      bool skip_health_check);
 
  protected:
   explicit XClusterTargetManager(
@@ -71,7 +73,8 @@ class XClusterTargetManager {
 
   ~XClusterTargetManager();
 
-  void Shutdown();
+  void StartShutdown() EXCLUDES(replication_setup_tasks_mutex_);
+  void CompleteShutdown() EXCLUDES(replication_setup_tasks_mutex_);
 
   Status Init();
 
@@ -172,7 +175,7 @@ class XClusterTargetManager {
       const LeaderEpoch& epoch);
 
   Result<IsOperationDoneResult> IsSetupUniverseReplicationDone(
-      const xcluster::ReplicationGroupId& replication_group_id);
+      const xcluster::ReplicationGroupId& replication_group_id, bool skip_health_check);
 
   Status SetupNamespaceReplicationWithBootstrap(
       const SetupNamespaceReplicationWithBootstrapRequestPB* req,
@@ -237,6 +240,11 @@ class XClusterTargetManager {
   std::unordered_map<TableId, std::unordered_map<xcluster::ReplicationGroupId, xrepl::StreamId>>
       table_stream_ids_map_ GUARDED_BY(table_stream_ids_map_mutex_);
 
+  mutable std::shared_mutex replication_setup_tasks_mutex_;
+  std::unordered_map<
+      xcluster::ReplicationGroupId, std::shared_ptr<XClusterInboundReplicationGroupSetupTaskIf>>
+      replication_setup_tasks_;
+
   DISALLOW_COPY_AND_ASSIGN(XClusterTargetManager);
 };
 
diff --git a/src/yb/master/xcluster/xcluster_universe_replication_setup_helper.cc b/src/yb/master/xcluster/xcluster_universe_replication_setup_helper.cc
deleted file mode 100644
index 90b6886168..0000000000
--- a/src/yb/master/xcluster/xcluster_universe_replication_setup_helper.cc
+++ /dev/null
@@ -1,1380 +0,0 @@
-// Copyright (c) YugabyteDB, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
-// in compliance with the License.  You may obtain a copy of the License at
-//
-// http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software distributed under the License
-// is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
-// or implied.  See the License for the specific language governing permissions and limitations
-// under the License.
-//
-
-#include "yb/master/xcluster/xcluster_universe_replication_setup_helper.h"
-
-#include "yb/client/table_info.h"
-#include "yb/client/table.h"
-#include "yb/client/xcluster_client.h"
-
-#include "yb/common/colocated_util.h"
-#include "yb/common/common_net.pb.h"
-#include "yb/common/xcluster_util.h"
-
-#include "yb/gutil/bind.h"
-
-#include "yb/master/catalog_manager-internal.h"
-#include "yb/master/catalog_manager.h"
-#include "yb/master/leader_epoch.h"
-#include "yb/master/master_ddl.pb.h"
-#include "yb/master/master_error.h"
-#include "yb/master/master_replication.pb.h"
-#include "yb/master/master_util.h"
-#include "yb/master/master.h"
-#include "yb/master/xcluster_consumer_registry_service.h"
-#include "yb/master/xcluster_rpc_tasks.h"
-#include "yb/master/xcluster/master_xcluster_util.h"
-#include "yb/master/xcluster/xcluster_manager.h"
-#include "yb/master/xcluster/xcluster_replication_group.h"
-
-#include "yb/rpc/rpc_context.h"
-
-#include "yb/util/flags/auto_flags_util.h"
-#include "yb/util/status.h"
-
-DEFINE_RUNTIME_bool(check_bootstrap_required, false,
-    "Is it necessary to check whether bootstrap is required for Universe Replication.");
-
-DEFINE_test_flag(bool, allow_ycql_transactional_xcluster, false,
-    "Determines if xCluster transactional replication on YCQL tables is allowed.");
-
-DEFINE_test_flag(
-    bool, fail_universe_replication_merge, false,
-    "Causes MergeUniverseReplication to fail with an error.");
-
-DEFINE_test_flag(bool, exit_unfinished_merging, false,
-    "Whether to exit part way through the merging universe process.");
-
-DECLARE_bool(enable_xcluster_auto_flag_validation);
-
-#define RETURN_ACTION_NOT_OK(expr, action) \
-  RETURN_NOT_OK_PREPEND((expr), Format("An error occurred while $0", action))
-
-namespace yb::master {
-
-namespace {
-
-Status ValidateTableListForDbScopedReplication(
-    UniverseReplicationInfo& universe, const std::vector<NamespaceId>& namespace_ids,
-    const std::set<TableId>& replicated_tables, const CatalogManager& catalog_manager) {
-  std::set<TableId> validated_tables;
-
-  for (const auto& namespace_id : namespace_ids) {
-    auto table_infos =
-        VERIFY_RESULT(GetTablesEligibleForXClusterReplication(catalog_manager, namespace_id));
-
-    std::vector<TableId> missing_tables;
-
-    for (const auto& table_info : table_infos) {
-      const auto& table_id = table_info->id();
-      if (replicated_tables.contains(table_id)) {
-        validated_tables.insert(table_id);
-      } else {
-        missing_tables.push_back(table_id);
-      }
-    }
-
-    SCHECK_FORMAT(
-        missing_tables.empty(), IllegalState,
-        "Namespace $0 has additional tables that were not added to xCluster DB Scoped replication "
-        "group $1: $2",
-        namespace_id, universe.id(), yb::ToString(missing_tables));
-  }
-
-  auto diff = STLSetSymmetricDifference(replicated_tables, validated_tables);
-  SCHECK_FORMAT(
-      diff.empty(), IllegalState,
-      "xCluster DB Scoped replication group $0 contains tables $1 that do not belong to replicated "
-      "namespaces $2",
-      universe.id(), yb::ToString(diff), yb::ToString(namespace_ids));
-
-  return Status::OK();
-}
-
-// Check if the local AutoFlags config is compatible with the source universe and returns the source
-// universe AutoFlags config version if they are compatible.
-// If they are not compatible, returns a bad status.
-// If the source universe is running an older version which does not support AutoFlags compatiblity
-// check, returns an invalid AutoFlags config version.
-Result<uint32> GetAutoFlagConfigVersionIfCompatible(
-    UniverseReplicationInfo& replication_info, const AutoFlagsConfigPB& local_config) {
-  const auto& replication_group_id = replication_info.ReplicationGroupId();
-
-  VLOG_WITH_FUNC(2) << "Validating AutoFlags config for replication group: " << replication_group_id
-                    << " with target config version: " << local_config.config_version();
-
-  auto validate_result = VERIFY_RESULT(ValidateAutoFlagsConfig(replication_info, local_config));
-
-  if (!validate_result) {
-    VLOG_WITH_FUNC(2)
-        << "Source universe of replication group " << replication_group_id
-        << " is running a version that does not support the AutoFlags compatibility check yet";
-    return kInvalidAutoFlagsConfigVersion;
-  }
-
-  auto& [is_valid, source_version] = *validate_result;
-
-  SCHECK(
-      is_valid, IllegalState,
-      "AutoFlags between the universes are not compatible. Upgrade the target universe to a "
-      "version higher than or equal to the source universe");
-
-  return source_version;
-}
-
-}  // namespace
-
-SetupUniverseReplicationHelper::SetupUniverseReplicationHelper(
-    Master& master, CatalogManager& catalog_manager, const LeaderEpoch& epoch)
-    : master_(master),
-      catalog_manager_(catalog_manager),
-      sys_catalog_(*catalog_manager.sys_catalog()),
-      xcluster_manager_(*catalog_manager.GetXClusterManagerImpl()),
-      epoch_(epoch) {}
-
-SetupUniverseReplicationHelper::~SetupUniverseReplicationHelper() {}
-
-Status SetupUniverseReplicationHelper::Setup(
-    Master& master, CatalogManager& catalog_manager, const SetupUniverseReplicationRequestPB* req,
-    SetupUniverseReplicationResponsePB* resp, const LeaderEpoch& epoch) {
-  auto helper = scoped_refptr<SetupUniverseReplicationHelper>(
-      new SetupUniverseReplicationHelper(master, catalog_manager, epoch));
-  return helper->SetupUniverseReplication(req, resp);
-}
-
-void SetupUniverseReplicationHelper::MarkUniverseReplicationFailed(
-    scoped_refptr<UniverseReplicationInfo> universe, const Status& failure_status) {
-  auto l = universe->LockForWrite();
-  MarkUniverseReplicationFailed(failure_status, &l, universe);
-}
-
-void SetupUniverseReplicationHelper::MarkUniverseReplicationFailed(
-    const Status& failure_status, CowWriteLock<PersistentUniverseReplicationInfo>* universe_lock,
-    scoped_refptr<UniverseReplicationInfo> universe) {
-  auto& l = *universe_lock;
-  if (l->pb.state() == SysUniverseReplicationEntryPB::DELETED) {
-    l.mutable_data()->pb.set_state(SysUniverseReplicationEntryPB::DELETED_ERROR);
-  } else {
-    l.mutable_data()->pb.set_state(SysUniverseReplicationEntryPB::FAILED);
-  }
-
-  LOG(WARNING) << "Universe replication " << universe->ToString()
-               << " failed: " << failure_status.ToString();
-
-  universe->SetSetupUniverseReplicationErrorStatus(failure_status);
-
-  // Update sys_catalog.
-  const Status s = sys_catalog_.Upsert(epoch_, universe);
-
-  l.CommitOrWarn(s, "updating universe replication info in sys-catalog");
-}
-
-/*
- * UniverseReplication is setup in 4 stages within the Catalog Manager
- * 1. SetupUniverseReplication: Validates user input & requests Producer schema.
- * 2. GetTableSchemaCallback:   Validates Schema compatibility & requests Producer xCluster init.
- * 3. AddStreamToUniverseAndInitConsumer:  Setup RPC connections for xCluster Streaming
- * 4. InitXClusterConsumer:          Initializes the Consumer settings to begin tailing data
- */
-Status SetupUniverseReplicationHelper::SetupUniverseReplication(
-    const SetupUniverseReplicationRequestPB* req, SetupUniverseReplicationResponsePB* resp) {
-  // Sanity checking section.
-  if (!req->has_replication_group_id()) {
-    return STATUS(
-        InvalidArgument, "Producer universe ID must be provided", req->ShortDebugString(),
-        MasterError(MasterErrorPB::INVALID_REQUEST));
-  }
-
-  if (req->producer_master_addresses_size() <= 0) {
-    return STATUS(
-        InvalidArgument, "Producer master address must be provided", req->ShortDebugString(),
-        MasterError(MasterErrorPB::INVALID_REQUEST));
-  }
-
-  if (req->producer_bootstrap_ids().size() > 0 &&
-      req->producer_bootstrap_ids().size() != req->producer_table_ids().size()) {
-    return STATUS(
-        InvalidArgument, "Number of bootstrap ids must be equal to number of tables",
-        req->ShortDebugString(), MasterError(MasterErrorPB::INVALID_REQUEST));
-  }
-
-  {
-    auto l = catalog_manager_.ClusterConfig()->LockForRead();
-    if (l->pb.cluster_uuid() == req->replication_group_id()) {
-      return STATUS(
-          InvalidArgument, "The request UUID and cluster UUID are identical.",
-          req->ShortDebugString(), MasterError(MasterErrorPB::INVALID_REQUEST));
-    }
-  }
-
-  RETURN_NOT_OK_PREPEND(
-      ValidateMasterAddressesBelongToDifferentCluster(master_, req->producer_master_addresses()),
-      req->ShortDebugString());
-
-  SetupReplicationInfo setup_info;
-  setup_info.transactional = req->transactional();
-  auto& table_id_to_bootstrap_id = setup_info.table_bootstrap_ids;
-
-  if (!req->producer_bootstrap_ids().empty()) {
-    if (req->producer_table_ids().size() != req->producer_bootstrap_ids_size()) {
-      return STATUS(
-          InvalidArgument, "Bootstrap ids must be provided for all tables", req->ShortDebugString(),
-          MasterError(MasterErrorPB::INVALID_REQUEST));
-    }
-
-    table_id_to_bootstrap_id.reserve(req->producer_table_ids().size());
-    for (int i = 0; i < req->producer_table_ids().size(); i++) {
-      table_id_to_bootstrap_id.insert_or_assign(
-          req->producer_table_ids(i),
-          VERIFY_RESULT(xrepl::StreamId::FromString(req->producer_bootstrap_ids(i))));
-    }
-  }
-
-  SCHECK(
-      req->producer_namespaces().empty() || req->transactional(), InvalidArgument,
-      "Transactional flag must be set for Db scoped replication groups");
-
-  std::vector<NamespaceId> producer_namespace_ids, consumer_namespace_ids;
-  for (const auto& producer_ns_id : req->producer_namespaces()) {
-    SCHECK(!producer_ns_id.id().empty(), InvalidArgument, "Invalid Namespace Id");
-    SCHECK(!producer_ns_id.name().empty(), InvalidArgument, "Invalid Namespace name");
-    SCHECK_EQ(
-        producer_ns_id.database_type(), YQLDatabase::YQL_DATABASE_PGSQL, InvalidArgument,
-        "Invalid Namespace database_type");
-
-    producer_namespace_ids.push_back(producer_ns_id.id());
-
-    NamespaceIdentifierPB consumer_ns_id;
-    consumer_ns_id.set_database_type(YQLDatabase::YQL_DATABASE_PGSQL);
-    consumer_ns_id.set_name(producer_ns_id.name());
-    auto ns_info = VERIFY_RESULT(catalog_manager_.FindNamespace(consumer_ns_id));
-    consumer_namespace_ids.push_back(ns_info->id());
-  }
-
-  // We should set the universe uuid even if we fail with AlreadyPresent error.
-  {
-    auto universe_uuid = catalog_manager_.GetUniverseUuidIfExists();
-    if (universe_uuid) {
-      resp->set_universe_uuid(universe_uuid->ToString());
-    }
-  }
-
-  auto ri = VERIFY_RESULT(CreateUniverseReplicationInfo(
-      xcluster::ReplicationGroupId(req->replication_group_id()), req->producer_master_addresses(),
-      producer_namespace_ids, consumer_namespace_ids, req->producer_table_ids(),
-      setup_info.transactional));
-
-  // Initialize the xCluster Stream by querying the Producer server for RPC sanity checks.
-  auto result = ri->GetOrCreateXClusterRpcTasks(req->producer_master_addresses());
-  if (!result.ok()) {
-    MarkUniverseReplicationFailed(ri, ResultToStatus(result));
-    return SetupError(resp->mutable_error(), MasterErrorPB::INVALID_REQUEST, result.status());
-  }
-  std::shared_ptr<XClusterRpcTasks> xcluster_rpc = *result;
-
-  // For each table, run an async RPC task to verify a sufficient Producer:Consumer schema match.
-  for (int i = 0; i < req->producer_table_ids_size(); i++) {
-    scoped_refptr<SetupUniverseReplicationHelper> shared_this = this;
-
-    // SETUP CONTINUES after this async call.
-    Status s;
-    if (IsColocatedDbParentTableId(req->producer_table_ids(i))) {
-      auto tables_info = std::make_shared<std::vector<client::YBTableInfo>>();
-      s = xcluster_rpc->client()->GetColocatedTabletSchemaByParentTableId(
-          req->producer_table_ids(i), tables_info,
-          Bind(
-              &SetupUniverseReplicationHelper::GetColocatedTabletSchemaCallback, shared_this,
-              ri->ReplicationGroupId(), tables_info, setup_info));
-    } else if (IsTablegroupParentTableId(req->producer_table_ids(i))) {
-      auto tablegroup_id = GetTablegroupIdFromParentTableId(req->producer_table_ids(i));
-      auto tables_info = std::make_shared<std::vector<client::YBTableInfo>>();
-      s = xcluster_rpc->client()->GetTablegroupSchemaById(
-          tablegroup_id, tables_info,
-          Bind(
-              &SetupUniverseReplicationHelper::GetTablegroupSchemaCallback, shared_this,
-              ri->ReplicationGroupId(), tables_info, tablegroup_id, setup_info));
-    } else {
-      auto table_info = std::make_shared<client::YBTableInfo>();
-      s = xcluster_rpc->client()->GetTableSchemaById(
-          req->producer_table_ids(i), table_info,
-          Bind(
-              &SetupUniverseReplicationHelper::GetTableSchemaCallback, shared_this,
-              ri->ReplicationGroupId(), table_info, setup_info));
-    }
-
-    if (!s.ok()) {
-      MarkUniverseReplicationFailed(ri, s);
-      return SetupError(resp->mutable_error(), MasterErrorPB::INVALID_REQUEST, s);
-    }
-  }
-
-  LOG(INFO) << "Started schema validation for universe replication " << ri->ToString();
-  return Status::OK();
-}
-
-Status SetupUniverseReplicationHelper::ValidateMasterAddressesBelongToDifferentCluster(
-    Master& master, const google::protobuf::RepeatedPtrField<HostPortPB>& master_addresses) {
-  std::vector<ServerEntryPB> cluster_master_addresses;
-  RETURN_NOT_OK(master.ListMasters(&cluster_master_addresses));
-  std::unordered_set<HostPort, HostPortHash> cluster_master_hps;
-
-  for (const auto& cluster_elem : cluster_master_addresses) {
-    if (cluster_elem.has_registration()) {
-      auto p_rpc_addresses = cluster_elem.registration().private_rpc_addresses();
-      for (const auto& p_rpc_elem : p_rpc_addresses) {
-        cluster_master_hps.insert(HostPort::FromPB(p_rpc_elem));
-      }
-
-      auto broadcast_addresses = cluster_elem.registration().broadcast_addresses();
-      for (const auto& bc_elem : broadcast_addresses) {
-        cluster_master_hps.insert(HostPort::FromPB(bc_elem));
-      }
-    }
-
-    for (const auto& master_address : master_addresses) {
-      auto master_hp = HostPort::FromPB(master_address);
-      SCHECK(
-          !cluster_master_hps.contains(master_hp), InvalidArgument,
-          "Master address $0 belongs to the target universe", master_hp);
-    }
-  }
-  return Status::OK();
-}
-
-void SetupUniverseReplicationHelper::GetTableSchemaCallback(
-    const xcluster::ReplicationGroupId& replication_group_id,
-    const std::shared_ptr<client::YBTableInfo>& producer_info,
-    const SetupReplicationInfo& setup_info, const Status& s) {
-  // First get the universe.
-  auto universe = catalog_manager_.GetUniverseReplication(replication_group_id);
-  if (universe == nullptr) {
-    LOG(ERROR) << "Universe not found: " << replication_group_id;
-    return;
-  }
-
-  std::string action = "getting schema for table";
-  auto status = s;
-  if (status.ok()) {
-    action = "validating table schema and creating xCluster stream";
-    status = ValidateTableAndCreateStreams(universe, producer_info, setup_info);
-  }
-
-  if (!status.ok()) {
-    LOG(ERROR) << "Error " << action << ". Universe: " << replication_group_id
-               << ", Table: " << producer_info->table_id << ": " << status;
-    MarkUniverseReplicationFailed(universe, status);
-  }
-}
-
-void SetupUniverseReplicationHelper::GetTablegroupSchemaCallback(
-    const xcluster::ReplicationGroupId& replication_group_id,
-    const std::shared_ptr<std::vector<client::YBTableInfo>>& infos,
-    const TablegroupId& producer_tablegroup_id, const SetupReplicationInfo& setup_info,
-    const Status& s) {
-  // First get the universe.
-  auto universe = catalog_manager_.GetUniverseReplication(replication_group_id);
-  if (universe == nullptr) {
-    LOG(ERROR) << "Universe not found: " << replication_group_id;
-    return;
-  }
-
-  auto status =
-      GetTablegroupSchemaCallbackInternal(universe, *infos, producer_tablegroup_id, setup_info, s);
-  if (!status.ok()) {
-    std::ostringstream oss;
-    for (size_t i = 0; i < infos->size(); ++i) {
-      oss << ((i == 0) ? "" : ", ") << (*infos)[i].table_id;
-    }
-    LOG(ERROR) << "Error processing for tables: [ " << oss.str()
-               << " ] for xCluster replication group " << replication_group_id << ": " << status;
-    MarkUniverseReplicationFailed(universe, status);
-  }
-}
-
-Status SetupUniverseReplicationHelper::GetTablegroupSchemaCallbackInternal(
-    scoped_refptr<UniverseReplicationInfo>& universe, const std::vector<client::YBTableInfo>& infos,
-    const TablegroupId& producer_tablegroup_id, const SetupReplicationInfo& setup_info,
-    const Status& s) {
-  RETURN_NOT_OK(s);
-
-  SCHECK(!infos.empty(), IllegalState, Format("Tablegroup $0 is empty", producer_tablegroup_id));
-
-  // validated_consumer_tables contains the table IDs corresponding to that
-  // from the producer tables.
-  std::unordered_set<TableId> validated_consumer_tables;
-  ColocationSchemaVersions colocated_schema_versions;
-  colocated_schema_versions.reserve(infos.size());
-  for (const auto& info : infos) {
-    // Validate each of the member table in the tablegroup.
-    GetTableSchemaResponsePB resp;
-    RETURN_NOT_OK(ValidateTableSchemaForXCluster(info, setup_info, &resp));
-
-    colocated_schema_versions.emplace_back(
-        resp.schema().colocated_table_id().colocation_id(), info.schema.version(), resp.version());
-    validated_consumer_tables.insert(resp.identifier().table_id());
-  }
-
-  // Get the consumer tablegroup ID. Since this call is expensive (one needs to reverse lookup
-  // the tablegroup ID from table ID), we only do this call once and do validation afterward.
-  TablegroupId consumer_tablegroup_id;
-  // Starting Colocation GA, colocated databases create implicit underlying tablegroups.
-  bool colocated_database;
-  RETURN_NOT_OK(catalog_manager_.GetTableGroupAndColocationInfo(
-      *validated_consumer_tables.begin(), consumer_tablegroup_id, colocated_database));
-
-  // tables_in_consumer_tablegroup are the tables listed within the consumer_tablegroup_id.
-  // We need validated_consumer_tables and tables_in_consumer_tablegroup to be identical.
-  std::unordered_set<TableId> tables_in_consumer_tablegroup;
-  {
-    GetTablegroupSchemaRequestPB req;
-    GetTablegroupSchemaResponsePB resp;
-    req.mutable_tablegroup()->set_id(consumer_tablegroup_id);
-    auto status = catalog_manager_.GetTablegroupSchema(&req, &resp);
-    if (status.ok() && resp.has_error()) {
-      status = StatusFromPB(resp.error().status());
-    }
-    RETURN_NOT_OK_PREPEND(
-        status,
-        Format("Error when getting consumer tablegroup schema: $0", consumer_tablegroup_id));
-
-    for (const auto& info : resp.get_table_schema_response_pbs()) {
-      tables_in_consumer_tablegroup.insert(info.identifier().table_id());
-    }
-  }
-
-  if (validated_consumer_tables != tables_in_consumer_tablegroup) {
-    return STATUS(
-        IllegalState,
-        Format(
-            "Mismatch between tables associated with producer tablegroup $0 and "
-            "tables in consumer tablegroup $1: ($2) vs ($3).",
-            producer_tablegroup_id, consumer_tablegroup_id, AsString(validated_consumer_tables),
-            AsString(tables_in_consumer_tablegroup)));
-  }
-
-  RETURN_NOT_OK_PREPEND(
-      IsBootstrapRequiredOnProducer(
-          universe, producer_tablegroup_id, setup_info.table_bootstrap_ids),
-      Format(
-          "Found error while checking if bootstrap is required for table $0",
-          producer_tablegroup_id));
-
-  TableId producer_parent_table_id;
-  TableId consumer_parent_table_id;
-  if (colocated_database) {
-    producer_parent_table_id = GetColocationParentTableId(producer_tablegroup_id);
-    consumer_parent_table_id = GetColocationParentTableId(consumer_tablegroup_id);
-  } else {
-    producer_parent_table_id = GetTablegroupParentTableId(producer_tablegroup_id);
-    consumer_parent_table_id = GetTablegroupParentTableId(consumer_tablegroup_id);
-  }
-
-  SCHECK(
-      !xcluster_manager_.IsTableReplicationConsumer(consumer_parent_table_id), IllegalState,
-      "N:1 replication topology not supported");
-
-  RETURN_NOT_OK(AddValidatedTableAndCreateStreams(
-      universe, setup_info.table_bootstrap_ids, producer_parent_table_id, consumer_parent_table_id,
-      colocated_schema_versions));
-  return Status::OK();
-}
-
-void SetupUniverseReplicationHelper::GetColocatedTabletSchemaCallback(
-    const xcluster::ReplicationGroupId& replication_group_id,
-    const std::shared_ptr<std::vector<client::YBTableInfo>>& infos,
-    const SetupReplicationInfo& setup_info, const Status& s) {
-  // First get the universe.
-  auto universe = catalog_manager_.GetUniverseReplication(replication_group_id);
-  if (universe == nullptr) {
-    LOG(ERROR) << "Universe not found: " << replication_group_id;
-    return;
-  }
-
-  if (!s.ok()) {
-    MarkUniverseReplicationFailed(universe, s);
-    std::ostringstream oss;
-    for (size_t i = 0; i < infos->size(); ++i) {
-      oss << ((i == 0) ? "" : ", ") << (*infos)[i].table_id;
-    }
-    LOG(ERROR) << "Error getting schema for tables: [ " << oss.str() << " ]: " << s;
-    return;
-  }
-
-  if (infos->empty()) {
-    LOG(WARNING) << "Received empty list of tables to validate: " << s;
-    return;
-  }
-
-  // Validate table schemas.
-  std::unordered_set<TableId> producer_parent_table_ids;
-  std::unordered_set<TableId> consumer_parent_table_ids;
-  ColocationSchemaVersions colocated_schema_versions;
-  colocated_schema_versions.reserve(infos->size());
-  for (const auto& info : *infos) {
-    // Verify that we have a colocated table.
-    if (!info.colocated) {
-      MarkUniverseReplicationFailed(
-          universe,
-          STATUS(InvalidArgument, Format("Received non-colocated table: $0", info.table_id)));
-      LOG(ERROR) << "Received non-colocated table: " << info.table_id;
-      return;
-    }
-    // Validate each table, and get the parent colocated table id for the consumer.
-    GetTableSchemaResponsePB resp;
-    Status table_status = ValidateTableSchemaForXCluster(info, setup_info, &resp);
-    if (!table_status.ok()) {
-      MarkUniverseReplicationFailed(universe, table_status);
-      LOG(ERROR) << "Found error while validating table schema for table " << info.table_id << ": "
-                 << table_status;
-      return;
-    }
-    // Store the parent table ids.
-    producer_parent_table_ids.insert(GetColocatedDbParentTableId(info.table_name.namespace_id()));
-    consumer_parent_table_ids.insert(
-        GetColocatedDbParentTableId(resp.identifier().namespace_().id()));
-    colocated_schema_versions.emplace_back(
-        resp.schema().colocated_table_id().colocation_id(), info.schema.version(), resp.version());
-  }
-
-  // Verify that we only found one producer and one consumer colocated parent table id.
-  if (producer_parent_table_ids.size() != 1) {
-    auto message = Format(
-        "Found incorrect number of producer colocated parent table ids. "
-        "Expected 1, but found: $0",
-        AsString(producer_parent_table_ids));
-    MarkUniverseReplicationFailed(universe, STATUS(InvalidArgument, message));
-    LOG(ERROR) << message;
-    return;
-  }
-  if (consumer_parent_table_ids.size() != 1) {
-    auto message = Format(
-        "Found incorrect number of consumer colocated parent table ids. "
-        "Expected 1, but found: $0",
-        AsString(consumer_parent_table_ids));
-    MarkUniverseReplicationFailed(universe, STATUS(InvalidArgument, message));
-    LOG(ERROR) << message;
-    return;
-  }
-
-  if (xcluster_manager_.IsTableReplicationConsumer(*consumer_parent_table_ids.begin())) {
-    std::string message = "N:1 replication topology not supported";
-    MarkUniverseReplicationFailed(universe, STATUS(IllegalState, message));
-    LOG(ERROR) << message;
-    return;
-  }
-
-  Status status = IsBootstrapRequiredOnProducer(
-      universe, *producer_parent_table_ids.begin(), setup_info.table_bootstrap_ids);
-  if (!status.ok()) {
-    MarkUniverseReplicationFailed(universe, status);
-    LOG(ERROR) << "Found error while checking if bootstrap is required for table "
-               << *producer_parent_table_ids.begin() << ": " << status;
-  }
-
-  status = AddValidatedTableAndCreateStreams(
-      universe, setup_info.table_bootstrap_ids, *producer_parent_table_ids.begin(),
-      *consumer_parent_table_ids.begin(), colocated_schema_versions);
-
-  if (!status.ok()) {
-    LOG(ERROR) << "Found error while adding validated table to system catalog: "
-               << *producer_parent_table_ids.begin() << ": " << status;
-    return;
-  }
-}
-
-Status SetupUniverseReplicationHelper::ValidateTableAndCreateStreams(
-    scoped_refptr<UniverseReplicationInfo> universe,
-    const std::shared_ptr<client::YBTableInfo>& producer_info,
-    const SetupReplicationInfo& setup_info) {
-  auto l = universe->LockForWrite();
-  if (producer_info->table_name.namespace_name() == master::kSystemNamespaceName) {
-    auto status = STATUS(IllegalState, "Cannot replicate system tables.");
-    MarkUniverseReplicationFailed(status, &l, universe);
-    return status;
-  }
-  RETURN_ACTION_NOT_OK(
-      sys_catalog_.Upsert(epoch_, universe), "updating system tables in universe replication");
-  l.Commit();
-
-  GetTableSchemaResponsePB consumer_schema;
-  RETURN_NOT_OK(ValidateTableSchemaForXCluster(*producer_info, setup_info, &consumer_schema));
-
-  // If Bootstrap Id is passed in then it must be provided for all tables.
-  const auto& producer_bootstrap_ids = setup_info.table_bootstrap_ids;
-  SCHECK(
-      producer_bootstrap_ids.empty() || producer_bootstrap_ids.contains(producer_info->table_id),
-      NotFound,
-      Format("Bootstrap id not found for table $0", producer_info->table_name.ToString()));
-
-  RETURN_NOT_OK(
-      IsBootstrapRequiredOnProducer(universe, producer_info->table_id, producer_bootstrap_ids));
-
-  SchemaVersion producer_schema_version = producer_info->schema.version();
-  SchemaVersion consumer_schema_version = consumer_schema.version();
-  ColocationSchemaVersions colocated_schema_versions;
-  RETURN_NOT_OK(AddValidatedTableToUniverseReplication(
-      universe, producer_info->table_id, consumer_schema.identifier().table_id(),
-      producer_schema_version, consumer_schema_version, colocated_schema_versions));
-
-  return CreateStreamsIfReplicationValidated(universe, producer_bootstrap_ids);
-}
-
-Status SetupUniverseReplicationHelper::ValidateTableSchemaForXCluster(
-    const client::YBTableInfo& info, const SetupReplicationInfo& setup_info,
-    GetTableSchemaResponsePB* resp) {
-  bool is_ysql_table = info.table_type == client::YBTableType::PGSQL_TABLE_TYPE;
-  if (setup_info.transactional && !GetAtomicFlag(&FLAGS_TEST_allow_ycql_transactional_xcluster) &&
-      !is_ysql_table) {
-    return STATUS_FORMAT(
-        NotSupported, "Transactional replication is not supported for non-YSQL tables: $0",
-        info.table_name.ToString());
-  }
-
-  // Get corresponding table schema on local universe.
-  GetTableSchemaRequestPB req;
-
-  auto* table = req.mutable_table();
-  table->set_table_name(info.table_name.table_name());
-  table->mutable_namespace_()->set_name(info.table_name.namespace_name());
-  table->mutable_namespace_()->set_database_type(
-      GetDatabaseTypeForTable(client::ClientToPBTableType(info.table_type)));
-
-  // Since YSQL tables are not present in table map, we first need to list tables to get the table
-  // ID and then get table schema.
-  // Remove this once table maps are fixed for YSQL.
-  ListTablesRequestPB list_req;
-  ListTablesResponsePB list_resp;
-
-  list_req.set_name_filter(info.table_name.table_name());
-  Status status = catalog_manager_.ListTables(&list_req, &list_resp);
-  SCHECK(
-      status.ok() && !list_resp.has_error(), NotFound,
-      Format("Error while listing table: $0", status.ToString()));
-
-  const auto& source_schema = client::internal::GetSchema(info.schema);
-  for (const auto& t : list_resp.tables()) {
-    // Check that table name and namespace both match.
-    if (t.name() != info.table_name.table_name() ||
-        t.namespace_().name() != info.table_name.namespace_name()) {
-      continue;
-    }
-
-    // Check that schema name matches for YSQL tables, if the field is empty, fill in that
-    // information during GetTableSchema call later.
-    bool has_valid_pgschema_name = !t.pgschema_name().empty();
-    if (is_ysql_table && has_valid_pgschema_name &&
-        t.pgschema_name() != source_schema.SchemaName()) {
-      continue;
-    }
-
-    // Get the table schema.
-    table->set_table_id(t.id());
-    status = catalog_manager_.GetTableSchema(&req, resp);
-    SCHECK(
-        status.ok() && !resp->has_error(), NotFound,
-        Format("Error while getting table schema: $0", status.ToString()));
-
-    // Double-check schema name here if the previous check was skipped.
-    if (is_ysql_table && !has_valid_pgschema_name) {
-      std::string target_schema_name = resp->schema().pgschema_name();
-      if (target_schema_name != source_schema.SchemaName()) {
-        table->clear_table_id();
-        continue;
-      }
-    }
-
-    // Verify that the table on the target side supports replication.
-    if (is_ysql_table && t.has_relation_type() && t.relation_type() == MATVIEW_TABLE_RELATION) {
-      return STATUS_FORMAT(
-          NotSupported, "Replication is not supported for materialized view: $0",
-          info.table_name.ToString());
-    }
-
-    Schema consumer_schema;
-    auto result = SchemaFromPB(resp->schema(), &consumer_schema);
-
-    // We now have a table match. Validate the schema.
-    SCHECK(
-        result.ok() && consumer_schema.EquivalentForDataCopy(source_schema), IllegalState,
-        Format(
-            "Source and target schemas don't match: "
-            "Source: $0, Target: $1, Source schema: $2, Target schema: $3",
-            info.table_id, resp->identifier().table_id(), info.schema.ToString(),
-            resp->schema().DebugString()));
-    break;
-  }
-
-  SCHECK(
-      table->has_table_id(), NotFound,
-      Format(
-          "Could not find matching table for $0$1", info.table_name.ToString(),
-          (is_ysql_table ? " pgschema_name: " + source_schema.SchemaName() : "")));
-
-  // Still need to make map of table id to resp table id (to add to validated map)
-  // For colocated tables, only add the parent table since we only added the parent table to the
-  // original pb (we use the number of tables in the pb to determine when validation is done).
-  if (info.colocated) {
-    // We require that colocated tables have the same colocation ID.
-    //
-    // Backward compatibility: tables created prior to #7378 use YSQL table OID as a colocation ID.
-    auto source_clc_id = info.schema.has_colocation_id()
-                             ? info.schema.colocation_id()
-                             : CHECK_RESULT(GetPgsqlTableOid(info.table_id));
-    auto target_clc_id = (resp->schema().has_colocated_table_id() &&
-                          resp->schema().colocated_table_id().has_colocation_id())
-                             ? resp->schema().colocated_table_id().colocation_id()
-                             : CHECK_RESULT(GetPgsqlTableOid(resp->identifier().table_id()));
-    SCHECK(
-        source_clc_id == target_clc_id, IllegalState,
-        Format(
-            "Source and target colocation IDs don't match for colocated table: "
-            "Source: $0, Target: $1, Source colocation ID: $2, Target colocation ID: $3",
-            info.table_id, resp->identifier().table_id(), source_clc_id, target_clc_id));
-  }
-
-  SCHECK(
-      !xcluster_manager_.IsTableReplicationConsumer(table->table_id()), IllegalState,
-      "N:1 replication topology not supported");
-
-  return Status::OK();
-}
-
-Status SetupUniverseReplicationHelper::IsBootstrapRequiredOnProducer(
-    scoped_refptr<UniverseReplicationInfo> universe, const TableId& producer_table,
-    const std::unordered_map<TableId, xrepl::StreamId>& table_bootstrap_ids) {
-  if (!FLAGS_check_bootstrap_required) {
-    return Status::OK();
-  }
-  auto master_addresses = universe->LockForRead()->pb.producer_master_addresses();
-  boost::optional<xrepl::StreamId> bootstrap_id;
-  if (table_bootstrap_ids.count(producer_table) > 0) {
-    bootstrap_id = table_bootstrap_ids.at(producer_table);
-  }
-
-  auto xcluster_rpc = VERIFY_RESULT(universe->GetOrCreateXClusterRpcTasks(master_addresses));
-  if (VERIFY_RESULT(xcluster_rpc->client()->IsBootstrapRequired({producer_table}, bootstrap_id))) {
-    return STATUS(
-        IllegalState,
-        Format(
-            "Error Missing Data in Logs. Bootstrap is required for producer $0", universe->id()));
-  }
-  return Status::OK();
-}
-
-Status SetupUniverseReplicationHelper::AddValidatedTableToUniverseReplication(
-    scoped_refptr<UniverseReplicationInfo> universe, const TableId& producer_table,
-    const TableId& consumer_table, const SchemaVersion& producer_schema_version,
-    const SchemaVersion& consumer_schema_version,
-    const ColocationSchemaVersions& colocated_schema_versions) {
-  auto l = universe->LockForWrite();
-
-  auto map = l.mutable_data()->pb.mutable_validated_tables();
-  (*map)[producer_table] = consumer_table;
-
-  SchemaVersionMappingEntryPB entry;
-  if (IsColocationParentTableId(consumer_table)) {
-    for (const auto& [colocation_id, producer_schema_version, consumer_schema_version] :
-         colocated_schema_versions) {
-      auto colocated_entry = entry.add_colocated_schema_versions();
-      auto colocation_mapping = colocated_entry->mutable_schema_version_mapping();
-      colocated_entry->set_colocation_id(colocation_id);
-      colocation_mapping->set_producer_schema_version(producer_schema_version);
-      colocation_mapping->set_consumer_schema_version(consumer_schema_version);
-    }
-  } else {
-    auto mapping = entry.mutable_schema_version_mapping();
-    mapping->set_producer_schema_version(producer_schema_version);
-    mapping->set_consumer_schema_version(consumer_schema_version);
-  }
-
-  auto schema_versions_map = l.mutable_data()->pb.mutable_schema_version_mappings();
-  (*schema_versions_map)[producer_table] = std::move(entry);
-
-  // TODO: end of config validation should be where SetupUniverseReplication exits back to user
-  LOG(INFO) << "UpdateItem in AddValidatedTable";
-
-  // Update sys_catalog.
-  RETURN_ACTION_NOT_OK(
-      sys_catalog_.Upsert(epoch_, universe), "updating universe replication info in sys-catalog");
-  l.Commit();
-
-  return Status::OK();
-}
-
-Status SetupUniverseReplicationHelper::AddValidatedTableAndCreateStreams(
-    scoped_refptr<UniverseReplicationInfo> universe,
-    const std::unordered_map<TableId, xrepl::StreamId>& table_bootstrap_ids,
-    const TableId& producer_table, const TableId& consumer_table,
-    const ColocationSchemaVersions& colocated_schema_versions) {
-  RETURN_NOT_OK(AddValidatedTableToUniverseReplication(
-      universe, producer_table, consumer_table, cdc::kInvalidSchemaVersion,
-      cdc::kInvalidSchemaVersion, colocated_schema_versions));
-  return CreateStreamsIfReplicationValidated(universe, table_bootstrap_ids);
-}
-
-Status SetupUniverseReplicationHelper::CreateStreamsIfReplicationValidated(
-    scoped_refptr<UniverseReplicationInfo> universe,
-    const std::unordered_map<TableId, xrepl::StreamId>& table_bootstrap_ids) {
-  auto l = universe->LockForWrite();
-  if (l->is_deleted_or_failed()) {
-    // Nothing to do since universe is being deleted.
-    return STATUS(Aborted, "Universe is being deleted");
-  }
-
-  auto* mutable_pb = &l.mutable_data()->pb;
-
-  if (mutable_pb->state() != SysUniverseReplicationEntryPB::INITIALIZING) {
-    VLOG_WITH_FUNC(2) << "Universe replication is in invalid state " << l->pb.state();
-
-    // Replication stream has already been validated, or is in FAILED state which cannot be
-    // recovered.
-    return Status::OK();
-  }
-
-  if (mutable_pb->validated_tables_size() != mutable_pb->tables_size()) {
-    // Replication stream is not yet ready. All the tables have to be validated.
-    return Status::OK();
-  }
-
-  auto master_addresses = mutable_pb->producer_master_addresses();
-  cdc::StreamModeTransactional transactional(mutable_pb->transactional());
-  auto res = universe->GetOrCreateXClusterRpcTasks(master_addresses);
-  if (!res.ok()) {
-    MarkUniverseReplicationFailed(res.status(), &l, universe);
-    return STATUS(
-        InternalError, Format(
-                           "Error while setting up client for producer $0: $1", universe->id(),
-                           res.status().ToString()));
-  }
-  std::shared_ptr<XClusterRpcTasks> xcluster_rpc = *res;
-
-  // Now, all tables are validated.
-  std::vector<TableId> validated_tables;
-  auto& tbl_iter = mutable_pb->tables();
-  validated_tables.insert(validated_tables.begin(), tbl_iter.begin(), tbl_iter.end());
-
-  mutable_pb->set_state(SysUniverseReplicationEntryPB::VALIDATED);
-  // Update sys_catalog.
-  RETURN_ACTION_NOT_OK(
-      sys_catalog_.Upsert(epoch_, universe), "updating universe replication info in sys-catalog");
-  l.Commit();
-
-  // Create xCluster stream for each validated table, after persisting the replication state change.
-  if (!validated_tables.empty()) {
-    // Keep track of the bootstrap_id, table_id, and options of streams to update after
-    // the last GetStreamCallback finishes. Will be updated by multiple async
-    // GetStreamCallback.
-    auto stream_update_infos = std::make_shared<StreamUpdateInfos>();
-    stream_update_infos->reserve(validated_tables.size());
-    auto update_infos_lock = std::make_shared<std::mutex>();
-
-    for (const auto& table : validated_tables) {
-      scoped_refptr<SetupUniverseReplicationHelper> shared_this = this;
-
-      auto producer_bootstrap_id = FindOrNull(table_bootstrap_ids, table);
-      if (producer_bootstrap_id && *producer_bootstrap_id) {
-        auto table_id = std::make_shared<TableId>();
-        auto stream_options = std::make_shared<std::unordered_map<std::string, std::string>>();
-        xcluster_rpc->client()->GetCDCStream(
-            *producer_bootstrap_id, table_id, stream_options,
-            std::bind(
-                &SetupUniverseReplicationHelper::GetStreamCallback, shared_this,
-                *producer_bootstrap_id, table_id, stream_options, universe->ReplicationGroupId(),
-                table, xcluster_rpc, std::placeholders::_1, stream_update_infos,
-                update_infos_lock));
-      } else {
-        // Streams are used as soon as they are created so set state to active.
-        client::XClusterClient(*xcluster_rpc->client())
-            .CreateXClusterStreamAsync(
-                table, /*active=*/true, transactional,
-                std::bind(
-                    &SetupUniverseReplicationHelper::AddStreamToUniverseAndInitConsumer,
-                    shared_this, universe->ReplicationGroupId(), table, std::placeholders::_1,
-                    nullptr /* on_success_cb */));
-      }
-    }
-  }
-  return Status::OK();
-}
-
-void SetupUniverseReplicationHelper::GetStreamCallback(
-    const xrepl::StreamId& bootstrap_id, std::shared_ptr<TableId> table_id,
-    std::shared_ptr<std::unordered_map<std::string, std::string>> options,
-    const xcluster::ReplicationGroupId& replication_group_id, const TableId& table,
-    std::shared_ptr<XClusterRpcTasks> xcluster_rpc, const Status& s,
-    std::shared_ptr<StreamUpdateInfos> stream_update_infos,
-    std::shared_ptr<std::mutex> update_infos_lock) {
-  if (!s.ok()) {
-    LOG(ERROR) << "Unable to find bootstrap id " << bootstrap_id;
-    AddStreamToUniverseAndInitConsumer(replication_group_id, table, s);
-    return;
-  }
-
-  if (*table_id != table) {
-    const Status invalid_bootstrap_id_status = STATUS_FORMAT(
-        InvalidArgument, "Invalid bootstrap id for table $0. Bootstrap id $1 belongs to table $2",
-        table, bootstrap_id, *table_id);
-    LOG(ERROR) << invalid_bootstrap_id_status;
-    AddStreamToUniverseAndInitConsumer(replication_group_id, table, invalid_bootstrap_id_status);
-    return;
-  }
-
-  auto original_universe = catalog_manager_.GetUniverseReplication(replication_group_id);
-
-  if (original_universe == nullptr) {
-    LOG(ERROR) << "Universe not found: " << replication_group_id;
-    return;
-  }
-
-  cdc::StreamModeTransactional transactional(original_universe->LockForRead()->pb.transactional());
-
-  // todo check options
-  {
-    std::lock_guard lock(*update_infos_lock);
-    stream_update_infos->push_back({bootstrap_id, *table_id, *options});
-  }
-
-  const auto update_xrepl_stream_func = [&]() -> Status {
-    // Extra callback on universe setup success - update the producer to let it know that
-    // the bootstrapping is complete. This callback will only be called once among all
-    // the GetStreamCallback calls, and we update all streams in batch at once.
-
-    std::vector<xrepl::StreamId> update_bootstrap_ids;
-    std::vector<SysCDCStreamEntryPB> update_entries;
-    {
-      std::lock_guard lock(*update_infos_lock);
-
-      for (const auto& [update_bootstrap_id, update_table_id, update_options] :
-           *stream_update_infos) {
-        SysCDCStreamEntryPB new_entry;
-        new_entry.add_table_id(update_table_id);
-        new_entry.mutable_options()->Reserve(narrow_cast<int>(update_options.size()));
-        for (const auto& [key, value] : update_options) {
-          if (key == cdc::kStreamState) {
-            // We will set state explicitly.
-            continue;
-          }
-          auto new_option = new_entry.add_options();
-          new_option->set_key(key);
-          new_option->set_value(value);
-        }
-        new_entry.set_state(master::SysCDCStreamEntryPB::ACTIVE);
-        new_entry.set_transactional(transactional);
-
-        update_bootstrap_ids.push_back(update_bootstrap_id);
-        update_entries.push_back(new_entry);
-      }
-    }
-
-    RETURN_NOT_OK_PREPEND(
-        xcluster_rpc->client()->UpdateCDCStream(update_bootstrap_ids, update_entries),
-        "Unable to update xrepl stream options on source universe");
-
-    {
-      std::lock_guard lock(*update_infos_lock);
-      stream_update_infos->clear();
-    }
-    return Status::OK();
-  };
-
-  AddStreamToUniverseAndInitConsumer(
-      replication_group_id, table, bootstrap_id, update_xrepl_stream_func);
-}
-
-void SetupUniverseReplicationHelper::AddStreamToUniverseAndInitConsumer(
-    const xcluster::ReplicationGroupId& replication_group_id, const TableId& table_id,
-    const Result<xrepl::StreamId>& stream_id, std::function<Status()> on_success_cb) {
-  auto universe = catalog_manager_.GetUniverseReplication(replication_group_id);
-  if (universe == nullptr) {
-    LOG(ERROR) << "Universe not found: " << replication_group_id;
-    return;
-  }
-
-  Status s;
-  if (!stream_id.ok()) {
-    s = std::move(stream_id).status();
-  } else {
-    s = AddStreamToUniverseAndInitConsumerInternal(
-        universe, table_id, *stream_id, std::move(on_success_cb));
-  }
-
-  if (!s.ok()) {
-    MarkUniverseReplicationFailed(universe, s);
-  }
-}
-
-Status SetupUniverseReplicationHelper::AddStreamToUniverseAndInitConsumerInternal(
-    scoped_refptr<UniverseReplicationInfo> universe, const TableId& table_id,
-    const xrepl::StreamId& stream_id, std::function<Status()> on_success_cb) {
-  bool merge_alter = false;
-  bool validated_all_tables = false;
-  std::vector<XClusterConsumerStreamInfo> consumer_info;
-  {
-    auto l = universe->LockForWrite();
-    if (l->is_deleted_or_failed()) {
-      // Nothing to do if universe is being deleted.
-      return Status::OK();
-    }
-
-    auto map = l.mutable_data()->pb.mutable_table_streams();
-    (*map)[table_id] = stream_id.ToString();
-
-    // This functions as a barrier: waiting for the last RPC call from GetTableSchemaCallback.
-    if (l.mutable_data()->pb.table_streams_size() == l->pb.tables_size()) {
-      // All tables successfully validated! Register xCluster consumers & start replication.
-      validated_all_tables = true;
-      LOG(INFO) << "Registering xCluster consumers for universe " << universe->id();
-
-      consumer_info.reserve(l->pb.tables_size());
-      std::set<TableId> consumer_table_ids;
-      for (const auto& [producer_table_id, consumer_table_id] : l->pb.validated_tables()) {
-        consumer_table_ids.insert(consumer_table_id);
-
-        XClusterConsumerStreamInfo info;
-        info.producer_table_id = producer_table_id;
-        info.consumer_table_id = consumer_table_id;
-        info.stream_id = VERIFY_RESULT(xrepl::StreamId::FromString((*map)[producer_table_id]));
-        consumer_info.push_back(info);
-      }
-
-      if (l->IsDbScoped()) {
-        std::vector<NamespaceId> consumer_namespace_ids;
-        for (const auto& ns_info : l->pb.db_scoped_info().namespace_infos()) {
-          consumer_namespace_ids.push_back(ns_info.consumer_namespace_id());
-        }
-        RETURN_NOT_OK(ValidateTableListForDbScopedReplication(
-            *universe, consumer_namespace_ids, consumer_table_ids, catalog_manager_));
-      }
-
-      std::vector<HostPort> hp;
-      HostPortsFromPBs(l->pb.producer_master_addresses(), &hp);
-      auto xcluster_rpc_tasks =
-          VERIFY_RESULT(universe->GetOrCreateXClusterRpcTasks(l->pb.producer_master_addresses()));
-      RETURN_NOT_OK(InitXClusterConsumer(
-          consumer_info, HostPort::ToCommaSeparatedString(hp), *universe.get(),
-          xcluster_rpc_tasks));
-
-      if (xcluster::IsAlterReplicationGroupId(universe->ReplicationGroupId())) {
-        // Don't enable ALTER universes, merge them into the main universe instead.
-        // on_success_cb will be invoked in MergeUniverseReplication.
-        merge_alter = true;
-      } else {
-        l.mutable_data()->pb.set_state(SysUniverseReplicationEntryPB::ACTIVE);
-        if (on_success_cb) {
-          // Before updating, run any callbacks on success.
-          RETURN_NOT_OK(on_success_cb());
-        }
-      }
-    }
-
-    // Update sys_catalog with new producer table id info.
-    RETURN_NOT_OK(sys_catalog_.Upsert(epoch_, universe));
-
-    l.Commit();
-  }
-
-  if (!validated_all_tables) {
-    return Status::OK();
-  }
-
-  auto final_id = xcluster::GetOriginalReplicationGroupId(universe->ReplicationGroupId());
-  // If this is an 'alter', merge back into primary command now that setup is a success.
-  if (merge_alter) {
-    RETURN_NOT_OK(MergeUniverseReplication(universe, final_id, std::move(on_success_cb)));
-  }
-  // Update the in-memory cache of consumer tables.
-  for (const auto& info : consumer_info) {
-    xcluster_manager_.RecordTableConsumerStream(info.consumer_table_id, final_id, info.stream_id);
-  }
-
-  return Status::OK();
-}
-
-Status SetupUniverseReplicationHelper::InitXClusterConsumer(
-    const std::vector<XClusterConsumerStreamInfo>& consumer_info, const std::string& master_addrs,
-    UniverseReplicationInfo& replication_info,
-    std::shared_ptr<XClusterRpcTasks> xcluster_rpc_tasks) {
-  auto universe_l = replication_info.LockForRead();
-  auto schema_version_mappings = universe_l->pb.schema_version_mappings();
-
-  // Get the tablets in the consumer table.
-  cdc::ProducerEntryPB producer_entry;
-
-  if (FLAGS_enable_xcluster_auto_flag_validation) {
-    auto compatible_auto_flag_config_version = VERIFY_RESULT(
-        GetAutoFlagConfigVersionIfCompatible(replication_info, master_.GetAutoFlagsConfig()));
-    producer_entry.set_compatible_auto_flag_config_version(compatible_auto_flag_config_version);
-    producer_entry.set_validated_auto_flags_config_version(compatible_auto_flag_config_version);
-  }
-
-  auto cluster_config = catalog_manager_.ClusterConfig();
-  auto l = cluster_config->LockForWrite();
-  auto* consumer_registry = l.mutable_data()->pb.mutable_consumer_registry();
-  auto transactional = universe_l->pb.transactional();
-  if (!xcluster::IsAlterReplicationGroupId(replication_info.ReplicationGroupId())) {
-    if (universe_l->IsDbScoped()) {
-      DCHECK(transactional);
-    }
-  }
-
-  for (const auto& stream_info : consumer_info) {
-    auto consumer_tablet_keys =
-        VERIFY_RESULT(catalog_manager_.GetTableKeyRanges(stream_info.consumer_table_id));
-    auto schema_version =
-        VERIFY_RESULT(catalog_manager_.GetTableSchemaVersion(stream_info.consumer_table_id));
-
-    cdc::StreamEntryPB stream_entry;
-    // Get producer tablets and map them to the consumer tablets
-    RETURN_NOT_OK(InitXClusterStream(
-        stream_info.producer_table_id, stream_info.consumer_table_id, consumer_tablet_keys,
-        &stream_entry, xcluster_rpc_tasks));
-    // Set the validated consumer schema version
-    auto* producer_schema_pb = stream_entry.mutable_producer_schema();
-    producer_schema_pb->set_last_compatible_consumer_schema_version(schema_version);
-    auto* schema_versions = stream_entry.mutable_schema_versions();
-    auto mapping = FindOrNull(schema_version_mappings, stream_info.producer_table_id);
-    SCHECK(mapping, NotFound, Format("No schema mapping for $0", stream_info.producer_table_id));
-    if (IsColocationParentTableId(stream_info.consumer_table_id)) {
-      // Get all the child tables and add their mappings
-      auto& colocated_schema_versions_pb = *stream_entry.mutable_colocated_schema_versions();
-      for (const auto& colocated_entry : mapping->colocated_schema_versions()) {
-        auto colocation_id = colocated_entry.colocation_id();
-        colocated_schema_versions_pb[colocation_id].set_current_producer_schema_version(
-            colocated_entry.schema_version_mapping().producer_schema_version());
-        colocated_schema_versions_pb[colocation_id].set_current_consumer_schema_version(
-            colocated_entry.schema_version_mapping().consumer_schema_version());
-      }
-    } else {
-      schema_versions->set_current_producer_schema_version(
-          mapping->schema_version_mapping().producer_schema_version());
-      schema_versions->set_current_consumer_schema_version(
-          mapping->schema_version_mapping().consumer_schema_version());
-    }
-
-    // Mark this stream as special if it is for the ddl_queue table.
-    auto table_info = catalog_manager_.GetTableInfo(stream_info.consumer_table_id);
-    stream_entry.set_is_ddl_queue_table(
-        table_info->GetTableType() == PGSQL_TABLE_TYPE &&
-        table_info->name() == xcluster::kDDLQueueTableName &&
-        table_info->pgschema_name() == xcluster::kDDLQueuePgSchemaName);
-
-    (*producer_entry.mutable_stream_map())[stream_info.stream_id.ToString()] =
-        std::move(stream_entry);
-  }
-
-  // Log the Network topology of the Producer Cluster
-  auto master_addrs_list = StringSplit(master_addrs, ',');
-  producer_entry.mutable_master_addrs()->Reserve(narrow_cast<int>(master_addrs_list.size()));
-  for (const auto& addr : master_addrs_list) {
-    auto hp = VERIFY_RESULT(HostPort::FromString(addr, 0));
-    HostPortToPB(hp, producer_entry.add_master_addrs());
-  }
-
-  auto* replication_group_map = consumer_registry->mutable_producer_map();
-  SCHECK_EQ(
-      replication_group_map->count(replication_info.id()), 0, InvalidArgument,
-      "Already created a consumer for this universe");
-
-  // TServers will use the ClusterConfig to create xCluster Consumers for applicable local tablets.
-  (*replication_group_map)[replication_info.id()] = std::move(producer_entry);
-
-  l.mutable_data()->pb.set_version(l.mutable_data()->pb.version() + 1);
-  RETURN_NOT_OK(CheckStatus(
-      sys_catalog_.Upsert(epoch_, cluster_config.get()), "updating cluster config in sys-catalog"));
-
-  xcluster_manager_.SyncConsumerReplicationStatusMap(
-      replication_info.ReplicationGroupId(), *replication_group_map);
-  l.Commit();
-
-  xcluster_manager_.CreateXClusterSafeTimeTableAndStartService();
-
-  return Status::OK();
-}
-
-Status SetupUniverseReplicationHelper::MergeUniverseReplication(
-    scoped_refptr<UniverseReplicationInfo> universe, xcluster::ReplicationGroupId original_id,
-    std::function<Status()> on_success_cb) {
-  // Merge back into primary command now that setup is a success.
-  LOG(INFO) << "Merging xCluster ReplicationGroup: " << universe->id() << " into " << original_id;
-
-  SCHECK(
-      !FLAGS_TEST_fail_universe_replication_merge, IllegalState,
-      "TEST_fail_universe_replication_merge");
-
-  auto original_universe = catalog_manager_.GetUniverseReplication(original_id);
-  if (original_universe == nullptr) {
-    LOG(ERROR) << "Universe not found: " << original_id;
-    return Status::OK();
-  }
-
-  {
-    auto cluster_config = catalog_manager_.ClusterConfig();
-    // Acquire Locks in order of Original Universe, Cluster Config, New Universe
-    auto original_lock = original_universe->LockForWrite();
-    auto alter_lock = universe->LockForWrite();
-    auto cl = cluster_config->LockForWrite();
-
-    // Merge Cluster Config for TServers.
-    auto* consumer_registry = cl.mutable_data()->pb.mutable_consumer_registry();
-    auto pm = consumer_registry->mutable_producer_map();
-    auto original_producer_entry = pm->find(original_universe->id());
-    auto alter_producer_entry = pm->find(universe->id());
-    if (original_producer_entry != pm->end() && alter_producer_entry != pm->end()) {
-      // Merge the Tables from the Alter into the original.
-      auto as = alter_producer_entry->second.stream_map();
-      original_producer_entry->second.mutable_stream_map()->insert(as.begin(), as.end());
-      // Delete the Alter
-      pm->erase(alter_producer_entry);
-    } else {
-      LOG(WARNING) << "Could not find both universes in Cluster Config: " << universe->id();
-    }
-    cl.mutable_data()->pb.set_version(cl.mutable_data()->pb.version() + 1);
-
-    // Merge Master Config on Consumer. (no need for Producer changes, since it uses stream_id)
-    // Merge Table->StreamID mapping.
-    auto& alter_pb = alter_lock.mutable_data()->pb;
-    auto& original_pb = original_lock.mutable_data()->pb;
-
-    auto* alter_tables = alter_pb.mutable_tables();
-    original_pb.mutable_tables()->MergeFrom(*alter_tables);
-    alter_tables->Clear();
-    auto* alter_table_streams = alter_pb.mutable_table_streams();
-    original_pb.mutable_table_streams()->insert(
-        alter_table_streams->begin(), alter_table_streams->end());
-    alter_table_streams->clear();
-    auto* alter_validated_tables = alter_pb.mutable_validated_tables();
-    original_pb.mutable_validated_tables()->insert(
-        alter_validated_tables->begin(), alter_validated_tables->end());
-    alter_validated_tables->clear();
-    if (alter_lock.mutable_data()->IsDbScoped()) {
-      auto* alter_namespace_info = alter_pb.mutable_db_scoped_info()->mutable_namespace_infos();
-      original_pb.mutable_db_scoped_info()->mutable_namespace_infos()->MergeFrom(
-          *alter_namespace_info);
-      alter_namespace_info->Clear();
-    }
-
-    alter_pb.set_state(SysUniverseReplicationEntryPB::DELETED);
-
-    if (PREDICT_FALSE(FLAGS_TEST_exit_unfinished_merging)) {
-      return Status::OK();
-    }
-
-    if (on_success_cb) {
-      RETURN_NOT_OK(on_success_cb());
-    }
-
-    {
-      // Need all three updates to be atomic.
-      auto s = CheckStatus(
-          sys_catalog_.Upsert(
-              epoch_, original_universe.get(), universe.get(), cluster_config.get()),
-          "Updating universe replication entries and cluster config in sys-catalog");
-    }
-
-    xcluster_manager_.SyncConsumerReplicationStatusMap(
-        original_universe->ReplicationGroupId(), *pm);
-    xcluster_manager_.SyncConsumerReplicationStatusMap(universe->ReplicationGroupId(), *pm);
-
-    alter_lock.Commit();
-    cl.Commit();
-    original_lock.Commit();
-  }
-
-  // Add alter temp universe to GC.
-  catalog_manager_.MarkUniverseForCleanup(universe->ReplicationGroupId());
-
-  LOG(INFO) << "Done with Merging " << universe->id() << " into " << original_universe->id();
-
-  xcluster_manager_.CreateXClusterSafeTimeTableAndStartService();
-
-  return Status::OK();
-}
-
-Result<scoped_refptr<UniverseReplicationInfo>>
-SetupUniverseReplicationHelper::CreateUniverseReplicationInfo(
-    const xcluster::ReplicationGroupId& replication_group_id,
-    const google::protobuf::RepeatedPtrField<HostPortPB>& master_addresses,
-    const std::vector<NamespaceId>& producer_namespace_ids,
-    const std::vector<NamespaceId>& consumer_namespace_ids,
-    const google::protobuf::RepeatedPtrField<std::string>& table_ids, bool transactional) {
-  SCHECK_EQ(
-      producer_namespace_ids.size(), consumer_namespace_ids.size(), InvalidArgument,
-      "We should have the namespaceIds from both producer and consumer");
-
-  SCHECK(
-      catalog_manager_.GetUniverseReplication(replication_group_id) == nullptr, AlreadyPresent,
-      "Replication group $0 already present", replication_group_id.ToString());
-
-  for (const auto& universe : catalog_manager_.GetAllUniverseReplications()) {
-    for (const auto& consumer_namespace_id : consumer_namespace_ids) {
-      SCHECK_FORMAT(
-          !IncludesConsumerNamespace(*universe, consumer_namespace_id), AlreadyPresent,
-          "Namespace $0 already included in replication group $1", consumer_namespace_id,
-          universe->ReplicationGroupId());
-    }
-  }
-
-  // Create an entry in the system catalog DocDB for this new universe replication.
-  scoped_refptr<UniverseReplicationInfo> ri = new UniverseReplicationInfo(replication_group_id);
-  ri->mutable_metadata()->StartMutation();
-  SysUniverseReplicationEntryPB* metadata = &ri->mutable_metadata()->mutable_dirty()->pb;
-  metadata->set_replication_group_id(replication_group_id.ToString());
-  metadata->mutable_producer_master_addresses()->CopyFrom(master_addresses);
-
-  if (!producer_namespace_ids.empty()) {
-    auto* db_scoped_info = metadata->mutable_db_scoped_info();
-    for (size_t i = 0; i < producer_namespace_ids.size(); i++) {
-      auto* ns_info = db_scoped_info->mutable_namespace_infos()->Add();
-      ns_info->set_producer_namespace_id(producer_namespace_ids[i]);
-      ns_info->set_consumer_namespace_id(consumer_namespace_ids[i]);
-    }
-  }
-  metadata->mutable_tables()->CopyFrom(table_ids);
-  metadata->set_state(SysUniverseReplicationEntryPB::INITIALIZING);
-  metadata->set_transactional(transactional);
-
-  RETURN_NOT_OK(CheckLeaderStatus(
-      sys_catalog_.Upsert(epoch_, ri), "inserting universe replication info into sys-catalog"));
-
-  // Commit the in-memory state now that it's added to the persistent catalog.
-  ri->mutable_metadata()->CommitMutation();
-  LOG(INFO) << "Setup universe replication from producer " << ri->ToString();
-
-  catalog_manager_.InsertNewUniverseReplication(*ri);
-
-  // Make sure the AutoFlags are compatible.
-  // This is done after the replication info is persisted since it performs RPC calls to source
-  // universe and we can crash during this call.
-  // TODO: When new master starts it can retry this step or mark the replication group as failed.
-  if (FLAGS_enable_xcluster_auto_flag_validation) {
-    const auto auto_flags_config = master_.GetAutoFlagsConfig();
-    auto status = ResultToStatus(GetAutoFlagConfigVersionIfCompatible(*ri, auto_flags_config));
-
-    if (!status.ok()) {
-      MarkUniverseReplicationFailed(ri, status);
-      return status.CloneAndAddErrorCode(MasterError(MasterErrorPB::INVALID_REQUEST));
-    }
-
-    auto l = ri->LockForWrite();
-    l.mutable_data()->pb.set_validated_local_auto_flags_config_version(
-        auto_flags_config.config_version());
-
-    RETURN_NOT_OK(CheckLeaderStatus(
-        sys_catalog_.Upsert(epoch_, ri), "inserting universe replication info into sys-catalog"));
-
-    l.Commit();
-  }
-  return ri;
-}
-
-}  // namespace yb::master
diff --git a/src/yb/master/xcluster/xcluster_universe_replication_setup_helper.h b/src/yb/master/xcluster/xcluster_universe_replication_setup_helper.h
index 3ec7f4a946..54fea777d4 100644
--- a/src/yb/master/xcluster/xcluster_universe_replication_setup_helper.h
+++ b/src/yb/master/xcluster/xcluster_universe_replication_setup_helper.h
@@ -13,177 +13,47 @@
 
 #pragma once
 
+#include <google/protobuf/repeated_field.h>
+
 #include "yb/cdc/xcluster_types.h"
-#include "yb/cdc/xrepl_types.h"
-#include "yb/common/common_fwd.h"
-#include "yb/common/entity_ids_types.h"
-#include "yb/gutil/ref_counted.h"
 #include "yb/master/leader_epoch.h"
 #include "yb/master/master_fwd.h"
-#include "yb/util/cow_object.h"
+#include "yb/util/is_operation_done_result.h"
 #include "yb/util/status_fwd.h"
-#include <google/protobuf/repeated_field.h>
 
 namespace yb {
 
-class HostPortPB;
-
-namespace rpc {
-class RpcContext;
-}  // namespace rpc
-
-namespace client {
-struct YBTableInfo;
-class YBTableName;
-}  // namespace client
-
 namespace master {
 
-class GetTableSchemaResponsePB;
-struct PersistentUniverseReplicationInfo;
 class SetupUniverseReplicationRequestPB;
 class SetupUniverseReplicationResponsePB;
-class SysCatalogTable;
-class UniverseReplicationInfo;
-class XClusterRpcTasks;
 
-// Helper class to handle SetupUniverseReplication RPC.
-// This object will only live as long as the operation is in progress.
-class SetupUniverseReplicationHelper : public RefCountedThreadSafe<SetupUniverseReplicationHelper> {
+// Helper class to setup an inbound xCluster replication group.
+// StartSetup should be called to start the task.
+// On a successful setup, a UniverseReplication group will be created.
+class XClusterInboundReplicationGroupSetupTaskIf {
  public:
-  ~SetupUniverseReplicationHelper();
-
-  static Status Setup(
-      Master& master, CatalogManager& catalog_manager, const SetupUniverseReplicationRequestPB* req,
-      SetupUniverseReplicationResponsePB* resp, const LeaderEpoch& epoch);
-
-  static Status ValidateMasterAddressesBelongToDifferentCluster(
-      Master& master, const google::protobuf::RepeatedPtrField<HostPortPB>& master_addresses);
-
- private:
-  SetupUniverseReplicationHelper(
-      Master& master, CatalogManager& catalog_manager, const LeaderEpoch& epoch);
-
-  struct SetupReplicationInfo {
-    std::unordered_map<TableId, xrepl::StreamId> table_bootstrap_ids;
-    bool transactional;
-  };
-
-  // Helper container to track colocationId and the producer to consumer schema version mapping.
-  typedef std::vector<std::tuple<ColocationId, SchemaVersion, SchemaVersion>>
-      ColocationSchemaVersions;
-
-  typedef std::vector<
-      std::tuple<xrepl::StreamId, TableId, std::unordered_map<std::string, std::string>>>
-      StreamUpdateInfos;
-
-  Status SetupUniverseReplication(
-      const SetupUniverseReplicationRequestPB* req, SetupUniverseReplicationResponsePB* resp);
-
-  void MarkUniverseReplicationFailed(
-      scoped_refptr<UniverseReplicationInfo> universe, const Status& failure_status);
-  // Sets the appropriate failure state and the error status on the universe and commits the
-  // mutation to the sys catalog.
-  void MarkUniverseReplicationFailed(
-      const Status& failure_status, CowWriteLock<PersistentUniverseReplicationInfo>* universe_lock,
-      scoped_refptr<UniverseReplicationInfo> universe);
-
-  void GetTableSchemaCallback(
-      const xcluster::ReplicationGroupId& replication_group_id,
-      const std::shared_ptr<client::YBTableInfo>& producer_info,
-      const SetupReplicationInfo& setup_info, const Status& s);
+  virtual ~XClusterInboundReplicationGroupSetupTaskIf() = default;
 
-  Status GetTablegroupSchemaCallbackInternal(
-      scoped_refptr<UniverseReplicationInfo>& universe,
-      const std::vector<client::YBTableInfo>& infos, const TablegroupId& producer_tablegroup_id,
-      const SetupReplicationInfo& setup_info, const Status& s);
+  virtual xcluster::ReplicationGroupId Id() const = 0;
 
-  void GetTablegroupSchemaCallback(
-      const xcluster::ReplicationGroupId& replication_group_id,
-      const std::shared_ptr<std::vector<client::YBTableInfo>>& infos,
-      const TablegroupId& producer_tablegroup_id, const SetupReplicationInfo& setup_info,
-      const Status& s);
+  virtual void StartSetup() = 0;
 
-  void GetColocatedTabletSchemaCallback(
-      const xcluster::ReplicationGroupId& replication_group_id,
-      const std::shared_ptr<std::vector<client::YBTableInfo>>& info,
-      const SetupReplicationInfo& setup_info, const Status& s);
+  virtual IsOperationDoneResult DoneResult() const = 0;
 
-  Status ValidateTableAndCreateStreams(
-      scoped_refptr<UniverseReplicationInfo> universe,
-      const std::shared_ptr<client::YBTableInfo>& producer_info,
-      const SetupReplicationInfo& setup_info);
-
-  // Validates a single table's schema with the corresponding table on the consumer side, and
-  // updates consumer_table_id with the new table id. Return the consumer table schema if the
-  // validation is successful.
-  Status ValidateTableSchemaForXCluster(
-      const client::YBTableInfo& info, const SetupReplicationInfo& setup_info,
-      GetTableSchemaResponsePB* resp);
-
-  // Consumer API: Find out if bootstrap is required for the Producer tables.
-  Status IsBootstrapRequiredOnProducer(
-      scoped_refptr<UniverseReplicationInfo> universe, const TableId& producer_table,
-      const std::unordered_map<TableId, xrepl::StreamId>& table_bootstrap_ids);
-
-  Status AddValidatedTableAndCreateStreams(
-      scoped_refptr<UniverseReplicationInfo> universe,
-      const std::unordered_map<TableId, xrepl::StreamId>& table_bootstrap_ids,
-      const TableId& producer_table, const TableId& consumer_table,
-      const ColocationSchemaVersions& colocated_schema_versions);
-
-  // Adds a validated table to the sys catalog table map for the given universe
-  Status AddValidatedTableToUniverseReplication(
-      scoped_refptr<UniverseReplicationInfo> universe, const TableId& producer_table,
-      const TableId& consumer_table, const SchemaVersion& producer_schema_version,
-      const SchemaVersion& consumer_schema_version,
-      const ColocationSchemaVersions& colocated_schema_versions);
-
-  // If all tables have been validated, creates a CDC stream for each table.
-  Status CreateStreamsIfReplicationValidated(
-      scoped_refptr<UniverseReplicationInfo> universe,
-      const std::unordered_map<TableId, xrepl::StreamId>& table_bootstrap_ids);
-
-  void GetStreamCallback(
-      const xrepl::StreamId& bootstrap_id, std::shared_ptr<TableId> table_id,
-      std::shared_ptr<std::unordered_map<std::string, std::string>> options,
-      const xcluster::ReplicationGroupId& replication_group_id, const TableId& table,
-      std::shared_ptr<XClusterRpcTasks> xcluster_rpc, const Status& s,
-      std::shared_ptr<StreamUpdateInfos> stream_update_infos,
-      std::shared_ptr<std::mutex> update_infos_lock);
-
-  void AddStreamToUniverseAndInitConsumer(
-      const xcluster::ReplicationGroupId& replication_group_id, const TableId& table,
-      const Result<xrepl::StreamId>& stream_id, std::function<Status()> on_success_cb = nullptr);
-
-  Status AddStreamToUniverseAndInitConsumerInternal(
-      scoped_refptr<UniverseReplicationInfo> universe, const TableId& table,
-      const xrepl::StreamId& stream_id, std::function<Status()> on_success_cb);
-
-  Status InitXClusterConsumer(
-      const std::vector<XClusterConsumerStreamInfo>& consumer_info, const std::string& master_addrs,
-      UniverseReplicationInfo& replication_info,
-      std::shared_ptr<XClusterRpcTasks> xcluster_rpc_tasks);
-
-  Status MergeUniverseReplication(
-      scoped_refptr<UniverseReplicationInfo> info, xcluster::ReplicationGroupId original_id,
-      std::function<Status()> on_success_cb);
-
-  Result<scoped_refptr<UniverseReplicationInfo>> CreateUniverseReplicationInfo(
-      const xcluster::ReplicationGroupId& replication_group_id,
-      const google::protobuf::RepeatedPtrField<HostPortPB>& master_addresses,
-      const std::vector<NamespaceId>& producer_namespace_ids,
-      const std::vector<NamespaceId>& consumer_namespace_ids,
-      const google::protobuf::RepeatedPtrField<std::string>& table_ids, bool transactional);
+  // Returns true if the setup was cancelled or it already failed, and false if the setup
+  // already completed successfully.
+  virtual bool TryCancel() = 0;
+};
 
-  Master& master_;
-  CatalogManager& catalog_manager_;
-  SysCatalogTable& sys_catalog_;
-  XClusterManager& xcluster_manager_;
-  const LeaderEpoch epoch_;
+Result<std::shared_ptr<XClusterInboundReplicationGroupSetupTaskIf>>
+CreateSetupUniverseReplicationTask(
+    Master& master, CatalogManager& catalog_manager, const SetupUniverseReplicationRequestPB* req,
+    const LeaderEpoch& epoch);
 
-  DISALLOW_COPY_AND_ASSIGN(SetupUniverseReplicationHelper);
-};
+Status ValidateMasterAddressesBelongToDifferentCluster(
+    Master& master, const google::protobuf::RepeatedPtrField<HostPortPB>& master_addresses);
 
 }  // namespace master
+
 }  // namespace yb
diff --git a/src/yb/master/xcluster_consumer_registry_service.cc b/src/yb/master/xcluster_consumer_registry_service.cc
index 313828a7c1..c147d03ba6 100644
--- a/src/yb/master/xcluster_consumer_registry_service.cc
+++ b/src/yb/master/xcluster_consumer_registry_service.cc
@@ -18,15 +18,11 @@
 #include "yb/master/xcluster_rpc_tasks.h"
 #include "yb/master/master_client.pb.h"
 #include "yb/master/master_ddl.pb.h"
-#include "yb/master/master_util.h"
 
 #include "yb/cdc/cdc_consumer.pb.h"
-#include "yb/client/client.h"
-#include "yb/client/yb_table_name.h"
 #include "yb/common/wire_protocol.h"
 #include "yb/dockv/partition.h"
 
-#include "yb/util/random_util.h"
 #include "yb/util/result.h"
 #include "yb/util/status_format.h"
 
@@ -174,17 +170,10 @@ Status ValidateKeyRanges(const std::map<std::string, KeyRange>& tablet_keys) {
   return Status::OK();
 }
 
-Status InitXClusterStream(
+Status PopulateXClusterStreamEntryTabletMapping(
     const std::string& producer_table_id, const std::string& consumer_table_id,
     const std::map<std::string, KeyRange>& consumer_tablet_keys, cdc::StreamEntryPB* stream_entry,
-    std::shared_ptr<XClusterRpcTasks> xcluster_rpc_tasks) {
-  // Get the tablets in the producer table.
-  auto producer_table_locations =
-      VERIFY_RESULT(xcluster_rpc_tasks->GetTableLocations(producer_table_id));
-
-  stream_entry->set_consumer_table_id(consumer_table_id);
-  stream_entry->set_producer_table_id(producer_table_id);
-
+    const google::protobuf::RepeatedPtrField<TabletLocationsPB>& producer_table_locations) {
   auto producer_tablet_keys = GetTabletKeys(producer_table_locations);
   RETURN_NOT_OK_PREPEND(ValidateKeyRanges(producer_tablet_keys), "Producer key ranges invalid");
   RETURN_NOT_OK_PREPEND(ValidateKeyRanges(consumer_tablet_keys), "Consumer key ranges invalid");
diff --git a/src/yb/master/xcluster_consumer_registry_service.h b/src/yb/master/xcluster_consumer_registry_service.h
index d270b6ee10..92f1cfe911 100644
--- a/src/yb/master/xcluster_consumer_registry_service.h
+++ b/src/yb/master/xcluster_consumer_registry_service.h
@@ -13,43 +13,38 @@
 
 #pragma once
 
-#include <vector>
-#include <unordered_set>
-
-#include "yb/cdc/cdc_types.h"
-
-#include "yb/master/master_fwd.h"
+#include <map>
+#include <string>
+#include <google/protobuf/repeated_field.h>
 
+#include "yb/common/entity_ids_types.h"
+#include "yb/master/catalog_entity_info.h"
 #include "yb/util/status_fwd.h"
-#include "yb/util/net/net_util.h"
 
 namespace yb {
 namespace cdc {
-
 class StreamEntryPB;
-
 }  // namespace cdc
 
+namespace client {
+class YBClient;
+}  // namespace client
+
 namespace master {
 
 class ListTablesResponsePB;
 class GetTableLocationsResponsePB;
+class TabletLocationsPB;
 
 struct KeyRange {
   std::string start_key;
   std::string end_key;
 };
 
-struct XClusterConsumerStreamInfo {
-  xrepl::StreamId stream_id = xrepl::StreamId::Nil();
-  TableId consumer_table_id;
-  TableId producer_table_id;
-};
-
-Status InitXClusterStream(
+Status PopulateXClusterStreamEntryTabletMapping(
     const TableId& producer_table_id, const TableId& consumer_table_id,
     const std::map<std::string, KeyRange>& consumer_tablet_keys, cdc::StreamEntryPB* stream_entry,
-    std::shared_ptr<XClusterRpcTasks> xcluster_rpc_tasks);
+    const google::protobuf::RepeatedPtrField<TabletLocationsPB>& producer_table_locations);
 
 Status UpdateTabletMappingOnConsumerSplit(
     const std::map<std::string, KeyRange>& consumer_tablet_keys,
diff --git a/src/yb/server/monitored_task.h b/src/yb/server/monitored_task.h
index c2c3b3d57e..63caa58f11 100644
--- a/src/yb/server/monitored_task.h
+++ b/src/yb/server/monitored_task.h
@@ -92,7 +92,9 @@ YB_DEFINE_ENUM(MonitoredTaskType,
   (kAddNamespaceToXClusterSource)
   (kNamespaceVerification)
   (TableSchemaVerification)
-  (kObjectLock));
+  (kObjectLock)
+  (kXClusterInboundReplicationGroupSetup)
+  (kXClusterTableSetup));
 
 class MonitoredTask : public std::enable_shared_from_this<MonitoredTask> {
  public:
