diff --git a/src/yb/client/ql-stress-test.cc b/src/yb/client/ql-stress-test.cc
index 40fd87eb51..01f2564bab 100644
--- a/src/yb/client/ql-stress-test.cc
+++ b/src/yb/client/ql-stress-test.cc
@@ -34,10 +34,9 @@
 #include "yb/consensus/retryable_requests.h"
 
 #include "yb/docdb/consensus_frontier.h"
-#include "yb/dockv/doc_key.h"
 #include "yb/docdb/docdb_rocksdb_util.h"
+#include "yb/dockv/doc_key.h"
 
-#include "yb/rocksdb/metadata.h"
 #include "yb/rocksdb/utilities/checkpoint.h"
 
 #include "yb/rpc/messenger.h"
@@ -108,8 +107,7 @@ using yb::docdb::InitRocksDBOptions;
 
 DECLARE_bool(enable_ysql);
 
-namespace yb {
-namespace client {
+namespace yb::client {
 
 namespace {
 
@@ -940,14 +938,14 @@ void QLStressTest::TestWriteRejection() {
   }, 30s, "Waiting tablets to sync up"));
 }
 
-typedef QLStressTestDelayWrite<4, 10> QLStressTestDelayWrite_4_10;
+using QLStressTestDelayWrite_4_10 = QLStressTestDelayWrite<4, 10>;
 
 TEST_F_EX(QLStressTest, DelayWrite, QLStressTestDelayWrite_4_10) {
   TestWriteRejection();
 }
 
 // Soft limit == hard limit to test write stop and recover after it.
-typedef QLStressTestDelayWrite<6, 6> QLStressTestDelayWrite_6_6;
+using QLStressTestDelayWrite_6_6 = QLStressTestDelayWrite<6, 6>;
 
 TEST_F_EX(QLStressTest, WriteStop, QLStressTestDelayWrite_6_6) {
   TestWriteRejection();
@@ -985,22 +983,30 @@ TEST_F_EX(QLStressTest, LongRemoteBootstrap, QLStressTestLongRemoteBootstrap) {
 
   std::this_thread::sleep_for(20s); // Wait some time to have logs.
 
-  ASSERT_OK(WaitFor([this]() -> Result<bool> {
-    RETURN_NOT_OK(cluster_->CleanTabletLogs());
-    auto leaders = ListTabletPeers(cluster_.get(), ListPeersFilter::kLeaders);
-    if (leaders.empty()) {
-      return false;
-    }
-
-    RETURN_NOT_OK(VERIFY_RESULT(leaders.front()->shared_tablet())->Flush(tablet::FlushMode::kSync));
-    RETURN_NOT_OK(leaders.front()->RunLogGC());
+  ASSERT_OK(WaitFor(
+      [this]() -> Result<bool> {
+        RETURN_NOT_OK(cluster_->CleanTabletLogs());
+        auto leaders = ListTabletPeers(cluster_.get(), ListPeersFilter::kLeaders);
+        if (leaders.empty()) {
+          return false;
+        }
 
-    // Check that first log was garbage collected, so remote bootstrap will be required.
-    consensus::ReplicateMsgs replicates;
-    int64_t starting_op_segment_seq_num;
-    return !leaders.front()->log()->GetLogReader()->ReadReplicatesInRange(
-        100, 101, 0, &replicates, &starting_op_segment_seq_num).ok();
-  }, 30s, "Logs cleaned"));
+        RETURN_NOT_OK(
+            VERIFY_RESULT(leaders.front()->shared_tablet())->Flush(tablet::FlushMode::kSync));
+        RETURN_NOT_OK(leaders.front()->RunLogGC());
+
+        // Check that first log was garbage collected, so remote bootstrap will be required.
+        consensus::ReplicateMsgs replicates;
+        int64_t starting_op_segment_seq_num;
+        return !leaders.front()
+                    ->log()
+                    ->GetLogReader()
+                    ->ReadReplicatesInRange(
+                        100, 101, 0, log::ObeyMemoryLimit::kFalse, &replicates,
+                        &starting_op_segment_seq_num)
+                    .ok();
+      },
+      30s, "Logs cleaned"));
 
   LOG(INFO) << "Bring replica back, keys written: " << key.load(std::memory_order_acquire);
   ASSERT_OK(cluster_->mini_tablet_server(0)->Start(tserver::WaitTabletsBootstrapped::kFalse));
@@ -1203,5 +1209,4 @@ TEST_F_EX(QLStressTest, SyncOldLeader, QLStressTestSingleTablet) {
   thread_holder.Stop();
 }
 
-} // namespace client
-} // namespace yb
+} // namespace yb::client
diff --git a/src/yb/consensus/consensus_queue.cc b/src/yb/consensus/consensus_queue.cc
index 54abf8301b..34a773d551 100644
--- a/src/yb/consensus/consensus_queue.cc
+++ b/src/yb/consensus/consensus_queue.cc
@@ -32,8 +32,6 @@
 
 #include "yb/consensus/consensus_queue.h"
 
-#include <shared_mutex>
-
 #include <algorithm>
 #include <mutex>
 #include <string>
@@ -43,6 +41,7 @@
 
 #include "yb/cdc/cdc_error.h"
 
+#include "yb/common/tablespace_parser.h"
 #include "yb/consensus/consensus.messages.h"
 #include "yb/consensus/consensus_context.h"
 #include "yb/consensus/log_util.h"
@@ -60,20 +59,17 @@
 #include "yb/util/enums.h"
 #include "yb/util/fault_injection.h"
 #include "yb/util/flags.h"
-#include "yb/util/flag_validators.h"
-#include "yb/util/locks.h"
 #include "yb/util/logging.h"
 #include "yb/util/mem_tracker.h"
 #include "yb/util/metrics.h"
 #include "yb/util/monotime.h"
 #include "yb/util/random_util.h"
 #include "yb/util/result.h"
+#include "yb/util/shared_lock.h"
 #include "yb/util/size_literals.h"
 #include "yb/util/status_log.h"
-#include "yb/util/threadpool.h"
 #include "yb/util/tostring.h"
 #include "yb/util/url-coding.h"
-#include "yb/util/shared_lock.h"
 
 using namespace std::literals;
 using namespace yb::size_literals;
@@ -137,8 +133,7 @@ DEFINE_RUNTIME_uint32(cdcsdk_wal_reads_deadline_buffer_secs, 5,
     "This flag determines the buffer time from the deadline at which we must stop reading the WAL "
     "messages and start processing the records we have read till now.");
 
-namespace yb {
-namespace consensus {
+namespace yb::consensus {
 
 using log::Log;
 using std::unique_ptr;
@@ -229,7 +224,7 @@ void PeerMessageQueue::Init(const OpId& last_locally_replicated) {
   }  // Ensure that the queue_lock_ is released.
 
   if (context_) {
-    context_->ListenNumSSTFilesChanged(std::bind(&PeerMessageQueue::NumSSTFilesChanged, this));
+    context_->ListenNumSSTFilesChanged([this] { NumSSTFilesChanged(); });
     installed_num_sst_files_changed_listener_ = true;
   }
 
@@ -605,9 +600,11 @@ Status PeerMessageQueue::RequestForPeer(const string& uuid,
   if (!is_new && num_log_ops_to_send > 0) {
     // The batch of messages to send to the peer.
     auto max_batch_size = FLAGS_consensus_max_batch_size_bytes - request->SerializedSize();
-    auto to_index = num_log_ops_to_send == kSendUnboundedLogOps ?
-        0 : previously_sent_index + num_log_ops_to_send;
-    auto result = ReadFromLogCache(previously_sent_index, to_index, max_batch_size, uuid);
+    auto to_index = num_log_ops_to_send == kSendUnboundedLogOps
+                        ? 0
+                        : previously_sent_index + num_log_ops_to_send;
+    auto result = ReadFromLogCache(
+        previously_sent_index, to_index, max_batch_size, uuid, log::ObeyMemoryLimit::kFalse);
 
     if (PREDICT_FALSE(!result.ok())) {
       if (PREDICT_TRUE(result.status().IsNotFound())) {
@@ -690,17 +687,22 @@ Status PeerMessageQueue::RequestForPeer(const string& uuid,
 
 Result<ReadOpsResult> PeerMessageQueue::ReadFromLogCache(
     int64_t after_index, int64_t to_index, size_t max_batch_size, const std::string& peer_uuid,
-    const CoarseTimePoint deadline, const bool fetch_single_entry) {
+    log::ObeyMemoryLimit obey_memory_limit, const CoarseTimePoint deadline,
+    const bool fetch_single_entry) {
   DCHECK_LT(FLAGS_consensus_max_batch_size_bytes + 1_KB, FLAGS_rpc_max_message_size);
 
   // We try to get the follower's next_index from our log.
   // Note this is not using "term" and needs to change
-  auto result =
-      log_cache_.ReadOps(after_index, to_index, max_batch_size, deadline, fetch_single_entry);
+  auto result = log_cache_.ReadOps(
+      after_index, to_index, max_batch_size, obey_memory_limit, deadline, fetch_single_entry);
   if (PREDICT_FALSE(!result.ok())) {
     auto s = result.status();
     if (PREDICT_TRUE(s.IsNotFound())) {
       return s;
+    } else if (s.IsBusy()) {
+      // We have insufficient memory to read any WAL entries at the moment; caller will need to
+      // retry.
+      return s;
     } else if (s.IsIncomplete()) {
       // IsIncomplete() means that we tried to read beyond the head of the log (in the future). This
       // is usually a sign that this peer is under load and is about to step down as leader. See
@@ -710,9 +712,9 @@ Result<ReadOpsResult> PeerMessageQueue::ReadFromLogCache(
           << ". Destination peer: " << peer_uuid;
       return s;
     } else {
-      LOG_WITH_PREFIX(FATAL) << "Error reading the log while preparing peer request: "
-                                      << s.ToString() << ". Destination peer: "
-                                      << peer_uuid;
+      LOG_WITH_PREFIX(FATAL)
+          << "Error reading the log while preparing peer request or GetChanges response: "
+          << s.ToString() << ". Destination peer: " << peer_uuid;
       return s;
     }
   }
@@ -730,13 +732,14 @@ int64_t PeerMessageQueue::GetStartOpIdIndex(int64_t start_index) {
 }
 
 Result<ReadOpsResult> PeerMessageQueue::ReadFromLogCacheForXRepl(
-    int64_t last_op_id_index, int64_t to_index, CoarseTimePoint deadline, bool fetch_single_entry) {
+    int64_t last_op_id_index, int64_t to_index, log::ObeyMemoryLimit obey_memory_limit,
+    CoarseTimePoint deadline, bool fetch_single_entry) {
   // If an empty OpID is only sent on the first read request, start at the earliest known entry.
   int64_t after_op_index = GetStartOpIdIndex(last_op_id_index);
 
   auto result = ReadFromLogCache(
-      after_op_index, to_index, FLAGS_consensus_max_batch_size_bytes, local_peer_uuid_, deadline,
-      fetch_single_entry);
+      after_op_index, to_index, FLAGS_consensus_max_batch_size_bytes, local_peer_uuid_,
+      log::ObeyMemoryLimit::kTrue, deadline, fetch_single_entry);
   if (PREDICT_FALSE(!result.ok()) && PREDICT_TRUE(result.status().IsNotFound())) {
     const std::string premature_gc_warning = Format(
         "The logs from index $0 have been garbage collected and cannot be read ", after_op_index);
@@ -766,8 +769,16 @@ Result<XClusterReadOpsResult> PeerMessageQueue::ReadReplicatedMessagesForXCluste
     return xcluster_result;
   }
 
-  xcluster_result.result = VERIFY_RESULT(
-      ReadFromLogCacheForXRepl(last_op_id.index, committed_index, deadline, fetch_single_entry));
+  auto read_result = ReadFromLogCacheForXRepl(
+      last_op_id.index, committed_index, log::ObeyMemoryLimit::kTrue, deadline, fetch_single_entry);
+  if (!read_result) {
+    if (read_result.status().IsBusy()) {
+      xcluster_result.result.have_more_messages = HaveMoreMessages(true);
+      return xcluster_result;
+    }
+    return read_result.status();
+  }
+  xcluster_result.result = *read_result;
 
   xcluster_result.result.have_more_messages =
       HaveMoreMessages(xcluster_result.result.have_more_messages.get() || pending_messages);
@@ -804,8 +815,9 @@ Result<ReadOpsResult> PeerMessageQueue::ReadReplicatedMessagesForCDC(
     };
   }
 
-  auto result = VERIFY_RESULT(
-      ReadFromLogCacheForXRepl(last_op_id.index, to_index, deadline, fetch_single_entry));
+  // TODO(#28779): Switch this to obeying the memory limit.
+  auto result = VERIFY_RESULT(ReadFromLogCacheForXRepl(
+      last_op_id.index, to_index, log::ObeyMemoryLimit::kFalse, deadline, fetch_single_entry));
 
   result.have_more_messages =
       HaveMoreMessages(result.have_more_messages.get() || pending_messages);
@@ -856,8 +868,10 @@ Result<ReadOpsResult> PeerMessageQueue::ReadReplicatedMessagesForConsistentCDC(
       }
     }
 
+  // TODO(#28779): Switch this to obeying the memory limit.
     auto result = VERIFY_RESULT(ReadFromLogCacheForXRepl(
-        last_op_id.index, committed_op_id_index, deadline, fetch_single_entry));
+        last_op_id.index, committed_op_id_index, log::ObeyMemoryLimit::kFalse, deadline,
+        fetch_single_entry));
 
     res.messages.insert(res.messages.end(), result.messages.begin(), result.messages.end());
     res.read_from_disk_size += result.read_from_disk_size;
@@ -981,8 +995,10 @@ Result<ReadOpsResult> PeerMessageQueue::ReadReplicatedMessagesInSegmentForCDC(
 
   // Read the ops from the segment starting from current_index + 1.
   while (current_index < segment_last_index) {
-    auto result = VERIFY_RESULT(
-        ReadFromLogCacheForXRepl(current_index, segment_last_index, deadline, fetch_single_entry));
+    // TODO(#28779): Switch this to obeying the memory limit.
+    auto result = VERIFY_RESULT(ReadFromLogCacheForXRepl(
+        current_index, segment_last_index, log::ObeyMemoryLimit::kFalse, deadline,
+        fetch_single_entry));
 
     read_ops.read_from_disk_size += result.read_from_disk_size;
     read_ops.messages.insert(
@@ -1299,7 +1315,7 @@ typename Policy::result_type PeerMessageQueue::GetWatermark() {
 
 CoarseTimePoint PeerMessageQueue::LeaderLeaseExpirationWatermark() {
   struct Policy {
-    typedef CoarseTimePoint result_type;
+    using result_type = CoarseTimePoint;
     // Workaround for a gcc bug. That does not understand that Comparator is actually being used.
     __attribute__((unused)) typedef std::less<result_type> Comparator;
 
@@ -1326,7 +1342,7 @@ CoarseTimePoint PeerMessageQueue::LeaderLeaseExpirationWatermark() {
 
 MicrosTime PeerMessageQueue::HybridTimeLeaseExpirationWatermark() {
   struct Policy {
-    typedef MicrosTime result_type;
+    using result_type = MicrosTime;
     // Workaround for a gcc bug. That does not understand that Comparator is actually being used.
     __attribute__((unused)) typedef std::less<result_type> Comparator;
 
@@ -1352,7 +1368,7 @@ MicrosTime PeerMessageQueue::HybridTimeLeaseExpirationWatermark() {
 
 uint64_t PeerMessageQueue::NumSSTFilesWatermark() {
   struct Policy {
-    typedef uint64_t result_type;
+    using result_type = uint64_t;
     // Workaround for a gcc bug. That does not understand that Comparator is actually being used.
     __attribute__((unused)) typedef std::greater<result_type> Comparator;
 
@@ -1375,7 +1391,7 @@ uint64_t PeerMessageQueue::NumSSTFilesWatermark() {
 
 OpId PeerMessageQueue::OpIdWatermark() {
   struct Policy {
-    typedef OpId result_type;
+    using result_type = OpId;
 
     static result_type NotEnoughPeersValue() {
       return OpId::Min();
@@ -1919,7 +1935,7 @@ string PeerMessageQueue::FindBestNewLeader() const {
   }
 
   if (candidates.empty()) {
-    return string();
+    return {};
   }
   // Choose randomly among candidates at the same op id.
   return RandomElement(candidates);
@@ -1994,5 +2010,4 @@ void PeerMessageQueue::TEST_WaitForNotificationToFinish() {
   notifications_strand_->TEST_BusyWait();
 }
 
-}  // namespace consensus
-}  // namespace yb
+} // namespace yb::consensus
diff --git a/src/yb/consensus/consensus_queue.h b/src/yb/consensus/consensus_queue.h
index fe1647da1f..81a59f362f 100644
--- a/src/yb/consensus/consensus_queue.h
+++ b/src/yb/consensus/consensus_queue.h
@@ -33,8 +33,6 @@
 #pragma once
 
 #include <iosfwd>
-#include <map>
-#include <set>
 #include <string>
 #include <unordered_map>
 #include <utility>
@@ -42,7 +40,6 @@
 
 #include "yb/common/entity_ids_types.h"
 #include "yb/common/hybrid_time.h"
-#include "yb/common/tablespace_parser.h"
 
 #include "yb/consensus/consensus_fwd.h"
 #include "yb/consensus/consensus_types.h"
@@ -52,12 +49,12 @@
 
 #include "yb/gutil/ref_counted.h"
 
-#include "yb/server/clock.h"
-
 #include "yb/rpc/strand.h"
 
-#include "yb/util/status_fwd.h"
+#include "yb/server/clock.h"
+
 #include "yb/util/locks.h"
+#include "yb/util/status_fwd.h"
 
 namespace yb {
 template<class T>
@@ -392,6 +389,8 @@ class PeerMessageQueue {
     return local_peer_pb_.cloud_info();
   }
 
+  // If we are unable to read any WAL entries due to memory limits, returns no operations with
+  // have_more_messages true.
   Result<XClusterReadOpsResult> ReadReplicatedMessagesForXCluster(
       const yb::OpId& last_op_id, const CoarseTimePoint deadline, bool fetch_single_entry);
 
@@ -523,7 +522,7 @@ class PeerMessageQueue {
   template <class Func>
   void NotifyObservers(const char* title, Func&& func);
 
-  typedef std::unordered_map<std::string, TrackedPeer*> PeersMap;
+  using PeersMap = std::unordered_map<std::string, TrackedPeer *>;
 
   std::string ToStringUnlocked() const;
 
@@ -579,17 +578,19 @@ class PeerMessageQueue {
   // Reads operations from the log cache in the range (after_index, to_index].
   //
   // If 'to_index' is 0, then all operations after 'after_index' will be included.
+  //
+  // May return status Busy if obey_memory_limit is true and reading even one operation would exceed
+  // the log reader memory tracker limit.
   Result<ReadOpsResult> ReadFromLogCache(
-      int64_t after_index,
-      int64_t to_index,
-      size_t max_batch_size,
-      const std::string& peer_uuid,
-      const CoarseTimePoint deadline = CoarseTimePoint::max(),
-      const bool fetch_single_entry = false);
+      int64_t after_index, int64_t to_index, size_t max_batch_size, const std::string& peer_uuid,
+      log::ObeyMemoryLimit obey_memory_limit,
+      const CoarseTimePoint deadline = CoarseTimePoint::max(), bool fetch_single_entry = false);
 
+  // May return status Busy if obey_memory_limit is true and reading even one operation would exceed
+  // the log reader memory tracker limit.
   Result<ReadOpsResult> ReadFromLogCacheForXRepl(
-      int64_t last_op_id_index, int64_t to_index, CoarseTimePoint deadline = CoarseTimePoint::max(),
-      bool fetch_single_entry = false);
+      int64_t last_op_id_index, int64_t to_index, log::ObeyMemoryLimit obey_memory_limit,
+      CoarseTimePoint deadline = CoarseTimePoint::max(), bool fetch_single_entry = false);
 
   std::pair<int64_t, int64_t> GetCommittedAndMajorityReplicatedIndex();
 
diff --git a/src/yb/consensus/log-test-base.h b/src/yb/consensus/log-test-base.h
index 3438081e97..fdc9592d89 100644
--- a/src/yb/consensus/log-test-base.h
+++ b/src/yb/consensus/log-test-base.h
@@ -176,8 +176,10 @@ class LogTestBase : public YBTest {
     ASSERT_OK(fs_manager_->CreateInitialFileSystemLayout());
   }
 
-  void BuildLog() {
+  void BuildLog(int64_t byte_limit = -1) {
     Schema schema_with_ids = SchemaBuilder(schema_).Build();
+    read_wal_mem_tracker_ =
+        MemTracker::FindOrCreateTracker(byte_limit, "Log Reader Memory");
     ASSERT_OK(Log::Open(options_,
                        kTestTablet,
                        tablet_wal_path_,
@@ -186,7 +188,7 @@ class LogTestBase : public YBTest {
                        0, // schema_version
                        table_metric_entity_.get(),
                        tablet_metric_entity_.get(),
-                       /*read_wal_mem_tracker=*/nullptr,
+                       read_wal_mem_tracker_,
                        log_thread_pool_.get(),
                        log_thread_pool_.get(),
                        log_thread_pool_.get(),
@@ -353,6 +355,7 @@ class LogTestBase : public YBTest {
   std::unique_ptr<MetricRegistry> metric_registry_;
   scoped_refptr<MetricEntity> table_metric_entity_;
   scoped_refptr<MetricEntity> tablet_metric_entity_;
+  std::shared_ptr<MemTracker> read_wal_mem_tracker_;
   std::unique_ptr<ThreadPool> log_thread_pool_;
   scoped_refptr<Log> log_;
   int64_t current_index_;
diff --git a/src/yb/consensus/log-test.cc b/src/yb/consensus/log-test.cc
index 1879d91492..1eb4fc3d26 100644
--- a/src/yb/consensus/log-test.cc
+++ b/src/yb/consensus/log-test.cc
@@ -58,15 +58,17 @@
 DEFINE_NON_RUNTIME_int32(num_batches, 10000,
              "Number of batches to write to/read from the Log in TestWriteManyBatches");
 
-DECLARE_int32(log_min_segments_to_retain);
+DECLARE_bool(TEST_simulate_abrupt_server_restart);
+DECLARE_bool(TEST_skip_file_close);
 DECLARE_bool(never_fsync);
 DECLARE_bool(writable_file_use_fsync);
+
+DECLARE_int32(log_min_segments_to_retain);
+DECLARE_int32(min_segment_size_bytes_to_rollover_at_flush);
 DECLARE_int32(o_direct_block_alignment_bytes);
 DECLARE_int32(o_direct_block_size_bytes);
-DECLARE_bool(TEST_simulate_abrupt_server_restart);
-DECLARE_bool(TEST_skip_file_close);
+
 DECLARE_int64(reuse_unclosed_segment_threshold_bytes);
-DECLARE_int32(min_segment_size_bytes_to_rollover_at_flush);
 
 namespace yb::log {
 
@@ -899,7 +901,8 @@ TEST_F(LogTest, TestGCWithLogRunning) {
     ReplicateMsgs repls;
     int64_t starting_op_segment_seq_num;
     Status s = log_->GetLogReader()->ReadReplicatesInRange(
-      1, 2, LogReader::kNoSizeLimit, &repls, &starting_op_segment_seq_num);
+        1, 2, LogReader::kNoSizeLimit, log::ObeyMemoryLimit::kFalse, &repls,
+        &starting_op_segment_seq_num);
     ASSERT_TRUE(s.IsNotFound()) << s.ToString();
   }
 
@@ -1295,9 +1298,9 @@ TEST_F(LogTest, TestReadLogWithReplacedReplicates) {
       {
         SCOPED_TRACE(Substitute("Reading $0-$1", start_index, end_index));
         consensus::ReplicateMsgs repls;
-        ASSERT_OK(reader->ReadReplicatesInRange(start_index, end_index,
-                                                LogReader::kNoSizeLimit, &repls,
-                                                &starting_op_segment_seq_num));
+        ASSERT_OK(reader->ReadReplicatesInRange(
+            start_index, end_index, LogReader::kNoSizeLimit, log::ObeyMemoryLimit::kFalse, &repls,
+            &starting_op_segment_seq_num));
         ASSERT_EQ(end_index - start_index + 1, repls.size());
         auto expected_index = start_index;
         for (const auto& repl : repls) {
@@ -1317,11 +1320,12 @@ TEST_F(LogTest, TestReadLogWithReplacedReplicates) {
       // Test a size-limited read.
       int size_limit = RandomUniformInt(1, 1000);
       {
-        SCOPED_TRACE(Substitute("Reading $0-$1 with size limit $2",
-                                start_index, end_index, size_limit));
+        SCOPED_TRACE(
+            Format("Reading $0-$1 with size limit $2", start_index, end_index, size_limit));
         ReplicateMsgs repls;
-        ASSERT_OK(reader->ReadReplicatesInRange(start_index, end_index, size_limit, &repls,
-                                                &starting_op_segment_seq_num));
+        ASSERT_OK(reader->ReadReplicatesInRange(
+            start_index, end_index, size_limit, log::ObeyMemoryLimit::kFalse, &repls,
+            &starting_op_segment_seq_num));
         ASSERT_LE(repls.size(), end_index - start_index + 1);
         int total_size = 0;
         auto expected_index = start_index;
@@ -1364,12 +1368,57 @@ TEST_F(LogTest, TestReadReplicatesHighIndex) {
   auto* reader = log_->GetLogReader();
   ReplicateMsgs repls;
   int64_t starting_op_segment_seq_num;
-  ASSERT_OK(reader->ReadReplicatesInRange(first_log_index, first_log_index + kSequenceLength - 1,
-                                          LogReader::kNoSizeLimit, &repls,
-                                          &starting_op_segment_seq_num));
+  ASSERT_OK(reader->ReadReplicatesInRange(
+      first_log_index, first_log_index + kSequenceLength - 1, LogReader::kNoSizeLimit,
+      log::ObeyMemoryLimit::kFalse, &repls, &starting_op_segment_seq_num));
   ASSERT_EQ(kSequenceLength, repls.size());
 }
 
+TEST_F(LogTest, TestReadReplicatesWithInsufficientMemory) {
+  const int64_t first_log_index = 1;
+  const int kSequenceLength = 10000;
+
+  BuildLog(10);
+  OpIdPB op_id;
+  op_id.set_term(first_log_index);
+  op_id.set_index(first_log_index);
+  ASSERT_OK(AppendNoOps(&op_id, kSequenceLength));
+
+  // Here the limit is so severe that we can't even read in a single batch; accordingly we expect to
+  // get Status Busy return.
+  auto* reader = log_->GetLogReader();
+  ReplicateMsgs repls;
+  int64_t starting_op_segment_seq_num;
+  auto s = reader->ReadReplicatesInRange(
+      first_log_index, first_log_index + kSequenceLength - 1, LogReader::kNoSizeLimit,
+      log::ObeyMemoryLimit::kTrue, &repls, &starting_op_segment_seq_num);
+  ASSERT_TRUE(s.IsBusy()) << s;
+}
+
+TEST_F(LogTest, TestReadReplicatesWithOnlyPartialMemory) {
+  const int64_t first_log_index = 1;
+  const int kSequenceLength = 10000;
+
+  BuildLog(10000);
+  OpIdPB op_id;
+  op_id.set_term(first_log_index);
+  op_id.set_index(first_log_index);
+  ASSERT_OK(AppendNoOps(&op_id, kSequenceLength));
+
+  // Here the limit is sufficient to get several batches but not all of them.  Accordingly we expect
+  // to get only some of the replicas returned.
+  auto* reader = log_->GetLogReader();
+  ReplicateMsgs repls;
+  int64_t starting_op_segment_seq_num;
+  auto s = reader->ReadReplicatesInRange(
+      first_log_index, first_log_index + kSequenceLength - 1, LogReader::kNoSizeLimit,
+      log::ObeyMemoryLimit::kTrue, &repls, &starting_op_segment_seq_num);
+  ASSERT_OK(s);
+  ASSERT_NE(repls.size(), kSequenceLength);
+  ASSERT_GE(repls.size(), 2);
+
+}
+
 TEST_F(LogTest, AllocateSegmentAndRollOver) {
   constexpr auto kNumIters = 10;
 
diff --git a/src/yb/consensus/log.cc b/src/yb/consensus/log.cc
index b68fc6000c..6286fbfed0 100644
--- a/src/yb/consensus/log.cc
+++ b/src/yb/consensus/log.cc
@@ -1846,8 +1846,7 @@ Result<SegmentOpIdRelation> Log::GetSegmentOpIdRelation(
 }
 
 Result<bool> Log::CopySegmentUpTo(
-    ReadableLogSegment* segment, const std::string& dest_wal_dir,
-    const OpId& max_included_op_id) {
+    ReadableLogSegment* segment, const std::string& dest_wal_dir, const OpId& max_included_op_id) {
   SegmentOpIdRelation relation = VERIFY_RESULT(GetSegmentOpIdRelation(segment, max_included_op_id));
   auto* const env = options_.env;
   const auto sequence_number = segment->header().sequence_number();
diff --git a/src/yb/consensus/log_cache-test.cc b/src/yb/consensus/log_cache-test.cc
index 6dc71421a2..3fc334c819 100644
--- a/src/yb/consensus/log_cache-test.cc
+++ b/src/yb/consensus/log_cache-test.cc
@@ -99,19 +99,21 @@ class LogCacheTest : public YBTest {
 
   void SetUp() override {
     YBTest::SetUp();
-    fs_manager_.reset(new FsManager(env_.get(), GetTestPath("fs_root"), "tserver_test"));
+    fs_manager_ = std::make_unique<FsManager>(env_.get(), GetTestPath("fs_root"), "tserver_test");
     ASSERT_OK(fs_manager_->CreateInitialFileSystemLayout());
     ASSERT_OK(fs_manager_->CheckAndOpenFileSystemRoots());
     ASSERT_OK(ThreadPoolBuilder("log").Build(&log_thread_pool_));
+    read_wal_mem_tracker_ =
+        MemTracker::FindOrCreateTracker(GetReadWalMemoryLimit(), "Log Reader Memory");
     ASSERT_OK(log::Log::Open(log::LogOptions(),
                             kTestTablet,
                             fs_manager_->GetFirstTabletWalDirOrDie(kTestTable, kTestTablet),
                             fs_manager_->uuid(),
                             schema_,
                             /*schema_version=*/0,
-                            /*table_metrics_entity=*/nullptr,
-                            /*tablet_metrics_entity=*/nullptr,
-                            /*read_wal_mem_tracker=*/nullptr,
+                            /*table_metric_entity=*/nullptr,
+                            /*tablet_metric_entity=*/nullptr,
+                            read_wal_mem_tracker_,
                             log_thread_pool_.get(),
                             log_thread_pool_.get(),
                             log_thread_pool_.get(),
@@ -130,12 +132,14 @@ class LogCacheTest : public YBTest {
     // Blow away the memtrackers before creating the new cache.
     cache_.reset();
 
-    cache_.reset(new LogCache(
-        metric_entity_, log_.get(), nullptr /* mem_tracker */, kPeerUuid, kTestTablet));
+    cache_ = std::make_unique<LogCache>(
+        metric_entity_, log_.get(), nullptr /* mem_tracker */, kPeerUuid, kTestTablet);
     cache_->Init(preceding_id);
   }
 
  protected:
+  virtual int64_t GetReadWalMemoryLimit() { return /*no limit*/ -1; }
+
   static void FatalOnError(const Status& s) {
     ASSERT_OK(s);
   }
@@ -162,6 +166,7 @@ class LogCacheTest : public YBTest {
   const Schema schema_;
   MetricRegistry metric_registry_;
   scoped_refptr<MetricEntity> metric_entity_;
+  std::shared_ptr<MemTracker> read_wal_mem_tracker_;
   std::unique_ptr<FsManager> fs_manager_;
   std::unique_ptr<ThreadPool> log_thread_pool_;
   std::unique_ptr<LogCache> cache_;
@@ -177,29 +182,31 @@ TEST_F(LogCacheTest, TestAppendAndGetMessages) {
   ASSERT_GE(cache_->metrics_.size->value(), 5 * kNumMessages);
   ASSERT_OK(log_->WaitUntilAllFlushed());
 
-  auto read_result = ASSERT_RESULT(cache_->ReadOps(0, 8_MB));
+  auto read_result = ASSERT_RESULT(cache_->ReadOps(0, 8_MB, log::ObeyMemoryLimit::kFalse));
   EXPECT_EQ(kNumMessages, read_result.messages.size());
   EXPECT_EQ(OpIdStrForIndex(0), OpIdToString(read_result.preceding_op));
 
   // Get starting in the middle of the cache.
-  read_result = ASSERT_RESULT(cache_->ReadOps(kMessageIndex1, 8_MB));
+  read_result = ASSERT_RESULT(cache_->ReadOps(kMessageIndex1, 8_MB, log::ObeyMemoryLimit::kFalse));
   EXPECT_EQ(kNumMessages - kMessageIndex1, read_result.messages.size());
   EXPECT_EQ(OpIdStrForIndex(kMessageIndex1), OpIdToString(read_result.preceding_op));
   EXPECT_EQ(MakeOpIdForIndex(kMessageIndex1 + 1), OpId::FromPB(read_result.messages[0]->id()));
 
   // Get at the end of the cache.
-  read_result = ASSERT_RESULT(cache_->ReadOps(kNumMessages, 8_MB));
+  read_result = ASSERT_RESULT(cache_->ReadOps(kNumMessages, 8_MB, log::ObeyMemoryLimit::kFalse));
   EXPECT_EQ(0, read_result.messages.size());
   EXPECT_EQ(OpIdStrForIndex(kNumMessages), OpIdToString(read_result.preceding_op));
 
   // Get messages from the beginning until some point in the middle of the cache.
-  read_result = ASSERT_RESULT(cache_->ReadOps(0, kMessageIndex1, 8_MB));
+  read_result =
+      ASSERT_RESULT(cache_->ReadOps(0, kMessageIndex1, 8_MB, log::ObeyMemoryLimit::kFalse));
   EXPECT_EQ(kMessageIndex1, read_result.messages.size());
   EXPECT_EQ(OpIdStrForIndex(0), OpIdToString(read_result.preceding_op));
   EXPECT_EQ(MakeOpIdForIndex(1), OpId::FromPB(read_result.messages[0]->id()));
 
   // Get messages from some point in the middle of the cache until another point.
-  read_result = ASSERT_RESULT(cache_->ReadOps(kMessageIndex1, kMessageIndex2, 8_MB));
+  read_result = ASSERT_RESULT(
+      cache_->ReadOps(kMessageIndex1, kMessageIndex2, 8_MB, log::ObeyMemoryLimit::kFalse));
   EXPECT_EQ(kMessageIndex2 - kMessageIndex1, read_result.messages.size());
   EXPECT_EQ(OpIdStrForIndex(kMessageIndex1), OpIdToString(read_result.preceding_op));
   EXPECT_EQ(MakeOpIdForIndex(kMessageIndex1 + 1), OpId::FromPB(read_result.messages[0]->id()));
@@ -210,7 +217,7 @@ TEST_F(LogCacheTest, TestAppendAndGetMessages) {
 
   // Can still read data that was evicted, since it got written through.
   int start = (kNumMessages / 2) - 10;
-  read_result = ASSERT_RESULT(cache_->ReadOps(start, 8_MB));
+  read_result = ASSERT_RESULT(cache_->ReadOps(start, 8_MB, log::ObeyMemoryLimit::kFalse));
   EXPECT_EQ(kNumMessages - start, read_result.messages.size());
   EXPECT_EQ(OpIdStrForIndex(start), OpIdToString(read_result.preceding_op));
   EXPECT_EQ(MakeOpIdForIndex(start + 1), OpId::FromPB(read_result.messages[0]->id()));
@@ -244,7 +251,7 @@ TEST_F(LogCacheTest, ShouldNotEvictUnsyncedOpFromCache) {
   ASSERT_EQ(cache_->num_cached_ops(), 1);
 
   // Can be read from cache.
-  auto read_result = ASSERT_RESULT(cache_->ReadOps(0, 8_MB));
+  auto read_result = ASSERT_RESULT(cache_->ReadOps(0, 8_MB, log::ObeyMemoryLimit::kFalse));
   EXPECT_EQ(1, read_result.messages.size());
   EXPECT_EQ(OpIdStrForIndex(0), OpIdToString(read_result.preceding_op));
 
@@ -275,15 +282,71 @@ TEST_F(LogCacheTest, TestAlwaysYieldsAtLeastOneMessage) {
   ASSERT_OK(log_->WaitUntilAllFlushed());
 
   // We should get one of them, even though we only ask for 100 bytes
-  auto read_result = ASSERT_RESULT(cache_->ReadOps(0, 100));
+  auto read_result = ASSERT_RESULT(cache_->ReadOps(0, 100, log::ObeyMemoryLimit::kFalse));
   ASSERT_EQ(1, read_result.messages.size());
 
   // Should yield one op also in the 'cache miss' case.
   cache_->EvictThroughOp(50);
-  read_result = ASSERT_RESULT(cache_->ReadOps(0, 100));
+  read_result = ASSERT_RESULT(cache_->ReadOps(0, 100, log::ObeyMemoryLimit::kFalse));
   ASSERT_EQ(1, read_result.messages.size());
 }
 
+class LogCacheLimitTest : public LogCacheTest {
+ protected:
+  int64_t GetReadWalMemoryLimit() override { return 1_MB; }
+};
+
+TEST_F(LogCacheLimitTest, TestAdequateLogReaderMemory) {
+  // Append a bunch of small ops to the cache.
+  const int kOps = 40;
+  const auto kPayloadSize = 100;
+  ASSERT_OK(AppendReplicateMessagesToCache(1, kOps, kPayloadSize));
+  // Ensure ops are only on disk.
+  ASSERT_OK(log_->WaitUntilAllFlushed());
+  cache_->EvictThroughOp(kOps + 10);
+
+  // All the ops together should fit below GetReadWalMemoryLimit() so we should expect to read
+  // everything.
+  auto read_result = cache_->ReadOps(0, 4_MB, log::ObeyMemoryLimit::kTrue);
+  ASSERT_OK(read_result);
+  EXPECT_EQ(read_result->messages.size(), kOps);
+  ASSERT_FALSE(read_result->have_more_messages);
+}
+
+TEST_F(LogCacheLimitTest, TestInsufficientLogReaderMemory) {
+  // Append a bunch of large ops to the cache.
+  const int kOps = 40;
+  const auto kPayloadSize = GetReadWalMemoryLimit() * 2;
+  ASSERT_OK(AppendReplicateMessagesToCache(1, kOps, kPayloadSize));
+  // Ensure ops are only on disk.
+  ASSERT_OK(log_->WaitUntilAllFlushed());
+  cache_->EvictThroughOp(kOps + 10);
+
+  // GetReadWalMemoryLimit() is smaller than a single op so we should not read anything so expect
+  // Busy result.
+  auto read_result = cache_->ReadOps(0, 4_MB, log::ObeyMemoryLimit::kTrue);
+  ASSERT_NOK(read_result);
+  ASSERT_TRUE(read_result.status().IsBusy()) << read_result.status();
+}
+
+TEST_F(LogCacheLimitTest, TestPartialLogReaderMemory) {
+  // Append a bunch of large ops to the cache.
+  const int kOps = 40;
+  const auto kPayloadSize = GetReadWalMemoryLimit() / 10;
+  ASSERT_OK(AppendReplicateMessagesToCache(1, kOps, kPayloadSize));
+  // Ensure ops are only on disk.
+  ASSERT_OK(log_->WaitUntilAllFlushed());
+  cache_->EvictThroughOp(kOps + 10);
+
+  // Here GetReadWalMemoryLimit() is sufficient to get several operations but not all of them.
+  // Accordingly we expect to get only some of the operations returned.
+  auto read_result = cache_->ReadOps(0, 4_MB, log::ObeyMemoryLimit::kTrue);
+  ASSERT_OK(read_result);
+  EXPECT_GE(read_result->messages.size(), 1);
+  EXPECT_LT(read_result->messages.size(), kOps);
+  EXPECT_TRUE(read_result->have_more_messages);
+}
+
 // Tests that the cache returns STATUS(NotFound, "") if queried for messages after an
 // index that is higher than it's latest, returns an empty set of messages when queried for
 // the last index and returns all messages when queried for MinimumOpId().
@@ -293,18 +356,18 @@ TEST_F(LogCacheTest, TestCacheEdgeCases) {
   ASSERT_OK(log_->WaitUntilAllFlushed());
 
   // Test when the searched index is MinimumOpId().index().
-  auto read_result = ASSERT_RESULT(cache_->ReadOps(0, 100));
+  auto read_result = ASSERT_RESULT(cache_->ReadOps(0, 100, log::ObeyMemoryLimit::kFalse));
   ASSERT_EQ(1, read_result.messages.size());
   ASSERT_EQ(yb::OpId(0, 0), read_result.preceding_op);
 
   // Test when 'after_op_index' is the last index in the cache.
-  read_result = ASSERT_RESULT(cache_->ReadOps(1, 100));
+  read_result = ASSERT_RESULT(cache_->ReadOps(1, 100, log::ObeyMemoryLimit::kFalse));
   ASSERT_EQ(0, read_result.messages.size());
   ASSERT_EQ(yb::OpId(0, 1), read_result.preceding_op);
 
   // Now test the case when 'after_op_index' is after the last index
   // in the cache.
-  auto failed_result = cache_->ReadOps(2, 100);
+  auto failed_result = cache_->ReadOps(2, 100, log::ObeyMemoryLimit::kFalse);
   ASSERT_FALSE(failed_result.ok());
   ASSERT_TRUE(failed_result.status().IsIncomplete())
       << "unexpected status: " << failed_result.status();
@@ -312,7 +375,7 @@ TEST_F(LogCacheTest, TestCacheEdgeCases) {
   // Evict entries from the cache, and ensure that we can still read
   // entries at the beginning of the log.
   cache_->EvictThroughOp(50);
-  read_result = ASSERT_RESULT(cache_->ReadOps(0, 100));
+  read_result = ASSERT_RESULT(cache_->ReadOps(0, 100, log::ObeyMemoryLimit::kFalse));
   ASSERT_EQ(1, read_result.messages.size());
   ASSERT_EQ(yb::OpId(0, 0), read_result.preceding_op);
 }
@@ -471,7 +534,7 @@ TEST_F(LogCacheTest, TestMTReadAndWrite) {
         std::this_thread::sleep_for(5ms);
         continue;
       }
-      auto read_result = ASSERT_RESULT(cache_->ReadOps(index, 1_MB));
+      auto read_result = ASSERT_RESULT(cache_->ReadOps(index, 1_MB, log::ObeyMemoryLimit::kFalse));
       index += read_result.messages.size();
     }
   });
diff --git a/src/yb/consensus/log_cache.cc b/src/yb/consensus/log_cache.cc
index c422e9ad13..f58719cbbf 100644
--- a/src/yb/consensus/log_cache.cc
+++ b/src/yb/consensus/log_cache.cc
@@ -49,7 +49,6 @@
 #include "yb/gutil/map-util.h"
 #include "yb/gutil/strings/human_readable.h"
 
-#include "yb/util/flags.h"
 #include "yb/util/format.h"
 #include "yb/util/locks.h"
 #include "yb/util/logging.h"
@@ -60,8 +59,8 @@
 #include "yb/util/size_literals.h"
 #include "yb/util/status_format.h"
 
-using std::vector;
 using std::string;
+using std::vector;
 
 using namespace std::literals;
 
@@ -99,8 +98,7 @@ METRIC_DEFINE_counter(tablet, log_cache_disk_reads, "Log Cache Disk Reads",
 
 DECLARE_bool(get_changes_honor_deadline);
 
-namespace yb {
-namespace consensus {
+namespace yb::consensus {
 
 namespace {
 
@@ -108,7 +106,7 @@ const std::string kParentMemTrackerId = "LogCache"s;
 
 }
 
-typedef vector<const ReplicateMsg*>::const_iterator MsgIter;
+using MsgIter = vector<const ReplicateMsg *>::const_iterator;
 
 LogCache::LogCache(const scoped_refptr<MetricEntity>& metric_entity,
                    const log::LogPtr& log,
@@ -182,7 +180,7 @@ LogCache::PrepareAppendResult LogCache::PrepareAppendOperations(const ReplicateM
   std::vector<CacheEntry> entries_to_insert;
   entries_to_insert.reserve(msgs.size());
   for (const auto& msg : msgs) {
-    CacheEntry e = { msg, msg->SpaceUsedLong() };
+    CacheEntry e = {.msg = msg, .mem_usage = msg->SpaceUsedLong()};
     result.mem_required += e.mem_usage;
     entries_to_insert.emplace_back(std::move(e));
   }
@@ -376,8 +374,9 @@ int64_t TotalByteSizeForMessage(const LWReplicateMsg& msg) {
 
 } // anonymous namespace
 
-Result<ReadOpsResult> LogCache::ReadOps(int64_t after_op_index, size_t max_size_bytes) {
-  return ReadOps(after_op_index, 0 /* to_op_index */, max_size_bytes);
+Result<ReadOpsResult> LogCache::ReadOps(
+    int64_t after_op_index, size_t max_size_bytes, log::ObeyMemoryLimit obey_memory_limit) {
+  return ReadOps(after_op_index, /*to_op_index=*/0, max_size_bytes, obey_memory_limit);
 }
 
 // Disabled thread safety analysis because the locking seems to be inconsistent, we capture
@@ -389,6 +388,7 @@ Result<ReadOpsResult> LogCache::ReadOps(
     int64_t after_op_index,
     int64_t to_op_index,
     size_t max_size_bytes,
+    log::ObeyMemoryLimit obey_memory_limit,
     CoarseTimePoint deadline,
     bool fetch_single_entry) NO_THREAD_SAFETY_ANALYSIS {
   DCHECK_GE(after_op_index, 0);
@@ -417,16 +417,19 @@ Result<ReadOpsResult> LogCache::ReadOps(
   }
 
   // Return as many operations as we can, up to the limit.
+  bool hit_memory_limit = false;
   int64_t remaining_space = max_size_bytes;
-  while (remaining_space >= 0 &&
-         (fetch_single_entry ? next_index == to_index : next_index < to_index)) {
+  while (fetch_single_entry ? next_index == to_index : next_index < to_index) {
+    if (hit_memory_limit || remaining_space < 0) {
+      break;
+    }
     // Stop reading if a deadline was specified and the deadline has been exceeded.
     if (deadline != CoarseTimePoint::max() && CoarseMonoClock::Now() >= deadline) {
       break;
     }
 
     // If the messages the peer needs haven't been loaded into the queue yet, load them.
-    MessageCache::const_iterator iter = cache_.lower_bound(next_index);
+    auto iter = cache_.lower_bound(next_index);
     if (iter == cache_.end() || iter->first != next_index) {
       int64_t up_to;
       if (fetch_single_entry) {
@@ -444,11 +447,16 @@ Result<ReadOpsResult> LogCache::ReadOps(
       l.unlock();
 
       ReplicateMsgs raw_replicate_ptrs;
-      RETURN_NOT_OK_PREPEND(
-          log_->GetLogReader()->ReadReplicatesInRange(
-              next_index, up_to, remaining_space, &raw_replicate_ptrs, &starting_op_segment_seq_num,
-              deadline),
-          Substitute("Failed to read ops $0..$1", next_index, up_to));
+      auto s = log_->GetLogReader()->ReadReplicatesInRange(
+          next_index, up_to, remaining_space, obey_memory_limit, &raw_replicate_ptrs,
+          &starting_op_segment_seq_num, deadline);
+      if (!s.ok()) {
+        if (s.IsBusy() && result.messages.size() > 0) {
+          hit_memory_limit = true;
+        } else {
+          RETURN_NOT_OK_PREPEND(s, Format("Failed to read ops $0..$1", next_index, up_to));
+        }
+      }
 
       metrics_.disk_reads->IncrementBy(raw_replicate_ptrs.size());
       VLOG_WITH_PREFIX(1) << "Successfully read " << raw_replicate_ptrs.size() << " ops from disk.";
@@ -497,7 +505,7 @@ Result<ReadOpsResult> LogCache::ReadOps(
       }
     }
   }
-  result.have_more_messages = HaveMoreMessages(remaining_space < 0);
+  result.have_more_messages = HaveMoreMessages(remaining_space < 0 || hit_memory_limit);
   return result;
 }
 
@@ -573,7 +581,8 @@ Result<OpId> LogCache::TEST_GetLastOpIdWithType(int64_t max_allowed_index, Opera
   constexpr int kStepSize = 20;
   for (auto end = max_allowed_index; end > 0; end -= kStepSize) {
     auto result = VERIFY_RESULT(ReadOps(
-        std::max<int64_t>(0, end - kStepSize), end, std::numeric_limits<int>::max()));
+        std::max<int64_t>(0, end - kStepSize), end, std::numeric_limits<int>::max(),
+        log::ObeyMemoryLimit::kFalse));
     for (auto it = result.messages.end(); it != result.messages.begin();) {
       --it;
       if ((**it).op_type() == op_type) {
@@ -581,8 +590,9 @@ Result<OpId> LogCache::TEST_GetLastOpIdWithType(int64_t max_allowed_index, Opera
       }
     }
   }
-  return STATUS_FORMAT(NotFound, "Operation of type $0 not found before $1",
-                       OperationType_Name(op_type), max_allowed_index);
+  return STATUS_FORMAT(
+      NotFound, "Operation of type $0 not found before $1", OperationType_Name(op_type),
+      max_allowed_index);
 }
 
 string LogCache::StatsString() const {
@@ -716,5 +726,4 @@ LogCache::Metrics::Metrics(const scoped_refptr<MetricEntity>& metric_entity)
 }
 #undef INSTANTIATE_METRIC
 
-} // namespace consensus
-} // namespace yb
+} // namespace yb::consensus
diff --git a/src/yb/consensus/log_cache.h b/src/yb/consensus/log_cache.h
index c1c5a14a78..fcfde3eb89 100644
--- a/src/yb/consensus/log_cache.h
+++ b/src/yb/consensus/log_cache.h
@@ -29,46 +29,40 @@
 // or implied.  See the License for the specific language governing permissions and limitations
 // under the License.
 //
+
 #pragma once
 
 #include <pthread.h>
 #include <sys/types.h>
 
-#include <atomic>
-#include <condition_variable>
 #include <map>
 #include <memory>
-#include <mutex>
 #include <string>
 #include <vector>
 
 #include <boost/container/small_vector.hpp>
-
-#include "yb/util/logging.h"
 #include <gtest/gtest_prod.h>
 
 #include "yb/common/opid.h"
 
 #include "yb/consensus/consensus_fwd.h"
+#include "yb/consensus/consensus_types.pb.h"
 #include "yb/consensus/log_fwd.h"
 #include "yb/consensus/log_util.h"
-#include "yb/consensus/consensus.pb.h"
-#include "yb/consensus/consensus_types.pb.h"
 
 #include "yb/gutil/macros.h"
 
-#include "yb/util/metrics_fwd.h"
 #include "yb/util/locks.h"
+#include "yb/util/metrics_fwd.h"
 #include "yb/util/monotime.h"
-#include "yb/util/mutex.h"
-#include "yb/util/trace.h"
 #include "yb/util/restart_safe_clock.h"
 #include "yb/util/status_callback.h"
+#include "yb/util/trace.h"
 
 namespace yb {
 
-class MetricEntity;
 class MemTracker;
+class MetricEntity;
 class OpIdPB;
 
 namespace consensus {
@@ -112,28 +106,35 @@ class LogCache {
   void Init(const OpIdPB& preceding_op);
 
   // Read operations from the log, following 'after_op_index'.
-  // If such an op exists in the log, an OK result will always include at least one operation.
   //
-  // The result will be limited such that the total ByteSize() of the returned ops is less than
-  // max_size_bytes, unless that would result in an empty result, in which case exactly one op is
-  // returned.
+  // If such an op exists in the log, an OK result will always include at least one operation.  Note
+  // that even if such an op exists, if obey_memory_limit is set and reading in that op would have
+  // required reading past the log reader memory limit, then status Busy will be returned instead.
+  //
+  // The result will be limited by the available log reader memory if obey_memory_limit is set.  It
+  // will also be limited such that the total ByteSize() of the returned ops is less than
+  // max_size_bytes, unless that would result in an empty result, in which case one over-size op is
+  // allowed.
   //
-  // The OpId which precedes the returned ops is returned in *preceding_op.  The index of this OpId
+  // The OpId that precedes the returned ops is returned in *preceding_op.  The index of this OpId
   // will match 'after_op_index'.
   //
   // If the ops being requested are not available in the log, this will synchronously read these ops
-  // from disk. Therefore, this function may take a substantial amount of time and should not be
+  // from disk.  Therefore, this function may take a substantial amount of time and should not be
   // called with important locks held, etc.
-  Result<ReadOpsResult> ReadOps(int64_t after_op_index, size_t max_size_bytes);
+  Result<ReadOpsResult> ReadOps(
+      int64_t after_op_index, size_t max_size_bytes, log::ObeyMemoryLimit obey_memory_limit);
 
   // Same as above but also includes a 'to_op_index' parameter which will be used to limit results
-  // until 'to_op_index' (inclusive).
+  // until 'to_op_index' (inclusive).  If deadline expires then no ops may be returned even if at
+  // least one op exists in the log.
   //
   // If 'to_op_index' is 0, then all operations after 'after_op_index' will be included.
   Result<ReadOpsResult> ReadOps(
       int64_t after_op_index,
       int64_t to_op_index,
       size_t max_size_bytes,
+      log::ObeyMemoryLimit obey_memory_limit,
       CoarseTimePoint deadline = CoarseTimePoint::max(),
       bool fetch_single_entry = false);
 
@@ -214,7 +215,7 @@ class LogCache {
     bool tracked = false;
   };
 
-  typedef boost::container::small_vector<ReplicateMsgPtr, 8> ReplicateMsgVector;
+  using ReplicateMsgVector = boost::container::small_vector<ReplicateMsgPtr, 8>;
 
   // Try to evict the oldest operations from the queue, stopping either when
   // 'bytes_to_evict' bytes have been evicted, or the op with index
@@ -269,7 +270,7 @@ class LogCache {
   // Maps from log index -> ReplicateMsg
   // An ordered map that serves as the buffer for the cached messages.  Maps from log index ->
   // CacheEntry
-  typedef std::map<int64_t, CacheEntry> MessageCache;
+  using MessageCache = std::map<int64_t, CacheEntry>;
   MessageCache cache_ GUARDED_BY(lock_);
 
   // The next log index to append. Each append operation must either start with this log index, or
diff --git a/src/yb/consensus/log_fwd.h b/src/yb/consensus/log_fwd.h
index 6f3243502a..b14a1161ae 100644
--- a/src/yb/consensus/log_fwd.h
+++ b/src/yb/consensus/log_fwd.h
@@ -13,8 +13,6 @@
 
 #pragma once
 
-#include <vector>
-
 #include "yb/gutil/ref_counted.h"
 
 #include "yb/util/numbered_deque.h"
@@ -26,7 +24,6 @@ namespace log {
 
 class Log;
 class LogAnchorRegistry;
-using LogPtr = scoped_refptr<Log>;
 class LogEntryBatchPB;
 class LogEntryPB;
 class LogIndex;
@@ -44,10 +41,13 @@ struct LogMetrics;
 struct LogOptions;
 
 using LogAnchorRegistryPtr = scoped_refptr<LogAnchorRegistry>;
+using LogPtr = scoped_refptr<Log>;
+using MinStartHTRunningTxnsCallback = std::function<HybridTime(void)>;
+using PreLogRolloverCallback = std::function<void()>;
 using ReadableLogSegmentPtr = scoped_refptr<ReadableLogSegment>;
 using SegmentSequence = NumberedDeque<int64_t, ReadableLogSegmentPtr>;
-using PreLogRolloverCallback = std::function<void()>;
-using MinStartHTRunningTxnsCallback = std::function<HybridTime(void)>;
+
+YB_STRONGLY_TYPED_BOOL(ObeyMemoryLimit);
 
 }  // namespace log
 }  // namespace yb
diff --git a/src/yb/consensus/log_index.cc b/src/yb/consensus/log_index.cc
index e7baa4aa32..0ee14dd1e3 100644
--- a/src/yb/consensus/log_index.cc
+++ b/src/yb/consensus/log_index.cc
@@ -53,19 +53,18 @@
 
 #include <boost/algorithm/string/predicate.hpp>
 
-#include "yb/consensus/log_util.h"
 #include "yb/consensus/log.messages.h"
+#include "yb/consensus/log_util.h"
 
 #include "yb/gutil/casts.h"
 #include "yb/gutil/map-util.h"
 
 #include "yb/util/atomic.h"
-#include "yb/util/scope_exit.h"
 #include "yb/util/env.h"
-#include "yb/util/file_util.h"
 #include "yb/util/flags.h"
 #include "yb/util/locks.h"
 #include "yb/util/logging.h"
+#include "yb/util/scope_exit.h"
 
 DEFINE_UNKNOWN_int32(
     entries_per_index_block, 10000, "Number of entries per index block stored in WAL segment file");
@@ -83,8 +82,7 @@ using std::vector;
   ret = expr; \
 } while ((ret == -1) && (errno == EINTR));
 
-namespace yb {
-namespace log {
+namespace yb::log {
 
 // The actual physical entry in the file.
 // This mirrors LogIndexEntry but uses simple primitives only so we can
@@ -385,7 +383,7 @@ Status LogIndex::GetEntry(int64_t index, LogIndexEntry* entry) {
         index, max_gced_op_index);
   }
   scoped_refptr<IndexChunk> chunk;
-  auto s = GetChunkForIndex(index, false /* do not create */, &chunk);
+  auto s = GetChunkForIndex(index, /*create=*/false, &chunk);
   if (s.IsNotFound()) {
     // Return Incomplete error, so upper layer can lazily load log index blocks from WAL segments
     // into LogIndex if they are not yet loaded.
@@ -661,5 +659,4 @@ string LogIndexEntry::ToString() const {
   return YB_STRUCT_TO_STRING(op_id, segment_sequence_number, offset_in_segment);
 }
 
-} // namespace log
-} // namespace yb
+} // namespace yb::log
diff --git a/src/yb/consensus/log_reader.cc b/src/yb/consensus/log_reader.cc
index 6cac6a6187..23d338bd84 100644
--- a/src/yb/consensus/log_reader.cc
+++ b/src/yb/consensus/log_reader.cc
@@ -360,7 +360,7 @@ Result<scoped_refptr<ReadableLogSegment>> LogReader::GetSegmentBySequenceNumber(
 }
 
 Result<std::shared_ptr<LWLogEntryBatchPB>> LogReader::ReadBatchUsingIndexEntry(
-    const LogIndexEntry& index_entry) const {
+    const LogIndexEntry& index_entry, ObeyMemoryLimit obey_memory_limit) const {
   const int64_t index = index_entry.op_id.index;
 
   const auto segment = VERIFY_RESULT_PREPEND(
@@ -370,7 +370,7 @@ Result<std::shared_ptr<LWLogEntryBatchPB>> LogReader::ReadBatchUsingIndexEntry(
   CHECK_GT(index_entry.offset_in_segment, 0);
   int64_t offset = index_entry.offset_in_segment;
   ScopedLatencyMetric<EventStats> scoped(read_batch_latency_.get());
-  auto result = segment->ReadEntryHeaderAndBatch(&offset);
+  auto result = segment->ReadEntryHeaderAndBatch(obey_memory_limit, &offset);
   RETURN_NOT_OK_PREPEND(
       result,
       Format("Failed to read LogEntry for index $0 from log segment $1 offset $2",
@@ -388,6 +388,7 @@ Status LogReader::ReadReplicatesInRange(
     const int64_t starting_at,
     const int64_t up_to,
     int64_t max_bytes_to_read,
+    ObeyMemoryLimit obey_memory_limit,
     ReplicateMsgs* replicates,
     int64_t* starting_op_segment_seq_num,
     CoarseTimePoint deadline) const {
@@ -430,7 +431,14 @@ Status LogReader::ReadReplicatesInRange(
         index_entry.segment_sequence_number != prev_index_entry.segment_sequence_number ||
         index_entry.offset_in_segment != prev_index_entry.offset_in_segment) {
       // Make read operation.
-      batch = VERIFY_RESULT(ReadBatchUsingIndexEntry(index_entry));
+      auto batch_result = ReadBatchUsingIndexEntry(index_entry, obey_memory_limit);
+      if (!batch_result) {
+        if (batch_result.status().IsBusy() && !replicates_tmp.empty()) {
+          break;
+        }
+        return batch_result.status();
+      }
+      batch = *batch_result;
 
       // Sanity-check the property that a batch should only have increasing indexes.
       int64_t prev_index = 0;
diff --git a/src/yb/consensus/log_reader.h b/src/yb/consensus/log_reader.h
index 90f21339de..cf0217c4fd 100644
--- a/src/yb/consensus/log_reader.h
+++ b/src/yb/consensus/log_reader.h
@@ -99,8 +99,10 @@ class LogReader {
   // The caller takes ownership of the returned ReplicateMsg objects.
   //
   // Will attempt to read no more than 'max_bytes_to_read', unless it is set to
-  // LogReader::kNoSizeLimit. If the size limit would prevent reading any operations at
-  // all, then will read exactly one operation.
+  // LogReader::kNoSizeLimit.  If the size limit would prevent reading any operations at all, then
+  // will read exactly one operation.  Exception: if obey_memory_limit is true then may fail to read
+  // even one operation if doing so would exceed the log reader memory tracker limit; returns Busy
+  // in that case.
   //
   // Requires that a LogIndex was passed into LogReader::Open().
   // Requires up_to operation index to be Raft-committed, otherwise might return NotFound error if
@@ -113,6 +115,7 @@ class LogReader {
       const int64_t starting_at,
       const int64_t up_to,
       int64_t max_bytes_to_read,
+      ObeyMemoryLimit obey_memory_limit,
       consensus::ReplicateMsgs* replicates,
       int64_t* starting_op_segment_seq_num,
       CoarseTimePoint deadline = CoarseTimePoint::max()) const;
@@ -200,8 +203,11 @@ class LogReader {
 
   // Read the LogEntryBatch pointed to by the provided index entry.
   // 'tmp_buf' is used as scratch space to avoid extra allocation.
+  //
+  // Returns status Busy if obey_memory_limit is set and there is insufficient memory to hold the
+  // batch.
   Result<std::shared_ptr<LWLogEntryBatchPB>> ReadBatchUsingIndexEntry(
-      const LogIndexEntry& index_entry) const;
+      const LogIndexEntry& index_entry, ObeyMemoryLimit obey_memory_limit) const;
 
   LogReader(
       Env* env, const scoped_refptr<LogIndex>& index, std::string log_prefix,
diff --git a/src/yb/consensus/log_util.cc b/src/yb/consensus/log_util.cc
index cb4b8d9d87..b6b0b24ad3 100644
--- a/src/yb/consensus/log_util.cc
+++ b/src/yb/consensus/log_util.cc
@@ -363,7 +363,7 @@ Status ReadableLogSegment::CopyTo(
       return STATUS(Corruption, Format("Truncated log entry at offset $0", offset));
     }
 
-    auto current_batch = VERIFY_RESULT(ReadEntryHeaderAndBatch(&offset));
+    auto current_batch = VERIFY_RESULT(ReadEntryHeaderAndBatch(ObeyMemoryLimit::kFalse, &offset));
 
     for (auto it = current_batch->entry().begin(); it != current_batch->entry().end(); ++it) {
       if (!it->has_replicate()) {
@@ -661,7 +661,7 @@ ReadEntriesResult ReadableLogSegment::ReadEntries(
     Status s;
     std::shared_ptr<LWLogEntryBatchPB> current_batch;
     if (offset + implicit_cast<ssize_t>(kEntryHeaderSize) < read_up_to) {
-      auto batch_result = ReadEntryHeaderAndBatch(&offset);
+      auto batch_result = ReadEntryHeaderAndBatch(ObeyMemoryLimit::kFalse, &offset);
       if (batch_result.ok()) {
         current_batch = std::move(*batch_result);
       } else {
@@ -889,14 +889,13 @@ Status ReadableLogSegment::MakeCorruptionStatus(
 }
 
 Result<std::shared_ptr<LWLogEntryBatchPB>> ReadableLogSegment::ReadEntryHeaderAndBatch(
-    int64_t* offset) {
+    ObeyMemoryLimit obey_memory_limit, int64_t* offset) {
   EntryHeader header;
   SCOPED_WAIT_STATUS(WAL_Read);
   RETURN_NOT_OK(ReadEntryHeader(offset, &header));
-  return ReadEntryBatch(offset, header);
+  return ReadEntryBatch(obey_memory_limit, offset, header);
 }
 
-
 Status ReadableLogSegment::ReadEntryHeader(int64_t *offset, EntryHeader* header) {
   uint8_t scratch[kEntryHeaderSize];
   Slice slice;
@@ -925,13 +924,11 @@ Status ReadableLogSegment::DecodeEntryHeader(const Slice& data, EntryHeader* hea
   return Status::OK();
 }
 
-
 Result<std::shared_ptr<LWLogEntryBatchPB>> ReadableLogSegment::ReadEntryBatch(
-    int64_t *offset, const EntryHeader& header) {
-  TRACE_EVENT2("log", "ReadableLogSegment::ReadEntryBatch",
-               "path", path_,
-               "range", Substitute("offset=$0 entry_len=$1",
-                                   *offset, header.msg_length));
+    ObeyMemoryLimit obey_memory_limit, int64_t* offset, const EntryHeader& header) {
+  TRACE_EVENT2(
+      "log", "ReadableLogSegment::ReadEntryBatch", "path", path_, "range",
+      Format("offset=$0 entry_len=$1", *offset, header.msg_length));
 
   if (header.msg_length == 0) {
     return STATUS(Corruption, "Invalid 0 entry length");
@@ -939,10 +936,11 @@ Result<std::shared_ptr<LWLogEntryBatchPB>> ReadableLogSegment::ReadEntryBatch(
   int64_t limit = readable_to_offset();
   if (PREDICT_FALSE(header.msg_length + *offset > limit)) {
     // The log was likely truncated during writing.
-    return STATUS(Corruption,
-        Substitute("Could not read $0-byte log entry from offset $1 in $2: "
-                   "log only readable up to offset $3",
-                   header.msg_length, *offset, path_, limit));
+    return STATUS(
+        Corruption, Format(
+                        "Could not read $0-byte log entry from offset $1 in $2: "
+                        "log only readable up to offset $3",
+                        header.msg_length, *offset, path_, limit));
   }
 
   // Use standard arena starting size for now; this seems too big in many cases though.
@@ -966,9 +964,15 @@ Result<std::shared_ptr<LWLogEntryBatchPB>> ReadableLogSegment::ReadEntryBatch(
     }
   };
 
-  // Estimate that arena after deserializing message will take about the same space as the message
-  // unless that would be below the arena minimum size.
-  int64_t estimated_memory_needed = header.msg_length;
+  // Estimate based on experiments that arena after deserializing message will take about 6 times
+  // the space as the message unless that would be below the arena minimum size.  This should be
+  // reasonable if the message contains a lot of integers and the like and and an overcount if the
+  // message contains a lot of strings (those point back into the message).
+  //
+  // To calibrate the estimate, several database tables were created and populated.  After a system
+  // restart, the verbose logs were examined to see how accurate the parameter (e.g., 6) was.  This
+  // was repeated until a sufficiently accurate parameter was found.
+  int64_t estimated_memory_needed = header.msg_length * 6ll;
   if (estimated_memory_needed < kStartBlockSize) {
     estimated_memory_needed = kStartBlockSize;
   }
@@ -976,7 +980,14 @@ Result<std::shared_ptr<LWLogEntryBatchPB>> ReadableLogSegment::ReadEntryBatch(
   estimated_memory_needed += header.msg_length;
 
   if (read_wal_mem_tracker_) {
-    read_wal_mem_tracker_->Consume(estimated_memory_needed);
+    if (!read_wal_mem_tracker_->TryConsume(estimated_memory_needed)) {
+      if (obey_memory_limit) {
+        YB_LOG_EVERY_N_SECS(WARNING, 5) << "Unable to read WAL batch due to insufficient memory";
+        return STATUS(Busy, "Unable to read WAL batch due to insufficient memory");
+      }
+      // No pre-consumption done; we will Consume once we know the actual memory usage.
+      estimated_memory_needed = 0;
+    }
   }
   RefCntBuffer buffer(header.msg_length);
   auto holder = std::make_shared<DataHolder>(buffer, read_wal_mem_tracker_);
@@ -1011,6 +1022,10 @@ Result<std::shared_ptr<LWLogEntryBatchPB>> ReadableLogSegment::ReadEntryBatch(
     int64_t actual_memory_used =
         header.msg_length + static_cast<int64_t>(holder->arena.memory_footprint());
     int64_t adjustment = actual_memory_used - holder->consumed;
+    if (holder->consumed > 0 && adjustment != 0) {
+      VLOG(2) << "memory adjustment is " << adjustment << "; it is off by "
+              << adjustment * 100.0 / (estimated_memory_needed - header.msg_length) << "%";
+    }
     read_wal_mem_tracker_->Consume(adjustment);  // This is a Release if adjustment is negative.
     holder->consumed = actual_memory_used;
   }
diff --git a/src/yb/consensus/log_util.h b/src/yb/consensus/log_util.h
index 30fcb0413f..aa817cac30 100644
--- a/src/yb/consensus/log_util.h
+++ b/src/yb/consensus/log_util.h
@@ -355,7 +355,10 @@ class ReadableLogSegment : public RefCountedThreadSafe<ReadableLogSegment> {
       size_t batch_number, int64_t batch_offset, std::vector<int64_t>* recent_offsets,
       const LogEntries& entries, const Status& status) const;
 
-  Result<std::shared_ptr<LWLogEntryBatchPB>> ReadEntryHeaderAndBatch(int64_t* offset);
+  // Returns status Busy if obey_memory_limit is set and there is insufficient memory to hold the
+  // batch.
+  Result<std::shared_ptr<LWLogEntryBatchPB>> ReadEntryHeaderAndBatch(
+      ObeyMemoryLimit obey_memory_limit, int64_t* offset);
 
   // Reads a log entry header from the segment.
   // Also increments the passed offset* by the length of the entry.
@@ -370,8 +373,11 @@ class ReadableLogSegment : public RefCountedThreadSafe<ReadableLogSegment> {
 
   // Reads a log entry batch from the provided readable segment, which gets decoded
   // into 'entry_batch' and increments 'offset' by the batch's length.
+  //
+  // Returns status Busy if obey_memory_limit is set and there is insufficient memory to hold the
+  // batch.
   Result<std::shared_ptr<LWLogEntryBatchPB>> ReadEntryBatch(
-      int64_t *offset, const EntryHeader& header);
+      ObeyMemoryLimit obey_memory_limit, int64_t* offset, const EntryHeader& header);
 
   void UpdateReadableToOffset(int64_t readable_to_offset);
 
diff --git a/src/yb/integration-tests/xcluster/xcluster_ysql-test.cc b/src/yb/integration-tests/xcluster/xcluster_ysql-test.cc
index 1c37a56310..66de7c8307 100644
--- a/src/yb/integration-tests/xcluster/xcluster_ysql-test.cc
+++ b/src/yb/integration-tests/xcluster/xcluster_ysql-test.cc
@@ -23,8 +23,6 @@
 #include <gtest/gtest.h>
 
 #include "yb/client/yb_table_name.h"
-#include "yb/integration-tests/xcluster/xcluster_test_utils.h"
-#include "yb/integration-tests/xcluster/xcluster_ysql_test_base.h"
 
 #include "yb/common/common.pb.h"
 #include "yb/common/wire_protocol.h"
@@ -44,15 +42,18 @@
 #include "yb/client/yb_op.h"
 
 #include "yb/gutil/strings/substitute.h"
+
 #include "yb/integration-tests/mini_cluster.h"
 #include "yb/integration-tests/xcluster/xcluster_test_base.h"
+#include "yb/integration-tests/xcluster/xcluster_test_utils.h"
+#include "yb/integration-tests/xcluster/xcluster_ysql_test_base.h"
 
 #include "yb/master/catalog_manager_if.h"
+#include "yb/master/master_client.pb.h"
 #include "yb/master/master_ddl.pb.h"
 #include "yb/master/master_defaults.h"
-#include "yb/master/mini_master.h"
-#include "yb/master/master_client.pb.h"
 #include "yb/master/master_replication.proxy.h"
+#include "yb/master/mini_master.h"
 
 #include "yb/rpc/rpc_controller.h"
 #include "yb/tablet/tablet.h"
@@ -1782,7 +1783,7 @@ TEST_F(XClusterYsqlTest, IsBootstrapRequiredNotFlushed) {
   std::vector<TabletId> tablet_ids;
   if (producer_tables_[0]) {
     ASSERT_OK(producer_cluster_.client_->GetTablets(
-        producer_tables_[0]->name(), (int32_t)3, &tablet_ids, NULL));
+        producer_tables_[0]->name(), (int32_t)3, &tablet_ids, nullptr));
     ASSERT_GT(tablet_ids.size(), 0);
   }
 
@@ -1869,7 +1870,8 @@ TEST_F(XClusterYsqlTest, IsBootstrapRequiredFlushed) {
         int64_t starting_op;
         return !tablet_peer->log()
                     ->GetLogReader()
-                    ->ReadReplicatesInRange(1, 2, 0, &replicates, &starting_op)
+                    ->ReadReplicatesInRange(
+                        1, 2, 0, log::ObeyMemoryLimit::kFalse, &replicates, &starting_op)
                     .ok();
       },
       MonoDelta::FromSeconds(30), "Logs cleaned"));
@@ -2477,8 +2479,8 @@ TEST_F(XClusterYsqlTest, DeletingDatabaseContainingReplicatedTable) {
   const string kNamespaceName2 = "test_namespace2";
 
   // Create the additional databases.
-  auto producer_db_2 = CreateDatabase(&producer_cluster_, kNamespaceName2, false);
-  auto consumer_db_2 = CreateDatabase(&consumer_cluster_, kNamespaceName2, false);
+  auto producer_db_2 = CreateDatabase(&producer_cluster_, kNamespaceName2, /*colocated=*/false);
+  auto consumer_db_2 = CreateDatabase(&consumer_cluster_, kNamespaceName2, /*colocated=*/false);
 
   std::vector<std::shared_ptr<client::YBTable>> producer_tables;
   std::vector<YBTableName> producer_table_names;
@@ -2935,7 +2937,7 @@ Status XClusterYSqlTestConsistentTransactionsTest::RunInsertUpdateDeleteTransact
       [&]() -> Result<bool> {
         std::vector<TabletId> tablet_ids;
         RETURN_NOT_OK(producer_cluster_.client_->GetTablets(
-            producer_table_->name(), 0 /* max_tablets */, &tablet_ids, NULL));
+            producer_table_->name(), /*max_tablets=*/0, &tablet_ids, /*ranges=*/nullptr));
         return tablet_ids.size() == 2;
       },
       MonoDelta::FromSeconds(kRpcTimeout), "Wait for tablet to be split."));
@@ -3212,8 +3214,8 @@ TEST_F(XClusterYsqlTest, DropTableOnProducerOnly) {
   auto stream_id = ASSERT_RESULT(GetCDCStreamID(producer_table_->id()));
   std::vector<TabletId> tablet_ids;
   if (producer_tables_[0]) {
-    ASSERT_OK(
-        producer_client()->GetTablets(producer_tables_[0]->name(), (int32_t)1, &tablet_ids, NULL));
+    ASSERT_OK(producer_client()->GetTablets(
+        producer_tables_[0]->name(), /*max_tablets=*/(int32_t)1, &tablet_ids, /*ranges=*/nullptr));
     ASSERT_GT(tablet_ids.size(), 0);
   }
   auto& tablet_id = tablet_ids.front();
diff --git a/src/yb/tserver/tablet_memory_manager.cc b/src/yb/tserver/tablet_memory_manager.cc
index 616dea785e..1d31bb320a 100644
--- a/src/yb/tserver/tablet_memory_manager.cc
+++ b/src/yb/tserver/tablet_memory_manager.cc
@@ -34,6 +34,7 @@
 #include "yb/util/flags.h"
 #include "yb/util/logging.h"
 #include "yb/util/mem_tracker.h"
+#include "yb/util/size_literals.h"
 #include "yb/util/status_log.h"
 
 using namespace std::literals;
@@ -98,6 +99,10 @@ TAG_FLAG(db_block_cache_num_shard_bits, advanced);
 DEFINE_test_flag(bool, pretend_memory_exceeded_enforce_flush, false,
                   "Always pretend memory has been exceeded to enforce background flush.");
 
+DEFINE_NON_RUNTIME_int64(read_wal_memory_bytes, 512_MB,
+    "Limit on amount of memory used to hold WAL records temporarily read in from disk; -1 means "
+    "no limit.");
+
 namespace yb::tserver {
 
 using strings::Substitute;
@@ -197,7 +202,7 @@ TabletMemoryManager::TabletMemoryManager(
   tablets_overhead_mem_tracker_ = MemTracker::FindOrCreateTracker(
       /*no limit*/ -1, "Tablets_overhead", server_mem_tracker_);
   read_wal_mem_tracker_ = MemTracker::FindOrCreateTracker(
-     /*no limit*/ -1, "Log Reader Memory", server_mem_tracker_);
+      FLAGS_read_wal_memory_bytes, "Log Reader Memory", server_mem_tracker_);
 
   InitBlockCache(metrics, default_block_cache_size_percentage, options);
   InitLogCacheGC();
