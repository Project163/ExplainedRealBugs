diff --git a/src/yb/cdc/xcluster_producer.cc b/src/yb/cdc/xcluster_producer.cc
index f6112bed72..07fe2cdd0b 100644
--- a/src/yb/cdc/xcluster_producer.cc
+++ b/src/yb/cdc/xcluster_producer.cc
@@ -288,6 +288,9 @@ Status GetChangesForXCluster(
       "xCluster only supports WAL record format");
 
   auto tablet = VERIFY_RESULT(tablet_peer->shared_tablet_safe());
+  auto consensus = VERIFY_RESULT(tablet_peer->GetConsensus());
+  auto term = consensus->LeaderTerm();
+  SCHECK_GT(term, 0, NotFound, Format("Leader term for tablet $0 is not valid", tablet_id));
 
   auto leader_safe_time = VERIFY_RESULT(tablet_peer->LeaderSafeTime());
   SCHECK(
@@ -298,6 +301,7 @@ Status GetChangesForXCluster(
   // There should only be one thread at a time calling GetChanges per tablet. But we cannot trust
   // calls we get over the network so we lock here.
   std::lock_guard l(stream_tablet_metadata->mutex_);
+  stream_tablet_metadata->ResetOnTermChange(term);
 
   bool update_apply_safe_time = false;
   auto now = MonoTime::Now();
@@ -329,19 +333,18 @@ Status GetChangesForXCluster(
     }
   }
 
-  consensus::ReadOpsResult read_ops;
-  {
-    auto consensus = VERIFY_RESULT(tablet_peer->GetConsensus());
-    read_ops = VERIFY_RESULT(
-        consensus->ReadReplicatedMessagesForCDC(from_op_id, last_readable_opid_index, deadline));
-  }
+  auto read_result = VERIFY_RESULT(consensus->ReadReplicatedMessagesForXCluster(
+      from_op_id, deadline, /*fetch_single_entry=*/false));
+  auto& read_ops = read_result.result;
+  *last_readable_opid_index = read_result.majority_replicated_index;
 
   if (update_apply_safe_time) {
     DCHECK(!stream_tablet_metadata->last_apply_safe_time_.is_valid());
     stream_tablet_metadata->last_apply_safe_time_ = leader_safe_time;
 
     DCHECK_EQ(stream_tablet_metadata->apply_safe_time_checkpoint_op_id_, 0);
-    stream_tablet_metadata->apply_safe_time_checkpoint_op_id_ = *last_readable_opid_index;
+    stream_tablet_metadata->apply_safe_time_checkpoint_op_id_ =
+        read_result.majority_replicated_index;
 
     stream_tablet_metadata->last_apply_safe_time_update_time_ = now;
   }
@@ -431,6 +434,12 @@ Status GetChangesForXCluster(
       nullptr, std::move(messages), std::move(consumption));
   (checkpoint.index > 0 ? checkpoint : from_op_id).ToPB(
       resp->mutable_checkpoint()->mutable_op_id());
+
+  // Make sure we are still the leader, and the entire function run while under the same term.
+  SCHECK_EQ(
+      term, consensus->LeaderTerm(), NotFound,
+      Format("Leader term for tablet has changed", tablet_id));
+
   return Status::OK();
 }
 
diff --git a/src/yb/cdc/xcluster_producer_bootstrap.cc b/src/yb/cdc/xcluster_producer_bootstrap.cc
index 4b761da873..5731461269 100644
--- a/src/yb/cdc/xcluster_producer_bootstrap.cc
+++ b/src/yb/cdc/xcluster_producer_bootstrap.cc
@@ -83,12 +83,11 @@ Result<bool> IsBootstrapRequiredForTablet(
   OpId next_index = min_op_id;
   next_index.index++;
 
-  int64_t last_readable_opid_index;
   auto consensus = VERIFY_RESULT_OR_SET_CODE(
       tablet_peer->GetConsensus(), CDCError(CDCErrorPB::LEADER_NOT_READY));
 
-  auto log_result = consensus->ReadReplicatedMessagesForCDC(
-      next_index, &last_readable_opid_index, deadline, true /* fetch_single_entry */);
+  auto log_result = consensus->ReadReplicatedMessagesForXCluster(
+      next_index, deadline, /*fetch_single_entry=*/true);
 
   if (!log_result.ok()) {
     if (log_result.status().IsNotFound()) {
diff --git a/src/yb/cdc/xrepl_stream_metadata.cc b/src/yb/cdc/xrepl_stream_metadata.cc
index ae718ea76b..95b18d2107 100644
--- a/src/yb/cdc/xrepl_stream_metadata.cc
+++ b/src/yb/cdc/xrepl_stream_metadata.cc
@@ -225,6 +225,17 @@ Result<std::optional<uint32_t>> StreamMetadata::GetDbOidToGetSequencesForUnlocke
   return GetPgsqlDatabaseOid(namespace_id);
 }
 
+void StreamMetadata::StreamTabletMetadata::ResetOnTermChange(int64_t term) {
+  if (term == term_) {
+    return;
+  }
+
+  term_ = term;
+  last_apply_safe_time_ = HybridTime::kInvalid;
+  last_apply_safe_time_update_time_ = MonoTime::kUninitialized;
+  apply_safe_time_checkpoint_op_id_ = 0;
+}
+
 void StreamMetadata::StreamTabletMetadata::UpdateStats(
     const MonoTime& start_time, const Status& status, int num_records, size_t bytes_sent,
     int64_t sent_index, int64_t latest_wal_index) {
diff --git a/src/yb/cdc/xrepl_stream_metadata.h b/src/yb/cdc/xrepl_stream_metadata.h
index f71589923a..67c76bed85 100644
--- a/src/yb/cdc/xrepl_stream_metadata.h
+++ b/src/yb/cdc/xrepl_stream_metadata.h
@@ -39,9 +39,14 @@ class StreamMetadata {
  public:
   struct StreamTabletMetadata {
     std::mutex mutex_;
+    int64_t term_ GUARDED_BY(mutex_) = 0;
     int64_t apply_safe_time_checkpoint_op_id_ GUARDED_BY(mutex_) = 0;
     HybridTime last_apply_safe_time_ GUARDED_BY(mutex_);
     MonoTime last_apply_safe_time_update_time_ GUARDED_BY(mutex_);
+
+    // Reset the cache if the term changes.
+    void ResetOnTermChange(int64_t term) REQUIRES(mutex_);
+
     // TODO(hari): #16774 Move last_readable_index and last sent opid here, and use them to make
     // UpdateCDCTabletMetrics run asynchronously.
 
diff --git a/src/yb/consensus/consensus.h b/src/yb/consensus/consensus.h
index 4913fe3f3f..9e7d26779d 100644
--- a/src/yb/consensus/consensus.h
+++ b/src/yb/consensus/consensus.h
@@ -336,6 +336,15 @@ class Consensus {
   virtual Result<MicrosTime> MajorityReplicatedHtLeaseExpiration(
       MicrosTime min_allowed, CoarseTimePoint deadline) const = 0;
 
+  // Read replicated messages for xCluster replication.
+  // Only committed messages (committed_op_id) are returned.
+  // There can be more valid messages that are within the majority_replicated_op_id and
+  // committed_op_id range.
+  // If any message is not returned due to delay in commit, or if the result reaches
+  // FLAGS_consensus_max_batch_size_bytes, have_more_messages is set.
+  virtual Result<XClusterReadOpsResult> ReadReplicatedMessagesForXCluster(
+      const yb::OpId& from, const CoarseTimePoint deadline, bool fetch_single_entry) = 0;
+
   // Read majority replicated messages for CDC producer.
   virtual Result<ReadOpsResult> ReadReplicatedMessagesForCDC(
       const yb::OpId& from, int64_t* repl_index, const CoarseTimePoint deadline,
diff --git a/src/yb/consensus/consensus_fwd.h b/src/yb/consensus/consensus_fwd.h
index a0e23921cf..f89804ef81 100644
--- a/src/yb/consensus/consensus_fwd.h
+++ b/src/yb/consensus/consensus_fwd.h
@@ -45,6 +45,7 @@ struct ConsensusOptions;
 struct ConsensusBootstrapInfo;
 struct LeaderState;
 struct ReadOpsResult;
+struct XClusterReadOpsResult;
 struct RetryableRequestsCounts;
 struct StateChangeContext;
 
diff --git a/src/yb/consensus/consensus_queue-test.cc b/src/yb/consensus/consensus_queue-test.cc
index b4a010f806..f5a3c896ca 100644
--- a/src/yb/consensus/consensus_queue-test.cc
+++ b/src/yb/consensus/consensus_queue-test.cc
@@ -107,6 +107,8 @@ class ConsensusQueueTest : public YBTest {
     CloseAndReopenQueue();
   }
 
+  virtual void SetupConsensus() { consensus_.reset(new TestRaftConsensusQueueIface()); }
+
   void CloseAndReopenQueue() {
     // Blow away the memtrackers before creating the new queue. Otherwise we will get an error
     // trying to create a duplicate log cache memtracker.
@@ -121,7 +123,7 @@ class ConsensusQueueTest : public YBTest {
                                       clock_,
                                       nullptr /* consensus_context */,
                                       std::move(notify_observers_strand)));
-    consensus_.reset(new TestRaftConsensusQueueIface());
+    SetupConsensus();
     queue_->RegisterObserver(consensus_.get());
   }
 
@@ -900,7 +902,7 @@ TEST_F(ConsensusQueueTest, TestTriggerRemoteBootstrapIfTabletNotFound) {
             rb_req.bootstrap_source_private_addr()[0].ShortDebugString());
 }
 
-// Tests that ReadReplicatedMessagesForCDC() only reads messages until the last known
+// Tests that TestReadReplicatedMessagesForCDC() only reads messages until the last known
 // committed index.
 TEST_F(ConsensusQueueTest, TestReadReplicatedMessagesForCDC) {
   auto start_op_id = MakeOpIdForIndex(3); // Starting after the normal first index.
@@ -948,5 +950,87 @@ TEST_F(ConsensusQueueTest, TestReadReplicatedMessagesForCDC) {
   ASSERT_EQ(last_committed_index - start, read_result.messages.size());
 }
 
+class DelayedCommitObserver : public TestRaftConsensusQueueIface {
+ public:
+  explicit DelayedCommitObserver(OpId max_commit_op_id) : max_commit_op_id_(max_commit_op_id) {}
+
+  void UpdateMajorityReplicated(
+      const MajorityReplicatedData& data, OpId* committed_index,
+      OpId* last_applied_op_id) override {
+    TestRaftConsensusQueueIface::UpdateMajorityReplicated(
+        data, committed_index, last_applied_op_id);
+    if (committed_index->index > max_commit_op_id_.index) {
+      *committed_index = max_commit_op_id_;
+      *last_applied_op_id = max_commit_op_id_;
+    }
+  }
+
+ private:
+  OpId max_commit_op_id_;
+};
+
+class ConsensusQueueDelayedCommitTest : public ConsensusQueueTest {
+ public:
+  const OpId kMaxCommitOpId = MakeOpIdForIndex(kNumMessages - 20);
+
+  ConsensusQueueDelayedCommitTest() = default;
+
+  void SetupConsensus() override { consensus_.reset(new DelayedCommitObserver(kMaxCommitOpId)); }
+};
+
+TEST_F(ConsensusQueueDelayedCommitTest, TestReadReplicatedMessagesForXCluster) {
+  const auto start_op_id = MakeOpIdForIndex(3);  // Starting after the normal first index.
+  queue_->Init(start_op_id);
+  queue_->SetLeaderMode(start_op_id, start_op_id.term, start_op_id, BuildRaftConfigPBForTests(2));
+  queue_->TrackPeer(kPeerUuid);
+
+  AppendReplicateMessagesToQueue(queue_.get(), clock_, start_op_id.index, kNumMessages);
+
+  // Wait for the local peer to append all messages.
+  WaitForLocalPeerToAckIndex(kNumMessages);
+
+  // Since only the local log might have ACKed at this point,
+  // the committed_index should be MinimumOpId().
+  queue_->TEST_WaitForNotificationToFinish();
+  ASSERT_EQ(queue_->TEST_GetCommittedIndex(), start_op_id);
+  ASSERT_EQ(queue_->TEST_GetLastAppliedOpId(), start_op_id);
+
+  ThreadSafeArena arena;
+  LWConsensusResponsePB response(&arena);
+  response.ref_responder_uuid(kPeerUuid);
+
+  const auto received_op_id = MakeOpIdForIndex(kNumMessages - 10);
+
+  // Ack last_committed_index messages.
+  SetLastReceivedAndLastCommitted(&response, received_op_id, kMaxCommitOpId.index);
+  ASSERT_TRUE(queue_->ResponseFromPeer(response.responder_uuid().ToBuffer(), response));
+  queue_->TEST_WaitForNotificationToFinish();
+  ASSERT_EQ(queue_->TEST_GetCommittedIndex(), kMaxCommitOpId);
+  consensus_->WaitForMajorityReplicatedIndex(received_op_id.index);
+  ASSERT_EQ(queue_->TEST_GetMajorityReplicatedOpId(), received_op_id);
+  ASSERT_EQ(queue_->TEST_GetLastAppliedOpId(), kMaxCommitOpId);
+
+  auto read_wal_for_xcluster = [this](const yb::OpId& last_op_id) {
+    return queue_->ReadReplicatedMessagesForXCluster(
+        last_op_id, /*deadline=*/CoarseTimePoint::max(), /*fetch_single_entry=*/false);
+  };
+
+  // Read from the start_op_id
+  auto read_result = ASSERT_RESULT(read_wal_for_xcluster(MakeOpIdForIndex(3)));
+  ASSERT_EQ(kMaxCommitOpId.index - start_op_id.index, read_result.result.messages.size());
+  ASSERT_EQ(received_op_id.index, read_result.majority_replicated_index);
+
+  // Start reading from 0.0 and ensure that we get the first known OpID.
+  read_result = ASSERT_RESULT(read_wal_for_xcluster(MakeOpIdForIndex(0)));
+  ASSERT_EQ(kMaxCommitOpId.index - start_op_id.index, read_result.result.messages.size());
+  ASSERT_EQ(received_op_id.index, read_result.majority_replicated_index);
+
+  // Read from some index > 0
+  int start = 10;
+  read_result = ASSERT_RESULT(read_wal_for_xcluster(MakeOpIdForIndex(start)));
+  ASSERT_EQ(kMaxCommitOpId.index - start, read_result.result.messages.size());
+  ASSERT_EQ(received_op_id.index, read_result.majority_replicated_index);
+}
+
 }  // namespace consensus
 }  // namespace yb
diff --git a/src/yb/consensus/consensus_queue.cc b/src/yb/consensus/consensus_queue.cc
index d54ef90213..c45e3a41a1 100644
--- a/src/yb/consensus/consensus_queue.cc
+++ b/src/yb/consensus/consensus_queue.cc
@@ -742,7 +742,7 @@ int64_t PeerMessageQueue::GetStartOpIdIndex(int64_t start_index) {
                           : start_index;
 }
 
-Result<ReadOpsResult> PeerMessageQueue::ReadFromLogCacheForCDC(
+Result<ReadOpsResult> PeerMessageQueue::ReadFromLogCacheForXRepl(
     int64_t last_op_id_index, int64_t to_index, CoarseTimePoint deadline, bool fetch_single_entry) {
   // If an empty OpID is only sent on the first read request, start at the earliest known entry.
   int64_t after_op_index = GetStartOpIdIndex(last_op_id_index);
@@ -761,6 +761,33 @@ Result<ReadOpsResult> PeerMessageQueue::ReadFromLogCacheForCDC(
   return result;
 }
 
+Result<XClusterReadOpsResult> PeerMessageQueue::ReadReplicatedMessagesForXCluster(
+    const yb::OpId& last_op_id, const CoarseTimePoint deadline, bool fetch_single_entry) {
+  const auto [committed_index, majority_replicated_index] =
+      GetCommittedAndMajorityReplicatedIndex();
+  bool pending_messages = committed_index != majority_replicated_index;
+
+  XClusterReadOpsResult xcluster_result{
+      .result =
+          {.messages = ReplicateMsgs(),
+           .preceding_op = OpId(),
+           .have_more_messages = HaveMoreMessages(pending_messages)},
+      .majority_replicated_index = majority_replicated_index};
+
+  if (last_op_id.index >= committed_index) {
+    // No committed records left to read.
+    return xcluster_result;
+  }
+
+  xcluster_result.result = VERIFY_RESULT(
+      ReadFromLogCacheForXRepl(last_op_id.index, committed_index, deadline, fetch_single_entry));
+
+  xcluster_result.result.have_more_messages =
+      HaveMoreMessages(xcluster_result.result.have_more_messages.get() || pending_messages);
+
+  return xcluster_result;
+}
+
 // Read majority replicated messages from cache for CDC.
 // CDC producer will use this to get the messages to send in response to cdc::GetChanges RPC.
 Result<ReadOpsResult> PeerMessageQueue::ReadReplicatedMessagesForCDC(
@@ -791,7 +818,7 @@ Result<ReadOpsResult> PeerMessageQueue::ReadReplicatedMessagesForCDC(
   }
 
   auto result = VERIFY_RESULT(
-      ReadFromLogCacheForCDC(last_op_id.index, to_index, deadline, fetch_single_entry));
+      ReadFromLogCacheForXRepl(last_op_id.index, to_index, deadline, fetch_single_entry));
 
   result.have_more_messages =
       HaveMoreMessages(result.have_more_messages.get() || pending_messages);
@@ -842,7 +869,7 @@ Result<ReadOpsResult> PeerMessageQueue::ReadReplicatedMessagesForConsistentCDC(
       }
     }
 
-    auto result = VERIFY_RESULT(ReadFromLogCacheForCDC(
+    auto result = VERIFY_RESULT(ReadFromLogCacheForXRepl(
         last_op_id.index, committed_op_id_index, deadline, fetch_single_entry));
 
     res.messages.insert(res.messages.end(), result.messages.begin(), result.messages.end());
@@ -968,7 +995,7 @@ Result<ReadOpsResult> PeerMessageQueue::ReadReplicatedMessagesInSegmentForCDC(
   // Read the ops from the segment starting from current_index + 1.
   while (current_index < segment_last_index) {
     auto result = VERIFY_RESULT(
-        ReadFromLogCacheForCDC(current_index, segment_last_index, deadline, fetch_single_entry));
+        ReadFromLogCacheForXRepl(current_index, segment_last_index, deadline, fetch_single_entry));
 
     read_ops.read_from_disk_size += result.read_from_disk_size;
     read_ops.messages.insert(
diff --git a/src/yb/consensus/consensus_queue.h b/src/yb/consensus/consensus_queue.h
index 2973c7f891..2c19548146 100644
--- a/src/yb/consensus/consensus_queue.h
+++ b/src/yb/consensus/consensus_queue.h
@@ -395,6 +395,9 @@ class PeerMessageQueue {
     return local_peer_pb_.cloud_info();
   }
 
+  Result<XClusterReadOpsResult> ReadReplicatedMessagesForXCluster(
+      const yb::OpId& last_op_id, const CoarseTimePoint deadline, bool fetch_single_entry);
+
   // Read replicated log records starting from the OpId immediately after last_op_id.
   Result<ReadOpsResult> ReadReplicatedMessagesForCDC(
       const yb::OpId& last_op_id, int64_t* last_replicated_opid_index = nullptr,
@@ -433,6 +436,7 @@ class PeerMessageQueue {
  private:
   FRIEND_TEST(ConsensusQueueTest, TestQueueAdvancesCommittedIndex);
   FRIEND_TEST(ConsensusQueueTest, TestReadReplicatedMessagesForCDC);
+  FRIEND_TEST(ConsensusQueueDelayedCommitTest, TestReadReplicatedMessagesForXCluster);
 
   // Mode specifies how the queue currently behaves:
   //
@@ -588,10 +592,8 @@ class PeerMessageQueue {
       const CoarseTimePoint deadline = CoarseTimePoint::max(),
       const bool fetch_single_entry = false);
 
-  Result<ReadOpsResult> ReadFromLogCacheForCDC(
-      int64_t last_op_id_index,
-      int64_t to_index,
-      CoarseTimePoint deadline = CoarseTimePoint::max(),
+  Result<ReadOpsResult> ReadFromLogCacheForXRepl(
+      int64_t last_op_id_index, int64_t to_index, CoarseTimePoint deadline = CoarseTimePoint::max(),
       bool fetch_single_entry = false);
 
   std::pair<int64_t, int64_t> GetCommittedAndMajorityReplicatedIndex();
diff --git a/src/yb/consensus/log_cache.h b/src/yb/consensus/log_cache.h
index f801cbdefa..1d9e83b1d3 100644
--- a/src/yb/consensus/log_cache.h
+++ b/src/yb/consensus/log_cache.h
@@ -84,6 +84,11 @@ struct ReadOpsResult {
   int64_t read_from_disk_size = 0;
 };
 
+struct XClusterReadOpsResult {
+  ReadOpsResult result;
+  int64_t majority_replicated_index;
+};
+
 // Write-through cache for the log.
 //
 // This stores a set of log messages by their index. New operations can be appended to the end as
diff --git a/src/yb/consensus/raft_consensus.cc b/src/yb/consensus/raft_consensus.cc
index b4a53181de..bee6c1e7c4 100644
--- a/src/yb/consensus/raft_consensus.cc
+++ b/src/yb/consensus/raft_consensus.cc
@@ -3637,6 +3637,11 @@ Status RaftConsensus::HandleTermAdvanceUnlocked(ConsensusTerm new_term) {
   return Status::OK();
 }
 
+Result<XClusterReadOpsResult> RaftConsensus::ReadReplicatedMessagesForXCluster(
+    const yb::OpId& from, const CoarseTimePoint deadline, bool fetch_single_entry) {
+  return queue_->ReadReplicatedMessagesForXCluster(from, deadline, fetch_single_entry);
+}
+
 Result<ReadOpsResult> RaftConsensus::ReadReplicatedMessagesForCDC(
     const yb::OpId& from, int64_t* last_replicated_opid_index, const CoarseTimePoint deadline,
     const bool fetch_single_entry) {
diff --git a/src/yb/consensus/raft_consensus.h b/src/yb/consensus/raft_consensus.h
index cfef2bc6dd..723c97d748 100644
--- a/src/yb/consensus/raft_consensus.h
+++ b/src/yb/consensus/raft_consensus.h
@@ -274,6 +274,9 @@ class RaftConsensus : public std::enable_shared_from_this<RaftConsensus>,
     TEST_delay_update_.store(duration, std::memory_order_release);
   }
 
+  Result<XClusterReadOpsResult> ReadReplicatedMessagesForXCluster(
+      const yb::OpId& from, const CoarseTimePoint deadline, bool fetch_single_entry) override;
+
   Result<ReadOpsResult> ReadReplicatedMessagesForCDC(
       const yb::OpId& from,
       int64_t* last_replicated_opid_index,
diff --git a/src/yb/integration-tests/xcluster/xcluster-test.cc b/src/yb/integration-tests/xcluster/xcluster-test.cc
index 016a98cba9..296aeb2532 100644
--- a/src/yb/integration-tests/xcluster/xcluster-test.cc
+++ b/src/yb/integration-tests/xcluster/xcluster-test.cc
@@ -1805,7 +1805,7 @@ TEST_P(XClusterTest, BiDirectionalWrites) {
 
 TEST_P(XClusterTest, BiDirectionalWritesWithLargeBatches) {
   // Simulate large batches by dropping FLAGS_consensus_max_batch_size_bytes to limit the size of
-  // the batches we read in GetChanges / ReadReplicatedMessagesForCDC.
+  // the batches we read in GetChanges / ReadReplicatedMessagesForXCluster.
   // We want to test that we don't get stuck if an entire batch messages read are all filtered out
   // (currently we only filter out external writes) and the checkpoint isn't updated.
   ANNOTATE_UNPROTECTED_WRITE(FLAGS_consensus_max_batch_size_bytes) = 1_KB;
diff --git a/src/yb/integration-tests/xcluster/xcluster_safe_time-itest.cc b/src/yb/integration-tests/xcluster/xcluster_safe_time-itest.cc
index 73a48c61a0..6314a3e4d9 100644
--- a/src/yb/integration-tests/xcluster/xcluster_safe_time-itest.cc
+++ b/src/yb/integration-tests/xcluster/xcluster_safe_time-itest.cc
@@ -364,6 +364,8 @@ TEST_F(XClusterSafeTimeTest, SafeTimeInTableDoesNotGoBackwards) {
     ASSERT_GT(safe_ht, low_safe_time);
   }
   ASSERT_OK(table_scan_status);
+
+  sync_point_instance->DisableProcessing();
 }
 
 // Make sure safe time computation is not affected by the yb-master leader lease loss.
diff --git a/src/yb/integration-tests/xcluster/xcluster_ysql-test.cc b/src/yb/integration-tests/xcluster/xcluster_ysql-test.cc
index c64fef5114..7b84107ad3 100644
--- a/src/yb/integration-tests/xcluster/xcluster_ysql-test.cc
+++ b/src/yb/integration-tests/xcluster/xcluster_ysql-test.cc
@@ -277,7 +277,7 @@ class XClusterYSqlTestConsistentTransactionsTest : public XClusterYsqlTest {
       auto query = Format("SELECT COUNT(*) FROM $0", GetCompleteTableName(consumer_table));
       while (CoarseMonoClock::Now() < now + duration) {
         auto count = ASSERT_RESULT(consumer_conn.FetchRow<pgwrapper::PGUint64>(query));
-        ASSERT_EQ(count % transaction_size, 0);
+        ASSERT_EQ(count % transaction_size, 0) << count;
       }
     });
   }
@@ -935,18 +935,25 @@ TEST_F(XClusterYSqlTestConsistentTransactionsTest, TransactionsSpanningConsensus
 TEST_F(XClusterYSqlTestConsistentTransactionsTest, ReplicationPause) {
   ASSERT_OK(CreateTableAndSetupReplication());
 
-  auto duration = MonoDelta::FromSeconds(kTransactionalConsistencyTestDurationSecs);
+  const auto run_duration = 10s;
+  const auto run_count = 12;  // Run for 2min.
   auto test_thread_holder = TestThreadHolder();
   AsyncTransactionConsistencyTest(
-      producer_table_->name(), consumer_table_->name(), &test_thread_holder, duration);
-  SleepFor(duration / 2);
-  // Pause replication here for half the duration of the workload.
-  ASSERT_OK(
-      ToggleUniverseReplication(consumer_cluster(), consumer_client(), kReplicationGroupId, false));
-  SleepFor(duration / 2);
-  // Resume replication.
-  ASSERT_OK(
-      ToggleUniverseReplication(consumer_cluster(), consumer_client(), kReplicationGroupId, true));
+      producer_table_->name(), consumer_table_->name(), &test_thread_holder,
+      run_duration * (run_count + 1));
+
+  for (int i = 0; i < run_count; i++) {
+    LOG(INFO) << "Run count: " << i;
+    SleepFor(run_duration / 2);
+    // Pause replication here for half the duration of the workload.
+    ASSERT_OK(ToggleUniverseReplication(
+        consumer_cluster(), consumer_client(), kReplicationGroupId, false));
+    SleepFor(run_duration / 2);
+    // Resume replication.
+    ASSERT_OK(ToggleUniverseReplication(
+        consumer_cluster(), consumer_client(), kReplicationGroupId, true));
+  }
+
   test_thread_holder.JoinAll();
   ASSERT_OK(VerifyWrittenRecords());
   ASSERT_OK(DeleteUniverseReplication());
diff --git a/src/yb/tserver/xcluster_consumer.cc b/src/yb/tserver/xcluster_consumer.cc
index 1c577c0302..0184901e72 100644
--- a/src/yb/tserver/xcluster_consumer.cc
+++ b/src/yb/tserver/xcluster_consumer.cc
@@ -780,6 +780,7 @@ Status XClusterConsumer::PublishXClusterSafeTimeInternal() {
 
     VLOG_WITH_FUNC(2) << "Key: " << key.ToString()
                       << ", Producer TableId: " << producer_info.table_id
+                      << ", Producer TabletId: " << producer_info.tablet_id
                       << ", SafeTime: " << safe_time.ToDebugString();
     session->Apply(std::move(op));
   }
diff --git a/src/yb/tserver/xcluster_poller.cc b/src/yb/tserver/xcluster_poller.cc
index 0a72d3e939..c39efb42a8 100644
--- a/src/yb/tserver/xcluster_poller.cc
+++ b/src/yb/tserver/xcluster_poller.cc
@@ -278,6 +278,7 @@ void XClusterPoller::SchedulePoll() {
   if (is_paused_) {
     // Run immediately.
     ScheduleFunc(BIND_FUNCTION_AND_ARGS(XClusterPoller::DoPoll));
+    return;
   }
 
   // determine if we should delay our upcoming poll
