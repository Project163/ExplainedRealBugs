diff --git a/src/yb/cdc/cdc_service.proto b/src/yb/cdc/cdc_service.proto
index 66b332b81d..e01ea80929 100644
--- a/src/yb/cdc/cdc_service.proto
+++ b/src/yb/cdc/cdc_service.proto
@@ -504,13 +504,20 @@ message UpdateCdcReplicatedIndexResponsePB {
 
 message BootstrapProducerRequestPB {
   repeated string table_ids = 1;
+  // If the stream_ids are not provided, a new stream for each table will be created.
+  repeated bytes xrepl_stream_ids = 2;
+  // Check if data bootstrap is required. Equivalent to the IsBootstrapRequired RPC.
+  optional bool check_if_bootstrap_required = 3; // [default = false]
 }
 
 message BootstrapProducerResponsePB {
   optional CDCErrorPB error = 1;
+  // New streams that were created. Only happens if the request does not have xrepl_stream_ids.
   repeated bytes cdc_bootstrap_ids = 2;
   // Minimum time after which data is available on all the bootstrapped streams.
   optional fixed64 bootstrap_time = 3;
+  // Only set if check_if_bootstrap_required is set on the request.
+  optional bool bootstrap_required = 4; // [default = false]
 }
 
 message GetLatestEntryOpIdRequestPB {
diff --git a/src/yb/cdc/cdc_state_table.cc b/src/yb/cdc/cdc_state_table.cc
index a068570afe..f8eb63438c 100644
--- a/src/yb/cdc/cdc_state_table.cc
+++ b/src/yb/cdc/cdc_state_table.cc
@@ -16,6 +16,7 @@
 
 #include "yb/client/async_initializer.h"
 #include "yb/client/client.h"
+#include "yb/client/error.h"
 #include "yb/client/schema.h"
 #include "yb/client/session.h"
 #include "yb/client/table_handle.h"
@@ -196,6 +197,13 @@ Result<CDCStateTableEntry> DeserializeRow(
 }
 }  // namespace
 
+CDCStateTable::CDCStateTable(client::AsyncClientInitializer* async_client_init)
+    : async_client_init_(async_client_init) {
+  CHECK_NOTNULL(async_client_init);
+}
+
+CDCStateTable::CDCStateTable(client::YBClient* client) : client_(client) { CHECK_NOTNULL(client); }
+
 std::string CDCStateTableKey::ToString() const {
   return Format(
       "TabletId: $0, StreamId: $1 $2", tablet_id, stream_id,
@@ -328,6 +336,7 @@ Result<std::shared_ptr<client::TableHandle>> CDCStateTable::GetTable() {
 
 Result<client::YBClient*> CDCStateTable::GetClient() {
   if (!client_) {
+    SCHECK_NOTNULL(async_client_init_);
     client_ = async_client_init_->client();
   }
 
@@ -342,11 +351,12 @@ Result<std::shared_ptr<client::YBSession>> CDCStateTable::GetSession() {
 }
 
 template <class CDCEntry>
-Status CDCStateTable::WriteEntries(
+Status CDCStateTable::WriteEntriesAsync(
     const std::vector<CDCEntry>& entries, QLWriteRequestPB::QLStmtType statement_type,
-    QLOperator condition_op, const bool replace_full_map,
+    StdStatusCallback callback, QLOperator condition_op, const bool replace_full_map,
     const std::vector<std::string>& keys_to_delete) {
   if (entries.empty()) {
+    callback(Status::OK());
     return Status::OK();
   }
 
@@ -386,15 +396,45 @@ Status CDCStateTable::WriteEntries(
     }
   }
 
+  session->Apply(std::move(ops));
+  session->FlushAsync([callback = std::move(callback)](client::FlushStatus* flush_status) {
+    for (auto& error : flush_status->errors) {
+      LOG_WITH_FUNC(WARNING) << "Flush of operation " << error->failed_op().ToString()
+                             << " failed: " << error->status();
+    }
+    callback(std::move(flush_status->status));
+  });
+  return Status::OK();
+}
+
+template <class CDCEntry>
+Status CDCStateTable::WriteEntries(
+    const std::vector<CDCEntry>& entries, QLWriteRequestPB::QLStmtType statement_type,
+    QLOperator condition_op, const bool replace_full_map,
+    const std::vector<std::string>& keys_to_delete) {
   // TODO(async_flush): https://github.com/yugabyte/yugabyte-db/issues/12173
-  return session->TEST_ApplyAndFlush(ops);
+  Synchronizer sync;
+  RETURN_NOT_OK(WriteEntriesAsync<CDCEntry>(
+      entries, statement_type, sync.AsStdStatusCallback(), condition_op, replace_full_map,
+      keys_to_delete));
+  return sync.Wait();
+}
+
+Status CDCStateTable::InsertEntriesAsync(
+    const std::vector<CDCStateTableEntry>& entries, StdStatusCallback callback) {
+  VLOG_WITH_FUNC(1) << yb::ToString(entries);
+  return WriteEntriesAsync(
+      entries, QLWriteRequestPB::QL_STMT_INSERT, std::move(callback), QL_OP_NOT_EXISTS,
+      /*replace_full_map=*/true);
 }
 
 Status CDCStateTable::InsertEntries(
     const std::vector<CDCStateTableEntry>& entries) {
   VLOG_WITH_FUNC(1) << yb::ToString(entries);
-  return WriteEntries(
-      entries, QLWriteRequestPB::QL_STMT_INSERT, QL_OP_NOT_EXISTS, true);
+  // TODO(async_flush): https://github.com/yugabyte/yugabyte-db/issues/12173
+  Synchronizer sync;
+  RETURN_NOT_OK(InsertEntriesAsync(entries, sync.AsStdStatusCallback()));
+  return sync.Wait();
 }
 
 Status CDCStateTable::UpdateEntries(
diff --git a/src/yb/cdc/cdc_state_table.h b/src/yb/cdc/cdc_state_table.h
index 6918ba4c09..f8f7a863ed 100644
--- a/src/yb/cdc/cdc_state_table.h
+++ b/src/yb/cdc/cdc_state_table.h
@@ -106,17 +106,16 @@ class CDCStateTableRange;
 // uses the YBClient and YBSession to access the table.
 class CDCStateTable {
  public:
-  explicit CDCStateTable(client::AsyncClientInitializer* async_client_init)
-      : async_client_init_(async_client_init) {}
-  explicit CDCStateTable(client::YBClient* client) : client_(client) {}
+  explicit CDCStateTable(client::AsyncClientInitializer* async_client_init);
+  explicit CDCStateTable(client::YBClient* client);
 
   static const std::string& GetNamespaceName();
   static const std::string& GetTableName();
   static Result<master::CreateTableRequestPB> GenerateCreateCdcStateTableRequest();
 
-  Status InsertEntries(
-      const std::vector<CDCStateTableEntry>& entries)
-      EXCLUDES(mutex_);
+  Status InsertEntries(const std::vector<CDCStateTableEntry>& entries) EXCLUDES(mutex_);
+  Status InsertEntriesAsync(
+      const std::vector<CDCStateTableEntry>& entries, StdStatusCallback callback) EXCLUDES(mutex_);
   Status UpdateEntries(
       const std::vector<CDCStateTableEntry>& entries, const bool replace_full_map = false,
       const std::vector<std::string>& keys_to_delete = {}) EXCLUDES(mutex_);
@@ -142,6 +141,13 @@ class CDCStateTable {
   Status WaitForCreateTableToFinishWithoutCache();
   Result<std::shared_ptr<client::TableHandle>> GetTable() EXCLUDES(mutex_);
   Status OpenTable(client::TableHandle* cdc_table);
+  template <class CDCEntry>
+  Status WriteEntriesAsync(
+      const std::vector<CDCEntry>& entries, QLWriteRequestPB::QLStmtType statement_type,
+      StdStatusCallback callback, QLOperator condition_op = QL_OP_NOOP,
+      const bool replace_full_map = false, const std::vector<std::string>& keys_to_delete = {})
+      EXCLUDES(mutex_);
+
   template <class CDCEntry>
   Status WriteEntries(
       const std::vector<CDCEntry>& entries, QLWriteRequestPB::QLStmtType statement_type,
diff --git a/src/yb/cdc/xcluster_producer_bootstrap.cc b/src/yb/cdc/xcluster_producer_bootstrap.cc
index 9587c5c1da..2e65e6019c 100644
--- a/src/yb/cdc/xcluster_producer_bootstrap.cc
+++ b/src/yb/cdc/xcluster_producer_bootstrap.cc
@@ -29,36 +29,44 @@ DECLARE_int32(cdc_write_rpc_timeout_ms);
 namespace yb {
 namespace cdc {
 
+namespace {
+
+Result<bool> IsDataPresent(tablet::TabletPeerPtr tablet_peer) {
+  // This is done by calling FetchNext on NewRowIterator to see if there is any row is visible
+  // (not deleted). For colocated tablets, each table has to be checked.
+  //
+  // We cannot rely on existence of data in WAL. Only locally generated data is
+  // replicated via xcluster. This is done to prevent infinite replication loop in bidirectional
+  // mode. If the data in the WAL was from a prior xcluster stream (xcluster DR cases) then it
+  // will not get replicated even if we can read the log. Reading the entire log to determine if
+  // any entries are external is can be too expensive. Also the user could have accidentally
+  // inserted data but then deleted it before adding the table to replication, which we cannot
+  // detect by scanning the WAL.
+
+  const auto tablet = VERIFY_RESULT(tablet_peer->shared_tablet_safe());
+  auto table_ids = tablet->metadata()->GetAllColocatedTables();
+  const dockv::ReaderProjection empty_projection;
+
+  // We will have multiple tables when this is a colocated table.
+  for (const auto& table_id : table_ids) {
+    auto iter = VERIFY_RESULT(
+        tablet->NewRowIterator(empty_projection, /* read_hybrid_time */ {}, table_id));
+    if (VERIFY_RESULT(iter->FetchNext(nullptr))) {
+      LOG(INFO) << "Tablet " << tablet_peer->tablet_id() << " has rows in table " << table_id
+                << ". Bootstrap is required when setting up xCluster.";
+      return true;
+    }
+  }
+  return false;
+}
+
+}  // namespace
+
 Result<bool> IsBootstrapRequiredForTablet(
     tablet::TabletPeerPtr tablet_peer, const OpId& min_op_id, const CoarseTimePoint& deadline) {
   if (min_op_id.index < 0) {
     // Bootstrap is needed if there is any data in the tablet.
-    // This is done by calling FetchNext on NewRowIterator to see if there is any row is visible
-    // (not deleted). For colocated tablets, each table has to be checked.
-    //
-    // We cannot rely on existence of data in WAL. Only locally generated data is
-    // replicated via xcluster. This is done to prevent infinite replication loop in bidirectional
-    // mode. If the data in the WAL was from a prior xcluster stream (xcluster DR cases) then it
-    // will not get replicated even if we can read the log. Reading the entire log to determine if
-    // any entries are external is can be too expensive. Also the user could have accidentally
-    // inserted data but then deleted it before adding the table to replication, which we cannot
-    // detect by scanning the WAL.
-
-    const auto tablet = VERIFY_RESULT(tablet_peer->shared_tablet_safe());
-    auto table_ids = tablet->metadata()->GetAllColocatedTables();
-    const dockv::ReaderProjection empty_projection;
-
-    // We will have multiple tables when this is a colocated table.
-    for (const auto& table_id : table_ids) {
-      auto iter = VERIFY_RESULT(
-          tablet->NewRowIterator(empty_projection, /* read_hybrid_time */ {}, table_id));
-      if (VERIFY_RESULT(iter->FetchNext(nullptr))) {
-        LOG(INFO) << "Tablet " << tablet_peer->tablet_id() << " has rows in table " << table_id
-                  << ". Bootstrap is required when setting up xCluster.";
-        return true;
-      }
-    }
-    return false;
+    return IsDataPresent(tablet_peer);
   }
 
   auto log = tablet_peer->log();
@@ -109,12 +117,29 @@ Status XClusterProducerBootstrap::RunBootstrapProducer() {
   LOG_WITH_FUNC(INFO) << "Updating cdc_state table with checkpoints.";
   RETURN_NOT_OK(UpdateCdcStateTableWithCheckpoints());
 
+  if (req_.check_if_bootstrap_required()) {
+    resp_->set_bootstrap_required(VERIFY_RESULT(IsBootstrapRequired()));
+  }
+
   PrepareResponse();
+
   LOG_WITH_FUNC(INFO) << "Finished.";
   return Status::OK();
 }
 
 Status XClusterProducerBootstrap::CreateAllBootstrapStreams() {
+  if (!req_.xrepl_stream_ids().empty()) {
+    SCHECK_EQ(
+        req_.xrepl_stream_ids_size(), req_.table_ids_size(), InvalidArgument,
+        "xrepl_stream_ids must be empty or have the same number of entries as table_ids");
+
+    for (int i = 0; i < req_.xrepl_stream_ids_size(); i++) {
+      auto stream_id = VERIFY_RESULT(xrepl::StreamId::FromString(req_.xrepl_stream_ids(i)));
+      bootstrap_ids_and_tables_.emplace_back(stream_id, req_.table_ids(i));
+    }
+    return Status::OK();
+  }
+
   std::unordered_map<std::string, std::string> options;
   options.reserve(2);
   options.emplace(cdc::kRecordType, CDCRecordType_Name(cdc::CDCRecordType::CHANGE));
@@ -150,16 +175,25 @@ Status XClusterProducerBootstrap::ConstructServerToTabletsMapping() {
       // Initially store invalid op_id to catch any missed streams later in VerifyTabletOpIds.
       tablet_op_ids_[bootstrap_tablet_pair] = yb::OpId::Invalid();
 
-      // Get remote servers for tablet.
+      auto remote_tablet = VERIFY_RESULT(cdc_service_->GetRemoteTablet(tablet_id));
+
       std::vector<client::internal::RemoteTabletServer*> servers;
-      RETURN_NOT_OK(cdc_service_->GetTServers(tablet_id, &servers));
+      remote_tablet->GetRemoteTabletServers(&servers);
+
+      auto ts_leader = remote_tablet->LeaderTServer();
+      SCHECK(ts_leader, NotFound, "Tablet leader not found for tablet", tablet_id);
+      if (ts_leader->IsLocal()) {
+        local_leader_tablets_.emplace_back(tablet_id);
+      } else {
+        server_to_remote_leader_tablets_[ts_leader->permanent_uuid()].emplace_back(tablet_id);
+      }
 
       // Store remote tablet information so we can do batched rpc calls.
-      bool has_local_peer = false;
+      bool has_any_local_peer = false;
       for (const auto& server : servers) {
         // We modify our log directly. Avoid calling itself through the proxy.
         if (server->IsLocal()) {
-          has_local_peer = true;
+          has_any_local_peer = true;
           local_tablets_.emplace(bootstrap_id, tablet_id);
           continue;
         }
@@ -170,11 +204,10 @@ Status XClusterProducerBootstrap::ConstructServerToTabletsMapping() {
         // Add tablet to the tablet list for this server
         server_to_remote_tablets_[server_id].push_back(bootstrap_tablet_pair);
       }
-      if (!has_local_peer) {
+      if (!has_any_local_peer) {
         remote_tablets_to_fetch_opids_for_.emplace(bootstrap_id, tablet_id);
       }
     }
-    bootstrap_ids_.push_back(std::move(bootstrap_id));
   }
   return Status::OK();
 }
@@ -251,9 +284,7 @@ Status XClusterProducerBootstrap::FetchLatestOpIdsFromRemotePeers() {
   }
 
   // Stores number of async rpc calls that have returned.
-  std::atomic<uint> finished_tasks{0};
-  std::promise<void> get_op_id_promise;
-  auto get_op_id_future = get_op_id_promise.get_future();
+  CountDownLatch rpcs_done(server_to_remote_tablet_leader.size());
   // Store references to the rpc and response objects so they don't go out of scope.
   std::vector<std::shared_ptr<rpc::RpcController>> rpcs;
   std::unordered_map<std::string, std::shared_ptr<GetLatestEntryOpIdResponsePB>>
@@ -277,14 +308,10 @@ Status XClusterProducerBootstrap::FetchLatestOpIdsFromRemotePeers() {
     rpc.get()->set_timeout(MonoDelta::FromMilliseconds(FLAGS_cdc_write_rpc_timeout_ms));
 
     proxy->GetLatestEntryOpIdAsync(
-        get_op_id_req, get_op_id_resp.get(), rpc.get(),
-        std::bind(
-            &XClusterProducerBootstrap::AsyncPromiseCallback, this, &get_op_id_promise,
-            &finished_tasks, server_to_remote_tablet_leader.size()));
+        get_op_id_req, get_op_id_resp.get(), rpc.get(), rpcs_done.CountDownCallback());
   }
 
-  // Wait for all async rpc calls to finish.
-  get_op_id_future.wait();
+  rpcs_done.Wait();
 
   // Parse responses and update producer_entries_modified and tablet_checkpoints_.
   std::string get_op_id_err_message;
@@ -363,9 +390,7 @@ Status XClusterProducerBootstrap::SetLogRetentionForRemoteTabletPeers() {
     return Status::OK();
   }
 
-  std::atomic<uint> finished_tasks{0};
-  std::promise<void> update_index_promise;
-  auto update_index_future = update_index_promise.get_future();
+  CountDownLatch rpcs_done(server_to_remote_tablets_.size());
   // Store references to the rpc and response objects so they don't go out of scope.
   std::vector<std::shared_ptr<rpc::RpcController>> rpcs;
   std::vector<std::shared_ptr<UpdateCdcReplicatedIndexResponsePB>> update_index_responses;
@@ -391,14 +416,11 @@ Status XClusterProducerBootstrap::SetLogRetentionForRemoteTabletPeers() {
     rpc.get()->set_timeout(MonoDelta::FromMilliseconds(FLAGS_cdc_write_rpc_timeout_ms));
 
     proxy->UpdateCdcReplicatedIndexAsync(
-        update_index_req, update_index_resp.get(), rpc.get(),
-        std::bind(
-            &XClusterProducerBootstrap::AsyncPromiseCallback, this, &update_index_promise,
-            &finished_tasks, server_to_remote_tablets_.size()));
+        update_index_req, update_index_resp.get(), rpc.get(), rpcs_done.CountDownCallback());
   }
 
   // Wait for all async calls to finish.
-  update_index_future.wait();
+  rpcs_done.Wait();
 
   // Check all responses for errors.
   for (const auto& update_index_resp : update_index_responses) {
@@ -424,21 +446,76 @@ Status XClusterProducerBootstrap::UpdateCdcStateTableWithCheckpoints() {
 }
 
 void XClusterProducerBootstrap::PrepareResponse() {
-  // Update response with bootstrap ids.
-  for (const auto& bootstrap_id : bootstrap_ids_) {
-    resp_->add_cdc_bootstrap_ids(bootstrap_id.ToString());
+  if (req_.xrepl_stream_ids().empty()) {
+    // Update response with bootstrap ids if it was not already provided.
+    for (const auto& [bootstrap_id, _] : bootstrap_ids_and_tables_) {
+      resp_->add_cdc_bootstrap_ids(bootstrap_id.ToString());
+    }
   }
+
   if (!bootstrap_time_.is_special()) {
     resp_->set_bootstrap_time(bootstrap_time_.ToUint64());
   }
 }
 
-void XClusterProducerBootstrap::AsyncPromiseCallback(
-    std::promise<void>* const promise, std::atomic<uint>* const finished_tasks, uint total_tasks) {
-  // If this is the last of the tasks to finish, then mark the promise as fulfilled.
-  if (++(*finished_tasks) == total_tasks) {
-    promise->set_value();
+Result<bool> XClusterProducerBootstrap::IsBootstrapRequired() {
+  // NOTE: There is a rare edge case where the data may be deleted between bootstrap time and now.
+  // By itself it will cause the consumer to miss some data for a brief duration during the setup of
+  // xCluster. We dont think this is going to be a common pattern, so we ignore it.
+
+  // Check local tablets first. Exit early even if one tablet has data in it.
+  for (const auto& tablet_id : local_leader_tablets_) {
+    auto tablet_peer = cdc_service_context_->LookupTablet(tablet_id);
+
+    SCHECK(
+        tablet_peer && tablet_peer->IsLeaderAndReady(), LeaderNotReadyToServe,
+        "Tablet leader is not ready $0", tablet_id);
+
+    if (VERIFY_RESULT(IsDataPresent(tablet_peer))) {
+      return true;
+    }
+  }
+
+  for (const auto& [server_id, _] : server_to_remote_leader_tablets_) {
+    SCHECK(server_to_proxy_.contains(server_id), TryAgain, "Tablets have moved.");
+  }
+
+  CountDownLatch rpcs_done(server_to_remote_leader_tablets_.size());
+
+  std::vector<std::shared_ptr<rpc::RpcController>> rpcs;
+  std::vector<std::shared_ptr<IsBootstrapRequiredResponsePB>> responses;
+
+  for (const auto& [server_id, tablet_list] : server_to_remote_leader_tablets_) {
+    IsBootstrapRequiredRequestPB req;
+    auto resp = std::make_shared<IsBootstrapRequiredResponsePB>();
+    auto rpc = std::make_shared<rpc::RpcController>();
+
+    // Store pointers to response and rpc object.
+    responses.push_back(resp);
+    rpcs.push_back(rpc);
+
+    req.mutable_tablet_ids()->Reserve(narrow_cast<int>(tablet_list.size()));
+    for (const auto& tablet_id : tablet_list) {
+      req.add_tablet_ids(tablet_id);
+    }
+
+    rpc->set_timeout(MonoDelta::FromMilliseconds(FLAGS_cdc_write_rpc_timeout_ms));
+
+    server_to_proxy_[server_id]->IsBootstrapRequiredAsync(
+        req, resp.get(), rpc.get(), rpcs_done.CountDownCallback());
+  }
+
+  rpcs_done.Wait();
+
+  for (const auto& resp : responses) {
+    if (resp->has_error()) {
+      return StatusFromPB(resp->error().status());
+    }
+    if (resp->bootstrap_required()) {
+      return true;
+    }
   }
+  return false;
 }
 
 }  // namespace cdc
diff --git a/src/yb/cdc/xcluster_producer_bootstrap.h b/src/yb/cdc/xcluster_producer_bootstrap.h
index c0e421e882..f47d4d6ed5 100644
--- a/src/yb/cdc/xcluster_producer_bootstrap.h
+++ b/src/yb/cdc/xcluster_producer_bootstrap.h
@@ -62,12 +62,7 @@ class XClusterProducerBootstrap {
 
   void PrepareResponse();
 
-  // Used as a callback function for parallelizing async cdc rpc calls.
-  // Given a finished tasks counter, and the number of total rpc calls
-  // in flight, the callback will increment the counter when called, and
-  // set the promise to be fulfilled when all tasks have completed.
-  void AsyncPromiseCallback(
-      std::promise<void>* const promise, std::atomic<uint>* const finished_tasks, uint total_tasks);
+  Result<bool> IsBootstrapRequired();
 
   CDCServiceImpl* cdc_service_;
   const BootstrapProducerRequestPB& req_;
@@ -82,13 +77,14 @@ class XClusterProducerBootstrap {
       std::unordered_map<std::string, std::vector<BootstrapTabletPair>>;
   using BTPHash = boost::hash<BootstrapTabletPair>;
 
-  std::vector<xrepl::StreamId> bootstrap_ids_;
   std::vector<std::pair<xrepl::StreamId, TableId>> bootstrap_ids_and_tables_;
   std::unordered_map<BootstrapTabletPair, yb::OpId, BTPHash> tablet_op_ids_;
   ServerToCDCServiceMap server_to_proxy_;
   ServerToBootstrapTabletPairMap server_to_remote_tablets_;
   std::unordered_set<BootstrapTabletPair, BTPHash> local_tablets_;
   std::unordered_set<BootstrapTabletPair, BTPHash> remote_tablets_to_fetch_opids_for_;
+  std::vector<TabletId> local_leader_tablets_;
+  std::unordered_map<std::string, std::vector<TabletId>> server_to_remote_leader_tablets_;
   HybridTime bootstrap_time_ = HybridTime::kMin;
 };
 
diff --git a/src/yb/client/client-internal.cc b/src/yb/client/client-internal.cc
index 455cd34e6e..ccabbc3ed8 100644
--- a/src/yb/client/client-internal.cc
+++ b/src/yb/client/client-internal.cc
@@ -2159,21 +2159,21 @@ class GetXClusterStreamsRpc
         std::bind(&GetXClusterStreamsRpc::Finished, this, Status::OK()));
   }
 
-  void ProcessResponse(const Status& status) override {
-    auto status_copy = status;
-    if (status_copy.ok() && resp_.has_error()) {
-      status_copy = StatusFromPB(resp_.error().status());
+  Status ResponseStatus() override {
+    RETURN_NOT_OK(internal::StatusFromResp(resp_));
+    if (resp_.not_ready()) {
+      VLOG(2) << ToString() << ": xClusterOutboundReplicationGroup is not ready yet, retrying.";
+      return STATUS(TryAgain, "xClusterOutboundReplicationGroup is not ready yet");
     }
+    return Status::OK();
+  }
 
-    if (!status_copy.ok()) {
-      LOG(WARNING) << ToString() << " failed: " << status_copy;
-      user_cb_(std::move(status_copy));
-      return;
-    }
+  bool ShouldRetry(const Status& status) override { return status.IsTryAgain(); }
 
-    if (resp_.not_ready()) {
-      VLOG(2) << ToString() << ": xClusterOutboundReplicationGroup is not ready yet, retrying.";
-      ScheduleRetry(STATUS(TryAgain, "xClusterOutboundReplicationGroup is not ready yet"));
+  void ProcessResponse(const Status& status) override {
+    if (!status.ok()) {
+      LOG(WARNING) << ToString() << " failed: " << status;
+      user_cb_(std::move(status));
       return;
     }
 
@@ -2214,21 +2214,21 @@ class IsXClusterBootstrapRequiredRpc : public ClientMasterRpc<
         std::bind(&IsXClusterBootstrapRequiredRpc::Finished, this, Status::OK()));
   }
 
-  void ProcessResponse(const Status& status) override {
-    auto status_copy = status;
-    if (status_copy.ok() && resp_.has_error()) {
-      status_copy = StatusFromPB(resp_.error().status());
+  Status ResponseStatus() override {
+    RETURN_NOT_OK(internal::StatusFromResp(resp_));
+    if (resp_.not_ready()) {
+      VLOG(2) << ToString() << ": xClusterOutboundReplicationGroup is not ready yet, retrying.";
+      return STATUS(TryAgain, "xClusterOutboundReplicationGroup is not ready yet");
     }
+    return Status::OK();
+  }
 
-    if (!status_copy.ok()) {
-      LOG(WARNING) << ToString() << " failed: " << status_copy;
-      user_cb_(std::move(status_copy));
-      return;
-    }
+  bool ShouldRetry(const Status& status) override { return status.IsTryAgain(); }
 
-    if (resp_.not_ready()) {
-      VLOG(2) << ToString() << ": xClusterOutboundReplicationGroup is not ready yet, retrying.";
-      ScheduleRetry(STATUS(TryAgain, "xClusterOutboundReplicationGroup is not ready yet"));
+  void ProcessResponse(const Status& status) override {
+    if (!status.ok()) {
+      LOG(WARNING) << ToString() << " failed: " << status;
+      user_cb_(std::move(status));
       return;
     }
 
diff --git a/src/yb/integration-tests/xcluster/xcluster_outbound_replication_group-itest.cc b/src/yb/integration-tests/xcluster/xcluster_outbound_replication_group-itest.cc
index 83d957197a..410cf7a8aa 100644
--- a/src/yb/integration-tests/xcluster/xcluster_outbound_replication_group-itest.cc
+++ b/src/yb/integration-tests/xcluster/xcluster_outbound_replication_group-itest.cc
@@ -20,6 +20,7 @@
 
 DECLARE_bool(TEST_enable_xcluster_api_v2);
 DECLARE_uint32(cdc_wal_retention_time_secs);
+DECLARE_uint32(max_xcluster_streams_to_checkpoint_in_parallel);
 
 namespace yb {
 namespace master {
@@ -139,6 +140,8 @@ class XClusterOutboundReplicationGroupTest : public XClusterYsqlTestBase {
 };
 
 TEST_F(XClusterOutboundReplicationGroupTest, TestMultipleTable) {
+  ANNOTATE_UNPROTECTED_WRITE(FLAGS_max_xcluster_streams_to_checkpoint_in_parallel) = 1;
+
   // Create two tables in two schemas.
   auto table_id_1 = ASSERT_RESULT(CreateYsqlTable(kNamespaceName, kTableName1));
   PgSchemaName pg_schema_name2 = "myschema";
@@ -204,6 +207,9 @@ TEST_F(XClusterOutboundReplicationGroupTest, AddDeleteNamespaces) {
   ASSERT_EQ(out_namespace_id.size(), 1);
   ASSERT_EQ(out_namespace_id[0], namespace_id_);
 
+  // Wait for the new streams to be ready.
+  auto ns1_info = ASSERT_RESULT(GetXClusterStreams(kReplicationGroupId, namespace_id_));
+
   // We should have 2 streams now.
   auto all_xcluster_streams_initial = CleanupAndGetAllXClusterStreams();
   ASSERT_EQ(all_xcluster_streams_initial.size(), 2);
@@ -214,7 +220,6 @@ TEST_F(XClusterOutboundReplicationGroupTest, AddDeleteNamespaces) {
   // Make sure only the namespace that was added is returned.
   ASSERT_NOK(GetXClusterStreams(kReplicationGroupId, namespace_id_2));
 
-  auto ns1_info = ASSERT_RESULT(GetXClusterStreams(kReplicationGroupId, namespace_id_));
   ASSERT_NO_FATALS(VerifyNamespaceCheckpointInfo(
       ns1_table_id_1, ns1_table_id_2, all_xcluster_streams_initial, ns1_info));
 
@@ -263,6 +268,9 @@ TEST_F(XClusterOutboundReplicationGroupTest, AddTable) {
 
   ASSERT_OK(client_->XClusterCreateOutboundReplicationGroup(kReplicationGroupId, {kNamespaceName}));
 
+  // Wait for the new streams to be ready.
+  ASSERT_OK(GetXClusterStreams(kReplicationGroupId, namespace_id_));
+
   auto all_xcluster_streams_initial = CleanupAndGetAllXClusterStreams();
   ASSERT_EQ(all_xcluster_streams_initial.size(), 1);
 
@@ -280,5 +288,64 @@ TEST_F(XClusterOutboundReplicationGroupTest, AddTable) {
   ASSERT_OK(VerifyWalRetentionOfTable(table_id_2));
 }
 
+TEST_F(XClusterOutboundReplicationGroupTest, IsBootstrapRequiredEmptyTable) {
+  ANNOTATE_UNPROTECTED_WRITE(FLAGS_max_xcluster_streams_to_checkpoint_in_parallel) = 1;
+
+  auto table_id_1 = ASSERT_RESULT(CreateYsqlTable(kNamespaceName, kTableName1));
+  ASSERT_OK(client_->XClusterCreateOutboundReplicationGroup(kReplicationGroupId, {kNamespaceName}));
+
+  std::promise<Result<bool>> promise;
+
+  ASSERT_OK(client_->IsXClusterBootstrapRequired(
+      CoarseMonoClock::Now() + kDeadline, kReplicationGroupId, namespace_id_,
+      [&promise](Result<bool> result) { promise.set_value(std::move(result)); }));
+
+  auto is_bootstrap_required = ASSERT_RESULT(promise.get_future().get());
+  ASSERT_FALSE(is_bootstrap_required);
+}
+
+TEST_F(XClusterOutboundReplicationGroupTest, IsBootstrapRequiredTableWithData) {
+  ANNOTATE_UNPROTECTED_WRITE(FLAGS_max_xcluster_streams_to_checkpoint_in_parallel) = 1;
+
+  auto table_id_1 = ASSERT_RESULT(CreateYsqlTable(kNamespaceName, kTableName1));
+  auto table_id_2 = ASSERT_RESULT(CreateYsqlTable(kNamespaceName, kTableName2));
+  std::shared_ptr<client::YBTable> table_2;
+  ASSERT_OK(producer_client()->OpenTable(table_id_2, &table_2));
+  ASSERT_OK(InsertRowsInProducer(0, 10, table_2));
+
+  ASSERT_OK(client_->XClusterCreateOutboundReplicationGroup(kReplicationGroupId, {kNamespaceName}));
+
+  std::promise<Result<bool>> promise;
+
+  ASSERT_OK(client_->IsXClusterBootstrapRequired(
+      CoarseMonoClock::Now() + kDeadline, kReplicationGroupId, namespace_id_,
+      [&promise](Result<bool> result) { promise.set_value(std::move(result)); }));
+
+  auto is_bootstrap_required = ASSERT_RESULT(promise.get_future().get());
+  ASSERT_TRUE(is_bootstrap_required);
+}
+
+TEST_F(XClusterOutboundReplicationGroupTest, IsBootstrapRequiredTableWithDeletedData) {
+  ANNOTATE_UNPROTECTED_WRITE(FLAGS_max_xcluster_streams_to_checkpoint_in_parallel) = 1;
+
+  auto table_id_1 = ASSERT_RESULT(CreateYsqlTable(kNamespaceName, kTableName1));
+  auto table_id_2 = ASSERT_RESULT(CreateYsqlTable(kNamespaceName, kTableName2));
+  std::shared_ptr<client::YBTable> table_2;
+  ASSERT_OK(producer_client()->OpenTable(table_id_2, &table_2));
+  ASSERT_OK(InsertRowsInProducer(0, 10, table_2));
+  ASSERT_OK(DeleteRowsInProducer(0, 10, table_2));
+
+  ASSERT_OK(client_->XClusterCreateOutboundReplicationGroup(kReplicationGroupId, {kNamespaceName}));
+
+  std::promise<Result<bool>> promise;
+
+  ASSERT_OK(client_->IsXClusterBootstrapRequired(
+      CoarseMonoClock::Now() + kDeadline, kReplicationGroupId, namespace_id_,
+      [&promise](Result<bool> result) { promise.set_value(std::move(result)); }));
+
+  auto is_bootstrap_required = ASSERT_RESULT(promise.get_future().get());
+  ASSERT_FALSE(is_bootstrap_required);
+}
+
 }  // namespace master
 }  // namespace yb
diff --git a/src/yb/master/CMakeLists.txt b/src/yb/master/CMakeLists.txt
index ca96efea30..254df78cb1 100644
--- a/src/yb/master/CMakeLists.txt
+++ b/src/yb/master/CMakeLists.txt
@@ -139,6 +139,7 @@ set(MASTER_SRCS
   xcluster/xcluster_consumer_metrics.cc
   xcluster/xcluster_manager.cc
   xcluster/xcluster_outbound_replication_group.cc
+  xcluster/xcluster_outbound_replication_group_tasks.cc
   xcluster/xcluster_replication_group.cc
   xcluster/xcluster_safe_time_service.cc
   xcluster/xcluster_source_manager.cc
diff --git a/src/yb/master/catalog_entity_info.proto b/src/yb/master/catalog_entity_info.proto
index 4f4a37634b..33997b607a 100644
--- a/src/yb/master/catalog_entity_info.proto
+++ b/src/yb/master/catalog_entity_info.proto
@@ -770,6 +770,7 @@ message SysXClusterOutboundReplicationGroupEntryPB {
     enum State {
       CHECKPOINTING = 0;
       READY = 1;
+      FAILED = 2;
     }
     required State state = 1;
 
@@ -784,6 +785,8 @@ message SysXClusterOutboundReplicationGroupEntryPB {
     map<string, TableInfoPB> table_infos = 2;
 
     optional bool initial_bootstrap_required = 3; // [default = false]
+
+    optional AppStatusPB error_status = 4; // Set when state is FAILED
   }
 
   // Namespace Id -> NamespaceInfoPB
diff --git a/src/yb/master/catalog_manager.cc b/src/yb/master/catalog_manager.cc
index e723d4a0dd..08a37d8114 100644
--- a/src/yb/master/catalog_manager.cc
+++ b/src/yb/master/catalog_manager.cc
@@ -4386,8 +4386,7 @@ Status CatalogManager::CreateTable(const CreateTableRequestPB* orig_req,
 
 Status CatalogManager::CreateTableIfNotFound(
     const std::string& namespace_name, const std::string& table_name,
-    std::function<Result<CreateTableRequestPB>()> generate_request, CreateTableResponsePB* resp,
-    rpc::RpcContext* rpc, const LeaderEpoch& epoch) {
+    std::function<Result<CreateTableRequestPB>()> generate_request, const LeaderEpoch& epoch) {
   // If table exists do nothing, otherwise create it.
   if (VERIFY_RESULT(TableExists(namespace_name, table_name))) {
     return Status::OK();
@@ -4397,9 +4396,13 @@ Status CatalogManager::CreateTableIfNotFound(
   DCHECK_EQ(req.namespace_().name(), namespace_name);
   DCHECK_EQ(req.name(), table_name);
 
-  Status s = CreateTable(&req, resp, /* RpcContext */ nullptr, epoch);
+  CreateTableResponsePB resp;
+  Status s = CreateTable(&req, &resp, /* RpcContext */ nullptr, epoch);
   // We do not lock here so it is technically possible that the table was already created.
   // If so, there is nothing to do so we just ignore the "AlreadyPresent" error.
+  if (s.ok() && resp.has_error()) {
+    s = StatusFromPB(resp.error().status());
+  }
   if (!s.ok() && !s.IsAlreadyPresent()) {
     return s;
   }
diff --git a/src/yb/master/catalog_manager.h b/src/yb/master/catalog_manager.h
index 1cce2dd21e..d3cd86a9a2 100644
--- a/src/yb/master/catalog_manager.h
+++ b/src/yb/master/catalog_manager.h
@@ -263,8 +263,8 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
 
   Status CreateTableIfNotFound(
       const std::string& namespace_name, const std::string& table_name,
-      std::function<Result<CreateTableRequestPB>()> generate_request, CreateTableResponsePB* resp,
-      rpc::RpcContext* rpc, const LeaderEpoch& epoch);
+      std::function<Result<CreateTableRequestPB>()> generate_request, const LeaderEpoch& epoch)
+      EXCLUDES(mutex_);
 
   // Create a new transaction status table.
   Status CreateTransactionStatusTable(const CreateTransactionStatusTableRequestPB* req,
@@ -1576,6 +1576,17 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
   void MarkUniverseForCleanup(const xcluster::ReplicationGroupId& replication_group_id)
       EXCLUDES(mutex_);
 
+  Status CreateCdcStateTableIfNotFound(const LeaderEpoch& epoch) EXCLUDES(mutex_);
+
+  // Create a new Xrepl stream, start mutation and add it to cdc_stream_map_.
+  // Caller is responsible for writing the stream to sys_catalog followed by CommitMutation.
+  // or calling ReleaseAbandonedXreplStream to release the unused stream.
+  Result<scoped_refptr<CDCStreamInfo>> InitNewXReplStream() EXCLUDES(mutex_);
+  void ReleaseAbandonedXReplStream(const xrepl::StreamId& stream_id) EXCLUDES(mutex_);
+
+  Status SetWalRetentionForTable(
+      const TableId& table_id, rpc::RpcContext* rpc, const LeaderEpoch& epoch) EXCLUDES(mutex_);
+
  protected:
   // TODO Get rid of these friend classes and introduce formal interface.
   friend class TableLoader;
@@ -2764,8 +2775,6 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
       const CreateCDCStreamRequestPB& req, rpc::RpcContext* rpc, const LeaderEpoch& epoch,
       const std::vector<TableId>& table_ids, const xrepl::StreamId& stream_id,
       const bool has_consistent_snapshot_option, bool require_history_cutoff);
-  Status SetWalRetentionForTable(
-      const TableId& table_id, rpc::RpcContext* rpc, const LeaderEpoch& epoch);
   Status BackfillMetadataForCDC(
       const TableId& table_id, rpc::RpcContext* rpc, const LeaderEpoch& epoch);
 
diff --git a/src/yb/master/xcluster/add_table_to_xcluster_source_task.cc b/src/yb/master/xcluster/add_table_to_xcluster_source_task.cc
index b037264aa0..a0f605defd 100644
--- a/src/yb/master/xcluster/add_table_to_xcluster_source_task.cc
+++ b/src/yb/master/xcluster/add_table_to_xcluster_source_task.cc
@@ -34,19 +34,39 @@ std::string AddTableToXClusterSourceTask::description() const {
 }
 
 Status AddTableToXClusterSourceTask::FirstStep() {
-  outbound_replication_group_->AddTable(
-      table_info_, epoch_, std::bind(&AddTableToXClusterSourceTask::CompletionCallback, this, _1));
+  RETURN_NOT_OK(outbound_replication_group_->CreateStreamForNewTable(
+      table_info_->namespace_id(), table_info_->id(), epoch_));
 
+  ScheduleNextStep(
+      std::bind(&AddTableToXClusterSourceTask::CheckpointStream, this), "CheckpointStream");
   return Status::OK();
 }
 
-void AddTableToXClusterSourceTask::CompletionCallback(const Status& status) {
-  if (status.ok()) {
-    Complete();
+Status AddTableToXClusterSourceTask::CheckpointStream() {
+  RETURN_NOT_OK(outbound_replication_group_->CheckpointNewTable(
+      table_info_->namespace_id(), table_info_->id(), epoch_,
+      std::bind(&AddTableToXClusterSourceTask::CheckpointCompletionCallback, this, _1)));
+
+  return Status::OK();
+}
+
+void AddTableToXClusterSourceTask::CheckpointCompletionCallback(const Status& status) {
+  if (!status.ok()) {
+    AbortAndReturnPrevState(status);
     return;
   }
 
-  AbortAndReturnPrevState(status);
+  ScheduleNextStep(
+      std::bind(&AddTableToXClusterSourceTask::MarkTableAsCheckpointed, this),
+      "MarkTableAsCheckpointed");
+}
+
+Status AddTableToXClusterSourceTask::MarkTableAsCheckpointed() {
+  RETURN_NOT_OK(outbound_replication_group_->MarkNewTablesAsCheckpointed(
+      table_info_->namespace_id(), table_info_->id(), epoch_));
+
+  Complete();
+  return Status::OK();
 }
 
 }  // namespace yb::master
diff --git a/src/yb/master/xcluster/add_table_to_xcluster_source_task.h b/src/yb/master/xcluster/add_table_to_xcluster_source_task.h
index 703761c638..245cdd405c 100644
--- a/src/yb/master/xcluster/add_table_to_xcluster_source_task.h
+++ b/src/yb/master/xcluster/add_table_to_xcluster_source_task.h
@@ -39,7 +39,11 @@ class AddTableToXClusterSourceTask : public PostTabletCreateTaskBase {
  private:
   Status FirstStep() override;
 
-  void CompletionCallback(const Status& status);
+  Status CheckpointStream();
+
+  void CheckpointCompletionCallback(const Status& status);
+
+  Status MarkTableAsCheckpointed();
 
   const std::shared_ptr<XClusterOutboundReplicationGroup> outbound_replication_group_;
 };
diff --git a/src/yb/master/xcluster/master_xcluster_types.h b/src/yb/master/xcluster/master_xcluster_types.h
index bed137e98c..0259b6e62d 100644
--- a/src/yb/master/xcluster/master_xcluster_types.h
+++ b/src/yb/master/xcluster/master_xcluster_types.h
@@ -18,6 +18,8 @@
 
 namespace yb::master {
 
+class CDCStreamInfo;
+
 // Map[NamespaceId]:xClusterSafeTime
 typedef std::unordered_map<NamespaceId, HybridTime> XClusterNamespaceToSafeTimeMap;
 
@@ -42,8 +44,29 @@ struct IsOperationDoneResult {
 
   bool done;      // Indicates of the operation completed.
   Status status;  // If the operation completed and it failed, this will contain the error.
+
+  std::string ToString() const { return YB_STRUCT_TO_STRING(done, status); }
 };
 
+inline std::ostream& operator<<(std::ostream& out, const IsOperationDoneResult& result) {
+  return out << result.ToString();
+}
+
 YB_DEFINE_ENUM(StreamCheckpointLocation, (kOpId0)(kCurrentEndOfWAL));
 
+using XClusterCheckpointStreamsResult = Result<std::pair<std::vector<TableId>, bool>>;
+
+class XClusterCreateStreamsContext {
+ public:
+  XClusterCreateStreamsContext() = default;
+  virtual ~XClusterCreateStreamsContext() = default;
+
+  virtual void Commit() {}
+
+  std::vector<scoped_refptr<CDCStreamInfo>> streams_;
+
+ private:
+  DISALLOW_COPY_AND_ASSIGN(XClusterCreateStreamsContext);
+};
+
 }  // namespace yb::master
diff --git a/src/yb/master/xcluster/xcluster_manager.cc b/src/yb/master/xcluster/xcluster_manager.cc
index cdf9c754a9..eef5339af6 100644
--- a/src/yb/master/xcluster/xcluster_manager.cc
+++ b/src/yb/master/xcluster/xcluster_manager.cc
@@ -49,6 +49,7 @@ XClusterManager::~XClusterManager() {}
 void XClusterManager::Shutdown() { XClusterTargetManager::Shutdown(); }
 
 Status XClusterManager::Init() {
+  RETURN_NOT_OK(XClusterSourceManager::Init());
   RETURN_NOT_OK(XClusterTargetManager::Init());
 
   return Status::OK();
@@ -197,8 +198,7 @@ Status XClusterManager::XClusterCreateOutboundReplicationGroup(
   }
 
   auto namespace_ids = VERIFY_RESULT(CreateOutboundReplicationGroup(
-      xcluster::ReplicationGroupId(req->replication_group_id()), namespace_names, epoch,
-      rpc->GetClientDeadline()));
+      xcluster::ReplicationGroupId(req->replication_group_id()), namespace_names, epoch));
   for (const auto& namespace_id : namespace_ids) {
     *resp->add_namespace_ids() = namespace_id;
   }
@@ -214,8 +214,7 @@ Status XClusterManager::XClusterAddNamespaceToOutboundReplicationGroup(
   SCHECK(req->has_namespace_name(), InvalidArgument, "Namespace name must be specified");
 
   auto namespace_id = VERIFY_RESULT(AddNamespaceToOutboundReplicationGroup(
-      xcluster::ReplicationGroupId(req->replication_group_id()), req->namespace_name(), epoch,
-      rpc->GetClientDeadline()));
+      xcluster::ReplicationGroupId(req->replication_group_id()), req->namespace_name(), epoch));
 
   resp->set_namespace_id(namespace_id);
   return Status::OK();
@@ -245,7 +244,7 @@ Status XClusterManager::XClusterDeleteOutboundReplicationGroup(
 Status XClusterManager::IsXClusterBootstrapRequired(
     const IsXClusterBootstrapRequiredRequestPB* req, IsXClusterBootstrapRequiredResponsePB* resp,
     rpc::RpcContext* rpc, const LeaderEpoch& epoch) {
-  SCHECK(req->has_namespace_id(), InvalidArgument, "Namespace id must be specified");
+  SCHECK_PB_FIELDS_ARE_SET(*req, replication_group_id, namespace_id);
 
   LOG_FUNC_AND_RPC;
 
diff --git a/src/yb/master/xcluster/xcluster_outbound_replication_group-test.cc b/src/yb/master/xcluster/xcluster_outbound_replication_group-test.cc
index 1c28ab2b0a..187572185f 100644
--- a/src/yb/master/xcluster/xcluster_outbound_replication_group-test.cc
+++ b/src/yb/master/xcluster/xcluster_outbound_replication_group-test.cc
@@ -13,14 +13,29 @@
 
 #include "yb/master/xcluster/xcluster_outbound_replication_group.h"
 
+#include <gmock/gmock.h>
+
 #include "yb/client/xcluster_client.h"
 #include "yb/master/catalog_entity_info.h"
+#include "yb/master/xcluster/xcluster_outbound_replication_group_tasks.h"
 
+#include "yb/rpc/messenger.h"
+#include "yb/util/async_util.h"
+#include "yb/util/backoff_waiter.h"
 #include "yb/util/test_util.h"
+#include "yb/util/tsan_util.h"
+
+using namespace std::placeholders;
+using testing::_;
+using testing::AtLeast;
+using testing::DefaultValue;
+using testing::Invoke;
+using testing::Return;
 
 namespace yb::master {
 
 const UniverseUuid kTargetUniverseUuid = UniverseUuid::GenerateRandom();
+const LeaderEpoch kEpoch = LeaderEpoch(1, 1);
 
 inline bool operator==(const NamespaceCheckpointInfo& lhs, const NamespaceCheckpointInfo& rhs) {
   return YB_STRUCT_EQUALS(initial_bootstrap_required, table_infos);
@@ -28,52 +43,46 @@ inline bool operator==(const NamespaceCheckpointInfo& lhs, const NamespaceCheckp
 
 class XClusterRemoteClientMocked : public client::XClusterRemoteClient {
  public:
-  XClusterRemoteClientMocked() : client::XClusterRemoteClient("na", MonoDelta::kMax) {}
-
-  Status Init(
-      const xcluster::ReplicationGroupId& replication_group_id,
-      const std::vector<HostPort>& remote_masters) override {
-    return Status::OK();
+  XClusterRemoteClientMocked() : client::XClusterRemoteClient("na", MonoDelta::kMax) {
+    DefaultValue<Result<UniverseUuid>>::Set(kTargetUniverseUuid);
+    DefaultValue<Result<IsOperationDoneResult>>::Set(IsOperationDoneResult(true, Status::OK()));
   }
 
-  Result<UniverseUuid> SetupDbScopedUniverseReplication(
-      const xcluster::ReplicationGroupId& replication_group_id,
-      const std::vector<HostPort>& source_master_addresses,
-      const std::vector<NamespaceName>& namespace_names,
-      const std::vector<NamespaceId>& namespace_ids, const std::vector<TableId>& source_table_ids,
-      const std::vector<xrepl::StreamId>& bootstrap_ids) override {
-    replication_group_id_ = replication_group_id;
-    source_master_addresses_ = source_master_addresses;
-    namespace_names_ = namespace_names;
-    namespace_ids_ = namespace_ids;
-    source_table_ids_ = source_table_ids;
-    bootstrap_ids_ = bootstrap_ids;
-
-    return kTargetUniverseUuid;
-  }
-
-  Result<IsOperationDoneResult> IsSetupUniverseReplicationDone(
-      const xcluster::ReplicationGroupId& replication_group_id) override {
-    return is_setup_universe_replication_done_;
-  }
+  MOCK_METHOD2(Init, Status(const xcluster::ReplicationGroupId&, const std::vector<HostPort>&));
+  MOCK_METHOD6(
+      SetupDbScopedUniverseReplication,
+      Result<UniverseUuid>(
+          const xcluster::ReplicationGroupId&, const std::vector<HostPort>&,
+          const std::vector<NamespaceName>&, const std::vector<NamespaceId>&,
+          const std::vector<TableId>&, const std::vector<xrepl::StreamId>&));
+
+  MOCK_METHOD1(
+      IsSetupUniverseReplicationDone,
+      Result<IsOperationDoneResult>(const xcluster::ReplicationGroupId&));
+};
 
-  xcluster::ReplicationGroupId replication_group_id_;
-  std::vector<HostPort> source_master_addresses_;
-  std::vector<NamespaceName> namespace_names_;
-  std::vector<NamespaceId> namespace_ids_;
-  std::vector<TableId> source_table_ids_;
-  std::vector<xrepl::StreamId> bootstrap_ids_;
+Status ValidateEpoch(const LeaderEpoch& epoch) {
+  SCHECK_EQ(epoch, kEpoch, IllegalState, "Epoch does not match");
+  return Status::OK();
+}
 
-  Result<IsOperationDoneResult> is_setup_universe_replication_done_ =
-      IsOperationDoneResult(true, Status::OK());
+class XClusterOutboundReplicationGroupTaskFactoryMocked
+    : public XClusterOutboundReplicationGroupTaskFactory {
+ public:
+  XClusterOutboundReplicationGroupTaskFactoryMocked(
+      ThreadPool& async_task_pool, rpc::Messenger& messenger)
+      : XClusterOutboundReplicationGroupTaskFactory(
+            std::bind(&ValidateEpoch, _1), async_task_pool, messenger) {}
 };
 
 class XClusterOutboundReplicationGroupMocked : public XClusterOutboundReplicationGroup {
  public:
   explicit XClusterOutboundReplicationGroupMocked(
-      const xcluster::ReplicationGroupId& replication_group_id, HelperFunctions helper_functions)
+      const xcluster::ReplicationGroupId& replication_group_id, HelperFunctions helper_functions,
+      XClusterOutboundReplicationGroupTaskFactoryMocked& task_factory)
       : XClusterOutboundReplicationGroup(
-            replication_group_id, {}, std::move(helper_functions), /*tasks_tracker=*/nullptr) {
+            replication_group_id, {}, std::move(helper_functions), /*tasks_tracker=*/nullptr,
+            task_factory) {
     remote_client_ = std::make_shared<XClusterRemoteClientMocked>();
   }
 
@@ -87,6 +96,46 @@ class XClusterOutboundReplicationGroupMocked : public XClusterOutboundReplicatio
            SysXClusterOutboundReplicationGroupEntryPB::DELETED;
   }
 
+  Status AddTable(const TableInfoPtr& table_info, const LeaderEpoch& epoch) {
+    // Same as AddTableToXClusterSourceTask.
+
+    RETURN_NOT_OK(CreateStreamForNewTable(table_info->namespace_id(), table_info->id(), epoch));
+
+    Synchronizer sync;
+    RETURN_NOT_OK(CheckpointNewTable(
+        table_info->namespace_id(), table_info->id(), epoch, sync.AsStdStatusCallback()));
+    RETURN_NOT_OK(sync.Wait());
+
+    return MarkNewTablesAsCheckpointed(table_info->namespace_id(), table_info->id(), epoch);
+  }
+
+  Result<NamespaceId> AddNamespaceSync(
+      const LeaderEpoch& epoch, const NamespaceName& namespace_name, MonoDelta delta) {
+    auto namespace_id = VERIFY_RESULT(AddNamespace(epoch, namespace_name));
+    RETURN_NOT_OK(LoggedWaitFor(
+        [this, namespace_id]() -> Result<bool> {
+          return VERIFY_RESULT(GetNamespaceCheckpointInfo(namespace_id)).has_value();
+        },
+        delta, "Waiting for namespace checkpoint"));
+
+    return namespace_id;
+  }
+
+  Result<std::vector<NamespaceId>> AddNamespacesSync(
+      const LeaderEpoch& epoch, const std::vector<NamespaceName>& namespace_names,
+      MonoDelta delta) {
+    auto namespace_ids = VERIFY_RESULT(AddNamespaces(epoch, namespace_names));
+    for (const auto& namespace_id : namespace_ids) {
+      RETURN_NOT_OK(LoggedWaitFor(
+          [this, namespace_id]() -> Result<bool> {
+            return VERIFY_RESULT(GetNamespaceCheckpointInfo(namespace_id)).has_value();
+          },
+          delta, "Waiting for namespace checkpoint"));
+    }
+
+    return namespace_ids;
+  }
+
  private:
   virtual Result<std::shared_ptr<client::XClusterRemoteClient>> GetRemoteClient(
       const std::vector<HostPort>& remote_masters) const override {
@@ -104,15 +153,32 @@ class XClusterOutboundReplicationGroupMockedTest : public YBTest {
   const xcluster::ReplicationGroupId kReplicationGroupId = xcluster::ReplicationGroupId("rg1");
   const TableName kTableName1 = "table1", kTableName2 = "table2";
   const TableId kTableId1 = "table_id_1", kTableId2 = "table_id_2";
-  const LeaderEpoch kEpoch = LeaderEpoch(1, 1);
-  const CoarseTimePoint kDeadline = CoarseTimePoint::max();
+  const MonoDelta kTimeout = MonoDelta::FromSeconds(5* kTimeMultiplier);
 
   XClusterOutboundReplicationGroupMockedTest() {
-    google::SetVLOGLevel("xcluster*", 4);
+    google::SetVLOGLevel("*", 4);
+
+    ThreadPoolBuilder thread_pool_builder("Test");
+    CHECK_OK(thread_pool_builder.Build(&thread_pool));
+
+    rpc::MessengerBuilder messenger_builder("Test");
+    messenger = CHECK_RESULT(messenger_builder.Build());
+
+    task_factory = std::make_unique<XClusterOutboundReplicationGroupTaskFactoryMocked>(
+        *thread_pool, *messenger);
 
     CreateNamespace(kNamespaceName, kNamespaceId);
   }
 
+  ~XClusterOutboundReplicationGroupMockedTest() {
+    if (thread_pool) {
+      thread_pool->Shutdown();
+    }
+    if (messenger) {
+      messenger->Shutdown();
+    }
+  }
+
   void CreateNamespace(const NamespaceName& namespace_name, const NamespaceId& namespace_id) {
     namespace_ids[namespace_name] = namespace_id;
   }
@@ -135,22 +201,25 @@ class XClusterOutboundReplicationGroupMockedTest : public YBTest {
 
   std::shared_ptr<XClusterOutboundReplicationGroupMocked> CreateReplicationGroup() {
     return std::make_shared<XClusterOutboundReplicationGroupMocked>(
-        kReplicationGroupId, helper_functions);
+        kReplicationGroupId, helper_functions, *task_factory);
   }
 
-  xrepl::StreamId CreateXClusterStream(const TableId& table_id) {
+  scoped_refptr<CDCStreamInfo> CreateXClusterStream(const TableId& table_id) {
     auto stream_id = xrepl::StreamId::GenerateRandom();
     xcluster_streams.insert(stream_id);
-    return stream_id;
+    return make_scoped_refptr<CDCStreamInfo>(stream_id);
   }
 
   std::unordered_map<NamespaceId, std::vector<TableInfoPtr>> namespace_tables;
   std::unordered_map<NamespaceName, NamespaceId> namespace_ids;
   std::unordered_set<xrepl::StreamId> xcluster_streams;
+  std::unique_ptr<ThreadPool> thread_pool;
+  std::unique_ptr<rpc::Messenger> messenger;
+  std::unique_ptr<XClusterOutboundReplicationGroupTaskFactoryMocked> task_factory;
 
   XClusterOutboundReplicationGroup::HelperFunctions helper_functions = {
       .get_namespace_id_func =
-          [this](YQLDatabase db_type, const NamespaceName& namespace_name) {
+          [this](YQLDatabase, const NamespaceName& namespace_name) {
             return namespace_ids[namespace_name];
           },
       .get_namespace_name_func = [this](const NamespaceId& namespace_id) -> Result<NamespaceName> {
@@ -163,20 +232,22 @@ class XClusterOutboundReplicationGroupMockedTest : public YBTest {
       },
       .get_tables_func =
           [this](const NamespaceId& namespace_id) { return namespace_tables[namespace_id]; },
-      .bootstrap_tables_func =
-          [this](
-              const std::vector<TableInfoPtr>& table_infos, CoarseTimePoint deadline,
-              StreamCheckpointLocation checkpoint_location,
-              const LeaderEpoch& epoch) -> Result<std::vector<xrepl::StreamId>> {
-        std::vector<xrepl::StreamId> stream_ids;
-        for (const auto& table_info : table_infos) {
-          stream_ids.emplace_back(CreateXClusterStream(table_info->id()));
-        }
-        return stream_ids;
-      },
-      .delete_cdc_stream_func = [this](
-                                    const DeleteCDCStreamRequestPB& req,
-                                    const LeaderEpoch& epoch) -> Result<DeleteCDCStreamResponsePB> {
+      .create_xcluster_streams_func =
+          [this](const std::vector<TableId>& table_ids, const LeaderEpoch&) {
+            auto create_context = std::make_unique<XClusterCreateStreamsContext>();
+            for (const auto& table_id : table_ids) {
+              create_context->streams_.emplace_back(CreateXClusterStream(table_id));
+            }
+            return create_context;
+          },
+      .checkpoint_xcluster_streams_func =
+          [](const std::vector<std::pair<TableId, xrepl::StreamId>>&, StreamCheckpointLocation,
+             const LeaderEpoch&, bool, std::function<void(Result<bool>)> user_callback) {
+            user_callback(false);  // No bootstrap required.
+            return Status::OK();
+          },
+      .delete_cdc_stream_func = [this](const DeleteCDCStreamRequestPB& req, const LeaderEpoch&)
+          -> Result<DeleteCDCStreamResponsePB> {
         DeleteCDCStreamResponsePB resp;
         for (const auto& stream_id_str : req.stream_id()) {
           auto stream_id = VERIFY_RESULT(xrepl::StreamId::FromString(stream_id_str));
@@ -186,13 +257,10 @@ class XClusterOutboundReplicationGroupMockedTest : public YBTest {
         return resp;
       },
       .upsert_to_sys_catalog_func =
-          [](const LeaderEpoch& epoch, XClusterOutboundReplicationGroupInfo* info) {
-            return Status::OK();
-          },
+          [](const LeaderEpoch&, XClusterOutboundReplicationGroupInfo*,
+             const std::vector<scoped_refptr<CDCStreamInfo>>&) { return Status::OK(); },
       .delete_from_sys_catalog_func =
-          [](const LeaderEpoch& epoch, XClusterOutboundReplicationGroupInfo* info) {
-            return Status::OK();
-          },
+          [](const LeaderEpoch&, XClusterOutboundReplicationGroupInfo*) { return Status::OK(); },
   };
 
   void VerifyNamespaceCheckpointInfo(
@@ -232,7 +300,7 @@ TEST_F(XClusterOutboundReplicationGroupMockedTest, TestMultipleTable) {
   auto& outbound_rg = *outbound_rg_ptr;
 
   ASSERT_FALSE(outbound_rg.HasNamespace(kNamespaceId));
-  auto namespace_id = ASSERT_RESULT(outbound_rg.AddNamespace(kEpoch, kNamespaceName, kDeadline));
+  auto namespace_id = ASSERT_RESULT(outbound_rg.AddNamespaceSync(kEpoch, kNamespaceName, kTimeout));
   ASSERT_EQ(namespace_id, kNamespaceId);
   ASSERT_TRUE(outbound_rg.HasNamespace(kNamespaceId));
 
@@ -290,7 +358,7 @@ TEST_F(XClusterOutboundReplicationGroupMockedTest, AddDeleteNamespaces) {
   auto outbound_rg_ptr = CreateReplicationGroup();
   auto& outbound_rg = *outbound_rg_ptr;
   auto out_namespace_id =
-      ASSERT_RESULT(outbound_rg.AddNamespaces(kEpoch, {kNamespaceName}, kDeadline));
+      ASSERT_RESULT(outbound_rg.AddNamespacesSync(kEpoch, {kNamespaceName}, kTimeout));
   ASSERT_EQ(out_namespace_id.size(), 1);
   ASSERT_EQ(out_namespace_id[0], kNamespaceId);
 
@@ -310,7 +378,7 @@ TEST_F(XClusterOutboundReplicationGroupMockedTest, AddDeleteNamespaces) {
 
   // Add the second namespace.
   auto out_namespace_id2 =
-      ASSERT_RESULT(outbound_rg.AddNamespace(kEpoch, namespace_name_2, kDeadline));
+      ASSERT_RESULT(outbound_rg.AddNamespaceSync(kEpoch, namespace_name_2, kTimeout));
   ASSERT_EQ(out_namespace_id2, namespace_id_2);
 
   // We should have 4 streams now.
@@ -352,24 +420,28 @@ TEST_F(XClusterOutboundReplicationGroupMockedTest, CreateTargetReplicationGroup)
   auto remote_client = std::make_shared<XClusterRemoteClientMocked>();
   outbound_rg.SetRemoteClient(remote_client);
 
-  ASSERT_OK(outbound_rg.AddNamespace(kEpoch, kNamespaceName, kDeadline));
+  ASSERT_OK(outbound_rg.AddNamespaceSync(kEpoch, kNamespaceName, kTimeout));
 
-  ASSERT_OK(outbound_rg.CreateXClusterReplication({}, {}, kEpoch));
+  std::vector<xrepl::StreamId> streams{xcluster_streams.begin(), xcluster_streams.end()};
+  EXPECT_CALL(
+      *remote_client,
+      SetupDbScopedUniverseReplication(
+          kReplicationGroupId, _, std::vector<NamespaceName>{kNamespaceName},
+          std::vector<NamespaceId>{kNamespaceId}, std::vector<TableId>{kTableId1}, streams))
+      .Times(AtLeast(1));
 
-  ASSERT_EQ(remote_client->replication_group_id_, kReplicationGroupId);
-  ASSERT_EQ(remote_client->namespace_names_, std::vector<NamespaceName>{kNamespaceName});
-  ASSERT_EQ(remote_client->source_table_ids_.size(), 1);
-  ASSERT_EQ(remote_client->source_table_ids_[0], kTableId1);
-  ASSERT_EQ(remote_client->bootstrap_ids_.size(), xcluster_streams.size());
+  ASSERT_OK(outbound_rg.CreateXClusterReplication({}, {}, kEpoch));
 
-  remote_client->is_setup_universe_replication_done_ = IsOperationDoneResult(false, Status::OK());
+  EXPECT_CALL(*remote_client, IsSetupUniverseReplicationDone(_))
+      .WillOnce(Return(IsOperationDoneResult(false, Status::OK())));
 
   auto create_result = ASSERT_RESULT(outbound_rg.IsCreateXClusterReplicationDone({}, kEpoch));
   ASSERT_FALSE(create_result.done);
 
   // Fail the Setup.
   const auto error_str = "Failed by test";
-  remote_client->is_setup_universe_replication_done_ = STATUS(IllegalState, error_str);
+  EXPECT_CALL(*remote_client, IsSetupUniverseReplicationDone(_))
+      .WillOnce(Return(STATUS(IllegalState, error_str)));
   auto result = outbound_rg.IsCreateXClusterReplicationDone({}, kEpoch);
   ASSERT_NOK(result);
   ASSERT_STR_CONTAINS(result.status().ToString(), error_str);
@@ -381,8 +453,8 @@ TEST_F(XClusterOutboundReplicationGroupMockedTest, CreateTargetReplicationGroup)
       pb.target_universe_info().state(),
       SysXClusterOutboundReplicationGroupEntryPB::TargetUniverseInfo::CREATING_REPLICATION_GROUP);
 
-  remote_client->is_setup_universe_replication_done_ =
-      IsOperationDoneResult(true, STATUS(IllegalState, error_str));
+  EXPECT_CALL(*remote_client, IsSetupUniverseReplicationDone(_))
+      .WillOnce(Return(IsOperationDoneResult(true, STATUS(IllegalState, error_str))));
   create_result = ASSERT_RESULT(outbound_rg.IsCreateXClusterReplicationDone({}, kEpoch));
   ASSERT_TRUE(create_result.done);
   ASSERT_STR_CONTAINS(create_result.status.ToString(), error_str);
@@ -391,8 +463,12 @@ TEST_F(XClusterOutboundReplicationGroupMockedTest, CreateTargetReplicationGroup)
   ASSERT_FALSE(pb.has_target_universe_info());
 
   // Success case.
-  remote_client->is_setup_universe_replication_done_ = IsOperationDoneResult(true, Status::OK());
+  EXPECT_CALL(*remote_client, IsSetupUniverseReplicationDone(_))
+      .WillOnce(Return(IsOperationDoneResult(true, Status::OK())));
 
+  EXPECT_CALL(*remote_client, SetupDbScopedUniverseReplication(_, _, _, _, _, _));
+
+  // Calling create again should not do anything.
   ASSERT_OK(outbound_rg.CreateXClusterReplication({}, {}, kEpoch));
   create_result = ASSERT_RESULT(outbound_rg.IsCreateXClusterReplicationDone({}, kEpoch));
   ASSERT_TRUE(create_result.done);
@@ -411,19 +487,16 @@ TEST_F(XClusterOutboundReplicationGroupMockedTest, AddTable) {
   CreateTable(kNamespaceId, kTableId2, kTableName2, kPgSchemaName2);
 
   auto outbound_rg = CreateReplicationGroup();
-  auto namespace_id = ASSERT_RESULT(outbound_rg->AddNamespace(kEpoch, kNamespaceName, kDeadline));
+  auto namespace_id =
+      ASSERT_RESULT(outbound_rg->AddNamespaceSync(kEpoch, kNamespaceName, kTimeout));
   ASSERT_TRUE(outbound_rg->HasNamespace(kNamespaceId));
   ASSERT_EQ(xcluster_streams.size(), 2);
 
   auto ns_info = ASSERT_RESULT(outbound_rg->GetNamespaceCheckpointInfo(kNamespaceId));
   ASSERT_EQ(ns_info->table_infos.size(), 2);
 
-  std::promise<Status> promise;
-  auto completion_cb = [&promise](const Status& status) { promise.set_value(status); };
-
   // Same table should not get added twice.
-  outbound_rg->AddTable(table_info1, kEpoch, completion_cb);
-  ASSERT_OK(promise.get_future().get());
+  ASSERT_OK(outbound_rg->AddTable(table_info1, kEpoch));
 
   ASSERT_EQ(ns_info->table_infos.size(), 2);
 
@@ -431,9 +504,7 @@ TEST_F(XClusterOutboundReplicationGroupMockedTest, AddTable) {
   const TableId table_id_3 = "table_id_3";
   auto table_info3 = CreateTable(kNamespaceId, table_id_3, table_3, kPgSchemaName);
 
-  promise = {};
-  outbound_rg->AddTable(table_info3, kEpoch, completion_cb);
-  ASSERT_OK(promise.get_future().get());
+  ASSERT_OK(outbound_rg->AddTable(table_info3, kEpoch));
 
   ASSERT_EQ(xcluster_streams.size(), 3);
   ns_info = ASSERT_RESULT(outbound_rg->GetNamespaceCheckpointInfo(kNamespaceId));
diff --git a/src/yb/master/xcluster/xcluster_outbound_replication_group.cc b/src/yb/master/xcluster/xcluster_outbound_replication_group.cc
index ec48fdc03b..3c1400a6af 100644
--- a/src/yb/master/xcluster/xcluster_outbound_replication_group.cc
+++ b/src/yb/master/xcluster/xcluster_outbound_replication_group.cc
@@ -14,11 +14,18 @@
 #include "yb/master/xcluster/xcluster_outbound_replication_group.h"
 #include "yb/client/xcluster_client.h"
 #include "yb/master/catalog_entity_info.h"
+#include "yb/master/xcluster/xcluster_outbound_replication_group_tasks.h"
 #include "yb/master/xcluster_rpc_tasks.h"
+#include "yb/util/status_log.h"
+
+DEFINE_RUNTIME_uint32(max_xcluster_streams_to_checkpoint_in_parallel, 200,
+    "Maximum number of xCluster streams to checkpoint in parallel");
 
 DECLARE_int32(cdc_read_rpc_timeout_ms);
 DECLARE_string(certs_for_cdc_dir);
 
+using namespace std::placeholders;
+
 namespace yb::master {
 
 namespace {
@@ -32,14 +39,32 @@ struct TableSchemaNamePairHash {
   }
 };
 
+bool TableNeedsInitialCheckpoint(
+    const SysXClusterOutboundReplicationGroupEntryPB::NamespaceInfoPB::TableInfoPB& table_info) {
+  return table_info.is_part_of_initial_bootstrap() && table_info.is_checkpointing();
+}
+
+Result<bool> IsReady(
+    const SysXClusterOutboundReplicationGroupEntryPB::NamespaceInfoPB& namespace_info) {
+  if (namespace_info.state() ==
+      SysXClusterOutboundReplicationGroupEntryPB::NamespaceInfoPB::FAILED) {
+    return StatusFromPB(namespace_info.error_status());
+  }
+
+  return namespace_info.state() ==
+         SysXClusterOutboundReplicationGroupEntryPB::NamespaceInfoPB::READY;
+}
+
 }  // namespace
 
 XClusterOutboundReplicationGroup::XClusterOutboundReplicationGroup(
     const xcluster::ReplicationGroupId& replication_group_id,
     const SysXClusterOutboundReplicationGroupEntryPB& outbound_replication_group_pb,
-    HelperFunctions helper_functions, scoped_refptr<TasksTracker> tasks_tracker)
+    HelperFunctions helper_functions, scoped_refptr<TasksTracker> tasks_tracker,
+    XClusterOutboundReplicationGroupTaskFactory& task_factory)
     : CatalogEntityWithTasks(std::move(tasks_tracker)),
-      helper_functions_(std::move(helper_functions)) {
+      helper_functions_(std::move(helper_functions)),
+      task_factory_(task_factory) {
   outbound_rg_info_ = std::make_unique<XClusterOutboundReplicationGroupInfo>(replication_group_id);
   outbound_rg_info_->Load(outbound_replication_group_pb);
 }
@@ -70,49 +95,227 @@ Result<SysXClusterOutboundReplicationGroupEntryPB> XClusterOutboundReplicationGr
 }
 
 Status XClusterOutboundReplicationGroup::Upsert(
-    XClusterOutboundReplicationGroupInfo::WriteLock& l, const LeaderEpoch& epoch) {
-  auto status = helper_functions_.upsert_to_sys_catalog_func(epoch, outbound_rg_info_.get());
-  l.CommitOrWarn(status, "updating xClusterOutboundReplicationGroup in sys-catalog");
-  return status;
+    XClusterOutboundReplicationGroupInfo::WriteLock& l, const LeaderEpoch& epoch,
+    const std::vector<scoped_refptr<CDCStreamInfo>>& streams) {
+  RETURN_NOT_OK_PREPEND(
+      helper_functions_.upsert_to_sys_catalog_func(epoch, outbound_rg_info_.get(), streams),
+      Format("$0 Failed to write to sys-catalog", LogPrefix()));
+  l.Commit();
+  return Status::OK();
 }
 
-Result<SysXClusterOutboundReplicationGroupEntryPB::NamespaceInfoPB>
-XClusterOutboundReplicationGroup::BootstrapTables(
-    const std::vector<TableInfoPtr>& table_infos, CoarseTimePoint deadline,
-    const LeaderEpoch& epoch) {
-  VLOG_WITH_PREFIX_AND_FUNC(1) << yb::ToString(table_infos);
+void XClusterOutboundReplicationGroup::StartNamespaceCheckpointTasks(
+    const std::vector<NamespaceId>& namespace_ids, const LeaderEpoch& epoch) {
+  for (const auto& namespace_id : namespace_ids) {
+    // Perform the rest of the work asynchronously.
+    // IsBootstrapRequired or GetNamespaceCheckpointInfo can be called to detect when the namespace
+    // is ready.
+    auto task = task_factory_.CreateCheckpointNamespaceTask(*this, namespace_id, epoch);
+    task->Start();
+  }
+}
 
-  SCHECK(!table_infos.empty(), InvalidArgument, "No tables to bootstrap");
-  SysXClusterOutboundReplicationGroupEntryPB::NamespaceInfoPB ns_info;
-  ns_info.set_state(SysXClusterOutboundReplicationGroupEntryPB::NamespaceInfoPB::CHECKPOINTING);
+Status XClusterOutboundReplicationGroup::CreateStreamsForInitialBootstrap(
+    const NamespaceId& namespace_id, const LeaderEpoch& epoch) {
+  std::lock_guard mutex_l(mutex_);
+  auto l = VERIFY_RESULT(LockForWrite());
 
-  auto bootstrap_ids = VERIFY_RESULT(helper_functions_.bootstrap_tables_func(
-      table_infos, deadline, StreamCheckpointLocation::kCurrentEndOfWAL, epoch));
+  auto* ns_info = VERIFY_RESULT(GetNamespaceInfo(namespace_id));
+  SCHECK_EQ(
+      ns_info->state(), NamespaceInfoPB::CHECKPOINTING, IllegalState,
+      "Namespace in unexpected state");
+
+  std::vector<TableId> table_ids;
+  // Get all the initial tables that do not yet have a stream.
+  for (const auto& [table_id, table_info] : ns_info->table_infos()) {
+    if (!table_info.is_part_of_initial_bootstrap()) {
+      // This is a table that was created after the creation of this replication group.
+      continue;
+    }
+
+    // Ensure idempotence of the function.
+    if (table_info.stream_id().empty()) {
+      SCHECK(
+          table_info.is_checkpointing(), IllegalState,
+          "$0 Table $1 does not have stream but is marked as completed checkpoint", LogPrefix(),
+          table_id);
+      table_ids.push_back(table_id);
+    }
+  }
+
+  if (table_ids.empty()) {
+    return Status::OK();
+  }
+
+  LOG_WITH_PREFIX(INFO) << "Creating xcluster streams for " << table_ids.size()
+                        << " table(s) in namespace " << namespace_id;
+
+  auto create_context =
+      VERIFY_RESULT(helper_functions_.create_xcluster_streams_func(table_ids, epoch));
 
   SCHECK_EQ(
-      table_infos.size(), bootstrap_ids.size(), IllegalState,
-      "Number of tables to bootstrap and number of bootstrap ids do not match");
+      create_context->streams_.size(), table_ids.size(), IllegalState,
+      "Unexpected number of streams");
+
+  for (size_t i = 0; i < table_ids.size(); ++i) {
+    const auto& table_id = table_ids[i];
+    auto* table_info = VERIFY_RESULT(GetTableInfo(*ns_info, table_id));
+    RSTATUS_DCHECK(
+        !table_info->has_stream_id(), IllegalState, "$0 Table $1 already has a stream $1",
+        LogPrefix(), table_id, table_info->stream_id());
+    table_info->set_stream_id(create_context->streams_[i]->id());
+  }
 
-  bool initial_bootstrap_required = false;
-  for (size_t i = 0; i < table_infos.size(); i++) {
-    // TODO(Hari): DB-9417 bootstrap_resp should return if bootstrap is required.
-    // initial_bootstrap_required |= bootstrap_resp.bootstrap_required(i);
+  RETURN_NOT_OK(Upsert(l, epoch, create_context->streams_));
+  create_context->Commit();
 
-    SysXClusterOutboundReplicationGroupEntryPB::NamespaceInfoPB::TableInfoPB table_info;
-    table_info.set_stream_id(bootstrap_ids[i].ToString());
+  return Status::OK();
+}
+
+Status XClusterOutboundReplicationGroup::CheckpointStreamsForInitialBootstrap(
+    const NamespaceId& namespace_id, const LeaderEpoch& epoch,
+    std::function<void(XClusterCheckpointStreamsResult)> completion_cb) {
+  std::vector<std::pair<TableId, xrepl::StreamId>> table_streams;
+  std::vector<TableId> table_ids;
+  bool check_if_bootstrap_required = true;
+
+  {
+    std::lock_guard mutex_l(mutex_);
+    auto l = VERIFY_RESULT(LockForWrite());
+
+    auto* ns_info = VERIFY_RESULT(GetNamespaceInfo(namespace_id));
+    SCHECK_EQ(
+        ns_info->state(), NamespaceInfoPB::CHECKPOINTING, IllegalState,
+        "Namespace in unexpected state");
+
+    // We only have to check only if bootstrap is not currently required.
+    check_if_bootstrap_required = !ns_info->initial_bootstrap_required();
+
+    for (const auto& [table_id, table_info] : ns_info->table_infos()) {
+      if (!TableNeedsInitialCheckpoint(table_info)) {
+        continue;
+      }
+      SCHECK(
+          table_info.has_stream_id(), IllegalState, "Stream id not found for table $0", table_id);
+
+      if (table_streams.size() >= FLAGS_max_xcluster_streams_to_checkpoint_in_parallel) {
+        break;
+      }
+
+      table_ids.emplace_back(table_id);
+      table_streams.emplace_back(
+          table_id, VERIFY_RESULT(xrepl::StreamId::FromString(table_info.stream_id())));
+    }
+  }
+
+  auto callback = [table_ids, user_cb = std::move(completion_cb)](Result<bool> result) {
+    if (!result.ok()) {
+      user_cb(result.status());
+      return;
+    }
+    user_cb(std::make_pair(std::move(table_ids), *result));
+  };
+
+  if (!table_ids.empty()) {
+    LOG_WITH_PREFIX(INFO) << "Checkpointing xcluster streams for " << table_ids.size()
+                          << " table(s) in namespace " << namespace_id;
+
+    RETURN_NOT_OK(helper_functions_.checkpoint_xcluster_streams_func(
+        table_streams, StreamCheckpointLocation::kCurrentEndOfWAL, epoch,
+        check_if_bootstrap_required, std::move(callback)));
+  } else {
+    callback(false);  // Bootstrap not required if we dont have any tables.
+    // When MarkBootstrapTablesAsCheckpointed handles this empty table result it will mark the
+    // namespace as READY.
+    // So, after all tables have been checkpointed this function and
+    // MarkBootstrapTablesAsCheckpointed have to be called one extra time.
+  }
+
+  return Status::OK();
+}
+
+Result<bool> XClusterOutboundReplicationGroup::MarkBootstrapTablesAsCheckpointed(
+    const NamespaceId& namespace_id, XClusterCheckpointStreamsResult checkpoint_result,
+    const LeaderEpoch& epoch) {
+  auto [table_ids, is_bootstrap_required] = VERIFY_RESULT(std::move(checkpoint_result));
+
+  std::lock_guard mutex_lock(mutex_);
+  auto l = VERIFY_RESULT(LockForWrite());
+  auto* ns_info = VERIFY_RESULT(GetNamespaceInfo(namespace_id));
+  bool done = false;
+
+  SCHECK_EQ(
+      ns_info->state(), NamespaceInfoPB::CHECKPOINTING, IllegalState,
+      "Namespace in unexpected state");
+
+  if (table_ids.empty()) {
+    LOG_WITH_PREFIX(INFO) << "Marking namespace " << namespace_id << " as READY";
+    ns_info->set_state(NamespaceInfoPB::READY);
+    done = true;
+  } else {
+    if (!ns_info->initial_bootstrap_required() && is_bootstrap_required) {
+      // Dont clear once set.
+      ns_info->set_initial_bootstrap_required(true);
+    }
+
+    for (const auto& table_id : table_ids) {
+      auto* table_info = VERIFY_RESULT(GetTableInfo(*ns_info, table_id));
+      SCHECK(
+          TableNeedsInitialCheckpoint(*table_info), IllegalState,
+          "$0 Table $1 is not checkpointing", LogPrefix(), table_id);
+      table_info->set_is_checkpointing(false);
+    }
+  }
+
+  RETURN_NOT_OK(Upsert(l, epoch));
+
+  return done;
+}
+
+void XClusterOutboundReplicationGroup::MarkCheckpointNamespaceAsFailed(
+    const NamespaceId& namespace_id, const LeaderEpoch& epoch, const Status& status) {
+  DCHECK(!status.ok());
+  LOG_WITH_PREFIX(WARNING) << "Failed to checkpoint namespace " << namespace_id << ": " << status;
+
+  std::lock_guard mutex_lock(mutex_);
+  auto lock_result = LockForWrite();
+  auto ns_info = GetNamespaceInfoSafe(lock_result, namespace_id);
+  if (!ns_info || ns_info->state() != NamespaceInfoPB::CHECKPOINTING) {
+    LOG_WITH_PREFIX(WARNING) << "Namespace " << namespace_id
+                             << " not in CHECKPOINTING state, so wont be marked as failed.";
+    return;
+  }
+  // FAILED is a terminal state. RemoveNamespace or Delete Replication Group is the way to clean it
+  // up.
+  ns_info->set_state(NamespaceInfoPB::FAILED);
+  StatusToPB(status, ns_info->mutable_error_status());
+
+  WARN_NOT_OK(Upsert(*lock_result, epoch), ToString());
+}
+
+Result<XClusterOutboundReplicationGroup::NamespaceInfoPB>
+XClusterOutboundReplicationGroup::CreateNamespaceInfo(
+    const NamespaceId& namespace_id, const LeaderEpoch& epoch) {
+  auto table_infos = VERIFY_RESULT(helper_functions_.get_tables_func(namespace_id));
+  VLOG_WITH_PREFIX_AND_FUNC(1) << "Tables: " << yb::ToString(table_infos);
+
+  SCHECK(!table_infos.empty(), InvalidArgument, "No tables to bootstrap");
+  NamespaceInfoPB ns_info;
+  ns_info.set_state(NamespaceInfoPB::CHECKPOINTING);
+
+  for (size_t i = 0; i < table_infos.size(); ++i) {
+    NamespaceInfoPB::TableInfoPB table_info;
+    table_info.set_is_checkpointing(true);
     table_info.set_is_part_of_initial_bootstrap(true);
     ns_info.mutable_table_infos()->insert({table_infos[i]->id(), std::move(table_info)});
   }
 
-  ns_info.set_state(SysXClusterOutboundReplicationGroupEntryPB::NamespaceInfoPB::READY);
-  ns_info.set_initial_bootstrap_required(initial_bootstrap_required);
-
   return ns_info;
 }
 
 Result<NamespaceId> XClusterOutboundReplicationGroup::AddNamespaceInternal(
-    const NamespaceName& namespace_name, CoarseTimePoint deadline,
-    XClusterOutboundReplicationGroupInfo::WriteLock& l, const LeaderEpoch& epoch) {
+    const NamespaceName& namespace_name, XClusterOutboundReplicationGroupInfo::WriteLock& l,
+    const LeaderEpoch& epoch) {
   SCHECK(!namespace_name.empty(), InvalidArgument, "Namespace name cannot be empty");
   VLOG_WITH_PREFIX_AND_FUNC(1) << namespace_name;
 
@@ -127,35 +330,40 @@ Result<NamespaceId> XClusterOutboundReplicationGroup::AddNamespaceInternal(
     return namespace_id;
   }
 
-  auto table_infos = VERIFY_RESULT(helper_functions_.get_tables_func(namespace_id));
-  auto ns_checkpoint_info = VERIFY_RESULT(BootstrapTables(table_infos, deadline, epoch));
-  (*outbound_group_pb.mutable_namespace_infos())[namespace_id] = std::move(ns_checkpoint_info);
+  auto ns_checkpoint_info = VERIFY_RESULT(CreateNamespaceInfo(namespace_id, epoch));
+  outbound_group_pb.mutable_namespace_infos()->insert(
+      {namespace_id, std::move(ns_checkpoint_info)});
 
   return namespace_id;
 }
 
 Result<std::vector<NamespaceId>> XClusterOutboundReplicationGroup::AddNamespaces(
-    const LeaderEpoch& epoch, const std::vector<NamespaceName>& namespace_names,
-    CoarseTimePoint deadline) {
+    const LeaderEpoch& epoch, const std::vector<NamespaceName>& namespace_names) {
+  std::vector<NamespaceId> namespace_ids;
   std::lock_guard mutex_lock(mutex_);
   auto l = VERIFY_RESULT(LockForWrite());
 
-  std::vector<NamespaceId> namespace_ids;
   for (const auto& namespace_name : namespace_names) {
-    auto namespace_id = VERIFY_RESULT(AddNamespaceInternal(namespace_name, deadline, l, epoch));
+    auto namespace_id = VERIFY_RESULT(AddNamespaceInternal(namespace_name, l, epoch));
     namespace_ids.push_back(std::move(namespace_id));
   }
   RETURN_NOT_OK(Upsert(l, epoch));
+
+  StartNamespaceCheckpointTasks(namespace_ids, epoch);
+
   return namespace_ids;
 }
 
 Result<NamespaceId> XClusterOutboundReplicationGroup::AddNamespace(
-    const LeaderEpoch& epoch, const NamespaceName& namespace_name, CoarseTimePoint deadline) {
+    const LeaderEpoch& epoch, const NamespaceName& namespace_name) {
+  NamespaceId namespace_id;
   std::lock_guard mutex_lock(mutex_);
   auto l = VERIFY_RESULT(LockForWrite());
-  auto namespace_id = VERIFY_RESULT(AddNamespaceInternal(namespace_name, deadline, l, epoch));
+  namespace_id = VERIFY_RESULT(AddNamespaceInternal(namespace_name, l, epoch));
   RETURN_NOT_OK(Upsert(l, epoch));
 
+  StartNamespaceCheckpointTasks({namespace_id}, epoch);
+
   return namespace_id;
 }
 
@@ -169,16 +377,14 @@ Status XClusterOutboundReplicationGroup::DeleteNamespaceStreams(
   LOG_WITH_PREFIX(INFO) << "Deleting streams for namespace " << namespace_id;
   auto& namespace_info = outbound_group_pb.namespace_infos().at(namespace_id);
 
-  bool has_streams_to_delete = false;
   DeleteCDCStreamRequestPB req;
   for (const auto& [table_id, table_info] : namespace_info.table_infos()) {
     if (!table_info.has_stream_id()) {
       continue;
     }
-    has_streams_to_delete = true;
     req.add_stream_id(table_info.stream_id());
   }
-  if (!has_streams_to_delete) {
+  if (!req.stream_id_size()) {
     return Status::OK();
   }
 
@@ -216,28 +422,24 @@ Status XClusterOutboundReplicationGroup::Delete(const LeaderEpoch& epoch) {
   outbound_group_pb.mutable_namespace_infos()->clear();
   outbound_group_pb.set_state(SysXClusterOutboundReplicationGroupEntryPB::DELETED);
 
-  auto status = helper_functions_.delete_from_sys_catalog_func(epoch, outbound_rg_info_.get());
-  l.CommitOrWarn(status, "updating xClusterOutboundReplicationGroup in sys-catalog");
+  RETURN_NOT_OK_PREPEND(
+      helper_functions_.delete_from_sys_catalog_func(epoch, outbound_rg_info_.get()),
+      "updating xClusterOutboundReplicationGroup in sys-catalog");
+  l.Commit();
 
-  return status;
+  return Status::OK();
 }
 
 Result<std::optional<bool>> XClusterOutboundReplicationGroup::IsBootstrapRequired(
     const NamespaceId& namespace_id) const {
   SharedLock mutex_lock(mutex_);
   auto l = VERIFY_RESULT(LockForRead());
-  auto& outbound_group = l->pb;
-  SCHECK(
-      HasNamespaceUnlocked(namespace_id), NotFound,
-      Format("Namespace $0 not found in $1", namespace_id, ToString()));
-
-  auto& namespace_info = outbound_group.namespace_infos().at(namespace_id);
-  if (namespace_info.state() ==
-      SysXClusterOutboundReplicationGroupEntryPB::NamespaceInfoPB::CHECKPOINTING) {
+  const auto* namespace_info = VERIFY_RESULT(GetNamespaceInfo(namespace_id));
+  if (!VERIFY_RESULT(IsReady(*namespace_info))) {
     return std::nullopt;
   }
 
-  return namespace_info.initial_bootstrap_required();
+  return namespace_info->initial_bootstrap_required();
 }
 
 Result<std::optional<NamespaceCheckpointInfo>>
@@ -246,20 +448,13 @@ XClusterOutboundReplicationGroup::GetNamespaceCheckpointInfo(
     const std::vector<std::pair<TableName, PgSchemaName>>& table_names) const {
   SharedLock mutex_lock(mutex_);
   auto l = VERIFY_RESULT(LockForRead());
-  auto& outbound_group = l->pb;
-  SCHECK(
-      HasNamespaceUnlocked(namespace_id), NotFound,
-      Format("Namespace $0 not found in xClusterOutboundReplicationGroup $1", namespace_id, Id()));
-
-  auto& namespace_info = outbound_group.namespace_infos().at(namespace_id);
-  if (namespace_info.state() !=
-      SysXClusterOutboundReplicationGroupEntryPB::NamespaceInfoPB::READY) {
-    VLOG_WITH_PREFIX_AND_FUNC(1) << "Namespace " << namespace_id << " is not ready yet.";
+  const auto* namespace_info = VERIFY_RESULT(GetNamespaceInfo(namespace_id));
+  if (!VERIFY_RESULT(IsReady(*namespace_info))) {
     return std::nullopt;
   }
 
   NamespaceCheckpointInfo ns_info;
-  ns_info.initial_bootstrap_required = namespace_info.initial_bootstrap_required();
+  ns_info.initial_bootstrap_required = namespace_info->initial_bootstrap_required();
 
   auto all_tables = VERIFY_RESULT(helper_functions_.get_tables_func(namespace_id));
   std::vector<scoped_refptr<TableInfo>> table_infos;
@@ -286,17 +481,17 @@ XClusterOutboundReplicationGroup::GetNamespaceCheckpointInfo(
   for (const auto& table_info : table_infos) {
     const auto& table_id = table_info->id();
     SCHECK(
-        namespace_info.table_infos().count(table_id), IllegalState,
+        namespace_info->table_infos().count(table_id), IllegalState,
         Format(
             "Table $0 exists in namespace $1, but not in $2", table_id, namespace_id, ToString()));
-    auto& namespace_table_info = namespace_info.table_infos().at(table_id);
+    auto& namespace_table_info = namespace_info->table_infos().at(table_id);
     if (!namespace_table_info.has_stream_id() || namespace_table_info.is_checkpointing()) {
       VLOG_WITH_PREFIX_AND_FUNC(1) << "xCluster stream for Table " << table_id << " in Namespace "
                                    << namespace_id << " is not ready yet.";
       return std::nullopt;
     }
     auto stream_id = VERIFY_RESULT(
-        xrepl::StreamId::FromString(namespace_info.table_infos().at(table_id).stream_id()));
+        xrepl::StreamId::FromString(namespace_info->table_infos().at(table_id).stream_id()));
     SCHECK(
         !stream_id.IsNil(), IllegalState,
         Format("Nil stream id found for table $0 in $1", table_id, ToString()));
@@ -341,8 +536,8 @@ Status XClusterOutboundReplicationGroup::CreateXClusterReplication(
   std::vector<xrepl::StreamId> bootstrap_ids;
   for (const auto& [ns_id, ns_info] : outbound_group.namespace_infos()) {
     SCHECK_EQ(
-        ns_info.state(), SysXClusterOutboundReplicationGroupEntryPB::NamespaceInfoPB::READY,
-        TryAgain, Format("Namespace $0 is not yet ready to start replicating", ns_id));
+        ns_info.state(), NamespaceInfoPB::READY, TryAgain,
+        Format("Namespace $0 is not yet ready to start replicating", ns_id));
 
     namespace_ids.push_back(ns_id);
     namespace_names.push_back(VERIFY_RESULT(helper_functions_.get_namespace_name_func(ns_id)));
@@ -450,52 +645,159 @@ bool XClusterOutboundReplicationGroup::HasNamespaceUnlocked(const NamespaceId& n
   return outbound_rg_info_->old_pb().namespace_infos().count(namespace_id) > 0;
 }
 
-Status XClusterOutboundReplicationGroup::AddTableInternal(
-    TableInfoPtr table_info, const LeaderEpoch& epoch) {
-  std::lock_guard mutex_lock(mutex_);
-  auto lock_result = LockForWrite();
+XClusterOutboundReplicationGroup::NamespaceInfoPB*
+XClusterOutboundReplicationGroup::GetNamespaceInfoSafe(
+    const Result<XClusterOutboundReplicationGroupInfo::WriteLock>& lock_result,
+    const NamespaceId& namespace_id) const {
   if (!lock_result.ok()) {
-    RSTATUS_DCHECK(
-        lock_result.status().IsNotFound(), IllegalState, "Unexpected lock outcome: $0",
-        lock_result.status());
+    LOG_IF(DFATAL, !lock_result.status().IsNotFound())
+        << "Unexpected lock outcome: " << lock_result.status();
     VLOG_WITH_PREFIX_AND_FUNC(2) << "Replication group deleted";
-    return Status::OK();
+    return nullptr;
   }
 
-  const auto table_id = table_info->id();
-  const auto namespace_id = table_info->namespace_id();
   auto& outbound_group_pb = lock_result->mutable_data()->pb;
   if (!HasNamespaceUnlocked(namespace_id)) {
     // Namespace was deleted between between locks. Since this is used in
     // AddTableToXClusterSourceTask, which runs asynchronously and we do not want to fail the task,
     // we can just return OK.
     VLOG_WITH_PREFIX_AND_FUNC(2) << "Namespace " << namespace_id
-                                 << " is no longer part of this replication group";
+                                 << " is not part of this replication group";
+    return nullptr;
+  }
+
+  return &outbound_group_pb.mutable_namespace_infos()->at(namespace_id);
+}
+
+Result<XClusterOutboundReplicationGroup::NamespaceInfoPB*>
+XClusterOutboundReplicationGroup::GetNamespaceInfo(const NamespaceId& namespace_id) {
+  CHECK(outbound_rg_info_->metadata().HasWriteLock());
+  SCHECK(
+      HasNamespaceUnlocked(namespace_id), NotFound,
+      Format("$0 Namespace $1 not found", LogPrefix(), namespace_id));
+
+  return &outbound_rg_info_->mutable_metadata()->mutable_dirty()->pb.mutable_namespace_infos()->at(
+      namespace_id);
+}
+
+Result<const XClusterOutboundReplicationGroup::NamespaceInfoPB*>
+XClusterOutboundReplicationGroup::GetNamespaceInfo(const NamespaceId& namespace_id) const {
+  SCHECK(
+      HasNamespaceUnlocked(namespace_id), NotFound,
+      Format("$0 Namespace $1 not found", LogPrefix(), namespace_id));
+
+  return &outbound_rg_info_->old_pb().namespace_infos().at(namespace_id);
+}
+
+Result<XClusterOutboundReplicationGroup::NamespaceInfoPB::TableInfoPB*>
+XClusterOutboundReplicationGroup::GetTableInfo(NamespaceInfoPB& ns_info, const TableId& table_id) {
+  auto table_info = FindOrNull(*ns_info.mutable_table_infos(), table_id);
+  SCHECK(table_info, NotFound, "$0 Table $1 not found", LogPrefix(), table_id);
+  return table_info;
+}
+
+Status XClusterOutboundReplicationGroup::CreateStreamForNewTable(
+    const NamespaceId& namespace_id, const TableId& table_id, const LeaderEpoch& epoch) {
+  VLOG_WITH_PREFIX_AND_FUNC(1) << YB_STRUCT_TO_STRING(namespace_id, table_id);
+
+  std::lock_guard mutex_lock(mutex_);
+  auto lock_result = LockForWrite();
+  auto ns_info = GetNamespaceInfoSafe(lock_result, namespace_id);
+  if (!ns_info || ns_info->state() == NamespaceInfoPB::FAILED) {
     return Status::OK();
   }
-  auto& ns_info = outbound_group_pb.mutable_namespace_infos()->at(namespace_id);
-  if (ns_info.table_infos().count(table_id)) {
-    VLOG_WITH_PREFIX_AND_FUNC(2) << "Table " << table_info
-                                 << " is already a part of this replication group";
+
+  if (ns_info->table_infos().count(table_id)) {
+    DCHECK(ns_info->table_infos().at(table_id).has_stream_id());
+    VLOG_WITH_PREFIX_AND_FUNC(2) << "Table " << table_id << " already has a stream "
+                                 << ns_info->table_infos().at(table_id).stream_id();
     return Status::OK();
   }
 
-  auto stream_ids = VERIFY_RESULT(helper_functions_.bootstrap_tables_func(
-      {table_info}, CoarseMonoClock::now(), StreamCheckpointLocation::kOpId0, epoch));
-  CHECK_EQ(stream_ids.size(), 1);
+  {
+    NamespaceInfoPB::TableInfoPB ns_table_info;
+    ns_table_info.set_is_checkpointing(true);
+    ns_table_info.set_is_part_of_initial_bootstrap(false);
+    ns_info->mutable_table_infos()->insert({table_id, std::move(ns_table_info)});
+  }
 
-  SysXClusterOutboundReplicationGroupEntryPB::NamespaceInfoPB::TableInfoPB ns_table_info;
-  ns_table_info.set_stream_id(stream_ids.front().ToString());
-  ns_info.mutable_table_infos()->insert({table_id, std::move(ns_table_info)});
+  auto create_context =
+      VERIFY_RESULT(helper_functions_.create_xcluster_streams_func({table_id}, epoch));
 
-  return Upsert(*lock_result, epoch);
+  SCHECK_EQ(create_context->streams_.size(), 1, IllegalState, "Unexpected number of streams");
+
+  auto* table_info = VERIFY_RESULT(GetTableInfo(*ns_info, table_id));
+  RSTATUS_DCHECK(
+      !table_info->has_stream_id(), IllegalState, "$0 Table $1 already has a stream $1",
+      LogPrefix(), table_id, table_info->stream_id());
+  table_info->set_stream_id(create_context->streams_.front()->id());
+
+  RETURN_NOT_OK(Upsert(*lock_result, epoch, create_context->streams_));
+  create_context->Commit();
+
+  return Status::OK();
+}
+
+Status XClusterOutboundReplicationGroup::CheckpointNewTable(
+    const NamespaceId& namespace_id, const TableId& table_id, const LeaderEpoch& epoch,
+    StdStatusCallback completion_cb) {
+  VLOG_WITH_PREFIX_AND_FUNC(1) << YB_STRUCT_TO_STRING(namespace_id, table_id);
+  xrepl::StreamId stream_id = xrepl::StreamId::Nil();
+
+  {
+    std::lock_guard mutex_lock(mutex_);
+    auto lock_result = LockForWrite();
+    auto ns_info = GetNamespaceInfoSafe(lock_result, namespace_id);
+    if (!ns_info || ns_info->state() == NamespaceInfoPB::FAILED) {
+      completion_cb(Status::OK());
+      return Status::OK();
+    }
+
+    DCHECK_GT(ns_info->table_infos().count(table_id), 0);
+    auto& ns_table_info = ns_info->table_infos().at(table_id);
+    if (!ns_table_info.is_checkpointing()) {
+      SCHECK(
+          ns_table_info.has_stream_id(), IllegalState, "$0 Stream id not found for table $1",
+          LogPrefix(), table_id);
+      VLOG_WITH_PREFIX_AND_FUNC(2)
+          << "Table " << table_id << " is already a part of this replication group";
+      completion_cb(Status::OK());
+      return Status::OK();
+    }
+
+    stream_id = VERIFY_RESULT(xrepl::StreamId::FromString(ns_table_info.stream_id()));
+  }
+
+  RETURN_NOT_OK(helper_functions_.checkpoint_xcluster_streams_func(
+      {{table_id, stream_id}}, StreamCheckpointLocation::kOpId0, epoch,
+      /*check_if_bootstrap_required=*/false,
+      [user_cb = std::move(completion_cb)](Result<bool> result) {
+        if (!result.ok()) {
+          user_cb(result.status());
+        }
+        LOG_IF(DFATAL, result.get()) << "No bootstrap should be needed when checkpointing OpId0";
+        user_cb(Status::OK());
+      }));
+
+  return Status::OK();
 }
 
-void XClusterOutboundReplicationGroup::AddTable(
-    const TableInfoPtr& table_info, const LeaderEpoch& epoch, StdStatusCallback completion_cb) {
-  // TODO(#20810): Perform checkpointing step which involves a cdc_state_table write asynchronously
-  // and push down the callback.
-  completion_cb(AddTableInternal(table_info, epoch));
+Status XClusterOutboundReplicationGroup::MarkNewTablesAsCheckpointed(
+    const NamespaceId& namespace_id, const TableId& table_id, const LeaderEpoch& epoch) {
+  std::lock_guard mutex_lock(mutex_);
+  auto lock_result = LockForWrite();
+  auto ns_info = GetNamespaceInfoSafe(lock_result, namespace_id);
+  if (!ns_info) {
+    return Status::OK();
+  }
+
+  auto* table_info = VERIFY_RESULT(GetTableInfo(*ns_info, table_id));
+  if (!table_info->is_checkpointing()) {
+    return Status::OK();
+  }
+
+  table_info->set_is_checkpointing(false);
+  return Upsert(*lock_result, epoch);
 }
 
 }  // namespace yb::master
diff --git a/src/yb/master/xcluster/xcluster_outbound_replication_group.h b/src/yb/master/xcluster/xcluster_outbound_replication_group.h
index 5d9393617d..3139d6cbf6 100644
--- a/src/yb/master/xcluster/xcluster_outbound_replication_group.h
+++ b/src/yb/master/xcluster/xcluster_outbound_replication_group.h
@@ -28,6 +28,8 @@ class XClusterRemoteClient;
 
 namespace master {
 
+class XClusterOutboundReplicationGroupTaskFactory;
+
 class XClusterOutboundReplicationGroup
     : public std::enable_shared_from_this<XClusterOutboundReplicationGroup>,
       public CatalogEntityWithTasks {
@@ -37,16 +39,21 @@ class XClusterOutboundReplicationGroup
         get_namespace_id_func;
     const std::function<Result<NamespaceName>(const NamespaceId&)> get_namespace_name_func;
     const std::function<Result<std::vector<TableInfoPtr>>(const NamespaceId&)> get_tables_func;
-    const std::function<Result<std::vector<xrepl::StreamId>>(
-        const std::vector<TableInfoPtr>&, CoarseTimePoint,
-        StreamCheckpointLocation checkpoint_location, const LeaderEpoch& epoch)>
-        bootstrap_tables_func;
+    const std::function<Result<std::unique_ptr<XClusterCreateStreamsContext>>(
+        const std::vector<TableId>&, const LeaderEpoch&)>
+        create_xcluster_streams_func;
+    const std::function<Status(
+        const std::vector<std::pair<TableId, xrepl::StreamId>>&, StreamCheckpointLocation,
+        const LeaderEpoch&, bool, std::function<void(Result<bool>)>)>
+        checkpoint_xcluster_streams_func;
     const std::function<Result<DeleteCDCStreamResponsePB>(
         const DeleteCDCStreamRequestPB&, const LeaderEpoch&)>
         delete_cdc_stream_func;
 
     // SysCatalog functions.
-    const std::function<Status(const LeaderEpoch& epoch, XClusterOutboundReplicationGroupInfo*)>
+    const std::function<Status(
+        const LeaderEpoch& epoch, XClusterOutboundReplicationGroupInfo*,
+        const std::vector<scoped_refptr<CDCStreamInfo>>&)>
         upsert_to_sys_catalog_func;
     const std::function<Status(const LeaderEpoch& epoch, XClusterOutboundReplicationGroupInfo*)>
         delete_from_sys_catalog_func;
@@ -55,7 +62,8 @@ class XClusterOutboundReplicationGroup
   explicit XClusterOutboundReplicationGroup(
       const xcluster::ReplicationGroupId& replication_group_id,
       const SysXClusterOutboundReplicationGroupEntryPB& outbound_replication_group_pb,
-      HelperFunctions helper_functions, scoped_refptr<TasksTracker> tasks_tracker);
+      HelperFunctions helper_functions, scoped_refptr<TasksTracker> tasks_tracker,
+      XClusterOutboundReplicationGroupTaskFactory& task_factory);
 
   virtual ~XClusterOutboundReplicationGroup() = default;
 
@@ -67,11 +75,9 @@ class XClusterOutboundReplicationGroup
   Result<SysXClusterOutboundReplicationGroupEntryPB> GetMetadata() const EXCLUDES(mutex_);
 
   Result<std::vector<NamespaceId>> AddNamespaces(
-      const LeaderEpoch& epoch, const std::vector<NamespaceName>& namespace_names,
-      CoarseTimePoint deadline) EXCLUDES(mutex_);
+      const LeaderEpoch& epoch, const std::vector<NamespaceName>& namespace_names) EXCLUDES(mutex_);
 
-  Result<NamespaceId> AddNamespace(
-      const LeaderEpoch& epoch, const NamespaceName& namespace_name, CoarseTimePoint deadline)
+  Result<NamespaceId> AddNamespace(const LeaderEpoch& epoch, const NamespaceName& namespace_name)
       EXCLUDES(mutex_);
 
   Status RemoveNamespace(const LeaderEpoch& epoch, const NamespaceId& namespace_id)
@@ -101,12 +107,12 @@ class XClusterOutboundReplicationGroup
 
   bool HasNamespace(const NamespaceId& namespace_id) const EXCLUDES(mutex_);
 
-  void AddTable(
-      const TableInfoPtr& table_info, const LeaderEpoch& epoch, StdStatusCallback completion_cb)
-      EXCLUDES(mutex_);
-
  private:
   friend class XClusterOutboundReplicationGroupMocked;
+  friend class AddTableToXClusterSourceTask;
+  friend class XClusterCheckpointNamespaceTask;
+
+  using NamespaceInfoPB = SysXClusterOutboundReplicationGroupEntryPB::NamespaceInfoPB;
 
   // LockForRead and LockForWrite are used to get a read or write lock on the outbound_rg_info_ and
   // ensure the group is not deleted.
@@ -117,22 +123,18 @@ class XClusterOutboundReplicationGroup
   Result<XClusterOutboundReplicationGroupInfo::WriteLock> LockForWrite() REQUIRES(mutex_);
 
   Result<NamespaceId> AddNamespaceInternal(
-      const NamespaceName& namespace_name, CoarseTimePoint deadline,
-      XClusterOutboundReplicationGroupInfo::WriteLock& l, const LeaderEpoch& epoch)
-      REQUIRES(mutex_);
+      const NamespaceName& namespace_name, XClusterOutboundReplicationGroupInfo::WriteLock& l,
+      const LeaderEpoch& epoch) REQUIRES(mutex_);
 
-  Status Upsert(XClusterOutboundReplicationGroupInfo::WriteLock& l, const LeaderEpoch& epoch)
-      REQUIRES(mutex_);
+  Status Upsert(
+      XClusterOutboundReplicationGroupInfo::WriteLock& l, const LeaderEpoch& epoch,
+      const std::vector<scoped_refptr<CDCStreamInfo>>& streams = {}) REQUIRES(mutex_);
 
   // Deletes all the streams for the given namespace.
   Status DeleteNamespaceStreams(
       const LeaderEpoch& epoch, const NamespaceId& namespace_id,
       const SysXClusterOutboundReplicationGroupEntryPB& pb) REQUIRES(mutex_);
 
-  Result<SysXClusterOutboundReplicationGroupEntryPB::NamespaceInfoPB> BootstrapTables(
-      const std::vector<TableInfoPtr>& table_infos, CoarseTimePoint deadline,
-      const LeaderEpoch& epoch) REQUIRES(mutex_);
-
   virtual Result<std::shared_ptr<client::XClusterRemoteClient>> GetRemoteClient(
       const std::vector<HostPort>& remote_masters) const;
 
@@ -143,12 +145,73 @@ class XClusterOutboundReplicationGroup
   Status AddTableInternal(TableInfoPtr table_info, const LeaderEpoch& epoch)
       EXCLUDES(mutex_);
 
+  Result<NamespaceInfoPB> CreateNamespaceInfo(
+      const NamespaceId& namespace_id, const LeaderEpoch& epoch) REQUIRES(mutex_);
+
+  // Returns the NamespaceInfoPB for the given namespace_id. If its not found returns a NotFound
+  // status. Caller must hold the WriteLock.
+  Result<NamespaceInfoPB*> GetNamespaceInfo(const NamespaceId& namespace_id) REQUIRES(mutex_);
+  // Const version of the above. Caller must hold the ReadLock.
+  Result<const NamespaceInfoPB*> GetNamespaceInfo(const NamespaceId& namespace_id) const
+      REQUIRES_SHARED(mutex_);
+
+  // Graceful version of the above that does not return a bad status.
+  // Validates the WriteLock result and returns the NamespaceInfoPB for the given namespace_id. If
+  // WriteLock failed due to the replication group being dropped of if the namespace is not found
+  // returns nullptr.
+  NamespaceInfoPB* GetNamespaceInfoSafe(
+      const Result<XClusterOutboundReplicationGroupInfo::WriteLock>& lock_result,
+      const NamespaceId& namespace_id) const REQUIRES(mutex_);
+
+  Result<NamespaceInfoPB::TableInfoPB*> GetTableInfo(
+      NamespaceInfoPB& ns_info, const TableId& table_id) REQUIRES(mutex_);
+
+  Status CreateStreamForNewTable(
+      const NamespaceId& namespace_id, const TableId& table_id, const LeaderEpoch& epoch)
+      EXCLUDES(mutex_);
+
+  Status CheckpointNewTable(
+      const NamespaceId& namespace_id, const TableId& table_id, const LeaderEpoch& epoch,
+      StdStatusCallback completion_cb) EXCLUDES(mutex_);
+
+  Status MarkNewTablesAsCheckpointed(
+      const NamespaceId& namespace_id, const TableId& table_id, const LeaderEpoch& epoch);
+
+  void StartNamespaceCheckpointTasks(
+      const std::vector<NamespaceId>& namespace_ids, const LeaderEpoch& epoch);
+
+  // Create streams for tables that were part of the initial bootstrap and do not yet have a stream.
+  // This function is idempotent.
+  Status CreateStreamsForInitialBootstrap(const NamespaceId& namespace_id, const LeaderEpoch& epoch)
+      EXCLUDES(mutex_);
+
+  // Checkpoint streams for tables that were part of the initial bootstrap and have not yet been
+  // checkpointed. This function is idempotent.
+  Status CheckpointStreamsForInitialBootstrap(
+      const NamespaceId& namespace_id, const LeaderEpoch& epoch,
+      std::function<void(XClusterCheckpointStreamsResult)> completion_cb) EXCLUDES(mutex_);
+
+  // Returns False if there are any more table from the initial bootstrap that are pending
+  // checkpointing.
+  // Returns True if all tables have been checkpointed and namespace is marked as READY.
+  //  This function is idempotent.
+  Result<bool> MarkBootstrapTablesAsCheckpointed(
+      const NamespaceId& namespace_id, XClusterCheckpointStreamsResult checkpoint_result,
+      const LeaderEpoch& epoch);
+
+  // If the checkpointing operation failed then set the namespace to FAILED state and record the
+  // error.
+  void MarkCheckpointNamespaceAsFailed(
+      const NamespaceId& namespace_id, const LeaderEpoch& epoch, const Status& status);
+
   HelperFunctions helper_functions_;
 
   // Mutex used to ensure reads are not allowed when writes are happening.
   mutable std::shared_mutex mutex_;
   std::unique_ptr<XClusterOutboundReplicationGroupInfo> outbound_rg_info_;
 
+  XClusterOutboundReplicationGroupTaskFactory& task_factory_;
+
   DISALLOW_COPY_AND_ASSIGN(XClusterOutboundReplicationGroup);
 };
 
diff --git a/src/yb/master/xcluster/xcluster_outbound_replication_group_tasks.cc b/src/yb/master/xcluster/xcluster_outbound_replication_group_tasks.cc
new file mode 100644
index 0000000000..2641bc1dee
--- /dev/null
+++ b/src/yb/master/xcluster/xcluster_outbound_replication_group_tasks.cc
@@ -0,0 +1,96 @@
+// Copyright (c) YugabyteDB, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
+// in compliance with the License.  You may obtain a copy of the License at
+//
+// http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software distributed under the License
+// is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
+// or implied.  See the License for the specific language governing permissions and limitations
+// under the License.
+//
+
+#include "yb/master/xcluster/xcluster_outbound_replication_group_tasks.h"
+
+#include "yb/master/catalog_manager.h"
+#include "yb/master/xcluster/xcluster_outbound_replication_group.h"
+
+using namespace std::placeholders;
+
+namespace yb::master {
+
+XClusterOutboundReplicationGroupTaskFactory::XClusterOutboundReplicationGroupTaskFactory(
+    std::function<Status(const LeaderEpoch& epoch)> validate_epoch_func,
+    ThreadPool& async_task_pool, rpc::Messenger& messenger)
+    : validate_epoch_func_(std::move(validate_epoch_func)),
+      async_task_pool_(async_task_pool),
+      messenger_(messenger) {}
+
+std::shared_ptr<XClusterCheckpointNamespaceTask>
+XClusterOutboundReplicationGroupTaskFactory::CreateCheckpointNamespaceTask(
+    XClusterOutboundReplicationGroup& outbound_replication_group, const NamespaceId& namespace_id,
+    const LeaderEpoch& epoch) {
+  return std::make_shared<XClusterCheckpointNamespaceTask>(
+      outbound_replication_group, namespace_id, validate_epoch_func_, async_task_pool_, messenger_,
+      epoch);
+}
+
+XClusterCheckpointNamespaceTask::XClusterCheckpointNamespaceTask(
+    XClusterOutboundReplicationGroup& outbound_replication_group, const NamespaceId& namespace_id,
+    std::function<Status(const LeaderEpoch& epoch)> validate_epoch_func,
+    ThreadPool& async_task_pool, rpc::Messenger& messenger, const LeaderEpoch& epoch)
+    : MultiStepCatalogEntityTask(
+          std::move(validate_epoch_func), async_task_pool, messenger, outbound_replication_group,
+          epoch),
+      namespace_id_(namespace_id),
+      outbound_replication_group_(outbound_replication_group) {}
+
+std::string XClusterCheckpointNamespaceTask::description() const {
+  return Format("XClusterCheckpointNamespaceTask [$0]", outbound_replication_group_.Id());
+}
+
+Status XClusterCheckpointNamespaceTask::FirstStep() {
+  RETURN_NOT_OK(
+      outbound_replication_group_.CreateStreamsForInitialBootstrap(namespace_id_, epoch_));
+  ScheduleNextStep(
+      std::bind(&XClusterCheckpointNamespaceTask::CheckpointStreams, this), "CheckpointStreams");
+  return Status::OK();
+}
+
+Status XClusterCheckpointNamespaceTask::CheckpointStreams() {
+  RETURN_NOT_OK(outbound_replication_group_.CheckpointStreamsForInitialBootstrap(
+      namespace_id_, epoch_,
+      std::bind(&XClusterCheckpointNamespaceTask::CheckpointStreamsCallback, this, _1)));
+
+  return Status::OK();
+}
+
+void XClusterCheckpointNamespaceTask::CheckpointStreamsCallback(
+    XClusterCheckpointStreamsResult result) {
+  ScheduleNextStep(
+      std::bind(
+          &XClusterCheckpointNamespaceTask::MarkTablesAsCheckpointed, this, std::move(result)),
+      "MarkTablesAsCheckpointed");
+}
+
+Status XClusterCheckpointNamespaceTask::MarkTablesAsCheckpointed(
+    XClusterCheckpointStreamsResult result) {
+  if (VERIFY_RESULT(outbound_replication_group_.MarkBootstrapTablesAsCheckpointed(
+          namespace_id_, std::move(result), epoch_))) {
+    // All tables have been checkpointed and the replication group is now READY.
+    Complete();
+  } else {
+    ScheduleNextStep(
+        std::bind(&XClusterCheckpointNamespaceTask::CheckpointStreams, this), "CheckpointStreams");
+  }
+  return Status::OK();
+}
+
+void XClusterCheckpointNamespaceTask::TaskCompleted(const Status& status) {
+  if (!status.ok()) {
+    outbound_replication_group_.MarkCheckpointNamespaceAsFailed(namespace_id_, epoch_, status);
+  }
+}
+
+} // namespace yb::master
diff --git a/src/yb/master/xcluster/xcluster_outbound_replication_group_tasks.h b/src/yb/master/xcluster/xcluster_outbound_replication_group_tasks.h
new file mode 100644
index 0000000000..6fa7e7c8f9
--- /dev/null
+++ b/src/yb/master/xcluster/xcluster_outbound_replication_group_tasks.h
@@ -0,0 +1,77 @@
+// Copyright (c) YugabyteDB, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
+// in compliance with the License.  You may obtain a copy of the License at
+//
+// http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software distributed under the License
+// is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
+// or implied.  See the License for the specific language governing permissions and limitations
+// under the License.
+//
+
+#pragma once
+
+#include "yb/common/entity_ids_types.h"
+#include "yb/master/multi_step_monitored_task.h"
+#include "yb/master/xcluster/master_xcluster_types.h"
+
+namespace yb {
+
+namespace master {
+
+class XClusterOutboundReplicationGroup;
+
+class XClusterCheckpointNamespaceTask : public MultiStepCatalogEntityTask {
+ public:
+  XClusterCheckpointNamespaceTask(
+      XClusterOutboundReplicationGroup& outbound_replication_group, const NamespaceId& namespace_id,
+      std::function<Status(const LeaderEpoch& epoch)> validate_epoch_func,
+      ThreadPool& async_task_pool, rpc::Messenger& messenger, const LeaderEpoch& epoch);
+
+  ~XClusterCheckpointNamespaceTask() = default;
+
+  server::MonitoredTaskType type() const override {
+    return server::MonitoredTaskType::kAddNamespaceToXClusterSource;
+  }
+
+  std::string type_name() const override {
+    return "Add namespace to xCluster outbound replication group";
+  }
+
+  std::string description() const override;
+
+  Status FirstStep() override;
+
+ private:
+  Status CheckpointStreams();
+  void CheckpointStreamsCallback(XClusterCheckpointStreamsResult result);
+  Status MarkTablesAsCheckpointed(XClusterCheckpointStreamsResult result);
+  void TaskCompleted(const Status& s) override;
+
+  const NamespaceId namespace_id_;
+  XClusterOutboundReplicationGroup& outbound_replication_group_;
+};
+
+class XClusterOutboundReplicationGroupTaskFactory {
+ public:
+  XClusterOutboundReplicationGroupTaskFactory(
+      std::function<Status(const LeaderEpoch& epoch)> validate_epoch_func,
+      ThreadPool& async_task_pool, rpc::Messenger& messenger);
+
+  virtual ~XClusterOutboundReplicationGroupTaskFactory() = default;
+
+  std::shared_ptr<XClusterCheckpointNamespaceTask> CreateCheckpointNamespaceTask(
+      XClusterOutboundReplicationGroup& outbound_replication_group, const NamespaceId& namespace_id,
+      const LeaderEpoch& epoch);
+
+ protected:
+  std::function<Status(const LeaderEpoch& epoch)> validate_epoch_func_;
+  ThreadPool& async_task_pool_;
+  rpc::Messenger& messenger_;
+};
+
+}  // namespace master
+
+}  // namespace yb
diff --git a/src/yb/master/xcluster/xcluster_source_manager.cc b/src/yb/master/xcluster/xcluster_source_manager.cc
index 7829d61dbc..a9425b8b30 100644
--- a/src/yb/master/xcluster/xcluster_source_manager.cc
+++ b/src/yb/master/xcluster/xcluster_source_manager.cc
@@ -14,12 +14,14 @@
 #include "yb/master/xcluster/xcluster_source_manager.h"
 
 #include "yb/cdc/cdc_service.h"
-
 #include "yb/cdc/cdc_service.proxy.h"
+#include "yb/cdc/cdc_state_table.h"
 #include "yb/master/catalog_manager.h"
 #include "yb/master/master.h"
 #include "yb/master/xcluster/add_table_to_xcluster_source_task.h"
 #include "yb/master/xcluster/xcluster_catalog_entity.h"
+#include "yb/master/xcluster/xcluster_outbound_replication_group.h"
+#include "yb/master/xcluster/xcluster_outbound_replication_group_tasks.h"
 
 #include "yb/rpc/rpc_context.h"
 #include "yb/util/scope_exit.h"
@@ -62,6 +64,15 @@ XClusterSourceManager::XClusterSourceManager(
 
 XClusterSourceManager::~XClusterSourceManager() {}
 
+Status XClusterSourceManager::Init() {
+  async_task_factory_ = std::make_unique<XClusterOutboundReplicationGroupTaskFactory>(
+      catalog_manager_.GetValidateEpochFunc(), *catalog_manager_.AsyncTaskPool(),
+      *master_.messenger());
+
+  cdc_state_table_ = std::make_unique<cdc::CDCStateTable>(&master_.cdc_state_client_initializer());
+  return Status::OK();
+}
+
 void XClusterSourceManager::Clear() {
   CatalogEntityWithTasks::CloseAbortAndWaitForAllTasks(GetAllOutboundGroups());
 
@@ -148,15 +159,11 @@ XClusterSourceManager::InitOutboundReplicationGroup(
           [&catalog_manager = catalog_manager_](const NamespaceId& namespace_id) {
             return catalog_manager.GetNamespaceName(namespace_id);
           },
-      .get_tables_func =
-          [this](const NamespaceId& namespace_id) { return GetTablesToReplicate(namespace_id); },
-      .bootstrap_tables_func =
-          [this](
-              const std::vector<TableInfoPtr>& table_infos, CoarseTimePoint deadline,
-              StreamCheckpointLocation checkpoint_location,
-              const LeaderEpoch& epoch) -> Result<std::vector<xrepl::StreamId>> {
-        return BootstrapTables(table_infos, deadline, checkpoint_location, epoch);
-      },
+      .get_tables_func = std::bind(&XClusterSourceManager::GetTablesToReplicate, this, _1),
+      .create_xcluster_streams_func =
+          std::bind(&XClusterSourceManager::CreateNewXClusterStreams, this, _1, _2),
+      .checkpoint_xcluster_streams_func =
+          std::bind(&XClusterSourceManager::CheckpointXClusterStreams, this, _1, _2, _3, _4, _5),
       .delete_cdc_stream_func = [&catalog_manager = catalog_manager_](
                                     const DeleteCDCStreamRequestPB& req,
                                     const LeaderEpoch& epoch) -> Result<DeleteCDCStreamResponsePB> {
@@ -166,8 +173,9 @@ XClusterSourceManager::InitOutboundReplicationGroup(
       },
       .upsert_to_sys_catalog_func =
           [&sys_catalog = sys_catalog_](
-              const LeaderEpoch& epoch, XClusterOutboundReplicationGroupInfo* info) {
-            return sys_catalog.Upsert(epoch.leader_term, info);
+              const LeaderEpoch& epoch, XClusterOutboundReplicationGroupInfo* info,
+              const std::vector<scoped_refptr<CDCStreamInfo>>& streams) {
+            return sys_catalog.Upsert(epoch.leader_term, info, streams);
           },
       .delete_from_sys_catalog_func =
           [&sys_catalog = sys_catalog_](
@@ -178,7 +186,7 @@ XClusterSourceManager::InitOutboundReplicationGroup(
 
   return std::make_shared<XClusterOutboundReplicationGroup>(
       replication_group_id, metadata, std::move(helper_functions),
-      catalog_manager_.GetTasksTracker());
+      catalog_manager_.GetTasksTracker(), *async_task_factory_);
 }
 
 Result<std::shared_ptr<XClusterOutboundReplicationGroup>>
@@ -202,77 +210,6 @@ Result<std::vector<TableInfoPtr>> XClusterSourceManager::GetTablesToReplicate(
   return table_infos;
 }
 
-Result<std::vector<xrepl::StreamId>> XClusterSourceManager::BootstrapTables(
-    const std::vector<TableInfoPtr>& table_infos, CoarseTimePoint deadline,
-    StreamCheckpointLocation checkpoint_location, const LeaderEpoch& epoch) {
-  if (checkpoint_location == StreamCheckpointLocation::kOpId0) {
-    std::vector<xrepl::StreamId> stream_ids;
-    for (const auto& table_info : table_infos) {
-      const auto& table_id = table_info->id();
-      master::CreateCDCStreamRequestPB create_stream_req;
-      master::CreateCDCStreamResponsePB create_stream_resp;
-      create_stream_req.set_table_id(table_info->id());
-
-      std::unordered_map<std::string, std::string> options;
-      auto record_type_option = create_stream_req.add_options();
-      record_type_option->set_key(cdc::kRecordType);
-      record_type_option->set_value(CDCRecordType_Name(cdc::CDCRecordType::CHANGE));
-      auto record_format_option = create_stream_req.add_options();
-      record_format_option->set_key(cdc::kRecordFormat);
-      record_format_option->set_value(CDCRecordFormat_Name(cdc::CDCRecordFormat::WAL));
-
-      RETURN_NOT_OK(catalog_manager_.CreateNewXReplStream(
-          create_stream_req, CreateNewCDCStreamMode::kXClusterTableIds, {table_id},
-          /*namespace_id=*/std::nullopt, &create_stream_resp, epoch, /*rpc=*/nullptr));
-
-      if (create_stream_resp.has_error()) {
-        return StatusFromPB(create_stream_resp.error().status());
-      }
-      stream_ids.emplace_back(
-          VERIFY_RESULT(xrepl::StreamId::FromString(create_stream_resp.stream_id())));
-    }
-
-    return stream_ids;
-  }
-
-  LOG_IF(DFATAL, checkpoint_location != StreamCheckpointLocation::kCurrentEndOfWAL)
-      << "Not implemented yet. Checkpoint location: " << checkpoint_location;
-
-  cdc::BootstrapProducerRequestPB bootstrap_req;
-  master::TSDescriptor* ts = nullptr;
-  for (const auto& table_info : table_infos) {
-    bootstrap_req.add_table_ids(table_info->id());
-
-    if (!ts) {
-      ts = VERIFY_RESULT(table_info->GetTablets().front()->GetLeader());
-    }
-  }
-  SCHECK(ts, IllegalState, "No valid tserver found to bootstrap from");
-
-  std::shared_ptr<cdc::CDCServiceProxy> proxy;
-  RETURN_NOT_OK(ts->GetProxy(&proxy));
-
-  cdc::BootstrapProducerResponsePB bootstrap_resp;
-  rpc::RpcController bootstrap_rpc;
-  bootstrap_rpc.set_deadline(deadline);
-
-  // TODO(Hari): DB-9416 Make this async and atomic with upsert of the outbound replication group.
-  RETURN_NOT_OK(proxy->BootstrapProducer(bootstrap_req, &bootstrap_resp, &bootstrap_rpc));
-  if (bootstrap_resp.has_error()) {
-    RETURN_NOT_OK(StatusFromPB(bootstrap_resp.error().status()));
-  }
-
-  SCHECK_EQ(
-      table_infos.size(), bootstrap_resp.cdc_bootstrap_ids_size(), IllegalState,
-      "Number of tables to bootstrap and number of bootstrap ids do not match");
-
-  std::vector<xrepl::StreamId> stream_ids;
-  for (const auto& bootstrap_id : bootstrap_resp.cdc_bootstrap_ids()) {
-    stream_ids.emplace_back(VERIFY_RESULT(xrepl::StreamId::FromString(bootstrap_id)));
-  }
-  return stream_ids;
-}
-
 std::vector<std::shared_ptr<PostTabletCreateTaskBase>>
 XClusterSourceManager::GetPostTabletCreateTasks(
     const TableInfoPtr& table_info, const LeaderEpoch& epoch) {
@@ -296,6 +233,10 @@ XClusterSourceManager::GetPostTabletCreateTasks(
 
 std::optional<uint32> XClusterSourceManager::GetDefaultWalRetentionSec(
     const NamespaceId& namespace_id) const {
+  if (namespace_id == kSystemNamespaceId) {
+    return std::nullopt;
+  }
+
   for (const auto& outbound_replication_group : GetAllOutboundGroups()) {
     if (outbound_replication_group->HasNamespace(namespace_id)) {
       return FLAGS_cdc_wal_retention_time_secs;
@@ -307,8 +248,7 @@ std::optional<uint32> XClusterSourceManager::GetDefaultWalRetentionSec(
 
 Result<std::vector<NamespaceId>> XClusterSourceManager::CreateOutboundReplicationGroup(
     const xcluster::ReplicationGroupId& replication_group_id,
-    const std::vector<NamespaceName>& namespace_names, const LeaderEpoch& epoch,
-    CoarseTimePoint deadline) {
+    const std::vector<NamespaceName>& namespace_names, const LeaderEpoch& epoch) {
   {
     std::lock_guard l(outbound_replication_group_map_mutex_);
     SCHECK(
@@ -330,7 +270,7 @@ Result<std::vector<NamespaceId>> XClusterSourceManager::CreateOutboundReplicatio
 
   // This will persist the group to SysCatalog.
   auto namespace_ids =
-      VERIFY_RESULT(outbound_replication_group->AddNamespaces(epoch, namespace_names, deadline));
+      VERIFY_RESULT(outbound_replication_group->AddNamespaces(epoch, namespace_names));
 
   se.Cancel();
   {
@@ -343,11 +283,11 @@ Result<std::vector<NamespaceId>> XClusterSourceManager::CreateOutboundReplicatio
 
 Result<NamespaceId> XClusterSourceManager::AddNamespaceToOutboundReplicationGroup(
     const xcluster::ReplicationGroupId& replication_group_id, const NamespaceName& namespace_name,
-    const LeaderEpoch& epoch, CoarseTimePoint deadline) {
+    const LeaderEpoch& epoch) {
   auto outbound_replication_group =
       VERIFY_RESULT(GetOutboundReplicationGroup(replication_group_id));
 
-  return outbound_replication_group->AddNamespace(epoch, namespace_name, deadline);
+  return outbound_replication_group->AddNamespace(epoch, namespace_name);
 }
 
 Status XClusterSourceManager::RemoveNamespaceFromOutboundReplicationGroup(
@@ -412,4 +352,160 @@ Result<IsOperationDoneResult> XClusterSourceManager::IsCreateXClusterReplication
       target_master_addresses, epoch);
 }
 
+class XClusterCreateStreamContextImpl : public XClusterCreateStreamsContext {
+ public:
+  explicit XClusterCreateStreamContextImpl(CatalogManager& catalog_manager)
+      : catalog_manager_(catalog_manager) {}
+  virtual ~XClusterCreateStreamContextImpl() { Rollback(); }
+
+  void Commit() override {
+    for (auto& stream : streams_) {
+      stream->mutable_metadata()->CommitMutation();
+    }
+    streams_.clear();
+  }
+
+  void Rollback() {
+    for (const auto& stream : streams_) {
+      catalog_manager_.ReleaseAbandonedXReplStream(stream->StreamId());
+    }
+  }
+
+  CatalogManager& catalog_manager_;
+};
+
+Result<std::unique_ptr<XClusterCreateStreamsContext>>
+XClusterSourceManager::CreateNewXClusterStreams(
+    const std::vector<TableId>& table_ids, const LeaderEpoch& epoch) {
+  const auto state = SysCDCStreamEntryPB::ACTIVE;
+  const bool transactional = true;
+
+  RETURN_NOT_OK(catalog_manager_.CreateCdcStateTableIfNotFound(epoch));
+
+  auto create_context = std::make_unique<XClusterCreateStreamContextImpl>(catalog_manager_);
+
+  for (const auto& table_id : table_ids) {
+    auto table_info = VERIFY_RESULT(catalog_manager_.FindTableById(table_id));
+    SCHECK(
+        table_info->LockForRead()->visible_to_client(), NotFound, "Table does not exist", table_id);
+
+    VLOG(1) << "Creating xcluster streams for table: " << table_id;
+
+    auto stream = VERIFY_RESULT(catalog_manager_.InitNewXReplStream());
+    auto& metadata = stream->mutable_metadata()->mutable_dirty()->pb;
+    metadata.add_table_id(table_id);
+    metadata.set_transactional(transactional);
+
+    std::unordered_map<std::string, std::string> options;
+    auto record_type_option = metadata.add_options();
+    record_type_option->set_key(cdc::kRecordType);
+    record_type_option->set_value(CDCRecordType_Name(cdc::CDCRecordType::CHANGE));
+    auto record_format_option = metadata.add_options();
+    record_format_option->set_key(cdc::kRecordFormat);
+    record_format_option->set_value(CDCRecordFormat_Name(cdc::CDCRecordFormat::WAL));
+    metadata.set_state(state);
+
+    create_context->streams_.emplace_back(std::move(stream));
+  }
+
+  return create_context;
+}
+
+Status XClusterSourceManager::CheckpointXClusterStreams(
+    const std::vector<std::pair<TableId, xrepl::StreamId>>& table_streams,
+    StreamCheckpointLocation checkpoint_location, const LeaderEpoch& epoch,
+    bool check_if_bootstrap_required, std::function<void(Result<bool>)> user_callback) {
+  switch (checkpoint_location) {
+    case StreamCheckpointLocation::kOpId0:
+      SCHECK(!check_if_bootstrap_required, InvalidArgument, "Bootstrap is not required for OpId0");
+      return CheckpointStreamsToOp0(
+          table_streams, [user_callback = std::move(user_callback)](const Status& status) {
+            if (!status.ok()) {
+              user_callback(status);
+              return;
+            }
+
+            // Bootstrap is not required for OpId0.
+            user_callback(false);
+          });
+    case StreamCheckpointLocation::kCurrentEndOfWAL:
+      return CheckpointStreamsToEndOfWAL(
+          table_streams, epoch, check_if_bootstrap_required, user_callback);
+  }
+
+  FATAL_INVALID_ENUM_VALUE(StreamCheckpointLocation, checkpoint_location);
+}
+
+Status XClusterSourceManager::CheckpointStreamsToOp0(
+    const std::vector<std::pair<TableId, xrepl::StreamId>>& table_streams,
+    StdStatusCallback user_callback) {
+  std::vector<cdc::CDCStateTableEntry> entries;
+  for (const auto& [table_id, stream_id] : table_streams) {
+    auto table = VERIFY_RESULT(catalog_manager_.FindTableById(table_id));
+    for (const auto& tablet : table->GetTablets()) {
+      cdc::CDCStateTableEntry entry(tablet->id(), stream_id);
+      entry.checkpoint = OpId().Min();
+      entry.last_replication_time = GetCurrentTimeMicros();
+      entries.push_back(std::move(entry));
+    }
+  }
+
+  return cdc_state_table_->InsertEntriesAsync(entries, std::move(user_callback));
+}
+
+Status XClusterSourceManager::CheckpointStreamsToEndOfWAL(
+    const std::vector<std::pair<TableId, xrepl::StreamId>>& table_streams, const LeaderEpoch& epoch,
+    bool check_if_bootstrap_required, std::function<void(Result<bool>)> user_callback) {
+  cdc::BootstrapProducerRequestPB bootstrap_req;
+  bootstrap_req.set_check_if_bootstrap_required(check_if_bootstrap_required);
+
+  master::TSDescriptor* ts_desc = nullptr;
+  for (const auto& [table_id, stream_id] : table_streams) {
+    VLOG(1) << "Checkpointing xcluster stream " << stream_id << " of table " << table_id;
+
+    // Set WAL retention here instead of during stream creation as we are processing smaller batches
+    // of tables during the checkpoint phase whereas we create all streams in one batch to reduce
+    // master IOs.
+    RETURN_NOT_OK(catalog_manager_.SetWalRetentionForTable(table_id, /*rpc=*/nullptr, epoch));
+
+    auto table_info = VERIFY_RESULT(catalog_manager_.FindTableById(table_id));
+    SCHECK(
+        table_info->LockForRead()->visible_to_client(), NotFound, "Table does not exist", table_id);
+    bootstrap_req.add_table_ids(table_info->id());
+    bootstrap_req.add_xrepl_stream_ids(stream_id.ToString());
+
+    if (!ts_desc) {
+      ts_desc = VERIFY_RESULT(table_info->GetTablets().front()->GetLeader());
+    }
+  }
+  SCHECK(ts_desc, IllegalState, "No valid tserver found to bootstrap from");
+
+  std::shared_ptr<cdc::CDCServiceProxy> proxy;
+  RETURN_NOT_OK(ts_desc->GetProxy(&proxy));
+
+  auto bootstrap_resp = std::make_shared<cdc::BootstrapProducerResponsePB>();
+  auto bootstrap_rpc = std::make_shared<rpc::RpcController>();
+
+  auto process_response = [bootstrap_resp, bootstrap_rpc]() -> Result<bool> {
+    RETURN_NOT_OK(bootstrap_rpc->status());
+    if (bootstrap_resp->has_error()) {
+      return StatusFromPB(bootstrap_resp->error().status());
+    }
+
+    SCHECK_EQ(
+        bootstrap_resp->cdc_bootstrap_ids_size(), 0, IllegalState,
+        "No new streams should have been created");
+
+    return bootstrap_resp->bootstrap_required();
+  };
+
+  proxy->BootstrapProducerAsync(
+      bootstrap_req, bootstrap_resp.get(), bootstrap_rpc.get(),
+      [user_callback = std::move(user_callback), process_response = std::move(process_response)]() {
+        user_callback(process_response());
+      });
+
+  return Status::OK();
+}
+
 }  // namespace yb::master
diff --git a/src/yb/master/xcluster/xcluster_source_manager.h b/src/yb/master/xcluster/xcluster_source_manager.h
index 54c1346840..0a41a58e15 100644
--- a/src/yb/master/xcluster/xcluster_source_manager.h
+++ b/src/yb/master/xcluster/xcluster_source_manager.h
@@ -19,8 +19,10 @@
 
 #include "yb/gutil/thread_annotations.h"
 
+#include "yb/master/leader_epoch.h"
+#include "yb/master/master_fwd.h"
 #include "yb/master/xcluster/master_xcluster_types.h"
-#include "yb/master/xcluster/xcluster_outbound_replication_group.h"
+#include "yb/util/status_callback.h"
 
 namespace yb {
 
@@ -33,17 +35,22 @@ class CatalogManager;
 class Master;
 class PostTabletCreateTaskBase;
 class SysCatalogTable;
+class XClusterOutboundReplicationGroup;
+class XClusterOutboundReplicationGroupInfo;
+class XClusterOutboundReplicationGroupTaskFactory;
 
 class XClusterSourceManager {
  public:
   std::optional<uint32> GetDefaultWalRetentionSec(const NamespaceId& namespace_id) const;
 
  protected:
-  explicit XClusterSourceManager(
+  XClusterSourceManager(
       Master& master, CatalogManager& catalog_manager, SysCatalogTable& sys_catalog);
 
   ~XClusterSourceManager();
 
+  Status Init();
+
   void Clear();
 
   Status RunLoaders();
@@ -57,12 +64,11 @@ class XClusterSourceManager {
 
   Result<std::vector<NamespaceId>> CreateOutboundReplicationGroup(
       const xcluster::ReplicationGroupId& replication_group_id,
-      const std::vector<NamespaceName>& namespace_names, const LeaderEpoch& epoch,
-      CoarseTimePoint deadline);
+      const std::vector<NamespaceName>& namespace_names, const LeaderEpoch& epoch);
 
   Result<NamespaceId> AddNamespaceToOutboundReplicationGroup(
       const xcluster::ReplicationGroupId& replication_group_id, const NamespaceName& namespace_name,
-      const LeaderEpoch& epoch, CoarseTimePoint deadline);
+      const LeaderEpoch& epoch);
 
   Status RemoveNamespaceFromOutboundReplicationGroup(
       const xcluster::ReplicationGroupId& replication_group_id, const NamespaceId& namespace_id,
@@ -88,6 +94,8 @@ class XClusterSourceManager {
       const std::vector<HostPort>& target_master_addresses, const LeaderEpoch& epoch);
 
  private:
+  friend class XClusterOutboundReplicationGroup;
+
   // Get all the outbound replication groups. Shared pointer are guaranteed not to be null.
   std::vector<std::shared_ptr<XClusterOutboundReplicationGroup>> GetAllOutboundGroups() const
       EXCLUDES(outbound_replication_group_map_mutex_);
@@ -107,9 +115,26 @@ class XClusterSourceManager {
 
   Result<std::vector<TableInfoPtr>> GetTablesToReplicate(const NamespaceId& namespace_id);
 
-  Result<std::vector<xrepl::StreamId>> BootstrapTables(
-      const std::vector<TableInfoPtr>& table_infos, CoarseTimePoint deadline,
-      StreamCheckpointLocation checkpoint_location, const LeaderEpoch& epoch);
+  Result<std::unique_ptr<XClusterCreateStreamsContext>> CreateNewXClusterStreams(
+      const std::vector<TableId>& table_ids, const LeaderEpoch& epoch);
+
+  // Checkpoint the xCluster stream to the given location. Invokes callback with true if bootstrap
+  // is required, and false is bootstrap is not required.
+  Status CheckpointXClusterStreams(
+      const std::vector<std::pair<TableId, xrepl::StreamId>>& table_streams,
+      StreamCheckpointLocation checkpoint_location, const LeaderEpoch& epoch,
+      bool check_if_bootstrap_required, std::function<void(Result<bool>)> user_callback);
+
+  Status CheckpointStreamsToOp0(
+      const std::vector<std::pair<TableId, xrepl::StreamId>>& table_streams,
+      StdStatusCallback user_callback);
+
+  Status SetWALRetentionOnTables(const std::vector<TableId>& table_ids, const LeaderEpoch& epoch);
+
+  Status CheckpointStreamsToEndOfWAL(
+      const std::vector<std::pair<TableId, xrepl::StreamId>>& table_streams,
+      const LeaderEpoch& epoch, bool check_if_bootstrap_required,
+      std::function<void(Result<bool>)> user_callback);
 
   Master& master_;
   CatalogManager& catalog_manager_;
@@ -122,6 +147,10 @@ class XClusterSourceManager {
   std::map<xcluster::ReplicationGroupId, std::shared_ptr<XClusterOutboundReplicationGroup>>
       outbound_replication_group_map_ GUARDED_BY(outbound_replication_group_map_mutex_);
 
+  std::unique_ptr<XClusterOutboundReplicationGroupTaskFactory> async_task_factory_;
+
+  std::unique_ptr<cdc::CDCStateTable> cdc_state_table_;
+
   DISALLOW_COPY_AND_ASSIGN(XClusterSourceManager);
 };
 
diff --git a/src/yb/master/xrepl_catalog_manager.cc b/src/yb/master/xrepl_catalog_manager.cc
index 1d21e2eff6..74bc48bf55 100644
--- a/src/yb/master/xrepl_catalog_manager.cc
+++ b/src/yb/master/xrepl_catalog_manager.cc
@@ -856,14 +856,7 @@ Status CatalogManager::CreateNewXReplStream(
   xrepl::StreamId stream_id = xrepl::StreamId::Nil();
 
   // Kick-off the CDC state table creation before any other logic.
-  CreateTableResponsePB table_resp;
-  RETURN_NOT_OK(CreateTableIfNotFound(
-      cdc::CDCStateTable::GetNamespaceName(), cdc::CDCStateTable::GetTableName(),
-      &cdc::CDCStateTable::GenerateCreateCdcStateTableRequest, &table_resp, /* rpc */ nullptr,
-      epoch));
-  // Mark the cluster as CDC enabled now that we have trigged the CDC state table creation.
-  SetCDCServiceEnabled();
-  TRACE("Created CDC state table");
+  RETURN_NOT_OK(CreateCdcStateTableIfNotFound(epoch));
 
   // TODO(#18934): Move to the DDL transactional atomicity model.
   CDCSDKStreamCreationState cdcsdk_stream_creation_state = CDCSDKStreamCreationState::kInitialized;
@@ -7429,5 +7422,39 @@ void CatalogManager::MarkUniverseForCleanup(
   universes_to_clear_.push_back(replication_group_id);
 }
 
+Status CatalogManager::CreateCdcStateTableIfNotFound(const LeaderEpoch& epoch) {
+  RETURN_NOT_OK(CreateTableIfNotFound(
+      cdc::CDCStateTable::GetNamespaceName(), cdc::CDCStateTable::GetTableName(),
+      &cdc::CDCStateTable::GenerateCreateCdcStateTableRequest, epoch));
+
+  TRACE("Created CDC state table");
+
+  // Mark the cluster as CDC enabled now that we have triggered the CDC state table creation.
+  SetCDCServiceEnabled();
+
+  return Status::OK();
+}
+
+Result<scoped_refptr<CDCStreamInfo>> CatalogManager::InitNewXReplStream() {
+  LockGuard lock(mutex_);
+  TRACE("Acquired catalog manager lock");
+
+  auto stream_id =
+      VERIFY_RESULT(xrepl::StreamId::FromString(GenerateIdUnlocked(SysRowEntryType::CDC_STREAM)));
+  auto stream = make_scoped_refptr<CDCStreamInfo>(stream_id);
+  stream->mutable_metadata()->StartMutation();
+
+  cdc_stream_map_[stream_id] = stream;
+
+  return stream;
+}
+
+void CatalogManager::ReleaseAbandonedXReplStream(const xrepl::StreamId& stream_id) {
+  LockGuard lock(mutex_);
+  TRACE("Acquired catalog manager lock");
+
+  cdc_stream_map_.erase(stream_id);
+}
+
 }  // namespace master
 }  // namespace yb
diff --git a/src/yb/server/monitored_task.h b/src/yb/server/monitored_task.h
index 2afcda1e97..6a3557d690 100644
--- a/src/yb/server/monitored_task.h
+++ b/src/yb/server/monitored_task.h
@@ -85,7 +85,8 @@ YB_DEFINE_ENUM(MonitoredTaskType,
   (kUpdateTransactionTablesVersion)
   (kAddTableToXClusterTarget)
   (kMarkTableAsRunning)
-  (kAddTableToXClusterSource));
+  (kAddTableToXClusterSource)
+  (kAddNamespaceToXClusterSource));
 
 class MonitoredTask : public std::enable_shared_from_this<MonitoredTask> {
  public:
