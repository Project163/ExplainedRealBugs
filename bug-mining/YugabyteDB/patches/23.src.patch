diff --git a/src/yb/cdc/CMakeLists.txt b/src/yb/cdc/CMakeLists.txt
index 2cfcd105ce..b507a39a43 100644
--- a/src/yb/cdc/CMakeLists.txt
+++ b/src/yb/cdc/CMakeLists.txt
@@ -88,9 +88,15 @@ ADD_YB_LIBRARY(xcluster_producer_proto
   DEPS ${XCLUSTER_PRODUCER_YRPC_LIBS}
   NONLINK_DEPS ${XCLUSTER_PRODUCER_YRPC_TGTS})
 
+set(CDC_UTIL_SRCS
+  cdc_util.cc
+  xcluster_types.cc
+  xcluster_util.cc
+)
+
 ADD_YB_LIBRARY(
     cdc_util
-    SRCS cdc_util.cc
+    SRCS ${CDC_UTIL_SRCS}
     DEPS yb_client ql_util)
 
 #########################################
diff --git a/src/yb/cdc/cdc_producer.h b/src/yb/cdc/cdc_producer.h
index 8cf12de3e9..9b545fd695 100644
--- a/src/yb/cdc/cdc_producer.h
+++ b/src/yb/cdc/cdc_producer.h
@@ -41,12 +41,6 @@ class MemTracker;
 
 namespace cdc {
 
-using EnumOidLabelMap = std::unordered_map<uint32_t, std::string>;
-using EnumLabelCache = std::unordered_map<NamespaceName, EnumOidLabelMap>;
-
-using CompositeAttsMap = std::unordered_map<uint32_t, std::vector<master::PgAttributePB>>;
-using CompositeTypeCache = std::unordered_map<NamespaceName, CompositeAttsMap>;
-
 struct SchemaDetails {
   SchemaVersion schema_version;
   std::shared_ptr<Schema> schema;
diff --git a/src/yb/cdc/cdc_service.cc b/src/yb/cdc/cdc_service.cc
index 420add7929..4607b5c7f4 100644
--- a/src/yb/cdc/cdc_service.cc
+++ b/src/yb/cdc/cdc_service.cc
@@ -23,6 +23,7 @@
 #include <boost/multi_index/member.hpp>
 #include <boost/multi_index_container.hpp>
 
+#include "yb/cdc/cdc_error.h"
 #include "yb/cdc/cdc_producer.h"
 #include "yb/cdc/cdc_util.h"
 #include "yb/cdc/xcluster_rpc.h"
@@ -223,7 +224,7 @@ namespace {
 // Map of checkpoints that have been sent to CDC consumer and stored in cdc_state.
 struct TabletCheckpointInfo {
  public:
-  ProducerTabletInfo producer_tablet_info;
+  TabletStreamInfo producer_tablet_info;
 
   mutable TabletCheckpoint cdc_state_checkpoint;
   mutable TabletCheckpoint sent_checkpoint;
@@ -235,7 +236,7 @@ struct TabletCheckpointInfo {
 };
 
 struct CDCStateMetadataInfo {
-  ProducerTabletInfo producer_tablet_info;
+  TabletStreamInfo producer_tablet_info;
 
   mutable uint64_t commit_timestamp;
   mutable OpId last_streamed_op_id;
@@ -255,7 +256,7 @@ using TabletCheckpoints = boost::multi_index_container<
     TabletCheckpointInfo,
     boost::multi_index::indexed_by<
         boost::multi_index::hashed_unique<boost::multi_index::member<
-            TabletCheckpointInfo, ProducerTabletInfo, &TabletCheckpointInfo::producer_tablet_info>>,
+            TabletCheckpointInfo, TabletStreamInfo, &TabletCheckpointInfo::producer_tablet_info>>,
         boost::multi_index::hashed_non_unique<
             boost::multi_index::tag<TabletTag>,
             boost::multi_index::const_mem_fun<
@@ -269,7 +270,7 @@ using CDCStateMetadata = boost::multi_index_container<
     CDCStateMetadataInfo,
     boost::multi_index::indexed_by<
         boost::multi_index::hashed_unique<boost::multi_index::member<
-            CDCStateMetadataInfo, ProducerTabletInfo, &CDCStateMetadataInfo::producer_tablet_info>>,
+            CDCStateMetadataInfo, TabletStreamInfo, &CDCStateMetadataInfo::producer_tablet_info>>,
         boost::multi_index::hashed_non_unique<
             boost::multi_index::tag<TabletTag>,
             boost::multi_index::const_mem_fun<
@@ -359,10 +360,8 @@ class CDCServiceImpl::Impl {
   }
 
   void UpdateCDCStateMetadata(
-      const ProducerTabletInfo& producer_tablet,
-      const uint64_t& timestamp,
-      const SchemaDetailsMap& schema_details,
-      const OpId& op_id) {
+      const TabletStreamInfo& producer_tablet, const uint64_t& timestamp,
+      const SchemaDetailsMap& schema_details, const OpId& op_id) {
     std::lock_guard l(mutex_);
     auto it = cdc_state_metadata_.find(producer_tablet);
     if (it == cdc_state_metadata_.end()) {
@@ -376,7 +375,7 @@ class CDCServiceImpl::Impl {
   }
 
   SchemaDetailsMap& GetOrAddSchema(
-      const ProducerTabletInfo& producer_tablet, const bool need_schema_info) {
+      const TabletStreamInfo& producer_tablet, const bool need_schema_info) {
     std::lock_guard l(mutex_);
     auto it = cdc_state_metadata_.find(producer_tablet);
 
@@ -397,7 +396,7 @@ class CDCServiceImpl::Impl {
     return it->schema_details_map;
   }
 
-  boost::optional<OpId> GetLastStreamedOpId(const ProducerTabletInfo& producer_tablet) {
+  boost::optional<OpId> GetLastStreamedOpId(const TabletStreamInfo& producer_tablet) {
     SharedLock<rw_spinlock> lock(mutex_);
     auto it = cdc_state_metadata_.find(producer_tablet);
     if (it != cdc_state_metadata_.end()) {
@@ -408,8 +407,7 @@ class CDCServiceImpl::Impl {
 
   void AddTabletCheckpoint(
       OpId op_id, const xrepl::StreamId& stream_id, const TabletId& tablet_id) {
-    ProducerTabletInfo producer_tablet{
-        .replication_group_id = {}, .stream_id = stream_id, .tablet_id = tablet_id};
+    TabletStreamInfo producer_tablet{.stream_id = stream_id, .tablet_id = tablet_id};
     CoarseTimePoint time = CoarseMonoClock::Now();
     int64_t active_time = GetCurrentTimeMicros();
 
@@ -424,7 +422,7 @@ class CDCServiceImpl::Impl {
   }
 
   void EraseStreams(
-      const std::vector<ProducerTabletInfo>& producer_entries_modified,
+      const std::vector<TabletStreamInfo>& producer_entries_modified,
       bool erase_cdc_states) NO_THREAD_SAFETY_ANALYSIS {
     for (const auto& entry : producer_entries_modified) {
       tablet_checkpoints_.get<StreamTag>().erase(entry.stream_id);
@@ -434,7 +432,7 @@ class CDCServiceImpl::Impl {
     }
   }
 
-  boost::optional<int64_t> GetLastActiveTime(const ProducerTabletInfo& producer_tablet) {
+  boost::optional<int64_t> GetLastActiveTime(const TabletStreamInfo& producer_tablet) {
     SharedLock<rw_spinlock> lock(mutex_);
     auto it = tablet_checkpoints_.find(producer_tablet);
     if (it != tablet_checkpoints_.end()) {
@@ -463,7 +461,7 @@ class CDCServiceImpl::Impl {
     return boost::none;
   }
 
-  Status EraseTabletAndStreamEntry(const ProducerTabletInfo& info) {
+  Status EraseTabletAndStreamEntry(const TabletStreamInfo& info) {
     std::lock_guard l(mutex_);
     // Here we just remove the entries of the tablet from the in-memory caches. The deletion from
     // the 'cdc_state' table will happen when the hidden parent tablet will be deleted
@@ -474,7 +472,7 @@ class CDCServiceImpl::Impl {
     return Status::OK();
   }
 
-  boost::optional<OpId> GetLastCheckpoint(const ProducerTabletInfo& producer_tablet) {
+  boost::optional<OpId> GetLastCheckpoint(const TabletStreamInfo& producer_tablet) {
     SharedLock<rw_spinlock> lock(mutex_);
     auto it = tablet_checkpoints_.find(producer_tablet);
     if (it != tablet_checkpoints_.end()) {
@@ -489,7 +487,7 @@ class CDCServiceImpl::Impl {
   }
 
   bool UpdateCheckpoint(
-      const ProducerTabletInfo& producer_tablet, const OpId& sent_op_id, const OpId& commit_op_id) {
+      const TabletStreamInfo& producer_tablet, const OpId& sent_op_id, const OpId& commit_op_id) {
     VLOG(1) << "T " << producer_tablet.tablet_id << " going to update the checkpoint with "
             << commit_op_id;
     auto now = CoarseMonoClock::Now();
@@ -559,7 +557,7 @@ class CDCServiceImpl::Impl {
 
   MemTrackerPtr GetMemTracker(
       const std::shared_ptr<tablet::TabletPeer>& tablet_peer,
-      const ProducerTabletInfo& producer_info) {
+      const TabletStreamInfo& producer_info) {
     {
       SharedLock<rw_spinlock> l(mutex_);
       auto it = tablet_checkpoints_.find(producer_info);
@@ -589,7 +587,7 @@ class CDCServiceImpl::Impl {
     return it->mem_tracker;
   }
 
-  Result<bool> PreCheckTabletValidForStream(const ProducerTabletInfo& info) {
+  Result<bool> PreCheckTabletValidForStream(const TabletStreamInfo& info) {
     SharedLock<rw_spinlock> l(mutex_);
     if (tablet_checkpoints_.count(info) != 0) {
       return true;
@@ -604,14 +602,12 @@ class CDCServiceImpl::Impl {
   }
 
   Status AddEntriesForChildrenTabletsOnSplitOp(
-      const ProducerTabletInfo& info,
-      const std::array<TabletId, 2>& tablets,
+      const TabletStreamInfo& info, const std::array<TabletId, 2>& tablets,
       const OpId& children_op_id) {
     std::lock_guard l(mutex_);
 
     for (const auto& tablet : tablets) {
-      ProducerTabletInfo producer_info{
-          info.replication_group_id, info.stream_id, tablet};
+      TabletStreamInfo producer_info{info.stream_id, tablet};
       tablet_checkpoints_.emplace(TabletCheckpointInfo{
           .producer_tablet_info = producer_info,
           .cdc_state_checkpoint =
@@ -635,15 +631,14 @@ class CDCServiceImpl::Impl {
   }
 
   Status CheckTabletValidForStream(
-      const ProducerTabletInfo& info,
+      const TabletStreamInfo& info,
       const google::protobuf::RepeatedPtrField<master::TabletLocationsPB>& tablets) {
     bool found = false;
     {
       std::lock_guard l(mutex_);
       for (const auto& tablet : tablets) {
         // Add every tablet in the stream.
-        ProducerTabletInfo producer_info{
-            info.replication_group_id, info.stream_id, tablet.tablet_id()};
+        TabletStreamInfo producer_info{info.stream_id, tablet.tablet_id()};
         tablet_checkpoints_.emplace(TabletCheckpointInfo{
             .producer_tablet_info = producer_info,
             .cdc_state_checkpoint =
@@ -696,7 +691,7 @@ class CDCServiceImpl::Impl {
     return tablet_checkpoints_;
   }
 
-  Result<TabletCheckpoint> TEST_GetTabletInfoFromCache(const ProducerTabletInfo& producer_tablet) {
+  Result<TabletCheckpoint> TEST_GetTabletInfoFromCache(const TabletStreamInfo& producer_tablet) {
     SharedLock<rw_spinlock> l(mutex_);
     auto it = tablet_checkpoints_.find(producer_tablet);
     if (it != tablet_checkpoints_.end()) {
@@ -706,7 +701,7 @@ class CDCServiceImpl::Impl {
         InternalError, "Tablet info: $0 not found in cache.", producer_tablet.ToString());
   }
 
-  void UpdateActiveTime(const ProducerTabletInfo& producer_tablet) {
+  void UpdateActiveTime(const TabletStreamInfo& producer_tablet) {
     SharedLock<rw_spinlock> l(mutex_);
     auto it = tablet_checkpoints_.find(producer_tablet);
     if (it != tablet_checkpoints_.end()) {
@@ -718,7 +713,7 @@ class CDCServiceImpl::Impl {
     }
   }
 
-  void ForceCdcStateUpdate(const ProducerTabletInfo& producer_tablet) {
+  void ForceCdcStateUpdate(const TabletStreamInfo& producer_tablet) {
     std::lock_guard l(mutex_);
     auto it = tablet_checkpoints_.find(producer_tablet);
     if (it != tablet_checkpoints_.end()) {
@@ -938,9 +933,8 @@ bool CDCServiceImpl::CheckOnline(const ReqType* req, RespType* resp, rpc::RpcCon
 }
 
 void CDCServiceImpl::InitNewTabletStreamEntry(
-    const xrepl::StreamId& stream_id,
-    const TabletId& tablet_id,
-    std::vector<ProducerTabletInfo>* producer_entries_modified,
+    const xrepl::StreamId& stream_id, const TabletId& tablet_id,
+    std::vector<TabletStreamInfo>* producer_entries_modified,
     std::vector<CDCStateTableEntry>* entries_to_insert) {
   // For CDCSDK the initial checkpoint for each tablet will be maintained
   // in cdc_state table as -1.-1(Invalid), which is the default value of 'op_id'. Checkpoint will be
@@ -952,8 +946,7 @@ void CDCServiceImpl::InitNewTabletStreamEntry(
   entry.cdc_sdk_safe_time = 0;
   entries_to_insert->push_back(std::move(entry));
 
-  producer_entries_modified->push_back(
-      {.replication_group_id = {}, .stream_id = stream_id, .tablet_id = tablet_id});
+  producer_entries_modified->push_back({.stream_id = stream_id, .tablet_id = tablet_id});
 
   impl_->AddTabletCheckpoint(op_id, stream_id, tablet_id);
 }
@@ -1183,8 +1176,7 @@ Result<SetCDCCheckpointResponsePB> CDCServiceImpl::SetCDCCheckpoint(
       CheckCanServeTabletData(*tablet_peer->tablet_metadata()),
       CDCError(CDCErrorPB::LEADER_NOT_READY));
 
-  ProducerTabletInfo producer_tablet{
-      {}, VERIFY_STRING_TO_STREAM_ID(req.stream_id()), req.tablet_id()};
+  TabletStreamInfo producer_tablet{VERIFY_STRING_TO_STREAM_ID(req.stream_id()), req.tablet_id()};
   RETURN_NOT_OK_SET_CODE(
       CheckTabletValidForStream(producer_tablet), CDCError(CDCErrorPB::INVALID_REQUEST));
 
@@ -1350,7 +1342,7 @@ void CDCServiceImpl::GetTabletListToPollForCDC(
     }
 
     for (const auto& child_tablet_id : child_tablet_ids) {
-      ProducerTabletInfo cur_child_tablet = {{}, req_stream_id, child_tablet_id};
+      TabletStreamInfo cur_child_tablet = {req_stream_id, child_tablet_id};
 
       auto tablet_checkpoint_pair_pb = resp->add_tablet_checkpoint_pairs();
       tablet_checkpoint_pair_pb->mutable_tablet_locations()->CopyFrom(
@@ -1490,7 +1482,7 @@ Result<google::protobuf::RepeatedPtrField<master::TabletLocationsPB>> CDCService
 }
 
 Result<TabletCheckpoint> CDCServiceImpl::TEST_GetTabletInfoFromCache(
-    const ProducerTabletInfo& producer_tablet) {
+    const TabletStreamInfo& producer_tablet) {
   return impl_->TEST_GetTabletInfoFromCache(producer_tablet);
 }
 
@@ -1532,7 +1524,7 @@ void CDCServiceImpl::GetChanges(
   CoarseTimePoint deadline = GetDeadline(context, client());
 
   // Check that requested tablet_id is part of the CDC stream.
-  ProducerTabletInfo producer_tablet = {{}, stream_id, req->tablet_id()};
+  TabletStreamInfo producer_tablet = {stream_id, req->tablet_id()};
 
   auto status = CheckTabletValidForStream(producer_tablet);
   if (!status.ok()) {
@@ -2037,7 +2029,7 @@ void CDCServiceImpl::ProcessMetricsForEmptyChildrenTablets(
 
       // Need to work our way up the hierarchy until we find a tablet with a valid value.
       auto parent_tablet = child_tablet_meta.parent_tablet_info;
-      std::unordered_set<ProducerTabletInfo, ProducerTabletInfo::Hash> tablet_hierarchy;
+      std::unordered_set<TabletStreamInfo, TabletStreamInfo::Hash> tablet_hierarchy;
       tablet_hierarchy.insert(child_tablet);
 
       while (!parent_tablet.tablet_id.empty()) {
@@ -2111,7 +2103,7 @@ void CDCServiceImpl::UpdateMetrics() {
 
     // Keep track of all tablets in cdc_state and their last_replication time. This is done to help
     // fill any empty split tablets later, as well as determine what metrics can be cleaned up.
-    ProducerTabletInfo tablet_info = {{}, entry.key.stream_id, entry.key.tablet_id};
+    TabletStreamInfo tablet_info = {entry.key.stream_id, entry.key.tablet_id};
     cdc_state_tablets_to_last_replication_time.emplace(tablet_info, entry.last_replication_time);
 
     auto tablet_peer = context_->LookupTablet(entry.key.tablet_id);
@@ -2198,7 +2190,7 @@ void CDCServiceImpl::UpdateMetrics() {
               // This is a child tablet which has not yet been polled yet, so it still has an empty
               // last_replication_time in cdc_state. In order to update metrics, we will use its
               // parent's last_replication_time.
-              ProducerTabletInfo parent_info = {{}, entry.key.stream_id, parent_tablet_id};
+              TabletStreamInfo parent_info = {entry.key.stream_id, parent_tablet_id};
               empty_children_tablets.insert(
                   {tablet_info, {parent_info, last_replicated_micros, tablet_metric}});
               continue;
@@ -2238,7 +2230,7 @@ void CDCServiceImpl::UpdateMetrics() {
   // Now, go through tablets in tablet_checkpoints_ and set lag to 0 for all tablets we're no
   // longer replicating.
   for (const auto& checkpoint : tablet_checkpoints) {
-    const ProducerTabletInfo& tablet_info = checkpoint.producer_tablet_info;
+    const TabletStreamInfo& tablet_info = checkpoint.producer_tablet_info;
     if (!cdc_state_tablets_to_last_replication_time.contains(tablet_info)) {
       // We're no longer replicating this tablet, so set lag to 0.
       auto tablet_peer = context_->LookupTablet(checkpoint.tablet_id());
@@ -2499,7 +2491,7 @@ Result<TabletIdCDCCheckpointMap> CDCServiceImpl::PopulateTabletCheckPointInfo(
     }
 
     // Check that requested tablet_id is part of the CDC stream.
-    ProducerTabletInfo producer_tablet = {{}, stream_id, tablet_id};
+    TabletStreamInfo producer_tablet = {stream_id, tablet_id};
 
     // Check stream associated with the tablet is active or not.
     // Don't consider those inactive stream for the min_checkpoint calculation.
@@ -3086,7 +3078,7 @@ void CDCServiceImpl::GetCheckpoint(
 
   auto req_stream_id = RPC_VERIFY_STRING_TO_STREAM_ID(req->stream_id());
   // Check that requested tablet_id is part of the CDC stream.
-  ProducerTabletInfo producer_tablet = {{}, req_stream_id, req->tablet_id()};
+  TabletStreamInfo producer_tablet = {req_stream_id, req->tablet_id()};
   auto s = CheckTabletValidForStream(producer_tablet);
   RPC_STATUS_RETURN_ERROR(s, resp->mutable_error(), CDCErrorPB::INVALID_REQUEST, context);
 
@@ -3445,7 +3437,7 @@ void CDCServiceImpl::Shutdown() {
 }
 
 Status CDCServiceImpl::CheckStreamActive(
-    const ProducerTabletInfo& producer_tablet, const int64_t& last_active_time_passed) {
+    const TabletStreamInfo& producer_tablet, const int64_t& last_active_time_passed) {
   auto last_active_time = (last_active_time_passed == 0)
                               ? VERIFY_RESULT(GetLastActiveTime(producer_tablet))
                               : last_active_time_passed;
@@ -3475,9 +3467,7 @@ Status CDCServiceImpl::CheckStreamActive(
 }
 
 Status CDCServiceImpl::CheckTabletNotOfInterest(
-    const ProducerTabletInfo& producer_tablet, int64_t last_active_time_passed,
-    bool deletion_check) {
-
+    const TabletStreamInfo& producer_tablet, int64_t last_active_time_passed, bool deletion_check) {
   // This is applicable only to Consistent Snapshot Streams,
   auto record = VERIFY_RESULT(GetStream(producer_tablet.stream_id));
   if (!record->GetSnapshotOption().has_value() || !record->GetStreamCreationTime().has_value()) {
@@ -3528,7 +3518,7 @@ Status CDCServiceImpl::CheckTabletNotOfInterest(
 }
 
 Result<int64_t> CDCServiceImpl::GetLastActiveTime(
-    const ProducerTabletInfo& producer_tablet, bool ignore_cache) {
+    const TabletStreamInfo& producer_tablet, bool ignore_cache) {
   DCHECK(producer_tablet.stream_id && !producer_tablet.tablet_id.empty());
 
   if (!ignore_cache) {
@@ -3615,7 +3605,7 @@ Result<CDCSDKCheckpointPB> CDCServiceImpl::GetLastCheckpointFromCdcState(
 }
 
 Result<OpId> CDCServiceImpl::GetLastCheckpoint(
-    const ProducerTabletInfo& producer_tablet, const CDCRequestSource& request_source,
+    const TabletStreamInfo& producer_tablet, const CDCRequestSource& request_source,
     const bool ignore_unpolled_tablets) {
   if (!PREDICT_FALSE(FLAGS_TEST_force_get_checkpoint_from_cdc_state)) {
     auto result = impl_->GetLastCheckpoint(producer_tablet);
@@ -3639,7 +3629,7 @@ Result<uint64_t> CDCServiceImpl::GetSafeTime(
 }
 
 void CDCServiceImpl::UpdateTabletMetrics(
-    const GetChangesResponsePB& resp, const ProducerTabletInfo& producer_tablet,
+    const GetChangesResponsePB& resp, const TabletStreamInfo& producer_tablet,
     const std::shared_ptr<tablet::TabletPeer>& tablet_peer, const OpId& op_id,
     const CDCRequestSource source_type, int64_t last_readable_index) {
   if (source_type == XCLUSTER) {
@@ -3650,7 +3640,7 @@ void CDCServiceImpl::UpdateTabletMetrics(
 }
 
 void CDCServiceImpl::UpdateTabletXClusterMetrics(
-    const GetChangesResponsePB& resp, const ProducerTabletInfo& producer_tablet,
+    const GetChangesResponsePB& resp, const TabletStreamInfo& producer_tablet,
     const std::shared_ptr<tablet::TabletPeer>& tablet_peer, const OpId& op_id,
     int64_t last_readable_index) {
   auto tablet_metric_result =
@@ -3705,7 +3695,7 @@ void CDCServiceImpl::UpdateTabletXClusterMetrics(
 }
 
 void CDCServiceImpl::UpdateTabletCDCSDKMetrics(
-    const GetChangesResponsePB& resp, const ProducerTabletInfo& producer_tablet,
+    const GetChangesResponsePB& resp, const TabletStreamInfo& producer_tablet,
     const std::shared_ptr<tablet::TabletPeer>& tablet_peer) {
   auto tablet_metric_result = GetCDCSDKTabletMetrics(*tablet_peer.get(), producer_tablet.stream_id);
   if (!tablet_metric_result) {
@@ -3756,10 +3746,8 @@ bool CDCServiceImpl::IsCDCSDKSnapshotBootstrapRequest(const CDCSDKCheckpointPB&
 }
 
 Status CDCServiceImpl::InsertRowForColocatedTableInCDCStateTable(
-    const ProducerTabletInfo& producer_tablet,
-    const TableId& colocated_table_id,
-    const OpId& commit_op_id,
-    const HybridTime& cdc_sdk_safe_time) {
+    const TabletStreamInfo& producer_tablet, const TableId& colocated_table_id,
+    const OpId& commit_op_id, const HybridTime& cdc_sdk_safe_time) {
   DCHECK(
       producer_tablet.stream_id && !producer_tablet.tablet_id.empty() &&
       !colocated_table_id.empty());
@@ -3778,16 +3766,10 @@ Status CDCServiceImpl::InsertRowForColocatedTableInCDCStateTable(
 }
 
 Status CDCServiceImpl::UpdateCheckpointAndActiveTime(
-    const ProducerTabletInfo& producer_tablet,
-    const OpId& sent_op_id,
-    const OpId& commit_op_id,
-    uint64_t last_record_hybrid_time,
-    const CDCRequestSource& request_source,
-    const bool snapshot_bootstrap,
-    const HybridTime& cdc_sdk_safe_time,
-    const bool is_snapshot,
-    const std::string& snapshot_key,
-    const TableId& colocated_table_id) {
+    const TabletStreamInfo& producer_tablet, const OpId& sent_op_id, const OpId& commit_op_id,
+    uint64_t last_record_hybrid_time, const CDCRequestSource& request_source,
+    const bool snapshot_bootstrap, const HybridTime& cdc_sdk_safe_time, const bool is_snapshot,
+    const std::string& snapshot_key, const TableId& colocated_table_id) {
   bool update_cdc_state = impl_->UpdateCheckpoint(producer_tablet, sent_op_id, commit_op_id);
   if (!update_cdc_state && !snapshot_bootstrap) {
     return Status::OK();
@@ -3964,7 +3946,7 @@ void CDCServiceImpl::AddStreamMetadataToCache(
   InsertOrUpdate(&stream_metadata_, stream_id, stream_metadata);
 }
 
-Status CDCServiceImpl::CheckTabletValidForStream(const ProducerTabletInfo& info) {
+Status CDCServiceImpl::CheckTabletValidForStream(const TabletStreamInfo& info) {
   auto result = VERIFY_RESULT(impl_->PreCheckTabletValidForStream(info));
   if (result) {
     return Status::OK();
@@ -4012,7 +3994,7 @@ void CDCServiceImpl::IsBootstrapRequired(
     if (req->has_stream_id() && !req->stream_id().empty()) {
       auto stream_id = RPC_VERIFY_STRING_TO_STREAM_ID(req->stream_id());
       // Check that requested tablet_id is part of the CDC stream.
-      ProducerTabletInfo producer_tablet = {{}, stream_id, tablet_id};
+      TabletStreamInfo producer_tablet = {stream_id, tablet_id};
       RPC_STATUS_RETURN_ERROR(
           CheckTabletValidForStream(producer_tablet), resp->mutable_error(),
           CDCErrorPB::INVALID_REQUEST, context);
@@ -4049,7 +4031,7 @@ void CDCServiceImpl::IsBootstrapRequired(
   context.RespondSuccess();
 }
 
-Status CDCServiceImpl::UpdateChildrenTabletsOnSplitOpForCDCSDK(const ProducerTabletInfo& info) {
+Status CDCServiceImpl::UpdateChildrenTabletsOnSplitOpForCDCSDK(const TabletStreamInfo& info) {
   auto tablets = VERIFY_RESULT(GetTablets(info.stream_id, true /* ignore_errors */));
 
   // Initializing the children to 0.0 to prevent garbage collection on them.
@@ -4083,7 +4065,7 @@ Status CDCServiceImpl::UpdateChildrenTabletsOnSplitOpForCDCSDK(const ProducerTab
 }
 
 Status CDCServiceImpl::UpdateChildrenTabletsOnSplitOpForXCluster(
-    const ProducerTabletInfo& producer_tablet, const consensus::ReplicateMsg& split_op_msg) {
+    const TabletStreamInfo& producer_tablet, const consensus::ReplicateMsg& split_op_msg) {
   const auto& split_req = split_op_msg.split_request();
   const auto parent_tablet = split_req.tablet_id();
   const vector<string> children_tablets = {split_req.new_tablet1_id(), split_req.new_tablet2_id()};
@@ -4165,7 +4147,7 @@ void CDCServiceImpl::CheckReplicationDrain(
         continue;
       }
 
-      ProducerTabletInfo producer_tablet = {{}, stream_id, tablet_id};
+      TabletStreamInfo producer_tablet = {stream_id, tablet_id};
       auto s = CheckTabletValidForStream(producer_tablet);
       if (!s.ok()) {
         LOG_WITH_FUNC(WARNING) << "Tablet not valid for stream: " << s << ". Skipping.";
diff --git a/src/yb/cdc/cdc_service.h b/src/yb/cdc/cdc_service.h
index 708d30063e..7cfbb1232d 100644
--- a/src/yb/cdc/cdc_service.h
+++ b/src/yb/cdc/cdc_service.h
@@ -14,8 +14,7 @@
 
 #include <memory>
 
-#include "yb/cdc/cdc_fwd.h"
-#include "yb/cdc/cdc_error.h"
+#include "yb/cdc/xrepl_types.h"
 #include "yb/cdc/xrepl_metrics.h"
 #include "yb/cdc/cdc_producer.h"
 #include "yb/cdc/cdc_service.proxy.h"
@@ -49,6 +48,12 @@ namespace yb {
 
 class Thread;
 
+namespace cdc {
+
+class CDCServiceContext;
+
+}  // namespace cdc
+
 namespace client {
 
 class TableHandle;
@@ -135,7 +140,7 @@ class CDCServiceImpl : public CDCServiceIf {
       GetCheckpointResponsePB* resp,
       rpc::RpcContext rpc) override;
 
-  Result<TabletCheckpoint> TEST_GetTabletInfoFromCache(const ProducerTabletInfo& producer_tablet);
+  Result<TabletCheckpoint> TEST_GetTabletInfoFromCache(const TabletStreamInfo& producer_tablet);
 
   // Update peers in other tablet servers about the latest minimum applied cdc index for a specific
   // tablet.
@@ -225,17 +230,17 @@ class CDCServiceImpl : public CDCServiceIf {
   Result<tablet::TabletPeerPtr> GetServingTablet(const TabletId& tablet_id) const;
 
   void UpdateTabletMetrics(
-      const GetChangesResponsePB& resp, const ProducerTabletInfo& producer_tablet,
+      const GetChangesResponsePB& resp, const TabletStreamInfo& producer_tablet,
       const std::shared_ptr<tablet::TabletPeer>& tablet_peer, const OpId& op_id,
       const CDCRequestSource source_type, int64_t last_readable_index);
 
   void UpdateTabletXClusterMetrics(
-      const GetChangesResponsePB& resp, const ProducerTabletInfo& producer_tablet,
+      const GetChangesResponsePB& resp, const TabletStreamInfo& producer_tablet,
       const std::shared_ptr<tablet::TabletPeer>& tablet_peer, const OpId& op_id,
       int64_t last_readable_index);
 
   void UpdateTabletCDCSDKMetrics(
-      const GetChangesResponsePB& resp, const ProducerTabletInfo& producer_tablet,
+      const GetChangesResponsePB& resp, const TabletStreamInfo& producer_tablet,
       const std::shared_ptr<tablet::TabletPeer>& tablet_peer);
 
  private:
@@ -250,17 +255,17 @@ class CDCServiceImpl : public CDCServiceIf {
   bool CheckOnline(const ReqType* req, RespType* resp, rpc::RpcContext* rpc);
 
   Status CheckStreamActive(
-      const ProducerTabletInfo& producer_tablet, const int64_t& last_active_time_passed = 0);
+      const TabletStreamInfo& producer_tablet, const int64_t& last_active_time_passed = 0);
 
   Status CheckTabletNotOfInterest(
-      const ProducerTabletInfo& producer_tablet, int64_t last_active_time_passed = 0,
+      const TabletStreamInfo& producer_tablet, int64_t last_active_time_passed = 0,
       bool deletion_check = false);
 
   Result<int64_t> GetLastActiveTime(
-      const ProducerTabletInfo& producer_tablet, bool ignore_cache = false);
+      const TabletStreamInfo& producer_tablet, bool ignore_cache = false);
 
   Result<OpId> GetLastCheckpoint(
-      const ProducerTabletInfo& producer_tablet, const CDCRequestSource& request_source,
+      const TabletStreamInfo& producer_tablet, const CDCRequestSource& request_source,
       const bool ignore_unpolled_tablets = true);
 
   Result<uint64_t> GetSafeTime(const xrepl::StreamId& stream_id, const TabletId& tablet_id);
@@ -271,22 +276,15 @@ class CDCServiceImpl : public CDCServiceIf {
       const bool ignore_unpolled_tablets = true);
 
   Status InsertRowForColocatedTableInCDCStateTable(
-      const ProducerTabletInfo& producer_tablet,
-      const TableId& colocated_table_id,
-      const OpId& commit_op_id,
-      const HybridTime& cdc_sdk_safe_time);
+      const TabletStreamInfo& producer_tablet, const TableId& colocated_table_id,
+      const OpId& commit_op_id, const HybridTime& cdc_sdk_safe_time);
 
   Status UpdateCheckpointAndActiveTime(
-      const ProducerTabletInfo& producer_tablet,
-      const OpId& sent_op_id,
-      const OpId& commit_op_id,
+      const TabletStreamInfo& producer_tablet, const OpId& sent_op_id, const OpId& commit_op_id,
       uint64_t last_record_hybrid_time,
-      const CDCRequestSource& request_source = CDCRequestSource::CDCSDK,
-      bool force_update = false,
-      const HybridTime& cdc_sdk_safe_time = HybridTime::kInvalid,
-      const bool is_snapshot = false,
-      const std::string& snapshot_key = "",
-      const TableId& colocated_table_id = "");
+      const CDCRequestSource& request_source = CDCRequestSource::CDCSDK, bool force_update = false,
+      const HybridTime& cdc_sdk_safe_time = HybridTime::kInvalid, const bool is_snapshot = false,
+      const std::string& snapshot_key = "", const TableId& colocated_table_id = "");
 
   Status UpdateSnapshotDone(
       const xrepl::StreamId& stream_id, const TabletId& tablet_id,
@@ -315,7 +313,7 @@ class CDCServiceImpl : public CDCServiceIf {
       const xrepl::StreamId& stream_id, const std::shared_ptr<StreamMetadata>& stream_metadata)
       EXCLUDES(mutex_);
 
-  Status CheckTabletValidForStream(const ProducerTabletInfo& producer_info);
+  Status CheckTabletValidForStream(const TabletStreamInfo& producer_info);
 
   void TabletLeaderGetChanges(
       const GetChangesRequestPB* req,
@@ -351,7 +349,7 @@ class CDCServiceImpl : public CDCServiceIf {
 
   std::shared_ptr<MemTracker> GetMemTracker(
       const std::shared_ptr<tablet::TabletPeer>& tablet_peer,
-      const ProducerTabletInfo& producer_info);
+      const TabletStreamInfo& producer_info);
 
   OpId GetMinAppliedCheckpointForTablet(
       const std::string& tablet_id, const client::YBSessionPtr& session);
@@ -361,14 +359,14 @@ class CDCServiceImpl : public CDCServiceIf {
       bool ignore_failures = true, bool initial_retention_barrier = false);
 
   struct ChildrenTabletMeta {
-    ProducerTabletInfo parent_tablet_info;
+    TabletStreamInfo parent_tablet_info;
     uint64_t last_replication_time;
     std::shared_ptr<yb::xrepl::XClusterTabletMetrics> tablet_metric;
   };
   using EmptyChildrenTabletMap =
-      std::unordered_map<ProducerTabletInfo, ChildrenTabletMeta, ProducerTabletInfo::Hash>;
+      std::unordered_map<TabletStreamInfo, ChildrenTabletMeta, TabletStreamInfo::Hash>;
   using TabletInfoToLastReplicationTimeMap =
-      std::unordered_map<ProducerTabletInfo, std::optional<uint64_t>, ProducerTabletInfo::Hash>;
+      std::unordered_map<TabletStreamInfo, std::optional<uint64_t>, TabletStreamInfo::Hash>;
   // Helper function for processing metrics of children tablets. We need to find an ancestor tablet
   // that has a last replicated time and use that with our latest op's time to find the full lag.
   void ProcessMetricsForEmptyChildrenTablets(
@@ -403,9 +401,8 @@ class CDCServiceImpl : public CDCServiceIf {
 
   // Initialize a new CDCStateTableEntry and adds the tablet stream to tablet_checkpoints_.
   void InitNewTabletStreamEntry(
-      const xrepl::StreamId& stream_id,
-      const TabletId& tablet_id,
-      std::vector<ProducerTabletInfo>* producer_entries_modified,
+      const xrepl::StreamId& stream_id, const TabletId& tablet_id,
+      std::vector<TabletStreamInfo>* producer_entries_modified,
       std::vector<CDCStateTableEntry>* entries_to_insert);
 
   Status CreateCDCStreamForNamespace(
@@ -427,9 +424,9 @@ class CDCServiceImpl : public CDCServiceIf {
       HybridTime cdc_sdk_safe_time = HybridTime::kInvalid);
 
   Status UpdateChildrenTabletsOnSplitOpForXCluster(
-      const ProducerTabletInfo& producer_tablet, const consensus::ReplicateMsg& split_op_msg);
+      const TabletStreamInfo& producer_tablet, const consensus::ReplicateMsg& split_op_msg);
 
-  Status UpdateChildrenTabletsOnSplitOpForCDCSDK(const ProducerTabletInfo& info);
+  Status UpdateChildrenTabletsOnSplitOpForCDCSDK(const TabletStreamInfo& info);
 
   // Get enum map from the cache.
   Result<EnumOidLabelMap> GetEnumMapFromCache(const NamespaceName& ns_name, bool cql_namespace);
diff --git a/src/yb/cdc/cdc_types.h b/src/yb/cdc/cdc_types.h
index 0475dedd51..9a75ee159b 100644
--- a/src/yb/cdc/cdc_types.h
+++ b/src/yb/cdc/cdc_types.h
@@ -16,7 +16,7 @@
 #include <string>
 #include <unordered_map>
 
-#include "yb/cdc/cdc_fwd.h"
+#include "yb/cdc/xrepl_types.h"
 
 #include "yb/common/common_fwd.h"
 #include "yb/common/common_types.pb.h"
@@ -28,16 +28,21 @@
 
 namespace yb {
 
+namespace master {
+
+class PgAttributePB;
+
+}  // namespace master
+
 // Object types used to manage eXternal REPLication (XREPL) of data from a YugabyteDB.
 // xCluster replicates data to another YugabyteDB, and CDC replicates the data to external
 // databases or files.
 
 namespace cdc {
+
 static const char* const kIdType = "id_type";
 static const char* const kTableId = "TABLEID";
 
-YB_STRONGLY_TYPED_STRING(ReplicationGroupId);
-
 typedef std::unordered_map<SchemaVersion, SchemaVersion> XClusterSchemaVersionMap;
 typedef std::unordered_map<uint32_t, XClusterSchemaVersionMap> ColocatedSchemaVersionMap;
 typedef std::unordered_map<xrepl::StreamId, XClusterSchemaVersionMap> StreamSchemaVersionMap;
@@ -49,5 +54,12 @@ constexpr uint32_t kInvalidSchemaVersion = std::numeric_limits<uint32_t>::max();
 
 YB_STRONGLY_TYPED_BOOL(StreamModeTransactional);
 YB_DEFINE_ENUM(RefreshStreamMapOption, (kNone)(kAlways)(kIfInitiatedState));
+
+using EnumOidLabelMap = std::unordered_map<uint32_t, std::string>;
+using EnumLabelCache = std::unordered_map<NamespaceName, EnumOidLabelMap>;
+
+using CompositeAttsMap = std::unordered_map<uint32_t, std::vector<master::PgAttributePB>>;
+using CompositeTypeCache = std::unordered_map<NamespaceName, CompositeAttsMap>;
+
 }  // namespace cdc
 }  // namespace yb
diff --git a/src/yb/cdc/cdc_util.cc b/src/yb/cdc/cdc_util.cc
index 06e821a5a3..c6d7bf997b 100644
--- a/src/yb/cdc/cdc_util.cc
+++ b/src/yb/cdc/cdc_util.cc
@@ -20,31 +20,16 @@
 
 namespace yb::cdc {
 
-std::string ProducerTabletInfo::ToString() const {
-  return Format(
-      "{ replication_group_id: $0 stream_id: $1 tablet_id: $2 }", replication_group_id, stream_id,
-      tablet_id);
+std::string TabletStreamInfo::ToString() const {
+  return Format("{ stream_id: $1 tablet_id: $2 }", stream_id, tablet_id);
 }
 
-std::size_t ProducerTabletInfo::Hash::operator()(const ProducerTabletInfo& p) const noexcept {
+std::size_t TabletStreamInfo::Hash::operator()(const TabletStreamInfo& p) const noexcept {
   std::size_t hash = 0;
-  boost::hash_combine(hash, p.replication_group_id);
   boost::hash_combine(hash, p.stream_id);
   boost::hash_combine(hash, p.tablet_id);
 
   return hash;
 }
 
-bool IsAlterReplicationGroupId(const ReplicationGroupId& replication_group_id) {
-  return GStringPiece(replication_group_id.ToString()).ends_with(kAlterReplicationGroupSuffix);
-}
-
-ReplicationGroupId GetOriginalReplicationGroupId(const ReplicationGroupId& replication_group_id) {
-  // Remove the .ALTER suffix from universe_uuid if applicable.
-  GStringPiece clean_id(replication_group_id.ToString());
-  if (clean_id.ends_with(kAlterReplicationGroupSuffix)) {
-    clean_id.remove_suffix(sizeof(kAlterReplicationGroupSuffix) - 1 /* exclude \0 ending */);
-  }
-  return ReplicationGroupId(clean_id.ToString());
-}
 }  // namespace yb::cdc
diff --git a/src/yb/cdc/cdc_util.h b/src/yb/cdc/cdc_util.h
index 059d654b38..040b18e3ae 100644
--- a/src/yb/cdc/cdc_util.h
+++ b/src/yb/cdc/cdc_util.h
@@ -14,46 +14,31 @@
 #pragma once
 
 #include <string>
-#include "yb/cdc/cdc_types.h"
+#include "yb/cdc/xrepl_types.h"
 #include "yb/common/entity_ids_types.h"
 
 namespace yb::cdc {
-struct ConsumerTabletInfo {
-  std::string tablet_id;
-  TableId table_id;
-};
 
-struct ProducerTabletInfo {
-  // Needed on Consumer side for uniqueness. Empty on Producer.
-  ReplicationGroupId replication_group_id;
+struct TabletStreamInfo {
   // Unique ID on Producer, but not on Consumer.
   xrepl::StreamId stream_id;
   TabletId tablet_id;
 
-  bool operator==(const ProducerTabletInfo& other) const {
-    return replication_group_id == other.replication_group_id && stream_id == other.stream_id &&
-           tablet_id == other.tablet_id;
+  bool operator==(const TabletStreamInfo& other) const {
+    return stream_id == other.stream_id && tablet_id == other.tablet_id;
   }
 
   std::string ToString() const;
 
   struct Hash {
-    std::size_t operator()(const ProducerTabletInfo& p) const noexcept;
+    std::size_t operator()(const TabletStreamInfo& p) const noexcept;
   };
 };
 
-struct XClusterTabletInfo {
-  ProducerTabletInfo producer_tablet_info;
-  ConsumerTabletInfo consumer_tablet_info;
-  // Whether or not replication has been paused for this tablet.
-  bool disable_stream;
-
-  const std::string& producer_tablet_id() const { return producer_tablet_info.tablet_id; }
-};
 
 struct CDCCreationState {
   std::vector<xrepl::StreamId> created_cdc_streams;
-  std::vector<ProducerTabletInfo> producer_entries_modified;
+  std::vector<TabletStreamInfo> producer_entries_modified;
 
   void Clear() {
     created_cdc_streams.clear();
@@ -61,23 +46,6 @@ struct CDCCreationState {
   }
 };
 
-inline size_t hash_value(const ProducerTabletInfo& p) noexcept {
-  return ProducerTabletInfo::Hash()(p);
-}
-
-constexpr char kAlterReplicationGroupSuffix[] = ".ALTER";
-
-inline ReplicationGroupId GetAlterReplicationGroupId(const std::string& replication_group_id) {
-  return ReplicationGroupId(replication_group_id + kAlterReplicationGroupSuffix);
-}
-
-inline ReplicationGroupId GetAlterReplicationGroupId(
-    const ReplicationGroupId& replication_group_id) {
-  return GetAlterReplicationGroupId(replication_group_id.ToString());
-}
-
-bool IsAlterReplicationGroupId(const ReplicationGroupId& replication_group_id);
-
-ReplicationGroupId GetOriginalReplicationGroupId(const ReplicationGroupId& replication_group_id);
+inline size_t hash_value(const TabletStreamInfo& p) noexcept { return TabletStreamInfo::Hash()(p); }
 
 }  // namespace yb::cdc
diff --git a/src/yb/cdc/xcluster_producer_bootstrap.cc b/src/yb/cdc/xcluster_producer_bootstrap.cc
index 9c0a0850da..9587c5c1da 100644
--- a/src/yb/cdc/xcluster_producer_bootstrap.cc
+++ b/src/yb/cdc/xcluster_producer_bootstrap.cc
@@ -12,6 +12,7 @@
 
 #include "yb/cdc/xcluster_producer_bootstrap.h"
 
+#include "yb/cdc/cdc_error.h"
 #include "yb/cdc/cdc_service_context.h"
 #include "yb/cdc/cdc_state_table.h"
 #include "yb/client/meta_cache.h"
@@ -144,7 +145,7 @@ Status XClusterProducerBootstrap::ConstructServerToTabletsMapping() {
 
       // Mark the tablet for rollback.
       creation_state_->producer_entries_modified.push_back(
-          {.replication_group_id = {}, .stream_id = bootstrap_id, .tablet_id = tablet_id});
+          {.stream_id = bootstrap_id, .tablet_id = tablet_id});
 
       // Initially store invalid op_id to catch any missed streams later in VerifyTabletOpIds.
       tablet_op_ids_[bootstrap_tablet_pair] = yb::OpId::Invalid();
@@ -305,7 +306,7 @@ Status XClusterProducerBootstrap::FetchLatestOpIdsFromRemotePeers() {
     for (int i = 0; i < get_op_id_resp->op_ids_size(); i++) {
       const auto& bootstrap_id = leader_tablets.at(i).first;
       const auto& tablet_id = leader_tablets.at(i).second;
-      ProducerTabletInfo producer_tablet{{} /* Universe UUID */, bootstrap_id, tablet_id};
+      TabletStreamInfo producer_tablet{bootstrap_id, tablet_id};
       OpId op_id = OpId::FromPB(get_op_id_resp->op_ids(i));
 
       // Add op_id for tablet.
diff --git a/src/yb/cdc/xcluster_types.cc b/src/yb/cdc/xcluster_types.cc
new file mode 100644
index 0000000000..e77370677f
--- /dev/null
+++ b/src/yb/cdc/xcluster_types.cc
@@ -0,0 +1,37 @@
+// Copyright (c) YugabyteDB, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
+// in compliance with the License.  You may obtain a copy of the License at
+//
+// http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software distributed under the License
+// is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
+// or implied.  See the License for the specific language governing permissions and limitations
+// under the License.
+//
+
+#include "yb/cdc/xcluster_types.h"
+
+#include <boost/container_hash/hash_fwd.hpp>
+
+#include "yb/util/format.h"
+
+namespace yb::xcluster {
+
+std::string ProducerTabletInfo::ToString() const {
+  return Format(
+      "{ replication_group_id: $0 stream_id: $1 tablet_id: $2 }", replication_group_id, stream_id,
+      tablet_id);
+}
+
+std::size_t ProducerTabletInfo::Hash::operator()(const ProducerTabletInfo& p) const noexcept {
+  std::size_t hash = 0;
+  boost::hash_combine(hash, p.replication_group_id);
+  boost::hash_combine(hash, p.stream_id);
+  boost::hash_combine(hash, p.tablet_id);
+
+  return hash;
+}
+
+}  // namespace yb::xcluster
diff --git a/src/yb/cdc/xcluster_types.h b/src/yb/cdc/xcluster_types.h
index 1c11da7b77..7494100972 100644
--- a/src/yb/cdc/xcluster_types.h
+++ b/src/yb/cdc/xcluster_types.h
@@ -13,11 +13,14 @@
 
 #pragma once
 
+#include "yb/cdc/xrepl_types.h"
 #include "yb/common/common_types.pb.h"
 #include "yb/common/entity_ids_types.h"
 
 namespace yb::xcluster {
 
+YB_STRONGLY_TYPED_STRING(ReplicationGroupId);
+
 struct TabletReplicationError {
   int64_t consumer_term = 0;
   ReplicationErrorPb error;
@@ -27,4 +30,39 @@ struct TabletReplicationError {
 using ReplicationGroupErrors =
     std::unordered_map<TableId, std::unordered_map<TabletId, TabletReplicationError>>;
 
+struct ProducerTabletInfo {
+  ReplicationGroupId replication_group_id;
+  // Unique ID on Producer, but not on Consumer.
+  xrepl::StreamId stream_id;
+  TabletId tablet_id;
+
+  bool operator==(const ProducerTabletInfo& other) const {
+    return replication_group_id == other.replication_group_id && stream_id == other.stream_id &&
+           tablet_id == other.tablet_id;
+  }
+
+  std::string ToString() const;
+
+  struct Hash {
+    std::size_t operator()(const ProducerTabletInfo& p) const noexcept;
+  };
+};
+
+inline size_t hash_value(const ProducerTabletInfo& p) noexcept {
+  return ProducerTabletInfo::Hash()(p);
+}
+
+struct ConsumerTabletInfo {
+  std::string tablet_id;
+  TableId table_id;
+};
+
+struct XClusterTabletInfo {
+  ProducerTabletInfo producer_tablet_info;
+  ConsumerTabletInfo consumer_tablet_info;
+  // Whether or not replication has been paused for this tablet.
+  bool disable_stream;
+
+  const std::string& producer_tablet_id() const { return producer_tablet_info.tablet_id; }
+};
 }  // namespace yb::xcluster
diff --git a/src/yb/cdc/xcluster_util.cc b/src/yb/cdc/xcluster_util.cc
new file mode 100644
index 0000000000..d4229fbf19
--- /dev/null
+++ b/src/yb/cdc/xcluster_util.cc
@@ -0,0 +1,38 @@
+// Copyright (c) YugabyteDB, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
+// in compliance with the License.  You may obtain a copy of the License at
+//
+// http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software distributed under the License
+// is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
+// or implied.  See the License for the specific language governing permissions and limitations
+// under the License.
+//
+
+#include "yb/cdc/xcluster_util.h"
+
+namespace yb::xcluster {
+
+namespace {
+constexpr char kAlterReplicationGroupSuffix[] = ".ALTER";
+}  // namespace
+
+ReplicationGroupId GetAlterReplicationGroupId(const ReplicationGroupId& replication_group_id) {
+  return ReplicationGroupId(replication_group_id.ToString() + kAlterReplicationGroupSuffix);
+}
+
+bool IsAlterReplicationGroupId(const ReplicationGroupId& replication_group_id) {
+  return GStringPiece(replication_group_id.ToString()).ends_with(kAlterReplicationGroupSuffix);
+}
+
+ReplicationGroupId GetOriginalReplicationGroupId(const ReplicationGroupId& replication_group_id) {
+  // Remove the .ALTER suffix from universe_uuid if applicable.
+  GStringPiece clean_id(replication_group_id.ToString());
+  if (clean_id.ends_with(kAlterReplicationGroupSuffix)) {
+    clean_id.remove_suffix(sizeof(kAlterReplicationGroupSuffix) - 1 /* exclude \0 ending */);
+  }
+  return ReplicationGroupId(clean_id.ToString());
+}
+}  // namespace yb::xcluster
diff --git a/src/yb/cdc/xcluster_util.h b/src/yb/cdc/xcluster_util.h
new file mode 100644
index 0000000000..3b25685314
--- /dev/null
+++ b/src/yb/cdc/xcluster_util.h
@@ -0,0 +1,26 @@
+// Copyright (c) YugabyteDB, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
+// in compliance with the License.  You may obtain a copy of the License at
+//
+// http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software distributed under the License
+// is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
+// or implied.  See the License for the specific language governing permissions and limitations
+// under the License.
+//
+
+#pragma once
+
+#include "yb/cdc/xcluster_types.h"
+
+namespace yb::xcluster {
+
+ReplicationGroupId GetAlterReplicationGroupId(const ReplicationGroupId& replication_group_id);
+
+bool IsAlterReplicationGroupId(const ReplicationGroupId& replication_group_id);
+
+ReplicationGroupId GetOriginalReplicationGroupId(const ReplicationGroupId& replication_group_id);
+
+}  // namespace yb::xcluster
diff --git a/src/yb/cdc/cdc_fwd.h b/src/yb/cdc/xrepl_types.h
similarity index 79%
rename from src/yb/cdc/cdc_fwd.h
rename to src/yb/cdc/xrepl_types.h
index 29920ab7f2..a6cf842e2d 100644
--- a/src/yb/cdc/cdc_fwd.h
+++ b/src/yb/cdc/xrepl_types.h
@@ -1,4 +1,4 @@
-// Copyright (c) YugaByte, Inc.
+// Copyright (c) YugabyteDB, Inc.
 //
 // Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
 // in compliance with the License.  You may obtain a copy of the License at
@@ -15,14 +15,8 @@
 
 #include "yb/util/strongly_typed_uuid.h"
 
-namespace yb {
-namespace xrepl {
-YB_STRONGLY_TYPED_UUID_DECL(StreamId);
-}
-namespace cdc {
+namespace yb::xrepl {
 
-class CDCServiceContext;
-class CDCServiceImpl;
+YB_STRONGLY_TYPED_UUID_DECL(StreamId);
 
-} // namespace cdc
-} // namespace yb
+}  // namespace yb::xrepl
diff --git a/src/yb/client/client-internal.cc b/src/yb/client/client-internal.cc
index 292fb9ffaf..0563b4f9ab 100644
--- a/src/yb/client/client-internal.cc
+++ b/src/yb/client/client-internal.cc
@@ -2117,7 +2117,7 @@ class GetXClusterStreamsRpc
       : ClientMasterRpc(client, deadline), user_cb_(std::move(user_cb)) {}
 
   Status Init(
-      const cdc::ReplicationGroupId& replication_group_id, const NamespaceId& namespace_id,
+      const xcluster::ReplicationGroupId& replication_group_id, const NamespaceId& namespace_id,
       const std::vector<TableName>& table_names, const std::vector<PgSchemaName>& pg_schema_names) {
     SCHECK_EQ(
         table_names.size(), pg_schema_names.size(), InvalidArgument,
@@ -2184,7 +2184,7 @@ class IsXClusterBootstrapRequiredRpc : public ClientMasterRpc<
       : ClientMasterRpc(client, deadline), user_cb_(std::move(user_cb)) {}
 
   Status Init(
-      const cdc::ReplicationGroupId& replication_group_id, const NamespaceId& namespace_id) {
+      const xcluster::ReplicationGroupId& replication_group_id, const NamespaceId& namespace_id) {
     req_.set_replication_group_id(replication_group_id.ToString());
     req_.set_namespace_id(namespace_id);
     return Status::OK();
@@ -2873,9 +2873,10 @@ Result<bool> YBClient::Data::CheckIfPitrActive(CoarseTimePoint deadline) {
 }
 
 Status YBClient::Data::GetXClusterStreams(
-    YBClient* client, CoarseTimePoint deadline, const cdc::ReplicationGroupId& replication_group_id,
-    const NamespaceId& namespace_id, const std::vector<TableName>& table_names,
-    const std::vector<PgSchemaName>& pg_schema_names, GetXClusterStreamsCallback user_cb) {
+    YBClient* client, CoarseTimePoint deadline,
+    const xcluster::ReplicationGroupId& replication_group_id, const NamespaceId& namespace_id,
+    const std::vector<TableName>& table_names, const std::vector<PgSchemaName>& pg_schema_names,
+    GetXClusterStreamsCallback user_cb) {
   auto rpc =
       std::make_shared<internal::GetXClusterStreamsRpc>(client, std::move(user_cb), deadline);
   RETURN_NOT_OK(rpc->Init(replication_group_id, namespace_id, table_names, pg_schema_names));
@@ -2885,8 +2886,9 @@ Status YBClient::Data::GetXClusterStreams(
 }
 
 Status YBClient::Data::IsXClusterBootstrapRequired(
-    YBClient* client, CoarseTimePoint deadline, const cdc::ReplicationGroupId& replication_group_id,
-    const NamespaceId& namespace_id, IsXClusterBootstrapRequiredCallback user_cb) {
+    YBClient* client, CoarseTimePoint deadline,
+    const xcluster::ReplicationGroupId& replication_group_id, const NamespaceId& namespace_id,
+    IsXClusterBootstrapRequiredCallback user_cb) {
   auto rpc = std::make_shared<internal::IsXClusterBootstrapRequiredRpc>(
       client, std::move(user_cb), deadline);
   RETURN_NOT_OK(rpc->Init(replication_group_id, namespace_id));
diff --git a/src/yb/client/client-internal.h b/src/yb/client/client-internal.h
index 4fbc5298dc..39e1f679d3 100644
--- a/src/yb/client/client-internal.h
+++ b/src/yb/client/client-internal.h
@@ -446,13 +446,13 @@ class YBClient::Data {
 
   Status GetXClusterStreams(
       YBClient* client, CoarseTimePoint deadline,
-      const cdc::ReplicationGroupId& replication_group_id, const NamespaceId& namespace_id,
+      const xcluster::ReplicationGroupId& replication_group_id, const NamespaceId& namespace_id,
       const std::vector<TableName>& table_names, const std::vector<PgSchemaName>& pg_schema_names,
       GetXClusterStreamsCallback user_cb);
 
   Status IsXClusterBootstrapRequired(
       YBClient* client, CoarseTimePoint deadline,
-      const cdc::ReplicationGroupId& replication_group_id, const NamespaceId& namespace_id,
+      const xcluster::ReplicationGroupId& replication_group_id, const NamespaceId& namespace_id,
       IsXClusterBootstrapRequiredCallback user_cb);
 
   template <class ProxyClass, class ReqClass, class RespClass>
diff --git a/src/yb/client/client.cc b/src/yb/client/client.cc
index ac0b25912c..f1add04b88 100644
--- a/src/yb/client/client.cc
+++ b/src/yb/client/client.cc
@@ -1746,7 +1746,7 @@ Status YBClient::BootstrapProducer(
 }
 
 Result<std::vector<NamespaceId>> YBClient::XClusterCreateOutboundReplicationGroup(
-    const cdc::ReplicationGroupId& replication_group_id,
+    const xcluster::ReplicationGroupId& replication_group_id,
     const std::vector<NamespaceName>& namespace_names) {
   SCHECK(!namespace_names.empty(), InvalidArgument, "At least one namespace name is required");
 
@@ -1772,7 +1772,7 @@ Result<std::vector<NamespaceId>> YBClient::XClusterCreateOutboundReplicationGrou
 }
 
 Status YBClient::GetXClusterStreams(
-    CoarseTimePoint deadline, const cdc::ReplicationGroupId& replication_group_id,
+    CoarseTimePoint deadline, const xcluster::ReplicationGroupId& replication_group_id,
     const NamespaceId& namespace_id, const std::vector<TableName>& table_names,
     const std::vector<PgSchemaName>& pg_schema_names, GetXClusterStreamsCallback callback) {
   return data_->GetXClusterStreams(
@@ -1781,14 +1781,14 @@ Status YBClient::GetXClusterStreams(
 }
 
 Status YBClient::IsXClusterBootstrapRequired(
-    CoarseTimePoint deadline, const cdc::ReplicationGroupId& replication_group_id,
+    CoarseTimePoint deadline, const xcluster::ReplicationGroupId& replication_group_id,
     const NamespaceId& namespace_id, IsXClusterBootstrapRequiredCallback callback) {
   return data_->IsXClusterBootstrapRequired(
       this, deadline, replication_group_id, namespace_id, std::move(callback));
 }
 
 Status YBClient::XClusterDeleteOutboundReplicationGroup(
-    const cdc::ReplicationGroupId& replication_group_id) {
+    const xcluster::ReplicationGroupId& replication_group_id) {
   master::XClusterDeleteOutboundReplicationGroupRequestPB req;
   req.set_replication_group_id(replication_group_id.ToString());
 
@@ -1802,7 +1802,7 @@ Status YBClient::XClusterDeleteOutboundReplicationGroup(
 }
 
 Result<NamespaceId> YBClient::XClusterAddNamespaceToOutboundReplicationGroup(
-    const cdc::ReplicationGroupId& replication_group_id, const NamespaceName& namespace_name) {
+    const xcluster::ReplicationGroupId& replication_group_id, const NamespaceName& namespace_name) {
   master::XClusterAddNamespaceToOutboundReplicationGroupRequestPB req;
   req.set_replication_group_id(replication_group_id.ToString());
   req.set_namespace_name(namespace_name);
@@ -1819,7 +1819,7 @@ Result<NamespaceId> YBClient::XClusterAddNamespaceToOutboundReplicationGroup(
 }
 
 Status YBClient::XClusterRemoveNamespaceFromOutboundReplicationGroup(
-    const cdc::ReplicationGroupId& replication_group_id, const NamespaceId& namespace_id) {
+    const xcluster::ReplicationGroupId& replication_group_id, const NamespaceId& namespace_id) {
   master::XClusterRemoveNamespaceFromOutboundReplicationGroupRequestPB req;
   req.set_replication_group_id(replication_group_id.ToString());
   req.set_namespace_id(namespace_id);
@@ -1836,8 +1836,7 @@ Status YBClient::XClusterRemoveNamespaceFromOutboundReplicationGroup(
 }
 
 Status YBClient::UpdateConsumerOnProducerSplit(
-    const cdc::ReplicationGroupId& replication_group_id,
-    const xrepl::StreamId& stream_id,
+    const xcluster::ReplicationGroupId& replication_group_id, const xrepl::StreamId& stream_id,
     const master::ProducerSplitTabletInfoPB& split_info) {
   SCHECK(!replication_group_id.empty(), InvalidArgument, "Producer id is required.");
   SCHECK(stream_id, InvalidArgument, "Stream id is required.");
@@ -1853,12 +1852,9 @@ Status YBClient::UpdateConsumerOnProducerSplit(
 }
 
 Status YBClient::UpdateConsumerOnProducerMetadata(
-    const cdc::ReplicationGroupId& replication_group_id,
-    const xrepl::StreamId& stream_id,
-    const tablet::ChangeMetadataRequestPB& meta_info,
-    uint32_t colocation_id,
-    uint32_t producer_schema_version,
-    uint32_t consumer_schema_version,
+    const xcluster::ReplicationGroupId& replication_group_id, const xrepl::StreamId& stream_id,
+    const tablet::ChangeMetadataRequestPB& meta_info, uint32_t colocation_id,
+    uint32_t producer_schema_version, uint32_t consumer_schema_version,
     master::UpdateConsumerOnProducerMetadataResponsePB* resp) {
   SCHECK(!replication_group_id.empty(), InvalidArgument, "ReplicationGroup id is required.");
   SCHECK(stream_id, InvalidArgument, "Stream id is required.");
@@ -1877,7 +1873,7 @@ Status YBClient::UpdateConsumerOnProducerMetadata(
 }
 
 Status YBClient::XClusterReportNewAutoFlagConfigVersion(
-    const cdc::ReplicationGroupId& replication_group_id, uint32 auto_flag_config_version) {
+    const xcluster::ReplicationGroupId& replication_group_id, uint32 auto_flag_config_version) {
   SCHECK(!replication_group_id.empty(), InvalidArgument, "ReplicationGroup id is required");
   master::XClusterReportNewAutoFlagConfigVersionRequestPB req;
   req.set_replication_group_id(replication_group_id.ToString());
diff --git a/src/yb/client/client.h b/src/yb/client/client.h
index 62f6579527..311d00e55b 100644
--- a/src/yb/client/client.h
+++ b/src/yb/client/client.h
@@ -46,8 +46,6 @@
 
 #include <gtest/gtest_prod.h>
 
-#include "yb/cdc/cdc_producer.h"
-
 #include "yb/client/client_fwd.h"
 
 #include "yb/common/clock.h"
@@ -104,6 +102,10 @@ class LocalTabletServer;
 class TabletServerServiceProxy;
 }
 
+namespace xcluster {
+YB_STRONGLY_TYPED_STRING(ReplicationGroupId);
+}
+
 namespace client {
 
 struct NamespaceInfo {
@@ -676,45 +678,42 @@ class YBClient {
       BootstrapProducerCallback callback);
 
   Result<std::vector<NamespaceId>> XClusterCreateOutboundReplicationGroup(
-      const cdc::ReplicationGroupId& replication_group_id,
+      const xcluster::ReplicationGroupId& replication_group_id,
       const std::vector<NamespaceName>& namespace_names);
 
   Status GetXClusterStreams(
-      CoarseTimePoint deadline, const cdc::ReplicationGroupId& replication_group_id,
+      CoarseTimePoint deadline, const xcluster::ReplicationGroupId& replication_group_id,
       const NamespaceId& namespace_id, const std::vector<TableName>& table_names,
       const std::vector<PgSchemaName>& pg_schema_names, GetXClusterStreamsCallback callback);
 
   Status IsXClusterBootstrapRequired(
-      CoarseTimePoint deadline, const cdc::ReplicationGroupId& replication_group_id,
+      CoarseTimePoint deadline, const xcluster::ReplicationGroupId& replication_group_id,
       const NamespaceId& namespace_id, IsXClusterBootstrapRequiredCallback callback);
 
   Status XClusterDeleteOutboundReplicationGroup(
-      const cdc::ReplicationGroupId& replication_group_id);
+      const xcluster::ReplicationGroupId& replication_group_id);
 
   Result<NamespaceId> XClusterAddNamespaceToOutboundReplicationGroup(
-      const cdc::ReplicationGroupId& replication_group_id, const NamespaceName& namespace_name);
+      const xcluster::ReplicationGroupId& replication_group_id,
+      const NamespaceName& namespace_name);
 
   Status XClusterRemoveNamespaceFromOutboundReplicationGroup(
-      const cdc::ReplicationGroupId& replication_group_id, const NamespaceId& namespace_id);
+      const xcluster::ReplicationGroupId& replication_group_id, const NamespaceId& namespace_id);
 
   // Update consumer pollers after a producer side tablet split.
   Status UpdateConsumerOnProducerSplit(
-      const cdc::ReplicationGroupId& replication_group_id,
-      const xrepl::StreamId& stream_id,
+      const xcluster::ReplicationGroupId& replication_group_id, const xrepl::StreamId& stream_id,
       const master::ProducerSplitTabletInfoPB& split_info);
 
   // Update after a producer DDL change. Returns if caller should wait for a similar Consumer DDL.
   Status UpdateConsumerOnProducerMetadata(
-      const cdc::ReplicationGroupId& replication_group_id,
-      const xrepl::StreamId& stream_id,
-      const tablet::ChangeMetadataRequestPB& meta_info,
-      uint32_t colocation_id,
-      uint32_t producer_schema_version,
-      uint32_t consumer_schema_version,
+      const xcluster::ReplicationGroupId& replication_group_id, const xrepl::StreamId& stream_id,
+      const tablet::ChangeMetadataRequestPB& meta_info, uint32_t colocation_id,
+      uint32_t producer_schema_version, uint32_t consumer_schema_version,
       master::UpdateConsumerOnProducerMetadataResponsePB* resp);
 
   Status XClusterReportNewAutoFlagConfigVersion(
-      const cdc::ReplicationGroupId& replication_group_id, uint32 auto_flag_config_version);
+      const xcluster::ReplicationGroupId& replication_group_id, uint32 auto_flag_config_version);
 
   void GetTableLocations(
       const TableId& table_id, int32_t max_tablets, RequireTabletsRunning require_tablets_running,
diff --git a/src/yb/docdb/conflict_resolution.h b/src/yb/docdb/conflict_resolution.h
index a03f0376d2..4afd70441e 100644
--- a/src/yb/docdb/conflict_resolution.h
+++ b/src/yb/docdb/conflict_resolution.h
@@ -23,6 +23,7 @@
 #include "yb/dockv/intent.h"
 #include "yb/docdb/shared_lock_manager.h"
 #include "yb/docdb/wait_queue.h"
+#include "yb/tablet/tablet_fwd.h"
 
 namespace rocksdb {
 
diff --git a/src/yb/docdb/docrowwiseiterator-test.cc b/src/yb/docdb/docrowwiseiterator-test.cc
index 3f97156d7c..b2230c786d 100644
--- a/src/yb/docdb/docrowwiseiterator-test.cc
+++ b/src/yb/docdb/docrowwiseiterator-test.cc
@@ -15,6 +15,7 @@
 #include <string>
 
 #include "yb/common/common.pb.h"
+#include "yb/common/pgsql_protocol.pb.h"
 #include "yb/qlexpr/ql_expr.h"
 #include "yb/common/ql_value.h"
 #include "yb/common/read_hybrid_time.h"
diff --git a/src/yb/integration-tests/cdc_service-int-test.cc b/src/yb/integration-tests/cdc_service-int-test.cc
index 999bbface3..f07c320fa7 100644
--- a/src/yb/integration-tests/cdc_service-int-test.cc
+++ b/src/yb/integration-tests/cdc_service-int-test.cc
@@ -1020,7 +1020,7 @@ TEST_F(CDCServiceTestMultipleServersOneTablet, TestUpdateLagMetrics) {
 
   stream_id_ = ASSERT_RESULT(CreateCDCStream(cdc_proxy_, table_.table()->id()));
   std::string tablet_id = GetTablet();
-  ProducerTabletInfo tablet_info = {{} /* empty universeid for producers */, stream_id_, tablet_id};
+  TabletStreamInfo tablet_info = {stream_id_, tablet_id};
   const auto& tservers = cluster_->mini_tablet_servers();
 
   // Test with t0 as the leader, by blacklisting other tservers.
diff --git a/src/yb/integration-tests/cdcsdk_ysql-test.cc b/src/yb/integration-tests/cdcsdk_ysql-test.cc
index fabbcfcba0..b4401973cf 100644
--- a/src/yb/integration-tests/cdcsdk_ysql-test.cc
+++ b/src/yb/integration-tests/cdcsdk_ysql-test.cc
@@ -3564,8 +3564,8 @@ TEST_F(CDCSDKYsqlTest, YB_DISABLE_TEST_IN_TSAN(TestCDCSDKActiveTimeCacheInSyncWi
                                                        ->TEST_service_pool("yb.cdc.CDCService")
                                                        ->TEST_get_service()
                                                        .get());
-  auto tablet_info = ASSERT_RESULT(
-      cdc_service->TEST_GetTabletInfoFromCache({{}, stream_id, tablets[0].tablet_id()}));
+  auto tablet_info =
+      ASSERT_RESULT(cdc_service->TEST_GetTabletInfoFromCache({stream_id, tablets[0].tablet_id()}));
   auto first_last_active_time = tablet_info.last_active_time;
   auto last_active_time_from_table = ASSERT_RESULT(
       GetLastActiveTimeFromCdcStateTable(stream_id, tablets[0].tablet_id(), test_client()));
@@ -3594,7 +3594,7 @@ TEST_F(CDCSDKYsqlTest, YB_DISABLE_TEST_IN_TSAN(TestCDCSDKActiveTimeCacheInSyncWi
                                                   ->TEST_get_service()
                                                   .get());
   tablet_info = ASSERT_RESULT(
-      cdc_service->TEST_GetTabletInfoFromCache({{}, stream_id, tablets[0].tablet_id()}));
+      cdc_service->TEST_GetTabletInfoFromCache({stream_id, tablets[0].tablet_id()}));
   auto second_last_active_time = tablet_info.last_active_time;
 
   last_active_time_from_table = ASSERT_RESULT(
diff --git a/src/yb/integration-tests/cdcsdk_ysql_test_base.cc b/src/yb/integration-tests/cdcsdk_ysql_test_base.cc
index cdb6ff3f47..6121e12a3e 100644
--- a/src/yb/integration-tests/cdcsdk_ysql_test_base.cc
+++ b/src/yb/integration-tests/cdcsdk_ysql_test_base.cc
@@ -17,7 +17,7 @@
 #include <vector>
 #include <gtest/gtest.h>
 
-#include "yb/cdc/cdc_fwd.h"
+#include "yb/cdc/xrepl_types.h"
 #include "yb/cdc/cdc_service.pb.h"
 #include "yb/cdc/cdc_state_table.h"
 
@@ -1185,7 +1185,7 @@ namespace cdc {
       const auto& tserver = test_cluster()->mini_tablet_server(i)->server();
       auto cdc_service = dynamic_cast<CDCServiceImpl*>(
           tserver->rpc_server()->TEST_service_pool("yb.cdc.CDCService")->TEST_get_service().get());
-      auto status = cdc_service->TEST_GetTabletInfoFromCache({{}, stream_id, tablet_id});
+      auto status = cdc_service->TEST_GetTabletInfoFromCache({stream_id, tablet_id});
       if (status.ok()) {
         count += 1;
       }
diff --git a/src/yb/integration-tests/cluster_itest_util.h b/src/yb/integration-tests/cluster_itest_util.h
index 3600a91acf..a41faf65d1 100644
--- a/src/yb/integration-tests/cluster_itest_util.h
+++ b/src/yb/integration-tests/cluster_itest_util.h
@@ -68,6 +68,7 @@
 
 #include "yb/integration-tests/mini_cluster.h"
 
+#include "yb/master/master_client.pb.h"
 #include "yb/master/master_fwd.h"
 #include "yb/master/master_client.fwd.h"
 
diff --git a/src/yb/integration-tests/mini_cluster_utils.cc b/src/yb/integration-tests/mini_cluster_utils.cc
index a1e8193ede..53da33280a 100644
--- a/src/yb/integration-tests/mini_cluster_utils.cc
+++ b/src/yb/integration-tests/mini_cluster_utils.cc
@@ -22,6 +22,7 @@
 
 #include "yb/master/catalog_manager_if.h"
 #include "yb/master/master-test-util.h"
+#include "yb/master/master_client.pb.h"
 #include "yb/master/master_ddl.pb.h"
 #include "yb/master/mini_master.h"
 
diff --git a/src/yb/integration-tests/xcluster/xcluster-test.cc b/src/yb/integration-tests/xcluster/xcluster-test.cc
index 90081b2dc4..81aa2d66c1 100644
--- a/src/yb/integration-tests/xcluster/xcluster-test.cc
+++ b/src/yb/integration-tests/xcluster/xcluster-test.cc
@@ -19,10 +19,10 @@
 #include <boost/assign.hpp>
 #include <gtest/gtest.h>
 
-#include "yb/cdc/cdc_service.h"
 #include "yb/cdc/cdc_service.pb.h"
 #include "yb/cdc/cdc_service.proxy.h"
 #include "yb/cdc/cdc_state_table.h"
+#include "yb/cdc/xcluster_util.h"
 #include "yb/cdc/xrepl_stream_metadata.h"
 
 #include "yb/client/client-test-util.h"
@@ -497,7 +497,7 @@ class XClusterTestNoParam : public XClusterYcqlTestBase {
 
   Status WaitForReplicationBootstrap(
       MiniCluster* consumer_cluster, YBClient* consumer_client,
-      const cdc::ReplicationGroupId& replication_group_id) {
+      const xcluster::ReplicationGroupId& replication_group_id) {
     return LoggedWaitFor(
         [=]() -> Result<bool> {
           master::IsSetupNamespaceReplicationWithBootstrapDoneRequestPB req;
@@ -519,7 +519,7 @@ class XClusterTestNoParam : public XClusterYcqlTestBase {
   }
 
   Status VerifyReplicationBootstrapCleanupOnFailure(
-      const cdc::ReplicationGroupId& replication_group_id) {
+      const xcluster::ReplicationGroupId& replication_group_id) {
     rpc::RpcController rpc;
     rpc.set_timeout(MonoDelta::FromSeconds(kRpcTimeout));
 
@@ -2622,8 +2622,8 @@ TEST_P(XClusterTest, TestFailedAlterUniverseOnRestart) {
   ASSERT_OK(consumer_cluster()->RestartSync());
 
   // Wait for alter universe to be deleted on start up
-  ASSERT_OK(
-      WaitForSetupUniverseReplicationCleanUp(GetAlterReplicationGroupId(kReplicationGroupId)));
+  ASSERT_OK(WaitForSetupUniverseReplicationCleanUp(
+      xcluster::GetAlterReplicationGroupId(kReplicationGroupId)));
 
   // Change should not have gone through
   new_req.set_replication_group_id(kReplicationGroupId.ToString());
@@ -2633,7 +2633,8 @@ TEST_P(XClusterTest, TestFailedAlterUniverseOnRestart) {
 
   // Check that the unfinished alter universe was deleted on start up
   rpc.Reset();
-  new_req.set_replication_group_id(GetAlterReplicationGroupId(kReplicationGroupId).ToString());
+  new_req.set_replication_group_id(
+      xcluster::GetAlterReplicationGroupId(kReplicationGroupId).ToString());
   Status s = master_proxy->GetUniverseReplication(new_req, &new_resp, &rpc);
   ASSERT_OK(s);
   ASSERT_TRUE(new_resp.has_error());
diff --git a/src/yb/integration-tests/xcluster/xcluster_external_mini_cluster_base.cc b/src/yb/integration-tests/xcluster/xcluster_external_mini_cluster_base.cc
index 69feb6b0d2..63db608d68 100644
--- a/src/yb/integration-tests/xcluster/xcluster_external_mini_cluster_base.cc
+++ b/src/yb/integration-tests/xcluster/xcluster_external_mini_cluster_base.cc
@@ -152,7 +152,7 @@ Result<client::TableHandle> XClusterExternalMiniClusterBase::CreateTable(
 }
 
 Status XClusterExternalMiniClusterBase::SetupReplication(
-    cdc::ReplicationGroupId replication_group_id,
+    xcluster::ReplicationGroupId replication_group_id,
     std::vector<std::shared_ptr<client::YBTable>> source_tables) {
   std::string table_ids;
   if (source_tables.empty()) {
diff --git a/src/yb/integration-tests/xcluster/xcluster_external_mini_cluster_base.h b/src/yb/integration-tests/xcluster/xcluster_external_mini_cluster_base.h
index cfb57b42b0..54bb28d87f 100644
--- a/src/yb/integration-tests/xcluster/xcluster_external_mini_cluster_base.h
+++ b/src/yb/integration-tests/xcluster/xcluster_external_mini_cluster_base.h
@@ -45,7 +45,7 @@ class XClusterExternalMiniClusterBase : public YBTest {
   virtual Status SetupClustersAndReplicationGroup();
 
   Status SetupReplication(
-      cdc::ReplicationGroupId replication_group_id = kReplicationGroupId,
+      xcluster::ReplicationGroupId replication_group_id = kReplicationGroupId,
       std::vector<std::shared_ptr<client::YBTable>> source_tables = {});
 
   Status VerifyReplicationError(
diff --git a/src/yb/integration-tests/xcluster/xcluster_outbound_replication_group-test.cc b/src/yb/integration-tests/xcluster/xcluster_outbound_replication_group-test.cc
index 36ec4315f9..b4c7c6e2a7 100644
--- a/src/yb/integration-tests/xcluster/xcluster_outbound_replication_group-test.cc
+++ b/src/yb/integration-tests/xcluster/xcluster_outbound_replication_group-test.cc
@@ -25,7 +25,7 @@ namespace master {
 const auto kDeadline = MonoDelta::FromSeconds(30);
 const NamespaceName kNamespaceName = "db1";
 const PgSchemaName kPgSchemaName = "public";
-const cdc::ReplicationGroupId kReplicationGroupId("rg1");
+const xcluster::ReplicationGroupId kReplicationGroupId("rg1");
 const TableName kTableName1 = "table1", kTableName2 = "table2";
 
 class XClusterOutboundReplicationGroupTest : public XClusterYsqlTestBase {
@@ -105,7 +105,7 @@ class XClusterOutboundReplicationGroupTest : public XClusterYsqlTestBase {
   }
 
   Result<master::GetXClusterStreamsResponsePB> GetXClusterStreams(
-      const cdc::ReplicationGroupId& replication_group_id, const NamespaceId& namespace_id,
+      const xcluster::ReplicationGroupId& replication_group_id, const NamespaceId& namespace_id,
       std::vector<TableName> table_names = {}, std::vector<PgSchemaName> pg_schema_names = {}) {
     std::promise<Result<master::GetXClusterStreamsResponsePB>> promise;
     RETURN_NOT_OK(client_->GetXClusterStreams(
diff --git a/src/yb/integration-tests/xcluster/xcluster_test_base.cc b/src/yb/integration-tests/xcluster/xcluster_test_base.cc
index b2f84182da..36d9fd4b56 100644
--- a/src/yb/integration-tests/xcluster/xcluster_test_base.cc
+++ b/src/yb/integration-tests/xcluster/xcluster_test_base.cc
@@ -15,7 +15,7 @@
 
 #include <string>
 
-#include "yb/cdc/cdc_service.h"
+#include "yb/cdc/xcluster_util.h"
 
 #include "yb/client/client.h"
 #include "yb/client/table.h"
@@ -247,7 +247,7 @@ Status XClusterTestBase::SetupUniverseReplication(
 }
 
 Status XClusterTestBase::SetupUniverseReplication(
-    const cdc::ReplicationGroupId& replication_group_id,
+    const xcluster::ReplicationGroupId& replication_group_id,
     const std::vector<std::shared_ptr<client::YBTable>>& producer_tables,
     SetupReplicationOptions opts) {
   return SetupUniverseReplication(
@@ -268,7 +268,7 @@ Status XClusterTestBase::SetupUniverseReplication() {
 
 Status XClusterTestBase::SetupUniverseReplication(
     MiniCluster* producer_cluster, MiniCluster* consumer_cluster, YBClient* consumer_client,
-    const cdc::ReplicationGroupId& replication_group_id,
+    const xcluster::ReplicationGroupId& replication_group_id,
     const std::vector<std::shared_ptr<client::YBTable>>& producer_tables,
     const std::vector<xrepl::StreamId>& bootstrap_ids, SetupReplicationOptions opts) {
   std::vector<string> producer_table_ids;
@@ -283,7 +283,7 @@ Status XClusterTestBase::SetupUniverseReplication(
 
 Status XClusterTestBase::SetupUniverseReplication(
     MiniCluster* producer_cluster, MiniCluster* consumer_cluster, YBClient* consumer_client,
-    const cdc::ReplicationGroupId& replication_group_id,
+    const xcluster::ReplicationGroupId& replication_group_id,
     const std::vector<TableId>& producer_table_ids,
     const std::vector<xrepl::StreamId>& bootstrap_ids, SetupReplicationOptions opts) {
   // If we have certs for encryption in FLAGS_certs_dir then we need to copy it over to the
@@ -350,7 +350,7 @@ Status XClusterTestBase::SetupUniverseReplication(
 
 Status XClusterTestBase::SetupNSUniverseReplication(
     MiniCluster* producer_cluster, MiniCluster* consumer_cluster, YBClient* consumer_client,
-    const cdc::ReplicationGroupId& replication_group_id, const std::string& producer_ns_name,
+    const xcluster::ReplicationGroupId& replication_group_id, const std::string& producer_ns_name,
     const YQLDatabase& producer_ns_type, SetupReplicationOptions opts) {
   master::SetupNSUniverseReplicationRequestPB req;
   master::SetupNSUniverseReplicationResponsePB resp;
@@ -386,7 +386,7 @@ Status XClusterTestBase::VerifyUniverseReplication(master::GetUniverseReplicatio
 }
 
 Status XClusterTestBase::VerifyUniverseReplication(
-    const cdc::ReplicationGroupId& replication_group_id,
+    const xcluster::ReplicationGroupId& replication_group_id,
     master::GetUniverseReplicationResponsePB* resp) {
   return VerifyUniverseReplication(
       consumer_cluster(), consumer_client(), replication_group_id, resp);
@@ -394,7 +394,7 @@ Status XClusterTestBase::VerifyUniverseReplication(
 
 Status XClusterTestBase::VerifyUniverseReplication(
     MiniCluster* consumer_cluster, YBClient* consumer_client,
-    const cdc::ReplicationGroupId& replication_group_id,
+    const xcluster::ReplicationGroupId& replication_group_id,
     master::GetUniverseReplicationResponsePB* resp) {
   master::IsSetupUniverseReplicationDoneResponsePB setup_resp;
   RETURN_NOT_OK(WaitForSetupUniverseReplication(
@@ -424,7 +424,7 @@ Status XClusterTestBase::VerifyUniverseReplication(
 
 Status XClusterTestBase::VerifyNSUniverseReplication(
     MiniCluster* consumer_cluster, YBClient* consumer_client,
-    const cdc::ReplicationGroupId& replication_group_id, int num_expected_table) {
+    const xcluster::ReplicationGroupId& replication_group_id, int num_expected_table) {
   return LoggedWaitFor([&]() -> Result<bool> {
     master::GetUniverseReplicationResponsePB resp;
     auto s =
@@ -436,7 +436,7 @@ Status XClusterTestBase::VerifyNSUniverseReplication(
 
 Status XClusterTestBase::ToggleUniverseReplication(
     MiniCluster* consumer_cluster, YBClient* consumer_client,
-    const cdc::ReplicationGroupId& replication_group_id, bool is_enabled) {
+    const xcluster::ReplicationGroupId& replication_group_id, bool is_enabled) {
   master::SetUniverseReplicationEnabledRequestPB req;
   master::SetUniverseReplicationEnabledResponsePB resp;
 
@@ -481,7 +481,7 @@ Status XClusterTestBase::ChangeXClusterRole(const cdc::XClusterRole role, Cluste
 
 Status XClusterTestBase::VerifyUniverseReplicationDeleted(
     MiniCluster* consumer_cluster, YBClient* consumer_client,
-    const cdc::ReplicationGroupId& replication_group_id, int timeout) {
+    const xcluster::ReplicationGroupId& replication_group_id, int timeout) {
   return LoggedWaitFor([=]() -> Result<bool> {
     master::GetUniverseReplicationRequestPB req;
     master::GetUniverseReplicationResponsePB resp;
@@ -500,7 +500,7 @@ Status XClusterTestBase::VerifyUniverseReplicationDeleted(
 
 Status XClusterTestBase::WaitForSetupUniverseReplication(
     MiniCluster* consumer_cluster, YBClient* consumer_client,
-    const cdc::ReplicationGroupId& replication_group_id,
+    const xcluster::ReplicationGroupId& replication_group_id,
     master::IsSetupUniverseReplicationDoneResponsePB* resp) {
   return LoggedWaitFor(
       [=]() -> Result<bool> {
@@ -553,12 +553,13 @@ uint32_t XClusterTestBase::GetSuccessfulWriteOps(MiniCluster* cluster) {
 }
 
 Status XClusterTestBase::DeleteUniverseReplication(
-    const cdc::ReplicationGroupId& replication_group_id) {
+    const xcluster::ReplicationGroupId& replication_group_id) {
   return DeleteUniverseReplication(replication_group_id, consumer_client(), consumer_cluster());
 }
 
 Status XClusterTestBase::DeleteUniverseReplication(
-    const cdc::ReplicationGroupId& replication_group_id, YBClient* client, MiniCluster* cluster) {
+    const xcluster::ReplicationGroupId& replication_group_id, YBClient* client,
+    MiniCluster* cluster) {
   master::DeleteUniverseReplicationRequestPB req;
   master::DeleteUniverseReplicationResponsePB resp;
 
@@ -576,9 +577,8 @@ Status XClusterTestBase::DeleteUniverseReplication(
 }
 
 Status XClusterTestBase::AlterUniverseReplication(
-    const cdc::ReplicationGroupId& replication_group_id,
-    const std::vector<std::shared_ptr<client::YBTable>>& tables,
-    bool add_tables) {
+    const xcluster::ReplicationGroupId& replication_group_id,
+    const std::vector<std::shared_ptr<client::YBTable>>& tables, bool add_tables) {
   master::AlterUniverseReplicationRequestPB alter_req;
   master::AlterUniverseReplicationResponsePB alter_resp;
   rpc::RpcController rpc;
@@ -599,7 +599,7 @@ Status XClusterTestBase::AlterUniverseReplication(
   RETURN_NOT_OK(master_proxy->AlterUniverseReplication(alter_req, &alter_resp, &rpc));
   SCHECK(!alter_resp.has_error(), IllegalState, alter_resp.ShortDebugString());
   master::IsSetupUniverseReplicationDoneResponsePB setup_resp;
-  auto alter_replication_group_id = cdc::GetAlterReplicationGroupId(replication_group_id);
+  auto alter_replication_group_id = xcluster::GetAlterReplicationGroupId(replication_group_id);
   RETURN_NOT_OK(WaitForSetupUniverseReplication(
       consumer_cluster(), consumer_client(), alter_replication_group_id, &setup_resp));
   SCHECK(!setup_resp.has_error(), IllegalState, alter_resp.ShortDebugString());
@@ -620,7 +620,7 @@ Status XClusterTestBase::CorrectlyPollingAllTablets(
 }
 
 Status XClusterTestBase::WaitForSetupUniverseReplicationCleanUp(
-    const cdc::ReplicationGroupId& replication_group_id) {
+    const xcluster::ReplicationGroupId& replication_group_id) {
   auto proxy = std::make_shared<master::MasterReplicationProxy>(
     &consumer_client()->proxy_cache(),
     VERIFY_RESULT(consumer_cluster()->GetLeaderMiniMaster())->bound_rpc_addr());
diff --git a/src/yb/integration-tests/xcluster/xcluster_test_base.h b/src/yb/integration-tests/xcluster/xcluster_test_base.h
index 75c4dd8bb4..fcb11bbc2e 100644
--- a/src/yb/integration-tests/xcluster/xcluster_test_base.h
+++ b/src/yb/integration-tests/xcluster/xcluster_test_base.h
@@ -49,7 +49,7 @@ using YBTables = std::vector<std::shared_ptr<client::YBTable>>;
 
 constexpr int kRpcTimeout = NonTsanVsTsan(60, 120);
 static const std::string kUniverseId = "test_universe";
-static const cdc::ReplicationGroupId kReplicationGroupId("test_replication_group");
+static const xcluster::ReplicationGroupId kReplicationGroupId("test_replication_group");
 static const std::string kKeyColumnName = "key";
 static const uint32_t kRangePartitionInterval = 500;
 
@@ -166,7 +166,7 @@ class XClusterTestBase : public YBTest {
       SetupReplicationOptions opts = SetupReplicationOptions());
 
   Status SetupUniverseReplication(
-      const cdc::ReplicationGroupId& replication_group_id,
+      const xcluster::ReplicationGroupId& replication_group_id,
       const std::vector<std::shared_ptr<client::YBTable>>& producer_tables,
       SetupReplicationOptions opts = SetupReplicationOptions());
 
@@ -175,53 +175,53 @@ class XClusterTestBase : public YBTest {
 
   Status SetupUniverseReplication(
       MiniCluster* producer_cluster, MiniCluster* consumer_cluster, YBClient* consumer_client,
-      const cdc::ReplicationGroupId& replication_group_id,
+      const xcluster::ReplicationGroupId& replication_group_id,
       const std::vector<std::shared_ptr<client::YBTable>>& producer_tables,
       const std::vector<xrepl::StreamId>& bootstrap_ids = {},
       SetupReplicationOptions opts = SetupReplicationOptions());
 
   Status SetupUniverseReplication(
       MiniCluster* producer_cluster, MiniCluster* consumer_cluster, YBClient* consumer_client,
-      const cdc::ReplicationGroupId& replication_group_id,
+      const xcluster::ReplicationGroupId& replication_group_id,
       const std::vector<TableId>& producer_table_ids,
       const std::vector<xrepl::StreamId>& bootstrap_ids = {},
       SetupReplicationOptions opts = SetupReplicationOptions());
 
   Status SetupNSUniverseReplication(
       MiniCluster* producer_cluster, MiniCluster* consumer_cluster, YBClient* consumer_client,
-      const cdc::ReplicationGroupId& replication_group_id, const std::string& producer_ns_name,
+      const xcluster::ReplicationGroupId& replication_group_id, const std::string& producer_ns_name,
       const YQLDatabase& producer_ns_type,
       SetupReplicationOptions opts = SetupReplicationOptions());
 
   Status VerifyUniverseReplication(master::GetUniverseReplicationResponsePB* resp);
 
   Status VerifyUniverseReplication(
-      const cdc::ReplicationGroupId& replication_group_id,
+      const xcluster::ReplicationGroupId& replication_group_id,
       master::GetUniverseReplicationResponsePB* resp);
 
   Status VerifyUniverseReplication(
       MiniCluster* consumer_cluster, YBClient* consumer_client,
-      const cdc::ReplicationGroupId& replication_group_id,
+      const xcluster::ReplicationGroupId& replication_group_id,
       master::GetUniverseReplicationResponsePB* resp);
 
   Status VerifyNSUniverseReplication(
       MiniCluster* consumer_cluster, YBClient* consumer_client,
-      const cdc::ReplicationGroupId& replication_group_id, int num_expected_table);
+      const xcluster::ReplicationGroupId& replication_group_id, int num_expected_table);
 
   Status ChangeXClusterRole(const cdc::XClusterRole role, Cluster* cluster = nullptr);
 
   Status ToggleUniverseReplication(
       MiniCluster* consumer_cluster, YBClient* consumer_client,
-      const cdc::ReplicationGroupId& replication_group_id, bool is_enabled);
+      const xcluster::ReplicationGroupId& replication_group_id, bool is_enabled);
 
   Status VerifyUniverseReplicationDeleted(
       MiniCluster* consumer_cluster, YBClient* consumer_client,
-      const cdc::ReplicationGroupId& replication_group_id, int timeout);
+      const xcluster::ReplicationGroupId& replication_group_id, int timeout);
 
   // Wait for SetupUniverseReplication to complete. resp will contain the errors if any.
   Status WaitForSetupUniverseReplication(
       MiniCluster* consumer_cluster, YBClient* consumer_client,
-      const cdc::ReplicationGroupId& replication_group_id,
+      const xcluster::ReplicationGroupId& replication_group_id,
       master::IsSetupUniverseReplicationDoneResponsePB* resp);
 
   Status GetCDCStreamForTable(const TableId& table_id, master::ListCDCStreamsResponsePB* resp);
@@ -229,21 +229,21 @@ class XClusterTestBase : public YBTest {
   uint32_t GetSuccessfulWriteOps(MiniCluster* cluster);
 
   Status DeleteUniverseReplication(
-      const cdc::ReplicationGroupId& replication_group_id = kReplicationGroupId);
+      const xcluster::ReplicationGroupId& replication_group_id = kReplicationGroupId);
 
   Status DeleteUniverseReplication(
-      const cdc::ReplicationGroupId& replication_group_id, YBClient* client, MiniCluster* cluster);
+      const xcluster::ReplicationGroupId& replication_group_id, YBClient* client,
+      MiniCluster* cluster);
 
   Status AlterUniverseReplication(
-      const cdc::ReplicationGroupId& replication_group_id,
-      const std::vector<std::shared_ptr<client::YBTable>>& tables,
-      bool add_tables);
+      const xcluster::ReplicationGroupId& replication_group_id,
+      const std::vector<std::shared_ptr<client::YBTable>>& tables, bool add_tables);
 
   Status CorrectlyPollingAllTablets(uint32_t num_producer_tablets);
   Status CorrectlyPollingAllTablets(MiniCluster* cluster, uint32_t num_producer_tablets);
 
   Status WaitForSetupUniverseReplicationCleanUp(
-      const cdc::ReplicationGroupId& replication_group_id);
+      const xcluster::ReplicationGroupId& replication_group_id);
 
   Status WaitForValidSafeTimeOnAllTServers(
       const NamespaceId& namespace_id, Cluster* cluster = nullptr,
diff --git a/src/yb/integration-tests/xcluster/xcluster_topologies-test.cc b/src/yb/integration-tests/xcluster/xcluster_topologies-test.cc
index 698fa33f23..4c1d949d42 100644
--- a/src/yb/integration-tests/xcluster/xcluster_topologies-test.cc
+++ b/src/yb/integration-tests/xcluster/xcluster_topologies-test.cc
@@ -344,7 +344,7 @@ TEST_F(XClusterTopologiesTest, TestNToOneReplicationFails) {
 
   for (size_t i = 0; i < producer_clusters_.size(); ++i) {
     MiniCluster* producer_cluster_mini_cluster = producer_clusters_[i]->mini_cluster_.get();
-    const cdc::ReplicationGroupId replication_group_id(Format("$0$1", kReplicationGroupId, i));
+    const xcluster::ReplicationGroupId replication_group_id(Format("$0$1", kReplicationGroupId, i));
     master::IsSetupUniverseReplicationDoneResponsePB setup_resp;
     master::GetUniverseReplicationResponsePB verify_resp;
     if (i == 0) {
diff --git a/src/yb/integration-tests/xcluster/xcluster_upgrade-test.cc b/src/yb/integration-tests/xcluster/xcluster_upgrade-test.cc
index 072a1bcb2c..9204e720e7 100644
--- a/src/yb/integration-tests/xcluster/xcluster_upgrade-test.cc
+++ b/src/yb/integration-tests/xcluster/xcluster_upgrade-test.cc
@@ -186,7 +186,7 @@ TEST_F(XClusterUpgradeTest, SetupWithLowerTargetUniverse) {
 
   const auto kAutoFlagsMismatchError = "AutoFlags between the universes are not compatible";
 
-  cdc::ReplicationGroupId replication_group_id1("rg1"), replication_group_id2("rg2"),
+  xcluster::ReplicationGroupId replication_group_id1("rg1"), replication_group_id2("rg2"),
       replication_group_id3("rg3");
   auto s = SetupUniverseReplication(replication_group_id1, producer_tables_);
   ASSERT_NOK(s);
diff --git a/src/yb/integration-tests/xcluster/xcluster_ysql-test.cc b/src/yb/integration-tests/xcluster/xcluster_ysql-test.cc
index d985e70fc7..784bb2f93d 100644
--- a/src/yb/integration-tests/xcluster/xcluster_ysql-test.cc
+++ b/src/yb/integration-tests/xcluster/xcluster_ysql-test.cc
@@ -1748,8 +1748,8 @@ TEST_F(XClusterYsqlTest, IsBootstrapRequiredNotFlushed) {
   ASSERT_TRUE(ASSERT_RESULT(producer_client()->IsBootstrapRequired({producer_table_->id()})));
 
   // 4. Setup replication with data should fail.
-  ASSERT_NOK(
-      SetupUniverseReplication(cdc::ReplicationGroupId("replication-group-2"), producer_tables_));
+  ASSERT_NOK(SetupUniverseReplication(
+      xcluster::ReplicationGroupId("replication-group-2"), producer_tables_));
 }
 
 // Checks that with missing logs, replication will require bootstrapping
diff --git a/src/yb/master/catalog_entity_info.h b/src/yb/master/catalog_entity_info.h
index a5373f6364..8d71ca0821 100644
--- a/src/yb/master/catalog_entity_info.h
+++ b/src/yb/master/catalog_entity_info.h
@@ -32,14 +32,13 @@
 
 #pragma once
 
-#include <shared_mutex>
 #include <mutex>
 #include <vector>
 
 #include <boost/bimap.hpp>
 
 #include "yb/cdc/cdc_types.h"
-#include "yb/common/entity_ids.h"
+#include "yb/cdc/xcluster_types.h"
 #include "yb/master/catalog_entity_base.h"
 #include "yb/master/leader_epoch.h"
 #include "yb/master/master_backup.pb.h"
@@ -60,7 +59,6 @@
 #include "yb/tablet/metadata.pb.h"
 
 #include "yb/util/cow_object.h"
-#include "yb/util/format.h"
 #include "yb/util/monotime.h"
 #include "yb/util/status_fwd.h"
 #include "yb/util/shared_lock.h"
@@ -1186,12 +1184,12 @@ class UniverseReplicationInfoBase {
       google::protobuf::RepeatedPtrField<HostPortPB> producer_masters);
 
  protected:
-  explicit UniverseReplicationInfoBase(cdc::ReplicationGroupId replication_group_id)
+  explicit UniverseReplicationInfoBase(xcluster::ReplicationGroupId replication_group_id)
       : replication_group_id_(std::move(replication_group_id)) {}
 
   virtual ~UniverseReplicationInfoBase() = default;
 
-  const cdc::ReplicationGroupId replication_group_id_;
+  const xcluster::ReplicationGroupId replication_group_id_;
 
   std::shared_ptr<XClusterRpcTasks> xcluster_rpc_tasks_;
   std::string master_addrs_;
@@ -1219,11 +1217,11 @@ class UniverseReplicationInfo : public UniverseReplicationInfoBase,
                                 public RefCountedThreadSafe<UniverseReplicationInfo>,
                                 public MetadataCowWrapper<PersistentUniverseReplicationInfo> {
  public:
-  explicit UniverseReplicationInfo(cdc::ReplicationGroupId replication_group_id)
+  explicit UniverseReplicationInfo(xcluster::ReplicationGroupId replication_group_id)
       : UniverseReplicationInfoBase(std::move(replication_group_id)) {}
 
   const std::string& id() const override { return replication_group_id_.ToString(); }
-  const cdc::ReplicationGroupId& ReplicationGroupId() const { return replication_group_id_; }
+  const xcluster::ReplicationGroupId& ReplicationGroupId() const { return replication_group_id_; }
 
   std::string ToString() const override;
 
@@ -1309,13 +1307,13 @@ class UniverseReplicationBootstrapInfo
       public RefCountedThreadSafe<UniverseReplicationBootstrapInfo>,
       public MetadataCowWrapper<PersistentUniverseReplicationBootstrapInfo> {
  public:
-  explicit UniverseReplicationBootstrapInfo(cdc::ReplicationGroupId replication_group_id)
+  explicit UniverseReplicationBootstrapInfo(xcluster::ReplicationGroupId replication_group_id)
       : UniverseReplicationInfoBase(std::move(replication_group_id)) {}
   UniverseReplicationBootstrapInfo(const UniverseReplicationBootstrapInfo&) = delete;
   UniverseReplicationBootstrapInfo& operator=(const UniverseReplicationBootstrapInfo&) = delete;
 
   const std::string& id() const override { return replication_group_id_.ToString(); }
-  const cdc::ReplicationGroupId& ReplicationGroupId() const { return replication_group_id_; }
+  const xcluster::ReplicationGroupId& ReplicationGroupId() const { return replication_group_id_; }
 
   std::string ToString() const override;
 
diff --git a/src/yb/master/catalog_manager.h b/src/yb/master/catalog_manager.h
index b8064b4826..3808ad299b 100644
--- a/src/yb/master/catalog_manager.h
+++ b/src/yb/master/catalog_manager.h
@@ -44,6 +44,7 @@
 #include <boost/functional/hash.hpp>
 #include <gtest/internal/gtest-internal.h>
 
+#include "yb/cdc/cdc_service.pb.h"
 #include "yb/cdc/xcluster_types.h"
 #include "yb/common/constants.h"
 #include "yb/common/entity_ids.h"
@@ -1108,7 +1109,7 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
       const std::optional<TabletLeaderMetricsPB>& leader_metrics);
 
   void SyncXClusterConsumerReplicationStatusMap(
-      const cdc::ReplicationGroupId& replication_group_id,
+      const xcluster::ReplicationGroupId& replication_group_id,
       const google::protobuf::Map<std::string, cdc::ProducerEntryPB>& producer_map)
       EXCLUDES(xcluster_consumer_replication_error_map_mutex_);
 
@@ -2078,7 +2079,8 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
     const SetupNamespaceReplicationWithBootstrapRequestPB* req);
 
   void DoReplicationBootstrap(
-      const cdc::ReplicationGroupId& replication_id, const std::vector<client::YBTableName>& tables,
+      const xcluster::ReplicationGroupId& replication_id,
+      const std::vector<client::YBTableName>& tables,
       Result<TableBootstrapIdsMap> bootstrap_producer_result);
 
   Result<SnapshotInfoPB> DoReplicationBootstrapCreateSnapshot(
@@ -2809,16 +2811,16 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
       const SetupReplicationInfo& setup_info);
 
   void GetTableSchemaCallback(
-      const cdc::ReplicationGroupId& replication_group_id,
+      const xcluster::ReplicationGroupId& replication_group_id,
       const std::shared_ptr<client::YBTableInfo>& producer_info,
       const SetupReplicationInfo& setup_info, const Status& s);
   void GetTablegroupSchemaCallback(
-      const cdc::ReplicationGroupId& replication_group_id,
+      const xcluster::ReplicationGroupId& replication_group_id,
       const std::shared_ptr<std::vector<client::YBTableInfo>>& info,
       const TablegroupId& producer_tablegroup_id, const SetupReplicationInfo& setup_info,
       const Status& s);
   void GetColocatedTabletSchemaCallback(
-      const cdc::ReplicationGroupId& replication_group_id,
+      const xcluster::ReplicationGroupId& replication_group_id,
       const std::shared_ptr<std::vector<client::YBTableInfo>>& info,
       const SetupReplicationInfo& setup_info, const Status& s);
 
@@ -2829,22 +2831,21 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
   void GetCDCStreamCallback(
       const xrepl::StreamId& bootstrap_id, std::shared_ptr<TableId> table_id,
       std::shared_ptr<std::unordered_map<std::string, std::string>> options,
-      const cdc::ReplicationGroupId& replication_group_id, const TableId& table,
+      const xcluster::ReplicationGroupId& replication_group_id, const TableId& table,
       std::shared_ptr<XClusterRpcTasks> xcluster_rpc, const Status& s,
       std::shared_ptr<StreamUpdateInfos> stream_update_infos,
       std::shared_ptr<std::mutex> update_infos_lock);
 
   void AddCDCStreamToUniverseAndInitConsumer(
-      const cdc::ReplicationGroupId& replication_group_id, const TableId& table,
+      const xcluster::ReplicationGroupId& replication_group_id, const TableId& table,
       const Result<xrepl::StreamId>& stream_id, std::function<void()> on_success_cb = nullptr);
 
   void MergeUniverseReplication(
-      scoped_refptr<UniverseReplicationInfo> info, cdc::ReplicationGroupId original_id);
+      scoped_refptr<UniverseReplicationInfo> info, xcluster::ReplicationGroupId original_id);
 
   Status DeleteUniverseReplicationUnlocked(scoped_refptr<UniverseReplicationInfo> info);
   Status DeleteUniverseReplication(
-      const cdc::ReplicationGroupId& replication_group_id,
-      bool ignore_errors,
+      const xcluster::ReplicationGroupId& replication_group_id, bool ignore_errors,
       DeleteUniverseReplicationResponsePB* resp);
 
   void MarkUniverseReplicationFailed(
@@ -2888,7 +2889,7 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
 
   // Maps replication group id to the corresponding xrepl stream for a table.
   using XClusterConsumerTableStreamIds =
-      std::unordered_map<cdc::ReplicationGroupId, xrepl::StreamId>;
+      std::unordered_map<xcluster::ReplicationGroupId, xrepl::StreamId>;
 
   // Gets the set of CDC stream info for an xCluster consumer table.
   XClusterConsumerTableStreamIds GetXClusterConsumerStreamIdsForTable(const TableId& table_id) const
@@ -2926,17 +2927,15 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
   Result<SysRowEntries> CollectEntriesForSequencesDataTable();
 
   Result<scoped_refptr<UniverseReplicationInfo>> CreateUniverseReplicationInfoForProducer(
-      const cdc::ReplicationGroupId& replication_group_id,
+      const xcluster::ReplicationGroupId& replication_group_id,
       const google::protobuf::RepeatedPtrField<HostPortPB>& master_addresses,
-      const google::protobuf::RepeatedPtrField<std::string>& table_ids,
-      bool transactional);
+      const google::protobuf::RepeatedPtrField<std::string>& table_ids, bool transactional);
 
   Result<scoped_refptr<UniverseReplicationBootstrapInfo>>
   CreateUniverseReplicationBootstrapInfoForProducer(
-      const cdc::ReplicationGroupId& replication_group_id,
+      const xcluster::ReplicationGroupId& replication_group_id,
       const google::protobuf::RepeatedPtrField<HostPortPB>& master_addresses,
-      const LeaderEpoch& epoch,
-      bool transactional);
+      const LeaderEpoch& epoch, bool transactional);
 
   void ProcessCDCParentTabletDeletionPeriodically();
 
@@ -2947,21 +2946,21 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
 
   // Populate the response with the errors for the given replication group.
   Status PopulateReplicationGroupErrors(
-      const cdc::ReplicationGroupId& replication_group_id,
+      const xcluster::ReplicationGroupId& replication_group_id,
       GetReplicationStatusResponsePB* resp) const
       REQUIRES_SHARED(mutex_, xcluster_consumer_replication_error_map_mutex_);
 
   // Update the UniverseReplicationInfo object when toggling replication.
   Status SetUniverseReplicationInfoEnabled(
-      const cdc::ReplicationGroupId& replication_group_id, bool is_enabled) EXCLUDES(mutex_);
+      const xcluster::ReplicationGroupId& replication_group_id, bool is_enabled) EXCLUDES(mutex_);
 
   // Update the cluster config and consumer registry objects when toggling replication.
   Status SetConsumerRegistryEnabled(
-      const cdc::ReplicationGroupId& replication_group_id, bool is_enabled,
+      const xcluster::ReplicationGroupId& replication_group_id, bool is_enabled,
       ClusterConfigInfo::WriteLock* l);
 
   void XClusterAddTableToNSReplication(
-      const cdc::ReplicationGroupId& replication_group_id, CoarseTimePoint deadline);
+      const xcluster::ReplicationGroupId& replication_group_id, CoarseTimePoint deadline);
 
   // Find the list of producer table IDs that can be added to the current NS-level replication.
   Status XClusterNSReplicationSyncWithProducer(
@@ -2979,7 +2978,7 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
   std::atomic<bool> namespace_replication_enabled_{false};
 
   Status WaitForSetupUniverseReplicationToFinish(
-      const cdc::ReplicationGroupId& replication_group_id, CoarseTimePoint deadline);
+      const xcluster::ReplicationGroupId& replication_group_id, CoarseTimePoint deadline);
 
   void RemoveTableFromCDCSDKUnprocessedMap(const TableId& table_id, const NamespaceId& ns_id);
 
@@ -3011,10 +3010,10 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
 
   void ScheduleAddTableToXClusterTaskForAllTables(const LeaderEpoch& epoch) EXCLUDES(mutex_);
 
-  Result<cdc::ReplicationGroupId> GetIndexesTableReplicationGroup(const TableInfo& index_info);
+  Result<xcluster::ReplicationGroupId> GetIndexesTableReplicationGroup(const TableInfo& index_info);
 
   Status BootstrapTable(
-      const cdc::ReplicationGroupId& replication_group_id, const TableInfo& index_info,
+      const xcluster::ReplicationGroupId& replication_group_id, const TableInfo& index_info,
       client::BootstrapProducerCallback callback);
 
   // Checks if the table is a consumer in an xCluster replication universe.
@@ -3146,19 +3145,19 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
   std::unordered_map<ReplicationSlotName, xrepl::StreamId> cdcsdk_replication_slots_to_stream_map_
       GUARDED_BY(mutex_);
 
-  typedef std::unordered_map<cdc::ReplicationGroupId, scoped_refptr<UniverseReplicationInfo>>
+  typedef std::unordered_map<xcluster::ReplicationGroupId, scoped_refptr<UniverseReplicationInfo>>
       UniverseReplicationInfoMap;
   UniverseReplicationInfoMap universe_replication_map_ GUARDED_BY(mutex_);
 
   // List of universe ids to universes that must be deleted
-  std::deque<cdc::ReplicationGroupId> universes_to_clear_ GUARDED_BY(mutex_);
+  std::deque<xcluster::ReplicationGroupId> universes_to_clear_ GUARDED_BY(mutex_);
 
   typedef std::unordered_map<
-      cdc::ReplicationGroupId, scoped_refptr<UniverseReplicationBootstrapInfo>>
+      xcluster::ReplicationGroupId, scoped_refptr<UniverseReplicationBootstrapInfo>>
       UniverseReplicationBootstrapInfoMap;
   UniverseReplicationBootstrapInfoMap universe_replication_bootstrap_map_ GUARDED_BY(mutex_);
 
-  std::deque<cdc::ReplicationGroupId> replication_bootstraps_to_clear_ GUARDED_BY(mutex_);
+  std::deque<xcluster::ReplicationGroupId> replication_bootstraps_to_clear_ GUARDED_BY(mutex_);
 
   // mutex on should_send_consumer_registry_mutex_.
   mutable simple_spinlock should_send_consumer_registry_mutex_;
@@ -3178,7 +3177,7 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
     CoarseTimePoint next_add_table_task_time = CoarseTimePoint::max();
     int num_accumulated_errors;
   };
-  std::unordered_map<cdc::ReplicationGroupId, NSReplicationInfo> namespace_replication_map_
+  std::unordered_map<xcluster::ReplicationGroupId, NSReplicationInfo> namespace_replication_map_
       GUARDED_BY(mutex_);
 
   std::atomic<bool> pg_catalog_versions_bg_task_running_ = {false};
@@ -3196,7 +3195,7 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
   std::unique_ptr<cdc::CDCStateTable> cdc_state_table_;
 
   mutable std::shared_mutex xcluster_consumer_replication_error_map_mutex_ ACQUIRED_AFTER(mutex_);
-  std::unordered_map<cdc::ReplicationGroupId, xcluster::ReplicationGroupErrors>
+  std::unordered_map<xcluster::ReplicationGroupId, xcluster::ReplicationGroupErrors>
       xcluster_consumer_replication_error_map_
           GUARDED_BY(xcluster_consumer_replication_error_map_mutex_);
 
diff --git a/src/yb/master/catalog_manager_if.h b/src/yb/master/catalog_manager_if.h
index 2330e5f8e7..df194d8125 100644
--- a/src/yb/master/catalog_manager_if.h
+++ b/src/yb/master/catalog_manager_if.h
@@ -13,7 +13,7 @@
 
 #pragma once
 
-#include "yb/cdc/cdc_fwd.h"
+#include "yb/cdc/xrepl_types.h"
 
 #include "yb/common/common_fwd.h"
 
diff --git a/src/yb/master/sys_catalog_xrepl-test.cc b/src/yb/master/sys_catalog_xrepl-test.cc
index c6874f30c9..4ea400dffd 100644
--- a/src/yb/master/sys_catalog_xrepl-test.cc
+++ b/src/yb/master/sys_catalog_xrepl-test.cc
@@ -63,7 +63,7 @@ class TestUniverseReplicationLoader : public Visitor<PersistentUniverseReplicati
       const SysUniverseReplicationEntryPB& metadata) override {
     // Setup the universe replication info.
     UniverseReplicationInfo* const universe =
-        new UniverseReplicationInfo(cdc::ReplicationGroupId(replication_group_id));
+        new UniverseReplicationInfo(xcluster::ReplicationGroupId(replication_group_id));
     auto l = universe->LockForWrite();
     l.mutable_data()->pb.CopyFrom(metadata);
     l.Commit();
@@ -116,7 +116,7 @@ TEST_F(SysCatalogTest, TestSysCatalogUniverseReplicationOperations) {
 
   // 1. CHECK ADD_UNIVERSE_REPLICATION.
   auto universe = make_scoped_refptr<UniverseReplicationInfo>(
-      cdc::ReplicationGroupId("deadbeafdeadbeafdeadbeafdeadbeaf"));
+      xcluster::ReplicationGroupId("deadbeafdeadbeafdeadbeafdeadbeaf"));
   {
     auto l = universe->LockForWrite();
     l.mutable_data()->pb.add_tables("producer_table_id");
diff --git a/src/yb/master/xcluster/add_table_to_xcluster_task.h b/src/yb/master/xcluster/add_table_to_xcluster_task.h
index 169e3d413a..79b64284fb 100644
--- a/src/yb/master/xcluster/add_table_to_xcluster_task.h
+++ b/src/yb/master/xcluster/add_table_to_xcluster_task.h
@@ -15,7 +15,7 @@
 
 #include <string>
 
-#include "yb/cdc/cdc_types.h"
+#include "yb/cdc/xcluster_types.h"
 #include "yb/client/client_fwd.h"
 #include "yb/common/hybrid_time.h"
 #include "yb/gutil/thread_annotations.h"
@@ -77,7 +77,7 @@ class AddTableToXClusterTask : public server::RunnableMonitoredTask {
   rpc::ScheduledTaskId reactor_task_id_ GUARDED_BY(schedule_task_mutex_) = rpc::kInvalidTaskId;
   HybridTime bootstrap_time_ = HybridTime::kInvalid;
   HybridTime initial_xcluster_safe_time_ = HybridTime::kInvalid;
-  cdc::ReplicationGroupId replication_group_id_;
+  xcluster::ReplicationGroupId replication_group_id_;
   LeaderEpoch epoch_;
 };
 }  // namespace yb::master
diff --git a/src/yb/master/xcluster/xcluster_catalog_entity.h b/src/yb/master/xcluster/xcluster_catalog_entity.h
index 8c3ba056a0..a9301edcbe 100644
--- a/src/yb/master/xcluster/xcluster_catalog_entity.h
+++ b/src/yb/master/xcluster/xcluster_catalog_entity.h
@@ -79,14 +79,15 @@ struct PersistentXClusterOutboundReplicationGroupInfo
 class XClusterOutboundReplicationGroupInfo
     : public MetadataCowWrapper<PersistentXClusterOutboundReplicationGroupInfo> {
  public:
-  explicit XClusterOutboundReplicationGroupInfo(const cdc::ReplicationGroupId& replication_group_id)
+  explicit XClusterOutboundReplicationGroupInfo(
+      const xcluster::ReplicationGroupId& replication_group_id)
       : replication_group_id_(replication_group_id) {}
 
   const std::string& id() const override { return replication_group_id_.ToString(); }
-  const cdc::ReplicationGroupId& ReplicationGroupId() const { return replication_group_id_; }
+  const xcluster::ReplicationGroupId& ReplicationGroupId() const { return replication_group_id_; }
 
  private:
-  const cdc::ReplicationGroupId replication_group_id_;
+  const xcluster::ReplicationGroupId replication_group_id_;
 };
 
 DECLARE_MULTI_INSTANCE_LOADER_CLASS(
diff --git a/src/yb/master/xcluster/xcluster_manager.cc b/src/yb/master/xcluster/xcluster_manager.cc
index 4987df4742..98693124f1 100644
--- a/src/yb/master/xcluster/xcluster_manager.cc
+++ b/src/yb/master/xcluster/xcluster_manager.cc
@@ -85,7 +85,7 @@ Status XClusterManager::RunLoaders() {
 Status XClusterManager::InsertOutboundReplicationGroup(
     const std::string& replication_group_id,
     const SysXClusterOutboundReplicationGroupEntryPB& metadata) {
-  cdc::ReplicationGroupId rg_id(replication_group_id);
+  xcluster::ReplicationGroupId rg_id(replication_group_id);
   std::lock_guard l(outbound_replication_group_map_mutex_);
 
   SCHECK(
@@ -101,7 +101,7 @@ Status XClusterManager::InsertOutboundReplicationGroup(
 }
 
 XClusterOutboundReplicationGroup XClusterManager::InitOutboundReplicationGroup(
-    const cdc::ReplicationGroupId& replication_group_id,
+    const xcluster::ReplicationGroupId& replication_group_id,
     const SysXClusterOutboundReplicationGroupEntryPB& metadata) {
   return XClusterOutboundReplicationGroup(
       replication_group_id, metadata, sys_catalog_,
@@ -122,13 +122,13 @@ XClusterOutboundReplicationGroup XClusterManager::InitOutboundReplicationGroup(
 }
 
 Result<XClusterOutboundReplicationGroup*> XClusterManager::GetOutboundReplicationGroup(
-    const cdc::ReplicationGroupId& replication_group_id) {
+    const xcluster::ReplicationGroupId& replication_group_id) {
   return const_cast<XClusterOutboundReplicationGroup*>(VERIFY_RESULT(
       const_cast<const XClusterManager*>(this)->GetOutboundReplicationGroup(replication_group_id)));
 }
 
 Result<const XClusterOutboundReplicationGroup*> XClusterManager::GetOutboundReplicationGroup(
-    const cdc::ReplicationGroupId& replication_group_id) const {
+    const xcluster::ReplicationGroupId& replication_group_id) const {
   auto outbound_replication_group =
       FindOrNull(outbound_replication_group_map_, replication_group_id);
   SCHECK(
@@ -322,7 +322,7 @@ Status XClusterManager::XClusterCreateOutboundReplicationGroup(
       "Replication group id cannot be empty");
   SCHECK(req->namespace_names_size() > 0, InvalidArgument, "Namespace names must be specified");
 
-  auto replication_group_id = cdc::ReplicationGroupId(req->replication_group_id());
+  auto replication_group_id = xcluster::ReplicationGroupId(req->replication_group_id());
 
   std::lock_guard l(outbound_replication_group_map_mutex_);
   SCHECK(
@@ -358,7 +358,7 @@ Status XClusterManager::XClusterAddNamespaceToOutboundReplicationGroup(
   LOG_FUNC_AND_RPC;
   SCHECK(req->has_namespace_name(), InvalidArgument, "Namespace name must be specified");
 
-  auto replication_group_id = cdc::ReplicationGroupId(req->replication_group_id());
+  auto replication_group_id = xcluster::ReplicationGroupId(req->replication_group_id());
   std::lock_guard l(outbound_replication_group_map_mutex_);
   auto outbound_replication_group =
       VERIFY_RESULT(GetOutboundReplicationGroup(replication_group_id));
@@ -377,7 +377,7 @@ Status XClusterManager::XClusterRemoveNamespaceFromOutboundReplicationGroup(
   LOG_FUNC_AND_RPC;
   SCHECK(req->has_namespace_id(), InvalidArgument, "Namespace id must be specified");
 
-  auto replication_group_id = cdc::ReplicationGroupId(req->replication_group_id());
+  auto replication_group_id = xcluster::ReplicationGroupId(req->replication_group_id());
 
   std::lock_guard l(outbound_replication_group_map_mutex_);
   auto outbound_replication_group =
@@ -392,7 +392,7 @@ Status XClusterManager::XClusterDeleteOutboundReplicationGroup(
     const LeaderEpoch& epoch) {
   LOG_FUNC_AND_RPC;
 
-  auto replication_group_id = cdc::ReplicationGroupId(req->replication_group_id());
+  auto replication_group_id = xcluster::ReplicationGroupId(req->replication_group_id());
 
   std::lock_guard l(outbound_replication_group_map_mutex_);
   auto outbound_replication_group =
@@ -411,7 +411,7 @@ Status XClusterManager::IsXClusterBootstrapRequired(
     rpc::RpcContext* rpc, const LeaderEpoch& epoch) {
   SCHECK(req->has_namespace_id(), InvalidArgument, "Namespace id must be specified");
 
-  auto replication_group_id = cdc::ReplicationGroupId(req->replication_group_id());
+  auto replication_group_id = xcluster::ReplicationGroupId(req->replication_group_id());
 
   SharedLock l(outbound_replication_group_map_mutex_);
   auto outbound_replication_group = VERIFY_RESULT(
@@ -434,7 +434,7 @@ Status XClusterManager::GetXClusterStreams(
     const GetXClusterStreamsRequestPB* req, GetXClusterStreamsResponsePB* resp,
     rpc::RpcContext* rpc, const LeaderEpoch& epoch) {
   SCHECK(req->has_namespace_id(), InvalidArgument, "Namespace id must be specified");
-  auto replication_group_id = cdc::ReplicationGroupId(req->replication_group_id());
+  auto replication_group_id = xcluster::ReplicationGroupId(req->replication_group_id());
 
   SharedLock l(outbound_replication_group_map_mutex_);
   auto outbound_replication_group = VERIFY_RESULT(
diff --git a/src/yb/master/xcluster/xcluster_manager.h b/src/yb/master/xcluster/xcluster_manager.h
index 895e1a948c..d34a464af7 100644
--- a/src/yb/master/xcluster/xcluster_manager.h
+++ b/src/yb/master/xcluster/xcluster_manager.h
@@ -135,15 +135,15 @@ class XClusterManager : public XClusterManagerIf {
       EXCLUDES(outbound_replication_group_map_mutex_);
 
   XClusterOutboundReplicationGroup InitOutboundReplicationGroup(
-      const cdc::ReplicationGroupId& replication_group_id,
+      const xcluster::ReplicationGroupId& replication_group_id,
       const SysXClusterOutboundReplicationGroupEntryPB& metadata);
 
   Result<XClusterOutboundReplicationGroup*> GetOutboundReplicationGroup(
-      const cdc::ReplicationGroupId& replication_group_id)
+      const xcluster::ReplicationGroupId& replication_group_id)
       REQUIRES(outbound_replication_group_map_mutex_);
 
   Result<const XClusterOutboundReplicationGroup*> GetOutboundReplicationGroup(
-      const cdc::ReplicationGroupId& replication_group_id) const
+      const xcluster::ReplicationGroupId& replication_group_id) const
       REQUIRES_SHARED(outbound_replication_group_map_mutex_);
 
   Master* const master_;
@@ -161,7 +161,7 @@ class XClusterManager : public XClusterManagerIf {
   XClusterSafeTimeInfo xcluster_safe_time_info_;
 
   mutable std::shared_mutex outbound_replication_group_map_mutex_;
-  std::map<cdc::ReplicationGroupId, XClusterOutboundReplicationGroup>
+  std::map<xcluster::ReplicationGroupId, XClusterOutboundReplicationGroup>
       outbound_replication_group_map_ GUARDED_BY(outbound_replication_group_map_mutex_);
 };
 
diff --git a/src/yb/master/xcluster/xcluster_outbound_replication_group.cc b/src/yb/master/xcluster/xcluster_outbound_replication_group.cc
index 3eb309ca42..8883f7d709 100644
--- a/src/yb/master/xcluster/xcluster_outbound_replication_group.cc
+++ b/src/yb/master/xcluster/xcluster_outbound_replication_group.cc
@@ -105,7 +105,7 @@ bool ShouldReplicateTable(const scoped_refptr<TableInfo>& table) {
 }  // namespace
 
 XClusterOutboundReplicationGroup::XClusterOutboundReplicationGroup(
-    const cdc::ReplicationGroupId& replication_group_id,
+    const xcluster::ReplicationGroupId& replication_group_id,
     const SysXClusterOutboundReplicationGroupEntryPB& outbound_replication_group_pb,
     SysCatalogTable* sys_catalog,
     std::function<Result<std::vector<scoped_refptr<TableInfo>>>(const NamespaceId&)>
diff --git a/src/yb/master/xcluster/xcluster_outbound_replication_group.h b/src/yb/master/xcluster/xcluster_outbound_replication_group.h
index 85bb8ec925..e63d5a7024 100644
--- a/src/yb/master/xcluster/xcluster_outbound_replication_group.h
+++ b/src/yb/master/xcluster/xcluster_outbound_replication_group.h
@@ -21,7 +21,7 @@ namespace master {
 class XClusterOutboundReplicationGroup {
  public:
   explicit XClusterOutboundReplicationGroup(
-      const cdc::ReplicationGroupId& replication_group_id,
+      const xcluster::ReplicationGroupId& replication_group_id,
       const SysXClusterOutboundReplicationGroupEntryPB& outbound_replication_group_pb,
       SysCatalogTable* sys_catalog,
       std::function<Result<std::vector<scoped_refptr<TableInfo>>>(const NamespaceId&)>
@@ -32,7 +32,7 @@ class XClusterOutboundReplicationGroup {
           const DeleteCDCStreamRequestPB&, const LeaderEpoch& epoch)>
           delete_cdc_stream_func);
 
-  const cdc::ReplicationGroupId& Id() const { return outbound_rg_info_->ReplicationGroupId(); }
+  const xcluster::ReplicationGroupId& Id() const { return outbound_rg_info_->ReplicationGroupId(); }
 
   std::string ToString() const { return Format("xClusterOutboundReplicationGroup $0", Id()); }
   std::string LogPrefix() const { return ToString(); }
diff --git a/src/yb/master/xcluster/xcluster_replication_group.cc b/src/yb/master/xcluster/xcluster_replication_group.cc
index 22f83f544e..2527d39ccb 100644
--- a/src/yb/master/xcluster/xcluster_replication_group.cc
+++ b/src/yb/master/xcluster/xcluster_replication_group.cc
@@ -49,7 +49,7 @@ Result<std::optional<std::pair<bool, uint32>>> ValidateAutoFlagsConfig(
 
 Result<cdc::ProducerEntryPB*> GetProducerEntry(
     SysClusterConfigEntryPB& cluster_config_pb,
-    const cdc::ReplicationGroupId& replication_group_id) {
+    const xcluster::ReplicationGroupId& replication_group_id) {
   auto producer_entry = FindOrNull(
       *cluster_config_pb.mutable_consumer_registry()->mutable_producer_map(),
       replication_group_id.ToString());
diff --git a/src/yb/master/xcluster_rpc_tasks.cc b/src/yb/master/xcluster_rpc_tasks.cc
index 240073bed7..e8e00dbb6f 100644
--- a/src/yb/master/xcluster_rpc_tasks.cc
+++ b/src/yb/master/xcluster_rpc_tasks.cc
@@ -12,14 +12,13 @@
 //
 
 #include "yb/master/xcluster_rpc_tasks.h"
-#include "yb/tools/yb-admin_util.h"
 
 #include "yb/client/client.h"
 #include "yb/client/yb_table_name.h"
 
-#include "yb/cdc/cdc_util.h"
+#include "yb/cdc/xcluster_util.h"
 
-#include "yb/gutil/bind.h"
+#include "yb/gutil/callback.h"
 
 #include "yb/master/master_backup.pb.h"
 #include "yb/master/master_client.pb.h"
@@ -64,7 +63,7 @@ namespace master {
 using yb::master::SysSnapshotEntryPB_State;
 
 Result<std::shared_ptr<XClusterRpcTasks>> XClusterRpcTasks::CreateWithMasterAddrs(
-    const cdc::ReplicationGroupId& replication_group_id, const std::string& master_addrs) {
+    const xcluster::ReplicationGroupId& replication_group_id, const std::string& master_addrs) {
   // NOTE: This is currently an expensive call (5+ sec). Encountered during Task #10611.
   auto xcluster_rpc_tasks = std::make_shared<XClusterRpcTasks>();
   std::string dir;
@@ -74,7 +73,7 @@ Result<std::shared_ptr<XClusterRpcTasks>> XClusterRpcTasks::CreateWithMasterAddr
     if (!FLAGS_certs_for_cdc_dir.empty()) {
       dir = JoinPathSegments(
           FLAGS_certs_for_cdc_dir,
-          cdc::GetOriginalReplicationGroupId(replication_group_id).ToString());
+          xcluster::GetOriginalReplicationGroupId(replication_group_id).ToString());
     }
     xcluster_rpc_tasks->secure_context_ = VERIFY_RESULT(server::SetupSecureContext(
         dir, "", "", server::SecureContextType::kInternal, &messenger_builder));
diff --git a/src/yb/master/xcluster_rpc_tasks.h b/src/yb/master/xcluster_rpc_tasks.h
index 1766a9cab4..a46735daa1 100644
--- a/src/yb/master/xcluster_rpc_tasks.h
+++ b/src/yb/master/xcluster_rpc_tasks.h
@@ -17,7 +17,7 @@
 
 #include <google/protobuf/repeated_field.h>
 
-#include "yb/cdc/cdc_types.h"
+#include "yb/cdc/xcluster_types.h"
 #include "yb/client/client_fwd.h"
 #include "yb/common/entity_ids.h"
 
@@ -52,7 +52,7 @@ typedef Callback<void(Result<TableBootstrapIdsMap>)> BootstrapProducerCallback;
 class XClusterRpcTasks {
  public:
   static Result<std::shared_ptr<XClusterRpcTasks>> CreateWithMasterAddrs(
-      const cdc::ReplicationGroupId& producer_id, const std::string& master_addrs);
+      const xcluster::ReplicationGroupId& producer_id, const std::string& master_addrs);
 
   ~XClusterRpcTasks();
 
diff --git a/src/yb/master/xrepl_catalog_manager.cc b/src/yb/master/xrepl_catalog_manager.cc
index 97fac4e69c..3eabfb4886 100644
--- a/src/yb/master/xrepl_catalog_manager.cc
+++ b/src/yb/master/xrepl_catalog_manager.cc
@@ -12,6 +12,7 @@
 
 #include "yb/cdc/cdc_service.h"
 #include "yb/cdc/cdc_state_table.h"
+#include "yb/cdc/xcluster_util.h"
 
 #include "yb/client/meta_cache.h"
 #include "yb/client/schema.h"
@@ -341,7 +342,7 @@ Status CatalogManager::LoadXReplStream() {
       auto& producer_map = l->pb.consumer_registry().producer_map();
       for (const auto& [replication_group_id, _] : producer_map) {
         SyncXClusterConsumerReplicationStatusMap(
-            cdc::ReplicationGroupId(replication_group_id), producer_map);
+            xcluster::ReplicationGroupId(replication_group_id), producer_map);
       }
     }
   }
@@ -388,16 +389,16 @@ class UniverseReplicationLoader : public Visitor<PersistentUniverseReplicationIn
   Status Visit(
       const std::string& replication_group_id_str, const SysUniverseReplicationEntryPB& metadata)
       REQUIRES(catalog_manager_->mutex_) {
-        const cdc::ReplicationGroupId replication_group_id(replication_group_id_str);
-        DCHECK(!ContainsKey(
-            catalog_manager_->universe_replication_map_,
-            cdc::ReplicationGroupId(replication_group_id)))
-            << "Producer universe already exists: " << replication_group_id;
-
-        // Setup the universe replication info.
-        scoped_refptr<UniverseReplicationInfo> const ri =
-            new UniverseReplicationInfo(replication_group_id);
-        {
+    const xcluster::ReplicationGroupId replication_group_id(replication_group_id_str);
+    DCHECK(!ContainsKey(
+        catalog_manager_->universe_replication_map_,
+        xcluster::ReplicationGroupId(replication_group_id)))
+        << "Producer universe already exists: " << replication_group_id;
+
+    // Setup the universe replication info.
+    scoped_refptr<UniverseReplicationInfo> const ri =
+        new UniverseReplicationInfo(replication_group_id);
+    {
       auto l = ri->LockForWrite();
       l.mutable_data()->pb.CopyFrom(metadata);
 
@@ -413,7 +414,8 @@ class UniverseReplicationLoader : public Visitor<PersistentUniverseReplicationIn
 
       // Add any failed universes to be cleared
       if (l->is_deleted_or_failed() || l->pb.state() == SysUniverseReplicationEntryPB::DELETING ||
-          cdc::IsAlterReplicationGroupId(cdc::ReplicationGroupId(l->pb.replication_group_id()))) {
+          xcluster::IsAlterReplicationGroupId(
+              xcluster::ReplicationGroupId(l->pb.replication_group_id()))) {
         catalog_manager_->universes_to_clear_.push_back(ri->ReplicationGroupId());
       }
 
@@ -480,10 +482,10 @@ class UniverseReplicationBootstrapLoader
   Status Visit(
       const std::string& replication_group_id_str,
       const SysUniverseReplicationBootstrapEntryPB& metadata) REQUIRES(catalog_manager_->mutex_) {
-    const cdc::ReplicationGroupId replication_group_id(replication_group_id_str);
+    const xcluster::ReplicationGroupId replication_group_id(replication_group_id_str);
     DCHECK(!ContainsKey(
         catalog_manager_->universe_replication_bootstrap_map_,
-        cdc::ReplicationGroupId(replication_group_id)))
+        xcluster::ReplicationGroupId(replication_group_id)))
         << "Producer universe already exists: " << replication_group_id;
 
     // Setup the universe replication info.
@@ -2382,10 +2384,9 @@ Status CatalogManager::IsBootstrapRequired(
 
 Result<scoped_refptr<UniverseReplicationInfo>>
 CatalogManager::CreateUniverseReplicationInfoForProducer(
-    const cdc::ReplicationGroupId& replication_group_id,
+    const xcluster::ReplicationGroupId& replication_group_id,
     const google::protobuf::RepeatedPtrField<HostPortPB>& master_addresses,
-    const google::protobuf::RepeatedPtrField<std::string>& table_ids,
-    bool transactional) {
+    const google::protobuf::RepeatedPtrField<std::string>& table_ids, bool transactional) {
   scoped_refptr<UniverseReplicationInfo> ri;
   {
     TRACE("Acquired catalog manager lock");
@@ -2450,7 +2451,7 @@ CatalogManager::CreateUniverseReplicationInfoForProducer(
 
 Result<scoped_refptr<UniverseReplicationBootstrapInfo>>
 CatalogManager::CreateUniverseReplicationBootstrapInfoForProducer(
-    const cdc::ReplicationGroupId& replication_group_id,
+    const xcluster::ReplicationGroupId& replication_group_id,
     const google::protobuf::RepeatedPtrField<HostPortPB>& master_addresses,
     const LeaderEpoch& epoch, bool transactional) {
   scoped_refptr<UniverseReplicationBootstrapInfo> bootstrap_info;
@@ -2715,7 +2716,8 @@ Status CatalogManager::ValidateReplicationBootstrapRequest(
 }
 
 void CatalogManager::DoReplicationBootstrap(
-    const cdc::ReplicationGroupId& replication_id, const std::vector<client::YBTableName>& tables,
+    const xcluster::ReplicationGroupId& replication_id,
+    const std::vector<client::YBTableName>& tables,
     Result<TableBootstrapIdsMap> bootstrap_producer_result) {
   // First get the universe.
   scoped_refptr<UniverseReplicationBootstrapInfo> bootstrap_info;
@@ -2803,7 +2805,7 @@ Status CatalogManager::SetupNamespaceReplicationWithBootstrap(
   RETURN_NOT_OK(ValidateReplicationBootstrapRequest(req));
 
   // Create entry in sys catalog.
-  auto replication_id = cdc::ReplicationGroupId(req->replication_id());
+  auto replication_id = xcluster::ReplicationGroupId(req->replication_id());
   auto transactional = req->has_transactional() ? req->transactional() : false;
   auto bootstrap_info = VERIFY_RESULT(CreateUniverseReplicationBootstrapInfoForProducer(
       replication_id, req->producer_master_addresses(), epoch, transactional));
@@ -2908,7 +2910,7 @@ Status CatalogManager::SetupUniverseReplication(
   }
 
   auto ri = VERIFY_RESULT(CreateUniverseReplicationInfoForProducer(
-      cdc::ReplicationGroupId(req->replication_group_id()), req->producer_master_addresses(),
+      xcluster::ReplicationGroupId(req->replication_group_id()), req->producer_master_addresses(),
       req->producer_table_ids(), setup_info.transactional));
 
   // Initialize the CDC Stream by querying the Producer server for RPC sanity checks.
@@ -3238,7 +3240,7 @@ Status CatalogManager::AddValidatedTableAndCreateCdcStreams(
 }
 
 void CatalogManager::GetTableSchemaCallback(
-    const cdc::ReplicationGroupId& replication_group_id,
+    const xcluster::ReplicationGroupId& replication_group_id,
     const std::shared_ptr<client::YBTableInfo>& producer_info,
     const SetupReplicationInfo& setup_info, const Status& s) {
   // First get the universe.
@@ -3307,10 +3309,9 @@ Status CatalogManager::ValidateTableAndCreateCdcStreams(
 }
 
 void CatalogManager::GetTablegroupSchemaCallback(
-    const cdc::ReplicationGroupId& replication_group_id,
+    const xcluster::ReplicationGroupId& replication_group_id,
     const std::shared_ptr<std::vector<client::YBTableInfo>>& infos,
-    const TablegroupId& producer_tablegroup_id,
-    const SetupReplicationInfo& setup_info,
+    const TablegroupId& producer_tablegroup_id, const SetupReplicationInfo& setup_info,
     const Status& s) {
   // First get the universe.
   scoped_refptr<UniverseReplicationInfo> universe;
@@ -3466,7 +3467,7 @@ void CatalogManager::GetTablegroupSchemaCallback(
 }
 
 void CatalogManager::GetColocatedTabletSchemaCallback(
-    const cdc::ReplicationGroupId& replication_group_id,
+    const xcluster::ReplicationGroupId& replication_group_id,
     const std::shared_ptr<std::vector<client::YBTableInfo>>& infos,
     const SetupReplicationInfo& setup_info, const Status& s) {
   // First get the universe.
@@ -3584,7 +3585,7 @@ void CatalogManager::GetColocatedTabletSchemaCallback(
 void CatalogManager::GetCDCStreamCallback(
     const xrepl::StreamId& bootstrap_id, std::shared_ptr<TableId> table_id,
     std::shared_ptr<std::unordered_map<std::string, std::string>> options,
-    const cdc::ReplicationGroupId& replication_group_id, const TableId& table,
+    const xcluster::ReplicationGroupId& replication_group_id, const TableId& table,
     std::shared_ptr<XClusterRpcTasks> xcluster_rpc, const Status& s,
     std::shared_ptr<StreamUpdateInfos> stream_update_infos,
     std::shared_ptr<std::mutex> update_infos_lock) {
@@ -3607,7 +3608,7 @@ void CatalogManager::GetCDCStreamCallback(
   {
     SharedLock lock(mutex_);
     original_universe = FindPtrOrNull(
-        universe_replication_map_, cdc::GetOriginalReplicationGroupId(replication_group_id));
+        universe_replication_map_, xcluster::GetOriginalReplicationGroupId(replication_group_id));
   }
 
   if (original_universe == nullptr) {
@@ -3660,7 +3661,7 @@ void CatalogManager::GetCDCStreamCallback(
 }
 
 void CatalogManager::AddCDCStreamToUniverseAndInitConsumer(
-    const cdc::ReplicationGroupId& replication_group_id, const TableId& table_id,
+    const xcluster::ReplicationGroupId& replication_group_id, const TableId& table_id,
     const Result<xrepl::StreamId>& stream_id, std::function<void()> on_success_cb) {
   scoped_refptr<UniverseReplicationInfo> universe;
   {
@@ -3740,7 +3741,7 @@ void CatalogManager::AddCDCStreamToUniverseAndInitConsumer(
             l.mutable_data()->pb.set_state(SysUniverseReplicationEntryPB::FAILED);
             universe->SetSetupUniverseReplicationErrorStatus(s);
           } else {
-            if (cdc::IsAlterReplicationGroupId(universe->ReplicationGroupId())) {
+            if (xcluster::IsAlterReplicationGroupId(universe->ReplicationGroupId())) {
               // Don't enable ALTER universes, merge them into the main universe instead.
               merge_alter = true;
             } else {
@@ -3764,7 +3765,7 @@ void CatalogManager::AddCDCStreamToUniverseAndInitConsumer(
   }
 
   if (validated_all_tables) {
-    auto final_id = cdc::GetOriginalReplicationGroupId(universe->ReplicationGroupId());
+    auto final_id = xcluster::GetOriginalReplicationGroupId(universe->ReplicationGroupId());
     // If this is an 'alter', merge back into primary command now that setup is a success.
     if (merge_alter) {
       MergeUniverseReplication(universe, final_id);
@@ -3915,7 +3916,7 @@ Status CatalogManager::InitXClusterConsumer(
   auto l = cluster_config->LockForWrite();
   auto* consumer_registry = l.mutable_data()->pb.mutable_consumer_registry();
   auto transactional = universe_l->pb.transactional();
-  if (!cdc::IsAlterReplicationGroupId(replication_info.ReplicationGroupId())) {
+  if (!xcluster::IsAlterReplicationGroupId(replication_info.ReplicationGroupId())) {
     consumer_registry->set_transactional(transactional);
   }
   for (const auto& stream_info : consumer_info) {
@@ -3984,7 +3985,7 @@ Status CatalogManager::InitXClusterConsumer(
 }
 
 void CatalogManager::MergeUniverseReplication(
-    scoped_refptr<UniverseReplicationInfo> universe, cdc::ReplicationGroupId original_id) {
+    scoped_refptr<UniverseReplicationInfo> universe, xcluster::ReplicationGroupId original_id) {
   // Merge back into primary command now that setup is a success.
   LOG(INFO) << "Merging CDC universe: " << universe->id() << " into " << original_id;
 
@@ -4054,7 +4055,8 @@ void CatalogManager::MergeUniverseReplication(
           "Updating universe replication entries and cluster config in sys-catalog");
     }
 
-    SyncXClusterConsumerReplicationStatusMap(cdc::ReplicationGroupId(original_universe->id()), *pm);
+    SyncXClusterConsumerReplicationStatusMap(
+        xcluster::ReplicationGroupId(original_universe->id()), *pm);
     alter_lock.Commit();
     cl.Commit();
     original_lock.Commit();
@@ -4072,7 +4074,7 @@ void CatalogManager::MergeUniverseReplication(
 }
 
 Status CatalogManager::DeleteUniverseReplication(
-    const cdc::ReplicationGroupId& replication_group_id, bool ignore_errors,
+    const xcluster::ReplicationGroupId& replication_group_id, bool ignore_errors,
     DeleteUniverseReplicationResponsePB* resp) {
   scoped_refptr<UniverseReplicationInfo> ri;
   {
@@ -4196,7 +4198,7 @@ Status CatalogManager::DeleteUniverseReplication(
   }
 
   RETURN_NOT_OK(DeleteUniverseReplication(
-      cdc::ReplicationGroupId(req->replication_group_id()), req->ignore_errors(), resp));
+      xcluster::ReplicationGroupId(req->replication_group_id()), req->ignore_errors(), resp));
   LOG(INFO) << "Successfully completed DeleteUniverseReplication request from "
             << RequestorString(rpc);
   return Status::OK();
@@ -4338,7 +4340,7 @@ Status CatalogManager::BootstrapProducer(
 }
 
 Status CatalogManager::SetUniverseReplicationInfoEnabled(
-    const cdc::ReplicationGroupId& replication_group_id, bool is_enabled) {
+    const xcluster::ReplicationGroupId& replication_group_id, bool is_enabled) {
   scoped_refptr<UniverseReplicationInfo> universe;
   {
     SharedLock lock(mutex_);
@@ -4379,7 +4381,7 @@ Status CatalogManager::SetUniverseReplicationInfoEnabled(
 }
 
 Status CatalogManager::SetConsumerRegistryEnabled(
-    const cdc::ReplicationGroupId& replication_group_id, bool is_enabled,
+    const xcluster::ReplicationGroupId& replication_group_id, bool is_enabled,
     ClusterConfigInfo::WriteLock* l) {
   // Modify the Consumer Registry, which will fan out this info to all TServers on heartbeat.
   {
@@ -4425,7 +4427,7 @@ Status CatalogManager::SetUniverseReplicationEnabled(
   auto cluster_config = ClusterConfig();
   auto l = cluster_config->LockForWrite();
   RETURN_NOT_OK(SetConsumerRegistryEnabled(
-      cdc::ReplicationGroupId(req->replication_group_id()), is_enabled, &l));
+      xcluster::ReplicationGroupId(req->replication_group_id()), is_enabled, &l));
   l.mutable_data()->pb.set_version(l.mutable_data()->pb.version() + 1);
   RETURN_NOT_OK(CheckStatus(
       sys_catalog_->Upsert(leader_ready_term(), cluster_config.get()),
@@ -4457,7 +4459,7 @@ Status CatalogManager::AlterUniverseReplication(
     SharedLock lock(mutex_);
 
     original_ri = FindPtrOrNull(
-        universe_replication_map_, cdc::ReplicationGroupId(req->replication_group_id()));
+        universe_replication_map_, xcluster::ReplicationGroupId(req->replication_group_id()));
     if (original_ri == nullptr) {
       return STATUS(
           NotFound, "Could not find CDC producer universe", req->ShortDebugString(),
@@ -4668,7 +4670,7 @@ Status CatalogManager::RemoveTablesFromReplication(
     }
 
     SyncXClusterConsumerReplicationStatusMap(
-        cdc::ReplicationGroupId(req->replication_group_id()), *pm);
+        xcluster::ReplicationGroupId(req->replication_group_id()), *pm);
 
     l.Commit();
     cl.Commit();
@@ -4677,7 +4679,7 @@ Status CatalogManager::RemoveTablesFromReplication(
     LockGuard lock(mutex_);
     for (const auto& table : consumer_table_ids_to_remove) {
       if (xcluster_consumer_table_stream_ids_map_[table].erase(
-              cdc::ReplicationGroupId(req->replication_group_id())) < 1) {
+              xcluster::ReplicationGroupId(req->replication_group_id())) < 1) {
         LOG(WARNING) << "Failed to remove consumer table from mapping. "
                      << "table_id: " << table
                      << ": replication_group_id: " << req->replication_group_id();
@@ -4696,8 +4698,8 @@ Status CatalogManager::AddTablesToReplication(
     AlterUniverseReplicationResponsePB* resp, rpc::RpcContext* rpc) {
   CHECK_GT(req->producer_table_ids_to_add_size() , 0);
 
-  cdc::ReplicationGroupId alter_replication_group_id(
-      cdc::GetAlterReplicationGroupId(req->replication_group_id()));
+  xcluster::ReplicationGroupId alter_replication_group_id(xcluster::GetAlterReplicationGroupId(
+      xcluster::ReplicationGroupId(req->replication_group_id())));
 
   // If user passed in bootstrap ids, check that there is a bootstrap id for every table.
   if (req->producer_bootstrap_ids_to_add().size() > 0 &&
@@ -4807,8 +4809,8 @@ Status CatalogManager::RenameUniverseReplication(
     scoped_refptr<UniverseReplicationInfo> universe, const AlterUniverseReplicationRequestPB* req) {
   CHECK(req->has_new_replication_group_id());
 
-  const cdc::ReplicationGroupId old_replication_group_id(universe->id());
-  const cdc::ReplicationGroupId new_replication_group_id(req->new_replication_group_id());
+  const xcluster::ReplicationGroupId old_replication_group_id(universe->id());
+  const xcluster::ReplicationGroupId new_replication_group_id(req->new_replication_group_id());
   if (old_replication_group_id == new_replication_group_id) {
     return STATUS(
         InvalidArgument, "Old and new replication ids must be different", req->ShortDebugString(),
@@ -4882,7 +4884,7 @@ Status CatalogManager::GetUniverseReplication(
     SharedLock lock(mutex_);
 
     universe = FindPtrOrNull(
-        universe_replication_map_, cdc::ReplicationGroupId(req->replication_group_id()));
+        universe_replication_map_, xcluster::ReplicationGroupId(req->replication_group_id()));
     if (universe == nullptr) {
       return STATUS(
           NotFound, "Could not find CDC producer universe", req->ShortDebugString(),
@@ -4912,8 +4914,8 @@ Status CatalogManager::IsSetupUniverseReplicationDone(
         InvalidArgument, "Producer universe ID must be provided", req->ShortDebugString(),
         MasterError(MasterErrorPB::INVALID_REQUEST));
   }
-  const cdc::ReplicationGroupId replication_group_id(req->replication_group_id());
-  bool is_alter_request = cdc::IsAlterReplicationGroupId(replication_group_id);
+  const xcluster::ReplicationGroupId replication_group_id(req->replication_group_id());
+  bool is_alter_request = xcluster::IsAlterReplicationGroupId(replication_group_id);
 
   GetUniverseReplicationRequestPB universe_req;
   GetUniverseReplicationResponsePB universe_resp;
@@ -4993,7 +4995,7 @@ Status CatalogManager::IsSetupNamespaceReplicationWithBootstrapDone(
       req->DebugString());
 
   SCHECK(req->has_replication_group_id(), InvalidArgument, "Replication group ID must be provided");
-  const cdc::ReplicationGroupId replication_group_id(req->replication_group_id());
+  const xcluster::ReplicationGroupId replication_group_id(req->replication_group_id());
 
   scoped_refptr<UniverseReplicationBootstrapInfo> bootstrap_info;
   {
@@ -5123,7 +5125,7 @@ Status CatalogManager::UpdateConsumerOnProducerSplit(
       "Updating cluster config in sys-catalog"));
 
   SyncXClusterConsumerReplicationStatusMap(
-      cdc::ReplicationGroupId(req->replication_group_id()), *replication_group_map);
+      xcluster::ReplicationGroupId(req->replication_group_id()), *replication_group_map);
   l.Commit();
 
   xcluster_manager_->CreateXClusterSafeTimeTableAndStartService();
@@ -5143,7 +5145,7 @@ Status CatalogManager::UpdateConsumerOnProducerMetadata(
     return Status::OK();
   }
 
-  const cdc::ReplicationGroupId replication_group_id(req->replication_group_id());
+  const xcluster::ReplicationGroupId replication_group_id(req->replication_group_id());
   const auto stream_id = VERIFY_RESULT(xrepl::StreamId::FromString(req->stream_id()));
 
   // Get corresponding local data for this stream.
@@ -5468,7 +5470,7 @@ Status CatalogManager::SetupNSUniverseReplication(
     HostPortsFromPBs(req->producer_master_addresses(), &hp);
     std::string producer_addrs = HostPort::ToCommaSeparatedString(hp);
     auto xcluster_rpc = VERIFY_RESULT(XClusterRpcTasks::CreateWithMasterAddrs(
-        cdc::ReplicationGroupId(req->replication_group_id()), producer_addrs));
+        xcluster::ReplicationGroupId(req->replication_group_id()), producer_addrs));
     producer_tables = VERIFY_RESULT(XClusterFindProducerConsumerOverlap(
         xcluster_rpc, &producer_namespace, &consumer_namespace, &num_non_matched_consumer_tables));
 
@@ -5510,7 +5512,7 @@ Status CatalogManager::SetupNSUniverseReplication(
   // TODO: Put all the following code in an async task to avoid this expensive wait.
   CoarseTimePoint deadline = rpc->GetClientDeadline();
   auto s = WaitForSetupUniverseReplicationToFinish(
-      cdc::ReplicationGroupId(req->replication_group_id()), deadline);
+      xcluster::ReplicationGroupId(req->replication_group_id()), deadline);
   if (!s.ok()) {
     return SetupError(resp->mutable_error(), s);
   }
@@ -5521,7 +5523,7 @@ Status CatalogManager::SetupNSUniverseReplication(
     SharedLock lock(mutex_);
     TRACE("Acquired catalog manager lock");
     universe = FindPtrOrNull(
-        universe_replication_map_, cdc::ReplicationGroupId(req->replication_group_id()));
+        universe_replication_map_, xcluster::ReplicationGroupId(req->replication_group_id()));
     if (universe == nullptr) {
       return STATUS(
           NotFound, "Could not find universe after SetupUniverseReplication",
@@ -5538,7 +5540,7 @@ Status CatalogManager::SetupNSUniverseReplication(
   {
     LockGuard lock(mutex_);
     auto& metadata =
-        namespace_replication_map_[cdc::ReplicationGroupId(req->replication_group_id())];
+        namespace_replication_map_[xcluster::ReplicationGroupId(req->replication_group_id())];
     if (num_non_matched_consumer_tables > 0) {
       // Start the periodic sync immediately.
       metadata.next_add_table_task_time =
@@ -5558,7 +5560,7 @@ Status CatalogManager::SetupNSUniverseReplication(
 
 // Sync xcluster_consumer_replication_error_map_ with the streams we have in our producer_map.
 void CatalogManager::SyncXClusterConsumerReplicationStatusMap(
-    const cdc::ReplicationGroupId& replication_group_id,
+    const xcluster::ReplicationGroupId& replication_group_id,
     const google::protobuf::Map<std::string, cdc::ProducerEntryPB>& producer_map) {
   std::lock_guard lock(xcluster_consumer_replication_error_map_mutex_);
 
@@ -5613,7 +5615,7 @@ void CatalogManager::StoreXClusterConsumerReplicationStatus(
   // xcluster_consumer_replication_error_map_.
 
   auto* replication_error_map = FindOrNull(
-      xcluster_consumer_replication_error_map_, cdc::ReplicationGroupId(replication_group_id));
+      xcluster_consumer_replication_error_map_, xcluster::ReplicationGroupId(replication_group_id));
   if (!replication_error_map) {
     VLOG_WITH_FUNC(2) << "Skipping deleted replication group " << replication_group_id;
     return;
@@ -5671,7 +5673,7 @@ Status CatalogManager::GetReplicationStatus(
   // ReplicationGroup. Otherwise, populate all the status for all groups.
   if (!req->replication_group_id().empty()) {
     return PopulateReplicationGroupErrors(
-        cdc::ReplicationGroupId(req->replication_group_id()), resp);
+        xcluster::ReplicationGroupId(req->replication_group_id()), resp);
   }
 
   for (const auto& [replication_id, _] : xcluster_consumer_replication_error_map_) {
@@ -5774,7 +5776,7 @@ Status CatalogManager::ReplicationSlotValidateName(const std::string& replicatio
 }
 
 Status CatalogManager::PopulateReplicationGroupErrors(
-    const cdc::ReplicationGroupId& replication_group_id,
+    const xcluster::ReplicationGroupId& replication_group_id,
     GetReplicationStatusResponsePB* resp) const {
   auto* replication_error_map =
       FindOrNull(xcluster_consumer_replication_error_map_, replication_group_id);
@@ -6126,7 +6128,7 @@ Status CatalogManager::ClearFailedUniverse() {
     return Status::OK();
   }
 
-  cdc::ReplicationGroupId replication_group_id;
+  xcluster::ReplicationGroupId replication_group_id;
   {
     LockGuard lock(mutex_);
 
@@ -6235,7 +6237,7 @@ Status CatalogManager::DoClearFailedReplicationBootstrap(
 }
 
 Status CatalogManager::ClearFailedReplicationBootstrap() {
-  cdc::ReplicationGroupId replication_id;
+  xcluster::ReplicationGroupId replication_id;
   {
     LockGuard lock(mutex_);
 
@@ -6626,7 +6628,7 @@ void CatalogManager::ScheduleXClusterNSReplicationAddTableTask() {
 }
 
 void CatalogManager::XClusterAddTableToNSReplication(
-    const cdc::ReplicationGroupId& replication_group_id, CoarseTimePoint deadline) {
+    const xcluster::ReplicationGroupId& replication_group_id, CoarseTimePoint deadline) {
   // TODO: In ScopeExit, find a way to report non-OK task_status to user.
   bool has_non_replicated_consumer_table = true;
   Status task_status = Status::OK();
@@ -6713,7 +6715,7 @@ void CatalogManager::XClusterAddTableToNSReplication(
 
   // 3. Wait for AlterUniverseReplication to finish.
   task_status = WaitForSetupUniverseReplicationToFinish(
-      cdc::GetAlterReplicationGroupId(replication_group_id), deadline);
+      xcluster::GetAlterReplicationGroupId(replication_group_id), deadline);
   if (!task_status.ok()) {
     LOG_WITH_FUNC(WARNING) << "Error while waiting for AlterUniverseReplication on "
                            << replication_group_id << " to complete: " << task_status;
@@ -6847,7 +6849,7 @@ Result<std::vector<TableId>> CatalogManager::XClusterFindProducerConsumerOverlap
 }
 
 Status CatalogManager::WaitForSetupUniverseReplicationToFinish(
-    const cdc::ReplicationGroupId& replication_group_id, CoarseTimePoint deadline) {
+    const xcluster::ReplicationGroupId& replication_group_id, CoarseTimePoint deadline) {
   while (true) {
     if (deadline - CoarseMonoClock::Now() <= 1ms) {
       return STATUS(TimedOut, "Timed out while waiting for SetupUniverseReplication to finish");
@@ -7008,7 +7010,7 @@ bool CatalogManager::ShouldAddTableToXClusterReplication(
   return true;
 }
 
-Result<cdc::ReplicationGroupId> CatalogManager::GetIndexesTableReplicationGroup(
+Result<xcluster::ReplicationGroupId> CatalogManager::GetIndexesTableReplicationGroup(
     const TableInfo& index_info) {
   const auto indexed_table_id = GetIndexedTableId(index_info.LockForRead()->pb);
   SCHECK(!indexed_table_id.empty(), IllegalState, "Indexed table id is empty");
@@ -7024,7 +7026,7 @@ Result<cdc::ReplicationGroupId> CatalogManager::GetIndexesTableReplicationGroup(
 }
 
 Status CatalogManager::BootstrapTable(
-    const cdc::ReplicationGroupId& replication_group_id, const TableInfo& table_info,
+    const xcluster::ReplicationGroupId& replication_group_id, const TableInfo& table_info,
     client::BootstrapProducerCallback callback) {
   VLOG_WITH_FUNC(1) << "Bootstrapping table " << table_info.ToString();
 
@@ -7050,7 +7052,7 @@ Status CatalogManager::BootstrapTable(
 }
 
 Status CatalogManager::RemoveTableFromXcluster(const vector<TabletId>& table_ids) {
-  std::map<cdc::ReplicationGroupId, std::unordered_set<TableId>> replication_group_tables_map;
+  std::map<xcluster::ReplicationGroupId, std::unordered_set<TableId>> replication_group_tables_map;
   {
     auto cluster_config = ClusterConfig();
     auto l = cluster_config->LockForRead();
@@ -7244,7 +7246,7 @@ Status CatalogManager::XClusterReportNewAutoFlagConfigVersion(
     const LeaderEpoch& epoch) {
   LOG_WITH_FUNC(INFO) << " from " << RequestorString(rpc) << ": " << req->DebugString();
 
-  const cdc::ReplicationGroupId replication_group_id(req->replication_group_id());
+  const xcluster::ReplicationGroupId replication_group_id(req->replication_group_id());
   const auto new_version = req->auto_flag_config_version();
 
   // Verify that there is an existing Universe config
@@ -7281,7 +7283,7 @@ Status CatalogManager::XClusterRefreshLocalAutoFlagConfig(const LeaderEpoch& epo
   });
   xcluster_auto_flags_revalidation_needed_ = false;
 
-  std::vector<cdc::ReplicationGroupId> replication_group_ids;
+  std::vector<xcluster::ReplicationGroupId> replication_group_ids;
   bool update_failed = false;
   {
     SharedLock lock(mutex_);
diff --git a/src/yb/tools/yb-admin_client.cc b/src/yb/tools/yb-admin_client.cc
index 38105f376a..3f37938169 100644
--- a/src/yb/tools/yb-admin_client.cc
+++ b/src/yb/tools/yb-admin_client.cc
@@ -45,6 +45,7 @@
 #include <gtest/gtest.h>
 
 #include "yb/cdc/cdc_service.h"
+#include "yb/cdc/xcluster_util.h"
 #include "yb/client/client.h"
 #include "yb/client/table.h"
 #include "yb/client/table_creator.h"
@@ -4212,7 +4213,8 @@ Status ClusterAdminClient::AlterUniverseReplication(
         // If we are adding tables, then wait for the altered producer to be deleted (this happens
         // once it is merged with the original).
         RETURN_NOT_OK(WaitForSetupUniverseReplicationToFinish(
-            cdc::GetAlterReplicationGroupId(replication_group_id).ToString()));
+            xcluster::GetAlterReplicationGroupId(xcluster::ReplicationGroupId(replication_group_id))
+                .ToString()));
   }
 
   cout << "Replication altered successfully" << endl;
diff --git a/src/yb/tools/yb-admin_client.h b/src/yb/tools/yb-admin_client.h
index c9bfa2da77..7bb0902147 100644
--- a/src/yb/tools/yb-admin_client.h
+++ b/src/yb/tools/yb-admin_client.h
@@ -36,6 +36,7 @@
 
 #include <boost/optional.hpp>
 
+#include "yb/cdc/cdc_service.pb.h"
 #include "yb/client/client.h"
 #include "yb/client/yb_table_name.h"
 
diff --git a/src/yb/tools/yb-backup/yb-backup-test_base.cc b/src/yb/tools/yb-backup/yb-backup-test_base.cc
index deb6f6cd19..df358fd654 100644
--- a/src/yb/tools/yb-backup/yb-backup-test_base.cc
+++ b/src/yb/tools/yb-backup/yb-backup-test_base.cc
@@ -12,6 +12,7 @@
 // under the License.
 
 #include "yb/common/redis_constants_common.h"
+#include "yb/common/redis_protocol.pb.h"
 
 #include "yb/client/session.h"
 #include "yb/client/table.h"
@@ -23,7 +24,7 @@
 #include "yb/master/master_backup.pb.h"
 #include "yb/master/master_client.pb.h"
 #include "yb/master/master_admin.proxy.h"
-
+#include "yb/tserver/tserver_service.pb.h"
 #include "yb/tools/yb-backup/yb-backup-test_base.h"
 
 #include "yb/util/backoff_waiter.h"
diff --git a/src/yb/tserver/pg_client_service.cc b/src/yb/tserver/pg_client_service.cc
index f3635c81c4..d9d4996432 100644
--- a/src/yb/tserver/pg_client_service.cc
+++ b/src/yb/tserver/pg_client_service.cc
@@ -41,6 +41,7 @@
 #include "yb/common/wire_protocol.h"
 
 #include "yb/master/master_admin.proxy.h"
+#include "yb/master/master_client.pb.h"
 #include "yb/master/master_heartbeat.pb.h"
 #include "yb/master/sys_catalog_constants.h"
 
diff --git a/src/yb/tserver/tablet_server.h b/src/yb/tserver/tablet_server.h
index 09b96dc861..0b3699e5d0 100644
--- a/src/yb/tserver/tablet_server.h
+++ b/src/yb/tserver/tablet_server.h
@@ -45,7 +45,7 @@
 #include "yb/common/common_util.h"
 #include "yb/common/pg_catversions.h"
 #include "yb/consensus/metadata.pb.h"
-#include "yb/cdc/cdc_fwd.h"
+#include "yb/cdc/xrepl_types.h"
 #include "yb/cdc/cdc_consumer.fwd.h"
 #include "yb/client/client_fwd.h"
 #include "yb/rpc/rpc_fwd.h"
@@ -80,6 +80,12 @@ class Env;
 class MaintenanceManager;
 class AutoFlagsManager;
 
+namespace cdc {
+
+class CDCServiceImpl;
+
+}
+
 namespace tserver {
 
 class PgClientServiceImpl;
diff --git a/src/yb/tserver/xcluster_consumer.cc b/src/yb/tserver/xcluster_consumer.cc
index d3878330f4..cb3cd6af34 100644
--- a/src/yb/tserver/xcluster_consumer.cc
+++ b/src/yb/tserver/xcluster_consumer.cc
@@ -11,9 +11,9 @@
 // under the License.
 //
 
-#include <shared_mutex>
 #include <chrono>
 
+#include "yb/cdc/xcluster_util.h"
 #include "yb/client/session.h"
 #include "yb/client/table_handle.h"
 #include "yb/client/yb_op.h"
@@ -35,7 +35,6 @@
 #include "yb/tserver/xcluster_poller.h"
 
 #include "yb/cdc/cdc_consumer.pb.h"
-#include "yb/cdc/cdc_types.h"
 
 #include "yb/client/error.h"
 #include "yb/client/client.h"
@@ -101,7 +100,6 @@ using namespace std::chrono_literals;
 namespace yb {
 
 namespace tserver {
-using cdc::ProducerTabletInfo;
 
 XClusterClient::~XClusterClient() {
   if (messenger) {
@@ -170,8 +168,7 @@ XClusterConsumer::XClusterConsumer(
       &FLAGS_apply_changes_max_send_rate_mbps, "xclusterConsumerRateLimiter",
       std::bind(&XClusterConsumer::SetRateLimiterSpeed, this)));
 
-  auto_flags_version_handler_ =
-      std::make_unique<xcluster::AutoFlagsVersionHandler>(local_client_->client);
+  auto_flags_version_handler_ = std::make_unique<AutoFlagsVersionHandler>(local_client_->client);
 }
 
 XClusterConsumer::~XClusterConsumer() {
@@ -340,7 +337,7 @@ void XClusterConsumer::HandleMasterHeartbeatResponse(
 
   for (const auto& [replication_group_id_str, producer_entry_pb] :
        DCHECK_NOTNULL(consumer_registry)->producer_map()) {
-    const cdc::ReplicationGroupId replication_group_id(replication_group_id_str);
+    const xcluster::ReplicationGroupId replication_group_id(replication_group_id_str);
 
     std::vector<HostPort> hp;
     HostPortsFromPBs(producer_entry_pb.master_addrs(), &hp);
@@ -360,7 +357,7 @@ void XClusterConsumer::HandleMasterHeartbeatResponse(
 
 // NOTE: This happens on TS.heartbeat, so it needs to finish quickly
 void XClusterConsumer::UpdateReplicationGroupInMemState(
-    const cdc::ReplicationGroupId& replication_group_id,
+    const xcluster::ReplicationGroupId& replication_group_id,
     const yb::cdc::ProducerEntryPB& producer_entry_pb) {
   auto_flags_version_handler_->InsertOrUpdate(
       replication_group_id, producer_entry_pb.compatible_auto_flag_config_version(),
@@ -430,7 +427,7 @@ void XClusterConsumer::UpdateReplicationGroupInMemState(
     for (const auto& [consumer_tablet_id, producer_tablet_list] :
          stream_entry_pb.consumer_producer_tablet_map()) {
       for (const auto& producer_tablet_id : producer_tablet_list.tablets()) {
-        auto xCluster_tablet_info = cdc::XClusterTabletInfo{
+        auto xCluster_tablet_info = xcluster::XClusterTabletInfo{
             .producer_tablet_info = {replication_group_id, stream_id, producer_tablet_id},
             .consumer_tablet_info = {consumer_tablet_id, stream_entry_pb.consumer_table_id()},
             .disable_stream = producer_entry_pb.disable_stream()};
@@ -513,7 +510,7 @@ void XClusterConsumer::TriggerPollForNewTablets() {
             if (!FLAGS_certs_for_cdc_dir.empty()) {
               dir = JoinPathSegments(
                   FLAGS_certs_for_cdc_dir,
-                  cdc::GetOriginalReplicationGroupId(replication_group_id).ToString());
+                  xcluster::GetOriginalReplicationGroupId(replication_group_id).ToString());
             }
 
             auto secure_context_result = server::SetupSecureContext(
@@ -621,7 +618,7 @@ void XClusterConsumer::TriggerDeletionOfOldPollers() {
     ACQUIRE_SHARED_LOCK_IF_ONLINE;
     std::lock_guard write_lock_pollers(pollers_map_mutex_);
     for (auto it = pollers_map_.cbegin(); it != pollers_map_.cend();) {
-      const ProducerTabletInfo producer_info = it->first;
+      const xcluster::ProducerTabletInfo producer_info = it->first;
       const std::shared_ptr<XClusterPoller> poller = it->second;
       // Check if we need to delete this poller.
       std::string reason;
@@ -630,7 +627,7 @@ void XClusterConsumer::TriggerDeletionOfOldPollers() {
         continue;
       }
 
-      const cdc::ConsumerTabletInfo& consumer_info = poller->GetConsumerTabletInfo();
+      const xcluster::ConsumerTabletInfo& consumer_info = poller->GetConsumerTabletInfo();
 
       LOG_WITH_PREFIX(INFO) << Format(
           "Stop polling for producer tablet $0, consumer tablet $1. Reason: $2.", producer_info,
@@ -669,7 +666,7 @@ void XClusterConsumer::TriggerDeletionOfOldPollers() {
 }
 
 bool XClusterConsumer::ShouldContinuePolling(
-    const ProducerTabletInfo producer_tablet_info, const XClusterPoller& poller,
+    const xcluster::ProducerTabletInfo producer_tablet_info, const XClusterPoller& poller,
     std::string& reason) {
   if (FLAGS_TEST_xcluster_disable_delete_old_pollers) {
     return true;
@@ -722,7 +719,7 @@ Status XClusterConsumer::ReloadCertificates() {
     if (!FLAGS_certs_for_cdc_dir.empty()) {
       cert_dir = JoinPathSegments(
           FLAGS_certs_for_cdc_dir,
-          cdc::GetOriginalReplicationGroupId(replication_group_id).ToString());
+          xcluster::GetOriginalReplicationGroupId(replication_group_id).ToString());
     }
     RETURN_NOT_OK(server::ReloadSecureContextKeysAndCertificates(
         client->secure_context.get(), cert_dir, "" /* node_name */));
@@ -764,7 +761,8 @@ Status XClusterConsumer::PublishXClusterSafeTime() {
     safe_time_table_.swap(table);
   }
 
-  std::unordered_map<ProducerTabletInfo, HybridTime, ProducerTabletInfo::Hash> safe_time_map;
+  std::unordered_map<xcluster::ProducerTabletInfo, HybridTime, xcluster::ProducerTabletInfo::Hash>
+      safe_time_map;
 
   {
     SharedLock read_lock(pollers_map_mutex_);
@@ -811,7 +809,8 @@ void XClusterConsumer::StoreReplicationError(
 // This happens on TS.heartbeat request, so it needs to finish quickly.
 void XClusterConsumer::PopulateMasterHeartbeatRequest(
     master::TSHeartbeatRequestPB* req, bool needs_full_tablet_report) {
-  // Map of ReplicationGroupId, consumer TableId, producer TabletId to consumer term and error.
+  // Map of ReplicationGroupId, consumer TableId, producer TabletId to consumer term and
+  // error.
   auto errors_to_send = error_collector_.GetErrorsToSend(needs_full_tablet_report);
 
   for (const auto& [replication_group_id, table_map] : errors_to_send) {
@@ -831,7 +830,7 @@ void XClusterConsumer::PopulateMasterHeartbeatRequest(
 }
 
 Status XClusterConsumer::ReportNewAutoFlagConfigVersion(
-    const cdc::ReplicationGroupId& replication_group_id, uint32_t new_version) const {
+    const xcluster::ReplicationGroupId& replication_group_id, uint32_t new_version) const {
   return auto_flags_version_handler_->ReportNewAutoFlagConfigVersion(
       replication_group_id, new_version);
 }
diff --git a/src/yb/tserver/xcluster_consumer.h b/src/yb/tserver/xcluster_consumer.h
index bf7cf344b4..eb7d94220b 100644
--- a/src/yb/tserver/xcluster_consumer.h
+++ b/src/yb/tserver/xcluster_consumer.h
@@ -29,12 +29,12 @@
 #include "yb/cdc/xcluster_types.h"
 #include "yb/client/client_fwd.h"
 #include "yb/common/common_types.pb.h"
-#include "yb/tablet/tablet_types.pb.h"
 
 #include "yb/tserver/xcluster_consumer_if.h"
 #include "yb/tserver/xcluster_consumer_replication_error.h"
 #include "yb/tserver/xcluster_poller_stats.h"
 
+#include "yb/gutil/ref_counted.h"
 #include "yb/util/flags/flags_callback.h"
 #include "yb/util/locks.h"
 #include "yb/util/monotime.h"
@@ -62,13 +62,10 @@ class TSHeartbeatRequestPB;
 }  // namespace master
 
 namespace tserver {
+class AutoFlagsVersionHandler;
 class XClusterPoller;
 class TabletServer;
 
-namespace xcluster {
-class AutoFlagsVersionHandler;
-}  // namespace xcluster
-
 struct XClusterClient {
   std::unique_ptr<rpc::Messenger> messenger;
   std::unique_ptr<rpc::SecureContext> secure_context;
@@ -125,7 +122,7 @@ class XClusterConsumer : public XClusterConsumerIf {
   void StoreReplicationError(const XClusterPollerId& poller_id, ReplicationErrorPb error);
 
   Status ReportNewAutoFlagConfigVersion(
-      const cdc::ReplicationGroupId& replication_group_id, uint32_t new_version) const;
+      const xcluster::ReplicationGroupId& replication_group_id, uint32_t new_version) const;
 
  private:
   // Runs a thread that periodically polls for any new threads.
@@ -134,7 +131,7 @@ class XClusterConsumer : public XClusterConsumerIf {
   // Loops through all the entries in the registry and creates a producer -> consumer tablet
   // mapping.
   void UpdateReplicationGroupInMemState(
-      const cdc::ReplicationGroupId& replication_group_id,
+      const xcluster::ReplicationGroupId& replication_group_id,
       const yb::cdc::ProducerEntryPB& producer_entry_pb) REQUIRES(master_data_mutex_);
 
   // Loops through all entries in registry from master to check if all producer tablets are being
@@ -145,14 +142,14 @@ class XClusterConsumer : public XClusterConsumerIf {
   void TriggerDeletionOfOldPollers() EXCLUDES (master_data_mutex_);
 
   bool ShouldContinuePolling(
-      const cdc::ProducerTabletInfo producer_tablet_info, const XClusterPoller& poller,
+      const xcluster::ProducerTabletInfo producer_tablet_info, const XClusterPoller& poller,
       std::string& reason) REQUIRES_SHARED(master_data_mutex_);
 
   void UpdatePollerSchemaVersionMaps(
       std::shared_ptr<XClusterPoller> xcluster_poller, const xrepl::StreamId& stream_id) const
       REQUIRES_SHARED(master_data_mutex_);
 
-  void RemoveFromPollersMap(const cdc::ProducerTabletInfo producer_tablet_info);
+  void RemoveFromPollersMap(const xcluster::ProducerTabletInfo producer_tablet_info);
 
   void SetRateLimiterSpeed();
 
@@ -170,23 +167,17 @@ class XClusterConsumer : public XClusterConsumerIf {
   std::function<int64_t(const TabletId&)> get_leader_term_func_;
 
   class TabletTag;
-  using ProducerConsumerTabletMap = boost::multi_index_container <
-    cdc::XClusterTabletInfo,
-    boost::multi_index::indexed_by <
-      boost::multi_index::hashed_unique <
-          boost::multi_index::member <
-              cdc::XClusterTabletInfo, cdc::ProducerTabletInfo,
-              &cdc::XClusterTabletInfo::producer_tablet_info>
-      >,
-      boost::multi_index::hashed_non_unique <
-          boost::multi_index::tag <TabletTag>,
-          boost::multi_index::const_mem_fun <
-              cdc::XClusterTabletInfo, const TabletId&,
-              &cdc::XClusterTabletInfo::producer_tablet_id
-          >
-      >
-    >
-  >;
+  using ProducerConsumerTabletMap = boost::multi_index_container<
+      xcluster::XClusterTabletInfo,
+      boost::multi_index::indexed_by<
+          boost::multi_index::hashed_unique<boost::multi_index::member<
+              xcluster::XClusterTabletInfo, xcluster::ProducerTabletInfo,
+              &xcluster::XClusterTabletInfo::producer_tablet_info>>,
+          boost::multi_index::hashed_non_unique<
+              boost::multi_index::tag<TabletTag>,
+              boost::multi_index::const_mem_fun<
+                  xcluster::XClusterTabletInfo, const TabletId&,
+                  &xcluster::XClusterTabletInfo::producer_tablet_id>>>>;
 
   ProducerConsumerTabletMap producer_consumer_tablet_map_from_master_
       GUARDED_BY(master_data_mutex_);
@@ -212,7 +203,8 @@ class XClusterConsumer : public XClusterConsumerIf {
   scoped_refptr<Thread> run_trigger_poll_thread_;
 
   std::unordered_map<
-      cdc::ProducerTabletInfo, std::shared_ptr<XClusterPoller>, cdc::ProducerTabletInfo::Hash>
+      xcluster::ProducerTabletInfo, std::shared_ptr<XClusterPoller>,
+      xcluster::ProducerTabletInfo::Hash>
       pollers_map_ GUARDED_BY(pollers_map_mutex_);
 
   std::unique_ptr<ThreadPool> thread_pool_;
@@ -222,11 +214,12 @@ class XClusterConsumer : public XClusterConsumerIf {
   std::shared_ptr<XClusterClient> local_client_;
 
   // map: {replication_group_id : ...}.
-  std::unordered_map<cdc::ReplicationGroupId, std::shared_ptr<XClusterClient>> remote_clients_
+  std::unordered_map<xcluster::ReplicationGroupId, std::shared_ptr<XClusterClient>> remote_clients_
       GUARDED_BY(pollers_map_mutex_);
-  std::unordered_map<cdc::ReplicationGroupId, std::string> uuid_master_addrs_
+  std::unordered_map<xcluster::ReplicationGroupId, std::string> uuid_master_addrs_
+      GUARDED_BY(master_data_mutex_);
+  std::unordered_set<xcluster::ReplicationGroupId> changed_master_addrs_
       GUARDED_BY(master_data_mutex_);
-  std::unordered_set<cdc::ReplicationGroupId> changed_master_addrs_ GUARDED_BY(master_data_mutex_);
 
   std::atomic<int32_t> cluster_config_version_ GUARDED_BY(master_data_mutex_) = {-1};
   std::atomic<cdc::XClusterRole> consumer_role_ = cdc::XClusterRole::ACTIVE;
@@ -248,7 +241,7 @@ class XClusterConsumer : public XClusterConsumerIf {
   std::unique_ptr<rocksdb::RateLimiter> rate_limiter_;
   FlagCallbackRegistration rate_limiter_callback_;
 
-  std::unique_ptr<xcluster::AutoFlagsVersionHandler> auto_flags_version_handler_;
+  std::unique_ptr<AutoFlagsVersionHandler> auto_flags_version_handler_;
 };
 
 } // namespace tserver
diff --git a/src/yb/tserver/xcluster_consumer_auto_flags_info-test.cc b/src/yb/tserver/xcluster_consumer_auto_flags_info-test.cc
index bb8c8864b3..788335302b 100644
--- a/src/yb/tserver/xcluster_consumer_auto_flags_info-test.cc
+++ b/src/yb/tserver/xcluster_consumer_auto_flags_info-test.cc
@@ -17,10 +17,10 @@
 
 using namespace std::chrono_literals;
 
-namespace yb::tserver::xcluster {
+namespace yb::tserver {
 
-const cdc::ReplicationGroupId kReplicationGroupId1("Group1");
-const cdc::ReplicationGroupId kReplicationGroupId2("Group2");
+const xcluster::ReplicationGroupId kReplicationGroupId1("Group1");
+const xcluster::ReplicationGroupId kReplicationGroupId2("Group2");
 
 class AutoFlagsVersionHandlerTest : public AutoFlagsVersionHandler {
  public:
@@ -30,7 +30,7 @@ class AutoFlagsVersionHandlerTest : public AutoFlagsVersionHandler {
 
  protected:
   Status XClusterReportNewAutoFlagConfigVersion(
-      const cdc::ReplicationGroupId& replication_group_id, uint32 new_version) const override {
+      const xcluster::ReplicationGroupId& replication_group_id, uint32 new_version) const override {
     if (report_auto_flags_version_callback_) {
       return report_auto_flags_version_callback_();
     }
@@ -189,4 +189,4 @@ TEST(AutoFlagsVersionHandlerTest, NonBlockingReporting) {
   ASSERT_EQ(handler.GetAutoFlagsCompatibleVersion(kReplicationGroupId1)->max_reported_version_, 3);
 }
 
-}  // namespace yb::tserver::xcluster
+}  // namespace yb::tserver
diff --git a/src/yb/tserver/xcluster_consumer_auto_flags_info.cc b/src/yb/tserver/xcluster_consumer_auto_flags_info.cc
index c48c4cbef2..778039b14f 100644
--- a/src/yb/tserver/xcluster_consumer_auto_flags_info.cc
+++ b/src/yb/tserver/xcluster_consumer_auto_flags_info.cc
@@ -16,7 +16,7 @@
 #include "yb/gutil/map-util.h"
 #include "yb/util/shared_lock.h"
 
-namespace yb::tserver::xcluster {
+namespace yb::tserver {
 
 AutoFlagsCompatibleVersion::AutoFlagsCompatibleVersion(uint32 compatible_version)
     : compatible_version_(compatible_version) {}
@@ -40,7 +40,7 @@ AutoFlagsVersionHandler::AutoFlagsVersionHandler(std::shared_ptr<client::YBClien
     : client_(std::move(client)) {}
 
 void AutoFlagsVersionHandler::InsertOrUpdate(
-    const cdc::ReplicationGroupId& replication_group_id, uint32 compatible_version,
+    const xcluster::ReplicationGroupId& replication_group_id, uint32 compatible_version,
     uint32 max_reported_version) {
   std::lock_guard l(mutex_);
   auto& auto_flags_info_ptr = version_info_map_[replication_group_id];
@@ -53,19 +53,19 @@ void AutoFlagsVersionHandler::InsertOrUpdate(
   }
 }
 
-void AutoFlagsVersionHandler::Delete(const cdc::ReplicationGroupId& replication_group_id) {
+void AutoFlagsVersionHandler::Delete(const xcluster::ReplicationGroupId& replication_group_id) {
   std::lock_guard l(mutex_);
   version_info_map_.erase(replication_group_id);
 }
 
 std::shared_ptr<AutoFlagVersionInfo> AutoFlagsVersionHandler::GetAutoFlagsCompatibleVersion(
-    const cdc::ReplicationGroupId& replication_group_id) const {
+    const xcluster::ReplicationGroupId& replication_group_id) const {
   SharedLock l(mutex_);
   return FindPtrOrNull(version_info_map_, replication_group_id);
 }
 
 Status AutoFlagsVersionHandler::ReportNewAutoFlagConfigVersion(
-    const cdc::ReplicationGroupId& replication_group_id, uint32_t new_version) const {
+    const xcluster::ReplicationGroupId& replication_group_id, uint32_t new_version) const {
   auto version_info = GetAutoFlagsCompatibleVersion(replication_group_id);
   SCHECK_FORMAT(version_info, NotFound, "Replication group $0 not found", replication_group_id);
 
@@ -104,8 +104,8 @@ Status AutoFlagsVersionHandler::ReportNewAutoFlagConfigVersion(
 }
 
 Status AutoFlagsVersionHandler::XClusterReportNewAutoFlagConfigVersion(
-    const cdc::ReplicationGroupId& replication_group_id, uint32 new_version) const {
+    const xcluster::ReplicationGroupId& replication_group_id, uint32 new_version) const {
   return client_->XClusterReportNewAutoFlagConfigVersion(replication_group_id, new_version);
 }
 
-}  // namespace yb::tserver::xcluster
+}  // namespace yb::tserver
diff --git a/src/yb/tserver/xcluster_consumer_auto_flags_info.h b/src/yb/tserver/xcluster_consumer_auto_flags_info.h
index ea6c00b559..f4457cda4b 100644
--- a/src/yb/tserver/xcluster_consumer_auto_flags_info.h
+++ b/src/yb/tserver/xcluster_consumer_auto_flags_info.h
@@ -16,7 +16,7 @@
 #include <atomic>
 #include <shared_mutex>
 
-#include "yb/cdc/cdc_types.h"
+#include "yb/cdc/xcluster_types.h"
 #include "yb/gutil/integral_types.h"
 #include "yb/gutil/thread_annotations.h"
 
@@ -26,7 +26,7 @@ namespace client {
 class YBClient;
 }  // namespace client
 
-namespace tserver::xcluster {
+namespace tserver {
 
 // Helper class to get the compatible target AutoFlags config version.
 class AutoFlagsCompatibleVersion {
@@ -58,29 +58,29 @@ class AutoFlagsVersionHandler {
   virtual ~AutoFlagsVersionHandler() = default;
 
   void InsertOrUpdate(
-      const cdc::ReplicationGroupId& replication_group_id, uint32 compatible_version,
+      const xcluster::ReplicationGroupId& replication_group_id, uint32 compatible_version,
       uint32 max_reported_version) EXCLUDES(mutex_);
 
-  void Delete(const cdc::ReplicationGroupId& replication_group_id) EXCLUDES(mutex_);
+  void Delete(const xcluster::ReplicationGroupId& replication_group_id) EXCLUDES(mutex_);
 
   std::shared_ptr<AutoFlagVersionInfo> GetAutoFlagsCompatibleVersion(
-      const cdc::ReplicationGroupId& replication_group_id) const EXCLUDES(mutex_);
+      const xcluster::ReplicationGroupId& replication_group_id) const EXCLUDES(mutex_);
 
   Status ReportNewAutoFlagConfigVersion(
-      const cdc::ReplicationGroupId& replication_group_id, uint32_t new_version) const
+      const xcluster::ReplicationGroupId& replication_group_id, uint32_t new_version) const
       EXCLUDES(mutex_);
 
  protected:
   virtual Status XClusterReportNewAutoFlagConfigVersion(
-      const cdc::ReplicationGroupId& replication_group_id, uint32 new_version) const;
+      const xcluster::ReplicationGroupId& replication_group_id, uint32 new_version) const;
 
  private:
   std::shared_ptr<client::YBClient> client_;
 
   mutable std::shared_mutex mutex_;
-  std::unordered_map<cdc::ReplicationGroupId, std::shared_ptr<AutoFlagVersionInfo>>
+  std::unordered_map<xcluster::ReplicationGroupId, std::shared_ptr<AutoFlagVersionInfo>>
       version_info_map_ GUARDED_BY(mutex_);
 };
 
-}  // namespace tserver::xcluster
+}  // namespace tserver
 }  // namespace yb
diff --git a/src/yb/tserver/xcluster_consumer_replication_error-test.cc b/src/yb/tserver/xcluster_consumer_replication_error-test.cc
index d95c61768c..43a920d277 100644
--- a/src/yb/tserver/xcluster_consumer_replication_error-test.cc
+++ b/src/yb/tserver/xcluster_consumer_replication_error-test.cc
@@ -43,11 +43,11 @@ TEST(XClusterConsumerReplicationError, TestCollector) {
 
   // Setup 3 pollers across 2 Replication Groups, 2 tables and 3 tablets.
   const XClusterPollerId poller1(
-      cdc::ReplicationGroupId("Group1"), "TableA", "Tablet1", /*leader_term=*/1);
+      xcluster::ReplicationGroupId("Group1"), "TableA", "Tablet1", /*leader_term=*/1);
   const XClusterPollerId poller2(
-      cdc::ReplicationGroupId("Group1"), "TableA", "Tablet2", /*leader_term=*/2);
+      xcluster::ReplicationGroupId("Group1"), "TableA", "Tablet2", /*leader_term=*/2);
   const XClusterPollerId poller3(
-      cdc::ReplicationGroupId("Group2"), "TableA", "Tablet1", /*leader_term=*/1);
+      xcluster::ReplicationGroupId("Group2"), "TableA", "Tablet1", /*leader_term=*/1);
   collector.AddPoller(poller1);
   collector.AddPoller(poller2);
   collector.AddPoller(poller3);
diff --git a/src/yb/tserver/xcluster_consumer_replication_error.h b/src/yb/tserver/xcluster_consumer_replication_error.h
index 3865831a00..53a94599ec 100644
--- a/src/yb/tserver/xcluster_consumer_replication_error.h
+++ b/src/yb/tserver/xcluster_consumer_replication_error.h
@@ -30,7 +30,7 @@ YB_DEFINE_ENUM(
     (kSent));   // Error has been sent and master has acknowledged it.
 
 using XClusterReplicationErrorsToSendMap =
-    std::unordered_map<cdc::ReplicationGroupId, yb::xcluster::ReplicationGroupErrors>;
+    std::unordered_map<xcluster::ReplicationGroupId, xcluster::ReplicationGroupErrors>;
 
 // Collection of errors from the various Pollers running on this tserver. This class helps keep
 // track of which errors were sent to master.
diff --git a/src/yb/tserver/xcluster_output_client.cc b/src/yb/tserver/xcluster_output_client.cc
index 2efbef640b..b1783cc47b 100644
--- a/src/yb/tserver/xcluster_output_client.cc
+++ b/src/yb/tserver/xcluster_output_client.cc
@@ -93,8 +93,8 @@ namespace yb {
 namespace tserver {
 
 XClusterOutputClient::XClusterOutputClient(
-    XClusterPoller* xcluster_poller, const cdc::ConsumerTabletInfo& consumer_tablet_info,
-    const cdc::ProducerTabletInfo& producer_tablet_info,
+    XClusterPoller* xcluster_poller, const xcluster::ConsumerTabletInfo& consumer_tablet_info,
+    const xcluster::ProducerTabletInfo& producer_tablet_info,
     const std::shared_ptr<XClusterClient>& local_client, ThreadPool* thread_pool, rpc::Rpcs* rpcs,
     bool use_local_tserver, rocksdb::RateLimiter* rate_limiter)
     : XClusterAsyncExecutor(thread_pool, local_client->messenger.get(), rpcs),
@@ -704,8 +704,8 @@ bool XClusterOutputClient::IncProcessedRecordCount() {
 }
 
 std::shared_ptr<XClusterOutputClient> CreateXClusterOutputClient(
-    XClusterPoller* xcluster_poller, const cdc::ConsumerTabletInfo& consumer_tablet_info,
-    const cdc::ProducerTabletInfo& producer_tablet_info,
+    XClusterPoller* xcluster_poller, const xcluster::ConsumerTabletInfo& consumer_tablet_info,
+    const xcluster::ProducerTabletInfo& producer_tablet_info,
     const std::shared_ptr<XClusterClient>& local_client, ThreadPool* thread_pool, rpc::Rpcs* rpcs,
     bool use_local_tserver, rocksdb::RateLimiter* rate_limiter) {
   return std::make_unique<XClusterOutputClient>(
diff --git a/src/yb/tserver/xcluster_output_client.h b/src/yb/tserver/xcluster_output_client.h
index 5ada1dd618..0d54b54b89 100644
--- a/src/yb/tserver/xcluster_output_client.h
+++ b/src/yb/tserver/xcluster_output_client.h
@@ -11,6 +11,7 @@
 // under the License.
 
 #include "yb/cdc/cdc_service.pb.h"
+#include "yb/cdc/xcluster_types.h"
 #include "yb/consensus/opid_util.h"
 #include "yb/tserver/xcluster_async_executor.h"
 #include "yb/cdc/cdc_util.h"
@@ -47,8 +48,8 @@ struct XClusterClient;
 class XClusterOutputClient : public XClusterAsyncExecutor {
  public:
   XClusterOutputClient(
-      XClusterPoller* xcluster_poller, const cdc::ConsumerTabletInfo& consumer_tablet_info,
-      const cdc::ProducerTabletInfo& producer_tablet_info,
+      XClusterPoller* xcluster_poller, const xcluster::ConsumerTabletInfo& consumer_tablet_info,
+      const xcluster::ProducerTabletInfo& producer_tablet_info,
       const std::shared_ptr<XClusterClient>& local_client, ThreadPool* thread_pool, rpc::Rpcs* rpcs,
       bool use_local_tserver, rocksdb::RateLimiter* rate_limiter);
   ~XClusterOutputClient();
@@ -128,8 +129,8 @@ class XClusterOutputClient : public XClusterAsyncExecutor {
   // TODO: Once we move the async execution logic to the Poller, it will guarantee that our lifetime
   // is less than the pollers lifetime, making this always safe to use.
   XClusterPoller* const xcluster_poller_ GUARDED_BY(lock_);
-  const cdc::ConsumerTabletInfo consumer_tablet_info_;
-  const cdc::ProducerTabletInfo producer_tablet_info_;
+  const xcluster::ConsumerTabletInfo consumer_tablet_info_;
+  const xcluster::ProducerTabletInfo producer_tablet_info_;
   cdc::XClusterSchemaVersionMap schema_versions_ GUARDED_BY(lock_);
   cdc::ColocatedSchemaVersionMap colocated_schema_version_map_ GUARDED_BY(lock_);
   std::shared_ptr<XClusterClient> local_client_;
@@ -167,8 +168,8 @@ class XClusterOutputClient : public XClusterAsyncExecutor {
 };
 
 std::shared_ptr<XClusterOutputClient> CreateXClusterOutputClient(
-    XClusterPoller* xcluster_poller, const cdc::ConsumerTabletInfo& consumer_tablet_info,
-    const cdc::ProducerTabletInfo& producer_tablet_info,
+    XClusterPoller* xcluster_poller, const xcluster::ConsumerTabletInfo& consumer_tablet_info,
+    const xcluster::ProducerTabletInfo& producer_tablet_info,
     const std::shared_ptr<XClusterClient>& local_client, ThreadPool* thread_pool, rpc::Rpcs* rpcs,
     bool use_local_tserver, rocksdb::RateLimiter* rate_limiter);
 
diff --git a/src/yb/tserver/xcluster_poller.cc b/src/yb/tserver/xcluster_poller.cc
index 7ed36bcb84..337f6c3c67 100644
--- a/src/yb/tserver/xcluster_poller.cc
+++ b/src/yb/tserver/xcluster_poller.cc
@@ -105,10 +105,10 @@ namespace yb {
 namespace tserver {
 
 XClusterPoller::XClusterPoller(
-    const cdc::ProducerTabletInfo& producer_tablet_info,
-    const cdc::ConsumerTabletInfo& consumer_tablet_info,
-    std::shared_ptr<const xcluster::AutoFlagsCompatibleVersion> auto_flags_version,
-    ThreadPool* thread_pool, rpc::Rpcs* rpcs, const std::shared_ptr<XClusterClient>& local_client,
+    const xcluster::ProducerTabletInfo& producer_tablet_info,
+    const xcluster::ConsumerTabletInfo& consumer_tablet_info,
+    std::shared_ptr<const AutoFlagsCompatibleVersion> auto_flags_version, ThreadPool* thread_pool,
+    rpc::Rpcs* rpcs, const std::shared_ptr<XClusterClient>& local_client,
     const std::shared_ptr<XClusterClient>& producer_client, XClusterConsumer* xcluster_consumer,
     SchemaVersion last_compatible_consumer_schema_version, int64_t leader_term,
     std::function<int64_t(const TabletId&)> get_leader_term)
diff --git a/src/yb/tserver/xcluster_poller.h b/src/yb/tserver/xcluster_poller.h
index d3aee54146..9db9c3e5e7 100644
--- a/src/yb/tserver/xcluster_poller.h
+++ b/src/yb/tserver/xcluster_poller.h
@@ -44,19 +44,16 @@ class CDCServiceProxy;
 
 namespace tserver {
 
-class XClusterConsumer;
-
-namespace xcluster {
 class AutoFlagsCompatibleVersion;
-}  // namespace xcluster
+class XClusterConsumer;
 
 class XClusterPoller : public XClusterAsyncExecutor {
  public:
   XClusterPoller(
-      const cdc::ProducerTabletInfo& producer_tablet_info,
-      const cdc::ConsumerTabletInfo& consumer_tablet_info,
-      std::shared_ptr<const xcluster::AutoFlagsCompatibleVersion> auto_flags_version,
-      ThreadPool* thread_pool, rpc::Rpcs* rpcs, const std::shared_ptr<XClusterClient>& local_client,
+      const xcluster::ProducerTabletInfo& producer_tablet_info,
+      const xcluster::ConsumerTabletInfo& consumer_tablet_info,
+      std::shared_ptr<const AutoFlagsCompatibleVersion> auto_flags_version, ThreadPool* thread_pool,
+      rpc::Rpcs* rpcs, const std::shared_ptr<XClusterClient>& local_client,
       const std::shared_ptr<XClusterClient>& producer_client, XClusterConsumer* xcluster_consumer,
       SchemaVersion last_compatible_consumer_schema_version, int64_t leader_term,
       std::function<int64_t(const TabletId&)> get_leader_term);
@@ -87,8 +84,12 @@ class XClusterPoller : public XClusterAsyncExecutor {
 
   HybridTime GetSafeTime() const EXCLUDES(safe_time_lock_);
 
-  const cdc::ConsumerTabletInfo& GetConsumerTabletInfo() const { return consumer_tablet_info_; }
-  const cdc::ProducerTabletInfo& GetProducerTabletInfo() const { return producer_tablet_info_; }
+  const xcluster::ConsumerTabletInfo& GetConsumerTabletInfo() const {
+    return consumer_tablet_info_;
+  }
+  const xcluster::ProducerTabletInfo& GetProducerTabletInfo() const {
+    return producer_tablet_info_;
+  }
 
   bool IsStuck() const;
   std::string State() const;
@@ -105,7 +106,7 @@ class XClusterPoller : public XClusterAsyncExecutor {
   void ApplyChangesCallback(XClusterOutputClientResponse&& response);
 
  private:
-  const cdc::ReplicationGroupId& GetReplicationGroupId() const {
+  const xcluster::ReplicationGroupId& GetReplicationGroupId() const {
     return producer_tablet_info_.replication_group_id;
   }
 
@@ -127,10 +128,10 @@ class XClusterPoller : public XClusterAsyncExecutor {
   bool IsLeaderTermValid() REQUIRES(data_mutex_);
   Status ProcessGetChangesResponseError(const cdc::GetChangesResponsePB& resp);
 
-  const cdc::ProducerTabletInfo producer_tablet_info_;
-  const cdc::ConsumerTabletInfo consumer_tablet_info_;
+  const xcluster::ProducerTabletInfo producer_tablet_info_;
+  const xcluster::ConsumerTabletInfo consumer_tablet_info_;
   const XClusterPollerId poller_id_;
-  const std::shared_ptr<const xcluster::AutoFlagsCompatibleVersion> auto_flags_version_;
+  const std::shared_ptr<const AutoFlagsCompatibleVersion> auto_flags_version_;
 
   mutable rw_spinlock schema_version_lock_;
   cdc::XClusterSchemaVersionMap schema_version_map_ GUARDED_BY(schema_version_lock_);
diff --git a/src/yb/tserver/xcluster_poller_id.cc b/src/yb/tserver/xcluster_poller_id.cc
index f5d3ecec76..4defcf7ba0 100644
--- a/src/yb/tserver/xcluster_poller_id.cc
+++ b/src/yb/tserver/xcluster_poller_id.cc
@@ -12,11 +12,12 @@
 //
 
 #include "yb/tserver/xcluster_poller_id.h"
+#include "yb/util/tostring.h"
 
 namespace yb {
 
 XClusterPollerId::XClusterPollerId(
-    cdc::ReplicationGroupId replication_group_id, TableId consumer_table_id,
+    xcluster::ReplicationGroupId replication_group_id, TableId consumer_table_id,
     TabletId producer_tablet_id, int64 leader_term)
     : replication_group_id(replication_group_id),
       consumer_table_id(consumer_table_id),
diff --git a/src/yb/tserver/xcluster_poller_id.h b/src/yb/tserver/xcluster_poller_id.h
index 0bdb83e614..fd4f3e7052 100644
--- a/src/yb/tserver/xcluster_poller_id.h
+++ b/src/yb/tserver/xcluster_poller_id.h
@@ -13,7 +13,7 @@
 
 #pragma once
 
-#include "yb/cdc/cdc_types.h"
+#include "yb/cdc/xcluster_types.h"
 
 namespace yb {
 
@@ -25,13 +25,13 @@ namespace yb {
 // and consumer do not match, a single Poller can write to multiple consumer tablets, and multiple
 // Pollers belonging to the same ReplicationGroup can write to the same consumer tablet.
 struct XClusterPollerId {
-  cdc::ReplicationGroupId replication_group_id;
+  xcluster::ReplicationGroupId replication_group_id;
   TableId consumer_table_id;
   TabletId producer_tablet_id;
   int64 leader_term;
 
   explicit XClusterPollerId(
-      cdc::ReplicationGroupId replication_group_id, TableId consumer_table_id,
+      xcluster::ReplicationGroupId replication_group_id, TableId consumer_table_id,
       TabletId producer_tablet_id, int64 leader_term);
 
   bool operator==(const XClusterPollerId& other) const;
diff --git a/src/yb/tserver/xcluster_poller_stats.cc b/src/yb/tserver/xcluster_poller_stats.cc
index 2d8575a5d0..24acdd8665 100644
--- a/src/yb/tserver/xcluster_poller_stats.cc
+++ b/src/yb/tserver/xcluster_poller_stats.cc
@@ -22,7 +22,8 @@ XClusterPollerStats::XClusterPollerStats(const std::string& replication_group_id
     : replication_group_id(replication_group_id) {}
 
 XClusterPollerStats::XClusterPollerStats(
-    const cdc::ProducerTabletInfo& producer_info, const cdc::ConsumerTabletInfo& consumer_info)
+    const xcluster::ProducerTabletInfo& producer_info,
+    const xcluster::ConsumerTabletInfo& consumer_info)
     : replication_group_id(producer_info.replication_group_id),
       stream_id_str(producer_info.stream_id.ToString()),
       producer_tablet_id(producer_info.tablet_id),
diff --git a/src/yb/tserver/xcluster_poller_stats.h b/src/yb/tserver/xcluster_poller_stats.h
index a41f3ff96a..2f22fd8fdb 100644
--- a/src/yb/tserver/xcluster_poller_stats.h
+++ b/src/yb/tserver/xcluster_poller_stats.h
@@ -17,20 +17,22 @@
 #include <string>
 #include <boost/circular_buffer.hpp>
 
-#include "yb/cdc/cdc_types.h"
 #include "yb/cdc/cdc_util.h"
+#include "yb/cdc/xcluster_types.h"
 #include "yb/common/entity_ids_types.h"
 #include "yb/gutil/thread_annotations.h"
 #include "yb/util/monotime.h"
+#include "yb/util/status.h"
 
 namespace yb {
 
 struct XClusterPollerStats {
   explicit XClusterPollerStats(const std::string& replication_group_id);
   explicit XClusterPollerStats(
-      const cdc::ProducerTabletInfo& producer_info, const cdc::ConsumerTabletInfo& consumer_info);
+      const xcluster::ProducerTabletInfo& producer_info,
+      const xcluster::ConsumerTabletInfo& consumer_info);
 
-  cdc::ReplicationGroupId replication_group_id;
+  xcluster::ReplicationGroupId replication_group_id;
   std::string stream_id_str;
   TabletId producer_tablet_id;
   TableId consumer_table_id;
diff --git a/src/yb/yql/pgwrapper/pg_locks_test_base.cc b/src/yb/yql/pgwrapper/pg_locks_test_base.cc
index cb44eb3217..63b2162751 100644
--- a/src/yb/yql/pgwrapper/pg_locks_test_base.cc
+++ b/src/yb/yql/pgwrapper/pg_locks_test_base.cc
@@ -20,6 +20,7 @@
 #include "yb/client/yb_table_name.h"
 #include "yb/common/transaction.h"
 #include "yb/common/wire_protocol.h"
+#include "yb/master/master_client.pb.h"
 #include "yb/master/master_defaults.h"
 #include "yb/rpc/rpc_controller.h"
 
