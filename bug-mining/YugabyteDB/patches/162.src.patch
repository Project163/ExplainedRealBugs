diff --git a/src/yb/client/client-internal.cc b/src/yb/client/client-internal.cc
index e5ca6ef3d5..192e68e7c5 100644
--- a/src/yb/client/client-internal.cc
+++ b/src/yb/client/client-internal.cc
@@ -332,6 +332,7 @@ YB_CLIENT_SPECIALIZE_SIMPLE_EX(Replication, GetXClusterOutboundReplicationGroups
 YB_CLIENT_SPECIALIZE_SIMPLE_EX(Replication, GetXClusterOutboundReplicationGroupInfo);
 YB_CLIENT_SPECIALIZE_SIMPLE_EX(Replication, GetUniverseReplications);
 YB_CLIENT_SPECIALIZE_SIMPLE_EX(Replication, GetUniverseReplicationInfo);
+YB_CLIENT_SPECIALIZE_SIMPLE_EX(Replication, WaitForReplicationDrain);
 
 #define YB_CLIENT_SPECIALIZE_SIMPLE_EX_EACH(i, data, set) YB_CLIENT_SPECIALIZE_SIMPLE_EX set
 
@@ -1001,6 +1002,42 @@ Status YBClient::Data::WaitForBackfillIndexToFinish(
           &YBClient::Data::IsBackfillIndexInProgress, this, client, table_id, index_id, _1, _2));
 }
 
+Result<bool> YBClient::Data::IsBackfillIndexStarted(
+    YBClient* client, const TableId& index_table_id, const TableId& indexed_table_id,
+    CoarseTimePoint deadline) {
+  YBTableInfo yb_table_info;
+  master::GetTableSchemaResponsePB resp;
+  RETURN_NOT_OK(GetTableSchema(
+      client, indexed_table_id, deadline, &yb_table_info, master::IncludeInactive::kFalse, &resp));
+
+  for (const auto& index : resp.indexes()) {
+    if (index.table_id() == index_table_id) {
+      SCHECK_FORMAT(
+          index.backfill_error_message().empty(), IllegalState, "Index $0 has backfill error: $1",
+          index_table_id, index.backfill_error_message());
+
+      auto index_permissions = index.index_permissions();
+
+      // Anything above INDEX_PERM_READ_WRITE_AND_DELETE means index is being deleted.
+      SCHECK_LE(
+          index_permissions, INDEX_PERM_READ_WRITE_AND_DELETE, IllegalState,
+          Format(
+              "Index permissions $0 for index $1 is unexpected",
+              IndexPermissions_Name(index_permissions), index_table_id));
+
+      // YSQL indexes start with permission INDEX_PERM_WRITE_AND_DELETE, and directly moves to
+      // INDEX_PERM_READ_WRITE_AND_DELETE. The actual stages are tracked in postgres fields
+      // indislive, indisready and indisvalid. So use index_permissions to determine if the index is
+      // currently backfilling.
+      return resp.is_backfilling() ||
+             index_permissions == IndexPermissions::INDEX_PERM_READ_WRITE_AND_DELETE;
+    }
+  }
+
+  // We can get here if the indexed table schema is not yet fully applied.
+  return false;
+}
+
 Result<master::GetBackfillStatusResponsePB> YBClient::Data::GetBackfillStatus(
     const std::vector<std::string_view>& table_ids, const CoarseTimePoint deadline) {
   if (table_ids.empty()) {
diff --git a/src/yb/client/client-internal.h b/src/yb/client/client-internal.h
index 086a5a8bcf..b40d2d5873 100644
--- a/src/yb/client/client-internal.h
+++ b/src/yb/client/client-internal.h
@@ -207,6 +207,15 @@ class YBClient::Data {
                                       const TableId& index_id,
                                       CoarseTimePoint deadline);
 
+  // Has the index entered the backfill stage yet?
+  // Returns true if the backfill has started or has already completed.
+  // Returns false if the index is not backfilling yet or is not yet part of the indexed table
+  // schema.
+  // Returns a bad Status if the index is being dropped, or it encountered a backfill error.
+  Result<bool> IsBackfillIndexStarted(
+      YBClient* client, const TableId& index_table_id, const TableId& indexed_table_id,
+      CoarseTimePoint deadline);
+
   Result<master::GetBackfillStatusResponsePB> GetBackfillStatus(
     const std::vector<std::string_view>& table_ids,
     const CoarseTimePoint deadline);
diff --git a/src/yb/client/client.cc b/src/yb/client/client.cc
index 9e986e60eb..30bf2bfebf 100644
--- a/src/yb/client/client.cc
+++ b/src/yb/client/client.cc
@@ -688,6 +688,11 @@ Result<master::GetBackfillStatusResponsePB> YBClient::GetBackfillStatus(
   return data_->GetBackfillStatus(table_ids, deadline);
 }
 
+Result<bool> YBClient::IsBackfillIndexStarted(
+    const TableId& index_table_id, const TableId& indexed_table_id, CoarseTimePoint deadline) {
+  return data_->IsBackfillIndexStarted(this, index_table_id, indexed_table_id, deadline);
+}
+
 Status YBClient::DeleteTable(const YBTableName& table_name, bool wait) {
   auto deadline = CoarseMonoClock::Now() + default_admin_operation_timeout();
   return data_->DeleteTable(this,
@@ -2580,10 +2585,8 @@ Result<bool> YBClient::CheckIfPitrActive() {
   return data_->CheckIfPitrActive(deadline);
 }
 
-Result<std::vector<YBTableName>> YBClient::ListTables(const std::string& filter,
-                                                      bool exclude_ysql,
-                                                      const std::string& ysql_db_filter,
-                                                      bool skip_hidden) {
+Result<master::ListTablesResponsePB> YBClient::ListTables(
+    const std::string& filter, const std::string& ysql_db_filter) {
   ListTablesRequestPB req;
   ListTablesResponsePB resp;
 
@@ -2597,6 +2600,19 @@ Result<std::vector<YBTableName>> YBClient::ListTables(const std::string& filter,
   }
 
   CALL_SYNC_LEADER_MASTER_RPC(req, resp, ListTables);
+
+  if (resp.has_error()) {
+    return StatusFromPB(resp.error().status());
+  }
+
+  return resp;
+}
+
+Result<std::vector<YBTableName>> YBClient::ListTables(
+    const std::string& filter, bool exclude_ysql, const std::string& ysql_db_filter,
+    bool skip_hidden) {
+  auto resp = VERIFY_RESULT(ListTables(filter, ysql_db_filter));
+
   std::vector<YBTableName> result;
   result.reserve(resp.tables_size());
 
diff --git a/src/yb/client/client.h b/src/yb/client/client.h
index 28308d8d30..ec968e057b 100644
--- a/src/yb/client/client.h
+++ b/src/yb/client/client.h
@@ -351,6 +351,9 @@ class YBClient {
   Result<master::GetBackfillStatusResponsePB> GetBackfillStatus(
       const std::vector<std::string_view>& table_ids);
 
+  Result<bool> IsBackfillIndexStarted(
+      const TableId& index_table_id, const TableId& indexed_table_id, CoarseTimePoint deadline);
+
   // Delete the specified table.
   // Set 'wait' to true if the call must wait for the table to be fully deleted before returning.
   Status DeleteTable(const YBTableName& table_name, bool wait = true);
@@ -736,6 +739,8 @@ class YBClient {
   // belong to.
   //
   // 'tables' is appended to only on success.
+  Result<master::ListTablesResponsePB> ListTables(
+      const std::string& filter, const std::string& ysql_db_filter);
   Result<std::vector<YBTableName>> ListTables(
       const std::string& filter = "",
       bool exclude_ysql = false,
@@ -746,8 +751,7 @@ class YBClient {
   //
   // 'tables' is appended to only on success.
   Result<std::vector<YBTableName>> ListUserTables(
-      const master::NamespaceIdentifierPB& ns_identifier,
-      bool include_indexes = false);
+      const master::NamespaceIdentifierPB& ns_identifier, bool include_indexes = false);
 
   Result<cdc::EnumOidLabelMap> GetPgEnumOidLabelMap(const NamespaceName& ns_name);
 
diff --git a/src/yb/client/xcluster_client.cc b/src/yb/client/xcluster_client.cc
index 1dfa2e9506..4945db8e2d 100644
--- a/src/yb/client/xcluster_client.cc
+++ b/src/yb/client/xcluster_client.cc
@@ -737,4 +737,40 @@ Status XClusterClient::AddNamespaceToDbScopedUniverseReplication(
   return Status::OK();
 }
 
+Status XClusterClient::WaitForReplicationDrain(
+    const xrepl::StreamId& stream_id, MicrosecondsInt64 target_time, CoarseTimePoint deadline) {
+  master::WaitForReplicationDrainRequestPB req;
+  req.set_target_time(target_time);
+  req.add_stream_ids(stream_id.ToString());
+
+  master::WaitForReplicationDrainResponsePB resp;
+  RETURN_NOT_OK(yb_client_.data_->SyncLeaderMasterRpc(
+      deadline, req, &resp, "WaitForReplicationDrain",
+      &master::MasterReplicationProxy::WaitForReplicationDrainAsync));
+
+  if (resp.has_error()) {
+    return StatusFromPB(resp.error().status());
+  }
+
+  return Status::OK();
+}
+
+Result<std::vector<xrepl::StreamId>> XClusterClient::GetXClusterStreams(const TableId& table_id) {
+  master::ListCDCStreamsRequestPB req;
+  req.set_id_type(yb::master::IdTypePB::TABLE_ID);
+  req.set_table_id(table_id);
+
+  auto resp = CALL_SYNC_LEADER_MASTER_RPC(ListCDCStreams, req);
+
+  if (resp.has_error()) {
+    return StatusFromPB(resp.error().status());
+  }
+
+  std::vector<xrepl::StreamId> stream_ids;
+  for (const auto& stream : resp.streams()) {
+    stream_ids.emplace_back(VERIFY_RESULT(xrepl::StreamId::FromString(stream.stream_id())));
+  }
+  return stream_ids;
+}
+
 }  // namespace yb::client
diff --git a/src/yb/client/xcluster_client.h b/src/yb/client/xcluster_client.h
index 845218cb02..c1059d90f6 100644
--- a/src/yb/client/xcluster_client.h
+++ b/src/yb/client/xcluster_client.h
@@ -236,6 +236,11 @@ class XClusterClient {
       const NamespaceId& source_namespace_id, const std::vector<TableId>& source_table_ids,
       const std::vector<xrepl::StreamId>& bootstrap_ids);
 
+  Status WaitForReplicationDrain(
+      const xrepl::StreamId& stream_id, MicrosecondsInt64 target_time, CoarseTimePoint deadline);
+
+  Result<std::vector<xrepl::StreamId>> GetXClusterStreams(const TableId& table_id);
+
  private:
   CoarseTimePoint GetDeadline() const;
 
diff --git a/src/yb/integration-tests/xcluster/xcluster_test_base.cc b/src/yb/integration-tests/xcluster/xcluster_test_base.cc
index 777dcc1369..3144b255b4 100644
--- a/src/yb/integration-tests/xcluster/xcluster_test_base.cc
+++ b/src/yb/integration-tests/xcluster/xcluster_test_base.cc
@@ -755,9 +755,18 @@ Result<std::vector<xrepl::StreamId>> XClusterTestBase::BootstrapProducer(
 
 Status XClusterTestBase::WaitForReplicationDrain(
     int expected_num_nondrained, int timeout_secs, std::optional<uint64> target_time,
-    std::vector<TableId> producer_table_ids) {
+    std::vector<TableId> producer_table_ids, YBClient* source_client) {
+  if (!source_client) {
+    source_client = producer_client();
+  }
+  if (producer_table_ids.empty()) {
+    for (const auto& producer_table : producer_tables_) {
+      producer_table_ids.push_back(producer_table->id());
+    }
+  }
+
   auto master_proxy = std::make_shared<master::MasterReplicationProxy>(
-      &producer_client()->proxy_cache(),
+      &source_client->proxy_cache(),
       VERIFY_RESULT(producer_cluster()->GetLeaderMiniMaster())->bound_rpc_addr());
 
   master::WaitForReplicationDrainRequestPB req;
@@ -765,12 +774,6 @@ Status XClusterTestBase::WaitForReplicationDrain(
   rpc::RpcController rpc;
   rpc.set_timeout(MonoDelta::FromSeconds(timeout_secs));
 
-  if (producer_table_ids.empty()) {
-    for (const auto& producer_table : producer_tables_) {
-      producer_table_ids.push_back(producer_table->id());
-    }
-  }
-
   for (const auto& table_id : producer_table_ids) {
     master::ListCDCStreamsResponsePB list_resp;
     RETURN_NOT_OK(GetCDCStreamForTable(table_id, &list_resp));
@@ -1021,16 +1024,20 @@ Result<TableId> GetTablegroupParentTable(
 
 }  // namespace
 
-Result<TableId> XClusterTestBase::GetColocatedDatabaseParentTableId() {
+Result<TableId> XClusterTestBase::GetColocatedDatabaseParentTableId(Cluster* cluster) {
+  if (!cluster) {
+    cluster = &producer_cluster_;
+  }
+
   if (FLAGS_ysql_legacy_colocated_database_creation) {
     // Legacy colocated database
     master::GetNamespaceInfoResponsePB ns_resp;
     RETURN_NOT_OK(
-        producer_client()->GetNamespaceInfo("", namespace_name, YQL_DATABASE_PGSQL, &ns_resp));
+        cluster->client_->GetNamespaceInfo("", namespace_name, YQL_DATABASE_PGSQL, &ns_resp));
     return GetColocatedDbParentTableId(ns_resp.namespace_().id());
   }
   // Colocated database
-  return GetTablegroupParentTable(&producer_cluster_, namespace_name);
+  return GetTablegroupParentTable(cluster, namespace_name);
 }
 
 Result<master::MasterReplicationProxy> XClusterTestBase::GetProducerMasterProxy() {
diff --git a/src/yb/integration-tests/xcluster/xcluster_test_base.h b/src/yb/integration-tests/xcluster/xcluster_test_base.h
index 7eca577c34..4b1ac045bf 100644
--- a/src/yb/integration-tests/xcluster/xcluster_test_base.h
+++ b/src/yb/integration-tests/xcluster/xcluster_test_base.h
@@ -279,7 +279,7 @@ class XClusterTestBase : public YBTest {
   Status WaitForReplicationDrain(
       int expected_num_nondrained = 0, int timeout_secs = kRpcTimeout,
       std::optional<uint64> target_time = std::nullopt,
-      std::vector<TableId> producer_table_ids = {});
+      std::vector<TableId> producer_table_ids = {}, YBClient* source_client = nullptr);
 
   YBClient* producer_client() {
     return producer_cluster_.client_.get();
@@ -345,7 +345,7 @@ class XClusterTestBase : public YBTest {
   Status PauseResumeXClusterProducerStreams(
       const std::vector<xrepl::StreamId>& stream_ids, bool is_paused);
 
-  Result<TableId> GetColocatedDatabaseParentTableId();
+  Result<TableId> GetColocatedDatabaseParentTableId(Cluster* cluster = nullptr);
 
   Result<master::MasterReplicationProxy> GetProducerMasterProxy();
 
diff --git a/src/yb/integration-tests/xcluster/xcluster_ysql_index-test.cc b/src/yb/integration-tests/xcluster/xcluster_ysql_index-test.cc
index 9b2da66827..ef8b103bc9 100644
--- a/src/yb/integration-tests/xcluster/xcluster_ysql_index-test.cc
+++ b/src/yb/integration-tests/xcluster/xcluster_ysql_index-test.cc
@@ -22,6 +22,7 @@
 #include "yb/tserver/mini_tablet_server.h"
 #include "yb/tserver/tablet_server.h"
 #include "yb/util/flags.h"
+#include "yb/util/logging_test_util.h"
 #include "yb/util/scope_exit.h"
 #include "yb/util/sync_point.h"
 #include "yb/util/test_thread_holder.h"
@@ -31,6 +32,9 @@ DECLARE_bool(TEST_disable_apply_committed_transactions);
 DECLARE_bool(TEST_xcluster_fail_table_create_during_bootstrap);
 DECLARE_int32(TEST_user_ddl_operation_timeout_sec);
 DECLARE_bool(TEST_fail_universe_replication_merge);
+DECLARE_bool(auto_add_new_index_to_bidirectional_xcluster);
+DECLARE_string(ysql_yb_test_block_index_phase);
+DECLARE_int32(ysql_yb_index_state_flags_update_delay);
 
 using std::string;
 using namespace std::chrono_literals;
@@ -63,13 +67,18 @@ class XClusterYsqlIndexTest : public XClusterYsqlTestBase {
 
     producer_master_ = ASSERT_RESULT(producer_cluster()->GetLeaderMiniMaster())->master();
 
-    yb_table_name_ = ASSERT_RESULT(
-        GetYsqlTable(&producer_cluster_, namespace_name, "" /* schema_name */, kTableName));
-
-    client::YBTablePtr producer_table;
-    ASSERT_OK(producer_client()->OpenTable(yb_table_name_, &producer_table));
-    namespace_id_ = producer_table->name().namespace_id();
-    producer_tables_.push_back(std::move(producer_table));
+    ASSERT_OK(RunOnBothClusters([this](Cluster* cluster) -> Status {
+      auto table_name =
+          VERIFY_RESULT(GetYsqlTable(cluster, namespace_name, "" /* schema_name */, kTableName));
+      client::YBTablePtr table;
+      RETURN_NOT_OK(cluster->client_->OpenTable(table_name, &table));
+      cluster->tables_.emplace_back(std::move(table));
+      return Status::OK();
+    }));
+    producer_table_ = producer_tables_.front();
+    consumer_table_ = consumer_tables_.front();
+    yb_table_name_ = producer_table_->name();
+    namespace_id_ = yb_table_name_.namespace_id();
 
     ASSERT_OK(SetupUniverseReplication(
         producer_cluster(), consumer_cluster(), consumer_client(), kReplicationGroupId,
@@ -97,8 +106,6 @@ class XClusterYsqlIndexTest : public XClusterYsqlTestBase {
 
     consumer_conn_ = std::make_unique<pgwrapper::PGConn>(
         ASSERT_RESULT(consumer_cluster_.ConnectToDB(namespace_name)));
-    // auto r = ASSERT_RESULT(consumer_conn_->HasIndexScan(kId1CountStmt));
-    // ASSERT_FALSE(r);
     ASSERT_FALSE(ASSERT_RESULT(consumer_conn_->HasIndexScan(kId2CountStmt)));
 
     ASSERT_OK(ValidateRows());
@@ -110,12 +117,30 @@ class XClusterYsqlIndexTest : public XClusterYsqlTestBase {
   }
 
   virtual Status CreateObjects() {
-    return RunOnBothClusters([&](Cluster* cluster) { return CreateTable(cluster); });
+    if (!IsColocated()) {
+      return RunOnBothClusters([&](Cluster* cluster) { return CreateTable(cluster); });
+    }
+
+    namespace_name = "colocated_db";
+
+    return RunOnBothClusters([&](Cluster* cluster) {
+      constexpr int colocation_id = 111111;
+      RETURN_NOT_OK(CreateDatabase(cluster, namespace_name, /* colocated = */ true));
+      auto conn = VERIFY_RESULT(cluster->ConnectToDB(namespace_name));
+      return conn.ExecuteFormat(
+          "CREATE TABLE $0(id1 INT PRIMARY KEY, id2 INT) WITH (colocation_id = $1);", kTableName,
+          colocation_id);
+    });
   }
 
   virtual Transactional IsTransactional() { return Transactional::kTrue; }
+  virtual bool IsColocated() { return false; }
 
   virtual Result<std::vector<TableId>> GetReplicationTableIds() {
+    if (IsColocated()) {
+      return std::vector<TableId>{VERIFY_RESULT(GetColocatedDatabaseParentTableId())};
+    }
+
     std::vector<TableId> result;
     for (const auto& table : producer_tables_) {
       result.push_back(table->id());
@@ -124,6 +149,11 @@ class XClusterYsqlIndexTest : public XClusterYsqlTestBase {
   }
 
   virtual Status CreateIndex(pgwrapper::PGConn& conn) {
+    if (IsColocated()) {
+      return conn.Execute(Format(
+          "CREATE INDEX $0 ON $1 (id2 ASC) WITH(colocation_id =111112)", kIndexName, kTableName));
+    }
+
     return conn.Execute(Format("CREATE INDEX $0 ON $1 (id2 ASC)", kIndexName, kTableName));
   }
 
@@ -132,9 +162,28 @@ class XClusterYsqlIndexTest : public XClusterYsqlTestBase {
   }
 
   Status ValidateRows() {
+    const auto get_rows = [](pgwrapper::PGConn& conn,
+                             const std::string& col_name) -> Result<std::string> {
+      return yb::ToString(VERIFY_RESULT(
+          conn.FetchRows<int32_t>(Format("SELECT $0 FROM $1 ORDER BY $0", col_name, kTableName))));
+    };
+
     // With should be less than or equal to row_count_ since some inserts may fail due to
     // transactions aborted by the DDLs.
     const auto all_prod_rows = VERIFY_RESULT(GetAllRows(producer_conn_.get()));
+
+    LOG(INFO) << "Producer all rows: " << yb::ToString(all_prod_rows);
+    LOG(INFO) << "Producer id1 values: " << VERIFY_RESULT(get_rows(*producer_conn_, "id1"));
+    LOG(INFO) << "Producer id2 values: " << VERIFY_RESULT(get_rows(*producer_conn_, "id2"));
+    LOG(INFO) << "Consumer all rows: "
+              << yb::ToString(VERIFY_RESULT(GetAllRows(producer_conn_.get())));
+    LOG(INFO) << "Consumer id1 values: " << VERIFY_RESULT(get_rows(*producer_conn_, "id1"));
+    LOG(INFO) << "Consumer id2 values: " << VERIFY_RESULT(get_rows(*producer_conn_, "id2"));
+
+    auto cons_id2 =
+        consumer_conn_->FetchRows<int32_t>(Format("SELECT id2 FROM $0 ORDER BY id2", kTableName));
+    LOG(WARNING) << "Consumer id2 values: " << yb::ToString(cons_id2);
+
     SCHECK_LE(all_prod_rows.size(), row_count_, IllegalState, "Producer row count mismatch.");
     const auto actual_count = all_prod_rows.size();
 
@@ -413,27 +462,7 @@ TEST_F(XClusterYsqlNonTransactionalTest, CreateIndex) {
 
 class XClusterColocatedIndexTest : public XClusterYsqlIndexTest {
  public:
-  virtual Status CreateObjects() override {
-    namespace_name = "colocated_db";
-
-    return RunOnBothClusters([&](Cluster* cluster) {
-      constexpr int colocation_id = 111111;
-      RETURN_NOT_OK(CreateDatabase(cluster, namespace_name, /* colocated = */ true));
-      auto conn = VERIFY_RESULT(cluster->ConnectToDB(namespace_name));
-      return conn.ExecuteFormat(
-          "CREATE TABLE $0(id1 INT PRIMARY KEY, id2 INT) WITH (colocation_id = $1);", kTableName,
-          colocation_id);
-    });
-  }
-
-  Result<std::vector<TableId>> GetReplicationTableIds() override {
-    return std::vector<TableId>{VERIFY_RESULT(GetColocatedDatabaseParentTableId())};
-  }
-
-  Status CreateIndex(pgwrapper::PGConn& conn) override {
-    return conn.Execute(Format(
-        "CREATE INDEX $0 ON $1 (id2 ASC) WITH(colocation_id =111112)", kIndexName, kTableName));
-  }
+  bool IsColocated() override { return true; }
 };
 
 TEST_F(XClusterColocatedIndexTest, CreateIndexWithWorkload) {
@@ -586,4 +615,244 @@ TEST_F(XClusterDbScopedYsqlIndexProducerOnlyTest, IndexCheckpointLocation) {
   ASSERT_GT(cdc_row->checkpoint->index, OpId().Min().index);
 }
 
+class XClusterBiDirectionalIndexTest : public XClusterYsqlNonTransactionalTest,
+                                       public ::testing::WithParamInterface<bool> {
+ public:
+  static const xcluster::ReplicationGroupId kReverseReplicationGroupId;
+  static constexpr auto kWaitForOtherUniverseToCreateMsg =
+      "Waiting for index to be created on other universe";
+
+  bool IsColocated() override { return GetParam(); }
+
+  Status WaitForLogMessage(const std::string& message) {
+    return StringWaiterLogSink(message).WaitFor(kRpcTimeout * 1s);
+  }
+
+  virtual Result<std::vector<TableId>> GetRevReplicationTableIds() {
+    if (IsColocated()) {
+      return std::vector<TableId>{
+          VERIFY_RESULT(GetColocatedDatabaseParentTableId(&consumer_cluster_))};
+    }
+
+    std::vector<TableId> consumer_table_ids;
+    for (const auto& table : consumer_tables_) {
+      consumer_table_ids.push_back(table->id());
+    }
+    return consumer_table_ids;
+  }
+
+  virtual Result<xrepl::StreamId> GetSourceTableStreamId() {
+    if (IsColocated()) {
+      return GetCDCStreamID(VERIFY_RESULT(GetColocatedDatabaseParentTableId(&consumer_cluster_)));
+    }
+
+    return GetCDCStreamID(producer_table_->id());
+  }
+
+  void SetUp() override {
+    YB_SKIP_TEST_IN_TSAN();
+    ANNOTATE_UNPROTECTED_WRITE(FLAGS_auto_add_new_index_to_bidirectional_xcluster) = true;
+
+    XClusterYsqlNonTransactionalTest::SetUp();
+
+    // Setup the reverse replication.
+    ASSERT_OK(SetupUniverseReplication(
+        consumer_cluster(), producer_cluster(), producer_client(), kReverseReplicationGroupId,
+        ASSERT_RESULT(GetRevReplicationTableIds())));
+  }
+
+  Status WaitForReplicationsToCatchup() {
+    RETURN_NOT_OK_PREPEND(
+        WaitForReplicationDrain(
+            /*expected_num_nondrained=*/0, /*timeout_secs=*/kRpcTimeout,
+            /*target_time=*/std::nullopt, VERIFY_RESULT(GetReplicationTableIds())),
+        "Failed wait for forward replication drain");
+
+    RETURN_NOT_OK_PREPEND(
+        WaitForReplicationDrain(
+            /* expected_num_nondrained */ 0, /* timeout_secs */ kRpcTimeout,
+            /* target_time */ std::nullopt, VERIFY_RESULT(GetRevReplicationTableIds()),
+            consumer_client()),
+        "Failed wait for reverse replication drain");
+
+    return Status::OK();
+  }
+
+  Status InsertRowsAndValidate() {
+    RETURN_NOT_OK(WaitForReplicationsToCatchup());
+    RETURN_NOT_OK_PREPEND(ValidateRows(), "Row count do not match before insert");
+
+    for (int i = 0; i < 20; i++, row_count_++) {
+      if (row_count_ % 2) {
+        RETURN_NOT_OK(producer_conn_->ExecuteFormat(kInsertStmtFormat, row_count_));
+      } else {
+        RETURN_NOT_OK(consumer_conn_->ExecuteFormat(kInsertStmtFormat, row_count_));
+      }
+    }
+
+    RETURN_NOT_OK(WaitForReplicationsToCatchup());
+    RETURN_NOT_OK(ValidateRows());
+
+    return Status::OK();
+  }
+
+  std::shared_ptr<Synchronizer> AsyncCreateIndex(pgwrapper::PGConn& conn) {
+    auto sync = std::make_shared<Synchronizer>();
+    test_thread_holder_.AddThread(
+        [this, &conn, sync]() { sync->AsStdStatusCallback()(CreateIndex(conn)); });
+
+    return sync;
+  }
+
+ public:
+  TestThreadHolder test_thread_holder_;
+};
+
+const xcluster::ReplicationGroupId XClusterBiDirectionalIndexTest::kReverseReplicationGroupId(
+    "reverse_replication_group");
+
+INSTANTIATE_TEST_CASE_P(Regular, XClusterBiDirectionalIndexTest, ::testing::Values(false));
+INSTANTIATE_TEST_CASE_P(Colocated, XClusterBiDirectionalIndexTest, ::testing::Values(true));
+
+TEST_P(XClusterBiDirectionalIndexTest, CreateIndex) {
+  // Create a dummy table on producer, so that the table ids for the indexes do not match.
+  ASSERT_OK(producer_conn_->Execute("create type dummy"));
+
+  auto starting_backfill_sink = StringWaiterLogSink("starting backfill with timestamp:");
+  // Create index on producer.
+  auto producer_sync = AsyncCreateIndex(*producer_conn_);
+
+  // Wait for producer to get blocked.
+  ASSERT_OK(WaitForLogMessage(kWaitForOtherUniverseToCreateMsg));
+
+  // Make sure it does not make progress.
+  ASSERT_NOK(producer_sync->WaitFor(5s * kTimeMultiplier));
+  ASSERT_FALSE(starting_backfill_sink.IsEventOccurred());
+
+  // Create index on consumer.
+  auto consumer_sync = AsyncCreateIndex(*consumer_conn_);
+
+  // Validate.
+  ASSERT_OK_PREPEND(producer_sync->Wait(), "CreateIndex on producer failed");
+  ASSERT_OK_PREPEND(consumer_sync->Wait(), "CreateIndex on consumer failed");
+  ASSERT_TRUE(starting_backfill_sink.IsEventOccurred());
+
+  // The colocated table uses range partitioning so gets a index scan, whereas the non-colocated
+  // table uses hash partitioning and does a seq scan.
+  ASSERT_EQ(ASSERT_RESULT(producer_conn_->HasIndexScan(kId1CountStmt)), IsColocated());
+  ASSERT_EQ(ASSERT_RESULT(consumer_conn_->HasIndexScan(kId1CountStmt)), IsColocated());
+
+  ASSERT_TRUE(ASSERT_RESULT(producer_conn_->HasIndexScan(kId2CountStmt)));
+  ASSERT_TRUE(ASSERT_RESULT(consumer_conn_->HasIndexScan(kId2CountStmt)));
+
+  // Make sure the DocDB table ids were different.
+  {
+    auto producer_index = ASSERT_RESULT(
+        GetYsqlTable(&producer_cluster_, namespace_name, "" /* schema_name */, kIndexName));
+    auto consumer_index = ASSERT_RESULT(
+        GetYsqlTable(&consumer_cluster_, namespace_name, "" /* schema_name */, kIndexName));
+    ASSERT_NE(producer_index.table_id(), consumer_index.table_id());
+  }
+
+  ASSERT_OK(InsertRowsAndValidate());
+}
+
+TEST_P(XClusterBiDirectionalIndexTest, BlockBeforeBackfill) {
+  auto producer_sync = AsyncCreateIndex(*producer_conn_);
+
+  // Wait for producer to get to the backfill phase, and then set the blocking flag so that only
+  // the consumer hits it.
+  ASSERT_OK(WaitForLogMessage(kWaitForOtherUniverseToCreateMsg));
+  ASSERT_OK(SET_FLAG(ysql_yb_test_block_index_phase, "backfill"));
+  SleepFor(3s);
+
+  auto consumer_sync = AsyncCreateIndex(*consumer_conn_);
+
+  // Wait for consumer to hit the block and make sure both side do not make progress.
+  ASSERT_NOK(producer_sync->WaitFor(5s * kTimeMultiplier));
+  ASSERT_NOK(consumer_sync->WaitFor(0s));
+
+  // Unblock and wait for DDL to complete.
+  ASSERT_OK(SET_FLAG(ysql_yb_test_block_index_phase, "none"));
+  ASSERT_OK_PREPEND(producer_sync->Wait(), "CreateIndex on producer failed");
+  ASSERT_OK_PREPEND(consumer_sync->Wait(), "CreateIndex on consumer failed");
+
+  ASSERT_OK(InsertRowsAndValidate());
+}
+
+TEST_P(XClusterBiDirectionalIndexTest, PauseIndexedTable) {
+  // Pause the stream on producer and insert some rows.
+  auto stream_id = ASSERT_RESULT(GetSourceTableStreamId());
+  ASSERT_OK(PauseResumeXClusterProducerStreams({stream_id}, /*is_paused=*/true));
+  // Needs to sleep to wait for heartbeat to propagate.
+  SleepFor(3s * kTimeMultiplier);
+  for (int i = 0; i < 10; i++, row_count_++) {
+    ASSERT_OK(producer_conn_->ExecuteFormat(kInsertStmtFormat, row_count_));
+  }
+
+  auto producer_sync = AsyncCreateIndex(*producer_conn_);
+  auto consumer_sync = AsyncCreateIndex(*consumer_conn_);
+
+  // Wait for consumer to hit the block and make consumer does not make progress.
+  ASSERT_OK(WaitForLogMessage("Waiting for replication of indexed table"));
+  ASSERT_NOK(consumer_sync->WaitFor(5s * kTimeMultiplier));
+
+  ASSERT_OK_PREPEND(producer_sync->Wait(), "CreateIndex on producer failed");
+
+  // Unblock and wait for DDL to complete.
+  ASSERT_OK(PauseResumeXClusterProducerStreams({stream_id}, /*is_paused=*/false));
+  ASSERT_OK_PREPEND(consumer_sync->Wait(), "CreateIndex on consumer failed");
+
+  ASSERT_OK(InsertRowsAndValidate());
+}
+
+TEST_P(XClusterBiDirectionalIndexTest, CreateIndexWithWorkload) {
+  std::atomic<bool> run_workload(true);
+  auto se = ScopeExit([&run_workload] { run_workload = false; });
+
+  // Run workload in background.
+  test_thread_holder_.AddThread([this, &run_workload]() {
+    auto producer_conn = std::make_unique<pgwrapper::PGConn>(
+        ASSERT_RESULT(producer_cluster_.ConnectToDB(namespace_name)));
+    auto consumer_conn = std::make_unique<pgwrapper::PGConn>(
+        ASSERT_RESULT(consumer_cluster_.ConnectToDB(namespace_name)));
+
+    while (run_workload) {
+      Status status;
+      if (row_count_ % 2) {
+        LOG(INFO) << "Inserting row on producer: " << row_count_;
+        status = producer_conn->ExecuteFormat(kInsertStmtFormat, row_count_);
+      } else {
+        LOG(INFO) << "Inserting row on consumer: " << row_count_;
+        status = consumer_conn->ExecuteFormat(kInsertStmtFormat, row_count_);
+      }
+      if (!status.ok()) {
+        // Failure expected from the index create DDL. DDL version is bumped and propagated to pg
+        // clients asynchronously leading to transient errors.
+        ASSERT_STR_CONTAINS(status.message().ToString(), "schema version mismatch");
+      }
+      row_count_++;
+      SleepFor(100ms * kTimeMultiplier);
+    }
+  });
+
+  // Run the workload for some time.
+  SleepFor(3s * kTimeMultiplier);
+
+  // Slow down the index backfill by 3s at every state.
+  ASSERT_OK(SET_FLAG(ysql_yb_index_state_flags_update_delay, 3 * 1000));
+
+  auto producer_sync = AsyncCreateIndex(*producer_conn_);
+  auto consumer_sync = AsyncCreateIndex(*consumer_conn_);
+
+  ASSERT_OK_PREPEND(producer_sync->Wait(), "CreateIndex on producer failed");
+  ASSERT_OK_PREPEND(consumer_sync->Wait(), "CreateIndex on consumer failed");
+
+  // Stop the background workload.
+  run_workload = false;
+  test_thread_holder_.JoinAll();
+
+  ASSERT_OK(InsertRowsAndValidate());
+}
+
 }  // namespace yb
diff --git a/src/yb/master/CMakeLists.txt b/src/yb/master/CMakeLists.txt
index 7207c71187..a77ff8332c 100644
--- a/src/yb/master/CMakeLists.txt
+++ b/src/yb/master/CMakeLists.txt
@@ -134,6 +134,7 @@ set(MASTER_SRCS
   ts_manager.cc
   universe_key_registry_service.cc
   util/yql_vtable_helpers.cc
+  xcluster/add_index_to_bidirectional_xcluster_target_task.cc
   xcluster/add_table_to_xcluster_source_task.cc
   xcluster/add_table_to_xcluster_target_task.cc
   xcluster/master_xcluster_util.cc
diff --git a/src/yb/master/backfill_index.cc b/src/yb/master/backfill_index.cc
index e5abe2b63b..dfe87da92d 100644
--- a/src/yb/master/backfill_index.cc
+++ b/src/yb/master/backfill_index.cc
@@ -18,42 +18,31 @@
 #include <sys/types.h>
 
 #include <algorithm>
-#include <bitset>
 #include <functional>
 #include <memory>
 #include <mutex>
-#include <set>
 #include <string>
 #include <unordered_map>
 #include <vector>
 
 #include <boost/optional.hpp>
 #include <boost/preprocessor/cat.hpp>
+#include "yb/tserver/tserver_admin.proxy.h"
 #include "yb/util/logging.h"
 
-#include "yb/dockv/partial_row.h"
-#include "yb/dockv/partition.h"
 #include "yb/common/wire_protocol.h"
 
 #include "yb/docdb/doc_rowwise_iterator.h"
 
-#include "yb/gutil/atomicops.h"
-#include "yb/gutil/callback.h"
 #include "yb/gutil/casts.h"
-#include "yb/gutil/map-util.h"
-#include "yb/gutil/mathlimits.h"
 #include "yb/gutil/ref_counted.h"
-#include "yb/gutil/stl_util.h"
 #include "yb/gutil/strings/escaping.h"
-#include "yb/gutil/strings/join.h"
 #include "yb/gutil/strings/substitute.h"
-#include "yb/gutil/sysinfo.h"
 
 #include "yb/master/master_fwd.h"
 #include "yb/master/async_rpc_tasks.h"
 #include "yb/master/catalog_manager.h"
 #include "yb/master/master.h"
-#include "yb/master/master_error.h"
 #include "yb/master/master_ddl.pb.h"
 #include "yb/master/sys_catalog.h"
 #include "yb/master/tablet_split_manager.h"
@@ -63,16 +52,6 @@
 #include "yb/tablet/tablet_metadata.h"
 #include "yb/tablet/tablet_peer.h"
 
-#include "yb/tserver/tserver_admin.proxy.h"
-
-#include "yb/util/flags.h"
-#include "yb/util/format.h"
-#include "yb/util/math_util.h"
-#include "yb/util/monotime.h"
-#include "yb/util/random_util.h"
-#include "yb/util/result.h"
-#include "yb/util/scope_exit.h"
-#include "yb/util/status_format.h"
 #include "yb/util/status_log.h"
 #include "yb/util/threadpool.h"
 #include "yb/util/trace.h"
@@ -142,6 +121,8 @@ DEFINE_test_flag(bool, simulate_cannot_enable_compactions, false,
 DEFINE_test_flag(int32, delay_clearing_fully_applied_ms, 0,
     "Amount of time to delay clearing the fully applied schema.");
 
+DECLARE_bool(auto_add_new_index_to_bidirectional_xcluster);
+
 namespace yb {
 namespace master {
 
@@ -794,32 +775,18 @@ void BackfillTable::LaunchBackfillOrAbort() {
 Status BackfillTable::LaunchComputeSafeTimeForRead() {
   RSTATUS_DCHECK(!timestamp_chosen(), IllegalState, "Backfill timestamp already set");
 
-  if (master_->xcluster_manager()->IsTableReplicationConsumer(indexed_table_->id())) {
-    auto res = master_->xcluster_manager()->GetXClusterSafeTime(indexed_table_->namespace_id());
-    if (res.ok()) {
-      SCHECK(!res->is_special(), InvalidArgument, "Invalid xCluster safe time for namespace ",
-             indexed_table_->namespace_id());
+  std::vector<TableId> index_table_ids;
+  std::transform(
+      index_infos_.begin(), index_infos_.end(), std::back_inserter(index_table_ids),
+      [](const IndexInfoPB& idx_info) { return idx_info.table_id(); });
 
-      LOG_WITH_PREFIX(INFO) << "Using xCluster safe time " << *res << " as the backfill read time";
-      return SetSafeTimeAndStartBackfill(*res);
-    } else {
-      if (res.status().IsNotFound()) {
-        VLOG_WITH_PREFIX(1) << "Table does not belong to transactional replication, continue with "
-                               "GetSafeTimeForTablet";
-      } else {
-        return res.status();
-      }
-    }
+  auto opt_xcluster_backfill_time =
+      VERIFY_RESULT(master_->xcluster_manager()->TryGetXClusterSafeTimeForBackfill(
+          index_table_ids, indexed_table_, epoch()));
+
+  if (opt_xcluster_backfill_time) {
+    return SetSafeTimeAndStartBackfill(*opt_xcluster_backfill_time);
   }
-  // NOTE: Colocated indexes in a transactional xCluster will use the regular tablet safe time.
-  // Only the parent table is part of the xCluster replication, so new data that is added to the
-  // index on the source universe automatically flows to the target universe even before the index
-  // is created on it.
-  // We still need to run backfill since the WAL entries for the backfill are NOT replicated via
-  // xCluster. This is because both backfill entries and xCluster replicated entries use the same
-  // external HT field. To ensure transactional correctness we just need to pick a time higher than
-  // the time that was picked on the source side. Since the table is created on the source universe
-  // before the target this is always guaranteed to be true.
 
   auto tablets = VERIFY_RESULT(indexed_table_->GetTablets());
   num_tablets_.store(tablets.size(), std::memory_order_release);
diff --git a/src/yb/master/catalog_manager.cc b/src/yb/master/catalog_manager.cc
index 2590b01195..17e4f9d3aa 100644
--- a/src/yb/master/catalog_manager.cc
+++ b/src/yb/master/catalog_manager.cc
@@ -7715,6 +7715,9 @@ Status CatalogManager::GetTableSchemaInternal(const GetTableSchemaRequestPB* req
             << ":\n"
             << yb::ToString(l->pb.indexes());
   }
+
+  resp->set_is_backfilling(table->IsBackfilling());
+
   resp->set_is_compatible_with_previous_version(l->pb.updates_only_index_permissions());
   resp->mutable_partition_schema()->CopyFrom(l->pb.partition_schema());
   if (IsReplicationInfoSet(l->pb.replication_info())) {
diff --git a/src/yb/master/master_ddl.proto b/src/yb/master/master_ddl.proto
index 4f900fee72..66f08b0e05 100644
--- a/src/yb/master/master_ddl.proto
+++ b/src/yb/master/master_ddl.proto
@@ -243,6 +243,11 @@ message GetTableSchemaResponsePB {
 
   // If the table is rewritten, this field explicitly stores the PG table id.
   optional bytes pg_table_id = 18;
+
+  // Added on Oct 2024 (v2025.1). This will not be set on versions before this
+  // change, and there is no direct AutoFlag protecting this, so use with
+  // caution.
+  optional bool is_backfilling = 19; // [default = false]
 }
 
 message GetTablegroupSchemaRequestPB {
diff --git a/src/yb/master/xcluster/add_index_to_bidirectional_xcluster_target_task.cc b/src/yb/master/xcluster/add_index_to_bidirectional_xcluster_target_task.cc
new file mode 100644
index 0000000000..e08ecd91e4
--- /dev/null
+++ b/src/yb/master/xcluster/add_index_to_bidirectional_xcluster_target_task.cc
@@ -0,0 +1,171 @@
+// Copyright (c) YugabyteDB, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
+// in compliance with the License.  You may obtain a copy of the License at
+//
+// http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software distributed under the License
+// is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
+// or implied.  See the License for the specific language governing permissions and limitations
+// under the License.
+//
+
+#include "yb/master/xcluster/add_index_to_bidirectional_xcluster_target_task.h"
+
+#include "yb/client/table_info.h"
+#include "yb/client/xcluster_client.h"
+#include "yb/client/yb_table_name.h"
+#include "yb/common/xcluster_util.h"
+
+#include "yb/master/catalog_manager.h"
+#include "yb/master/master.h"
+#include "yb/master/master_ddl.pb.h"
+#include "yb/master/xcluster/xcluster_replication_group.h"
+#include "yb/master/xcluster/xcluster_manager_if.h"
+#include "yb/util/is_operation_done_result.h"
+
+namespace yb::master {
+
+#define SCHEDULE_WITH_DELAY_AND_RETURN(task, ...) \
+  SCHECK_LT(CoarseMonoClock::Now(), deadline_, TimedOut, Format("$0 Timed out", LogPrefix())); \
+  ScheduleNextStepWithDelay( \
+      std::bind(&AddBiDirectionalIndexToXClusterTargetTask::task, this, ##__VA_ARGS__), #task, \
+      kScheduleDelay); \
+  return Status::OK()
+
+#define SCHEDULE_AND_RETURN(task, ...) \
+  ScheduleNextStep( \
+      std::bind(&AddBiDirectionalIndexToXClusterTargetTask::task, this, ##__VA_ARGS__), #task); \
+  return Status::OK()
+
+namespace {
+const MonoDelta kScheduleDelay = MonoDelta::FromMilliseconds(200);
+}
+
+AddBiDirectionalIndexToXClusterTargetTask::AddBiDirectionalIndexToXClusterTargetTask(
+    TableInfoPtr index_table_info, scoped_refptr<UniverseReplicationInfo> universe,
+    std::shared_ptr<client::XClusterRemoteClientHolder> remote_client, Master& master,
+    const LeaderEpoch& epoch, CoarseTimePoint deadline)
+    : MultiStepTableTaskBase(
+          *master.catalog_manager_impl(), *master.catalog_manager()->AsyncTaskPool(),
+          *master.messenger(), std::move(index_table_info), epoch),
+      universe_(universe),
+      remote_client_(std::move(remote_client)),
+      xcluster_manager_(*master.catalog_manager()->GetXClusterManager()),
+      master_(master),
+      deadline_(deadline) {}
+
+std::string AddBiDirectionalIndexToXClusterTargetTask::description() const {
+  return Format("AddBiDirectionalIndexToXClusterTargetTask [$0]", table_info_->id());
+}
+
+Status AddBiDirectionalIndexToXClusterTargetTask::FirstStep() {
+  SCHECK_FORMAT(table_info_->is_index(), IllegalState, "$0 is not an index", table_info_->id());
+
+  // Skip if the table has already been added to this replication group.
+  if (VERIFY_RESULT(HasTable(*universe_, *table_info_, catalog_manager_))) {
+    LOG_WITH_PREFIX(INFO) << "Table " << table_info_->ToString()
+                          << " is already part of xCluster universe replication "
+                          << universe_->ToString();
+    Complete();
+    return Status::OK();
+  }
+
+  return GetSourceIndexTableId();
+}
+
+Status AddBiDirectionalIndexToXClusterTargetTask::GetSourceIndexTableId() {
+  auto source_tables_resp = VERIFY_RESULT(
+      remote_client_->GetYbClient().ListTables(table_info_->name(), table_info_->namespace_name()));
+
+  // Filter on the pgschema_name.
+  const auto pgschema_name = table_info_->pgschema_name();
+  auto source_table = std::find_if(
+      source_tables_resp.tables().begin(), source_tables_resp.tables().end(),
+      [&pgschema_name](const auto& table) { return table.pgschema_name() == pgschema_name; });
+
+  if (source_table == source_tables_resp.tables().end() ||
+      source_table->state() != SysTablesEntryPB_State_RUNNING) {
+    YB_LOG_WITH_PREFIX_EVERY_N_SECS(WARNING, 10)
+        << "Waiting for index to be created on other universe";
+    SCHEDULE_WITH_DELAY_AND_RETURN(GetSourceIndexTableId);
+  }
+
+  SCHECK(
+      source_table->has_indexed_table_id() && !source_table->indexed_table_id().empty(),
+      IllegalState, Format("Source table $0 is not an index", source_table->id()));
+
+  source_index_table_id_ = source_table->id();
+  source_indexed_table_id_ = source_table->indexed_table_id();
+
+  SCHEDULE_AND_RETURN(WaitForBackfillIndexToStartOnSource);
+}
+
+Status AddBiDirectionalIndexToXClusterTargetTask::WaitForBackfillIndexToStartOnSource() {
+  SCHECK(!source_index_table_id_.empty(), IllegalState, "Source index table id is empty");
+  SCHECK(!source_indexed_table_id_.empty(), IllegalState, "Source indexed table id is empty");
+
+  auto backfill_started = VERIFY_RESULT(remote_client_->GetYbClient().IsBackfillIndexStarted(
+      source_index_table_id_, source_indexed_table_id_, deadline_));
+
+  if (!backfill_started) {
+    YB_LOG_WITH_PREFIX_EVERY_N_SECS(INFO, 10)
+        << "Waiting for backfill of index " << source_index_table_id_
+        << " to start on other universe";
+    SCHEDULE_WITH_DELAY_AND_RETURN(WaitForBackfillIndexToStartOnSource);
+  }
+
+  if (table_info_->colocated()) {
+    // The parent tablet is already part of xCluster replication.
+    Complete();
+    return Status::OK();
+  }
+
+  SCHEDULE_AND_RETURN(GetSourceIndexStreamId);
+}
+
+Status AddBiDirectionalIndexToXClusterTargetTask::GetSourceIndexStreamId() {
+  RSTATUS_DCHECK(!source_index_table_id_.empty(), IllegalState, "Source index table id is empty");
+
+  auto stream_ids =
+      VERIFY_RESULT(remote_client_->GetXClusterClient().GetXClusterStreams(source_index_table_id_));
+
+  SCHECK_EQ(
+      stream_ids.size(), 1, IllegalState,
+      Format("Expected exactly one stream for source table $0", source_index_table_id_));
+
+  auto& stream_id = stream_ids.front();
+
+  SCHEDULE_AND_RETURN(AddIndexToReplicationGroup, stream_id);
+}
+
+Status AddBiDirectionalIndexToXClusterTargetTask::AddIndexToReplicationGroup(
+    const xrepl::StreamId& stream_id) {
+  RETURN_NOT_OK(xcluster_manager_.AddTableToReplicationGroup(
+      universe_->ReplicationGroupId(), source_index_table_id_, stream_id,
+      /*target_table_id=*/std::nullopt, epoch_));
+
+  SCHEDULE_WITH_DELAY_AND_RETURN(WaitForSetupReplication);
+}
+
+Status AddBiDirectionalIndexToXClusterTargetTask::WaitForSetupReplication() {
+  // Perform health checks so that the Create Index DDL fails when replication is not healthy.
+  auto operation_result = VERIFY_RESULT(xcluster_manager_.IsSetupUniverseReplicationDone(
+      xcluster::GetAlterReplicationGroupId(universe_->ReplicationGroupId()),
+      /*skip_health_check=*/false));
+
+  if (!operation_result.done()) {
+    VLOG_WITH_PREFIX(2) << "Waiting for setup universe replication to finish";
+    // If this takes too long the table creation will timeout and abort the task.
+    SCHEDULE_WITH_DELAY_AND_RETURN(WaitForSetupReplication);
+  }
+
+  RETURN_NOT_OK(operation_result.status());
+
+  Complete();
+
+  return Status::OK();
+}
+
+}  // namespace yb::master
diff --git a/src/yb/master/xcluster/add_index_to_bidirectional_xcluster_target_task.h b/src/yb/master/xcluster/add_index_to_bidirectional_xcluster_target_task.h
new file mode 100644
index 0000000000..8a1cfb7416
--- /dev/null
+++ b/src/yb/master/xcluster/add_index_to_bidirectional_xcluster_target_task.h
@@ -0,0 +1,65 @@
+// Copyright (c) YugabyteDB, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
+// in compliance with the License.  You may obtain a copy of the License at
+//
+// http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software distributed under the License
+// is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
+// or implied.  See the License for the specific language governing permissions and limitations
+// under the License.
+//
+
+#pragma once
+
+#include "yb/master/catalog_entity_info.h"
+#include "yb/master/catalog_entity_tasks.h"
+
+namespace yb {
+
+namespace client {
+class XClusterRemoteClientHolder;
+}  // namespace client
+
+namespace master {
+
+// Wait for the index DocDB table on the other universe to get created, reach the backfill stage
+// and then add the index to replication.
+// For colocated indexes exits after waiting for the backfill to start, since parent tablet is
+// already part of replication.
+class AddBiDirectionalIndexToXClusterTargetTask : public MultiStepTableTaskBase {
+ public:
+  AddBiDirectionalIndexToXClusterTargetTask(
+      TableInfoPtr index_table_info, scoped_refptr<UniverseReplicationInfo> universe,
+      std::shared_ptr<client::XClusterRemoteClientHolder> remote_client, Master& master,
+      const LeaderEpoch& epoch, CoarseTimePoint deadline);
+
+  server::MonitoredTaskType type() const override {
+    return server::MonitoredTaskType::kAddTableToXClusterTarget;
+  }
+
+  std::string type_name() const override { return "Add Index to bi-directional xCluster"; }
+
+  std::string description() const override;
+
+ private:
+  Status FirstStep() override;
+  Status GetSourceIndexTableId();
+  Status WaitForBackfillIndexToStartOnSource();
+  Status GetSourceIndexStreamId();
+  Status AddIndexToReplicationGroup(const xrepl::StreamId& stream_id);
+  Status WaitForSetupReplication();
+
+  scoped_refptr<UniverseReplicationInfo> universe_;
+  std::shared_ptr<client::XClusterRemoteClientHolder> remote_client_;
+  XClusterManagerIf& xcluster_manager_;
+  Master& master_;
+  const CoarseTimePoint deadline_;
+
+  TableId source_index_table_id_;
+  TableId source_indexed_table_id_;
+};
+
+}  // namespace master
+}  // namespace yb
diff --git a/src/yb/master/xcluster/add_table_to_xcluster_source_task.cc b/src/yb/master/xcluster/add_table_to_xcluster_source_task.cc
index 2a2d0d5815..49751dfbea 100644
--- a/src/yb/master/xcluster/add_table_to_xcluster_source_task.cc
+++ b/src/yb/master/xcluster/add_table_to_xcluster_source_task.cc
@@ -75,4 +75,38 @@ Status AddTableToXClusterSourceTask::MarkTableAsCheckpointed() {
   return Status::OK();
 }
 
+CreateXClusterStreamForBiDirectionalIndexTask::CreateXClusterStreamForBiDirectionalIndexTask(
+    std::function<Result<xrepl::StreamId>(
+        const TableId&, const LeaderEpoch& epoch, StdStatusCallback callback)>
+        checkpoint_table_func,
+    CatalogManager& catalog_manager, rpc::Messenger& messenger, TableInfoPtr table_info,
+    const LeaderEpoch& epoch)
+    : PostTabletCreateTaskBase(
+          catalog_manager, *catalog_manager.AsyncTaskPool(), messenger, std::move(table_info),
+          epoch),
+      checkpoint_table_func_(std::move(checkpoint_table_func)) {}
+
+std::string CreateXClusterStreamForBiDirectionalIndexTask::description() const {
+  return Format("CreateXClusterStreamForBiDirectionalIndexTask [$0]", table_info_->id());
+}
+
+Status CreateXClusterStreamForBiDirectionalIndexTask::FirstStep() {
+  RETURN_NOT_OK(checkpoint_table_func_(
+      table_info_->id(), epoch_,
+      std::bind(
+          &CreateXClusterStreamForBiDirectionalIndexTask::CheckpointCompletionCallback, this, _1)));
+
+  return Status::OK();
+}
+
+void CreateXClusterStreamForBiDirectionalIndexTask::CheckpointCompletionCallback(
+    const Status& status) {
+  if (!status.ok()) {
+    AbortAndReturnPrevState(status);
+    return;
+  }
+
+  Complete();
+}
+
 }  // namespace yb::master
diff --git a/src/yb/master/xcluster/add_table_to_xcluster_source_task.h b/src/yb/master/xcluster/add_table_to_xcluster_source_task.h
index 245cdd405c..fd3ff41d0b 100644
--- a/src/yb/master/xcluster/add_table_to_xcluster_source_task.h
+++ b/src/yb/master/xcluster/add_table_to_xcluster_source_task.h
@@ -48,4 +48,32 @@ class AddTableToXClusterSourceTask : public PostTabletCreateTaskBase {
   const std::shared_ptr<XClusterOutboundReplicationGroup> outbound_replication_group_;
 };
 
+// Same as AddTableToXClusterSourceTask but does not require an outbound replication group.
+// This is used for bi-directional xCluster indexes only.
+class CreateXClusterStreamForBiDirectionalIndexTask : public PostTabletCreateTaskBase {
+ public:
+  explicit CreateXClusterStreamForBiDirectionalIndexTask(
+      std::function<Result<xrepl::StreamId>(const TableId&, const LeaderEpoch&, StdStatusCallback)>
+          checkpoint_table_func,
+      CatalogManager& catalog_manager, rpc::Messenger& messenger, TableInfoPtr table_info,
+      const LeaderEpoch& epoch);
+
+  server::MonitoredTaskType type() const override {
+    return server::MonitoredTaskType::kAddTableToXClusterSource;
+  }
+
+  std::string type_name() const override { return "Create xCluster stream for table"; }
+
+  std::string description() const override;
+
+ private:
+  Status FirstStep() override;
+
+  void CheckpointCompletionCallback(const Status& status);
+
+  std::function<Result<xrepl::StreamId>(
+      const TableId&, const LeaderEpoch& epoch, StdStatusCallback callback)>
+      checkpoint_table_func_;
+};
+
 }  // namespace yb::master
diff --git a/src/yb/master/xcluster/xcluster_manager.cc b/src/yb/master/xcluster/xcluster_manager.cc
index be8d17a0e9..558066b2ff 100644
--- a/src/yb/master/xcluster/xcluster_manager.cc
+++ b/src/yb/master/xcluster/xcluster_manager.cc
@@ -58,6 +58,11 @@ DEFINE_test_flag(bool, force_automatic_ddl_replication_mode, false,
     "Make XClusterCreateOutboundReplicationGroup always use automatic instead of semi-automatic "
     "xCluster replication mode.");
 
+DEFINE_RUNTIME_bool(auto_add_new_index_to_bidirectional_xcluster, false,
+    "If the indexed table is part of a bi-directional xCluster setup, then automatically add new "
+    "indexes for this table to replication. This flag must be set on both universes, and the index "
+    "must be created concurrently on both universes.");
+
 #define LOG_FUNC_AND_RPC \
   LOG_WITH_FUNC(INFO) << req->ShortDebugString() << ", from: " << RequestorString(rpc)
 
@@ -269,8 +274,11 @@ Status XClusterManager::GetXClusterSafeTime(
   return XClusterTargetManager::GetXClusterSafeTime(resp, epoch);
 }
 
-Result<HybridTime> XClusterManager::GetXClusterSafeTime(const NamespaceId& namespace_id) const {
-  return XClusterTargetManager::GetXClusterSafeTime(namespace_id);
+Result<std::optional<HybridTime>> XClusterManager::TryGetXClusterSafeTimeForBackfill(
+    const std::vector<TableId>& index_table_ids, const TableInfoPtr& indexed_table,
+    const LeaderEpoch& epoch) const {
+  return XClusterTargetManager::TryGetXClusterSafeTimeForBackfill(
+      index_table_ids, indexed_table, epoch);
 }
 
 Status XClusterManager::GetXClusterSafeTimeForNamespace(
@@ -715,6 +723,14 @@ bool XClusterManager::IsTableReplicated(const TableId& table_id) const {
          XClusterTargetManager::IsTableReplicated(table_id);
 }
 
+bool XClusterManager::IsTableBiDirectionallyReplicated(const TableId& table_id) const {
+  // In theory this would return true for B in the case of chaining A -> B -> C, but we don't
+  // support chaining in xCluster.
+  // Replicating between 3 universes will need bi-directional xCluster A <=> B <=> C <=> A.
+  return XClusterSourceManager::IsTableReplicated(table_id) &&
+         XClusterTargetManager::IsTableReplicated(table_id);
+}
+
 Status XClusterManager::HandleTabletSplit(
     const TableId& consumer_table_id, const SplitTabletIds& split_tablet_ids,
     const LeaderEpoch& epoch) {
diff --git a/src/yb/master/xcluster/xcluster_manager.h b/src/yb/master/xcluster/xcluster_manager.h
index d41248cef0..7ddd0dc4ff 100644
--- a/src/yb/master/xcluster/xcluster_manager.h
+++ b/src/yb/master/xcluster/xcluster_manager.h
@@ -91,7 +91,9 @@ class XClusterManager : public XClusterManagerIf,
   Status GetXClusterSafeTime(
       const GetXClusterSafeTimeRequestPB* req, GetXClusterSafeTimeResponsePB* resp,
       rpc::RpcContext* rpc, const LeaderEpoch& epoch);
-  Result<HybridTime> GetXClusterSafeTime(const NamespaceId& namespace_id) const override;
+  Result<std::optional<HybridTime>> TryGetXClusterSafeTimeForBackfill(
+      const std::vector<TableId>& index_table_ids, const TableInfoPtr& indexed_table,
+      const LeaderEpoch& epoch) const override;
 
   Status GetXClusterSafeTimeForNamespace(
       const GetXClusterSafeTimeForNamespaceRequestPB* req,
@@ -250,6 +252,8 @@ class XClusterManager : public XClusterManagerIf,
 
   bool IsTableReplicationConsumer(const TableId& table_id) const override;
 
+  bool IsTableBiDirectionallyReplicated(const TableId& table_id) const override;
+
   Status HandleTabletSplit(
       const TableId& consumer_table_id, const SplitTabletIds& split_tablet_ids,
       const LeaderEpoch& epoch) override;
diff --git a/src/yb/master/xcluster/xcluster_manager_if.h b/src/yb/master/xcluster/xcluster_manager_if.h
index 3798a3477e..6de344ddf0 100644
--- a/src/yb/master/xcluster/xcluster_manager_if.h
+++ b/src/yb/master/xcluster/xcluster_manager_if.h
@@ -44,7 +44,9 @@ struct XClusterStatus;
 
 class XClusterManagerIf {
  public:
-  virtual Result<HybridTime> GetXClusterSafeTime(const NamespaceId& namespace_id) const = 0;
+  virtual Result<std::optional<HybridTime>> TryGetXClusterSafeTimeForBackfill(
+      const std::vector<TableId>& index_table_ids, const TableInfoPtr& indexed_table,
+      const LeaderEpoch& epoch) const = 0;
   virtual Status RefreshXClusterSafeTimeMap(const LeaderEpoch& epoch) = 0;
   virtual Result<XClusterNamespaceToSafeTimeMap> GetXClusterNamespaceToSafeTimeMap() const = 0;
   virtual Status SetXClusterNamespaceToSafeTimeMap(
@@ -102,6 +104,8 @@ class XClusterManagerIf {
   virtual Result<IsOperationDoneResult> IsSetupUniverseReplicationDone(
       const xcluster::ReplicationGroupId& replication_group_id, bool skip_health_check) = 0;
 
+  virtual bool IsTableBiDirectionallyReplicated(const TableId& table_id) const = 0;
+
  protected:
   virtual ~XClusterManagerIf() = default;
 };
diff --git a/src/yb/master/xcluster/xcluster_replication_group.cc b/src/yb/master/xcluster/xcluster_replication_group.cc
index ecc54a696e..6dc751c0bd 100644
--- a/src/yb/master/xcluster/xcluster_replication_group.cc
+++ b/src/yb/master/xcluster/xcluster_replication_group.cc
@@ -38,6 +38,8 @@ DEFINE_RUNTIME_bool(xcluster_skip_health_check_on_replication_setup, false,
 DEFINE_test_flag(bool, exit_unfinished_deleting, false,
     "Whether to exit part way through the deleting universe process.");
 
+DECLARE_bool(auto_add_new_index_to_bidirectional_xcluster);
+
 namespace yb::master {
 
 namespace {
@@ -307,6 +309,13 @@ Result<bool> ShouldAddTableToReplicationGroup(
     CatalogManager& catalog_manager) {
   const auto& table_pb = table_info.old_pb();
 
+  SCHECK(
+      !table_info.IsColocationParentTable(), IllegalState,
+      Format(
+          "Colocated parent tables can only be added during the initial xCluster replication "
+          "setup: $0",
+          table_info.ToString()));
+
   if (!IsTableEligibleForXClusterReplication(table_info)) {
     return false;
   }
@@ -339,37 +348,37 @@ Result<bool> ShouldAddTableToReplicationGroup(
   }
 
   // Skip if the table has already been added to this replication group.
+  return !VERIFY_RESULT(HasTable(universe, table_info, catalog_manager));
+}
+
+Result<bool> HasTable(
+    UniverseReplicationInfo& universe, const TableInfo& table_info,
+    CatalogManager& catalog_manager) {
   auto cluster_config = catalog_manager.ClusterConfig();
-  {
-    auto l = cluster_config->LockForRead();
-    const auto& consumer_registry = l->pb.consumer_registry();
+  auto l = cluster_config->LockForRead();
+  const auto& consumer_registry = l->pb.consumer_registry();
 
-    auto producer_entry =
-        FindOrNull(consumer_registry.producer_map(), universe.ReplicationGroupId().ToString());
-    if (producer_entry) {
-      SCHECK(
-          !producer_entry->disable_stream(), IllegalState,
-          "Table belongs to xCluster replication group $0 which is currently disabled",
-          universe.ReplicationGroupId());
-      for (auto& [stream_id, stream_info] : producer_entry->stream_map()) {
-        if (stream_info.consumer_table_id() == table_info.id()) {
-          VLOG(1) << "Table " << table_info.ToString()
-                  << " is already part of xcluster replication " << stream_id;
-
-          return false;
-        }
-      }
-    }
+  auto producer_entry =
+      FindOrNull(consumer_registry.producer_map(), universe.ReplicationGroupId().ToString());
+  if (!producer_entry) {
+    return false;
   }
 
   SCHECK(
-      !table_info.IsColocationParentTable(), IllegalState,
-      Format(
-          "Colocated parent tables can only be added during the initial xCluster replication "
-          "setup: $0",
-          table_info.ToString()));
+      !producer_entry->disable_stream(), IllegalState,
+      "Table belongs to xCluster replication group $0 which is currently disabled",
+      universe.ReplicationGroupId());
 
-  return true;
+  for (auto& [stream_id, stream_info] : producer_entry->stream_map()) {
+    if (stream_info.consumer_table_id() == table_info.id()) {
+      VLOG(1) << "Table " << table_info.ToString() << " is already part of xcluster replication "
+              << stream_id;
+
+      return true;
+    }
+  }
+
+  return false;
 }
 
 Result<NamespaceId> GetProducerNamespaceId(
diff --git a/src/yb/master/xcluster/xcluster_replication_group.h b/src/yb/master/xcluster/xcluster_replication_group.h
index 658835f4d2..516e3f7f7d 100644
--- a/src/yb/master/xcluster/xcluster_replication_group.h
+++ b/src/yb/master/xcluster/xcluster_replication_group.h
@@ -67,6 +67,11 @@ Result<bool> ShouldAddTableToReplicationGroup(
     UniverseReplicationInfo& universe, const TableInfo& table_info,
     CatalogManager& catalog_manager);
 
+// Check if the table is part of the replication group.
+Result<bool> HasTable(
+    UniverseReplicationInfo& universe, const TableInfo& table_info,
+    CatalogManager& catalog_manager);
+
 Result<NamespaceId> GetProducerNamespaceId(
     UniverseReplicationInfo& universe, const NamespaceId& consumer_namespace_id);
 
diff --git a/src/yb/master/xcluster/xcluster_source_manager.cc b/src/yb/master/xcluster/xcluster_source_manager.cc
index 9508bfae32..921c5f1486 100644
--- a/src/yb/master/xcluster/xcluster_source_manager.cc
+++ b/src/yb/master/xcluster/xcluster_source_manager.cc
@@ -26,6 +26,7 @@
 #include "yb/master/xcluster/add_table_to_xcluster_source_task.h"
 #include "yb/master/xcluster/master_xcluster_util.h"
 #include "yb/master/xcluster/xcluster_catalog_entity.h"
+#include "yb/master/xcluster/xcluster_manager_if.h"
 #include "yb/master/xcluster/xcluster_outbound_replication_group.h"
 #include "yb/master/xcluster/xcluster_outbound_replication_group_tasks.h"
 #include "yb/master/xcluster/xcluster_status.h"
@@ -39,6 +40,7 @@ DEFINE_RUNTIME_bool(enable_tablet_split_of_xcluster_bootstrapping_tables, false,
     "When set, it enables automatic tablet splitting for tables that are part of an "
     "xCluster replication setup and are currently being bootstrapped for xCluster.");
 
+DECLARE_bool(auto_add_new_index_to_bidirectional_xcluster);
 DECLARE_int32(master_yb_client_default_timeout_ms);
 DECLARE_uint32(cdc_wal_retention_time_secs);
 DECLARE_bool(TEST_disable_cdc_state_insert_on_setup);
@@ -271,6 +273,26 @@ XClusterSourceManager::GetPostTabletCreateTasks(
     }
   }
 
+  if (FLAGS_auto_add_new_index_to_bidirectional_xcluster && table_info->is_index() &&
+      master_.xcluster_manager()->IsTableBiDirectionallyReplicated(
+          table_info->indexed_table_id())) {
+    DCHECK(tasks.empty()) << "BiDirectional table should not have any DB Scoped table tasks";
+    if (!tasks.empty()) {
+      // During a switch over we will have Bi-directional xCluster with DB scoped replication
+      // groups. But we do not expect to receive any DDLs at this time.
+      table_info->SetCreateTableErrorStatus(STATUS_FORMAT(
+          IllegalState,
+          "Index $0 created while its base table $1 is under bi-directional xCluster replication, "
+          "and has DB scoped replication groups",
+          table_info->id(), table_info->indexed_table_id()));
+      return {};
+    }
+
+    tasks.emplace_back(std::make_shared<CreateXClusterStreamForBiDirectionalIndexTask>(
+        std::bind(&XClusterSourceManager::CreateNonTxnStreamForNewTable, this, _1, _2, _3),
+        catalog_manager_, *master_.messenger(), table_info, epoch));
+  }
+
   return tasks;
 }
 
@@ -460,6 +482,16 @@ XClusterSourceManager::CreateStreamsForDbScoped(
       table_ids, SysCDCStreamEntryPB::INITIATED, cdc::StreamModeTransactional::kTrue, epoch);
 }
 
+Result<xrepl::StreamId> XClusterSourceManager::CreateNonTxnStreamForNewTable(
+    const TableId& table_id, const LeaderEpoch& epoch, StdStatusCallback callback) {
+  auto stream_id = VERIFY_RESULT(CreateNewXClusterStreamForTable(
+      table_id, cdc::StreamModeTransactional::kFalse, SysCDCStreamEntryPB::INITIATED, epoch,
+      /*callback=*/nullptr));
+  // We have to explicitly checkpoint the stream since it is created in INITIATED state.
+  RETURN_NOT_OK(CheckpointStreamsToOp0({{table_id, stream_id}}, std::move(callback)));
+  return stream_id;
+}
+
 Result<std::unique_ptr<XClusterCreateStreamsContext>> XClusterSourceManager::CreateStreamsInternal(
     const std::vector<TableId>& table_ids, SysCDCStreamEntryPB::State state,
     cdc::StreamModeTransactional transactional, const LeaderEpoch& epoch) {
@@ -525,6 +557,8 @@ Status XClusterSourceManager::CheckpointXClusterStreams(
 Status XClusterSourceManager::CheckpointStreamsToOp0(
     const std::vector<std::pair<TableId, xrepl::StreamId>>& table_streams,
     StdStatusCallback user_callback) {
+  VLOG_WITH_FUNC(1) << yb::ToString(table_streams);
+
   std::vector<cdc::CDCStateTableEntry> entries;
   for (const auto& [table_id, stream_id] : table_streams) {
     auto stripped_table_id = xcluster::StripSequencesDataAliasIfPresent(table_id);
@@ -907,7 +941,10 @@ std::vector<CDCStreamInfoPtr> XClusterSourceManager::GetStreamsForTable(
 
 Result<xrepl::StreamId> XClusterSourceManager::CreateNewXClusterStreamForTable(
     const TableId& table_id, cdc::StreamModeTransactional transactional,
-    const std::optional<SysCDCStreamEntryPB::State>& initial_state, const LeaderEpoch& epoch) {
+    const std::optional<SysCDCStreamEntryPB::State>& initial_state, const LeaderEpoch& epoch,
+    StdStatusCallback callback) {
+  VLOG_WITH_FUNC(1) << YB_STRUCT_TO_STRING(table_id, transactional, initial_state);
+
   auto stripped_table_id = xcluster::StripSequencesDataAliasIfPresent(table_id);
   auto table_info = VERIFY_RESULT(catalog_manager_.FindTableById(stripped_table_id));
 
@@ -931,12 +968,13 @@ Result<xrepl::StreamId> XClusterSourceManager::CreateNewXClusterStreamForTable(
   // populating entries in cdc_state.
   if (PREDICT_FALSE(FLAGS_TEST_disable_cdc_state_insert_on_setup) ||
       state != master::SysCDCStreamEntryPB::ACTIVE) {
+    if (callback) {
+      callback(Status::OK());
+    }
     return stream_id;
   }
 
-  Synchronizer sync;
-  RETURN_NOT_OK(CheckpointStreamsToOp0({{table_id, stream_id}}, sync.AsStatusFunctor()));
-  RETURN_NOT_OK(sync.Wait());
+  RETURN_NOT_OK(CheckpointStreamsToOp0({{table_id, stream_id}}, std::move(callback)));
 
   return stream_id;
 }
@@ -1091,12 +1129,15 @@ Status XClusterSourceManager::MarkIndexBackfillCompleted(
   // Checkpoint xCluster streams of indexes after the backfill completes. The backfilled data is not
   // replicated, and the target cluster performs its own backfill, so we can skip streaming changes
   // before the backfill completion.
+  // For BiDirectional indexes we create the index on both sides at the same time, and add them to
+  // replication before backfill completes, so we cannot perform this optimization.
 
   std::vector<std::pair<TableId, xrepl::StreamId>> table_streams;
   {
     SharedLock l(tables_to_stream_map_mutex_);
     for (const auto& index_id : index_ids) {
-      if (tables_to_stream_map_.contains(index_id)) {
+      if (tables_to_stream_map_.contains(index_id) &&
+          !master_.xcluster_manager()->IsTableBiDirectionallyReplicated(index_id)) {
         for (const auto& stream : tables_to_stream_map_.at(index_id)) {
           LOG(INFO) << "Checkpointing xCluster stream " << stream->StreamId() << " of index "
                     << index_id << " to its end of WAL";
diff --git a/src/yb/master/xcluster/xcluster_source_manager.h b/src/yb/master/xcluster/xcluster_source_manager.h
index 88c5046c3b..d389319cfa 100644
--- a/src/yb/master/xcluster/xcluster_source_manager.h
+++ b/src/yb/master/xcluster/xcluster_source_manager.h
@@ -70,7 +70,8 @@ class XClusterSourceManager {
 
   Result<xrepl::StreamId> CreateNewXClusterStreamForTable(
       const TableId& table_id, cdc::StreamModeTransactional transactional,
-      const std::optional<SysCDCStreamEntryPB::State>& initial_state, const LeaderEpoch& epoch);
+      const std::optional<SysCDCStreamEntryPB::State>& initial_state, const LeaderEpoch& epoch,
+      StdStatusCallback callback);
 
  protected:
   XClusterSourceManager(
@@ -202,6 +203,8 @@ class XClusterSourceManager {
 
   Result<std::unique_ptr<XClusterCreateStreamsContext>> CreateStreamsForDbScoped(
       const std::vector<TableId>& table_ids, const LeaderEpoch& epoch);
+  Result<xrepl::StreamId> CreateNonTxnStreamForNewTable(
+      const TableId& table_id, const LeaderEpoch& epoch, StdStatusCallback callback);
 
   Result<std::unique_ptr<XClusterCreateStreamsContext>> CreateStreamsInternal(
       const std::vector<TableId>& table_ids, SysCDCStreamEntryPB::State state,
diff --git a/src/yb/master/xcluster/xcluster_target_manager.cc b/src/yb/master/xcluster/xcluster_target_manager.cc
index 4a6b55ca17..8b3ff502f3 100644
--- a/src/yb/master/xcluster/xcluster_target_manager.cc
+++ b/src/yb/master/xcluster/xcluster_target_manager.cc
@@ -13,6 +13,7 @@
 
 #include "yb/master/xcluster/xcluster_target_manager.h"
 
+#include "yb/client/xcluster_client.h"
 #include "yb/common/xcluster_util.h"
 
 #include "yb/gutil/strings/util.h"
@@ -21,6 +22,7 @@
 #include "yb/master/catalog_entity_info.pb.h"
 #include "yb/master/catalog_manager.h"
 #include "yb/master/master.h"
+#include "yb/master/xcluster/add_index_to_bidirectional_xcluster_target_task.h"
 #include "yb/master/xcluster/add_table_to_xcluster_target_task.h"
 #include "yb/master/xcluster/master_xcluster_util.h"
 #include "yb/master/xcluster/xcluster_bootstrap_helper.h"
@@ -31,6 +33,7 @@
 #include "yb/master/xcluster/xcluster_universe_replication_setup_helper.h"
 #include "yb/master/xcluster_consumer_registry_service.h"
 
+#include "yb/util/async_util.h"
 #include "yb/util/backoff_waiter.h"
 #include "yb/util/is_operation_done_result.h"
 #include "yb/util/jsonwriter.h"
@@ -41,6 +44,15 @@ DEFINE_RUNTIME_bool(xcluster_wait_on_ddl_alter, true,
     "When xCluster replication sends a DDL change, wait for the user to enter a "
     "compatible/matching entry.  Note: Can also set at runtime to resume after stall.");
 
+DEFINE_RUNTIME_uint32(add_new_index_to_bidirectional_xcluster_timeout_secs, 10 * 60,
+    "Time in seconds within which index must be created on other universe when the indexed table "
+    "is part of bidirectional xCluster replication. Applies only when "
+    "--auto_add_new_index_to_bidirectional_xcluster "
+    "is set.");
+
+DECLARE_bool(auto_add_new_index_to_bidirectional_xcluster);
+DECLARE_uint64(max_clock_skew_usec);
+
 namespace yb::master {
 
 XClusterTargetManager::XClusterTargetManager(
@@ -1323,4 +1335,154 @@ Status XClusterTargetManager::AddTableToReplicationGroup(
       master_, catalog_manager_, data, epoch);
 }
 
+Result<std::optional<HybridTime>> XClusterTargetManager::TryGetXClusterSafeTimeForBackfill(
+    const std::vector<TableId>& index_table_ids, const TableInfoPtr& indexed_table,
+    const LeaderEpoch& epoch) const {
+  auto& xcluster_manager = *master_.xcluster_manager();
+
+  SCHECK_FORMAT(
+      indexed_table->IsBackfilling(), IllegalState, "$0 is not backfilling", indexed_table->id());
+
+  const bool is_colocated = indexed_table->colocated();
+  auto indexed_table_id = indexed_table->id();
+
+  if (is_colocated) {
+    indexed_table_id = indexed_table->LockForRead()->pb.parent_table_id();
+  }
+
+  if (FLAGS_auto_add_new_index_to_bidirectional_xcluster &&
+      xcluster_manager.IsTableBiDirectionallyReplicated(indexed_table_id)) {
+    auto backfill_ht = VERIFY_RESULT_PREPEND(
+        PrepareAndGetBackfillTimeForBiDirectionalIndex(index_table_ids, indexed_table_id, epoch),
+        "Failed while preparing index for xCluster");
+
+    LOG(INFO) << "Using " << backfill_ht << " as the backfill read time";
+    return backfill_ht;
+  }
+
+  if (is_colocated) {
+    // Colocated indexes in a transactional xCluster will use the regular tablet safe time.
+    // Only the parent table is part of the xCluster replication, so new data that is added to the
+    // index on the source universe automatically flows to the target universe even before the index
+    // is created on it.
+    // We still need to run backfill since the WAL entries for the backfill are NOT replicated via
+    // xCluster. This is because both backfill entries and xCluster replicated entries use the same
+    // external HT field. To ensure transactional correctness we just need to pick a time higher
+    // than the time that was picked on the source side. Since the table is created on the source
+    // universe before the target this is always guaranteed to be true.
+
+    return std::nullopt;
+  }
+
+  if (xcluster_manager.IsTableReplicationConsumer(indexed_table_id)) {
+    auto safe_time_result = GetXClusterSafeTime(indexed_table->namespace_id());
+    if (safe_time_result.ok()) {
+      SCHECK(
+          !safe_time_result->is_special(), InvalidArgument,
+          "Invalid xCluster safe time for namespace ", indexed_table->namespace_id());
+
+      LOG(INFO) << "Using xCluster safe time " << *safe_time_result << " as the backfill read time";
+      return *safe_time_result;
+    }
+
+    if (safe_time_result.status().IsNotFound()) {
+      VLOG(1) << "Table " << indexed_table->id()
+              << "does not belong to transactional replication, continue with "
+                 "GetSafeTimeForTablet";
+      return std::nullopt;
+    }
+
+    return safe_time_result.status();
+  }
+
+  return std::nullopt;
+}
+
+Result<HybridTime> XClusterTargetManager::PrepareAndGetBackfillTimeForBiDirectionalIndex(
+    const std::vector<TableId>& index_table_ids, const TableId& indexed_table_id,
+    const LeaderEpoch& epoch) const {
+  // Online index creation in Yugabyte is based on the F1 paper, and has 4 stages: Delete Only,
+  // Insert and Delete Only, Backfill, Read, Insert and delete. Consistency is guaranteed as long as
+  // no two nodes in the system are more than 2 stages apart. Within a single universe the
+  // CatalogVersion is bumped to enforce this.
+  //
+  // With bi-directional xCluster writes happen on both universes, each with its own CatalogVersion.
+  // The regular Create Index flow does not have a way to synchronize the stages across the
+  // universes, which before this change meant the user could not write to the indexed table while
+  // creating indexes. Its important to note that in bi-directional xCluster the users have the
+  // responsibility to insert into different key ranges on each universe. This is important as
+  // xCluster which operates at the physical layer does not enforce YSQL layer constraints like
+  // Foreign keys.
+  //
+  // In order to allow Online Create Index we synchronize the two universe but only at the Backfill
+  // stage. We allow them to diverse by more than 2 stages, since for any given key range each
+  // universe will locally ensures the 2 stage apart policy. Only the backfill stage reads and
+  // writes data across the entire key range, so this alone needs to be synchronized across the two
+  // universe.
+  //
+  // The stream for the new index table has already been created as part of its DocDB table
+  // creation. (See CreateXClusterStreamForBiDirectionalIndexTask) This function performs the
+  // following steps:
+  // 1. Wait for the index and its stream to get created on the other universe.
+  // 2. Wait for the index on the other universe to reach atleast its backfill stage.
+  // 3. Add our index table to the xCluster replication group.
+  // 4. Pick the backfill read time.
+  // 5. Wait for the indexed table to catch up to the backfill time.
+  //
+  // For colocated tables, only the parent table is part of replication, so we skip the steps
+  // related to stream creation, and adding index to replication.
+
+  const auto deadline =
+      CoarseMonoClock::Now() +
+      MonoDelta::FromSeconds(FLAGS_add_new_index_to_bidirectional_xcluster_timeout_secs);
+
+  LOG(INFO) << "Preparing indexes " << yb::ToString(index_table_ids) << " on table "
+            << indexed_table_id << " for bi-directional xCluster replication";
+
+  auto indexed_table_streams = GetStreamIdsForTable(indexed_table_id);
+  // Complex scenarios like replicating between 3 universes with multiple bi-directional replication
+  // groups A <=> B <=> C <=> A is not supported.
+  SCHECK_EQ(
+      indexed_table_streams.size(), 1, IllegalState,
+      Format("Expected 1 xCluster stream for table $0", indexed_table_id));
+
+  auto& [replication_id, indexed_table_stream_id] = *indexed_table_streams.begin();
+
+  auto replication_group = catalog_manager_.GetUniverseReplication(replication_id);
+  SCHECK_FORMAT(replication_group, NotFound, "Replication group $0 not found", replication_id);
+
+  auto remote_client = VERIFY_RESULT(GetXClusterRemoteClientHolder(*replication_group));
+
+  std::vector<std::shared_ptr<MultiStepMonitoredTask>> tasks;
+  for (const auto& index_table_id : index_table_ids) {
+    auto index_table_info = VERIFY_RESULT(catalog_manager_.GetTableById(index_table_id));
+    tasks.emplace_back(std::make_shared<AddBiDirectionalIndexToXClusterTargetTask>(
+        std::move(index_table_info), replication_group, remote_client, master_, epoch, deadline));
+  }
+
+  Synchronizer sync;
+  RETURN_NOT_OK(MultiStepMonitoredTask::StartTasks(
+      tasks, [&sync](const Status& status) { sync.AsStdStatusCallback()(status); }));
+  RETURN_NOT_OK(sync.Wait());
+
+  // All indexes have reached the backfill stage on both universes. We can proceed once the indexed
+  // table is caught up.
+  const auto backfill_ht = master_.clock()->Now();
+  const auto backfill_time_micros =
+      backfill_ht.GetPhysicalValueMicros() + (3 * FLAGS_max_clock_skew_usec);
+
+  LOG(INFO) << "Waiting for replication of indexed table " << indexed_table_id << " stream "
+            << indexed_table_stream_id << " to catch up to backfill time " << backfill_ht;
+  RETURN_NOT_OK_PREPEND(
+      remote_client->GetXClusterClient().WaitForReplicationDrain(
+          indexed_table_stream_id, backfill_time_micros, deadline),
+      Format(
+          "Error waiting for replication drain of indexed table $0 stream $1", indexed_table_id,
+          indexed_table_stream_id));
+  LOG(INFO) << "Indexed table " << indexed_table_id << " xCluster stream "
+            << indexed_table_stream_id << " caught up to backfill time " << backfill_ht;
+
+  return backfill_ht;
+}
+
 }  // namespace yb::master
diff --git a/src/yb/master/xcluster/xcluster_target_manager.h b/src/yb/master/xcluster/xcluster_target_manager.h
index 4b285c8dd5..3f8f5d83b6 100644
--- a/src/yb/master/xcluster/xcluster_target_manager.h
+++ b/src/yb/master/xcluster/xcluster_target_manager.h
@@ -203,6 +203,10 @@ class XClusterTargetManager {
       const xrepl::StreamId& bootstrap_id, const std::optional<TableId>& target_table_id,
       const LeaderEpoch& epoch);
 
+  Result<std::optional<HybridTime>> TryGetXClusterSafeTimeForBackfill(
+      const std::vector<TableId>& index_table_ids, const TableInfoPtr& indexed_table,
+      const LeaderEpoch& epoch) const;
+
  private:
   // Gets the replication group status for the given replication group id. Does not populate the
   // table statuses.
@@ -223,6 +227,10 @@ class XClusterTargetManager {
 
   Status ProcessPendingSchemaChanges(const LeaderEpoch& epoch);
 
+  Result<HybridTime> PrepareAndGetBackfillTimeForBiDirectionalIndex(
+      const std::vector<TableId>& index_table_ids, const TableId& indexed_table,
+      const LeaderEpoch& epoch) const;
+
   Master& master_;
   CatalogManager& catalog_manager_;
   SysCatalogTable& sys_catalog_;
diff --git a/src/yb/master/xrepl_catalog_manager.cc b/src/yb/master/xrepl_catalog_manager.cc
index 9c268d3a86..3c41ea0c70 100644
--- a/src/yb/master/xrepl_catalog_manager.cc
+++ b/src/yb/master/xrepl_catalog_manager.cc
@@ -779,8 +779,13 @@ Status CatalogManager::CreateCDCStream(
     if (req->has_initial_state()) {
       initial_state = req->initial_state();
     }
+
+    Synchronizer sync;
     auto stream_id = VERIFY_RESULT(xcluster_manager_->CreateNewXClusterStreamForTable(
-        req->table_id(), cdc::StreamModeTransactional(req->transactional()), initial_state, epoch));
+        req->table_id(), cdc::StreamModeTransactional(req->transactional()), initial_state, epoch,
+        [&sync](const Status& status) { sync.AsStdStatusCallback()(status); }));
+    RETURN_NOT_OK(sync.Wait());
+
     resp->set_stream_id(stream_id.ToString());
     return Status::OK();
   }
