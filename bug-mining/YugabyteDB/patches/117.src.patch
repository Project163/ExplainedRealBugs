diff --git a/src/yb/master/backfill_index.cc b/src/yb/master/backfill_index.cc
index 074646befb..2ad3103ba4 100644
--- a/src/yb/master/backfill_index.cc
+++ b/src/yb/master/backfill_index.cc
@@ -793,7 +793,7 @@ void BackfillTable::LaunchBackfillOrAbort() {
 Status BackfillTable::LaunchComputeSafeTimeForRead() {
   RSTATUS_DCHECK(!timestamp_chosen(), IllegalState, "Backfill timestamp already set");
 
-  if (master_->catalog_manager_impl()->IsTableXClusterConsumer(indexed_table_->id())) {
+  if (master_->xcluster_manager()->IsTableReplicationConsumer(indexed_table_->id())) {
     auto res = master_->xcluster_manager()->GetXClusterSafeTime(indexed_table_->namespace_id());
     if (res.ok()) {
       SCHECK(!res->is_special(), InvalidArgument, "Invalid xCluster safe time for namespace ",
diff --git a/src/yb/master/catalog_manager.cc b/src/yb/master/catalog_manager.cc
index f843f5a703..5d645aeb20 100644
--- a/src/yb/master/catalog_manager.cc
+++ b/src/yb/master/catalog_manager.cc
@@ -471,10 +471,6 @@ DEFINE_RUNTIME_bool(enable_delete_truncate_xcluster_replicated_table, false,
             "When set, enables deleting/truncating YCQL tables currently in xCluster replication. "
             "For YSQL tables, deletion is always allowed and TRUNCATE is always disallowed.");
 
-DEFINE_RUNTIME_bool(xcluster_wait_on_ddl_alter, true,
-    "When xCluster replication sends a DDL change, wait for the user to enter a "
-    "compatible/matching entry.  Note: Can also set at runtime to resume after stall.");
-
 DEFINE_test_flag(bool, sequential_colocation_ids, false,
                  "When set, colocation IDs will be assigned sequentially (starting from 20001) "
                  "rather than at random. This is especially useful for making pg_regress "
@@ -515,14 +511,6 @@ DEFINE_RUNTIME_bool(
     enable_truncate_cdcsdk_table, false,
     "When set, enables truncating tables currently part of a CDCSDK Stream");
 
-DEFINE_RUNTIME_AUTO_bool(enable_tablet_split_of_xcluster_replicated_tables, kExternal, false, true,
-    "When set, it enables automatic tablet splitting for tables that are part of an "
-    "xCluster replication setup");
-
-DEFINE_RUNTIME_bool(enable_tablet_split_of_xcluster_bootstrapping_tables, false,
-    "When set, it enables automatic tablet splitting for tables that are part of an "
-    "xCluster replication setup and are currently being bootstrapped for xCluster.");
-
 DEFINE_RUNTIME_bool(enable_tablet_split_of_cdcsdk_streamed_tables, false,
     "When set, it enables automatic tablet splitting for tables that are part of a "
     "CDCSDK stream");
@@ -2221,7 +2209,7 @@ bool CatalogManager::StartShutdown() {
 
   refresh_ysql_tablespace_info_task_.StartShutdown();
 
-  cdc_parent_tablet_deletion_task_.StartShutdown();
+  xrepl_parent_tablet_deletion_task_.StartShutdown();
 
   refresh_ysql_pg_catalog_versions_task_.StartShutdown();
 
@@ -2236,7 +2224,7 @@ void CatalogManager::CompleteShutdown() {
   snapshot_coordinator_.Shutdown();
   refresh_yql_partitions_task_.CompleteShutdown();
   refresh_ysql_tablespace_info_task_.CompleteShutdown();
-  cdc_parent_tablet_deletion_task_.CompleteShutdown();
+  xrepl_parent_tablet_deletion_task_.CompleteShutdown();
   refresh_ysql_pg_catalog_versions_task_.CompleteShutdown();
   xcluster_manager_->Shutdown();
 
@@ -3098,8 +3086,7 @@ Status CatalogManager::DoSplitTablet(
     .children = { child_tablet_ids_sorted[0], child_tablet_ids_sorted[1] }
   };
   RETURN_NOT_OK(tablet_split_manager_.ProcessSplitTabletResult(
-      source_tablet_info->table()->id(),
-      split_tablet_ids));
+      source_tablet_info->table()->id(), split_tablet_ids, epoch));
 
   // TODO(tsplit): what if source tablet will be deleted before or during TS leader is processing
   // split? Add unit-test.
@@ -3213,24 +3200,8 @@ Status CatalogManager::XReplValidateSplitCandidateTable(const TableId& table_id)
 }
 
 Status CatalogManager::XReplValidateSplitCandidateTableUnlocked(const TableId& table_id) const {
-  // Check if this table is part of a cdc stream.
-  if (PREDICT_TRUE(!FLAGS_enable_tablet_split_of_xcluster_replicated_tables) &&
-      IsTablePartOfXClusterUnlocked(table_id)) {
-    return STATUS_FORMAT(
-        NotSupported,
-        "Tablet splitting is not supported for tables that are a part of"
-        " a CDC stream, table_id: $0",
-        table_id);
-  }
-  // Check if the table is in the bootstrapping phase of xCluster.
-  if (PREDICT_TRUE(!FLAGS_enable_tablet_split_of_xcluster_bootstrapping_tables) &&
-      xcluster_manager_->DoesTableHaveAnyBootstrappingStream(table_id)) {
-    return STATUS_FORMAT(
-        NotSupported,
-        "Tablet splitting is not supported for tables that are a part of"
-        " a bootstrapping CDC stream, table_id: $0",
-        table_id);
-  }
+  RETURN_NOT_OK(xcluster_manager_->ValidateSplitCandidateTable(table_id));
+
   // Check if this table is part of a cdcsdk stream.
   if (!FLAGS_enable_tablet_split_of_cdcsdk_streamed_tables && IsTablePartOfCDCSDK(table_id)) {
     return STATUS_FORMAT(
@@ -3769,8 +3740,9 @@ Status CatalogManager::CreateTable(const CreateTableRequestPB* orig_req,
     SharedLock lock(mutex_);
     // Fail rewrites on tables that are part of CDC or XCluster replication, except for
     // TRUNCATEs on CDC tables when FLAGS_enable_truncate_cdcsdk_table is enabled.
-    if (IsTablePartOfXClusterUnlocked(table_id) || (IsTablePartOfCDCSDK(table_id) &&
-        (!orig_req->is_truncate() || !FLAGS_enable_truncate_cdcsdk_table))) {
+    if (xcluster_manager_->IsTableReplicated(table_id) ||
+        (IsTablePartOfCDCSDK(table_id) &&
+         (!orig_req->is_truncate() || !FLAGS_enable_truncate_cdcsdk_table))) {
       return STATUS(
           NotSupported,
           "cannot rewrite a table that is a part of CDC or XCluster replication."
@@ -5783,7 +5755,8 @@ Status CatalogManager::TruncateTable(const TableId& table_id,
   }
 
   SCHECK_EC_FORMAT(
-      FLAGS_enable_delete_truncate_xcluster_replicated_table || !IsTablePartOfXCluster(table_id),
+      FLAGS_enable_delete_truncate_xcluster_replicated_table ||
+          !xcluster_manager_->IsTableReplicated(table_id),
       NotSupported, MasterError(MasterErrorPB::INVALID_REQUEST),
       "Cannot truncate table $0 that is in xCluster replication", table_id);
 
@@ -6283,7 +6256,8 @@ Status CatalogManager::DeleteTable(
   }
 
   // For now, only disable dropping YCQL tables under xCluster replication.
-  bool result = table->GetTableType() == YQL_TABLE_TYPE && IsTablePartOfXCluster(table->id());
+  bool result =
+      table->GetTableType() == YQL_TABLE_TYPE && xcluster_manager_->IsTableReplicated(table->id());
   if (!FLAGS_enable_delete_truncate_xcluster_replicated_table && result) {
     return STATUS(NotSupported,
                   "Cannot delete a table in replication.",
@@ -7291,11 +7265,9 @@ Status CatalogManager::AlterTable(const AlterTableRequestPB* req,
     SchemaToPB(new_schema, table_pb.mutable_schema());
   }
 
-  if (GetAtomicFlag(&FLAGS_xcluster_wait_on_ddl_alter) && IsTableXClusterConsumer(table->id())) {
-    // If we're waiting for a Schema because we saw the a replication source with a change,
-    // ensure this alter is compatible with what we're expecting.
-    RETURN_NOT_OK(ValidateNewSchemaWithCdc(*table, new_schema));
-  }
+  // If we're waiting for a Schema because we saw the a replication source with a change,
+  // ensure this alter is compatible with what we're expecting.
+  RETURN_NOT_OK(xcluster_manager_->ValidateNewSchema(*table, new_schema));
 
   // Only increment the version number if it is a schema change (AddTable change goes through a
   // different path and it's not processed here).
@@ -8714,7 +8686,7 @@ Status CatalogManager::CheckIfDatabaseHasReplication(const scoped_refptr<Namespa
     if (ltm->namespace_id() != database->id() || ltm->started_deleting()) {
       continue;
     }
-    if (IsTablePartOfXClusterUnlocked(table->id())) {
+    if (xcluster_manager_->IsTableReplicated(table->id())) {
       LOG(ERROR) << "Error deleting database: " << database->id() << ", table: " << table->id()
                  << " is under replication"
                  << ". Cannot delete a database that contains tables under replication.";
@@ -9916,7 +9888,7 @@ Status CatalogManager::EnableBgTasks() {
   RETURN_NOT_OK(background_tasks_thread_pool_->SubmitFunc(
       [this]() { RebuildYQLSystemPartitions(); }));
 
-  cdc_parent_tablet_deletion_task_.Bind(&master_->messenger()->scheduler());
+  xrepl_parent_tablet_deletion_task_.Bind(&master_->messenger()->scheduler());
 
   return Status::OK();
 }
@@ -10603,13 +10575,7 @@ Status CatalogManager::HandleTabletSchemaVersionReport(
   // Clean up any DDL verification state that is waiting for this Alter to complete.
   RemoveDdlTransactionState(table->id(), table->EraseDdlTxnsWaitingForSchemaVersion(version));
 
-  // With Replication Enabled, verify that we've finished applying the New Schema.
-  // This may need to be refactored when we support Replication + Active Index Backfill in #7613.
-  if (IsTableXClusterConsumer(table->id())) {
-    // If we're waiting for a Schema because we saw the a replication source with a change,
-    // resume replication now that the alter is complete.
-    RETURN_NOT_OK(ResumeXClusterConsumerAfterNewSchema(*table, version));
-  }
+  RETURN_NOT_OK(xcluster_manager_->HandleTabletSchemaVersionReport(*table, version, epoch));
 
   return MultiStageAlterTable::LaunchNextTableInfoVersionIfNecessary(this, table, version, epoch);
 }
diff --git a/src/yb/master/catalog_manager.h b/src/yb/master/catalog_manager.h
index 138638e527..183f2be9ad 100644
--- a/src/yb/master/catalog_manager.h
+++ b/src/yb/master/catalog_manager.h
@@ -710,9 +710,6 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
   Status GetFullUniverseKeyRegistry(const GetFullUniverseKeyRegistryRequestPB* req,
                                     GetFullUniverseKeyRegistryResponsePB* resp);
 
-  Status UpdateXClusterConsumerOnTabletSplit(
-      const TableId& consumer_table_id, const SplitTabletIds& split_tablet_ids) override;
-
   Status UpdateCDCProducerOnTabletSplit(
       const TableId& producer_table_id, const SplitTabletIds& split_tablet_ids) override;
 
@@ -1150,14 +1147,6 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
   Result<boost::optional<TablespaceId>> GetTablespaceForTable(
       const scoped_refptr<TableInfo>& table) const override;
 
-  void SyncXClusterConsumerReplicationStatusMap(
-      const xcluster::ReplicationGroupId& replication_group_id,
-      const google::protobuf::Map<std::string, cdc::ProducerEntryPB>& producer_map)
-      EXCLUDES(xcluster_consumer_replication_error_map_mutex_);
-
-  void StoreXClusterConsumerReplicationStatus(
-      const XClusterConsumerReplicationStatusPB& consumer_replication_status) EXCLUDES(mutex_);
-
   void CheckTableDeleted(const TableInfoPtr& table, const LeaderEpoch& epoch) override;
 
   Status ShouldSplitValidCandidate(
@@ -1463,11 +1452,6 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
       UpdateConsumerOnProducerMetadataResponsePB* resp,
       rpc::RpcContext* rpc);
 
-  Status XClusterReportNewAutoFlagConfigVersion(
-      const XClusterReportNewAutoFlagConfigVersionRequestPB* req,
-      XClusterReportNewAutoFlagConfigVersionResponsePB* resp, rpc::RpcContext* rpc,
-      const LeaderEpoch& epoch);
-
   // Wait for replication to drain on CDC streams.
   typedef std::pair<xrepl::StreamId, TabletId> StreamTabletIdPair;
   typedef boost::hash<StreamTabletIdPair> StreamTabletIdHash;
@@ -1476,12 +1460,6 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
       WaitForReplicationDrainResponsePB* resp,
       rpc::RpcContext* rpc);
 
-  // Returns the replication status.
-  Status GetReplicationStatus(
-      const GetReplicationStatusRequestPB* req,
-      GetReplicationStatusResponsePB* resp,
-      rpc::RpcContext* rpc);
-
   std::vector<SysUniverseReplicationEntryPB> GetAllXClusterUniverseReplicationInfos();
 
   typedef std::unordered_map<TableId, std::list<CDCStreamInfoPtr>> TableStreamIdsMap;
@@ -1598,19 +1576,6 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
 
   std::unordered_set<xrepl::StreamId> GetAllXReplStreamIds() const EXCLUDES(mutex_);
 
-  void NotifyAutoFlagsConfigChanged();
-
-  // Maps replication group id to the corresponding xrepl stream for a table.
-  using XClusterConsumerTableStreamIds =
-      std::unordered_map<xcluster::ReplicationGroupId, xrepl::StreamId>;
-  // Gets the set of CDC stream info for an xCluster consumer table.
-  XClusterConsumerTableStreamIds GetXClusterConsumerStreamIdsForTable(const TableId& table_id) const
-      EXCLUDES(mutex_);
-
-  void ClearXClusterConsumerTableStreams(
-      const xcluster::ReplicationGroupId& replication_group_id,
-      const std::set<TableId>& tables_to_clear) EXCLUDES(mutex_);
-
   std::shared_ptr<ClusterConfigInfo> ClusterConfig() const;
 
   auto GetTasksTracker() { return tasks_tracker_; }
@@ -1641,9 +1606,6 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
   Status DropXReplStreams(
       const std::vector<CDCStreamInfoPtr>& streams, SysCDCStreamEntryPB::State delete_state);
 
-  std::unordered_map<TableId, XClusterConsumerTableStreamIds> GetXClusterConsumerTableStreams()
-      const EXCLUDES(mutex_);
-
   std::optional<UniverseUuid> GetUniverseUuidIfExists() const;
 
   // Calculate the total number of replicas which are being handled by servers in state.
@@ -1651,8 +1613,7 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
 
   std::shared_ptr<YsqlTablespaceManager> GetTablespaceManager() const;
 
-  Result<bool> HasReplicationGroupErrors(
-      const xcluster::ReplicationGroupId& replication_group_id) const;
+  Result<std::map<std::string, KeyRange>> GetTableKeyRanges(const TableId& table_id);
 
  protected:
   // TODO Get rid of these friend classes and introduce formal interface.
@@ -2139,17 +2100,8 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
   // Is this table part of xCluster or CDCSDK?
   bool IsTablePartOfXRepl(const TableId& table_id) const REQUIRES_SHARED(mutex_);
 
-  bool IsTablePartOfXCluster(const TableId& table_id) const EXCLUDES(mutex_);
-
-  bool IsTablePartOfXClusterUnlocked(const TableId& table_id) const REQUIRES_SHARED(mutex_);
-
   bool IsTablePartOfCDCSDK(const TableId& table_id) const REQUIRES_SHARED(mutex_);
 
-  Status ValidateNewSchemaWithCdc(const TableInfo& table_info, const Schema& new_schema) const;
-
-  Status ResumeXClusterConsumerAfterNewSchema(
-      const TableInfo& table_info, SchemaVersion last_compatible_consumer_schema_version);
-
   bool IsPitrActive();
 
   // Checks if the database being deleted contains any replicated tables.
@@ -2278,7 +2230,7 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
   // TODO(jhe) Cleanup how we use ScheduledTaskTracker, move is_running and util functions to class.
   // Background task for deleting parent split tablets retained by xCluster streams.
   std::atomic<bool> xrepl_parent_tablet_deletion_task_running_{false};
-  rpc::ScheduledTaskTracker cdc_parent_tablet_deletion_task_;
+  rpc::ScheduledTaskTracker xrepl_parent_tablet_deletion_task_;
 
   // Namespace maps: namespace-id -> NamespaceInfo and namespace-name -> NamespaceInfo
   NamespaceInfoMap namespace_ids_map_ GUARDED_BY(mutex_);
@@ -2681,8 +2633,6 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
   Status PreprocessTabletEntry(const SysRowEntry& entry, ExternalTableSnapshotDataMap* table_map);
   Status ImportTabletEntry(const SysRowEntry& entry, ExternalTableSnapshotDataMap* table_map);
 
-  Result<std::map<std::string, KeyRange>> GetTableKeyRanges(const TableId& table_id);
-
   Result<SchemaVersion> GetTableSchemaVersion(const TableId& table_id);
 
   Result<SysRowEntries> CollectEntries(
@@ -3006,12 +2956,6 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
       const std::vector<TabletInfoPtr>& tablets, const TabletDeleteRetainerInfo& delete_retainer)
       REQUIRES(mutex_);
 
-  // Populate the response with the errors for the given replication group.
-  Status PopulateReplicationGroupErrors(
-      const xcluster::ReplicationGroupId& replication_group_id,
-      GetReplicationStatusResponsePB* resp) const
-      REQUIRES_SHARED(mutex_, xcluster_consumer_replication_error_map_mutex_);
-
   // Update the UniverseReplicationInfo object when toggling replication.
   Status SetUniverseReplicationInfoEnabled(
       const xcluster::ReplicationGroupId& replication_group_id, bool is_enabled) EXCLUDES(mutex_);
@@ -3044,15 +2988,10 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
 
   void StartWriteTableToSysCatalogTasks(TableIdSet&& tables_to_persist);
 
-  bool IsTableXClusterConsumerUnlocked(const TableId& table_id) const REQUIRES_SHARED(mutex_);
-
   Status DropXClusterStreamsOfTables(const std::unordered_set<TableId>& table_ids) EXCLUDES(mutex_);
 
   void SchedulePostTabletCreationTasksForPendingTables(const LeaderEpoch& epoch) EXCLUDES(mutex_);
 
-  // Checks if the table is a consumer in an xCluster replication universe.
-  bool IsTableXClusterConsumer(const TableId& table_id) const EXCLUDES(mutex_);
-
   Status BumpVersionAndStoreClusterConfig(
       ClusterConfigInfo* cluster_config, ClusterConfigInfo::WriteLock* l);
 
@@ -3073,10 +3012,6 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
   Status CreateGlobalTransactionStatusTableIfNotPresent(
       rpc::RpcContext* rpc, const LeaderEpoch& epoch);
 
-  Status XClusterRefreshLocalAutoFlagConfig(const LeaderEpoch& epoch);
-
-  Status XClusterProcessPendingSchemaChanges(const LeaderEpoch& epoch);
-
   Status MaybeRestoreInitialSysCatalogSnapshotAndReloadSysCatalog(SysCatalogLoadingState* state)
       REQUIRES(mutex_);
 
@@ -3204,10 +3139,6 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
   std::unordered_map<NamespaceId, std::unordered_set<TableId>>
       namespace_to_cdcsdk_non_eligible_table_map_ GUARDED_BY(cdcsdk_non_eligible_table_mutex_);
 
-  // Map of all consumer tables that are part of xcluster replication, to a map of the stream infos.
-  std::unordered_map<TableId, XClusterConsumerTableStreamIds>
-      xcluster_consumer_table_stream_ids_map_ GUARDED_BY(mutex_);
-
   std::unordered_map<TableId, std::unordered_set<xrepl::StreamId>> cdcsdk_tables_to_stream_map_
       GUARDED_BY(mutex_);
 
@@ -3269,12 +3200,6 @@ class CatalogManager : public tserver::TabletPeerLookupIf,
 
   std::unique_ptr<cdc::CDCStateTable> cdc_state_table_;
 
-  mutable std::shared_mutex xcluster_consumer_replication_error_map_mutex_ ACQUIRED_AFTER(mutex_);
-  std::unordered_map<xcluster::ReplicationGroupId, xcluster::ReplicationGroupErrors>
-      xcluster_consumer_replication_error_map_
-          GUARDED_BY(xcluster_consumer_replication_error_map_mutex_);
-
-  std::atomic<bool> xcluster_auto_flags_revalidation_needed_{true};
   std::atomic<bool> pg_cron_service_created_{false};
 
   DISALLOW_COPY_AND_ASSIGN(CatalogManager);
diff --git a/src/yb/master/catalog_manager_if.h b/src/yb/master/catalog_manager_if.h
index 6a75d6f5b5..bc0435b220 100644
--- a/src/yb/master/catalog_manager_if.h
+++ b/src/yb/master/catalog_manager_if.h
@@ -344,9 +344,6 @@ class CatalogManagerIf {
 
   virtual Status XReplValidateSplitCandidateTable(const TableId& table_id) const = 0;
 
-  virtual Status UpdateXClusterConsumerOnTabletSplit(
-      const TableId& consumer_table_id, const SplitTabletIds& split_tablet_ids) = 0;
-
   virtual Status UpdateCDCProducerOnTabletSplit(
       const TableId& producer_table_id, const SplitTabletIds& split_tablet_ids) = 0;
   virtual Status ShouldSplitValidCandidate(
diff --git a/src/yb/master/master_auto_flags_manager.cc b/src/yb/master/master_auto_flags_manager.cc
index cb607d8c70..03e41fa9ce 100644
--- a/src/yb/master/master_auto_flags_manager.cc
+++ b/src/yb/master/master_auto_flags_manager.cc
@@ -14,6 +14,7 @@
 #include "yb/master/master_auto_flags_manager.h"
 #include "yb/consensus/consensus.pb.h"
 #include "yb/master/catalog_manager.h"
+#include "yb/master/xcluster/xcluster_manager_if.h"
 #include "yb/tablet/operations/change_auto_flags_config_operation.h"
 #include "yb/util/scope_exit.h"
 
@@ -331,7 +332,7 @@ Status MasterAutoFlagsManager::PersistConfigInSysCatalog(AutoFlagsConfigPB& new_
     LOG(WARNING) << "Failed to apply new AutoFlags config: " << status.ToString();
     return status;
   }
-  catalog_manager_->NotifyAutoFlagsConfigChanged();
+  catalog_manager_->GetXClusterManager()->NotifyAutoFlagsConfigChanged();
 
   return Status::OK();
 }
diff --git a/src/yb/master/master_heartbeat_service.cc b/src/yb/master/master_heartbeat_service.cc
index de65f31f5c..e3f8bf1870 100644
--- a/src/yb/master/master_heartbeat_service.cc
+++ b/src/yb/master/master_heartbeat_service.cc
@@ -33,6 +33,7 @@
 #include "yb/master/master_service_base-internal.h"
 #include "yb/master/ts_descriptor.h"
 #include "yb/master/ts_manager.h"
+#include "yb/master/xcluster/xcluster_manager_if.h"
 #include "yb/master/yql_partitions_vtable.h"
 
 #include "yb/util/debug/trace_event.h"
@@ -431,7 +432,7 @@ void MasterHeartbeatServiceImpl::TSHeartbeat(
     }
 
     for (const auto& consumer_replication_state : req->xcluster_consumer_replication_status()) {
-      catalog_manager_->StoreXClusterConsumerReplicationStatus(
+      catalog_manager_->GetXClusterManager()->StoreConsumerReplicationStatus(
           consumer_replication_state);
     }
 
diff --git a/src/yb/master/master_replication_service.cc b/src/yb/master/master_replication_service.cc
index 4e3fa3b267..201ba5cf25 100644
--- a/src/yb/master/master_replication_service.cc
+++ b/src/yb/master/master_replication_service.cc
@@ -41,7 +41,6 @@ class MasterReplicationServiceImpl : public MasterServiceBase, public MasterRepl
     (IsSetupNamespaceReplicationWithBootstrapDone)
     (UpdateConsumerOnProducerSplit)
     (UpdateConsumerOnProducerMetadata)
-    (XClusterReportNewAutoFlagConfigVersion)
     (ListCDCStreams)
     (IsObjectPartOfXRepl)
     (SetUniverseReplicationEnabled)
@@ -51,7 +50,6 @@ class MasterReplicationServiceImpl : public MasterServiceBase, public MasterRepl
     (GetCDCDBStreamInfo)
     (IsBootstrapRequired)
     (WaitForReplicationDrain)
-    (GetReplicationStatus)
     (GetTableSchemaFromSysCatalog)
     (ChangeXClusterRole)
     (BootstrapProducer)
@@ -82,6 +80,8 @@ class MasterReplicationServiceImpl : public MasterServiceBase, public MasterRepl
       (GetXClusterOutboundReplicationGroupInfo)
       (GetUniverseReplications)
       (GetUniverseReplicationInfo)
+      (GetReplicationStatus)
+      (XClusterReportNewAutoFlagConfigVersion)
   )
 };
 
diff --git a/src/yb/master/tablet_split_manager.cc b/src/yb/master/tablet_split_manager.cc
index 301494fbb8..e7bcc39c23 100644
--- a/src/yb/master/tablet_split_manager.cc
+++ b/src/yb/master/tablet_split_manager.cc
@@ -31,6 +31,7 @@
 #include "yb/master/master_error.h"
 #include "yb/master/master_snapshot_coordinator.h"
 #include "yb/master/ts_descriptor.h"
+#include "yb/master/xcluster/xcluster_manager_if.h"
 
 #include "yb/server/monitored_task.h"
 
@@ -974,23 +975,21 @@ void TabletSplitManager::MaybeDoSplitting(
 }
 
 Status TabletSplitManager::ProcessSplitTabletResult(
-    const TableId& split_table_id,
-    const SplitTabletIds& split_tablet_ids) {
+    const TableId& split_table_id, const SplitTabletIds& split_tablet_ids,
+    const LeaderEpoch& epoch) {
   // Since this can get called multiple times from DoSplitTablet (if a tablet split is retried),
   // everything here needs to be idempotent.
   LOG(INFO) << "Processing split tablet result for table " << split_table_id
             << ", split tablet ids: " << split_tablet_ids.ToString();
 
-  // Update the xCluster tablet mapping.
-  Status s =
-      catalog_manager_.UpdateXClusterConsumerOnTabletSplit(split_table_id, split_tablet_ids);
+  Status s = catalog_manager_.GetXClusterManager()->HandleTabletSplit(
+      split_table_id, split_tablet_ids, epoch);
   RETURN_NOT_OK_PREPEND(
       s, Format(
              "Encountered an error while updating the xCluster consumer tablet mapping. "
              "Table id: $0, Split Tablets: $1",
              split_table_id, split_tablet_ids.ToString()));
 
-  // Update the CDCSDK and xCluster producer tablet mapping.
   s = catalog_manager_.UpdateCDCProducerOnTabletSplit(split_table_id, split_tablet_ids);
   RETURN_NOT_OK_PREPEND(
       s, Format(
diff --git a/src/yb/master/tablet_split_manager.h b/src/yb/master/tablet_split_manager.h
index 0264043616..a6e2320f06 100644
--- a/src/yb/master/tablet_split_manager.h
+++ b/src/yb/master/tablet_split_manager.h
@@ -57,7 +57,8 @@ class TabletSplitManager {
       const LeaderEpoch& epoch);
 
   Status ProcessSplitTabletResult(
-      const TableId& split_table_id, const SplitTabletIds& split_tablet_ids);
+      const TableId& split_table_id, const SplitTabletIds& split_tablet_ids,
+      const LeaderEpoch& epoch);
 
   // Table-level checks for splitting that are checked not only as a best-effort
   // filter, but also after acquiring the table/tablet locks in CatalogManager::DoSplit.
diff --git a/src/yb/master/xcluster/xcluster_catalog_entity.cc b/src/yb/master/xcluster/xcluster_catalog_entity.cc
index 00036f91c8..1773cd743b 100644
--- a/src/yb/master/xcluster/xcluster_catalog_entity.cc
+++ b/src/yb/master/xcluster/xcluster_catalog_entity.cc
@@ -16,7 +16,7 @@
 namespace yb::master {
 
 void XClusterSafeTimeInfo::Load(const XClusterSafeTimePB& metadata) {
-  // Debug confirm that there is no xcluster_safe_time_info_ set. This also ensures that this does
+  // Debug confirm that there is no safe_time_info_ set. This also ensures that this does
   // not visit multiple rows.
   DCHECK(LockForRead()->pb.safe_time_map().empty()) << "Already have XCluster Safe Time data!";
 
diff --git a/src/yb/master/xcluster/xcluster_manager.cc b/src/yb/master/xcluster/xcluster_manager.cc
index 228b058764..74ca3fcdfb 100644
--- a/src/yb/master/xcluster/xcluster_manager.cc
+++ b/src/yb/master/xcluster/xcluster_manager.cc
@@ -37,6 +37,10 @@ DEFINE_RUNTIME_bool(disable_xcluster_db_scoped_new_table_processing, false,
     "table to inbound replication group on target");
 TAG_FLAG(disable_xcluster_db_scoped_new_table_processing, advanced);
 
+DEFINE_RUNTIME_AUTO_bool(enable_tablet_split_of_xcluster_replicated_tables, kExternal, false, true,
+    "When set, it enables automatic tablet splitting for tables that are part of an "
+    "xCluster replication setup");
+
 #define LOG_FUNC_AND_RPC \
   LOG_WITH_FUNC(INFO) << req->ShortDebugString() << ", from: " << RequestorString(rpc)
 
@@ -492,6 +496,8 @@ Status XClusterManager::GetXClusterOutboundReplicationGroupInfo(
 Status XClusterManager::GetUniverseReplications(
     const GetUniverseReplicationsRequestPB* req, GetUniverseReplicationsResponsePB* resp,
     rpc::RpcContext* rpc, const LeaderEpoch& epoch) {
+  LOG_FUNC_AND_RPC;
+
   const auto& namespace_filter = req->namespace_id();
 
   const auto replication_groups = XClusterTargetManager::GetUniverseReplications(namespace_filter);
@@ -502,9 +508,18 @@ Status XClusterManager::GetUniverseReplications(
   return Status::OK();
 }
 
+Status XClusterManager::GetReplicationStatus(
+    const GetReplicationStatusRequestPB* req, GetReplicationStatusResponsePB* resp,
+    rpc::RpcContext* rpc) {
+  LOG_FUNC_AND_RPC;
+
+  return XClusterTargetManager::GetReplicationStatus(req, resp);
+}
+
 Status XClusterManager::GetUniverseReplicationInfo(
     const GetUniverseReplicationInfoRequestPB* req, GetUniverseReplicationInfoResponsePB* resp,
     rpc::RpcContext* rpc, const LeaderEpoch& epoch) {
+  LOG_FUNC_AND_RPC;
   SCHECK_PB_FIELDS_NOT_EMPTY(*req, replication_group_id);
 
   const auto replication_info = VERIFY_RESULT(XClusterTargetManager::GetUniverseReplicationInfo(
@@ -532,6 +547,19 @@ Status XClusterManager::GetUniverseReplicationInfo(
   return Status::OK();
 }
 
+Status XClusterManager::XClusterReportNewAutoFlagConfigVersion(
+    const XClusterReportNewAutoFlagConfigVersionRequestPB* req,
+    XClusterReportNewAutoFlagConfigVersionResponsePB* resp, rpc::RpcContext* rpc,
+    const LeaderEpoch& epoch) {
+  LOG_FUNC_AND_RPC;
+
+  const xcluster::ReplicationGroupId replication_group_id(req->replication_group_id());
+  const auto new_version = req->auto_flag_config_version();
+
+  return XClusterTargetManager::ReportNewAutoFlagConfigVersion(
+      replication_group_id, new_version, epoch);
+}
+
 std::vector<std::shared_ptr<PostTabletCreateTaskBase>> XClusterManager::GetPostTabletCreateTasks(
     const TableInfoPtr& table_info, const LeaderEpoch& epoch) {
   std::vector<std::shared_ptr<PostTabletCreateTaskBase>> result;
@@ -567,4 +595,86 @@ Status XClusterManager::ClearXClusterSourceTableId(
   return XClusterTargetManager::ClearXClusterSourceTableId(table_info, epoch);
 }
 
+void XClusterManager::NotifyAutoFlagsConfigChanged() {
+  XClusterTargetManager::NotifyAutoFlagsConfigChanged();
+}
+
+void XClusterManager::StoreConsumerReplicationStatus(
+    const XClusterConsumerReplicationStatusPB& consumer_replication_status) {
+  XClusterTargetManager::StoreReplicationStatus(consumer_replication_status);
+}
+
+void XClusterManager::SyncConsumerReplicationStatusMap(
+    const xcluster::ReplicationGroupId& replication_group_id,
+    const google::protobuf::Map<std::string, cdc::ProducerEntryPB>& producer_map) {
+  XClusterTargetManager::SyncReplicationStatusMap(replication_group_id, producer_map);
+}
+
+Result<bool> XClusterManager::HasReplicationGroupErrors(
+    const xcluster::ReplicationGroupId& replication_group_id) {
+  return XClusterTargetManager::HasReplicationGroupErrors(replication_group_id);
+}
+
+void XClusterManager::RecordTableConsumerStream(
+    const TableId& table_id, const xcluster::ReplicationGroupId& replication_group_id,
+    const xrepl::StreamId& stream_id) {
+  XClusterTargetManager::RecordTableStream(table_id, replication_group_id, stream_id);
+}
+
+void XClusterManager::RemoveTableConsumerStream(
+    const TableId& table_id, const xcluster::ReplicationGroupId& replication_group_id) {
+  XClusterTargetManager::RemoveTableStream(table_id, replication_group_id);
+}
+
+void XClusterManager::RemoveTableConsumerStreams(
+    const xcluster::ReplicationGroupId& replication_group_id,
+    const std::set<TableId>& tables_to_clear) {
+  XClusterTargetManager::RemoveTableStreams(replication_group_id, tables_to_clear);
+}
+
+Result<TableId> XClusterManager::GetConsumerTableIdForStreamId(
+    const xcluster::ReplicationGroupId& replication_group_id,
+    const xrepl::StreamId& stream_id) const {
+  return XClusterTargetManager::GetTableIdForStreamId(replication_group_id, stream_id);
+}
+
+bool XClusterManager::IsTableReplicationConsumer(const TableId& table_id) const {
+  return XClusterTargetManager::IsTableReplicated(table_id);
+}
+
+bool XClusterManager::IsTableReplicated(const TableId& table_id) const {
+  return XClusterSourceManager::IsTableReplicated(table_id) ||
+         XClusterTargetManager::IsTableReplicated(table_id);
+}
+
+Status XClusterManager::HandleTabletSplit(
+    const TableId& consumer_table_id, const SplitTabletIds& split_tablet_ids,
+    const LeaderEpoch& epoch) {
+  return XClusterTargetManager::HandleTabletSplit(consumer_table_id, split_tablet_ids, epoch);
+}
+
+Status XClusterManager::ValidateNewSchema(
+    const TableInfo& table_info, const Schema& consumer_schema) const {
+  return XClusterTargetManager::ValidateNewSchema(table_info, consumer_schema);
+}
+
+Status XClusterManager::ValidateSplitCandidateTable(const TableId& table_id) const {
+  if (!FLAGS_enable_tablet_split_of_xcluster_replicated_tables && IsTableReplicated(table_id)) {
+    return STATUS_FORMAT(
+        NotSupported,
+        "Tablet splitting is not supported for table $0 since it is part of xCluster replication",
+        table_id);
+  }
+
+  RETURN_NOT_OK(XClusterSourceManager::ValidateSplitCandidateTable(table_id));
+
+  return Status::OK();
+}
+
+Status XClusterManager::HandleTabletSchemaVersionReport(
+    const TableInfo& table_info, SchemaVersion consumer_schema_version, const LeaderEpoch& epoch) {
+  return XClusterTargetManager::ResumeStreamsAfterNewSchema(
+      table_info, consumer_schema_version, epoch);
+}
+
 }  // namespace yb::master
diff --git a/src/yb/master/xcluster/xcluster_manager.h b/src/yb/master/xcluster/xcluster_manager.h
index 642f792f31..665fc66e1c 100644
--- a/src/yb/master/xcluster/xcluster_manager.h
+++ b/src/yb/master/xcluster/xcluster_manager.h
@@ -14,15 +14,11 @@
 #pragma once
 
 #include <memory>
-#include <shared_mutex>
 #include <vector>
 
 #include "yb/common/entity_ids_types.h"
-#include "yb/gutil/thread_annotations.h"
 
-#include "yb/master/xcluster/xcluster_catalog_entity.h"
 #include "yb/master/xcluster/xcluster_manager_if.h"
-#include "yb/master/xcluster/xcluster_outbound_replication_group.h"
 #include "yb/master/xcluster/xcluster_source_manager.h"
 #include "yb/master/xcluster/xcluster_target_manager.h"
 
@@ -165,6 +161,13 @@ class XClusterManager : public XClusterManagerIf,
   Status GetUniverseReplicationInfo(
       const GetUniverseReplicationInfoRequestPB* req, GetUniverseReplicationInfoResponsePB* resp,
       rpc::RpcContext* rpc, const LeaderEpoch& epoch);
+  Status GetReplicationStatus(
+      const GetReplicationStatusRequestPB* req, GetReplicationStatusResponsePB* resp,
+      rpc::RpcContext* rpc);
+  Status XClusterReportNewAutoFlagConfigVersion(
+      const XClusterReportNewAutoFlagConfigVersionRequestPB* req,
+      XClusterReportNewAutoFlagConfigVersionResponsePB* resp, rpc::RpcContext* rpc,
+      const LeaderEpoch& epoch);
 
   std::vector<std::shared_ptr<PostTabletCreateTaskBase>> GetPostTabletCreateTasks(
       const TableInfoPtr& table_info, const LeaderEpoch& epoch);
@@ -177,6 +180,49 @@ class XClusterManager : public XClusterManagerIf,
 
   Status ClearXClusterSourceTableId(TableInfoPtr table_info, const LeaderEpoch& epoch) override;
 
+  void NotifyAutoFlagsConfigChanged() override;
+
+  // TODO: Remove these *Consumer* functions once Universe Replication is fully moved into
+  // XClusterManager.
+
+  void StoreConsumerReplicationStatus(
+      const XClusterConsumerReplicationStatusPB& consumer_replication_status) override;
+  void SyncConsumerReplicationStatusMap(
+      const xcluster::ReplicationGroupId& replication_group_id,
+      const google::protobuf::Map<std::string, cdc::ProducerEntryPB>& producer_map) override;
+  Result<bool> HasReplicationGroupErrors(
+      const xcluster::ReplicationGroupId& replication_group_id) override;
+
+  void RecordTableConsumerStream(
+      const TableId& table_id, const xcluster::ReplicationGroupId& replication_group_id,
+      const xrepl::StreamId& stream_id);
+
+  void RemoveTableConsumerStream(
+      const TableId& table_id, const xcluster::ReplicationGroupId& replication_group_id);
+
+  void RemoveTableConsumerStreams(
+      const xcluster::ReplicationGroupId& replication_group_id,
+      const std::set<TableId>& tables_to_clear) override;
+
+  Result<TableId> GetConsumerTableIdForStreamId(
+      const xcluster::ReplicationGroupId& replication_group_id,
+      const xrepl::StreamId& stream_id) const;
+
+  bool IsTableReplicated(const TableId& table_id) const;
+
+  bool IsTableReplicationConsumer(const TableId& table_id) const override;
+
+  Status HandleTabletSplit(
+      const TableId& consumer_table_id, const SplitTabletIds& split_tablet_ids,
+      const LeaderEpoch& epoch) override;
+
+  Status ValidateNewSchema(const TableInfo& table_info, const Schema& consumer_schema) const;
+
+  Status ValidateSplitCandidateTable(const TableId& table_id) const;
+
+  Status HandleTabletSchemaVersionReport(
+      const TableInfo& table_info, SchemaVersion consumer_schema_version, const LeaderEpoch& epoch);
+
  private:
   CatalogManager& catalog_manager_;
   SysCatalogTable& sys_catalog_;
diff --git a/src/yb/master/xcluster/xcluster_manager_if.h b/src/yb/master/xcluster/xcluster_manager_if.h
index 376c553a7f..2dfdc81c83 100644
--- a/src/yb/master/xcluster/xcluster_manager_if.h
+++ b/src/yb/master/xcluster/xcluster_manager_if.h
@@ -38,6 +38,7 @@ namespace master {
 class GetXClusterSafeTimeRequestPB;
 class GetXClusterSafeTimeResponsePB;
 struct LeaderEpoch;
+class XClusterConsumerReplicationStatusPB;
 struct XClusterStatus;
 
 class XClusterManagerIf {
@@ -63,6 +64,28 @@ class XClusterManagerIf {
 
   virtual Status ClearXClusterSourceTableId(TableInfoPtr table_info, const LeaderEpoch& epoch) = 0;
 
+  virtual void NotifyAutoFlagsConfigChanged() = 0;
+
+  virtual void StoreConsumerReplicationStatus(
+      const XClusterConsumerReplicationStatusPB& consumer_replication_status) = 0;
+
+  virtual void SyncConsumerReplicationStatusMap(
+      const xcluster::ReplicationGroupId& replication_group_id,
+      const google::protobuf::Map<std::string, cdc::ProducerEntryPB>& producer_map) = 0;
+
+  virtual Result<bool> HasReplicationGroupErrors(
+      const xcluster::ReplicationGroupId& replication_group_id) = 0;
+
+  virtual bool IsTableReplicationConsumer(const TableId& table_id) const = 0;
+
+  virtual void RemoveTableConsumerStreams(
+      const xcluster::ReplicationGroupId& replication_group_id,
+      const std::set<TableId>& tables_to_clear) = 0;
+
+  virtual Status HandleTabletSplit(
+      const TableId& consumer_table_id, const SplitTabletIds& split_tablet_ids,
+      const LeaderEpoch& epoch) = 0;
+
  protected:
   virtual ~XClusterManagerIf() = default;
 };
diff --git a/src/yb/master/xcluster/xcluster_replication_group.cc b/src/yb/master/xcluster/xcluster_replication_group.cc
index e99b3ea2d6..2545bf823e 100644
--- a/src/yb/master/xcluster/xcluster_replication_group.cc
+++ b/src/yb/master/xcluster/xcluster_replication_group.cc
@@ -171,15 +171,15 @@ Result<bool> IsSafeTimeReady(
 }
 
 Result<bool> IsReplicationGroupReady(
-    const UniverseReplicationInfo& universe, CatalogManager& catalog_manager) {
+    const UniverseReplicationInfo& universe, XClusterManagerIf& xcluster_manager) {
   // The replication group must be in a healthy state.
   if (!FLAGS_xcluster_skip_health_check_on_replication_setup &&
-      VERIFY_RESULT(catalog_manager.HasReplicationGroupErrors(universe.ReplicationGroupId()))) {
+      VERIFY_RESULT(xcluster_manager.HasReplicationGroupErrors(universe.ReplicationGroupId()))) {
     return false;
   }
 
   auto l = universe.LockForRead();
-  return IsSafeTimeReady(l->pb, *catalog_manager.GetXClusterManager());
+  return IsSafeTimeReady(l->pb, xcluster_manager);
 }
 
 }  // namespace
@@ -329,9 +329,7 @@ Result<bool> ShouldAddTableToReplicationGroup(
     }
 
     const auto& indexed_table_id = GetIndexedTableId(table_pb);
-    auto indexed_table_stream_ids =
-        catalog_manager.GetXClusterConsumerStreamIdsForTable(indexed_table_id);
-    if (indexed_table_stream_ids.empty()) {
+    if (!catalog_manager.GetXClusterManager()->IsTableReplicationConsumer(indexed_table_id)) {
       return false;
     }
   }
@@ -447,7 +445,8 @@ Result<IsOperationDoneResult> IsSetupUniverseReplicationDone(
 
   bool is_done = false;
   if (!is_alter_request && state == SysUniverseReplicationEntryPB::ACTIVE) {
-    is_done = VERIFY_RESULT(IsReplicationGroupReady(*universe, catalog_manager));
+    is_done =
+        VERIFY_RESULT(IsReplicationGroupReady(*universe, *catalog_manager.GetXClusterManager()));
   }
 
   return is_done ? IsOperationDoneResult::Done() : IsOperationDoneResult::NotDone();
@@ -599,9 +598,10 @@ Status RemoveTablesFromReplicationGroupInternal(
   RETURN_NOT_OK(catalog_manager.sys_catalog()->Upsert(epoch, &universe, cluster_config.get()));
 
   // 4. Clear in-mem maps.
-  catalog_manager.SyncXClusterConsumerReplicationStatusMap(replication_group_id, producer_map);
+  catalog_manager.GetXClusterManager()->SyncConsumerReplicationStatusMap(
+      replication_group_id, producer_map);
 
-  catalog_manager.ClearXClusterConsumerTableStreams(
+  catalog_manager.GetXClusterManager()->RemoveTableConsumerStreams(
       replication_group_id, consumer_table_ids_to_remove);
 
   l.Commit();
diff --git a/src/yb/master/xcluster/xcluster_source_manager.cc b/src/yb/master/xcluster/xcluster_source_manager.cc
index c2b83f0289..bd88536b63 100644
--- a/src/yb/master/xcluster/xcluster_source_manager.cc
+++ b/src/yb/master/xcluster/xcluster_source_manager.cc
@@ -31,6 +31,10 @@
 
 #include "yb/util/scope_exit.h"
 
+DEFINE_RUNTIME_bool(enable_tablet_split_of_xcluster_bootstrapping_tables, false,
+    "When set, it enables automatic tablet splitting for tables that are part of an "
+    "xCluster replication setup and are currently being bootstrapped for xCluster.");
+
 DECLARE_uint32(cdc_wal_retention_time_secs);
 DECLARE_bool(TEST_disable_cdc_state_insert_on_setup);
 
@@ -1157,4 +1161,17 @@ XClusterSourceManager::GetXClusterOutboundReplicationGroupInfo(
   return result;
 }
 
+Status XClusterSourceManager::ValidateSplitCandidateTable(const TableId& table_id) const {
+  if (!FLAGS_enable_tablet_split_of_xcluster_bootstrapping_tables &&
+      DoesTableHaveAnyBootstrappingStream(table_id)) {
+    return STATUS_FORMAT(
+        NotSupported,
+        "Tablet splitting is not supported for tables that are a part of"
+        " a bootstrapping CDC stream, table_id: $0",
+        table_id);
+  }
+
+  return Status::OK();
+}
+
 }  // namespace yb::master
diff --git a/src/yb/master/xcluster/xcluster_source_manager.h b/src/yb/master/xcluster/xcluster_source_manager.h
index 7a60841768..4af8293cc8 100644
--- a/src/yb/master/xcluster/xcluster_source_manager.h
+++ b/src/yb/master/xcluster/xcluster_source_manager.h
@@ -51,10 +51,6 @@ class XClusterSourceManager {
 
   std::optional<uint32> GetDefaultWalRetentionSec(const NamespaceId& namespace_id) const;
 
-  bool IsTableReplicated(const TableId& table_id) const EXCLUDES(tables_to_stream_map_mutex_);
-  bool DoesTableHaveAnyBootstrappingStream(const TableId& table_id) const
-      EXCLUDES(tables_to_stream_map_mutex_);
-
   void PopulateTabletDeleteRetainerInfoForTabletDrop(
       const TabletInfo& tablet_info, TabletDeleteRetainerInfo& delete_retainer) const
       EXCLUDES(tables_to_stream_map_mutex_);
@@ -167,6 +163,10 @@ class XClusterSourceManager {
   Result<std::unordered_map<NamespaceId, std::unordered_map<TableId, xrepl::StreamId>>>
   GetXClusterOutboundReplicationGroupInfo(const xcluster::ReplicationGroupId& replication_group_id);
 
+  bool IsTableReplicated(const TableId& table_id) const EXCLUDES(tables_to_stream_map_mutex_);
+
+  Status ValidateSplitCandidateTable(const TableId& table_id) const;
+
  private:
   friend class XClusterOutboundReplicationGroup;
 
@@ -232,6 +232,9 @@ class XClusterSourceManager {
       const std::vector<CDCStreamInfoPtr>& outbound_streams,
       std::vector<cdc::CDCStateTableKey>& entries_to_delete);
 
+  bool DoesTableHaveAnyBootstrappingStream(const TableId& table_id) const
+      EXCLUDES(tables_to_stream_map_mutex_);
+
   Master& master_;
   CatalogManager& catalog_manager_;
   SysCatalogTable& sys_catalog_;
diff --git a/src/yb/master/xcluster/xcluster_target_manager.cc b/src/yb/master/xcluster/xcluster_target_manager.cc
index 260094ac38..b46b461eac 100644
--- a/src/yb/master/xcluster/xcluster_target_manager.cc
+++ b/src/yb/master/xcluster/xcluster_target_manager.cc
@@ -24,11 +24,17 @@
 #include "yb/master/xcluster/xcluster_safe_time_service.h"
 
 #include "yb/master/xcluster/xcluster_status.h"
+#include "yb/master/xcluster_consumer_registry_service.h"
 #include "yb/util/jsonwriter.h"
 #include "yb/util/backoff_waiter.h"
 #include "yb/util/is_operation_done_result.h"
+#include "yb/util/scope_exit.h"
 #include "yb/util/status.h"
 
+DEFINE_RUNTIME_bool(xcluster_wait_on_ddl_alter, true,
+    "When xCluster replication sends a DDL change, wait for the user to enter a "
+    "compatible/matching entry.  Note: Can also set at runtime to resume after stall.");
+
 namespace yb::master {
 
 XClusterTargetManager::XClusterTargetManager(
@@ -38,33 +44,57 @@ XClusterTargetManager::XClusterTargetManager(
 XClusterTargetManager::~XClusterTargetManager() {}
 
 void XClusterTargetManager::Shutdown() {
-  if (xcluster_safe_time_service_) {
-    xcluster_safe_time_service_->Shutdown();
+  if (safe_time_service_) {
+    safe_time_service_->Shutdown();
   }
 }
 
 Status XClusterTargetManager::Init() {
-  DCHECK(!xcluster_safe_time_service_);
-  xcluster_safe_time_service_ = std::make_unique<XClusterSafeTimeService>(
+  DCHECK(!safe_time_service_);
+  safe_time_service_ = std::make_unique<XClusterSafeTimeService>(
       &master_, &catalog_manager_, master_.metric_registry());
-  RETURN_NOT_OK(xcluster_safe_time_service_->Init());
+  RETURN_NOT_OK(safe_time_service_->Init());
 
   return Status::OK();
 }
 
 void XClusterTargetManager::Clear() {
-  xcluster_safe_time_info_.Clear();
+  safe_time_info_.Clear();
+
   removed_deleted_tables_from_replication_ = false;
+
+  auto_flags_revalidation_needed_ = true;
+
+  {
+    std::lock_guard l(replication_error_map_mutex_);
+    replication_error_map_.clear();
+  }
+
+  {
+    std::lock_guard l(table_stream_ids_map_mutex_);
+    table_stream_ids_map_.clear();
+  }
 }
 
 Status XClusterTargetManager::RunLoaders() {
-  RETURN_NOT_OK(
-      sys_catalog_.Load<XClusterSafeTimeLoader>("XCluster safe time", xcluster_safe_time_info_));
+  RETURN_NOT_OK(sys_catalog_.Load<XClusterSafeTimeLoader>("XCluster safe time", safe_time_info_));
   return Status::OK();
 }
 
 void XClusterTargetManager::SysCatalogLoaded() {
-  xcluster_safe_time_service_->ScheduleTaskIfNeeded();
+  // Refresh the Consumer registry.
+  auto cluster_config = catalog_manager_.ClusterConfig();
+  if (cluster_config) {
+    auto l = cluster_config->LockForRead();
+    if (l->pb.has_consumer_registry()) {
+      auto& producer_map = l->pb.consumer_registry().producer_map();
+      for (const auto& [replication_group_id, _] : producer_map) {
+        SyncReplicationStatusMap(xcluster::ReplicationGroupId(replication_group_id), producer_map);
+      }
+    }
+  }
+
+  safe_time_service_->ScheduleTaskIfNeeded();
 }
 
 void XClusterTargetManager::DumpState(std::ostream& out, bool on_disk_dump) const {
@@ -72,7 +102,7 @@ void XClusterTargetManager::DumpState(std::ostream& out, bool on_disk_dump) cons
     return;
   }
 
-  auto l = xcluster_safe_time_info_.LockForRead();
+  auto l = safe_time_info_.LockForRead();
   if (l->pb.safe_time_map().empty()) {
     return;
   }
@@ -81,16 +111,16 @@ void XClusterTargetManager::DumpState(std::ostream& out, bool on_disk_dump) cons
 
 void XClusterTargetManager::CreateXClusterSafeTimeTableAndStartService() {
   WARN_NOT_OK(
-      xcluster_safe_time_service_->CreateXClusterSafeTimeTableIfNotFound(),
+      safe_time_service_->CreateXClusterSafeTimeTableIfNotFound(),
       "Creation of XClusterSafeTime table failed");
 
-  xcluster_safe_time_service_->ScheduleTaskIfNeeded();
+  safe_time_service_->ScheduleTaskIfNeeded();
 }
 
 Status XClusterTargetManager::GetXClusterSafeTime(
     GetXClusterSafeTimeResponsePB* resp, const LeaderEpoch& epoch) {
   RETURN_NOT_OK_SET_CODE(
-      xcluster_safe_time_service_->GetXClusterSafeTimeInfoFromMap(epoch, resp),
+      safe_time_service_->GetXClusterSafeTimeInfoFromMap(epoch, resp),
       MasterError(MasterErrorPB::INTERNAL_ERROR));
 
   // Also fill out the namespace_name for each entry.
@@ -109,7 +139,7 @@ Status XClusterTargetManager::GetXClusterSafeTime(
 
 Result<HybridTime> XClusterTargetManager::GetXClusterSafeTime(
     const NamespaceId& namespace_id) const {
-  auto l = xcluster_safe_time_info_.LockForRead();
+  auto l = safe_time_info_.LockForRead();
   SCHECK(
       l->pb.safe_time_map().count(namespace_id), NotFound,
       "XCluster safe time not found for namespace $0", namespace_id);
@@ -120,7 +150,7 @@ Result<HybridTime> XClusterTargetManager::GetXClusterSafeTime(
 Result<XClusterNamespaceToSafeTimeMap> XClusterTargetManager::GetXClusterNamespaceToSafeTimeMap()
     const {
   XClusterNamespaceToSafeTimeMap result;
-  auto l = xcluster_safe_time_info_.LockForRead();
+  auto l = safe_time_info_.LockForRead();
 
   for (auto& [namespace_id, hybrid_time] : l->pb.safe_time_map()) {
     result[namespace_id] = HybridTime(hybrid_time);
@@ -141,18 +171,18 @@ Status XClusterTargetManager::GetXClusterSafeTimeForNamespace(
 Result<HybridTime> XClusterTargetManager::GetXClusterSafeTimeForNamespace(
     const LeaderEpoch& epoch, const NamespaceId& namespace_id,
     const XClusterSafeTimeFilter& filter) {
-  return xcluster_safe_time_service_->GetXClusterSafeTimeForNamespace(
+  return safe_time_service_->GetXClusterSafeTimeForNamespace(
       epoch.leader_term, namespace_id, filter);
 }
 
 Status XClusterTargetManager::RefreshXClusterSafeTimeMap(const LeaderEpoch& epoch) {
-  RETURN_NOT_OK(xcluster_safe_time_service_->ComputeSafeTime(epoch.leader_term));
+  RETURN_NOT_OK(safe_time_service_->ComputeSafeTime(epoch.leader_term));
   return Status::OK();
 }
 
 Status XClusterTargetManager::SetXClusterNamespaceToSafeTimeMap(
     const int64_t leader_term, const XClusterNamespaceToSafeTimeMap& safe_time_map) {
-  auto l = xcluster_safe_time_info_.LockForWrite();
+  auto l = safe_time_info_.LockForWrite();
   auto& safe_time_map_pb = *l.mutable_data()->pb.mutable_safe_time_map();
   safe_time_map_pb.clear();
   for (auto& [namespace_id, hybrid_time] : safe_time_map) {
@@ -160,7 +190,7 @@ Status XClusterTargetManager::SetXClusterNamespaceToSafeTimeMap(
   }
 
   RETURN_NOT_OK_PREPEND(
-      sys_catalog_.Upsert(leader_term, &xcluster_safe_time_info_),
+      sys_catalog_.Upsert(leader_term, &safe_time_info_),
       "Updating XCluster safe time in sys-catalog");
 
   l.Commit();
@@ -170,7 +200,7 @@ Status XClusterTargetManager::SetXClusterNamespaceToSafeTimeMap(
 
 Status XClusterTargetManager::FillHeartbeatResponse(
     const TSHeartbeatRequestPB& req, TSHeartbeatResponsePB* resp) const {
-  auto l = xcluster_safe_time_info_.LockForRead();
+  auto l = safe_time_info_.LockForRead();
   if (!l->pb.safe_time_map().empty()) {
     *resp->mutable_xcluster_namespace_to_safe_time() = l->pb.safe_time_map();
   }
@@ -221,24 +251,23 @@ Status XClusterTargetManager::WaitForSetupUniverseReplicationToFinish(
 
 Status XClusterTargetManager::RemoveDroppedTablesOnConsumer(
     const std::unordered_set<TabletId>& table_ids, const LeaderEpoch& epoch) {
-  auto table_stream_map = catalog_manager_.GetXClusterConsumerTableStreams();
-  if (table_stream_map.empty()) {
-    return Status::OK();
-  }
-
   std::map<xcluster::ReplicationGroupId, std::vector<TableId>> replication_group_producer_tables;
   {
+    SharedLock table_stream_l(table_stream_ids_map_mutex_);
+    if (table_stream_ids_map_.empty()) {
+      return Status::OK();
+    }
+
     auto cluster_config = catalog_manager_.ClusterConfig();
     auto l = cluster_config->LockForRead();
     auto& replication_group_map = l->pb.consumer_registry().producer_map();
 
     for (auto& table_id : table_ids) {
-      auto stream_ids = FindOrNull(table_stream_map, table_id);
-      if (!stream_ids) {
+      if (!table_stream_ids_map_.contains(table_id)) {
         continue;
       }
 
-      for (const auto& [replication_group_id, stream_id] : *stream_ids) {
+      for (const auto& [replication_group_id, stream_id] : table_stream_ids_map_.at(table_id)) {
         // Fetch the stream entry so we can update the mappings.
         auto producer_entry = FindOrNull(replication_group_map, replication_group_id.ToString());
         // If we can't find the entries, then the stream has been deleted.
@@ -281,6 +310,11 @@ void XClusterTargetManager::RunBgTasks(const LeaderEpoch& epoch) {
   WARN_NOT_OK(
       RemoveDroppedTablesFromReplication(epoch),
       "Failed to remove dropped tables from consumer replication groups");
+
+  WARN_NOT_OK(RefreshLocalAutoFlagConfig(epoch), "Failed refreshing local AutoFlags config");
+
+  WARN_NOT_OK(
+      ProcessPendingSchemaChanges(epoch), "Failed processing xCluster Pending Schema Changes");
 }
 
 Status XClusterTargetManager::RemoveDroppedTablesFromReplication(const LeaderEpoch& epoch) {
@@ -289,12 +323,14 @@ Status XClusterTargetManager::RemoveDroppedTablesFromReplication(const LeaderEpo
   }
 
   std::unordered_set<TabletId> tables_to_remove;
-  auto table_stream_map = catalog_manager_.GetXClusterConsumerTableStreams();
-  for (const auto& [table_id, _] : table_stream_map) {
-    auto table_info = catalog_manager_.GetTableInfo(table_id);
-    if (!table_info || !table_info->LockForRead()->visible_to_client()) {
-      tables_to_remove.insert(table_id);
-      continue;
+  {
+    SharedLock table_stream_l(table_stream_ids_map_mutex_);
+    for (const auto& [table_id, _] : table_stream_ids_map_) {
+      auto table_info = catalog_manager_.GetTableInfo(table_id);
+      if (!table_info || !table_info->LockForRead()->visible_to_client()) {
+        tables_to_remove.insert(table_id);
+        continue;
+      }
     }
   }
 
@@ -344,7 +380,7 @@ XClusterTargetManager::GetXClusterStatus() const {
 
   GetReplicationStatusResponsePB replication_status;
   GetReplicationStatusRequestPB req;
-  RETURN_NOT_OK(catalog_manager_.GetReplicationStatus(&req, &replication_status, /*rpc=*/nullptr));
+  RETURN_NOT_OK(GetReplicationStatus(&req, &replication_status));
 
   std::unordered_map<xrepl::StreamId, std::string> stream_status;
   for (const auto& table_stream_status : replication_status.statuses()) {
@@ -465,7 +501,7 @@ Result<XClusterInboundReplicationGroupStatus> XClusterTargetManager::GetUniverse
 Status XClusterTargetManager::PopulateXClusterStatusJson(JsonWriter& jw) const {
   GetReplicationStatusResponsePB replication_status;
   GetReplicationStatusRequestPB req;
-  RETURN_NOT_OK(catalog_manager_.GetReplicationStatus(&req, &replication_status, /*rpc=*/nullptr));
+  RETURN_NOT_OK(GetReplicationStatus(&req, &replication_status));
 
   SysClusterConfigEntryPB cluster_config =
       VERIFY_RESULT(catalog_manager_.GetClusterConfig());
@@ -532,4 +568,537 @@ Status XClusterTargetManager::ClearXClusterSourceTableId(
   return Status::OK();
 }
 
+void XClusterTargetManager::NotifyAutoFlagsConfigChanged() {
+  auto_flags_revalidation_needed_ = true;
+}
+
+Status XClusterTargetManager::ReportNewAutoFlagConfigVersion(
+    const xcluster::ReplicationGroupId& replication_group_id, uint32 new_version,
+    const LeaderEpoch& epoch) {
+  auto replication_info = catalog_manager_.GetUniverseReplication(replication_group_id);
+  SCHECK(
+      replication_info, NotFound, "Missing replication group $0", replication_group_id.ToString());
+
+  auto cluster_config = catalog_manager_.ClusterConfig();
+
+  return RefreshAutoFlagConfigVersion(
+      sys_catalog_, *replication_info, *cluster_config.get(), new_version,
+      [&master = master_]() { return master.GetAutoFlagsConfig(); }, epoch);
+}
+
+Status XClusterTargetManager::RefreshLocalAutoFlagConfig(const LeaderEpoch& epoch) {
+  if (!auto_flags_revalidation_needed_) {
+    return Status::OK();
+  }
+
+  auto se = ScopeExit([this] { auto_flags_revalidation_needed_ = true; });
+  auto_flags_revalidation_needed_ = false;
+
+  auto replication_groups = catalog_manager_.GetAllUniverseReplications();
+  if (replication_groups.empty()) {
+    return Status::OK();
+  }
+
+  const auto local_auto_flags_config = master_.GetAutoFlagsConfig();
+  auto cluster_config = catalog_manager_.ClusterConfig();
+
+  bool update_failed = false;
+  for (const auto& replication_info : replication_groups) {
+    auto status = HandleLocalAutoFlagsConfigChange(
+        sys_catalog_, *replication_info, *cluster_config.get(), local_auto_flags_config, epoch);
+    if (!status.ok()) {
+      LOG(WARNING) << "Failed to handle local AutoFlags config change for replication group "
+                   << replication_info->id() << ": " << status;
+      update_failed = true;
+    }
+  }
+
+  SCHECK(!update_failed, IllegalState, "Failed to handle local AutoFlags config change");
+
+  se.Cancel();
+
+  return Status::OK();
+}
+
+void XClusterTargetManager::SyncReplicationStatusMap(
+    const xcluster::ReplicationGroupId& replication_group_id,
+    const google::protobuf::Map<std::string, cdc::ProducerEntryPB>& producer_map) {
+  std::lock_guard lock(replication_error_map_mutex_);
+
+  if (producer_map.count(replication_group_id.ToString()) == 0) {
+    // Replication group has been deleted.
+    replication_error_map_.erase(replication_group_id);
+    return;
+  }
+
+  auto& replication_group_errors = replication_error_map_[replication_group_id];
+  auto& producer_entry = producer_map.at(replication_group_id.ToString());
+
+  std::unordered_set<TableId> all_consumer_table_ids;
+  for (auto& [_, stream_map] : producer_entry.stream_map()) {
+    const auto& consumer_table_id = stream_map.consumer_table_id();
+
+    std::unordered_set<TabletId> all_producer_tablet_ids;
+    for (auto& [_, producer_tablet_ids] : stream_map.consumer_producer_tablet_map()) {
+      all_producer_tablet_ids.insert(
+          producer_tablet_ids.tablets().begin(), producer_tablet_ids.tablets().end());
+    }
+
+    if (all_producer_tablet_ids.empty()) {
+      continue;
+    }
+    all_consumer_table_ids.insert(consumer_table_id);
+
+    auto& tablet_error_map = replication_group_errors[consumer_table_id];
+    // Remove tablets that are no longer part of replication.
+    std::erase_if(tablet_error_map, [&all_producer_tablet_ids](const auto& entry) {
+      return !all_producer_tablet_ids.contains(entry.first);
+    });
+
+    // Add new tablets.
+    for (const auto& producer_tablet_id : all_producer_tablet_ids) {
+      if (!tablet_error_map.contains(producer_tablet_id)) {
+        // Default to UNINITIALIZED error. Once the Pollers send the status, this will be updated.
+        tablet_error_map[producer_tablet_id].error =
+            ReplicationErrorPb::REPLICATION_ERROR_UNINITIALIZED;
+      }
+    }
+  }
+
+  // Remove tables that are no longer part of replication.
+  std::erase_if(replication_group_errors, [&all_consumer_table_ids](const auto& entry) {
+    return !all_consumer_table_ids.contains(entry.first);
+  });
+
+  if (VLOG_IS_ON(1)) {
+    LOG(INFO) << "Synced replication_error_map_ for replication group " << replication_group_id;
+    for (const auto& [consumer_table_id, tablet_error_map] : replication_group_errors) {
+      LOG(INFO) << "Table: " << consumer_table_id;
+      for (const auto& [tablet_id, error] : tablet_error_map) {
+        LOG(INFO) << "Tablet: " << tablet_id << ", Error: " << ReplicationErrorPb_Name(error.error);
+      }
+    }
+  }
+}
+
+void XClusterTargetManager::StoreReplicationStatus(
+    const XClusterConsumerReplicationStatusPB& consumer_replication_status) {
+  const auto& replication_group_id = consumer_replication_status.replication_group_id();
+
+  std::lock_guard lock(replication_error_map_mutex_);
+  // Heartbeats can report stale entries. So we skip anything that is not in
+  // replication_error_map_.
+
+  auto* replication_error_map =
+      FindOrNull(replication_error_map_, xcluster::ReplicationGroupId(replication_group_id));
+  if (!replication_error_map) {
+    VLOG_WITH_FUNC(2) << "Skipping deleted replication group " << replication_group_id;
+    return;
+  }
+
+  for (const auto& table_status : consumer_replication_status.table_status()) {
+    const auto& consumer_table_id = table_status.consumer_table_id();
+    auto* consumer_table_map = FindOrNull(*replication_error_map, consumer_table_id);
+    if (!consumer_table_map) {
+      VLOG_WITH_FUNC(2) << "Skipping removed table " << consumer_table_id
+                        << " in replication group " << replication_group_id;
+      continue;
+    }
+
+    for (const auto& stream_tablet_status : table_status.stream_tablet_status()) {
+      const auto& producer_tablet_id = stream_tablet_status.producer_tablet_id();
+      auto* tablet_status_map = FindOrNull(*consumer_table_map, producer_tablet_id);
+      if (!tablet_status_map) {
+        VLOG_WITH_FUNC(2) << "Skipping removed tablet " << producer_tablet_id
+                          << " in replication group " << replication_group_id;
+        continue;
+      }
+
+      // Get status from highest term only. When consumer leaders move we may get stale status
+      // from older leaders.
+      if (tablet_status_map->consumer_term <= stream_tablet_status.consumer_term()) {
+        DCHECK_NE(
+            stream_tablet_status.error(), ReplicationErrorPb::REPLICATION_ERROR_UNINITIALIZED);
+        tablet_status_map->consumer_term = stream_tablet_status.consumer_term();
+        tablet_status_map->error = stream_tablet_status.error();
+        VLOG_WITH_FUNC(2) << "Storing error for replication group: " << replication_group_id
+                          << ", consumer table: " << consumer_table_id
+                          << ", tablet: " << producer_tablet_id
+                          << ", term: " << stream_tablet_status.consumer_term()
+                          << ", error: " << ReplicationErrorPb_Name(stream_tablet_status.error());
+      } else {
+        VLOG_WITH_FUNC(2) << "Skipping stale error for  replication group: " << replication_group_id
+                          << ", consumer table: " << consumer_table_id
+                          << ", tablet: " << producer_tablet_id
+                          << ", term: " << stream_tablet_status.consumer_term();
+      }
+    }
+  }
+}
+
+Result<bool> XClusterTargetManager::HasReplicationGroupErrors(
+    const xcluster::ReplicationGroupId& replication_group_id) const {
+  SharedLock l(replication_error_map_mutex_);
+
+  auto* replication_error_map = FindOrNull(replication_error_map_, replication_group_id);
+  SCHECK(
+      replication_error_map, NotFound, "Could not find replication group $0",
+      replication_group_id.ToString());
+
+  std::unordered_map<TableId, std::unordered_set<std::string>> table_errors;
+  for (const auto& [consumer_table_id, tablet_error_map] : *replication_error_map) {
+    for (const auto& [_, error_info] : tablet_error_map) {
+      if (error_info.error != ReplicationErrorPb::REPLICATION_OK) {
+        table_errors[consumer_table_id].insert(ReplicationErrorPb_Name(error_info.error));
+      }
+    }
+  }
+
+  if (!table_errors.empty()) {
+    YB_LOG_EVERY_N_SECS(WARNING, 60)
+        << "Replication group " << replication_group_id
+        << " has errors for the following tables:" << AsString(table_errors);
+  }
+  return !table_errors.empty();
+}
+
+Status XClusterTargetManager::PopulateReplicationGroupErrors(
+    const xcluster::ReplicationGroupId& replication_group_id,
+    GetReplicationStatusResponsePB* resp) const {
+  auto* replication_error_map = FindOrNull(replication_error_map_, replication_group_id);
+  SCHECK(
+      replication_error_map, NotFound, "Could not find replication group $0",
+      replication_group_id.ToString());
+
+  SharedLock l(table_stream_ids_map_mutex_);
+
+  for (const auto& [consumer_table_id, tablet_error_map] : *replication_error_map) {
+    if (!table_stream_ids_map_.contains(consumer_table_id) ||
+        !table_stream_ids_map_.at(consumer_table_id).contains(replication_group_id)) {
+      // This is not expected. The two maps should be kept in sync.
+      LOG(DFATAL) << "replication_error_map_ contains consumer table " << consumer_table_id
+                  << " in replication group " << replication_group_id
+                  << " but table_stream_ids_map_ does not.";
+      continue;
+    }
+
+    // Map from error to list of producer tablet IDs/Pollers reporting them.
+    std::unordered_map<ReplicationErrorPb, std::vector<TabletId>> errors;
+    for (const auto& [tablet_id, error_info] : tablet_error_map) {
+      errors[error_info.error].push_back(tablet_id);
+    }
+
+    if (errors.empty()) {
+      continue;
+    }
+
+    auto resp_status = resp->add_statuses();
+    resp_status->set_table_id(consumer_table_id);
+    const auto& stream_id = table_stream_ids_map_.at(consumer_table_id).at(replication_group_id);
+    resp_status->set_stream_id(stream_id.ToString());
+    for (const auto& [error_pb, tablet_ids] : errors) {
+      if (error_pb == ReplicationErrorPb::REPLICATION_OK) {
+        // Do not add errors for healthy tablets.
+        continue;
+      }
+
+      auto* resp_error = resp_status->add_errors();
+      resp_error->set_error(error_pb);
+      // Only include the first 20 tablet IDs to limit response size. VLOG(4) will log all tablet to
+      // the log.
+      resp_error->set_error_detail(
+          Format("Producer Tablet IDs: $0", JoinStringsLimitCount(tablet_ids, ",", 20)));
+      if (VLOG_IS_ON(4)) {
+        VLOG(4) << "Replication error " << ReplicationErrorPb_Name(error_pb)
+                << " for ReplicationGroup: " << replication_group_id << ", stream id: " << stream_id
+                << ", consumer table: " << consumer_table_id << ", producer tablet IDs:";
+        for (const auto& tablet_id : tablet_ids) {
+          VLOG(4) << tablet_id;
+        }
+      }
+    }
+  }
+
+  return Status::OK();
+}
+
+Status XClusterTargetManager::GetReplicationStatus(
+    const GetReplicationStatusRequestPB* req, GetReplicationStatusResponsePB* resp) const {
+  SharedLock l(replication_error_map_mutex_);
+
+  // If the 'replication_group_id' is given, only populate the status for the streams in that
+  // ReplicationGroup. Otherwise, populate all the status for all groups.
+  if (!req->replication_group_id().empty()) {
+    return PopulateReplicationGroupErrors(
+        xcluster::ReplicationGroupId(req->replication_group_id()), resp);
+  }
+
+  for (const auto& [replication_id, _] : replication_error_map_) {
+    RETURN_NOT_OK(PopulateReplicationGroupErrors(replication_id, resp));
+  }
+
+  return Status::OK();
+}
+
+void XClusterTargetManager::RecordTableStream(
+    const TableId& table_id, const xcluster::ReplicationGroupId& replication_group_id,
+    const xrepl::StreamId& stream_id) {
+  std::lock_guard lock(table_stream_ids_map_mutex_);
+  table_stream_ids_map_[table_id].emplace(replication_group_id, stream_id);
+}
+
+void XClusterTargetManager::RemoveTableStream(
+    const TableId& table_id, const xcluster::ReplicationGroupId& replication_group_id) {
+  RemoveTableStreams(replication_group_id, {table_id});
+}
+
+void XClusterTargetManager::RemoveTableStreams(
+    const xcluster::ReplicationGroupId& replication_group_id,
+    const std::set<TableId>& tables_to_clear) {
+  std::lock_guard lock(table_stream_ids_map_mutex_);
+  for (const auto& table_id : tables_to_clear) {
+    if (table_stream_ids_map_[table_id].erase(replication_group_id) < 1) {
+      LOG(WARNING) << "Failed to remove consumer table from mapping. " << "table_id: " << table_id
+                   << ": replication_group_id: " << replication_group_id;
+    }
+    if (table_stream_ids_map_[table_id].empty()) {
+      table_stream_ids_map_.erase(table_id);
+    }
+  }
+}
+
+std::unordered_map<xcluster::ReplicationGroupId, xrepl::StreamId>
+XClusterTargetManager::GetStreamIdsForTable(const TableId& table_id) const {
+  SharedLock lock(table_stream_ids_map_mutex_);
+  auto* stream_ids = FindOrNull(table_stream_ids_map_, table_id);
+  if (!stream_ids) {
+    return {};
+  }
+
+  return *stream_ids;
+}
+
+bool XClusterTargetManager::IsTableReplicated(const TableId& table_id) const {
+  return !GetStreamIdsForTable(table_id).empty();
+}
+
+Result<TableId> XClusterTargetManager::GetTableIdForStreamId(
+    const xcluster::ReplicationGroupId& replication_group_id,
+    const xrepl::StreamId& stream_id) const {
+  SharedLock l(table_stream_ids_map_mutex_);
+
+  auto iter = std::find_if(
+      table_stream_ids_map_.begin(), table_stream_ids_map_.end(),
+      [&replication_group_id, &stream_id](auto& id_map) {
+        return ContainsKeyValuePair(id_map.second, replication_group_id, stream_id);
+      });
+  SCHECK(
+      iter != table_stream_ids_map_.end(), NotFound,
+      Format("Unable to find the stream id $0", stream_id));
+
+  return iter->first;
+}
+
+Status XClusterTargetManager::HandleTabletSplit(
+    const TableId& consumer_table_id, const SplitTabletIds& split_tablet_ids,
+    const LeaderEpoch& epoch) {
+  // Check if this table is consuming a stream.
+  auto stream_ids = GetStreamIdsForTable(consumer_table_id);
+  if (stream_ids.empty()) {
+    return Status::OK();
+  }
+
+  auto consumer_tablet_keys = VERIFY_RESULT(catalog_manager_.GetTableKeyRanges(consumer_table_id));
+  auto cluster_config = catalog_manager_.ClusterConfig();
+  auto l = cluster_config->LockForWrite();
+  for (const auto& [replication_group_id, stream_id] : stream_ids) {
+    // Fetch the stream entry so we can update the mappings.
+    auto replication_group_map =
+        l.mutable_data()->pb.mutable_consumer_registry()->mutable_producer_map();
+    auto producer_entry = FindOrNull(*replication_group_map, replication_group_id.ToString());
+    // If we can't find the entries, then the stream has been deleted.
+    if (!producer_entry) {
+      LOG(WARNING) << "Unable to find the producer entry for universe " << replication_group_id;
+      continue;
+    }
+    auto stream_entry = FindOrNull(*producer_entry->mutable_stream_map(), stream_id.ToString());
+    if (!stream_entry) {
+      LOG(WARNING) << "Unable to find the producer entry for universe " << replication_group_id
+                   << ", stream " << stream_id;
+      continue;
+    }
+    DCHECK(stream_entry->consumer_table_id() == consumer_table_id);
+
+    RETURN_NOT_OK(
+        UpdateTabletMappingOnConsumerSplit(consumer_tablet_keys, split_tablet_ids, stream_entry));
+  }
+
+  // Also bump the cluster_config_ version so that changes are propagated to tservers.
+  l.mutable_data()->pb.set_version(l.mutable_data()->pb.version() + 1);
+
+  RETURN_NOT_OK_PREPEND(
+      sys_catalog_.Upsert(epoch, cluster_config.get()),
+      "Failed updating cluster config in sys-catalog");
+  l.Commit();
+
+  CreateXClusterSafeTimeTableAndStartService();
+
+  return Status::OK();
+}
+
+Status XClusterTargetManager::ValidateNewSchema(
+    const TableInfo& table_info, const Schema& consumer_schema) const {
+  if (!FLAGS_xcluster_wait_on_ddl_alter) {
+    return Status::OK();
+  }
+
+  // Check if this table is consuming a stream.
+  auto stream_ids = GetStreamIdsForTable(table_info.id());
+  if (stream_ids.empty()) {
+    return Status::OK();
+  }
+
+  auto cluster_config = catalog_manager_.ClusterConfig();
+  auto l = cluster_config->LockForRead();
+  for (const auto& [replication_group_id, stream_id] : stream_ids) {
+    // Fetch the stream entry to get Schema information.
+    auto& replication_group_map = l.data().pb.consumer_registry().producer_map();
+    auto producer_entry = FindOrNull(replication_group_map, replication_group_id.ToString());
+    SCHECK(producer_entry, NotFound, Format("Missing universe $0", replication_group_id));
+    auto stream_entry = FindOrNull(producer_entry->stream_map(), stream_id.ToString());
+    SCHECK(stream_entry, NotFound, Format("Missing stream $0:$1", replication_group_id, stream_id));
+
+    // If we are halted on a Schema update as a Consumer...
+    auto& producer_schema_pb = stream_entry->producer_schema();
+    if (producer_schema_pb.has_pending_schema()) {
+      // Compare our new schema to the Producer's pending schema.
+      Schema producer_schema;
+      RETURN_NOT_OK(SchemaFromPB(producer_schema_pb.pending_schema(), &producer_schema));
+
+      // This new schema should allow us to consume data for the Producer's next schema.
+      // If we instead diverge, we will be unable to consume any more of the Producer's data.
+      bool can_apply = consumer_schema.EquivalentForDataCopy(producer_schema);
+      SCHECK(
+          can_apply, IllegalState,
+          Format(
+              "New Schema not compatible with XCluster Producer Schema:\n new={$0}\n producer={$1}",
+              consumer_schema.ToString(), producer_schema.ToString()));
+    }
+  }
+
+  return Status::OK();
+}
+
+Status XClusterTargetManager::ResumeStreamsAfterNewSchema(
+    const TableInfo& table_info, SchemaVersion consumer_schema_version, const LeaderEpoch& epoch) {
+  if (!FLAGS_xcluster_wait_on_ddl_alter) {
+    return Status::OK();
+  }
+
+  // With Replication Enabled, verify that we've finished applying the New Schema.
+  // If we're waiting for a Schema because we saw the a replication source with a change,
+  // resume replication now that the alter is complete.
+
+  auto stream_ids = GetStreamIdsForTable(table_info.id());
+  if (stream_ids.empty()) {
+    return Status::OK();
+  }
+
+  bool found_schema = false, resuming_replication = false;
+
+  // Now that we've applied the new schema: find pending replication, clear state, resume.
+  auto cluster_config = catalog_manager_.ClusterConfig();
+  auto l = cluster_config->LockForWrite();
+  for (const auto& [replication_group_id, stream_id] : stream_ids) {
+    // Fetch the stream entry to get Schema information.
+    auto replication_group_map =
+        l.mutable_data()->pb.mutable_consumer_registry()->mutable_producer_map();
+    auto producer_entry = FindOrNull(*replication_group_map, replication_group_id.ToString());
+    if (!producer_entry) {
+      continue;
+    }
+    auto stream_entry = FindOrNull(*producer_entry->mutable_stream_map(), stream_id.ToString());
+    if (!stream_entry) {
+      continue;
+    }
+
+    auto producer_schema_pb = stream_entry->mutable_producer_schema();
+    if (producer_schema_pb->has_pending_schema()) {
+      found_schema = true;
+      Schema consumer_schema, producer_schema;
+      RETURN_NOT_OK(table_info.GetSchema(&consumer_schema));
+      RETURN_NOT_OK(SchemaFromPB(producer_schema_pb->pending_schema(), &producer_schema));
+      if (consumer_schema.EquivalentForDataCopy(producer_schema)) {
+        resuming_replication = true;
+        auto pending_version = producer_schema_pb->pending_schema_version();
+        LOG(INFO) << "Consumer schema @ version " << consumer_schema_version
+                  << " is now data copy compatible with Producer: " << stream_id
+                  << " @ schema version " << pending_version;
+        // Clear meta we use to track progress on receiving all WAL entries with old schema.
+        producer_schema_pb->set_validated_schema_version(
+            std::max(producer_schema_pb->validated_schema_version(), pending_version));
+        producer_schema_pb->set_last_compatible_consumer_schema_version(consumer_schema_version);
+        producer_schema_pb->clear_pending_schema();
+        // Bump the ClusterConfig version so we'll broadcast new schema version & resume operation.
+        l.mutable_data()->pb.set_version(l.mutable_data()->pb.version() + 1);
+      } else {
+        LOG(INFO) << "Consumer schema not compatible for data copy of next Producer schema.";
+      }
+    }
+  }
+
+  if (resuming_replication) {
+    RETURN_NOT_OK_PREPEND(
+        sys_catalog_.Upsert(epoch, cluster_config.get()), "Failed updating cluster config");
+    l.Commit();
+    LOG(INFO) << "Resuming Replication on " << table_info.id() << " after Consumer ALTER.";
+  } else if (!found_schema) {
+    LOG(INFO) << "No pending schema change from Producer.";
+  }
+
+  return Status::OK();
+}
+
+Status XClusterTargetManager::ProcessPendingSchemaChanges(const LeaderEpoch& epoch) {
+  if (!GetAtomicFlag(&FLAGS_xcluster_wait_on_ddl_alter)) {
+    // See if any Streams are waiting on a pending_schema.
+    bool found_pending_schema = false;
+    auto cluster_config = catalog_manager_.ClusterConfig();
+    auto cl = cluster_config->LockForWrite();
+    auto replication_group_map =
+        cl.mutable_data()->pb.mutable_consumer_registry()->mutable_producer_map();
+    // For each user entry.
+    for (auto& replication_group_id_and_entry : *replication_group_map) {
+      // For each CDC stream in that Universe.
+      for (auto& stream_id_and_entry :
+           *replication_group_id_and_entry.second.mutable_stream_map()) {
+        auto& stream_entry = stream_id_and_entry.second;
+        if (stream_entry.has_producer_schema() &&
+            stream_entry.producer_schema().has_pending_schema()) {
+          // Force resume this stream.
+          auto schema = stream_entry.mutable_producer_schema();
+          schema->set_validated_schema_version(
+              std::max(schema->validated_schema_version(), schema->pending_schema_version()));
+          schema->clear_pending_schema();
+
+          found_pending_schema = true;
+          LOG(INFO) << "Force Resume Consumer schema: " << stream_id_and_entry.first
+                    << " @ schema version " << schema->pending_schema_version();
+        }
+      }
+    }
+
+    if (found_pending_schema) {
+      // Bump the ClusterConfig version so we'll broadcast new schema version & resume operation.
+      cl.mutable_data()->pb.set_version(cl.mutable_data()->pb.version() + 1);
+      RETURN_NOT_OK_PREPEND(
+          sys_catalog_.Upsert(epoch.leader_term, cluster_config.get()),
+          "Failed updating cluster config");
+      cl.Commit();
+    }
+  }
+
+  return Status::OK();
+}
+
 }  // namespace yb::master
diff --git a/src/yb/master/xcluster/xcluster_target_manager.h b/src/yb/master/xcluster/xcluster_target_manager.h
index bec6756776..22132effd1 100644
--- a/src/yb/master/xcluster/xcluster_target_manager.h
+++ b/src/yb/master/xcluster/xcluster_target_manager.h
@@ -29,6 +29,7 @@ class PostTabletCreateTaskBase;
 class UniverseReplicationInfo;
 class XClusterSafeTimeService;
 struct XClusterInboundReplicationGroupStatus;
+class XClusterConsumerReplicationStatusPB;
 
 class XClusterTargetManager {
  public:
@@ -54,12 +55,11 @@ class XClusterTargetManager {
 
   Status RefreshXClusterSafeTimeMap(const LeaderEpoch& epoch);
 
-  XClusterSafeTimeService* TEST_xcluster_safe_time_service() {
-    return xcluster_safe_time_service_.get();
-  }
+  XClusterSafeTimeService* TEST_xcluster_safe_time_service() { return safe_time_service_.get(); }
 
   Status RemoveDroppedTablesOnConsumer(
-      const std::unordered_set<TabletId>& table_ids, const LeaderEpoch& epoch);
+      const std::unordered_set<TabletId>& table_ids, const LeaderEpoch& epoch)
+      EXCLUDES(table_stream_ids_map_mutex_);
 
   Status WaitForSetupUniverseReplicationToFinish(
       const xcluster::ReplicationGroupId& replication_group_id, CoarseTimePoint deadline);
@@ -89,7 +89,8 @@ class XClusterTargetManager {
   // After a table is marked for drop we remove them from replication. If master leader failed over
   // after persisting the table state but before the xcluster cleanup we will have deletes tables in
   // our replication group that needs to be lazily cleaned up.
-  Status RemoveDroppedTablesFromReplication(const LeaderEpoch& epoch);
+  Status RemoveDroppedTablesFromReplication(const LeaderEpoch& epoch)
+      EXCLUDES(table_stream_ids_map_mutex_);
 
   std::vector<std::shared_ptr<PostTabletCreateTaskBase>> GetPostTabletCreateTasks(
       const TableInfoPtr& table_info, const LeaderEpoch& epoch);
@@ -113,6 +114,58 @@ class XClusterTargetManager {
 
   Status ClearXClusterSourceTableId(TableInfoPtr table_info, const LeaderEpoch& epoch);
 
+  void NotifyAutoFlagsConfigChanged();
+
+  Status ReportNewAutoFlagConfigVersion(
+      const xcluster::ReplicationGroupId& replication_group_id, uint32 new_version,
+      const LeaderEpoch& epoch);
+
+  Status GetReplicationStatus(
+      const GetReplicationStatusRequestPB* req, GetReplicationStatusResponsePB* resp) const
+      EXCLUDES(replication_error_map_mutex_);
+
+  // Sync replication_error_map_ with the streams we have in our producer_map.
+  void SyncReplicationStatusMap(
+      const xcluster::ReplicationGroupId& replication_group_id,
+      const google::protobuf::Map<std::string, cdc::ProducerEntryPB>& producer_map)
+      EXCLUDES(replication_error_map_mutex_);
+
+  void StoreReplicationStatus(
+      const XClusterConsumerReplicationStatusPB& consumer_replication_status)
+      EXCLUDES(replication_error_map_mutex_);
+
+  Result<bool> HasReplicationGroupErrors(const xcluster::ReplicationGroupId& replication_group_id)
+      const EXCLUDES(replication_error_map_mutex_);
+
+  void RecordTableStream(
+      const TableId& table_id, const xcluster::ReplicationGroupId& replication_group_id,
+      const xrepl::StreamId& stream_id) EXCLUDES(table_stream_ids_map_mutex_);
+
+  void RemoveTableStream(
+      const TableId& table_id, const xcluster::ReplicationGroupId& replication_group_id)
+      EXCLUDES(table_stream_ids_map_mutex_);
+
+  void RemoveTableStreams(
+      const xcluster::ReplicationGroupId& replication_group_id,
+      const std::set<TableId>& tables_to_clear) EXCLUDES(table_stream_ids_map_mutex_);
+
+  bool IsTableReplicated(const TableId& table_id) const;
+
+  Result<TableId> GetTableIdForStreamId(
+      const xcluster::ReplicationGroupId& replication_group_id,
+      const xrepl::StreamId& stream_id) const EXCLUDES(table_stream_ids_map_mutex_);
+
+  Status HandleTabletSplit(
+      const TableId& consumer_table_id, const SplitTabletIds& split_tablet_ids,
+      const LeaderEpoch& epoch) EXCLUDES(table_stream_ids_map_mutex_);
+
+  Status ValidateNewSchema(const TableInfo& table_info, const Schema& consumer_schema) const
+      EXCLUDES(table_stream_ids_map_mutex_);
+
+  Status ResumeStreamsAfterNewSchema(
+      const TableInfo& table_info, SchemaVersion consumer_schema_version, const LeaderEpoch& epoch)
+      EXCLUDES(table_stream_ids_map_mutex_);
+
  private:
   // Gets the replication group status for the given replication group id. Does not populate the
   // table statuses.
@@ -120,17 +173,44 @@ class XClusterTargetManager {
       const SysUniverseReplicationEntryPB& replication_info_pb,
       const SysClusterConfigEntryPB& cluster_config_pb) const;
 
+  Status RefreshLocalAutoFlagConfig(const LeaderEpoch& epoch);
+
+  // Populate the response with the errors for the given replication group.
+  Status PopulateReplicationGroupErrors(
+      const xcluster::ReplicationGroupId& replication_group_id,
+      GetReplicationStatusResponsePB* resp) const REQUIRES_SHARED(replication_error_map_mutex_)
+      EXCLUDES(table_stream_ids_map_mutex_);
+
+  std::unordered_map<xcluster::ReplicationGroupId, xrepl::StreamId> GetStreamIdsForTable(
+      const TableId& table_id) const EXCLUDES(table_stream_ids_map_mutex_);
+
+  Status ProcessPendingSchemaChanges(const LeaderEpoch& epoch);
+
   Master& master_;
   CatalogManager& catalog_manager_;
   SysCatalogTable& sys_catalog_;
 
-  std::unique_ptr<XClusterSafeTimeService> xcluster_safe_time_service_;
+  std::unique_ptr<XClusterSafeTimeService> safe_time_service_;
 
   bool removed_deleted_tables_from_replication_ = false;
 
   // The Catalog Entity is stored outside of XClusterSafeTimeService, since we may want to move the
   // service out of master at a later time.
-  XClusterSafeTimeInfo xcluster_safe_time_info_;
+  XClusterSafeTimeInfo safe_time_info_;
+
+  std::atomic<bool> auto_flags_revalidation_needed_{true};
+
+  // TODO: Wrap this and its associated function into its own class, and move it inside
+  // InboundReplicationGroup.
+  mutable std::shared_mutex replication_error_map_mutex_;
+  std::unordered_map<xcluster::ReplicationGroupId, xcluster::ReplicationGroupErrors>
+      replication_error_map_ GUARDED_BY(replication_error_map_mutex_);
+
+  mutable std::shared_mutex table_stream_ids_map_mutex_;
+
+  // Map of all consumer tables that are part of xcluster replication, to a map of the stream infos.
+  std::unordered_map<TableId, std::unordered_map<xcluster::ReplicationGroupId, xrepl::StreamId>>
+      table_stream_ids_map_ GUARDED_BY(table_stream_ids_map_mutex_);
 
   DISALLOW_COPY_AND_ASSIGN(XClusterTargetManager);
 };
diff --git a/src/yb/master/xrepl_catalog_manager.cc b/src/yb/master/xrepl_catalog_manager.cc
index 89e7fb0026..322caab049 100644
--- a/src/yb/master/xrepl_catalog_manager.cc
+++ b/src/yb/master/xrepl_catalog_manager.cc
@@ -172,7 +172,6 @@ DEFINE_RUNTIME_AUTO_bool(cdcsdk_enable_identification_of_non_eligible_tables,
 TAG_FLAG(cdcsdk_enable_identification_of_non_eligible_tables, advanced);
 TAG_FLAG(cdcsdk_enable_identification_of_non_eligible_tables, hidden);
 
-DECLARE_bool(xcluster_wait_on_ddl_alter);
 DECLARE_int32(master_rpc_timeout_ms);
 DECLARE_bool(ysql_yb_enable_replication_commands);
 DECLARE_bool(yb_enable_cdc_consistent_snapshot_streams);
@@ -377,8 +376,6 @@ class CDCStreamLoader : public Visitor<PersistentCDCStreamInfo> {
 };
 
 void CatalogManager::ClearXReplState() {
-  xcluster_auto_flags_revalidation_needed_ = true;
-
   // Clear CDC stream map.
   xrepl_maps_loaded_ = false;
   {
@@ -393,11 +390,6 @@ void CatalogManager::ClearXReplState() {
 
   // Clear universe replication map.
   universe_replication_map_.clear();
-  xcluster_consumer_table_stream_ids_map_.clear();
-  {
-    std::lock_guard l(xcluster_consumer_replication_error_map_mutex_);
-    xcluster_consumer_replication_error_map_.clear();
-  }
 }
 
 Status CatalogManager::LoadXReplStream() {
@@ -415,18 +407,6 @@ Status CatalogManager::LoadXReplStream() {
     RecordCDCSDKHiddenTablets({tablet}, delete_retainer);
   }
 
-  // Refresh the Consumer registry.
-  if (cluster_config_) {
-    auto l = cluster_config_->LockForRead();
-    if (l->pb.has_consumer_registry()) {
-      auto& producer_map = l->pb.consumer_registry().producer_map();
-      for (const auto& [replication_group_id, _] : producer_map) {
-        SyncXClusterConsumerReplicationStatusMap(
-            xcluster::ReplicationGroupId(replication_group_id), producer_map);
-      }
-    }
-  }
-
   return Status::OK();
 }
 
@@ -501,8 +481,9 @@ class UniverseReplicationLoader : public Visitor<PersistentUniverseReplicationIn
         LOG(WARNING) << "Unable to find stream id for table: " << table.first;
         continue;
       }
-      catalog_manager_->xcluster_consumer_table_stream_ids_map_[table.second].emplace(
-          metadata.replication_group_id(), VERIFY_RESULT(xrepl::StreamId::FromString(stream_id)));
+      catalog_manager_->GetXClusterManagerImpl()->RecordTableConsumerStream(
+          table.second, ri->ReplicationGroupId(),
+          VERIFY_RESULT(xrepl::StreamId::FromString(stream_id)));
     }
 
     LOG(INFO) << "Loaded metadata for universe replication " << ri->ToString();
@@ -2879,7 +2860,8 @@ Status CatalogManager::IsObjectPartOfXRepl(
   SCHECK(table_info, NotFound, "Table with id $0 does not exist", req->table_id());
   SharedLock lock(mutex_);
   resp->set_is_object_part_of_xrepl(
-      IsTablePartOfXClusterUnlocked(table_info->id()) || IsTablePartOfCDCSDK(table_info->id()));
+      xcluster_manager_->IsTableReplicated(table_info->id()) ||
+      IsTablePartOfCDCSDK(table_info->id()));
   return Status::OK();
 }
 
@@ -4163,7 +4145,7 @@ Status CatalogManager::GetTablegroupSchemaCallbackInternal(
   {
     SharedLock lock(mutex_);
     SCHECK(
-        !xcluster_consumer_table_stream_ids_map_.contains(consumer_parent_table_id), IllegalState,
+        !xcluster_manager_->IsTableReplicationConsumer(consumer_parent_table_id), IllegalState,
         "N:1 replication topology not supported");
   }
 
@@ -4258,7 +4240,7 @@ void CatalogManager::GetColocatedTabletSchemaCallback(
 
   {
     SharedLock lock(mutex_);
-    if (xcluster_consumer_table_stream_ids_map_.contains(*consumer_parent_table_ids.begin())) {
+    if (xcluster_manager_->IsTableReplicationConsumer(*consumer_parent_table_ids.begin())) {
       std::string message = "N:1 replication topology not supported";
       MarkUniverseReplicationFailed(universe, STATUS(IllegalState, message));
       LOG(ERROR) << message;
@@ -4485,65 +4467,13 @@ Status CatalogManager::AddCDCStreamToUniverseAndInitConsumerInternal(
     RETURN_NOT_OK(MergeUniverseReplication(universe, final_id, std::move(on_success_cb)));
   }
   // Update the in-memory cache of consumer tables.
-  LockGuard lock(mutex_);
   for (const auto& info : consumer_info) {
-    const auto& c_table_id = info.consumer_table_id;
-    const auto& c_stream_id = info.stream_id;
-    xcluster_consumer_table_stream_ids_map_[c_table_id].emplace(final_id, c_stream_id);
+    xcluster_manager_->RecordTableConsumerStream(info.consumer_table_id, final_id, info.stream_id);
   }
 
   return Status::OK();
 }
 
-/*
- * UpdateXClusterConsumerOnTabletSplit updates the consumer -> producer tablet mapping after a local
- * tablet split.
- */
-Status CatalogManager::UpdateXClusterConsumerOnTabletSplit(
-    const TableId& consumer_table_id, const SplitTabletIds& split_tablet_ids) {
-  // Check if this table is consuming a stream.
-  auto stream_ids = GetXClusterConsumerStreamIdsForTable(consumer_table_id);
-  if (stream_ids.empty()) {
-    return Status::OK();
-  }
-
-  auto consumer_tablet_keys = VERIFY_RESULT(GetTableKeyRanges(consumer_table_id));
-  auto cluster_config = ClusterConfig();
-  auto l = cluster_config->LockForWrite();
-  for (const auto& [replication_group_id, stream_id] : stream_ids) {
-    // Fetch the stream entry so we can update the mappings.
-    auto replication_group_map =
-        l.mutable_data()->pb.mutable_consumer_registry()->mutable_producer_map();
-    auto producer_entry = FindOrNull(*replication_group_map, replication_group_id.ToString());
-    // If we can't find the entries, then the stream has been deleted.
-    if (!producer_entry) {
-      LOG(WARNING) << "Unable to find the producer entry for universe " << replication_group_id;
-      continue;
-    }
-    auto stream_entry = FindOrNull(*producer_entry->mutable_stream_map(), stream_id.ToString());
-    if (!stream_entry) {
-      LOG(WARNING) << "Unable to find the producer entry for universe " << replication_group_id
-                   << ", stream " << stream_id;
-      continue;
-    }
-    DCHECK(stream_entry->consumer_table_id() == consumer_table_id);
-
-    RETURN_NOT_OK(
-        UpdateTabletMappingOnConsumerSplit(consumer_tablet_keys, split_tablet_ids, stream_entry));
-  }
-
-  // Also bump the cluster_config_ version so that changes are propagated to tservers.
-  l.mutable_data()->pb.set_version(l.mutable_data()->pb.version() + 1);
-
-  RETURN_NOT_OK(CheckStatus(
-      sys_catalog_->Upsert(leader_ready_term(), cluster_config.get()),
-      "Updating cluster config in sys-catalog"));
-  l.Commit();
-
-  xcluster_manager_->CreateXClusterSafeTimeTableAndStartService();
-
-  return Status::OK();
-}
 
 Status CatalogManager::UpdateCDCProducerOnTabletSplit(
     const TableId& producer_table_id, const SplitTabletIds& split_tablet_ids) {
@@ -4732,7 +4662,7 @@ Status CatalogManager::InitXClusterConsumer(
       sys_catalog_->Upsert(leader_ready_term(), cluster_config.get()),
       "updating cluster config in sys-catalog"));
 
-  SyncXClusterConsumerReplicationStatusMap(
+  xcluster_manager_->SyncConsumerReplicationStatusMap(
       replication_info.ReplicationGroupId(), *replication_group_map);
   l.Commit();
 
@@ -4832,8 +4762,9 @@ Status CatalogManager::MergeUniverseReplication(
           "Updating universe replication entries and cluster config in sys-catalog");
     }
 
-    SyncXClusterConsumerReplicationStatusMap(original_universe->ReplicationGroupId(), *pm);
-    SyncXClusterConsumerReplicationStatusMap(universe->ReplicationGroupId(), *pm);
+    xcluster_manager_->SyncConsumerReplicationStatusMap(
+        original_universe->ReplicationGroupId(), *pm);
+    xcluster_manager_->SyncConsumerReplicationStatusMap(universe->ReplicationGroupId(), *pm);
 
     alter_lock.Commit();
     cl.Commit();
@@ -4900,7 +4831,8 @@ Status CatalogManager::DeleteUniverseReplication(
           sys_catalog_->Upsert(leader_ready_term(), cluster_config.get()),
           "updating cluster config in sys-catalog"));
 
-      SyncXClusterConsumerReplicationStatusMap(replication_group_id, *replication_group_map);
+      xcluster_manager_->SyncConsumerReplicationStatusMap(
+          replication_group_id, *replication_group_map);
       cl.Commit();
     }
   }
@@ -5010,14 +4942,7 @@ Status CatalogManager::DeleteUniverseReplicationUnlocked(
 
   // Also update the mapping of consumer tables.
   for (const auto& table : universe->metadata().state().pb.validated_tables()) {
-    if (xcluster_consumer_table_stream_ids_map_[table.second].erase(
-            universe->ReplicationGroupId()) < 1) {
-      LOG(WARNING) << "Failed to remove consumer table from mapping. "
-                   << "table_id: " << table.second << ": replication_group_id: " << universe->id();
-    }
-    if (xcluster_consumer_table_stream_ids_map_[table.second].empty()) {
-      xcluster_consumer_table_stream_ids_map_.erase(table.second);
-    }
+    xcluster_manager_->RemoveTableConsumerStream(table.second, universe->ReplicationGroupId());
   }
   return Status::OK();
 }
@@ -5692,7 +5617,7 @@ Status CatalogManager::UpdateConsumerOnProducerSplit(
       sys_catalog_->Upsert(leader_ready_term(), cluster_config.get()),
       "Updating cluster config in sys-catalog"));
 
-  SyncXClusterConsumerReplicationStatusMap(
+  xcluster_manager_->SyncConsumerReplicationStatusMap(
       xcluster::ReplicationGroupId(req->replication_group_id()), *replication_group_map);
   l.Commit();
 
@@ -5717,20 +5642,12 @@ Status CatalogManager::UpdateConsumerOnProducerMetadata(
   const auto stream_id = VERIFY_RESULT(xrepl::StreamId::FromString(req->stream_id()));
 
   // Get corresponding local data for this stream.
-  TableId consumer_table_id;
+  TableId consumer_table_id = VERIFY_RESULT(
+      xcluster_manager_->GetConsumerTableIdForStreamId(replication_group_id, stream_id));
+
   scoped_refptr<TableInfo> table;
   {
     SharedLock lock(mutex_);
-    auto iter = std::find_if(
-        xcluster_consumer_table_stream_ids_map_.begin(),
-        xcluster_consumer_table_stream_ids_map_.end(),
-        [&replication_group_id, &stream_id](auto& id_map) {
-          return ContainsKeyValuePair(id_map.second, replication_group_id, stream_id);
-        });
-    SCHECK(
-        iter != xcluster_consumer_table_stream_ids_map_.end(), NotFound,
-        Format("Unable to find the stream id $0", stream_id));
-    consumer_table_id = iter->first;
 
     // The destination table should be found or created by now.
     table = tables_->FindTableOrNull(consumer_table_id);
@@ -5990,146 +5907,6 @@ Status CatalogManager::WaitForReplicationDrain(
   return Status::OK();
 }
 
-// Sync xcluster_consumer_replication_error_map_ with the streams we have in our producer_map.
-void CatalogManager::SyncXClusterConsumerReplicationStatusMap(
-    const xcluster::ReplicationGroupId& replication_group_id,
-    const google::protobuf::Map<std::string, cdc::ProducerEntryPB>& producer_map) {
-  std::lock_guard lock(xcluster_consumer_replication_error_map_mutex_);
-
-  if (producer_map.count(replication_group_id.ToString()) == 0) {
-    // Replication group has been deleted.
-    xcluster_consumer_replication_error_map_.erase(replication_group_id);
-    return;
-  }
-
-  auto& replication_group_errors = xcluster_consumer_replication_error_map_[replication_group_id];
-  auto& producer_entry = producer_map.at(replication_group_id.ToString());
-
-  std::unordered_set<TableId> all_consumer_table_ids;
-  for (auto& [_, stream_map] : producer_entry.stream_map()) {
-    const auto& consumer_table_id = stream_map.consumer_table_id();
-
-    std::unordered_set<TabletId> all_producer_tablet_ids;
-    for (auto& [_, producer_tablet_ids] : stream_map.consumer_producer_tablet_map()) {
-      all_producer_tablet_ids.insert(
-          producer_tablet_ids.tablets().begin(), producer_tablet_ids.tablets().end());
-    }
-
-    if (all_producer_tablet_ids.empty()) {
-      continue;
-    }
-    all_consumer_table_ids.insert(consumer_table_id);
-
-    auto& tablet_error_map = replication_group_errors[consumer_table_id];
-    // Remove tablets that are no longer part of replication.
-    std::erase_if(tablet_error_map, [&all_producer_tablet_ids](const auto& entry) {
-      return !all_producer_tablet_ids.contains(entry.first);
-    });
-
-    // Add new tablets.
-    for (const auto& producer_tablet_id : all_producer_tablet_ids) {
-      if (!tablet_error_map.contains(producer_tablet_id)) {
-        // Default to UNINITIALIZED error. Once the Pollers send the status, this will be updated.
-        tablet_error_map[producer_tablet_id].error =
-            ReplicationErrorPb::REPLICATION_ERROR_UNINITIALIZED;
-      }
-    }
-  }
-
-  // Remove tables that are no longer part of replication.
-  std::erase_if(replication_group_errors, [&all_consumer_table_ids](const auto& entry) {
-    return !all_consumer_table_ids.contains(entry.first);
-  });
-
-  if (VLOG_IS_ON(1)) {
-    LOG(INFO) << "Synced xcluster_consumer_replication_error_map_ for replication group "
-              << replication_group_id;
-    for (const auto& [consumer_table_id, tablet_error_map] : replication_group_errors) {
-      LOG(INFO) << "Table: " << consumer_table_id;
-      for (const auto& [tablet_id, error] : tablet_error_map) {
-        LOG(INFO) << "Tablet: " << tablet_id << ", Error: " << ReplicationErrorPb_Name(error.error);
-      }
-    }
-  }
-}
-
-void CatalogManager::StoreXClusterConsumerReplicationStatus(
-    const XClusterConsumerReplicationStatusPB& consumer_replication_status) {
-  const auto& replication_group_id = consumer_replication_status.replication_group_id();
-
-  std::lock_guard lock(xcluster_consumer_replication_error_map_mutex_);
-  // Heartbeats can report stale entries. So we skip anything that is not in
-  // xcluster_consumer_replication_error_map_.
-
-  auto* replication_error_map = FindOrNull(
-      xcluster_consumer_replication_error_map_, xcluster::ReplicationGroupId(replication_group_id));
-  if (!replication_error_map) {
-    VLOG_WITH_FUNC(2) << "Skipping deleted replication group " << replication_group_id;
-    return;
-  }
-
-  for (const auto& table_status : consumer_replication_status.table_status()) {
-    const auto& consumer_table_id = table_status.consumer_table_id();
-    auto* consumer_table_map = FindOrNull(*replication_error_map, consumer_table_id);
-    if (!consumer_table_map) {
-      VLOG_WITH_FUNC(2) << "Skipping removed table " << consumer_table_id
-                        << " in replication group " << replication_group_id;
-      continue;
-    }
-
-    for (const auto& stream_tablet_status : table_status.stream_tablet_status()) {
-      const auto& producer_tablet_id = stream_tablet_status.producer_tablet_id();
-      auto* tablet_status_map = FindOrNull(*consumer_table_map, producer_tablet_id);
-      if (!tablet_status_map) {
-        VLOG_WITH_FUNC(2) << "Skipping removed tablet " << producer_tablet_id
-                          << " in replication group " << replication_group_id;
-        continue;
-      }
-
-      // Get status from highest term only. When consumer leaders move we may get stale status
-      // from older leaders.
-      if (tablet_status_map->consumer_term <= stream_tablet_status.consumer_term()) {
-        DCHECK_NE(
-            stream_tablet_status.error(), ReplicationErrorPb::REPLICATION_ERROR_UNINITIALIZED);
-        tablet_status_map->consumer_term = stream_tablet_status.consumer_term();
-        tablet_status_map->error = stream_tablet_status.error();
-        VLOG_WITH_FUNC(2) << "Storing error for replication group: " << replication_group_id
-                          << ", consumer table: " << consumer_table_id
-                          << ", tablet: " << producer_tablet_id
-                          << ", term: " << stream_tablet_status.consumer_term()
-                          << ", error: " << ReplicationErrorPb_Name(stream_tablet_status.error());
-      } else {
-        VLOG_WITH_FUNC(2) << "Skipping stale error for  replication group: " << replication_group_id
-                          << ", consumer table: " << consumer_table_id
-                          << ", tablet: " << producer_tablet_id
-                          << ", term: " << stream_tablet_status.consumer_term();
-      }
-    }
-  }
-}
-
-Status CatalogManager::GetReplicationStatus(
-    const GetReplicationStatusRequestPB* req, GetReplicationStatusResponsePB* resp,
-    rpc::RpcContext* rpc) {
-  LOG(INFO) << "GetReplicationStatus from " << RequestorString(rpc) << ": " << req->DebugString();
-
-  SharedLock lock(mutex_);
-  yb::SharedLock l(xcluster_consumer_replication_error_map_mutex_);
-
-  // If the 'replication_group_id' is given, only populate the status for the streams in that
-  // ReplicationGroup. Otherwise, populate all the status for all groups.
-  if (!req->replication_group_id().empty()) {
-    return PopulateReplicationGroupErrors(
-        xcluster::ReplicationGroupId(req->replication_group_id()), resp);
-  }
-
-  for (const auto& [replication_id, _] : xcluster_consumer_replication_error_map_) {
-    RETURN_NOT_OK(PopulateReplicationGroupErrors(replication_id, resp));
-  }
-
-  return Status::OK();
-}
-
 PgReplicaIdentity GetReplicaIdentityFromRecordType(std::string record_type_name) {
   cdc::CDCRecordType record_type = cdc::CDCRecordType::CHANGE;
   CDCRecordType_Parse(record_type_name, &record_type);
@@ -6508,106 +6285,9 @@ Status CatalogManager::TEST_CDCSDKFailCreateStreamRequestIfNeeded(const std::str
   }
   return Status::OK();
 }
-Result<bool> CatalogManager::HasReplicationGroupErrors(
-    const xcluster::ReplicationGroupId& replication_group_id) const {
-  yb::SharedLock l(xcluster_consumer_replication_error_map_mutex_);
-
-  auto* replication_error_map =
-      FindOrNull(xcluster_consumer_replication_error_map_, replication_group_id);
-  SCHECK(
-      replication_error_map, NotFound, "Could not find replication group $0",
-      replication_group_id.ToString());
-
-  std::unordered_map<TableId, std::unordered_set<std::string>> table_errors;
-  for (const auto& [consumer_table_id, tablet_error_map] : *replication_error_map) {
-    for (const auto& [_, error_info] : tablet_error_map) {
-      if (error_info.error != ReplicationErrorPb::REPLICATION_OK) {
-        table_errors[consumer_table_id].insert(ReplicationErrorPb_Name(error_info.error));
-      }
-    }
-  }
-
-  if (!table_errors.empty()) {
-    YB_LOG_EVERY_N_SECS(WARNING, 60)
-        << "Replication group " << replication_group_id
-        << " has errors for the following tables:" << AsString(table_errors);
-  }
-  return !table_errors.empty();
-}
-
-Status CatalogManager::PopulateReplicationGroupErrors(
-    const xcluster::ReplicationGroupId& replication_group_id,
-    GetReplicationStatusResponsePB* resp) const {
-  auto* replication_error_map =
-      FindOrNull(xcluster_consumer_replication_error_map_, replication_group_id);
-  SCHECK(
-      replication_error_map, NotFound, "Could not find replication group $0",
-      replication_group_id.ToString());
-
-  for (const auto& [consumer_table_id, tablet_error_map] : *replication_error_map) {
-    if (!xcluster_consumer_table_stream_ids_map_.contains(consumer_table_id) ||
-        !xcluster_consumer_table_stream_ids_map_.at(consumer_table_id)
-             .contains(replication_group_id)) {
-      // This is not expected. The two maps should be kept in sync.
-      LOG(DFATAL) << "xcluster_consumer_replication_error_map_ contains consumer table "
-                  << consumer_table_id << " in replication group " << replication_group_id
-                  << " but xcluster_consumer_table_stream_ids_map_ does not.";
-      continue;
-    }
-
-    // Map from error to list of producer tablet IDs/Pollers reporting them.
-    std::unordered_map<ReplicationErrorPb, std::vector<TabletId>> errors;
-    for (const auto& [tablet_id, error_info] : tablet_error_map) {
-      errors[error_info.error].push_back(tablet_id);
-    }
-
-    if (errors.empty()) {
-      continue;
-    }
-
-    auto resp_status = resp->add_statuses();
-    resp_status->set_table_id(consumer_table_id);
-    const auto& stream_id =
-        xcluster_consumer_table_stream_ids_map_.at(consumer_table_id).at(replication_group_id);
-    resp_status->set_stream_id(stream_id.ToString());
-    for (const auto& [error_pb, tablet_ids] : errors) {
-      if (error_pb == ReplicationErrorPb::REPLICATION_OK) {
-        // Do not add errors for healthy tablets.
-        continue;
-      }
-
-      auto* resp_error = resp_status->add_errors();
-      resp_error->set_error(error_pb);
-      // Only include the first 20 tablet IDs to limit response size. VLOG(4) will log all tablet to
-      // the log.
-      resp_error->set_error_detail(
-          Format("Producer Tablet IDs: $0", JoinStringsLimitCount(tablet_ids, ",", 20)));
-      if (VLOG_IS_ON(4)) {
-        VLOG(4) << "Replication error " << ReplicationErrorPb_Name(error_pb)
-                << " for ReplicationGroup: " << replication_group_id << ", stream id: " << stream_id
-                << ", consumer table: " << consumer_table_id << ", producer tablet IDs:";
-        for (const auto& tablet_id : tablet_ids) {
-          VLOG(4) << tablet_id;
-        }
-      }
-    }
-  }
-
-  return Status::OK();
-}
 
 bool CatalogManager::IsTablePartOfXRepl(const TableId& table_id) const {
-  return IsTablePartOfXClusterUnlocked(table_id) || IsTablePartOfCDCSDK(table_id);
-}
-
-bool CatalogManager::IsTableXClusterConsumer(const TableId& table_id) const {
-  SharedLock lock(mutex_);
-  return IsTableXClusterConsumerUnlocked(table_id);
-}
-
-bool CatalogManager::IsTableXClusterConsumerUnlocked(const TableId& table_id) const {
-  auto* stream_ids = FindOrNull(xcluster_consumer_table_stream_ids_map_, table_id);
-  return stream_ids && !stream_ids->empty();
+  return xcluster_manager_->IsTableReplicated(table_id) || IsTablePartOfCDCSDK(table_id);
 }
 
 bool CatalogManager::IsTablePartOfCDCSDK(const TableId& table_id) const {
@@ -6641,122 +6321,6 @@ std::unordered_set<xrepl::StreamId> CatalogManager::GetCDCSDKStreamsForTable(
   return *table_ids;
 }
 
-bool CatalogManager::IsTablePartOfXCluster(const TableId& table_id) const {
-  SharedLock lock(mutex_);
-  return IsTablePartOfXClusterUnlocked(table_id);
-}
-
-bool CatalogManager::IsTablePartOfXClusterUnlocked(const TableId& table_id) const {
-  return xcluster_manager_->IsTableReplicated(table_id) ||
-         IsTableXClusterConsumerUnlocked(table_id);
-}
-
-Status CatalogManager::ValidateNewSchemaWithCdc(
-    const TableInfo& table_info, const Schema& consumer_schema) const {
-  // Check if this table is consuming a stream.
-  auto stream_ids = GetXClusterConsumerStreamIdsForTable(table_info.id());
-  if (stream_ids.empty()) {
-    return Status::OK();
-  }
-
-  auto cluster_config = ClusterConfig();
-  auto l = cluster_config->LockForRead();
-  for (const auto& [replication_group_id, stream_id] : stream_ids) {
-    // Fetch the stream entry to get Schema information.
-    auto& replication_group_map = l.data().pb.consumer_registry().producer_map();
-    auto producer_entry = FindOrNull(replication_group_map, replication_group_id.ToString());
-    SCHECK(producer_entry, NotFound, Format("Missing universe $0", replication_group_id));
-    auto stream_entry = FindOrNull(producer_entry->stream_map(), stream_id.ToString());
-    SCHECK(stream_entry, NotFound, Format("Missing stream $0:$1", replication_group_id, stream_id));
-
-    // If we are halted on a Schema update as a Consumer...
-    auto& producer_schema_pb = stream_entry->producer_schema();
-    if (producer_schema_pb.has_pending_schema()) {
-      // Compare our new schema to the Producer's pending schema.
-      Schema producer_schema;
-      RETURN_NOT_OK(SchemaFromPB(producer_schema_pb.pending_schema(), &producer_schema));
-
-      // This new schema should allow us to consume data for the Producer's next schema.
-      // If we instead diverge, we will be unable to consume any more of the Producer's data.
-      bool can_apply = consumer_schema.EquivalentForDataCopy(producer_schema);
-      SCHECK(
-          can_apply, IllegalState,
-          Format(
-              "New Schema not compatible with XCluster Producer Schema:\n new={$0}\n producer={$1}",
-              consumer_schema.ToString(), producer_schema.ToString()));
-    }
-  }
-
-  return Status::OK();
-}
-
-Status CatalogManager::ResumeXClusterConsumerAfterNewSchema(
-    const TableInfo& table_info, SchemaVersion consumer_schema_version) {
-  if (PREDICT_FALSE(!GetAtomicFlag(&FLAGS_xcluster_wait_on_ddl_alter))) {
-    return Status::OK();
-  }
-
-  // Verify that this table is consuming a stream.
-  auto stream_ids = GetXClusterConsumerStreamIdsForTable(table_info.id());
-  if (stream_ids.empty()) {
-    return Status::OK();
-  }
-
-  bool found_schema = false, resuming_replication = false;
-
-  // Now that we've applied the new schema: find pending replication, clear state, resume.
-  auto cluster_config = ClusterConfig();
-  auto l = cluster_config->LockForWrite();
-  for (const auto& [replication_group_id, stream_id] : stream_ids) {
-    // Fetch the stream entry to get Schema information.
-    auto replication_group_map =
-        l.mutable_data()->pb.mutable_consumer_registry()->mutable_producer_map();
-    auto producer_entry = FindOrNull(*replication_group_map, replication_group_id.ToString());
-    if (!producer_entry) {
-      continue;
-    }
-    auto stream_entry = FindOrNull(*producer_entry->mutable_stream_map(), stream_id.ToString());
-    if (!stream_entry) {
-      continue;
-    }
-
-    auto producer_schema_pb = stream_entry->mutable_producer_schema();
-    if (producer_schema_pb->has_pending_schema()) {
-      found_schema = true;
-      Schema consumer_schema, producer_schema;
-      RETURN_NOT_OK(table_info.GetSchema(&consumer_schema));
-      RETURN_NOT_OK(SchemaFromPB(producer_schema_pb->pending_schema(), &producer_schema));
-      if (consumer_schema.EquivalentForDataCopy(producer_schema)) {
-        resuming_replication = true;
-        auto pending_version = producer_schema_pb->pending_schema_version();
-        LOG(INFO) << "Consumer schema @ version " << consumer_schema_version
-                  << " is now data copy compatible with Producer: " << stream_id
-                  << " @ schema version " << pending_version;
-        // Clear meta we use to track progress on receiving all WAL entries with old schema.
-        producer_schema_pb->set_validated_schema_version(
-            std::max(producer_schema_pb->validated_schema_version(), pending_version));
-        producer_schema_pb->set_last_compatible_consumer_schema_version(consumer_schema_version);
-        producer_schema_pb->clear_pending_schema();
-        // Bump the ClusterConfig version so we'll broadcast new schema version & resume operation.
-        l.mutable_data()->pb.set_version(l.mutable_data()->pb.version() + 1);
-      } else {
-        LOG(INFO) << "Consumer schema not compatible for data copy of next Producer schema.";
-      }
-    }
-  }
-
-  if (resuming_replication) {
-    RETURN_NOT_OK(CheckStatus(
-        sys_catalog_->Upsert(leader_ready_term(), cluster_config.get()),
-        "updating cluster config after Schema for CDC"));
-    l.Commit();
-    LOG(INFO) << "Resuming Replication on " << table_info.id() << " after Consumer ALTER.";
-  } else if (!found_schema) {
-    LOG(INFO) << "No pending schema change from Producer.";
-  }
-
-  return Status::OK();
-}
 
 void CatalogManager::RunXReplBgTasks(const LeaderEpoch& epoch) {
   WARN_NOT_OK(CleanUpDeletedXReplStreams(epoch), "Failed Cleaning Deleted XRepl Streams");
@@ -6772,56 +6336,8 @@ void CatalogManager::RunXReplBgTasks(const LeaderEpoch& epoch) {
 
   // Restart xCluster and CDCSDK parent tablet deletion bg task.
   StartXReplParentTabletDeletionTaskIfStopped();
-
-  WARN_NOT_OK(
-      XClusterProcessPendingSchemaChanges(epoch),
-      "Failed processing xCluster Pending Schema Changes");
-
-  WARN_NOT_OK(
-      XClusterRefreshLocalAutoFlagConfig(epoch), "Failed refreshing local AutoFlags config");
 }
 
-Status CatalogManager::XClusterProcessPendingSchemaChanges(const LeaderEpoch& epoch) {
-  if (PREDICT_FALSE(!GetAtomicFlag(&FLAGS_xcluster_wait_on_ddl_alter))) {
-    // See if any Streams are waiting on a pending_schema.
-    bool found_pending_schema = false;
-    auto cluster_config = ClusterConfig();
-    auto cl = cluster_config->LockForWrite();
-    auto replication_group_map =
-        cl.mutable_data()->pb.mutable_consumer_registry()->mutable_producer_map();
-    // For each user entry.
-    for (auto& replication_group_id_and_entry : *replication_group_map) {
-      // For each CDC stream in that Universe.
-      for (auto& stream_id_and_entry :
-           *replication_group_id_and_entry.second.mutable_stream_map()) {
-        auto& stream_entry = stream_id_and_entry.second;
-        if (stream_entry.has_producer_schema() &&
-            stream_entry.producer_schema().has_pending_schema()) {
-          // Force resume this stream.
-          auto schema = stream_entry.mutable_producer_schema();
-          schema->set_validated_schema_version(
-              std::max(schema->validated_schema_version(), schema->pending_schema_version()));
-          schema->clear_pending_schema();
-
-          found_pending_schema = true;
-          LOG(INFO) << "Force Resume Consumer schema: " << stream_id_and_entry.first
-                    << " @ schema version " << schema->pending_schema_version();
-        }
-      }
-    }
-
-    if (found_pending_schema) {
-      // Bump the ClusterConfig version so we'll broadcast new schema version & resume operation.
-      cl.mutable_data()->pb.set_version(cl.mutable_data()->pb.version() + 1);
-      RETURN_NOT_OK(CheckStatus(
-          sys_catalog_->Upsert(epoch.leader_term, cluster_config.get()),
-          "updating cluster config after Schema for CDC"));
-      cl.Commit();
-    }
-  }
-
-  return Status::OK();
-}
 
 Status CatalogManager::ClearFailedUniverse() {
   // Delete a single failed universe from universes_to_clear_.
@@ -7044,7 +6560,7 @@ void CatalogManager::ScheduleXReplParentTabletDeletionTask() {
   }
 
   // Submit to run async in diff thread pool, since this involves accessing cdc_state.
-  cdc_parent_tablet_deletion_task_.Schedule(
+  xrepl_parent_tablet_deletion_task_.Schedule(
       [this](const Status& status) {
         Status s = background_tasks_thread_pool_->SubmitFunc(
             std::bind(&CatalogManager::ProcessXReplParentTabletDeletionPeriodically, this));
@@ -7228,38 +6744,6 @@ Status CatalogManager::FillHeartbeatResponseCDC(
   return Status::OK();
 }
 
-std::unordered_map<TableId, CatalogManager::XClusterConsumerTableStreamIds>
-CatalogManager::GetXClusterConsumerTableStreams() const {
-  SharedLock lock(mutex_);
-  return xcluster_consumer_table_stream_ids_map_;
-}
-
-CatalogManager::XClusterConsumerTableStreamIds CatalogManager::GetXClusterConsumerStreamIdsForTable(
-    const TableId& table_id) const {
-  SharedLock lock(mutex_);
-  auto* stream_ids = FindOrNull(xcluster_consumer_table_stream_ids_map_, table_id);
-  if (!stream_ids) {
-    return {};
-  }
-
-  return *stream_ids;
-}
-
-void CatalogManager::ClearXClusterConsumerTableStreams(
-    const xcluster::ReplicationGroupId& replication_group_id,
-    const std::set<TableId>& tables_to_clear) {
-  LockGuard lock(mutex_);
-  for (const auto& table_id : tables_to_clear) {
-    if (xcluster_consumer_table_stream_ids_map_[table_id].erase(replication_group_id) < 1) {
-      LOG(WARNING) << "Failed to remove consumer table from mapping. " << "table_id: " << table_id
-                   << ": replication_group_id: " << replication_group_id;
-    }
-    if (xcluster_consumer_table_stream_ids_map_[table_id].empty()) {
-      xcluster_consumer_table_stream_ids_map_.erase(table_id);
-    }
-  }
-}
-
 bool CatalogManager::CDCSDKShouldRetainHiddenTablet(const TabletId& tablet_id) {
   SharedLock read_lock(mutex_);
   return retained_by_cdcsdk_.contains(tablet_id);
@@ -7390,12 +6874,9 @@ Status CatalogManager::ValidateTableSchemaForXCluster(
             info.table_id, resp->identifier().table_id(), source_clc_id, target_clc_id));
   }
 
-  {
-    SharedLock lock(mutex_);
-    if (xcluster_consumer_table_stream_ids_map_.contains(table->table_id())) {
-      return STATUS(IllegalState, "N:1 replication topology not supported");
-    }
-  }
+  SCHECK(
+      !xcluster_manager_->IsTableReplicationConsumer(table->table_id()), IllegalState,
+      "N:1 replication topology not supported");
 
   return Status::OK();
 }
@@ -7410,87 +6891,6 @@ std::unordered_set<xrepl::StreamId> CatalogManager::GetAllXReplStreamIds() const
   return result;
 }
 
-Status CatalogManager::XClusterReportNewAutoFlagConfigVersion(
-    const XClusterReportNewAutoFlagConfigVersionRequestPB* req,
-    XClusterReportNewAutoFlagConfigVersionResponsePB* resp, rpc::RpcContext* rpc,
-    const LeaderEpoch& epoch) {
-  LOG_WITH_FUNC(INFO) << " from " << RequestorString(rpc) << ": " << req->DebugString();
-
-  const xcluster::ReplicationGroupId replication_group_id(req->replication_group_id());
-  const auto new_version = req->auto_flag_config_version();
-
-  // Verify that there is an existing Universe config
-  scoped_refptr<UniverseReplicationInfo> replication_info;
-  {
-    SharedLock lock(mutex_);
-    replication_info = FindPtrOrNull(universe_replication_map_, replication_group_id);
-    SCHECK(
-        replication_info, NotFound, "Missing replication group $0",
-        replication_group_id.ToString());
-  }
-
-  auto cluster_config = ClusterConfig();
-
-  return RefreshAutoFlagConfigVersion(
-      *sys_catalog_, *replication_info, *cluster_config.get(), new_version,
-      [master = master_]() { return master->GetAutoFlagsConfig(); }, epoch);
-}
-
-void CatalogManager::NotifyAutoFlagsConfigChanged() {
-  xcluster_auto_flags_revalidation_needed_ = true;
-}
-
-Status CatalogManager::XClusterRefreshLocalAutoFlagConfig(const LeaderEpoch& epoch) {
-  if (!xcluster_auto_flags_revalidation_needed_) {
-    return Status::OK();
-  }
-
-  auto se = ScopeExit([this] { xcluster_auto_flags_revalidation_needed_ = true; });
-  xcluster_auto_flags_revalidation_needed_ = false;
-
-  std::vector<xcluster::ReplicationGroupId> replication_group_ids;
-  bool update_failed = false;
-  {
-    SharedLock lock(mutex_);
-    for (const auto& [replication_group_id, _] : universe_replication_map_) {
-      replication_group_ids.push_back(replication_group_id);
-    }
-  }
-
-  if (replication_group_ids.empty()) {
-    return Status::OK();
-  }
-
-  const auto local_auto_flags_config = master_->GetAutoFlagsConfig();
-  auto cluster_config = ClusterConfig();
-
-  for (const auto& replication_group_id : replication_group_ids) {
-    scoped_refptr<UniverseReplicationInfo> replication_info;
-    {
-      SharedLock lock(mutex_);
-      replication_info = FindPtrOrNull(universe_replication_map_, replication_group_id);
-    }
-    if (!replication_info) {
-      // Replication group was deleted before we could process it.
-      continue;
-    }
-
-    auto status = HandleLocalAutoFlagsConfigChange(
-        *sys_catalog_, *replication_info, *cluster_config.get(), local_auto_flags_config, epoch);
-    if (!status.ok()) {
-      LOG(WARNING) << "Failed to handle local AutoFlags config change for replication group "
-                   << replication_group_id << ": " << status;
-      update_failed = true;
-    }
-  }
-
-  SCHECK(!update_failed, IllegalState, "Failed to handle local AutoFlags config change");
-
-  se.Cancel();
-
-  return Status::OK();
-}
-
 scoped_refptr<UniverseReplicationInfo> CatalogManager::GetUniverseReplication(
     const xcluster::ReplicationGroupId& replication_group_id) {
   SharedLock lock(mutex_);
