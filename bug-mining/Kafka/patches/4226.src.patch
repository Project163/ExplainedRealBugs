diff --git a/clients/src/main/java/org/apache/kafka/common/requests/LeaderAndIsrRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/LeaderAndIsrRequest.java
deleted file mode 100644
index 90c0add47a..0000000000
--- a/clients/src/main/java/org/apache/kafka/common/requests/LeaderAndIsrRequest.java
+++ /dev/null
@@ -1,403 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License. You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.kafka.common.requests;
-
-import org.apache.kafka.common.Node;
-import org.apache.kafka.common.Uuid;
-import org.apache.kafka.common.protocol.Errors;
-import org.apache.kafka.common.protocol.MessageUtil;
-import org.apache.kafka.common.utils.FlattenedIterator;
-
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.LinkedHashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Objects;
-import java.util.stream.Collectors;
-
-public final class LeaderAndIsrRequest {
-
-    /**
-     * Indicates if a controller request is incremental, full, or unknown.
-     */
-    public enum Type {
-        UNKNOWN(0),
-        INCREMENTAL(1),
-        FULL(2);
-
-        private final byte type;
-        Type(int type) {
-            this.type = (byte) type;
-        }
-
-        public byte toByte() {
-            return type;
-        }
-    }
-
-    public static class Builder {
-        protected final int controllerId;
-        protected final int controllerEpoch;
-        protected final long brokerEpoch;
-        private final List<PartitionState> partitionStates;
-        private final Map<String, Uuid> topicIds;
-        private final Collection<Node> liveLeaders;
-        private final Type updateType;
-
-        public Builder(int controllerId, int controllerEpoch, long brokerEpoch,
-                       List<PartitionState> partitionStates, Map<String, Uuid> topicIds,
-                       Collection<Node> liveLeaders) {
-            this(controllerId, controllerEpoch, brokerEpoch, partitionStates, topicIds, liveLeaders, Type.UNKNOWN);
-        }
-
-        public Builder(int controllerId, int controllerEpoch, long brokerEpoch,
-                       List<PartitionState> partitionStates, Map<String, Uuid> topicIds,
-                       Collection<Node> liveLeaders, Type updateType) {
-            this.controllerId = controllerId;
-            this.controllerEpoch = controllerEpoch;
-            this.brokerEpoch = brokerEpoch;
-            this.partitionStates = partitionStates;
-            this.topicIds = topicIds;
-            this.liveLeaders = liveLeaders;
-            this.updateType = updateType;
-        }
-
-        public LeaderAndIsrRequest build() {
-            return new LeaderAndIsrRequest(this);
-        }
-
-        @Override
-        public String toString() {
-            return "(type=LeaderAndIsRequest" +
-                    ", controllerId=" + controllerId +
-                    ", controllerEpoch=" + controllerEpoch +
-                    ", brokerEpoch=" + brokerEpoch +
-                    ", partitionStates=" + partitionStates +
-                    ", topicIds=" + topicIds +
-                    ", liveLeaders=(" + liveLeaders.stream().map(Node::toString).collect(Collectors.joining(", ")) + ")" +
-                    ")";
-
-        }
-    }
-
-    private final int controllerId;
-    private final int controllerEpoch;
-    private final long brokerEpoch;
-    private final List<Node> liveLeaders;
-    private final List<TopicState> topicStates;
-    private final Type requestType;
-
-    public LeaderAndIsrRequest(Builder builder) {
-        this.controllerId = builder.controllerId;
-        this.controllerEpoch = builder.controllerEpoch;
-        this.brokerEpoch = builder.brokerEpoch;
-        this.requestType = builder.updateType;
-        this.liveLeaders = new ArrayList<>(builder.liveLeaders);
-        this.topicStates = new ArrayList<>(groupByTopic(builder.partitionStates, builder.topicIds).values());
-    }
-
-    private static Map<String, TopicState> groupByTopic(List<PartitionState> partitionStates, Map<String, Uuid> topicIds) {
-        Map<String, TopicState> topicStates = new HashMap<>();
-        for (PartitionState partition : partitionStates) {
-            TopicState topicState = topicStates.computeIfAbsent(partition.topicName(), t -> {
-                var topic = new TopicState();
-                topic.topicName = partition.topicName();
-                topic.topicId = topicIds.getOrDefault(partition.topicName(), Uuid.ZERO_UUID);
-                return topic;
-            });
-            topicState.partitionStates().add(partition);
-        }
-        return topicStates;
-    }
-
-    public int controllerId() {
-        return controllerId;
-    }
-
-    public int controllerEpoch() {
-        return controllerEpoch;
-    }
-
-    public long brokerEpoch() {
-        return brokerEpoch;
-    }
-
-    public Iterable<PartitionState> partitionStates() {
-        return () -> new FlattenedIterator<>(topicStates.iterator(),
-                topicState -> topicState.partitionStates().iterator());
-    }
-
-    public Map<String, Uuid> topicIds() {
-        return topicStates.stream()
-                .collect(Collectors.toMap(TopicState::topicName, TopicState::topicId));
-    }
-
-    public List<Node> liveLeaders() {
-        return Collections.unmodifiableList(liveLeaders);
-    }
-
-    public Type requestType() {
-        return requestType;
-    }
-
-    public LeaderAndIsrResponse getErrorResponse(Exception e) {
-        LinkedHashMap<Uuid, List<LeaderAndIsrResponse.PartitionError>> errorsMap = new LinkedHashMap<>();
-        Errors error = Errors.forException(e);
-
-        for (TopicState topicState : topicStates) {
-            List<LeaderAndIsrResponse.PartitionError> partitions = new ArrayList<>(topicState.partitionStates().size());
-            for (PartitionState partition : topicState.partitionStates()) {
-                partitions.add(new LeaderAndIsrResponse.PartitionError(partition.partitionIndex, error.code()));
-            }
-            errorsMap.put(topicState.topicId, partitions);
-        }
-
-        return new LeaderAndIsrResponse(error, errorsMap);
-
-    }
-
-    public static class TopicState {
-        String topicName;
-        Uuid topicId;
-        List<PartitionState> partitionStates;
-
-        public TopicState() {
-            this.topicName = "";
-            this.topicId = Uuid.ZERO_UUID;
-            this.partitionStates = new ArrayList<>(0);
-        }
-
-        public String topicName() {
-            return this.topicName;
-        }
-
-        public Uuid topicId() {
-            return this.topicId;
-        }
-
-        public List<PartitionState> partitionStates() {
-            return this.partitionStates;
-        }
-
-        @Override
-        public boolean equals(Object o) {
-            if (o == null || getClass() != o.getClass())
-                return false;
-            TopicState that = (TopicState) o;
-            return Objects.equals(topicName, that.topicName) &&
-                    Objects.equals(topicId, that.topicId) &&
-                    Objects.equals(partitionStates, that.partitionStates);
-        }
-
-        @Override
-        public int hashCode() {
-            return Objects.hash(topicName, topicId, partitionStates);
-        }
-
-        @Override
-        public String toString() {
-            return "LeaderAndIsrTopicState("
-                    + "topicName='" + topicName + "'"
-                    + ", topicId=" + topicId
-                    + ", partitionStates=" + MessageUtil.deepToString(partitionStates.iterator())
-                    + ")";
-        }
-    }
-
-    public static class PartitionState {
-        String topicName;
-        int partitionIndex;
-        int controllerEpoch;
-        int leader;
-        int leaderEpoch;
-        List<Integer> isr;
-        int partitionEpoch;
-        List<Integer> replicas;
-        List<Integer> addingReplicas;
-        List<Integer> removingReplicas;
-        boolean isNew;
-        byte leaderRecoveryState;
-
-        public PartitionState() {
-            this.topicName = "";
-            this.partitionIndex = 0;
-            this.controllerEpoch = 0;
-            this.leader = 0;
-            this.leaderEpoch = 0;
-            this.isr = new ArrayList<>(0);
-            this.partitionEpoch = 0;
-            this.replicas = new ArrayList<>(0);
-            this.addingReplicas = new ArrayList<>(0);
-            this.removingReplicas = new ArrayList<>(0);
-            this.isNew = false;
-            this.leaderRecoveryState = (byte) 0;
-        }
-
-        @Override
-        public boolean equals(Object o) {
-            if (o == null || getClass() != o.getClass()) return false;
-            PartitionState that = (PartitionState) o;
-            return partitionIndex == that.partitionIndex &&
-                    controllerEpoch == that.controllerEpoch &&
-                    leader == that.leader &&
-                    leaderEpoch == that.leaderEpoch &&
-                    partitionEpoch == that.partitionEpoch &&
-                    isNew == that.isNew &&
-                    leaderRecoveryState == that.leaderRecoveryState &&
-                    Objects.equals(topicName, that.topicName) &&
-                    Objects.equals(isr, that.isr) &&
-                    Objects.equals(replicas, that.replicas) &&
-                    Objects.equals(addingReplicas, that.addingReplicas) &&
-                    Objects.equals(removingReplicas, that.removingReplicas);
-        }
-
-        @Override
-        public int hashCode() {
-            return Objects.hash(topicName, partitionIndex, controllerEpoch, leader, leaderEpoch, isr, partitionEpoch,
-                    replicas, addingReplicas, removingReplicas, isNew, leaderRecoveryState);
-        }
-
-        @Override
-        public String toString() {
-            return "LeaderAndIsrPartitionState("
-                    + "topicName='" + topicName + "'"
-                    + ", partitionIndex=" + partitionIndex
-                    + ", controllerEpoch=" + controllerEpoch
-                    + ", leader=" + leader
-                    + ", leaderEpoch=" + leaderEpoch
-                    + ", isr=" + MessageUtil.deepToString(isr.iterator())
-                    + ", partitionEpoch=" + partitionEpoch
-                    + ", replicas=" + MessageUtil.deepToString(replicas.iterator())
-                    + ", addingReplicas=" + MessageUtil.deepToString(addingReplicas.iterator())
-                    + ", removingReplicas=" + MessageUtil.deepToString(removingReplicas.iterator())
-                    + ", isNew=" + (isNew ? "true" : "false")
-                    + ", leaderRecoveryState=" + leaderRecoveryState
-                    + ")";
-        }
-
-        public String topicName() {
-            return this.topicName;
-        }
-
-        public int partitionIndex() {
-            return this.partitionIndex;
-        }
-
-        public int controllerEpoch() {
-            return this.controllerEpoch;
-        }
-
-        public int leader() {
-            return this.leader;
-        }
-
-        public int leaderEpoch() {
-            return this.leaderEpoch;
-        }
-
-        public List<Integer> isr() {
-            return this.isr;
-        }
-
-        public int partitionEpoch() {
-            return this.partitionEpoch;
-        }
-
-        public List<Integer> replicas() {
-            return this.replicas;
-        }
-
-        public List<Integer> addingReplicas() {
-            return this.addingReplicas;
-        }
-
-        public List<Integer> removingReplicas() {
-            return this.removingReplicas;
-        }
-
-        public boolean isNew() {
-            return this.isNew;
-        }
-
-        public byte leaderRecoveryState() {
-            return this.leaderRecoveryState;
-        }
-
-        public PartitionState setTopicName(String v) {
-            this.topicName = v;
-            return this;
-        }
-
-        public PartitionState setPartitionIndex(int v) {
-            this.partitionIndex = v;
-            return this;
-        }
-
-        public PartitionState setControllerEpoch(int v) {
-            this.controllerEpoch = v;
-            return this;
-        }
-
-        public PartitionState setLeader(int v) {
-            this.leader = v;
-            return this;
-        }
-
-        public PartitionState setLeaderEpoch(int v) {
-            this.leaderEpoch = v;
-            return this;
-        }
-
-        public PartitionState setIsr(List<Integer> v) {
-            this.isr = v;
-            return this;
-        }
-
-        public PartitionState setPartitionEpoch(int v) {
-            this.partitionEpoch = v;
-            return this;
-        }
-
-        public PartitionState setReplicas(List<Integer> v) {
-            this.replicas = v;
-            return this;
-        }
-
-        public PartitionState setAddingReplicas(List<Integer> v) {
-            this.addingReplicas = v;
-            return this;
-        }
-
-        public PartitionState setRemovingReplicas(List<Integer> v) {
-            this.removingReplicas = v;
-            return this;
-        }
-
-        public PartitionState setIsNew(boolean v) {
-            this.isNew = v;
-            return this;
-        }
-
-        public PartitionState setLeaderRecoveryState(byte v) {
-            this.leaderRecoveryState = v;
-            return this;
-        }
-    }
-}
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/LeaderAndIsrResponse.java b/clients/src/main/java/org/apache/kafka/common/requests/LeaderAndIsrResponse.java
deleted file mode 100644
index bcdcf35b5d..0000000000
--- a/clients/src/main/java/org/apache/kafka/common/requests/LeaderAndIsrResponse.java
+++ /dev/null
@@ -1,95 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License. You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.kafka.common.requests;
-
-import org.apache.kafka.common.TopicPartition;
-import org.apache.kafka.common.Uuid;
-import org.apache.kafka.common.protocol.Errors;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.LinkedHashMap;
-import java.util.List;
-import java.util.Map;
-
-public class LeaderAndIsrResponse {
-
-    /**
-     * Possible error code:
-     *
-     * STALE_CONTROLLER_EPOCH (11)
-     * STALE_BROKER_EPOCH (77)
-     */
-    private final Errors error;
-    private final LinkedHashMap<Uuid, List<PartitionError>> topicErrors;
-
-    public LeaderAndIsrResponse(Errors error, LinkedHashMap<Uuid, List<PartitionError>> topicErrors) {
-        this.error = error;
-        this.topicErrors = topicErrors;
-    }
-
-    public LinkedHashMap<Uuid, List<PartitionError>> topics() {
-        return topicErrors;
-    }
-
-    public Errors error() {
-        return error;
-    }
-
-    public Map<Errors, Integer> errorCounts() {
-        Errors error = error();
-        if (error != Errors.NONE) {
-            // Minor optimization since the top-level error applies to all partitions
-            return Collections.singletonMap(error, topics().values().stream().mapToInt(partitionErrors ->
-                    partitionErrors.size()).sum() + 1);
-        }
-        Map<Errors, Integer> errors = AbstractResponse.errorCounts(topics().values().stream().flatMap(partitionErrors ->
-                partitionErrors.stream()).map(p -> Errors.forCode(p.errorCode)));
-        AbstractResponse.updateErrorCounts(errors, Errors.NONE);
-        return errors;
-    }
-
-    public Map<TopicPartition, Errors> partitionErrors(Map<Uuid, String> topicNames) {
-        Map<TopicPartition, Errors> errors = new HashMap<>();
-        topics().forEach((topicId, partitionErrors) -> {
-            String topicName = topicNames.get(topicId);
-            if (topicName != null) {
-                partitionErrors.forEach(partition ->
-                    errors.put(new TopicPartition(topicName, partition.partitionIndex), Errors.forCode(partition.errorCode)));
-            }
-        });
-        return errors;
-    }
-
-    @Override
-    public String toString() {
-        return "LeaderAndIsrResponse{" +
-                "error=" + error +
-                ", topicErrors=" + topicErrors +
-                '}';
-    }
-
-    public static class PartitionError {
-        public final int partitionIndex;
-        public final short errorCode;
-
-        public PartitionError(int partitionIndex, short errorCode) {
-            this.partitionIndex = partitionIndex;
-            this.errorCode = errorCode;
-        }
-    }
-}
diff --git a/core/src/main/java/kafka/server/builders/ReplicaManagerBuilder.java b/core/src/main/java/kafka/server/builders/ReplicaManagerBuilder.java
index f001a039ae..ba6221732f 100644
--- a/core/src/main/java/kafka/server/builders/ReplicaManagerBuilder.java
+++ b/core/src/main/java/kafka/server/builders/ReplicaManagerBuilder.java
@@ -33,7 +33,6 @@ import org.apache.kafka.storage.internals.log.LogDirFailureChannel;
 import org.apache.kafka.storage.log.metrics.BrokerTopicStats;
 
 import java.util.Map;
-import java.util.concurrent.atomic.AtomicBoolean;
 
 import scala.Option;
 
@@ -121,7 +120,6 @@ public class ReplicaManagerBuilder {
                              logDirFailureChannel,
                              alterPartitionManager,
                              brokerTopicStats,
-                             new AtomicBoolean(false),
                              Option.empty(),
                              Option.empty(),
                              Option.empty(),
diff --git a/core/src/main/scala/kafka/cluster/Partition.scala b/core/src/main/scala/kafka/cluster/Partition.scala
index 9e9b66f2ed..d896217b2f 100755
--- a/core/src/main/scala/kafka/cluster/Partition.scala
+++ b/core/src/main/scala/kafka/cluster/Partition.scala
@@ -36,6 +36,7 @@ import org.apache.kafka.common.record.FileRecords.TimestampAndOffset
 import org.apache.kafka.common.record.{FileRecords, MemoryRecords, RecordBatch}
 import org.apache.kafka.common.requests._
 import org.apache.kafka.common.requests.OffsetsForLeaderEpochResponse.{UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET}
+import org.apache.kafka.common.{PartitionState => JPartitionState}
 import org.apache.kafka.common.utils.Time
 import org.apache.kafka.metadata.{LeaderAndIsr, LeaderRecoveryState, MetadataCache}
 import org.apache.kafka.server.common.RequestLocal
@@ -730,7 +731,7 @@ class Partition(val topicPartition: TopicPartition,
    * from the time when this broker was the leader last time) and setting the new leader and ISR.
    * If the leader replica id does not change, return false to indicate the replica manager.
    */
-  def makeLeader(partitionState: LeaderAndIsrRequest.PartitionState,
+  def makeLeader(partitionState: JPartitionState,
                  highWatermarkCheckpoints: OffsetCheckpoints,
                  topicId: Option[Uuid],
                  targetDirectoryId: Option[Uuid] = None): Boolean = {
@@ -836,7 +837,7 @@ class Partition(val topicPartition: TopicPartition,
    * replica manager that state is already correct and the become-follower steps can
    * be skipped.
    */
-  def makeFollower(partitionState: LeaderAndIsrRequest.PartitionState,
+  def makeFollower(partitionState: JPartitionState,
                    highWatermarkCheckpoints: OffsetCheckpoints,
                    topicId: Option[Uuid],
                    targetLogDirectoryId: Option[Uuid] = None): Boolean = {
@@ -884,7 +885,7 @@ class Partition(val topicPartition: TopicPartition,
     }
   }
 
-  private def createLogInAssignedDirectoryId(partitionState: LeaderAndIsrRequest.PartitionState, highWatermarkCheckpoints: OffsetCheckpoints, topicId: Option[Uuid], targetLogDirectoryId: Option[Uuid]): Unit = {
+  private def createLogInAssignedDirectoryId(partitionState: JPartitionState, highWatermarkCheckpoints: OffsetCheckpoints, topicId: Option[Uuid], targetLogDirectoryId: Option[Uuid]): Unit = {
     targetLogDirectoryId match {
       case Some(directoryId) =>
         if (logManager.onlineLogDirId(directoryId) || !logManager.hasOfflineLogDirs() || directoryId == DirectoryId.UNASSIGNED) {
diff --git a/core/src/main/scala/kafka/server/BrokerServer.scala b/core/src/main/scala/kafka/server/BrokerServer.scala
index c56e178ca2..8f2e15e7dc 100644
--- a/core/src/main/scala/kafka/server/BrokerServer.scala
+++ b/core/src/main/scala/kafka/server/BrokerServer.scala
@@ -62,7 +62,6 @@ import org.apache.kafka.storage.log.metrics.BrokerTopicStats
 import java.time.Duration
 import java.util
 import java.util.Optional
-import java.util.concurrent.atomic.AtomicBoolean
 import java.util.concurrent.locks.{Condition, ReentrantLock}
 import java.util.concurrent.{CompletableFuture, ExecutionException, TimeUnit, TimeoutException}
 import scala.collection.Map
@@ -96,8 +95,6 @@ class BrokerServer(
 
   private var assignmentsManager: AssignmentsManager = _
 
-  private val isShuttingDown = new AtomicBoolean(false)
-
   val lock: ReentrantLock = new ReentrantLock()
   val awaitShutdownCond: Condition = lock.newCondition()
   var status: ProcessStatus = SHUTDOWN
@@ -172,10 +169,7 @@ class BrokerServer(
       info(s"Transition from $status to $to")
 
       status = to
-      if (to == SHUTTING_DOWN) {
-        isShuttingDown.set(true)
-      } else if (to == SHUTDOWN) {
-        isShuttingDown.set(false)
+      if (to == SHUTDOWN) {
         awaitShutdownCond.signalAll()
       }
     } finally {
@@ -354,7 +348,6 @@ class BrokerServer(
         logDirFailureChannel = logDirFailureChannel,
         alterPartitionManager = alterPartitionManager,
         brokerTopicStats = brokerTopicStats,
-        isShuttingDown = isShuttingDown,
         threadNamePrefix = None, // The ReplicaManager only runs on the broker, and already includes the ID in thread names.
         delayedRemoteFetchPurgatoryParam = None,
         brokerEpochSupplier = () => lifecycleManager.brokerEpoch,
@@ -822,8 +815,6 @@ class BrokerServer(
       if (persister != null)
         CoreUtils.swallow(persister.stop(), this)
 
-      isShuttingDown.set(false)
-
       if (lifecycleManager != null)
         CoreUtils.swallow(lifecycleManager.close(), this)
 
diff --git a/core/src/main/scala/kafka/server/ReplicaManager.scala b/core/src/main/scala/kafka/server/ReplicaManager.scala
index 78268af589..f18fe120e2 100644
--- a/core/src/main/scala/kafka/server/ReplicaManager.scala
+++ b/core/src/main/scala/kafka/server/ReplicaManager.scala
@@ -219,7 +219,6 @@ class ReplicaManager(val config: KafkaConfig,
                      logDirFailureChannel: LogDirFailureChannel,
                      val alterPartitionManager: AlterPartitionManager,
                      val brokerTopicStats: BrokerTopicStats = new BrokerTopicStats(),
-                     val isShuttingDown: AtomicBoolean = new AtomicBoolean(false),
                      delayedProducePurgatoryParam: Option[DelayedOperationPurgatory[DelayedProduce]] = None,
                      delayedFetchPurgatoryParam: Option[DelayedOperationPurgatory[DelayedFetch]] = None,
                      delayedDeleteRecordsPurgatoryParam: Option[DelayedOperationPurgatory[DelayedDeleteRecords]] = None,
@@ -261,7 +260,6 @@ class ReplicaManager(val config: KafkaConfig,
       config.shareGroupConfig.shareFetchPurgatoryPurgeIntervalRequests))
 
   /* epoch of the controller that last changed the leader */
-  @volatile private[server] var controllerEpoch: Int = 0
   protected val localBrokerId = config.brokerId
   protected val allPartitions = new ConcurrentHashMap[TopicPartition, HostedPartition]
   private val replicaStateChangeLock = new Object
@@ -1989,189 +1987,6 @@ class ReplicaManager(val config: KafkaConfig,
 
   def getLogConfig(topicPartition: TopicPartition): Option[LogConfig] = localLog(topicPartition).map(_.config)
 
-  def becomeLeaderOrFollower(correlationId: Int,
-                             leaderAndIsrRequest: LeaderAndIsrRequest,
-                             onLeadershipChange: (Iterable[Partition], Iterable[Partition]) => Unit): LeaderAndIsrResponse = {
-    val startMs = time.milliseconds()
-    replicaStateChangeLock synchronized {
-      val controllerId = leaderAndIsrRequest.controllerId
-      val requestPartitionStates = leaderAndIsrRequest.partitionStates.asScala
-      stateChangeLogger.info(s"Handling LeaderAndIsr request correlationId $correlationId from controller " +
-        s"$controllerId for ${requestPartitionStates.size} partitions")
-      if (stateChangeLogger.isTraceEnabled)
-        requestPartitionStates.foreach { partitionState =>
-          stateChangeLogger.trace(s"Received LeaderAndIsr request $partitionState " +
-            s"correlation id $correlationId from controller $controllerId " +
-            s"epoch ${leaderAndIsrRequest.controllerEpoch}")
-        }
-      val topicIds = leaderAndIsrRequest.topicIds()
-      def topicIdFromRequest(topicName: String): Option[Uuid] = {
-        val topicId = topicIds.get(topicName)
-        // if invalid topic ID return None
-        if (topicId == null || topicId == Uuid.ZERO_UUID)
-          None
-        else
-          Some(topicId)
-      }
-
-      val response = {
-        if (leaderAndIsrRequest.controllerEpoch < controllerEpoch) {
-          stateChangeLogger.warn(s"Ignoring LeaderAndIsr request from controller $controllerId with " +
-            s"correlation id $correlationId since its controller epoch ${leaderAndIsrRequest.controllerEpoch} is old. " +
-            s"Latest known controller epoch is $controllerEpoch")
-          leaderAndIsrRequest.getErrorResponse(Errors.STALE_CONTROLLER_EPOCH.exception)
-        } else {
-          val responseMap = new mutable.HashMap[TopicPartition, Errors]
-          controllerEpoch = leaderAndIsrRequest.controllerEpoch
-
-          val partitions = new mutable.HashSet[Partition]()
-          val partitionsToBeLeader = new mutable.HashMap[Partition, LeaderAndIsrRequest.PartitionState]()
-          val partitionsToBeFollower = new mutable.HashMap[Partition, LeaderAndIsrRequest.PartitionState]()
-          val topicIdUpdateFollowerPartitions = new mutable.HashSet[Partition]()
-          val allTopicPartitionsInRequest = new mutable.HashSet[TopicPartition]()
-
-          // First create the partition if it doesn't exist already
-          requestPartitionStates.foreach { partitionState =>
-            val topicPartition = new TopicPartition(partitionState.topicName, partitionState.partitionIndex)
-            allTopicPartitionsInRequest += topicPartition
-            val partitionOpt = getPartition(topicPartition) match {
-              case HostedPartition.Offline(_) =>
-                stateChangeLogger.warn(s"Ignoring LeaderAndIsr request from " +
-                  s"controller $controllerId with correlation id $correlationId " +
-                  s"epoch $controllerEpoch for partition $topicPartition as the local replica for the " +
-                  "partition is in an offline log directory")
-                responseMap.put(topicPartition, Errors.KAFKA_STORAGE_ERROR)
-                None
-
-              case HostedPartition.Online(partition) =>
-                Some(partition)
-
-              case HostedPartition.None =>
-                val partition = Partition(topicPartition, time, this)
-                allPartitions.putIfAbsent(topicPartition, HostedPartition.Online(partition))
-                Some(partition)
-            }
-
-            // Next check the topic ID and the partition's leader epoch
-            partitionOpt.foreach { partition =>
-              val currentLeaderEpoch = partition.getLeaderEpoch
-              val requestLeaderEpoch = partitionState.leaderEpoch
-              val requestTopicId = topicIdFromRequest(topicPartition.topic)
-              val logTopicId = partition.topicId
-
-              if (!hasConsistentTopicId(requestTopicId, logTopicId)) {
-                stateChangeLogger.error(s"Topic ID in memory: ${logTopicId.get} does not" +
-                  s" match the topic ID for partition $topicPartition received: " +
-                  s"${requestTopicId.get}.")
-                responseMap.put(topicPartition, Errors.INCONSISTENT_TOPIC_ID)
-              } else if (requestLeaderEpoch >= currentLeaderEpoch) {
-                // If the leader epoch is valid record the epoch of the controller that made the leadership decision.
-                // This is useful while updating the isr to maintain the decision maker controller's epoch in the zookeeper path
-                if (partitionState.replicas.contains(localBrokerId)) {
-                  partitions += partition
-                  if (partitionState.leader == localBrokerId) {
-                    partitionsToBeLeader.put(partition, partitionState)
-                  } else {
-                    partitionsToBeFollower.put(partition, partitionState)
-                  }
-                } else {
-                  stateChangeLogger.warn(s"Ignoring LeaderAndIsr request from controller $controllerId with " +
-                    s"correlation id $correlationId epoch $controllerEpoch for partition $topicPartition as itself is not " +
-                    s"in assigned replica list ${partitionState.replicas.asScala.mkString(",")}")
-                  responseMap.put(topicPartition, Errors.UNKNOWN_TOPIC_OR_PARTITION)
-                }
-              } else if (requestLeaderEpoch < currentLeaderEpoch) {
-                stateChangeLogger.warn(s"Ignoring LeaderAndIsr request from " +
-                  s"controller $controllerId with correlation id $correlationId " +
-                  s"epoch $controllerEpoch for partition $topicPartition since its associated " +
-                  s"leader epoch $requestLeaderEpoch is smaller than the current " +
-                  s"leader epoch $currentLeaderEpoch")
-                responseMap.put(topicPartition, Errors.STALE_CONTROLLER_EPOCH)
-              } else {
-                val error = requestTopicId match {
-                  case Some(topicId) if logTopicId.isEmpty =>
-                    // The controller may send LeaderAndIsr to upgrade to using topic IDs without bumping the epoch.
-                    // If we have a matching epoch, we expect the log to be defined.
-                    val log = localLogOrException(partition.topicPartition)
-                    log.assignTopicId(topicId)
-                    stateChangeLogger.info(s"Updating log for $topicPartition to assign topic ID " +
-                      s"$topicId from LeaderAndIsr request from controller $controllerId with correlation " +
-                      s"id $correlationId epoch $controllerEpoch")
-                    if (partitionState.leader != localBrokerId)
-                      topicIdUpdateFollowerPartitions.add(partition)
-                    Errors.NONE
-                  case None if logTopicId.isDefined && partitionState.leader != localBrokerId =>
-                    // If we have a topic ID in the log but not in the request, we must have previously had topic IDs but
-                    // are now downgrading. If we are a follower, remove the topic ID from the PartitionFetchState.
-                    stateChangeLogger.info(s"Updating PartitionFetchState for $topicPartition to remove log topic ID " +
-                      s"${logTopicId.get} since LeaderAndIsr request from controller $controllerId with correlation " +
-                      s"id $correlationId epoch $controllerEpoch did not contain a topic ID")
-                    topicIdUpdateFollowerPartitions.add(partition)
-                    Errors.NONE
-                  case _ =>
-                    stateChangeLogger.info(s"Ignoring LeaderAndIsr request from " +
-                      s"controller $controllerId with correlation id $correlationId " +
-                      s"epoch $controllerEpoch for partition $topicPartition since its associated " +
-                      s"leader epoch $requestLeaderEpoch matches the current leader epoch")
-                    Errors.STALE_CONTROLLER_EPOCH
-                }
-                responseMap.put(topicPartition, error)
-              }
-            }
-          }
-
-          val highWatermarkCheckpoints = new LazyOffsetCheckpoints(this.highWatermarkCheckpoints.asJava)
-          val partitionsBecomeLeader = if (partitionsToBeLeader.nonEmpty)
-            makeLeaders(controllerId, controllerEpoch, partitionsToBeLeader, correlationId, responseMap,
-              highWatermarkCheckpoints, topicIdFromRequest)
-          else
-            Set.empty[Partition]
-          val partitionsBecomeFollower = if (partitionsToBeFollower.nonEmpty)
-            makeFollowers(controllerId, controllerEpoch, partitionsToBeFollower, correlationId, responseMap,
-              highWatermarkCheckpoints, topicIdFromRequest)
-          else
-            Set.empty[Partition]
-
-          val followerTopicSet = partitionsBecomeFollower.map(_.topic).toSet
-          updateLeaderAndFollowerMetrics(followerTopicSet)
-
-          if (topicIdUpdateFollowerPartitions.nonEmpty)
-            updateTopicIdForFollowers(controllerId, controllerEpoch, topicIdUpdateFollowerPartitions, correlationId, topicIdFromRequest)
-
-          // We initialize highwatermark thread after the first LeaderAndIsr request. This ensures that all the partitions
-          // have been completely populated before starting the checkpointing there by avoiding weird race conditions
-          startHighWatermarkCheckPointThread()
-
-          maybeAddLogDirFetchers(partitions, highWatermarkCheckpoints, topicIdFromRequest)
-
-          replicaFetcherManager.shutdownIdleFetcherThreads()
-          replicaAlterLogDirsManager.shutdownIdleFetcherThreads()
-
-          remoteLogManager.foreach(rlm => rlm.onLeadershipChange((partitionsBecomeLeader.toSet: Set[TopicPartitionLog]).asJava, (partitionsBecomeFollower.toSet: Set[TopicPartitionLog]).asJava, topicIds))
-
-          onLeadershipChange(partitionsBecomeLeader, partitionsBecomeFollower)
-
-          val topics = new util.LinkedHashMap[Uuid, util.List[LeaderAndIsrResponse.PartitionError]]
-          responseMap.foreachEntry { (tp, error) =>
-            val topicId = topicIds.get(tp.topic)
-            var partitionErrors = topics.get(topicId)
-            if (partitionErrors == null) {
-              partitionErrors = new util.ArrayList[LeaderAndIsrResponse.PartitionError]()
-              topics.put(topicId, partitionErrors)
-            }
-            partitionErrors.add(new LeaderAndIsrResponse.PartitionError(tp.partition(), error.code))
-          }
-          new LeaderAndIsrResponse(Errors.NONE, topics)
-        }
-      }
-      val endMs = time.milliseconds()
-      val elapsedMs = endMs - startMs
-      stateChangeLogger.info(s"Finished LeaderAndIsr request in ${elapsedMs}ms correlationId $correlationId from controller " +
-        s"$controllerId for ${requestPartitionStates.size} partitions")
-      response
-    }
-  }
-
   /**
    * Checks if the topic ID provided in the request is consistent with the topic ID in the log.
    * When using this method to handle a Fetch request, the topic ID may have been provided by an earlier request.
@@ -2234,243 +2049,6 @@ class ReplicaManager(val config: KafkaConfig,
     }
   }
 
-  /*
-   * Make the current broker to become leader for a given set of partitions by:
-   *
-   * 1. Stop fetchers for these partitions
-   * 2. Update the partition metadata in cache
-   * 3. Add these partitions to the leader partitions set
-   *
-   * If an unexpected error is thrown in this function, it will be propagated to KafkaApis where
-   * the error message will be set on each partition since we do not know which partition caused it. Otherwise,
-   * return the set of partitions that are made leader due to this method
-   *
-   *  TODO: the above may need to be fixed later
-   */
-  private def makeLeaders(controllerId: Int,
-                          controllerEpoch: Int,
-                          partitionStates: Map[Partition, LeaderAndIsrRequest.PartitionState],
-                          correlationId: Int,
-                          responseMap: mutable.Map[TopicPartition, Errors],
-                          highWatermarkCheckpoints: OffsetCheckpoints,
-                          topicIds: String => Option[Uuid]): Set[Partition] = {
-    val traceEnabled = stateChangeLogger.isTraceEnabled
-    partitionStates.keys.foreach { partition =>
-      if (traceEnabled)
-        stateChangeLogger.trace(s"Handling LeaderAndIsr request correlationId $correlationId from " +
-          s"controller $controllerId epoch $controllerEpoch starting the become-leader transition for " +
-          s"partition ${partition.topicPartition}")
-      responseMap.put(partition.topicPartition, Errors.NONE)
-    }
-
-    val partitionsToMakeLeaders = mutable.Set[Partition]()
-
-    try {
-      // First stop fetchers for all the partitions
-      replicaFetcherManager.removeFetcherForPartitions(partitionStates.keySet.map(_.topicPartition))
-      stateChangeLogger.info(s"Stopped fetchers as part of LeaderAndIsr request correlationId $correlationId from " +
-        s"controller $controllerId epoch $controllerEpoch as part of the become-leader transition for " +
-        s"${partitionStates.size} partitions")
-      // Update the partition information to be the leader
-      partitionStates.foreachEntry { (partition, partitionState) =>
-        try {
-          if (partition.makeLeader(partitionState, highWatermarkCheckpoints, topicIds(partitionState.topicName))) {
-            partitionsToMakeLeaders += partition
-          }
-        } catch {
-          case e: KafkaStorageException =>
-            stateChangeLogger.error(s"Skipped the become-leader state change with " +
-              s"correlation id $correlationId from controller $controllerId epoch $controllerEpoch for partition ${partition.topicPartition} " +
-              s"(last update controller epoch ${partitionState.controllerEpoch}) since " +
-              s"the replica for the partition is offline due to storage error $e")
-            // If there is an offline log directory, a Partition object may have been created and have been added
-            // to `ReplicaManager.allPartitions` before `createLogIfNotExists()` failed to create local replica due
-            // to KafkaStorageException. In this case `ReplicaManager.allPartitions` will map this topic-partition
-            // to an empty Partition object. We need to map this topic-partition to OfflinePartition instead.
-            markPartitionOffline(partition.topicPartition)
-            responseMap.put(partition.topicPartition, Errors.KAFKA_STORAGE_ERROR)
-        }
-      }
-
-    } catch {
-      case e: Throwable =>
-        partitionStates.keys.foreach { partition =>
-          stateChangeLogger.error(s"Error while processing LeaderAndIsr request correlationId $correlationId received " +
-            s"from controller $controllerId epoch $controllerEpoch for partition ${partition.topicPartition}", e)
-        }
-        // Re-throw the exception for it to be caught in KafkaApis
-        throw e
-    }
-
-    if (traceEnabled)
-      partitionStates.keys.foreach { partition =>
-        stateChangeLogger.trace(s"Completed LeaderAndIsr request correlationId $correlationId from controller $controllerId " +
-          s"epoch $controllerEpoch for the become-leader transition for partition ${partition.topicPartition}")
-      }
-
-    partitionsToMakeLeaders
-  }
-
-  /*
-   * Make the current broker to become follower for a given set of partitions by:
-   *
-   * 1. Remove these partitions from the leader partitions set.
-   * 2. Mark the replicas as followers so that no more data can be added from the producer clients.
-   * 3. Stop fetchers for these partitions so that no more data can be added by the replica fetcher threads.
-   * 4. Truncate the log and checkpoint offsets for these partitions.
-   * 5. Clear the produce and fetch requests in the purgatory
-   * 6. If the broker is not shutting down, add the fetcher to the new leaders.
-   *
-   * The ordering of doing these steps make sure that the replicas in transition will not
-   * take any more messages before checkpointing offsets so that all messages before the checkpoint
-   * are guaranteed to be flushed to disks
-   *
-   * If an unexpected error is thrown in this function, it will be propagated to KafkaApis where
-   * the error message will be set on each partition since we do not know which partition caused it. Otherwise,
-   * return the set of partitions that are made follower due to this method
-   */
-  private def makeFollowers(controllerId: Int,
-                            controllerEpoch: Int,
-                            partitionStates: Map[Partition, LeaderAndIsrRequest.PartitionState],
-                            correlationId: Int,
-                            responseMap: mutable.Map[TopicPartition, Errors],
-                            highWatermarkCheckpoints: OffsetCheckpoints,
-                            topicIds: String => Option[Uuid]) : Set[Partition] = {
-    val traceLoggingEnabled = stateChangeLogger.isTraceEnabled
-    partitionStates.foreachEntry { (partition, partitionState) =>
-      if (traceLoggingEnabled)
-        stateChangeLogger.trace(s"Handling LeaderAndIsr request correlationId $correlationId from controller $controllerId " +
-          s"epoch $controllerEpoch starting the become-follower transition for partition ${partition.topicPartition} with leader " +
-          s"${partitionState.leader}")
-      responseMap.put(partition.topicPartition, Errors.NONE)
-    }
-
-    val partitionsToMakeFollower: mutable.Set[Partition] = mutable.Set()
-    try {
-      partitionStates.foreachEntry { (partition, partitionState) =>
-        val newLeaderBrokerId = partitionState.leader
-        try {
-          if (metadataCache.hasAliveBroker(newLeaderBrokerId)) {
-            // Only change partition state when the leader is available
-            if (partition.makeFollower(partitionState, highWatermarkCheckpoints, topicIds(partitionState.topicName))) {
-              // Skip invoking onBecomingFollower listeners as the listeners are not registered for zk-based features.
-              partitionsToMakeFollower += partition
-            }
-          } else {
-            // The leader broker should always be present in the metadata cache.
-            // If not, we should record the error message and abort the transition process for this partition
-            stateChangeLogger.error(s"Received LeaderAndIsrRequest with correlation id $correlationId from " +
-              s"controller $controllerId epoch $controllerEpoch for partition ${partition.topicPartition} " +
-              s"(last update controller epoch ${partitionState.controllerEpoch}) " +
-              s"but cannot become follower since the new leader $newLeaderBrokerId is unavailable.")
-            // Create the local replica even if the leader is unavailable. This is required to ensure that we include
-            // the partition's high watermark in the checkpoint file (see KAFKA-1647)
-            partition.createLogIfNotExists(isNew = partitionState.isNew, isFutureReplica = false,
-              highWatermarkCheckpoints, topicIds(partitionState.topicName))
-          }
-        } catch {
-          case e: KafkaStorageException =>
-            stateChangeLogger.error(s"Skipped the become-follower state change with correlation id $correlationId from " +
-              s"controller $controllerId epoch $controllerEpoch for partition ${partition.topicPartition} " +
-              s"(last update controller epoch ${partitionState.controllerEpoch}) with leader " +
-              s"$newLeaderBrokerId since the replica for the partition is offline due to storage error $e")
-            // If there is an offline log directory, a Partition object may have been created and have been added
-            // to `ReplicaManager.allPartitions` before `createLogIfNotExists()` failed to create local replica due
-            // to KafkaStorageException. In this case `ReplicaManager.allPartitions` will map this topic-partition
-            // to an empty Partition object. We need to map this topic-partition to OfflinePartition instead.
-            markPartitionOffline(partition.topicPartition)
-            responseMap.put(partition.topicPartition, Errors.KAFKA_STORAGE_ERROR)
-        }
-      }
-
-      // Stopping the fetchers must be done first in order to initialize the fetch
-      // position correctly.
-      replicaFetcherManager.removeFetcherForPartitions(partitionsToMakeFollower.map(_.topicPartition))
-      stateChangeLogger.info(s"Stopped fetchers as part of become-follower request from controller $controllerId " +
-        s"epoch $controllerEpoch with correlation id $correlationId for ${partitionsToMakeFollower.size} partitions")
-
-      partitionsToMakeFollower.foreach { partition =>
-        completeDelayedOperationsWhenNotPartitionLeader(partition.topicPartition, partition.topicId)
-      }
-
-      if (isShuttingDown.get()) {
-        if (traceLoggingEnabled) {
-          partitionsToMakeFollower.foreach { partition =>
-            stateChangeLogger.trace(s"Skipped the adding-fetcher step of the become-follower state " +
-              s"change with correlation id $correlationId from controller $controllerId epoch $controllerEpoch for " +
-              s"partition ${partition.topicPartition} with leader ${partitionStates(partition).leader} " +
-              "since it is shutting down")
-          }
-        }
-      } else {
-        // we do not need to check if the leader exists again since this has been done at the beginning of this process
-        val partitionsToMakeFollowerWithLeaderAndOffset = partitionsToMakeFollower.map { partition =>
-          val leaderNode = partition.leaderReplicaIdOpt match {
-            case Some(leaderId) => metadataCache.getAliveBrokerNode(leaderId, config.interBrokerListenerName).orElse(Node.noNode())
-            case None => Node.noNode()
-          }
-          val leader = new BrokerEndPoint(leaderNode.id(), leaderNode.host(), leaderNode.port())
-          val log = partition.localLogOrException
-          val fetchOffset = initialFetchOffset(log)
-          partition.topicPartition -> InitialFetchState(topicIds(partition.topic), leader, partition.getLeaderEpoch, fetchOffset)
-        }.toMap
-
-        replicaFetcherManager.addFetcherForPartitions(partitionsToMakeFollowerWithLeaderAndOffset)
-      }
-    } catch {
-      case e: Throwable =>
-        stateChangeLogger.error(s"Error while processing LeaderAndIsr request with correlationId $correlationId " +
-          s"received from controller $controllerId epoch $controllerEpoch", e)
-        // Re-throw the exception for it to be caught in KafkaApis
-        throw e
-    }
-
-    if (traceLoggingEnabled)
-      partitionStates.keys.foreach { partition =>
-        stateChangeLogger.trace(s"Completed LeaderAndIsr request correlationId $correlationId from controller $controllerId " +
-          s"epoch $controllerEpoch for the become-follower transition for partition ${partition.topicPartition} with leader " +
-          s"${partitionStates(partition).leader}")
-      }
-
-    partitionsToMakeFollower
-  }
-
-  private def updateTopicIdForFollowers(controllerId: Int,
-                                        controllerEpoch: Int,
-                                        partitions: Set[Partition],
-                                        correlationId: Int,
-                                        topicIds: String => Option[Uuid]): Unit = {
-    val traceLoggingEnabled = stateChangeLogger.isTraceEnabled
-
-    try {
-      if (isShuttingDown.get()) {
-        if (traceLoggingEnabled) {
-          partitions.foreach { partition =>
-            stateChangeLogger.trace(s"Skipped the update topic ID step of the become-follower state " +
-              s"change with correlation id $correlationId from controller $controllerId epoch $controllerEpoch for " +
-              s"partition ${partition.topicPartition} since it is shutting down")
-          }
-        }
-      } else {
-        val partitionsToUpdateFollowerWithLeader = mutable.Map.empty[TopicPartition, Int]
-        partitions.foreach { partition =>
-          partition.leaderReplicaIdOpt.foreach { leader =>
-            if (metadataCache.hasAliveBroker(leader)) {
-              partitionsToUpdateFollowerWithLeader += partition.topicPartition -> leader
-            }
-          }
-        }
-        replicaFetcherManager.maybeUpdateTopicIds(partitionsToUpdateFollowerWithLeader, topicIds)
-      }
-    } catch {
-      case e: Throwable =>
-        stateChangeLogger.error(s"Error while processing LeaderAndIsr request with correlationId $correlationId " +
-          s"received from controller $controllerId epoch $controllerEpoch when trying to update topic IDs in the fetchers", e)
-        // Re-throw the exception for it to be caught in KafkaApis
-        throw e
-    }
-  }
-
   /**
    * From IBP 2.7 onwards, we send latest fetch epoch in the request and truncate if a
    * diverging epoch is returned in the response, avoiding the need for a separate
diff --git a/core/src/test/scala/unit/kafka/cluster/AbstractPartitionTest.scala b/core/src/test/scala/unit/kafka/cluster/AbstractPartitionTest.scala
index ae1e86421e..c15f62df51 100644
--- a/core/src/test/scala/unit/kafka/cluster/AbstractPartitionTest.scala
+++ b/core/src/test/scala/unit/kafka/cluster/AbstractPartitionTest.scala
@@ -21,7 +21,7 @@ import kafka.utils.TestUtils
 import kafka.utils.TestUtils.MockAlterPartitionManager
 import org.apache.kafka.common.{TopicPartition, Uuid}
 import org.apache.kafka.common.config.TopicConfig
-import org.apache.kafka.common.requests.LeaderAndIsrRequest
+import org.apache.kafka.common.PartitionState
 import org.apache.kafka.common.utils.Utils
 import org.apache.kafka.metadata.{MetadataCache, MockConfigRepository}
 import org.apache.kafka.server.common.MetadataVersion
@@ -119,13 +119,11 @@ class AbstractPartitionTest {
                                         isLeader: Boolean): Partition = {
     partition.createLogIfNotExists(isNew = false, isFutureReplica = false, offsetCheckpoints, None)
 
-    val controllerEpoch = 0
     val replicas = java.util.List.of[Integer](brokerId, remoteReplicaId)
     val isr = replicas
 
     if (isLeader) {
-      assertTrue(partition.makeLeader(new LeaderAndIsrRequest.PartitionState()
-        .setControllerEpoch(controllerEpoch)
+      assertTrue(partition.makeLeader(new PartitionState()
         .setLeader(brokerId)
         .setLeaderEpoch(leaderEpoch)
         .setIsr(isr)
@@ -134,8 +132,7 @@ class AbstractPartitionTest {
         .setIsNew(true), offsetCheckpoints, None), "Expected become leader transition to succeed")
       assertEquals(leaderEpoch, partition.getLeaderEpoch)
     } else {
-      assertTrue(partition.makeFollower(new LeaderAndIsrRequest.PartitionState()
-        .setControllerEpoch(controllerEpoch)
+      assertTrue(partition.makeFollower(new PartitionState()
         .setLeader(remoteReplicaId)
         .setLeaderEpoch(leaderEpoch)
         .setIsr(isr)
diff --git a/core/src/test/scala/unit/kafka/cluster/AssignmentStateTest.scala b/core/src/test/scala/unit/kafka/cluster/AssignmentStateTest.scala
index fe8f67eed5..0f995661d3 100644
--- a/core/src/test/scala/unit/kafka/cluster/AssignmentStateTest.scala
+++ b/core/src/test/scala/unit/kafka/cluster/AssignmentStateTest.scala
@@ -16,7 +16,7 @@
   */
 package kafka.cluster
 
-import org.apache.kafka.common.requests.LeaderAndIsrRequest
+import org.apache.kafka.common.PartitionState
 import org.junit.jupiter.api.Assertions.{assertEquals, assertFalse, assertTrue}
 import org.junit.jupiter.params.ParameterizedTest
 import org.junit.jupiter.params.provider.{Arguments, MethodSource}
@@ -89,10 +89,7 @@ class AssignmentStateTest extends AbstractPartitionTest {
   def testPartitionAssignmentStatus(isr: util.List[Integer], replicas: util.List[Integer],
                                     adding: util.List[Integer], removing: util.List[Integer],
                                     original: util.List[Int], isUnderReplicated: Boolean): Unit = {
-    val controllerEpoch = 3
-
-    val leaderState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(controllerEpoch)
+    val leaderState = new PartitionState()
       .setLeader(brokerId)
       .setLeaderEpoch(6)
       .setIsr(isr)
diff --git a/core/src/test/scala/unit/kafka/cluster/PartitionLockTest.scala b/core/src/test/scala/unit/kafka/cluster/PartitionLockTest.scala
index 81a72b2732..7e4800ce5b 100644
--- a/core/src/test/scala/unit/kafka/cluster/PartitionLockTest.scala
+++ b/core/src/test/scala/unit/kafka/cluster/PartitionLockTest.scala
@@ -27,7 +27,8 @@ import kafka.server._
 import kafka.utils._
 import org.apache.kafka.common.config.TopicConfig
 import org.apache.kafka.common.record.{MemoryRecords, SimpleRecord}
-import org.apache.kafka.common.requests.{FetchRequest, LeaderAndIsrRequest}
+import org.apache.kafka.common.requests.FetchRequest
+import org.apache.kafka.common.PartitionState
 import org.apache.kafka.common.utils.Utils
 import org.apache.kafka.common.{TopicPartition, Uuid}
 import org.apache.kafka.coordinator.transaction.TransactionLogConfig
@@ -142,8 +143,7 @@ class PartitionLockTest extends Logging {
     val replicaToCheck = 3
     val firstReplicaSet = util.List.of[Integer](3, 4, 5)
     val secondReplicaSet = util.List.of[Integer](1, 2, 3)
-    def partitionState(replicas: util.List[Integer]) = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(1)
+    def partitionState(replicas: util.List[Integer]) = new PartitionState()
       .setLeader(replicas.get(0))
       .setLeaderEpoch(1)
       .setIsr(replicas)
@@ -344,13 +344,11 @@ class PartitionLockTest extends Logging {
 
     partition.createLogIfNotExists(isNew = false, isFutureReplica = false, offsetCheckpoints, Some(topicId))
 
-    val controllerEpoch = 0
     val replicas = (0 to numReplicaFetchers).map(i => Integer.valueOf(brokerId + i)).toList.asJava
     val isr = replicas
     replicas.forEach(replicaId => when(metadataCache.getAliveBrokerEpoch(replicaId)).thenReturn(Optional.of(1L)))
 
-    assertTrue(partition.makeLeader(new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(controllerEpoch)
+    assertTrue(partition.makeLeader(new PartitionState()
       .setLeader(brokerId)
       .setLeaderEpoch(leaderEpoch)
       .setIsr(isr)
diff --git a/core/src/test/scala/unit/kafka/cluster/PartitionTest.scala b/core/src/test/scala/unit/kafka/cluster/PartitionTest.scala
index 91fff3408e..e309943427 100644
--- a/core/src/test/scala/unit/kafka/cluster/PartitionTest.scala
+++ b/core/src/test/scala/unit/kafka/cluster/PartitionTest.scala
@@ -26,7 +26,8 @@ import org.apache.kafka.common.message.{AlterPartitionResponseData, FetchRespons
 import org.apache.kafka.common.protocol.{ApiKeys, Errors}
 import org.apache.kafka.common.record.FileRecords.TimestampAndOffset
 import org.apache.kafka.common.record._
-import org.apache.kafka.common.requests.{AlterPartitionResponse, FetchRequest, LeaderAndIsrRequest, ListOffsetsRequest, RequestHeader}
+import org.apache.kafka.common.requests.{AlterPartitionResponse, FetchRequest, ListOffsetsRequest, RequestHeader}
+import org.apache.kafka.common.{PartitionState => JPartitionState}
 import org.apache.kafka.common.utils.Time
 import org.apache.kafka.common.{DirectoryId, IsolationLevel, TopicPartition, Uuid}
 import org.apache.kafka.metadata.MetadataCache
@@ -307,7 +308,6 @@ class PartitionTest extends AbstractPartitionTest {
 
   @Test
   def testReplicaFetchToFollower(): Unit = {
-    val controllerEpoch = 3
     val followerId = brokerId + 1
     val leaderId = brokerId + 2
     val replicas = util.List.of[Integer](brokerId, followerId, leaderId)
@@ -315,8 +315,7 @@ class PartitionTest extends AbstractPartitionTest {
     val leaderEpoch = 8
     val partitionEpoch = 1
 
-    assertTrue(partition.makeFollower(new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(controllerEpoch)
+    assertTrue(partition.makeFollower(new JPartitionState()
       .setLeader(leaderId)
       .setLeaderEpoch(leaderEpoch)
       .setIsr(isr)
@@ -348,7 +347,6 @@ class PartitionTest extends AbstractPartitionTest {
 
   @Test
   def testFetchFromUnrecognizedFollower(): Unit = {
-    val controllerEpoch = 3
     val leader = brokerId
     val validReplica = brokerId + 1
     val addingReplica1 = brokerId + 2
@@ -359,8 +357,7 @@ class PartitionTest extends AbstractPartitionTest {
     val partitionEpoch = 1
     addBrokerEpochToMockMetadataCache(metadataCache, util.List.of[Integer](leader, addingReplica1, addingReplica2))
 
-    assertTrue(partition.makeLeader(new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(controllerEpoch)
+    assertTrue(partition.makeLeader(new JPartitionState()
       .setLeader(leader)
       .setLeaderEpoch(leaderEpoch)
       .setIsr(isr)
@@ -395,8 +392,7 @@ class PartitionTest extends AbstractPartitionTest {
     val newPartitionEpoch = partitionEpoch + 1
     val addingReplicas = util.List.of[Integer](addingReplica1, addingReplica2)
 
-    assertFalse(partition.makeLeader(new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(controllerEpoch)
+    assertFalse(partition.makeLeader(new JPartitionState()
       .setLeader(leader)
       .setLeaderEpoch(leaderEpoch)
       .setIsr(isr)
@@ -483,8 +479,7 @@ class PartitionTest extends AbstractPartitionTest {
     }
 
     partition.createLogIfNotExists(isNew = true, isFutureReplica = false, offsetCheckpoints, None)
-    var partitionState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(0)
+    var partitionState = new JPartitionState()
       .setLeader(2)
       .setLeaderEpoch(prevLeaderEpoch)
       .setIsr(util.List.of[Integer](0, 1, 2, brokerId))
@@ -509,8 +504,7 @@ class PartitionTest extends AbstractPartitionTest {
     appendThread.start()
     TestUtils.waitUntilTrue(() => appendSemaphore.hasQueuedThreads, "follower log append is not called.")
 
-    partitionState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(0)
+    partitionState = new JPartitionState()
       .setLeader(2)
       .setLeaderEpoch(prevLeaderEpoch + 1)
       .setIsr(util.List.of[Integer](0, 1, 2, brokerId))
@@ -802,7 +796,6 @@ class PartitionTest extends AbstractPartitionTest {
     */
   @Test
   def testMonotonicOffsetsAfterLeaderChange(): Unit = {
-    val controllerEpoch = 3
     val leader = brokerId
     val follower1 = brokerId + 1
     val follower2 = brokerId + 2
@@ -817,8 +810,7 @@ class PartitionTest extends AbstractPartitionTest {
       new SimpleRecord(21,"k5".getBytes, "v3".getBytes)))
     addBrokerEpochToMockMetadataCache(metadataCache, replicas)
 
-    val leaderState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(controllerEpoch)
+    val leaderState = new JPartitionState()
       .setLeader(leader)
       .setLeaderEpoch(leaderEpoch)
       .setIsr(isr)
@@ -888,8 +880,7 @@ class PartitionTest extends AbstractPartitionTest {
     assertEquals(Right(None), fetchOffsetsForTimestamp(30, Some(IsolationLevel.READ_UNCOMMITTED)))
 
     // Make into a follower
-    val followerState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(controllerEpoch)
+    val followerState = new JPartitionState()
       .setLeader(follower2)
       .setLeaderEpoch(leaderEpoch + 1)
       .setIsr(isr)
@@ -900,8 +891,7 @@ class PartitionTest extends AbstractPartitionTest {
     assertTrue(partition.makeFollower(followerState, offsetCheckpoints, None))
 
     // Back to leader, this resets the startLogOffset for this epoch (to 2), we're now in the fault condition
-    val newLeaderState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(controllerEpoch)
+    val newLeaderState = new JPartitionState()
       .setLeader(leader)
       .setLeaderEpoch(leaderEpoch + 2)
       .setIsr(isr)
@@ -982,8 +972,7 @@ class PartitionTest extends AbstractPartitionTest {
     val epoch = 1
 
     // Start off as follower
-    val partitionState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(0)
+    val partitionState = new JPartitionState()
       .setLeader(1)
       .setLeaderEpoch(epoch)
       .setIsr(util.List.of[Integer](0, 1, 2, brokerId))
@@ -1054,15 +1043,13 @@ class PartitionTest extends AbstractPartitionTest {
 
   @Test
   def testListOffsetIsolationLevels(): Unit = {
-    val controllerEpoch = 0
     val leaderEpoch = 5
     val replicas = util.List.of[Integer](brokerId, brokerId + 1)
     val isr = replicas
 
     partition.createLogIfNotExists(isNew = false, isFutureReplica = false, offsetCheckpoints, None)
 
-    assertTrue(partition.makeLeader(new LeaderAndIsrRequest.PartitionState()
-        .setControllerEpoch(controllerEpoch)
+    assertTrue(partition.makeLeader(new JPartitionState()
         .setLeader(brokerId)
         .setLeaderEpoch(leaderEpoch)
         .setIsr(isr)
@@ -1143,8 +1130,7 @@ class PartitionTest extends AbstractPartitionTest {
   @Test
   def testMakeFollowerWithNoLeaderIdChange(): Unit = {
     // Start off as follower
-    var partitionState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(0)
+    var partitionState = new JPartitionState()
       .setLeader(1)
       .setLeaderEpoch(1)
       .setIsr(util.List.of[Integer](0, 1, 2, brokerId))
@@ -1154,8 +1140,7 @@ class PartitionTest extends AbstractPartitionTest {
     partition.makeFollower(partitionState, offsetCheckpoints, None)
 
     // Request with same leader and epoch increases by only 1, do become-follower steps
-    partitionState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(0)
+    partitionState = new JPartitionState()
       .setLeader(1)
       .setLeaderEpoch(4)
       .setIsr(util.List.of[Integer](0, 1, 2, brokerId))
@@ -1165,8 +1150,7 @@ class PartitionTest extends AbstractPartitionTest {
     assertTrue(partition.makeFollower(partitionState, offsetCheckpoints, None))
 
     // Request with same leader and same epoch, skip become-follower steps
-    partitionState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(0)
+    partitionState = new JPartitionState()
       .setLeader(1)
       .setLeaderEpoch(4)
       .setIsr(util.List.of[Integer](0, 1, 2, brokerId))
@@ -1177,7 +1161,6 @@ class PartitionTest extends AbstractPartitionTest {
 
   @Test
   def testFollowerDoesNotJoinISRUntilCaughtUpToOffsetWithinCurrentLeaderEpoch(): Unit = {
-    val controllerEpoch = 3
     val leader = brokerId
     val follower1 = brokerId + 1
     val follower2 = brokerId + 2
@@ -1193,8 +1176,7 @@ class PartitionTest extends AbstractPartitionTest {
                                                   new SimpleRecord("k7".getBytes, "v2".getBytes)))
     addBrokerEpochToMockMetadataCache(metadataCache, replicas)
 
-    val leaderState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(controllerEpoch)
+    val leaderState = new JPartitionState()
       .setLeader(leader)
       .setLeaderEpoch(leaderEpoch)
       .setIsr(isr)
@@ -1220,8 +1202,7 @@ class PartitionTest extends AbstractPartitionTest {
     assertEquals(lastOffsetOfFirstBatch + 1, partition.log.get.highWatermark, "Expected leader's HW")
 
     // current leader becomes follower and then leader again (without any new records appended)
-    val followerState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(controllerEpoch)
+    val followerState = new JPartitionState()
       .setLeader(follower2)
       .setLeaderEpoch(leaderEpoch + 1)
       .setIsr(isr)
@@ -1230,8 +1211,7 @@ class PartitionTest extends AbstractPartitionTest {
       .setIsNew(false)
     partition.makeFollower(followerState, offsetCheckpoints, None)
 
-    val newLeaderState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(controllerEpoch)
+    val newLeaderState = new JPartitionState()
       .setLeader(leader)
       .setLeaderEpoch(leaderEpoch + 2)
       .setIsr(isr)
@@ -1302,7 +1282,6 @@ class PartitionTest extends AbstractPartitionTest {
     */
   @Test
   def testAtMinIsr(): Unit = {
-    val controllerEpoch = 3
     val leader = brokerId
     val follower1 = brokerId + 1
     val follower2 = brokerId + 2
@@ -1312,8 +1291,7 @@ class PartitionTest extends AbstractPartitionTest {
 
     assertFalse(partition.isAtMinIsr)
     // Make isr set to only have leader to trigger AtMinIsr (default min isr config is 1)
-    val leaderState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(controllerEpoch)
+    val leaderState = new JPartitionState()
       .setLeader(leader)
       .setLeaderEpoch(leaderEpoch)
       .setIsr(isr)
@@ -1339,8 +1317,7 @@ class PartitionTest extends AbstractPartitionTest {
       alterPartitionManager)
     partition.createLogIfNotExists(isNew = false, isFutureReplica = false, offsetCheckpoints, topicId = None)
     partition.makeLeader(
-      new LeaderAndIsrRequest.PartitionState()
-        .setControllerEpoch(0)
+      new JPartitionState()
         .setLeader(brokerId)
         .setLeaderEpoch(0)
         .setIsr(util.List.of[Integer](brokerId, brokerId + 1))
@@ -1351,8 +1328,7 @@ class PartitionTest extends AbstractPartitionTest {
       topicId = None)
     assertFalse(partition.isUnderMinIsr)
 
-    val LeaderState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(0)
+    val LeaderState = new JPartitionState()
       .setLeader(brokerId)
       .setLeaderEpoch(1)
       .setIsr(util.List.of[Integer](brokerId))
@@ -1369,7 +1345,6 @@ class PartitionTest extends AbstractPartitionTest {
     val log = logManager.getOrCreateLog(topicPartition, topicId = Optional.empty)
     seedLogData(log, numRecords = 6, leaderEpoch = 4)
 
-    val controllerEpoch = 0
     val leaderEpoch = 5
     val remoteBrokerId = brokerId + 1
     val replicas = util.List.of[Integer](brokerId, remoteBrokerId)
@@ -1380,8 +1355,7 @@ class PartitionTest extends AbstractPartitionTest {
 
     val initializeTimeMs = time.milliseconds()
     assertTrue(partition.makeLeader(
-        new LeaderAndIsrRequest.PartitionState()
-          .setControllerEpoch(controllerEpoch)
+        new JPartitionState()
           .setLeader(brokerId)
           .setLeaderEpoch(leaderEpoch)
           .setIsr(isr)
@@ -1433,7 +1407,6 @@ class PartitionTest extends AbstractPartitionTest {
     val log = logManager.getOrCreateLog(topicPartition, topicId = Optional.empty)
     seedLogData(log, numRecords = 6, leaderEpoch = 4)
 
-    val controllerEpoch = 0
     val leaderEpoch = 5
     val remoteBrokerId = brokerId + 1
     val replicas = util.List.of[Integer](brokerId, remoteBrokerId)
@@ -1443,8 +1416,7 @@ class PartitionTest extends AbstractPartitionTest {
 
     val initializeTimeMs = time.milliseconds()
     assertTrue(partition.makeLeader(
-      new LeaderAndIsrRequest.PartitionState()
-        .setControllerEpoch(controllerEpoch)
+      new JPartitionState()
         .setLeader(brokerId)
         .setLeaderEpoch(leaderEpoch)
         .setIsr(util.List.of[Integer](brokerId))
@@ -1469,7 +1441,6 @@ class PartitionTest extends AbstractPartitionTest {
     val log = logManager.getOrCreateLog(topicPartition, topicId = topicId.toJava)
     seedLogData(log, numRecords = 10, leaderEpoch = 4)
 
-    val controllerEpoch = 0
     val leaderEpoch = 5
     val remoteBrokerId = brokerId + 1
     val replicas = util.List.of[Integer](brokerId, remoteBrokerId)
@@ -1478,8 +1449,7 @@ class PartitionTest extends AbstractPartitionTest {
 
     partition.createLogIfNotExists(isNew = false, isFutureReplica = false, offsetCheckpoints, None)
     assertTrue(partition.makeLeader(
-        new LeaderAndIsrRequest.PartitionState()
-          .setControllerEpoch(controllerEpoch)
+        new JPartitionState()
           .setLeader(brokerId)
           .setLeaderEpoch(leaderEpoch)
           .setIsr(isr)
@@ -1521,7 +1491,6 @@ class PartitionTest extends AbstractPartitionTest {
     val log = logManager.getOrCreateLog(topicPartition, topicId = topicId.toJava)
     seedLogData(log, numRecords = 10, leaderEpoch = 4)
 
-    val controllerEpoch = 0
     val leaderEpoch = 5
     val remoteBrokerId = brokerId + 1
     val replicas = util.List.of[Integer](brokerId, remoteBrokerId)
@@ -1530,8 +1499,7 @@ class PartitionTest extends AbstractPartitionTest {
 
     partition.createLogIfNotExists(isNew = false, isFutureReplica = false, offsetCheckpoints, None)
     assertTrue(partition.makeLeader(
-        new LeaderAndIsrRequest.PartitionState()
-          .setControllerEpoch(controllerEpoch)
+        new JPartitionState()
           .setLeader(brokerId)
           .setLeaderEpoch(leaderEpoch)
           .setIsr(isr)
@@ -1585,7 +1553,6 @@ class PartitionTest extends AbstractPartitionTest {
     val log = logManager.getOrCreateLog(topicPartition, topicId = topicId.toJava)
     seedLogData(log, numRecords = 10, leaderEpoch = 4)
 
-    val controllerEpoch = 0
     val leaderEpoch = 5
     val remoteBrokerId = brokerId + 1
     val replicas = util.List.of[Integer](brokerId, remoteBrokerId)
@@ -1594,8 +1561,7 @@ class PartitionTest extends AbstractPartitionTest {
 
     partition.createLogIfNotExists(isNew = false, isFutureReplica = false, offsetCheckpoints, None)
     assertTrue(partition.makeLeader(
-        new LeaderAndIsrRequest.PartitionState()
-          .setControllerEpoch(controllerEpoch)
+        new JPartitionState()
           .setLeader(brokerId)
           .setLeaderEpoch(leaderEpoch)
           .setIsr(isr)
@@ -1641,7 +1607,6 @@ class PartitionTest extends AbstractPartitionTest {
     val log = logManager.getOrCreateLog(topicPartition, topicId = Optional.empty)
     seedLogData(log, numRecords = 10, leaderEpoch = 4)
 
-    val controllerEpoch = 0
     val leaderEpoch = 5
     val remoteBrokerId = brokerId + 1
     val replicas = util.List.of[Integer](brokerId, remoteBrokerId)
@@ -1665,8 +1630,7 @@ class PartitionTest extends AbstractPartitionTest {
     partition.createLogIfNotExists(isNew = false, isFutureReplica = false, offsetCheckpoints, None)
     assertTrue(
       partition.makeLeader(
-        new LeaderAndIsrRequest.PartitionState()
-          .setControllerEpoch(controllerEpoch)
+        new JPartitionState()
           .setLeader(brokerId)
           .setLeaderEpoch(leaderEpoch)
           .setIsr(replicas)
@@ -1707,8 +1671,7 @@ class PartitionTest extends AbstractPartitionTest {
     // Controller shrinks the ISR after
     assertFalse(
       partition.makeLeader(
-        new LeaderAndIsrRequest.PartitionState()
-          .setControllerEpoch(controllerEpoch)
+        new JPartitionState()
           .setLeader(brokerId)
           .setLeaderEpoch(leaderEpoch)
           .setIsr(shrinkedIsr)
@@ -1740,7 +1703,6 @@ class PartitionTest extends AbstractPartitionTest {
     val log = logManager.getOrCreateLog(topicPartition, topicId = topicId.toJava)
     seedLogData(log, numRecords = 10, leaderEpoch = 4)
 
-    val controllerEpoch = 0
     val leaderEpoch = 5
     val remoteBrokerId = brokerId + 1
     val replicas = util.List.of[Integer](brokerId, remoteBrokerId)
@@ -1769,8 +1731,7 @@ class PartitionTest extends AbstractPartitionTest {
 
     partition.createLogIfNotExists(isNew = false, isFutureReplica = false, offsetCheckpoints, None)
     assertTrue(partition.makeLeader(
-      new LeaderAndIsrRequest.PartitionState()
-        .setControllerEpoch(controllerEpoch)
+      new JPartitionState()
         .setLeader(brokerId)
         .setLeaderEpoch(leaderEpoch)
         .setIsr(isr)
@@ -1847,7 +1808,6 @@ class PartitionTest extends AbstractPartitionTest {
     val log = logManager.getOrCreateLog(topicPartition, topicId = topicId.toJava)
     seedLogData(log, numRecords = 10, leaderEpoch = 4)
 
-    val controllerEpoch = 0
     val leaderEpoch = 5
     val remoteBrokerId1 = brokerId + 1
     val remoteBrokerId2 = brokerId + 2
@@ -1875,8 +1835,7 @@ class PartitionTest extends AbstractPartitionTest {
 
     partition.createLogIfNotExists(isNew = false, isFutureReplica = false, offsetCheckpoints, None)
     assertTrue(partition.makeLeader(
-      new LeaderAndIsrRequest.PartitionState()
-        .setControllerEpoch(controllerEpoch)
+      new JPartitionState()
         .setLeader(brokerId)
         .setLeaderEpoch(leaderEpoch)
         .setIsr(isr)
@@ -1942,7 +1901,6 @@ class PartitionTest extends AbstractPartitionTest {
   def testFenceFollowerFetchWithStaleBrokerEpoch(): Unit = {
     val log = logManager.getOrCreateLog(topicPartition, topicId = Optional.empty)
     seedLogData(log, numRecords = 10, leaderEpoch = 4)
-    val controllerEpoch = 0
     val leaderEpoch = 5
     val remoteBrokerId1 = brokerId + 1
     val replicas = util.List.of[Integer](brokerId, remoteBrokerId1)
@@ -1965,8 +1923,7 @@ class PartitionTest extends AbstractPartitionTest {
 
     partition.createLogIfNotExists(isNew = false, isFutureReplica = false, offsetCheckpoints, None)
     assertTrue(partition.makeLeader(
-      new LeaderAndIsrRequest.PartitionState()
-        .setControllerEpoch(controllerEpoch)
+      new JPartitionState()
         .setLeader(brokerId)
         .setLeaderEpoch(leaderEpoch)
         .setIsr(isr)
@@ -2007,7 +1964,6 @@ class PartitionTest extends AbstractPartitionTest {
     val log = logManager.getOrCreateLog(topicPartition, topicId = topicId.toJava)
     seedLogData(log, numRecords = 10, leaderEpoch = 4)
 
-    val controllerEpoch = 0
     val leaderEpoch = 5
     val remoteBrokerId = brokerId + 1
     val replicas = util.List.of[Integer](brokerId, remoteBrokerId)
@@ -2029,8 +1985,7 @@ class PartitionTest extends AbstractPartitionTest {
 
     partition.createLogIfNotExists(isNew = false, isFutureReplica = false, offsetCheckpoints, None)
     assertTrue(partition.makeLeader(
-      new LeaderAndIsrRequest.PartitionState()
-        .setControllerEpoch(controllerEpoch)
+      new JPartitionState()
         .setLeader(brokerId)
         .setLeaderEpoch(leaderEpoch)
         .setIsr(isr)
@@ -2105,7 +2060,6 @@ class PartitionTest extends AbstractPartitionTest {
     val log = logManager.getOrCreateLog(topicPartition, topicId = Optional.empty)
     seedLogData(log, numRecords = 10, leaderEpoch = 4)
 
-    val controllerEpoch = 0
     val leaderEpoch = 5
     val remoteBrokerId = brokerId + 1
     val replicas = util.List.of[Integer](brokerId, remoteBrokerId)
@@ -2114,7 +2068,6 @@ class PartitionTest extends AbstractPartitionTest {
 
     assertTrue(makeLeader(
       topicId = Some(topicId),
-      controllerEpoch = controllerEpoch,
       leaderEpoch = leaderEpoch,
       isr = isr,
       replicas = replicas,
@@ -2158,7 +2111,6 @@ class PartitionTest extends AbstractPartitionTest {
     val log = logManager.getOrCreateLog(topicPartition, topicId = topicId.toJava)
     seedLogData(log, numRecords = 10, leaderEpoch = 4)
 
-    val controllerEpoch = 0
     val leaderEpoch = 5
     val remoteBrokerId1 = brokerId + 1
     val remoteBrokerId2 = brokerId + 2
@@ -2184,8 +2136,7 @@ class PartitionTest extends AbstractPartitionTest {
     partition.createLogIfNotExists(isNew = false, isFutureReplica = false, offsetCheckpoints, None)
 
     assertTrue(partition.makeLeader(
-      new LeaderAndIsrRequest.PartitionState()
-        .setControllerEpoch(controllerEpoch)
+      new JPartitionState()
         .setLeader(brokerId)
         .setLeaderEpoch(leaderEpoch)
         .setIsr(isr)
@@ -2241,7 +2192,6 @@ class PartitionTest extends AbstractPartitionTest {
     val log = logManager.getOrCreateLog(topicPartition, topicId = topicId.toJava)
     seedLogData(log, numRecords = 10, leaderEpoch = 4)
 
-    val controllerEpoch = 0
     val leaderEpoch = 5
     val remoteBrokerId1 = brokerId + 1
     val remoteBrokerId2 = brokerId + 2
@@ -2265,8 +2215,7 @@ class PartitionTest extends AbstractPartitionTest {
     partition.createLogIfNotExists(isNew = false, isFutureReplica = false, offsetCheckpoints, None)
 
     assertTrue(partition.makeLeader(
-      new LeaderAndIsrRequest.PartitionState()
-        .setControllerEpoch(controllerEpoch)
+      new JPartitionState()
         .setLeader(brokerId)
         .setLeaderEpoch(leaderEpoch)
         .setIsr(isr)
@@ -2298,7 +2247,6 @@ class PartitionTest extends AbstractPartitionTest {
     val log = logManager.getOrCreateLog(topicPartition, topicId = topicId.toJava)
     seedLogData(log, numRecords = 10, leaderEpoch = 4)
 
-    val controllerEpoch = 0
     val leaderEpoch = 5
     val remoteBrokerId = brokerId + 1
     val replicas = util.List.of[Integer](brokerId, remoteBrokerId)
@@ -2307,7 +2255,6 @@ class PartitionTest extends AbstractPartitionTest {
 
     assertTrue(makeLeader(
       topicId = topicId,
-      controllerEpoch = controllerEpoch,
       leaderEpoch = leaderEpoch,
       isr = isr,
       replicas = replicas,
@@ -2330,7 +2277,6 @@ class PartitionTest extends AbstractPartitionTest {
     // Become leader again, reset the ISR state
     assertFalse(makeLeader(
       topicId = topicId,
-      controllerEpoch = controllerEpoch,
       leaderEpoch = leaderEpoch,
       isr = isr,
       replicas = replicas,
@@ -2356,7 +2302,6 @@ class PartitionTest extends AbstractPartitionTest {
     val log = logManager.getOrCreateLog(topicPartition, topicId = topicId.toJava)
     seedLogData(log, numRecords = 10, leaderEpoch = 4)
 
-    val controllerEpoch = 0
     val leaderEpoch = 5
     val remoteBrokerId = brokerId + 1
     val replicas = util.List.of[Integer](brokerId, remoteBrokerId)
@@ -2366,7 +2311,6 @@ class PartitionTest extends AbstractPartitionTest {
 
     assertTrue(makeLeader(
       topicId = topicId,
-      controllerEpoch = controllerEpoch,
       leaderEpoch = leaderEpoch,
       isr = isr,
       replicas = replicas,
@@ -2416,7 +2360,6 @@ class PartitionTest extends AbstractPartitionTest {
     val log = logManager.getOrCreateLog(topicPartition, topicId = topicId.toJava)
     seedLogData(log, numRecords = 10, leaderEpoch = 4)
 
-    val controllerEpoch = 0
     val leaderEpoch = 5
     val remoteBrokerId = brokerId + 1
     val replicas = util.List.of[Integer](brokerId, remoteBrokerId)
@@ -2426,7 +2369,6 @@ class PartitionTest extends AbstractPartitionTest {
 
     assertTrue(makeLeader(
       topicId = topicId,
-      controllerEpoch = controllerEpoch,
       leaderEpoch = leaderEpoch,
       isr = isr,
       replicas = replicas,
@@ -2464,7 +2406,6 @@ class PartitionTest extends AbstractPartitionTest {
     val log = logManager.getOrCreateLog(topicPartition, topicId = topicId.toJava)
     seedLogData(log, numRecords = 10, leaderEpoch = 4)
 
-    val controllerEpoch = 0
     val leaderEpoch = 5
     val remoteBrokerId = brokerId + 1
     val replicas = util.List.of[Integer](brokerId, remoteBrokerId)
@@ -2473,7 +2414,6 @@ class PartitionTest extends AbstractPartitionTest {
 
     assertTrue(makeLeader(
       topicId = topicId,
-      controllerEpoch = controllerEpoch,
       leaderEpoch = leaderEpoch,
       isr = isr,
       replicas = replicas,
@@ -2551,7 +2491,6 @@ class PartitionTest extends AbstractPartitionTest {
     val log = logManager.getOrCreateLog(topicPartition, topicId = topicId.toJava)
     seedLogData(log, numRecords = 10, leaderEpoch = 4)
 
-    val controllerEpoch = 0
     val leaderEpoch = 5
     val remoteBrokerId = brokerId + 1
     val replicas = util.List.of[Integer](brokerId, remoteBrokerId)
@@ -2560,7 +2499,6 @@ class PartitionTest extends AbstractPartitionTest {
 
     assertTrue(makeLeader(
       topicId = topicId,
-      controllerEpoch = controllerEpoch,
       leaderEpoch = leaderEpoch,
       isr = isr,
       replicas = replicas,
@@ -2643,7 +2581,6 @@ class PartitionTest extends AbstractPartitionTest {
     val log = logManager.getOrCreateLog(topicPartition, topicId = topicId.toJava)
     seedLogData(log, numRecords = 10, leaderEpoch = 4)
 
-    val controllerEpoch = 0
     val leaderEpoch = 5
     val follower1 = brokerId + 1
     val follower2 = brokerId + 2
@@ -2675,7 +2612,6 @@ class PartitionTest extends AbstractPartitionTest {
 
     assertTrue(makeLeader(
       topicId = topicId,
-      controllerEpoch,
       leaderEpoch,
       isr,
       replicas,
@@ -2700,7 +2636,6 @@ class PartitionTest extends AbstractPartitionTest {
     val log = logManager.getOrCreateLog(topicPartition, topicId = topicId.toJava)
     seedLogData(log, numRecords = 10, leaderEpoch = 4)
 
-    val controllerEpoch = 0
     val leaderEpoch = 5
     val follower1 = brokerId + 1
     val follower2 = brokerId + 2
@@ -2713,7 +2648,6 @@ class PartitionTest extends AbstractPartitionTest {
 
     assertTrue(makeLeader(
       topicId = topicId,
-      controllerEpoch = controllerEpoch,
       leaderEpoch = leaderEpoch,
       isr = isr,
       replicas = replicas,
@@ -2744,10 +2678,8 @@ class PartitionTest extends AbstractPartitionTest {
     when(offsetCheckpoints.fetch(logDir1.getAbsolutePath, topicPartition))
       .thenReturn(Optional.of(long2Long(4L)))
 
-    val controllerEpoch = 3
     val replicas = util.List.of[Integer](brokerId, brokerId + 1)
-    val leaderState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(controllerEpoch)
+    val leaderState = new JPartitionState()
       .setLeader(brokerId)
       .setLeaderEpoch(6)
       .setIsr(replicas)
@@ -2760,12 +2692,10 @@ class PartitionTest extends AbstractPartitionTest {
 
   @Test
   def testTopicIdAndPartitionMetadataFileForLeader(): Unit = {
-    val controllerEpoch = 3
     val leaderEpoch = 5
     val topicId = Uuid.randomUuid()
     val replicas = util.List.of[Integer](brokerId, brokerId + 1)
-    val leaderState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(controllerEpoch)
+    val leaderState = new JPartitionState()
       .setLeader(brokerId)
       .setLeaderEpoch(leaderEpoch)
       .setIsr(replicas)
@@ -2804,12 +2734,10 @@ class PartitionTest extends AbstractPartitionTest {
 
   @Test
   def testTopicIdAndPartitionMetadataFileForFollower(): Unit = {
-    val controllerEpoch = 3
     val leaderEpoch = 5
     val topicId = Uuid.randomUuid()
     val replicas = util.List.of[Integer](brokerId, brokerId + 1)
-    val leaderState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(controllerEpoch)
+    val leaderState = new JPartitionState()
       .setLeader(brokerId)
       .setLeaderEpoch(leaderEpoch)
       .setIsr(replicas)
@@ -2881,12 +2809,10 @@ class PartitionTest extends AbstractPartitionTest {
 
   @Test
   def testUnderReplicatedPartitionsCorrectSemantics(): Unit = {
-    val controllerEpoch = 3
     val replicas = util.List.of[Integer](brokerId, brokerId + 1, brokerId + 2)
     val isr = util.List.of[Integer](brokerId, brokerId + 1)
 
-    var leaderState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(controllerEpoch)
+    var leaderState = new JPartitionState()
       .setLeader(brokerId)
       .setLeaderEpoch(6)
       .setIsr(isr)
@@ -3081,7 +3007,6 @@ class PartitionTest extends AbstractPartitionTest {
 
   @Test
   def testDoNotResetReplicaStateIfLeaderEpochIsNotBumped(): Unit = {
-    val controllerEpoch = 3
     val leaderId = brokerId
     val followerId = brokerId + 1
     val replicas = util.List.of[Integer](leaderId, followerId)
@@ -3089,8 +3014,7 @@ class PartitionTest extends AbstractPartitionTest {
     val topicId = Uuid.randomUuid()
     addBrokerEpochToMockMetadataCache(metadataCache, replicas)
 
-    val initialLeaderState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(controllerEpoch)
+    val initialLeaderState = new JPartitionState()
       .setLeader(leaderId)
       .setLeaderEpoch(leaderEpoch)
       .setIsr(util.List.of[Integer](leaderId))
@@ -3122,8 +3046,7 @@ class PartitionTest extends AbstractPartitionTest {
     // makeLeader is called again with the same leader epoch but with
     // a newer partition epoch. This can happen in KRaft when a partition
     // is reassigned. The leader epoch is not bumped when we add replicas.
-    val updatedLeaderState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(controllerEpoch)
+    val updatedLeaderState = new JPartitionState()
       .setLeader(leaderId)
       .setLeaderEpoch(leaderEpoch)
       .setIsr(util.List.of[Integer](leaderId))
@@ -3146,15 +3069,13 @@ class PartitionTest extends AbstractPartitionTest {
 
   @Test
   def testDoNotUpdateEpochStartOffsetIfLeaderEpochIsNotBumped(): Unit = {
-    val controllerEpoch = 3
     val leaderId = brokerId
     val followerId = brokerId + 1
     val replicas = util.List.of[Integer](leaderId, followerId)
     val leaderEpoch = 8
     val topicId = Uuid.randomUuid()
 
-    val initialLeaderState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(controllerEpoch)
+    val initialLeaderState = new JPartitionState()
       .setLeader(leaderId)
       .setLeaderEpoch(leaderEpoch)
       .setIsr(util.List.of[Integer](leaderId))
@@ -3179,8 +3100,7 @@ class PartitionTest extends AbstractPartitionTest {
 
     // makeLeader is called again with the same leader epoch but with
     // a newer partition epoch.
-    val updatedLeaderState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(controllerEpoch)
+    val updatedLeaderState = new JPartitionState()
       .setLeader(leaderId)
       .setLeaderEpoch(leaderEpoch)
       .setIsr(util.List.of[Integer](leaderId))
@@ -3198,14 +3118,12 @@ class PartitionTest extends AbstractPartitionTest {
 
   @Test
   def testIgnoreLeaderPartitionStateChangeWithOlderPartitionEpoch(): Unit = {
-    val controllerEpoch = 3
     val leaderId = brokerId
     val replicas = util.List.of[Integer](leaderId)
     val leaderEpoch = 8
     val topicId = Uuid.randomUuid()
 
-    val initialLeaderState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(controllerEpoch)
+    val initialLeaderState = new JPartitionState()
       .setLeader(leaderId)
       .setLeaderEpoch(leaderEpoch)
       .setIsr(util.List.of[Integer](leaderId))
@@ -3219,8 +3137,7 @@ class PartitionTest extends AbstractPartitionTest {
 
     // makeLeader is called again with the same leader epoch but with
     // a older partition epoch.
-    val updatedLeaderState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(controllerEpoch)
+    val updatedLeaderState = new JPartitionState()
       .setLeader(leaderId)
       .setLeaderEpoch(leaderEpoch)
       .setIsr(util.List.of[Integer](leaderId))
@@ -3235,15 +3152,13 @@ class PartitionTest extends AbstractPartitionTest {
 
   @Test
   def testIgnoreFollowerPartitionStateChangeWithOlderPartitionEpoch(): Unit = {
-    val controllerEpoch = 3
     val leaderId = brokerId
     val followerId = brokerId + 1
     val replicas = util.List.of[Integer](leaderId, followerId)
     val leaderEpoch = 8
     val topicId = Uuid.randomUuid()
 
-    val initialFollowerState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(controllerEpoch)
+    val initialFollowerState = new JPartitionState()
       .setLeader(followerId)
       .setLeaderEpoch(leaderEpoch)
       .setIsr(util.List.of[Integer](leaderId, followerId))
@@ -3257,8 +3172,7 @@ class PartitionTest extends AbstractPartitionTest {
 
     // makeLeader is called again with the same leader epoch but with
     // a older partition epoch.
-    val updatedFollowerState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(controllerEpoch)
+    val updatedFollowerState = new JPartitionState()
       .setLeader(followerId)
       .setLeaderEpoch(leaderEpoch)
       .setIsr(util.List.of[Integer](leaderId, followerId))
@@ -3273,7 +3187,6 @@ class PartitionTest extends AbstractPartitionTest {
 
   @Test
   def testFollowerShouldNotHaveAnyRemoteReplicaStates(): Unit = {
-    val controllerEpoch = 3
     val localReplica = brokerId
     val remoteReplica1 = brokerId + 1
     val remoteReplica2 = brokerId + 2
@@ -3281,8 +3194,7 @@ class PartitionTest extends AbstractPartitionTest {
     val topicId = Uuid.randomUuid()
 
     // The local replica is the leader.
-    val initialLeaderState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(controllerEpoch)
+    val initialLeaderState = new JPartitionState()
       .setLeader(localReplica)
       .setLeaderEpoch(1)
       .setIsr(replicas)
@@ -3299,8 +3211,7 @@ class PartitionTest extends AbstractPartitionTest {
     assertEquals(replicas.asScala, partition.assignmentState.replicas)
 
     // The local replica becomes a follower.
-    val updatedLeaderState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(controllerEpoch)
+    val updatedLeaderState = new JPartitionState()
       .setLeader(remoteReplica1)
       .setLeaderEpoch(2)
       .setIsr(replicas)
@@ -3325,8 +3236,7 @@ class PartitionTest extends AbstractPartitionTest {
     val isr = replicas
     addBrokerEpochToMockMetadataCache(metadataCache, replicas)
     partition.makeLeader(
-      new LeaderAndIsrRequest.PartitionState()
-        .setControllerEpoch(0)
+      new JPartitionState()
         .setLeader(brokerId)
         .setLeaderEpoch(0)
         .setIsr(isr)
@@ -3395,8 +3305,7 @@ class PartitionTest extends AbstractPartitionTest {
     partition.createLogIfNotExists(isNew = false, isFutureReplica = false, offsetCheckpoints, topicId = topicId)
 
     partition.makeLeader(
-      new LeaderAndIsrRequest.PartitionState()
-        .setControllerEpoch(0)
+      new JPartitionState()
         .setLeader(brokerId)
         .setLeaderEpoch(0)
         .setIsr(util.List.of[Integer](brokerId, brokerId + 1))
@@ -3419,8 +3328,7 @@ class PartitionTest extends AbstractPartitionTest {
     val isr = util.List.of[Integer](brokerId, brokerId + 1)
     addBrokerEpochToMockMetadataCache(metadataCache, replicas)
     partition.makeLeader(
-      new LeaderAndIsrRequest.PartitionState()
-        .setControllerEpoch(0)
+      new JPartitionState()
         .setLeader(brokerId)
         .setLeaderEpoch(0)
         .setIsr(isr)
@@ -3461,8 +3369,7 @@ class PartitionTest extends AbstractPartitionTest {
     partition.createLogIfNotExists(isNew = false, isFutureReplica = false, offsetCheckpoints, topicId = topicId)
 
     partition.makeLeader(
-      new LeaderAndIsrRequest.PartitionState()
-        .setControllerEpoch(0)
+      new JPartitionState()
         .setLeader(brokerId)
         .setLeaderEpoch(0)
         .setIsr(util.List.of[Integer](brokerId, brokerId + 1))
@@ -3485,8 +3392,7 @@ class PartitionTest extends AbstractPartitionTest {
     partition.createLogIfNotExists(isNew = false, isFutureReplica = false, offsetCheckpoints, topicId = topicId)
 
     partition.makeLeader(
-      new LeaderAndIsrRequest.PartitionState()
-        .setControllerEpoch(0)
+      new JPartitionState()
         .setLeader(brokerId)
         .setLeaderEpoch(0)
         .setIsr(util.List.of[Integer](brokerId, brokerId + 1))
@@ -3515,8 +3421,7 @@ class PartitionTest extends AbstractPartitionTest {
     val epoch = 0
     addBrokerEpochToMockMetadataCache(metadataCache, replicas)
     partition.makeLeader(
-      new LeaderAndIsrRequest.PartitionState()
-        .setControllerEpoch(0)
+      new JPartitionState()
         .setLeader(brokerId)
         .setLeaderEpoch(epoch)
         .setIsr(isr)
@@ -3578,7 +3483,6 @@ class PartitionTest extends AbstractPartitionTest {
 
   @Test
   def testMaybeStartTransactionVerification(): Unit = {
-    val controllerEpoch = 0
     val leaderEpoch = 5
     val replicas = util.List.of[Integer](brokerId, brokerId + 1)
     val isr = replicas
@@ -3586,8 +3490,7 @@ class PartitionTest extends AbstractPartitionTest {
 
     partition.createLogIfNotExists(isNew = false, isFutureReplica = false, offsetCheckpoints, None)
 
-    assertTrue(partition.makeLeader(new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(controllerEpoch)
+    assertTrue(partition.makeLeader(new JPartitionState()
       .setLeader(brokerId)
       .setLeaderEpoch(leaderEpoch)
       .setIsr(isr)
@@ -3636,7 +3539,6 @@ class PartitionTest extends AbstractPartitionTest {
 
   private def makeLeader(
     topicId: Option[Uuid],
-    controllerEpoch: Int,
     leaderEpoch: Int,
     isr: util.List[Integer],
     replicas: util.List[Integer],
@@ -3651,8 +3553,7 @@ class PartitionTest extends AbstractPartitionTest {
       topicId
     )
     val newLeader = partition.makeLeader(
-      new LeaderAndIsrRequest.PartitionState()
-        .setControllerEpoch(controllerEpoch)
+      new JPartitionState()
         .setLeader(brokerId)
         .setLeaderEpoch(leaderEpoch)
         .setIsr(isr)
@@ -3832,12 +3733,10 @@ class PartitionTest extends AbstractPartitionTest {
       metadataCache,
       spyLogManager,
       alterPartitionManager)
-    val controllerEpoch = 0
     val leaderEpoch = 1
     val replicas = util.List.of[Integer](brokerId, brokerId + 1)
     val isr = replicas
-    val leaderAndIsrPartitionState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(controllerEpoch)
+    val leaderAndIsrPartitionState = new JPartitionState()
       .setLeader(brokerId)
       .setLeaderEpoch(leaderEpoch)
       .setIsr(isr)
@@ -3877,12 +3776,10 @@ class PartitionTest extends AbstractPartitionTest {
       metadataCache,
       spyLogManager,
       alterPartitionManager)
-    val controllerEpoch = 0
     val leaderEpoch = 1
     val replicas = util.List.of[Integer](brokerId, brokerId + 1)
     val isr = replicas
-    val leaderAndIsrPartitionState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(controllerEpoch)
+    val leaderAndIsrPartitionState = new JPartitionState()
       .setLeader(brokerId)
       .setLeaderEpoch(leaderEpoch)
       .setIsr(isr)
@@ -3922,12 +3819,10 @@ class PartitionTest extends AbstractPartitionTest {
       metadataCache,
       spyLogManager,
       alterPartitionManager)
-    val controllerEpoch = 0
     val leaderEpoch = 1
     val replicas = util.List.of[Integer](brokerId, brokerId + 1)
     val isr = replicas
-    val leaderAndIsrPartitionState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(controllerEpoch)
+    val leaderAndIsrPartitionState = new JPartitionState()
       .setLeader(brokerId)
       .setLeaderEpoch(leaderEpoch)
       .setIsr(isr)
@@ -3967,12 +3862,10 @@ class PartitionTest extends AbstractPartitionTest {
       metadataCache,
       spyLogManager,
       alterPartitionManager)
-    val controllerEpoch = 0
     val leaderEpoch = 1
     val replicas = util.List.of[Integer](brokerId, brokerId + 1)
     val isr = replicas
-    val leaderAndIsrPartitionState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(controllerEpoch)
+    val leaderAndIsrPartitionState = new JPartitionState()
       .setLeader(brokerId)
       .setLeaderEpoch(leaderEpoch)
       .setIsr(isr)
@@ -4012,12 +3905,10 @@ class PartitionTest extends AbstractPartitionTest {
       metadataCache,
       spyLogManager,
       alterPartitionManager)
-    val controllerEpoch = 0
     val leaderEpoch = 1
     val replicas = util.List.of[Integer](brokerId, brokerId + 1)
     val isr = replicas
-    val leaderAndIsrPartitionState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(controllerEpoch)
+    val leaderAndIsrPartitionState = new JPartitionState()
       .setLeader(brokerId)
       .setLeaderEpoch(leaderEpoch)
       .setIsr(isr)
@@ -4058,12 +3949,10 @@ class PartitionTest extends AbstractPartitionTest {
       metadataCache,
       spyLogManager,
       alterPartitionManager)
-    val controllerEpoch = 0
     val leaderEpoch = 1
     val replicas = util.List.of[Integer](brokerId, brokerId + 1)
     val isr = replicas
-    val leaderAndIsrPartitionState = new LeaderAndIsrRequest.PartitionState()
-      .setControllerEpoch(controllerEpoch)
+    val leaderAndIsrPartitionState = new JPartitionState()
       .setLeader(brokerId)
       .setLeaderEpoch(leaderEpoch)
       .setIsr(isr)
diff --git a/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala b/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala
index 12f7d23164..cd758529df 100644
--- a/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala
+++ b/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala
@@ -43,6 +43,7 @@ import org.apache.kafka.common.metrics.Monitorable
 import org.apache.kafka.common.metrics.PluginMetrics
 import org.apache.kafka.common.network.ListenerName
 import org.apache.kafka.common.protocol.{ApiKeys, Errors}
+import org.apache.kafka.common.{PartitionState => JPartitionState}
 import org.apache.kafka.common.record._
 import org.apache.kafka.common.replica.ClientMetadata.DefaultClientMetadata
 import org.apache.kafka.common.replica.ReplicaView.DefaultReplicaView
@@ -92,7 +93,7 @@ import java.io.{ByteArrayInputStream, File}
 import java.net.InetAddress
 import java.nio.file.{Files, Paths}
 import java.util
-import java.util.concurrent.atomic.{AtomicBoolean, AtomicLong, AtomicReference}
+import java.util.concurrent.atomic.{AtomicLong, AtomicReference}
 import java.util.concurrent.{Callable, CompletableFuture, ConcurrentHashMap, CountDownLatch, TimeUnit}
 import java.util.function.BiConsumer
 import java.util.stream.IntStream
@@ -133,7 +134,6 @@ class ReplicaManagerTest {
 
   // Constants defined for readability
   private val zkVersion = 0
-  private val controllerEpoch = 0
   private val brokerEpoch = 0L
 
   // These metrics are static and once we remove them after each test, they won't be created and verified anymore
@@ -2480,11 +2480,6 @@ class ReplicaManagerTest {
     produceResult
   }
 
-  /**
-   * This method assumes that the test using created ReplicaManager calls
-   * ReplicaManager.becomeLeaderOrFollower() once with LeaderAndIsrRequest containing
-   * 'leaderEpochInLeaderAndIsr' leader epoch for partition 'topicPartition'.
-   */
   private def prepareReplicaManagerAndLogManager(timer: MockTimer,
                                                  topicPartition: Int,
                                                  leaderEpochInLeaderAndIsr: Int,
@@ -2665,11 +2660,10 @@ class ReplicaManagerTest {
                                          leaderEpoch: Int,
                                          leaderBrokerId: Int,
                                          aliveBrokerIds: Seq[Integer],
-                                         isNew: Boolean = false): LeaderAndIsrRequest.PartitionState = {
-    new LeaderAndIsrRequest.PartitionState()
+                                         isNew: Boolean = false): JPartitionState = {
+    new JPartitionState()
       .setTopicName(topic)
       .setPartitionIndex(topicPartition.partition)
-      .setControllerEpoch(controllerEpoch)
       .setLeader(leaderBrokerId)
       .setLeaderEpoch(leaderEpoch)
       .setIsr(aliveBrokerIds.asJava)
@@ -2965,7 +2959,6 @@ class ReplicaManagerTest {
     propsModifier: Properties => Unit = _ => {},
     mockReplicaFetcherManager: Option[ReplicaFetcherManager] = None,
     mockReplicaAlterLogDirsManager: Option[ReplicaAlterLogDirsManager] = None,
-    isShuttingDown: AtomicBoolean = new AtomicBoolean(false),
     enableRemoteStorage: Boolean = false,
     shouldMockLog: Boolean = false,
     remoteLogManager: Option[RemoteLogManager] = None,
@@ -3050,7 +3043,6 @@ class ReplicaManagerTest {
       logDirFailureChannel = new LogDirFailureChannel(config.logDirs.size),
       alterPartitionManager = alterPartitionManager,
       brokerTopicStats = brokerTopicStats,
-      isShuttingDown = isShuttingDown,
       delayedProducePurgatoryParam = Some(mockProducePurgatory),
       delayedFetchPurgatoryParam = Some(mockFetchPurgatory),
       delayedDeleteRecordsPurgatoryParam = Some(mockDeleteRecordsPurgatory),
@@ -4963,12 +4955,10 @@ class ReplicaManagerTest {
     val foo2 = new TopicPartition("foo", 2)
 
     val mockReplicaFetcherManager = mock(classOf[ReplicaFetcherManager])
-    val isShuttingDown = new AtomicBoolean(false)
     val replicaManager = setupReplicaManagerWithMockedPurgatories(
       timer = new MockTimer(time),
       brokerId = localId,
       mockReplicaFetcherManager = Some(mockReplicaFetcherManager),
-      isShuttingDown = isShuttingDown,
       enableRemoteStorage = enableRemoteStorage
     )
 
@@ -5046,10 +5036,6 @@ class ReplicaManagerTest {
 
       reset(mockReplicaFetcherManager)
 
-      // The broker transitions to SHUTTING_DOWN state. This should not have
-      // any impact in KRaft mode.
-      isShuttingDown.set(true)
-
       // The replica begins the controlled shutdown.
       replicaManager.beginControlledShutdown()
 
diff --git a/jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetcher/ReplicaFetcherThreadBenchmark.java b/jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetcher/ReplicaFetcherThreadBenchmark.java
index c165069085..c05f3001b9 100644
--- a/jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetcher/ReplicaFetcherThreadBenchmark.java
+++ b/jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetcher/ReplicaFetcherThreadBenchmark.java
@@ -35,6 +35,7 @@ import kafka.server.metadata.KRaftMetadataCache;
 import kafka.utils.TestUtils;
 
 import org.apache.kafka.clients.FetchSessionHandler;
+import org.apache.kafka.common.PartitionState;
 import org.apache.kafka.common.TopicIdPartition;
 import org.apache.kafka.common.TopicPartition;
 import org.apache.kafka.common.Uuid;
@@ -48,7 +49,6 @@ import org.apache.kafka.common.record.BaseRecords;
 import org.apache.kafka.common.record.RecordsSend;
 import org.apache.kafka.common.requests.FetchRequest;
 import org.apache.kafka.common.requests.FetchResponse;
-import org.apache.kafka.common.requests.LeaderAndIsrRequest;
 import org.apache.kafka.common.utils.LogContext;
 import org.apache.kafka.common.utils.Time;
 import org.apache.kafka.common.utils.Utils;
@@ -166,8 +166,7 @@ public class ReplicaFetcherThreadBenchmark {
             TopicPartition tp = new TopicPartition("topic", i);
 
             List<Integer> replicas = Arrays.asList(0, 1, 2);
-            LeaderAndIsrRequest.PartitionState partitionState = new LeaderAndIsrRequest.PartitionState()
-                    .setControllerEpoch(0)
+            PartitionState partitionState = new PartitionState()
                     .setLeader(0)
                     .setLeaderEpoch(0)
                     .setIsr(replicas)
diff --git a/jmh-benchmarks/src/main/java/org/apache/kafka/jmh/partition/PartitionMakeFollowerBenchmark.java b/jmh-benchmarks/src/main/java/org/apache/kafka/jmh/partition/PartitionMakeFollowerBenchmark.java
index 49c3034966..72f6427264 100644
--- a/jmh-benchmarks/src/main/java/org/apache/kafka/jmh/partition/PartitionMakeFollowerBenchmark.java
+++ b/jmh-benchmarks/src/main/java/org/apache/kafka/jmh/partition/PartitionMakeFollowerBenchmark.java
@@ -24,12 +24,12 @@ import kafka.log.LogManager;
 import kafka.server.AlterPartitionManager;
 import kafka.server.builders.LogManagerBuilder;
 
+import org.apache.kafka.common.PartitionState;
 import org.apache.kafka.common.TopicPartition;
 import org.apache.kafka.common.Uuid;
 import org.apache.kafka.common.compress.Compression;
 import org.apache.kafka.common.record.MemoryRecords;
 import org.apache.kafka.common.record.SimpleRecord;
-import org.apache.kafka.common.requests.LeaderAndIsrRequest;
 import org.apache.kafka.common.utils.Time;
 import org.apache.kafka.common.utils.Utils;
 import org.apache.kafka.metadata.MetadataCache;
@@ -150,8 +150,7 @@ public class PartitionMakeFollowerBenchmark {
 
     @Benchmark
     public boolean testMakeFollower() {
-        LeaderAndIsrRequest.PartitionState partitionState = new LeaderAndIsrRequest.PartitionState()
-            .setControllerEpoch(0)
+        PartitionState partitionState = new PartitionState()
             .setLeader(0)
             .setLeaderEpoch(0)
             .setIsr(replicas)
diff --git a/jmh-benchmarks/src/main/java/org/apache/kafka/jmh/partition/UpdateFollowerFetchStateBenchmark.java b/jmh-benchmarks/src/main/java/org/apache/kafka/jmh/partition/UpdateFollowerFetchStateBenchmark.java
index d6650be55a..1d58c9dd66 100644
--- a/jmh-benchmarks/src/main/java/org/apache/kafka/jmh/partition/UpdateFollowerFetchStateBenchmark.java
+++ b/jmh-benchmarks/src/main/java/org/apache/kafka/jmh/partition/UpdateFollowerFetchStateBenchmark.java
@@ -24,9 +24,9 @@ import kafka.log.LogManager;
 import kafka.server.AlterPartitionManager;
 import kafka.server.builders.LogManagerBuilder;
 
+import org.apache.kafka.common.PartitionState;
 import org.apache.kafka.common.TopicPartition;
 import org.apache.kafka.common.Uuid;
-import org.apache.kafka.common.requests.LeaderAndIsrRequest;
 import org.apache.kafka.common.utils.Time;
 import org.apache.kafka.metadata.MetadataCache;
 import org.apache.kafka.metadata.MockConfigRepository;
@@ -114,8 +114,7 @@ public class UpdateFollowerFetchStateBenchmark {
         replicas.add(0);
         replicas.add(1);
         replicas.add(2);
-        LeaderAndIsrRequest.PartitionState partitionState = new LeaderAndIsrRequest.PartitionState()
-            .setControllerEpoch(0)
+        PartitionState partitionState = new PartitionState()
             .setLeader(0)
             .setLeaderEpoch(0)
             .setIsr(replicas)
diff --git a/jmh-benchmarks/src/main/java/org/apache/kafka/jmh/server/PartitionCreationBench.java b/jmh-benchmarks/src/main/java/org/apache/kafka/jmh/server/PartitionCreationBench.java
index 11f9ca54c0..2022f779a9 100644
--- a/jmh-benchmarks/src/main/java/org/apache/kafka/jmh/server/PartitionCreationBench.java
+++ b/jmh-benchmarks/src/main/java/org/apache/kafka/jmh/server/PartitionCreationBench.java
@@ -27,10 +27,10 @@ import kafka.server.builders.ReplicaManagerBuilder;
 import kafka.server.metadata.KRaftMetadataCache;
 import kafka.utils.TestUtils;
 
+import org.apache.kafka.common.PartitionState;
 import org.apache.kafka.common.TopicPartition;
 import org.apache.kafka.common.Uuid;
 import org.apache.kafka.common.metrics.Metrics;
-import org.apache.kafka.common.requests.LeaderAndIsrRequest;
 import org.apache.kafka.common.utils.Time;
 import org.apache.kafka.common.utils.Utils;
 import org.apache.kafka.metadata.ConfigRepository;
@@ -195,8 +195,7 @@ public class PartitionCreationBench {
             inSync.add(1);
             inSync.add(2);
 
-            LeaderAndIsrRequest.PartitionState partitionState = new LeaderAndIsrRequest.PartitionState()
-                    .setControllerEpoch(0)
+            PartitionState partitionState = new PartitionState()
                     .setLeader(0)
                     .setLeaderEpoch(0)
                     .setIsr(inSync)
diff --git a/metadata/src/main/java/org/apache/kafka/metadata/PartitionRegistration.java b/metadata/src/main/java/org/apache/kafka/metadata/PartitionRegistration.java
index f74ceded9c..1c837aa4e1 100644
--- a/metadata/src/main/java/org/apache/kafka/metadata/PartitionRegistration.java
+++ b/metadata/src/main/java/org/apache/kafka/metadata/PartitionRegistration.java
@@ -18,12 +18,12 @@
 package org.apache.kafka.metadata;
 
 import org.apache.kafka.common.DirectoryId;
+import org.apache.kafka.common.PartitionState;
 import org.apache.kafka.common.TopicPartition;
 import org.apache.kafka.common.Uuid;
 import org.apache.kafka.common.errors.InvalidReplicaDirectoriesException;
 import org.apache.kafka.common.metadata.PartitionChangeRecord;
 import org.apache.kafka.common.metadata.PartitionRecord;
-import org.apache.kafka.common.requests.LeaderAndIsrRequest;
 import org.apache.kafka.image.writer.ImageWriterOptions;
 import org.apache.kafka.server.common.ApiMessageAndVersion;
 
@@ -410,12 +410,11 @@ public class PartitionRegistration {
         return new ApiMessageAndVersion(record, options.metadataVersion().partitionRecordVersion());
     }
 
-    public LeaderAndIsrRequest.PartitionState toLeaderAndIsrPartitionState(TopicPartition tp,
+    public PartitionState toLeaderAndIsrPartitionState(TopicPartition tp,
                                                                            boolean isNew) {
-        return new LeaderAndIsrRequest.PartitionState().
+        return new PartitionState().
             setTopicName(tp.topic()).
             setPartitionIndex(tp.partition()).
-            setControllerEpoch(-1).
             setLeader(leader).
             setLeaderEpoch(leaderEpoch).
             setIsr(Replicas.toList(isr)).
diff --git a/metadata/src/test/java/org/apache/kafka/metadata/PartitionRegistrationTest.java b/metadata/src/test/java/org/apache/kafka/metadata/PartitionRegistrationTest.java
index 8089812358..4774507bee 100644
--- a/metadata/src/test/java/org/apache/kafka/metadata/PartitionRegistrationTest.java
+++ b/metadata/src/test/java/org/apache/kafka/metadata/PartitionRegistrationTest.java
@@ -18,11 +18,11 @@
 package org.apache.kafka.metadata;
 
 import org.apache.kafka.common.DirectoryId;
+import org.apache.kafka.common.PartitionState;
 import org.apache.kafka.common.TopicPartition;
 import org.apache.kafka.common.Uuid;
 import org.apache.kafka.common.metadata.PartitionChangeRecord;
 import org.apache.kafka.common.metadata.PartitionRecord;
-import org.apache.kafka.common.requests.LeaderAndIsrRequest;
 import org.apache.kafka.image.writer.ImageWriterOptions;
 import org.apache.kafka.image.writer.UnwritableMetadataException;
 import org.apache.kafka.server.common.ApiMessageAndVersion;
@@ -112,10 +112,9 @@ public class PartitionRegistrationTest {
                     Uuid.fromString("bAAlGAz1TN2doZjtWlvhRQ")
                 }).
             setIsr(new int[]{2, 3, 4}).setLeader(2).setLeaderRecoveryState(LeaderRecoveryState.RECOVERED).setLeaderEpoch(234).setPartitionEpoch(567).build();
-        assertEquals(new LeaderAndIsrRequest.PartitionState().
+        assertEquals(new PartitionState().
                 setTopicName("foo").
                 setPartitionIndex(1).
-                setControllerEpoch(-1).
                 setLeader(1).
                 setLeaderEpoch(123).
                 setIsr(List.of(1, 2)).
@@ -125,10 +124,9 @@ public class PartitionRegistrationTest {
                 setRemovingReplicas(List.of()).
                 setIsNew(true).toString(),
             a.toLeaderAndIsrPartitionState(new TopicPartition("foo", 1), true).toString());
-        assertEquals(new LeaderAndIsrRequest.PartitionState().
+        assertEquals(new PartitionState().
                 setTopicName("bar").
                 setPartitionIndex(0).
-                setControllerEpoch(-1).
                 setLeader(2).
                 setLeaderEpoch(234).
                 setIsr(List.of(2, 3, 4)).
diff --git a/server-common/src/main/java/org/apache/kafka/common/PartitionState.java b/server-common/src/main/java/org/apache/kafka/common/PartitionState.java
new file mode 100644
index 0000000000..7c104f05a6
--- /dev/null
+++ b/server-common/src/main/java/org/apache/kafka/common/PartitionState.java
@@ -0,0 +1,178 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+package org.apache.kafka.common;
+
+import org.apache.kafka.common.protocol.MessageUtil;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Objects;
+
+public class PartitionState {
+    String topicName = "";
+    int partitionIndex = 0;
+    int leader = 0;
+    int leaderEpoch = 0;
+    List<Integer> isr = new ArrayList<>(0);
+    int partitionEpoch = 0;
+    List<Integer> replicas = new ArrayList<>(0);
+    List<Integer> addingReplicas = new ArrayList<>(0);
+    List<Integer> removingReplicas = new ArrayList<>(0);
+    boolean isNew = false;
+    byte leaderRecoveryState = (byte) 0;
+
+    @Override
+    public boolean equals(Object o) {
+        if (o == null || getClass() != o.getClass()) return false;
+        PartitionState that = (PartitionState) o;
+        return partitionIndex == that.partitionIndex &&
+                leader == that.leader &&
+                leaderEpoch == that.leaderEpoch &&
+                partitionEpoch == that.partitionEpoch &&
+                isNew == that.isNew &&
+                leaderRecoveryState == that.leaderRecoveryState &&
+                Objects.equals(topicName, that.topicName) &&
+                Objects.equals(isr, that.isr) &&
+                Objects.equals(replicas, that.replicas) &&
+                Objects.equals(addingReplicas, that.addingReplicas) &&
+                Objects.equals(removingReplicas, that.removingReplicas);
+    }
+
+    @Override
+    public int hashCode() {
+        return Objects.hash(topicName, partitionIndex, leader, leaderEpoch, isr, partitionEpoch,
+                replicas, addingReplicas, removingReplicas, isNew, leaderRecoveryState);
+    }
+
+    @Override
+    public String toString() {
+        return "PartitionState("
+                + "topicName='" + topicName + "'"
+                + ", partitionIndex=" + partitionIndex
+                + ", leader=" + leader
+                + ", leaderEpoch=" + leaderEpoch
+                + ", isr=" + MessageUtil.deepToString(isr.iterator())
+                + ", partitionEpoch=" + partitionEpoch
+                + ", replicas=" + MessageUtil.deepToString(replicas.iterator())
+                + ", addingReplicas=" + MessageUtil.deepToString(addingReplicas.iterator())
+                + ", removingReplicas=" + MessageUtil.deepToString(removingReplicas.iterator())
+                + ", isNew=" + (isNew ? "true" : "false")
+                + ", leaderRecoveryState=" + leaderRecoveryState
+                + ")";
+    }
+
+    public String topicName() {
+        return this.topicName;
+    }
+
+    public int partitionIndex() {
+        return this.partitionIndex;
+    }
+
+    public int leader() {
+        return this.leader;
+    }
+
+    public int leaderEpoch() {
+        return this.leaderEpoch;
+    }
+
+    public List<Integer> isr() {
+        return this.isr;
+    }
+
+    public int partitionEpoch() {
+        return this.partitionEpoch;
+    }
+
+    public List<Integer> replicas() {
+        return this.replicas;
+    }
+
+    public List<Integer> addingReplicas() {
+        return this.addingReplicas;
+    }
+
+    public List<Integer> removingReplicas() {
+        return this.removingReplicas;
+    }
+
+    public boolean isNew() {
+        return this.isNew;
+    }
+
+    public byte leaderRecoveryState() {
+        return this.leaderRecoveryState;
+    }
+
+    public PartitionState setTopicName(String v) {
+        this.topicName = v;
+        return this;
+    }
+
+    public PartitionState setPartitionIndex(int v) {
+        this.partitionIndex = v;
+        return this;
+    }
+
+    public PartitionState setLeader(int v) {
+        this.leader = v;
+        return this;
+    }
+
+    public PartitionState setLeaderEpoch(int v) {
+        this.leaderEpoch = v;
+        return this;
+    }
+
+    public PartitionState setIsr(List<Integer> v) {
+        this.isr = v;
+        return this;
+    }
+
+    public PartitionState setPartitionEpoch(int v) {
+        this.partitionEpoch = v;
+        return this;
+    }
+
+    public PartitionState setReplicas(List<Integer> v) {
+        this.replicas = v;
+        return this;
+    }
+
+    public PartitionState setAddingReplicas(List<Integer> v) {
+        this.addingReplicas = v;
+        return this;
+    }
+
+    public PartitionState setRemovingReplicas(List<Integer> v) {
+        this.removingReplicas = v;
+        return this;
+    }
+
+    public PartitionState setIsNew(boolean v) {
+        this.isNew = v;
+        return this;
+    }
+
+    public PartitionState setLeaderRecoveryState(byte v) {
+        this.leaderRecoveryState = v;
+        return this;
+    }
+}
\ No newline at end of file
