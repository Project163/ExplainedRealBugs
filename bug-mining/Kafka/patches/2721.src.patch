diff --git a/streams/examples/src/test/java/org/apache/kafka/streams/examples/wordcount/WordCountProcessorTest.java b/streams/examples/src/test/java/org/apache/kafka/streams/examples/wordcount/WordCountProcessorTest.java
index eedd91ad29..bec77e6532 100644
--- a/streams/examples/src/test/java/org/apache/kafka/streams/examples/wordcount/WordCountProcessorTest.java
+++ b/streams/examples/src/test/java/org/apache/kafka/streams/examples/wordcount/WordCountProcessorTest.java
@@ -42,6 +42,7 @@ public class WordCountProcessorTest {
         final KeyValueStore<String, Integer> store =
             Stores.keyValueStoreBuilder(Stores.inMemoryKeyValueStore("Counts"), Serdes.String(), Serdes.Integer())
                 .withLoggingDisabled() // Changelog is not supported by MockProcessorContext.
+                // Caching is disabled by default, but FYI: caching is also not supported by MockProcessorContext.
                 .build();
         store.init(context, store);
         context.register(store, null);
diff --git a/streams/examples/src/test/java/org/apache/kafka/streams/examples/wordcount/WordCountTransformerTest.java b/streams/examples/src/test/java/org/apache/kafka/streams/examples/wordcount/WordCountTransformerTest.java
index 3f0e7d0626..98d50123bb 100644
--- a/streams/examples/src/test/java/org/apache/kafka/streams/examples/wordcount/WordCountTransformerTest.java
+++ b/streams/examples/src/test/java/org/apache/kafka/streams/examples/wordcount/WordCountTransformerTest.java
@@ -40,7 +40,10 @@ public class WordCountTransformerTest {
         // Create and initialize the transformer under test; including its provided store
         final WordCountTransformerDemo.MyTransformerSupplier supplier = new WordCountTransformerDemo.MyTransformerSupplier();
         for (final StoreBuilder<?> storeBuilder : supplier.stores()) {
-            final StateStore store = storeBuilder.withLoggingDisabled().build(); // Changelog is not supported by MockProcessorContext.
+            final StateStore store = storeBuilder
+                .withLoggingDisabled() // Changelog is not supported by MockProcessorContext.
+                // Caching is disabled by default, but FYI: caching is also not supported by MockProcessorContext.
+                .build();
             store.init(context, store);
             context.register(store, null);
         }
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextUtils.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextUtils.java
new file mode 100644
index 0000000000..8f33b67e4d
--- /dev/null
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextUtils.java
@@ -0,0 +1,49 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.processor.internals;
+
+import org.apache.kafka.streams.processor.ProcessorContext;
+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;
+
+/**
+ * This class bridges the gap for components that _should_ be compatible with
+ * the public ProcessorContext interface, but have come to depend on features
+ * in InternalProcessorContext. In theory, all the features adapted here could
+ * migrate to the public interface, so each method in this class should reference
+ * the ticket that would ultimately obviate it.
+ */
+public final class ProcessorContextUtils {
+
+    private ProcessorContextUtils() {}
+
+    /**
+     * Note that KIP-622 would move currentSystemTimeMs to ProcessorContext,
+     * removing the need for this method.
+     */
+    public static long getCurrentSystemTime(final ProcessorContext context) {
+        return context instanceof InternalProcessorContext
+            ? ((InternalProcessorContext) context).currentSystemTimeMs()
+            : System.currentTimeMillis();
+    }
+
+    /**
+     * Should be removed as part of KAFKA-10217
+     */
+    public static StreamsMetricsImpl getMetricsImpl(final ProcessorContext context) {
+        return (StreamsMetricsImpl) context.metrics();
+    }
+}
diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/AbstractRocksDBSegmentedBytesStore.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/AbstractRocksDBSegmentedBytesStore.java
index f0979f6525..dc1ae86ab0 100644
--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/AbstractRocksDBSegmentedBytesStore.java
+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/AbstractRocksDBSegmentedBytesStore.java
@@ -24,7 +24,7 @@ import org.apache.kafka.streams.errors.ProcessorStateException;
 import org.apache.kafka.streams.processor.BatchingStateRestoreCallback;
 import org.apache.kafka.streams.processor.ProcessorContext;
 import org.apache.kafka.streams.processor.StateStore;
-import org.apache.kafka.streams.processor.internals.InternalProcessorContext;
+import org.apache.kafka.streams.processor.internals.ProcessorContextUtils;
 import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;
 import org.apache.kafka.streams.processor.internals.metrics.TaskMetrics;
 import org.apache.kafka.streams.state.KeyValueIterator;
@@ -44,7 +44,7 @@ public class AbstractRocksDBSegmentedBytesStore<S extends Segment> implements Se
     private final AbstractSegments<S> segments;
     private final String metricScope;
     private final KeySchema keySchema;
-    private InternalProcessorContext context;
+    private ProcessorContext context;
     private volatile boolean open;
     private Sensor expiredRecordSensor;
     private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;
@@ -141,7 +141,7 @@ public class AbstractRocksDBSegmentedBytesStore<S extends Segment> implements Se
         final long segmentId = segments.segmentId(timestamp);
         final S segment = segments.getOrCreateSegmentIfLive(segmentId, context, observedStreamTime);
         if (segment == null) {
-            expiredRecordSensor.record(1.0d, context.currentSystemTimeMs());
+            expiredRecordSensor.record(1.0d, ProcessorContextUtils.getCurrentSystemTime(context));
             LOG.warn("Skipping record for expired segment.");
         } else {
             segment.put(key, value);
@@ -165,9 +165,9 @@ public class AbstractRocksDBSegmentedBytesStore<S extends Segment> implements Se
     @Override
     public void init(final ProcessorContext context,
                      final StateStore root) {
-        this.context = (InternalProcessorContext) context;
+        this.context = context;
 
-        final StreamsMetricsImpl metrics = this.context.metrics();
+        final StreamsMetricsImpl metrics = ProcessorContextUtils.getMetricsImpl(context);
         final String threadId = Thread.currentThread().getName();
         final String taskName = context.taskId().toString();
 
diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/AbstractSegments.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/AbstractSegments.java
index 4790dc2abc..044a574485 100644
--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/AbstractSegments.java
+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/AbstractSegments.java
@@ -17,7 +17,7 @@
 package org.apache.kafka.streams.state.internals;
 
 import org.apache.kafka.streams.errors.ProcessorStateException;
-import org.apache.kafka.streams.processor.internals.InternalProcessorContext;
+import org.apache.kafka.streams.processor.ProcessorContext;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -73,7 +73,7 @@ abstract class AbstractSegments<S extends Segment> implements Segments<S> {
 
     @Override
     public S getOrCreateSegmentIfLive(final long segmentId,
-                                      final InternalProcessorContext context,
+                                      final ProcessorContext context,
                                       final long streamTime) {
         final long minLiveTimestamp = streamTime - retentionPeriod;
         final long minLiveSegment = segmentId(minLiveTimestamp);
@@ -91,7 +91,7 @@ abstract class AbstractSegments<S extends Segment> implements Segments<S> {
     }
 
     @Override
-    public void openExisting(final InternalProcessorContext context, final long streamTime) {
+    public void openExisting(final ProcessorContext context, final long streamTime) {
         try {
             final File dir = new File(context.stateDir(), name);
             if (dir.exists()) {
diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/CachingWindowStore.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/CachingWindowStore.java
index 78e16a923f..cb42926635 100644
--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/CachingWindowStore.java
+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/CachingWindowStore.java
@@ -69,6 +69,11 @@ class CachingWindowStore
 
     @Override
     public void init(final ProcessorContext context, final StateStore root) {
+        if (!(context instanceof InternalProcessorContext)) {
+            throw new IllegalArgumentException(
+                "Caching requires internal features of KafkaStreams and must be disabled for unit tests."
+            );
+        }
         initInternal((InternalProcessorContext) context);
         super.init(context, root);
     }
diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/ChangeLoggingWindowBytesStore.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/ChangeLoggingWindowBytesStore.java
index a04eb2e025..0ba6cad0a1 100644
--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/ChangeLoggingWindowBytesStore.java
+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/ChangeLoggingWindowBytesStore.java
@@ -46,6 +46,11 @@ class ChangeLoggingWindowBytesStore
     @Override
     public void init(final ProcessorContext context,
                      final StateStore root) {
+        if (!(context instanceof InternalProcessorContext)) {
+            throw new IllegalArgumentException(
+                "Change logging requires internal features of KafkaStreams and must be disabled for unit tests."
+            );
+        }
         this.context = (InternalProcessorContext) context;
         super.init(context, root);
     }
diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryWindowStore.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryWindowStore.java
index 926d75ea3b..e7220e8017 100644
--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryWindowStore.java
+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryWindowStore.java
@@ -16,13 +16,6 @@
  */
 package org.apache.kafka.streams.state.internals;
 
-import java.nio.ByteBuffer;
-import java.util.Iterator;
-import java.util.Objects;
-import java.util.Set;
-import java.util.concurrent.ConcurrentHashMap;
-import java.util.concurrent.ConcurrentNavigableMap;
-import java.util.concurrent.ConcurrentSkipListMap;
 import org.apache.kafka.clients.consumer.ConsumerRecord;
 import org.apache.kafka.common.metrics.Sensor;
 import org.apache.kafka.common.utils.Bytes;
@@ -31,17 +24,24 @@ import org.apache.kafka.streams.kstream.Windowed;
 import org.apache.kafka.streams.kstream.internals.TimeWindow;
 import org.apache.kafka.streams.processor.ProcessorContext;
 import org.apache.kafka.streams.processor.StateStore;
-import org.apache.kafka.streams.processor.internals.InternalProcessorContext;
+import org.apache.kafka.streams.processor.internals.ProcessorContextUtils;
 import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;
 import org.apache.kafka.streams.processor.internals.metrics.TaskMetrics;
+import org.apache.kafka.streams.state.KeyValueIterator;
 import org.apache.kafka.streams.state.WindowStore;
 import org.apache.kafka.streams.state.WindowStoreIterator;
-import org.apache.kafka.streams.state.KeyValueIterator;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import java.nio.ByteBuffer;
+import java.util.Iterator;
 import java.util.Map;
 import java.util.NoSuchElementException;
+import java.util.Objects;
+import java.util.Set;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ConcurrentNavigableMap;
+import java.util.concurrent.ConcurrentSkipListMap;
 
 import static org.apache.kafka.streams.state.internals.WindowKeySchema.extractStoreKeyBytes;
 import static org.apache.kafka.streams.state.internals.WindowKeySchema.extractStoreTimestamp;
@@ -54,7 +54,7 @@ public class InMemoryWindowStore implements WindowStore<Bytes, byte[]> {
 
     private final String name;
     private final String metricScope;
-    private InternalProcessorContext context;
+    private ProcessorContext context;
     private Sensor expiredRecordSensor;
     private int seqnum = 0;
     private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;
@@ -87,9 +87,9 @@ public class InMemoryWindowStore implements WindowStore<Bytes, byte[]> {
 
     @Override
     public void init(final ProcessorContext context, final StateStore root) {
-        this.context = (InternalProcessorContext) context;
+        this.context = context;
 
-        final StreamsMetricsImpl metrics = this.context.metrics();
+        final StreamsMetricsImpl metrics = ProcessorContextUtils.getMetricsImpl(context);
         final String threadId = Thread.currentThread().getName();
         final String taskName = context.taskId().toString();
         expiredRecordSensor = TaskMetrics.droppedRecordsSensorOrExpiredWindowRecordDropSensor(
@@ -120,7 +120,7 @@ public class InMemoryWindowStore implements WindowStore<Bytes, byte[]> {
         observedStreamTime = Math.max(observedStreamTime, windowStartTimestamp);
 
         if (windowStartTimestamp <= observedStreamTime - retentionPeriod) {
-            expiredRecordSensor.record(1.0d, context.currentSystemTimeMs());
+            expiredRecordSensor.record(1.0d, ProcessorContextUtils.getCurrentSystemTime(context));
             LOG.warn("Skipping record for expired segment.");
         } else {
             if (value != null) {
@@ -257,7 +257,7 @@ public class InMemoryWindowStore implements WindowStore<Bytes, byte[]> {
                 it.close();
             }
         }
-        
+
         segmentMap.clear();
         open = false;
     }
diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/KeyValueSegments.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/KeyValueSegments.java
index 9dbbae4667..e2e8571667 100644
--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/KeyValueSegments.java
+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/KeyValueSegments.java
@@ -16,7 +16,8 @@
  */
 package org.apache.kafka.streams.state.internals;
 
-import org.apache.kafka.streams.processor.internals.InternalProcessorContext;
+import org.apache.kafka.streams.processor.ProcessorContext;
+import org.apache.kafka.streams.processor.internals.ProcessorContextUtils;
 import org.apache.kafka.streams.state.internals.metrics.RocksDBMetricsRecorder;
 
 /**
@@ -36,7 +37,7 @@ class KeyValueSegments extends AbstractSegments<KeyValueSegment> {
 
     @Override
     public KeyValueSegment getOrCreateSegment(final long segmentId,
-                                              final InternalProcessorContext context) {
+                                              final ProcessorContext context) {
         if (segments.containsKey(segmentId)) {
             return segments.get(segmentId);
         } else {
@@ -53,8 +54,8 @@ class KeyValueSegments extends AbstractSegments<KeyValueSegment> {
     }
 
     @Override
-    public void openExisting(final InternalProcessorContext context, final long streamTime) {
-        metricsRecorder.init(context.metrics(), context.taskId());
+    public void openExisting(final ProcessorContext context, final long streamTime) {
+        metricsRecorder.init(ProcessorContextUtils.getMetricsImpl(context), context.taskId());
         super.openExisting(context, streamTime);
     }
 }
\ No newline at end of file
diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/Segments.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/Segments.java
index 8e2c40f35a..bcfb02ebaf 100644
--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/Segments.java
+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/Segments.java
@@ -16,7 +16,7 @@
  */
 package org.apache.kafka.streams.state.internals;
 
-import org.apache.kafka.streams.processor.internals.InternalProcessorContext;
+import org.apache.kafka.streams.processor.ProcessorContext;
 
 import java.util.List;
 
@@ -28,11 +28,11 @@ interface Segments<S extends Segment> {
 
     S getSegmentForTimestamp(final long timestamp);
 
-    S getOrCreateSegmentIfLive(final long segmentId, final InternalProcessorContext context, final long streamTime);
+    S getOrCreateSegmentIfLive(final long segmentId, final ProcessorContext context, final long streamTime);
 
-    S getOrCreateSegment(final long segmentId, final InternalProcessorContext context);
+    S getOrCreateSegment(final long segmentId, final ProcessorContext context);
 
-    void openExisting(final InternalProcessorContext context, final long streamTime);
+    void openExisting(final ProcessorContext context, final long streamTime);
 
     List<S> segments(final long timeFrom, final long timeTo);
 
diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/TimestampedSegments.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/TimestampedSegments.java
index e7c2edbf14..0d5b129f9c 100644
--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/TimestampedSegments.java
+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/TimestampedSegments.java
@@ -16,7 +16,8 @@
  */
 package org.apache.kafka.streams.state.internals;
 
-import org.apache.kafka.streams.processor.internals.InternalProcessorContext;
+import org.apache.kafka.streams.processor.ProcessorContext;
+import org.apache.kafka.streams.processor.internals.ProcessorContextUtils;
 import org.apache.kafka.streams.state.internals.metrics.RocksDBMetricsRecorder;
 
 /**
@@ -36,7 +37,7 @@ class TimestampedSegments extends AbstractSegments<TimestampedSegment> {
 
     @Override
     public TimestampedSegment getOrCreateSegment(final long segmentId,
-                                                 final InternalProcessorContext context) {
+                                                 final ProcessorContext context) {
         if (segments.containsKey(segmentId)) {
             return segments.get(segmentId);
         } else {
@@ -53,8 +54,8 @@ class TimestampedSegments extends AbstractSegments<TimestampedSegment> {
     }
 
     @Override
-    public void openExisting(final InternalProcessorContext context, final long streamTime) {
-        metricsRecorder.init(context.metrics(), context.taskId());
+    public void openExisting(final ProcessorContext context, final long streamTime) {
+        metricsRecorder.init(ProcessorContextUtils.getMetricsImpl(context), context.taskId());
         super.openExisting(context, streamTime);
     }
 }
diff --git a/streams/test-utils/src/test/java/org/apache/kafka/streams/test/wordcount/WindowedWordCountProcessorSupplier.java b/streams/test-utils/src/test/java/org/apache/kafka/streams/test/wordcount/WindowedWordCountProcessorSupplier.java
new file mode 100644
index 0000000000..7318f45489
--- /dev/null
+++ b/streams/test-utils/src/test/java/org/apache/kafka/streams/test/wordcount/WindowedWordCountProcessorSupplier.java
@@ -0,0 +1,78 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.test.wordcount;
+
+import org.apache.kafka.streams.KeyValue;
+import org.apache.kafka.streams.kstream.Windowed;
+import org.apache.kafka.streams.processor.Processor;
+import org.apache.kafka.streams.processor.ProcessorContext;
+import org.apache.kafka.streams.processor.ProcessorSupplier;
+import org.apache.kafka.streams.processor.PunctuationType;
+import org.apache.kafka.streams.state.KeyValueIterator;
+import org.apache.kafka.streams.state.WindowStore;
+
+import java.time.Duration;
+import java.util.Locale;
+
+public final class WindowedWordCountProcessorSupplier implements ProcessorSupplier<String, String> {
+
+    @Override
+    public Processor<String, String> get() {
+        return new Processor<String, String>() {
+            private ProcessorContext context;
+            private WindowStore<String, Integer> windowStore;
+
+            @Override
+            @SuppressWarnings("unchecked")
+            public void init(final ProcessorContext context) {
+                this.context = context;
+                this.context.schedule(Duration.ofSeconds(1), PunctuationType.STREAM_TIME, timestamp -> {
+                    try (final KeyValueIterator<Windowed<String>, Integer> iter = windowStore.all()) {
+                        while (iter.hasNext()) {
+                            final KeyValue<Windowed<String>, Integer> entry = iter.next();
+                            context.forward(entry.key.toString(), entry.value.toString());
+                        }
+                    }
+                });
+                windowStore = (WindowStore<String, Integer>) context.getStateStore("WindowedCounts");
+            }
+
+            @Override
+            public void process(final String key, final String value) {
+                final String[] words = value.toLowerCase(Locale.getDefault()).split(" ");
+                final long timestamp = context.timestamp();
+
+                // calculate the window as every 100 ms
+                // Note this has to be aligned with the configuration for the window store you register separately
+                final long windowStart = timestamp / 100 * 100;
+
+                for (final String word : words) {
+                    final Integer oldValue = windowStore.fetch(word, windowStart);
+
+                    if (oldValue == null) {
+                        windowStore.put(word, 1, windowStart);
+                    } else {
+                        windowStore.put(word, oldValue + 1, windowStart);
+                    }
+                }
+            }
+
+            @Override
+            public void close() {}
+        };
+    }
+}
diff --git a/streams/test-utils/src/test/java/org/apache/kafka/streams/test/wordcount/WindowedWordCountProcessorTest.java b/streams/test-utils/src/test/java/org/apache/kafka/streams/test/wordcount/WindowedWordCountProcessorTest.java
new file mode 100644
index 0000000000..4995a77258
--- /dev/null
+++ b/streams/test-utils/src/test/java/org/apache/kafka/streams/test/wordcount/WindowedWordCountProcessorTest.java
@@ -0,0 +1,185 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.test.wordcount;
+
+import org.apache.kafka.common.serialization.Serdes;
+import org.apache.kafka.common.utils.Utils;
+import org.apache.kafka.streams.KeyValue;
+import org.apache.kafka.streams.StreamsConfig;
+import org.apache.kafka.streams.processor.MockProcessorContext;
+import org.apache.kafka.streams.processor.Processor;
+import org.apache.kafka.streams.processor.TaskId;
+import org.apache.kafka.streams.state.Stores;
+import org.apache.kafka.streams.state.WindowStore;
+import org.apache.kafka.test.TestUtils;
+import org.junit.Test;
+
+import java.io.File;
+import java.io.IOException;
+import java.time.Duration;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.Properties;
+
+import static org.hamcrest.MatcherAssert.assertThat;
+import static org.hamcrest.Matchers.is;
+import static org.junit.Assert.assertThrows;
+
+public class WindowedWordCountProcessorTest {
+    @Test
+    public void shouldWorkWithInMemoryStore() {
+        final MockProcessorContext context = new MockProcessorContext();
+
+        // Create, initialize, and register the state store.
+        final WindowStore<String, Integer> store =
+            Stores.windowStoreBuilder(Stores.inMemoryWindowStore("WindowedCounts",
+                                                                 Duration.ofDays(24),
+                                                                 Duration.ofMillis(100),
+                                                                 false),
+                                      Serdes.String(),
+                                      Serdes.Integer())
+                  .withLoggingDisabled() // Changelog is not supported by MockProcessorContext.
+                  .withCachingDisabled() // Caching is not supported by MockProcessorContext.
+                  .build();
+        store.init(context, store);
+        context.register(store, null);
+
+        // Create and initialize the processor under test
+        final Processor<String, String> processor = new WindowedWordCountProcessorSupplier().get();
+        processor.init(context);
+
+        // send a record to the processor
+        context.setTimestamp(101);
+        processor.process("key", "alpha beta gamma alpha");
+
+        // send a record to the processor in a new window
+        context.setTimestamp(221);
+        processor.process("key", "gamma delta");
+
+        // note that the processor does not forward during process()
+        assertThat(context.forwarded().isEmpty(), is(true));
+
+        // now, we trigger the punctuator, which iterates over the state store and forwards the contents.
+        context.scheduledPunctuators().get(0).getPunctuator().punctuate(1_000L);
+
+        // finally, we can verify the output.
+        final Iterator<MockProcessorContext.CapturedForward> capturedForwards = context.forwarded().iterator();
+        assertThat(capturedForwards.next().keyValue(), is(new KeyValue<>("[alpha@100/200]", "2")));
+        assertThat(capturedForwards.next().keyValue(), is(new KeyValue<>("[beta@100/200]", "1")));
+        assertThat(capturedForwards.next().keyValue(), is(new KeyValue<>("[gamma@100/200]", "1")));
+        assertThat(capturedForwards.next().keyValue(), is(new KeyValue<>("[delta@200/300]", "1")));
+        assertThat(capturedForwards.next().keyValue(), is(new KeyValue<>("[gamma@200/300]", "1")));
+        assertThat(capturedForwards.hasNext(), is(false));
+    }
+
+    @Test
+    public void shouldWorkWithPersistentStore() throws IOException {
+        final Properties properties = new Properties();
+        properties.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, "");
+        properties.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "");
+
+        final File stateDir = TestUtils.tempDirectory();
+
+        try {
+            final MockProcessorContext context = new MockProcessorContext(
+                properties,
+                new TaskId(0, 0),
+                stateDir
+            );
+
+            // Create, initialize, and register the state store.
+            final WindowStore<String, Integer> store =
+                Stores.windowStoreBuilder(Stores.persistentWindowStore("WindowedCounts",
+                                                                       Duration.ofDays(24),
+                                                                       Duration.ofMillis(100),
+                                                                       false),
+                                          Serdes.String(),
+                                          Serdes.Integer())
+                      .withLoggingDisabled() // Changelog is not supported by MockProcessorContext.
+                      .withCachingDisabled() // Caching is not supported by MockProcessorContext.
+                      .build();
+            store.init(context, store);
+            context.register(store, null);
+
+            // Create and initialize the processor under test
+            final Processor<String, String> processor = new WindowedWordCountProcessorSupplier().get();
+            processor.init(context);
+
+            // send a record to the processor
+            context.setTimestamp(101);
+            processor.process("key", "alpha beta gamma alpha");
+
+            // send a record to the processor in a new window
+            context.setTimestamp(221);
+            processor.process("key", "gamma delta");
+
+            // note that the processor does not forward during process()
+            assertThat(context.forwarded().isEmpty(), is(true));
+
+            // now, we trigger the punctuator, which iterates over the state store and forwards the contents.
+            context.scheduledPunctuators().get(0).getPunctuator().punctuate(1_000L);
+
+            // finally, we can verify the output.
+            final Iterator<MockProcessorContext.CapturedForward> capturedForwards = context.forwarded().iterator();
+            assertThat(capturedForwards.next().keyValue(), is(new KeyValue<>("[alpha@100/200]", "2")));
+            assertThat(capturedForwards.next().keyValue(), is(new KeyValue<>("[beta@100/200]", "1")));
+            assertThat(capturedForwards.next().keyValue(), is(new KeyValue<>("[delta@200/300]", "1")));
+            assertThat(capturedForwards.next().keyValue(), is(new KeyValue<>("[gamma@100/200]", "1")));
+            assertThat(capturedForwards.next().keyValue(), is(new KeyValue<>("[gamma@200/300]", "1")));
+            assertThat(capturedForwards.hasNext(), is(false));
+        } finally {
+            Utils.delete(stateDir);
+        }
+    }
+
+    @Test
+    public void shouldFailWithLogging() {
+        final MockProcessorContext context = new MockProcessorContext();
+
+        // Create, initialize, and register the state store.
+        final WindowStore<String, Integer> store =
+            Stores.windowStoreBuilder(Stores.inMemoryWindowStore("WindowedCounts",
+                                                                 Duration.ofDays(24),
+                                                                 Duration.ofMillis(100),
+                                                                 false),
+                                      Serdes.String(),
+                                      Serdes.Integer())
+                  .withLoggingEnabled(new HashMap<>()) // Changelog is not supported by MockProcessorContext.
+                  .withCachingDisabled() // Caching is not supported by MockProcessorContext.
+                  .build();
+        assertThrows(IllegalArgumentException.class, () -> store.init(context, store));
+    }
+
+    @Test
+    public void shouldFailWithCaching() {
+        final MockProcessorContext context = new MockProcessorContext();
+
+        // Create, initialize, and register the state store.
+        final WindowStore<String, Integer> store =
+            Stores.windowStoreBuilder(Stores.inMemoryWindowStore("WindowedCounts",
+                                                                 Duration.ofDays(24),
+                                                                 Duration.ofMillis(100),
+                                                                 false),
+                                      Serdes.String(),
+                                      Serdes.Integer())
+                  .withLoggingDisabled() // Changelog is not supported by MockProcessorContext.
+                  .withCachingEnabled() // Caching is not supported by MockProcessorContext.
+                  .build();
+
+        assertThrows(IllegalArgumentException.class, () -> store.init(context, store));
+    }
+}
