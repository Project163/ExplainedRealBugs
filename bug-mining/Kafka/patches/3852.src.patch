diff --git a/core/src/main/scala/kafka/server/KafkaRequestHandler.scala b/core/src/main/scala/kafka/server/KafkaRequestHandler.scala
index d054d9604c..209db53171 100755
--- a/core/src/main/scala/kafka/server/KafkaRequestHandler.scala
+++ b/core/src/main/scala/kafka/server/KafkaRequestHandler.scala
@@ -23,7 +23,7 @@ import kafka.server.KafkaRequestHandler.{threadCurrentRequest, threadRequestChan
 
 import java.util.concurrent.{ConcurrentHashMap, CountDownLatch, TimeUnit}
 import java.util.concurrent.atomic.AtomicInteger
-import com.yammer.metrics.core.{Gauge, Meter}
+import com.yammer.metrics.core.Meter
 import org.apache.kafka.common.internals.FatalExitError
 import org.apache.kafka.common.utils.{KafkaThread, Time}
 import org.apache.kafka.server.log.remote.storage.RemoteStorageMetrics
@@ -284,26 +284,35 @@ class BrokerTopicMetrics(name: Option[String], remoteStorageEnabled: Boolean = f
   }
 
   case class GaugeWrapper(metricType: String) {
-    @volatile private var gaugeObject: Gauge[Long] = _
-    final private val gaugeLock = new Object
-    final val aggregatedMetric = new AggregatedMetric()
+    // The map to store:
+    //   - per-partition value for topic-level metrics. The key will be the partition number
+    //   - per-topic value for broker-level metrics. The key will be the topic name
+    private val metricValues = new ConcurrentHashMap[String, Long]()
+
+    def setValue(key: String, value: Long): Unit = {
+      newGaugeIfNeed()
+      metricValues.put(key, value)
+    }
 
-    def gauge(): Gauge[Long] = gaugeLock synchronized {
-      if (gaugeObject == null) {
-        gaugeObject = metricsGroup.newGauge(metricType, () => aggregatedMetric.value(), tags)
-      }
-      return gaugeObject
+    def removeKey(key: String): Unit = {
+      newGaugeIfNeed()
+      metricValues.remove(key)
     }
 
-    def close(): Unit = gaugeLock synchronized {
-      if (gaugeObject != null) {
-        metricsGroup.removeMetric(metricType, tags)
-        aggregatedMetric.close()
-        gaugeObject = null
-      }
+    // metricsGroup uses ConcurrentMap to store gauges, so we don't need to use synchronized block here
+    def close(): Unit = {
+      metricsGroup.removeMetric(metricType, tags)
+      metricValues.clear()
     }
 
-    gauge()
+    def value(): Long = metricValues.values().stream().mapToLong(v => v).sum()
+
+    // metricsGroup uses ConcurrentMap to store gauges, so we don't need to use synchronized block here
+    private def newGaugeIfNeed(): Unit = {
+      metricsGroup.newGauge(metricType, () => value(), tags)
+    }
+
+    newGaugeIfNeed()
   }
 
   // an internal map for "lazy initialization" of certain metrics
@@ -407,47 +416,47 @@ class BrokerTopicMetrics(name: Option[String], remoteStorageEnabled: Boolean = f
 
   def invalidOffsetOrSequenceRecordsPerSec: Meter = metricTypeMap.get(BrokerTopicStats.InvalidOffsetOrSequenceRecordsPerSec).meter()
 
-  def remoteCopyLagBytesAggrMetric(): AggregatedMetric = {
-    metricGaugeTypeMap.get(RemoteStorageMetrics.REMOTE_COPY_LAG_BYTES_METRIC.getName).aggregatedMetric
+  def remoteCopyLagBytesAggrMetric(): GaugeWrapper = {
+    metricGaugeTypeMap.get(RemoteStorageMetrics.REMOTE_COPY_LAG_BYTES_METRIC.getName)
   }
 
   // Visible for testing
   def remoteCopyLagBytes: Long = remoteCopyLagBytesAggrMetric().value()
 
-  def remoteCopyLagSegmentsAggrMetric(): AggregatedMetric = {
-    metricGaugeTypeMap.get(RemoteStorageMetrics.REMOTE_COPY_LAG_SEGMENTS_METRIC.getName).aggregatedMetric
+  def remoteCopyLagSegmentsAggrMetric(): GaugeWrapper = {
+    metricGaugeTypeMap.get(RemoteStorageMetrics.REMOTE_COPY_LAG_SEGMENTS_METRIC.getName)
   }
 
   // Visible for testing
   def remoteCopyLagSegments: Long = remoteCopyLagSegmentsAggrMetric().value()
 
-  def remoteLogMetadataCountAggrMetric(): AggregatedMetric = {
-    metricGaugeTypeMap.get(RemoteStorageMetrics.REMOTE_LOG_METADATA_COUNT_METRIC.getName).aggregatedMetric
+  def remoteLogMetadataCountAggrMetric(): GaugeWrapper = {
+    metricGaugeTypeMap.get(RemoteStorageMetrics.REMOTE_LOG_METADATA_COUNT_METRIC.getName)
   }
 
   def remoteLogMetadataCount: Long = remoteLogMetadataCountAggrMetric().value()
 
-  def remoteLogSizeBytesAggrMetric(): AggregatedMetric = {
-    metricGaugeTypeMap.get(RemoteStorageMetrics.REMOTE_LOG_SIZE_BYTES_METRIC.getName).aggregatedMetric
+  def remoteLogSizeBytesAggrMetric(): GaugeWrapper = {
+    metricGaugeTypeMap.get(RemoteStorageMetrics.REMOTE_LOG_SIZE_BYTES_METRIC.getName)
   }
 
   def remoteLogSizeBytes: Long = remoteLogSizeBytesAggrMetric().value()
 
-  def remoteLogSizeComputationTimeAggrMetric(): AggregatedMetric = {
-    metricGaugeTypeMap.get(RemoteStorageMetrics.REMOTE_LOG_SIZE_COMPUTATION_TIME_METRIC.getName).aggregatedMetric
+  def remoteLogSizeComputationTimeAggrMetric(): GaugeWrapper = {
+    metricGaugeTypeMap.get(RemoteStorageMetrics.REMOTE_LOG_SIZE_COMPUTATION_TIME_METRIC.getName)
   }
 
   def remoteLogSizeComputationTime: Long = remoteLogSizeComputationTimeAggrMetric().value()
 
-  def remoteDeleteLagBytesAggrMetric(): AggregatedMetric = {
-    metricGaugeTypeMap.get(RemoteStorageMetrics.REMOTE_DELETE_LAG_BYTES_METRIC.getName).aggregatedMetric
+  def remoteDeleteLagBytesAggrMetric(): GaugeWrapper = {
+    metricGaugeTypeMap.get(RemoteStorageMetrics.REMOTE_DELETE_LAG_BYTES_METRIC.getName)
   }
 
   // Visible for testing
   def remoteDeleteLagBytes: Long = remoteDeleteLagBytesAggrMetric().value()
 
-  def remoteDeleteLagSegmentsAggrMetric(): AggregatedMetric = {
-    metricGaugeTypeMap.get(RemoteStorageMetrics.REMOTE_DELETE_LAG_SEGMENTS_METRIC.getName).aggregatedMetric
+  def remoteDeleteLagSegmentsAggrMetric(): GaugeWrapper = {
+    metricGaugeTypeMap.get(RemoteStorageMetrics.REMOTE_DELETE_LAG_SEGMENTS_METRIC.getName)
   }
 
   // Visible for testing
@@ -488,18 +497,6 @@ class BrokerTopicMetrics(name: Option[String], remoteStorageEnabled: Boolean = f
   }
 }
 
-class AggregatedMetric {
-  // The map to store:
-  //   - per-partition value for topic-level metrics. The key will be the partition number
-  //   - per-topic value for broker-level metrics. The key will be the topic name
-  private val metricValues = new ConcurrentHashMap[String, Long]()
-  def setValue(key: String, value: Long): Unit = metricValues.put(key, value)
-  def removeKey(key: String): Option[Long] = Option.apply(metricValues.remove(key))
-  // Sum all values in the metricValues map
-  def value(): Long = metricValues.values().stream().mapToLong(v => v).sum()
-  def close(): Unit = metricValues.clear()
-}
-
 object BrokerTopicStats {
   val MessagesInPerSec = "MessagesInPerSec"
   val BytesInPerSec = "BytesInPerSec"
diff --git a/core/src/test/scala/kafka/server/KafkaRequestHandlerTest.scala b/core/src/test/scala/kafka/server/KafkaRequestHandlerTest.scala
index 7be9b47c12..a71e8cc6a0 100644
--- a/core/src/test/scala/kafka/server/KafkaRequestHandlerTest.scala
+++ b/core/src/test/scala/kafka/server/KafkaRequestHandlerTest.scala
@@ -26,6 +26,7 @@ import org.apache.kafka.common.requests.{RequestContext, RequestHeader}
 import org.apache.kafka.common.security.auth.{KafkaPrincipal, SecurityProtocol}
 import org.apache.kafka.common.utils.{BufferSupplier, MockTime, Time}
 import org.apache.kafka.server.log.remote.storage.RemoteStorageMetrics
+import org.apache.kafka.server.metrics.KafkaYammerMetrics
 import org.junit.jupiter.api.Assertions.{assertEquals, assertFalse, assertTrue}
 import org.junit.jupiter.api.Test
 import org.junit.jupiter.params.ParameterizedTest
@@ -38,6 +39,7 @@ import java.net.InetAddress
 import java.nio.ByteBuffer
 import java.util.concurrent.CompletableFuture
 import java.util.concurrent.atomic.AtomicInteger
+import java.util.stream.Collectors
 
 class KafkaRequestHandlerTest {
   val brokerTopicStats = new BrokerTopicStats(true)
@@ -671,4 +673,26 @@ class KafkaRequestHandlerTest {
     assertEquals(0, allTopicMetrics.remoteLogSizeBytes)
   }
 
+  @Test
+  def testGaugeClose(): Unit = {
+    val brokerTopicStats = new BrokerTopicStats(true)
+    val topic = "close-test-topic"
+    val brokerTopicMetrics: BrokerTopicMetrics = brokerTopicStats.topicStats(topic)
+    assertEquals(7, KafkaYammerMetrics.defaultRegistry.allMetrics().keySet().stream().filter(metricName => metricName.getMBeanName.contains(s"topic=$topic")).collect(Collectors.toList()).size())
+
+    brokerTopicMetrics.close()
+    assertEquals(0, KafkaYammerMetrics.defaultRegistry.allMetrics().keySet().stream().filter(metricName => metricName.getMBeanName.contains(s"topic=$topic")).collect(Collectors.toList()).size())
+
+    brokerTopicStats.recordRemoteCopyLagBytes(topic, 0, 1)
+    brokerTopicStats.recordRemoteCopyLagSegments(topic, 0, 1)
+    brokerTopicStats.recordRemoteDeleteLagBytes(topic, 0, 1)
+    brokerTopicStats.recordRemoteDeleteLagSegments(topic, 0, 1)
+    brokerTopicStats.recordRemoteLogMetadataCount(topic, 0, 1)
+    brokerTopicStats.recordRemoteLogSizeComputationTime(topic, 0, 1)
+    brokerTopicStats.recordRemoteLogSizeBytes(topic, 0, 1)
+    assertEquals(7, KafkaYammerMetrics.defaultRegistry.allMetrics().keySet().stream().filter(metricName => metricName.getMBeanName.contains(s"topic=$topic")).collect(Collectors.toList()).size())
+
+    // cleanup
+    brokerTopicStats.close()
+  }
 }
