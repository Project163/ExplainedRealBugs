diff --git a/checkstyle/suppressions.xml b/checkstyle/suppressions.xml
index 6218794f62..f610e88e23 100644
--- a/checkstyle/suppressions.xml
+++ b/checkstyle/suppressions.xml
@@ -69,6 +69,9 @@
     <suppress checks="ClassFanOutComplexity"
               files="(ConsumerCoordinator|KafkaConsumer|RequestResponse|Fetcher|KafkaAdminClient)Test.java"/>
 
+    <suppress checks="ClassFanOutComplexity"
+              files="MockAdminClient.java"/>
+
     <suppress checks="JavaNCSS"
               files="RequestResponseTest.java"/>
 
diff --git a/clients/src/main/java/org/apache/kafka/clients/admin/TopicDescription.java b/clients/src/main/java/org/apache/kafka/clients/admin/TopicDescription.java
index c220892326..4e3e59a30f 100644
--- a/clients/src/main/java/org/apache/kafka/clients/admin/TopicDescription.java
+++ b/clients/src/main/java/org/apache/kafka/clients/admin/TopicDescription.java
@@ -30,6 +30,26 @@ public class TopicDescription {
     private final boolean internal;
     private final List<TopicPartitionInfo> partitions;
 
+    @Override
+    public boolean equals(Object o) {
+        if (this == o) return true;
+        if (o == null || getClass() != o.getClass()) return false;
+
+        TopicDescription that = (TopicDescription) o;
+
+        if (internal != that.internal) return false;
+        if (name != null ? !name.equals(that.name) : that.name != null) return false;
+        return partitions != null ? partitions.equals(that.partitions) : that.partitions == null;
+    }
+
+    @Override
+    public int hashCode() {
+        int result = name != null ? name.hashCode() : 0;
+        result = 31 * result + (internal ? 1 : 0);
+        result = 31 * result + (partitions != null ? partitions.hashCode() : 0);
+        return result;
+    }
+
     /**
      * Create an instance with the specified parameters.
      *
diff --git a/clients/src/main/java/org/apache/kafka/common/TopicPartitionInfo.java b/clients/src/main/java/org/apache/kafka/common/TopicPartitionInfo.java
index be693181aa..7edf71408f 100644
--- a/clients/src/main/java/org/apache/kafka/common/TopicPartitionInfo.java
+++ b/clients/src/main/java/org/apache/kafka/common/TopicPartitionInfo.java
@@ -82,4 +82,26 @@ public class TopicPartitionInfo {
         return "(partition=" + partition + ", leader=" + leader + ", replicas=" +
             Utils.join(replicas, ", ") + ", isr=" + Utils.join(isr, ", ") + ")";
     }
+
+    @Override
+    public boolean equals(Object o) {
+        if (this == o) return true;
+        if (o == null || getClass() != o.getClass()) return false;
+
+        TopicPartitionInfo that = (TopicPartitionInfo) o;
+
+        if (partition != that.partition) return false;
+        if (leader != null ? !leader.equals(that.leader) : that.leader != null) return false;
+        if (replicas != null ? !replicas.equals(that.replicas) : that.replicas != null) return false;
+        return isr != null ? isr.equals(that.isr) : that.isr == null;
+    }
+
+    @Override
+    public int hashCode() {
+        int result = partition;
+        result = 31 * result + (leader != null ? leader.hashCode() : 0);
+        result = 31 * result + (replicas != null ? replicas.hashCode() : 0);
+        result = 31 * result + (isr != null ? isr.hashCode() : 0);
+        return result;
+    }
 }
diff --git a/clients/src/test/java/org/apache/kafka/clients/admin/MockAdminClient.java b/clients/src/test/java/org/apache/kafka/clients/admin/MockAdminClient.java
new file mode 100644
index 0000000000..11fe428b73
--- /dev/null
+++ b/clients/src/test/java/org/apache/kafka/clients/admin/MockAdminClient.java
@@ -0,0 +1,264 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.clients.admin;
+
+import org.apache.kafka.common.KafkaFuture;
+import org.apache.kafka.common.Node;
+import org.apache.kafka.common.TopicPartition;
+import org.apache.kafka.common.TopicPartitionInfo;
+import org.apache.kafka.common.TopicPartitionReplica;
+import org.apache.kafka.common.acl.AclBinding;
+import org.apache.kafka.common.acl.AclBindingFilter;
+import org.apache.kafka.common.config.ConfigResource;
+import org.apache.kafka.common.errors.TimeoutException;
+import org.apache.kafka.common.errors.TopicExistsException;
+import org.apache.kafka.common.errors.UnknownTopicOrPartitionException;
+import org.apache.kafka.common.internals.KafkaFutureImpl;
+
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.TimeUnit;
+
+public class MockAdminClient extends AdminClient {
+    private final List<Node> brokers;
+    private final Map<String, TopicMetadata> allTopics = new HashMap<>();
+
+    private int timeoutNextRequests = 0;
+
+    public MockAdminClient(List<Node> brokers) {
+        this.brokers = brokers;
+    }
+
+    public void addTopic(boolean internal,
+                         String name,
+                         List<TopicPartitionInfo> partitions,
+                         Map<String, String> configs) {
+        if (allTopics.containsKey(name)) {
+            throw new IllegalArgumentException(String.format("Topic %s was already added.", name));
+        }
+        List<Node> replicas = null;
+        for (TopicPartitionInfo partition : partitions) {
+            if (!brokers.contains(partition.leader())) {
+                throw new IllegalArgumentException("Leader broker unknown");
+            }
+            if (!brokers.containsAll(partition.replicas())) {
+                throw new IllegalArgumentException("Unknown brokers in replica list");
+            }
+            if (!brokers.containsAll(partition.isr())) {
+                throw new IllegalArgumentException("Unknown brokers in isr list");
+            }
+
+            if (replicas == null) {
+                replicas = partition.replicas();
+            } else if (!replicas.equals(partition.replicas())) {
+                throw new IllegalArgumentException("All partitions need to have the same replica nodes.");
+            }
+        }
+
+        allTopics.put(name, new TopicMetadata(internal, partitions, configs));
+    }
+
+    public void timeoutNextRequest(int numberOfRequest) {
+        timeoutNextRequests = numberOfRequest;
+    }
+
+    @Override
+    public DescribeClusterResult describeCluster(DescribeClusterOptions options) {
+        throw new UnsupportedOperationException("Not implemented yet");
+    }
+
+    @Override
+    public CreateTopicsResult createTopics(Collection<NewTopic> newTopics, CreateTopicsOptions options) {
+        Map<String, KafkaFuture<Void>> createTopicResult = new HashMap<>();
+
+        if (timeoutNextRequests > 0) {
+            for (final NewTopic newTopic : newTopics) {
+                String topicName = newTopic.name();
+
+                KafkaFutureImpl<Void> future = new KafkaFutureImpl<>();
+                future.completeExceptionally(new TimeoutException());
+                createTopicResult.put(topicName, future);
+            }
+
+            --timeoutNextRequests;
+            return new CreateTopicsResult(createTopicResult);
+        }
+
+        for (final NewTopic newTopic : newTopics) {
+            KafkaFutureImpl<Void> future = new KafkaFutureImpl<>();
+
+            String topicName = newTopic.name();
+            if (allTopics.containsKey(topicName)) {
+                future.completeExceptionally(new TopicExistsException(String.format("Topic %s exists already.", topicName)));
+                createTopicResult.put(topicName, future);
+            }
+            int replicationFactor = newTopic.replicationFactor();
+            List<Node> replicas = new ArrayList<>(replicationFactor);
+            for (int i = 0; i < replicationFactor; ++i) {
+                replicas.add(brokers.get(i));
+            }
+
+            int numberOfPartitions = newTopic.numPartitions();
+            List<TopicPartitionInfo> partitions = new ArrayList<>(numberOfPartitions);
+            for (int p = 0; p < numberOfPartitions; ++p) {
+                partitions.add(new TopicPartitionInfo(p, brokers.get(0), replicas, Collections.<Node>emptyList()));
+            }
+            allTopics.put(topicName, new TopicMetadata(false, partitions, newTopic.convertToTopicDetails().configs));
+            future.complete(null);
+            createTopicResult.put(topicName, future);
+        }
+
+        return new CreateTopicsResult(createTopicResult);
+    }
+
+    @Override
+    public ListTopicsResult listTopics(ListTopicsOptions options) {
+        Map<String, TopicListing> topicListings = new HashMap<>();
+
+        if (timeoutNextRequests > 0) {
+            KafkaFutureImpl<Map<String, TopicListing>> future = new KafkaFutureImpl<>();
+            future.completeExceptionally(new TimeoutException());
+
+            --timeoutNextRequests;
+            return new ListTopicsResult(future);
+        }
+
+        for (Map.Entry<String, TopicMetadata> topicDescription : allTopics.entrySet()) {
+            String topicName = topicDescription.getKey();
+            topicListings.put(topicName, new TopicListing(topicName, topicDescription.getValue().isInternalTopic));
+        }
+
+        KafkaFutureImpl<Map<String, TopicListing>> future = new KafkaFutureImpl<>();
+        future.complete(topicListings);
+        return new ListTopicsResult(future);
+    }
+
+    @Override
+    public DescribeTopicsResult describeTopics(Collection<String> topicNames, DescribeTopicsOptions options) {
+        Map<String, KafkaFuture<TopicDescription>> topicDescriptions = new HashMap<>();
+
+        if (timeoutNextRequests > 0) {
+            for (String requestedTopic : topicNames) {
+                KafkaFutureImpl<TopicDescription> future = new KafkaFutureImpl<>();
+                future.completeExceptionally(new TimeoutException());
+                topicDescriptions.put(requestedTopic, future);
+            }
+
+            --timeoutNextRequests;
+            return new DescribeTopicsResult(topicDescriptions);
+        }
+
+        for (String requestedTopic : topicNames) {
+            for (Map.Entry<String, TopicMetadata> topicDescription : allTopics.entrySet()) {
+                String topicName = topicDescription.getKey();
+                if (topicName.equals(requestedTopic)) {
+                    TopicMetadata topicMetadata = topicDescription.getValue();
+                    KafkaFutureImpl<TopicDescription> future = new KafkaFutureImpl<>();
+                    future.complete(new TopicDescription(topicName, topicMetadata.isInternalTopic, topicMetadata.partitions));
+                    topicDescriptions.put(topicName, future);
+                    break;
+                }
+            }
+            if (!topicDescriptions.containsKey(requestedTopic)) {
+                KafkaFutureImpl<TopicDescription> future = new KafkaFutureImpl<>();
+                future.completeExceptionally(new UnknownTopicOrPartitionException(
+                    String.format("Topic %s unknown.", requestedTopic)));
+                topicDescriptions.put(requestedTopic, future);
+            }
+        }
+
+        return new DescribeTopicsResult(topicDescriptions);
+    }
+
+    @Override
+    public DeleteTopicsResult deleteTopics(Collection<String> topics, DeleteTopicsOptions options) {
+        throw new UnsupportedOperationException("Not implemented yet");
+    }
+
+    @Override
+    public CreatePartitionsResult createPartitions(Map<String, NewPartitions> newPartitions, CreatePartitionsOptions options) {
+        throw new UnsupportedOperationException("Not implemented yet");
+    }
+
+    @Override
+    public DeleteRecordsResult deleteRecords(Map<TopicPartition, RecordsToDelete> recordsToDelete, DeleteRecordsOptions options) {
+        throw new UnsupportedOperationException("Not implemented yet");
+    }
+
+    @Override
+    public CreateAclsResult createAcls(Collection<AclBinding> acls, CreateAclsOptions options) {
+        throw new UnsupportedOperationException("Not implemented yet");
+    }
+
+    @Override
+    public DescribeAclsResult describeAcls(AclBindingFilter filter, DescribeAclsOptions options) {
+        throw new UnsupportedOperationException("Not implemented yet");
+    }
+
+    @Override
+    public DeleteAclsResult deleteAcls(Collection<AclBindingFilter> filters, DeleteAclsOptions options) {
+        throw new UnsupportedOperationException("Not implemented yet");
+    }
+
+    @Override
+    public DescribeConfigsResult describeConfigs(Collection<ConfigResource> resources, DescribeConfigsOptions options) {
+        throw new UnsupportedOperationException("Not implemented yet");
+    }
+
+    @Override
+    public AlterConfigsResult alterConfigs(Map<ConfigResource, Config> configs, AlterConfigsOptions options) {
+        throw new UnsupportedOperationException("Not implemented yet");
+    }
+
+    @Override
+    public AlterReplicaLogDirsResult alterReplicaLogDirs(Map<TopicPartitionReplica, String> replicaAssignment, AlterReplicaLogDirsOptions options) {
+        throw new UnsupportedOperationException("Not implemented yet");
+    }
+
+    @Override
+    public DescribeLogDirsResult describeLogDirs(Collection<Integer> brokers, DescribeLogDirsOptions options) {
+        throw new UnsupportedOperationException("Not implemented yet");
+    }
+
+    @Override
+    public DescribeReplicaLogDirsResult describeReplicaLogDirs(Collection<TopicPartitionReplica> replicas, DescribeReplicaLogDirsOptions options) {
+        throw new UnsupportedOperationException("Not implemented yet");
+    }
+
+    @Override
+    public void close(long duration, TimeUnit unit) {}
+
+
+    private final static class TopicMetadata {
+        final boolean isInternalTopic;
+        final List<TopicPartitionInfo> partitions;
+        final Map<String, String> configs;
+
+        TopicMetadata(boolean isInternalTopic,
+                      List<TopicPartitionInfo> partitions,
+                      Map<String, String> configs) {
+            this.isInternalTopic = isInternalTopic;
+            this.partitions = partitions;
+            this.configs = configs != null ? configs : Collections.<String, String>emptyMap();
+        }
+    }
+
+}
diff --git a/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java b/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java
index 1844cdeef8..641455bc98 100644
--- a/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java
+++ b/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java
@@ -651,7 +651,7 @@ public class KafkaStreams {
         }
 
         // use client id instead of thread client id since this admin client may be shared among threads
-        this.adminClient = clientSupplier.getAdminClient(config.getAdminConfigs(clientId));
+        adminClient = clientSupplier.getAdminClient(config.getAdminConfigs(clientId));
 
         final Map<Long, StreamThread.State> threadState = new HashMap<>(threads.length);
         final ArrayList<StateStoreProvider> storeProviders = new ArrayList<>();
diff --git a/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java b/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java
index 26919c67a9..49b8a3c119 100644
--- a/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java
+++ b/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java
@@ -740,8 +740,9 @@ public class StreamsConfig extends AbstractConfig {
         consumerProps.put(NUM_STANDBY_REPLICAS_CONFIG, getInt(NUM_STANDBY_REPLICAS_CONFIG));
         consumerProps.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, StreamPartitionAssignor.class.getName());
         consumerProps.put(WINDOW_STORE_CHANGE_LOG_ADDITIONAL_RETENTION_MS_CONFIG, getLong(WINDOW_STORE_CHANGE_LOG_ADDITIONAL_RETENTION_MS_CONFIG));
-
         consumerProps.put(APPLICATION_SERVER_CONFIG, getString(APPLICATION_SERVER_CONFIG));
+        final AdminClientConfig config = new AdminClientConfig(getClientPropsWithPrefix(ADMIN_CLIENT_PREFIX, AdminClientConfig.configNames()));
+        consumerProps.put(adminClientPrefix(AdminClientConfig.RETRIES_CONFIG), config.getInt(AdminClientConfig.RETRIES_CONFIG));
 
         return consumerProps;
     }
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopicConfig.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopicConfig.java
index 7931f327b0..c6758d3a05 100644
--- a/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopicConfig.java
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopicConfig.java
@@ -31,17 +31,20 @@ public class InternalTopicConfig {
     public enum CleanupPolicy { compact, delete }
 
     private final String name;
+    private int numberOfPartitions = -1;
     private final Map<String, String> logConfig;
     private final Set<CleanupPolicy> cleanupPolicies;
 
     private Long retentionMs;
 
-    public InternalTopicConfig(final String name, final Set<CleanupPolicy> defaultCleanupPolicies, final Map<String, String> logConfig) {
+    public InternalTopicConfig(final String name,
+                               final Set<CleanupPolicy> defaultCleanupPolicies,
+                               final Map<String, String> logConfig) {
         Objects.requireNonNull(name, "name can't be null");
         Topic.validate(name);
 
         if (defaultCleanupPolicies.isEmpty()) {
-            throw new IllegalArgumentException("Must provide at least one cleanup policy");
+            throw new IllegalArgumentException("Must provide at least one cleanup policy.");
         }
         this.name = name;
         this.cleanupPolicies = defaultCleanupPolicies;
@@ -91,7 +94,21 @@ public class InternalTopicConfig {
         return name;
     }
 
-    public void setRetentionMs(final long retentionMs) {
+    public int numberOfPartitions() {
+        if (numberOfPartitions == -1) {
+            throw new IllegalStateException("Number of partitions not specified.");
+        }
+        return numberOfPartitions;
+    }
+
+    void setNumberOfPartitions(final int numberOfPartitions) {
+        if (numberOfPartitions < 1) {
+            throw new IllegalArgumentException("Number of partitions must be at least 1.");
+        }
+        this.numberOfPartitions = numberOfPartitions;
+    }
+
+    void setRetentionMs(final long retentionMs) {
         if (!logConfig.containsKey(InternalTopicManager.RETENTION_MS)) {
             this.retentionMs = retentionMs;
         }
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopicManager.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopicManager.java
index f8d4eec7aa..cae31287d3 100644
--- a/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopicManager.java
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopicManager.java
@@ -16,44 +16,76 @@
  */
 package org.apache.kafka.streams.processor.internals;
 
-import org.apache.kafka.common.requests.MetadataResponse;
+import org.apache.kafka.clients.admin.AdminClient;
+import org.apache.kafka.clients.admin.AdminClientConfig;
+import org.apache.kafka.clients.admin.CreateTopicsResult;
+import org.apache.kafka.clients.admin.DescribeTopicsResult;
+import org.apache.kafka.clients.admin.NewTopic;
+import org.apache.kafka.clients.admin.TopicDescription;
+import org.apache.kafka.common.KafkaFuture;
+import org.apache.kafka.common.errors.TimeoutException;
+import org.apache.kafka.common.errors.TopicExistsException;
 import org.apache.kafka.common.utils.LogContext;
-import org.apache.kafka.common.utils.Time;
+import org.apache.kafka.common.utils.Utils;
+import org.apache.kafka.streams.StreamsConfig;
 import org.apache.kafka.streams.errors.StreamsException;
 import org.slf4j.Logger;
 
 import java.util.Collection;
+import java.util.Collections;
 import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Iterator;
 import java.util.Map;
+import java.util.Properties;
 import java.util.Set;
+import java.util.concurrent.ExecutionException;
 import java.util.concurrent.TimeUnit;
 
 public class InternalTopicManager {
 
-    static final Long WINDOW_CHANGE_LOG_ADDITIONAL_RETENTION_DEFAULT = TimeUnit.MILLISECONDS.convert(1, TimeUnit.DAYS);
+    private static final Long WINDOW_CHANGE_LOG_ADDITIONAL_RETENTION_DEFAULT = TimeUnit.MILLISECONDS.convert(1, TimeUnit.DAYS);
 
-    public static final String CLEANUP_POLICY_PROP = "cleanup.policy";
-    public static final String RETENTION_MS = "retention.ms";
-    private static final int MAX_TOPIC_READY_TRY = 5;
+    public final static String CLEANUP_POLICY_PROP = "cleanup.policy";
+    public final static String RETENTION_MS = "retention.ms";
+
+    private final static String INTERRUPTED_ERROR_MESSAGE = "Thread got interrupted. This indicates a bug. " +
+        "Please report at https://issues.apache.org/jira/projects/KAFKA or dev-mailing list (https://kafka.apache.org/contact).";
 
     private final Logger log;
-    private final Time time;
     private final long windowChangeLogAdditionalRetention;
+    private final Map<String, String> defaultTopicConfigs = new HashMap<>();
+
+    private final short replicationFactor;
+    private final AdminClient adminClient;
 
-    private final int replicationFactor;
-    private final StreamsKafkaClient streamsKafkaClient;
+    private final int retries;
 
-    public InternalTopicManager(final StreamsKafkaClient streamsKafkaClient,
-                                final int replicationFactor,
-                                final long windowChangeLogAdditionalRetention,
-                                final Time time) {
-        this.streamsKafkaClient = streamsKafkaClient;
-        this.replicationFactor = replicationFactor;
-        this.windowChangeLogAdditionalRetention = windowChangeLogAdditionalRetention;
-        this.time = time;
+    public InternalTopicManager(final AdminClient adminClient,
+                                final Map<String, ?> config) {
+        this.adminClient = adminClient;
+        final StreamsConfig streamsConfig = new StreamsConfig(config);
 
         LogContext logContext = new LogContext(String.format("stream-thread [%s] ", Thread.currentThread().getName()));
-        this.log = logContext.logger(getClass());
+        log = logContext.logger(getClass());
+
+        replicationFactor = streamsConfig.getInt(StreamsConfig.REPLICATION_FACTOR_CONFIG).shortValue();
+        windowChangeLogAdditionalRetention = streamsConfig.getLong(StreamsConfig.WINDOW_STORE_CHANGE_LOG_ADDITIONAL_RETENTION_MS_CONFIG);
+        retries = new AdminClientConfig(streamsConfig.getAdminConfigs("dummy")).getInt(AdminClientConfig.RETRIES_CONFIG);
+
+        log.debug("Configs:" + Utils.NL,
+            "\t{} = {}" + Utils.NL,
+            "\t{} = {}" + Utils.NL,
+            "\t{} = {}",
+            AdminClientConfig.RETRIES_CONFIG, retries,
+            StreamsConfig.REPLICATION_FACTOR_CONFIG, replicationFactor,
+            StreamsConfig.WINDOW_STORE_CHANGE_LOG_ADDITIONAL_RETENTION_MS_CONFIG, windowChangeLogAdditionalRetention);
+
+        for (final Map.Entry<String, Object> entry : streamsConfig.originalsWithPrefix(StreamsConfig.TOPIC_PREFIX).entrySet()) {
+            if (entry.getValue() != null) {
+                defaultTopicConfigs.put(entry.getKey(), entry.getValue().toString());
+            }
+        }
     }
 
     /**
@@ -63,83 +95,172 @@ public class InternalTopicManager {
      * If a topic with the correct number of partitions exists ignores it.
      * If a topic exists already but has different number of partitions we fail and throw exception requesting user to reset the app before restarting again.
      */
-    public void makeReady(final Map<InternalTopicConfig, Integer> topics) {
-        for (int i = 0; i < MAX_TOPIC_READY_TRY; i++) {
-            try {
-                final MetadataResponse metadata = streamsKafkaClient.fetchMetadata();
-                final Map<String, Integer> existingTopicPartitions = fetchExistingPartitionCountByTopic(metadata);
-                final Map<InternalTopicConfig, Integer> topicsToBeCreated = validateTopicPartitions(topics, existingTopicPartitions);
-                if (topicsToBeCreated.size() > 0) {
-                    if (metadata.brokers().size() < replicationFactor) {
-                        throw new StreamsException("Found only " + metadata.brokers().size() + " brokers, " +
-                            " but replication factor is " + replicationFactor + "." +
-                            " Decrease replication factor for internal topics via StreamsConfig parameter \"replication.factor\"" +
-                            " or add more brokers to your cluster.");
+    public void makeReady(final Map<String, InternalTopicConfig> topics) {
+        final Map<String, Integer> existingTopicPartitions = getNumPartitions(topics.keySet(), true);
+        final Set<InternalTopicConfig> topicsToBeCreated = validateTopicPartitions(topics.values(), existingTopicPartitions);
+        if (topicsToBeCreated.size() > 0) {
+            final Set<NewTopic> newTopics = new HashSet<>();
+
+            for (final InternalTopicConfig internalTopicConfig : topicsToBeCreated) {
+                final Properties topicProperties = internalTopicConfig.toProperties(windowChangeLogAdditionalRetention);
+                final Map<String, String> topicConfig = new HashMap<>(defaultTopicConfigs);
+                for (final String key : topicProperties.stringPropertyNames()) {
+                    topicConfig.put(key, topicProperties.getProperty(key));
+                }
+                newTopics.add(
+                    new NewTopic(
+                        internalTopicConfig.name(),
+                        internalTopicConfig.numberOfPartitions(),
+                        replicationFactor)
+                    .configs(topicConfig));
+            }
+
+            int remainingRetries = retries;
+            boolean retry;
+            do {
+                retry = false;
+
+                final CreateTopicsResult createTopicsResult = adminClient.createTopics(newTopics);
+
+                final Set<String> createTopicNames = new HashSet<>();
+                for (final Map.Entry<String, KafkaFuture<Void>> createTopicResult : createTopicsResult.values().entrySet()) {
+                    try {
+                        createTopicResult.getValue().get();
+                        createTopicNames.add(createTopicResult.getKey());
+                    } catch (final ExecutionException couldNotCreateTopic) {
+                        final Throwable cause = couldNotCreateTopic.getCause();
+                        final String topicName = createTopicResult.getKey();
+
+                        if (cause instanceof TimeoutException) {
+                            retry = true;
+                            log.debug("Could not get number of partitions for topic {} due to timeout. " +
+                                "Will try again (remaining retries {}).", topicName, remainingRetries - 1);
+                        } else if (cause instanceof TopicExistsException) {
+                            createTopicNames.add(createTopicResult.getKey());
+                            log.info(String.format("Topic %s exist already: %s",
+                                topicName,
+                                couldNotCreateTopic.getMessage()));
+                        } else {
+                            throw new StreamsException(String.format("Could not create topic %s.", topicName),
+                                couldNotCreateTopic);
+                        }
+                    } catch (final InterruptedException fatalException) {
+                        Thread.currentThread().interrupt();
+                        log.error(INTERRUPTED_ERROR_MESSAGE, fatalException);
+                        throw new IllegalStateException(INTERRUPTED_ERROR_MESSAGE, fatalException);
                     }
-                    streamsKafkaClient.createTopics(topicsToBeCreated, replicationFactor, windowChangeLogAdditionalRetention, metadata);
                 }
+
+                if (retry) {
+                    final Iterator<NewTopic> it = newTopics.iterator();
+                    while (it.hasNext()) {
+                        if (createTopicNames.contains(it.next().name())) {
+                            it.remove();
+                        }
+                    }
+
+                    continue;
+                }
+
                 return;
-            } catch (StreamsException ex) {
-                log.warn("Could not create internal topics: {} Retry #{}", ex.getMessage(), i);
-            }
-            // backoff
-            time.sleep(100L);
+            } while (remainingRetries-- > 0);
+
+            final String timeoutAndRetryError = "Could not create topics. " +
+                "This can happen if the Kafka cluster is temporary not available. " +
+                "You can increase admin client config `retries` to be resilient against this error.";
+            log.error(timeoutAndRetryError);
+            throw new StreamsException(timeoutAndRetryError);
         }
-        throw new StreamsException("Could not create internal topics.");
     }
 
     /**
      * Get the number of partitions for the given topics
      */
     public Map<String, Integer> getNumPartitions(final Set<String> topics) {
-        for (int i = 0; i < MAX_TOPIC_READY_TRY; i++) {
-            try {
-                final MetadataResponse metadata = streamsKafkaClient.fetchMetadata();
-                final Map<String, Integer> existingTopicPartitions = fetchExistingPartitionCountByTopic(metadata);
-                existingTopicPartitions.keySet().retainAll(topics);
-
-                return existingTopicPartitions;
-            } catch (StreamsException ex) {
-                log.warn("Could not get number of partitions: {} Retry #{}", ex.getMessage(), i);
+        return getNumPartitions(topics, false);
+    }
+
+    private Map<String, Integer> getNumPartitions(final Set<String> topics,
+                                                  final boolean bestEffort) {
+        int remainingRetries = retries;
+        boolean retry;
+        do {
+            retry = false;
+
+            final DescribeTopicsResult describeTopicsResult = adminClient.describeTopics(topics);
+            final Map<String, KafkaFuture<TopicDescription>> futures = describeTopicsResult.values();
+
+            final Map<String, Integer> existingNumberOfPartitionsPerTopic = new HashMap<>();
+            for (final Map.Entry<String, KafkaFuture<TopicDescription>> topicFuture : futures.entrySet()) {
+                try {
+                    final TopicDescription topicDescription = topicFuture.getValue().get();
+                    existingNumberOfPartitionsPerTopic.put(
+                        topicFuture.getKey(),
+                        topicDescription.partitions().size());
+                } catch (final InterruptedException fatalException) {
+                    Thread.currentThread().interrupt();
+                    log.error(INTERRUPTED_ERROR_MESSAGE, fatalException);
+                    throw new IllegalStateException(INTERRUPTED_ERROR_MESSAGE, fatalException);
+                } catch (final ExecutionException couldNotDescribeTopicException) {
+                    final Throwable cause = couldNotDescribeTopicException.getCause();
+                    if (cause instanceof TimeoutException) {
+                        retry = true;
+                        log.debug("Could not get number of partitions for topic {} due to timeout. " +
+                            "Will try again (remaining retries {}).", topicFuture.getKey(), remainingRetries - 1);
+                    } else {
+                        final String error = "Could not get number of partitions for topic {}.";
+                        if (bestEffort) {
+                            log.debug(error, topicFuture.getKey(), cause.getMessage());
+                        } else {
+                            log.error(error, topicFuture.getKey(), cause);
+                            throw new StreamsException(cause);
+                        }
+                    }
+                }
             }
-            // backoff
-            time.sleep(100L);
+
+            if (retry) {
+                topics.removeAll(existingNumberOfPartitionsPerTopic.keySet());
+                continue;
+            }
+
+            return existingNumberOfPartitionsPerTopic;
+        } while (remainingRetries-- > 0);
+
+        if (bestEffort) {
+            return Collections.emptyMap();
         }
-        throw new StreamsException("Could not get number of partitions.");
+
+        final String timeoutAndRetryError = "Could not get number of partitions from brokers. " +
+            "This can happen if the Kafka cluster is temporary not available. " +
+            "You can increase admin client config `retries` to be resilient against this error.";
+        log.error(timeoutAndRetryError);
+        throw new StreamsException(timeoutAndRetryError);
     }
 
     /**
      * Check the existing topics to have correct number of partitions; and return the non existing topics to be created
      */
-    private Map<InternalTopicConfig, Integer> validateTopicPartitions(final Map<InternalTopicConfig, Integer> topicsPartitionsMap,
-                                                                      final Map<String, Integer> existingTopicNamesPartitions) {
-        final Map<InternalTopicConfig, Integer> topicsToBeCreated = new HashMap<>();
-        for (Map.Entry<InternalTopicConfig, Integer> entry : topicsPartitionsMap.entrySet()) {
-            InternalTopicConfig topic = entry.getKey();
-            Integer partition = entry.getValue();
+    private Set<InternalTopicConfig> validateTopicPartitions(final Collection<InternalTopicConfig> topicsPartitionsMap,
+                                                             final Map<String, Integer> existingTopicNamesPartitions) {
+        final Set<InternalTopicConfig> topicsToBeCreated = new HashSet<>();
+        for (final InternalTopicConfig topic : topicsPartitionsMap) {
+            final Integer numberOfPartitions = topic.numberOfPartitions();
             if (existingTopicNamesPartitions.containsKey(topic.name())) {
-                if (!existingTopicNamesPartitions.get(topic.name()).equals(partition)) {
-                    throw new StreamsException("Existing internal topic " + topic.name() + " has invalid partitions." +
-                            " Expected: " + partition + " Actual: " + existingTopicNamesPartitions.get(topic.name()) +
-                            ". Use 'kafka.tools.StreamsResetter' tool to clean up invalid topics before processing.");
+                if (!existingTopicNamesPartitions.get(topic.name()).equals(numberOfPartitions)) {
+                    final String errorMsg = String.format("Existing internal topic %s has invalid partitions: " +
+                            "expected: %d; actual: %d. " +
+                            "Use 'kafka.tools.StreamsResetter' tool to clean up invalid topics before processing.",
+                        topic.name(), numberOfPartitions, existingTopicNamesPartitions.get(topic.name()));
+                    log.error(errorMsg);
+                    throw new StreamsException(errorMsg);
                 }
             } else {
-                topicsToBeCreated.put(topic, partition);
+                topicsToBeCreated.add(topic);
             }
         }
 
         return topicsToBeCreated;
     }
 
-    private Map<String, Integer> fetchExistingPartitionCountByTopic(final MetadataResponse metadata) {
-        // The names of existing topics and corresponding partition counts
-        final Map<String, Integer> existingPartitionCountByTopic = new HashMap<>();
-        final Collection<MetadataResponse.TopicMetadata> topicsMetadata = metadata.topicMetadata();
-
-        for (MetadataResponse.TopicMetadata topicMetadata: topicsMetadata) {
-            existingPartitionCountByTopic.put(topicMetadata.topic(), topicMetadata.partitionMetadata().size());
-        }
-
-        return existingPartitionCountByTopic;
-    }
 }
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java
index ec42a86aaa..57c69c8fd7 100644
--- a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java
@@ -26,7 +26,6 @@ import org.apache.kafka.common.PartitionInfo;
 import org.apache.kafka.common.TopicPartition;
 import org.apache.kafka.common.config.ConfigException;
 import org.apache.kafka.common.utils.LogContext;
-import org.apache.kafka.common.utils.Time;
 import org.apache.kafka.common.utils.Utils;
 import org.apache.kafka.streams.StreamsConfig;
 import org.apache.kafka.streams.errors.TaskAssignmentException;
@@ -56,7 +55,6 @@ import static org.apache.kafka.common.utils.Utils.getPort;
 
 public class StreamPartitionAssignor implements PartitionAssignor, Configurable {
 
-    private Time time = Time.SYSTEM;
     private final static int UNKNOWN = -1;
     public final static int NOT_AVAILABLE = -2;
 
@@ -178,14 +176,6 @@ public class StreamPartitionAssignor implements PartitionAssignor, Configurable
     private InternalTopicManager internalTopicManager;
     private CopartitionedTopicsValidator copartitionedTopicsValidator;
 
-    /**
-     * Package-private method to set the time. Used for tests.
-     * @param time Time to be used.
-     */
-    void time(final Time time) {
-        this.time = time;
-    }
-
     /**
      * We need to have the PartitionAssignor and its StreamThread to be mutually accessible
      * since the former needs later's cached metadata while sending subscriptions,
@@ -193,7 +183,7 @@ public class StreamPartitionAssignor implements PartitionAssignor, Configurable
      * @throws KafkaException if the stream thread is not specified
      */
     @Override
-    public void configure(Map<String, ?> configs) {
+    public void configure(final Map<String, ?> configs) {
         final StreamsConfig streamsConfig = new StreamsConfig(configs);
 
         // Setting the logger with the passed in client thread name
@@ -238,11 +228,7 @@ public class StreamPartitionAssignor implements PartitionAssignor, Configurable
             this.userEndPoint = userEndPoint;
         }
 
-        internalTopicManager = new InternalTopicManager(
-                taskManager.streamsKafkaClient,
-                streamsConfig.getInt(StreamsConfig.REPLICATION_FACTOR_CONFIG),
-                streamsConfig.getLong(StreamsConfig.WINDOW_STORE_CHANGE_LOG_ADDITIONAL_RETENTION_MS_CONFIG),
-                time);
+        internalTopicManager = new InternalTopicManager(taskManager.adminClient, configs);
 
         copartitionedTopicsValidator = new CopartitionedTopicsValidator(logPrefix);
     }
@@ -626,12 +612,11 @@ public class StreamPartitionAssignor implements PartitionAssignor, Configurable
         log.debug("Starting to validate internal topics in partition assignor.");
 
         // first construct the topics to make ready
-        Map<InternalTopicConfig, Integer> topicsToMakeReady = new HashMap<>();
-        Set<String> topicNamesToMakeReady = new HashSet<>();
+        final Map<String, InternalTopicConfig> topicsToMakeReady = new HashMap<>();
 
-        for (InternalTopicMetadata metadata : topicPartitions.values()) {
-            InternalTopicConfig topic = metadata.config;
-            Integer numPartitions = metadata.numPartitions;
+        for (final InternalTopicMetadata metadata : topicPartitions.values()) {
+            final InternalTopicConfig topic = metadata.config;
+            final Integer numPartitions = metadata.numPartitions;
 
             if (numPartitions == NOT_AVAILABLE) {
                 continue;
@@ -640,18 +625,19 @@ public class StreamPartitionAssignor implements PartitionAssignor, Configurable
                 throw new org.apache.kafka.streams.errors.TopologyBuilderException(String.format("%sTopic [%s] number of partitions not defined", logPrefix, topic.name()));
             }
 
-            topicsToMakeReady.put(topic, numPartitions);
-            topicNamesToMakeReady.add(topic.name());
+            topic.setNumberOfPartitions(numPartitions);
+            topicsToMakeReady.put(topic.name(), topic);
         }
 
         if (!topicsToMakeReady.isEmpty()) {
             internalTopicManager.makeReady(topicsToMakeReady);
 
             // wait until each one of the topic metadata has been propagated to at least one broker
-            while (!allTopicsCreated(topicNamesToMakeReady, topicsToMakeReady)) {
+            while (!allTopicsCreated(topicsToMakeReady)) {
                 try {
                     Thread.sleep(50L);
                 } catch (InterruptedException e) {
+                    Thread.currentThread().interrupt();
                     // ignore
                 }
             }
@@ -660,11 +646,11 @@ public class StreamPartitionAssignor implements PartitionAssignor, Configurable
         log.debug("Completed validating internal topics in partition assignor.");
     }
 
-    private boolean allTopicsCreated(final Set<String> topicNamesToMakeReady, final Map<InternalTopicConfig, Integer> topicsToMakeReady) {
-        final Map<String, Integer> partitions = internalTopicManager.getNumPartitions(topicNamesToMakeReady);
-        for (Map.Entry<InternalTopicConfig, Integer> entry : topicsToMakeReady.entrySet()) {
-            final Integer numPartitions = partitions.get(entry.getKey().name());
-            if (numPartitions == null || !numPartitions.equals(entry.getValue())) {
+    private boolean allTopicsCreated(final Map<String, InternalTopicConfig> topicsToMakeReady) {
+        final Map<String, Integer> partitions = internalTopicManager.getNumPartitions(topicsToMakeReady.keySet());
+        for (final InternalTopicConfig topic : topicsToMakeReady.values()) {
+            final Integer numPartitions = partitions.get(topic.name());
+            if (numPartitions == null || !numPartitions.equals(topic.numberOfPartitions())) {
                 return false;
             }
         }
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java
index 5460c0e66f..55456d0a72 100644
--- a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java
@@ -97,6 +97,8 @@ public class StreamTask extends AbstractTask implements ProcessorNodePunctuator
      * @param config                the {@link StreamsConfig} specified by the user
      * @param metrics               the {@link StreamsMetrics} created by the thread
      * @param stateDirectory        the {@link StateDirectory} created by the thread
+     * @param cache                 the {@link ThreadCache} created by the thread
+     * @param time                  the system {@link Time} of the thread
      * @param producer              the instance of {@link Producer} used to produce records
      * @throws TaskMigratedException if the task producer got fenced (EOS only)
      */
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java
index d930a6753d..a9786f906d 100644
--- a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java
@@ -560,8 +560,6 @@ public class StreamThread extends Thread {
     private final Object stateLock;
     private final Logger log;
     private final String logPrefix;
-    // TODO: adminClient will be passeed to taskManager to be accessed in StreamPartitionAssignor
-    private final AdminClient adminClient;
     private final TaskManager taskManager;
     private final StreamsMetricsThreadImpl streamsMetrics;
 
@@ -620,8 +618,6 @@ public class StreamThread extends Thread {
 
         final ThreadCache cache = new ThreadCache(logContext, cacheSizeBytes, streamsMetrics);
 
-        final StreamsKafkaClient streamsKafkaClient = StreamsKafkaClient.create(config.originals());
-
         final AbstractTaskCreator<StreamTask> activeTaskCreator = new TaskCreator(builder,
                                                                                   config,
                                                                                   streamsMetrics,
@@ -649,7 +645,6 @@ public class StreamThread extends Thread {
                                                   streamsMetadataState,
                                                   activeTaskCreator,
                                                   standbyTaskCreator,
-                                                  streamsKafkaClient,
                                                   adminClient,
                                                   new AssignedStreamsTasks(logContext),
                                                   new AssignedStandbyTasks(logContext));
@@ -671,7 +666,6 @@ public class StreamThread extends Thread {
                 restoreConsumer,
                 consumer,
                 originalReset,
-                adminClient,
                 taskManager,
                 streamsMetrics,
                 builder,
@@ -684,7 +678,6 @@ public class StreamThread extends Thread {
                         final Consumer<byte[], byte[]> restoreConsumer,
                         final Consumer<byte[], byte[]> consumer,
                         final String originalReset,
-                        final AdminClient adminClient,
                         final TaskManager taskManager,
                         final StreamsMetricsThreadImpl streamsMetrics,
                         final InternalTopologyBuilder builder,
@@ -705,7 +698,6 @@ public class StreamThread extends Thread {
         this.restoreConsumer = restoreConsumer;
         this.consumer = consumer;
         this.originalReset = originalReset;
-        this.adminClient = adminClient;
 
         this.pollTimeMs = config.getLong(StreamsConfig.POLL_MS_CONFIG);
         this.commitTimeMs = config.getLong(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG);
@@ -716,8 +708,8 @@ public class StreamThread extends Thread {
     /**
      * Execute the stream processors
      *
-     * @throws KafkaException for any Kafka-related exceptions
-     * @throws Exception      for any other non-Kafka exceptions
+     * @throws KafkaException    for any Kafka-related exceptions
+     * @throws RuntimeException  for any other non-Kafka exceptions
      */
     @Override
     public void run() {
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsKafkaClient.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsKafkaClient.java
deleted file mode 100644
index 1e21878c0f..0000000000
--- a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsKafkaClient.java
+++ /dev/null
@@ -1,357 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License. You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.kafka.streams.processor.internals;
-
-import org.apache.kafka.clients.ApiVersions;
-import org.apache.kafka.clients.ClientRequest;
-import org.apache.kafka.clients.ClientResponse;
-import org.apache.kafka.clients.ClientUtils;
-import org.apache.kafka.clients.CommonClientConfigs;
-import org.apache.kafka.clients.KafkaClient;
-import org.apache.kafka.clients.Metadata;
-import org.apache.kafka.clients.NetworkClient;
-import org.apache.kafka.clients.consumer.ConsumerConfig;
-import org.apache.kafka.clients.producer.ProducerConfig;
-import org.apache.kafka.common.Cluster;
-import org.apache.kafka.common.Node;
-import org.apache.kafka.common.config.AbstractConfig;
-import org.apache.kafka.common.config.ConfigDef;
-import org.apache.kafka.common.errors.TimeoutException;
-import org.apache.kafka.common.metrics.JmxReporter;
-import org.apache.kafka.common.metrics.MetricConfig;
-import org.apache.kafka.common.metrics.Metrics;
-import org.apache.kafka.common.metrics.MetricsReporter;
-import org.apache.kafka.common.network.ChannelBuilder;
-import org.apache.kafka.common.network.Selector;
-import org.apache.kafka.common.protocol.Errors;
-import org.apache.kafka.common.requests.ApiError;
-import org.apache.kafka.common.requests.CreateTopicsRequest;
-import org.apache.kafka.common.requests.CreateTopicsResponse;
-import org.apache.kafka.common.requests.MetadataRequest;
-import org.apache.kafka.common.requests.MetadataResponse;
-import org.apache.kafka.common.utils.LogContext;
-import org.apache.kafka.common.utils.SystemTime;
-import org.apache.kafka.common.utils.Time;
-import org.apache.kafka.streams.StreamsConfig;
-import org.apache.kafka.streams.errors.BrokerNotFoundException;
-import org.apache.kafka.streams.errors.StreamsException;
-import org.slf4j.Logger;
-
-import java.io.IOException;
-import java.net.InetSocketAddress;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.LinkedHashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Properties;
-import java.util.concurrent.TimeUnit;
-
-public class StreamsKafkaClient {
-
-    private static final ConfigDef CONFIG = StreamsConfig.configDef()
-            .withClientSslSupport()
-            .withClientSaslSupport();
-
-    public static class Config extends AbstractConfig {
-
-        static Config fromStreamsConfig(Map<String, ?> props) {
-            return new Config(props);
-        }
-
-        Config(Map<?, ?> originals) {
-            super(CONFIG, originals, false);
-        }
-
-    }
-    private final KafkaClient kafkaClient;
-    private final List<MetricsReporter> reporters;
-    private final Config streamsConfig;
-    private final Logger log;
-    private final Map<String, String> defaultTopicConfigs = new HashMap<>();
-    private static final int MAX_INFLIGHT_REQUESTS = 100;
-
-    StreamsKafkaClient(final Config streamsConfig,
-                       final KafkaClient kafkaClient,
-                       final List<MetricsReporter> reporters,
-                       final LogContext log) {
-        this.streamsConfig = streamsConfig;
-        this.kafkaClient = kafkaClient;
-        this.reporters = reporters;
-        this.log = log.logger(StreamsKafkaClient.class);
-        extractDefaultTopicConfigs(streamsConfig.originalsWithPrefix(StreamsConfig.TOPIC_PREFIX));
-    }
-
-    private void extractDefaultTopicConfigs(final Map<String, Object> configs) {
-        for (final Map.Entry<String, Object> entry : configs.entrySet()) {
-            if (entry.getValue() != null) {
-                defaultTopicConfigs.put(entry.getKey(), entry.getValue().toString());
-            }
-        }
-    }
-
-
-    public static StreamsKafkaClient create(final Config streamsConfig) {
-        final Time time = new SystemTime();
-
-        final Map<String, String> metricTags = new LinkedHashMap<>();
-        final String clientId = streamsConfig.getString(StreamsConfig.CLIENT_ID_CONFIG);
-        metricTags.put("client-id", clientId);
-
-        final Metadata metadata = new Metadata(streamsConfig.getLong(StreamsConfig.RETRY_BACKOFF_MS_CONFIG),
-                                               streamsConfig.getLong(StreamsConfig.METADATA_MAX_AGE_CONFIG),
-                                               false);
-        final List<InetSocketAddress> addresses = ClientUtils.parseAndValidateAddresses(streamsConfig.getList(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG));
-        metadata.update(Cluster.bootstrap(addresses), Collections.<String>emptySet(), time.milliseconds());
-
-        final MetricConfig metricConfig = new MetricConfig().samples(streamsConfig.getInt(CommonClientConfigs.METRICS_NUM_SAMPLES_CONFIG))
-                .timeWindow(streamsConfig.getLong(CommonClientConfigs.METRICS_SAMPLE_WINDOW_MS_CONFIG), TimeUnit.MILLISECONDS)
-                .tags(metricTags);
-        final List<MetricsReporter> reporters = streamsConfig.getConfiguredInstances(
-            ProducerConfig.METRIC_REPORTER_CLASSES_CONFIG,
-            MetricsReporter.class);
-        // TODO: This should come from the KafkaStream
-        reporters.add(new JmxReporter("kafka.admin.client"));
-        final Metrics metrics = new Metrics(metricConfig, reporters, time);
-
-        final ChannelBuilder channelBuilder = ClientUtils.createChannelBuilder(streamsConfig);
-        final LogContext logContext = createLogContext(clientId);
-
-        final Selector selector = new Selector(
-                streamsConfig.getLong(StreamsConfig.CONNECTIONS_MAX_IDLE_MS_CONFIG),
-                metrics,
-                time,
-                "kafka-client",
-                channelBuilder,
-                logContext);
-
-        final KafkaClient kafkaClient = new NetworkClient(
-                selector,
-                metadata,
-                clientId,
-                MAX_INFLIGHT_REQUESTS, // a fixed large enough value will suffice
-                streamsConfig.getLong(StreamsConfig.RECONNECT_BACKOFF_MS_CONFIG),
-                streamsConfig.getLong(StreamsConfig.RECONNECT_BACKOFF_MAX_MS_CONFIG),
-                streamsConfig.getInt(StreamsConfig.SEND_BUFFER_CONFIG),
-                streamsConfig.getInt(StreamsConfig.RECEIVE_BUFFER_CONFIG),
-                streamsConfig.getInt(StreamsConfig.REQUEST_TIMEOUT_MS_CONFIG),
-                time,
-                true,
-                new ApiVersions(),
-                logContext);
-        return new StreamsKafkaClient(streamsConfig, kafkaClient, reporters, logContext);
-    }
-
-    private static LogContext createLogContext(String clientId) {
-        return new LogContext("[StreamsKafkaClient clientId=" + clientId + "] ");
-    }
-
-    public static StreamsKafkaClient create(final Map<String, ?> props) {
-        return create(Config.fromStreamsConfig(props));
-    }
-
-    public void close() {
-        try {
-            kafkaClient.close();
-        } catch (final IOException impossible) {
-            // this can actually never happen, because NetworkClient doesn't throw any exception on close()
-            // we log just in case
-            log.error("This error indicates a bug in the code. Please report to dev@kafka.apache.org.", impossible);
-        } finally {
-            for (MetricsReporter metricsReporter: this.reporters) {
-                metricsReporter.close();
-            }
-        }
-    }
-
-    /**
-     * Create a set of new topics using batch request.
-     *
-     * @throws BrokerNotFoundException if connecting failed within {@code request.timeout.ms}
-     * @throws TimeoutException if there was no response within {@code request.timeout.ms}
-     * @throws StreamsException for any other fatal error
-     */
-    public void createTopics(final Map<InternalTopicConfig, Integer> topicsMap,
-                             final int replicationFactor,
-                             final long windowChangeLogAdditionalRetention,
-                             final MetadataResponse metadata) {
-        final Map<String, CreateTopicsRequest.TopicDetails> topicRequestDetails = new HashMap<>();
-        for (final Map.Entry<InternalTopicConfig, Integer> entry : topicsMap.entrySet()) {
-            final InternalTopicConfig internalTopicConfig = entry.getKey();
-            final Integer partitions = entry.getValue();
-            final Properties topicProperties = internalTopicConfig.toProperties(windowChangeLogAdditionalRetention);
-            final Map<String, String> topicConfig = new HashMap<>(defaultTopicConfigs);
-            for (final String key : topicProperties.stringPropertyNames()) {
-                topicConfig.put(key, topicProperties.getProperty(key));
-            }
-            final CreateTopicsRequest.TopicDetails topicDetails = new CreateTopicsRequest.TopicDetails(
-                partitions,
-                (short) replicationFactor,
-                topicConfig);
-
-            topicRequestDetails.put(internalTopicConfig.name(), topicDetails);
-        }
-
-        final ClientRequest clientRequest = kafkaClient.newClientRequest(
-            getControllerReadyBrokerId(metadata),
-            new CreateTopicsRequest.Builder(
-                topicRequestDetails,
-                streamsConfig.getInt(StreamsConfig.REQUEST_TIMEOUT_MS_CONFIG)),
-            Time.SYSTEM.milliseconds(),
-            true);
-        final ClientResponse clientResponse = sendRequestSync(clientRequest);
-
-        if (!clientResponse.hasResponse()) {
-            throw new StreamsException("Empty response for client request.");
-        }
-        if (!(clientResponse.responseBody() instanceof CreateTopicsResponse)) {
-            throw new StreamsException("Inconsistent response type for internal topic creation request. " +
-                "Expected CreateTopicsResponse but received " + clientResponse.responseBody().getClass().getName());
-        }
-        final CreateTopicsResponse createTopicsResponse =  (CreateTopicsResponse) clientResponse.responseBody();
-
-        for (InternalTopicConfig internalTopicConfig : topicsMap.keySet()) {
-            ApiError error = createTopicsResponse.errors().get(internalTopicConfig.name());
-            if (error.isFailure() && !error.is(Errors.TOPIC_ALREADY_EXISTS)) {
-                throw new StreamsException("Could not create topic: " + internalTopicConfig.name() + " due to " + error.messageWithFallback());
-            }
-        }
-    }
-
-    /**
-     *
-     * @param nodes List of nodes to pick from.
-     * @return The first node that is ready to accept requests.
-     * @throws BrokerNotFoundException if connecting failed within {@code request.timeout.ms}
-     */
-    private String ensureOneNodeIsReady(final List<Node> nodes) {
-        String brokerId = null;
-        final long readyTimeout = Time.SYSTEM.milliseconds() + streamsConfig.getInt(StreamsConfig.REQUEST_TIMEOUT_MS_CONFIG);
-        boolean foundNode = false;
-        while (!foundNode && (Time.SYSTEM.milliseconds() < readyTimeout)) {
-            for (Node node: nodes) {
-                if (kafkaClient.ready(node, Time.SYSTEM.milliseconds())) {
-                    brokerId = Integer.toString(node.id());
-                    foundNode = true;
-                    break;
-                }
-            }
-            try {
-                kafkaClient.poll(50, Time.SYSTEM.milliseconds());
-            } catch (final RuntimeException e) {
-                throw new StreamsException("Could not poll.", e);
-            }
-        }
-        if (brokerId == null) {
-            throw new BrokerNotFoundException("Could not find any available broker. " +
-                "Check your StreamsConfig setting '" + StreamsConfig.BOOTSTRAP_SERVERS_CONFIG + "'. " +
-                "This error might also occur, if you try to connect to pre-0.10 brokers. " +
-                "Kafka Streams requires broker version 0.10.1.x or higher.");
-        }
-        return brokerId;
-    }
-
-    /**
-     * @return if Id of the controller node, or an exception if no controller is found or
-     * controller is not ready
-     * @throws BrokerNotFoundException if connecting failed within {@code request.timeout.ms}
-     */
-    private String getControllerReadyBrokerId(final MetadataResponse metadata) {
-        return ensureOneNodeIsReady(Collections.singletonList(metadata.controller()));
-    }
-
-    /**
-     * @return the Id of any broker that is ready, or an exception if no broker is ready.
-     * @throws BrokerNotFoundException if connecting failed within {@code request.timeout.ms}
-     */
-    private String getAnyReadyBrokerId() {
-        final Metadata metadata = new Metadata(
-            streamsConfig.getLong(StreamsConfig.RETRY_BACKOFF_MS_CONFIG),
-            streamsConfig.getLong(StreamsConfig.METADATA_MAX_AGE_CONFIG),
-            false);
-        final List<InetSocketAddress> addresses = ClientUtils.parseAndValidateAddresses(streamsConfig.getList(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG));
-        metadata.update(Cluster.bootstrap(addresses), Collections.<String>emptySet(), Time.SYSTEM.milliseconds());
-
-        final List<Node> nodes = metadata.fetch().nodes();
-        return ensureOneNodeIsReady(nodes);
-    }
-
-    /**
-     * @return the response to the request
-     * @throws TimeoutException if there was no response within {@code request.timeout.ms}
-     * @throws StreamsException any other fatal error
-     */
-    private ClientResponse sendRequestSync(final ClientRequest clientRequest) {
-        try {
-            kafkaClient.send(clientRequest, Time.SYSTEM.milliseconds());
-        } catch (final RuntimeException e) {
-            throw new StreamsException("Could not send request.", e);
-        }
-
-        // Poll for the response.
-        final long responseTimeout = Time.SYSTEM.milliseconds() + streamsConfig.getInt(StreamsConfig.REQUEST_TIMEOUT_MS_CONFIG);
-        while (Time.SYSTEM.milliseconds() < responseTimeout) {
-            final List<ClientResponse> responseList;
-            try {
-                responseList = kafkaClient.poll(100, Time.SYSTEM.milliseconds());
-            } catch (final RuntimeException e) {
-                throw new StreamsException("Could not poll.", e);
-            }
-            if (!responseList.isEmpty()) {
-                if (responseList.size() > 1) {
-                    throw new StreamsException("Sent one request but received multiple or no responses.");
-                }
-                final ClientResponse response = responseList.get(0);
-                if (response.requestHeader().correlationId() == clientRequest.correlationId()) {
-                    return response;
-                } else {
-                    throw new StreamsException("Inconsistent response received from the broker "
-                        + clientRequest.destination() + ", expected correlation id " + clientRequest.correlationId()
-                        + ", but received " + response.requestHeader().correlationId());
-                }
-            }
-        }
-
-        throw new TimeoutException("Failed to get response from broker within timeout");
-    }
-
-    /**
-     * Fetch the metadata for all topics.
-     *
-     * @throws BrokerNotFoundException if connecting failed within {@code request.timeout.ms}
-     * @throws TimeoutException if there was no response within {@code request.timeout.ms}
-     * @throws StreamsException for any other fatal error
-     */
-    public MetadataResponse fetchMetadata() {
-        final ClientRequest clientRequest = kafkaClient.newClientRequest(
-            getAnyReadyBrokerId(),
-            MetadataRequest.Builder.allTopics(),
-            Time.SYSTEM.milliseconds(),
-            true);
-        final ClientResponse clientResponse = sendRequestSync(clientRequest);
-
-        if (!clientResponse.hasResponse()) {
-            throw new StreamsException("Empty response for client request.");
-        }
-        if (!(clientResponse.responseBody() instanceof MetadataResponse)) {
-            throw new StreamsException("Inconsistent response type for internal topic metadata request. " +
-                "Expected MetadataResponse but received " + clientResponse.responseBody().getClass().getName());
-        }
-        return (MetadataResponse) clientResponse.responseBody();
-    }
-
-}
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java
index e6f05cd0af..d70c8f393e 100644
--- a/streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java
@@ -57,10 +57,7 @@ class TaskManager {
     private final StreamThread.AbstractTaskCreator<StandbyTask> standbyTaskCreator;
     private final StreamsMetadataState streamsMetadataState;
 
-    // TODO: this is going to be replaced by AdminClient
-    final StreamsKafkaClient streamsKafkaClient;
-
-    private final AdminClient adminClient;
+    final AdminClient adminClient;
     private DeleteRecordsResult deleteRecordsResult;
 
     // following information is updated during rebalance phase by the partition assignor
@@ -77,7 +74,6 @@ class TaskManager {
                 final StreamsMetadataState streamsMetadataState,
                 final StreamThread.AbstractTaskCreator<StreamTask> taskCreator,
                 final StreamThread.AbstractTaskCreator<StandbyTask> standbyTaskCreator,
-                final StreamsKafkaClient streamsKafkaClient,
                 final AdminClient adminClient,
                 final AssignedStreamsTasks active,
                 final AssignedStandbyTasks standby) {
@@ -95,7 +91,6 @@ class TaskManager {
 
         this.log = logContext.logger(getClass());
 
-        this.streamsKafkaClient = streamsKafkaClient;
         this.adminClient = adminClient;
     }
 
@@ -283,8 +278,6 @@ class TaskManager {
         taskCreator.close();
         standbyTaskCreator.close();
 
-        streamsKafkaClient.close();
-
         final RuntimeException fatalException = firstException.get();
         if (fatalException != null) {
             throw fatalException;
diff --git a/streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java b/streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java
index 1a4cfb1d48..a06f7e86d3 100644
--- a/streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java
+++ b/streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java
@@ -16,6 +16,7 @@
  */
 package org.apache.kafka.streams;
 
+import org.apache.kafka.clients.admin.AdminClientConfig;
 import org.apache.kafka.clients.consumer.ConsumerConfig;
 import org.apache.kafka.clients.consumer.ConsumerRecord;
 import org.apache.kafka.clients.producer.ProducerConfig;
@@ -28,6 +29,7 @@ import org.apache.kafka.common.utils.Utils;
 import org.apache.kafka.streams.errors.StreamsException;
 import org.apache.kafka.streams.processor.FailOnInvalidTimestamp;
 import org.apache.kafka.streams.processor.TimestampExtractor;
+import org.apache.kafka.streams.processor.internals.StreamPartitionAssignor;
 import org.hamcrest.CoreMatchers;
 import org.junit.Before;
 import org.junit.Test;
@@ -101,6 +103,40 @@ public class StreamsConfigTest {
         assertNull(returnedProps.get("DUMMY"));
     }
 
+    @Test
+    public void consumerConfigMustContainStreamPartitionAssignorConfig() {
+        props.put(StreamsConfig.REPLICATION_FACTOR_CONFIG, 42);
+        props.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 1);
+        props.put(StreamsConfig.WINDOW_STORE_CHANGE_LOG_ADDITIONAL_RETENTION_MS_CONFIG, 7L);
+        props.put(StreamsConfig.APPLICATION_SERVER_CONFIG, "dummy:host");
+        props.put(StreamsConfig.RETRIES_CONFIG, 10);
+        final StreamsConfig streamsConfig = new StreamsConfig(props);
+
+        final String groupId = "example-application";
+        final String clientId = "client";
+        final Map<String, Object> returnedProps = streamsConfig.getConsumerConfigs(groupId, clientId);
+
+        assertEquals(42, returnedProps.get(StreamsConfig.REPLICATION_FACTOR_CONFIG));
+        assertEquals(1, returnedProps.get(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG));
+        assertEquals(StreamPartitionAssignor.class.getName(), returnedProps.get(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG));
+        assertEquals(7L, returnedProps.get(StreamsConfig.WINDOW_STORE_CHANGE_LOG_ADDITIONAL_RETENTION_MS_CONFIG));
+        assertEquals("dummy:host", returnedProps.get(StreamsConfig.APPLICATION_SERVER_CONFIG));
+        assertEquals(10, returnedProps.get(StreamsConfig.adminClientPrefix(StreamsConfig.RETRIES_CONFIG)));
+    }
+
+    @Test
+    public void consumerConfigMustUseAdminClientConfigForRetries() {
+        props.put(StreamsConfig.adminClientPrefix(StreamsConfig.RETRIES_CONFIG), 20);
+        props.put(StreamsConfig.RETRIES_CONFIG, 10);
+        final StreamsConfig streamsConfig = new StreamsConfig(props);
+
+        final String groupId = "example-application";
+        final String clientId = "client";
+        final Map<String, Object> returnedProps = streamsConfig.getConsumerConfigs(groupId, clientId);
+
+        assertEquals(20, returnedProps.get(StreamsConfig.adminClientPrefix(StreamsConfig.RETRIES_CONFIG)));
+    }
+
     @Test
     public void testGetRestoreConsumerConfigs() {
         final String clientId = "client";
@@ -227,7 +263,13 @@ public class StreamsConfigTest {
         assertEquals(1, configs.get(ProducerConfig.METRICS_NUM_SAMPLES_CONFIG));
     }
 
-
+    @Test
+    public void shouldSupportNonPrefixedAdminConfigs() {
+        props.put(AdminClientConfig.RETRIES_CONFIG, 10);
+        final StreamsConfig streamsConfig = new StreamsConfig(props);
+        final Map<String, Object> configs = streamsConfig.getAdminConfigs("clientId");
+        assertEquals(10, configs.get(AdminClientConfig.RETRIES_CONFIG));
+    }
 
     @Test(expected = StreamsException.class)
     public void shouldThrowStreamsExceptionIfKeySerdeConfigFails() {
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/InternalTopicConfigTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/InternalTopicConfigTest.java
index 044e82ab09..581c8cbbd4 100644
--- a/streams/src/test/java/org/apache/kafka/streams/processor/internals/InternalTopicConfigTest.java
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/InternalTopicConfigTest.java
@@ -88,8 +88,9 @@ public class InternalTopicConfigTest {
         assertTrue(new InternalTopicConfig("name",
                                            Collections.singleton(InternalTopicConfig.CleanupPolicy.compact),
                                            Collections.<String, String>emptyMap()).isCompacted());
-        assertTrue(new InternalTopicConfig("name", Utils.mkSet(InternalTopicConfig.CleanupPolicy.compact,
-                                                               InternalTopicConfig.CleanupPolicy.delete),
+        assertTrue(new InternalTopicConfig("name",
+                                           Utils.mkSet(InternalTopicConfig.CleanupPolicy.compact,
+                                                       InternalTopicConfig.CleanupPolicy.delete),
                                            Collections.<String, String>emptyMap()).isCompacted());
     }
 
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/InternalTopicManagerTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/InternalTopicManagerTest.java
index e914f9e663..10d3dd9482 100644
--- a/streams/src/test/java/org/apache/kafka/streams/processor/internals/InternalTopicManagerTest.java
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/InternalTopicManagerTest.java
@@ -16,19 +16,14 @@
  */
 package org.apache.kafka.streams.processor.internals;
 
-import org.apache.kafka.clients.MockClient;
+import org.apache.kafka.clients.admin.MockAdminClient;
+import org.apache.kafka.clients.admin.TopicDescription;
 import org.apache.kafka.common.Node;
-import org.apache.kafka.common.metrics.MetricsReporter;
-import org.apache.kafka.common.protocol.Errors;
-import org.apache.kafka.common.requests.MetadataResponse;
-import org.apache.kafka.common.utils.LogContext;
-import org.apache.kafka.common.utils.MockTime;
-import org.apache.kafka.common.utils.Time;
+import org.apache.kafka.common.TopicPartitionInfo;
+import org.apache.kafka.common.errors.UnknownTopicOrPartitionException;
 import org.apache.kafka.streams.StreamsConfig;
 import org.apache.kafka.streams.errors.StreamsException;
-import org.apache.kafka.test.MockTimestampExtractor;
 import org.junit.After;
-import org.junit.Assert;
 import org.junit.Before;
 import org.junit.Test;
 
@@ -36,172 +31,169 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Collections;
 import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
 import java.util.Map;
-import java.util.Properties;
 
-import static org.apache.kafka.streams.processor.internals.InternalTopicManager.WINDOW_CHANGE_LOG_ADDITIONAL_RETENTION_DEFAULT;
 import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertNull;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
 
 public class InternalTopicManagerTest {
 
+    private final Node broker1 = new Node(0, "dummyHost-1", 1234);
+    private final Node broker2 = new Node(1, "dummyHost-2", 1234);
+    private final List<Node> cluster = new ArrayList<Node>(2) {
+        {
+            add(broker1);
+            add(broker2);
+        }
+    };
     private final String topic = "test_topic";
-    private final String userEndPoint = "localhost:2171";
-    private MockStreamKafkaClient streamsKafkaClient;
-    private final Time time = new MockTime();
+    private final String topic2 = "test_topic_2";
+    private final List<Node> singleReplica = Collections.singletonList(broker1);
+
+    private MockAdminClient mockAdminClient;
+    private InternalTopicManager internalTopicManager;
+
+    private final Map<String, Object> config = new HashMap<String, Object>() {
+        {
+            put(StreamsConfig.APPLICATION_ID_CONFIG, "app-id");
+            put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, broker1.host() + ":" + broker1.port());
+            put(StreamsConfig.REPLICATION_FACTOR_CONFIG, 1);
+            put(StreamsConfig.adminClientPrefix(StreamsConfig.RETRIES_CONFIG), 1);
+        }
+    };
 
     @Before
     public void init() {
-        final StreamsConfig config = new StreamsConfig(configProps());
-        streamsKafkaClient = new MockStreamKafkaClient(config);
+        mockAdminClient = new MockAdminClient(cluster);
+        internalTopicManager = new InternalTopicManager(
+            mockAdminClient,
+            config);
     }
 
     @After
     public void shutdown() throws IOException {
-        streamsKafkaClient.close();
+        mockAdminClient.close();
     }
 
     @Test
     public void shouldReturnCorrectPartitionCounts() {
-        final InternalTopicManager internalTopicManager = new InternalTopicManager(
-            streamsKafkaClient,
-            1,
-            WINDOW_CHANGE_LOG_ADDITIONAL_RETENTION_DEFAULT,
-            time);
+        mockAdminClient.addTopic(
+            false,
+            topic,
+            Collections.singletonList(new TopicPartitionInfo(0, broker1, singleReplica, Collections.<Node>emptyList())),
+            null);
         assertEquals(Collections.singletonMap(topic, 1), internalTopicManager.getNumPartitions(Collections.singleton(topic)));
     }
 
     @Test
-    public void shouldCreateRequiredTopics() {
-        streamsKafkaClient.returnNoMetadata = true;
+    public void shouldFailWithUnknownTopicException() {
+        mockAdminClient.addTopic(
+            false,
+            topic,
+            Collections.singletonList(new TopicPartitionInfo(0, broker1, singleReplica, Collections.<Node>emptyList())),
+            null);
 
-        final InternalTopicManager internalTopicManager = new InternalTopicManager(
-            streamsKafkaClient,
-            1,
-            WINDOW_CHANGE_LOG_ADDITIONAL_RETENTION_DEFAULT,
-            time);
+        try {
+            internalTopicManager.getNumPartitions(new HashSet<String>() {
+                {
+                    add(topic);
+                    add(topic2);
+                }
+            });
+            fail("Should have thrown UnknownTopicOrPartitionException.");
+        } catch (final StreamsException expected) {
+            assertTrue(expected.getCause() instanceof UnknownTopicOrPartitionException);
+        }
+    }
 
-        final InternalTopicConfig topicConfig = new InternalTopicConfig(topic, Collections.singleton(InternalTopicConfig.CleanupPolicy.compact), null);
-        internalTopicManager.makeReady(Collections.singletonMap(topicConfig, 1));
+    @Test
+    public void shouldExhaustRetriesOnTimeoutExceptionForGetNumPartitions() {
+        mockAdminClient.timeoutNextRequest(2);
 
-        assertEquals(Collections.singletonMap(topic, topicConfig), streamsKafkaClient.createdTopics);
-        assertEquals(Collections.singletonMap(topic, 1), streamsKafkaClient.numberOfPartitionsPerTopic);
-        assertEquals(Collections.singletonMap(topic, 1), streamsKafkaClient.replicationFactorPerTopic);
+        try {
+            internalTopicManager.getNumPartitions(Collections.singleton(topic));
+            fail("Should have thrown StreamsException.");
+        } catch (final StreamsException expected) {
+            assertNull(expected.getCause());
+            assertEquals("Could not get number of partitions from brokers. This can happen if the Kafka cluster is temporary not available. You can increase admin client config `retries` to be resilient against this error.", expected.getMessage());
+        }
+    }
+
+    @Test
+    public void shouldCreateRequiredTopics() throws Exception {
+        final InternalTopicConfig topicConfig = new InternalTopicConfig(topic,  Collections.singleton(InternalTopicConfig.CleanupPolicy.compact), Collections.<String, String>emptyMap());
+        topicConfig.setNumberOfPartitions(1);
+        internalTopicManager.makeReady(Collections.singletonMap(topic, topicConfig));
+
+        assertEquals(Collections.singleton(topic), mockAdminClient.listTopics().names().get());
+        assertEquals(new TopicDescription(topic, false, new ArrayList<TopicPartitionInfo>() {
+            {
+                add(new TopicPartitionInfo(0, broker1, singleReplica, Collections.<Node>emptyList()));
+            }
+        }), mockAdminClient.describeTopics(Collections.singleton(topic)).values().get(topic).get());
     }
 
     @Test
     public void shouldNotCreateTopicIfExistsWithDifferentPartitions() {
-        final InternalTopicManager internalTopicManager = new InternalTopicManager(
-            streamsKafkaClient,
-            1,
-            WINDOW_CHANGE_LOG_ADDITIONAL_RETENTION_DEFAULT,
-            time);
+        mockAdminClient.addTopic(
+            false,
+            topic,
+            new ArrayList<TopicPartitionInfo>() {
+                {
+                    add(new TopicPartitionInfo(0, broker1, singleReplica, Collections.<Node>emptyList()));
+                    add(new TopicPartitionInfo(1, broker1, singleReplica, Collections.<Node>emptyList()));
+                }
+            },
+            null);
+
         try {
-            internalTopicManager.makeReady(Collections.singletonMap(new InternalTopicConfig(topic, Collections.singleton(InternalTopicConfig.CleanupPolicy.compact), null), 2));
-            Assert.fail("Should have thrown StreamsException");
+            final InternalTopicConfig internalTopicConfig = new InternalTopicConfig(topic, Collections.singleton(InternalTopicConfig.CleanupPolicy.delete), Collections.<String, String>emptyMap());
+            internalTopicConfig.setNumberOfPartitions(1);
+            internalTopicManager.makeReady(Collections.singletonMap(topic, internalTopicConfig));
+            fail("Should have thrown StreamsException");
         } catch (StreamsException expected) { /* pass */ }
     }
 
     @Test
     public void shouldNotThrowExceptionIfExistsWithDifferentReplication() {
-
-        // create topic the first time with replication 2
-        final InternalTopicManager internalTopicManager = new InternalTopicManager(
-            streamsKafkaClient,
-            2,
-            WINDOW_CHANGE_LOG_ADDITIONAL_RETENTION_DEFAULT,
-            time);
-        internalTopicManager.makeReady(Collections.singletonMap(
-            new InternalTopicConfig(topic,
-                                    Collections.singleton(InternalTopicConfig.CleanupPolicy.compact),
-                                    null),
-            1));
+        mockAdminClient.addTopic(
+            false,
+            topic,
+            Collections.singletonList(new TopicPartitionInfo(0, broker1, cluster, Collections.<Node>emptyList())),
+            null);
 
         // attempt to create it again with replication 1
         final InternalTopicManager internalTopicManager2 = new InternalTopicManager(
-            streamsKafkaClient,
-            1,
-            WINDOW_CHANGE_LOG_ADDITIONAL_RETENTION_DEFAULT,
-            time);
-
-        internalTopicManager2.makeReady(Collections.singletonMap(
-            new InternalTopicConfig(topic,
-                                    Collections.singleton(InternalTopicConfig.CleanupPolicy.compact),
-                                   null),
-            1));
+            mockAdminClient,
+            config);
+
+        final InternalTopicConfig internalTopicConfig = new InternalTopicConfig(topic, Collections.singleton(InternalTopicConfig.CleanupPolicy.delete), Collections.<String, String>emptyMap());
+        internalTopicConfig.setNumberOfPartitions(1);
+        internalTopicManager2.makeReady(Collections.singletonMap(topic, internalTopicConfig));
     }
 
     @Test
     public void shouldNotThrowExceptionForEmptyTopicMap() {
-        final InternalTopicManager internalTopicManager = new InternalTopicManager(
-            streamsKafkaClient,
-            1,
-            WINDOW_CHANGE_LOG_ADDITIONAL_RETENTION_DEFAULT,
-            time);
-
-        internalTopicManager.makeReady(Collections.<InternalTopicConfig, Integer>emptyMap());
-    }
-
-    private Properties configProps() {
-        return new Properties() {
-            {
-                setProperty(StreamsConfig.APPLICATION_ID_CONFIG, "Internal-Topic-ManagerTest");
-                setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, userEndPoint);
-                setProperty(StreamsConfig.BUFFERED_RECORDS_PER_PARTITION_CONFIG, "3");
-                setProperty(StreamsConfig.DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_CONFIG, MockTimestampExtractor.class.getName());
-            }
-        };
+        internalTopicManager.makeReady(Collections.<String, InternalTopicConfig>emptyMap());
     }
 
-    private class MockStreamKafkaClient extends StreamsKafkaClient {
-
-        boolean returnNoMetadata = false;
-
-        Map<String, InternalTopicConfig> createdTopics = new HashMap<>();
-        Map<String, Integer> numberOfPartitionsPerTopic = new HashMap<>();
-        Map<String, Integer> replicationFactorPerTopic = new HashMap<>();
-
-        MockStreamKafkaClient(final StreamsConfig streamsConfig) {
-            super(StreamsKafkaClient.Config.fromStreamsConfig(streamsConfig.originals()),
-                  new MockClient(new MockTime()),
-                  Collections.<MetricsReporter>emptyList(),
-                  new LogContext());
-        }
-
-        @Override
-        public void createTopics(final Map<InternalTopicConfig, Integer> topicsMap,
-                                 final int replicationFactor,
-                                 final long windowChangeLogAdditionalRetention,
-                                 final MetadataResponse metadata) {
-            for (final Map.Entry<InternalTopicConfig, Integer> topic : topicsMap.entrySet()) {
-                final InternalTopicConfig config = topic.getKey();
-                final String topicName = config.name();
-                createdTopics.put(topicName, config);
-                numberOfPartitionsPerTopic.put(topicName, topic.getValue());
-                replicationFactorPerTopic.put(topicName, replicationFactor);
-            }
-        }
-
-        @Override
-        public MetadataResponse fetchMetadata() {
-            final Node node = new Node(1, "host1", 1001);
-            final MetadataResponse.PartitionMetadata partitionMetadata = new MetadataResponse.PartitionMetadata(Errors.NONE, 1, node, new ArrayList<Node>(), new ArrayList<Node>(), new ArrayList<Node>());
-            final MetadataResponse.TopicMetadata topicMetadata = new MetadataResponse.TopicMetadata(Errors.NONE, topic, true, Collections.singletonList(partitionMetadata));
-            final MetadataResponse metadataResponse;
-            if (returnNoMetadata) {
-                metadataResponse = new MetadataResponse(
-                    Collections.<Node>singletonList(node),
-                    null,
-                    MetadataResponse.NO_CONTROLLER_ID,
-                    Collections.<MetadataResponse.TopicMetadata>emptyList());
-            } else {
-                metadataResponse = new MetadataResponse(
-                    Collections.<Node>singletonList(node),
-                    null,
-                    MetadataResponse.NO_CONTROLLER_ID,
-                    Collections.singletonList(topicMetadata));
-            }
+    @Test
+    public void shouldExhaustRetriesOnTimeoutExceptionForMakeReady() {
+        mockAdminClient.timeoutNextRequest(4);
 
-            return metadataResponse;
+        final InternalTopicConfig internalTopicConfig = new InternalTopicConfig(topic, Collections.singleton(InternalTopicConfig.CleanupPolicy.delete), Collections.<String, String>emptyMap());
+        internalTopicConfig.setNumberOfPartitions(1);
+        try {
+            internalTopicManager.makeReady(Collections.singletonMap(topic, internalTopicConfig));
+            fail("Should have thrown StreamsException.");
+        } catch (final StreamsException expected) {
+            assertNull(expected.getCause());
+            assertEquals("Could not create topics. This can happen if the Kafka cluster is temporary not available. You can increase admin client config `retries` to be resilient against this error.", expected.getMessage());
         }
     }
+
 }
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignorTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignorTest.java
index 99bb56d566..d9e0ee7bb3 100644
--- a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignorTest.java
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignorTest.java
@@ -105,7 +105,7 @@ public class StreamPartitionAssignorTest {
     private final StreamPartitionAssignor partitionAssignor = new StreamPartitionAssignor();
     private final MockClientSupplier mockClientSupplier = new MockClientSupplier();
     private final InternalTopologyBuilder builder = new InternalTopologyBuilder();
-    private final StreamsConfig config = new StreamsConfig(configProps());
+    private final StreamsConfig streamsConfig = new StreamsConfig(configProps());
     private final String userEndPoint = "localhost:8080";
     private final String applicationId = "stream-partition-assignor-test";
 
@@ -136,7 +136,6 @@ public class StreamPartitionAssignorTest {
         EasyMock.replay(taskManager);
     }
 
-
     @Test
     public void testSubscription() throws Exception {
         builder.addSource(null, "source1", null, null, null, "topic1");
@@ -165,7 +164,6 @@ public class StreamPartitionAssignorTest {
         assertEquals(info.encode(), subscription.userData());
     }
 
-
     @Test
     public void testAssignBasic() throws Exception {
         builder.addSource(null, "source1", null, null, null, "topic1");
@@ -187,7 +185,7 @@ public class StreamPartitionAssignorTest {
         mockTaskManager(prevTasks10, standbyTasks10, uuid1, builder);
         configurePartitionAssignor(Collections.<String, Object>emptyMap());
 
-        partitionAssignor.setInternalTopicManager(new MockInternalTopicManager(config, mockClientSupplier.restoreConsumer));
+        partitionAssignor.setInternalTopicManager(new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer));
 
         Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
         subscriptions.put("consumer10",
@@ -246,7 +244,7 @@ public class StreamPartitionAssignorTest {
         mockTaskManager(Collections.<TaskId>emptySet(), Collections.<TaskId>emptySet(), uuid1, builder);
         configurePartitionAssignor(Collections.singletonMap(StreamsConfig.PARTITION_GROUPER_CLASS_CONFIG, (Object) SingleGroupPartitionGrouperStub.class));
 
-        partitionAssignor.setInternalTopicManager(new MockInternalTopicManager(new StreamsConfig(configProps()), mockClientSupplier.restoreConsumer));
+        partitionAssignor.setInternalTopicManager(new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer));
         Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
         subscriptions.put("consumer10",
             new PartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid1, Collections.<TaskId>emptySet(), Collections.<TaskId>emptySet(), userEndPoint).encode()));
@@ -339,7 +337,7 @@ public class StreamPartitionAssignorTest {
         mockTaskManager(prevTasks10, Collections.<TaskId>emptySet(), uuid1, builder);
         configurePartitionAssignor(Collections.<String, Object>emptyMap());
 
-        partitionAssignor.setInternalTopicManager(new MockInternalTopicManager(config, mockClientSupplier.restoreConsumer));
+        partitionAssignor.setInternalTopicManager(new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer));
 
         Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
         subscriptions.put("consumer10",
@@ -406,7 +404,7 @@ public class StreamPartitionAssignorTest {
                 builder);
         configurePartitionAssignor(Collections.<String, Object>emptyMap());
 
-        partitionAssignor.setInternalTopicManager(new MockInternalTopicManager(config, mockClientSupplier.restoreConsumer));
+        partitionAssignor.setInternalTopicManager(new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer));
 
         Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
         subscriptions.put("consumer10",
@@ -466,7 +464,7 @@ public class StreamPartitionAssignorTest {
     public void testAssignWithStandbyReplicas() throws Exception {
         Map<String, Object> props = configProps();
         props.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, "1");
-        StreamsConfig config = new StreamsConfig(props);
+        StreamsConfig streamsConfig = new StreamsConfig(props);
 
         builder.addSource(null, "source1", null, null, null, "topic1");
         builder.addSource(null, "source2", null, null, null, "topic2");
@@ -489,7 +487,7 @@ public class StreamPartitionAssignorTest {
 
         configurePartitionAssignor(Collections.<String, Object>singletonMap(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 1));
 
-        partitionAssignor.setInternalTopicManager(new MockInternalTopicManager(config, mockClientSupplier.restoreConsumer));
+        partitionAssignor.setInternalTopicManager(new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer));
 
         Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
         subscriptions.put("consumer10",
@@ -584,7 +582,7 @@ public class StreamPartitionAssignorTest {
         UUID uuid1 = UUID.randomUUID();
         mockTaskManager(Collections.<TaskId>emptySet(), Collections.<TaskId>emptySet(), uuid1, builder);
         configurePartitionAssignor(Collections.<String, Object>emptyMap());
-        MockInternalTopicManager internalTopicManager = new MockInternalTopicManager(config, mockClientSupplier.restoreConsumer);
+        MockInternalTopicManager internalTopicManager = new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer);
         partitionAssignor.setInternalTopicManager(internalTopicManager);
 
         Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
@@ -619,7 +617,7 @@ public class StreamPartitionAssignorTest {
         mockTaskManager(Collections.<TaskId>emptySet(), Collections.<TaskId>emptySet(), uuid1, builder);
 
         configurePartitionAssignor(Collections.<String, Object>emptyMap());
-        MockInternalTopicManager internalTopicManager = new MockInternalTopicManager(config, mockClientSupplier.restoreConsumer);
+        MockInternalTopicManager internalTopicManager = new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer);
         partitionAssignor.setInternalTopicManager(internalTopicManager);
 
         Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
@@ -666,7 +664,7 @@ public class StreamPartitionAssignorTest {
         mockTaskManager(Collections.<TaskId>emptySet(), Collections.<TaskId>emptySet(), uuid1, builder);
         configurePartitionAssignor(Collections.singletonMap(StreamsConfig.APPLICATION_SERVER_CONFIG, (Object) userEndPoint));
 
-        partitionAssignor.setInternalTopicManager(new MockInternalTopicManager(config, mockClientSupplier.restoreConsumer));
+        partitionAssignor.setInternalTopicManager(new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer));
 
         final Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
         final Set<TaskId> emptyTasks = Collections.emptySet();
@@ -687,7 +685,7 @@ public class StreamPartitionAssignorTest {
         builder.setApplicationId(applicationId);
 
         mockTaskManager(Collections.<TaskId>emptySet(), Collections.<TaskId>emptySet(), UUID.randomUUID(), builder);
-        partitionAssignor.setInternalTopicManager(new MockInternalTopicManager(config, mockClientSupplier.restoreConsumer));
+        partitionAssignor.setInternalTopicManager(new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer));
 
         try {
             configurePartitionAssignor(Collections.singletonMap(StreamsConfig.APPLICATION_SERVER_CONFIG, (Object) "localhost"));
@@ -779,7 +777,7 @@ public class StreamPartitionAssignorTest {
         configurePartitionAssignor(Collections.<String, Object>emptyMap());
 
         final MockInternalTopicManager mockInternalTopicManager = new MockInternalTopicManager(
-            config,
+            streamsConfig,
             mockClientSupplier.restoreConsumer);
         partitionAssignor.setInternalTopicManager(mockInternalTopicManager);
 
@@ -849,7 +847,7 @@ public class StreamPartitionAssignorTest {
         props.put(StreamsConfig.APPLICATION_SERVER_CONFIG, userEndPoint);
         configurePartitionAssignor(props);
         partitionAssignor.setInternalTopicManager(new MockInternalTopicManager(
-            config,
+            streamsConfig,
             mockClientSupplier.restoreConsumer));
 
         final Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java
index daf6fadf96..69ae07c6cf 100644
--- a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java
@@ -249,7 +249,6 @@ public class StreamThreadTest {
                 consumer,
                 consumer,
                 null,
-                clientSupplier.getAdminClient(config.getAdminConfigs(clientId)),
                 taskManager,
                 streamsMetrics,
                 internalTopologyBuilder,
@@ -281,7 +280,6 @@ public class StreamThreadTest {
                 consumer,
                 consumer,
                 null,
-                clientSupplier.getAdminClient(config.getAdminConfigs(clientId)),
                 taskManager,
                 streamsMetrics,
                 internalTopologyBuilder,
@@ -313,7 +311,6 @@ public class StreamThreadTest {
                 consumer,
                 consumer,
                 null,
-                clientSupplier.getAdminClient(config.getAdminConfigs(clientId)),
                 taskManager,
                 streamsMetrics,
                 internalTopologyBuilder,
@@ -443,7 +440,6 @@ public class StreamThreadTest {
                 consumer,
                 consumer,
                 null,
-                clientSupplier.getAdminClient(config.getAdminConfigs(clientId)),
                 taskManager,
                 streamsMetrics,
                 internalTopologyBuilder,
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsKafkaClientTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsKafkaClientTest.java
deleted file mode 100644
index 660a622a2d..0000000000
--- a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsKafkaClientTest.java
+++ /dev/null
@@ -1,219 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License. You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.kafka.streams.processor.internals;
-
-import org.apache.kafka.clients.MockClient;
-import org.apache.kafka.common.MetricName;
-import org.apache.kafka.common.Node;
-import org.apache.kafka.common.TopicPartition;
-import org.apache.kafka.common.config.AbstractConfig;
-import org.apache.kafka.common.config.SaslConfigs;
-import org.apache.kafka.common.config.TopicConfig;
-import org.apache.kafka.common.metrics.KafkaMetric;
-import org.apache.kafka.common.metrics.MetricsReporter;
-import org.apache.kafka.common.requests.AbstractRequest;
-import org.apache.kafka.common.requests.ApiError;
-import org.apache.kafka.common.requests.CreateTopicsRequest;
-import org.apache.kafka.common.requests.CreateTopicsResponse;
-import org.apache.kafka.common.requests.MetadataResponse;
-import org.apache.kafka.common.requests.ProduceResponse;
-import org.apache.kafka.common.utils.LogContext;
-import org.apache.kafka.common.utils.MockTime;
-import org.apache.kafka.streams.StreamsConfig;
-import org.apache.kafka.streams.errors.StreamsException;
-import org.junit.Before;
-import org.junit.Test;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import static java.util.Arrays.asList;
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertFalse;
-import static org.junit.Assert.assertThat;
-
-public class StreamsKafkaClientTest {
-
-    private static final String TOPIC = "topic";
-    private final MockClient kafkaClient = new MockClient(new MockTime());
-    private final List<MetricsReporter> reporters = Collections.emptyList();
-    private final MetadataResponse metadata = new MetadataResponse(Collections.singletonList(new Node(1, "host", 90)), "cluster", 1, Collections.<MetadataResponse.TopicMetadata>emptyList());
-    private final Map<String, Object> config = new HashMap<>();
-    private final InternalTopicConfig topicConfigWithNoOverrides = new InternalTopicConfig(TOPIC,
-                                                                                           Collections.singleton(InternalTopicConfig.CleanupPolicy.delete),
-                                                                                           Collections.<String, String>emptyMap());
-
-    private final Map<String, String> overridenTopicConfig = Collections.singletonMap(TopicConfig.DELETE_RETENTION_MS_CONFIG, "100");
-    private final InternalTopicConfig topicConfigWithOverrides = new InternalTopicConfig(TOPIC,
-                                                                                         Collections.singleton(InternalTopicConfig.CleanupPolicy.compact),
-                                                                                         overridenTopicConfig);
-
-
-    @Before
-    public void before() {
-        config.put(StreamsConfig.APPLICATION_ID_CONFIG, "some_app_id");
-        config.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9000");
-    }
-
-    @Test
-    public void testConfigFromStreamsConfig() {
-        for (final String expectedMechanism : asList("PLAIN", "SCRAM-SHA-512")) {
-            config.put(SaslConfigs.SASL_MECHANISM, expectedMechanism);
-            final AbstractConfig abstractConfig = StreamsKafkaClient.Config.fromStreamsConfig(config);
-            assertEquals(expectedMechanism, abstractConfig.values().get(SaslConfigs.SASL_MECHANISM));
-            assertEquals(expectedMechanism, abstractConfig.getString(SaslConfigs.SASL_MECHANISM));
-        }
-    }
-
-    @Test
-    public void shouldAddCleanupPolicyToTopicConfigWhenCreatingTopic() throws Exception {
-        final StreamsKafkaClient streamsKafkaClient = createStreamsKafkaClient();
-        verifyCorrectTopicConfigs(streamsKafkaClient, topicConfigWithNoOverrides, Collections.singletonMap("cleanup.policy", "delete"));
-    }
-
-
-    @Test
-    public void shouldAddDefaultTopicConfigFromStreamConfig() throws Exception {
-        config.put(StreamsConfig.topicPrefix(TopicConfig.SEGMENT_MS_CONFIG), "100");
-        config.put(StreamsConfig.topicPrefix(TopicConfig.COMPRESSION_TYPE_CONFIG), "gzip");
-
-        final Map<String, String> expectedConfigs = new HashMap<>();
-        expectedConfigs.put(TopicConfig.SEGMENT_MS_CONFIG, "100");
-        expectedConfigs.put(TopicConfig.COMPRESSION_TYPE_CONFIG, "gzip");
-        expectedConfigs.put(TopicConfig.CLEANUP_POLICY_CONFIG, TopicConfig.CLEANUP_POLICY_DELETE);
-        final StreamsKafkaClient streamsKafkaClient = createStreamsKafkaClient();
-        verifyCorrectTopicConfigs(streamsKafkaClient, topicConfigWithNoOverrides, expectedConfigs);
-    }
-
-    @Test
-    public void shouldSetPropertiesDefinedByInternalTopicConfig() throws Exception {
-        final Map<String, String> expectedConfigs = new HashMap<>(overridenTopicConfig);
-        expectedConfigs.put(TopicConfig.CLEANUP_POLICY_CONFIG, TopicConfig.CLEANUP_POLICY_COMPACT);
-        final StreamsKafkaClient streamsKafkaClient = createStreamsKafkaClient();
-        verifyCorrectTopicConfigs(streamsKafkaClient, topicConfigWithOverrides, expectedConfigs);
-    }
-
-    @Test
-    public void shouldOverrideDefaultTopicConfigsFromStreamsConfig() throws Exception {
-        config.put(StreamsConfig.topicPrefix(TopicConfig.DELETE_RETENTION_MS_CONFIG), "99999");
-        config.put(StreamsConfig.topicPrefix(TopicConfig.SEGMENT_MS_CONFIG), "988");
-
-        final Map<String, String> expectedConfigs = new HashMap<>(overridenTopicConfig);
-        expectedConfigs.put(TopicConfig.CLEANUP_POLICY_CONFIG, TopicConfig.CLEANUP_POLICY_COMPACT);
-        expectedConfigs.put(TopicConfig.DELETE_RETENTION_MS_CONFIG, "100");
-        expectedConfigs.put(TopicConfig.SEGMENT_MS_CONFIG, "988");
-        final StreamsKafkaClient streamsKafkaClient = createStreamsKafkaClient();
-        verifyCorrectTopicConfigs(streamsKafkaClient, topicConfigWithOverrides, expectedConfigs);
-    }
-
-    @Test
-    public void shouldNotAllowNullTopicConfigs() throws Exception {
-        config.put(StreamsConfig.topicPrefix(TopicConfig.DELETE_RETENTION_MS_CONFIG), null);
-        final StreamsKafkaClient streamsKafkaClient = createStreamsKafkaClient();
-        verifyCorrectTopicConfigs(streamsKafkaClient, topicConfigWithNoOverrides, Collections.singletonMap("cleanup.policy", "delete"));
-    }
-
-    @Test
-    public void metricsShouldBeTaggedWithClientId() {
-        config.put(StreamsConfig.CLIENT_ID_CONFIG, "some_client_id");
-        config.put(StreamsConfig.METRIC_REPORTER_CLASSES_CONFIG, TestMetricsReporter.class.getName());
-        StreamsKafkaClient.create(config);
-        assertFalse(TestMetricsReporter.METRICS.isEmpty());
-        for (KafkaMetric kafkaMetric : TestMetricsReporter.METRICS.values()) {
-            assertEquals("some_client_id", kafkaMetric.metricName().tags().get("client-id"));
-        }
-    }
-
-    @Test(expected = StreamsException.class)
-    public void shouldThrowStreamsExceptionOnEmptyFetchMetadataResponse() {
-        kafkaClient.prepareResponse(null);
-        final StreamsKafkaClient streamsKafkaClient = createStreamsKafkaClient();
-        streamsKafkaClient.fetchMetadata();
-    }
-
-    @Test(expected = StreamsException.class)
-    public void shouldThrowStreamsExceptionWhenFetchMetadataResponseInconsistent() {
-        kafkaClient.prepareResponse(new ProduceResponse(Collections.<TopicPartition, ProduceResponse.PartitionResponse>emptyMap()));
-        final StreamsKafkaClient streamsKafkaClient = createStreamsKafkaClient();
-        streamsKafkaClient.fetchMetadata();
-    }
-
-    private void verifyCorrectTopicConfigs(final StreamsKafkaClient streamsKafkaClient,
-                                           final InternalTopicConfig internalTopicConfig,
-                                           final Map<String, String> expectedConfigs) {
-        final Map<String, String> requestedTopicConfigs = new HashMap<>();
-
-        kafkaClient.prepareResponse(new MockClient.RequestMatcher() {
-            @Override
-            public boolean matches(final AbstractRequest body) {
-                if (!(body instanceof CreateTopicsRequest)) {
-                    return false;
-                }
-                final CreateTopicsRequest request = (CreateTopicsRequest) body;
-                final Map<String, CreateTopicsRequest.TopicDetails> topics =
-                        request.topics();
-                final CreateTopicsRequest.TopicDetails topicDetails = topics.get(TOPIC);
-                requestedTopicConfigs.putAll(topicDetails.configs);
-                return true;
-            }
-        }, new CreateTopicsResponse(Collections.singletonMap(TOPIC, ApiError.NONE)));
-
-        streamsKafkaClient.createTopics(Collections.singletonMap(internalTopicConfig, 1), 1, 1, metadata);
-
-        assertThat(requestedTopicConfigs, equalTo(expectedConfigs));
-    }
-
-    private StreamsKafkaClient createStreamsKafkaClient() {
-        return new StreamsKafkaClient(StreamsKafkaClient.Config.fromStreamsConfig(config),
-                                      kafkaClient,
-                                      reporters,
-                                      new LogContext());
-    }
-
-
-    public static class TestMetricsReporter implements MetricsReporter {
-        static final Map<MetricName, KafkaMetric> METRICS = new HashMap<>();
-
-        @Override
-        public void configure(final Map<String, ?> configs) { }
-
-        @Override
-        public void init(final List<KafkaMetric> metrics) {
-            for (final KafkaMetric metric : metrics) {
-                metricChange(metric);
-            }
-        }
-
-        @Override
-        public void metricChange(final KafkaMetric metric) {
-            METRICS.put(metric.metricName(), metric);
-        }
-
-        @Override
-        public void metricRemoval(final KafkaMetric metric) {
-            METRICS.remove(metric.metricName());
-        }
-
-        @Override
-        public void close() {
-            METRICS.clear();
-        }
-    }
-}
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java
index 37a683cb92..648e9b080c 100644
--- a/streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java
@@ -28,7 +28,6 @@ import org.apache.kafka.common.internals.KafkaFutureImpl;
 import org.apache.kafka.common.utils.Utils;
 import org.apache.kafka.streams.errors.StreamsException;
 import org.apache.kafka.streams.processor.TaskId;
-
 import org.easymock.EasyMock;
 import org.easymock.EasyMockRunner;
 import org.easymock.Mock;
@@ -88,8 +87,6 @@ public class TaskManagerTest {
     @Mock(type = MockType.NICE)
     private StreamThread.AbstractTaskCreator<StandbyTask> standbyTaskCreator;
     @Mock(type = MockType.NICE)
-    private StreamsKafkaClient streamsKafkaClient;
-    @Mock(type = MockType.NICE)
     private AdminClient adminClient;
     @Mock(type = MockType.NICE)
     private StreamTask streamTask;
@@ -128,7 +125,6 @@ public class TaskManagerTest {
                                       streamsMetadataState,
                                       activeTaskCreator,
                                       standbyTaskCreator,
-                                      streamsKafkaClient,
                                       adminClient,
                                       active,
                                       standby);
@@ -697,4 +693,4 @@ public class TaskManagerTest {
         expect(topologyBuilder.sourceTopicPattern()).andReturn(Pattern.compile("abc"));
         expect(topologyBuilder.subscriptionUpdates()).andReturn(subscriptionUpdates);
     }
-}
\ No newline at end of file
+}
diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/BrokerCompatibilityTest.java b/streams/src/test/java/org/apache/kafka/streams/tests/BrokerCompatibilityTest.java
index ef1f63c953..b308782a6f 100644
--- a/streams/src/test/java/org/apache/kafka/streams/tests/BrokerCompatibilityTest.java
+++ b/streams/src/test/java/org/apache/kafka/streams/tests/BrokerCompatibilityTest.java
@@ -94,6 +94,7 @@ public class BrokerCompatibilityTest {
             @Override
             public void uncaughtException(final Thread t, final Throwable e) {
                 System.err.println("FATAL: An unexpected exception " + e);
+                e.printStackTrace(System.err);
                 System.err.flush();
                 streams.close(30, TimeUnit.SECONDS);
             }
diff --git a/streams/src/test/java/org/apache/kafka/test/MockInternalTopicManager.java b/streams/src/test/java/org/apache/kafka/test/MockInternalTopicManager.java
index 598ca8d348..3db7e53ab8 100644
--- a/streams/src/test/java/org/apache/kafka/test/MockInternalTopicManager.java
+++ b/streams/src/test/java/org/apache/kafka/test/MockInternalTopicManager.java
@@ -16,13 +16,12 @@
  */
 package org.apache.kafka.test;
 
+import org.apache.kafka.clients.admin.KafkaAdminClient;
 import org.apache.kafka.clients.consumer.MockConsumer;
 import org.apache.kafka.common.PartitionInfo;
-import org.apache.kafka.common.utils.MockTime;
 import org.apache.kafka.streams.StreamsConfig;
 import org.apache.kafka.streams.processor.internals.InternalTopicConfig;
 import org.apache.kafka.streams.processor.internals.InternalTopicManager;
-import org.apache.kafka.streams.processor.internals.StreamsKafkaClient;
 
 import java.util.ArrayList;
 import java.util.HashMap;
@@ -34,26 +33,29 @@ import java.util.Set;
 
 public class MockInternalTopicManager extends InternalTopicManager {
 
-    public Map<String, Integer> readyTopics = new HashMap<>();
-    private MockConsumer<byte[], byte[]> restoreConsumer;
+    final public Map<String, Integer> readyTopics = new HashMap<>();
+    final private MockConsumer<byte[], byte[]> restoreConsumer;
 
-    public MockInternalTopicManager(StreamsConfig streamsConfig, MockConsumer<byte[], byte[]> restoreConsumer) {
-        super(StreamsKafkaClient.create(streamsConfig.originals()), 0, 0, new MockTime());
+    public MockInternalTopicManager(final StreamsConfig streamsConfig,
+                                    final MockConsumer<byte[], byte[]> restoreConsumer) {
+        super(KafkaAdminClient.create(streamsConfig.originals()), streamsConfig.originals());
 
         this.restoreConsumer = restoreConsumer;
     }
 
     @Override
-    public void makeReady(final Map<InternalTopicConfig, Integer> topics) {
-        for (Map.Entry<InternalTopicConfig, Integer> entry : topics.entrySet()) {
-            readyTopics.put(entry.getKey().name(), entry.getValue());
+    public void makeReady(final Map<String, InternalTopicConfig> topics) {
+        for (final InternalTopicConfig topic : topics.values()) {
+            final String topicName = topic.name();
+            final int numberOfPartitions = topic.numberOfPartitions();
+            readyTopics.put(topicName, numberOfPartitions);
 
             final List<PartitionInfo> partitions = new ArrayList<>();
-            for (int i = 0; i < entry.getValue(); i++) {
-                partitions.add(new PartitionInfo(entry.getKey().name(), i, null, null, null));
+            for (int i = 0; i < numberOfPartitions; i++) {
+                partitions.add(new PartitionInfo(topicName, i, null, null, null));
             }
 
-            restoreConsumer.updatePartitions(entry.getKey().name(), partitions);
+            restoreConsumer.updatePartitions(topicName, partitions);
         }
     }
 
@@ -66,4 +68,4 @@ public class MockInternalTopicManager extends InternalTopicManager {
 
         return partitions;
     }
-}
\ No newline at end of file
+}
diff --git a/tests/kafkatest/tests/streams/streams_broker_compatibility_test.py b/tests/kafkatest/tests/streams/streams_broker_compatibility_test.py
index 92a8c1e84d..1eb46efea7 100644
--- a/tests/kafkatest/tests/streams/streams_broker_compatibility_test.py
+++ b/tests/kafkatest/tests/streams/streams_broker_compatibility_test.py
@@ -102,9 +102,9 @@ class StreamsBrokerCompatibility(Test):
 
         processor.node.account.ssh(processor.start_cmd(processor.node))
         with processor.node.account.monitor_log(processor.STDERR_FILE) as monitor:
-            monitor.wait_until('FATAL: An unexpected exception org.apache.kafka.streams.errors.StreamsException: Could not create internal topics.',
+            monitor.wait_until('FATAL: An unexpected exception org.apache.kafka.streams.errors.StreamsException: Could not create topic kafka-streams-system-test-broker-compatibility-KSTREAM-AGGREGATE-STATE-STORE-0000000001-changelog.',
                         timeout_sec=60,
-                        err_msg="Never saw 'FATAL: An unexpected exception org.apache.kafka.streams.errors.StreamsException: Could not create internal topics.' error message " + str(processor.node.account))
+                        err_msg="Never saw 'FATAL: An unexpected exception org.apache.kafka.streams.errors.StreamsException: Could not create topic kafka-streams-system-test-broker-compatibility-KSTREAM-AGGREGATE-STATE-STORE-0000000001-changelog.' error message " + str(processor.node.account))
 
         self.kafka.stop()
 
