diff --git a/core/src/main/scala/kafka/zk/KafkaZkClient.scala b/core/src/main/scala/kafka/zk/KafkaZkClient.scala
index c8c9f361a6..cb22d3cacc 100644
--- a/core/src/main/scala/kafka/zk/KafkaZkClient.scala
+++ b/core/src/main/scala/kafka/zk/KafkaZkClient.scala
@@ -2043,10 +2043,10 @@ class KafkaZkClient private[zk] (
                 case Some(value) =>
                   val failedPayload = MigrationZNode.decode(value, version, -1)
                   throw new RuntimeException(
-                    s"Conditional update on KRaft Migration ZNode failed. Expected zkVersion = $version. The failed " +
+                    s"Conditional update on KRaft Migration ZNode failed. Sent zkVersion = $version. The failed " +
                     s"write was: $failedPayload. This indicates that another KRaft controller is making writes to ZooKeeper.")
                 case None =>
-                  throw new RuntimeException(s"Check op on KRaft Migration ZNode failed. Expected zkVersion = $version. " +
+                  throw new RuntimeException(s"Check op on KRaft Migration ZNode failed. Sent zkVersion = $version. " +
                     s"This indicates that another KRaft controller is making writes to ZooKeeper.")
               }
             } else if (errorCode == Code.OK) {
diff --git a/core/src/test/scala/integration/kafka/zk/ZkMigrationFailoverTest.scala b/core/src/test/scala/integration/kafka/zk/ZkMigrationFailoverTest.scala
index 7d84393f4c..10c9df4577 100644
--- a/core/src/test/scala/integration/kafka/zk/ZkMigrationFailoverTest.scala
+++ b/core/src/test/scala/integration/kafka/zk/ZkMigrationFailoverTest.scala
@@ -17,6 +17,7 @@
 package kafka.zk
 
 import kafka.utils.{Logging, TestUtils}
+import kafka.zk.migration.{ZkAclMigrationClient, ZkConfigMigrationClient, ZkDelegationTokenMigrationClient, ZkTopicMigrationClient}
 import org.apache.kafka.common.Uuid
 import org.apache.kafka.common.metadata.{FeatureLevelRecord, TopicRecord}
 import org.apache.kafka.common.utils.{Time, Utils}
@@ -25,14 +26,14 @@ import org.apache.kafka.controller.metrics.QuorumControllerMetrics
 import org.apache.kafka.image.loader.LogDeltaManifest
 import org.apache.kafka.image.publisher.MetadataPublisher
 import org.apache.kafka.image.{MetadataDelta, MetadataImage, MetadataProvenance}
-import org.apache.kafka.metadata.KafkaConfigSchema
+import org.apache.kafka.metadata.{KafkaConfigSchema, PartitionRegistration}
 import org.apache.kafka.metadata.migration._
 import org.apache.kafka.raft.{LeaderAndEpoch, OffsetAndEpoch}
 import org.apache.kafka.security.PasswordEncoder
 import org.apache.kafka.server.common.{ApiMessageAndVersion, MetadataVersion}
 import org.apache.kafka.server.fault.FaultHandler
 import org.apache.zookeeper.client.ZKClientConfig
-import org.junit.jupiter.api.Assertions.{assertTrue, fail}
+import org.junit.jupiter.api.Assertions.{assertTrue, assertEquals, fail}
 import org.junit.jupiter.api.Test
 
 import java.util
@@ -72,6 +73,19 @@ class ZkMigrationFailoverTest extends Logging {
     }
   }
 
+  class CapturingZkTopicMigrationClient(zkClient: KafkaZkClient) extends ZkTopicMigrationClient(zkClient) {
+    val createdTopics = mutable.Set[String]()
+    override def createTopic(
+      topicName: String,
+      topicId: Uuid,
+      partitions: util.Map[Integer, PartitionRegistration],
+      state: ZkMigrationLeadershipState
+    ): ZkMigrationLeadershipState = {
+      createdTopics.add(topicName)
+      super.createTopic(topicName, topicId, partitions, state)
+    }
+  }
+
   def buildMigrationDriver(nodeId: Int, zkMigrationClient: ZkMigrationClient): (KRaftMigrationDriver, CapturingFaultHandler) = {
     val faultHandler = new CapturingFaultHandler(nodeId)
     val driver = KRaftMigrationDriver.newBuilder
@@ -269,7 +283,135 @@ class ZkMigrationFailoverTest extends Logging {
     } finally {
       driver1.close()
       driver2.close()
-      Utils.closeQuietly(zookeeper, "EmbeddedZookeeper")
+      zookeeper.shutdown()
+      if (zkClient != null) Utils.closeQuietly(zkClient, "KafkaZkClient")
+    }
+  }
+
+  @Test
+  def testDriverSkipsEventsFromOlderEpoch(): Unit = {
+    val zookeeper = new EmbeddedZookeeper()
+    var zkClient: KafkaZkClient = null
+    val zkConnect = s"127.0.0.1:${zookeeper.port}"
+    try {
+      zkClient = KafkaZkClient(
+        zkConnect,
+        isSecure = false,
+        30000,
+        60000,
+        1,
+        Time.SYSTEM,
+        name = "ZkMigrationFailoverTest",
+        new ZKClientConfig)
+    } catch {
+      case t: Throwable =>
+        zookeeper.shutdown()
+        if (zkClient != null) Utils.closeQuietly(zkClient, "KafkaZkClient")
+        throw t
+    }
+
+    val topicClient1 = new CapturingZkTopicMigrationClient(zkClient)
+    val topicClient2 = new CapturingZkTopicMigrationClient(zkClient)
+
+    def buildZkMigrationClient(topicClient: TopicMigrationClient): ZkMigrationClient = {
+      val configClient = new ZkConfigMigrationClient(zkClient, PasswordEncoder.NOOP)
+      val aclClient = new ZkAclMigrationClient(zkClient)
+      val delegationTokenClient = new ZkDelegationTokenMigrationClient(zkClient)
+      new ZkMigrationClient(zkClient, topicClient, configClient, aclClient, delegationTokenClient)
+    }
+
+    val zkMigrationClient1 = buildZkMigrationClient(topicClient1)
+    val zkMigrationClient2 = buildZkMigrationClient(topicClient2)
+
+    val (driver1, faultHandler1) = buildMigrationDriver(3000, zkMigrationClient1)
+    val (driver2, faultHandler2) = buildMigrationDriver(3001, zkMigrationClient2)
+
+    // Initialize data into /controller and /controller_epoch
+    zkClient.registerControllerAndIncrementControllerEpoch(0)
+    var zkState = zkMigrationClient1.claimControllerLeadership(
+      ZkMigrationLeadershipState.EMPTY.withNewKRaftController(3000, 1)
+    )
+
+    // Fake a complete migration
+    zkState = zkState.withKRaftMetadataOffsetAndEpoch(100, 10)
+    zkState = zkMigrationClient1.getOrCreateMigrationRecoveryState(zkState)
+
+    try {
+      driver1.start()
+      driver2.start()
+
+      val leader1 = new LeaderAndEpoch(OptionalInt.of(3000), 2)
+      var image = MetadataImage.EMPTY
+      val delta = new MetadataDelta(image)
+      delta.replay(new FeatureLevelRecord()
+        .setName(MetadataVersion.FEATURE_NAME)
+        .setFeatureLevel(MetadataVersion.latestProduction().featureLevel))
+      delta.replay(ZkMigrationState.MIGRATION.toRecord.message)
+
+      val provenance = new MetadataProvenance(210, 11, 1)
+      image = delta.apply(provenance)
+
+      val manifest = LogDeltaManifest.newBuilder()
+        .provenance(provenance)
+        .leaderAndEpoch(leader1)
+        .numBatches(1)
+        .elapsedNs(100)
+        .numBytes(42)
+        .build()
+
+      // Load an image into 3000 image and a leader event, this lets it become active and sync to ZK
+      driver1.onMetadataUpdate(delta, image, manifest)
+      driver1.onControllerChange(leader1)
+      driver2.onControllerChange(leader1)
+
+      // Wait until new leader has sync'd to ZK
+      TestUtils.waitUntilTrue(
+        () => safeGet(driver1.migrationState()).equals(MigrationDriverState.DUAL_WRITE),
+        "waiting for driver to enter DUAL_WRITE"
+      )
+
+      // Enqueue several metadata deltas and then process a leader change.
+      for (i <- 1 to 1000) {
+        val delta = new MetadataDelta(image)
+        delta.replay(new TopicRecord().setTopicId(Uuid.randomUuid()).setName(s"topic-$i"))
+        val provenance = new MetadataProvenance(210 + i, 11, 1)
+        image = delta.apply(provenance)
+        val manifest = LogDeltaManifest.newBuilder()
+          .provenance(provenance)
+          .leaderAndEpoch(leader1)
+          .numBatches(1)
+          .elapsedNs(100)
+          .numBytes(42)
+          .build()
+        driver1.onMetadataUpdate(delta, image, manifest)
+        driver2.onMetadataUpdate(delta, image, manifest)
+      }
+      Thread.sleep(50) // Give some events a chance to run
+
+      val leader2 = new LeaderAndEpoch(OptionalInt.of(3001), 3)
+      driver1.onControllerChange(leader2)
+
+      Thread.sleep(50) // Artificial delay for 3001 to see the leader change.
+      driver2.onControllerChange(leader2)
+
+      // Wait for driver 2 to become leader in ZK
+      TestUtils.waitUntilTrue(() => zkClient.getControllerId match {
+        case Some(nodeId) => nodeId == 3001
+        case None => false
+      }, "waiting for 3001 to claim ZK leadership")
+
+      TestUtils.waitUntilTrue(() => {
+        val topics = zkClient.getAllTopicsInCluster(false)
+        topics.size == 1000
+      }, "waiting for topics to be created in ZK.")
+
+      assertTrue(topicClient1.createdTopics.nonEmpty, "Expect first leader to write some topics")
+      assertTrue(topicClient2.createdTopics.nonEmpty, "Expect second leader to write some topics")
+      assertEquals(1000, topicClient1.createdTopics.size + topicClient2.createdTopics.size,
+        "Expect drivers to only write to ZK if they are the leader")
+    } finally {
+      driver1.close()
+      driver2.close()
       zookeeper.shutdown()
       if (zkClient != null) Utils.closeQuietly(zkClient, "KafkaZkClient")
     }
diff --git a/metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java b/metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java
index 61c4cf2592..61b95c091d 100644
--- a/metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java
+++ b/metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java
@@ -115,6 +115,10 @@ public class KRaftMigrationDriver implements MetadataPublisher {
     private volatile MetadataImage image;
     private volatile boolean firstPublish;
 
+    // This is updated by the MetadataPublisher thread. When processing events in the migration driver thread,
+    // we should check if a newer leader has been seen by examining this variable.
+    private volatile LeaderAndEpoch curLeaderAndEpoch;
+
     KRaftMigrationDriver(
         int nodeId,
         ZkRecordConsumer zkRecordConsumer,
@@ -231,16 +235,24 @@ public class KRaftMigrationDriver implements MetadataPublisher {
      * @param name         A descriptive name of the function that is being applied
      * @param migrationOp  A function which performs some migration operations and possibly transforms our internal state
      */
+
     private void applyMigrationOperation(String name, KRaftMigrationOperation migrationOp) {
+        applyMigrationOperation(name, migrationOp, false);
+    }
+
+    private void applyMigrationOperation(String name, KRaftMigrationOperation migrationOp, boolean alwaysLog) {
         ZkMigrationLeadershipState beforeState = this.migrationLeadershipState;
+        long startTimeNs = time.nanoseconds();
         ZkMigrationLeadershipState afterState = migrationOp.apply(beforeState);
-        if (afterState.loggableChangeSinceState(beforeState)) {
-            log.info("{}. Transitioned migration state from {} to {}", name, beforeState, afterState);
+        long durationNs = time.nanoseconds() - startTimeNs;
+        if (afterState.loggableChangeSinceState(beforeState) || alwaysLog) {
+            log.info("{} in {} ns. Transitioned migration state from {} to {}",
+                name, durationNs, beforeState, afterState);
         } else if (afterState.equals(beforeState)) {
-            log.trace("{}. Kept migration state as {}", name, afterState);
+            log.trace("{} in {} ns. Kept migration state as {}", name, durationNs, afterState);
         } else {
-            log.trace("{}. Transitioned migration state from {} to {}", name, beforeState, afterState);
-
+            log.trace("{} in {} ns. Transitioned migration state from {} to {}",
+                name, durationNs, beforeState, afterState);
         }
         this.migrationLeadershipState = afterState;
     }
@@ -291,14 +303,28 @@ public class KRaftMigrationDriver implements MetadataPublisher {
         }
     }
 
+    /**
+     * Check that the migration driver is in the correct state for a given event. If the event causes
+     * updates (i.e., to ZK or broker RPCs), also check that the event is for the current KRaft controller epoch.
+     */
     private boolean checkDriverState(MigrationDriverState expectedState, MigrationEvent migrationEvent) {
-        if (migrationState.equals(expectedState)) {
-            return true;
-        } else {
+        if (migrationEvent instanceof MigrationWriteEvent) {
+            LeaderAndEpoch curLeaderAndEpoch = KRaftMigrationDriver.this.curLeaderAndEpoch;
+            LeaderAndEpoch eventLeaderAndEpoch = ((MigrationWriteEvent) migrationEvent).eventLeaderAndEpoch();
+            if (!eventLeaderAndEpoch.equals(curLeaderAndEpoch)) {
+                log.info("Current leader epoch is {}, but event was created with epoch {}. Not running this event {}.",
+                        curLeaderAndEpoch, eventLeaderAndEpoch, migrationEvent);
+                return false;
+            }
+        }
+
+        if (!migrationState.equals(expectedState)) {
             log.info("Expected driver state {} but found {}. Not running this event {}.",
-                expectedState, migrationState, migrationEvent.getClass().getSimpleName());
+                expectedState, migrationState, migrationEvent);
             return false;
         }
+
+        return true;
     }
 
     // Visible for testing
@@ -332,6 +358,7 @@ public class KRaftMigrationDriver implements MetadataPublisher {
 
     @Override
     public void onControllerChange(LeaderAndEpoch newLeaderAndEpoch) {
+        curLeaderAndEpoch = newLeaderAndEpoch;
         eventQueue.append(new KRaftLeaderEvent(newLeaderAndEpoch));
     }
 
@@ -367,12 +394,14 @@ public class KRaftMigrationDriver implements MetadataPublisher {
         boolean isSnapshot,
         Consumer<Throwable> completionHandler
     ) {
+        LeaderAndEpoch eventLeaderAndEpoch = KRaftMigrationDriver.this.curLeaderAndEpoch;
         MetadataChangeEvent metadataChangeEvent = new MetadataChangeEvent(
             delta,
             newImage,
             provenance,
             isSnapshot,
-            completionHandler
+            completionHandler,
+            eventLeaderAndEpoch
         );
         eventQueue.append(metadataChangeEvent);
     }
@@ -399,6 +428,18 @@ public class KRaftMigrationDriver implements MetadataPublisher {
         }
     }
 
+    /**
+     * An event that has some side effects like updating ZK or sending RPCs to brokers.
+     * These event should only run if they are for the current KRaft controller epoch.
+     * See {@link #checkDriverState(MigrationDriverState, MigrationEvent)}
+     */
+    interface MigrationWriteEvent {
+        /**
+         * @return The LeaderAndEpoch as seen at the time of event creation.
+         */
+        LeaderAndEpoch eventLeaderAndEpoch();
+    }
+
     /**
      * An event generated by a call to {@link MetadataPublisher#onControllerChange}. This will not be called until
      * this class is registered with {@link org.apache.kafka.image.loader.MetadataLoader}. The registration happens
@@ -420,14 +461,15 @@ public class KRaftMigrationDriver implements MetadataPublisher {
                 applyMigrationOperation("Became inactive migration driver", state ->
                     state.withNewKRaftController(
                         leaderAndEpoch.leaderId().orElse(ZkMigrationLeadershipState.EMPTY.kraftControllerId()),
-                        leaderAndEpoch.epoch())
+                        leaderAndEpoch.epoch()
+                    ).withUnknownZkController()
                 );
                 transitionTo(MigrationDriverState.INACTIVE);
             } else {
                 // Load the existing migration state and apply the new KRaft state
                 applyMigrationOperation("Became active migration driver", state -> {
                     ZkMigrationLeadershipState recoveredState = zkMigrationClient.getOrCreateMigrationRecoveryState(state);
-                    return recoveredState.withNewKRaftController(nodeId, leaderAndEpoch.epoch());
+                    return recoveredState.withNewKRaftController(nodeId, leaderAndEpoch.epoch()).withUnknownZkController();
                 });
 
                 // Before becoming the controller fo ZkBrokers, we need to make sure the
@@ -443,19 +485,22 @@ public class KRaftMigrationDriver implements MetadataPublisher {
         private final MetadataProvenance provenance;
         private final boolean isSnapshot;
         private final Consumer<Throwable> completionHandler;
+        private final LeaderAndEpoch leaderAndEpoch;
 
         MetadataChangeEvent(
                 MetadataDelta delta,
                 MetadataImage image,
                 MetadataProvenance provenance,
                 boolean isSnapshot,
-                Consumer<Throwable> completionHandler
+                Consumer<Throwable> completionHandler,
+                LeaderAndEpoch leaderAndEpoch
         ) {
             this.delta = delta;
             this.image = image;
             this.provenance = provenance;
             this.isSnapshot = isSnapshot;
             this.completionHandler = completionHandler;
+            this.leaderAndEpoch = leaderAndEpoch;
         }
 
         @Override
@@ -468,7 +513,7 @@ public class KRaftMigrationDriver implements MetadataPublisher {
                 completionHandler.accept(null);
                 return;
             }
-
+            LeaderAndEpoch curLeaderAndEpoch = KRaftMigrationDriver.this.curLeaderAndEpoch;
             KRaftMigrationDriver.this.firstPublish = true;
             MetadataImage prevImage = KRaftMigrationDriver.this.image;
             KRaftMigrationDriver.this.image = image;
@@ -482,7 +527,7 @@ public class KRaftMigrationDriver implements MetadataPublisher {
 
             if (!migrationState.allowDualWrite()) {
                 log.trace("Received metadata {}, but the controller is not in dual-write " +
-                        "mode. Ignoring the change to be replicated to Zookeeper", metadataType);
+                    "mode. Ignoring this metadata update.", metadataType);
                 completionHandler.accept(null);
                 // If the driver is active and dual-write is not yet enabled, then the migration has not yet begun.
                 // Only wake up the thread if the broker registrations have changed
@@ -492,6 +537,13 @@ public class KRaftMigrationDriver implements MetadataPublisher {
                 return;
             }
 
+            if (!curLeaderAndEpoch.equals(leaderAndEpoch)) {
+                log.trace("Received metadata {} with {}, but the current leader and epoch is {}." +
+                    "Ignoring this metadata update.", metadataType, leaderAndEpoch, curLeaderAndEpoch);
+                completionHandler.accept(null);
+                return;
+            }
+
             // Until the metadata has been migrated, the migrationLeadershipState offset is -1. We need to ignore
             // metadata images until we see that the migration has happened and the image exceeds the offset of the
             // migration
@@ -534,8 +586,8 @@ public class KRaftMigrationDriver implements MetadataPublisher {
             //update the dual write offset metric
             controllerMetrics.updateDualWriteOffset(image.highestOffsetAndEpoch().offset());
 
-            applyMigrationOperation("Updating ZK migration state after " + metadataType,
-                    state -> zkMigrationClient.setMigrationRecoveryState(zkStateAfterDualWrite));
+            applyMigrationOperation("Updated ZK migration state after " + metadataType,
+                state -> zkMigrationClient.setMigrationRecoveryState(zkStateAfterDualWrite));
 
             if (isSnapshot) {
                 // When we load a snapshot, need to send full metadata updates to the brokers
@@ -559,6 +611,14 @@ public class KRaftMigrationDriver implements MetadataPublisher {
             completionHandler.accept(e);
             super.handleException(e);
         }
+
+        @Override
+        public String toString() {
+            return "MetadataChangeEvent{" +
+                "provenance=" + provenance +
+                ", isSnapshot=" + isSnapshot +
+                '}';
+        }
     }
 
     class WaitForControllerQuorumEvent extends MigrationEvent {
@@ -620,34 +680,46 @@ public class KRaftMigrationDriver implements MetadataPublisher {
         }
     }
 
-    class BecomeZkControllerEvent extends MigrationEvent {
+    class BecomeZkControllerEvent extends MigrationEvent implements MigrationWriteEvent {
+        private final LeaderAndEpoch leaderAndEpoch;
+
+        BecomeZkControllerEvent(LeaderAndEpoch leaderAndEpoch) {
+            this.leaderAndEpoch = leaderAndEpoch;
+        }
+
         @Override
         public void run() throws Exception {
+            // The leader epoch check in checkDriverState prevents us from getting stuck retrying this event after a
+            // new leader has been seen.
             if (checkDriverState(MigrationDriverState.BECOME_CONTROLLER, this)) {
-                applyMigrationOperation("Claiming ZK controller leadership", zkMigrationClient::claimControllerLeadership);
+                applyMigrationOperation("Claimed ZK controller leadership", zkMigrationClient::claimControllerLeadership, true);
                 if (migrationLeadershipState.zkControllerEpochZkVersion() == ZkMigrationLeadershipState.UNKNOWN_ZK_VERSION) {
                     log.info("Unable to claim leadership, will retry until we learn of a different KRaft leader");
+                    return; // Stay in BECOME_CONTROLLER state and retry
+                }
+
+                // KAFKA-16171 and KAFKA-16667: Prior writing to /controller and /controller_epoch ZNodes above,
+                // the previous controller could have modified the /migration ZNode. Since ZK does grant us linearizability
+                // between writes and reads on different ZNodes, we need to write something to the /migration ZNode to
+                // ensure we have the latest /migration zkVersion.
+                applyMigrationOperation("Updated migration state", state -> {
+                    // ZkVersion of -1 causes an unconditional update on /migration via KafkaZkClient#retryRequestsUntilConnected
+                    state = state.withMigrationZkVersion(-1);
+                    return zkMigrationClient.setMigrationRecoveryState(state);
+                });
 
+                if (!migrationLeadershipState.initialZkMigrationComplete()) {
+                    transitionTo(MigrationDriverState.ZK_MIGRATION);
                 } else {
-                    if (!migrationLeadershipState.initialZkMigrationComplete()) {
-                        transitionTo(MigrationDriverState.ZK_MIGRATION);
-                    } else {
-                        // KAFKA-16171 after loading the migration state in KRaftLeaderEvent, the previous controller
-                        // could have modified the /migration ZNode. Re-read it here after claiming the controller ZNode
-                        applyMigrationOperation("Re-reading migration state", state -> {
-                            ZkMigrationLeadershipState reloadedState =
-                                zkMigrationClient.getOrCreateMigrationRecoveryState(ZkMigrationLeadershipState.EMPTY);
-                            return KRaftMigrationDriver.this.migrationLeadershipState
-                                .withMigrationZkVersion(reloadedState.migrationZkVersion())
-                                .withKRaftMetadataOffsetAndEpoch(
-                                    reloadedState.kraftMetadataOffset(),
-                                    reloadedState.kraftMetadataEpoch());
-                        });
-                        transitionTo(MigrationDriverState.SYNC_KRAFT_TO_ZK);
-                    }
+                    transitionTo(MigrationDriverState.SYNC_KRAFT_TO_ZK);
                 }
             }
         }
+
+        @Override
+        public LeaderAndEpoch eventLeaderAndEpoch() {
+            return leaderAndEpoch;
+        }
     }
 
     private BufferingBatchConsumer<ApiMessageAndVersion> buildMigrationBatchConsumer(
@@ -673,7 +745,14 @@ public class KRaftMigrationDriver implements MetadataPublisher {
         }, minBatchSize);
     }
 
-    class MigrateMetadataEvent extends MigrationEvent {
+    class MigrateMetadataEvent extends MigrationEvent implements MigrationWriteEvent {
+
+        private final LeaderAndEpoch leaderAndEpoch;
+
+        MigrateMetadataEvent(LeaderAndEpoch leaderAndEpoch) {
+            this.leaderAndEpoch = leaderAndEpoch;
+        }
+
         @Override
         public void run() throws Exception {
             if (!checkDriverState(MigrationDriverState.ZK_MIGRATION, this)) {
@@ -715,7 +794,11 @@ public class KRaftMigrationDriver implements MetadataPublisher {
                 ZkMigrationLeadershipState newState = migrationLeadershipState.withKRaftMetadataOffsetAndEpoch(
                     offsetAndEpochAfterMigration.offset(),
                     offsetAndEpochAfterMigration.epoch());
-                applyMigrationOperation("Finished initial migration of ZK metadata to KRaft", state -> zkMigrationClient.setMigrationRecoveryState(newState));
+                applyMigrationOperation(
+                    "Finished initial migration of ZK metadata to KRaft",
+                    state -> zkMigrationClient.setMigrationRecoveryState(newState),
+                    true
+                );
                 // Even though we just migrated everything, we still pass through the SYNC_KRAFT_TO_ZK state. This
                 // accomplishes two things: ensuring we have consistent metadata state between KRaft and ZK, and
                 // exercising the snapshot handling code in KRaftMigrationZkWriter.
@@ -727,9 +810,20 @@ public class KRaftMigrationDriver implements MetadataPublisher {
                 super.handleException(t);
             }
         }
+
+        @Override
+        public LeaderAndEpoch eventLeaderAndEpoch() {
+            return leaderAndEpoch;
+        }
     }
 
-    class SyncKRaftMetadataEvent extends MigrationEvent {
+    class SyncKRaftMetadataEvent extends MigrationEvent implements MigrationWriteEvent {
+        private final LeaderAndEpoch leaderAndEpoch;
+
+        SyncKRaftMetadataEvent(LeaderAndEpoch leaderAndEpoch) {
+            this.leaderAndEpoch = leaderAndEpoch;
+        }
+
         @Override
         public void run() throws Exception {
             if (checkDriverState(MigrationDriverState.SYNC_KRAFT_TO_ZK, this)) {
@@ -747,13 +841,28 @@ public class KRaftMigrationDriver implements MetadataPublisher {
                     dualWriteCounts, KRaftMigrationDriver.this::applyMigrationOperation));
                 long endTime = time.nanoseconds();
                 controllerMetrics.updateZkWriteSnapshotTimeMs(NANOSECONDS.toMillis(startTime - endTime));
-                log.info("Made the following ZK writes when reconciling with KRaft state: {}", dualWriteCounts);
+                if (dualWriteCounts.isEmpty()) {
+                    log.info("Did not make any ZK writes when reconciling with KRaft state.");
+                } else {
+                    log.info("Made the following ZK writes when reconciling with KRaft state: {}", dualWriteCounts);
+                }
                 transitionTo(MigrationDriverState.KRAFT_CONTROLLER_TO_BROKER_COMM);
             }
         }
+
+        @Override
+        public LeaderAndEpoch eventLeaderAndEpoch() {
+            return leaderAndEpoch;
+        }
     }
 
-    class SendRPCsToBrokersEvent extends MigrationEvent {
+    class SendRPCsToBrokersEvent extends MigrationEvent implements MigrationWriteEvent {
+
+        private final LeaderAndEpoch leaderAndEpoch;
+
+        SendRPCsToBrokersEvent(LeaderAndEpoch leaderAndEpoch) {
+            this.leaderAndEpoch = leaderAndEpoch;
+        }
 
         @Override
         public void run() throws Exception {
@@ -772,13 +881,18 @@ public class KRaftMigrationDriver implements MetadataPublisher {
                 }
             }
         }
+
+        @Override
+        public LeaderAndEpoch eventLeaderAndEpoch() {
+            return leaderAndEpoch;
+        }
     }
 
     class RecoverMigrationStateFromZKEvent extends MigrationEvent {
         @Override
         public void run() throws Exception {
             if (checkDriverState(MigrationDriverState.UNINITIALIZED, this)) {
-                applyMigrationOperation("Recovering migration state from ZK", zkMigrationClient::getOrCreateMigrationRecoveryState);
+                applyMigrationOperation("Recovered migration state from ZK", zkMigrationClient::getOrCreateMigrationRecoveryState);
                 String maybeDone = migrationLeadershipState.initialZkMigrationComplete() ? "done" : "not done";
                 log.info("Initial migration of ZK metadata is {}.", maybeDone);
 
@@ -796,6 +910,7 @@ public class KRaftMigrationDriver implements MetadataPublisher {
 
         @Override
         public void run() throws Exception {
+            LeaderAndEpoch eventLeaderAndEpoch = KRaftMigrationDriver.this.curLeaderAndEpoch;
             switch (migrationState) {
                 case UNINITIALIZED:
                     eventQueue.append(new RecoverMigrationStateFromZKEvent());
@@ -811,16 +926,16 @@ public class KRaftMigrationDriver implements MetadataPublisher {
                     eventQueue.append(new WaitForZkBrokersEvent());
                     break;
                 case BECOME_CONTROLLER:
-                    eventQueue.append(new BecomeZkControllerEvent());
+                    eventQueue.append(new BecomeZkControllerEvent(eventLeaderAndEpoch));
                     break;
                 case ZK_MIGRATION:
-                    eventQueue.append(new MigrateMetadataEvent());
+                    eventQueue.append(new MigrateMetadataEvent(eventLeaderAndEpoch));
                     break;
                 case SYNC_KRAFT_TO_ZK:
-                    eventQueue.append(new SyncKRaftMetadataEvent());
+                    eventQueue.append(new SyncKRaftMetadataEvent(eventLeaderAndEpoch));
                     break;
                 case KRAFT_CONTROLLER_TO_BROKER_COMM:
-                    eventQueue.append(new SendRPCsToBrokersEvent());
+                    eventQueue.append(new SendRPCsToBrokersEvent(eventLeaderAndEpoch));
                     break;
                 case DUAL_WRITE:
                     // Nothing to do in the PollEvent. If there's metadata change, we use
diff --git a/metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java b/metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java
index 6870ad8f0c..aa9468a6bc 100644
--- a/metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java
+++ b/metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java
@@ -154,7 +154,7 @@ public class KRaftMigrationZkWriter {
         if (!pendingTopicDeletions.isEmpty()) {
             operationConsumer.accept(
                 DELETE_PENDING_TOPIC_DELETION,
-                "Delete pending topic deletions",
+                "Deleted pending topic deletions",
                 migrationState -> migrationClient.topicClient().clearPendingTopicDeletions(pendingTopicDeletions, migrationState)
             );
         }
@@ -229,7 +229,7 @@ public class KRaftMigrationZkWriter {
             TopicImage topic = topicsImage.getTopic(topicId);
             operationConsumer.accept(
                 CREATE_TOPIC,
-                "Create Topic " + topic.name() + ", ID " + topicId,
+                "Created Topic " + topic.name() + ", ID " + topicId,
                 migrationState -> migrationClient.topicClient().createTopic(topic.name(), topicId, topic.partitions(), migrationState)
             );
         });
@@ -246,7 +246,7 @@ public class KRaftMigrationZkWriter {
         deletedTopics.forEach((topicId, topicName) -> {
             operationConsumer.accept(
                 DELETE_TOPIC,
-                "Delete Topic " + topicName + ", ID " + topicId,
+                "Deleted Topic " + topicName + ", ID " + topicId,
                 migrationState -> migrationClient.topicClient().deleteTopic(topicName, migrationState)
             );
             ConfigResource resource = new ConfigResource(ConfigResource.Type.TOPIC, topicName);
@@ -256,7 +256,7 @@ public class KRaftMigrationZkWriter {
             TopicImage topic = topicsImage.getTopic(topicId);
             operationConsumer.accept(
                 UPDATE_PARTITION,
-                "Creating additional partitions for Topic " + topic.name() + ", ID " + topicId,
+                "Created additional partitions for Topic " + topic.name() + ", ID " + topicId,
                 migrationState -> migrationClient.topicClient().updateTopicPartitions(
                     Collections.singletonMap(topic.name(), partitionMap),
                     migrationState));
@@ -266,7 +266,7 @@ public class KRaftMigrationZkWriter {
             TopicImage topic = topicsImage.getTopic(topicId);
             operationConsumer.accept(
                 UPDATE_PARTITION,
-                "Updating Partitions for Topic " + topic.name() + ", ID " + topicId,
+                "Updated Partitions for Topic " + topic.name() + ", ID " + topicId,
                 migrationState -> migrationClient.topicClient().updateTopicPartitions(
                     Collections.singletonMap(topic.name(), partitionMap),
                     migrationState));
@@ -275,7 +275,7 @@ public class KRaftMigrationZkWriter {
         extraneousPartitionsInZk.forEach((topicName, partitions) -> {
             operationConsumer.accept(
                 DELETE_PARTITION,
-                "Deleting extraneous Partitions " + partitions + " for Topic " + topicName,
+                "Deleted extraneous Partitions " + partitions + " for Topic " + topicName,
                 migrationState -> migrationClient.topicClient().deleteTopicPartitions(
                     Collections.singletonMap(topicName, partitions),
                     migrationState));
@@ -290,7 +290,7 @@ public class KRaftMigrationZkWriter {
     ) {
         topicsDelta.deletedTopicIds().forEach(topicId -> {
             String name = deletedTopicNameResolver.apply(topicId);
-            operationConsumer.accept(DELETE_TOPIC, "Deleting topic " + name + ", ID " + topicId,
+            operationConsumer.accept(DELETE_TOPIC, "Deleted topic " + name + ", ID " + topicId,
                 migrationState -> migrationClient.topicClient().deleteTopic(name, migrationState));
         });
 
@@ -298,7 +298,7 @@ public class KRaftMigrationZkWriter {
             if (topicsDelta.createdTopicIds().contains(topicId)) {
                 operationConsumer.accept(
                     CREATE_TOPIC,
-                    "Create Topic " + topicDelta.name() + ", ID " + topicId,
+                    "Created Topic " + topicDelta.name() + ", ID " + topicId,
                     migrationState -> migrationClient.topicClient().createTopic(
                         topicDelta.name(),
                         topicId,
@@ -308,7 +308,7 @@ public class KRaftMigrationZkWriter {
                 if (topicDelta.hasPartitionsWithAssignmentChanges())
                     operationConsumer.accept(
                         UPDATE_TOPIC,
-                        "Updating Topic " + topicDelta.name() + ", ID " + topicId,
+                        "Updated Topic " + topicDelta.name() + ", ID " + topicId,
                         migrationState -> migrationClient.topicClient().updateTopic(
                             topicDelta.name(),
                             topicId,
@@ -319,7 +319,7 @@ public class KRaftMigrationZkWriter {
                 if (!newPartitions.isEmpty()) {
                     operationConsumer.accept(
                         UPDATE_PARTITION,
-                        "Create new partitions for Topic " + topicDelta.name() + ", ID " + topicId,
+                        "Created new partitions for Topic " + topicDelta.name() + ", ID " + topicId,
                         migrationState -> migrationClient.topicClient().createTopicPartitions(
                             Collections.singletonMap(topicDelta.name(), newPartitions),
                             migrationState));
@@ -330,7 +330,7 @@ public class KRaftMigrationZkWriter {
                     final Map<Integer, PartitionRegistration> finalChangedPartitions = changedPartitions;
                     operationConsumer.accept(
                         UPDATE_PARTITION,
-                        "Updating Partitions for Topic " + topicDelta.name() + ", ID " + topicId,
+                        "Updated Partitions for Topic " + topicDelta.name() + ", ID " + topicId,
                         migrationState -> migrationClient.topicClient().updateTopicPartitions(
                             Collections.singletonMap(topicDelta.name(), finalChangedPartitions),
                             migrationState));
@@ -378,7 +378,7 @@ public class KRaftMigrationZkWriter {
             Map<String, String> props = configsImage.configMapForResource(resource);
             if (!props.isEmpty()) {
                 String opType = brokerOrTopicOpType(resource, UPDATE_BROKER_CONFIG, UPDATE_TOPIC_CONFIG);
-                operationConsumer.accept(opType, "Create configs for " + resource.type().name() + " " + resource.name(),
+                operationConsumer.accept(opType, "Created configs for " + resource.type().name() + " " + resource.name(),
                     migrationState -> migrationClient.configClient().writeConfigs(resource, props, migrationState));
             }
         });
@@ -387,11 +387,11 @@ public class KRaftMigrationZkWriter {
             Map<String, String> props = configsImage.configMapForResource(resource);
             if (props.isEmpty()) {
                 String opType = brokerOrTopicOpType(resource, DELETE_BROKER_CONFIG, DELETE_TOPIC_CONFIG);
-                operationConsumer.accept(opType, "Delete configs for " + resource.type().name() + " " + resource.name(),
+                operationConsumer.accept(opType, "Deleted configs for " + resource.type().name() + " " + resource.name(),
                     migrationState -> migrationClient.configClient().deleteConfigs(resource, migrationState));
             } else {
                 String opType = brokerOrTopicOpType(resource, UPDATE_BROKER_CONFIG, UPDATE_TOPIC_CONFIG);
-                operationConsumer.accept(opType, "Update configs for " + resource.type().name() + " " + resource.name(),
+                operationConsumer.accept(opType, "Updated configs for " + resource.type().name() + " " + resource.name(),
                     migrationState -> migrationClient.configClient().writeConfigs(resource, props, migrationState));
             }
         });
@@ -464,7 +464,7 @@ public class KRaftMigrationZkWriter {
 
         changedNonUserEntities.forEach(entity -> {
             Map<String, Double> quotaMap = clientQuotasImage.entities().getOrDefault(entity, ClientQuotaImage.EMPTY).quotaMap();
-            opConsumer.accept(UPDATE_CLIENT_QUOTA, "Update client quotas for " + entity, migrationState ->
+            opConsumer.accept(UPDATE_CLIENT_QUOTA, "Updated client quotas for " + entity, migrationState ->
                 migrationClient.configClient().writeClientQuotas(entity.entries(), quotaMap, Collections.emptyMap(), migrationState));
         });
 
@@ -473,7 +473,7 @@ public class KRaftMigrationZkWriter {
             Map<String, Double> quotaMap = clientQuotasImage.entities().
                 getOrDefault(entity, ClientQuotaImage.EMPTY).quotaMap();
             Map<String, String> scramMap = getScramCredentialStringsForUser(scramImage, userName);
-            opConsumer.accept(UPDATE_CLIENT_QUOTA, "Update client quotas for " + userName, migrationState ->
+            opConsumer.accept(UPDATE_CLIENT_QUOTA, "Updated client quotas for " + userName, migrationState ->
                 migrationClient.configClient().writeClientQuotas(entity.entries(), quotaMap, scramMap, migrationState));
         });
     }
@@ -486,11 +486,11 @@ public class KRaftMigrationZkWriter {
         Optional<ProducerIdsBlock> zkProducerId = migrationClient.readProducerId();
         if (zkProducerId.isPresent()) {
             if (zkProducerId.get().nextBlockFirstId() != image.nextProducerId()) {
-                operationConsumer.accept(UPDATE_PRODUCER_ID, "Setting next producer ID", migrationState ->
+                operationConsumer.accept(UPDATE_PRODUCER_ID, "Set next producer ID", migrationState ->
                     migrationClient.writeProducerId(image.nextProducerId(), migrationState));
             }
         } else {
-            operationConsumer.accept(UPDATE_PRODUCER_ID, "Setting next producer ID", migrationState ->
+            operationConsumer.accept(UPDATE_PRODUCER_ID, "Set next producer ID", migrationState ->
                 migrationClient.writeProducerId(image.nextProducerId(), migrationState));
         }
     }
@@ -500,10 +500,10 @@ public class KRaftMigrationZkWriter {
         updatedResources.forEach(configResource -> {
             Map<String, String> props = configsImage.configMapForResource(configResource);
             if (props.isEmpty()) {
-                operationConsumer.accept("DeleteConfig", "Delete configs for " + configResource, migrationState ->
+                operationConsumer.accept("DeleteConfig", "Deleted configs for " + configResource, migrationState ->
                     migrationClient.configClient().deleteConfigs(configResource, migrationState));
             } else {
-                operationConsumer.accept("UpdateConfig", "Update configs for " + configResource, migrationState ->
+                operationConsumer.accept("UpdateConfig", "Updated configs for " + configResource, migrationState ->
                     migrationClient.configClient().writeConfigs(configResource, props, migrationState));
             }
         });
@@ -532,7 +532,7 @@ public class KRaftMigrationZkWriter {
                         users.add(userName);
                     } else {
                         Map<String, Double> quotaMap = metadataImage.clientQuotas().entities().get(clientQuotaEntity).quotaMap();
-                        operationConsumer.accept(UPDATE_CLIENT_QUOTA, "Updating client quota " + clientQuotaEntity, migrationState ->
+                        operationConsumer.accept(UPDATE_CLIENT_QUOTA, "Updated client quota " + clientQuotaEntity, migrationState ->
                             migrationClient.configClient().writeClientQuotas(clientQuotaEntity.entries(), quotaMap, Collections.emptyMap(), migrationState));
                     }
                 });
@@ -544,11 +544,11 @@ public class KRaftMigrationZkWriter {
                 ClientQuotaEntity clientQuotaEntity = new ClientQuotaEntity(Collections.singletonMap(ClientQuotaEntity.USER, userName));
                 if ((metadataImage.clientQuotas() == null) ||
                     (metadataImage.clientQuotas().entities().get(clientQuotaEntity) == null)) {
-                    operationConsumer.accept(UPDATE_CLIENT_QUOTA, "Updating scram credentials for " + clientQuotaEntity, migrationState ->
+                    operationConsumer.accept(UPDATE_CLIENT_QUOTA, "Updated scram credentials for " + clientQuotaEntity, migrationState ->
                         migrationClient.configClient().writeClientQuotas(clientQuotaEntity.entries(), Collections.emptyMap(), userScramMap, migrationState));
                 } else {
                     Map<String, Double> quotaMap = metadataImage.clientQuotas().entities().get(clientQuotaEntity).quotaMap();
-                    operationConsumer.accept(UPDATE_CLIENT_QUOTA, "Updating client quota for " + clientQuotaEntity, migrationState ->
+                    operationConsumer.accept(UPDATE_CLIENT_QUOTA, "Updated client quota for " + clientQuotaEntity, migrationState ->
                         migrationClient.configClient().writeClientQuotas(clientQuotaEntity.entries(), quotaMap, userScramMap, migrationState));
                 }
             });
@@ -556,7 +556,7 @@ public class KRaftMigrationZkWriter {
     }
 
     void handleProducerIdDelta(ProducerIdsDelta delta, KRaftMigrationOperationConsumer operationConsumer) {
-        operationConsumer.accept(UPDATE_PRODUCER_ID, "Setting next producer ID", migrationState ->
+        operationConsumer.accept(UPDATE_PRODUCER_ID, "Set next producer ID", migrationState ->
             migrationClient.writeProducerId(delta.nextProducerId(), migrationState));
     }
 
@@ -593,20 +593,20 @@ public class KRaftMigrationZkWriter {
         newResources.forEach(resourcePattern -> {
             // newResources is generated from allAclsInSnapshot, and we don't remove from that map, so this unguarded .get() is safe
             Set<AccessControlEntry> accessControlEntries = allAclsInSnapshot.get(resourcePattern);
-            String name = "Writing " + accessControlEntries.size() + " for resource " + resourcePattern;
-            operationConsumer.accept(UPDATE_ACL, name, migrationState ->
+            String logMsg = "Wrote " + accessControlEntries.size() + " for resource " + resourcePattern;
+            operationConsumer.accept(UPDATE_ACL, logMsg, migrationState ->
                 migrationClient.aclClient().writeResourceAcls(resourcePattern, accessControlEntries, migrationState));
         });
 
         resourcesToDelete.forEach(deletedResource -> {
-            String name = "Deleting resource " + deletedResource + " which has no ACLs in snapshot";
-            operationConsumer.accept(DELETE_ACL, name, migrationState ->
+            String logMsg = "Deleted resource " + deletedResource + " which has no ACLs in snapshot";
+            operationConsumer.accept(DELETE_ACL, logMsg, migrationState ->
                 migrationClient.aclClient().deleteResource(deletedResource, migrationState));
         });
 
         changedResources.forEach((resourcePattern, accessControlEntries) -> {
-            String name = "Writing " + accessControlEntries.size() + " for resource " + resourcePattern;
-            operationConsumer.accept(UPDATE_ACL, name, migrationState ->
+            String logMsg = "Wrote " + accessControlEntries.size() + " for resource " + resourcePattern;
+            operationConsumer.accept(UPDATE_ACL, logMsg, migrationState ->
                 migrationClient.aclClient().writeResourceAcls(resourcePattern, accessControlEntries, migrationState));
         });
     }
@@ -642,12 +642,12 @@ public class KRaftMigrationZkWriter {
         // If there are no more ACLs for a resource, delete it. Otherwise, update it with the new set of ACLs
         aclsToWrite.forEach((resourcePattern, accessControlEntries) -> {
             if (accessControlEntries.isEmpty()) {
-                String name = "Deleting resource " + resourcePattern + " which has no more ACLs";
-                operationConsumer.accept(DELETE_ACL, name, migrationState ->
+                String logMsg = "Deleted resource " + resourcePattern + " which has no more ACLs";
+                operationConsumer.accept(DELETE_ACL, logMsg, migrationState ->
                     migrationClient.aclClient().deleteResource(resourcePattern, migrationState));
             } else {
-                String name = "Writing " + accessControlEntries.size() + " for resource " + resourcePattern;
-                operationConsumer.accept(UPDATE_ACL, name, migrationState ->
+                String logMsg = "Wrote " + accessControlEntries.size() + " for resource " + resourcePattern;
+                operationConsumer.accept(UPDATE_ACL, logMsg, migrationState ->
                     migrationClient.aclClient().writeResourceAcls(resourcePattern, accessControlEntries, migrationState));
             }
         });
@@ -658,10 +658,10 @@ public class KRaftMigrationZkWriter {
         updatedTokens.forEach(tokenId -> {
             DelegationTokenData tokenData = image.tokens().get(tokenId);
             if (tokenData == null) {
-                operationConsumer.accept("DeleteDelegationToken", "Delete DelegationToken for " + tokenId, migrationState ->
+                operationConsumer.accept("DeleteDelegationToken", "Deleted DelegationToken for " + tokenId, migrationState ->
                     migrationClient.delegationTokenClient().deleteDelegationToken(tokenId, migrationState));
             } else {
-                operationConsumer.accept("UpdateDelegationToken", "Update DelegationToken for " + tokenId, migrationState ->
+                operationConsumer.accept("UpdateDelegationToken", "Updated DelegationToken for " + tokenId, migrationState ->
                     migrationClient.delegationTokenClient().writeDelegationToken(tokenId, tokenData.tokenInformation(), migrationState));
             }
         });
@@ -670,14 +670,14 @@ public class KRaftMigrationZkWriter {
     void handleDelegationTokenSnapshot(DelegationTokenImage image, KRaftMigrationOperationConsumer operationConsumer) {
         image.tokens().keySet().forEach(tokenId -> {
             DelegationTokenData tokenData = image.tokens().get(tokenId);
-            operationConsumer.accept("UpdateDelegationToken", "Update DelegationToken for " + tokenId, migrationState ->
+            operationConsumer.accept("UpdateDelegationToken", "Updated DelegationToken for " + tokenId, migrationState ->
                 migrationClient.delegationTokenClient().writeDelegationToken(tokenId, tokenData.tokenInformation(), migrationState));
         });
 
         List<String> tokens = migrationClient.delegationTokenClient().getDelegationTokens();
         tokens.forEach(tokenId -> {
             if (!image.tokens().containsKey(tokenId)) {
-                operationConsumer.accept("DeleteDelegationToken", "Delete DelegationToken for " + tokenId, migrationState ->
+                operationConsumer.accept("DeleteDelegationToken", "Deleted DelegationToken for " + tokenId, migrationState ->
                     migrationClient.delegationTokenClient().deleteDelegationToken(tokenId, migrationState));
             }
         });
diff --git a/tests/kafkatest/tests/core/zookeeper_migration_test.py b/tests/kafkatest/tests/core/zookeeper_migration_test.py
index 530179da5b..5ddd86236a 100644
--- a/tests/kafkatest/tests/core/zookeeper_migration_test.py
+++ b/tests/kafkatest/tests/core/zookeeper_migration_test.py
@@ -203,84 +203,6 @@ class TestMigration(ProduceConsumeValidateTest):
 
         assert saw_expected_error, "Did not see expected ERROR log in the controller logs"
 
-    @cluster(num_nodes=5)
-    def test_upgrade_after_3_4_migration(self):
-        """
-        Perform a migration on version 3.4.0. Then do a rolling upgrade to 3.5+ and ensure we see
-        the correct migration state in the log.
-        """
-        zk_quorum = partial(ServiceQuorumInfo, zk)
-        self.zk = ZookeeperService(self.test_context, num_nodes=1, version=LATEST_3_4)
-        self.kafka = KafkaService(self.test_context,
-                                  num_nodes=3,
-                                  zk=self.zk,
-                                  version=LATEST_3_4,
-                                  quorum_info_provider=zk_quorum,
-                                  allow_zk_with_kraft=True,
-                                  server_prop_overrides=[
-                                      ["zookeeper.metadata.migration.enable", "true"],
-                                  ])
-
-        remote_quorum = partial(ServiceQuorumInfo, isolated_kraft)
-        controller = KafkaService(self.test_context, num_nodes=1, zk=self.zk, version=LATEST_3_4,
-                                  allow_zk_with_kraft=True,
-                                  isolated_kafka=self.kafka,
-                                  server_prop_overrides=[["zookeeper.connect", self.zk.connect_setting()],
-                                                         ["zookeeper.metadata.migration.enable", "true"]],
-                                  quorum_info_provider=remote_quorum)
-
-        self.kafka.security_protocol = "PLAINTEXT"
-        self.kafka.interbroker_security_protocol = "PLAINTEXT"
-        self.zk.start()
-
-        controller.start()
-
-        self.logger.info("Pre-generating clusterId for ZK.")
-        cluster_id_json = """{"version": "1", "id": "%s"}""" % CLUSTER_ID
-        self.zk.create(path="/cluster")
-        self.zk.create(path="/cluster/id", value=cluster_id_json)
-        self.kafka.reconfigure_zk_for_migration(controller)
-        self.kafka.start()
-
-        topic_cfg = {
-            "topic": self.topic,
-            "partitions": self.partitions,
-            "replication-factor": self.replication_factor,
-            "configs": {"min.insync.replicas": 2}
-        }
-        self.kafka.create_topic(topic_cfg)
-
-        # Now we're in dual-write mode. The 3.4 controller will have written a PRE_MIGRATION record (1) into the log.
-        # We now upgrade the controller to 3.5+ where 1 is redefined as MIGRATION.
-        for node in controller.nodes:
-            self.logger.info("Stopping controller node %s" % node.account.hostname)
-            self.kafka.controller_quorum.stop_node(node)
-            node.version = DEV_BRANCH
-            self.logger.info("Restarting controller node %s" % node.account.hostname)
-            self.kafka.controller_quorum.start_node(node)
-            self.wait_until_rejoin()
-            self.logger.info("Successfully restarted controller node %s" % node.account.hostname)
-
-        # Check the controller's logs for the INFO message that we're still in the migration state
-        saw_expected_log = False
-        for node in self.kafka.controller_quorum.nodes:
-            with node.account.monitor_log(KafkaService.STDOUT_STDERR_CAPTURE) as monitor:
-                monitor.offset = 0
-                try:
-                    # Shouldn't have to wait too long to see this log message after startup
-                    monitor.wait_until(
-                        "Staying in ZK migration",
-                        timeout_sec=10.0, backoff_sec=.25,
-                        err_msg=""
-                    )
-                    saw_expected_log = True
-                    break
-                except TimeoutError:
-                    continue
-
-        assert saw_expected_log, "Did not see expected INFO log after upgrading from a 3.4 migration"
-        self.kafka.stop()
-
     @cluster(num_nodes=5)
     def test_reconcile_kraft_to_zk(self):
         """
