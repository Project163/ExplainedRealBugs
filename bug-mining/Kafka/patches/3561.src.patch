diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/AbstractHerder.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/AbstractHerder.java
index 7bbcf0f686..6802737266 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/AbstractHerder.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/AbstractHerder.java
@@ -371,7 +371,9 @@ public abstract class AbstractHerder implements Herder, TaskStatus.Listener, Con
     }
 
     protected Map<String, ConfigValue> validateSinkConnectorConfig(SinkConnector connector, ConfigDef configDef, Map<String, String> config) {
-        return configDef.validateAll(config);
+        Map<String, ConfigValue> result = configDef.validateAll(config);
+        SinkConnectorConfig.validate(config, result);
+        return result;
     }
 
     protected Map<String, ConfigValue> validateSourceConnectorConfig(SourceConnector connector, ConfigDef configDef, Map<String, String> config) {
@@ -478,7 +480,6 @@ public abstract class AbstractHerder implements Herder, TaskStatus.Listener, Con
                 enrichedConfigDef = ConnectorConfig.enrich(plugins(), SourceConnectorConfig.configDef(), connectorProps, false);
                 validatedConnectorConfig = validateSourceConnectorConfig((SourceConnector) connector, enrichedConfigDef, connectorProps);
             } else {
-                SinkConnectorConfig.validate(connectorProps);
                 connectorType = org.apache.kafka.connect.health.ConnectorType.SINK;
                 enrichedConfigDef = ConnectorConfig.enrich(plugins(), SinkConnectorConfig.configDef(), connectorProps, false);
                 validatedConnectorConfig = validateSinkConnectorConfig((SinkConnector) connector, enrichedConfigDef, connectorProps);
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java
index aaf15f4eeb..4f20a32f81 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java
@@ -429,7 +429,10 @@ public class ConnectorConfig extends AbstractConfig {
                 final ConfigDef.Validator typeValidator = ConfigDef.LambdaValidator.with(
                     (String name, Object value) -> {
                         validateProps(prefix);
-                        getConfigDefFromConfigProvidingClass(typeConfig, (Class<?>) value);
+                        // The value will be null if the class couldn't be found; no point in performing follow-up validation
+                        if (value != null) {
+                            getConfigDefFromConfigProvidingClass(typeConfig, (Class<?>) value);
+                        }
                     },
                     () -> "valid configs for " + alias + " " + aliasKind.toLowerCase(Locale.ENGLISH));
                 newDef.define(typeConfig, Type.CLASS, ConfigDef.NO_DEFAULT_VALUE, typeValidator, Importance.HIGH,
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/SinkConnectorConfig.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/SinkConnectorConfig.java
index e17adbf33d..d9cb149489 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/SinkConnectorConfig.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/SinkConnectorConfig.java
@@ -19,13 +19,16 @@ package org.apache.kafka.connect.runtime;
 import org.apache.kafka.common.config.ConfigDef;
 import org.apache.kafka.common.config.ConfigDef.Importance;
 import org.apache.kafka.common.config.ConfigDef.Type;
+import org.apache.kafka.common.config.ConfigValue;
 import org.apache.kafka.common.utils.Utils;
 import org.apache.kafka.common.config.ConfigException;
 import org.apache.kafka.connect.runtime.isolation.Plugins;
 import org.apache.kafka.connect.sink.SinkTask;
 import org.apache.kafka.connect.transforms.util.RegexValidator;
 
+import java.util.ArrayList;
 import java.util.Collections;
+import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.regex.Pattern;
@@ -90,50 +93,83 @@ public class SinkConnectorConfig extends ConnectorConfig {
      * @param props sink configuration properties
      */
     public static void validate(Map<String, String> props) {
-        final boolean hasTopicsConfig = hasTopicsConfig(props);
-        final boolean hasTopicsRegexConfig = hasTopicsRegexConfig(props);
-        final boolean hasDlqTopicConfig = hasDlqTopicConfig(props);
+        Map<String, ConfigValue> validatedConfig = new LinkedHashMap<>();
+        validate(props, validatedConfig);
+        validatedConfig.values().stream()
+                .filter(configValue -> !configValue.errorMessages().isEmpty())
+                .findFirst()
+                .ifPresent(configValue -> {
+                    throw new ConfigException(configValue.name(), configValue.value(), configValue.errorMessages().get(0));
+                });
+    }
+
+    /**
+     * Perform preflight validation for the sink-specific properties for a connector.
+     *
+     * @param props           the configuration for the sink connector
+     * @param validatedConfig any already-known {@link ConfigValue validation results} for the configuration.
+     *                        May be empty, but may not be null. Any configuration errors discovered by this method will
+     *                        be {@link ConfigValue#addErrorMessage(String) added} to a value in this map, adding a new
+     *                        entry if one for the problematic property does not already exist.
+     */
+    public static void validate(Map<String, String> props, Map<String, ConfigValue> validatedConfig) {
+        final String topicsList = props.get(TOPICS_CONFIG);
+        final String topicsRegex = props.get(TOPICS_REGEX_CONFIG);
+        final String dlqTopic = props.getOrDefault(DLQ_TOPIC_NAME_CONFIG, "").trim();
+        final boolean hasTopicsConfig = !Utils.isBlank(topicsList);
+        final boolean hasTopicsRegexConfig = !Utils.isBlank(topicsRegex);
+        final boolean hasDlqTopicConfig = !Utils.isBlank(dlqTopic);
 
         if (hasTopicsConfig && hasTopicsRegexConfig) {
-            throw new ConfigException(SinkTask.TOPICS_CONFIG + " and " + SinkTask.TOPICS_REGEX_CONFIG +
-                " are mutually exclusive options, but both are set.");
+            String errorMessage = TOPICS_CONFIG + " and " + TOPICS_REGEX_CONFIG + " are mutually exclusive options, but both are set.";
+            addErrorMessage(validatedConfig, TOPICS_CONFIG, topicsList, errorMessage);
+            addErrorMessage(validatedConfig, TOPICS_REGEX_CONFIG, topicsRegex, errorMessage);
         }
 
         if (!hasTopicsConfig && !hasTopicsRegexConfig) {
-            throw new ConfigException("Must configure one of " +
-                SinkTask.TOPICS_CONFIG + " or " + SinkTask.TOPICS_REGEX_CONFIG);
+            String errorMessage = "Must configure one of " + TOPICS_CONFIG + " or " + TOPICS_REGEX_CONFIG;
+            addErrorMessage(validatedConfig, TOPICS_CONFIG, topicsList, errorMessage);
+            addErrorMessage(validatedConfig, TOPICS_REGEX_CONFIG, topicsRegex, errorMessage);
         }
 
         if (hasDlqTopicConfig) {
-            String dlqTopic = props.get(DLQ_TOPIC_NAME_CONFIG).trim();
             if (hasTopicsConfig) {
                 List<String> topics = parseTopicsList(props);
                 if (topics.contains(dlqTopic)) {
-                    throw new ConfigException(String.format("The DLQ topic '%s' may not be included in the list of "
-                            + "topics ('%s=%s') consumed by the connector", dlqTopic, SinkTask.TOPICS_REGEX_CONFIG, topics));
+                    String errorMessage = String.format(
+                            "The DLQ topic '%s' may not be included in the list of topics ('%s=%s') consumed by the connector",
+                            dlqTopic, TOPICS_CONFIG, topics
+                    );
+                    addErrorMessage(validatedConfig, TOPICS_CONFIG, topicsList, errorMessage);
                 }
             }
             if (hasTopicsRegexConfig) {
-                String topicsRegexStr = props.get(SinkTask.TOPICS_REGEX_CONFIG);
-                Pattern pattern = Pattern.compile(topicsRegexStr);
+                Pattern pattern = Pattern.compile(topicsRegex);
                 if (pattern.matcher(dlqTopic).matches()) {
-                    throw new ConfigException(String.format("The DLQ topic '%s' may not be included in the regex matching the "
-                            + "topics ('%s=%s') consumed by the connector", dlqTopic, SinkTask.TOPICS_REGEX_CONFIG, topicsRegexStr));
+                    String errorMessage = String.format(
+                            "The DLQ topic '%s' may not be matched by the regex for the topics ('%s=%s') consumed by the connector",
+                            dlqTopic, TOPICS_REGEX_CONFIG, topicsRegex
+                    );
+                    addErrorMessage(validatedConfig, TOPICS_REGEX_CONFIG, topicsRegex, errorMessage);
                 }
             }
         }
     }
 
+    private static void addErrorMessage(Map<String, ConfigValue> validatedConfig, String name, String value, String errorMessage) {
+        validatedConfig.computeIfAbsent(
+                name,
+                p -> new ConfigValue(name, value, Collections.emptyList(), new ArrayList<>())
+        ).addErrorMessage(
+                errorMessage
+        );
+    }
+
     public static boolean hasTopicsConfig(Map<String, String> props) {
         String topicsStr = props.get(TOPICS_CONFIG);
         return !Utils.isBlank(topicsStr);
     }
 
-    public static boolean hasTopicsRegexConfig(Map<String, String> props) {
-        String topicsRegexStr = props.get(TOPICS_REGEX_CONFIG);
-        return !Utils.isBlank(topicsRegexStr);
-    }
-
     public static boolean hasDlqTopicConfig(Map<String, String> props) {
         String dqlTopicStr = props.get(DLQ_TOPIC_NAME_CONFIG);
         return !Utils.isBlank(dqlTopicStr);
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java
index 5f3f40360e..94d19207cd 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java
@@ -110,6 +110,8 @@ import java.util.concurrent.atomic.AtomicLong;
 import java.util.concurrent.atomic.AtomicReference;
 import java.util.stream.Collectors;
 
+import static org.apache.kafka.clients.consumer.ConsumerConfig.GROUP_ID_CONFIG;
+import static org.apache.kafka.connect.runtime.ConnectorConfig.CONNECTOR_CLIENT_CONSUMER_OVERRIDES_PREFIX;
 import static org.apache.kafka.connect.runtime.WorkerConfig.TOPIC_TRACKING_ENABLE_CONFIG;
 import static org.apache.kafka.connect.runtime.distributed.ConnectProtocol.CONNECT_PROTOCOL_V0;
 import static org.apache.kafka.connect.runtime.distributed.ConnectProtocolCompatibility.EAGER;
@@ -906,7 +908,7 @@ public class DistributedHerder extends AbstractHerder implements Runnable {
     @Override
     protected Map<String, ConfigValue> validateSinkConnectorConfig(SinkConnector connector, ConfigDef configDef, Map<String, String> config) {
         Map<String, ConfigValue> result = super.validateSinkConnectorConfig(connector, configDef, config);
-        validateSinkConnectorGroupId(result);
+        validateSinkConnectorGroupId(config, result);
         return result;
     }
 
@@ -919,12 +921,25 @@ public class DistributedHerder extends AbstractHerder implements Runnable {
     }
 
 
-    private void validateSinkConnectorGroupId(Map<String, ConfigValue> validatedConfig) {
-        ConfigValue validatedName = validatedConfig.get(ConnectorConfig.NAME_CONFIG);
-        String name = (String) validatedName.value();
-        if (workerGroupId.equals(SinkUtils.consumerGroupId(name))) {
-            validatedName.addErrorMessage("Consumer group for sink connector named " + name +
-                    " conflicts with Connect worker group " + workerGroupId);
+    private void validateSinkConnectorGroupId(Map<String, String> config, Map<String, ConfigValue> validatedConfig) {
+        String overriddenConsumerGroupIdConfig = CONNECTOR_CLIENT_CONSUMER_OVERRIDES_PREFIX + GROUP_ID_CONFIG;
+        if (config.containsKey(overriddenConsumerGroupIdConfig)) {
+            String consumerGroupId = config.get(overriddenConsumerGroupIdConfig);
+            ConfigValue validatedGroupId = validatedConfig.computeIfAbsent(
+                    overriddenConsumerGroupIdConfig,
+                    p -> new ConfigValue(overriddenConsumerGroupIdConfig, consumerGroupId, Collections.emptyList(), new ArrayList<>())
+            );
+            if (workerGroupId.equals(consumerGroupId)) {
+                validatedGroupId.addErrorMessage("Consumer group " + consumerGroupId +
+                        " conflicts with Connect worker group " + workerGroupId);
+            }
+        } else {
+            ConfigValue validatedName = validatedConfig.get(ConnectorConfig.NAME_CONFIG);
+            String name = (String) validatedName.value();
+            if (workerGroupId.equals(SinkUtils.consumerGroupId(name))) {
+                validatedName.addErrorMessage("Consumer group for sink connector named " + name +
+                        " conflicts with Connect worker group " + workerGroupId);
+            }
         }
     }
 
diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/integration/ConnectorValidationIntegrationTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/integration/ConnectorValidationIntegrationTest.java
new file mode 100644
index 0000000000..88646112ff
--- /dev/null
+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/integration/ConnectorValidationIntegrationTest.java
@@ -0,0 +1,336 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.connect.integration;
+
+import org.apache.kafka.common.utils.Utils;
+import org.apache.kafka.connect.storage.StringConverter;
+import org.apache.kafka.connect.transforms.Filter;
+import org.apache.kafka.connect.transforms.predicates.RecordIsTombstone;
+import org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster;
+import org.apache.kafka.test.IntegrationTest;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.apache.kafka.clients.consumer.ConsumerConfig.GROUP_ID_CONFIG;
+import static org.apache.kafka.connect.integration.MonitorableSourceConnector.TOPIC_CONFIG;
+import static org.apache.kafka.connect.runtime.ConnectorConfig.CONNECTOR_CLASS_CONFIG;
+import static org.apache.kafka.connect.runtime.ConnectorConfig.CONNECTOR_CLIENT_CONSUMER_OVERRIDES_PREFIX;
+import static org.apache.kafka.connect.runtime.ConnectorConfig.HEADER_CONVERTER_CLASS_CONFIG;
+import static org.apache.kafka.connect.runtime.ConnectorConfig.KEY_CONVERTER_CLASS_CONFIG;
+import static org.apache.kafka.connect.runtime.ConnectorConfig.NAME_CONFIG;
+import static org.apache.kafka.connect.runtime.ConnectorConfig.PREDICATES_CONFIG;
+import static org.apache.kafka.connect.runtime.ConnectorConfig.TASKS_MAX_CONFIG;
+import static org.apache.kafka.connect.runtime.ConnectorConfig.TRANSFORMS_CONFIG;
+import static org.apache.kafka.connect.runtime.ConnectorConfig.VALUE_CONVERTER_CLASS_CONFIG;
+import static org.apache.kafka.connect.runtime.SinkConnectorConfig.DLQ_TOPIC_NAME_CONFIG;
+import static org.apache.kafka.connect.runtime.SinkConnectorConfig.TOPICS_CONFIG;
+import static org.apache.kafka.connect.runtime.SinkConnectorConfig.TOPICS_REGEX_CONFIG;
+import static org.apache.kafka.connect.runtime.SourceConnectorConfig.TOPIC_CREATION_GROUPS_CONFIG;
+
+/**
+ * Integration test for preflight connector config validation
+ */
+@Category(IntegrationTest.class)
+public class ConnectorValidationIntegrationTest {
+
+    private static final String WORKER_GROUP_ID = "connect-worker-group-id";
+
+    // Use a single embedded cluster for all test cases in order to cut down on runtime
+    private static EmbeddedConnectCluster connect;
+
+    @BeforeClass
+    public static void setup() {
+        Map<String, String> workerProps = new HashMap<>();
+        workerProps.put(GROUP_ID_CONFIG, WORKER_GROUP_ID);
+
+        // build a Connect cluster backed by Kafka and Zk
+        connect = new EmbeddedConnectCluster.Builder()
+                .name("connector-validation-connect-cluster")
+                .workerProps(workerProps)
+                .numBrokers(1)
+                .numWorkers(1)
+                .build();
+        connect.start();
+    }
+
+    @AfterClass
+    public static void close() {
+        if (connect != null) {
+            // stop all Connect, Kafka and Zk threads.
+            Utils.closeQuietly(connect::stop, "Embedded Connect cluster");
+        }
+    }
+
+    @Test
+    public void testSinkConnectorHasNeitherTopicsListNorTopicsRegex() throws InterruptedException {
+        Map<String, String> config = defaultSinkConnectorProps();
+        config.remove(TOPICS_CONFIG);
+        config.remove(TOPICS_REGEX_CONFIG);
+        connect.assertions().assertExactlyNumErrorsOnConnectorConfigValidation(
+                config.get(CONNECTOR_CLASS_CONFIG),
+                config,
+                2, // One error each for topics list and topics regex
+                "Sink connector config should fail preflight validation when neither topics list nor topics regex are provided",
+                0
+        );
+    }
+
+    @Test
+    public void testSinkConnectorHasBothTopicsListAndTopicsRegex() throws InterruptedException {
+        Map<String, String> config = defaultSinkConnectorProps();
+        config.put(TOPICS_CONFIG, "t1");
+        config.put(TOPICS_REGEX_CONFIG, "r.*");
+        connect.assertions().assertExactlyNumErrorsOnConnectorConfigValidation(
+                config.get(CONNECTOR_CLASS_CONFIG),
+                config,
+                2, // One error each for topics list and topics regex
+                "Sink connector config should fail preflight validation when both topics list and topics regex are provided",
+                0
+        );
+    }
+
+    @Test
+    public void testSinkConnectorDeadLetterQueueTopicInTopicsList() throws InterruptedException {
+        Map<String, String> config = defaultSinkConnectorProps();
+        config.put(TOPICS_CONFIG, "t1");
+        config.put(DLQ_TOPIC_NAME_CONFIG, "t1");
+        connect.assertions().assertExactlyNumErrorsOnConnectorConfigValidation(
+                config.get(CONNECTOR_CLASS_CONFIG),
+                config,
+                1,
+                "Sink connector config should fail preflight validation when DLQ topic is included in topics list",
+                0
+        );
+    }
+
+    @Test
+    public void testSinkConnectorDeadLetterQueueTopicMatchesTopicsRegex() throws InterruptedException {
+        Map<String, String> config = defaultSinkConnectorProps();
+        config.put(TOPICS_REGEX_CONFIG, "r.*");
+        config.put(DLQ_TOPIC_NAME_CONFIG, "ruh.roh");
+        config.remove(TOPICS_CONFIG);
+        connect.assertions().assertExactlyNumErrorsOnConnectorConfigValidation(
+                config.get(CONNECTOR_CLASS_CONFIG),
+                config,
+                1,
+                "Sink connector config should fail preflight validation when DLQ topic matches topics regex",
+                0
+        );
+    }
+
+    @Test
+    public void testSinkConnectorDefaultGroupIdConflictsWithWorkerGroupId() throws InterruptedException {
+        Map<String, String> config = defaultSinkConnectorProps();
+        // Combined with the logic in SinkUtils::consumerGroupId, this should conflict with the worker group ID
+        config.put(NAME_CONFIG, "worker-group-id");
+        connect.assertions().assertExactlyNumErrorsOnConnectorConfigValidation(
+                config.get(CONNECTOR_CLASS_CONFIG),
+                config,
+                1,
+                "Sink connector config should fail preflight validation when default consumer group ID conflicts with Connect worker group ID",
+                0
+        );
+    }
+
+    @Test
+    public void testSinkConnectorOverriddenGroupIdConflictsWithWorkerGroupId() throws InterruptedException {
+        Map<String, String> config = defaultSinkConnectorProps();
+        config.put(CONNECTOR_CLIENT_CONSUMER_OVERRIDES_PREFIX + GROUP_ID_CONFIG, WORKER_GROUP_ID);
+        connect.assertions().assertExactlyNumErrorsOnConnectorConfigValidation(
+                config.get(CONNECTOR_CLASS_CONFIG),
+                config,
+                1,
+                "Sink connector config should fail preflight validation when overridden consumer group ID conflicts with Connect worker group ID",
+                0
+        );
+    }
+
+    @Test
+    public void testSourceConnectorHasDuplicateTopicCreationGroups() throws InterruptedException {
+        Map<String, String> config = defaultSourceConnectorProps();
+        config.put(TOPIC_CREATION_GROUPS_CONFIG, "g1, g2, g1");
+        connect.assertions().assertExactlyNumErrorsOnConnectorConfigValidation(
+                config.get(CONNECTOR_CLASS_CONFIG),
+                config,
+                1,
+                "Source connector config should fail preflight validation when the same topic creation group is specified multiple times",
+                0
+        );
+    }
+
+    @Test
+    public void testConnectorHasDuplicateTransformations() throws InterruptedException {
+        Map<String, String> config = defaultSinkConnectorProps();
+        String transformName = "t";
+        config.put(TRANSFORMS_CONFIG, transformName + ", " + transformName);
+        config.put(TRANSFORMS_CONFIG + "." + transformName + ".type", Filter.class.getName());
+        connect.assertions().assertExactlyNumErrorsOnConnectorConfigValidation(
+                config.get(CONNECTOR_CLASS_CONFIG),
+                config,
+                1,
+                "Connector config should fail preflight validation when the same transformation is specified multiple times",
+                0
+        );
+    }
+
+    @Test
+    public void testConnectorHasMissingTransformClass() throws InterruptedException {
+        Map<String, String> config = defaultSinkConnectorProps();
+        String transformName = "t";
+        config.put(TRANSFORMS_CONFIG, transformName);
+        config.put(TRANSFORMS_CONFIG + "." + transformName + ".type", "WheresTheFruit");
+        connect.assertions().assertExactlyNumErrorsOnConnectorConfigValidation(
+                config.get(CONNECTOR_CLASS_CONFIG),
+                config,
+                1,
+                "Connector config should fail preflight validation when a transformation with a class not found on the worker is specified",
+                0
+        );
+    }
+
+    @Test
+    public void testConnectorHasInvalidTransformClass() throws InterruptedException {
+        Map<String, String> config = defaultSinkConnectorProps();
+        String transformName = "t";
+        config.put(TRANSFORMS_CONFIG, transformName);
+        config.put(TRANSFORMS_CONFIG + "." + transformName + ".type", MonitorableSinkConnector.class.getName());
+        connect.assertions().assertExactlyNumErrorsOnConnectorConfigValidation(
+                config.get(CONNECTOR_CLASS_CONFIG),
+                config,
+                1,
+                "Connector config should fail preflight validation when a transformation with a class of the wrong type is specified",
+                0
+        );
+    }
+
+    @Test
+    public void testConnectorHasNegationForUndefinedPredicate() throws InterruptedException {
+        Map<String, String> config = defaultSinkConnectorProps();
+        String transformName = "t";
+        config.put(TRANSFORMS_CONFIG, transformName);
+        config.put(TRANSFORMS_CONFIG + "." + transformName + ".type", Filter.class.getName());
+        config.put(TRANSFORMS_CONFIG + "." + transformName + ".negate", "true");
+        connect.assertions().assertExactlyNumErrorsOnConnectorConfigValidation(
+                config.get(CONNECTOR_CLASS_CONFIG),
+                config,
+                1,
+                "Connector config should fail preflight validation when an undefined predicate is negated",
+                0
+        );
+    }
+
+    @Test
+    public void testConnectorHasDuplicatePredicates() throws InterruptedException {
+        Map<String, String> config = defaultSinkConnectorProps();
+        String predicateName = "p";
+        config.put(PREDICATES_CONFIG, predicateName + ", " + predicateName);
+        config.put(PREDICATES_CONFIG + "." + predicateName + ".type", RecordIsTombstone.class.getName());
+        connect.assertions().assertExactlyNumErrorsOnConnectorConfigValidation(
+                config.get(CONNECTOR_CLASS_CONFIG),
+                config,
+                1,
+                "Connector config should fail preflight validation when the same predicate is specified multiple times",
+                0
+        );
+    }
+
+    @Test
+    public void testConnectorHasMissingPredicateClass() throws InterruptedException {
+        Map<String, String> config = defaultSinkConnectorProps();
+        String predicateName = "p";
+        config.put(PREDICATES_CONFIG, predicateName);
+        config.put(PREDICATES_CONFIG + "." + predicateName + ".type", "WheresTheFruit");
+        connect.assertions().assertExactlyNumErrorsOnConnectorConfigValidation(
+                config.get(CONNECTOR_CLASS_CONFIG),
+                config,
+                1,
+                "Connector config should fail preflight validation when a predicate with a class not found on the worker is specified",
+                0
+        );
+    }
+
+    @Test
+    public void testConnectorHasInvalidPredicateClass() throws InterruptedException {
+        Map<String, String> config = defaultSinkConnectorProps();
+        String predicateName = "p";
+        config.put(PREDICATES_CONFIG, predicateName);
+        config.put(PREDICATES_CONFIG + "." + predicateName + ".type", MonitorableSinkConnector.class.getName());
+        connect.assertions().assertExactlyNumErrorsOnConnectorConfigValidation(
+                config.get(CONNECTOR_CLASS_CONFIG),
+                config,
+                1,
+                "Connector config should fail preflight validation when a predicate with a class of the wrong type is specified",
+                0
+        );
+    }
+
+    @Test
+    public void testConnectorHasMissingConverterClass() throws InterruptedException {
+        Map<String, String> config = defaultSinkConnectorProps();
+        config.put(KEY_CONVERTER_CLASS_CONFIG, "WheresTheFruit");
+        connect.assertions().assertExactlyNumErrorsOnConnectorConfigValidation(
+                config.get(CONNECTOR_CLASS_CONFIG),
+                config,
+                1,
+                "Connector config should fail preflight validation when a converter with a class not found on the worker is specified",
+                0
+        );
+    }
+
+    @Test
+    public void testConnectorHasMissingHeaderConverterClass() throws InterruptedException {
+        Map<String, String> config = defaultSinkConnectorProps();
+        config.put(HEADER_CONVERTER_CLASS_CONFIG, "WheresTheFruit");
+        connect.assertions().assertExactlyNumErrorsOnConnectorConfigValidation(
+                config.get(CONNECTOR_CLASS_CONFIG),
+                config,
+                1,
+                "Connector config should fail preflight validation when a header converter with a class not found on the worker is specified",
+                0
+        );
+    }
+
+    private Map<String, String> defaultSourceConnectorProps() {
+        // setup up props for the source connector
+        Map<String, String> props = new HashMap<>();
+        props.put(NAME_CONFIG, "source-connector");
+        props.put(CONNECTOR_CLASS_CONFIG, MonitorableSourceConnector.class.getSimpleName());
+        props.put(TASKS_MAX_CONFIG, "1");
+        props.put(TOPIC_CONFIG, "t1");
+        props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());
+        props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());
+        return props;
+    }
+
+    private Map<String, String> defaultSinkConnectorProps() {
+        // setup up props for the sink connector
+        Map<String, String> props = new HashMap<>();
+        props.put(NAME_CONFIG, "sink-connector");
+        props.put(CONNECTOR_CLASS_CONFIG, MonitorableSinkConnector.class.getSimpleName());
+        props.put(TASKS_MAX_CONFIG, "1");
+        props.put(TOPICS_CONFIG, "t1");
+        props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());
+        props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());
+        return props;
+    }
+
+}
diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/AbstractHerderTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/AbstractHerderTest.java
index 68ee2ce156..6b06c43ce3 100644
--- a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/AbstractHerderTest.java
+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/AbstractHerderTest.java
@@ -19,7 +19,6 @@ package org.apache.kafka.connect.runtime;
 import org.apache.kafka.clients.CommonClientConfigs;
 import org.apache.kafka.clients.producer.ProducerConfig;
 import org.apache.kafka.common.config.ConfigDef;
-import org.apache.kafka.common.config.ConfigException;
 import org.apache.kafka.common.config.ConfigTransformer;
 import org.apache.kafka.common.config.ConfigValue;
 import org.apache.kafka.common.config.SaslConfigs;
@@ -517,7 +516,15 @@ public class AbstractHerderTest {
         config.put(SinkConnectorConfig.TOPICS_CONFIG, "topic1,topic2");
         config.put(SinkConnectorConfig.TOPICS_REGEX_CONFIG, "topic.*");
 
-        assertThrows(ConfigException.class, () -> herder.validateConnectorConfig(config, false));
+        ConfigInfos validation = herder.validateConnectorConfig(config, false);
+
+        ConfigInfo topicsListInfo = findInfo(validation, SinkConnectorConfig.TOPICS_CONFIG);
+        assertNotNull(topicsListInfo);
+        assertEquals(1, topicsListInfo.configValue().errors().size());
+
+        ConfigInfo topicsRegexInfo = findInfo(validation, SinkConnectorConfig.TOPICS_REGEX_CONFIG);
+        assertNotNull(topicsRegexInfo);
+        assertEquals(1, topicsRegexInfo.configValue().errors().size());
 
         verifyValidationIsolation();
     }
@@ -532,7 +539,11 @@ public class AbstractHerderTest {
         config.put(SinkConnectorConfig.TOPICS_CONFIG, "topic1");
         config.put(SinkConnectorConfig.DLQ_TOPIC_NAME_CONFIG, "topic1");
 
-        assertThrows(ConfigException.class, () -> herder.validateConnectorConfig(config, false));
+        ConfigInfos validation = herder.validateConnectorConfig(config, false);
+
+        ConfigInfo topicsListInfo = findInfo(validation, SinkConnectorConfig.TOPICS_CONFIG);
+        assertNotNull(topicsListInfo);
+        assertEquals(1, topicsListInfo.configValue().errors().size());
 
         verifyValidationIsolation();
     }
@@ -547,7 +558,11 @@ public class AbstractHerderTest {
         config.put(SinkConnectorConfig.TOPICS_REGEX_CONFIG, "topic.*");
         config.put(SinkConnectorConfig.DLQ_TOPIC_NAME_CONFIG, "topic1");
 
-        assertThrows(ConfigException.class, () -> herder.validateConnectorConfig(config, false));
+        ConfigInfos validation = herder.validateConnectorConfig(config, false);
+
+        ConfigInfo topicsRegexInfo = findInfo(validation, SinkConnectorConfig.TOPICS_REGEX_CONFIG);
+        assertNotNull(topicsRegexInfo);
+        assertEquals(1, topicsRegexInfo.configValue().errors().size());
 
         verifyValidationIsolation();
     }
@@ -587,7 +602,7 @@ public class AbstractHerderTest {
                 "Transforms: xformB"
         );
         assertEquals(expectedGroups, result.groups());
-        assertEquals(2, result.errorCount());
+        assertEquals(1, result.errorCount());
         Map<String, ConfigInfo> infos = result.values().stream()
                 .collect(Collectors.toMap(info -> info.configKey().name(), Function.identity()));
         assertEquals(26, infos.size());
@@ -644,7 +659,7 @@ public class AbstractHerderTest {
                 "Predicates: predY"
         );
         assertEquals(expectedGroups, result.groups());
-        assertEquals(2, result.errorCount());
+        assertEquals(1, result.errorCount());
         Map<String, ConfigInfo> infos = result.values().stream()
                 .collect(Collectors.toMap(info -> info.configKey().name(), Function.identity()));
         assertEquals(28, infos.size());
diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/DistributedHerderTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/DistributedHerderTest.java
index 2b774133d3..4c08ea2a44 100644
--- a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/DistributedHerderTest.java
+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/DistributedHerderTest.java
@@ -108,9 +108,11 @@ import static java.util.Collections.singletonList;
 import static javax.ws.rs.core.Response.Status.FORBIDDEN;
 import static javax.ws.rs.core.Response.Status.SERVICE_UNAVAILABLE;
 import static org.apache.kafka.connect.runtime.AbstractStatus.State.FAILED;
+import static org.apache.kafka.connect.runtime.ConnectorConfig.CONNECTOR_CLIENT_CONSUMER_OVERRIDES_PREFIX;
 import static org.apache.kafka.connect.runtime.SourceConnectorConfig.ExactlyOnceSupportLevel.REQUIRED;
 import static org.apache.kafka.connect.runtime.distributed.ConnectProtocol.CONNECT_PROTOCOL_V0;
 import static org.apache.kafka.connect.runtime.distributed.DistributedConfig.EXACTLY_ONCE_SOURCE_SUPPORT_CONFIG;
+import static org.apache.kafka.connect.runtime.distributed.DistributedConfig.GROUP_ID_CONFIG;
 import static org.apache.kafka.connect.runtime.distributed.DistributedConfig.INTER_WORKER_KEY_GENERATION_ALGORITHM_DEFAULT;
 import static org.apache.kafka.connect.runtime.distributed.IncrementalCooperativeConnectProtocol.CONNECT_PROTOCOL_V1;
 import static org.apache.kafka.connect.runtime.distributed.IncrementalCooperativeConnectProtocol.CONNECT_PROTOCOL_V2;
@@ -808,6 +810,31 @@ public class DistributedHerderTest {
                 nameConfig.errorMessages());
     }
 
+    @Test
+    public void testConnectorGroupIdConflictsWithWorkerGroupId() {
+        String overriddenGroupId = CONNECTOR_CLIENT_CONSUMER_OVERRIDES_PREFIX + GROUP_ID_CONFIG;
+        Map<String, String> config = new HashMap<>(CONN2_CONFIG);
+        config.put(overriddenGroupId, "connect-test-group");
+
+        SinkConnector connectorMock = mock(SinkConnector.class);
+
+        // CONN2 creation should fail because the worker group id (connect-test-group) conflicts with
+        // the consumer group id we would use for this sink
+        Map<String, ConfigValue> validatedConfigs = herder.validateSinkConnectorConfig(
+                connectorMock, SinkConnectorConfig.configDef(), config);
+
+        ConfigValue overriddenGroupIdConfig = validatedConfigs.get(overriddenGroupId);
+        assertEquals(
+                Collections.singletonList("Consumer group connect-test-group conflicts with Connect worker group connect-test-group"),
+                overriddenGroupIdConfig.errorMessages());
+
+        ConfigValue nameConfig = validatedConfigs.get(ConnectorConfig.NAME_CONFIG);
+        assertEquals(
+                Collections.emptyList(),
+                nameConfig.errorMessages()
+        );
+    }
+
     @Test
     public void testCreateConnectorAlreadyExists() throws Exception {
         when(member.memberId()).thenReturn("leader");
diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedConnectClusterAssertions.java b/connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedConnectClusterAssertions.java
index 49dd1dbb24..c4ff5018ed 100644
--- a/connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedConnectClusterAssertions.java
+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedConnectClusterAssertions.java
@@ -237,9 +237,27 @@ public class EmbeddedConnectClusterAssertions {
      * @param connectorClass the class of the connector to validate
      * @param connConfig     the intended configuration
      * @param numErrors      the number of errors expected
+     * @param detailMessage  the assertion message
      */
     public void assertExactlyNumErrorsOnConnectorConfigValidation(String connectorClass, Map<String, String> connConfig,
-        int numErrors, String detailMessage) throws InterruptedException {
+                                                                  int numErrors, String detailMessage) throws InterruptedException {
+        assertExactlyNumErrorsOnConnectorConfigValidation(connectorClass, connConfig, numErrors, detailMessage, VALIDATION_DURATION_MS);
+    }
+
+    /**
+     * Assert that the required number of errors are produced by a connector config validation.
+     *
+     * @param connectorClass the class of the connector to validate
+     * @param connConfig     the intended configuration
+     * @param numErrors      the number of errors expected
+     * @param detailMessage  the assertion message
+     * @param timeout        how long to retry for before throwing an exception
+     *
+     * @throws AssertionError if the exact number of errors is not produced during config
+     * validation before the timeout expires
+     */
+    public void assertExactlyNumErrorsOnConnectorConfigValidation(String connectorClass, Map<String, String> connConfig,
+        int numErrors, String detailMessage, long timeout) throws InterruptedException {
         try {
             waitForCondition(
                 () -> checkValidationErrors(
@@ -248,7 +266,7 @@ public class EmbeddedConnectClusterAssertions {
                     numErrors,
                     (actual, expected) -> actual == expected
                 ).orElse(false),
-                VALIDATION_DURATION_MS,
+                timeout,
                 "Didn't meet the exact requested number of validation errors: " + numErrors);
         } catch (AssertionError e) {
             throw new AssertionError(detailMessage, e);
