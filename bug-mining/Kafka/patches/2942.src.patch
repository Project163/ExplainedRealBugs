diff --git a/checkstyle/import-control-core.xml b/checkstyle/import-control-core.xml
index e5abccace0..7d6374c5b0 100644
--- a/checkstyle/import-control-core.xml
+++ b/checkstyle/import-control-core.xml
@@ -43,6 +43,19 @@
   <disallow class="com.yammer.metrics.Metrics" />
   <allow pkg="com.yammer.metrics"/>
 
+  <subpackage name="testkit">
+    <allow pkg="kafka.metrics"/>
+    <allow pkg="kafka.raft"/>
+    <allow pkg="kafka.server"/>
+    <allow pkg="kafka.tools"/>
+    <allow pkg="org.apache.kafka.clients"/>
+    <allow pkg="org.apache.kafka.controller"/>
+    <allow pkg="org.apache.kafka.raft"/>
+    <allow pkg="org.apache.kafka.test"/>
+    <allow pkg="org.apache.kafka.metadata" />
+    <allow pkg="org.apache.kafka.metalog" />
+  </subpackage>
+
   <subpackage name="tools">
     <allow pkg="org.apache.kafka.clients.admin" />
     <allow pkg="kafka.admin" />
@@ -73,6 +86,9 @@
     </subpackage>
     <subpackage name="junit">
       <allow pkg="kafka.test"/>
+      <allow pkg="kafka.testkit"/>
+      <allow pkg="org.apache.kafka.clients"/>
+      <allow pkg="org.apache.kafka.metadata" />
     </subpackage>
   </subpackage>
 </import-control>
diff --git a/core/src/main/scala/kafka/raft/RaftManager.scala b/core/src/main/scala/kafka/raft/RaftManager.scala
index 4a5837a4f2..db578d2fcd 100644
--- a/core/src/main/scala/kafka/raft/RaftManager.scala
+++ b/core/src/main/scala/kafka/raft/RaftManager.scala
@@ -110,7 +110,8 @@ class KafkaRaftManager[T](
   topicPartition: TopicPartition,
   time: Time,
   metrics: Metrics,
-  threadNamePrefixOpt: Option[String]
+  threadNamePrefixOpt: Option[String],
+  val controllerQuorumVotersFuture: CompletableFuture[util.Map[Integer, AddressSpec]]
 ) extends RaftManager[T] with Logging {
 
   private val raftConfig = new RaftConfig(config)
@@ -131,7 +132,7 @@ class KafkaRaftManager[T](
 
   def startup(): Unit = {
     // Update the voter endpoints (if valid) with what's in RaftConfig
-    val voterAddresses: util.Map[Integer, AddressSpec] = raftConfig.quorumVoterConnections
+    val voterAddresses: util.Map[Integer, AddressSpec] = controllerQuorumVotersFuture.get()
     for (voterAddressEntry <- voterAddresses.entrySet.asScala) {
       voterAddressEntry.getValue match {
         case spec: InetAddressSpec =>
diff --git a/core/src/main/scala/kafka/server/BrokerServer.scala b/core/src/main/scala/kafka/server/BrokerServer.scala
index 14b77fd7cf..321302be17 100644
--- a/core/src/main/scala/kafka/server/BrokerServer.scala
+++ b/core/src/main/scala/kafka/server/BrokerServer.scala
@@ -45,6 +45,7 @@ import org.apache.kafka.common.{ClusterResource, Endpoint, KafkaException}
 import org.apache.kafka.metadata.{BrokerState, VersionRange}
 import org.apache.kafka.metalog.MetaLogManager
 import org.apache.kafka.raft.RaftConfig
+import org.apache.kafka.raft.RaftConfig.AddressSpec
 import org.apache.kafka.server.authorizer.Authorizer
 
 import scala.collection.{Map, Seq}
@@ -61,7 +62,7 @@ class BrokerServer(
                     val metrics: Metrics,
                     val threadNamePrefix: Option[String],
                     val initialOfflineDirs: Seq[String],
-                    val controllerQuorumVotersFuture: CompletableFuture[util.List[String]],
+                    val controllerQuorumVotersFuture: CompletableFuture[util.Map[Integer, AddressSpec]],
                     val supportedFeatures: util.Map[String, VersionRange]
                   ) extends KafkaBroker {
 
@@ -178,7 +179,7 @@ class BrokerServer(
       tokenCache = new DelegationTokenCache(ScramMechanism.mechanismNames)
       credentialProvider = new CredentialProvider(ScramMechanism.mechanismNames, tokenCache)
 
-      val controllerNodes = RaftConfig.quorumVoterStringsToNodes(controllerQuorumVotersFuture.get()).asScala
+      val controllerNodes = RaftConfig.voterConnectionsToNodes(controllerQuorumVotersFuture.get()).asScala
       val controllerNodeProvider = RaftControllerNodeProvider(metaLogManager, config, controllerNodes)
 
       clientToControllerChannelManager = BrokerToControllerChannelManager(
diff --git a/core/src/main/scala/kafka/server/ControllerServer.scala b/core/src/main/scala/kafka/server/ControllerServer.scala
index f99cd6892d..b635449832 100644
--- a/core/src/main/scala/kafka/server/ControllerServer.scala
+++ b/core/src/main/scala/kafka/server/ControllerServer.scala
@@ -40,6 +40,7 @@ import org.apache.kafka.controller.{Controller, QuorumController, QuorumControll
 import org.apache.kafka.metadata.{ApiMessageAndVersion, VersionRange}
 import org.apache.kafka.metalog.MetaLogManager
 import org.apache.kafka.raft.RaftConfig
+import org.apache.kafka.raft.RaftConfig.AddressSpec
 import org.apache.kafka.server.authorizer.Authorizer
 
 import scala.jdk.CollectionConverters._
@@ -55,7 +56,7 @@ class ControllerServer(
                         val time: Time,
                         val metrics: Metrics,
                         val threadNamePrefix: Option[String],
-                        val controllerQuorumVotersFuture: CompletableFuture[util.List[String]]
+                        val controllerQuorumVotersFuture: CompletableFuture[util.Map[Integer, AddressSpec]]
                       ) extends Logging with KafkaMetricsGroup {
   import kafka.server.Server._
 
@@ -157,8 +158,7 @@ class ControllerServer(
 
 
       quotaManagers = QuotaFactory.instantiate(config, metrics, time, threadNamePrefix.getOrElse(""))
-      val controllerNodes =
-        RaftConfig.quorumVoterStringsToNodes(controllerQuorumVotersFuture.get()).asScala
+      val controllerNodes = RaftConfig.voterConnectionsToNodes(controllerQuorumVotersFuture.get()).asScala
       controllerApis = new ControllerApis(socketServer.dataPlaneRequestChannel,
         authorizer,
         quotaManagers,
diff --git a/core/src/main/scala/kafka/server/KafkaRaftServer.scala b/core/src/main/scala/kafka/server/KafkaRaftServer.scala
index 14e5d3a1a6..4aa499971f 100644
--- a/core/src/main/scala/kafka/server/KafkaRaftServer.scala
+++ b/core/src/main/scala/kafka/server/KafkaRaftServer.scala
@@ -28,6 +28,7 @@ import kafka.utils.{CoreUtils, Logging, Mx4jLoader, VerifiableProperties}
 import org.apache.kafka.common.TopicPartition
 import org.apache.kafka.common.utils.{AppInfoParser, Time}
 import org.apache.kafka.metadata.ApiMessageAndVersion
+import org.apache.kafka.raft.RaftConfig
 import org.apache.kafka.raft.metadata.{MetaLogRaftShim, MetadataRecordSerde}
 
 import scala.collection.Seq
@@ -58,7 +59,8 @@ class KafkaRaftServer(
     metaProps.clusterId.toString
   )
 
-  private val controllerQuorumVotersFuture = CompletableFuture.completedFuture(config.quorumVoters)
+  private val controllerQuorumVotersFuture = CompletableFuture.completedFuture(
+    RaftConfig.parseVoterConnections(config.quorumVoters))
 
   private val raftManager = new KafkaRaftManager[ApiMessageAndVersion](
     metaProps,
@@ -67,7 +69,8 @@ class KafkaRaftServer(
     KafkaRaftServer.MetadataPartition,
     time,
     metrics,
-    threadNamePrefix
+    threadNamePrefix,
+    controllerQuorumVotersFuture
   )
 
   private val metaLogShim = new MetaLogRaftShim(raftManager.kafkaRaftClient, config.nodeId)
diff --git a/core/src/main/scala/kafka/tools/TestRaftServer.scala b/core/src/main/scala/kafka/tools/TestRaftServer.scala
index c25e551ad9..e52a960168 100644
--- a/core/src/main/scala/kafka/tools/TestRaftServer.scala
+++ b/core/src/main/scala/kafka/tools/TestRaftServer.scala
@@ -18,8 +18,7 @@
 package kafka.tools
 
 import java.util.concurrent.atomic.{AtomicInteger, AtomicLong}
-import java.util.concurrent.{CountDownLatch, LinkedBlockingDeque, TimeUnit}
-
+import java.util.concurrent.{CompletableFuture, CountDownLatch, LinkedBlockingDeque, TimeUnit}
 import joptsimple.OptionException
 import kafka.network.SocketServer
 import kafka.raft.{KafkaRaftManager, RaftManager}
@@ -37,7 +36,7 @@ import org.apache.kafka.common.security.token.delegation.internals.DelegationTok
 import org.apache.kafka.common.utils.{Time, Utils}
 import org.apache.kafka.common.{TopicPartition, Uuid, protocol}
 import org.apache.kafka.raft.BatchReader.Batch
-import org.apache.kafka.raft.{BatchReader, RaftClient, RecordSerde}
+import org.apache.kafka.raft.{BatchReader, RaftClient, RaftConfig, RecordSerde}
 
 import scala.jdk.CollectionConverters._
 
@@ -85,7 +84,8 @@ class TestRaftServer(
       partition,
       time,
       metrics,
-      Some(threadNamePrefix)
+      Some(threadNamePrefix),
+      CompletableFuture.completedFuture(RaftConfig.parseVoterConnections(config.quorumVoters))
     )
 
     workloadGenerator = new RaftWorkloadGenerator(
diff --git a/core/src/test/java/kafka/test/ClusterInstance.java b/core/src/test/java/kafka/test/ClusterInstance.java
index 8732aa90ec..cac986fdc3 100644
--- a/core/src/test/java/kafka/test/ClusterInstance.java
+++ b/core/src/test/java/kafka/test/ClusterInstance.java
@@ -29,7 +29,7 @@ public interface ClusterInstance {
 
     enum ClusterType {
         ZK,
-        // RAFT
+        RAFT
     }
 
     /**
diff --git a/core/src/test/java/kafka/test/ClusterTestExtensionsTest.java b/core/src/test/java/kafka/test/ClusterTestExtensionsTest.java
index 6818e4332f..7d07d44d0a 100644
--- a/core/src/test/java/kafka/test/ClusterTestExtensionsTest.java
+++ b/core/src/test/java/kafka/test/ClusterTestExtensionsTest.java
@@ -86,20 +86,20 @@ public class ClusterTestExtensionsTest {
             @ClusterConfigProperty(key = "foo", value = "bar"),
             @ClusterConfigProperty(key = "spam", value = "eggs")
         }),
-        @ClusterTest(name = "cluster-tests-2", clusterType = Type.ZK, serverProperties = {
+        @ClusterTest(name = "cluster-tests-2", clusterType = Type.RAFT, serverProperties = {
             @ClusterConfigProperty(key = "foo", value = "baz"),
             @ClusterConfigProperty(key = "spam", value = "eggz")
         })
     })
     public void testClusterTests() {
-        if (clusterInstance.config().name().filter(name -> name.equals("cluster-tests-1")).isPresent()) {
+        if (clusterInstance.clusterType().equals(ClusterInstance.ClusterType.ZK)) {
             Assertions.assertEquals(clusterInstance.config().serverProperties().getProperty("foo"), "bar");
             Assertions.assertEquals(clusterInstance.config().serverProperties().getProperty("spam"), "eggs");
-        } else if (clusterInstance.config().name().filter(name -> name.equals("cluster-tests-2")).isPresent()) {
+        } else if (clusterInstance.clusterType().equals(ClusterInstance.ClusterType.RAFT)) {
             Assertions.assertEquals(clusterInstance.config().serverProperties().getProperty("foo"), "baz");
             Assertions.assertEquals(clusterInstance.config().serverProperties().getProperty("spam"), "eggz");
         } else {
-            Assertions.fail("Unknown cluster config " + clusterInstance.config().name());
+            Assertions.fail("Unknown cluster type " + clusterInstance.clusterType());
         }
     }
 
diff --git a/core/src/test/java/kafka/test/annotation/Type.java b/core/src/test/java/kafka/test/annotation/Type.java
index 8e8f23627c..7d2deb8570 100644
--- a/core/src/test/java/kafka/test/annotation/Type.java
+++ b/core/src/test/java/kafka/test/annotation/Type.java
@@ -17,12 +17,42 @@
 
 package kafka.test.annotation;
 
+import kafka.test.ClusterConfig;
+import kafka.test.junit.RaftClusterInvocationContext;
+import kafka.test.junit.ZkClusterInvocationContext;
+import org.junit.jupiter.api.extension.TestTemplateInvocationContext;
+
+import java.util.function.Consumer;
+
 /**
  * The type of cluster config being requested. Used by {@link kafka.test.ClusterConfig} and the test annotations.
  */
 public enum Type {
-    // RAFT,
-    ZK,
-    BOTH,
-    DEFAULT
+    RAFT {
+        @Override
+        public void invocationContexts(ClusterConfig config, Consumer<TestTemplateInvocationContext> invocationConsumer) {
+            invocationConsumer.accept(new RaftClusterInvocationContext(config.copyOf()));
+        }
+    },
+    ZK {
+        @Override
+        public void invocationContexts(ClusterConfig config, Consumer<TestTemplateInvocationContext> invocationConsumer) {
+            invocationConsumer.accept(new ZkClusterInvocationContext(config.copyOf()));
+        }
+    },
+    BOTH {
+        @Override
+        public void invocationContexts(ClusterConfig config, Consumer<TestTemplateInvocationContext> invocationConsumer) {
+            invocationConsumer.accept(new RaftClusterInvocationContext(config.copyOf()));
+            invocationConsumer.accept(new ZkClusterInvocationContext(config.copyOf()));
+        }
+    },
+    DEFAULT {
+        @Override
+        public void invocationContexts(ClusterConfig config, Consumer<TestTemplateInvocationContext> invocationConsumer) {
+            throw new UnsupportedOperationException("Cannot create invocation contexts for DEFAULT type");
+        }
+    };
+
+    public abstract void invocationContexts(ClusterConfig config, Consumer<TestTemplateInvocationContext> invocationConsumer);
 }
diff --git a/core/src/test/java/kafka/test/junit/ClusterTestExtensions.java b/core/src/test/java/kafka/test/junit/ClusterTestExtensions.java
index 872b669e21..ced1e6fcbd 100644
--- a/core/src/test/java/kafka/test/junit/ClusterTestExtensions.java
+++ b/core/src/test/java/kafka/test/junit/ClusterTestExtensions.java
@@ -128,13 +128,7 @@ public class ClusterTestExtensions implements TestTemplateInvocationContextProvi
             generatedClusterConfigs.add(ClusterConfig.defaultClusterBuilder().build());
         }
 
-        generatedClusterConfigs.forEach(config -> {
-            if (config.clusterType() == Type.ZK) {
-                testInvocations.accept(new ZkClusterInvocationContext(config.copyOf()));
-            } else {
-                throw new IllegalStateException("Unknown cluster type " + config.clusterType());
-            }
-        });
+        generatedClusterConfigs.forEach(config -> config.clusterType().invocationContexts(config, testInvocations));
     }
 
     private void generateClusterConfigurations(ExtensionContext context, String generateClustersMethods, ClusterGenerator generator) {
@@ -198,14 +192,9 @@ public class ClusterTestExtensions implements TestTemplateInvocationContextProvi
             properties.put(property.key(), property.value());
         }
 
-        switch (type) {
-            case ZK:
-            case BOTH:
-                ClusterConfig config = builder.build();
-                config.serverProperties().putAll(properties);
-                testInvocations.accept(new ZkClusterInvocationContext(config));
-                break;
-        }
+        ClusterConfig config = builder.build();
+        config.serverProperties().putAll(properties);
+        type.invocationContexts(config, testInvocations);
     }
 
     private ClusterTestDefaults getClusterTestDefaults(Class<?> testClass) {
diff --git a/core/src/test/java/kafka/test/junit/RaftClusterInvocationContext.java b/core/src/test/java/kafka/test/junit/RaftClusterInvocationContext.java
new file mode 100644
index 0000000000..aa3fa057b0
--- /dev/null
+++ b/core/src/test/java/kafka/test/junit/RaftClusterInvocationContext.java
@@ -0,0 +1,196 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package kafka.test.junit;
+
+import kafka.network.SocketServer;
+import kafka.server.BrokerServer;
+import kafka.server.ControllerServer;
+import kafka.test.ClusterConfig;
+import kafka.test.ClusterInstance;
+import kafka.testkit.KafkaClusterTestKit;
+import kafka.testkit.TestKitNodes;
+import org.apache.kafka.clients.CommonClientConfigs;
+import org.apache.kafka.clients.admin.Admin;
+import org.apache.kafka.common.network.ListenerName;
+import org.apache.kafka.metadata.BrokerState;
+import org.junit.jupiter.api.extension.AfterTestExecutionCallback;
+import org.junit.jupiter.api.extension.BeforeTestExecutionCallback;
+import org.junit.jupiter.api.extension.Extension;
+import org.junit.jupiter.api.extension.TestTemplateInvocationContext;
+
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.List;
+import java.util.Properties;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.atomic.AtomicReference;
+import java.util.stream.Collectors;
+
+/**
+ * Wraps a {@link KafkaClusterTestKit} inside lifecycle methods for a test invocation. Each instance of this
+ * class is provided with a configuration for the cluster.
+ *
+ * This context also provides parameter resolvers for:
+ *
+ * <ul>
+ *     <li>ClusterConfig (the same instance passed to the constructor)</li>
+ *     <li>ClusterInstance (includes methods to expose underlying SocketServer-s)</li>
+ *     <li>IntegrationTestHelper (helper methods)</li>
+ * </ul>
+ */
+public class RaftClusterInvocationContext implements TestTemplateInvocationContext {
+
+    private final ClusterConfig clusterConfig;
+    private final AtomicReference<KafkaClusterTestKit> clusterReference;
+
+    public RaftClusterInvocationContext(ClusterConfig clusterConfig) {
+        this.clusterConfig = clusterConfig;
+        this.clusterReference = new AtomicReference<>();
+    }
+
+    @Override
+    public String getDisplayName(int invocationIndex) {
+        String clusterDesc = clusterConfig.nameTags().entrySet().stream()
+            .map(Object::toString)
+            .collect(Collectors.joining(", "));
+        return String.format("[Quorum %d] %s", invocationIndex, clusterDesc);
+    }
+
+    @Override
+    public List<Extension> getAdditionalExtensions() {
+        return Arrays.asList(
+            (BeforeTestExecutionCallback) context -> {
+                KafkaClusterTestKit.Builder builder = new KafkaClusterTestKit.Builder(
+                    new TestKitNodes.Builder().
+                        setNumKip500BrokerNodes(clusterConfig.numBrokers()).
+                        setNumControllerNodes(clusterConfig.numControllers()).build());
+
+                // Copy properties into the TestKit builder
+                clusterConfig.serverProperties().forEach((key, value) -> builder.setConfigProp(key.toString(), value.toString()));
+                // KAFKA-12512 need to pass security protocol and listener name here
+                KafkaClusterTestKit cluster = builder.build();
+                clusterReference.set(cluster);
+                cluster.format();
+                cluster.startup();
+                kafka.utils.TestUtils.waitUntilTrue(
+                    () -> cluster.brokers().get(0).currentState() == BrokerState.RUNNING,
+                    () -> "Broker never made it to RUNNING state.",
+                    org.apache.kafka.test.TestUtils.DEFAULT_MAX_WAIT_MS,
+                    100L);
+            },
+            (AfterTestExecutionCallback) context -> clusterReference.get().close(),
+            new ClusterInstanceParameterResolver(new RaftClusterInstance(clusterReference, clusterConfig)),
+            new GenericParameterResolver<>(clusterConfig, ClusterConfig.class)
+        );
+    }
+
+    public static class RaftClusterInstance implements ClusterInstance {
+
+        private final AtomicReference<KafkaClusterTestKit> clusterReference;
+        private final ClusterConfig clusterConfig;
+        final AtomicBoolean started = new AtomicBoolean(false);
+        final AtomicBoolean stopped = new AtomicBoolean(false);
+
+        RaftClusterInstance(AtomicReference<KafkaClusterTestKit> clusterReference, ClusterConfig clusterConfig) {
+            this.clusterReference = clusterReference;
+            this.clusterConfig = clusterConfig;
+        }
+
+        @Override
+        public String bootstrapServers() {
+            return clusterReference.get().clientProperties().getProperty(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG);
+        }
+
+        @Override
+        public Collection<SocketServer> brokerSocketServers() {
+            return clusterReference.get().brokers().values().stream()
+                .map(BrokerServer::socketServer)
+                .collect(Collectors.toList());
+        }
+
+        @Override
+        public ListenerName clientListener() {
+            return ListenerName.normalised("EXTERNAL");
+        }
+
+        @Override
+        public Collection<SocketServer> controllerSocketServers() {
+            return clusterReference.get().controllers().values().stream()
+                .map(ControllerServer::socketServer)
+                .collect(Collectors.toList());
+        }
+
+        @Override
+        public SocketServer anyBrokerSocketServer() {
+            return clusterReference.get().brokers().values().stream()
+                .map(BrokerServer::socketServer)
+                .findFirst()
+                .orElseThrow(() -> new RuntimeException("No broker SocketServers found"));
+        }
+
+        @Override
+        public SocketServer anyControllerSocketServer() {
+            return clusterReference.get().controllers().values().stream()
+                .map(ControllerServer::socketServer)
+                .findFirst()
+                .orElseThrow(() -> new RuntimeException("No controller SocketServers found"));
+        }
+
+        @Override
+        public ClusterType clusterType() {
+            return ClusterType.RAFT;
+        }
+
+        @Override
+        public ClusterConfig config() {
+            return clusterConfig;
+        }
+
+        @Override
+        public KafkaClusterTestKit getUnderlying() {
+            return clusterReference.get();
+        }
+
+        @Override
+        public Admin createAdminClient(Properties configOverrides) {
+            return Admin.create(clusterReference.get().clientProperties());
+        }
+
+        @Override
+        public void start() {
+            if (started.compareAndSet(false, true)) {
+                try {
+                    clusterReference.get().startup();
+                } catch (Exception e) {
+                    throw new RuntimeException("Failed to start Raft server", e);
+                }
+            }
+        }
+
+        @Override
+        public void stop() {
+            if (stopped.compareAndSet(false, true)) {
+                try {
+                    clusterReference.get().close();
+                } catch (Exception e) {
+                    throw new RuntimeException("Failed to stop Raft server", e);
+                }
+            }
+        }
+    }
+}
\ No newline at end of file
diff --git a/core/src/test/java/kafka/testkit/BrokerNode.java b/core/src/test/java/kafka/testkit/BrokerNode.java
new file mode 100644
index 0000000000..0b5859404b
--- /dev/null
+++ b/core/src/test/java/kafka/testkit/BrokerNode.java
@@ -0,0 +1,99 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package kafka.testkit;
+
+import org.apache.kafka.common.Uuid;
+
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.List;
+
+public class BrokerNode implements TestKitNode {
+    public static class Builder {
+        private int id = -1;
+        private Uuid incarnationId = null;
+        private String metadataDirectory = null;
+        private List<String> logDataDirectories = null;
+
+        public Builder setId(int id) {
+            this.id = id;
+            return this;
+        }
+
+        public Builder setLogDirectories(List<String> logDataDirectories) {
+            this.logDataDirectories = logDataDirectories;
+            return this;
+        }
+
+        public Builder setMetadataDirectory(String metadataDirectory) {
+            this.metadataDirectory = metadataDirectory;
+            return this;
+        }
+
+        public BrokerNode build() {
+            if (id == -1) {
+                throw new RuntimeException("You must set the node id");
+            }
+            if (incarnationId == null) {
+                incarnationId = Uuid.randomUuid();
+            }
+            if (logDataDirectories == null) {
+                logDataDirectories = Collections.
+                    singletonList(String.format("broker_%d_data0", id));
+            }
+            if (metadataDirectory == null) {
+                metadataDirectory = logDataDirectories.get(0);
+            }
+            return new BrokerNode(id, incarnationId, metadataDirectory,
+                logDataDirectories);
+        }
+    }
+
+    private final int id;
+    private final Uuid incarnationId;
+    private final String metadataDirectory;
+    private final List<String> logDataDirectories;
+
+    BrokerNode(int id,
+               Uuid incarnationId,
+               String metadataDirectory,
+               List<String> logDataDirectories) {
+        this.id = id;
+        this.incarnationId = incarnationId;
+        this.metadataDirectory = metadataDirectory;
+        this.logDataDirectories = new ArrayList<>(logDataDirectories);
+    }
+
+    @Override
+    public int id() {
+        return id;
+    }
+
+    public Uuid incarnationId() {
+        return incarnationId;
+    }
+
+    @Override
+    public String metadataDirectory() {
+        return metadataDirectory;
+    }
+
+    public List<String> logDataDirectories() {
+        return logDataDirectories;
+    }
+}
diff --git a/core/src/test/java/kafka/testkit/ControllerNode.java b/core/src/test/java/kafka/testkit/ControllerNode.java
new file mode 100644
index 0000000000..be6c8067f1
--- /dev/null
+++ b/core/src/test/java/kafka/testkit/ControllerNode.java
@@ -0,0 +1,63 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package kafka.testkit;
+
+public class ControllerNode implements TestKitNode {
+    public static class Builder {
+        private int id = -1;
+        private String metadataDirectory = null;
+
+        public Builder setId(int id) {
+            this.id = id;
+            return this;
+        }
+
+        public Builder setMetadataDirectory() {
+            this.metadataDirectory = metadataDirectory;
+            return this;
+        }
+
+        public ControllerNode build() {
+            if (id == -1) {
+                throw new RuntimeException("You must set the node id");
+            }
+            if (metadataDirectory == null) {
+                metadataDirectory = String.format("controller_%d", id);
+            }
+            return new ControllerNode(id, metadataDirectory);
+        }
+    }
+
+    private final int id;
+    private final String metadataDirectory;
+
+    ControllerNode(int id, String metadataDirectory) {
+        this.id = id;
+        this.metadataDirectory = metadataDirectory;
+    }
+
+    @Override
+    public int id() {
+        return id;
+    }
+
+    @Override
+    public String metadataDirectory() {
+        return metadataDirectory;
+    }
+}
diff --git a/core/src/test/java/kafka/testkit/KafkaClusterTestKit.java b/core/src/test/java/kafka/testkit/KafkaClusterTestKit.java
new file mode 100644
index 0000000000..6e3cebb525
--- /dev/null
+++ b/core/src/test/java/kafka/testkit/KafkaClusterTestKit.java
@@ -0,0 +1,494 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package kafka.testkit;
+
+import kafka.raft.KafkaRaftManager;
+import kafka.server.BrokerServer;
+import kafka.server.ControllerServer;
+import kafka.server.KafkaConfig;
+import kafka.server.KafkaConfig$;
+import kafka.server.KafkaRaftServer;
+import kafka.server.MetaProperties;
+import kafka.server.Server;
+import kafka.tools.StorageTool;
+import kafka.utils.Logging;
+import org.apache.kafka.clients.CommonClientConfigs;
+import org.apache.kafka.common.Node;
+import org.apache.kafka.common.TopicPartition;
+import org.apache.kafka.common.metrics.Metrics;
+import org.apache.kafka.common.network.ListenerName;
+import org.apache.kafka.common.utils.ThreadUtils;
+import org.apache.kafka.common.utils.Time;
+import org.apache.kafka.common.utils.Utils;
+import org.apache.kafka.controller.Controller;
+import org.apache.kafka.metadata.ApiMessageAndVersion;
+import org.apache.kafka.metalog.MetaLogManager;
+import org.apache.kafka.raft.RaftConfig;
+import org.apache.kafka.raft.metadata.MetaLogRaftShim;
+import org.apache.kafka.raft.metadata.MetadataRecordSerde;
+import org.apache.kafka.test.TestUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import scala.Option;
+import scala.collection.JavaConverters;
+
+import java.io.ByteArrayOutputStream;
+import java.io.File;
+import java.io.IOException;
+import java.io.PrintStream;
+import java.net.InetSocketAddress;
+import java.nio.file.Files;
+import java.nio.file.Paths;
+import java.util.AbstractMap.SimpleImmutableEntry;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Properties;
+import java.util.TreeMap;
+import java.util.concurrent.CompletableFuture;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.Future;
+import java.util.concurrent.TimeUnit;
+import java.util.function.Consumer;
+import java.util.stream.Collectors;
+
+
+@SuppressWarnings("deprecation") // Needed for Scala 2.12 compatibility
+public class KafkaClusterTestKit implements AutoCloseable {
+    private final static Logger log = LoggerFactory.getLogger(KafkaClusterTestKit.class);
+
+    /**
+     * This class manages a future which is completed with the proper value for
+     * controller.quorum.voters once the randomly assigned ports for all the controllers are
+     * known.
+     */
+    private static class ControllerQuorumVotersFutureManager implements AutoCloseable {
+        private final int expectedControllers;
+        private final CompletableFuture<Map<Integer, RaftConfig.AddressSpec>> future = new CompletableFuture<>();
+        private final Map<Integer, Integer> controllerPorts = new TreeMap<>();
+
+        ControllerQuorumVotersFutureManager(int expectedControllers) {
+            this.expectedControllers = expectedControllers;
+        }
+
+        synchronized void registerPort(int nodeId, int port) {
+            controllerPorts.put(nodeId, port);
+            if (controllerPorts.size() >= expectedControllers) {
+                future.complete(controllerPorts.entrySet().stream().
+                    collect(Collectors.toMap(
+                        Map.Entry::getKey,
+                        entry -> new RaftConfig.InetAddressSpec(new InetSocketAddress("localhost", entry.getValue()))
+                    )));
+            }
+        }
+
+        void fail(Throwable e) {
+            future.completeExceptionally(e);
+        }
+
+        @Override
+        public void close() {
+            future.cancel(true);
+        }
+    }
+
+    public static class Builder {
+        private TestKitNodes nodes;
+        private Map<String, String> configProps = new HashMap<>();
+
+        public Builder(TestKitNodes nodes) {
+            this.nodes = nodes;
+        }
+
+        public Builder setConfigProp(String key, String value) {
+            this.configProps.put(key, value);
+            return this;
+        }
+
+        public KafkaClusterTestKit build() throws Exception {
+            Map<Integer, ControllerServer> controllers = new HashMap<>();
+            Map<Integer, BrokerServer> brokers = new HashMap<>();
+            Map<Integer, KafkaRaftManager> raftManagers = new HashMap<>();
+            String uninitializedQuorumVotersString = nodes.controllerNodes().keySet().stream().
+                map(controllerNode -> String.format("%d@0.0.0.0:0", controllerNode)).
+                collect(Collectors.joining(","));
+            /*
+              Number of threads = Total number of brokers + Total number of controllers + Total number of Raft Managers
+                                = Total number of brokers + Total number of controllers * 2
+                                  (Raft Manager per broker/controller)
+             */
+            int numOfExecutorThreads = (nodes.brokerNodes().size() + nodes.controllerNodes().size()) * 2;
+            ExecutorService executorService = null;
+            ControllerQuorumVotersFutureManager connectFutureManager =
+                new ControllerQuorumVotersFutureManager(nodes.controllerNodes().size());
+            File baseDirectory = null;
+
+            try {
+                baseDirectory = TestUtils.tempDirectory();
+                nodes = nodes.copyWithAbsolutePaths(baseDirectory.getAbsolutePath());
+                executorService = Executors.newFixedThreadPool(numOfExecutorThreads,
+                    ThreadUtils.createThreadFactory("KafkaClusterTestKit%d", false));
+                for (ControllerNode node : nodes.controllerNodes().values()) {
+                    Map<String, String> props = new HashMap<>(configProps);
+                    props.put(KafkaConfig$.MODULE$.ProcessRolesProp(), "controller");
+                    props.put(KafkaConfig$.MODULE$.NodeIdProp(),
+                        Integer.toString(node.id()));
+                    props.put(KafkaConfig$.MODULE$.MetadataLogDirProp(),
+                        node.metadataDirectory());
+                    props.put(KafkaConfig$.MODULE$.ListenerSecurityProtocolMapProp(),
+                        "CONTROLLER:PLAINTEXT");
+                    props.put(KafkaConfig$.MODULE$.ListenersProp(),
+                        "CONTROLLER://localhost:0");
+                    props.put(KafkaConfig$.MODULE$.ControllerListenerNamesProp(),
+                        "CONTROLLER");
+                    // Note: we can't accurately set controller.quorum.voters yet, since we don't
+                    // yet know what ports each controller will pick.  Set it to a dummy string \
+                    // for now as a placeholder.
+                    props.put(RaftConfig.QUORUM_VOTERS_CONFIG, uninitializedQuorumVotersString);
+                    setupNodeDirectories(baseDirectory, node.metadataDirectory(), Collections.emptyList());
+                    KafkaConfig config = new KafkaConfig(props, false, Option.empty());
+
+                    String threadNamePrefix = String.format("controller%d_", node.id());
+                    MetaProperties metaProperties = MetaProperties.apply(nodes.clusterId().toString(), node.id());
+                    TopicPartition metadataPartition = new TopicPartition(KafkaRaftServer.MetadataTopic(), 0);
+                    KafkaRaftManager<ApiMessageAndVersion> raftManager = new KafkaRaftManager<>(
+                        metaProperties, config, new MetadataRecordSerde(), metadataPartition,
+                        Time.SYSTEM, new Metrics(), Option.apply(threadNamePrefix), connectFutureManager.future);
+                    MetaLogManager metaLogShim = new MetaLogRaftShim(raftManager.kafkaRaftClient(), config.nodeId());
+                    ControllerServer controller = new ControllerServer(
+                        nodes.controllerProperties(node.id()),
+                        config,
+                        metaLogShim,
+                        raftManager,
+                        Time.SYSTEM,
+                        new Metrics(),
+                        Option.apply(threadNamePrefix),
+                        connectFutureManager.future
+                    );
+                    controllers.put(node.id(), controller);
+                    controller.socketServerFirstBoundPortFuture().whenComplete((port, e) -> {
+                        if (e != null) {
+                            connectFutureManager.fail(e);
+                        } else {
+                            connectFutureManager.registerPort(node.id(), port);
+                        }
+                    });
+                    raftManagers.put(node.id(), raftManager);
+                }
+                for (BrokerNode node : nodes.brokerNodes().values()) {
+                    Map<String, String> props = new HashMap<>(configProps);
+                    props.put(KafkaConfig$.MODULE$.ProcessRolesProp(), "broker");
+                    props.put(KafkaConfig$.MODULE$.BrokerIdProp(),
+                        Integer.toString(node.id()));
+                    props.put(KafkaConfig$.MODULE$.MetadataLogDirProp(),
+                        node.metadataDirectory());
+                    props.put(KafkaConfig$.MODULE$.LogDirsProp(),
+                        String.join(",", node.logDataDirectories()));
+                    props.put(KafkaConfig$.MODULE$.ListenerSecurityProtocolMapProp(),
+                        "EXTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT");
+                    props.put(KafkaConfig$.MODULE$.ListenersProp(),
+                        "EXTERNAL://localhost:0");
+                    props.put(KafkaConfig$.MODULE$.InterBrokerListenerNameProp(),
+                        nodes.interBrokerListenerName().value());
+                    props.put(KafkaConfig$.MODULE$.ControllerListenerNamesProp(),
+                        "CONTROLLER");
+
+                    setupNodeDirectories(baseDirectory, node.metadataDirectory(),
+                        node.logDataDirectories());
+
+                    // Just like above, we set a placeholder voter list here until we
+                    // find out what ports the controllers picked.
+                    props.put(RaftConfig.QUORUM_VOTERS_CONFIG, uninitializedQuorumVotersString);
+                    KafkaConfig config = new KafkaConfig(props, false, Option.empty());
+
+                    String threadNamePrefix = String.format("broker%d_", node.id());
+                    MetaProperties metaProperties = MetaProperties.apply(nodes.clusterId().toString(), node.id());
+                    TopicPartition metadataPartition = new TopicPartition(KafkaRaftServer.MetadataTopic(), 0);
+                    KafkaRaftManager<ApiMessageAndVersion> raftManager = new KafkaRaftManager<>(
+                            metaProperties, config, new MetadataRecordSerde(), metadataPartition,
+                            Time.SYSTEM, new Metrics(), Option.apply(threadNamePrefix), connectFutureManager.future);
+                    MetaLogManager metaLogShim = new MetaLogRaftShim(raftManager.kafkaRaftClient(), config.nodeId());
+                    BrokerServer broker = new BrokerServer(
+                        config,
+                        nodes.brokerProperties(node.id()),
+                        metaLogShim,
+                        Time.SYSTEM,
+                        new Metrics(),
+                        Option.apply(threadNamePrefix),
+                        JavaConverters.asScalaBuffer(Collections.<String>emptyList()).toSeq(),
+                        connectFutureManager.future,
+                        Server.SUPPORTED_FEATURES()
+                    );
+                    brokers.put(node.id(), broker);
+                    raftManagers.put(node.id(), raftManager);
+                }
+            } catch (Exception e) {
+                if (executorService != null) {
+                    executorService.shutdownNow();
+                    executorService.awaitTermination(5, TimeUnit.MINUTES);
+                }
+                for (ControllerServer controller : controllers.values()) {
+                    controller.shutdown();
+                }
+                for (BrokerServer brokerServer : brokers.values()) {
+                    brokerServer.shutdown();
+                }
+                for (KafkaRaftManager raftManager : raftManagers.values()) {
+                    raftManager.shutdown();
+                }
+                connectFutureManager.close();
+                if (baseDirectory != null) {
+                    Utils.delete(baseDirectory);
+                }
+                throw e;
+            }
+            return new KafkaClusterTestKit(executorService, nodes, controllers,
+                brokers, raftManagers, connectFutureManager, baseDirectory);
+        }
+
+        static private void setupNodeDirectories(File baseDirectory,
+                                                 String metadataDirectory,
+                                                 Collection<String> logDataDirectories) throws Exception {
+            Files.createDirectories(new File(baseDirectory, "local").toPath());
+            Files.createDirectories(Paths.get(metadataDirectory));
+            for (String logDataDirectory : logDataDirectories) {
+                Files.createDirectories(Paths.get(logDataDirectory));
+            }
+        }
+    }
+
+    private final ExecutorService executorService;
+    private final TestKitNodes nodes;
+    private final Map<Integer, ControllerServer> controllers;
+    private final Map<Integer, BrokerServer> brokers;
+    private final Map<Integer, KafkaRaftManager> raftManagers;
+    private final ControllerQuorumVotersFutureManager controllerQuorumVotersFutureManager;
+    private final File baseDirectory;
+
+    private KafkaClusterTestKit(ExecutorService executorService,
+                                TestKitNodes nodes,
+                                Map<Integer, ControllerServer> controllers,
+                                Map<Integer, BrokerServer> brokers,
+                                Map<Integer, KafkaRaftManager> raftManagers,
+                                ControllerQuorumVotersFutureManager controllerQuorumVotersFutureManager,
+                                File baseDirectory) {
+        this.executorService = executorService;
+        this.nodes = nodes;
+        this.controllers = controllers;
+        this.brokers = brokers;
+        this.raftManagers = raftManagers;
+        this.controllerQuorumVotersFutureManager = controllerQuorumVotersFutureManager;
+        this.baseDirectory = baseDirectory;
+    }
+
+    public void format() throws Exception {
+        List<Future<?>> futures = new ArrayList<>();
+        try {
+            for (Entry<Integer, ControllerServer> entry : controllers.entrySet()) {
+                int nodeId = entry.getKey();
+                ControllerServer controller = entry.getValue();
+                formatNodeAndLog(nodes.controllerProperties(nodeId), controller.config().metadataLogDir(),
+                    controller, futures::add);
+            }
+            for (Entry<Integer, BrokerServer> entry : brokers.entrySet()) {
+                int nodeId = entry.getKey();
+                BrokerServer broker = entry.getValue();
+                formatNodeAndLog(nodes.brokerProperties(nodeId), broker.config().metadataLogDir(),
+                    broker, futures::add);
+            }
+            for (Future<?> future: futures) {
+                future.get();
+            }
+        } catch (Exception e) {
+            for (Future<?> future: futures) {
+                future.cancel(true);
+            }
+            throw e;
+        }
+    }
+
+    private void formatNodeAndLog(MetaProperties properties, String metadataLogDir, Logging loggingMixin,
+                                  Consumer<Future<?>> futureConsumer) {
+        futureConsumer.accept(executorService.submit(() -> {
+            try (ByteArrayOutputStream stream = new ByteArrayOutputStream()) {
+                try (PrintStream out = new PrintStream(stream)) {
+                    StorageTool.formatCommand(out,
+                            JavaConverters.asScalaBuffer(Collections.singletonList(metadataLogDir)).toSeq(),
+                            properties,
+                            false);
+                } finally {
+                    for (String line : stream.toString().split(String.format("%n"))) {
+                        loggingMixin.info(() -> line);
+                    }
+                }
+            } catch (IOException e) {
+                throw new RuntimeException(e);
+            }
+        }));
+    }
+
+    public void startup() throws ExecutionException, InterruptedException {
+        List<Future<?>> futures = new ArrayList<>();
+        try {
+            for (ControllerServer controller : controllers.values()) {
+                futures.add(executorService.submit(controller::startup));
+            }
+            for (KafkaRaftManager raftManager : raftManagers.values()) {
+                futures.add(controllerQuorumVotersFutureManager.future.thenRunAsync(raftManager::startup));
+            }
+            for (BrokerServer broker : brokers.values()) {
+                futures.add(executorService.submit(broker::startup));
+            }
+            for (Future<?> future: futures) {
+                future.get();
+            }
+        } catch (Exception e) {
+            for (Future<?> future: futures) {
+                future.cancel(true);
+            }
+            throw e;
+        }
+    }
+
+    /**
+     * Wait for a controller to mark all the brokers as ready (registered and unfenced).
+     */
+    public void waitForReadyBrokers() throws ExecutionException, InterruptedException {
+        // We can choose any controller, not just the active controller.
+        // If we choose a standby controller, we will wait slightly longer.
+        ControllerServer controllerServer = controllers.values().iterator().next();
+        Controller controller = controllerServer.controller();
+        controller.waitForReadyBrokers(brokers.size()).get();
+    }
+
+    public Properties controllerClientProperties() throws ExecutionException, InterruptedException {
+        Properties properties = new Properties();
+        if (!controllers.isEmpty()) {
+            Collection<Node> controllerNodes = RaftConfig.voterConnectionsToNodes(
+                controllerQuorumVotersFutureManager.future.get());
+
+            StringBuilder bld = new StringBuilder();
+            String prefix = "";
+            for (Node node : controllerNodes) {
+                bld.append(prefix).append(node.id()).append('@');
+                bld.append(node.host()).append(":").append(node.port());
+                prefix = ",";
+            }
+            properties.setProperty(RaftConfig.QUORUM_VOTERS_CONFIG, bld.toString());
+            properties.setProperty(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG,
+                controllerNodes.stream().map(n -> n.host() + ":" + n.port()).
+                    collect(Collectors.joining(",")));
+        }
+        return properties;
+    }
+
+    public Properties clientProperties() {
+        Properties properties = new Properties();
+        if (!brokers.isEmpty()) {
+            StringBuilder bld = new StringBuilder();
+            String prefix = "";
+            for (Entry<Integer, BrokerServer> entry : brokers.entrySet()) {
+                int brokerId = entry.getKey();
+                BrokerServer broker = entry.getValue();
+                ListenerName listenerName = nodes.externalListenerName();
+                int port = broker.boundPort(listenerName);
+                if (port <= 0) {
+                    throw new RuntimeException("Broker " + brokerId + " does not yet " +
+                        "have a bound port for " + listenerName + ".  Did you start " +
+                        "the cluster yet?");
+                }
+                bld.append(prefix).append("localhost:").append(port);
+                prefix = ",";
+            }
+            properties.setProperty(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, bld.toString());
+        }
+        return properties;
+    }
+
+    public Map<Integer, ControllerServer> controllers() {
+        return controllers;
+    }
+
+    public Map<Integer, BrokerServer> brokers() {
+        return brokers;
+    }
+
+    public Map<Integer, KafkaRaftManager> raftManagers() {
+        return raftManagers;
+    }
+
+    public TestKitNodes nodes() {
+        return nodes;
+    }
+
+    @Override
+    public void close() throws Exception {
+        List<Entry<String, Future<?>>> futureEntries = new ArrayList<>();
+        try {
+            controllerQuorumVotersFutureManager.close();
+            for (Entry<Integer, BrokerServer> entry : brokers.entrySet()) {
+                int brokerId = entry.getKey();
+                BrokerServer broker = entry.getValue();
+                futureEntries.add(new SimpleImmutableEntry<>("broker" + brokerId,
+                    executorService.submit(broker::shutdown)));
+            }
+            waitForAllFutures(futureEntries);
+            futureEntries.clear();
+            for (Entry<Integer, ControllerServer> entry : controllers.entrySet()) {
+                int controllerId = entry.getKey();
+                ControllerServer controller = entry.getValue();
+                futureEntries.add(new SimpleImmutableEntry<>("controller" + controllerId,
+                    executorService.submit(controller::shutdown)));
+            }
+            waitForAllFutures(futureEntries);
+            futureEntries.clear();
+            for (Entry<Integer, KafkaRaftManager> entry : raftManagers.entrySet()) {
+                int raftManagerId = entry.getKey();
+                KafkaRaftManager raftManager = entry.getValue();
+                futureEntries.add(new SimpleImmutableEntry<>("raftManager" + raftManagerId,
+                    executorService.submit(raftManager::shutdown)));
+            }
+            waitForAllFutures(futureEntries);
+            futureEntries.clear();
+            Utils.delete(baseDirectory);
+        } catch (Exception e) {
+            for (Entry<String, Future<?>> entry : futureEntries) {
+                entry.getValue().cancel(true);
+            }
+            throw e;
+        } finally {
+            executorService.shutdownNow();
+            executorService.awaitTermination(5, TimeUnit.MINUTES);
+        }
+    }
+
+    private void waitForAllFutures(List<Entry<String, Future<?>>> futureEntries)
+            throws Exception {
+        for (Entry<String, Future<?>> entry : futureEntries) {
+            log.debug("waiting for {} to shut down.", entry.getKey());
+            entry.getValue().get();
+            log.debug("{} successfully shut down.", entry.getKey());
+        }
+    }
+}
diff --git a/core/src/test/java/kafka/testkit/TestKitNode.java b/core/src/test/java/kafka/testkit/TestKitNode.java
new file mode 100644
index 0000000000..a5423d135f
--- /dev/null
+++ b/core/src/test/java/kafka/testkit/TestKitNode.java
@@ -0,0 +1,23 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package kafka.testkit;
+
+public interface TestKitNode {
+    int id();
+    String metadataDirectory();
+}
diff --git a/core/src/test/java/kafka/testkit/TestKitNodes.java b/core/src/test/java/kafka/testkit/TestKitNodes.java
new file mode 100644
index 0000000000..b2d9504158
--- /dev/null
+++ b/core/src/test/java/kafka/testkit/TestKitNodes.java
@@ -0,0 +1,181 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package kafka.testkit;
+
+import kafka.server.MetaProperties;
+import org.apache.kafka.common.Uuid;
+import org.apache.kafka.common.network.ListenerName;
+
+import java.nio.file.Paths;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.NavigableMap;
+import java.util.TreeMap;
+
+public class TestKitNodes {
+    public static class Builder {
+        private Uuid clusterId = null;
+        private final NavigableMap<Integer, ControllerNode> controllerNodes = new TreeMap<>();
+        private final NavigableMap<Integer, BrokerNode> brokerNodes = new TreeMap<>();
+
+        public Builder setClusterId(Uuid clusterId) {
+            this.clusterId = clusterId;
+            return this;
+        }
+
+        public Builder addNodes(TestKitNode[] nodes) {
+            for (TestKitNode node : nodes) {
+                addNode(node);
+            }
+            return this;
+        }
+
+        public Builder addNode(TestKitNode node) {
+            if (node instanceof ControllerNode) {
+                ControllerNode controllerNode = (ControllerNode) node;
+                controllerNodes.put(node.id(), controllerNode);
+            } else if (node instanceof BrokerNode) {
+                BrokerNode brokerNode = (BrokerNode) node;
+                brokerNodes.put(node.id(), brokerNode);
+            } else {
+                throw new RuntimeException("Can't handle TestKitNode subclass " +
+                        node.getClass().getSimpleName());
+            }
+            return this;
+        }
+
+        public Builder setNumControllerNodes(int numControllerNodes) {
+            if (numControllerNodes < 0) {
+                throw new RuntimeException("Invalid negative value for numControllerNodes");
+            }
+
+            while (controllerNodes.size() > numControllerNodes) {
+                controllerNodes.pollFirstEntry();
+            }
+            while (controllerNodes.size() < numControllerNodes) {
+                int nextId = 3000;
+                if (!controllerNodes.isEmpty()) {
+                    nextId = controllerNodes.lastKey() + 1;
+                }
+                controllerNodes.put(nextId, new ControllerNode.Builder().
+                    setId(nextId).build());
+            }
+            return this;
+        }
+
+        public Builder setNumKip500BrokerNodes(int numBrokerNodes) {
+            if (numBrokerNodes < 0) {
+                throw new RuntimeException("Invalid negative value for numBrokerNodes");
+            }
+            while (brokerNodes.size() > numBrokerNodes) {
+                brokerNodes.pollFirstEntry();
+            }
+            while (brokerNodes.size() < numBrokerNodes) {
+                int nextId = 0;
+                if (!brokerNodes.isEmpty()) {
+                    nextId = brokerNodes.lastKey() + 1;
+                }
+                brokerNodes.put(nextId, new BrokerNode.Builder().
+                    setId(nextId).build());
+            }
+            return this;
+        }
+
+        public TestKitNodes build() {
+            if (clusterId == null) {
+                clusterId = Uuid.randomUuid();
+            }
+            return new TestKitNodes(clusterId, controllerNodes, brokerNodes);
+        }
+    }
+
+    private final Uuid clusterId;
+    private final NavigableMap<Integer, ControllerNode> controllerNodes;
+    private final NavigableMap<Integer, BrokerNode> brokerNodes;
+
+    private TestKitNodes(Uuid clusterId,
+                         NavigableMap<Integer, ControllerNode> controllerNodes,
+                         NavigableMap<Integer, BrokerNode> brokerNodes) {
+        this.clusterId = clusterId;
+        this.controllerNodes = controllerNodes;
+        this.brokerNodes = brokerNodes;
+    }
+
+    public Uuid clusterId() {
+        return clusterId;
+    }
+
+    public Map<Integer, ControllerNode> controllerNodes() {
+        return controllerNodes;
+    }
+
+    public NavigableMap<Integer, BrokerNode> brokerNodes() {
+        return brokerNodes;
+    }
+
+    public MetaProperties controllerProperties(int id) {
+        return MetaProperties.apply(clusterId.toString(), id);
+    }
+
+    public MetaProperties brokerProperties(int id) {
+        return MetaProperties.apply(clusterId.toString(), id);
+    }
+
+    public ListenerName interBrokerListenerName() {
+        return new ListenerName("EXTERNAL");
+    }
+
+    public ListenerName externalListenerName() {
+        return new ListenerName("EXTERNAL");
+    }
+
+    public TestKitNodes copyWithAbsolutePaths(String baseDirectory) {
+        NavigableMap<Integer, ControllerNode> newControllerNodes = new TreeMap<>();
+        NavigableMap<Integer, BrokerNode> newBrokerNodes = new TreeMap<>();
+        for (Entry<Integer, ControllerNode> entry : controllerNodes.entrySet()) {
+            ControllerNode node = entry.getValue();
+            newControllerNodes.put(entry.getKey(), new ControllerNode(node.id(),
+                absolutize(baseDirectory, node.metadataDirectory())));
+        }
+        for (Entry<Integer, BrokerNode> entry : brokerNodes.entrySet()) {
+            BrokerNode node = entry.getValue();
+            newBrokerNodes.put(entry.getKey(), new BrokerNode(node.id(),
+                node.incarnationId(), absolutize(baseDirectory, node.metadataDirectory()),
+                absolutize(baseDirectory, node.logDataDirectories())));
+        }
+        return new TestKitNodes(clusterId, newControllerNodes, newBrokerNodes);
+    }
+
+    private static List<String> absolutize(String base, Collection<String> directories) {
+        List<String> newDirectories = new ArrayList<>();
+        for (String directory : directories) {
+            newDirectories.add(absolutize(base, directory));
+        }
+        return newDirectories;
+    }
+
+    private static String absolutize(String base, String directory) {
+        if (Paths.get(directory).isAbsolute()) {
+            return directory;
+        }
+        return Paths.get(base, directory).toAbsolutePath().toString();
+    }
+}
diff --git a/core/src/test/resources/log4j.properties b/core/src/test/resources/log4j.properties
index 0648d7f3fc..f7fb7364a3 100644
--- a/core/src/test/resources/log4j.properties
+++ b/core/src/test/resources/log4j.properties
@@ -18,8 +18,9 @@ log4j.appender.stdout=org.apache.log4j.ConsoleAppender
 log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
 log4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c:%L)%n
 
-log4j.logger.kafka=ERROR
-log4j.logger.org.apache.kafka=ERROR
+log4j.logger.kafka=WARN
+log4j.logger.org.apache.kafka=WARN
+
 
 # zkclient can be verbose, during debugging it is common to adjust it separately
 log4j.logger.org.apache.zookeeper=WARN
diff --git a/core/src/test/scala/integration/kafka/server/RaftClusterTest.scala b/core/src/test/scala/integration/kafka/server/RaftClusterTest.scala
new file mode 100644
index 0000000000..a057432587
--- /dev/null
+++ b/core/src/test/scala/integration/kafka/server/RaftClusterTest.scala
@@ -0,0 +1,216 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package kafka.server
+
+import kafka.testkit.{KafkaClusterTestKit, TestKitNodes}
+import kafka.utils.TestUtils
+import org.apache.kafka.clients.admin.{Admin, NewTopic}
+import org.apache.kafka.metadata.BrokerState
+import org.junit.jupiter.api.{Test, Timeout}
+import org.junit.jupiter.api.Assertions._
+
+import java.util
+import java.util.Collections
+import java.util.concurrent.TimeUnit
+
+@Timeout(120000)
+class RaftClusterTest {
+
+  @Test
+  def testCreateClusterAndClose(): Unit = {
+    val cluster = new KafkaClusterTestKit.Builder(
+      new TestKitNodes.Builder().
+        setNumKip500BrokerNodes(1).
+        setNumControllerNodes(1).build()).build()
+    try {
+      cluster.format()
+      cluster.startup()
+    } finally {
+      cluster.close()
+    }
+  }
+
+  @Test
+  def testCreateClusterAndWaitForBrokerInRunningState(): Unit = {
+    val cluster = new KafkaClusterTestKit.Builder(
+      new TestKitNodes.Builder().
+        setNumKip500BrokerNodes(3).
+        setNumControllerNodes(3).build()).build()
+    try {
+      cluster.format()
+      cluster.startup()
+      TestUtils.waitUntilTrue(() => cluster.brokers().get(0).currentState() == BrokerState.RUNNING,
+        "Broker never made it to RUNNING state.")
+      TestUtils.waitUntilTrue(() => cluster.raftManagers().get(0).kafkaRaftClient.leaderAndEpoch().leaderId.isPresent,
+        "RaftManager was not initialized.")
+      val admin = Admin.create(cluster.clientProperties())
+      try {
+        assertEquals(cluster.nodes().clusterId().toString,
+          admin.describeCluster().clusterId().get())
+      } finally {
+        admin.close()
+      }
+    } finally {
+      cluster.close()
+    }
+  }
+
+  @Test
+  def testCreateClusterAndCreateListDeleteTopic(): Unit = {
+    val cluster = new KafkaClusterTestKit.Builder(
+      new TestKitNodes.Builder().
+        setNumKip500BrokerNodes(3).
+        setNumControllerNodes(3).build()).build()
+    try {
+      cluster.format()
+      cluster.startup()
+      cluster.waitForReadyBrokers()
+      TestUtils.waitUntilTrue(() => cluster.brokers().get(0).currentState() == BrokerState.RUNNING,
+        "Broker never made it to RUNNING state.")
+      TestUtils.waitUntilTrue(() => cluster.raftManagers().get(0).kafkaRaftClient.leaderAndEpoch().leaderId.isPresent,
+        "RaftManager was not initialized.")
+
+      val admin = Admin.create(cluster.clientProperties())
+      try {
+        // Create a test topic
+        val newTopic = Collections.singletonList(new NewTopic("test-topic", 1, 3.toShort))
+        val createTopicResult = admin.createTopics(newTopic)
+        createTopicResult.all().get(60, TimeUnit.SECONDS)
+
+        // List created topic
+        TestUtils.waitUntilTrue(() => {
+          val listTopicsResult = admin.listTopics()
+          val result = listTopicsResult.names().get(5, TimeUnit.SECONDS).size() == newTopic.size()
+          if (result) {
+            newTopic forEach(topic => {
+              assertTrue(listTopicsResult.names().get().contains(topic.name()))
+            })
+          }
+          result
+        }, "Topics created were not listed.")
+
+        // Delete topic
+        val deleteResult = admin.deleteTopics(Collections.singletonList("test-topic"))
+        deleteResult.all().get(60, TimeUnit.SECONDS)
+
+        // List again
+        TestUtils.waitUntilTrue(() => {
+          val listTopicsResult = admin.listTopics()
+          val result = listTopicsResult.names().get(5, TimeUnit.SECONDS).size() != newTopic.size()
+          if (result) {
+            newTopic forEach(topic => {
+              assertFalse(listTopicsResult.names().get().contains(topic.name()))
+            })
+          }
+          result
+        }, "Topic was not removed from list.")
+
+      } finally {
+        admin.close()
+      }
+    } finally {
+      cluster.close()
+    }
+  }
+
+  @Test
+  def testCreateClusterAndCreateAndManyTopics(): Unit = {
+    val cluster = new KafkaClusterTestKit.Builder(
+      new TestKitNodes.Builder().
+        setNumKip500BrokerNodes(3).
+        setNumControllerNodes(3).build()).build()
+    try {
+      cluster.format()
+      cluster.startup()
+      cluster.waitForReadyBrokers()
+      TestUtils.waitUntilTrue(() => cluster.brokers().get(0).currentState() == BrokerState.RUNNING,
+        "Broker never made it to RUNNING state.")
+      TestUtils.waitUntilTrue(() => cluster.raftManagers().get(0).kafkaRaftClient.leaderAndEpoch().leaderId.isPresent,
+        "RaftManager was not initialized.")
+      val admin = Admin.create(cluster.clientProperties())
+      try {
+        // Create many topics
+        val newTopic = new util.ArrayList[NewTopic]()
+        newTopic.add(new NewTopic("test-topic-1", 1, 3.toShort))
+        newTopic.add(new NewTopic("test-topic-2", 1, 3.toShort))
+        newTopic.add(new NewTopic("test-topic-3", 1, 3.toShort))
+        val createTopicResult = admin.createTopics(newTopic)
+        createTopicResult.all().get(60, TimeUnit.SECONDS)
+
+        // List created topic
+        TestUtils.waitUntilTrue(() => {
+          val listTopicsResult = admin.listTopics()
+          val result = listTopicsResult.names().get(5, TimeUnit.SECONDS).size() == newTopic.size()
+          if (result) {
+            newTopic forEach(topic => {
+              assertTrue(listTopicsResult.names().get().contains(topic.name()))
+            })
+          }
+          result
+        }, "Topics created were not listed.")
+      } finally {
+        admin.close()
+      }
+    } finally {
+      cluster.close()
+    }
+  }
+
+  @Test
+  def testCreateClusterAndCreateAndManyTopicsWithManyPartitions(): Unit = {
+    val cluster = new KafkaClusterTestKit.Builder(
+      new TestKitNodes.Builder().
+        setNumKip500BrokerNodes(3).
+        setNumControllerNodes(3).build()).build()
+    try {
+      cluster.format()
+      cluster.startup()
+      cluster.waitForReadyBrokers()
+      TestUtils.waitUntilTrue(() => cluster.brokers().get(0).currentState() == BrokerState.RUNNING,
+        "Broker never made it to RUNNING state.")
+      TestUtils.waitUntilTrue(() => cluster.raftManagers().get(0).kafkaRaftClient.leaderAndEpoch().leaderId.isPresent,
+        "RaftManager was not initialized.")
+      val admin = Admin.create(cluster.clientProperties())
+      try {
+        // Create many topics
+        val newTopic = new util.ArrayList[NewTopic]()
+        newTopic.add(new NewTopic("test-topic-1", 3, 3.toShort))
+        newTopic.add(new NewTopic("test-topic-2", 3, 3.toShort))
+        newTopic.add(new NewTopic("test-topic-3", 3, 3.toShort))
+        val createTopicResult = admin.createTopics(newTopic)
+        createTopicResult.all().get(60, TimeUnit.SECONDS)
+
+        // List created topic
+        TestUtils.waitUntilTrue(() => {
+          val listTopicsResult = admin.listTopics()
+          val result = listTopicsResult.names().get(5, TimeUnit.SECONDS).size() == newTopic.size()
+          if (result) {
+            newTopic forEach(topic => {
+              assertTrue(listTopicsResult.names().get().contains(topic.name()))
+            })
+          }
+          result
+        }, "Topics created were not listed.")
+      } finally {
+        admin.close()
+      }
+    } finally {
+      cluster.close()
+    }
+  }
+}
diff --git a/raft/src/main/java/org/apache/kafka/raft/RaftConfig.java b/raft/src/main/java/org/apache/kafka/raft/RaftConfig.java
index 358d5f3d5e..0833df0bb2 100644
--- a/raft/src/main/java/org/apache/kafka/raft/RaftConfig.java
+++ b/raft/src/main/java/org/apache/kafka/raft/RaftConfig.java
@@ -28,6 +28,7 @@ import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
+import java.util.Objects;
 import java.util.Set;
 import java.util.stream.Collectors;
 
@@ -241,12 +242,16 @@ public class RaftConfig {
     }
 
     public static List<Node> quorumVoterStringsToNodes(List<String> voters) {
-        return parseVoterConnections(voters).entrySet().stream()
+        return voterConnectionsToNodes(parseVoterConnections(voters));
+    }
+
+    public static List<Node> voterConnectionsToNodes(Map<Integer, RaftConfig.AddressSpec> voterConnections) {
+        return voterConnections.entrySet().stream()
+            .filter(Objects::nonNull)
             .filter(connection -> connection.getValue() instanceof InetAddressSpec)
             .map(connection -> {
-                InetAddressSpec inetAddressSpec = InetAddressSpec.class.cast(connection.getValue());
-                return new Node(connection.getKey(), inetAddressSpec.address.getHostName(),
-                    inetAddressSpec.address.getPort());
+                InetAddressSpec spec = (InetAddressSpec) connection.getValue();
+                return new Node(connection.getKey(), spec.address.getHostName(), spec.address.getPort());
             })
             .collect(Collectors.toList());
     }
