diff --git a/core/src/main/scala/kafka/cluster/Partition.scala b/core/src/main/scala/kafka/cluster/Partition.scala
index 76d4193ef9..3fc8041051 100755
--- a/core/src/main/scala/kafka/cluster/Partition.scala
+++ b/core/src/main/scala/kafka/cluster/Partition.scala
@@ -907,38 +907,34 @@ class Partition(val topicPartition: TopicPartition,
   private def tryCompleteDelayedRequests(): Unit = delayedOperations.checkAndCompleteAll()
 
   def maybeShrinkIsr(): Unit = {
-    val needsIsrUpdate = !isrState.isInflight && inReadLock(leaderIsrUpdateLock) {
-      needsShrinkIsr()
+    def needsIsrUpdate: Boolean = {
+      !isrState.isInflight && inReadLock(leaderIsrUpdateLock) {
+        needsShrinkIsr()
+      }
     }
-    val leaderHWIncremented = needsIsrUpdate && inWriteLock(leaderIsrUpdateLock) {
-      leaderLogIfLocal.exists { leaderLog =>
-        val outOfSyncReplicaIds = getOutOfSyncReplicas(replicaLagTimeMaxMs)
-        if (outOfSyncReplicaIds.nonEmpty) {
-          val outOfSyncReplicaLog = outOfSyncReplicaIds.map { replicaId =>
-            val logEndOffsetMessage = getReplica(replicaId)
-              .map(_.logEndOffset.toString)
-              .getOrElse("unknown")
-            s"(brokerId: $replicaId, endOffset: $logEndOffsetMessage)"
-          }.mkString(" ")
-          val newIsrLog = (isrState.isr -- outOfSyncReplicaIds).mkString(",")
-          info(s"Shrinking ISR from ${isrState.isr.mkString(",")} to $newIsrLog. " +
-               s"Leader: (highWatermark: ${leaderLog.highWatermark}, " +
-               s"endOffset: ${leaderLog.logEndOffset}). " +
-               s"Out of sync replicas: $outOfSyncReplicaLog.")
-
-          shrinkIsr(outOfSyncReplicaIds)
 
-          // we may need to increment high watermark since ISR could be down to 1
-          maybeIncrementLeaderHW(leaderLog)
-        } else {
-          false
+    if (needsIsrUpdate) {
+      inWriteLock(leaderIsrUpdateLock) {
+        leaderLogIfLocal.foreach { leaderLog =>
+          val outOfSyncReplicaIds = getOutOfSyncReplicas(replicaLagTimeMaxMs)
+          if (outOfSyncReplicaIds.nonEmpty) {
+            val outOfSyncReplicaLog = outOfSyncReplicaIds.map { replicaId =>
+              val logEndOffsetMessage = getReplica(replicaId)
+                .map(_.logEndOffset.toString)
+                .getOrElse("unknown")
+              s"(brokerId: $replicaId, endOffset: $logEndOffsetMessage)"
+            }.mkString(" ")
+            val newIsrLog = (isrState.isr -- outOfSyncReplicaIds).mkString(",")
+            info(s"Shrinking ISR from ${isrState.isr.mkString(",")} to $newIsrLog. " +
+              s"Leader: (highWatermark: ${leaderLog.highWatermark}, " +
+              s"endOffset: ${leaderLog.logEndOffset}). " +
+              s"Out of sync replicas: $outOfSyncReplicaLog.")
+
+            shrinkIsr(outOfSyncReplicaIds)
+          }
         }
       }
     }
-
-    // some delayed operations may be unblocked after HW changed
-    if (leaderHWIncremented)
-      tryCompleteDelayedRequests()
   }
 
   private def needsShrinkIsr(): Boolean = {
@@ -1365,54 +1361,65 @@ class Partition(val topicPartition: TopicPartition,
    * or LeaderAndIsr
    */
   private def handleAlterIsrResponse(proposedIsrState: IsrState)(result: Either[Errors, LeaderAndIsr]): Unit = {
-    inWriteLock(leaderIsrUpdateLock) {
+    val hwIncremented = inWriteLock(leaderIsrUpdateLock) {
       if (isrState != proposedIsrState) {
         // This means isrState was updated through leader election or some other mechanism before we got the AlterIsr
         // response. We don't know what happened on the controller exactly, but we do know this response is out of date
         // so we ignore it.
         debug(s"Ignoring failed ISR update to $proposedIsrState since we have already updated state to $isrState")
-        return
-      }
-
-      result match {
-        case Left(error: Errors) =>
-          isrChangeListener.markFailed()
-          error match {
-            case Errors.UNKNOWN_TOPIC_OR_PARTITION =>
-              debug(s"Failed to update ISR to $proposedIsrState since it doesn't know about this topic or partition. Giving up.")
-            case Errors.FENCED_LEADER_EPOCH =>
-              debug(s"Failed to update ISR to $proposedIsrState since we sent an old leader epoch. Giving up.")
-            case Errors.INVALID_UPDATE_VERSION =>
-              debug(s"Failed to update ISR to $proposedIsrState due to invalid version. Giving up.")
-            case _ =>
-              warn(s"Failed to update ISR to $proposedIsrState due to unexpected $error. Retrying.")
-              sendAlterIsrRequest(proposedIsrState)
-          }
-        case Right(leaderAndIsr: LeaderAndIsr) =>
-          // Success from controller, still need to check a few things
-          if (leaderAndIsr.leaderEpoch != leaderEpoch) {
-            debug(s"Ignoring new ISR ${leaderAndIsr} since we have a stale leader epoch $leaderEpoch.")
-            isrChangeListener.markFailed()
-          } else if (leaderAndIsr.zkVersion < zkVersion) {
-            debug(s"Ignoring new ISR ${leaderAndIsr} since we have a newer version $zkVersion.")
+        false
+      } else {
+        result match {
+          case Left(error: Errors) =>
             isrChangeListener.markFailed()
-          } else {
-            // This is one of two states:
-            //   1) leaderAndIsr.zkVersion > zkVersion: Controller updated to new version with proposedIsrState.
-            //   2) leaderAndIsr.zkVersion == zkVersion: No update was performed since proposed and actual state are the same.
-            // In both cases, we want to move from Pending to Committed state to ensure new updates are processed.
-
-            isrState = CommittedIsr(leaderAndIsr.isr.toSet)
-            zkVersion = leaderAndIsr.zkVersion
-            info(s"ISR updated to ${isrState.isr.mkString(",")} and version updated to [$zkVersion]")
-            proposedIsrState match {
-              case PendingExpandIsr(_, _) => isrChangeListener.markExpand()
-              case PendingShrinkIsr(_, _) => isrChangeListener.markShrink()
-              case _ => // nothing to do, shouldn't get here
+            error match {
+              case Errors.UNKNOWN_TOPIC_OR_PARTITION =>
+                debug(s"Failed to update ISR to $proposedIsrState since it doesn't know about this topic or partition. Giving up.")
+              case Errors.FENCED_LEADER_EPOCH =>
+                debug(s"Failed to update ISR to $proposedIsrState since we sent an old leader epoch. Giving up.")
+              case Errors.INVALID_UPDATE_VERSION =>
+                debug(s"Failed to update ISR to $proposedIsrState due to invalid version. Giving up.")
+              case _ =>
+                warn(s"Failed to update ISR to $proposedIsrState due to unexpected $error. Retrying.")
+                sendAlterIsrRequest(proposedIsrState)
             }
-          }
+            false
+
+          case Right(leaderAndIsr: LeaderAndIsr) =>
+            // Success from controller, still need to check a few things
+            if (leaderAndIsr.leaderEpoch != leaderEpoch) {
+              debug(s"Ignoring new ISR ${leaderAndIsr} since we have a stale leader epoch $leaderEpoch.")
+              isrChangeListener.markFailed()
+              false
+            } else if (leaderAndIsr.zkVersion < zkVersion) {
+              debug(s"Ignoring new ISR ${leaderAndIsr} since we have a newer version $zkVersion.")
+              isrChangeListener.markFailed()
+              false
+            } else {
+              // This is one of two states:
+              //   1) leaderAndIsr.zkVersion > zkVersion: Controller updated to new version with proposedIsrState.
+              //   2) leaderAndIsr.zkVersion == zkVersion: No update was performed since proposed and actual state are the same.
+              // In both cases, we want to move from Pending to Committed state to ensure new updates are processed.
+
+              isrState = CommittedIsr(leaderAndIsr.isr.toSet)
+              zkVersion = leaderAndIsr.zkVersion
+              info(s"ISR updated to ${isrState.isr.mkString(",")} and version updated to [$zkVersion]")
+              proposedIsrState match {
+                case PendingExpandIsr(_, _) => isrChangeListener.markExpand()
+                case PendingShrinkIsr(_, _) => isrChangeListener.markShrink()
+                case _ => // nothing to do, shouldn't get here
+              }
+
+              // we may need to increment high watermark since ISR could be down to 1
+              leaderLogIfLocal.exists(log => maybeIncrementLeaderHW(log))
+            }
+        }
       }
     }
+
+    if (hwIncremented) {
+      tryCompleteDelayedRequests()
+    }
   }
 
   override def equals(that: Any): Boolean = that match {
diff --git a/core/src/test/scala/unit/kafka/cluster/PartitionTest.scala b/core/src/test/scala/unit/kafka/cluster/PartitionTest.scala
index 65f4148e32..206a8ee1f6 100644
--- a/core/src/test/scala/unit/kafka/cluster/PartitionTest.scala
+++ b/core/src/test/scala/unit/kafka/cluster/PartitionTest.scala
@@ -1186,29 +1186,79 @@ class PartitionTest extends AbstractPartitionTest {
   }
 
   @Test
-  def testMaybeShrinkIsr(): Unit = {
+  def testRetryShrinkIsr(): Unit = {
     val log = logManager.getOrCreateLog(topicPartition, topicId = None)
     seedLogData(log, numRecords = 10, leaderEpoch = 4)
 
     val controllerEpoch = 0
     val leaderEpoch = 5
     val remoteBrokerId = brokerId + 1
-    val replicas = List(brokerId, remoteBrokerId)
-    val isr = List[Integer](brokerId, remoteBrokerId).asJava
+    val replicas = Seq(brokerId, remoteBrokerId)
+    val isr = Seq(brokerId, remoteBrokerId)
+    val topicId = Uuid.randomUuid()
 
-    val initializeTimeMs = time.milliseconds()
-    partition.createLogIfNotExists(isNew = false, isFutureReplica = false, offsetCheckpoints, None)
-    assertTrue(partition.makeLeader(
-        new LeaderAndIsrPartitionState()
-          .setControllerEpoch(controllerEpoch)
-          .setLeader(brokerId)
-          .setLeaderEpoch(leaderEpoch)
-          .setIsr(isr)
-          .setZkVersion(1)
-          .setReplicas(replicas.map(Int.box).asJava)
-          .setIsNew(true),
-        offsetCheckpoints, None), "Expected become leader transition to succeed")
+    assertTrue(makeLeader(
+      topicId = Some(topicId),
+      controllerEpoch = controllerEpoch,
+      leaderEpoch = leaderEpoch,
+      isr = isr,
+      replicas = replicas,
+      zkVersion = 1,
+      isNew = true
+    ))
+    assertEquals(0L, partition.localLogOrException.highWatermark)
+
+    // Sleep enough time to shrink the ISR
+    time.sleep(partition.replicaLagTimeMaxMs + 1)
+
+    // Try to shrink the ISR
+    partition.maybeShrinkIsr()
+    assertEquals(alterIsrManager.isrUpdates.size, 1)
+    assertEquals(alterIsrManager.isrUpdates.head.leaderAndIsr.isr, List(brokerId))
+    assertEquals(Set(brokerId, remoteBrokerId), partition.isrState.isr)
+    assertEquals(Set(brokerId, remoteBrokerId), partition.isrState.maximalIsr)
+
+    // The shrink fails and we retry
+    alterIsrManager.failIsrUpdate(Errors.NETWORK_EXCEPTION)
+    assertEquals(0, isrChangeListener.shrinks.get)
+    assertEquals(1, isrChangeListener.failures.get)
+    assertEquals(1, partition.getZkVersion)
+    assertEquals(alterIsrManager.isrUpdates.size, 1)
     assertEquals(Set(brokerId, remoteBrokerId), partition.isrState.isr)
+    assertEquals(Set(brokerId, remoteBrokerId), partition.isrState.maximalIsr)
+    assertEquals(0L, partition.localLogOrException.highWatermark)
+
+    // The shrink succeeds after retrying
+    alterIsrManager.completeIsrUpdate(newZkVersion = 2)
+    assertEquals(1, isrChangeListener.shrinks.get)
+    assertEquals(2, partition.getZkVersion)
+    assertEquals(alterIsrManager.isrUpdates.size, 0)
+    assertEquals(Set(brokerId), partition.isrState.isr)
+    assertEquals(Set(brokerId), partition.isrState.maximalIsr)
+    assertEquals(log.logEndOffset, partition.localLogOrException.highWatermark)
+  }
+
+  @Test
+  def testMaybeShrinkIsr(): Unit = {
+    val log = logManager.getOrCreateLog(topicPartition, topicId = None)
+    seedLogData(log, numRecords = 10, leaderEpoch = 4)
+
+    val controllerEpoch = 0
+    val leaderEpoch = 5
+    val remoteBrokerId = brokerId + 1
+    val replicas = Seq(brokerId, remoteBrokerId)
+    val isr = Seq(brokerId, remoteBrokerId)
+    val initializeTimeMs = time.milliseconds()
+
+    assertTrue(makeLeader(
+      topicId = None,
+      controllerEpoch = controllerEpoch,
+      leaderEpoch = leaderEpoch,
+      isr = isr,
+      replicas = replicas,
+      zkVersion = 1,
+      isNew = true
+    ))
     assertEquals(0L, partition.localLogOrException.highWatermark)
 
     val remoteReplica = partition.getReplica(remoteBrokerId).get
@@ -1225,11 +1275,22 @@ class PartitionTest extends AbstractPartitionTest {
 
     // Shrink the ISR
     partition.maybeShrinkIsr()
+    assertEquals(0, isrChangeListener.shrinks.get)
     assertEquals(alterIsrManager.isrUpdates.size, 1)
     assertEquals(alterIsrManager.isrUpdates.head.leaderAndIsr.isr, List(brokerId))
     assertEquals(Set(brokerId, remoteBrokerId), partition.isrState.isr)
     assertEquals(Set(brokerId, remoteBrokerId), partition.isrState.maximalIsr)
     assertEquals(0L, partition.localLogOrException.highWatermark)
+
+    // After the ISR shrink completes, the ISR state should be updated and the
+    // high watermark should be advanced
+    alterIsrManager.completeIsrUpdate(newZkVersion = 2)
+    assertEquals(1, isrChangeListener.shrinks.get)
+    assertEquals(2, partition.getZkVersion)
+    assertEquals(alterIsrManager.isrUpdates.size, 0)
+    assertEquals(Set(brokerId), partition.isrState.isr)
+    assertEquals(Set(brokerId), partition.isrState.maximalIsr)
+    assertEquals(log.logEndOffset, partition.localLogOrException.highWatermark)
   }
 
   @Test
@@ -1240,22 +1301,19 @@ class PartitionTest extends AbstractPartitionTest {
     val controllerEpoch = 0
     val leaderEpoch = 5
     val remoteBrokerId = brokerId + 1
-    val replicas = List(brokerId, remoteBrokerId)
-    val isr = List[Integer](brokerId, remoteBrokerId).asJava
-
+    val replicas = Seq(brokerId, remoteBrokerId)
+    val isr = Seq(brokerId, remoteBrokerId)
     val initializeTimeMs = time.milliseconds()
-    partition.createLogIfNotExists(isNew = false, isFutureReplica = false, offsetCheckpoints, None)
-    assertTrue(partition.makeLeader(
-      new LeaderAndIsrPartitionState()
-        .setControllerEpoch(controllerEpoch)
-        .setLeader(brokerId)
-        .setLeaderEpoch(leaderEpoch)
-        .setIsr(isr)
-        .setZkVersion(1)
-        .setReplicas(replicas.map(Int.box).asJava)
-        .setIsNew(true),
-      offsetCheckpoints, None), "Expected become leader transition to succeed")
-    assertEquals(Set(brokerId, remoteBrokerId), partition.isrState.isr)
+
+    assertTrue(makeLeader(
+      topicId = None,
+      controllerEpoch = controllerEpoch,
+      leaderEpoch = leaderEpoch,
+      isr = isr,
+      replicas = replicas,
+      zkVersion = 1,
+      isNew = true
+    ))
     assertEquals(0L, partition.localLogOrException.highWatermark)
 
     val remoteReplica = partition.getReplica(remoteBrokerId).get
@@ -1269,16 +1327,16 @@ class PartitionTest extends AbstractPartitionTest {
     assertTrue(partition.isrState.isInflight)
 
     // Become leader again, reset the ISR state
-    assertFalse(partition.makeLeader(
-      new LeaderAndIsrPartitionState()
-        .setControllerEpoch(controllerEpoch)
-        .setLeader(brokerId)
-        .setLeaderEpoch(leaderEpoch)
-        .setIsr(isr)
-        .setZkVersion(2)
-        .setReplicas(replicas.map(Int.box).asJava)
-        .setIsNew(false),
-      offsetCheckpoints, None))
+    assertFalse(makeLeader(
+      topicId = None,
+      controllerEpoch = controllerEpoch,
+      leaderEpoch = leaderEpoch,
+      isr = isr,
+      replicas = replicas,
+      zkVersion = 2,
+      isNew = false
+    ))
+    assertEquals(0L, partition.localLogOrException.highWatermark)
     assertFalse(partition.isrState.isInflight, "ISR should be committed and not inflight")
 
     // Try the shrink again, should not submit until AlterIsr response arrives
@@ -1300,22 +1358,19 @@ class PartitionTest extends AbstractPartitionTest {
     val controllerEpoch = 0
     val leaderEpoch = 5
     val remoteBrokerId = brokerId + 1
-    val replicas = List(brokerId, remoteBrokerId)
-    val isr = List[Integer](brokerId, remoteBrokerId).asJava
-
+    val replicas = Seq(brokerId, remoteBrokerId)
+    val isr = Seq(brokerId, remoteBrokerId)
     val initializeTimeMs = time.milliseconds()
-    partition.createLogIfNotExists(isNew = false, isFutureReplica = false, offsetCheckpoints, None)
-    assertTrue(partition.makeLeader(
-        new LeaderAndIsrPartitionState()
-          .setControllerEpoch(controllerEpoch)
-          .setLeader(brokerId)
-          .setLeaderEpoch(leaderEpoch)
-          .setIsr(isr)
-          .setZkVersion(1)
-          .setReplicas(replicas.map(Int.box).asJava)
-          .setIsNew(true),
-        offsetCheckpoints, None), "Expected become leader transition to succeed")
-    assertEquals(Set(brokerId, remoteBrokerId), partition.isrState.isr)
+
+    assertTrue(makeLeader(
+      topicId = None,
+      controllerEpoch = controllerEpoch,
+      leaderEpoch = leaderEpoch,
+      isr = isr,
+      replicas = replicas,
+      zkVersion = 1,
+      isNew = true
+    ))
     assertEquals(0L, partition.localLogOrException.highWatermark)
 
     val remoteReplica = partition.getReplica(remoteBrokerId).get
@@ -1365,22 +1420,19 @@ class PartitionTest extends AbstractPartitionTest {
     val controllerEpoch = 0
     val leaderEpoch = 5
     val remoteBrokerId = brokerId + 1
-    val replicas = List(brokerId, remoteBrokerId)
-    val isr = List[Integer](brokerId, remoteBrokerId).asJava
-
+    val replicas = Seq(brokerId, remoteBrokerId)
+    val isr = Seq(brokerId, remoteBrokerId)
     val initializeTimeMs = time.milliseconds()
-    partition.createLogIfNotExists(isNew = false, isFutureReplica = false, offsetCheckpoints, None)
-    assertTrue(partition.makeLeader(
-        new LeaderAndIsrPartitionState()
-          .setControllerEpoch(controllerEpoch)
-          .setLeader(brokerId)
-          .setLeaderEpoch(leaderEpoch)
-          .setIsr(isr)
-          .setZkVersion(1)
-          .setReplicas(replicas.map(Int.box).asJava)
-          .setIsNew(true),
-        offsetCheckpoints, None), "Expected become leader transition to succeed")
-    assertEquals(Set(brokerId, remoteBrokerId), partition.isrState.isr)
+
+    assertTrue(makeLeader(
+      topicId = None,
+      controllerEpoch = controllerEpoch,
+      leaderEpoch = leaderEpoch,
+      isr = isr,
+      replicas = replicas,
+      zkVersion = 1,
+      isNew = true
+    ))
     assertEquals(0L, partition.localLogOrException.highWatermark)
 
     val remoteReplica = partition.getReplica(remoteBrokerId).get
@@ -1416,22 +1468,19 @@ class PartitionTest extends AbstractPartitionTest {
     val controllerEpoch = 0
     val leaderEpoch = 5
     val remoteBrokerId = brokerId + 1
-    val replicas = List[Integer](brokerId, remoteBrokerId).asJava
-    val isr = List[Integer](brokerId, remoteBrokerId).asJava
-
+    val replicas = Seq(brokerId, remoteBrokerId)
+    val isr = Seq(brokerId, remoteBrokerId)
     val initializeTimeMs = time.milliseconds()
-    partition.createLogIfNotExists(isNew = false, isFutureReplica = false, offsetCheckpoints, None)
-    assertTrue(partition.makeLeader(
-        new LeaderAndIsrPartitionState()
-          .setControllerEpoch(controllerEpoch)
-          .setLeader(brokerId)
-          .setLeaderEpoch(leaderEpoch)
-          .setIsr(isr)
-          .setZkVersion(1)
-          .setReplicas(replicas)
-          .setIsNew(true),
-        offsetCheckpoints, None), "Expected become leader transition to succeed")
-    assertEquals(Set(brokerId, remoteBrokerId), partition.inSyncReplicaIds)
+
+    assertTrue(makeLeader(
+      topicId = None,
+      controllerEpoch = controllerEpoch,
+      leaderEpoch = leaderEpoch,
+      isr = isr,
+      replicas = replicas,
+      zkVersion = 1,
+      isNew = true
+    ))
     assertEquals(0L, partition.localLogOrException.highWatermark)
 
     val remoteReplica = partition.getReplica(remoteBrokerId).get
@@ -1495,21 +1544,19 @@ class PartitionTest extends AbstractPartitionTest {
     val controllerEpoch = 0
     val leaderEpoch = 5
     val remoteBrokerId = brokerId + 1
-    val replicas = List[Integer](brokerId, remoteBrokerId).asJava
-    val isr = List[Integer](brokerId).asJava
-
-    partition.createLogIfNotExists(isNew = false, isFutureReplica = false, offsetCheckpoints, None)
-    assertTrue(partition.makeLeader(
-        new LeaderAndIsrPartitionState()
-          .setControllerEpoch(controllerEpoch)
-          .setLeader(brokerId)
-          .setLeaderEpoch(leaderEpoch)
-          .setIsr(isr)
-          .setZkVersion(1)
-          .setReplicas(replicas)
-          .setIsNew(true),
-        offsetCheckpoints, None), "Expected become leader transition to succeed")
-    assertEquals(Set(brokerId), partition.isrState.isr)
+    val replicas = Seq(brokerId, remoteBrokerId)
+    val isr = Seq(brokerId)
+
+    assertTrue(makeLeader(
+      topicId = None,
+      controllerEpoch = controllerEpoch,
+      leaderEpoch = leaderEpoch,
+      isr = isr,
+      replicas = replicas,
+      zkVersion = 1,
+      isNew = true
+    ))
+    assertEquals(10L, partition.localLogOrException.highWatermark)
 
     val remoteReplica = partition.getReplica(remoteBrokerId).get
     assertEquals(LogOffsetMetadata.UnknownOffsetMetadata.messageOffset, remoteReplica.logEndOffset)
@@ -1544,23 +1591,20 @@ class PartitionTest extends AbstractPartitionTest {
     val follower1 = brokerId + 1
     val follower2 = brokerId + 2
     val follower3 = brokerId + 3
-    val replicas = List[Integer](brokerId, follower1, follower2, follower3).asJava
-    val isr = List[Integer](brokerId, follower1, follower2).asJava
+    val replicas = Seq(brokerId, follower1, follower2, follower3)
+    val isr = Seq(brokerId, follower1, follower2)
 
     doNothing().when(delayedOperations).checkAndCompleteAll()
 
-    partition.createLogIfNotExists(isNew = false, isFutureReplica = false, offsetCheckpoints, None)
-    assertTrue(partition.makeLeader(
-        new LeaderAndIsrPartitionState()
-          .setControllerEpoch(controllerEpoch)
-          .setLeader(brokerId)
-          .setLeaderEpoch(leaderEpoch)
-          .setIsr(isr)
-          .setZkVersion(1)
-          .setReplicas(replicas)
-          .setIsNew(true),
-        offsetCheckpoints, None), "Expected become leader transition to succeed")
-    assertEquals(Set(brokerId, follower1, follower2), partition.isrState.isr)
+    assertTrue(makeLeader(
+      topicId = None,
+      controllerEpoch = controllerEpoch,
+      leaderEpoch = leaderEpoch,
+      isr = isr,
+      replicas = replicas,
+      zkVersion = 1,
+      isNew = true
+    ))
     assertEquals(0L, partition.localLogOrException.highWatermark)
 
     // Expand ISR
@@ -1609,23 +1653,21 @@ class PartitionTest extends AbstractPartitionTest {
     val follower1 = brokerId + 1
     val follower2 = brokerId + 2
     val follower3 = brokerId + 3
-    val replicas = List[Integer](brokerId, follower1, follower2, follower3).asJava
-    val isr = List[Integer](brokerId, follower1, follower2).asJava
+    val replicas = Seq(brokerId, follower1, follower2, follower3)
+    val isr = Seq(brokerId, follower1, follower2)
 
     doNothing().when(delayedOperations).checkAndCompleteAll()
 
-    partition.createLogIfNotExists(isNew = false, isFutureReplica = false, offsetCheckpoints, None)
-    assertTrue(partition.makeLeader(
-        new LeaderAndIsrPartitionState()
-          .setControllerEpoch(controllerEpoch)
-          .setLeader(brokerId)
-          .setLeaderEpoch(leaderEpoch)
-          .setIsr(isr)
-          .setZkVersion(1)
-          .setReplicas(replicas)
-          .setIsNew(true),
-        offsetCheckpoints, None), "Expected become leader transition to succeed")
-    assertEquals(Set(brokerId, follower1, follower2), partition.isrState.isr)
+    assertTrue(makeLeader(
+      partition = partition,
+      topicId = None,
+      controllerEpoch = controllerEpoch,
+      leaderEpoch = leaderEpoch,
+      isr = isr,
+      replicas = replicas,
+      zkVersion = 1,
+      isNew = true
+    ))
     assertEquals(0L, partition.localLogOrException.highWatermark)
 
     // Expand ISR
@@ -1960,6 +2002,44 @@ class PartitionTest extends AbstractPartitionTest {
     verify(spyConfigRepository, times(2)).topicConfig(topicPartition.topic())
   }
 
+  private def makeLeader(
+    topicId: Option[Uuid],
+    controllerEpoch: Int,
+    leaderEpoch: Int,
+    isr: Seq[Int],
+    replicas: Seq[Int],
+    zkVersion: Int,
+    isNew: Boolean,
+    partition: Partition = partition
+  ): Boolean = {
+    partition.createLogIfNotExists(
+      isNew = isNew,
+      isFutureReplica = false,
+      offsetCheckpoints,
+      topicId
+    )
+    val newLeader = partition.makeLeader(
+      new LeaderAndIsrPartitionState()
+        .setControllerEpoch(controllerEpoch)
+        .setLeader(brokerId)
+        .setLeaderEpoch(leaderEpoch)
+        .setIsr(isr.map(Int.box).asJava)
+        .setZkVersion(zkVersion)
+        .setReplicas(replicas.map(Int.box).asJava)
+        .setIsNew(isNew),
+      offsetCheckpoints,
+      topicId
+    )
+    assertTrue(partition.isLeader)
+    assertFalse(partition.isrState.isInflight)
+    assertEquals(topicId, partition.topicId)
+    assertEquals(leaderEpoch, partition.getLeaderEpoch)
+    assertEquals(isr.toSet, partition.isrState.isr)
+    assertEquals(isr.toSet, partition.isrState.maximalIsr)
+    assertEquals(zkVersion, partition.getZkVersion)
+    newLeader
+  }
+
   private def seedLogData(log: UnifiedLog, numRecords: Int, leaderEpoch: Int): Unit = {
     for (i <- 0 until numRecords) {
       val records = MemoryRecords.withRecords(0L, CompressionType.NONE, leaderEpoch,
diff --git a/core/src/test/scala/unit/kafka/utils/TestUtils.scala b/core/src/test/scala/unit/kafka/utils/TestUtils.scala
index a03535d630..5620bf7f52 100755
--- a/core/src/test/scala/unit/kafka/utils/TestUtils.scala
+++ b/core/src/test/scala/unit/kafka/utils/TestUtils.scala
@@ -1130,7 +1130,7 @@ object TestUtils extends Logging {
 
     def completeIsrUpdate(newZkVersion: Int): Unit = {
       if (inFlight.compareAndSet(true, false)) {
-        val item = isrUpdates.head
+        val item = isrUpdates.dequeue()
         item.callback.apply(Right(item.leaderAndIsr.withZkVersion(newZkVersion)))
       } else {
         fail("Expected an in-flight ISR update, but there was none")
