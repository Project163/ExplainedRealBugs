diff --git a/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamImplJoin.java b/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamImplJoin.java
index 27fce0418a..2327cb421b 100644
--- a/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamImplJoin.java
+++ b/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamImplJoin.java
@@ -29,22 +29,21 @@ import org.apache.kafka.streams.kstream.internals.graph.GraphNode;
 import org.apache.kafka.streams.kstream.internals.graph.ProcessorGraphNode;
 import org.apache.kafka.streams.kstream.internals.graph.ProcessorParameters;
 import org.apache.kafka.streams.kstream.internals.graph.StreamStreamJoinNode;
-import org.apache.kafka.streams.state.internals.KeyAndJoinSide;
+import org.apache.kafka.streams.state.KeyValueStore;
+import org.apache.kafka.streams.state.internals.TimestampedKeyAndJoinSide;
 import org.apache.kafka.streams.state.StoreBuilder;
 import org.apache.kafka.streams.state.Stores;
 import org.apache.kafka.streams.state.internals.LeftOrRightValue;
 import org.apache.kafka.streams.state.WindowBytesStoreSupplier;
 import org.apache.kafka.streams.state.WindowStore;
-import org.apache.kafka.streams.state.internals.KeyAndJoinSideSerde;
-import org.apache.kafka.streams.state.internals.RocksDbWindowBytesStoreSupplier;
-import org.apache.kafka.streams.state.internals.TimeOrderedWindowStoreBuilder;
+import org.apache.kafka.streams.state.internals.TimestampedKeyAndJoinSideSerde;
+import org.apache.kafka.streams.state.internals.ListValueStoreBuilder;
 import org.apache.kafka.streams.state.internals.LeftOrRightValueSerde;
 
 import java.time.Duration;
 import java.util.Arrays;
 import java.util.HashSet;
 import java.util.Map;
-import java.util.Objects;
 import java.util.Optional;
 import java.util.Set;
 
@@ -153,7 +152,7 @@ class KStreamImplJoin {
         final ProcessorGraphNode<K1, V2> otherWindowedStreamsNode = new ProcessorGraphNode<>(otherWindowStreamProcessorName, otherWindowStreamProcessorParams);
         builder.addGraphNode(otherGraphNode, otherWindowedStreamsNode);
 
-        Optional<StoreBuilder<WindowStore<KeyAndJoinSide<K1>, LeftOrRightValue<V1, V2>>>> outerJoinWindowStore = Optional.empty();
+        Optional<StoreBuilder<KeyValueStore<TimestampedKeyAndJoinSide<K1>, LeftOrRightValue<V1, V2>>>> outerJoinWindowStore = Optional.empty();
         if (leftOuter) {
             outerJoinWindowStore = Optional.of(sharedOuterJoinWindowStoreBuilder(windows, streamJoinedInternal, joinThisGeneratedName));
         }
@@ -280,85 +279,51 @@ class KStreamImplJoin {
         }
     }
 
-    @SuppressWarnings("unchecked")
-    private <K, V1, V2> StoreBuilder<WindowStore<KeyAndJoinSide<K>, LeftOrRightValue<V1, V2>>> sharedOuterJoinWindowStoreBuilder(final JoinWindows windows,
-                                                                                                                                 final StreamJoinedInternal<K, V1, V2> streamJoinedInternal,
-                                                                                                                                 final String joinThisGeneratedName) {
+    private <K, V1, V2> StoreBuilder<KeyValueStore<TimestampedKeyAndJoinSide<K>, LeftOrRightValue<V1, V2>>> sharedOuterJoinWindowStoreBuilder(final JoinWindows windows,
+                                                                                                                                              final StreamJoinedInternal<K, V1, V2> streamJoinedInternal,
+                                                                                                                                              final String joinThisGeneratedName) {
         final boolean persistent = streamJoinedInternal.thisStoreSupplier() == null || streamJoinedInternal.thisStoreSupplier().get().persistent();
-        final String storeName = buildOuterJoinWindowStoreName(streamJoinedInternal, joinThisGeneratedName);
-
-        final KeyAndJoinSideSerde keyAndJoinSideSerde = new KeyAndJoinSideSerde<>(streamJoinedInternal.keySerde());
-        final LeftOrRightValueSerde leftOrRightValueSerde = new LeftOrRightValueSerde(streamJoinedInternal.valueSerde(), streamJoinedInternal.otherValueSerde());
-
-        final StoreBuilder<WindowStore<KeyAndJoinSide<K>, LeftOrRightValue<V1, V2>>> builder;
-        if (persistent) {
-            builder = new TimeOrderedWindowStoreBuilder<KeyAndJoinSide<K>, LeftOrRightValue<V1, V2>>(
-                persistentTimeOrderedWindowStore(
-                    storeName + "-store",
-                    Duration.ofMillis(windows.size() + windows.gracePeriodMs()),
-                    Duration.ofMillis(windows.size())
-                ),
-                keyAndJoinSideSerde,
-                leftOrRightValueSerde,
-                Time.SYSTEM
-            );
-        } else {
-            builder = Stores.windowStoreBuilder(
-                Stores.inMemoryWindowStore(
-                    storeName + "-store",
-                    Duration.ofMillis(windows.size() + windows.gracePeriodMs()),
-                    Duration.ofMillis(windows.size()),
-                    false
-                ),
-                keyAndJoinSideSerde,
-                leftOrRightValueSerde
-            );
-        }
-
-        if (streamJoinedInternal.loggingEnabled()) {
-            builder.withLoggingEnabled(streamJoinedInternal.logConfig());
-        } else {
-            builder.withLoggingDisabled();
-        }
-
-        return builder;
-    }
+        final String storeName = buildOuterJoinWindowStoreName(streamJoinedInternal, joinThisGeneratedName) + "-store";
 
-    // This method has same code as Store.persistentWindowStore(). But TimeOrderedWindowStore is
-    // a non-public API, so we need to keep duplicate code until it becomes public.
-    private static WindowBytesStoreSupplier persistentTimeOrderedWindowStore(final String storeName,
-                                                                             final Duration retentionPeriod,
-                                                                             final Duration windowSize) {
-        Objects.requireNonNull(storeName, "name cannot be null");
+        // we are using a key-value store with list-values for the shared store, and have the window retention / grace period
+        // handled totally on the processor node level, and hence here we are only validating these values but not using them at all
+        final Duration retentionPeriod = Duration.ofMillis(windows.size() + windows.gracePeriodMs());
+        final Duration windowSize = Duration.ofMillis(windows.size());
         final String rpMsgPrefix = prepareMillisCheckFailMsgPrefix(retentionPeriod, "retentionPeriod");
         final long retentionMs = validateMillisecondDuration(retentionPeriod, rpMsgPrefix);
         final String wsMsgPrefix = prepareMillisCheckFailMsgPrefix(windowSize, "windowSize");
         final long windowSizeMs = validateMillisecondDuration(windowSize, wsMsgPrefix);
 
-        final long segmentInterval = Math.max(retentionMs / 2, 60_000L);
-
         if (retentionMs < 0L) {
             throw new IllegalArgumentException("retentionPeriod cannot be negative");
         }
         if (windowSizeMs < 0L) {
             throw new IllegalArgumentException("windowSize cannot be negative");
         }
-        if (segmentInterval < 1L) {
-            throw new IllegalArgumentException("segmentInterval cannot be zero or negative");
-        }
         if (windowSizeMs > retentionMs) {
             throw new IllegalArgumentException("The retention period of the window store "
-                + storeName + " must be no smaller than its window size. Got size=["
-                + windowSizeMs + "], retention=[" + retentionMs + "]");
+                    + storeName + " must be no smaller than its window size. Got size=["
+                    + windowSizeMs + "], retention=[" + retentionMs + "]");
         }
 
-        return new RocksDbWindowBytesStoreSupplier(
-            storeName,
-            retentionMs,
-            segmentInterval,
-            windowSizeMs,
-            true,
-            RocksDbWindowBytesStoreSupplier.WindowStoreTypes.TIME_ORDERED_WINDOW_STORE);
+        final TimestampedKeyAndJoinSideSerde<K> timestampedKeyAndJoinSideSerde = new TimestampedKeyAndJoinSideSerde<>(streamJoinedInternal.keySerde());
+        final LeftOrRightValueSerde<V1, V2> leftOrRightValueSerde = new LeftOrRightValueSerde<>(streamJoinedInternal.valueSerde(), streamJoinedInternal.otherValueSerde());
+
+        final StoreBuilder<KeyValueStore<TimestampedKeyAndJoinSide<K>, LeftOrRightValue<V1, V2>>> builder =
+            new ListValueStoreBuilder<>(
+                persistent ? Stores.persistentKeyValueStore(storeName) : Stores.inMemoryKeyValueStore(storeName),
+                timestampedKeyAndJoinSideSerde,
+                leftOrRightValueSerde,
+                Time.SYSTEM
+            );
+
+        if (streamJoinedInternal.loggingEnabled()) {
+            builder.withLoggingEnabled(streamJoinedInternal.logConfig());
+        } else {
+            builder.withLoggingDisabled();
+        }
+
+        return builder;
     }
 
     private static <K, V> StoreBuilder<WindowStore<K, V>> joinWindowStoreBuilderFromSupplier(final WindowBytesStoreSupplier storeSupplier,
diff --git a/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamKStreamJoin.java b/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamKStreamJoin.java
index 196cd9a864..c9f4ac8a92 100644
--- a/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamKStreamJoin.java
+++ b/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamKStreamJoin.java
@@ -19,14 +19,14 @@ package org.apache.kafka.streams.kstream.internals;
 import org.apache.kafka.common.metrics.Sensor;
 import org.apache.kafka.streams.KeyValue;
 import org.apache.kafka.streams.kstream.ValueJoinerWithKey;
-import org.apache.kafka.streams.kstream.Windowed;
 import org.apache.kafka.streams.kstream.internals.KStreamImplJoin.TimeTracker;
 import org.apache.kafka.streams.processor.To;
 import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;
 import org.apache.kafka.streams.state.KeyValueIterator;
+import org.apache.kafka.streams.state.KeyValueStore;
 import org.apache.kafka.streams.state.WindowStore;
 import org.apache.kafka.streams.state.WindowStoreIterator;
-import org.apache.kafka.streams.state.internals.KeyAndJoinSide;
+import org.apache.kafka.streams.state.internals.TimestampedKeyAndJoinSide;
 import org.apache.kafka.streams.state.internals.LeftOrRightValue;
 import org.apache.kafka.streams.StreamsConfig;
 import org.slf4j.Logger;
@@ -37,7 +37,7 @@ import java.util.Optional;
 import static org.apache.kafka.streams.StreamsConfig.InternalConfig.EMIT_INTERVAL_MS_KSTREAMS_OUTER_JOIN_SPURIOUS_RESULTS_FIX;
 import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensor;
 
-@SuppressWarnings("deprecation") // Old PAPI. Needs to be migrated.
+@SuppressWarnings({"deprecation", "unchecked"}) // Old PAPI. Needs to be migrated.
 class KStreamKStreamJoin<K, R, V1, V2> implements org.apache.kafka.streams.processor.ProcessorSupplier<K, V1> {
     private static final Logger LOG = LoggerFactory.getLogger(KStreamKStreamJoin.class);
 
@@ -47,10 +47,10 @@ class KStreamKStreamJoin<K, R, V1, V2> implements org.apache.kafka.streams.proce
     private final long joinGraceMs;
     private final boolean enableSpuriousResultFix;
 
-    private final ValueJoinerWithKey<? super K, ? super V1, ? super V2, ? extends R> joiner;
     private final boolean outer;
-    private final Optional<String> outerJoinWindowName;
     private final boolean isLeftSide;
+    private final Optional<String> outerJoinWindowName;
+    private final ValueJoinerWithKey<? super K, ? super V1, ? super V2, ? extends R> joiner;
 
     private final TimeTracker sharedTimeTracker;
 
@@ -86,7 +86,7 @@ class KStreamKStreamJoin<K, R, V1, V2> implements org.apache.kafka.streams.proce
     private class KStreamKStreamJoinProcessor extends org.apache.kafka.streams.processor.AbstractProcessor<K, V1> {
         private WindowStore<K, V2> otherWindowStore;
         private Sensor droppedRecordsSensor;
-        private Optional<WindowStore<KeyAndJoinSide<K>, LeftOrRightValue>> outerJoinWindowStore = Optional.empty();
+        private Optional<KeyValueStore<TimestampedKeyAndJoinSide<K>, LeftOrRightValue<V1, V2>>> outerJoinStore = Optional.empty();
 
         @Override
         public void init(final org.apache.kafka.streams.processor.ProcessorContext context) {
@@ -96,7 +96,7 @@ class KStreamKStreamJoin<K, R, V1, V2> implements org.apache.kafka.streams.proce
             otherWindowStore = context.getStateStore(otherWindowName);
 
             if (enableSpuriousResultFix) {
-                outerJoinWindowStore = outerJoinWindowName.map(context::getStateStore);
+                outerJoinStore = outerJoinWindowName.map(context::getStateStore);
 
                 sharedTimeTracker.setEmitInterval(
                     StreamsConfig.InternalConfig.getLong(
@@ -135,7 +135,7 @@ class KStreamKStreamJoin<K, R, V1, V2> implements org.apache.kafka.streams.proce
 
             // Emit all non-joined records which window has closed
             if (inputRecordTimestamp == sharedTimeTracker.streamTime) {
-                outerJoinWindowStore.ifPresent(this::emitNonJoinedOuterRecords);
+                outerJoinStore.ifPresent(this::emitNonJoinedOuterRecords);
             }
 
             try (final WindowStoreIterator<V2> iter = otherWindowStore.fetch(key, timeFrom, timeTo)) {
@@ -144,9 +144,13 @@ class KStreamKStreamJoin<K, R, V1, V2> implements org.apache.kafka.streams.proce
                     final KeyValue<Long, V2> otherRecord = iter.next();
                     final long otherRecordTimestamp = otherRecord.key;
 
-                    outerJoinWindowStore.ifPresent(store -> {
-                        // Delete the joined record from the non-joined outer window store
-                        store.put(KeyAndJoinSide.make(!isLeftSide, key), null, otherRecordTimestamp);
+                    outerJoinStore.ifPresent(store -> {
+                        // use putIfAbsent to first read and see if there's any values for the key,
+                        // if yes delete the key, otherwise do not issue a put;
+                        // we may delete some values with the same key early but since we are going
+                        // range over all values of the same key even after failure, since the other window-store
+                        // is only cleaned up by stream time, so this is okay for at-least-once.
+                        store.putIfAbsent(TimestampedKeyAndJoinSide.make(!isLeftSide, key, otherRecordTimestamp), null);
                     });
 
                     context().forward(
@@ -173,21 +177,19 @@ class KStreamKStreamJoin<K, R, V1, V2> implements org.apache.kafka.streams.proce
                     //
                     // This condition below allows us to process the out-of-order records without the need
                     // to hold it in the temporary outer store
-                    if (!outerJoinWindowStore.isPresent() || timeTo < sharedTimeTracker.streamTime) {
+                    if (!outerJoinStore.isPresent() || timeTo < sharedTimeTracker.streamTime) {
                         context().forward(key, joiner.apply(key, value, null));
                     } else {
                         sharedTimeTracker.updatedMinTime(inputRecordTimestamp);
-                        outerJoinWindowStore.ifPresent(store -> store.put(
-                            KeyAndJoinSide.make(isLeftSide, key),
-                            LeftOrRightValue.make(isLeftSide, value),
-                            inputRecordTimestamp));
+                        outerJoinStore.ifPresent(store -> store.put(
+                            TimestampedKeyAndJoinSide.make(isLeftSide, key, inputRecordTimestamp),
+                            LeftOrRightValue.make(isLeftSide, value)));
                     }
                 }
             }
         }
 
-        @SuppressWarnings("unchecked")
-        private void emitNonJoinedOuterRecords(final WindowStore<KeyAndJoinSide<K>, LeftOrRightValue> store) {
+        private void emitNonJoinedOuterRecords(final KeyValueStore<TimestampedKeyAndJoinSide<K>, LeftOrRightValue<V1, V2>> store) {
             // calling `store.all()` creates an iterator what is an expensive operation on RocksDB;
             // to reduce runtime cost, we try to avoid paying those cost
 
@@ -209,37 +211,51 @@ class KStreamKStreamJoin<K, R, V1, V2> implements org.apache.kafka.streams.proce
             // reset to MAX_VALUE in case the store is empty
             sharedTimeTracker.minTime = Long.MAX_VALUE;
 
-            try (final KeyValueIterator<Windowed<KeyAndJoinSide<K>>, LeftOrRightValue> it = store.all()) {
+            try (final KeyValueIterator<TimestampedKeyAndJoinSide<K>, LeftOrRightValue<V1, V2>> it = store.all()) {
+                TimestampedKeyAndJoinSide<K> prevKey = null;
+
                 while (it.hasNext()) {
-                    final KeyValue<Windowed<KeyAndJoinSide<K>>, LeftOrRightValue> record = it.next();
+                    final KeyValue<TimestampedKeyAndJoinSide<K>, LeftOrRightValue<V1, V2>> record = it.next();
 
-                    final Windowed<KeyAndJoinSide<K>> windowedKey = record.key;
-                    final LeftOrRightValue value = record.value;
-                    sharedTimeTracker.minTime = windowedKey.window().start();
+                    final TimestampedKeyAndJoinSide<K> timestampedKeyAndJoinSide = record.key;
+                    final LeftOrRightValue<V1, V2> value = record.value;
+                    final K key = timestampedKeyAndJoinSide.getKey();
+                    final long timestamp = timestampedKeyAndJoinSide.getTimestamp();
+                    sharedTimeTracker.minTime = timestamp;
 
                     // Skip next records if window has not closed
-                    if (windowedKey.window().start() + joinAfterMs + joinGraceMs >= sharedTimeTracker.streamTime) {
+                    if (timestamp + joinAfterMs + joinGraceMs >= sharedTimeTracker.streamTime) {
                         break;
                     }
 
-                    final K key = windowedKey.key().getKey();
-                    final long time = windowedKey.window().start();
-
                     final R nullJoinedValue;
                     if (isLeftSide) {
                         nullJoinedValue = joiner.apply(key,
-                            (V1) value.getLeftValue(),
-                            (V2) value.getRightValue());
+                                value.getLeftValue(),
+                                value.getRightValue());
                     } else {
                         nullJoinedValue = joiner.apply(key,
-                            (V1) value.getRightValue(),
-                            (V2) value.getLeftValue());
+                                (V1) value.getRightValue(),
+                                (V2) value.getLeftValue());
                     }
 
-                    context().forward(key, nullJoinedValue, To.all().withTimestamp(time));
+                    context().forward(key, nullJoinedValue, To.all().withTimestamp(timestamp));
+
+                    if (prevKey != null && !prevKey.equals(timestampedKeyAndJoinSide)) {
+                        // blind-delete the previous key from the outer window store now it is emitted;
+                        // we do this because this delete would remove the whole list of values of the same key,
+                        // and hence if we delete eagerly and then fail, we would miss emitting join results of the later
+                        // values in the list.
+                        // we do not use delete() calls since it would incur extra get()
+                        store.put(prevKey, null);
+                    }
+
+                    prevKey = timestampedKeyAndJoinSide;
+                }
 
-                    // Delete the key from the outer window store now it is emitted
-                    store.put(record.key.key(), null, record.key.window().start());
+                // at the end of the iteration, we need to delete the last key
+                if (prevKey != null) {
+                    store.put(prevKey, null);
                 }
             }
         }
diff --git a/streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/StreamStreamJoinNode.java b/streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/StreamStreamJoinNode.java
index 40bfff047d..48f95873b5 100644
--- a/streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/StreamStreamJoinNode.java
+++ b/streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/StreamStreamJoinNode.java
@@ -20,9 +20,10 @@ package org.apache.kafka.streams.kstream.internals.graph;
 import org.apache.kafka.streams.kstream.Joined;
 import org.apache.kafka.streams.kstream.ValueJoinerWithKey;
 import org.apache.kafka.streams.processor.internals.InternalTopologyBuilder;
+import org.apache.kafka.streams.state.KeyValueStore;
 import org.apache.kafka.streams.state.StoreBuilder;
 import org.apache.kafka.streams.state.WindowStore;
-import org.apache.kafka.streams.state.internals.KeyAndJoinSide;
+import org.apache.kafka.streams.state.internals.TimestampedKeyAndJoinSide;
 import org.apache.kafka.streams.state.internals.LeftOrRightValue;
 
 import java.util.Optional;
@@ -35,7 +36,7 @@ public class StreamStreamJoinNode<K, V1, V2, VR> extends BaseJoinProcessorNode<K
     private final ProcessorParameters<K, V2, ?, ?> otherWindowedStreamProcessorParameters;
     private final StoreBuilder<WindowStore<K, V1>> thisWindowStoreBuilder;
     private final StoreBuilder<WindowStore<K, V2>> otherWindowStoreBuilder;
-    private final Optional<StoreBuilder<WindowStore<KeyAndJoinSide<K>, LeftOrRightValue<V1, V2>>>> outerJoinWindowStoreBuilder;
+    private final Optional<StoreBuilder<KeyValueStore<TimestampedKeyAndJoinSide<K>, LeftOrRightValue<V1, V2>>>> outerJoinWindowStoreBuilder;
     private final Joined<K, V1, V2> joined;
     private final boolean enableSpuriousResultFix;
 
@@ -48,7 +49,7 @@ public class StreamStreamJoinNode<K, V1, V2, VR> extends BaseJoinProcessorNode<K
                                  final ProcessorParameters<K, V2, ?, ?> otherWindowedStreamProcessorParameters,
                                  final StoreBuilder<WindowStore<K, V1>> thisWindowStoreBuilder,
                                  final StoreBuilder<WindowStore<K, V2>> otherWindowStoreBuilder,
-                                 final Optional<StoreBuilder<WindowStore<KeyAndJoinSide<K>, LeftOrRightValue<V1, V2>>>> outerJoinWindowStoreBuilder,
+                                 final Optional<StoreBuilder<KeyValueStore<TimestampedKeyAndJoinSide<K>, LeftOrRightValue<V1, V2>>>> outerJoinWindowStoreBuilder,
                                  final Joined<K, V1, V2> joined,
                                  final boolean enableSpuriousResultFix) {
 
@@ -117,7 +118,7 @@ public class StreamStreamJoinNode<K, V1, V2, VR> extends BaseJoinProcessorNode<K
         private ProcessorParameters<K, V2, ?, ?> otherWindowedStreamProcessorParameters;
         private StoreBuilder<WindowStore<K, V1>> thisWindowStoreBuilder;
         private StoreBuilder<WindowStore<K, V2>> otherWindowStoreBuilder;
-        private Optional<StoreBuilder<WindowStore<KeyAndJoinSide<K>, LeftOrRightValue<V1, V2>>>> outerJoinWindowStoreBuilder;
+        private Optional<StoreBuilder<KeyValueStore<TimestampedKeyAndJoinSide<K>, LeftOrRightValue<V1, V2>>>> outerJoinWindowStoreBuilder;
         private Joined<K, V1, V2> joined;
         private boolean enableSpuriousResultFix = false;
 
@@ -170,7 +171,7 @@ public class StreamStreamJoinNode<K, V1, V2, VR> extends BaseJoinProcessorNode<K
             return this;
         }
 
-        public StreamStreamJoinNodeBuilder<K, V1, V2, VR> withOuterJoinWindowStoreBuilder(final Optional<StoreBuilder<WindowStore<KeyAndJoinSide<K>, LeftOrRightValue<V1, V2>>>> outerJoinWindowStoreBuilder) {
+        public StreamStreamJoinNodeBuilder<K, V1, V2, VR> withOuterJoinWindowStoreBuilder(final Optional<StoreBuilder<KeyValueStore<TimestampedKeyAndJoinSide<K>, LeftOrRightValue<V1, V2>>>> outerJoinWindowStoreBuilder) {
             this.outerJoinWindowStoreBuilder = outerJoinWindowStoreBuilder;
             return this;
         }
diff --git a/streams/src/main/java/org/apache/kafka/streams/state/StateSerdes.java b/streams/src/main/java/org/apache/kafka/streams/state/StateSerdes.java
index 1182e50a88..f9f0bdcdea 100644
--- a/streams/src/main/java/org/apache/kafka/streams/state/StateSerdes.java
+++ b/streams/src/main/java/org/apache/kafka/streams/state/StateSerdes.java
@@ -33,6 +33,9 @@ import java.util.Objects;
  */
 public final class StateSerdes<K, V> {
 
+    public static final int TIMESTAMP_SIZE = 8;
+    public static final int BOOLEAN_SIZE = 1;
+
     /**
      * Create a new instance of {@link StateSerdes} for the given state name and key-/value-type classes.
      *
diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/ChangeLoggingListValueBytesStore.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/ChangeLoggingListValueBytesStore.java
new file mode 100644
index 0000000000..d2af28dd22
--- /dev/null
+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/ChangeLoggingListValueBytesStore.java
@@ -0,0 +1,62 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.state.internals;
+
+import org.apache.kafka.common.utils.Bytes;
+import org.apache.kafka.streams.state.KeyValueStore;
+
+public class ChangeLoggingListValueBytesStore extends ChangeLoggingKeyValueBytesStore {
+
+    ChangeLoggingListValueBytesStore(final KeyValueStore<Bytes, byte[]> inner) {
+        super(inner);
+    }
+
+    @Override
+    public void put(final Bytes key, final byte[] value) {
+        wrapped().put(key, value);
+        // the provided new value will be added to the list in the inner put()
+        // we need to log the full new list and thus call get() on the inner store below
+        // if the value is a tombstone, we delete the whole list and thus can save the get call
+        if (value == null) {
+            log(key, null);
+        } else {
+            log(key, wrapped().get(key));
+        }
+    }
+
+    @Override
+    public byte[] putIfAbsent(final Bytes key, final byte[] value) {
+        final byte[] oldValue = wrapped().putIfAbsent(key, value);
+
+        if (oldValue != null) {
+            // the provided new value will be added to the list in the inner put()
+            // we need to log the full new list and thus call get() on the inner store below
+            // if the value is a tombstone, we delete the whole list and thus can save the get call
+            if (value == null) {
+                log(key, null);
+            } else {
+                log(key, wrapped().get(key));
+            }
+        }
+
+        // TODO: here we always return null so that deser would not fail.
+        //       we only do this since we know the only caller (stream-stream join processor)
+        //       would not need the actual value at all
+        return null;
+    }
+
+}
diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/LeftOrRightValue.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/LeftOrRightValue.java
index cbdba0796f..bb7b516edd 100644
--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/LeftOrRightValue.java
+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/LeftOrRightValue.java
@@ -19,7 +19,7 @@ package org.apache.kafka.streams.state.internals;
 import java.util.Objects;
 
 /**
- * This class is used in combination of {@link KeyAndJoinSide}. The {@link KeyAndJoinSide} class
+ * This class is used in combination of {@link TimestampedKeyAndJoinSide}. The {@link TimestampedKeyAndJoinSide} class
  * combines a key with a boolean value that specifies if the key is found in the left side of a
  * join or on the right side. This {@link LeftOrRightValue} object contains either the V1 value,
  * which is found in the left topic, or V2 value if it is found in the right topic.
diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/ListValueStore.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/ListValueStore.java
new file mode 100644
index 0000000000..f61c8b1093
--- /dev/null
+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/ListValueStore.java
@@ -0,0 +1,166 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.state.internals;
+
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.List;
+import java.util.NoSuchElementException;
+
+import org.apache.kafka.common.utils.AbstractIterator;
+import org.apache.kafka.common.utils.Bytes;
+import org.apache.kafka.streams.KeyValue;
+import org.apache.kafka.streams.state.KeyValueIterator;
+import org.apache.kafka.streams.state.KeyValueStore;
+import org.apache.kafka.common.serialization.Serdes;
+import org.apache.kafka.common.serialization.Serde;
+
+/**
+ * A wrapper key-value store that serializes the record values bytes as a list.
+ * As a result put calls would be interpreted as a get-append-put to the underlying RocksDB store.
+ * A put(k,null) will still delete the key, ie, the full list of all values of this key.
+ * Range iterators would also flatten the value lists and return the values one-by-one.
+ *
+ * This store is used for cases where we do not want to de-duplicate values of the same keys but want to retain all such values.
+ */
+@SuppressWarnings("unchecked")
+public class ListValueStore
+    extends WrappedStateStore<KeyValueStore<Bytes, byte[]>, Bytes, byte[]>
+    implements KeyValueStore<Bytes, byte[]> {
+
+    static private final Serde<List<byte[]>> LIST_SERDE = Serdes.ListSerde(ArrayList.class, Serdes.ByteArray());
+
+    ListValueStore(final KeyValueStore<Bytes, byte[]> bytesStore) {
+        super(bytesStore);
+    }
+
+    @Override
+    public void put(final Bytes key, final byte[] addedValue) {
+        // if the value is null we can skip the get and blind delete
+        if (addedValue == null) {
+            wrapped().put(key, null);
+        } else {
+            final byte[] oldValue = wrapped().get(key);
+            putInternal(key, addedValue, oldValue);
+        }
+    }
+
+    @Override
+    public byte[] putIfAbsent(final Bytes key, final byte[] addedValue) {
+        final byte[] oldValue = wrapped().get(key);
+
+        if (oldValue != null) {
+            // if the value is null we can skip the get and blind delete
+            if (addedValue == null) {
+                wrapped().put(key, null);
+            } else {
+                putInternal(key, addedValue, oldValue);
+            }
+        }
+
+        return oldValue;
+    }
+
+    // this function assumes the addedValue is not null; callers should check null themselves
+    private void putInternal(final Bytes key, final byte[] addedValue, final byte[] oldValue) {
+        if (oldValue == null) {
+            wrapped().put(key, LIST_SERDE.serializer().serialize(null, Collections.singletonList(addedValue)));
+        } else {
+            final List<byte[]> list = LIST_SERDE.deserializer().deserialize(null, oldValue);
+            list.add(addedValue);
+
+            wrapped().put(key, LIST_SERDE.serializer().serialize(null, list));
+        }
+    }
+
+    @Override
+    public void putAll(final List<KeyValue<Bytes, byte[]>> entries) {
+        throw new UnsupportedOperationException("putAll not supported");
+    }
+
+    @Override
+    public byte[] delete(final Bytes key) {
+        // we intentionally disable delete calls since the returned bytes would
+        // represent a list, not a single value; we need to have a new API for delete if we do need it
+        throw new UnsupportedOperationException("delete not supported");
+    }
+
+    @Override
+    public byte[] get(final Bytes key) {
+        return wrapped().get(key);
+    }
+
+    @Override
+    public KeyValueIterator<Bytes, byte[]> range(final Bytes from, final Bytes to) {
+        throw new UnsupportedOperationException("range not supported");
+    }
+
+    @Override
+    public KeyValueIterator<Bytes, byte[]> all() {
+        return new ValueListIterator(wrapped().all());
+    }
+
+    @Override
+    public long approximateNumEntries() {
+        return wrapped().approximateNumEntries();
+    }
+
+    private static class ValueListIterator extends AbstractIterator<KeyValue<Bytes, byte[]>>
+        implements KeyValueIterator<Bytes, byte[]> {
+
+        private final KeyValueIterator<Bytes, byte[]> bytesIterator;
+        private final List<byte[]> currList = new ArrayList<>();
+        private KeyValue<Bytes, byte[]> next;
+        private Bytes nextKey;
+
+        ValueListIterator(final KeyValueIterator<Bytes, byte[]> bytesIterator) {
+            this.bytesIterator = bytesIterator;
+        }
+
+        @Override
+        public Bytes peekNextKey() {
+            if (!hasNext()) {
+                throw new NoSuchElementException();
+            }
+            return next.key;
+        }
+
+        @Override
+        public KeyValue<Bytes, byte[]> makeNext() {
+            while (currList.isEmpty() && bytesIterator.hasNext()) {
+                final KeyValue<Bytes, byte[]> next = bytesIterator.next();
+                nextKey = next.key;
+                currList.addAll(LIST_SERDE.deserializer().deserialize(null, next.value));
+            }
+
+            if (currList.isEmpty()) {
+                return allDone();
+            } else {
+                next = KeyValue.pair(nextKey, currList.remove(0));
+                return next;
+            }
+        }
+
+        @Override
+        public void close() {
+            bytesIterator.close();
+            // also need to clear the current list buffer since
+            // otherwise even after close the iter can still return data
+            currList.clear();
+        }
+    }
+}
\ No newline at end of file
diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/ListValueStoreBuilder.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/ListValueStoreBuilder.java
new file mode 100644
index 0000000000..34e2e8b231
--- /dev/null
+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/ListValueStoreBuilder.java
@@ -0,0 +1,63 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.state.internals;
+
+import org.apache.kafka.common.serialization.Serde;
+import org.apache.kafka.common.utils.Bytes;
+import org.apache.kafka.common.utils.Time;
+import org.apache.kafka.streams.state.KeyValueBytesStoreSupplier;
+import org.apache.kafka.streams.state.KeyValueStore;
+
+import java.util.Objects;
+
+public class ListValueStoreBuilder<K, V> extends AbstractStoreBuilder<K, V, KeyValueStore<K, V>> {
+    private final KeyValueBytesStoreSupplier storeSupplier;
+
+    public ListValueStoreBuilder(final KeyValueBytesStoreSupplier storeSupplier,
+                                 final Serde<K> keySerde,
+                                 final Serde<V> valueSerde,
+                                 final Time time) {
+        super(storeSupplier.name(), keySerde, valueSerde, time);
+        Objects.requireNonNull(storeSupplier, "storeSupplier can't be null");
+        Objects.requireNonNull(storeSupplier.metricsScope(), "storeSupplier's metricsScope can't be null");
+        this.storeSupplier = storeSupplier;
+    }
+
+    @Override
+    public KeyValueStore<K, V> build() {
+        return new MeteredKeyValueStore<>(
+                maybeWrapCaching(maybeWrapLogging(new ListValueStore(storeSupplier.get()))),
+                storeSupplier.metricsScope(),
+                time,
+                keySerde,
+                valueSerde);
+    }
+
+    private KeyValueStore<Bytes, byte[]> maybeWrapCaching(final KeyValueStore<Bytes, byte[]> inner) {
+        if (!enableCaching) {
+            return inner;
+        }
+        return new CachingKeyValueStore(inner);
+    }
+
+    private KeyValueStore<Bytes, byte[]> maybeWrapLogging(final KeyValueStore<Bytes, byte[]> inner) {
+        if (!enableLogging) {
+            return inner;
+        }
+        return new ChangeLoggingListValueBytesStore(inner);
+    }
+}
diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBTimeOrderedWindowStore.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBTimeOrderedWindowStore.java
deleted file mode 100644
index 37aaa27e2f..0000000000
--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBTimeOrderedWindowStore.java
+++ /dev/null
@@ -1,204 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License. You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.kafka.streams.state.internals;
-
-import org.apache.kafka.common.utils.Bytes;
-import org.apache.kafka.streams.KeyValue;
-import org.apache.kafka.streams.kstream.Windowed;
-import org.apache.kafka.streams.state.KeyValueIterator;
-import org.apache.kafka.streams.state.WindowStore;
-import org.apache.kafka.streams.state.WindowStoreIterator;
-
-/**
- * A persistent (time-key)-value store based on RocksDB.
- *
- * The store uses the {@link TimeOrderedKeySchema} to serialize the record key bytes to generate the
- * combined (time-key) store key. This key schema is efficient when doing time range queries in
- * the store (i.e. fetchAll(from, to) ).
- *
- * For key range queries, like fetch(key, fromTime, toTime), use the {@link RocksDBWindowStore}
- * which uses the {@link WindowKeySchema} to serialize the record bytes for efficient key queries.
- */
-public class RocksDBTimeOrderedWindowStore
-    extends WrappedStateStore<SegmentedBytesStore, Object, Object>
-    implements WindowStore<Bytes, byte[]> {
-
-    private final boolean retainDuplicates;
-    private final long windowSize;
-
-    private int seqnum = 0;
-
-    RocksDBTimeOrderedWindowStore(final SegmentedBytesStore bytesStore,
-                                  final boolean retainDuplicates,
-                                  final long windowSize) {
-        super(bytesStore);
-        this.retainDuplicates = retainDuplicates;
-        this.windowSize = windowSize;
-    }
-
-    @Override
-    public void put(final Bytes key, final byte[] value, final long timestamp) {
-        if (!(value == null && retainDuplicates)) {
-            maybeUpdateSeqnumForDups();
-            wrapped().put(TimeOrderedKeySchema.toStoreKeyBinary(key, timestamp, seqnum), value);
-        } else {
-            // Delete all duplicates for the specified key and timestamp
-            wrapped().remove(key, timestamp);
-        }
-    }
-
-    @Override
-    public byte[] fetch(final Bytes key, final long timestamp) {
-        throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public WindowStoreIterator<byte[]> fetch(final Bytes key, final long timeFrom, final long timeTo) {
-        throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public WindowStoreIterator<byte[]> backwardFetch(final Bytes key, final long timeFrom, final long timeTo) {
-        throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public KeyValueIterator<Windowed<Bytes>, byte[]> fetch(final Bytes keyFrom,
-                                                           final Bytes keyTo,
-                                                           final long timeFrom,
-                                                           final long timeTo) {
-        throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public KeyValueIterator<Windowed<Bytes>, byte[]> backwardFetch(final Bytes keyFrom,
-                                                                   final Bytes keyTo,
-                                                                   final long timeFrom,
-                                                                   final long timeTo) {
-        throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public KeyValueIterator<Windowed<Bytes>, byte[]> all() {
-        final KeyValueIterator<Bytes, byte[]> bytesIterator = wrapped().all();
-        return new TimeOrderedWindowStoreIteratorWrapper(bytesIterator, windowSize).keyValueIterator();
-    }
-
-    @Override
-    public KeyValueIterator<Windowed<Bytes>, byte[]> backwardAll() {
-        throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public KeyValueIterator<Windowed<Bytes>, byte[]> fetchAll(final long timeFrom, final long timeTo) {
-        throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public KeyValueIterator<Windowed<Bytes>, byte[]> backwardFetchAll(final long timeFrom, final long timeTo) {
-        throw new UnsupportedOperationException();
-    }
-
-    private void maybeUpdateSeqnumForDups() {
-        if (retainDuplicates) {
-            seqnum = (seqnum + 1) & 0x7FFFFFFF;
-        }
-    }
-
-    static class TimeOrderedWindowStoreIteratorWrapper {
-        private final KeyValueIterator<Bytes, byte[]> bytesIterator;
-        private final long windowSize;
-
-        TimeOrderedWindowStoreIteratorWrapper(final KeyValueIterator<Bytes, byte[]> bytesIterator,
-                                              final long windowSize) {
-            this.bytesIterator = bytesIterator;
-            this.windowSize = windowSize;
-        }
-
-        public WindowStoreIterator<byte[]> valuesIterator() {
-            return new WrappedWindowStoreIterator(bytesIterator);
-        }
-
-        public KeyValueIterator<Windowed<Bytes>, byte[]> keyValueIterator() {
-            return new WrappedKeyValueIterator(bytesIterator, windowSize);
-        }
-
-        private static class WrappedWindowStoreIterator implements WindowStoreIterator<byte[]> {
-            final KeyValueIterator<Bytes, byte[]> bytesIterator;
-
-            WrappedWindowStoreIterator(
-                final KeyValueIterator<Bytes, byte[]> bytesIterator) {
-                this.bytesIterator = bytesIterator;
-            }
-
-            @Override
-            public Long peekNextKey() {
-                return TimeOrderedKeySchema.extractStoreTimestamp(bytesIterator.peekNextKey().get());
-            }
-
-            @Override
-            public boolean hasNext() {
-                return bytesIterator.hasNext();
-            }
-
-            @Override
-            public KeyValue<Long, byte[]> next() {
-                final KeyValue<Bytes, byte[]> next = bytesIterator.next();
-                final long timestamp = TimeOrderedKeySchema.extractStoreTimestamp(next.key.get());
-                return KeyValue.pair(timestamp, next.value);
-            }
-
-            @Override
-            public void close() {
-                bytesIterator.close();
-            }
-        }
-
-        private static class WrappedKeyValueIterator implements KeyValueIterator<Windowed<Bytes>, byte[]> {
-            final KeyValueIterator<Bytes, byte[]> bytesIterator;
-            final long windowSize;
-
-            WrappedKeyValueIterator(final KeyValueIterator<Bytes, byte[]> bytesIterator,
-                                    final long windowSize) {
-                this.bytesIterator = bytesIterator;
-                this.windowSize = windowSize;
-            }
-
-            @Override
-            public Windowed<Bytes> peekNextKey() {
-                final byte[] nextKey = bytesIterator.peekNextKey().get();
-                return TimeOrderedKeySchema.fromStoreBytesKey(nextKey, windowSize);
-            }
-
-            @Override
-            public boolean hasNext() {
-                return bytesIterator.hasNext();
-            }
-
-            @Override
-            public KeyValue<Windowed<Bytes>, byte[]> next() {
-                final KeyValue<Bytes, byte[]> next = bytesIterator.next();
-                return KeyValue.pair(TimeOrderedKeySchema.fromStoreBytesKey(next.key.get(), windowSize), next.value);
-            }
-
-            @Override
-            public void close() {
-                bytesIterator.close();
-            }
-        }
-    }
-}
diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDbWindowBytesStoreSupplier.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDbWindowBytesStoreSupplier.java
index 937e70e703..3ee5b88328 100644
--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDbWindowBytesStoreSupplier.java
+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDbWindowBytesStoreSupplier.java
@@ -23,8 +23,7 @@ import org.apache.kafka.streams.state.WindowStore;
 public class RocksDbWindowBytesStoreSupplier implements WindowBytesStoreSupplier {
     public enum WindowStoreTypes {
         DEFAULT_WINDOW_STORE,
-        TIMESTAMPED_WINDOW_STORE,
-        TIME_ORDERED_WINDOW_STORE
+        TIMESTAMPED_WINDOW_STORE
     }
 
     private final String name;
@@ -88,18 +87,6 @@ public class RocksDbWindowBytesStoreSupplier implements WindowBytesStoreSupplier
                         new WindowKeySchema()),
                     retainDuplicates,
                     windowSize);
-            case TIME_ORDERED_WINDOW_STORE:
-                return new RocksDBTimeOrderedWindowStore(
-                    new RocksDBSegmentedBytesStore(
-                        name,
-                        metricsScope(),
-                        retentionPeriod,
-                        segmentInterval,
-                        new TimeOrderedKeySchema()
-                    ),
-                    retainDuplicates,
-                    windowSize
-                );
             default:
                 throw new IllegalArgumentException("invalid window store type: " + windowStoreType);
         }
diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/SessionKeySchema.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/SessionKeySchema.java
index 326b86945f..8bb50e51a8 100644
--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/SessionKeySchema.java
+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/SessionKeySchema.java
@@ -23,13 +23,14 @@ import org.apache.kafka.streams.kstream.Window;
 import org.apache.kafka.streams.kstream.Windowed;
 import org.apache.kafka.streams.kstream.internals.SessionWindow;
 
+import static org.apache.kafka.streams.state.StateSerdes.TIMESTAMP_SIZE;
+
 import java.nio.ByteBuffer;
 import java.util.List;
 
 
 public class SessionKeySchema implements SegmentedBytesStore.KeySchema {
 
-    private static final int TIMESTAMP_SIZE = 8;
     private static final int SUFFIX_SIZE = 2 * TIMESTAMP_SIZE;
     private static final byte[] MIN_SUFFIX = new byte[SUFFIX_SIZE];
 
diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/TimeOrderedKeySchema.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/TimeOrderedKeySchema.java
deleted file mode 100644
index f191d2f708..0000000000
--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/TimeOrderedKeySchema.java
+++ /dev/null
@@ -1,191 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License. You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.kafka.streams.state.internals;
-
-import org.apache.kafka.common.serialization.Deserializer;
-import org.apache.kafka.common.utils.Bytes;
-import org.apache.kafka.streams.kstream.Window;
-import org.apache.kafka.streams.kstream.Windowed;
-import org.apache.kafka.streams.kstream.internals.TimeWindow;
-import org.apache.kafka.streams.state.StateSerdes;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.nio.ByteBuffer;
-import java.util.List;
-
-/**
- * A {@link RocksDBSegmentedBytesStore.KeySchema} to serialize/deserialize a RocksDB store
- * key into a schema combined of (time,key,seq). Since key is variable length while time/seq is
- * fixed length, when formatting in this order, varying time range query would be very inefficient
- * since we'd need to be very conservative in picking the from / to boundaries; however for now
- * we do not expect any varying time range access at all, only fixed time range only.
- */
-public class TimeOrderedKeySchema implements RocksDBSegmentedBytesStore.KeySchema {
-    private static final Logger LOG = LoggerFactory.getLogger(TimeOrderedKeySchema.class);
-
-    private static final int TIMESTAMP_SIZE = 8;
-    private static final int SEQNUM_SIZE = 4;
-
-    @Override
-    public Bytes upperRange(final Bytes key, final long to) {
-        throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public Bytes lowerRange(final Bytes key, final long from) {
-        throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public Bytes toStoreBinaryKeyPrefix(final Bytes key, final long timestamp) {
-        return toStoreKeyBinaryPrefix(key, timestamp);
-    }
-
-    @Override
-    public Bytes upperRangeFixedSize(final Bytes key, final long to) {
-        throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public Bytes lowerRangeFixedSize(final Bytes key, final long from) {
-        throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public long segmentTimestamp(final Bytes key) {
-        return extractStoreTimestamp(key.get());
-    }
-
-    /**
-     * {@inheritdoc}
-     *
-     * This method is optimized for {@link RocksDBTimeOrderedWindowStore#all()} only. Key and time
-     * range queries are not supported.
-     */
-    @Override
-    public HasNextCondition hasNextCondition(final Bytes binaryKeyFrom, final Bytes binaryKeyTo, final long from, final long to) {
-        if (binaryKeyFrom != null || binaryKeyTo != null) {
-            throw new IllegalArgumentException("binaryKeyFrom/binaryKeyTo keys cannot be non-null. Key and time range queries are not supported.");
-        }
-
-        if (from != 0 && to != Long.MAX_VALUE) {
-            throw new IllegalArgumentException("from/to time ranges should be 0 to Long.MAX_VALUE. Key and time range queries are not supported.");
-        }
-
-        return iterator -> iterator.hasNext();
-    }
-
-    @Override
-    public <S extends Segment> List<S> segmentsToSearch(final Segments<S> segments, final long from, final long to, final boolean forward) {
-        throw new UnsupportedOperationException();
-    }
-
-    public static Bytes toStoreKeyBinaryPrefix(final Bytes key,
-                                         final long timestamp) {
-        final byte[] serializedKey = key.get();
-
-        final ByteBuffer buf = ByteBuffer.allocate(TIMESTAMP_SIZE + serializedKey.length);
-        buf.putLong(timestamp);
-        buf.put(serializedKey);
-
-        return Bytes.wrap(buf.array());
-    }
-
-    public static Bytes toStoreKeyBinary(final Bytes key,
-                                         final long timestamp,
-                                         final int seqnum) {
-        final byte[] serializedKey = key.get();
-        return toStoreKeyBinary(serializedKey, timestamp, seqnum);
-    }
-
-    public static Bytes toStoreKeyBinary(final Windowed<Bytes> timeKey,
-                                         final int seqnum) {
-        final byte[] bytes = timeKey.key().get();
-        return toStoreKeyBinary(bytes, timeKey.window().start(), seqnum);
-    }
-
-    public static <K> Bytes toStoreKeyBinary(final Windowed<K> timeKey,
-                                             final int seqnum,
-                                             final StateSerdes<K, ?> serdes) {
-        final byte[] serializedKey = serdes.rawKey(timeKey.key());
-        return toStoreKeyBinary(serializedKey, timeKey.window().start(), seqnum);
-    }
-
-    // package private for testing
-    static Bytes toStoreKeyBinary(final byte[] serializedKey,
-                                  final long timestamp,
-                                  final int seqnum) {
-        final ByteBuffer buf = ByteBuffer.allocate(TIMESTAMP_SIZE + serializedKey.length + SEQNUM_SIZE);
-        buf.putLong(timestamp);
-        buf.put(serializedKey);
-        buf.putInt(seqnum);
-        return Bytes.wrap(buf.array());
-    }
-
-    static byte[] extractStoreKeyBytes(final byte[] binaryKey) {
-        final byte[] bytes = new byte[binaryKey.length - TIMESTAMP_SIZE - SEQNUM_SIZE];
-        System.arraycopy(binaryKey, TIMESTAMP_SIZE, bytes, 0, bytes.length);
-        return bytes;
-    }
-
-    static long extractStoreTimestamp(final byte[] binaryKey) {
-        return ByteBuffer.wrap(binaryKey).getLong(0);
-    }
-
-    static int extractStoreSequence(final byte[] binaryKey) {
-        return ByteBuffer.wrap(binaryKey).getInt(binaryKey.length - SEQNUM_SIZE);
-    }
-
-    static <K> Windowed<K> fromStoreKey(final byte[] binaryKey,
-                                        final long windowSize,
-                                        final Deserializer<K> deserializer,
-                                        final String topic) {
-        final K key = deserializer.deserialize(topic, extractStoreKeyBytes(binaryKey));
-        final Window window = extractStoreWindow(binaryKey, windowSize);
-        return new Windowed<>(key, window);
-    }
-
-    static Windowed<Bytes> fromStoreBytesKey(final byte[] binaryKey,
-                                             final long windowSize) {
-        final Bytes key = Bytes.wrap(extractStoreKeyBytes(binaryKey));
-        final Window window = extractStoreWindow(binaryKey, windowSize);
-        return new Windowed<>(key, window);
-    }
-
-    static Window extractStoreWindow(final byte[] binaryKey,
-                                     final long windowSize) {
-        final ByteBuffer buffer = ByteBuffer.wrap(binaryKey);
-        final long start = buffer.getLong(0);
-        return timeWindowForSize(start, windowSize);
-    }
-
-    /**
-     * Safely construct a time window of the given size,
-     * taking care of bounding endMs to Long.MAX_VALUE if necessary
-     */
-    private static TimeWindow timeWindowForSize(final long startMs,
-                                                final long windowSize) {
-        long endMs = startMs + windowSize;
-
-        if (endMs < 0) {
-            LOG.warn("Warning: window end time was truncated to Long.MAX");
-            endMs = Long.MAX_VALUE;
-        }
-        return new TimeWindow(startMs, endMs);
-    }
-}
diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/TimeOrderedWindowStoreBuilder.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/TimeOrderedWindowStoreBuilder.java
deleted file mode 100644
index bce22eec8a..0000000000
--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/TimeOrderedWindowStoreBuilder.java
+++ /dev/null
@@ -1,80 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License. You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.kafka.streams.state.internals;
-
-import org.apache.kafka.common.serialization.Serde;
-import org.apache.kafka.common.utils.Bytes;
-import org.apache.kafka.common.utils.Time;
-import org.apache.kafka.streams.state.WindowBytesStoreSupplier;
-import org.apache.kafka.streams.state.WindowStore;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.util.Objects;
-
-public class TimeOrderedWindowStoreBuilder<K, V> extends AbstractStoreBuilder<K, V, WindowStore<K, V>> {
-    private final Logger log = LoggerFactory.getLogger(WindowStoreBuilder.class);
-
-    private final WindowBytesStoreSupplier storeSupplier;
-
-    public TimeOrderedWindowStoreBuilder(final WindowBytesStoreSupplier storeSupplier,
-                                         final Serde<K> keySerde,
-                                         final Serde<V> valueSerde,
-                                         final Time time) {
-        super(storeSupplier.name(), keySerde, valueSerde, time);
-        Objects.requireNonNull(storeSupplier, "storeSupplier can't be null");
-        Objects.requireNonNull(storeSupplier.metricsScope(), "storeSupplier's metricsScope can't be null");
-        this.storeSupplier = storeSupplier;
-    }
-
-    @Override
-    public WindowStore<K, V> build() {
-        if (storeSupplier.retainDuplicates() && enableCaching) {
-            log.warn("Disabling caching for {} since store was configured to retain duplicates", storeSupplier.name());
-            enableCaching = false;
-        }
-
-        return new MeteredWindowStore<>(
-            maybeWrapCaching(maybeWrapLogging(storeSupplier.get())),
-            storeSupplier.windowSize(),
-            storeSupplier.metricsScope(),
-            time,
-            keySerde,
-            valueSerde);
-    }
-
-    private WindowStore<Bytes, byte[]> maybeWrapCaching(final WindowStore<Bytes, byte[]> inner) {
-        if (!enableCaching) {
-            return inner;
-        }
-        return new CachingWindowStore(
-            inner,
-            storeSupplier.windowSize(),
-            storeSupplier.segmentIntervalMs());
-    }
-
-    private WindowStore<Bytes, byte[]> maybeWrapLogging(final WindowStore<Bytes, byte[]> inner) {
-        if (!enableLogging) {
-            return inner;
-        }
-        return new ChangeLoggingWindowBytesStore(
-            inner,
-            storeSupplier.retainDuplicates(),
-            TimeOrderedKeySchema::toStoreKeyBinary
-        );
-    }
-}
diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/KeyAndJoinSide.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/TimestampedKeyAndJoinSide.java
similarity index 59%
rename from streams/src/main/java/org/apache/kafka/streams/state/internals/KeyAndJoinSide.java
rename to streams/src/main/java/org/apache/kafka/streams/state/internals/TimestampedKeyAndJoinSide.java
index c3a7b7aa10..c0516e1015 100644
--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/KeyAndJoinSide.java
+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/TimestampedKeyAndJoinSide.java
@@ -21,30 +21,36 @@ import org.apache.kafka.streams.KeyValue;
 import java.util.Objects;
 
 /**
- * Combines a key from a {@link KeyValue} with a boolean value referencing if the key is
+ * Combines a timestamped key from a {@link KeyValue} with a boolean value referencing if the key is
  * part of the left join (true) or right join (false). This class is only useful when a state
  * store needs to be shared between left and right processors, and each processor needs to
  * access the key of the other processor.
+ *
+ * Note that it might be cleaner to have two layers for such usages: first a KeyAndJoinSide, where the Key
+ * is in the form of a <timestamp, key>; but with the nested structure serdes would need extra byte array copies.
+ * Since it is only used in a single place today we decided to combine them into a single type / serde.
  */
-public class KeyAndJoinSide<K> {
+public class TimestampedKeyAndJoinSide<K> {
     private final K key;
+    private final long timestamp;
     private final boolean leftSide;
 
-    private KeyAndJoinSide(final boolean leftSide, final K key) {
+    private TimestampedKeyAndJoinSide(final boolean leftSide, final K key, final long timestamp) {
         this.key = Objects.requireNonNull(key, "key cannot be null");
         this.leftSide = leftSide;
+        this.timestamp = timestamp;
     }
 
     /**
-     * Create a new {@link KeyAndJoinSide} instance if the provide {@code key} is not {@code null}.
+     * Create a new {@link TimestampedKeyAndJoinSide} instance if the provide {@code key} is not {@code null}.
      *
      * @param leftSide True if the key is part of the left join side; False if it is from the right join side
      * @param key      the key
      * @param <K>      the type of the key
-     * @return a new {@link KeyAndJoinSide} instance if the provide {@code key} is not {@code null}
+     * @return a new {@link TimestampedKeyAndJoinSide} instance if the provide {@code key} is not {@code null}
      */
-    public static <K> KeyAndJoinSide<K> make(final boolean leftSide, final K key) {
-        return new KeyAndJoinSide<>(leftSide, key);
+    public static <K> TimestampedKeyAndJoinSide<K> make(final boolean leftSide, final K key, final long timestamp) {
+        return new TimestampedKeyAndJoinSide<>(leftSide, key, timestamp);
     }
 
     public boolean isLeftSide() {
@@ -55,10 +61,14 @@ public class KeyAndJoinSide<K> {
         return key;
     }
 
+    public long getTimestamp() {
+        return timestamp;
+    }
+
     @Override
     public String toString() {
         final String joinSide = leftSide ? "left" : "right";
-        return "<" + joinSide + "," + key + ">";
+        return "<" + joinSide + "," + key + ":" + timestamp + ">";
     }
 
     @Override
@@ -69,13 +79,14 @@ public class KeyAndJoinSide<K> {
         if (o == null || getClass() != o.getClass()) {
             return false;
         }
-        final KeyAndJoinSide<?> that = (KeyAndJoinSide<?>) o;
+        final TimestampedKeyAndJoinSide<?> that = (TimestampedKeyAndJoinSide<?>) o;
         return leftSide == that.leftSide &&
-            Objects.equals(key, that.key);
+            Objects.equals(key, that.key) &&
+            timestamp == that.timestamp;
     }
 
     @Override
     public int hashCode() {
-        return Objects.hash(leftSide, key);
+        return Objects.hash(leftSide, key, timestamp);
     }
 }
\ No newline at end of file
diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/KeyAndJoinSideDeserializer.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/TimestampedKeyAndJoinSideDeserializer.java
similarity index 59%
rename from streams/src/main/java/org/apache/kafka/streams/state/internals/KeyAndJoinSideDeserializer.java
rename to streams/src/main/java/org/apache/kafka/streams/state/internals/TimestampedKeyAndJoinSideDeserializer.java
index a7c8bee5cc..9ecea46c84 100644
--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/KeyAndJoinSideDeserializer.java
+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/TimestampedKeyAndJoinSideDeserializer.java
@@ -17,17 +17,24 @@
 package org.apache.kafka.streams.state.internals;
 
 import org.apache.kafka.common.serialization.Deserializer;
+import org.apache.kafka.common.serialization.LongDeserializer;
 import org.apache.kafka.streams.kstream.internals.WrappingNullableDeserializer;
 import org.apache.kafka.streams.processor.internals.SerdeGetter;
+import org.apache.kafka.streams.state.StateSerdes;
 
 import java.util.Map;
 
 import static org.apache.kafka.streams.kstream.internals.WrappingNullableUtils.initNullableDeserializer;
 
-public class KeyAndJoinSideDeserializer<K> implements WrappingNullableDeserializer<KeyAndJoinSide<K>, K, Void> {
+/**
+ * The deserializer that is used for {@link TimestampedKeyAndJoinSide}, which is a combo key format of <timestamp, left/right flag, raw-key>
+ * @param <K> the raw key type
+ */
+public class TimestampedKeyAndJoinSideDeserializer<K> implements WrappingNullableDeserializer<TimestampedKeyAndJoinSide<K>, K, Void> {
     private Deserializer<K> keyDeserializer;
+    private final Deserializer<Long> timestampDeserializer = new LongDeserializer();
 
-    KeyAndJoinSideDeserializer(final Deserializer<K> keyDeserializer) {
+    TimestampedKeyAndJoinSideDeserializer(final Deserializer<K> keyDeserializer) {
         this.keyDeserializer = keyDeserializer;
     }
 
@@ -47,16 +54,23 @@ public class KeyAndJoinSideDeserializer<K> implements WrappingNullableDeserializ
     }
 
     @Override
-    public KeyAndJoinSide<K> deserialize(final String topic, final byte[] data) {
-        final boolean bool = data[0] == 1;
+    public TimestampedKeyAndJoinSide<K> deserialize(final String topic, final byte[] data) {
+        final boolean bool = data[StateSerdes.TIMESTAMP_SIZE] == 1;
         final K key = keyDeserializer.deserialize(topic, rawKey(data));
+        final long timestamp = timestampDeserializer.deserialize(topic, rawTimestamp(data));
+
+        return TimestampedKeyAndJoinSide.make(bool, key, timestamp);
+    }
 
-        return KeyAndJoinSide.make(bool, key);
+    private byte[] rawTimestamp(final byte[] data) {
+        final byte[] rawTimestamp = new byte[8];
+        System.arraycopy(data, 0, rawTimestamp, 0, 8);
+        return rawTimestamp;
     }
 
     private byte[] rawKey(final byte[] data) {
-        final byte[] rawKey = new byte[data.length - 1];
-        System.arraycopy(data, 1, rawKey, 0, rawKey.length);
+        final byte[] rawKey = new byte[data.length - StateSerdes.TIMESTAMP_SIZE - StateSerdes.BOOLEAN_SIZE];
+        System.arraycopy(data, StateSerdes.TIMESTAMP_SIZE + StateSerdes.BOOLEAN_SIZE, rawKey, 0, rawKey.length);
         return rawKey;
     }
 
diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/KeyAndJoinSideSerde.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/TimestampedKeyAndJoinSideSerde.java
similarity index 71%
rename from streams/src/main/java/org/apache/kafka/streams/state/internals/KeyAndJoinSideSerde.java
rename to streams/src/main/java/org/apache/kafka/streams/state/internals/TimestampedKeyAndJoinSideSerde.java
index 6ed0d50048..6ae1923bc2 100644
--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/KeyAndJoinSideSerde.java
+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/TimestampedKeyAndJoinSideSerde.java
@@ -19,11 +19,12 @@ package org.apache.kafka.streams.state.internals;
 import org.apache.kafka.common.serialization.Serde;
 import org.apache.kafka.streams.kstream.internals.WrappingNullableSerde;
 
-public class KeyAndJoinSideSerde<K> extends WrappingNullableSerde<KeyAndJoinSide<K>, K, Void> {
-    public KeyAndJoinSideSerde(final Serde<K> keySerde) {
+public class TimestampedKeyAndJoinSideSerde<K> extends WrappingNullableSerde<TimestampedKeyAndJoinSide<K>, K, Void> {
+
+    public TimestampedKeyAndJoinSideSerde(final Serde<K> keySerde) {
         super(
-            new KeyAndJoinSideSerializer<>(keySerde != null ? keySerde.serializer() : null),
-            new KeyAndJoinSideDeserializer<>(keySerde != null ? keySerde.deserializer() : null)
+            new TimestampedKeyAndJoinSideSerializer<>(keySerde != null ? keySerde.serializer() : null),
+            new TimestampedKeyAndJoinSideDeserializer<>(keySerde != null ? keySerde.deserializer() : null)
         );
     }
 }
diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/KeyAndJoinSideSerializer.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/TimestampedKeyAndJoinSideSerializer.java
similarity index 72%
rename from streams/src/main/java/org/apache/kafka/streams/state/internals/KeyAndJoinSideSerializer.java
rename to streams/src/main/java/org/apache/kafka/streams/state/internals/TimestampedKeyAndJoinSideSerializer.java
index ab01d14f92..801c417e1e 100644
--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/KeyAndJoinSideSerializer.java
+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/TimestampedKeyAndJoinSideSerializer.java
@@ -16,6 +16,7 @@
  */
 package org.apache.kafka.streams.state.internals;
 
+import org.apache.kafka.common.serialization.LongSerializer;
 import org.apache.kafka.common.serialization.Serializer;
 import org.apache.kafka.streams.kstream.internals.WrappingNullableSerializer;
 import org.apache.kafka.streams.processor.internals.SerdeGetter;
@@ -26,13 +27,14 @@ import java.util.Map;
 import static org.apache.kafka.streams.kstream.internals.WrappingNullableUtils.initNullableSerializer;
 
 /**
- * Serializes a {@link KeyAndJoinSide}. The serialized bytes starts with a byte that references
- * to the join side of the key followed by the key in bytes.
+ * The serializer that is used for {@link TimestampedKeyAndJoinSide}, which is a combo key format of <timestamp, left/right flag, raw-key>
+ * @param <K> the raw key type
  */
-public class KeyAndJoinSideSerializer<K> implements WrappingNullableSerializer<KeyAndJoinSide<K>, K, Void> {
+public class TimestampedKeyAndJoinSideSerializer<K> implements WrappingNullableSerializer<TimestampedKeyAndJoinSide<K>, K, Void> {
     private Serializer<K> keySerializer;
+    private final Serializer<Long> timestampSerializer = new LongSerializer();
 
-    KeyAndJoinSideSerializer(final Serializer<K> keySerializer) {
+    TimestampedKeyAndJoinSideSerializer(final Serializer<K> keySerializer) {
         this.keySerializer = keySerializer;
     }
 
@@ -52,12 +54,14 @@ public class KeyAndJoinSideSerializer<K> implements WrappingNullableSerializer<K
     }
 
     @Override
-    public byte[] serialize(final String topic, final KeyAndJoinSide<K> data) {
+    public byte[] serialize(final String topic, final TimestampedKeyAndJoinSide<K> data) {
         final byte boolByte = (byte) (data.isLeftSide() ? 1 : 0);
         final byte[] keyBytes = keySerializer.serialize(topic, data.getKey());
+        final byte[] timestampBytes = timestampSerializer.serialize(topic, data.getTimestamp());
 
         return ByteBuffer
-            .allocate(keyBytes.length + 1)
+            .allocate(timestampBytes.length + 1 + keyBytes.length)
+            .put(timestampBytes)
             .put(boolByte)
             .put(keyBytes)
             .array();
diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/WindowKeySchema.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/WindowKeySchema.java
index a467af8cf5..5834f9468d 100644
--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/WindowKeySchema.java
+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/WindowKeySchema.java
@@ -29,12 +29,13 @@ import org.slf4j.LoggerFactory;
 import java.nio.ByteBuffer;
 import java.util.List;
 
+import static org.apache.kafka.streams.state.StateSerdes.TIMESTAMP_SIZE;
+
 public class WindowKeySchema implements RocksDBSegmentedBytesStore.KeySchema {
 
     private static final Logger LOG = LoggerFactory.getLogger(WindowKeySchema.class);
 
     private static final int SEQNUM_SIZE = 4;
-    private static final int TIMESTAMP_SIZE = 8;
     private static final int SUFFIX_SIZE = TIMESTAMP_SIZE + SEQNUM_SIZE;
     private static final byte[] MIN_SUFFIX = new byte[SUFFIX_SIZE];
 
diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/AbstractRocksDBSegmentedBytesStoreTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/AbstractRocksDBSegmentedBytesStoreTest.java
index 21aaf0fd83..514bb790cf 100644
--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/AbstractRocksDBSegmentedBytesStoreTest.java
+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/AbstractRocksDBSegmentedBytesStoreTest.java
@@ -109,7 +109,7 @@ public abstract class AbstractRocksDBSegmentedBytesStoreTest<S extends Segment>
             // expire it.
             nextSegmentWindow = new SessionWindow(segmentInterval + retention, segmentInterval + retention);
         }
-        if (schema instanceof WindowKeySchema || schema instanceof TimeOrderedKeySchema) {
+        if (schema instanceof WindowKeySchema) {
             windows[0] = timeWindowForSize(10L, windowSizeForTimeWindow);
             windows[1] = timeWindowForSize(500L, windowSizeForTimeWindow);
             windows[2] = timeWindowForSize(1_000L, windowSizeForTimeWindow);
@@ -463,10 +463,10 @@ public abstract class AbstractRocksDBSegmentedBytesStoreTest<S extends Segment>
         final StateSerdes<String, Long> stateSerdes = StateSerdes.withBuiltinTypes("dummy", String.class, Long.class);
         if (schema instanceof SessionKeySchema) {
             return Bytes.wrap(SessionKeySchema.toBinary(key, stateSerdes.keySerializer(), "dummy"));
-        } else if (schema instanceof TimeOrderedKeySchema) {
-            return TimeOrderedKeySchema.toStoreKeyBinary(key, 0, stateSerdes);
-        } else {
+        } else if (schema instanceof WindowKeySchema) {
             return WindowKeySchema.toStoreKeyBinary(key, 0, stateSerdes);
+        } else {
+            throw new IllegalStateException("Unrecognized serde schema");
         }
     }
 
@@ -490,23 +490,14 @@ public abstract class AbstractRocksDBSegmentedBytesStoreTest<S extends Segment>
                     stateSerdes.valueDeserializer().deserialize("dummy", next.value)
                 );
                 results.add(deserialized);
-            } else if (schema instanceof TimeOrderedKeySchema) {
-                final KeyValue<Windowed<String>, Long> deserialized = KeyValue.pair(
-                    TimeOrderedKeySchema.fromStoreKey(
-                        next.key.get(),
-                        windowSizeForTimeWindow,
-                        stateSerdes.keyDeserializer(),
-                        stateSerdes.topic()
-                    ),
-                    stateSerdes.valueDeserializer().deserialize("dummy", next.value)
-                );
-                results.add(deserialized);
-            } else {
+            } else if (schema instanceof SessionKeySchema) {
                 final KeyValue<Windowed<String>, Long> deserialized = KeyValue.pair(
                     SessionKeySchema.from(next.key.get(), stateSerdes.keyDeserializer(), "dummy"),
                     stateSerdes.valueDeserializer().deserialize("dummy", next.value)
                 );
                 results.add(deserialized);
+            } else {
+                throw new IllegalStateException("Unrecognized serde schema");
             }
         }
         return results;
diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/CachingInMemorySessionStoreTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/CachingInMemorySessionStoreTest.java
index 9775046287..681cd5112c 100644
--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/CachingInMemorySessionStoreTest.java
+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/CachingInMemorySessionStoreTest.java
@@ -316,13 +316,11 @@ public class CachingInMemorySessionStoreTest {
     @Test
     public void shouldQueryItemsInCacheAndStore() {
         final List<KeyValue<Windowed<Bytes>, byte[]>> added = addSessionsUntilOverflow("a");
-        try (final KeyValueIterator<Windowed<Bytes>, byte[]> iterator = cachingStore.findSessions(
-            Bytes.wrap("a".getBytes(StandardCharsets.UTF_8)),
-            0,
-            added.size() * 10L)) {
-            final List<KeyValue<Windowed<Bytes>, byte[]>> actual = toList(iterator);
-            verifyKeyValueList(added, actual);
-        }
+        final List<KeyValue<Windowed<Bytes>, byte[]>> actual = toList(cachingStore.findSessions(
+                Bytes.wrap("a".getBytes(StandardCharsets.UTF_8)),
+                0,
+                added.size() * 10L));
+        verifyKeyValueList(added, actual);
     }
 
     @Test
diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/CachingPersistentSessionStoreTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/CachingPersistentSessionStoreTest.java
index 9af329df53..95ebf770db 100644
--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/CachingPersistentSessionStoreTest.java
+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/CachingPersistentSessionStoreTest.java
@@ -305,14 +305,12 @@ public class CachingPersistentSessionStoreTest {
     @Test
     public void shouldQueryItemsInCacheAndStore() {
         final List<KeyValue<Windowed<Bytes>, byte[]>> added = addSessionsUntilOverflow("a");
-        try (final KeyValueIterator<Windowed<Bytes>, byte[]> iterator = cachingStore.findSessions(
-            Bytes.wrap("a".getBytes(StandardCharsets.UTF_8)),
-            0,
-            added.size() * 10L
-        )) {
-            final List<KeyValue<Windowed<Bytes>, byte[]>> actual = toList(iterator);
-            verifyKeyValueList(added, actual);
-        }
+        final List<KeyValue<Windowed<Bytes>, byte[]>> actual = toList(cachingStore.findSessions(
+                Bytes.wrap("a".getBytes(StandardCharsets.UTF_8)),
+                0,
+                added.size() * 10L
+        ));
+        verifyKeyValueList(added, actual);
     }
 
     @Test
diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyWindowStoreTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyWindowStoreTest.java
index 3c486c34fd..73439b195f 100644
--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyWindowStoreTest.java
+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyWindowStoreTest.java
@@ -80,14 +80,10 @@ public class CompositeReadOnlyWindowStoreTest {
         underlyingWindowStore.put("my-key", "my-value", 0L);
         underlyingWindowStore.put("my-key", "my-later-value", 10L);
 
-        try (final WindowStoreIterator<String> iterator =
-                 windowStore.fetch("my-key", ofEpochMilli(0L), ofEpochMilli(25L))) {
-            final List<KeyValue<Long, String>> results = StreamsTestUtils.toList(iterator);
-
-            assertEquals(
+        assertEquals(
                 asList(new KeyValue<>(0L, "my-value"), new KeyValue<>(10L, "my-later-value")),
-                results);
-        }
+                StreamsTestUtils.toList(windowStore.fetch("my-key", ofEpochMilli(0L), ofEpochMilli(25L)))
+        );
     }
 
     @Test
@@ -95,14 +91,10 @@ public class CompositeReadOnlyWindowStoreTest {
         underlyingWindowStore.put("my-key", "my-value", 0L);
         underlyingWindowStore.put("my-key", "my-later-value", 10L);
 
-        try (final WindowStoreIterator<String> iterator =
-                 windowStore.backwardFetch("my-key", ofEpochMilli(0L), ofEpochMilli(25L))) {
-            final List<KeyValue<Long, String>> results = StreamsTestUtils.toList(iterator);
-
-            assertEquals(
+        assertEquals(
                 asList(new KeyValue<>(10L, "my-later-value"), new KeyValue<>(0L, "my-value")),
-                results);
-        }
+                StreamsTestUtils.toList(windowStore.backwardFetch("my-key", ofEpochMilli(0L), ofEpochMilli(25L)))
+        );
     }
 
     @Test
diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/ListValueStoreTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/ListValueStoreTest.java
new file mode 100644
index 0000000000..220eb9fecb
--- /dev/null
+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/ListValueStoreTest.java
@@ -0,0 +1,216 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.state.internals;
+
+import org.apache.kafka.common.metrics.Metrics;
+import org.apache.kafka.common.serialization.Serde;
+import org.apache.kafka.common.serialization.Serdes;
+import org.apache.kafka.common.utils.LogContext;
+import org.apache.kafka.common.utils.Time;
+import org.apache.kafka.streams.KeyValue;
+import org.apache.kafka.streams.errors.InvalidStateStoreException;
+import org.apache.kafka.streams.processor.StateStoreContext;
+import org.apache.kafka.streams.processor.internals.MockStreamsMetrics;
+import org.apache.kafka.streams.state.KeyValueIterator;
+import org.apache.kafka.streams.state.KeyValueStore;
+import org.apache.kafka.streams.state.Stores;
+import org.apache.kafka.test.InternalMockProcessorContext;
+import org.apache.kafka.test.MockRecordCollector;
+import org.apache.kafka.test.TestUtils;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
+
+import java.io.File;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.List;
+
+import static java.util.Arrays.asList;
+import static org.apache.kafka.test.StreamsTestUtils.toList;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertThrows;
+
+@RunWith(Parameterized.class)
+public class ListValueStoreTest {
+    private enum StoreType { InMemory, RocksDB }
+
+    private final StoreType storeType;
+    private KeyValueStore<Integer, String> listStore;
+
+    final File baseDir = TestUtils.tempDirectory("test");
+
+    public ListValueStoreTest(final StoreType type) {
+        this.storeType = type;
+    }
+
+    @Parameterized.Parameters(name = "store type = {0}")
+    public static Collection<Object[]> data() {
+        final List<Object[]> values = new ArrayList<>();
+        for (final StoreType type : Arrays.asList(StoreType.InMemory, StoreType.RocksDB)) {
+            values.add(new Object[]{type});
+        }
+        return values;
+    }
+
+    @Before
+    public void setup() {
+        listStore = buildStore(Serdes.Integer(), Serdes.String());
+
+        final MockRecordCollector recordCollector = new MockRecordCollector();
+        final InternalMockProcessorContext<Integer, String> context = new InternalMockProcessorContext<>(
+            baseDir,
+            Serdes.String(),
+            Serdes.Integer(),
+            recordCollector,
+            new ThreadCache(
+                new LogContext("testCache"),
+                0,
+                new MockStreamsMetrics(new Metrics())));
+        context.setTime(1L);
+
+        listStore.init((StateStoreContext) context, listStore);
+    }
+
+    @After
+    public void after() {
+        listStore.close();
+    }
+
+    <K, V> KeyValueStore<K, V> buildStore(final Serde<K> keySerde,
+                                          final Serde<V> valueSerde) {
+        return new ListValueStoreBuilder<>(
+            storeType == StoreType.RocksDB ? Stores.persistentKeyValueStore("rocksDB list store")
+                : Stores.inMemoryKeyValueStore("in-memory list store"),
+            keySerde,
+            valueSerde,
+            Time.SYSTEM)
+            .build();
+    }
+
+    @Test
+    public void shouldGetAll() {
+        listStore.put(0, "zero");
+        // should retain duplicates
+        listStore.put(0, "zero again");
+        listStore.put(1, "one");
+        listStore.put(2, "two");
+
+        final KeyValue<Integer, String> zero = KeyValue.pair(0, "zero");
+        final KeyValue<Integer, String> zeroAgain = KeyValue.pair(0, "zero again");
+        final KeyValue<Integer, String> one = KeyValue.pair(1, "one");
+        final KeyValue<Integer, String> two = KeyValue.pair(2, "two");
+
+        assertEquals(
+            asList(zero, zeroAgain, one, two),
+            toList(listStore.all())
+        );
+    }
+
+    @Test
+    public void shouldGetAllNonDeletedRecords() {
+        // Add some records
+        listStore.put(0, "zero");
+        listStore.put(1, "one");
+        listStore.put(1, "one again");
+        listStore.put(2, "two");
+        listStore.put(3, "three");
+        listStore.put(4, "four");
+
+        // Delete some records
+        listStore.put(1, null);
+        listStore.put(3, null);
+
+        // Only non-deleted records should appear in the all() iterator
+        final KeyValue<Integer, String> zero = KeyValue.pair(0, "zero");
+        final KeyValue<Integer, String> two = KeyValue.pair(2, "two");
+        final KeyValue<Integer, String> four = KeyValue.pair(4, "four");
+
+        assertEquals(
+            asList(zero, two, four),
+            toList(listStore.all())
+        );
+    }
+
+    @Test
+    public void shouldGetAllReturnTimestampOrderedRecords() {
+        // Add some records in different order
+        listStore.put(4, "four");
+        listStore.put(0, "zero");
+        listStore.put(2, "two1");
+        listStore.put(3, "three");
+        listStore.put(1, "one");
+
+        // Add duplicates
+        listStore.put(2, "two2");
+
+        // Only non-deleted records should appear in the all() iterator
+        final KeyValue<Integer, String> zero = KeyValue.pair(0, "zero");
+        final KeyValue<Integer, String> one = KeyValue.pair(1, "one");
+        final KeyValue<Integer, String> two1 = KeyValue.pair(2, "two1");
+        final KeyValue<Integer, String> two2 = KeyValue.pair(2, "two2");
+        final KeyValue<Integer, String> three = KeyValue.pair(3, "three");
+        final KeyValue<Integer, String> four = KeyValue.pair(4, "four");
+
+        assertEquals(
+            asList(zero, one, two1, two2, three, four),
+            toList(listStore.all())
+        );
+    }
+
+    @Test
+    public void shouldAllowDeleteWhileIterateRecords() {
+        listStore.put(0, "zero1");
+        listStore.put(0, "zero2");
+        listStore.put(1, "one");
+
+        final KeyValue<Integer, String> zero1 = KeyValue.pair(0, "zero1");
+        final KeyValue<Integer, String> zero2 = KeyValue.pair(0, "zero2");
+        final KeyValue<Integer, String> one = KeyValue.pair(1, "one");
+
+        final KeyValueIterator<Integer, String> it = listStore.all();
+        assertEquals(zero1, it.next());
+
+        listStore.put(0, null);
+
+        // zero2 should still be returned from the iterator after the delete call
+        assertEquals(zero2, it.next());
+
+        it.close();
+
+        // A new all() iterator after a previous all() iterator was closed should not return deleted records.
+        assertEquals(Collections.singletonList(one), toList(listStore.all()));
+    }
+
+    @Test
+    public void shouldNotReturnMoreDataWhenIteratorClosed() {
+        listStore.put(0, "zero1");
+        listStore.put(0, "zero2");
+        listStore.put(1, "one");
+
+        final KeyValueIterator<Integer, String> it = listStore.all();
+
+        it.close();
+
+        // A new all() iterator after a previous all() iterator was closed should not return deleted records.
+        assertThrows(InvalidStateStoreException.class, it::next);
+    }
+}
diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBTimeOrderedWindowStoreTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBTimeOrderedWindowStoreTest.java
deleted file mode 100644
index 95c88ed157..0000000000
--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBTimeOrderedWindowStoreTest.java
+++ /dev/null
@@ -1,239 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License. You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.kafka.streams.state.internals;
-
-import org.apache.kafka.common.metrics.Metrics;
-import org.apache.kafka.common.serialization.Serde;
-import org.apache.kafka.common.serialization.Serdes;
-import org.apache.kafka.common.utils.LogContext;
-import org.apache.kafka.common.utils.Time;
-import org.apache.kafka.streams.KeyValue;
-import org.apache.kafka.streams.kstream.Windowed;
-import org.apache.kafka.streams.processor.StateStoreContext;
-import org.apache.kafka.streams.processor.internals.MockStreamsMetrics;
-import org.apache.kafka.streams.state.KeyValueIterator;
-import org.apache.kafka.streams.state.WindowStore;
-import org.apache.kafka.test.InternalMockProcessorContext;
-import org.apache.kafka.test.MockRecordCollector;
-import org.apache.kafka.test.TestUtils;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Test;
-
-import java.io.File;
-
-import static java.util.Arrays.asList;
-import static org.apache.kafka.streams.state.internals.RocksDbWindowBytesStoreSupplier.WindowStoreTypes;
-import static org.apache.kafka.test.StreamsTestUtils.toList;
-import static org.junit.Assert.assertEquals;
-
-public class RocksDBTimeOrderedWindowStoreTest {
-    private static final long WINDOW_SIZE = 3L;
-    private static final long SEGMENT_INTERVAL = 60_000L;
-    private static final long RETENTION_PERIOD = 2 * SEGMENT_INTERVAL;
-
-    private static final String STORE_NAME = "rocksDB time-ordered window store";
-
-    WindowStore<Integer, String> windowStore;
-    InternalMockProcessorContext context;
-    MockRecordCollector recordCollector;
-
-    final File baseDir = TestUtils.tempDirectory("test");
-
-    @Before
-    public void setup() {
-        windowStore = buildWindowStore(RETENTION_PERIOD, WINDOW_SIZE, true, Serdes.Integer(), Serdes.String());
-
-        recordCollector = new MockRecordCollector();
-        context = new InternalMockProcessorContext<>(
-            baseDir,
-            Serdes.String(),
-            Serdes.Integer(),
-            recordCollector,
-            new ThreadCache(
-                new LogContext("testCache"),
-                0,
-                new MockStreamsMetrics(new Metrics())));
-        context.setTime(1L);
-
-        windowStore.init((StateStoreContext) context, windowStore);
-    }
-
-    @After
-    public void after() {
-        windowStore.close();
-    }
-
-    <K, V> WindowStore<K, V> buildWindowStore(final long retentionPeriod,
-                                              final long windowSize,
-                                              final boolean retainDuplicates,
-                                              final Serde<K> keySerde,
-                                              final Serde<V> valueSerde) {
-        return new TimeOrderedWindowStoreBuilder<>(
-            new RocksDbWindowBytesStoreSupplier(
-                STORE_NAME,
-                retentionPeriod,
-                Math.max(retentionPeriod / 2, 60_000L),
-                windowSize,
-                retainDuplicates,
-                WindowStoreTypes.TIME_ORDERED_WINDOW_STORE),
-            keySerde,
-            valueSerde,
-            Time.SYSTEM)
-            .build();
-    }
-
-    @Test
-    public void shouldGetAll() {
-        final long startTime = SEGMENT_INTERVAL - 4L;
-
-        windowStore.put(0, "zero", startTime + 0);
-        windowStore.put(1, "one", startTime + 1);
-        windowStore.put(2, "two", startTime + 2);
-
-        final KeyValue<Windowed<Integer>, String> zero = windowedPair(0, "zero", startTime + 0);
-        final KeyValue<Windowed<Integer>, String> one = windowedPair(1, "one", startTime + 1);
-        final KeyValue<Windowed<Integer>, String> two = windowedPair(2, "two", startTime + 2);
-
-        assertEquals(
-            asList(zero, one, two),
-            toList(windowStore.all())
-        );
-    }
-
-    @Test
-    public void shouldGetAllDuplicates() {
-        final long startTime = SEGMENT_INTERVAL - 4L;
-
-        windowStore.put(0, "zero1", startTime + 0);
-        windowStore.put(0, "zero2", startTime + 0);
-        windowStore.put(0, "zero3", startTime + 0);
-
-        final KeyValue<Windowed<Integer>, String> zero1 = windowedPair(0, "zero1", startTime + 0);
-        final KeyValue<Windowed<Integer>, String> zero2 = windowedPair(0, "zero2", startTime + 0);
-        final KeyValue<Windowed<Integer>, String> zero3 = windowedPair(0, "zero3", startTime + 0);
-
-        assertEquals(
-            asList(zero1, zero2, zero3),
-            toList(windowStore.all())
-        );
-    }
-
-    @Test
-    public void shouldGetAllNonDeletedRecords() {
-        final long startTime = SEGMENT_INTERVAL - 4L;
-
-        // Add some records
-        windowStore.put(0, "zero", startTime + 0);
-        windowStore.put(1, "one", startTime + 1);
-        windowStore.put(2, "two", startTime + 2);
-        windowStore.put(3, "three", startTime + 3);
-        windowStore.put(4, "four", startTime + 4);
-
-        // Delete some records
-        windowStore.put(1, null, startTime + 1);
-        windowStore.put(3, null, startTime + 3);
-
-        // Only non-deleted records should appear in the all() iterator
-        final KeyValue<Windowed<Integer>, String> zero = windowedPair(0, "zero", startTime + 0);
-        final KeyValue<Windowed<Integer>, String> two = windowedPair(2, "two", startTime + 2);
-        final KeyValue<Windowed<Integer>, String> four = windowedPair(4, "four", startTime + 4);
-
-        assertEquals(
-            asList(zero, two, four),
-            toList(windowStore.all())
-        );
-    }
-
-    @Test
-    public void shouldDeleteAllDuplicates() {
-        final long startTime = SEGMENT_INTERVAL - 4L;
-
-        windowStore.put(0, "zero1", startTime + 0);
-        windowStore.put(0, "zero2", startTime + 0);
-        windowStore.put(0, "zero3", startTime + 0);
-        windowStore.put(1, "one1", startTime + 1);
-        windowStore.put(1, "one2", startTime + 1);
-
-        windowStore.put(0, null, startTime + 0);
-
-        final KeyValue<Windowed<Integer>, String> one1 = windowedPair(1, "one1", startTime + 1);
-        final KeyValue<Windowed<Integer>, String> one2 = windowedPair(1, "one2", startTime + 1);
-
-        assertEquals(
-            asList(one1, one2),
-            toList(windowStore.all())
-        );
-    }
-
-    @Test
-    public void shouldGetAllReturnTimestampOrderedRecords() {
-        final long startTime = SEGMENT_INTERVAL - 4L;
-
-        // Add some records in different order
-        windowStore.put(4, "four", startTime + 4);
-        windowStore.put(0, "zero", startTime + 0);
-        windowStore.put(2, "two1", startTime + 2);
-        windowStore.put(3, "three", startTime + 3);
-        windowStore.put(1, "one", startTime + 1);
-
-        // Add duplicates
-        windowStore.put(2, "two2", startTime + 2);
-
-        // Only non-deleted records should appear in the all() iterator
-        final KeyValue<Windowed<Integer>, String> zero = windowedPair(0, "zero", startTime + 0);
-        final KeyValue<Windowed<Integer>, String> one = windowedPair(1, "one", startTime + 1);
-        final KeyValue<Windowed<Integer>, String> two1 = windowedPair(2, "two1", startTime + 2);
-        final KeyValue<Windowed<Integer>, String> two2 = windowedPair(2, "two2", startTime + 2);
-        final KeyValue<Windowed<Integer>, String> three = windowedPair(3, "three", startTime + 3);
-        final KeyValue<Windowed<Integer>, String> four = windowedPair(4, "four", startTime + 4);
-
-        assertEquals(
-            asList(zero, one, two1, two2, three, four),
-            toList(windowStore.all())
-        );
-    }
-
-    @Test
-    public void shouldEarlyClosedIteratorStillGetAllRecords() {
-        final long startTime = SEGMENT_INTERVAL - 4L;
-
-        windowStore.put(0, "zero", startTime + 0);
-        windowStore.put(1, "one", startTime + 1);
-
-        final KeyValue<Windowed<Integer>, String> zero = windowedPair(0, "zero", startTime + 0);
-        final KeyValue<Windowed<Integer>, String> one = windowedPair(1, "one", startTime + 1);
-
-        final KeyValueIterator<Windowed<Integer>, String> it = windowStore.all();
-        assertEquals(zero, it.next());
-        it.close();
-
-        // A new all() iterator after a previous all() iterator was closed should return all elements.
-        assertEquals(
-            asList(zero, one),
-            toList(windowStore.all())
-        );
-    }
-
-    private static <K, V> KeyValue<Windowed<K>, V> windowedPair(final K key, final V value, final long timestamp) {
-        return windowedPair(key, value, timestamp, WINDOW_SIZE);
-    }
-
-    private static <K, V> KeyValue<Windowed<K>, V> windowedPair(final K key, final V value, final long timestamp, final long windowSize) {
-        return KeyValue.pair(new Windowed<>(key, WindowKeySchema.timeWindowForSize(timestamp, windowSize)), value);
-    }
-}
diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/TimeOrderedKeySchemaTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/TimeOrderedKeySchemaTest.java
deleted file mode 100644
index 03f81e3d72..0000000000
--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/TimeOrderedKeySchemaTest.java
+++ /dev/null
@@ -1,84 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License. You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.kafka.streams.state.internals;
-
-import org.apache.kafka.common.serialization.Serde;
-import org.apache.kafka.common.serialization.Serdes;
-import org.apache.kafka.common.utils.Bytes;
-import org.apache.kafka.streams.kstream.Window;
-import org.apache.kafka.streams.kstream.Windowed;
-import org.apache.kafka.streams.kstream.internals.TimeWindow;
-import org.apache.kafka.streams.state.StateSerdes;
-import org.junit.Test;
-
-import static org.junit.Assert.assertArrayEquals;
-import static org.junit.Assert.assertEquals;
-
-public class TimeOrderedKeySchemaTest {
-    final private String key = "key";
-    final private long startTime = 50L;
-    final private long endTime = 100L;
-    final private Serde<String> serde = Serdes.String();
-
-    final private Window window = new TimeWindow(startTime, endTime);
-    final private Windowed<String> windowedKey = new Windowed<>(key, window);
-    final private StateSerdes<String, byte[]> stateSerdes = new StateSerdes<>("dummy", serde, Serdes.ByteArray());
-
-    @Test
-    public void shouldConvertToBinaryAndBack() {
-        final Bytes serialized = TimeOrderedKeySchema.toStoreKeyBinary(windowedKey, 0, stateSerdes);
-        final Windowed<String> result = TimeOrderedKeySchema.fromStoreKey(serialized.get(), endTime - startTime, stateSerdes.keyDeserializer(), stateSerdes.topic());
-        assertEquals(windowedKey, result);
-    }
-
-    @Test
-    public void shouldExtractSequenceFromBinary() {
-        final Bytes serialized = TimeOrderedKeySchema.toStoreKeyBinary(windowedKey, 0, stateSerdes);
-        assertEquals(0, TimeOrderedKeySchema.extractStoreSequence(serialized.get()));
-    }
-
-    @Test
-    public void shouldExtractStartTimeFromBinary() {
-        final Bytes serialized = TimeOrderedKeySchema.toStoreKeyBinary(windowedKey, 0, stateSerdes);
-        assertEquals(startTime, TimeOrderedKeySchema.extractStoreTimestamp(serialized.get()));
-    }
-
-    @Test
-    public void shouldExtractWindowFromBinary() {
-        final Bytes serialized = TimeOrderedKeySchema.toStoreKeyBinary(windowedKey, 0, stateSerdes);
-        assertEquals(window, TimeOrderedKeySchema.extractStoreWindow(serialized.get(), endTime - startTime));
-    }
-
-    @Test
-    public void shouldExtractKeyBytesFromBinary() {
-        final Bytes serialized = TimeOrderedKeySchema.toStoreKeyBinary(windowedKey, 0, stateSerdes);
-        assertArrayEquals(key.getBytes(), TimeOrderedKeySchema.extractStoreKeyBytes(serialized.get()));
-    }
-
-    @Test
-    public void shouldExtractKeyFromBinary() {
-        final Bytes serialized = TimeOrderedKeySchema.toStoreKeyBinary(windowedKey, 0, stateSerdes);
-        assertEquals(windowedKey, TimeOrderedKeySchema.fromStoreKey(serialized.get(), endTime - startTime, stateSerdes.keyDeserializer(), stateSerdes.topic()));
-    }
-
-    @Test
-    public void shouldExtractBytesKeyFromBinary() {
-        final Windowed<Bytes> windowedBytesKey = new Windowed<>(Bytes.wrap(key.getBytes()), window);
-        final Bytes serialized = TimeOrderedKeySchema.toStoreKeyBinary(windowedBytesKey, 0);
-        assertEquals(windowedBytesKey, TimeOrderedKeySchema.fromStoreBytesKey(serialized.get(), endTime - startTime));
-    }
-}
diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/KeyAndJoinSideSerializerTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/TimestampedKeyAndJoinSideSerializerTest.java
similarity index 65%
rename from streams/src/test/java/org/apache/kafka/streams/state/internals/KeyAndJoinSideSerializerTest.java
rename to streams/src/test/java/org/apache/kafka/streams/state/internals/TimestampedKeyAndJoinSideSerializerTest.java
index f3ad1562ee..5cca8f6ba5 100644
--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/KeyAndJoinSideSerializerTest.java
+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/TimestampedKeyAndJoinSideSerializerTest.java
@@ -24,49 +24,49 @@ import static org.hamcrest.Matchers.is;
 import static org.hamcrest.Matchers.notNullValue;
 import static org.junit.Assert.assertThrows;
 
-public class KeyAndJoinSideSerializerTest {
+public class TimestampedKeyAndJoinSideSerializerTest {
     private static final String TOPIC = "some-topic";
 
-    private static final KeyAndJoinSideSerde<String> STRING_SERDE =
-        new KeyAndJoinSideSerde<>(Serdes.String());
+    private static final TimestampedKeyAndJoinSideSerde<String> STRING_SERDE =
+        new TimestampedKeyAndJoinSideSerde<>(Serdes.String());
 
     @Test
     public void shouldSerializeKeyWithJoinSideAsTrue() {
         final String value = "some-string";
 
-        final KeyAndJoinSide<String> keyAndJoinSide = KeyAndJoinSide.make(true, value);
+        final TimestampedKeyAndJoinSide<String> timestampedKeyAndJoinSide = TimestampedKeyAndJoinSide.make(true, value, 10);
 
         final byte[] serialized =
-            STRING_SERDE.serializer().serialize(TOPIC, keyAndJoinSide);
+            STRING_SERDE.serializer().serialize(TOPIC, timestampedKeyAndJoinSide);
 
         assertThat(serialized, is(notNullValue()));
 
-        final KeyAndJoinSide<String> deserialized =
+        final TimestampedKeyAndJoinSide<String> deserialized =
             STRING_SERDE.deserializer().deserialize(TOPIC, serialized);
 
-        assertThat(deserialized, is(keyAndJoinSide));
+        assertThat(deserialized, is(timestampedKeyAndJoinSide));
     }
 
     @Test
     public void shouldSerializeKeyWithJoinSideAsFalse() {
         final String value = "some-string";
 
-        final KeyAndJoinSide<String> keyAndJoinSide = KeyAndJoinSide.make(false, value);
+        final TimestampedKeyAndJoinSide<String> timestampedKeyAndJoinSide = TimestampedKeyAndJoinSide.make(false, value, 20);
 
         final byte[] serialized =
-            STRING_SERDE.serializer().serialize(TOPIC, keyAndJoinSide);
+            STRING_SERDE.serializer().serialize(TOPIC, timestampedKeyAndJoinSide);
 
         assertThat(serialized, is(notNullValue()));
 
-        final KeyAndJoinSide<String> deserialized =
+        final TimestampedKeyAndJoinSide<String> deserialized =
             STRING_SERDE.deserializer().deserialize(TOPIC, serialized);
 
-        assertThat(deserialized, is(keyAndJoinSide));
+        assertThat(deserialized, is(timestampedKeyAndJoinSide));
     }
 
     @Test
     public void shouldThrowIfSerializeNullData() {
         assertThrows(NullPointerException.class,
-            () -> STRING_SERDE.serializer().serialize(TOPIC, KeyAndJoinSide.make(true, null)));
+            () -> STRING_SERDE.serializer().serialize(TOPIC, TimestampedKeyAndJoinSide.make(true, null, 0)));
     }
 }
diff --git a/streams/src/test/java/org/apache/kafka/test/StreamsTestUtils.java b/streams/src/test/java/org/apache/kafka/test/StreamsTestUtils.java
index 8d0bb34bf0..c4c023515f 100644
--- a/streams/src/test/java/org/apache/kafka/test/StreamsTestUtils.java
+++ b/streams/src/test/java/org/apache/kafka/test/StreamsTestUtils.java
@@ -26,6 +26,8 @@ import org.apache.kafka.streams.KeyValue;
 import org.apache.kafka.streams.StreamsConfig;
 import org.apache.kafka.streams.kstream.Windowed;
 
+import java.io.Closeable;
+import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.HashSet;
@@ -130,6 +132,13 @@ public final class StreamsTestUtils {
         while (iterator.hasNext()) {
             results.add(iterator.next());
         }
+
+        if (iterator instanceof Closeable) {
+            try {
+                ((Closeable) iterator).close();
+            } catch (IOException e) { /* do nothing */ }
+        }
+
         return results;
     }
 
