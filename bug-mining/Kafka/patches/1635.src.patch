diff --git a/core/src/main/scala/kafka/log/LogManager.scala b/core/src/main/scala/kafka/log/LogManager.scala
index 88a0e21e69..406800113a 100755
--- a/core/src/main/scala/kafka/log/LogManager.scala
+++ b/core/src/main/scala/kafka/log/LogManager.scala
@@ -74,7 +74,7 @@ class LogManager(logDirs: Array[File],
   private val _liveLogDirs: ConcurrentLinkedQueue[File] = createAndValidateLogDirs(logDirs, initialOfflineDirs)
 
   def liveLogDirs: Array[File] = {
-    if (_liveLogDirs.size() == logDirs.size)
+    if (_liveLogDirs.size == logDirs.size)
       logDirs
     else
       _liveLogDirs.asScala.toArray
@@ -416,10 +416,9 @@ class LogManager(logDirs: Array[File],
         CoreUtils.swallow(Files.createFile(new File(dir, Log.CleanShutdownFile).toPath))
       }
     } catch {
-      case e: ExecutionException => {
+      case e: ExecutionException =>
         error("There was an error in one of the threads during LogManager shutdown: " + e.getCause)
         throw e.getCause
-      }
     } finally {
       threadPools.foreach(_.shutdown())
       // regardless of whether the close succeeded, we need to unlock the data directories
@@ -607,8 +606,9 @@ class LogManager(logDirs: Array[File],
     * Rename the directory of the given topic-partition "logdir" as "logdir.uuid.delete" and
     * add it in the queue for deletion.
     * @param topicPartition TopicPartition that needs to be deleted
+    * @return the removed log
     */
-  def asyncDelete(topicPartition: TopicPartition) = {
+  def asyncDelete(topicPartition: TopicPartition): Log = {
     val removedLog: Log = logCreationOrDeletionLock synchronized {
       logs.remove(topicPartition)
     }
@@ -627,11 +627,7 @@ class LogManager(logDirs: Array[File],
           checkpointLogStartOffsetsInDir(removedLog.dir.getParentFile)
           removedLog.dir = renamedDir
           // change the file pointers for log and index file
-          for (logSegment <- removedLog.logSegments) {
-            logSegment.log.setFile(new File(renamedDir, logSegment.log.file.getName))
-            logSegment.index.file = new File(renamedDir, logSegment.index.file.getName)
-          }
-
+          removedLog.logSegments.foreach(_.updateDir(renamedDir))
           logsToBeDeleted.add(removedLog)
           removedLog.removeLogMetrics()
           info(s"Log for partition ${removedLog.topicPartition} is renamed to ${removedLog.dir.getAbsolutePath} and is scheduled for deletion")
@@ -647,6 +643,7 @@ class LogManager(logDirs: Array[File],
     } else if (offlineLogDirs.nonEmpty) {
       throw new KafkaStorageException("Failed to delete log for " + topicPartition + " because it may be in one of the offline directories " + offlineLogDirs.mkString(","))
     }
+    removedLog
   }
 
   /**
diff --git a/core/src/main/scala/kafka/log/LogSegment.scala b/core/src/main/scala/kafka/log/LogSegment.scala
index 0449a4acb5..06c4e2d931 100755
--- a/core/src/main/scala/kafka/log/LogSegment.scala
+++ b/core/src/main/scala/kafka/log/LogSegment.scala
@@ -379,6 +379,17 @@ class LogSegment(val log: FileRecords,
     }
   }
 
+  /**
+   * Update the directory reference for the log and indices in this segment. This would typically be called after a
+   * directory is renamed.
+   */
+  def updateDir(dir: File): Unit = {
+    log.setFile(new File(dir, log.file.getName))
+    index.file = new File(dir, index.file.getName)
+    timeIndex.file = new File(dir, timeIndex.file.getName)
+    txnIndex.file = new File(dir, txnIndex.file.getName)
+  }
+
   /**
    * Change the suffix for the index and log file for this log segment
    * IOException from this method should be handled by the caller
diff --git a/core/src/test/scala/unit/kafka/log/LogManagerTest.scala b/core/src/test/scala/unit/kafka/log/LogManagerTest.scala
index 61ac5ed02d..dd59e60048 100755
--- a/core/src/test/scala/unit/kafka/log/LogManagerTest.scala
+++ b/core/src/test/scala/unit/kafka/log/LogManagerTest.scala
@@ -55,7 +55,7 @@ class LogManagerTest {
 
   @After
   def tearDown() {
-    if(logManager != null)
+    if (logManager != null)
       logManager.shutdown()
     Utils.delete(logDir)
     logManager.liveLogDirs.foreach(Utils.delete)
@@ -297,10 +297,8 @@ class LogManagerTest {
     logManager.checkpointLogRecoveryOffsets()
     val checkpoints = new OffsetCheckpointFile(new File(logDir, logManager.RecoveryPointCheckpointFile)).read()
 
-    topicPartitions.zip(logs).foreach {
-      case(tp, log) => {
-        assertEquals("Recovery point should equal checkpoint", checkpoints(tp), log.recoveryPoint)
-      }
+    topicPartitions.zip(logs).foreach { case (tp, log) =>
+      assertEquals("Recovery point should equal checkpoint", checkpoints(tp), log.recoveryPoint)
     }
   }
 
@@ -310,4 +308,35 @@ class LogManagerTest {
       logDirs = logDirs,
       time = this.time)
   }
+
+  @Test
+  def testFileReferencesAfterAsyncDelete() {
+    val log = logManager.getOrCreateLog(new TopicPartition(name, 0), logConfig)
+    val activeSegment = log.activeSegment
+    val logName = activeSegment.log.file.getName
+    val indexName = activeSegment.index.file.getName
+    val timeIndexName = activeSegment.timeIndex.file.getName
+    val txnIndexName = activeSegment.txnIndex.file.getName
+    val indexFilesOnDiskBeforeDelete = activeSegment.log.file.getParentFile.listFiles.filter(_.getName.endsWith("index"))
+
+    val removedLog = logManager.asyncDelete(new TopicPartition(name, 0))
+    val removedSegment = removedLog.activeSegment
+    val indexFilesAfterDelete = Seq(removedSegment.index.file, removedSegment.timeIndex.file,
+      removedSegment.txnIndex.file)
+
+    assertEquals(new File(removedLog.dir, logName), removedSegment.log.file)
+    assertEquals(new File(removedLog.dir, indexName), removedSegment.index.file)
+    assertEquals(new File(removedLog.dir, timeIndexName), removedSegment.timeIndex.file)
+    assertEquals(new File(removedLog.dir, txnIndexName), removedSegment.txnIndex.file)
+
+    // Try to detect the case where a new index type was added and we forgot to update the pointer
+    // This will only catch cases where the index file is created eagerly instead of lazily
+    indexFilesOnDiskBeforeDelete.foreach { fileBeforeDelete =>
+      val fileInIndex = indexFilesAfterDelete.find(_.getName == fileBeforeDelete.getName)
+      assertEquals(s"Could not find index file ${fileBeforeDelete.getName} in indexFilesAfterDelete",
+        Some(fileBeforeDelete.getName), fileInIndex.map(_.getName))
+      assertNotEquals("File reference was not updated in index", fileBeforeDelete.getAbsolutePath,
+        fileInIndex.get.getAbsolutePath)
+    }
+  }
 }
diff --git a/core/src/test/scala/unit/kafka/utils/TestUtils.scala b/core/src/test/scala/unit/kafka/utils/TestUtils.scala
index d141a26e2b..ab840eed24 100755
--- a/core/src/test/scala/unit/kafka/utils/TestUtils.scala
+++ b/core/src/test/scala/unit/kafka/utils/TestUtils.scala
@@ -1167,10 +1167,10 @@ object TestUtils extends Logging {
       "Replica manager's should have deleted all of this topic's partitions")
     // ensure that logs from all replicas are deleted if delete topic is marked successful in zookeeper
     assertTrue("Replica logs not deleted after delete topic is complete",
-      servers.forall(server => topicPartitions.forall(tp => server.getLogManager().getLog(tp).isEmpty)))
+      servers.forall(server => topicPartitions.forall(tp => server.getLogManager.getLog(tp).isEmpty)))
     // ensure that topic is removed from all cleaner offsets
     TestUtils.waitUntilTrue(() => servers.forall(server => topicPartitions.forall { tp =>
-      val checkpoints = server.getLogManager().liveLogDirs.map { logDir =>
+      val checkpoints = server.getLogManager.liveLogDirs.map { logDir =>
         new OffsetCheckpointFile(new File(logDir, "cleaner-offset-checkpoint")).read()
       }
       checkpoints.forall(checkpointsPerLogDir => !checkpointsPerLogDir.contains(tp))
