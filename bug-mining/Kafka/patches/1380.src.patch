diff --git a/clients/src/main/java/org/apache/kafka/clients/consumer/MockConsumer.java b/clients/src/main/java/org/apache/kafka/clients/consumer/MockConsumer.java
index a88f4324cc..9f312a71de 100644
--- a/clients/src/main/java/org/apache/kafka/clients/consumer/MockConsumer.java
+++ b/clients/src/main/java/org/apache/kafka/clients/consumer/MockConsumer.java
@@ -152,17 +152,24 @@ public class MockConsumer<K, V> implements Consumer<K, V> {
             updateFetchPosition(tp);
 
         // update the consumed offset
+        final Map<TopicPartition, List<ConsumerRecord<K, V>>> results = new HashMap<>();
+        for (final TopicPartition topicPartition : records.keySet()) {
+            results.put(topicPartition, new ArrayList<ConsumerRecord<K, V>>());
+        }
+
         for (Map.Entry<TopicPartition, List<ConsumerRecord<K, V>>> entry : this.records.entrySet()) {
             if (!subscriptions.isPaused(entry.getKey())) {
-                List<ConsumerRecord<K, V>> recs = entry.getValue();
-                if (!recs.isEmpty())
-                    this.subscriptions.position(entry.getKey(), recs.get(recs.size() - 1).offset() + 1);
+                final List<ConsumerRecord<K, V>> recs = entry.getValue();
+                for (final ConsumerRecord<K, V> rec : recs) {
+                    if (assignment().contains(entry.getKey()) && rec.offset() >= subscriptions.position(entry.getKey())) {
+                        results.get(entry.getKey()).add(rec);
+                        subscriptions.position(entry.getKey(), rec.offset() + 1);
+                    }
+                }
             }
         }
-
-        ConsumerRecords<K, V> copy = new ConsumerRecords<K, V>(this.records);
-        this.records = new HashMap<TopicPartition, List<ConsumerRecord<K, V>>>();
-        return copy;
+        this.records.clear();
+        return new ConsumerRecords<>(results);
     }
 
     public void addRecord(ConsumerRecord<K, V> record) {
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java
index 8de5d2304d..be145d9191 100644
--- a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java
@@ -57,7 +57,7 @@ public abstract class AbstractTask {
                            final Collection<TopicPartition> partitions,
                            final ProcessorTopology topology,
                            final Consumer<byte[], byte[]> consumer,
-                           final Consumer<byte[], byte[]> restoreConsumer,
+                           final ChangelogReader changelogReader,
                            final boolean isStandby,
                            final StateDirectory stateDirectory,
                            final ThreadCache cache) {
@@ -70,7 +70,7 @@ public abstract class AbstractTask {
 
         // create the processor state manager
         try {
-            stateMgr = new ProcessorStateManager(id, partitions, restoreConsumer, isStandby, stateDirectory, topology.storeToChangelogTopic());
+            stateMgr = new ProcessorStateManager(id, partitions, isStandby, stateDirectory, topology.storeToChangelogTopic(), changelogReader);
         } catch (IOException e) {
             throw new ProcessorStateException(String.format("task [%s] Error while creating the state manager", id), e);
         }
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/ChangelogReader.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/ChangelogReader.java
new file mode 100644
index 0000000000..384eb6d466
--- /dev/null
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/ChangelogReader.java
@@ -0,0 +1,52 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.processor.internals;
+
+import org.apache.kafka.common.TopicPartition;
+
+import java.util.Map;
+
+/**
+ * Performs bulk read operations from a set of partitions. Used to
+ * restore  {@link org.apache.kafka.streams.processor.StateStore}s from their
+ * change logs
+ */
+public interface ChangelogReader {
+    /**
+     * Validate that the partition exists on the cluster.
+     * @param topicPartition    partition to validate.
+     * @param storeName         name of the store the partition is for.
+     * @throws org.apache.kafka.streams.errors.StreamsException if partition doesn't exist
+     */
+    void validatePartitionExists(final TopicPartition topicPartition, final String storeName);
+
+    /**
+     * Register a state store and it's partition for later restoration.
+     * @param restorationInfo
+     */
+    void register(final StateRestorer restorationInfo);
+
+    /**
+     * Restore all registered state stores by reading from their changelogs.
+     */
+    void restore();
+
+    /**
+     * @return the restored offsets for all persistent stores.
+     */
+    Map<TopicPartition, Long> restoredOffsets();
+}
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImpl.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImpl.java
index 3819bb5a09..1338387a76 100644
--- a/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImpl.java
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImpl.java
@@ -131,7 +131,6 @@ public class GlobalStateManagerImpl implements GlobalStateManager {
 
         log.info("restoring state for global store {}", store.name());
         final List<TopicPartition> topicPartitions = topicPartitionsForStore(store);
-        consumer.assign(topicPartitions);
         final Map<TopicPartition, Long> highWatermarks = consumer.endOffsets(topicPartitions);
         try {
             restoreState(stateRestoreCallback, topicPartitions, highWatermarks);
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java
index 0e8caa2b76..a06baed0c5 100644
--- a/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java
@@ -17,11 +17,8 @@
 
 package org.apache.kafka.streams.processor.internals;
 
-import org.apache.kafka.clients.consumer.Consumer;
 import org.apache.kafka.clients.consumer.ConsumerRecord;
-import org.apache.kafka.common.PartitionInfo;
 import org.apache.kafka.common.TopicPartition;
-import org.apache.kafka.common.errors.TimeoutException;
 import org.apache.kafka.streams.errors.LockException;
 import org.apache.kafka.streams.errors.ProcessorStateException;
 import org.apache.kafka.streams.errors.StreamsException;
@@ -36,13 +33,11 @@ import java.io.File;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Collection;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Map;
 
-import static java.util.Collections.singleton;
 
 public class ProcessorStateManager implements StateManager {
 
@@ -56,9 +51,9 @@ public class ProcessorStateManager implements StateManager {
     private final String logPrefix;
     private final boolean isStandby;
     private final StateDirectory stateDirectory;
+    private final ChangelogReader changelogReader;
     private final Map<String, StateStore> stores;
     private final Map<String, StateStore> globalStores;
-    private final Consumer<byte[], byte[]> restoreConsumer;
     private final Map<TopicPartition, Long> offsetLimits;
     private final Map<TopicPartition, Long> restoredOffsets;
     private final Map<TopicPartition, Long> checkpointedOffsets;
@@ -77,12 +72,13 @@ public class ProcessorStateManager implements StateManager {
      */
     public ProcessorStateManager(final TaskId taskId,
                                  final Collection<TopicPartition> sources,
-                                 final Consumer<byte[], byte[]> restoreConsumer,
                                  final boolean isStandby,
                                  final StateDirectory stateDirectory,
-                                 final Map<String, String> storeToChangelogTopic) throws LockException, IOException {
+                                 final Map<String, String> storeToChangelogTopic,
+                                 final ChangelogReader changelogReader) throws LockException, IOException {
         this.taskId = taskId;
         this.stateDirectory = stateDirectory;
+        this.changelogReader = changelogReader;
         this.baseDir  = stateDirectory.directoryForTask(taskId);
         this.partitionForTopic = new HashMap<>();
         for (TopicPartition source : sources) {
@@ -90,7 +86,6 @@ public class ProcessorStateManager implements StateManager {
         }
         this.stores = new LinkedHashMap<>();
         this.globalStores = new HashMap<>();
-        this.restoreConsumer = restoreConsumer;
         this.offsetLimits = new HashMap<>();
         this.restoredOffsets = new HashMap<>();
         this.isStandby = isStandby;
@@ -143,40 +138,8 @@ public class ProcessorStateManager implements StateManager {
             return;
         }
 
-        // block until the partition is ready for this state changelog topic or time has elapsed
-        int partition = getPartition(topic);
-        boolean partitionNotFound = true;
-        long startTime = System.currentTimeMillis();
-        long waitTime = 5000L;      // hard-code the value since we should not block after KIP-4
-
-        do {
-            try {
-                Thread.sleep(50L);
-            } catch (InterruptedException e) {
-                // ignore
-            }
-
-            List<PartitionInfo> partitions;
-            try {
-                partitions = restoreConsumer.partitionsFor(topic);
-            } catch (TimeoutException e) {
-                throw new StreamsException(String.format("%s Could not fetch partition info for topic: %s before expiration of the configured request timeout", logPrefix, topic));
-            }
-            if (partitions == null) {
-                throw new StreamsException(String.format("%s Could not find partition info for topic: %s", logPrefix, topic));
-            }
-            for (PartitionInfo partitionInfo : partitions) {
-                if (partitionInfo.partition() == partition) {
-                    partitionNotFound = false;
-                    break;
-                }
-            }
-        } while (partitionNotFound && System.currentTimeMillis() < startTime + waitTime);
-
-        if (partitionNotFound) {
-            throw new StreamsException(String.format("%s Store %s's change log (%s) does not contain partition %s",
-                    logPrefix, store.name(), topic, partition));
-        }
+        final TopicPartition storePartition = new TopicPartition(topic, getPartition(topic));
+        changelogReader.validatePartitionExists(storePartition, store.name());
 
         if (isStandby) {
             if (store.persistent()) {
@@ -186,67 +149,17 @@ public class ProcessorStateManager implements StateManager {
             }
         } else {
             log.trace("{} Restoring state store {} from changelog topic {}", logPrefix, store.name(), topic);
-
-            restoreActiveState(topic, stateRestoreCallback);
+            final StateRestorer restorer = new StateRestorer(storePartition,
+                                                             stateRestoreCallback,
+                                                             checkpointedOffsets.get(storePartition),
+                                                             offsetLimit(storePartition),
+                                                             store.persistent());
+            changelogReader.register(restorer);
         }
 
         this.stores.put(store.name(), store);
     }
 
-    private void restoreActiveState(String topicName, StateRestoreCallback stateRestoreCallback) {
-        // ---- try to restore the state from change-log ---- //
-
-        // subscribe to the store's partition
-        if (!restoreConsumer.subscription().isEmpty()) {
-            throw new IllegalStateException(String.format("%s Restore consumer should have not subscribed to any partitions (%s) beforehand", logPrefix, restoreConsumer.subscription()));
-        }
-        TopicPartition storePartition = new TopicPartition(topicName, getPartition(topicName));
-        restoreConsumer.assign(Collections.singletonList(storePartition));
-
-        try {
-            // calculate the end offset of the partition
-            // TODO: this is a bit hacky to first seek then position to get the end offset
-            restoreConsumer.seekToEnd(singleton(storePartition));
-            long endOffset = restoreConsumer.position(storePartition);
-
-            // restore from the checkpointed offset of the change log if it is persistent and the offset exists;
-            // restore the state from the beginning of the change log otherwise
-            if (checkpointedOffsets.containsKey(storePartition)) {
-                restoreConsumer.seek(storePartition, checkpointedOffsets.get(storePartition));
-            } else {
-                restoreConsumer.seekToBeginning(singleton(storePartition));
-            }
-
-            // restore its state from changelog records
-            long limit = offsetLimit(storePartition);
-            while (true) {
-                long offset = 0L;
-                for (ConsumerRecord<byte[], byte[]> record : restoreConsumer.poll(100).records(storePartition)) {
-                    offset = record.offset();
-                    if (offset >= limit) break;
-                    stateRestoreCallback.restore(record.key(), record.value());
-                }
-
-                if (offset >= limit) {
-                    break;
-                } else if (restoreConsumer.position(storePartition) == endOffset) {
-                    break;
-                } else if (restoreConsumer.position(storePartition) > endOffset) {
-                    // For a logging enabled changelog (no offset limit),
-                    // the log end offset should not change while restoring since it is only written by this thread.
-                    throw new IllegalStateException(String.format("%s Log end offset of %s should not change while restoring: old end offset %d, current offset %d",
-                            logPrefix, storePartition, endOffset, restoreConsumer.position(storePartition)));
-                }
-            }
-
-            // record the restored offset for its change log partition
-            long newOffset = Math.min(limit, restoreConsumer.position(storePartition));
-            restoredOffsets.put(storePartition, newOffset);
-        } finally {
-            // un-assign the change log partition
-            restoreConsumer.assign(Collections.<TopicPartition>emptyList());
-        }
-    }
 
     public Map<TopicPartition, Long> checkpointed() {
         Map<TopicPartition, Long> partitionsAndOffsets = new HashMap<>();
@@ -358,6 +271,7 @@ public class ProcessorStateManager implements StateManager {
     // write the checkpoint
     @Override
     public void checkpoint(final Map<TopicPartition, Long> ackedOffsets) {
+        checkpointedOffsets.putAll(changelogReader.restoredOffsets());
         for (String storeName : stores.keySet()) {
             // only checkpoint the offset to the offsets file if
             // it is persistent AND changelog enabled
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java
index a27098c34f..712740c59d 100644
--- a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java
@@ -46,7 +46,6 @@ public class StandbyTask extends AbstractTask {
      * @param partitions            the collection of assigned {@link TopicPartition}
      * @param topology              the instance of {@link ProcessorTopology}
      * @param consumer              the instance of {@link Consumer}
-     * @param restoreConsumer       the instance of {@link Consumer} used when restoring state
      * @param config                the {@link StreamsConfig} specified by the user
      * @param metrics               the {@link StreamsMetrics} created by the thread
      * @param stateDirectory        the {@link StateDirectory} created by the thread
@@ -56,11 +55,11 @@ public class StandbyTask extends AbstractTask {
                        final Collection<TopicPartition> partitions,
                        final ProcessorTopology topology,
                        final Consumer<byte[], byte[]> consumer,
-                       final Consumer<byte[], byte[]> restoreConsumer,
+                       final ChangelogReader changelogReader,
                        final StreamsConfig config,
                        final StreamsMetrics metrics,
                        final StateDirectory stateDirectory) {
-        super(id, applicationId, partitions, topology, consumer, restoreConsumer, true, stateDirectory, null);
+        super(id, applicationId, partitions, topology, consumer, changelogReader, true, stateDirectory, null);
 
         // initialize the topology with its own context
         this.processorContext = new StandbyContextImpl(id, applicationId, config, stateMgr, metrics);
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateRestorer.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateRestorer.java
new file mode 100644
index 0000000000..b2de0f0175
--- /dev/null
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateRestorer.java
@@ -0,0 +1,78 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.processor.internals;
+
+import org.apache.kafka.common.TopicPartition;
+import org.apache.kafka.streams.processor.StateRestoreCallback;
+
+public class StateRestorer {
+    static final int NO_CHECKPOINT = -1;
+    private final TopicPartition partition;
+    private final StateRestoreCallback stateRestoreCallback;
+    private final Long checkpoint;
+    private final long offsetLimit;
+    private final boolean persistent;
+    private long restoredOffset;
+
+    StateRestorer(final TopicPartition partition,
+                  final StateRestoreCallback stateRestoreCallback,
+                  final Long checkpoint,
+                  final long offsetLimit,
+                  final boolean persistent) {
+        this.partition = partition;
+        this.stateRestoreCallback = stateRestoreCallback;
+        this.checkpoint = checkpoint;
+        this.offsetLimit = offsetLimit;
+        this.persistent = persistent;
+    }
+
+    public TopicPartition partition() {
+        return partition;
+    }
+
+    public long checkpoint() {
+        return checkpoint == null ? NO_CHECKPOINT : checkpoint;
+    }
+
+    public void restore(final byte[] key, final byte[] value) {
+        stateRestoreCallback.restore(key, value);
+    }
+
+    public boolean isPersistent() {
+        return persistent;
+    }
+
+    void setRestoredOffset(final long restoredOffset) {
+        this.restoredOffset = Math.min(offsetLimit, restoredOffset);
+    }
+
+    boolean hasCompleted(final long recordOffset, final long endOffset) {
+        return endOffset == 0 || recordOffset >= readTo(endOffset);
+    }
+
+    Long restoredOffset() {
+        return restoredOffset;
+    }
+
+    long offsetLimit() {
+        return offsetLimit;
+    }
+
+    private Long readTo(final long endOffset) {
+        return endOffset < offsetLimit ? endOffset : offsetLimit;
+    }
+}
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java
new file mode 100644
index 0000000000..ec9e4543d8
--- /dev/null
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java
@@ -0,0 +1,199 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.processor.internals;
+
+import org.apache.kafka.clients.consumer.Consumer;
+import org.apache.kafka.clients.consumer.ConsumerRecord;
+import org.apache.kafka.clients.consumer.ConsumerRecords;
+import org.apache.kafka.common.PartitionInfo;
+import org.apache.kafka.common.TopicPartition;
+import org.apache.kafka.common.errors.TimeoutException;
+import org.apache.kafka.common.utils.Time;
+import org.apache.kafka.streams.errors.StreamsException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+public class StoreChangelogReader implements ChangelogReader {
+    private static final Logger log = LoggerFactory.getLogger(StoreChangelogReader.class);
+
+    private final Consumer<byte[], byte[]> consumer;
+    private final Time time;
+    private final long partitionValidationTimeoutMs;
+    private final Map<String, List<PartitionInfo>> partitionInfo = new HashMap<>();
+    private final Map<TopicPartition, StateRestorer> stateRestorers = new HashMap<>();
+
+
+    public StoreChangelogReader(final Consumer<byte[], byte[]> consumer, final Time time, final long partitionValidationTimeoutMs) {
+        this.consumer = consumer;
+        this.time = time;
+        this.partitionValidationTimeoutMs = partitionValidationTimeoutMs;
+    }
+
+
+    @Override
+    public void validatePartitionExists(final TopicPartition topicPartition, final String storeName) {
+        final long start = time.milliseconds();
+        // fetch all info on all topics to avoid multiple remote calls
+        if (partitionInfo.isEmpty()) {
+            try {
+                partitionInfo.putAll(consumer.listTopics());
+            } catch (final TimeoutException e) {
+                log.warn("Could not list topics so will fall back to partition by partition fetching");
+            }
+        }
+
+        final long endTime = time.milliseconds() + partitionValidationTimeoutMs;
+        while (!hasPartition(topicPartition) && time.milliseconds() < endTime) {
+            try {
+                final List<PartitionInfo> partitions = consumer.partitionsFor(topicPartition.topic());
+                if (partitions != null) {
+                    partitionInfo.put(topicPartition.topic(), partitions);
+                }
+            } catch (final TimeoutException e) {
+                throw new StreamsException(String.format("Could not fetch partition info for topic: %s before expiration of the configured request timeout",
+                                                         topicPartition.topic()));
+            }
+        }
+
+        if (!hasPartition(topicPartition)) {
+            throw new StreamsException(String.format("Store %s's change log (%s) does not contain partition %s",
+                                                     storeName, topicPartition.topic(), topicPartition.partition()));
+        }
+        log.debug("Took {} ms to validate that partition {} exists", time.milliseconds() - start, topicPartition);
+    }
+
+    @Override
+    public void register(final StateRestorer restorer) {
+        if (restorer.offsetLimit() > 0) {
+            stateRestorers.put(restorer.partition(), restorer);
+        }
+    }
+
+    public void restore() {
+        final long start = time.milliseconds();
+        try {
+            if (!consumer.subscription().isEmpty()) {
+                throw new IllegalStateException(String.format("Restore consumer should have not subscribed to any partitions (%s) beforehand", consumer.subscription()));
+            }
+            final Map<TopicPartition, Long> endOffsets = consumer.endOffsets(stateRestorers.keySet());
+
+
+            // remove any partitions where we already have all of the data
+            final Map<TopicPartition, StateRestorer> needsRestoring = new HashMap<>();
+            for (final TopicPartition topicPartition : endOffsets.keySet()) {
+                final StateRestorer restorer = stateRestorers.get(topicPartition);
+                if (restorer.checkpoint() >= endOffsets.get(topicPartition)) {
+                    restorer.setRestoredOffset(restorer.checkpoint());
+                } else {
+                    needsRestoring.put(topicPartition, restorer);
+                }
+            }
+
+            consumer.assign(needsRestoring.keySet());
+
+            for (final StateRestorer restorer : needsRestoring.values()) {
+                if (restorer.checkpoint() != StateRestorer.NO_CHECKPOINT) {
+                    consumer.seek(restorer.partition(), restorer.checkpoint());
+                } else {
+                    consumer.seekToBeginning(Collections.singletonList(restorer.partition()));
+                }
+            }
+
+            final Set<TopicPartition> partitions = new HashSet<>(needsRestoring.keySet());
+            while (!partitions.isEmpty()) {
+                final ConsumerRecords<byte[], byte[]> allRecords = consumer.poll(10);
+                final Iterator<TopicPartition> partitionIterator = partitions.iterator();
+                while (partitionIterator.hasNext()) {
+                    restorePartition(endOffsets, allRecords, partitionIterator);
+                }
+            }
+        } finally {
+            consumer.assign(Collections.<TopicPartition>emptyList());
+            log.debug("Took {} ms to restore active state", time.milliseconds() - start);
+        }
+    }
+
+    @Override
+    public Map<TopicPartition, Long> restoredOffsets() {
+        final Map<TopicPartition, Long> restoredOffsets = new HashMap<>();
+        for (final Map.Entry<TopicPartition, StateRestorer> entry : stateRestorers.entrySet()) {
+            final StateRestorer restorer = entry.getValue();
+            if (restorer.isPersistent()) {
+                restoredOffsets.put(entry.getKey(), restorer.restoredOffset());
+            }
+        }
+        return restoredOffsets;
+    }
+
+    private void restorePartition(final Map<TopicPartition, Long> endOffsets,
+                                  final ConsumerRecords<byte[], byte[]> allRecords,
+                                  final Iterator<TopicPartition> partitionIterator) {
+        final TopicPartition topicPartition = partitionIterator.next();
+        final StateRestorer restorer = stateRestorers.get(topicPartition);
+        final Long endOffset = endOffsets.get(topicPartition);
+        final long pos = processNext(allRecords.records(topicPartition), restorer, endOffset);
+        if (restorer.hasCompleted(pos, endOffset)) {
+            if (pos > endOffset + 1) {
+                throw new IllegalStateException(
+                        String.format("Log end offset of %s should not change while restoring: old end offset %d, current offset %d",
+                                      topicPartition,
+                                      endOffset,
+                                      pos));
+            }
+            restorer.setRestoredOffset(pos);
+            partitionIterator.remove();
+        }
+    }
+
+    private long processNext(final List<ConsumerRecord<byte[], byte[]>> records, final StateRestorer restorer, final Long endOffset) {
+        for (final ConsumerRecord<byte[], byte[]> record : records) {
+            final long offset = record.offset();
+            if (restorer.hasCompleted(offset, endOffset)) {
+                return offset;
+            }
+            restorer.restore(record.key(), record.value());
+        }
+        return consumer.position(restorer.partition());
+    }
+
+    private boolean hasPartition(final TopicPartition topicPartition) {
+        final List<PartitionInfo> partitions = partitionInfo.get(topicPartition.topic());
+
+        if (partitions == null) {
+            return false;
+        }
+
+        for (final PartitionInfo partition : partitions) {
+            if (partition.partition() == topicPartition.partition()) {
+                return true;
+            }
+        }
+
+        return false;
+
+    }
+
+
+}
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java
index d95ac4bac6..983c07e3f8 100644
--- a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java
@@ -101,14 +101,14 @@ public class StreamTask extends AbstractTask implements Punctuator {
                       Collection<TopicPartition> partitions,
                       ProcessorTopology topology,
                       Consumer<byte[], byte[]> consumer,
-                      Consumer<byte[], byte[]> restoreConsumer,
+                      final ChangelogReader changelogReader,
                       StreamsConfig config,
                       StreamsMetrics metrics,
                       StateDirectory stateDirectory,
                       ThreadCache cache,
                       Time time,
                       final RecordCollector recordCollector) {
-        super(id, applicationId, partitions, topology, consumer, restoreConsumer, false, stateDirectory, cache);
+        super(id, applicationId, partitions, topology, consumer, changelogReader, false, stateDirectory, cache);
         this.punctuationQueue = new PunctuationQueue();
         this.maxBufferedSize = config.getInt(StreamsConfig.BUFFERED_RECORDS_PER_PARTITION_CONFIG);
         this.metrics = new TaskMetrics(metrics);
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java
index e9a91f6255..e5a4562c7d 100644
--- a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java
@@ -70,7 +70,6 @@ public class StreamThread extends Thread {
 
     private static final Logger log = LoggerFactory.getLogger(StreamThread.class);
     private static final AtomicInteger STREAM_THREAD_ID_SEQUENCE = new AtomicInteger(1);
-
     /**
      * Stream thread states are the possible states that a stream thread can be in.
      * A thread must only be in one state at a time
@@ -213,60 +212,11 @@ public class StreamThread extends Thread {
     private boolean processStandbyRecords = false;
 
     private ThreadCache cache;
+    private StoreChangelogReader storeChangelogReader;
 
     private final TaskCreator taskCreator = new TaskCreator();
 
-    final ConsumerRebalanceListener rebalanceListener = new ConsumerRebalanceListener() {
-        @Override
-        public void onPartitionsAssigned(Collection<TopicPartition> assignment) {
-
-            try {
-                if (state == State.PENDING_SHUTDOWN) {
-                    log.info("stream-thread [{}] New partitions [{}] assigned while shutting down.",
-                        StreamThread.this.getName(), assignment);
-                }
-                log.info("stream-thread [{}] New partitions [{}] assigned at the end of consumer rebalance.",
-                    StreamThread.this.getName(), assignment);
-
-                setStateWhenNotInPendingShutdown(State.ASSIGNING_PARTITIONS);
-                // do this first as we may have suspended standby tasks that
-                // will become active or vice versa
-                closeNonAssignedSuspendedStandbyTasks();
-                closeNonAssignedSuspendedTasks();
-                addStreamTasks(assignment);
-                addStandbyTasks();
-                lastCleanMs = time.milliseconds(); // start the cleaning cycle
-                streamsMetadataState.onChange(partitionAssignor.getPartitionsByHostState(), partitionAssignor.clusterMetadata());
-                setStateWhenNotInPendingShutdown(State.RUNNING);
-            } catch (Throwable t) {
-                rebalanceException = t;
-                throw t;
-            }
-        }
-
-        @Override
-        public void onPartitionsRevoked(Collection<TopicPartition> assignment) {
-            try {
-                if (state == State.PENDING_SHUTDOWN) {
-                    log.info("stream-thread [{}] New partitions [{}] revoked while shutting down.",
-                             StreamThread.this.getName(), assignment);
-                }
-                log.info("stream-thread [{}] partitions [{}] revoked at the beginning of consumer rebalance.",
-                         StreamThread.this.getName(), assignment);
-                setStateWhenNotInPendingShutdown(State.PARTITIONS_REVOKED);
-                lastCleanMs = Long.MAX_VALUE; // stop the cleaning cycle until partitions are assigned
-                // suspend active tasks
-                suspendTasksAndState();
-            } catch (Throwable t) {
-                rebalanceException = t;
-                throw t;
-            } finally {
-                streamsMetadataState.onChange(Collections.<HostInfo, Set<TopicPartition>>emptyMap(), partitionAssignor.clusterMetadata());
-                removeStreamTasks();
-                removeStandbyTasks();
-            }
-        }
-    };
+    final ConsumerRebalanceListener rebalanceListener;
 
     public synchronized boolean isInitialized() {
         return state == State.RUNNING;
@@ -323,7 +273,6 @@ public class StreamThread extends Thread {
         this.consumer = clientSupplier.getConsumer(consumerConfigs);
         log.info("{} Creating restore consumer client", logPrefix);
         this.restoreConsumer = clientSupplier.getRestoreConsumer(config.getRestoreConsumerConfigs(threadClientId));
-
         // initialize the task list
         // activeTasks needs to be concurrent as it can be accessed
         // by QueryableState
@@ -347,7 +296,9 @@ public class StreamThread extends Thread {
         this.timerStartedMs = time.milliseconds();
         this.lastCleanMs = Long.MAX_VALUE; // the cleaning cycle won't start until partition assignment
         this.lastCommitMs = timerStartedMs;
+        this.rebalanceListener = new RebalanceListener(time, config.getInt(ConsumerConfig.REQUEST_TIMEOUT_MS_CONFIG));
         setState(state.RUNNING);
+
     }
 
     public void partitionAssignor(StreamPartitionAssignor partitionAssignor) {
@@ -833,9 +784,15 @@ public class StreamThread extends Thread {
 
         final ProcessorTopology topology = builder.build(id.topicGroupId);
         final RecordCollector recordCollector = new RecordCollectorImpl(producer, id.toString());
-        return new StreamTask(id, applicationId, partitions, topology, consumer, restoreConsumer, config, streamsMetrics, stateDirectory, cache, time, recordCollector);
+        final long start = time.milliseconds();
+        try {
+            return new StreamTask(id, applicationId, partitions, topology, consumer, storeChangelogReader, config, streamsMetrics, stateDirectory, cache, time, recordCollector);
+        } finally {
+            log.debug("{} creation of active task {} took {} ms", logPrefix, id, time.milliseconds() - start);
+        }
     }
 
+
     private StreamTask findMatchingSuspendedTask(final TaskId taskId, final Set<TopicPartition> partitions) {
         if (suspendedTasks.containsKey(taskId)) {
             final StreamTask task = suspendedTasks.get(taskId);
@@ -947,7 +904,7 @@ public class StreamThread extends Thread {
         ProcessorTopology topology = builder.build(id.topicGroupId);
 
         if (!topology.stateStores().isEmpty()) {
-            return new StandbyTask(id, applicationId, partitions, topology, consumer, restoreConsumer, config, streamsMetrics, stateDirectory);
+            return new StandbyTask(id, applicationId, partitions, topology, consumer, storeChangelogReader, config, streamsMetrics, stateDirectory);
         } else {
             return null;
         }
@@ -1230,4 +1187,66 @@ public class StreamThread extends Thread {
         }
     }
 
+    private class RebalanceListener implements ConsumerRebalanceListener {
+        private final Time time;
+        private final int requestTimeOut;
+
+        RebalanceListener(final Time time, final int requestTimeOut) {
+            this.time = time;
+            this.requestTimeOut = requestTimeOut;
+        }
+
+        @Override
+        public void onPartitionsAssigned(Collection<TopicPartition> assignment) {
+            final long start = time.milliseconds();
+            try {
+                if (state == State.PENDING_SHUTDOWN) {
+                    log.info("stream-thread [{}] New partitions [{}] assigned while shutting down.",
+                        StreamThread.this.getName(), assignment);
+                }
+                log.info("stream-thread [{}] New partitions [{}] assigned at the end of consumer rebalance.",
+                    StreamThread.this.getName(), assignment);
+                storeChangelogReader = new StoreChangelogReader(restoreConsumer, time, requestTimeOut);
+                setStateWhenNotInPendingShutdown(State.ASSIGNING_PARTITIONS);
+                // do this first as we may have suspended standby tasks that
+                // will become active or vice versa
+                closeNonAssignedSuspendedStandbyTasks();
+                closeNonAssignedSuspendedTasks();
+                addStreamTasks(assignment);
+                storeChangelogReader.restore();
+                addStandbyTasks();
+                streamsMetadataState.onChange(partitionAssignor.getPartitionsByHostState(), partitionAssignor.clusterMetadata());
+                lastCleanMs = time.milliseconds(); // start the cleaning cycle
+                setStateWhenNotInPendingShutdown(State.RUNNING);
+            } catch (Throwable t) {
+                rebalanceException = t;
+                throw t;
+            } finally {
+                log.debug("{} partition assignment took {} ms", logPrefix, time.milliseconds() - start);
+            }
+        }
+
+        @Override
+        public void onPartitionsRevoked(Collection<TopicPartition> assignment) {
+            try {
+                if (state == State.PENDING_SHUTDOWN) {
+                    log.info("stream-thread [{}] New partitions [{}] revoked while shutting down.",
+                             StreamThread.this.getName(), assignment);
+                }
+                log.info("stream-thread [{}] partitions [{}] revoked at the beginning of consumer rebalance.",
+                         StreamThread.this.getName(), assignment);
+                setStateWhenNotInPendingShutdown(State.PARTITIONS_REVOKED);
+                lastCleanMs = Long.MAX_VALUE; // stop the cleaning cycle until partitions are assigned
+                // suspend active tasks
+                suspendTasksAndState();
+            } catch (Throwable t) {
+                rebalanceException = t;
+                throw t;
+            } finally {
+                streamsMetadataState.onChange(Collections.<HostInfo, Set<TopicPartition>>emptyMap(), partitionAssignor.clusterMetadata());
+                removeStreamTasks();
+                removeStandbyTasks();
+            }
+        }
+    }
 }
diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java
index 911c6a89cf..5e8b69e02b 100644
--- a/streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java
@@ -188,7 +188,7 @@ public class QueryableStateIntegrationTest {
     @After
     public void shutdown() throws IOException {
         if (kafkaStreams != null) {
-            kafkaStreams.close();
+            kafkaStreams.close(30, TimeUnit.SECONDS);
         }
         IntegrationTestUtils.purgeLocalStreamsState(streamsConfiguration);
     }
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/AbstractTaskTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/AbstractTaskTest.java
index f288f984d1..dd2ed009fd 100644
--- a/streams/src/test/java/org/apache/kafka/streams/processor/internals/AbstractTaskTest.java
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/AbstractTaskTest.java
@@ -27,6 +27,7 @@ import org.apache.kafka.common.errors.AuthorizationException;
 import org.apache.kafka.common.errors.WakeupException;
 import org.apache.kafka.common.metrics.Metrics;
 import org.apache.kafka.common.utils.MockTime;
+import org.apache.kafka.common.utils.Time;
 import org.apache.kafka.streams.errors.ProcessorStateException;
 import org.apache.kafka.streams.processor.StateStore;
 import org.apache.kafka.streams.processor.TaskId;
@@ -71,7 +72,7 @@ public class AbstractTaskTest {
                                                       Collections.<String, String>emptyMap(),
                                                       Collections.<StateStore>emptyList()),
                                 consumer,
-                                consumer,
+                                new StoreChangelogReader(consumer, Time.SYSTEM, 5000),
                                 false,
                                 new StateDirectory("app", TestUtils.tempDirectory().getPath(), time),
                                 new ThreadCache("testCache", 0, new MockStreamsMetrics(new Metrics()))) {
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java
index f1d3090b2c..31a5d579eb 100644
--- a/streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java
@@ -18,25 +18,16 @@
 package org.apache.kafka.streams.processor.internals;
 
 import org.apache.kafka.clients.consumer.ConsumerRecord;
-import org.apache.kafka.clients.consumer.ConsumerRecords;
-import org.apache.kafka.clients.consumer.MockConsumer;
-import org.apache.kafka.clients.consumer.OffsetResetStrategy;
-import org.apache.kafka.common.Node;
-import org.apache.kafka.common.PartitionInfo;
 import org.apache.kafka.common.TopicPartition;
-import org.apache.kafka.common.errors.TimeoutException;
-import org.apache.kafka.common.record.TimestampType;
-import org.apache.kafka.common.serialization.IntegerSerializer;
 import org.apache.kafka.common.serialization.Serdes;
-import org.apache.kafka.common.serialization.Serializer;
 import org.apache.kafka.common.utils.MockTime;
 import org.apache.kafka.common.utils.Utils;
 import org.apache.kafka.streams.errors.LockException;
 import org.apache.kafka.streams.errors.ProcessorStateException;
-import org.apache.kafka.streams.errors.StreamsException;
 import org.apache.kafka.streams.processor.TaskId;
 import org.apache.kafka.streams.state.StateSerdes;
 import org.apache.kafka.streams.state.internals.OffsetCheckpoint;
+import org.apache.kafka.test.MockChangelogReader;
 import org.apache.kafka.test.MockProcessorContext;
 import org.apache.kafka.test.MockStateStoreSupplier;
 import org.apache.kafka.test.NoOpRecordCollector;
@@ -50,14 +41,10 @@ import java.io.IOException;
 import java.nio.channels.FileChannel;
 import java.nio.channels.FileLock;
 import java.nio.file.StandardOpenOption;
-import java.util.ArrayList;
-import java.util.Collection;
 import java.util.Collections;
 import java.util.HashMap;
-import java.util.List;
 import java.util.Map;
 import java.util.Set;
-import java.util.concurrent.atomic.AtomicInteger;
 
 import static org.hamcrest.CoreMatchers.equalTo;
 import static org.hamcrest.MatcherAssert.assertThat;
@@ -65,139 +52,10 @@ import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertNotNull;
 import static org.junit.Assert.assertNull;
 import static org.junit.Assert.assertTrue;
-import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.fail;
 
 public class ProcessorStateManagerTest {
 
-    public static class MockRestoreConsumer extends MockConsumer<byte[], byte[]> {
-        private final Serializer<Integer> serializer = new IntegerSerializer();
-
-        private TopicPartition assignedPartition = null;
-        private long seekOffset = -1L;
-        private boolean seekToBeginingCalled = false;
-        private boolean seekToEndCalled = false;
-        private long endOffset = 0L;
-        private long currentOffset = 0L;
-
-        private ArrayList<ConsumerRecord<byte[], byte[]>> recordBuffer = new ArrayList<>();
-
-        MockRestoreConsumer() {
-            super(OffsetResetStrategy.EARLIEST);
-
-            reset();
-        }
-
-        // reset this mock restore consumer for a state store registration
-        public void reset() {
-            assignedPartition = null;
-            seekOffset = -1L;
-            seekToBeginingCalled = false;
-            seekToEndCalled = false;
-            endOffset = 0L;
-            recordBuffer.clear();
-        }
-
-        // buffer a record (we cannot use addRecord because we need to add records before assigning a partition)
-        public void bufferRecord(ConsumerRecord<Integer, Integer> record) {
-            recordBuffer.add(
-                new ConsumerRecord<>(record.topic(), record.partition(), record.offset(), 0L,
-                    TimestampType.CREATE_TIME, 0L, 0, 0,
-                    serializer.serialize(record.topic(), record.key()),
-                    serializer.serialize(record.topic(), record.value())));
-            endOffset = record.offset();
-
-            super.updateEndOffsets(Collections.singletonMap(assignedPartition, endOffset));
-        }
-
-        @Override
-        public synchronized void assign(Collection<TopicPartition> partitions) {
-            int numPartitions = partitions.size();
-            if (numPartitions > 1)
-                throw new IllegalArgumentException("RestoreConsumer: more than one partition specified");
-
-            if (numPartitions == 1) {
-                if (assignedPartition != null)
-                    throw new IllegalStateException("RestoreConsumer: partition already assigned");
-                assignedPartition = partitions.iterator().next();
-
-                // set the beginning offset to 0
-                // NOTE: this is users responsible to set the initial lEO.
-                super.updateBeginningOffsets(Collections.singletonMap(assignedPartition, 0L));
-            }
-
-            super.assign(partitions);
-        }
-
-        @Override
-        public ConsumerRecords<byte[], byte[]> poll(long timeout) {
-            // add buffered records to MockConsumer
-            for (ConsumerRecord<byte[], byte[]> record : recordBuffer) {
-                super.addRecord(record);
-            }
-            recordBuffer.clear();
-
-            ConsumerRecords<byte[], byte[]> records = super.poll(timeout);
-
-            // set the current offset
-            Iterable<ConsumerRecord<byte[], byte[]>> partitionRecords = records.records(assignedPartition);
-            for (ConsumerRecord<byte[], byte[]> record : partitionRecords) {
-                currentOffset = record.offset();
-            }
-
-            return records;
-        }
-
-        @Override
-        public synchronized long position(TopicPartition partition) {
-            if (!partition.equals(assignedPartition))
-                throw new IllegalStateException("RestoreConsumer: unassigned partition");
-
-            return currentOffset;
-        }
-
-        @Override
-        public synchronized void seek(TopicPartition partition, long offset) {
-            if (offset < 0)
-                throw new IllegalArgumentException("RestoreConsumer: offset should not be negative");
-
-            if (seekOffset >= 0)
-                throw new IllegalStateException("RestoreConsumer: offset already seeked");
-
-            seekOffset = offset;
-            currentOffset = offset;
-            super.seek(partition, offset);
-        }
-
-        @Override
-        public synchronized void seekToBeginning(Collection<TopicPartition> partitions) {
-            if (partitions.size() != 1)
-                throw new IllegalStateException("RestoreConsumer: other than one partition specified");
-
-            for (TopicPartition partition : partitions) {
-                if (!partition.equals(assignedPartition))
-                    throw new IllegalStateException("RestoreConsumer: seek-to-end not on the assigned partition");
-            }
-
-            seekToBeginingCalled = true;
-            currentOffset = 0L;
-        }
-
-        @Override
-        public synchronized void seekToEnd(Collection<TopicPartition> partitions) {
-            if (partitions.size() != 1)
-                throw new IllegalStateException("RestoreConsumer: other than one partition specified");
-
-            for (TopicPartition partition : partitions) {
-                if (!partition.equals(assignedPartition))
-                    throw new IllegalStateException("RestoreConsumer: seek-to-end not on the assigned partition");
-            }
-
-            seekToEndCalled = true;
-            currentOffset = endOffset;
-        }
-    }
-
     private final Set<TopicPartition> noPartitions = Collections.emptySet();
     private final String applicationId = "test-application";
     private final String persistentStoreName = "persistentStore";
@@ -211,7 +69,7 @@ public class ProcessorStateManagerTest {
     private final String changelogTopic = ProcessorStateManager.storeChangelogTopic(applicationId, storeName);
     private final TopicPartition changelogTopicPartition = new TopicPartition(changelogTopic, 0);
     private final TaskId taskId = new TaskId(0, 1);
-    private final MockRestoreConsumer restoreConsumer = new MockRestoreConsumer();
+    private final MockChangelogReader changelogReader = new MockChangelogReader();
     private final MockStateStoreSupplier.MockStateStore mockStateStore = new MockStateStoreSupplier.MockStateStore(storeName, true);
     private File baseDir;
     private File checkpointFile;
@@ -225,12 +83,6 @@ public class ProcessorStateManagerTest {
         stateDirectory = new StateDirectory(applicationId, baseDir.getPath(), new MockTime());
         checkpointFile = new File(stateDirectory.directoryForTask(taskId), ProcessorStateManager.CHECKPOINT_FILE_NAME);
         checkpoint = new OffsetCheckpoint(checkpointFile);
-        restoreConsumer.updatePartitions(persistentStoreTopicName, Utils.mkList(
-                new PartitionInfo(persistentStoreTopicName, 1, Node.noNode(), new Node[0], new Node[0])
-        ));
-        restoreConsumer.updatePartitions(nonPersistentStoreTopicName, Utils.mkList(
-                new PartitionInfo(nonPersistentStoreTopicName, 1, Node.noNode(), new Node[0], new Node[0])
-        ));
     }
 
     @After
@@ -238,120 +90,39 @@ public class ProcessorStateManagerTest {
         Utils.delete(baseDir);
     }
 
-    @Test(expected = StreamsException.class)
-    public void testNoTopic() throws IOException {
-        MockStateStoreSupplier.MockStateStore mockStateStore = new MockStateStoreSupplier.MockStateStore(nonPersistentStoreName, false);
-
-        ProcessorStateManager stateMgr = new ProcessorStateManager(new TaskId(0, 1), noPartitions, new MockRestoreConsumer(), false, stateDirectory, new HashMap<String, String>() {
-            {
-                put(nonPersistentStoreName, nonPersistentStoreName);
-            }
-        });
-
-        try {
-            stateMgr.register(mockStateStore, true, mockStateStore.stateRestoreCallback);
-        } finally {
-            stateMgr.close(Collections.<TopicPartition, Long>emptyMap());
-        }
-    }
-
     @Test
     public void testRegisterPersistentStore() throws IOException {
         final TaskId taskId = new TaskId(0, 2);
-        long lastCheckpointedOffset = 10L;
-
-        OffsetCheckpoint checkpoint = new OffsetCheckpoint(new File(stateDirectory.directoryForTask(taskId), ProcessorStateManager.CHECKPOINT_FILE_NAME));
-        checkpoint.write(Collections.singletonMap(new TopicPartition(persistentStoreTopicName, 2), lastCheckpointedOffset));
-
-        MockRestoreConsumer restoreConsumer = new MockRestoreConsumer();
-
-        restoreConsumer.updatePartitions(persistentStoreTopicName, Utils.mkList(
-                new PartitionInfo(persistentStoreTopicName, 1, Node.noNode(), new Node[0], new Node[0]),
-                new PartitionInfo(persistentStoreTopicName, 2, Node.noNode(), new Node[0], new Node[0])
-        ));
-
-        TopicPartition partition = new TopicPartition(persistentStoreTopicName, 2);
-        restoreConsumer.updateEndOffsets(Collections.singletonMap(partition, 13L));
 
         MockStateStoreSupplier.MockStateStore persistentStore = new MockStateStoreSupplier.MockStateStore("persistentStore", true); // persistent store
-
-        ProcessorStateManager stateMgr = new ProcessorStateManager(taskId, noPartitions, restoreConsumer, false, stateDirectory, new HashMap<String, String>() {
+        ProcessorStateManager stateMgr = new ProcessorStateManager(taskId, noPartitions, false, stateDirectory, new HashMap<String, String>() {
             {
                 put(persistentStoreName, persistentStoreTopicName);
                 put(nonPersistentStoreName, nonPersistentStoreName);
             }
-        });
+        }, changelogReader);
         try {
-            restoreConsumer.reset();
-
-            ArrayList<Integer> expectedKeys = new ArrayList<>();
-            long offset;
-            for (int i = 1; i <= 3; i++) {
-                offset = (long) i;
-                int key = i * 10;
-                expectedKeys.add(key);
-                restoreConsumer.bufferRecord(
-                        new ConsumerRecord<>(persistentStoreTopicName, 2, 0L, offset, TimestampType.CREATE_TIME, 0L, 0, 0, key, 0)
-                );
-            }
 
             stateMgr.register(persistentStore, true, persistentStore.stateRestoreCallback);
-
-            assertEquals(new TopicPartition(persistentStoreTopicName, 2), restoreConsumer.assignedPartition);
-            assertEquals(lastCheckpointedOffset, restoreConsumer.seekOffset);
-            assertFalse(restoreConsumer.seekToBeginingCalled);
-            assertTrue(restoreConsumer.seekToEndCalled);
-            assertEquals(expectedKeys, persistentStore.keys);
-
+            assertTrue(changelogReader.wasRegistered(new TopicPartition(persistentStoreTopicName, 2)));
         } finally {
             stateMgr.close(Collections.<TopicPartition, Long>emptyMap());
         }
-
-
     }
 
     @Test
     public void testRegisterNonPersistentStore() throws IOException {
-        long lastCheckpointedOffset = 10L;
-
-        OffsetCheckpoint checkpoint = new OffsetCheckpoint(new File(baseDir, ProcessorStateManager.CHECKPOINT_FILE_NAME));
-        checkpoint.write(Collections.singletonMap(new TopicPartition(persistentStoreTopicName, 2), lastCheckpointedOffset));
-
-        restoreConsumer.updatePartitions(nonPersistentStoreTopicName, Utils.mkList(
-                new PartitionInfo(nonPersistentStoreTopicName, 1, Node.noNode(), new Node[0], new Node[0]),
-                new PartitionInfo(nonPersistentStoreTopicName, 2, Node.noNode(), new Node[0], new Node[0])
-        ));
-
-        TopicPartition partition = new TopicPartition(persistentStoreTopicName, 2);
-        restoreConsumer.updateEndOffsets(Collections.singletonMap(partition, 13L));
-
-        ProcessorStateManager stateMgr = new ProcessorStateManager(new TaskId(0, 2), noPartitions, restoreConsumer, false, stateDirectory, new HashMap<String, String>() {
+        MockStateStoreSupplier.MockStateStore nonPersistentStore = new MockStateStoreSupplier.MockStateStore(nonPersistentStoreName, false); // non persistent store
+        ProcessorStateManager stateMgr = new ProcessorStateManager(new TaskId(0, 2), noPartitions, false, stateDirectory, new HashMap<String, String>() {
             {
                 put(persistentStoreName, persistentStoreTopicName);
                 put(nonPersistentStoreName, nonPersistentStoreTopicName);
             }
-        });
+        }, changelogReader);
         try {
-            restoreConsumer.reset();
-
-            ArrayList<Integer> expectedKeys = new ArrayList<>();
-            long offset;
-            for (int i = 1; i <= 3; i++) {
-                offset = (long) (i + 100);
-                int key = i;
-                expectedKeys.add(i);
-                restoreConsumer.bufferRecord(
-                        new ConsumerRecord<>(nonPersistentStoreTopicName, 2, 0L, offset, TimestampType.CREATE_TIME, 0L, 0, 0, key, 0)
-                );
-            }
 
             stateMgr.register(nonPersistentStore, true, nonPersistentStore.stateRestoreCallback);
-
-            assertEquals(new TopicPartition(nonPersistentStoreTopicName, 2), restoreConsumer.assignedPartition);
-            assertEquals(0L, restoreConsumer.seekOffset);
-            assertTrue(restoreConsumer.seekToBeginingCalled);
-            assertTrue(restoreConsumer.seekToEndCalled);
-            assertEquals(expectedKeys, nonPersistentStore.keys);
+            assertTrue(changelogReader.wasRegistered(new TopicPartition(nonPersistentStoreTopicName, 2)));
 
         } finally {
             stateMgr.close(Collections.<TopicPartition, Long>emptyMap());
@@ -361,8 +132,6 @@ public class ProcessorStateManagerTest {
     @Test
     public void testChangeLogOffsets() throws IOException {
         final TaskId taskId = new TaskId(0, 0);
-        final OffsetCheckpoint offsetCheckpoint = new OffsetCheckpoint(
-                new File(stateDirectory.directoryForTask(taskId), ProcessorStateManager.CHECKPOINT_FILE_NAME));
         long lastCheckpointedOffset = 10L;
         String storeName1 = "store1";
         String storeName2 = "store2";
@@ -377,28 +146,13 @@ public class ProcessorStateManagerTest {
         storeToChangelogTopic.put(storeName2, storeTopicName2);
         storeToChangelogTopic.put(storeName3, storeTopicName3);
 
-        offsetCheckpoint.write(Collections.singletonMap(new TopicPartition(storeTopicName1, 0), lastCheckpointedOffset));
-
-        restoreConsumer.updatePartitions(storeTopicName1, Utils.mkList(
-                new PartitionInfo(storeTopicName1, 0, Node.noNode(), new Node[0], new Node[0])
-        ));
-        restoreConsumer.updatePartitions(storeTopicName2, Utils.mkList(
-                new PartitionInfo(storeTopicName2, 0, Node.noNode(), new Node[0], new Node[0])
-        ));
-        restoreConsumer.updatePartitions(storeTopicName3, Utils.mkList(
-                new PartitionInfo(storeTopicName3, 0, Node.noNode(), new Node[0], new Node[0]),
-                new PartitionInfo(storeTopicName3, 1, Node.noNode(), new Node[0], new Node[0])
-        ));
+        OffsetCheckpoint checkpoint = new OffsetCheckpoint(new File(stateDirectory.directoryForTask(taskId), ProcessorStateManager.CHECKPOINT_FILE_NAME));
+        checkpoint.write(Collections.singletonMap(new TopicPartition(storeTopicName1, 0), lastCheckpointedOffset));
 
         TopicPartition partition1 = new TopicPartition(storeTopicName1, 0);
         TopicPartition partition2 = new TopicPartition(storeTopicName2, 0);
         TopicPartition partition3 = new TopicPartition(storeTopicName3, 1);
 
-        Map<TopicPartition, Long> endOffsets = new HashMap<>();
-        endOffsets.put(partition1, 13L);
-        endOffsets.put(partition2, 17L);
-        restoreConsumer.updateEndOffsets(endOffsets);
-
         MockStateStoreSupplier.MockStateStore store1 = new MockStateStoreSupplier.MockStateStore(storeName1, true);
         MockStateStoreSupplier.MockStateStore store2 = new MockStateStoreSupplier.MockStateStore(storeName2, true);
         MockStateStoreSupplier.MockStateStore store3 = new MockStateStoreSupplier.MockStateStore(storeName3, true);
@@ -406,10 +160,8 @@ public class ProcessorStateManagerTest {
         // if there is an source partition, inherit the partition id
         Set<TopicPartition> sourcePartitions = Utils.mkSet(new TopicPartition(storeTopicName3, 1));
 
-        ProcessorStateManager stateMgr = new ProcessorStateManager(taskId, sourcePartitions, restoreConsumer, true, stateDirectory, storeToChangelogTopic); // standby
+        ProcessorStateManager stateMgr = new ProcessorStateManager(taskId, sourcePartitions, true, stateDirectory, storeToChangelogTopic, changelogReader); // standby
         try {
-            restoreConsumer.reset();
-
             stateMgr.register(store1, true, store1.stateRestoreCallback);
             stateMgr.register(store2, true, store2.stateRestoreCallback);
             stateMgr.register(store3, true, store3.stateRestoreCallback);
@@ -427,17 +179,17 @@ public class ProcessorStateManagerTest {
         } finally {
             stateMgr.close(Collections.<TopicPartition, Long>emptyMap());
         }
-
     }
 
     @Test
     public void testGetStore() throws IOException {
-        final ProcessorStateManager stateMgr = new ProcessorStateManager(new TaskId(0, 1), noPartitions, restoreConsumer, false, stateDirectory, Collections.<String, String>emptyMap());
+        final MockStateStoreSupplier.MockStateStore mockStateStore = new MockStateStoreSupplier.MockStateStore(nonPersistentStoreName, false);
+        final ProcessorStateManager stateMgr = new ProcessorStateManager(new TaskId(0, 1), noPartitions, false, stateDirectory, Collections.<String, String>emptyMap(), changelogReader);
         try {
-            stateMgr.register(nonPersistentStore, true, nonPersistentStore.stateRestoreCallback);
+            stateMgr.register(mockStateStore, true, mockStateStore.stateRestoreCallback);
 
             assertNull(stateMgr.getStore("noSuchStore"));
-            assertEquals(nonPersistentStore, stateMgr.getStore(nonPersistentStoreName));
+            assertEquals(mockStateStore, stateMgr.getStore(nonPersistentStoreName));
 
         } finally {
             stateMgr.close(Collections.<TopicPartition, Long>emptyMap());
@@ -446,7 +198,6 @@ public class ProcessorStateManagerTest {
 
     @Test
     public void testFlushAndClose() throws IOException {
-        // write an empty checkpoint file
         checkpoint.write(Collections.<TopicPartition, Long>emptyMap());
 
         // set up ack'ed offsets
@@ -455,20 +206,17 @@ public class ProcessorStateManagerTest {
         ackedOffsets.put(new TopicPartition(nonPersistentStoreTopicName, 1), 456L);
         ackedOffsets.put(new TopicPartition(ProcessorStateManager.storeChangelogTopic(applicationId, "otherTopic"), 1), 789L);
 
-        ProcessorStateManager stateMgr = new ProcessorStateManager(taskId, noPartitions, restoreConsumer, false, stateDirectory, new HashMap<String, String>() {
+        ProcessorStateManager stateMgr = new ProcessorStateManager(taskId, noPartitions, false, stateDirectory, new HashMap<String, String>() {
             {
                 put(persistentStoreName, persistentStoreTopicName);
                 put(nonPersistentStoreName, nonPersistentStoreTopicName);
             }
-        });
+        }, changelogReader);
         try {
             // make sure the checkpoint file isn't deleted
             assertTrue(checkpointFile.exists());
 
-            restoreConsumer.reset();
             stateMgr.register(persistentStore, true, persistentStore.stateRestoreCallback);
-
-            restoreConsumer.reset();
             stateMgr.register(nonPersistentStore, true, nonPersistentStore.stateRestoreCallback);
         } finally {
             // close the state manager with the ack'ed offsets
@@ -486,12 +234,12 @@ public class ProcessorStateManagerTest {
         // the checkpoint file should contain an offset from the persistent store only.
         final Map<TopicPartition, Long> checkpointedOffsets = checkpoint.read();
         assertEquals(1, checkpointedOffsets.size());
-        assertEquals(new Long(123L + 1L), checkpointedOffsets.get(new TopicPartition(persistentStoreTopicName, 1)));
+        assertEquals(new Long(124), checkpointedOffsets.get(new TopicPartition(persistentStoreTopicName, 1)));
     }
 
     @Test
     public void shouldRegisterStoreWithoutLoggingEnabledAndNotBackedByATopic() throws Exception {
-        ProcessorStateManager stateMgr = new ProcessorStateManager(new TaskId(0, 1), noPartitions, new MockRestoreConsumer(), false, stateDirectory, Collections.<String, String>emptyMap());
+        final ProcessorStateManager stateMgr = new ProcessorStateManager(new TaskId(0, 1), noPartitions, false, stateDirectory, Collections.<String, String>emptyMap(), changelogReader);
         stateMgr.register(nonPersistentStore, false, nonPersistentStore.stateRestoreCallback);
         assertNotNull(stateMgr.getStore(nonPersistentStoreName));
     }
@@ -501,14 +249,8 @@ public class ProcessorStateManagerTest {
         final Map<TopicPartition, Long> offsets = Collections.singletonMap(persistentStorePartition, 99L);
         checkpoint.write(offsets);
 
-        final ProcessorStateManager stateMgr = new ProcessorStateManager(taskId,
-                                                                         noPartitions,
-                                                                         restoreConsumer,
-                                                                         false,
-                                                                         stateDirectory,
-                                                                         Collections.<String, String>emptyMap());
-
-        restoreConsumer.reset();
+        final MockStateStoreSupplier.MockStateStore persistentStore = new MockStateStoreSupplier.MockStateStore(persistentStoreName, true);
+        final ProcessorStateManager stateMgr = new ProcessorStateManager(taskId, noPartitions, false, stateDirectory, Collections.<String, String>emptyMap(), changelogReader);
         stateMgr.register(persistentStore, true, persistentStore.stateRestoreCallback);
         stateMgr.close(null);
         final Map<TopicPartition, Long> read = checkpoint.read();
@@ -519,12 +261,11 @@ public class ProcessorStateManagerTest {
     public void shouldWriteCheckpointForPersistentLogEnabledStore() throws Exception {
         final ProcessorStateManager stateMgr = new ProcessorStateManager(taskId,
                                                                          noPartitions,
-                                                                         restoreConsumer,
                                                                          false,
                                                                          stateDirectory,
                                                                          Collections.singletonMap(persistentStore.name(),
-                                                                                                  persistentStoreTopicName));
-        restoreConsumer.reset();
+                                                                                                  persistentStoreTopicName),
+                                                                         changelogReader);
         stateMgr.register(persistentStore, true, persistentStore.stateRestoreCallback);
 
 
@@ -537,22 +278,21 @@ public class ProcessorStateManagerTest {
     public void shouldWriteCheckpointForStandbyReplica() throws Exception {
         final ProcessorStateManager stateMgr = new ProcessorStateManager(taskId,
                                                                          noPartitions,
-                                                                         restoreConsumer,
                                                                          true,
                                                                          stateDirectory,
                                                                          Collections.singletonMap(persistentStore.name(),
-                                                                                                  persistentStoreTopicName));
+                                                                                                  persistentStoreTopicName),
+                                                                         changelogReader);
 
-        restoreConsumer.reset();
         stateMgr.register(persistentStore, true, persistentStore.stateRestoreCallback);
         final byte[] bytes = Serdes.Integer().serializer().serialize("", 10);
         stateMgr.updateStandbyStates(persistentStorePartition,
                                      Collections.singletonList(
                                              new ConsumerRecord<>(persistentStorePartition.topic(),
-                                                                                persistentStorePartition.partition(),
-                                                                                888L,
-                                                                                bytes,
-                                                                                bytes)));
+                                                                  persistentStorePartition.partition(),
+                                                                  888L,
+                                                                  bytes,
+                                                                  bytes)));
 
         stateMgr.checkpoint(Collections.<TopicPartition, Long>emptyMap());
 
@@ -565,19 +305,15 @@ public class ProcessorStateManagerTest {
     public void shouldNotWriteCheckpointForNonPersistent() throws Exception {
         final TopicPartition topicPartition = new TopicPartition(nonPersistentStoreTopicName, 1);
 
-        restoreConsumer.updatePartitions(nonPersistentStoreTopicName, Utils.mkList(
-                new PartitionInfo(nonPersistentStoreTopicName, 1, Node.noNode(), new Node[0], new Node[0])
-        ));
 
         final ProcessorStateManager stateMgr = new ProcessorStateManager(taskId,
                                                                          noPartitions,
-                                                                         restoreConsumer,
                                                                          true,
                                                                          stateDirectory,
                                                                          Collections.singletonMap(nonPersistentStoreName,
-                                                                                                  nonPersistentStoreTopicName));
+                                                                                                  nonPersistentStoreTopicName),
+                                                                         changelogReader);
 
-        restoreConsumer.reset();
         stateMgr.register(nonPersistentStore, true, nonPersistentStore.stateRestoreCallback);
         stateMgr.checkpoint(Collections.singletonMap(topicPartition, 876L));
 
@@ -589,10 +325,10 @@ public class ProcessorStateManagerTest {
     public void shouldNotWriteCheckpointForStoresWithoutChangelogTopic() throws Exception {
         final ProcessorStateManager stateMgr = new ProcessorStateManager(taskId,
                                                                          noPartitions,
-                                                                         restoreConsumer,
                                                                          true,
                                                                          stateDirectory,
-                                                                         Collections.<String, String>emptyMap());
+                                                                         Collections.<String, String>emptyMap(),
+                                                                         changelogReader);
 
         stateMgr.register(persistentStore, true, persistentStore.stateRestoreCallback);
 
@@ -613,7 +349,7 @@ public class ProcessorStateManagerTest {
         final FileLock lock = channel.lock();
 
         try {
-            new ProcessorStateManager(taskId, noPartitions, restoreConsumer, false, stateDirectory, Collections.<String, String>emptyMap());
+            new ProcessorStateManager(taskId, noPartitions, false, stateDirectory, Collections.<String, String>emptyMap(), changelogReader);
             fail("Should have thrown LockException");
         } catch (final LockException e) {
            // pass
@@ -627,10 +363,9 @@ public class ProcessorStateManagerTest {
     public void shouldThrowIllegalArgumentExceptionIfStoreNameIsSameAsCheckpointFileName() throws Exception {
         final ProcessorStateManager stateManager = new ProcessorStateManager(taskId,
                                                                              noPartitions,
-                                                                             restoreConsumer,
                                                                              false,
                                                                              stateDirectory,
-                                                                             Collections.<String, String>emptyMap());
+                                                                             Collections.<String, String>emptyMap(), changelogReader);
 
         try {
             stateManager.register(new MockStateStoreSupplier.MockStateStore(ProcessorStateManager.CHECKPOINT_FILE_NAME, true), true, null);
@@ -644,10 +379,9 @@ public class ProcessorStateManagerTest {
     public void shouldThrowIllegalArgumentExceptionOnRegisterWhenStoreHasAlreadyBeenRegistered() throws Exception {
         final ProcessorStateManager stateManager = new ProcessorStateManager(taskId,
                                                                              noPartitions,
-                                                                             restoreConsumer,
                                                                              false,
                                                                              stateDirectory,
-                                                                             Collections.<String, String>emptyMap());
+                                                                             Collections.<String, String>emptyMap(), changelogReader);
         stateManager.register(mockStateStore, false, null);
 
         try {
@@ -659,142 +393,15 @@ public class ProcessorStateManagerTest {
         
     }
 
-    @Test
-    public void shouldThrowStreamsExceptionWhenRestoreConsumerThrowsTimeoutException() throws Exception {
-        final MockRestoreConsumer mockRestoreConsumer = new MockRestoreConsumer() {
-            @Override
-            public List<PartitionInfo> partitionsFor(final String topic) {
-                throw new TimeoutException("KABOOM!");
-            }
-        };
-        final ProcessorStateManager stateManager = new ProcessorStateManager(taskId,
-                                                                             noPartitions,
-                                                                             mockRestoreConsumer,
-                                                                             false,
-                                                                             stateDirectory,
-                                                                             Collections.singletonMap(storeName, changelogTopic));
-        try {
-            stateManager.register(mockStateStore, false, null);
-            fail("should have thrown StreamsException due to timeout exception");
-        } catch (final StreamsException e) {
-            // pass
-        }
-    }
-
-    @Test
-    public void shouldThrowStreamsExceptionWhenRestoreConsumerReturnsNullPartitions() throws Exception {
-        final MockRestoreConsumer mockRestoreConsumer = new MockRestoreConsumer() {
-            @Override
-            public List<PartitionInfo> partitionsFor(final String topic) {
-                return null;
-            }
-        };
-        final ProcessorStateManager stateManager = new ProcessorStateManager(taskId,
-                                                                             noPartitions,
-                                                                             mockRestoreConsumer,
-                                                                             false,
-                                                                             stateDirectory,
-                                                                             Collections.singletonMap(storeName, changelogTopic));
-        try {
-            stateManager.register(mockStateStore, false, null);
-            fail("should have thrown StreamsException due to timeout exception");
-        } catch (final StreamsException e) {
-            // pass
-        }
-    }
-
-    @Test
-    public void shouldThrowStreamsExceptionWhenPartitionForTopicNotFound() throws Exception {
-        final MockRestoreConsumer mockRestoreConsumer = new MockRestoreConsumer() {
-            @Override
-            public List<PartitionInfo> partitionsFor(final String topic) {
-                return Collections.singletonList(new PartitionInfo(changelogTopic, 0, null, null, null));
-            }
-        };
-        final ProcessorStateManager stateManager = new ProcessorStateManager(taskId,
-                                                                             Collections.singleton(new TopicPartition(changelogTopic, 1)),
-                                                                             mockRestoreConsumer,
-                                                                             false,
-                                                                             stateDirectory,
-                                                                             Collections.singletonMap(storeName, changelogTopic));
-
-        try {
-            stateManager.register(mockStateStore, false, null);
-            fail("should have thrown StreamsException due to partition for topic not found");
-        } catch (final StreamsException e) {
-            // pass
-        }
-    }
-
-    @Test
-    public void shouldThrowIllegalStateExceptionWhenRestoringStateAndSubscriptionsNonEmpty() throws Exception {
-        final MockRestoreConsumer mockRestoreConsumer = new MockRestoreConsumer() {
-            @Override
-            public List<PartitionInfo> partitionsFor(final String topic) {
-                return Collections.singletonList(new PartitionInfo(changelogTopic, 0, null, null, null));
-            }
-        };
-        final ProcessorStateManager stateManager = new ProcessorStateManager(taskId,
-                                                                             Collections.singleton(changelogTopicPartition),
-                                                                             mockRestoreConsumer,
-                                                                             false,
-                                                                             stateDirectory,
-                                                                             Collections.singletonMap(storeName, changelogTopic));
-
-        mockRestoreConsumer.subscribe(Collections.singleton("sometopic"));
-
-        try {
-            stateManager.register(mockStateStore, false, null);
-            fail("should throw IllegalStateException when restore consumer has non-empty subscriptions");
-        } catch (final IllegalStateException e) {
-            // pass
-        }
-    }
-
-    @Test
-    public void shouldThrowIllegalStateExceptionWhenRestoreConsumerPositionGreaterThanEndOffset() throws Exception {
-        final AtomicInteger position = new AtomicInteger(10);
-        final MockRestoreConsumer mockRestoreConsumer = new MockRestoreConsumer() {
-            @Override
-            public synchronized long position(final TopicPartition partition) {
-                // need to make the end position change to trigger the exception
-                return position.getAndIncrement();
-            }
-        };
-
-        mockRestoreConsumer.updatePartitions(changelogTopic, Collections.singletonList(new PartitionInfo(changelogTopic, 0, null, null, null)));
-
-        final ProcessorStateManager stateManager = new ProcessorStateManager(taskId,
-                                                                             Collections.singleton(changelogTopicPartition),
-                                                                             mockRestoreConsumer,
-                                                                             false,
-                                                                             stateDirectory,
-                                                                             Collections.singletonMap(storeName, changelogTopic));
-
-        stateManager.putOffsetLimit(changelogTopicPartition, 1);
-        // add a record with an offset less than the limit of 1
-        mockRestoreConsumer.bufferRecord(new ConsumerRecord<>(changelogTopic, 0, 0, 1, 1));
-
-
-        try {
-            stateManager.register(mockStateStore, false, mockStateStore.stateRestoreCallback);
-            fail("should have thrown IllegalStateException as end offset has changed");
-        } catch (final IllegalStateException e) {
-            // pass
-        }
-
-    }
-
     @Test
     public void shouldThrowProcessorStateExceptionOnCloseIfStoreThrowsAnException() throws Exception {
-        restoreConsumer.updatePartitions(changelogTopic, Collections.singletonList(new PartitionInfo(changelogTopic, 0, null, null, null)));
 
         final ProcessorStateManager stateManager = new ProcessorStateManager(taskId,
                                                                              Collections.singleton(changelogTopicPartition),
-                                                                             restoreConsumer,
                                                                              false,
                                                                              stateDirectory,
-                                                                             Collections.singletonMap(storeName, changelogTopic));
+                                                                             Collections.singletonMap(storeName, changelogTopic),
+                                                                             changelogReader);
 
         final MockStateStoreSupplier.MockStateStore stateStore = new MockStateStoreSupplier.MockStateStore(storeName, true) {
             @Override
@@ -802,8 +409,6 @@ public class ProcessorStateManagerTest {
                 throw new RuntimeException("KABOOM!");
             }
         };
-        stateManager.putOffsetLimit(changelogTopicPartition, 1);
-        restoreConsumer.bufferRecord(new ConsumerRecord<>(changelogTopic, 0, 1, 1, 1));
         stateManager.register(stateStore, false, stateStore.stateRestoreCallback);
 
         try {
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StandbyTaskTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StandbyTaskTest.java
index ef4ebcc7c6..31aa4a563e 100644
--- a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StandbyTaskTest.java
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StandbyTaskTest.java
@@ -30,12 +30,14 @@ import org.apache.kafka.common.serialization.IntegerSerializer;
 import org.apache.kafka.common.serialization.Serdes;
 import org.apache.kafka.common.serialization.Serializer;
 import org.apache.kafka.common.utils.MockTime;
+import org.apache.kafka.common.utils.Time;
 import org.apache.kafka.common.utils.Utils;
 import org.apache.kafka.streams.StreamsConfig;
 import org.apache.kafka.streams.kstream.KStreamBuilder;
 import org.apache.kafka.streams.processor.StateStore;
 import org.apache.kafka.streams.processor.TaskId;
 import org.apache.kafka.streams.state.internals.OffsetCheckpoint;
+import org.apache.kafka.test.MockRestoreConsumer;
 import org.apache.kafka.test.MockStateStoreSupplier;
 import org.apache.kafka.test.MockTimestampExtractor;
 import org.apache.kafka.test.TestUtils;
@@ -123,7 +125,8 @@ public class StandbyTaskTest {
     }
 
     private final MockConsumer<byte[], byte[]> consumer = new MockConsumer<>(OffsetResetStrategy.EARLIEST);
-    private final ProcessorStateManagerTest.MockRestoreConsumer restoreStateConsumer = new ProcessorStateManagerTest.MockRestoreConsumer();
+    private final MockRestoreConsumer restoreStateConsumer = new MockRestoreConsumer();
+    private final StoreChangelogReader changelogReader = new StoreChangelogReader(restoreStateConsumer, Time.SYSTEM, 5000);
 
     private final byte[] recordValue = intSerializer.serialize(null, 10);
     private final byte[] recordKey = intSerializer.serialize(null, 1);
@@ -154,7 +157,7 @@ public class StandbyTaskTest {
     @Test
     public void testStorePartitions() throws Exception {
         StreamsConfig config = createConfig(baseDir);
-        StandbyTask task = new StandbyTask(taskId, applicationId, topicPartitions, topology, consumer, restoreStateConsumer, config, null, stateDirectory);
+        StandbyTask task = new StandbyTask(taskId, applicationId, topicPartitions, topology, consumer, changelogReader, config, null, stateDirectory);
 
         assertEquals(Utils.mkSet(partition2), new HashSet<>(task.changeLogPartitions()));
 
@@ -164,7 +167,7 @@ public class StandbyTaskTest {
     @Test(expected = Exception.class)
     public void testUpdateNonPersistentStore() throws Exception {
         StreamsConfig config = createConfig(baseDir);
-        StandbyTask task = new StandbyTask(taskId, applicationId, topicPartitions, topology, consumer, restoreStateConsumer, config, null, stateDirectory);
+        StandbyTask task = new StandbyTask(taskId, applicationId, topicPartitions, topology, consumer, changelogReader, config, null, stateDirectory);
 
         restoreStateConsumer.assign(new ArrayList<>(task.changeLogPartitions()));
 
@@ -178,7 +181,7 @@ public class StandbyTaskTest {
     @Test
     public void testUpdate() throws Exception {
         StreamsConfig config = createConfig(baseDir);
-        StandbyTask task = new StandbyTask(taskId, applicationId, topicPartitions, topology, consumer, restoreStateConsumer, config, null, stateDirectory);
+        StandbyTask task = new StandbyTask(taskId, applicationId, topicPartitions, topology, consumer, changelogReader, config, null, stateDirectory);
 
         restoreStateConsumer.assign(new ArrayList<>(task.changeLogPartitions()));
 
@@ -236,7 +239,7 @@ public class StandbyTaskTest {
         ));
 
         StreamsConfig config = createConfig(baseDir);
-        StandbyTask task = new StandbyTask(taskId, applicationId, ktablePartitions, ktableTopology, consumer, restoreStateConsumer, config, null, stateDirectory);
+        StandbyTask task = new StandbyTask(taskId, applicationId, ktablePartitions, ktableTopology, consumer, changelogReader, config, null, stateDirectory);
 
         restoreStateConsumer.assign(new ArrayList<>(task.changeLogPartitions()));
 
@@ -329,9 +332,9 @@ public class StandbyTaskTest {
         builder.stream("topic").groupByKey().count("my-store");
         final ProcessorTopology topology = builder.setApplicationId(applicationId).build(0);
         StreamsConfig config = createConfig(baseDir);
-        new StandbyTask(taskId, applicationId, partitions, topology, consumer, restoreStateConsumer, config,
-                        new MockStreamsMetrics(new Metrics()), stateDirectory);
 
+        new StandbyTask(taskId, applicationId, partitions, topology, consumer, changelogReader, config,
+            new MockStreamsMetrics(new Metrics()), stateDirectory);
     }
 
     @Test
@@ -352,7 +355,7 @@ public class StandbyTaskTest {
                                                  ktablePartitions,
                                                  ktableTopology,
                                                  consumer,
-                                                 restoreStateConsumer,
+                                                 changelogReader,
                                                  config,
                                                  null,
                                                  stateDirectory
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StateRestorerTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StateRestorerTest.java
new file mode 100644
index 0000000000..d457887c44
--- /dev/null
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StateRestorerTest.java
@@ -0,0 +1,70 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.kafka.streams.processor.internals;
+
+import org.apache.kafka.common.TopicPartition;
+import org.apache.kafka.test.MockRestoreCallback;
+import org.junit.Test;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.MatcherAssert.assertThat;
+import static org.junit.Assert.assertTrue;
+
+public class StateRestorerTest {
+
+    private static final long OFFSET_LIMIT = 50;
+    private final MockRestoreCallback callback = new MockRestoreCallback();
+    private final StateRestorer restorer = new StateRestorer(new TopicPartition("topic", 1), callback, null, OFFSET_LIMIT, true);
+
+    @Test
+    public void shouldCallRestoreOnRestoreCallback() throws Exception {
+        restorer.restore(new byte[0], new byte[0]);
+        assertThat(callback.restoreCount, equalTo(1));
+    }
+
+    @Test
+    public void shouldBeCompletedIfRecordOffsetGreaterThanEndOffset() throws Exception {
+        assertTrue(restorer.hasCompleted(11, 10));
+    }
+
+    @Test
+    public void shouldBeCompletedIfRecordOffsetGreaterThanOffsetLimit() throws Exception {
+        assertTrue(restorer.hasCompleted(51, 100));
+    }
+
+    @Test
+    public void shouldBeCompletedIfEndOffsetAndRecordOffsetAreZero() throws Exception {
+        assertTrue(restorer.hasCompleted(0, 0));
+    }
+
+    @Test
+    public void shouldBeCompletedIfOffsetAndOffsetLimitAreZero() throws Exception {
+        final StateRestorer restorer = new StateRestorer(new TopicPartition("topic", 1), callback, null, 0, true);
+        assertTrue(restorer.hasCompleted(0, 10));
+    }
+
+    @Test
+    public void shouldSetRestoredOffsetToMinOfLimitAndOffset() throws Exception {
+        restorer.setRestoredOffset(20);
+        assertThat(restorer.restoredOffset(), equalTo(20L));
+        restorer.setRestoredOffset(100);
+        assertThat(restorer.restoredOffset(), equalTo(OFFSET_LIMIT));
+    }
+
+
+}
\ No newline at end of file
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java
new file mode 100644
index 0000000000..d6fae66bd7
--- /dev/null
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java
@@ -0,0 +1,258 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.kafka.streams.processor.internals;
+
+import org.apache.kafka.clients.consumer.ConsumerRecord;
+import org.apache.kafka.clients.consumer.MockConsumer;
+import org.apache.kafka.clients.consumer.OffsetResetStrategy;
+import org.apache.kafka.common.PartitionInfo;
+import org.apache.kafka.common.TopicPartition;
+import org.apache.kafka.common.errors.TimeoutException;
+import org.apache.kafka.common.utils.MockTime;
+import org.apache.kafka.common.utils.Time;
+import org.apache.kafka.streams.errors.StreamsException;
+import org.apache.kafka.test.MockRestoreCallback;
+import org.junit.Test;
+
+import java.util.Collections;
+import java.util.List;
+import java.util.Map;
+
+import static org.hamcrest.MatcherAssert.assertThat;
+import static org.hamcrest.core.IsEqual.equalTo;
+import static org.junit.Assert.fail;
+
+public class StoreChangelogReaderTest {
+
+    private final MockRestoreCallback callback = new MockRestoreCallback();
+    private MockConsumer<byte[], byte[]> consumer = new MockConsumer<>(OffsetResetStrategy.EARLIEST);
+    private StoreChangelogReader changelogReader = new StoreChangelogReader(consumer, new MockTime(), 0);
+    private final TopicPartition topicPartition = new TopicPartition("topic", 0);
+    private final PartitionInfo partitionInfo = new PartitionInfo(topicPartition.topic(), 0, null, null, null);
+
+    @SuppressWarnings("unchecked")
+    @Test
+    public void shouldThrowStreamsExceptionWhenTimeoutExceptionThrown() throws Exception {
+        final MockConsumer<byte[], byte[]> consumer = new MockConsumer(OffsetResetStrategy.EARLIEST) {
+            @Override
+            public Map<String, List<PartitionInfo>> listTopics() {
+                throw new TimeoutException("KABOOM!");
+            }
+        };
+        final StoreChangelogReader changelogReader = new StoreChangelogReader(consumer, new MockTime(), 0);
+        try {
+            changelogReader.validatePartitionExists(topicPartition, "store");
+            fail("Should have thrown streams exception");
+        } catch (final StreamsException e) {
+            // pass
+        }
+    }
+
+    @Test(expected = StreamsException.class)
+    public void shouldThrowStreamsExceptionIfPartitionDoesntExistAfterMaxWait() throws Exception {
+        changelogReader.validatePartitionExists(topicPartition, "store");
+    }
+
+    @SuppressWarnings("unchecked")
+    @Test
+    public void shouldFallbackToPartitionsForIfPartitionNotInAllPartitionsList() throws Exception {
+        final MockConsumer<byte[], byte[]> consumer = new MockConsumer(OffsetResetStrategy.EARLIEST) {
+            @Override
+            public List<PartitionInfo> partitionsFor(final String topic) {
+                return Collections.singletonList(partitionInfo);
+            }
+        };
+
+        final StoreChangelogReader changelogReader = new StoreChangelogReader(consumer, new MockTime(), 10);
+        changelogReader.validatePartitionExists(topicPartition, "store");
+    }
+
+    @SuppressWarnings("unchecked")
+    @Test
+    public void shouldThrowStreamsExceptionIfTimeoutOccursDuringPartitionsFor() throws Exception {
+        final MockConsumer<byte[], byte[]> consumer = new MockConsumer(OffsetResetStrategy.EARLIEST) {
+            @Override
+            public List<PartitionInfo> partitionsFor(final String topic) {
+                throw new TimeoutException("KABOOM!");
+            }
+        };
+        final StoreChangelogReader changelogReader = new StoreChangelogReader(consumer, new MockTime(), 5);
+        try {
+            changelogReader.validatePartitionExists(topicPartition, "store");
+            fail("Should have thrown streams exception");
+        } catch (final StreamsException e) {
+            // pass
+        }
+    }
+
+    @Test
+    public void shouldPassIfTopicPartitionExists() throws Exception {
+        consumer.updatePartitions(topicPartition.topic(), Collections.singletonList(partitionInfo));
+        changelogReader.validatePartitionExists(topicPartition, "store");
+    }
+
+    @SuppressWarnings("unchecked")
+    @Test
+    public void shouldRequestPartitionInfoIfItDoesntExist() throws Exception {
+        final MockConsumer<byte[], byte[]> consumer = new MockConsumer(OffsetResetStrategy.EARLIEST) {
+            @Override
+            public Map<String, List<PartitionInfo>> listTopics() {
+                return Collections.emptyMap();
+            }
+        };
+
+        consumer.updatePartitions(topicPartition.topic(), Collections.singletonList(partitionInfo));
+        final StoreChangelogReader changelogReader = new StoreChangelogReader(consumer, Time.SYSTEM, 5000);
+        changelogReader.validatePartitionExists(topicPartition, "store");
+    }
+
+
+    @Test
+    public void shouldThrowExceptionIfConsumerHasCurrentSubscription() throws Exception {
+        consumer.subscribe(Collections.singleton("sometopic"));
+        try {
+            changelogReader.restore();
+            fail("Should have thrown IllegalStateException");
+        } catch (final IllegalStateException e) {
+            // ok
+        }
+    }
+
+    @Test
+    public void shouldRestoreAllMessagesFromBeginningWhenCheckpointNull() throws Exception {
+        final int messages = 10;
+        setupConsumer(messages, topicPartition);
+        changelogReader.register(new StateRestorer(topicPartition, callback, null, Long.MAX_VALUE, true));
+
+        changelogReader.restore();
+        assertThat(callback.restoreCount, equalTo(messages));
+    }
+
+    @Test
+    public void shouldRestoreMessagesFromCheckpoint() throws Exception {
+        final int messages = 10;
+        setupConsumer(messages, topicPartition);
+        changelogReader.register(new StateRestorer(topicPartition, callback, 5L, Long.MAX_VALUE, true));
+
+        changelogReader.restore();
+        assertThat(callback.restoreCount, equalTo(5));
+    }
+
+    @Test
+    public void shouldClearAssignmentAtEndOfRestore() throws Exception {
+        final int messages = 1;
+        setupConsumer(messages, topicPartition);
+        changelogReader.register(new StateRestorer(topicPartition, callback, null, Long.MAX_VALUE, true));
+
+        changelogReader.restore();
+        assertThat(consumer.assignment(), equalTo(Collections.<TopicPartition>emptySet()));
+    }
+
+    @Test
+    public void shouldRestoreToLimitWhenSupplied() throws Exception {
+        setupConsumer(10, topicPartition);
+        final StateRestorer restorer = new StateRestorer(topicPartition, callback, null, 3, true);
+        changelogReader.register(restorer);
+
+        changelogReader.restore();
+        assertThat(callback.restoreCount, equalTo(3));
+        assertThat(restorer.restoredOffset(), equalTo(3L));
+    }
+
+    @Test
+    public void shouldRestoreMultipleStores() throws Exception {
+        final TopicPartition one = new TopicPartition("one", 0);
+        final TopicPartition two = new TopicPartition("two", 0);
+        final MockRestoreCallback callbackOne = new MockRestoreCallback();
+        final MockRestoreCallback callbackTwo = new MockRestoreCallback();
+        setupConsumer(10, topicPartition);
+        setupConsumer(5, one);
+        setupConsumer(3, two);
+
+        changelogReader.register(new StateRestorer(topicPartition, callback, null, Long.MAX_VALUE, true));
+        changelogReader.register(new StateRestorer(one, callbackOne, null, Long.MAX_VALUE, true));
+        changelogReader.register(new StateRestorer(two, callbackTwo, null, Long.MAX_VALUE, true));
+
+        changelogReader.restore();
+
+        assertThat(callback.restoreCount, equalTo(10));
+        assertThat(callbackOne.restoreCount, equalTo(5));
+        assertThat(callbackTwo.restoreCount, equalTo(3));
+    }
+
+    @Test
+    public void shouldNotRestoreAnythingWhenPartitionIsEmpty() throws Exception {
+        final StateRestorer restorer = new StateRestorer(topicPartition, callback, null, Long.MAX_VALUE, true);
+        setupConsumer(0, topicPartition);
+        changelogReader.register(restorer);
+
+        changelogReader.restore();
+        assertThat(callback.restoreCount, equalTo(0));
+        assertThat(restorer.restoredOffset(), equalTo(0L));
+    }
+
+    @Test
+    public void shouldNotRestoreAnythingWhenCheckpointAtEndOffset() throws Exception {
+        final Long endOffset = 10L;
+        setupConsumer(endOffset, topicPartition);
+        final StateRestorer restorer = new StateRestorer(topicPartition, callback, endOffset, Long.MAX_VALUE, true);
+
+        changelogReader.register(restorer);
+
+        changelogReader.restore();
+        assertThat(callback.restoreCount, equalTo(0));
+        assertThat(restorer.restoredOffset(), equalTo(endOffset));
+    }
+
+    @Test
+    public void shouldReturnRestoredOffsetsForPersistentStores() throws Exception {
+        setupConsumer(10, topicPartition);
+        changelogReader.register(new StateRestorer(topicPartition, callback, null, Long.MAX_VALUE, true));
+        changelogReader.restore();
+        final Map<TopicPartition, Long> restoredOffsets = changelogReader.restoredOffsets();
+        assertThat(restoredOffsets, equalTo(Collections.singletonMap(topicPartition, 10L)));
+    }
+
+    @Test
+    public void shouldNotReturnRestoredOffsetsForNonPersistentStore() throws Exception {
+        setupConsumer(10, topicPartition);
+        changelogReader.register(new StateRestorer(topicPartition, callback, null, Long.MAX_VALUE, false));
+        changelogReader.restore();
+        final Map<TopicPartition, Long> restoredOffsets = changelogReader.restoredOffsets();
+        assertThat(restoredOffsets, equalTo(Collections.<TopicPartition, Long>emptyMap()));
+    }
+
+    private void setupConsumer(final long messages, final TopicPartition topicPartition) {
+        consumer.updatePartitions(topicPartition.topic(),
+                                  Collections.singletonList(
+                                          new PartitionInfo(topicPartition.topic(),
+                                                            topicPartition.partition(),
+                                                            null,
+                                                            null,
+                                                            null)));
+        consumer.updateBeginningOffsets(Collections.singletonMap(topicPartition, 0L));
+        consumer.updateEndOffsets(Collections.singletonMap(topicPartition, Math.max(0, messages)));
+        consumer.assign(Collections.singletonList(topicPartition));
+
+        for (int i = 0; i < messages; i++) {
+            consumer.addRecord(new ConsumerRecord<>(topicPartition.topic(), topicPartition.partition(), i, new byte[0], new byte[0]));
+        }
+        consumer.assign(Collections.<TopicPartition>emptyList());
+    }
+
+}
\ No newline at end of file
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java
index 5c72fc9bff..14fda49d6c 100644
--- a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java
@@ -33,6 +33,7 @@ import org.apache.kafka.common.serialization.IntegerDeserializer;
 import org.apache.kafka.common.serialization.IntegerSerializer;
 import org.apache.kafka.common.serialization.Serializer;
 import org.apache.kafka.common.utils.MockTime;
+import org.apache.kafka.common.utils.Time;
 import org.apache.kafka.common.utils.Utils;
 import org.apache.kafka.streams.StreamsConfig;
 import org.apache.kafka.streams.StreamsMetrics;
@@ -103,6 +104,7 @@ public class StreamTaskTest {
     private final MockConsumer<byte[], byte[]> consumer = new MockConsumer<>(OffsetResetStrategy.EARLIEST);
     private final MockProducer<byte[], byte[]> producer = new MockProducer<>(false, bytesSerializer, bytesSerializer);
     private final MockConsumer<byte[], byte[]> restoreStateConsumer = new MockConsumer<>(OffsetResetStrategy.EARLIEST);
+    private final StoreChangelogReader changelogReader = new StoreChangelogReader(restoreStateConsumer, Time.SYSTEM, 5000);
     private final byte[] recordValue = intSerializer.serialize(null, 10);
     private final byte[] recordKey = intSerializer.serialize(null, 1);
     private final String applicationId = "applicationId";
@@ -141,7 +143,7 @@ public class StreamTaskTest {
         config = createConfig(baseDir);
         stateDirectory = new StateDirectory("applicationId", baseDir.getPath(), new MockTime());
         task = new StreamTask(taskId00, applicationId, partitions, topology, consumer,
-                              restoreStateConsumer, config, streamsMetrics, stateDirectory, null, time, recordCollector);
+                              changelogReader, config, streamsMetrics, stateDirectory, null, time, recordCollector);
     }
 
     @After
@@ -352,7 +354,7 @@ public class StreamTaskTest {
         task.close();
 
         task  = new StreamTask(taskId00, applicationId, partitions,
-                                                     topology, consumer, restoreStateConsumer, config, streamsMetrics, stateDirectory, testCache, time, recordCollector);
+                                                     topology, consumer, changelogReader, config, streamsMetrics, stateDirectory, testCache, time, recordCollector);
         final int offset = 20;
         task.addRecords(partition1, Collections.singletonList(
                 new ConsumerRecord<>(partition1.topic(), partition1.partition(), offset, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, recordKey, recordValue)));
@@ -413,7 +415,7 @@ public class StreamTaskTest {
         };
         final StreamsMetrics streamsMetrics = new MockStreamsMetrics(new Metrics());
         final StreamTask streamTask = new StreamTask(taskId00, "appId", partitions, topology, consumer,
-                                                     restoreStateConsumer, createConfig(baseDir), streamsMetrics,
+                                                     changelogReader, createConfig(baseDir), streamsMetrics,
                                                      stateDirectory, testCache, time, recordCollector);
         streamTask.flushState();
         assertTrue(flushed.get());
@@ -462,7 +464,7 @@ public class StreamTaskTest {
         final MockTime time = new MockTime();
         final StreamsConfig config = createConfig(baseDir);
         final StreamTask streamTask = new StreamTask(taskId, "appId", partitions, topology, consumer,
-                                                     restoreStateConsumer, config, streamsMetrics,
+                                                     changelogReader, config, streamsMetrics,
                                                      stateDirectory, new ThreadCache("testCache", 0, streamsMetrics),
                                                      time, recordCollector);
 
@@ -558,7 +560,7 @@ public class StreamTaskTest {
 
 
         return new StreamTask(taskId00, applicationId, partitions,
-                              topology, consumer, restoreStateConsumer, config, streamsMetrics, stateDirectory, testCache, time, recordCollector);
+                              topology, consumer, changelogReader, config, streamsMetrics, stateDirectory, testCache, time, recordCollector);
     }
 
     private Iterable<ConsumerRecord<byte[], byte[]>> records(ConsumerRecord<byte[], byte[]>... recs) {
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java
index ef8dc92110..38797100f4 100644
--- a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java
@@ -159,8 +159,8 @@ public class StreamThreadTest {
                               StreamsConfig config,
                               StreamsMetrics metrics,
                               StateDirectory stateDirectory) {
-            super(id, applicationId, partitions, topology, consumer, restoreConsumer, config, metrics,
-                stateDirectory, null, new MockTime(), new RecordCollectorImpl(producer, id.toString()));
+            super(id, applicationId, partitions, topology, consumer, new StoreChangelogReader(restoreConsumer, Time.SYSTEM, 5000), config, metrics,
+                  stateDirectory, null, new MockTime(), new RecordCollectorImpl(producer, id.toString()));
         }
 
         @Override
diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/StreamThreadStateStoreProviderTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/StreamThreadStateStoreProviderTest.java
index dfe8d8b3a3..007cf7fd24 100644
--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/StreamThreadStateStoreProviderTest.java
+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/StreamThreadStateStoreProviderTest.java
@@ -27,6 +27,7 @@ import org.apache.kafka.streams.processor.TaskId;
 import org.apache.kafka.streams.processor.TopologyBuilder;
 import org.apache.kafka.streams.processor.internals.ProcessorTopology;
 import org.apache.kafka.streams.processor.internals.StateDirectory;
+import org.apache.kafka.streams.processor.internals.StoreChangelogReader;
 import org.apache.kafka.streams.processor.internals.StreamTask;
 import org.apache.kafka.streams.processor.internals.StreamThread;
 import org.apache.kafka.streams.processor.internals.StreamsMetadataState;
@@ -187,7 +188,7 @@ public class StreamThreadStateStoreProviderTest {
         return new StreamTask(taskId, applicationId, Collections
                 .singletonList(new TopicPartition("topic", taskId.partition)), topology,
                               clientSupplier.consumer,
-                              clientSupplier.restoreConsumer,
+                              new StoreChangelogReader(clientSupplier.restoreConsumer, Time.SYSTEM, 5000),
                               streamsConfig, new MockStreamsMetrics(new Metrics()), stateDirectory, null, new MockTime(), new NoOpRecordCollector()) {
             @Override
             protected void initializeOffsetLimits() {
diff --git a/streams/src/test/java/org/apache/kafka/test/MockChangelogReader.java b/streams/src/test/java/org/apache/kafka/test/MockChangelogReader.java
new file mode 100644
index 0000000000..afa87a896a
--- /dev/null
+++ b/streams/src/test/java/org/apache/kafka/test/MockChangelogReader.java
@@ -0,0 +1,54 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.test;
+
+import org.apache.kafka.common.TopicPartition;
+import org.apache.kafka.streams.processor.internals.ChangelogReader;
+import org.apache.kafka.streams.processor.internals.StateRestorer;
+
+import java.util.Collections;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+
+public class MockChangelogReader implements ChangelogReader {
+    private final Set<TopicPartition> registered = new HashSet<>();
+
+    @Override
+    public void validatePartitionExists(final TopicPartition topicPartition, final String storeName) {
+
+    }
+
+    @Override
+    public void register(final StateRestorer restorationInfo) {
+        registered.add(restorationInfo.partition());
+    }
+
+    @Override
+    public void restore() {
+
+    }
+
+    @Override
+    public Map<TopicPartition, Long> restoredOffsets() {
+        return Collections.emptyMap();
+    }
+
+    public boolean wasRegistered(final TopicPartition partition) {
+        return registered.contains(partition);
+    }
+}
diff --git a/streams/src/test/java/org/apache/kafka/test/MockRestoreCallback.java b/streams/src/test/java/org/apache/kafka/test/MockRestoreCallback.java
new file mode 100644
index 0000000000..f61048eb5d
--- /dev/null
+++ b/streams/src/test/java/org/apache/kafka/test/MockRestoreCallback.java
@@ -0,0 +1,28 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.test;
+
+import org.apache.kafka.streams.processor.StateRestoreCallback;
+
+public class MockRestoreCallback implements StateRestoreCallback {
+    public int restoreCount = 0;
+
+    @Override
+    public void restore(final byte[] key, final byte[] value) {
+        restoreCount++;
+    }
+}
diff --git a/streams/src/test/java/org/apache/kafka/test/MockRestoreConsumer.java b/streams/src/test/java/org/apache/kafka/test/MockRestoreConsumer.java
new file mode 100644
index 0000000000..2178c16949
--- /dev/null
+++ b/streams/src/test/java/org/apache/kafka/test/MockRestoreConsumer.java
@@ -0,0 +1,155 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.test;
+
+import org.apache.kafka.clients.consumer.ConsumerRecord;
+import org.apache.kafka.clients.consumer.ConsumerRecords;
+import org.apache.kafka.clients.consumer.MockConsumer;
+import org.apache.kafka.clients.consumer.OffsetResetStrategy;
+import org.apache.kafka.common.TopicPartition;
+import org.apache.kafka.common.record.TimestampType;
+import org.apache.kafka.common.serialization.IntegerSerializer;
+import org.apache.kafka.common.serialization.Serializer;
+
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.Map;
+
+public class MockRestoreConsumer extends MockConsumer<byte[], byte[]> {
+    private final Serializer<Integer> serializer = new IntegerSerializer();
+
+    private TopicPartition assignedPartition = null;
+    private long seekOffset = -1L;
+    private long endOffset = 0L;
+    private long currentOffset = 0L;
+
+    private ArrayList<ConsumerRecord<byte[], byte[]>> recordBuffer = new ArrayList<>();
+
+    public MockRestoreConsumer() {
+        super(OffsetResetStrategy.EARLIEST);
+
+        reset();
+    }
+
+    // reset this mock restore consumer for a state store registration
+    public void reset() {
+        assignedPartition = null;
+        seekOffset = -1L;
+        endOffset = 0L;
+        recordBuffer.clear();
+    }
+
+    // buffer a record (we cannot use addRecord because we need to add records before assigning a partition)
+    public void bufferRecord(ConsumerRecord<Integer, Integer> record) {
+        recordBuffer.add(
+            new ConsumerRecord<>(record.topic(), record.partition(), record.offset(), 0L,
+                                 TimestampType.CREATE_TIME, 0L, 0, 0,
+                                 serializer.serialize(record.topic(), record.key()),
+                                 serializer.serialize(record.topic(), record.value())));
+        endOffset = record.offset();
+
+        super.updateEndOffsets(Collections.singletonMap(assignedPartition, endOffset));
+    }
+
+    @Override
+    public synchronized void assign(Collection<TopicPartition> partitions) {
+        int numPartitions = partitions.size();
+        if (numPartitions > 1)
+            throw new IllegalArgumentException("RestoreConsumer: more than one partition specified");
+
+        if (numPartitions == 1) {
+            if (assignedPartition != null)
+                throw new IllegalStateException("RestoreConsumer: partition already assigned");
+            assignedPartition = partitions.iterator().next();
+
+            // set the beginning offset to 0
+            // NOTE: this is users responsible to set the initial lEO.
+            super.updateBeginningOffsets(Collections.singletonMap(assignedPartition, 0L));
+        }
+
+        super.assign(partitions);
+    }
+
+    @Override
+    public ConsumerRecords<byte[], byte[]> poll(long timeout) {
+        // add buffered records to MockConsumer
+        for (ConsumerRecord<byte[], byte[]> record : recordBuffer) {
+            super.addRecord(record);
+        }
+        recordBuffer.clear();
+
+        ConsumerRecords<byte[], byte[]> records = super.poll(timeout);
+
+        // set the current offset
+        Iterable<ConsumerRecord<byte[], byte[]>> partitionRecords = records.records(assignedPartition);
+        for (ConsumerRecord<byte[], byte[]> record : partitionRecords) {
+            currentOffset = record.offset();
+        }
+
+        return records;
+    }
+
+    @Override
+    public synchronized long position(TopicPartition partition) {
+        if (!partition.equals(assignedPartition))
+            throw new IllegalStateException("RestoreConsumer: unassigned partition");
+
+        return currentOffset;
+    }
+
+    @Override
+    public synchronized void seek(TopicPartition partition, long offset) {
+        if (offset < 0)
+            throw new IllegalArgumentException("RestoreConsumer: offset should not be negative");
+
+        if (seekOffset >= 0)
+            throw new IllegalStateException("RestoreConsumer: offset already seeked");
+
+        seekOffset = offset;
+        currentOffset = offset;
+        super.seek(partition, offset);
+    }
+
+    @Override
+    public synchronized void seekToBeginning(Collection<TopicPartition> partitions) {
+        if (partitions.size() != 1)
+            throw new IllegalStateException("RestoreConsumer: other than one partition specified");
+
+        for (TopicPartition partition : partitions) {
+            if (!partition.equals(assignedPartition))
+                throw new IllegalStateException("RestoreConsumer: seek-to-end not on the assigned partition");
+        }
+
+        currentOffset = 0L;
+    }
+
+
+    @Override
+    public Map<TopicPartition, Long> endOffsets(final Collection<TopicPartition> partitions) {
+        if (partitions.size() != 1)
+            throw new IllegalStateException("RestoreConsumer: other than one partition specified");
+
+        for (TopicPartition partition : partitions) {
+            if (!partition.equals(assignedPartition))
+                throw new IllegalStateException("RestoreConsumer: seek-to-end not on the assigned partition");
+        }
+
+        currentOffset = endOffset;
+        return super.endOffsets(partitions);
+    }
+}
diff --git a/streams/src/test/java/org/apache/kafka/test/ProcessorTopologyTestDriver.java b/streams/src/test/java/org/apache/kafka/test/ProcessorTopologyTestDriver.java
index ac8933dcb6..842f30cd3e 100644
--- a/streams/src/test/java/org/apache/kafka/test/ProcessorTopologyTestDriver.java
+++ b/streams/src/test/java/org/apache/kafka/test/ProcessorTopologyTestDriver.java
@@ -56,6 +56,7 @@ import org.apache.kafka.streams.processor.internals.ProcessorStateManager;
 import org.apache.kafka.streams.processor.internals.ProcessorTopology;
 import org.apache.kafka.streams.processor.internals.RecordCollectorImpl;
 import org.apache.kafka.streams.processor.internals.StateDirectory;
+import org.apache.kafka.streams.processor.internals.StoreChangelogReader;
 import org.apache.kafka.streams.processor.internals.StreamTask;
 import org.apache.kafka.streams.processor.internals.MockStreamsMetrics;
 import org.apache.kafka.streams.state.KeyValueStore;
@@ -222,7 +223,7 @@ public class ProcessorTopologyTestDriver {
                                   partitionsByTopic.values(),
                                   topology,
                                   consumer,
-                                  restoreStateConsumer,
+                                  new StoreChangelogReader(restoreStateConsumer, Time.SYSTEM, 5000),
                                   config,
                                   streamsMetrics, stateDirectory,
                                   cache,
