diff --git a/core/src/main/scala/kafka/server/ReplicaManager.scala b/core/src/main/scala/kafka/server/ReplicaManager.scala
index 1044085aec..4e6c8ea746 100644
--- a/core/src/main/scala/kafka/server/ReplicaManager.scala
+++ b/core/src/main/scala/kafka/server/ReplicaManager.scala
@@ -49,6 +49,7 @@ class ReplicaManager(val config: KafkaConfig,
   this.logIdent = "Replica Manager on Broker " + config.brokerId + ": "
   private val highWatermarkCheckPointThreadStarted = new AtomicBoolean(false)
   val highWatermarkCheckpoints = config.logDirs.map(dir => (dir, new HighwaterMarkCheckpoint(dir))).toMap
+  private var hwThreadInitialized = false
 
   newGauge(
     "LeaderCount",
@@ -92,8 +93,6 @@ class ReplicaManager(val config: KafkaConfig,
   def startup() {
     // start ISR expiration thread
     kafkaScheduler.scheduleWithRate(maybeShrinkIsr, "isr-expiration-thread-", 0, config.replicaLagTimeMaxMs)
-    // start high watermark checkpoint thread
-    startHighWaterMarksCheckPointThread()
   }
 
   def stopReplica(topic: String, partitionId: Int, deletePartition: Boolean): Short  = {
@@ -209,6 +208,12 @@ class ReplicaManager(val config: KafkaConfig,
         responseMap.put(topicAndPartition, errorCode)
       }
       info("Completed leader and isr request %s".format(leaderAndISRRequest))
+      // we initialize highwatermark thread after the first leaderisrrequest. This ensures that all the partitions
+      // have been completely populated before starting the checkpointing there by avoiding weird race conditions
+      if (!hwThreadInitialized) {
+        startHighWaterMarksCheckPointThread()
+        hwThreadInitialized = true
+      }
       replicaFetcherManager.shutdownIdleFetcherThreads()
       (responseMap, ErrorMapping.NoError)
     }
